[{"type": "text", "text": "Harnessing small projectors and multiple views for efficient vision pretraining ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Arna Ghosh \u22171 Kumar Krishna Agrawal \u22172 Shagun Sodhani 3 ", "page_idx": 0}, {"type": "text", "text": "Adam M. Oberman\u2020 4 Blake A. Richards \u2020 1 5 6 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent progress in self-supervised (SSL) visual representation learning has led to the development of several different proposed frameworks that rely on augmentations of images but use different loss functions. However, there are few theoretically grounded principles to guide practice, so practical implementation of each SSL framework requires several heuristics to achieve competitive performance. In this work, we build on recent analytical results to design practical recommendations for competitive and efficient SSL that are grounded in theory. Specifically, recent theory tells us that existing SSL frameworks are actually minimizing the same idealized loss, which is to learn features that best match the data similarity kernel defined by the augmentations used. We show how this idealized loss can be reformulated to a functionally equivalent loss that is more efficient to compute. We study the implicit bias of using gradient descent to minimize our reformulated loss function, and find that using a stronger orthogonalization constraint with a reduced projector dimensionality should yield good representations. Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence. We empirically verify our findings on CIFAR, STL and Imagenet datasets, wherein we demonstrate an improved linear readout performance when training a ResNet-backbone using our theoretically grounded recommendations. Remarkably, we also demonstrate that by leveraging these insights, we can reduce the pretraining dataset size by up to $2\\times$ while maintaining downstream accuracy simply by using more data augmentations. Taken together, our work provides theoretically grounded recommendations that can be used to improve SSL convergence and efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unsupervised representation learning, i.e., learning features without human-annotated labels, is critical for progress in computer vision. Modern approaches, grouped under the self-supervised learning (SSL) umbrella, build on the core insight that similar images should map to nearby points in the learned feature space \u2013 often termed as the invariance criterion. Current SSL methods can be broadly categorized into contrastive and non-contrastive algorithms, based on whether they formulate their loss functions using negative samples or not, respectively. ", "page_idx": 0}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/27c98e55a836f2951bd51170c4a53961241a9c69afc5826f943078d76f9240cc.jpg", "img_caption": ["Figure 1: Design of existing SSL algorithms relies on heuristics. (A) Augmentation graphs are common in vision pretraining, providing generalizable features for downstream tasks. (B) We propose an equivalent loss function for SSL pretraining that recovers the same eigenfunctions more efficiently than existing approaches. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Despite this difference in their loss formulations, recent theoretical work has established an equivalence between the contrastive and non-contrastive SSL frameworks [19]. This work shows that these different SSL formulations are ultimately minimizing a loss that encourages the learning of features that best match the data similarity kernel defined by the augmentations used. However, this notion of theoretical equivalence holds only in the limit of ideal pretraining settings, i.e. with access to infinite data and compute budget, and the feature learning behavior of different SSL algorithms in practical scenarios is still not well understood. Therefore, researchers often use empirically driven heuristics that are theoretically ungrounded to design successful applications, such as (i) a high-dimensional projector head for non-contrastive SSL or (ii) the use of two augmentations per image [3]. Moreover, existing SSL algorithms are extremely data-hungry, relying on large-scale datasets [33] or data engines [30] to achieve good representations. While this strategy works exceptionally well in data-rich settings (like training on natural-images), it is not viable in data-constrained settings (like medical imaging), where samples are relatively scarce. ", "page_idx": 1}, {"type": "text", "text": "With these challenges in mind, the primary focus of this work is to develop theoretically grounded recommendations for improving the effectiveness and efficiency of feature learning, both with respect to the required compute budget as well as data points. Like any unsupervised representation learning algorithm, features learned through SSL depend on three factors: (i) implicit bias of the architecture, (ii) explicit invariance imposed by data augmentations, (iii) implicit bias of the learning rule. While previous works predominantly studied the role of the model architecture capacity and loss function, and their interplay with data augmentations [9, 44], our approach broadens this perspective by also considering the role of the learning rule (gradient descent) in optimizing these loss functions. Specifically, we extend the previous theoretical findings [44] that unified the desiderata of different SSL algorithms. We reformulate the idealized unifying loss to propose a functionally equivalent loss that is more compute-efficient (see Figure 1). Based on our loss formulation, we provide two practical recommendations that can help improve the efficiency of SSL pipelines while maintaining good performance. First, we show that optimizing the reformulated loss using gradient descent can often reduce the orthogonality among the learned embeddings, thereby leading to an inefficient use of the projector network\u2019s capacity. Consequently, we recommend using a stronger orthogonalization constraint to eliminate the requirement of high-dimensional projector heads, thereby significantly reducing the parameter overhead of good feature learning. Second, we show that increasing the number of augmentations leads to a better estimate of the data similarity kernel. Consequently, we recommend using more augmentations to improve optimization convergence and learn better features earlier in training. ", "page_idx": 1}, {"type": "text", "text": "We empirically verify our theoretically grounded recommendations using the popular ResNet backbone on benchmark datasets: CIFAR, STL and Imagenet. Strikingly, we show that our multiaugmentation approach can learn good features even with half of the samples in the pretraining dataset. Our recommendations provide a path towards making SSL pretraining more data and compute-efficient without harming performance and could unlock massive performance gains in data-constrained setups. In summary, our core contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Efficient SSL loss formulation: We propose an functionally equivalent and computeefficient formulation of the SSL desiderata that yields the eigenfunctions of the augmentationdefined data similarity kernel.   \n\u2022 Role of heuristics: Based on our loss formulation and the implicit bias of gradient descent in optimizing this loss, we provide a mechanistic explanation for the role of projector dimensionality and the number of data augmentations. Consequently, we empirically demonstrate that low-dimensional projector heads are sufficient and that using more augmentations leads to learning better representations.   \n\u2022 Data efficient SSL: Leveraging the convergence benefits of the multi-augmentation SSL framework, we empirically demonstrate that we can learn good features with significantly smaller datasets (up to $2\\times$ ) without harming downstream linear probe performance. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Existing SSL approaches in computer vision In recent years machine learning researchers have developed a number of effective approaches for learning from data without labels. The most popular approaches use augmentations of data points as targets for themselves. One of the first was a Simple framework for Contrastive Learning (SimCLR), which relied on an infoNCE loss with augmentations of an image as positive targets and augmentations of other images as negative samples (to construct the contrastive loss) [12]. Other works have relied on non-contrastive approaches, notably BarlowTwins [43] and VICReg [5]. BarlowsTwins, which was inspired by the ideas of the neuroscientist Horace Barlow (cite), also uses augmentations of images, but it instead aims to optimize the covariance structure of the representations in order to reduce redundancies in the feature space [43]. Variance Invarance Covariance Regularization (VICReg) was a modification of BarlowTwins that added a variance term in the loss in order to ensure that every feature dimension has a finite variance [5]. In this paper we will focus on non-contrastive methods like BarlowTwins and VICReg, but in line with previous work [44], we also consider how these approaches relate to contrastive methods like SimCLR. ", "page_idx": 2}, {"type": "text", "text": "Formalizing the self-supervised learning problem Now, we will the unsupervised representation learning problem for computer vision. In particular, we assume access to a dataset $\\bar{D}\\,=\\,\\{x_{1},x_{2},\\bar{...,}x_{n}\\}$ with $x_{i}\\,\\in\\,\\mathbb{R}^{p}$ consisting of unlabeled images. The objective is to learn a $d$ -dimensional representation $[d<p)$ that is useful across multiple downstream applications. We focus on learning the parameters of a deep neural network $f_{\\theta}\\in\\mathcal{F}_{\\Theta}$ , using the multi-augmentation SSL framework, wherein multiple views of an image are used to optimize the pretraining loss function, $\\mathcal{L}_{p r e t r a i n}(f_{\\theta},\\mathcal{D})$ ", "page_idx": 2}, {"type": "text", "text": "Non-Contrastive Self-Supervised Learning (NC-SSL) algorithms impose invariance to data augmentations, while imposing regularization on the geometry of the learned feature space. More generally, $\\mathcal{L}_{p r e t r a i n}$ can be formulated with two terms (i) $\\mathcal{L}_{i n v a r i a n c e}$ : to learn invariance to data augmentations and (ii) $\\mathcal{L}_{c o l l a p s e}$ : regularization to prevent collapsing the feature space to a trivial solution. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p r e t r a i n}:=\\mathcal{L}_{i n v a r i a n c e}+\\beta\\mathcal{L}_{c o l l a p s e}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\beta$ denotes a hyperparameter that controls the importance of the collapse-preventing term relative to the invariance term. This formulation separates features that are invariant to the augmentations from those that are sensitive to them. Intuitively, the ideal feature space is more sensitive to semantic attributes (e.g. \u201cthat\u2019s a dog\u201d) and less sensitive to irrelevant attributes (e.g. \u201cdirection the dog is facing\u201d), facilitating generalization to new examples. ", "page_idx": 2}, {"type": "text", "text": "Data Augmentation graph was introduced by [22] to analyze contrastive losses, like SimCLR [12]. Briefly, we define a graph $\\mathcal{G}(A,\\mathcal{W})$ that captures the relationship between images derived from all possible data augmentations. The vertex set $({\\mathcal{A}},\\rho_{A})$ is each augmented sample in a dataset, $\\mathcal{X}$ , and the adjacency matrix, $\\mathcal{W}$ , denotes the similarity between pairs of vertices. Let $x_{0}$ be an image in $\\mathcal{X}$ , and let $z={\\dot{M}}(x_{0})\\in{\\mathcal{A}}$ be a random data augmentation of the image, $x_{0}$ . We define the probability density of reaching $z$ from $x_{0}$ via a choice of mapping $M$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\np({\\boldsymbol{z}}\\mid{\\boldsymbol{x}}_{0})=\\mathbb{P}({\\boldsymbol{z}}=M({\\boldsymbol{x}}_{0})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since the mapping is not generally invertible (e.g., cropping), we observe that $p(x_{0}\\mid z)\\neq p(z\\mid x_{0})$ . Using this definition, we now formally define the strength of the edge between nodes $x,z\\in A$ of the ", "page_idx": 2}, {"type": "text", "text": "augmentation graph as the joint probability of generating augmentations $x,z$ from the same image $x_{0}\\sim\\rho_{X}$ . Notably, the edge strength of the (degree-normalized) augmentation graph is equivalent to the data similarity kernel, defined in [44]. Formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\nk^{D A F}(x,z)=w_{x z}:=\\mathbb{E}_{x_{0}\\sim\\rho x}\\,\\left[\\frac{p(x\\mid x_{0})}{p(x)}\\frac{p(z\\mid x_{0})}{p(z)}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The magnitude of $w_{x z}$ captures the augmentation-defined similarity between $x$ and $z$ . A higher value of $w_{x z}$ indicates that both patches are more likely to come from the same image and, thereby, are more similar. ", "page_idx": 3}, {"type": "text", "text": "The desiderata of different SSL algorithms can be understood as learning features $F$ that best capture $k^{D A F}(x,z)$ , i.e. $F(x)^{T}F(z)\\,\\approx\\,{\\'k}^{D A F}(x,z)$ . Recent theoretical work has shows that different SSL losses can be formulated as special cases of the objective function that recovers the top-d eigenfunctions of $k^{D A F}(x,z)$ [44]. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s s l}(F)=\\mathbf{E}_{x,z\\in\\mathcal{A}}\\left[(k^{D A F}(x,z)-F(x)^{T}F(z))^{2}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that all rotations of $F$ that don\u2019t change its span define an equivalence class of solutions to Equation (4) and make no difference for the downstream generalization of a linear probe. Based on this insight, we define an equivalence among learned feature spaces: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1. Let $F(x)\\;=\\;(f_{1}(x),\\ldots{}\\:f_{d}(x))$ be a $d$ -dimensional feature vector (a vector of functions). Define the subspace ", "page_idx": 3}, {"type": "equation", "text": "$$\nV=V(F)=\\{h:X\\to\\mathbb{R}\\mid h(x)=w\\cdot F(x),\\quad w\\in\\mathbb{R}^{d}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "to be the span of the components of $F$ . Given an $n$ -dimensional feature vector, $G(x)\\ =$ $(g_{1}(x),\\ldots,{\\bar{g}}_{n}(x))$ we say the features $G$ and $F$ are equivalent, if $V(F)=V(G)$ . ", "page_idx": 3}, {"type": "text", "text": "3 Implicit bias of non-contrastive SSL loss and optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We extend the recent theoretical results [44] to propose a compute-efficient reformulation of the loss function of the SSL desiderata that yields equivalent features, i.e. the functions spanning the eigenfunctions of the augmentation-defined data similarity kernel, $k^{D A F}$ . Furthermore, we study the role of gradient descent in optimizing this loss function and uncover a selection and primacy bias in feature learning. Specifically, we find that gradient descent tends to learn the dominant eigenfunctions, i.e. eigenfunctions corresponding to larger eigenvalues, earlier during training, and often over-represents these eigenfunctions under weak orthogonalization constraints. ", "page_idx": 3}, {"type": "text", "text": "Consequently, we propose employing a stronger orthogonalization constraint during optimization when using a low-dimensional projector to ensure that learned features are equivalent to those learned with a high-dimensional projector. Furthermore, we argue that using more augmentations improves our sample estimate of $k^{\\dot{D}A\\check{F}}$ , thereby aiding the eigenfunction optimization problem. We dedicate the rest of this section to highlight our key theoretical insights, and practical recommendations that follow them. ", "page_idx": 3}, {"type": "text", "text": "3.1 Features in terms of data augmentation kernels ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let us define a kernel operator, Tk, for a positive semi-definite data augmentation kernel, kDAF . ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{k}f(x)=\\mathbb{E}_{z\\sim\\rho_{X}}[k(z,x)f(z)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "such that Equation (4) can be equivalently written as (Equation 5 of [44]) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s s l}(F)=\\langle F,(I-T_{k})F\\rangle_{\\rho_{A}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can now use Mercer\u2019s theorem to factorize $k^{D A F}$ into corresponding spectral features $G:X\\rightarrow\\ell_{2}$ (where $\\ell_{2}$ represents square summable sequences) [15, 16, 31]. However, note that computing $k^{D A\\bar{F}}$ (or $T_{k}$ ) is expensive as it requires computing the overlap among all augmentations of every pair of data points. Instead of computing the eigenfunctions of $T_{k}$ directly, we propose using an alternative operator $T_{M}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{M}f(x)=\\mathbb{E}_{x_{0}\\sim M(x)}\\left[f(x_{0})\\right]=\\sum_{x_{0}}\\left[p(x_{0}\\mid x)f(x_{0})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which averages the values of the function, $f$ , over the augmented images $x_{0}=M(x)$ of the data, $x$ .   \nWe show that $T_{M}^{T}T_{M}$ is equivalent to $T_{k}$ , and therefore $T_{M}$ and $T_{k}$ have shared eigenfunctions. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Let $G(x)$ be the infinite Mercer features of the backward data augmentation covariance kernels, kDAB. Let F(x) = (f1(x), . . . , fNk(x)) be the features given by minimizing the following data augmentation invariance loss ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(F)=\\sum_{i=1}^{N_{k}}\\|T_{M}f_{i}-f_{i}\\|_{L^{2}(\\rho_{X})}^{2},\\quad s u b j e c t\\,t o\\quad(f_{i},f_{j})_{\\rho_{X}}=\\delta_{i j}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which includes the orthogonality constraint. Then, $\\begin{array}{r}{V(F)\\subset V(G)\\,,\\,\\operatorname*{lim}_{N_{k}\\to\\infty}V(F)=V(G).}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "As shown in the Appendix B, $L(F)$ is equivalent to a constrained optimization formulation of the BarlowTwins loss. Furthermore, $L(F)$ with the additional constraint that $(f_{i},f_{i})\\,\\geq\\,\\gamma\\,\\,\\forall i\\,\\in$ $\\{1,2\\ldots N_{k}\\}$ is the constrained optimization formulation of the VICReg loss. ", "page_idx": 4}, {"type": "text", "text": "3.2 The implicit bias of gradient descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we investigate how the use of gradient descent for optimizing $L(F)$ influences the characteristics of the learned feature space, $V(F)$ . Given the similarity in its form with that of the BarlowTwins loss, we build on recent findings that demonstrate the sequential nature of learning eigenfunctions when optimizing the BarlowTwins loss under a strong orthogonalization regularization [36]. Since strong orthogonalization is seldom used in practice due to instabilities in training [43, 5], we believe studying the learning dynamics under weak orthogonalization regularization (i.e. low values of $\\beta$ in Equation (1)) is more relevant to provide recommendations for practitioners. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. (Informal) Let us denote the span of the feature space at initialization as $V(F_{0})$ and after training as $V(F_{T})$ . For small initialization of the network\u2019s weights, the alignment of $V(F_{T})$ with the eigenfunctions of $T_{k}$ depend on two factors: $(i)$ alignment of $V(F_{0})$ with the eigenfunctions of $T_{k}$ ; (ii) singular values of $T_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "Under weak orthogonalization constraints, the network tends to learn features that are strongly aligned with eigenfunctions corresponding to large singular values. We refer to this property as the \u201cselection\u201d bias of gradient descent, wherein gradient descent selects certain eigenfunctions based on the corresponding singular values. This selection bias leads to redundancy among the learned feature space, thereby reducing the effective dimensionality of the network\u2019s output space compared to its ambient dimensionality. We will leverage this finding to improve the parameter overhead of good feature learning using BarlowTwins and VICReg loss frameworks. ", "page_idx": 4}, {"type": "text", "text": "3.3 Takeaway 1: Low-dimensional projectors can yield good representations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the proximity of the formulation of Equation (9) to that of BarlowTwins and VICReg losses, we will leverage existing heuristics that have been shown to work in practice. As such, BarlowTwins and VICReg frameworks call for high-dimensional projectors while using a weak orthogonalization regularization to facilitate good feature learning. We know, from Theorem 3.1, that the eventual goal of these frameworks is to learn the eigenfunctions of the underlying data similarity graph. For example, since the intrinsic dimensionality of Imagenet is estimated to be $\\sim40$ [32], it is not unreasonable to expect that the span of desired features would be of similar dimensionality. It is, thus, intriguing that the current practice would suggest using an $\\sim8192$ -dim projector head to capture the intricacies of the corresponding augmentation-defined data similarity kernel. This discrepancy can be explained by analyzing the learning dynamics, as in Theorem 3.2. Notably, a high-dimensional projector is likelier to have a greater initialization span than its low-dimensional counterpart, increasing the alignment between $\\bar{V}(F_{0})$ and relevant eigenfunctions of $T_{k}$ . We hypothesize that a stronger orthogonalization constraint for low-dimensional projectors can rectify this issue, reducing the redundancy in the network\u2019s output space and rendering it sufficient for good feature learning. ", "page_idx": 4}, {"type": "text", "text": "3.4 Takeaway 2: Multiple augmentations improve kernel approximation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By comparing the invariance criterion formulation in the standard BarlowTwins and VICReg losses to Equation (7), it can be inferred that current practices use a sample estimate of $T_{k}$ . Using only two augmentations per sample yields a noisy estimate of $T_{k}$ , yielding spurious eigenpairs [41] (see ", "page_idx": 4}, {"type": "text", "text": "Appendix C). These spurious eigenpairs add stochasticity in the learning dynamics, and coupled with Theorem 3.2, increase the redundancy in the learned feature space [11]. We hypothesize that improving this estimation error by increasing the number of augmentations could alleviate this issue and improve the speed and quality of feature learning. ", "page_idx": 5}, {"type": "text", "text": "Of course, increasing the number of augmentations $(m)$ in the standard BarlowTwins and VICReg loss improves the estimate of $T_{k}$ but comes with added compute costs \u2013 a straightforward approach would involve calculating the invariance loss for every pair of augmentations, resulting in $\\bar{\\mathcal{O}}(m^{2})$ operations. However, Theorem 3.1 proposes an alternative method that uses the sample estimate of $T_{M}$ , thereby requiring only $\\mathcal{O}(m)$ operations, and hence is computationally more efficient while yielding functionally equivalent features (see Appendix B). In summary, Theorem 3.1 establishes a mechanistic role for the number of data augmentations, paving the way for a computationally efficient multi-augmentation framework: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{L}(F)=\\mathbb{E}_{x\\sim\\rho_{X}}\\left[\\sum_{i=1}^{N_{k}}\\sum_{j=1}^{m}\\|\\overline{{f_{i}(x)}}-f_{i}(x_{j})\\|_{L^{2}(\\rho_{X})}^{2}\\right],\\quad\\mathrm{subject}\\,\\,\\mathrm{to}\\quad(f_{i},f_{j})_{\\rho_{X}}=\\delta_{i j}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{f_{i}(x)}}=\\frac{1}{m}\\sum_{j=1}^{m}f_{i}(x_{j})}\\end{array}$ is the sample estimate of $T_{M}f_{i}(x)$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our experiments, we seek to (i) provide empirical support for our theoretical insights and (ii) present practical primitives for designing efficient SSL routines. Since our proposed loss function is closest to the formulation of BarlowTwins/VICReg, we present empirical evidence comparing our proposal to these baselines. In summary, with extensive experiments across learning algorithms (BarlowTwins & VICReg) and training datasets (CIFAR-10, STL-10 & Imagenet-100), we establish the following: ", "page_idx": 5}, {"type": "text", "text": "\u2022 low-dimensional projectors can yield good representations. ", "page_idx": 5}, {"type": "text", "text": "\u2022 multi-augmentation improves downstream accuracy, as well as convergence rate. ", "page_idx": 5}, {"type": "text", "text": "\u2022 multi-augmentation improves sample efficiency in SSL pretraining, i.e., recovering similar performance with significantly fewer unique unlabelled samples. ", "page_idx": 5}, {"type": "text", "text": "Experiment Setup: We evaluate the effectiveness of different pretraining approaches using image classification as the downstream task. Across all experiments, we pretrain a Resnet feature encoder backbone for 100 epochs (see Appendix E.1 for longer pretraining results) and use linear probing on the learned representations1. All runs are averaged over 3 seeds; error bars indicate standard deviation. Other details related to optimizers, learning rate, etc., are presented in the Appendix D. ", "page_idx": 5}, {"type": "table", "img_path": "Y5DPSJzpra/tmp/aba74dd7f757390e3f99af902224547bbc42ead7cde8e77ffa2d3e4d3b1fb9b4.jpg", "table_caption": ["4.1 Low-dimensional projectors can yield good representations "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 1: Optimizing for orthogonality appropriately allows low-dimensional projectors to match the performance (on CIFAR-10) of much higher-dimensional projectors. ", "page_idx": 5}, {"type": "text", "text": "Existing works recommend using high-dimensional MLPs as projectors (e.g., ${\\mathrm{d}}{=}8192$ for Imagenet in [43, 5]), and show significant degradation in performance when using lower-dimensional projectors for a fixed redundancy coefficient $(\\beta)$ . To reproduce this result, we run a grid search to find the optimal coefficient $(\\beta_{8192}^{*})$ for $d=8192$ and show that performance progressively degrades for lower $d$ if the same coefficient $\\beta_{8192}^{*}$ is reused for $d\\in\\{64,1\\bar{2}8,256,512,\\bar{1}0\\bar{2}\\bar{4},2048,\\bar{4}09\\bar{6},8192\\}.$ . ", "page_idx": 5}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/4e167112285854f245659e74613ea6e024e08205fee0e98066c3b07a872ef80e.jpg", "img_caption": ["Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, $\\beta$ , for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions $(d)$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Our insights in Section 3.3 suggest low-dimensional projectors should recover similar performance with appropriate orthogonalization. To test this, we find the best $\\beta$ by performing a grid search independently for each $\\!\\!d\\in\\{64,128,256,512,1024,2048,4096,8192\\}\\!\\!\\!$ . As illustrated in Figure 2, using low-dimensional projectors yield features with similar downstream task performance, compared to the features obtained using high-dimensional projectors. Strikingly, we also observe that the optimal $\\beta_{d}\\propto1/d$ , which aligns with our theoretical insights. ", "page_idx": 6}, {"type": "text", "text": "Recommendation: Start with low-dimensional projector, using $\\begin{array}{r}{\\beta=\\mathcal{O}(\\frac{1}{d})}\\end{array}$ , and sweep over $\\begin{array}{r}{(p d i m=d,\\beta=\\mathcal{O}\\left(\\frac{1}{d}\\right))}\\end{array}$ if needed. ", "page_idx": 6}, {"type": "text", "text": "4.2 Multiple Augmentations Improve Performance and Convergence ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Although some SSL pretraining approaches, like SWaV [10], incorporate more than two views, the most widely used heuristic in non-contrastive SSL algorithms involves using two views jointly encoded by a shared backbone. In line with this observation, our baselines for examining the role of multiple augmentations use two views for computing the cross-correlation matrix. ", "page_idx": 6}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/e1b5fda0678aa39d65bb873326f3d00bc7286f06538f67dce3f0d1cd3b741b59.jpg", "img_caption": ["Figure 3: Using multiple augmentations improves representation learning performance and convergence. (A-C) Across BarlowTwins for CIFAR-10, STL-10 and Imagenet-100 pretraining, using 4 augmentations instead of 2 helps improve performance. Please see Appendix E.3 for more results. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "To demonstrate the role of multiple augmentations in pretraining, we adapt the invariance criterion of BarlowTwins/VICReg to be in line with Equation (10). In particular, for $\\#a u g s\\in\\{2,4,8\\}$ , we pretrain a Resnet-50 encoder with our proposed loss. Building on the insight from the previous section, we use a 256-dimensional projector head for all multi-augmentation experiments. ", "page_idx": 6}, {"type": "table", "img_path": "Y5DPSJzpra/tmp/f959cb618b6e6acded1052e72aa1e80c5c34896c69c29c490de004df509c08f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In Figure 3, we track the downstream performance of the pretrained models across training epochs. For performance evaluation, we use the linear evaluation protocol as outlined by [13]. Figure 3(AC) shows that pretraining with multiple augmentations outperforms the 2-augmentation baseline. Furthermore, we observe that the four-augmentation pretrained models converge faster (both in terms of the number of epochs and wall-clock time) than their two-augmentation counterparts (see Figure 3(D-F)). Additionally, we show in Appendix E.2 that our framework can also be applied to multi-augmentation settings like SWaV, where not all augmentations are of the same resolution. ", "page_idx": 7}, {"type": "text", "text": "Recommendatation: Using multiple augmentations $(\\phantom{\\,\\!}-2)$ is likely to improve convergence as well as downstream accuracy. ", "page_idx": 7}, {"type": "text", "text": "4.3 Sample Efficient Multi-augmentation Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Data Augmentation can be viewed as a form of data inflation, where the number of training samples is increased by $k$ (for $k$ augmentations). In this section, we examine the role of multi-augmentation in improving sample efficiency. In particular, we are interested in understanding if the same performance can be achieved with a fraction of the pretraining dataset, simply by using more augmentations. ", "page_idx": 7}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/8d96aaff0f5f7db2b852b8cb4fb6d364c487feda61b18ba84f31e5f60cd33f36.jpg", "img_caption": ["Table 2: Using multiple augmentations yields faster convergence, with reduced time to reach baseline performance on CIFAR-10, i.e. performance of feature encoder pretrained with an 8192-dim projector and 2 augmentations. ", "Figure 4: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins pretraining on CIFAR10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Y5DPSJzpra/tmp/495320f154c0fe5567009848f306b2d6be1409d172fc987c1be887e37ba30728.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Time required to pass $\\overline{{80\\%}}$ accuracy on CIFAR-10 when pretraining on fraction of the dataset, while using multiple augmentations. See Figure 5 for further discussion. ", "page_idx": 7}, {"type": "text", "text": "To examine the relation between the number of augmentations and sample efficiency, we fixed the effective size of the inflated dataset. This is achieved by varying the fraction of the unique samples in the pretraining dataset depending on the number of augmentations $k\\in\\{2,4,8\\}$ , e.g., we use $50\\%$ of the dataset for 4 views. We then evaluate the performance of the pretrained models on the downstream task, where the linear classifier is trained on the same set of labeled samples. Strikingly, Figure 4 shows that using multiple augmentations can achieve similar (sometimes even better) performance with lesser pretraining samples, thereby indicating that more data augmentations can be used for feature learning to compensate for smaller pretraining datasets. ", "page_idx": 8}, {"type": "text", "text": "Recommendation: In a low-data regime, using diverse & multiple augmentations can be as effective as acquiring more unique samples. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Self-Supervised Pretraining requires significant compute resources and most practitioners rely on empirical heuristics (see SSL cookbook [3] for a summary). While recent advances in SSL theory explore learning dynamics in linear (or shallow) models [39, 40], with a focus on understanding dimensionality collapse [20, 24], the theoretical underpinnings of most of the heuristics considered essential for good feature learning, are missing. ", "page_idx": 8}, {"type": "text", "text": "Contrastive SSL has received more theoretical attention, owing to its connection with metric learning and noise contrastive estimation [29, 4, 25]. In particular, HaoChen et al. [22] provide a theoretical framework for the SimCLR loss from an augmentation graph perspective, which leads to practical recommendations. Subsequently, Garrido et al. [19] establish a duality between contrastive and non-contrastive learning objectives, further bridging the gap between theory and practice. ", "page_idx": 8}, {"type": "text", "text": "Non-contrastive SSL algorithms\u2019 theoretical foundations have received more attention recently [9, 44]. Prior works [2, 19, 18] have demonstrated that with modified learning objectives, lowdimensional projectors yield representations with good downstream performance. Similarly, previous works have demonstrated notable performance boosts when using a multi-patch framework in contrastive [17] and non-contrastive SSL [10, 42]. However, the theoretical basis for the beneftis and trade-offs of either low-dimensional projectors or multiple augmentations is largely unclear. It is worth noting that Schaeffer et al. [34] present an information-theoretic perspective of the recently proposed non-contrastive SSL loss that leverages multiple augmentations, namely MMCR [42], but the computational advantages of using multiple augmentations on the learning dynamics is an active area of research. ", "page_idx": 8}, {"type": "text", "text": "Deep Learning theory has made significant strides in understanding the optimization landscape and dynamics of supervised learning [1]. In concurrent works [44, 9], the interplay between the inductive bias of data augmentations, architectures, and generalization has been explored from a purely theoretical perspective, establishing an equivalence among different SSL losses [44]. Furthermore, Simon et al. [36] used a more straightforward formulation of the BarlowTwins loss and investigated the learning dynamics in linearized models for the case when the invariance and orthogonalization losses have equal penalties. Although such a setting rarely used in practice, their approach serves as an inspiration for our work in studying the learning dynamics of non-contrastive SSL losses. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Summary: Our work builds on existing theoretical results that establish an equivalence among different SSL frameworks, and proposes a compute-efficient reformulation of the common SSL loss. Using this loss reformulation and a study of the optimization dynamics, we proposed practical recommendations to improve the sample and compute efficiency of SSL algorithms. Specifically, we recommended low-dimensional projectors with increased orthogonality constraints and multiaugmentation frameworks, and we verified the effectiveness of these recommendations empirically. It is worth noting that our multi-augmentation formulation improves the efficiency of learning without altering the desiderata of SSL, i.e. the network learns the same feature space using our proposed multi-augmentation framework as with the original SSL formulation in the limit of infinite pretraining budget. To demonstrate this equivalence between the original SSL loss and our proposed version, we show in Appendix E.1 that longer pretraining on the 2-augmentation loss leads to similar downstream performance as the multi-augmentation versions (4 and 8 augmentations). ", "page_idx": 8}, {"type": "text", "text": "We also showed that the multi-augmentation framework can be used to learn good features from fewer unique samples in the pretraining dataset simply by improving the estimation of the data augmentation kernel. This result has direct implications on improving the Pareto frontier of samples-vs-performance for SSL pretraining, wherein we can achieve better downstream performance when limited number of samples are available in the pretraining dataset. ", "page_idx": 9}, {"type": "text", "text": "Pareto Optimal SSL In the context of sample efficiency, training a model using two augmentations with different fractions of the dataset leads to a natural Pareto frontier, i.e., training on the full dataset achieves the best error but takes the most time (Baseline (2-Aug)). Our extensive experiments demonstrate that using more than two augmentations improves the overall Pareto frontier, i.e., achieves better convergence while maintaining accuracy (Multi-Aug). Strikingly, as shown in Figure 5, we observe that we can either use a larger pretraining dataset or more augmentations for a target error level. Therefore, the number of augmentations can be used as a knob to control the sample efficiency of the pretraining routine. ", "page_idx": 9}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/404861b5467630b700f6dc281a7324609d72e7350b1a0bc75df9ce145734ab4b.jpg", "img_caption": ["Figure 5: Using $>2$ augmentations with a fraction of the dataset improves overall Pareto frontier, speeding runtime up to $\\sim2\\times$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Connections to Downstream performance: While our core theoretical results are aimed at accelerating convergence of the SSL loss itself, our empirical results highlight an improved downstream task performance earlier during pretraining. While this discrepancy might seem counter-intuitive at first, it is worth noting that the SSL loss inherent influences downstream performance as it encourages clustering of semantically similar images in the representation space. Such clustering properties in the representation space facilitates easier classification through methods $\\mathbf{k}$ -nearest neighbors or linear decoding for a large number of tasks that rely on the semantic content of images. Previous works [20, 2, 18, 37] have discussed in detail how certain geometric properties of the learned representation space are connected to the linear classification performance for arbitrary decision boundaries, in expectation. However, an in-depth analysis of downstream tasks that are more amenable to linear decoding from the learned SSL representation space requires framing metrics of alignment between the pretraining objective (SSL desiderate) and the downstream task labels, and is an active area of research. ", "page_idx": 9}, {"type": "text", "text": "Open Questions: Looking ahead, it would be exciting to extend this analysis to other categories of SSL algorithms, such as Masked AutoEncoders (MAE). Furthermore, our insights provide opportunities to explore sample-efficient methods that rely on less data, which is particularly important in critical domains such as medical imaging, where data is often relatively scarce and expensive. On a different note, it is intriguing that animals often spend extended periods of time exploring novel objects, likely to gain multiple views of the object [6, 28]. Given the theoretical underpinnings of the computational benefits of multi-augmentation SSL outlined in our work, it would be exciting to develop models of biological learning that leverage these insights and enable sample-efficient continual learning in similar environments. ", "page_idx": 9}, {"type": "text", "text": "Limitations: Our algorithm relies on multiple augmentations of the same image to improve the estimation of the data-augmentation kernel. Though this approach speeds up the learning process, it also adds some extra computational overhead, which means that the impact of faster learning on wall-clock time is less than might be hoped for. One way to mitigate the effects of this limitation would be to scale up to a multi-GPU setting, since the computations for each augmentation can be run on a separate GPU in parallel. This could help ensure that the improved speed of learning directly translates to a significantly reduced wall-clock time for training. ", "page_idx": 9}, {"type": "text", "text": "Impact Statement: The goal of our work is to advance the general field of visual representation learning. Although there are potential downstream societal consequences of our work, we feel there are no direct consequences that must be specifically highlighted here. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Arnab Kumar Mondal for insightful discussions that helped shape the project\u2019s scope, Colleen Gillon for aesthetic contribution to the figures and Florian Bordes for helping setup the FFCV-SSL library. The authors are also grateful to Chen Sun, Jonathan Cornford, Roman Pogodin, Zahraa Chorghay and the anonymous reviewers whose comments and suggestions have significantly enhanced the quality and presentation of the results. This research was generously supported by Vanier Canada Graduate Scholarship (A.G.); NSERC (Discovery Grant: RGPIN-2020- 05105; RGPIN-2018-04821; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022), Healthy Brains, Healthy Lives (New Investigator Award: 2b-NISU-8), and CIFAR Learning in Machines & Brains Program (B.A.R.); Canada CIFAR AI Chair program (A.O. & B.A.R.). The authors also acknowledge the material support of NVIDIA in the form of computational resources, as well as the compute resources, software and technical help provided by Mila (mila.quebec). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428\u2013446, 2020.   \n[2] Kumar K Agrawal, Arnab Kumar Mondal, Arna Ghosh, and Blake Richards. $\\alpha$ -req: Assessing representation quality in self-supervised learning by measuring eigenspectrum decay. Advances in Neural Information Processing Systems, 35:17626\u201317638, 2022.   \n[3] Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023.   \n[4] Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods. In NeurIPS, 2022.   \n[5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.   \n[6] Daniel E Berlyne. Novelty and curiosity as determinants of exploratory behaviour. British journal of psychology, 41(1):68, 1950.   \n[7] Florian Bordes, Randall Balestriero, and Pascal Vincent. Towards democratizing jointembedding self-supervised learning, 2023.   \n[8] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[9] Vivien Cabannes, Bobak Kiani, Randall Balestriero, Yann LeCun, and Alberto Bietti. The ssl interplay: Augmentations, inductive bias, and generalization. In International Conference on Machine Learning, pages 3252\u20133298. PMLR, 2023.   \n[10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pages 132\u2013149, 2018.   \n[11] Feng Chen, Daniel Kunin, Atsushi Yamamura, and Surya Ganguli. Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks. arXiv preprint arXiv:2306.04251, 2023.   \n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[13] Yubei Chen, Adrien Bardes, Zengyi Li, and Yann LeCun. Intra-instance vicreg: Bag of self-supervised image patch embedding. arXiv preprint arXiv:2206.08954, 2022.   \n[14] Adam Coates, Andrew $\\mathrm{Ng}$ , and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n[15] Zhijie Deng, Jiaxin Shi, Hao Zhang, Peng Cui, Cewu Lu, and Jun Zhu. Neural eigenfunctions are structured representation learners. arXiv preprint arXiv:2210.12637, 2022.   \n[16] Zhijie Deng, Jiaxin Shi, and Jun Zhu. Neuralef: Deconstructing kernels by deep neural networks. In International Conference on Machine Learning, pages 4976\u20134992. PMLR, 2022.   \n[17] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9588\u20139597, 2021.   \n[18] Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. Rankme: Assessing the downstream performance of pretrained self-supervised representations by their rank. In International Conference on Machine Learning, pages 10929\u201310974. PMLR, 2023.   \n[19] Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, and Yann LeCun. On the duality between contrastive and non-contrastive self-supervised learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[20] Arna Ghosh, Arnab Kumar Mondal, Kumar Krishna Agrawal, and Blake Richards. Investigating power laws in deep representation learning. arXiv preprint arXiv:2202.05808, 2022.   \n[21] I Gohberg. Classes of linear operator theory. Advances and Applications, 49, 1990.   \n[22] Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for selfsupervised deep learning with spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000\u20135011, 2021.   \n[23] Lajos Horv\u00e1th and Piotr Kokoszka. Inference for functional data with applications, volume 200. Springer Science & Business Media, 2012.   \n[24] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. arXiv preprint arXiv:2110.09348, 2021.   \n[25] Daniel D. Johnson, Ayoub El Hanchi, and Chris J. Maddison. Contrastive learning can find an optimal basis for approximately view-invariant functions. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[26] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.   \n[27] Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. ffcv. https://github.com/libffcv/ffcv/, 2022. commit 3a12966.   \n[28] Marianne Leger, Anne Quiedeville, Valentine Bouet, Beno\u00eet Haelewyn, Michel Boulouard, Pascale Schumann-Bard, and Thomas Freret. Object recognition test in mice. Nature protocols, 8(12):2531\u20132537, 2013.   \n[29] Yazhe Li, Roman Pogodin, Danica J Sutherland, and Arthur Gretton. Self-supervised learning with kernel dependence maximization. Advances in Neural Information Processing Systems, 34:15543\u201315556, 2021.   \n[30] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[31] David Pfau, Stig Petersen, Ashish Agarwal, David GT Barrett, and Kimberly L Stachenfeld. Spectral inference networks: Unifying deep and spectral learning. arXiv preprint arXiv:1806.02215, 2018.   \n[32] Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. In International Conference on Learning Representations, 2021.   \n[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n[34] Rylan Schaeffer, Victor Lecomte, Dhruv Bhandarkar Pai, Andres Carranza, Berivan Isik, Alyssa Unell, Mikail Khona, Thomas Yerxa, Yann LeCun, SueYeon Chung, et al. Towards an improved understanding and utilization of maximum manifold capacity representations. arXiv preprint arXiv:2406.09366, 2024.   \n[35] Kendrick Shen, Robbie M Jones, Ananya Kumar, Sang Michael Xie, Jeff Z HaoChen, Tengyu Ma, and Percy Liang. Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In International Conference on Machine Learning, pages 19847\u201319878. PMLR, 2022.   \n[36] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua Albrecht. On the stepwise nature of self-supervised learning. arXiv preprint arXiv:2303.15438, 2023.   \n[37] Vimal Thilak, Chen Huang, Omid Saremi, Laurent Dinh, Hanlin Goh, Preetum Nakkiran, Joshua M Susskind, and Etai Littwin. Lidar: Sensing linear probing performance in joint embedding ssl architectures. In The Twelfth International Conference on Learning Representations, 2024.   \n[38] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n[39] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In International Conference on Machine Learning, pages 10268\u201310278. PMLR, 2021.   \n[40] Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning with dual deep networks. arXiv preprint arXiv:2010.00578, 2020.   \n[41] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.   \n[42] Thomas Yerxa, Yilun Kuang, Eero Simoncelli, and SueYeon Chung. Learning efficient coding of natural images with maximum manifold capacity representations. Advances in Neural Information Processing Systems, 36:24103\u201324128, 2023.   \n[43] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Selfsupervised learning via redundancy reduction. In International Conference on Machine Learning, pages 12310\u201312320. PMLR, 2021.   \n[44] Runtian Zhai, Bingbin Liu, Andrej Risteski, Zico Kolter, and Pradeep Ravikumar. Understanding augmentation-based self-supervised representation learning via rkhs approximation. arXiv preprint arXiv:2306.00788, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Our main claims are backed by theoretical and empirical results. Theorem 3.1 presents our functionally-equivalent compute-efficient formulation of the SSL objective, and Theorem 3.2 demonstrates the implicit bias of gradient descent during optimizing the SSL loss. Our empirical results demonstrate the utility of our theoretical insights in improving the parameter overhead of good feature learning, optimization convergence and the sample efficiency. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We have a section on limitations in the discussion. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The formal statements alongside proofs are presented in the supplementary material (Appendices A to C). ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We provide implementation details in the supplementary material (Appendix D). We have also released our code base in the public github repo, FastSSL, to facilitate the implementation of our proposed framework. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 13}, {"type": "text", "text": "Justification: We use open-access datasets, like CIFAR, STL and Imagenet. Our code base can be found in the public github repo, FastSSL ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We present details of the experiment setup and results in Section 4 of the main paper, and additional implementation details in Appendix D. ", "page_idx": 13}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We report standard error bars, computed over 3 seeds, for all result plots and tables. ", "page_idx": 14}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: All our CIFAR and STL experiments were done on a single 48-GB RTX8000 GPU and all Imagenet experiments were performed on 2 40-GB A100 GPUs. All experiments were performed on the Mila cluster, aided by compute resources, software and technical help provided by Mila (mila.quebec). ", "page_idx": 14}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We adhere to the NeurIPS Code of Ethics in our work, and research in general. ", "page_idx": 14}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We add a statement on societal impact in the Discussion section. Since the goal of our work is to advance the general field of visual representation learning, we feel there are no direct consequences that must be specifically highlighted here. Although we recognize that there might be potential downstream consequences that warrant attention while building intelligent systems that leverage this work. ", "page_idx": 14}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: We do not release any new data or state-of-the-art models. ", "page_idx": 14}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We acknowledge and cite the datasets and model architectures used in this work. Our codebase is publicly available on our github repo, FastSSL. Moreover, our codebase relies on the Python packages of PyTorch, FFCV [27] and FFCV-SSL [7], which are referred to in the github repo README. ", "page_idx": 14}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: No new assets are released. ", "page_idx": 14}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 15}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] Justification: Our work does not involve research with human subjects. ", "page_idx": 15}, {"type": "text", "text": "A Hilbert Space of functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Functions and inner product space ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition A.1. Given $X,\\rho_{X}$ , and $f,g:X\\,\\rightarrow\\,\\mathbb{R}$ , define the $L^{2}(\\rho_{X})$ inner product and norm, respectively, ", "page_idx": 16}, {"type": "equation", "text": "$$\n(f,g)_{\\rho_{X}}=\\int f(x)g(x)d\\rho_{X}(x),\\quad\\|f\\|_{\\rho_{X}}^{2}=(f,f)_{\\rho_{X}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define ", "page_idx": 16}, {"type": "equation", "text": "$$\nL^{2}(\\rho,X)=\\left\\{f:X\\to\\mathbb{R}\\ |\\ |f||_{\\rho x}^{2}<\\infty\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "to be the (equivalence class) of functions with finite $\\rho_{X}$ norm. ", "page_idx": 16}, {"type": "text", "text": "A.2 Spectral theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we quote the relevant (abstract) Hilbert Space theory. ", "page_idx": 16}, {"type": "text", "text": "Definition A.2 (Spectral Operator). Given orthogonal functions, $\\Phi\\;=\\;(\\phi_{i})_{i\\in I}$ in $L^{2}(\\rho_{X})$ , and non-negative $\\Lambda={\\overline{{(\\lambda_{i})_{i\\in I}}}}$ , with $\\begin{array}{r}{\\|\\bar{\\Lambda}\\|_{2}^{2}=\\sum_{i\\in I}{\\lambda_{i}^{2}}^{\\underline{{\\bullet}}}<\\infty}\\end{array}$ . Call $(\\Phi,\\Lambda)$ a spectral pair and define the corresponding spectral operator by ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{\\Phi,\\Lambda}(h)=\\sum_{j=1}^{\\infty}\\lambda_{j}\\left(h,\\phi_{j}\\right)\\phi_{j},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem A.3 (Spectral Decomposition). Suppose $H$ is a Hilbert space. $A$ symmetric positivedefinite Hilbert-Schmidt operator $T:\\mathbb{H}\\to\\mathbb{H}$ admits the spectral decomposition equation $^{12}$ with orthonormal $\\phi_{j}$ which are the eigenfunctions of $T$ , i.e. $T\\left(\\phi_{j}\\right)=\\lambda_{j}\\phi_{j}$ . The $\\phi_{j}$ can be extended to a basis by adding a complete orthonormal system in the orthogonal complement of the subspace spanned by the original $\\phi_{j}$ . ", "page_idx": 16}, {"type": "text", "text": "Remark A.4. The $\\phi_{j}$ in equation 12 can thus be assumed to form a basis, but some $\\lambda_{j}$ may be zero. ", "page_idx": 16}, {"type": "text", "text": "From [23]. Theorem proved in [21]. Denote by $\\mathcal{L}$ the space of bounded (continuous) linear operators on $\\mathbb{H}$ with the norm ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|T\\|_{\\mathcal{L}}=\\operatorname*{sup}\\{\\|T(x)\\|\\ |\\ \\|x\\|\\leq1\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition A.5 (Compact Operators). An operator $T\\in{\\mathcal{L}}$ is said to be compact if there exist two orthonormal bases $\\{g_{j}\\}$ and $\\{f_{j}\\}$ , and a real sequence $\\{\\lambda_{j}\\}$ converging to zero, such that ", "page_idx": 16}, {"type": "equation", "text": "$$\nT(h)=\\sum_{j=1}^{\\infty}\\lambda_{j}(h,g_{j})f_{j},\\quad h\\in\\mathbb{H},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The $\\lambda_{j}$ may be assumed positive. The existence of representation equation Compact is equivalent to the condition: $T$ maps every bounded set into a compact set. Compact operators are also called completely continuous operators. Representation equation Compact is called the singular value decomposition. ", "page_idx": 16}, {"type": "text", "text": "Definition A.6 (Hilbert-Schmidt Operators). A compact operator admitting representation equation Compact is said to be a Hilbert-Schmidt operator if j\u221e=1 \u03bb $\\dot{\\sum}_{j=1}^{\\infty}\\lambda_{j}^{2}<\\infty$ . The space $\\boldsymbol{S}$ of HilbertSchmidt operators is a separable Hilbert space with the scalar product ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle T_{1},T_{2}\\rangle_{S}=\\sum_{i=1}^{\\infty}\\left(T_{1}\\left(f_{i}\\right),T_{2}\\left(f_{i}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\{f_{i}\\}$ is an arbitrary orthonormal basis. Note the value of equation 13 is independent of the basis. The corresponding norm is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|T\\|_{S}^{2}=\\sum_{j\\geq1}\\lambda_{j}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "One can show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|T\\|_{\\mathcal{L}}\\leq\\|T\\|_{\\mathcal{S}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition A.7. An operator $T\\in{\\mathcal{L}}$ is said to be symmetric if ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle T(f),g\\rangle=\\langle f,T(g)\\rangle,\\quad f,g\\in\\mathbb{H},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and positive-definite if ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle T(f),f\\rangle\\geq0,\\quad f\\in\\mathbb{H}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(An operator with the last property is sometimes called positive semidefinite, and the term positivedefinite is used when the inequality is strict.) ", "page_idx": 17}, {"type": "text", "text": "B Data augmentation kernel perspective of non-contrastive SSL ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem B.1. Let $G(x)$ be the infinite Mercer features of the backward data augmentation covariance kernels, $k^{D A B}$ . Let $\\dot{F}(x)\\;=\\;(f_{1}(x),f_{2}(x),\\ldots,f_{k}(x))$ be the features given by minimizing the following data augmentation invariance loss ", "page_idx": 17}, {"type": "equation", "text": "$$\nL(F)=\\sum_{i=1}^{N_{k}}\\|T_{M}f_{i}-f_{i}\\|_{L^{2}(\\rho_{X})}^{2},\\quad s u b j e c t\\,t o\\quad(f_{i},f_{j})_{\\rho_{X}}=\\delta_{i j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which includes the orthogonality constraint. Then, $V(F)\\subset V(G)\\,,\\,V(F)\\to V(G)$ as $N_{k}\\rightarrow\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "The idea of the proof uses the fact that, as linear operators, $T_{k^{D A B}}\\,=\\,T_{M}^{\\top}T_{M}$ and that $T_{k^{D A F}}\\,=$ $T_{M}T_{M}^{\\top}$ . Then we use spectral theory of compact operators, which is analogue of the Singular Value tDheocseo mobptoasiintieod nf rion mH iolpbteirmt iSzipnagc . hAo ws itmhialta re irgeesnulftu cnacnti obne so obft $T_{M}^{\\top}T_{M}$ ionpg aaren dt .ame as $L(F)$ $k^{D A F}$ $T_{M}^{\\top}$ ", "page_idx": 17}, {"type": "text", "text": "Note that $L(F)$ is the constrained optimization formulation of the BarlowTwins loss. Furthermore, $L(F)$ with the additional constraint that $(f_{i},f_{i})\\ge\\gamma\\,\\forall i\\in\\{1,2\\,.\\,.\\,N_{k}\\}$ is the constrained optimization formulation of the VICReg loss. ", "page_idx": 17}, {"type": "text", "text": "B.1 Proof of theorem 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show we can factor the linear operator, leading to a practical algorithm. Here, we show that we can capture the backward data augmentation kernel with the forward data augmentation averaging operator ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2. Using the definitions above, and with $k$ in equation $6$ given by $k^{D A B}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nT_{k}=T_{M}^{\\top}T_{M}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. First, define the non-negative definite bilinear form ", "page_idx": 17}, {"type": "equation", "text": "$$\nB^{V A R}(f,g)=(T_{M}f,T_{M}g)_{\\rho_{X}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given the backwards data augmentation covariance kernel, $k^{D A B}$ , define ", "page_idx": 17}, {"type": "equation", "text": "$$\nB^{D A B}(f,g)=(T_{k}f,g)_{\\rho_{X}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We claim, that ", "page_idx": 17}, {"type": "equation", "text": "$$\nB^{V A R}=B^{D A,B}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This follows from the following calculation, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B^{D A,B}(f,g)=(T_{k}f,g)_{\\rho_{X}}}&{}\\\\ &{=\\mathbb{E}_{x}[T_{k}f(x),g(x)]=\\mathbb{E}_{x}\\mathbb{E}_{z}[k_{D A,B}(z,x)f(z)g(x)]}\\\\ &{=\\mathbb{E}_{x}\\mathbb{E}_{z}\\mathbb{E}_{x_{0}}\\left[\\frac{p(x_{0}\\mid x)}{\\rho(x_{0})}\\frac{p(x_{0}\\mid z)}{\\rho(x_{0})}f(z)g(x)\\right]}\\\\ &{=\\mathbb{E}_{x_{0}}\\left[\\sum_{x}\\left(\\frac{\\rho(x)p(x_{0}\\mid x)}{\\rho(x_{0})}g(x)\\right)\\sum_{z}\\left(\\frac{\\rho(z)p(x_{0}\\mid z)}{\\rho(x_{0})}f(z)\\right)\\right]}\\\\ &{=\\mathbb{E}_{x_{0}}\\left[\\sum_{x}\\left(p(x\\mid x_{0})g(x)\\right)\\sum_{z}\\left(p(z\\mid x_{0})f(z)\\right)\\right]\\qquad[\\mathrm{Using~Bayes'~nule}]}\\\\ &{=\\mathbb{E}_{x_{0}}\\left[T_{M}f(x_{0})T_{M}g(x_{0})\\right]=(T_{M}f,T_{M}g)_{\\rho_{X}}=B^{V A R}(f,g)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For implementations, it is more natural to consider invariance to data augmentations. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.3 (equivalent eigenfunctions). Assume that $T_{M}$ is a compact operator. Define the invariance bilinear form ", "page_idx": 18}, {"type": "equation", "text": "$$\nB^{I N V}(f,g)=(T_{M}f-f,T_{M}g-g)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then $B^{I N V}$ , $B^{V A R}$ share the same set of eigenfunctions. Moreover, these are the same as the eigenfunctions of $B^{D A,B}$ . In particular, for any eigenfunction $f_{j}$ of $B^{V A R}$ , with eigenvalue $\\lambda_{j}$ , then $f_{j}$ is also and eigenfunction of $B^{I N V}$ , with the corresponding eigenvalue given by $(\\sqrt{\\lambda_{j}}-\\dot{1})^{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Define $T_{M M}$ by, ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{M M}f=T_{M}^{\\top}T_{M}f\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{M S}=(T_{M}-I)^{\\top}(T_{M}-I)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note, by the assumption of compactness, $T_{M}$ has the Singular Value Decomposition, (see the Hilbert Space section for equation SVD), ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{M}(h)=\\sum_{j=1}^{\\infty}\\lambda_{j}(h,g_{j})f_{j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $f_{j}$ be any right eigenvector of $T_{M}$ , with eigenvalue $\\mu_{j}$ . Then $f_{j}$ is also a right eigenvector $T_{M}-I$ , with eigenvalue $\\mu_{j}-1$ . So we see that $T_{M M}$ has $f_{j}$ as an eigenvector, with eigenvalue $\\lambda_{j}=\\mu_{j}^{2}$ and $T_{M S}$ has $f_{j}$ as an eigenvector, with eigenvalue $(\\sqrt{\\lambda_{j}}-1)^{2}$ . Finally, the fact that there are no other eigenfunctions also follows from equation SVD. ", "page_idx": 18}, {"type": "text", "text": "The final part follows from the previous lemma. ", "page_idx": 18}, {"type": "text", "text": "Equivalence of Barlow Twins loss to Equation (9). The BarlowTwins loss from [43] is as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{B T}=\\sum_{i}(C_{i i}-1)^{2}+\\beta\\sum_{i}\\sum_{j\\neq i}C_{i j}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C$ is the cross-correlation matrix computed between the outputs of the network to two different augmentations. First, the BarlowTwins loss can be seen as the unconstrained optimization form of the following constrained optimization objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{B T}=\\sum_{i}(C_{i i}-1)^{2}\\quad,\\mathrm{subject\\;to}\\quad C_{i j}=0\\quad\\forall j\\neq i\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\beta$ is the Lagrangian multiplier [8]. In [43], the cross-correlation matrix $C$ is computed by a dot product between normalized functions $f_{i}$ \u2019s such that $(f_{i},f_{i})_{\\rho_{X}}=1\\,\\forall i$ . The network output for one augmentation of $x,\\,a,$ can be thought of as a Monte-Carlo estimate (with one sample) of $T_{M}f_{i}(x)$ , where $f_{i}$ is the $i^{t h}$ dimension of the network\u2019s output. Therefore, the BarlowTwins loss can be written in its following equivalent form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{L}^{B T}(F)=\\sum_{i=1}^{N_{k}}\\left((T_{M}f_{i},T_{M}f_{i})_{\\rho_{X}}-1\\right)^{2}\\quad,\\mathrm{subject\\,to}\\quad(f_{i},f_{j})_{\\rho_{X}}=\\delta_{i j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As shown by [44], the eigenvalues of $T_{M}^{T}T_{M}$ are always less than 1. Therefore, we do not need the square in Equation (28). Rewriting it, we get the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{L}^{B T}(F)=\\sum_{i=1}^{N_{k}}(T_{M}f_{i},T_{M}f_{i})_{\\rho_{X}}\\quad,\\mathrm{subject}\\,\\mathrm{to}\\quad\\left(f_{i},f_{j}\\right)_{\\rho_{X}}=\\delta_{i j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using Theorem B.3, we show that the loss recovers the equivalent eigenfunctions for the following reason. We can rewrite the loss as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\hat{L}^{B T}(F)=\\sum_{i=1}^{N_{k}}((T_{M}-I)f_{i},(T_{M}-I)f_{i})_{\\rho_{X}}\\quad,\\mathrm{subject}\\ \\mathrm{to}\\quad(f_{i},f_{j})_{\\rho_{X}}=\\delta_{i j}}\\\\ &{\\Longrightarrow\\hat{L}^{B T}(F)=\\displaystyle\\sum_{i=1}^{N_{k}}\\lVert T_{M}f_{i}-f_{i}\\rVert_{L^{2}(\\rho_{X})}^{2}\\quad,\\mathrm{subject}\\ \\mathrm{to}\\quad(f_{i},f_{j})_{\\rho_{X}}=\\delta_{i j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which recovers the loss Equation (9). Note that the VICReg loss [5], in addition to the constraints imposed by the BarlowTwins loss, ensures that the norm of $f_{i}$ \u2019s are more than some threshold. This can be easily incorporated into the constraint with a constant along with $\\delta_{i j}$ . In conclusion, both BarlowTwins and VICReg losses can be seen as equivalent forms of the loss Equation (9). ", "page_idx": 19}, {"type": "text", "text": "Theorem B.4. (Informal) Let us denote the span of the feature space at initialization as $V(F_{0})$ and after training as $V(F_{T})$ . For small initialization of the network\u2019s weights, the alignment of $V(F_{T})$ with the eigenfunctions of $\\tau$ depend on two factors: (i) alignment of $V(F_{0})$ with the eigenfunctions of $\\tau$ ; (ii) singular values of $\\tau$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem B.4. (Formal) Let $\\Gamma=V\\Lambda V^{T}$ represent the eigendecomposition of $\\Gamma$ , and define $z$ as the projection of the weight vectors in $W$ onto singular vectors of \u0393, $V.$ Formally, $z=W V$ . Assuming small initialization (as in Simon et al. (2023), i.e. $|z_{p i}(0)|\\,<<\\,1$ for all $p,i$ , we can derive the following conclusions: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\ \\,s i g n(\\frac{\\Delta z_{p i}(t)}{z_{p i}(t)})=s i g n(\\lambda_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We will first show that the above holds for a linear network, i.e. the output of the network with weights $W\\in\\mathbf{R}^{m\\times n}$ is $W X$ for some input $X\\in\\mathbf{R}^{n\\times b}$ , where $m$ is the output dimensionality, $n$ is the input dimensionality and $b$ is the batch size. ", "page_idx": 19}, {"type": "text", "text": "Let us first analytically compute the cross-correlation matrix $C$ following [36]. ", "page_idx": 19}, {"type": "equation", "text": "$$\nC_{p q}=\\sum_{i,j}{W_{p i}T_{i j}W_{q j}}\\quad,\\quad C_{p p}=\\sum_{i,j}{W_{p i}T_{i j}W_{p j}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $X$ and $X^{\\prime}$ are matrices $\\in\\mathbf{R}^{n\\times b}$ containing two augmentations of a each image in a batch of images. Also, we have defined ${\\mathcal{T}}=X X^{\\prime T}$ , i.e. the augmentation-defined data correlation matrix. Rewriting the BarlowTwins loss function from [43]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{B T}=\\sum_{i}(C_{i i}-1)^{2}+\\beta\\sum_{i}\\sum_{j\\neq i}C_{i j}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To study the learning dynamics, we need to compute the gradient of $\\mathcal{L}_{B T}$ w.r.t. the parameters $W$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d W_{p q}}{d t}=-\\eta\\frac{\\partial\\mathcal{L}_{B T}}{\\partial W_{p q}}=-2\\eta\\sum_{i}(C_{i i}-1)\\frac{\\partial C_{i i}}{\\partial W_{p q}}-2\\eta\\beta\\sum_{i}\\sum_{j\\neq i}C_{i j}\\frac{\\partial C_{i j}}{\\partial W_{p q}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us now analytically compute the derivatives of $C_{i i}$ and $C_{i j}$ w.r.t $W_{p q}$ to simplify each of the terms in Equation (31). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\frac{\\partial C_{i i}}{\\partial W_{p q}}}=\\frac{\\partial}{\\partial W_{p q}}\\sum_{j,k}W_{i j}T_{j k}W_{i k}=\\frac{\\partial}{\\partial W_{p q}}\\sum_{j,k}W_{i j}T_{j k}W_{i k}\\delta_{p}}\\\\ &{\\phantom{\\frac{\\partial}{\\partial W_{p q}}}=\\left(\\sum_{j,k}\\7_{j,k}W_{p k}\\delta_{j q}+\\sum_{j,k}W_{p j}T_{j k}\\delta_{k q}\\right)\\delta_{p i}}\\\\ &{\\phantom{\\frac{\\partial}{\\partial W_{p q}}}=\\left(\\sum_{k}\\7_{q k}W_{p k}+\\sum_{j}W_{p j}T_{j q}\\right)\\delta_{p i}}\\\\ &{\\phantom{\\frac{\\partial}{\\partial W_{p q}}}=2\\left[W T\\right]_{p q}\\delta_{p i}}\\\\ &{\\Longrightarrow\\sum_{i}(C_{i i}-1)\\ensuremath{\\frac{\\partial C_{i i}}{\\partial W_{p q}}}=2(C_{p p}-1)[W T]_{p q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using similar algebra steps, we can simplify the second term: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial C_{i i}}{\\partial W_{p q}}=\\left[W\\mathcal{T}\\right]_{j q}\\delta_{p i}+\\left[W\\mathcal{T}\\right]_{i q}\\delta_{p j}}}\\\\ {{\\Longrightarrow\\displaystyle\\sum_{i}\\sum_{j\\neq i}C_{i j}\\frac{\\partial C_{i i}}{\\partial W_{p q}}=\\sum_{i}\\sum_{j\\neq i}C_{i j}\\left(\\left[W\\mathcal{T}\\right]_{j q}\\delta_{p i}+\\left[W\\mathcal{T}\\right]_{i q}\\delta_{p j}\\right)}}\\\\ {{=\\displaystyle\\sum_{j\\neq q}C_{p j}\\left[W\\mathcal{T}\\right]_{j q}+\\sum_{i\\neq q}C_{i p}\\left[W\\mathcal{T}\\right]_{i q}}}\\\\ {{=2\\left[\\left(C-I\\right)W\\mathcal{T}\\right]_{p q}-2(C_{p p}-1)\\left[W\\mathcal{T}\\right]_{p q}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substituting Equations (32) and (33) into Equation (31), we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\frac{d W_{p q}}{d t}=-\\eta\\frac{\\partial C_{B T}}{\\partial W_{p q}}=-4\\eta(C_{p p}-1)\\left[W7\\right]_{p q}-4\\eta\\beta\\left[(C-I)W7\\right]_{p q}+4\\eta\\beta(C_{p p}-1)\\left[W7\\right]_{p q}}\\\\ {=-4\\eta(1-\\beta)(C_{p p}-1)\\left[W7\\right]_{p q}-4\\eta\\beta\\left[(C-I)W7\\right]_{p q}\\;\\;}&{\\;\\;(34)^{2}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that setting $\\beta=1$ yields the dynamics equation presented by [36]. However, in practice, $\\beta$ is orders of magnitude less that 1. For sake of simplicity, we will analyze the extreme case of $\\beta=0$ , which will yield us insights into the weak-orthogonality constraint case. Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d W_{p q}}{d t}\\approx-4\\eta(C_{p p}-1)\\left[W\\mathcal{T}\\right]_{p q}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us denote the eigendecomposition of $\\tau$ be written as $\\mathcal{T}=V\\Lambda V^{T}$ . Here, $\\Lambda$ is a diagonal matrix with singular values as the diagonal elements. Let us also denote the projection of the weight vectors onto the singular vectors of $\\tau$ , i.e. $V$ as $z$ . So, $z=W V$ . ", "page_idx": 20}, {"type": "text", "text": "Therefore, using these definitions, we can write the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{C_{p p}=\\left[{W T W^{T}}\\right]_{p p}=\\left[{Z\\Lambda Z^{T}}\\right]_{p p}=\\displaystyle\\sum_{i}z_{p i}^{2}\\lambda_{i}}}\\\\ {{W T=W V\\Lambda V^{T}=Z\\Lambda V^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, writing the update equations Equation (35) in terms of $z_{p i}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{d z_{p i}}{d t}=\\sum_{q}\\frac{d W_{p q}}{d t}V_{q i}}}\\\\ {{\\displaystyle\\qquad=-4\\eta\\left(\\sum_{j}z_{p j}^{2}\\lambda_{j}-1\\right)\\sum_{k}z_{p k}\\lambda_{k}(\\sum_{q}V_{q k}V_{q i})}}\\\\ {{\\displaystyle\\qquad=-4\\eta\\left(\\sum_{j}z_{p j}^{2}\\lambda_{j}-1\\right)z_{p i}\\lambda_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assuming small initialization of weights $W$ , we can assume that $\\mid z_{p i}(0)\\mid<<1$ , i.e. magnitude $z_{p i}$ at time 0 is very small. ", "page_idx": 20}, {"type": "text", "text": "Let us define $\\begin{array}{r}{\\dot{h_{p}}(t)=1-\\sum_{j}z_{p j}(t)^{2}\\lambda_{j}}\\end{array}$ . For small initialization, $h_{p}(t)>0\\,\\forall t$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\ns i g n\\left(\\frac{d z_{p i}(t)}{d t}\\frac{1}{z_{p i}}\\right)=s i g n(\\lambda_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is clear from Equation (37) that if $\\lambda_{i}~<~0$ , $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}z_{p i}(t)\\;=\\;0}\\end{array}$ . Similarly, if $\\lambda_{i}~=~0$ , then $z_{p i}(t)=z_{p i}(0)\\,\\forall t$ . ", "page_idx": 20}, {"type": "text", "text": "Therefore, akin to the conclusions of [36], the BarlowTwins loss recovers directions corresponding to positive singular values in the augmentation-defined covariance matrix, $\\tau$ and suppresses directions corresponding to negative singular values. Thus, the network outputs span the top singular vectors of $\\tau$ . ", "page_idx": 20}, {"type": "text", "text": "It is worth noting from Equation (36) that the following holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\frac{1}{\\lambda_{i}}\\frac{d l o g(z_{p i})}{d t}=\\frac{1}{\\lambda_{j}}\\frac{d l o g(z_{p j})}{d t}}\\\\ {\\implies\\frac{1}{\\lambda_{i}}l o g\\left(\\frac{z_{p i}(t)}{z_{p i}(0)}\\right)=\\frac{1}{\\lambda_{j}}l o g\\left(\\frac{z_{p j}(t)}{z_{p j}(0)}\\right)}\\\\ {\\implies\\frac{z_{p i}(t)}{z_{p i}(0)}=\\left(\\frac{z_{p j}(t)}{z_{p j}(0)}\\right)^{\\frac{\\lambda_{i}}{\\lambda_{j}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Without loss of generality, if $\\lambda_{i}<<\\lambda_{j}$ , then $z_{p i}(t)\\approx z_{p i}(0)$ . Therefore, under small initialization, i.e. $z_{p i}(0)$ is small $\\forall i$ , gradient descent biases the $p^{t h}$ weight vector to be more strongly aligned to the eigenvector corresponding to the strongest eigenvalue, for all $p$ \u2019s. Hence, under weak orthogonalization constraints, the BarlowTwins loss will over \"represent\" the strong singular vectors of the augmentation-defined cross-correlation matrix. ", "page_idx": 21}, {"type": "text", "text": "When using high-dimensional projectors, specifically when $m>>\\sum{\\bf1}_{\\lambda_{i}>0}$ , wherein ${\\bf1}_{\\zeta}$ is the indicator function that is 1 when condition $\\zeta$ is true and 0 otherwise, this problem might be ameliorated because there are multiple weight vectors that might be aligned with the top singular vectors of $\\tau$ at initialization. However, when using low-dimensional projectors, we do not have such a luxury and therefore, using a weak orthogonalization constraint leads to dimensionality collapse in the representation space. ", "page_idx": 21}, {"type": "text", "text": "Extending to deep non-linear networks. Similar to the analysis in [36], we can repeat the above analysis by replace $X$ and $X^{\\prime}$ by the corresponding kernel versions, where the kernel corresponds to the Neural Tangent Kernel (NTK) of the network. Therefore, the implicit bias of gradient descent to yield dimensionality collapse in the representation space when using weak orthogonalization constraints still remains. ", "page_idx": 21}, {"type": "text", "text": "Dimensionality collapse under noisy optimization. From the rest of this section, we have seen that the BarlowTwins loss is a Monte-Carlo estimate of the true data-augmentation defined covariance matrix. Moreover, stochastic gradient descent adds noise due to mini-batch sampling to the optimization process. Note that there exist symmetries in our linear network, i.e. an orthogonal rotation of the weight matrix yields the same loss function. As explained in [11], such symmetry-invariant sets are potential candidates for stochastic collapse when performing noisy gradient-based optimization. Therefore, the presence of noise in the data-augmentation covariance matrix, $\\tau$ , as well as the batch noise can further worsen the dimensionality collapse problem where different weight vectors become parallel to each other due to noise in updates. One possible mitigation strategy is to obtain a better estimate of the true augmentation-defined covariance matrix (see Figure 7), which we discuss in the next section. ", "page_idx": 21}, {"type": "text", "text": "Empirical validation. We empirically validate our results on the learning dynamics on simplistic 2-dimensional settings. These results, demonstrating the difference in feature learning dynamics for weak vs strong orthogonalization, are presented as GIFs in the supplementary material, and can also be viewed at the project website. ", "page_idx": 21}, {"type": "text", "text": "C Multi-Augmentation Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Augmentation graph ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use the population augmentation graph formulation introduced in [22]. Briefly, we define a graph $\\mathcal{G}(\\mathcal{X},\\mathcal{W})$ , where the vertex set $\\mathcal{X}$ comprises of all augmentations from the dataset (could be infinite when continuous augmentation functions are used) and $\\mathcal{W}$ denotes the adjacency matrix with edge weights as defined below: ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{x x^{\\prime}}:=\\mathbb{E}_{\\bar{x}\\sim\\mathcal{P}_{\\bar{X}}}\\left[A(x|\\bar{x})A(x^{\\prime}|\\bar{x})\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": ", i.e. the joint probability of generating \u2018patches\u2019 $x,x^{\\prime}$ from the same image $\\textstyle{\\bar{x}}$ . Here $\\boldsymbol{\\mathcal{A}}$ defines the set of augmentation functions used in the SSL pipeline. It is worth noting that the magnitude of $w_{x x^{\\prime}}$ captures the relative similarity between $x$ and $x^{\\prime}$ . A higher value of $w_{x x^{\\prime}}$ indicates that it is more ", "page_idx": 21}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/4cda2a7f530867444e97577c204cdced1b678f26a3532149fb14151301a9d62c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 6: Schematic of augmentation graph. (A) Augmentations from each image span a region in the image space which could overlap with the augmentation span of other images. (B) An augmentation graph schematic that uses probabilities to characterize the interactions among augmentation spans of different instances. ", "page_idx": 22}, {"type": "text", "text": "likely that both patches came from the same image, and thereby are more similar. The marginal likelihood of each patch $x$ can also be derived from this formulation: ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{x}=\\mathbb{E}_{x^{\\prime}\\sim\\mathcal{X}}\\left[w_{x x^{\\prime}}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.2 Contrastive and non-contrastive losses suffer from the same issues ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We will now show that the proposal of using multiple patches for the $\\mathcal{L}_{i n v a r i a n c e}$ is pertinent to both the contrastive and non-contrastive SSL. Following [22], we use the spectral contrastive loss formulation and incorporate the augmentation graph relations: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{c}=-\\mathbb{E}_{x,x^{+}}\\left[f(x)^{T}f(x^{+})\\right]+\\beta\\mathbb{E}_{x,x^{\\prime}}\\left[\\left(f(x)^{T}f(x^{\\prime})\\right)^{2}\\right]}\\\\ &{\\mathcal{L}_{c}\\propto\\|Z Z^{T}-D^{-\\frac{1}{2}}\\mathcal{W}D^{-\\frac{1}{2}}\\|_{F}^{2}=\\|Z Z^{T}-\\bar{\\mathcal{W}}\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $z:=\\sqrt{w_{x}}f(x)$ , $D$ is a $N\\times N$ diagonal matrix with entries $\\{w_{x}\\}$ and $\\bar{\\mathcal{W}}=D^{-\\frac{1}{2}}\\mathcal{W}D^{-\\frac{1}{2}}$ . ", "page_idx": 22}, {"type": "text", "text": "We extend the duality results between contrastive and non-contrastive SSL loss, established by [19], to demonstrate how Equation (41) can be decomposed into the invariance and collapse-preventing loss terms. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|Z Z^{T}-\\bar{\\mathcal{W}}\\|_{F}^{2}=\\|Z^{T}Z-I_{d}\\|_{F}^{2}+2T r\\left[Z^{T}(I_{N}-\\bar{\\mathcal{W}})Z\\right]+\\kappa\\ll\\quad\\quad}\\\\ {=\\|Z^{T}Z-I_{d}\\|_{F}^{2}+2\\displaystyle\\sum_{i}\\sum_{x}(1-\\bar{w}_{x})z_{i}^{2}-2\\displaystyle\\sum_{i}\\sum_{x,x^{\\prime}}\\bar{w}_{x x^{\\prime}}z_{i}z_{i}^{\\prime}+\\kappa}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\kappa$ is some constant independent of $Z$ . The first term in Equation (42) is the covariance regularization term in non-contrastive losses like BarlowTwins (implicit) or VIC-Reg (explicit), and the second term in Equation (43) is the variance regularization. Simplifying the third term in Equation (43) gives us: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i}\\sum_{x,x^{\\prime}}\\overline{{v}}_{x x^{\\prime}}z_{i}z_{i}^{\\prime}=\\sum_{i}\\sum_{x,x^{\\prime}}w_{x x^{\\prime}}f(x)_{i}f(x^{\\prime})_{i}=\\sum_{i}\\sum_{x,x^{\\prime}}\\mathbb{E}_{\\bar{x}\\sim\\mathcal{P}_{\\mathcal{K}}}\\left[A(x|\\bar{x})A(x^{\\prime}|\\bar{x})f(x)_{i}f(x^{\\prime})_{i}\\right]}}\\\\ &{}&{=\\sum_{i}\\mathbb{E}_{\\bar{x}\\sim\\mathcal{P}_{\\bar{X}}}\\left[\\displaystyle\\sum_{x}A(x|\\bar{x})\\big(f(x)_{i}\\overline{{f(x)}}_{i}-f(x)_{i}^{2}\\big)\\right]}\\\\ &{}&{=\\mathbb{E}_{\\bar{x}\\sim\\mathcal{P}_{\\mathcal{K}}}\\left[\\displaystyle\\sum_{x}A(x|\\bar{x})\\left(f(x)^{T}\\overline{{f(x)}}-\\|f(x)\\|^{2}\\right)\\right]}\\end{array}~~~}&{{}}&{\\mathrm{or~~~}}\\\\ &{}&{\\quad~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This term encourages $f(x)$ to be similar to $\\overline{{f(x)}}$ , i.e. the mean representation across all augmentations of $\\textstyle{\\bar{x}}$ , thereby requiring to \u201csufficiently\u201d sample $A(.|\\bar{x})$ . Given that both the contrastive and noncontrastive losses rely on learning invariance properties from data augmentations, we believe that our multi-patch proposal would improve the probability density estimation of $A(.|\\bar{x})$ and yield better performance with few training epochs. ", "page_idx": 22}, {"type": "text", "text": "C.3 Explaining training dynamics in low patch sampling regime ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now turn to a simple form of the augmentation graph to understand how using low number of augmentations affects the evolution of $\\bar{Z}Z^{T}$ . Minimizing Equation (41) implies that the spectral decomposition of $Z$ would align with the top eigenvectors (and values) of $\\overline{{\\mathcal{W}}}$ . We will demonstrate that in the low sampling regime (using few augmentations), the eigenvectors of the sampled augmentation graph $\\tilde{\\mathcal{W}}$ may not align with those of $\\overline{{\\mathcal{W}}}$ . ", "page_idx": 23}, {"type": "text", "text": "Augmentation graph setup. We define an augmentation graph with only two instances from two different classes, similar to the one presented in [35]. Let us denote the four instances as ${\\bar{x}}_{i}$ for $i\\,\\in\\,{1,2,3,4}$ , where ${\\bar{x}}_{1},{\\bar{x}}_{2}$ belong to class 1 (i.e. $y_{1},y_{2}\\;=\\;1)$ ) and ${\\bar{x}}_{3},{\\bar{x}}_{4}$ belong to class 2 (i.e. $y_{3},y_{4}=4_{,}$ ). Let us further assume that ${\\bar{x}}_{1},{\\bar{x}}_{3}$ have the highest pixel-level similarity among $(\\bar{x}_{1},\\bar{x}_{i})\\forall i\\in2,3,4$ , thereby making it more likely to have similar patches. We denote this relationship among input examples using $\\mathcal{G}$ to indicate (pixel-wise) global similarity groups. So, $\\mathcal{G}_{1},\\mathcal{G}_{3}=1$ and $\\mathcal{G}_{2},\\mathcal{G}_{4}=2$ . We can use the following probabilistic formulation to model our augmentation functions (see Figure 6B): ", "page_idx": 23}, {"type": "equation", "text": "$$\nA(x_{j}|\\bar{x}_{i})=\\left\\{\\begin{array}{l l}{\\rho^{\\prime}}&{\\mathrm{if~}j=i}\\\\ {\\mu^{\\prime}}&{\\mathrm{if~}j\\neq i\\mathrm{~and~}y_{j}=y_{i}\\mathrm{~and~}\\mathcal{G}_{j}\\neq\\mathcal{G}_{i}}\\\\ {\\nu^{\\prime}}&{\\mathrm{if~}j\\neq i\\mathrm{~and~}y_{j}\\neq y_{i}\\mathrm{~and~}\\mathcal{G}_{j}=\\mathcal{G}_{i}}\\\\ {\\delta^{\\prime}}&{\\mathrm{if~}j\\neq i\\mathrm{~and~}y_{j}\\neq y_{i}\\mathrm{~and~}\\mathcal{G}_{j}\\neq\\mathcal{G}_{i}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In our setting, $\\rho^{\\prime}+\\mu^{\\prime}+\\nu^{\\prime}+\\delta^{\\prime}=1$ . The adjacency matrix of our augmentation graph (as shown in Figure 6C) is as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\overline{{{\\mathcal{W}}}}=\\left[\\begin{array}{l l l l}{\\rho}&{\\mu}&{\\nu}&{\\delta}\\\\ {\\mu}&{\\rho}&{\\delta}&{\\nu}\\\\ {\\nu}&{\\delta}&{\\rho}&{\\mu}\\\\ {\\delta}&{\\nu}&{\\mu}&{\\rho}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We defer the relations between $\\rho^{\\prime},\\mu^{\\prime},\\nu^{\\prime}\\delta^{\\prime}$ and $\\rho,\\mu,\\nu,\\delta$ to the appendix. The eigenvalues of this matrix are: $(\\rho+\\mu+\\nu+\\delta$ , $\\rho+\\mu-\\nu-\\delta,\\rho-\\mu+\\nu-\\delta,\\,\\rho-\\mu-\\nu+\\delta),$ . Corresponding eigenvectors are along $\\left[1,1,1,1\\right]^{T}$ , $\\left[1,1,-1,-1\\right]^{T}$ . $\\left[1,-1,1,-1\\right]^{T}$ , $\\left[1,-1,-1,1\\right]^{T}$ . Assuming that the augmentation functions induce semantically-relevant invariance properties that are relevant for identifying $y_{i}$ from $f(x_{i})$ , we can say that $\\rho^{\\prime}>\\dot{m}a x\\{\\mu^{\\prime},\\nu^{\\prime}\\}$ and $m\\bar{i n}\\{\\bar{\\nu^{\\prime}},\\mu^{\\prime}\\}>\\delta^{\\prime}$ . When we have sufficiently sampled the augmentations, any SSL loss will learn $Z$ such that its singular values are span the top eigenvectors of the augmentation graph, and the eigenspectrum of $Z Z^{T}$ would simply be the above eigenvalues. In practical settings, the augmentation graph would have significantly higher dimension that the feature/embedding dimension 2. Therefore, singular vectors of $Z$ would span the top eigenvectors of $\\overline{{\\mathcal{W}}}$ and the smaller eigenmodes are not learned. When we have accurately sampled the augmentation graph, $\\mu>\\nu$ and therefore, the class-information preserving information is preferred over pixel-level preserving information during learning. But what happens when we do not sufficiently sample the augmentation space? ", "page_idx": 23}, {"type": "text", "text": "Ansatz. Based on our empirical experience, we define an ansatz pertaining to the eigenvalues of a sampled augmentation graph and validate it in tractable toy settings, such as the one described above. Specifically, we claim that when the augmentation space is not sufficiently sampled, $\\{|\\mu-\\nu|,\\delta\\}\\rightarrow0$ . In other words, we claim that when only few augmentations per example are used, it is more likely to have an equal empirical likelihood for augmentations that preserve (pixel-level) global information and class/context information. Moreover, it is very unlikely to have augmentations that change both the class and global information. This is demonstrated in Figure 7. ", "page_idx": 23}, {"type": "text", "text": "Consequences of the Ansatz. When only a few augmentations are sampled, learning can suppress the class information at the cost of preserving the pixel-level information, thereby leading to an increased smoothness in the learned feature space. ", "page_idx": 23}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/fff70bacf4299b807429416d054f624b59f84981d7fdf5a7d7b907038a84ca17.jpg", "img_caption": ["Figure 7: Empirical verification of the subsampling Ansatz. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Image Classification Datasets Across all experiments, our settings mainly follow [13]. In particular, Table 4a summarizes our pretraining settings on Cifar-10 [26], STL-10 [14] and Imagenet-100 [33]. The Imagenet-100 dataset was generated by sampling 100 classes from the original Imagenet-1k dataset, according to this list [38]. In Table 4b, we outline the corresponding linear evaluation settings for Resnet-50 (for CIFAR-10 and STL-10) and ResNet-18 (for Imagenet). Note that we add a linear classifier layer to the encoder\u2019s features and discard the projection layers for evaluation. Our code base is publicly available on github. ", "page_idx": 24}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/63f2c20f3d44fee2be87401e83126b6c4a972d1f197153daac69824170f0f40a.jpg", "img_caption": ["Table 4: Experiment Protocol for comparing SSL algorithms "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The key SSL loss functions that we use in this work are BarlowTwins [43] and VICReg [5]. Let us suppose that the embeddings of two augmentations of a batch of images are denoted as $z$ and $z^{\\prime}$ . The BarlowTwins loss function is as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{B T}=\\displaystyle\\sum_{i}(C_{i i}-1)^{2}+\\beta\\displaystyle\\sum_{i}\\sum_{j\\neq i}C_{i j}^{2}}&{{}}\\\\ {\\mathrm{where}\\quad C=\\displaystyle\\frac{1}{n-1}\\sum_{k=1}^{n}(z_{k}-\\bar{z})(z_{k}^{\\prime}-\\bar{z}^{\\prime})^{T}}&{{}}\\\\ {\\mathrm{and}\\quad\\bar{z}=\\displaystyle\\frac{1}{n}\\sum_{k=1}^{n}z_{k}}&{{},\\quad\\bar{z^{\\prime}}=\\displaystyle\\frac{1}{n}\\sum_{k=1}^{n}z_{k}^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$C_{i j}$ is the element of $C$ at row $i$ , column $j$ and $n$ is the batch size. For each projector dimensionality, $d$ , we search for the hyperparameter, $\\beta$ , that yields the best downstream task performance. ", "page_idx": 24}, {"type": "text", "text": "The VICReg loss function is as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c c l}{{}}&{{\\displaystyle\\mathcal{L}_{V I C}=\\frac{1}{n}\\mu\\sum_{k=1}^{n}\\|z_{k}-z_{k}^{\\prime}\\|^{2}+\\frac{1}{2}\\mu\\left[v(Z)+v(Z^{\\prime})\\right]+\\frac{1}{2}\\left[c(Z)+c(Z^{\\prime})\\right]}}\\\\ {{}}&{{}}&{{\\mathrm{where}\\quad v(Z)=\\displaystyle\\frac{1}{d}\\sum_{i=1}^{d}m a x(0,1-S t d e v(z_{;i,i}))}}\\\\ {{}}&{{}}&{{\\mathrm{and}\\quad c(Z)=\\displaystyle\\frac{1}{d}\\sum_{i}\\sum_{j\\neq i}\\left[C(Z)_{i j}\\right]^{2}\\quad,\\quad C(Z)=\\displaystyle\\frac{1}{n-1}\\sum_{k=1}^{n}(z_{k}-\\bar{z})(z_{k}^{\\prime}-\\bar{z}^{\\prime})^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For each projector dimensionality, $d$ , we search for the hyperparameter, $\\mu$ , that yields the best downstream task performance. ", "page_idx": 25}, {"type": "text", "text": "D.1 Empirical results for low-dimensional projectors ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/af6d0f12e45af3b3b7c135b7fb1e467431d940eb7d18c343a7c42c3773847811.jpg", "img_caption": ["Figure 8: Low-dimensional projectors can yield good representations for both BarlowTwins and VICReg. We demonstrate that using a higher orthogonality constraint, $\\beta$ , for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions $(d)$ . Note that for VICReg, we plot the ratio of the coefficient of the covariance loss to the coefficient of the invariance loss, i.e. $\\begin{array}{r}{\\vec{\\beta}=\\frac{1}{d*\\mu}}\\end{array}$ d\u22171\u00b5, where \u00b5 is the coefficient of the invariance loss. (See Equation (49) for details of the loss formulation.) "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "Y5DPSJzpra/tmp/ca68debaf4b0c43db0f3df4c2715d55e5be03be0ba1b32ea91ae309d20c93eb7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 5: Extended version of Table 1. Optimizing for orthogonality appropriately allows lowdimensional projectors to match the performance for BarlowTwins and VICReg (on CIFAR-10) of much higher-dimensional projectors. ", "page_idx": 25}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/0ce3468a511ff4e03c414720a802e6860a0f640826c16522c23c83098a8fa1c2.jpg", "img_caption": ["D.2 Empirical results with multi-augmentations along with Time ", "Figure 9: Using multiple augmentations improves representation learning performance and convergence. (A-C) Across BarlowTwins and VICReg for CIFAR-10 and STL-10 pretraining, using 4 augmentations instead of 2 helps improve performance. (D-F) Although the 4-augmentations take longer for each epoch, its performance still trumps the 2-augmentation version of the algorithm at the same wall clock time. Please see Appendix E.3 for more results. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/6291245cbf5004ca808b5bb0356a66b1b73d7a6f8429fb993e535ea9b3e0f628.jpg", "img_caption": ["Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10 and STL-10, for the same effective dataset size (#augs\u00d7#unique_samples), using more patches improves performance at the same epoch (A-C) or wall clock time (D-F). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/0d733782d912b06865f3d1beb5683a6791737e134750c9cc6da9953cf8ee7bd9.jpg", "img_caption": ["Figure 11: BarlowTwins pretraining on full Imagenet-100 dataset with 2, 4 and 8 augmentations. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/439be770279c430ff0972b00a1b62df4ed32d58f931f9e1f28c33a5059f67fa1.jpg", "img_caption": ["Figure 12: BarlowTwins pretraining on fraction of Imagenet-100 dataset with 2, 4 and 8 augmentations. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.3 Empirical results on transfer learning ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we present extended version of results presented in Figure 3, Figure 4 but pretraining on CIFAR-10 (or STL-10) and evaluating on STL-10 (or CIFAR-10). These results, coupled with the ones in Figure 3 Figure 4, present a strong case for the advantage of using the proposed multiaugmentation loss for better convergence as well as downstream accuracy. ", "page_idx": 28}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/1ddc4b1cf54459372ac7cfe356f7e664cc3401ceed3d0f24eebd30982bd43be9.jpg", "img_caption": ["Figure 13: BarlowTwins pretraining on CIFAR-10, linear evaluation on STL-10 labelled set. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/48ab8d673f162cd49bee650b41255cb7732cf5ae6c665c46e352a3d977a5e437.jpg", "img_caption": ["Figure 14: VICReg pretraining on CIFAR-10, linear evaluation on STL-10 labelled set. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/6dc745bfc8a6c1f3ae81c7747a56f09eb36926540e2d760ed3c281676b1d3a5d.jpg", "img_caption": ["Figure 15: BarlowTwins pretraining on STL-10, linear evaluation on CIFAR-10 labelled set. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/515b4774c08c494379176cc4d8745c2800735c75c483b19c05501884241f891a.jpg", "img_caption": ["Figure 16: BarlowTwins pretraining on fraction of CIFAR-10 trainset, linear evaluation on STL-10 labelled set. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/48f3d6fc8338e90b0aa9d95e817982837fe16baa39706ba3413476cf75af77a0.jpg", "img_caption": ["Figure 17: VICReg loss pretraining on fraction of CIFAR-10 trainset, linear evaluation on STL-10 labelled set. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/56b1330292dca79ec1339814ff436a038c96b50a527ab558f5f2b1df3308ac33.jpg", "img_caption": ["Figure 18: BarlowTwins loss pretraining on fraction of STL-10 unlabelled set, linear evaluation on CIFAR-10 train set. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "E Additional Experiments probing multi-augmentation learning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.1 Longer Pretraining to determine early stopping ", "page_idx": 30}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/7e227958959b037b05ec795c0dbf1cc27ed91ef20dc825adaf5d56e8b507e986.jpg", "img_caption": ["Figure 19: BarlowTwins pretraining on full CIFAR-10 dataset for 400 epochs. "], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "Y5DPSJzpra/tmp/99f4644fe1d030fa0a56e4d49a1a8a9335131e50b6bfb32e27bd7be4cf4d6fe8.jpg", "table_caption": [], "table_footnote": ["Table 6: BarlowTwins pretraining on full CIFAR-10 dataset at 400 epochs (with early stopping) "], "page_idx": 30}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/a25e53848edc8b52a022f5f0e03992209d484963ab4ea1942b51666e06011ccf.jpg", "img_caption": ["E.2 SwAV-like augmentations for compute efficient multi-augmentation framework "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 20: BarlowTwins pretraining on full STL-10 dataset for 100 epochs using SwAV-like augmentations. Specifically, the 2-augmentations setting uses two views that are $64\\times64$ , whereas the 4 (or 8) augmentation setting uses additional two (or six) augmentations that are $32\\times32$ . ", "page_idx": 30}, {"type": "image", "img_path": "Y5DPSJzpra/tmp/7f5c1b8c323c394174728efebbf43353243f11f58d7ab0549fb507d4625b6cf9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 21: BarlowTwins pretraining on full CIFAR-10 dataset with 2, 4 and 8 augmentations. ", "page_idx": 31}, {"type": "table", "img_path": "Y5DPSJzpra/tmp/909201ef7bc930e53f79f68fe109994ee5c40ec02d0d3d91fc81920f9f81588f.jpg", "table_caption": [], "table_footnote": ["Table 7: BarlowTwins pretraining on full CIFAR-10 dataset at 100 epochs "], "page_idx": 31}]