[{"figure_path": "jBf3eIyD2x/figures/figures_4_1.jpg", "caption": "Figure 1: Harmful strings for open models. We show white-box results in (a), where we see Llama-2 is more robust than Vicuna. In (b), we show transfer attacks within the Vicuna 1.3 model family, where we see that transfer attacks are most successful when the models are of similar size.", "description": "This figure shows the results of white-box and transfer attacks on open-source language models.  The left panel (a) displays the cumulative attack success rate for white-box attacks on different sized Vicuna and Llama 2 models, showing Llama 2 to be more resistant to attacks than Vicuna. The right panel (b) illustrates the cumulative attack success rate for transfer attacks within the Vicuna 1.3 model family (7B, 13B, and 33B parameters), revealing that transferability is higher when the source and target models have similar sizes. ", "section": "4.1 Harmful strings for open models"}, {"figure_path": "jBf3eIyD2x/figures/figures_5_1.jpg", "caption": "Figure 2: Attack success rate at generating harmful strings on GPT-3.5 Turbo, as a function of cost and iterations.", "description": "This figure shows the trade-off between cost and attack success rate for generating harmful strings using the GPT-3.5 Turbo model. The x-axis represents the cost in USD, and the y-axis shows the cumulative attack success rate.  The solid line represents the performance of the Q-GC attack, demonstrating a rapid increase in success rate with increasing cost. The dashed line shows the baseline success rate achieved by only using the initialization technique, highlighting the significant improvement provided by the Q-GC attack.", "section": "4.3 Harmful strings for GPT-3.5 Turbo"}, {"figure_path": "jBf3eIyD2x/figures/figures_5_2.jpg", "caption": "Figure 2: Attack success rate at generating harmful strings on GPT-3.5 Turbo, as a function of cost and iterations.", "description": "This figure shows the relationship between the attack success rate and the cost (in USD) and number of iterations for generating harmful strings using GPT-3.5 Turbo.  The plot reveals that the attack success rate increases rapidly with more cost and iterations.  It also illustrates the difference between using the full Q-GC attack versus only using the initialization step.  The dotted line represents the success rate achieved using only the initialization method. The solid line illustrates the success rate of the Q-GC method.", "section": "4.3 Harmful strings for GPT-3.5 Turbo"}, {"figure_path": "jBf3eIyD2x/figures/figures_6_1.jpg", "caption": "Figure 3: Tradeoff between attack success rate and target string length for a 20 token prompt. Attacks succeed almost always when shorter than the adversarial prompt, and infrequently when longer.", "description": "This figure shows the relationship between the attack success rate and the length of the target string when using a 20-token prompt.  The x-axis represents the target string length in tokens, and the y-axis shows the attack success rate.  The plot demonstrates that the attack is highly successful when the target string is shorter than the prompt, with success rates approaching 100%. However, as the target string length increases and becomes longer than the prompt, the attack success rate drops dramatically to near zero. The dashed vertical line indicates the length of the prompt (20 tokens). This illustrates a limitation of the attack:  its effectiveness is significantly reduced when the target string is substantially longer than the prompt length.", "section": "4.1 Harmful strings for open models"}, {"figure_path": "jBf3eIyD2x/figures/figures_6_2.jpg", "caption": "Figure 4: Our optimizations to the GCG attack require about 2\u00d7 fewer loss queries to reach the same attack success rate. When we remove the gradient information entirely to perform the original GCG by about 30%.", "description": "This figure compares the cumulative success rate of three different attack methods against the Vicuna 7B language model as a function of the number of loss queries.  The three methods are the original Greedy Coordinate Gradient (GCG) attack, an optimized white-box version of the proposed Greedy Coordinate Query (GCQ) attack, and an optimized black-box version of the GCQ attack. The results demonstrate that the optimized versions of the GCQ attack require significantly fewer queries to achieve a given success rate compared to the original GCG attack, highlighting the efficiency gains of the proposed optimization techniques, particularly in the black-box setting where gradient information is unavailable.", "section": "4.4 Proxy-free harmful strings for open models"}, {"figure_path": "jBf3eIyD2x/figures/figures_7_1.jpg", "caption": "Figure 5: Universal content moderation attack success rate as a function of the number of requests for 5 and 20 token suffixes.", "description": "This figure shows the results of universal content moderation attacks using suffixes of 5 and 20 tokens.  The x-axis represents the number of requests made to the OpenAI content moderation API, and the y-axis shows the cumulative attack success rate.  Separate curves are shown for the training and validation sets, demonstrating the model's ability to generalize to unseen strings.", "section": "4.5 Proxy-free attack on OpenAI text-moderation-007"}, {"figure_path": "jBf3eIyD2x/figures/figures_8_1.jpg", "caption": "Figure 6: Nonuniversal content moderation attacks reach nearly 100% success rate with a moderate number of queries. Note that each OpenAI request corresponds to 32 queries.", "description": "This figure shows the cumulative attack success rate against two content moderation models: OpenAI's text-moderation-007 and Llama Guard 7B.  The x-axis represents the number of queries made to the model, and the y-axis shows the percentage of harmful strings that were successfully evaded (i.e., not flagged as harmful). Two different prompt lengths are compared: 5-token prompts and 20-token prompts. The results demonstrate that the query-based attack is highly effective, achieving near-perfect evasion rates with a relatively small number of queries, even surpassing transfer-based attacks. Note that the scale of the x-axis is logarithmic.", "section": "4.6 Proxy-free attack on Llama Guard 7B"}, {"figure_path": "jBf3eIyD2x/figures/figures_12_1.jpg", "caption": "Figure 7: Histogram of cumulative logprob of a fixed 8 token target given fixed 20 token prompt, sampled 1,000 times.", "description": "This figure shows the distribution of cumulative log probabilities obtained by sampling the same prompt and target string pair 1000 times using OpenAI's API. It illustrates the non-determinism of the API, where the same input yields different log probabilities each time it is evaluated. The x-axis represents the cumulative log probability, and the y-axis represents the count of occurrences. The distribution is concentrated around a mean value, but there is significant variance, suggesting that relying on a single API evaluation for optimization may not be robust.", "section": "E OpenAI API Nondeterminism"}]