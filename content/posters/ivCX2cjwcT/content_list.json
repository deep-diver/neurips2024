[{"type": "text", "text": "Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subash Timilsina ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of EECS Oregon State University Corvallis, OR 97331 timilsis@oregonstate.edu ", "page_idx": 0}, {"type": "text", "text": "Sagar Shrestha   \nSchool of EECS   \nOregon State University   \nCorvallis, OR 97331   \nshressag@oregonstate.edu   \nXiao Fu   \nSchool of EECS   \nOregon State University   \nCorvallis, OR 97331   \nxiao.fu@oregonstate.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A core task in multi-modal learning is to integrate information from multiple feature spaces (e.g., text and audio), offering modality-invariant essential representations of data. Recent research showed that, classical tools such as canonical correlation analysis (CCA) provably identify the shared components up to minor ambiguities, when samples in each modality are generated from a linear mixture of shared and private components. Such identifiability results were obtained under the condition that the cross-modality samples are aligned/paired according to their shared information. This work takes a step further, investigating shared component identifiability from multi-modal linear mixtures where cross-modality samples are unaligned. A distribution divergence minimization-based loss is proposed, under which a suite of sufficient conditions ensuring identifiability of the shared components are derived. Our conditions are based on cross-modality distribution discrepancy characterization and density-preserving transform removal, which are much milder than existing studies relying on independent component analysis. More relaxed conditions are also provided via adding reasonable structural constraints, motivated by available side information in various applications. The identifiability claims are thoroughly validated using synthetic and real-world data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The same data entities can often be represented in different feature spaces (e.g., audio, text and image), due to the variety of sensing modalities or domains. Learning common latent components of data from multiple modalities is well-motivated in representation learning. The shared components are considered modality-invariant essential representations of data, which can often enhance performance of downstream tasks by shedding modality-specific noise [1\u20134] and avoiding over-fitting [5\u20137]. ", "page_idx": 0}, {"type": "text", "text": "A prominent theoretical aspect of shared component learning lies in identifiability of the components of interest. The literature posed an intriguing theoretical question [1, 2, 8]: If every modality of data is represented by a linear mixture of shared and private components with an unknown mixing system, are the shared components identifiable (up to acceptable ambiguities)? Such component identification problems are often nontrivial due to the ill-posed nature of any linear mixture model (see, e.g., [9\u201314]). Interestingly, the work [1] showed that using the classical canonical correlation analysis (CCA) provably find the shared components up to rotation and scaling. In fact, shared component identification from multimodal/multiview linear mixtures were considered in various contexts (see, e.g., [15\u201318]), although some of these works did not model private components. The identifiability results in [1, 2] were generalized to nonlinear mixture models as well [4, 19]. The shared component identification perspective was also related to the success of representation learning in self-supervised learning (SSL) [5\u20137]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Nonetheless, the treatment in [1, 2] and the related works [15\u201317] all assumed that the cross-modality data are aligned (i.e., paired) according to their shared components. In many applications, such as cross-language information retrieval [20\u201322], domain adaptation [23\u201325], and biological data translation [26, 27], aligned cross-modality data are hard to acquire, if not outright unavailable. A natural question is: When the multimodal linear mixtures are unaligned, can the shared latent components still be provably identified under reasonably mild conditions? ", "page_idx": 1}, {"type": "text", "text": "Existing Studies. Theoretical characteristics of unaligned multimodal learning were studied under various settings. The work [28] considered a case where one modality is a linear transform of another modality, and showed that the linear transformation is potentially identifiable. The recent work [29] extended this model to a nonlinear transform setting. However, these works did not consider latent component models\u2014yet the latter are more versatile in many ways, e.g., facilitating one-to-many cross-domain translations [30, 31]. The work [32] considered unaligned mixtures of shared and private components, but the assumptions (e.g., the availability of a large amount of modalities) to ensure identifiability may not be easy to satisfy. The most related work is perhaps [8]. But their approach also relied on somewhat stringent assumptions, e.g., that all the latent components are element-wise statistically independent with at most one component being Gaussian. This is because their procedure had to invoke the classical independent component analysis (ICA) [33]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we provide a suite of sufficient conditions under which the shared components can be provably identified from unaligned multimodal linear mixtures up to reasonable ambiguities. The model and identification problem are referred to as unaligned shared component analysis (unaligned SCA) in the sequel. ", "page_idx": 1}, {"type": "text", "text": "(i) An Identifiable Learning Loss for Unaligned SCA. We propose to tackle the unaligned SCA problem by matching the probability distributions of linearly embedded multi-modal data. We show that under reasonable conditions, the linear transformations identifies the shared components up to the same ambiguities as those in the aligned case [1, 2]. The conditions are considerably milder compared to the existing unaligned SCA work [8]. ", "page_idx": 1}, {"type": "text", "text": "(ii) Enhanced Identifiability via Structural Constraints. We come up with two types of structural constraints, motivated by available side information in applications, to further relax the identifiability conditions. Specifically, we look into cases where the multi-modal data have similar linear mixing systems and cases where a few cross-domain aligned samples available. We show that by adding constraints accordingly, unaligned SCA are identifiable under much milder conditions. ", "page_idx": 1}, {"type": "text", "text": "Our contributions primarily lie in identifiability analysis. Nonetheless, we also show the usefulness of our results in real-world applications, namely, cross-lingual word retrieval, genetic information alignment and image data domain adaptation. Particularly, it shows that our succinct multimodal linear mixture model can effectively post-process outputs of pre-trained encoders, e.g., those in [34, 35], to improve data representations and enhance downstream task performance. ", "page_idx": 1}, {"type": "text", "text": "Notation. Notation definitions can be found in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Generative Model of Interest. Following the classical settings in [1, 2, 15, 16, 18], we consider modeling the multi-modal data as linear mixtures. More specifically, we adopt the model in [1, 2]that splits the latent representation of data into shared components and private components: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pmb{x}^{(q)}=\\pmb{A}^{(q)}\\pmb{z}^{(q)},\\quad\\pmb{z}^{(q)}=[\\pmb{c}^{\\top},(\\pmb{p}^{(q)})^{\\top}]^{\\top},\\,q=1,2,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where x(q) \u2208 Rd(q) represents the data from the $q$ th modality, $\\boldsymbol{z}^{(q)}~\\in~\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)}}$ represents the corresponding latent code, $c\\in\\mathbb{R}^{d_{\\mathrm{C}}}$ and $\\pmb{p}^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{P}}^{(q)}}$ stand for the shared components and the private components, respectively. The data $\\pmb{x}^{(q)}$ \u2019s are assumed to be zero-mean, which can be enforced by centering. Note that the positions of $^c$ and $\\scriptstyle{\\mathbf{\\mathit{p}}}_{q}$ are not necessarily arranged as $[\\pmb{c}^{\\top},(\\pmb{p}^{(q)})^{\\top}]^{\\top}$ (more generally, $\\pmb{z}^{(q)}=\\pmb{\\Pi}^{(q)}[\\pmb{c}^{\\top},(\\pmb{p}^{(q)})^{\\top}]^{\\top}$ with an unknown permutation matrix $\\Pi^{(q)}$ ). However, the representation in (1) is without loss of generality as one can define $\\pmb{A}^{(q)}:=\\pmb{A}^{(q)}(\\pmb{\\Pi}^{(q)})^{\\top}$ to reach the representation in (1). For all the domains, we have ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{c}\\sim\\mathbb{P}_{c},\\quad\\pmb{p}^{(q)}\\sim\\mathbb{P}_{p^{(q)}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{P}_{c}$ and $\\mathbb{P}_{p^{(q)}}$ represent the distributions of the shared components and the domain-private components, respectively. Under (1), the two different range spaces $\\mathrm{range}(\\pmb{A}^{(q)})$ for $q\\,=\\,1,2$ represent two feature spaces. Then latent $\\pmb{p}^{(q)}$ further distinguishes the modalities and often has interesting physical interpretation. For example, some vision literature use $^c$ to model \u201ccontent\u201d and $\\pmb{p}^{(q)}$ \u201cstyle\u201d of the images [31, 36]. In cross-lingual word embedding retrieval [2], $^c$ represents the semantic meaning of the words, while $\\pmb{p}^{(q)}$ represents the language-specific components. The goal of SCA boils down to finding linear operators to recover $^c$ to a reasonable extent. ", "page_idx": 2}, {"type": "text", "text": "Aligned SCA: Identifiability of CCA and Extensions. Learning $^c$ without knowing $A^{(q)}$ is a typical component analysis problem. Learning latent components from linear mixture models (LMMs) like $x=A z$ lacks identifiability in general, due to the bilinear nature of the models. This is because one can find an infinite number of invertible matrices $_B$ such that $x=A B B^{-1}z$ . Then, both $\\left(A,z\\right)$ and $(A B,B^{-1}z)$ can fti to the data $\\textbf{\\em x}$ , making the problem ill-posed in terms of solution uniqueness; see, e.g., [9, 37] and more discussions in Sec. 5. Nonetheless, the works [1, 2] studied the identifiability of $^c$ under the model (1), using the assumption that the cross-modality samples share the same $^c$ are aligned. In particular, [1] formulated the $\\pmb{c}_{\\l}$ -identification problem as a CCA problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\{Q^{(q)}\\}_{q=1}^{2}}{\\mathrm{minimize}}}&{\\mathbb{E}\\left[\\left|\\left|Q^{(1)}x^{(1)}-Q^{(2)}x^{(2)}\\right|\\right|_{2}^{2}\\right]}\\\\ {\\mathrm{subject~to}}&{Q^{(q)}\\mathbb{E}\\left[x^{(q)}(x^{(q)})^{\\top}\\right](Q^{(q)})^{\\top}=I\\quad q=1,2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Q^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times d^{(q)}}$ . The expectation in (3a) is taken from the joint distribution of the aligned pairs $\\mathbb{P}_{\\pmb{x}^{(1)},\\pmb{x}^{(2)}}$ , where every pair $(\\pmb{x}^{(1)},\\pmb{x}^{(2)})$ shares the same $^c$ . The formulation aims to find $\\mathbf{\\bar{\\alpha}}^{Q^{(q)}}$ such that the transformed representations of the aligned pairs ${\\pmb Q}^{(1)}{\\pmb x}^{(1)}$ and ${\\pmb Q}^{(2)}{\\pmb x}^{(2)}$ are equal. In [1], it was shown that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{Q}}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}c\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "under mild conditions (see Appendix E.1 for details), where $(\\hat{\\boldsymbol{Q}}^{(1)},\\hat{\\boldsymbol{Q}}^{(2)})$ is an optimal solution of the CCA formulation and $\\Theta$ is a certain non-singular matrix. E q. (4)  means that $\\widehat{\\boldsymbol{Q}}^{(q)}$ finds the range space where $^c$ lives in, i.e., $\\mathrm{range}(A_{1:d_{\\mathrm{C}}}^{(q)})$ under our notation. ", "page_idx": 2}, {"type": "text", "text": "Unaligned SCA: Existing Result and Theoretical Gap. The work in [8] studied the identifiability of $^c$ under (1) when $\\pmb{x}^{(1)}$ and $\\pmb{x}^{(2)}$ are unaligned. Their approach works under the condition that the elements of $z^{(q)}=[{\\pmb{c}}^{\\top},({\\pmb{p}}^{(q)})^{\\top}]^{\\top}$ are mutually statistically independent. There, $\\widehat{z}^{\\left(q\\right)}=$ $\\ensuremath{\\mathbf{\\Pi}}\\ensuremath{\\mathbf{\\Pi}}^{(q)}\\ensuremath{\\Sigma}^{(q)}\\ensuremath{\\boldsymbol{z}}^{(q)}\\ensuremath{\\boldsymbol{z}}^{(q)}$ is assumed to have been estimated by ICA, where $\\Pi^{(q)}$ and $\\pmb{\\Sigma}^{(q)}$ represent the  s caling and permutation ambiguities, respectively, which cannot be removed by ICA. The work [8] assumed $\\pmb{\\Sigma}^{(q)}=\\pmb{I}$ by imposing a unit-variance assumption on all the zi(q)\u2019s. Then, a cross-domain matching algorithm is used to match the shared elements in $\\widehat{z}^{(1)}$ and $\\widehat{z}^{(2)}$ . The formulation can be summarized as finding $d_{\\mathrm{C}}$ pairs of non-repetitive $(i,j)$ such t h at $e_{i}^{\\top}\\hat{z}^{(1)}$ and $e_{j}^{\\top}\\widehat{z}^{(2)}$ have identical distributions, where $e_{i}$ is the $i$ th unit vector. Denote $\\widehat{c}_{m}^{(1)}=e_{i_{m}}^{\\top}\\widehat{z}^{(1)}$ a nd c(m2) =   ej\u22a4mz(2) for m \u2208[dC]. It can be shown that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{c}_{m}^{(q)}=k c_{\\pi(m)}^{(q)},\\;m\\in[d_{\\mathrm{C}}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $k\\in\\{+1,-1\\}$ and $\\pi$ is a permutation of $\\{1,\\dots,d_{\\mathrm{C}}\\}$ (see details in Appendix E.2 summarized from [8]). This method effectively applies ICA to each modality, and thus the ICA identifiability conditions [33] have to met by $\\pmb{x}^{(1)}$ and $\\pmb{x}^{(2)}$ individually. However, if one only aims to extract $\\Theta c$ as in CCA, these assumptions appear to be overly stringent. ", "page_idx": 2}, {"type": "image", "img_path": "ivCX2cjwcT/tmp/f792bb12647ddd40eda9ca8291055170aa34c9c3691c9c5cfc69862ca934d9e8.jpg", "img_caption": ["Figure 1: Scatter plots of matched distribution $\\Theta^{(1)}c$ Figure 2: Illustration of $\\mathcal{A}^{(1)}$ in (left) and $\\Theta^{(2)}c$ (right) when $^c$ follows the Gaussian Assumption 1 in a case where distribution. Colors in the scatter plot represent align- $d_{\\mathrm{C}}=2$ and $d_{\\mathrm{P}}^{(1)}=1$ . ment; same color represent the data are aligned. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Proposed Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Unaligned SCA: Problem Formulation We assume that $\\pmb{x}^{(q)}$ \u2019s are zero-mean. We use the notation from CCA in (3a). However, since no aligned samples are available, we replace the sample-level matching objective with a distribution matching (DM) module, as DM can be carried out without sample level alignment: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{find}\\quad Q^{(q)}\\in{\\mathbb R}^{d_{\\mathrm{C}}\\times d^{(q)}},~q=1,2,}\\\\ {\\mathrm{subject~to}\\quad Q^{(1)}x^{(1)}\\xrightarrow{(\\mathrm{d})}Q^{(2)}x^{(2)},}\\\\ &{Q^{(q)}{\\mathbb E}\\left[x^{(q)}(x^{(q)})^{\\top}\\right](Q^{(q)})^{\\top}=I\\quad q=1,2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u201c $\\cdot\\mathbf{\\boldsymbol{u}}\\triangleq\\frac{(\\mathsf{d})}{\\mathsf{v}^{\\ast}}\\,\\mathsf{\\boldsymbol{v}}^{,\\bullet}$ means the distributions of $\\textbf{\\em u}$ and $\\pmb{v}$ are the same. ", "page_idx": 3}, {"type": "text", "text": "The formulation in (6) can be realized using various distribution matching tools, e.g., maximum mean discrepancy (MMD) [38] and Wasserstein distance [39]. We use the adversarial loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q^{(1)},Q^{(2)}}\\operatorname*{max}_{f}\\,\\mathbb{E}_{x^{(1)}}\\log\\left(f(Q^{(1)}x^{(1)})\\right)+\\mathbb{E}_{x^{(2)}}\\log\\left(1-f(Q^{(2)}x^{(2)})\\right)+\\lambda\\sum_{q=1}^{2}\\mathcal{R}\\left(Q^{(q)}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first and second terms comprise the adversarial loss from GAN [40]. It finds $Q^{(q)}$ to confuse the best-possible discriminator $f:\\dot{\\mathbb{R}}^{d_{\\mathrm{C}}}\\rightarrow\\mathbb{R},$ , where $f$ is represented by a neural network in practice. It is well known that the minimax optimal point of the first two terms is attained when (6b) is met [40]. We use $\\mathcal{R}(\\pmb{Q}^{(q)})=\\|\\pmb{Q}^{(q)}\\mathbb{E}[\\pmb{x}^{(q)}(\\pmb{x}^{(q)})^{\\top}](\\pmb{Q}^{(q)})^{\\top}-\\pmb{I}\\|_{\\mathrm{F}}^{2}$ to \u201clift\u201d the constraints. This way, the learning criterion in (7) can be readily handled by any off-the-shelf adverserial learning tools. ", "page_idx": 3}, {"type": "text", "text": "Identifiability of Unaligned SCA As we saw in Theorem 4, CCA identifies $\\widehat{\\pmb{Q}}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}c$ where $\\Theta\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times d_{\\mathrm{C}}}$ under the settings of aligned SCA. Establishing a similar resul t  for unaligned SCA is much more challenging. First, it is unclear if (6b) could disentangle $^c$ from $\\pmb{p}^{(q)}$ . In general, $Q^{(q)}x^{(q)}$ could still be a mixture of $^c$ and $\\pmb{p}^{(q)}$ yet (6b) still holds (e.g., when both $^c$ and $\\pmb{p}^{(q)}$ are Gaussian.) ", "page_idx": 3}, {"type": "text", "text": "Second, even when the disentanglement is attained via enforcing (6b) and we have $\\pmb{Q}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}^{(q)}\\pmb{c},$ , in general it does not hold that $\\Theta^{(1)}=\\Theta^{(2)}$ . This is because $\\Theta^{(1)}c\\overset{(\\mathsf{d})}{=}\\Theta^{(2)}c$ where $\\Theta^{(1)}\\neq\\Theta^{(2)}$ can still be perfectly met (e.g., when $\\mathbb{P}_{\\Theta^{(q)}c}$ is symmetric Gaussian in Fig. 1 ). However, $\\Theta^{(1)}\\neq\\Theta^{(2)}$ means that the extracted representations from the two modalities are not matched. This creates challenges for applications like cross-domain information retrieval, language translation, or domain adaptation. ", "page_idx": 3}, {"type": "text", "text": "Our intuition is as follows: If the two distributions $\\mathbb{P}_{c,p^{(1)}}$ and $\\mathbb{P}_{c,p^{(2)}}$ are very different, then $\\pmb{Q}^{(1)}\\pmb{x}^{(1)}\\stackrel{(\\mathsf{d})}{=}\\pmb{Q}^{(2)}\\pmb{x}^{(2)}$ cannot hold unless $Q^{(q)}A^{(q)}=[\\Theta^{(q)},\\mathbf{0}]$ . We use the following to characterize such difference between the joint distributions: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Modality Variability). For any two linear subspaces P(q) \u2282RdC+d(Pq ), $q=1,2,$ , with $\\mathrm{dim}(\\mathcal{P}^{(q)})=d_{\\mathrm{P}}^{(q)}$ , $\\mathcal{P}^{(q)}\\neq\\mathbf{0}\\times\\mathbb{R}^{d_{\\mathrm{P}}^{(q)}}$ and linearly independent vectors $\\{\\pmb{y}_{i}^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)}}\\}_{i=1}^{d_{\\mathrm{C}}}$ , $q=$ $1,2,$ , the sets $\\mathcal{A}^{(q)}=\\mathrm{conv}\\{\\mathbf{0},y_{1}^{(q)},\\dots,y_{d_{\\mathrm{C}}}^{(q)}\\}+\\mathcal{P}^{(q)}$ , $q=1,2$ , are such that i $f\\,\\mathbb{P}_{c,\\pmb{p}^{(q)}}[\\pmb{\\mathscr{A}}^{(q)}]>0$ for $q\\,=\\,1$ or $q\\,=\\,2$ , then there exists a $k\\,\\in\\,\\mathbb{R}$ such that the joint distributions $\\mathbb{P}_{c,p^{(1)}}[k\\mathcal{A}^{(1)}]\\neq$ $\\mathbb{P}_{c,p^{(2)}}[k\\mathcal{A}^{(2)}]$ , where $k\\mathcal{A}^{(q)}=\\{k\\pmb{a}\\mid\\pmb{a}\\in\\mathcal{A}^{(q)}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "The condition in Assumption 1 is a geometric way to characterize the difference between $\\mathbb{P}_{c,p^{(1)}}$ and $\\mathbb{P}_{c,p^{(2)}}$ \u2014if the joint distributions have different measures for all possible \u201cstripes\u201d, each being a direct sum of a subspace and a convex hull (see Fig. 2), then $\\mathbb{P}_{c,p^{(1)}}$ and $\\mathbb{P}_{c,p^{(2)}}$ must be very different. Note that the difference is contributed by the modality-specific term $\\pmb{p}^{(q)}$ , and thus we call this condition \u201cmodality variability\u201d. Modality variability is similar to the \u201cdomain variablity\u201d used in [32, 41]\u2014both characterize the discrepancy of the joint probabilities $\\mathbb{P}_{c,p^{(1)}}$ and $\\mathbb{P}_{c,p^{(2)}}$ . However, there are key differences: The domain variability was defined in a unified latent domain over arbitrary sets $\\boldsymbol{\\mathcal{A}}$ , which could be stringent. Instead, we use the fact that (6) relies on linear operations to construct $\\boldsymbol{\\mathcal{A}}^{(q)}$ , which makes the condition defined over a much smaller class of sets\u2014thereby largely relaxing the requirements. Restricting $\\boldsymbol{\\mathcal{A}}^{(q)}$ to be stripes also makes the modality variability condition much more relaxed compared to the domain variability condition. ", "page_idx": 4}, {"type": "text", "text": "We show the following: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under Assumption 1 and the generative model in (1), denote any solution of (6) asQ(q) $q=1,2$ . Then, if the mixing matrices $A^{(q)}$ are full column ranks and $\\mathbb{E}[c c^{\\top}])$ is full rank, we  h ave Q(q)x(q) = \u0398(q)c. In addition, assume that either of the following is satisfied: ", "page_idx": 4}, {"type": "text", "text": "(a) The individual elements of the content components are statistically independent and non(d) (d) Gaussian. In addition, $c_{i}\\;\\neq\\;k c_{j},\\forall i\\;\\neq\\;j,\\forall k\\;\\in\\;\\mathbb{R}$ and $c_{i}\\neq-c_{i},\\forall i_{i}$ , i.e., the marginal distributions of the content elements cannot be matched with each other by mere scaling.   \n(b) The support of $\\mathbb{P}_{c},$ denoted by $\\mathcal{C}$ , is a hyper-rectangle, i.e., $\\mathcal{C}=[-a_{1},a_{1}]\\times\\cdot\\cdot\\cdot\\times[-a_{\\mathrm{d_{C}}},a_{d_{C}}]$ . (d) (d) Further, suppose that $c_{i}\\neq k c_{j},\\forall i\\neq j,\\forall k\\in\\mathbb{R}$ and $c_{i}\\neq-c_{i},\\forall i$ . ", "page_idx": 4}, {"type": "text", "text": "In Theorem 1, Assumption 1 is used to guarantee $\\widehat{\\boldsymbol{Q}}^{(q)}\\boldsymbol{\\mathbf{x}}^{(q)}=\\boldsymbol{\\Theta}^{(q)}\\boldsymbol{c}$ and either of conditions (a) or (b) is used to make sure $\\Theta^{(1)}=\\Theta^{(2)}$ . Note that   both (a) and (b) are milder than those in [8] (cf. Theorem 5), where the element-wise statistical independence of $\\bar{z}^{(q)}$ was relied on to find shared representation of $\\pmb{x}^{(1)}$ and $\\pmb{x}^{(2)}$ . The proof is in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Numerical Validation. In Fig. 3, the top and bottom rows validate Theorem 1 under the assumptions in (a) and (b), respectively. In the top row, we set $\\pm\\nobreakspace\\in\\mathbb{R}^{2}\\nobreakspace$ , where $c_{1}$ is sampled from Gaussian mixtures with three components and $c_{2}$ is sampled from a Gamma distribution (and $c_{1}\\perp\\!\\!\\!\\perp c_{2}$ ). We set $\\ensuremath{p^{(1)}}$ and $p^{(2)}$ as one-dimensional Laplacian and uniform distributions. In the bottom row, the dimensions of $^c$ and $\\pmb{p}^{(q)}$ for $q=1,2$ are unchanged, but their distributions are replaced in order to satisfy conditions in (b) (see details in Appendix F). One can see that clearly $\\widehat{\\pmb{c}}^{(\\bar{q})}=\\pmb{\\Theta}c$ ; i.e., the learned $\\widehat{\\mathbf{c}}^{\\left(q\\right)}$ for $q=1,2$ are identically rotated and scaled versions of $^c$ . ", "page_idx": 4}, {"type": "text", "text": "A rema r k is that our framework still allows to identify individual $c_{i}$ \u2019s as in [8]. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. Under the conditions in Theorem 1 (a), Assume that at most one $c_{i}$ for $i\\in[d_{\\mathrm{C}}]$ is Gaussian. Then, the components of c are identifiable up to permutation and scaling ambiguities by applying ICA to $\\widehat{\\pmb{c}}^{(q)}=\\bar{\\pmb{Q}}^{(q)}\\pmb{x}^{(q)}$ for either $q=1$ or $q=2$ . ", "page_idx": 4}, {"type": "text", "text": "The corollary means that to identify individual $c_{i}$ , using our formulation still enjoys much milder conditions relative to [8]. Specifically, our condition only specifies the independence among elements of $^c$ , but the condition in [8] needs that all the elements in $\\pmb{z}^{(q)}=[\\pmb{c}^{\\top},(\\pmb{p}^{(\\bar{q})})^{\\top}]^{\\top}$ are independent. ", "page_idx": 4}, {"type": "image", "img_path": "ivCX2cjwcT/tmp/9e7eb4931813521681ce8d83d8c828925a286ad44264c7c7efc8042f7dd18e68.jpg", "img_caption": ["Figure 3: Validation of Theorem 1. Top row: results under assumption (a). Bottom row: results under assumption (b). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Enhanced Identifiability via Structural Constraints ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem 1 was well-supported by the synthetic data experiments. However, our experiments found that the learning criterion (6) often struggles to produce sensible results in some applications. Our conjecture is that the Assumptions in Theorem 1 (a) and (b) might not have been satisfied by the real data under our tests. Although they are not necessary conditions for identifiability, these conditions do indicate that the requirements to guarantee identifiability of unaligned SCA using (6) are nontrivial to meet. In this section, we explore a couple of structural constraints arising from side information in applications to remove the need for the relatively stringent assumptions on $^c$ . ", "page_idx": 5}, {"type": "text", "text": "Homogeneous Domains. The first structural constraint that we consider is $A^{(q)}\\,=\\,A$ for $q=$ $1,2$ . This model is motivated by the fact that advanced representation learning tools, e.g., selfsupervised learning tools (e.g., SimCLR [42]) and foundation models (e.g., CLIP [35]), are already capable of mapping the data clusters to a shared linearly separable space\u2014which indicates that the representations share a subspace, i.e., $\\pmb{x}^{(q)}\\approx\\pmb{A}\\pmb{z}^{(q)}$ . Under such circumstances, the proposed model and method can be used to further process the data by discarding the private components in the latent representation. ", "page_idx": 5}, {"type": "text", "text": "Here, we consider the special case of generative process in (1) where, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{x}^{(q)}=\\pmb{A}[\\pmb{c}^{\\top},(\\pmb{p}^{(q)})^{\\top}]^{\\top}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Under this model, we look for the shared components by solving (6) with a single $Q=Q^{(1)}=Q^{(2)}$ . We use the following version of the modality variability condition: ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. For any linear subspace $\\mathcal{P}\\subset\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}}$ , $d_{\\mathrm{P}}\\,=\\,d_{\\mathrm{P}}^{(1)}\\,=\\,d_{\\mathrm{P}}^{(2)}$ , with $\\dim({\\mathcal{P}})=d_{\\mathrm{P}}$ $\\mathcal{P}\\,\\neq\\,\\mathbf{0}\\,\\times\\,\\mathbb{R}^{d_{\\mathrm{P}}}$ and linearly independent vectors $\\{\\pmb{y}_{i}~\\in~\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}}\\}_{i=1}^{d_{\\mathrm{C}}}$ , $q\\,=\\,1,2$ , the sets ${\\mathcal{A}}=$ $\\mathrm{conv}\\{\\mathbf{0},y_{1},\\dots,y_{{d_{\\mathrm{C}}}}\\}+\\mathcal{P}$ , $q=1,2$ . are such that if $\\mathbb{P}_{c,p^{(q)}}[A]>0$ for $q=1$ or $q=2$ , then the joint distributions $\\mathbb{P}_{c,p^{(1)}}[k\\mathcal{A}]\\neq\\mathbb{P}_{c,p^{(2)}}[k\\mathcal{A}]$ for some $k\\in\\mathbb{R}.$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Consider the mixture model in (8). Assume that $\\operatorname{rank}(A)\\ =\\ d_{\\mathrm{C}}\\,+\\,d_{\\mathrm{P}}$ and $\\operatorname{rank}(\\mathbb{E}[c c^{\\top}])=d_{\\mathrm{C}}.$ , and that Assumption 2 holds. Denote $\\widehat{\\pmb{Q}}$ as any solution of (6) by constraining $Q=Q^{(1)}=Q^{(2)}$ . Then, we have $\\widehat{Q}x^{\\left(q\\right)}=\\Theta c$ . ", "page_idx": 5}, {"type": "text", "text": "One can see that the conditions (a) and (b) in Theorem 1 are completely removed, if the structure $\\pmb{A}^{(1)}=\\pmb{A}^{(2)}$ is imposed. In fact, the result in Theorem 2 is expected and readily seen from the proof of Theorem 1, as the cause for $\\Theta^{(1)}\\neq\\Theta^{(2)}$ is the use of two different $Q^{(q)}$ \u2019s. Nonetheless, this simple variation will prove useful in a series of real-data experiments. ", "page_idx": 5}, {"type": "text", "text": "The Weakly Supervised Case. Another way to add structural constraints is to use available auxiliary information. For example, some datasets have weak annotations and selected pairs; see, e.g., [43, 44]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Weak Supervision). There exist a set of available aligned samples $(\\pmb{x}_{\\ell}^{(1)},\\pmb{x}_{\\ell}^{(2)})f o r$ $\\ell\\in{\\mathcal{L}}$ such that $\\pmb{x}_{\\ell}^{(q)}=\\pmb{A}^{(q)}\\pmb{z}_{\\ell}^{(q)}$ , $z_{\\ell}^{(q)}=[{\\pmb{c}}_{\\ell}^{\\top},({\\pmb{p}}_{\\ell}^{(q)})^{\\top}]^{\\top}$ ; i.e., $(\\mathbf{\\boldsymbol{x}}_{\\ell}^{(1)},\\mathbf{\\boldsymbol{x}}_{\\ell}^{(2)})$ share the same $c_{\\ell}$ . ", "page_idx": 5}, {"type": "text", "text": "The condition can be added into our formulation in (6) as a constraint, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{(1)}\\pmb{x}_{\\ell}^{(1)}=\\pmb{Q}^{(2)}\\pmb{x}_{\\ell}^{(2)},\\;\\forall\\ell\\in\\mathcal{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the next theorem, we show that the incorporation of aligned samples helps relax conditions (a) and (b) in Theorem 1: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Assume that Assumption $^{\\,l}$ is satisfied, that $|{\\mathcal{L}}|\\geq d_{\\mathrm{C}}$ paired samples $(\\mathbf{\\boldsymbol{x}}_{\\ell}^{(1)},\\mathbf{\\boldsymbol{x}}_{\\ell}^{(2)})$ are available, that $A^{(q)}$ for $q=1,2$ have full column rank, and that $\\mathbb{P}_{c}$ is absolutely continuous. Denote $(\\widehat{\\boldsymbol{Q}}^{(1)},\\widehat{\\boldsymbol{Q}}^{(2)})$ as any optimal solution of (6) under the constraint (9). Then, we have $\\widehat{\\pmb{Q}}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}c.$ . The proof and synthetic data validation can be found in Appendices $\\mathrm{D}$ and F, respectively. Note that to realize (9), one only needs to add a regularization term $\\begin{array}{r}{\\beta\\sum_{\\ell\\in\\mathcal{L}}\\|Q^{(1)}\\pmb{x}_{\\ell}^{(1)}-\\pmb{Q}^{(2)}\\pmb{x}_{\\ell}^{(2)}\\|_{2}^{2}}\\end{array}$ to the loss in (7), where $\\beta\\geq0$ is a tunable parameter. The overall lo ss is still differentiable and thus can be easily handled by gradient based approaches. A remark is that our weakly supervised formulation can use as few as dC pairs of (x(\u21131), x(\u21132) to establish identifiability of shared component. In contrast, CCA requires at least $d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(1)}+d_{\\mathrm{P}}^{(2)}$ pairs to attain the same identifiability (cf. Appendix. E.1). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Private Component Identifiability. Although our focus is shared component identification, we show that private components are also identifiable with additional assumptions; see Appendix $_\\mathrm{H}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Identifiability of Component Analysis under Linear Mixture Models. Various component analysis models were studied in the past several decades, e.g., principal component analysis [45], independent component analysis [33], sparse component analysis [10, 12], bounded component analysis [13], simplex component analysis [46, 47], and polytopic component analysis [14]\u2014motivated by their applications in dimensionality reduction, representation learning, and latent variable identification (see, e.g., topic mining [48, 49], hyperspectral unmixing [46, 47], audio/speech separation [33] and community detection [50]). The classical component analysis tools mostly study a single modality. The identifiability results under these models are well developed and documented. ", "page_idx": 6}, {"type": "text", "text": "Identifiability of Shared Components from Aligned Modalities. Modeling multimodal data as two or more linear/nonlinear mixtures of latent components was considered in CCA-related works [1, 2, 15, 19], independent vector analysis (IVA) works [17, 18], multiview ICA works [16, 51], and SSL works [5\u20137, 52]. Partitioning the latent components into shared and private blocks was considered in [1, 2, 4, 5, 7, 52]. Shared component identifiability was established at the block level (see, e.g., [1, 2, 5]) and the individual component level (e.g., [51]) in these works. Nonetheless, they all rely on completely paired/aligned cross-modality samples, which we do not use in this work. ", "page_idx": 6}, {"type": "text", "text": "Distribution Matching and Unaligned Multimodal Analysis. Using distribution matching in unaligned multimodal data analytics for different purpose also has a long history; see applications in imageto-image translation [53], domain adaptation [54], cross-platform image super-resolution [55], and cross-domain information retrieval [21]. The recent works [56] and [57] pointed out the identifiability challenge and the existence of density-preserving transforms. The works in [28, 29] started studying the uniqueness issues in distribution matching. However, the latent mixture models were not studied in this line of work. ", "page_idx": 6}, {"type": "text", "text": "Identifiability of Unaligned SCA. The works in [32, 41] investigated the shared component identifiability when the multimodal data are nonlinear mixtures of content and style (which are shared and private components, respectively) under the same mixing system. Hence, our identical linear mixing case in Theorem 2 can be understood as a special case of theirs. But their analysis relies on the assumption that all the latent components are statistically independent, which is much stronger than our conditions in Theorem 2. Their results also require that there are a large amount of modalities available. But our proof works for just two modalities. The most related work is [8], which uses the model in (1) in the context of multi-view causal graph learning. As discussed before, their assumptions on the latent components are much stronger than ours (see Corollary 1 and Appendix E.2). ", "page_idx": 6}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/7a76e8d06e0d420a7fb17e3c4a73c778c2d716e6abf5fcd0d233d8e85f2edf18.jpg", "table_caption": ["Table 1: Classification accuracy on the target domain of office-31 dataset (ResNet50 embedding) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Numerical Validation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "More Synthetic-Data Validation. We first validate our proposed method on synthetic data that follows our model; see Appendix F for details. ", "page_idx": 7}, {"type": "text", "text": "Application (i) - Domain Adaptation. We first test the proposed methods over a number of domain adaptation (DA) tasks. In DA, we have the source domain data $\\{\\pmb{x}^{(1)}\\}$ and the target domain $\\{\\pmb{x}^{(2)}\\}$ , respectively. Only the source domain data have labels and the two domains are unaligned. We hope to use our method to find shared representations of source and target, and thus the classifier trained using source data can also work well on the target data. ", "page_idx": 7}, {"type": "text", "text": "Dataset: We use two standard benchmarks of DA, i.e., Office-31 [58] and Office-Home [59]. The Office-31 dataset has 4652 images and 31 categories from three domains, namely, Amazon images $(\\mathbf{A})$ , Webcam images (W) and DSLR images $({\\bf D})$ . The Office-Home dataset contains 15,500 images with 65 object classes from four domains, i.e., Artistic images (Ar), Clip art images $(\\mathbf{Cl})$ , Product images $(\\mathbf{Pr})$ , and Real-world images ${\\bf(R w)}$ . ", "page_idx": 7}, {"type": "text", "text": "Setup: We first test the homogeneous domain model in Sec. 4. The images are pre-processed using a ResNet50-based image encoder pre-trained over ImageNet1k [42]. As mentioned, it was observed that self-supervised representation encoders find embeddings that are linearly separable [42], which justifies the use of the model $\\pmb{x}^{(q)}\\approx\\pmb{A}\\pmb{z}^{(q)}$ in the embedding domain. After pre-processing, each image is represented by $d^{(q)}\\,=\\,2048$ features for $q\\,=\\,1,2$ . We set $d_{\\mathrm{C}}\\,=\\,256$ for Office-31 and $d_{\\mathrm{C}}=512$ for Office-Home. More detailed settings are in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Baselines and Training Setup: The baselines are representative DA methods, namely, DANN [25], MDD [60], MCC [61], SDAT [62], and ELS [63]. All the baselines use the same encoder-produced embeddings as inputs; see Appendix G.1 for their configurations. We also use ResNet encoder\u2019s outputs as an extra baseline as it learns informative and transferable features from the ImageNet1K dataset. We follow the training strategies adopted by the baselines [25, 60, 62] to learn a classifier jointly with the shared latent components. This strategy arguably regularizes towards more classification-friendly geometry of the shared features. Therefore we append a cross-entropy (CE) based classifier training module to our loss in (7) that learns our feature extractor $Q$ . More details are in Appendix G.1. ", "page_idx": 7}, {"type": "text", "text": "Metric: The evaluation metric is the classification accuracy in the target domain $\\{\\pmb{x}^{(2)}\\}$ . The classifier is trained with the projected source domain $\\widehat{\\mathbf{Q}}\\mathbf{x}^{(1)}$ and the associated labels. ", "page_idx": 7}, {"type": "text", "text": "Result: Table 1 and Table 2 show the classification accuracy (mean $\\pm$ std) on Office- $^{3l}$ and OfficeHome, respectively. The results are averaged over 5 runs. One can observe that the proposed method offers the best and second best performance in most of the cases. In some tasks (e.g., $\\mathbf{\\cdots}\\mathbf{A}\\mathbf{\\to}\\mathbf{W}^{\\bullet}$ , $\\mathbf{\\dot{A}r}{\\rightarrow}\\mathbf{Cl}^{\\circ}$ , \u201c $\\mathbf{Ar}{\\rightarrow}\\mathbf{Pr}^{{\\circ}}$ and $\\mathbf{\\nabla}^{\\bullet\\bullet}\\mathbf{Rw}{\\rightarrow}\\mathbf{Cl}^{\\circ\\setminus}$ , the proposed method outperforms the best-performing baselines by at least $2\\%$ in accuracy. ", "page_idx": 7}, {"type": "text", "text": "More results on the DA task can be found in Appendix G.1. ", "page_idx": 7}, {"type": "text", "text": "Application (ii) - Single Cell Sequence Analysis. In biomedical research, it is desired to fuse measurements from multiple sensorial modalities of the same cells, in order to have better characterizations of the cells. However, obtaining multimodal data of the same cells simultaneously is almost impossible, due to the sensing limitations. Therefore, many methods are proposed in the literature for aligning unpaired multi-modal single cell data [27, 64, 65]. We focus on the following two modalities of single-cell data [66]: (1) the RNA sequences $\\{\\pmb{x}^{(1)}\\}$ and (2) the ATAC sequences $\\{\\pmb{x}^{(2)}\\}$ . ", "page_idx": 7}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/9de1a64d46a65860e0795562a8414a777b087d0845976e6be2ca31edf7f339a0.jpg", "table_caption": ["Table 2: Classification accuracy on the target domain of office-Home dataset (ResNet50 embedding). "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ivCX2cjwcT/tmp/e5ccab830f7ed62e9ed1934924f77a44fda013a8aa206fdd9116bd26e880cc88.jpg", "img_caption": ["Figure 4: $k$ -NN accuracy for single-cell sequence alignment. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/e03003eed613fece53992720b5883ef890122724537131622cee0e257773c8f7.jpg", "table_caption": ["Table 3: Average precision $\\mathrm{P}@1$ of cross-language information retrieval. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Dataset: We use human lung adenocarcinoma A549 cells data from [66]. The dataset contains 1,874 samples of RNA sequences $\\{\\pmb{x}^{(1)}\\}$ and ATAC sequences $\\{\\pmb{x}^{(2)}\\}$ . Each data set is split into 1534 training samples and 340 testing samples as in [27]. The data have labeled associations between the two domains\u2014part of which will be used to test our weakly supervised formulation. For this experiment, features of RNA sequence and the ATAC sequence have dimensions of $d^{(1)}=815$ and $d^{(\\bar{2})}=2613$ , respectively. We set $d_{\\mathrm{C}}=256$ . We use our weakly supervised formulation as shown in (9). We uniformly sampled a set of indices from the training set to serve as $\\mathcal{L}$ . ", "page_idx": 8}, {"type": "text", "text": "Baseline and Metric: We use weakly supervised algorithm, namely, cross-modal autoencoder (CMAE) work in [27], as a baseline, which also learns the shared representation between unaligned RNA and ATAC sequences. We use the $K$ -nearest neighbor ( $k$ -NN) accuracy to evaluate the performance as suggested in [27]. ", "page_idx": 8}, {"type": "text", "text": "Result: The plot in Fig. 4 shows the $k$ -NN accuracy of the methods on the test set. Results show the mean and standard deviation over 10 runs, each having a different random initialization. For the proposed method, we vary the number of available paired samples from 0 (cf. Theorem 1) to $d_{\\mathrm{C}}=256$ (cf. Theorem 3). Note that the baseline uses more (i.e., 256 and 770) paired samples. It also needs additional class labels, i.e., $y_{i}^{(q)}$ for the ith sample $\\pmb{x}_{i}^{(q)}$ . Here, $y_{i}^{(q)}$ represents the number of hours $[0,1$ or 3) of cell treatment [27, 66]. The proposed method without any supervision (i.e., 0 paired samples) already exhibits around 3 times greater $k$ -NN accuracy compared to the baseline for all $k$ . Moreover, including just one paired sample boosts the $k$ -NN accuracy of the proposed method to around 5 times higher than the baseline for all $k$ . Finally, one can observe a steadily increasing $k$ -NN accuracy with respect to the number of available paired samples. This corroborates with our Theorem 3. ", "page_idx": 8}, {"type": "text", "text": "Application (iii) - Multi-lingual Information Retrieval. We also evaluated our method on a word embedding association problem from the natural language processing literature [20, 21]. This task aims to associate high-dimensional word embeddings across different languages according to their semantic meaning. The word embeddings in two languages are represented using two sets of vectors, i.e., $\\{{\\pmb x}_{i}^{(1)}\\}_{i=1}^{I}$ and $\\{\\pmb{x}_{j}^{(2)}\\}_{j=1}^{J}$ . The postulate is that if $\\bar{\\pmb{x}}_{i}^{(1)}$ and $\\mathbf{\\bar{x}}_{j}^{(2)}$ have the same meaning (e.g., both representing \u201ccat\u201d) in two languages (e.g., English and German), they should share a latent components $^c$ . ", "page_idx": 9}, {"type": "text", "text": "Dataset: We use the word embeddings from the MUSE dataset (https://github.com/ facebookresearch/MUSE) [21]. These monolingual word embedding are generated using fastText [67] and has dimensions of $d^{(q)}=300$ for $q=1,2$ . The training dataset include 200,000 word embeddings in each language. In our experiment we set $d_{\\mathrm{C}}=256$ . We follow the generative model under (8) and run the formulation in (7) to learn the linear transformation $Q$ . ", "page_idx": 9}, {"type": "text", "text": "Baseline: We use Adv [21] as the baseline which also uses distribution matching between two language domains. Unlike our method, Adv does not use linear mixture models. ", "page_idx": 9}, {"type": "text", "text": "Metric: We follow [21] to use the average precision score calculated based on nearest neighbor (NN) and cross domain similarity local scaling (CSLS). Precision at $k$ ( $^{\\ast}k$ precision\u201d) is computed by the number of times that one of the correct translations of source word is retrieved at top- $k$ results $k=\\!\\{1$ , 5, 10}). The final score is normalized to be in the range of 0 to 100, with 100 being the highest score indicating the best performance. To evaluate the performance, we use the same test data as in [21]. For each source and target language pair, this dataset includes 1,500 source word embeddings. The source embeddings are used to retrieve corresponding embeddings from a pool of 200,000 target word embeddings. ", "page_idx": 9}, {"type": "text", "text": "Result: Table 3 reports the $\\boldsymbol{\\mathrm{P}@1}$ scores over the test data calculated for each source and target language pair. The languages are denoted as as en - English, es - Spanish, it - Italian, fr - French, de - Germany, ru - Russian, ar - Arabic and vi - Vietnamese. One can observe that the proposed method exhibits a better precision performance than that of Adv in most of the translation tasks. In particular, the proposed method significantly outperforms the baseline on the tasks $\\mathbf{en\\toar}$ , ar $\\mathbf{\\Gamma}\\longrightarrow\\mathbf{en}$ , $\\mathbf{en\\toei}$ and $\\mathbf{vi\\toen}$ , showing at least $10\\%$ precision gains. Similarly, our method shows at least $5\\%$ improvements in both NN and CSLS based precision metrics in $\\scriptstyle\\mathbf{en}\\to\\mathbf{es}$ and $\\mathbf{es\\longrightarrowen}$ tasks. ", "page_idx": 9}, {"type": "text", "text": "More details and additional experiments can be found in Appendix G.3. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we considered the problem of identifying shared components from unaligned multidomain mixtures. We proposed a learning loss that matches the distributions of linearly transformed data. Based on this loss, we came up with a suite of sufficient conditions to ensure the identifiability of shared components. Furthermore, we proposed modified models and losses that enjoy more relaxed conditions for shared component identifiability. This was achieved via introducing structural constraints, namely, the homogeneity of the mixing systems and the existence of weak supervision. Our theoretical claims were validated with both synthetic and real-world data, demonstrating soundness of the theorems and usefulness of the models/algorithms. ", "page_idx": 9}, {"type": "text", "text": "Limitations. First, our conditions for shared component identification are sufficient. The necessary conditions are not underpinned, but necessary conditions assist understanding the limitations of the models and algorithms. Second, our methods were developed under the linear mixture model, which has limited expressiveness, and thus often requires pre-processing to approximately meet the model specification. We expect that results with similar flavors to be derived for nonlinear models in the future. Third, the results were derived under an unlimited data assumption. It would be interesting have a finite sample analysis. Finally, optimizing GAN-based losses is sensitive to hyperparameter settings. Back-propagation based minimax optimization occasionally fails to converge. More optimization-friendly losses and more stable algorithms are desirable in the context of distribution matching. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported in part by the Army Research Office (ARO) under Project ARO W911NF-21- 1-0227, and in part by the National Science Foundation (NSF) CAREER Award ECCS-2144889. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. S. Ibrahim, A. S. Zamzam, A. Konar, and N. D. Sidiropoulos, \u201cCell-edge detection via selective cooperation and generalized canonical correlation,\u201d IEEE Transactions on Wireless Communications, vol. 20, no. 11, pp. 7431\u20137444, 2021.   \n[2] M. S\u00f8rensen, C. I. Kanatsoulis, and N. D. Sidiropoulos, \u201cGeneralized canonical correlation analysis: A subspace intersection approach,\u201d IEEE Transactions on Signal Processing, vol. 69, pp. 2452\u20132467, 2021.   \n[3] P. Rastogi, B. Van Durme, and R. Arora, \u201cMultiview LSA: Representation learning via generalized CCA,\u201d in Proc. Conference of the North American chapter of the Association for Computational Linguistics: human language technologies, 2015, pp. 556\u2013566.   \n[4] Q. Lyu and X. Fu, \u201cNonlinear multiview analysis: Identifiability and neural network-assisted implementation,\u201d IEEE Transactions on Signal Processing, vol. 68, pp. 2697\u20132712, 2020.   \n[5] J. Von K\u00fcgelgen, Y. Sharma, L. Gresele, W. Brendel, B. Sch\u00f6lkopf, M. Besserve, and F. Locatello, \u201cSelf-supervised learning with data augmentations provably isolates content from style,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 34, 2021, pp. 16 451\u201316 467.   \n[6] Q. Lyu, X. Fu, W. Wang, and S. Lu, \u201cUnderstanding latent correlation-based multiview learning and self-supervision: An identifiability perspective,\u201d in Proc. International Conference on Learning Representations (ICLR), 2022.   \n[7] I. Daunhawer, A. Bizeul, E. Palumbo, A. Marx, and J. E. Vogt, \u201cIdentifiability results for multimodal contrastive learning,\u201d in Proc. International Conference on Learning Representations (ICLR), 2022.   \n[8] N. Sturma, C. Squires, M. Drton, and C. Uhler, \u201cUnpaired multi-domain causal representation learning,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 34, 2021.   \n[9] P. Comon and C. Jutten, Handbook of Blind Source Separation: Independent component analysis and applications. Academic press, 2010.   \n[10] J. Hu and K. Huang, \u201cGlobal identifiability of $\\ell_{1}$ -based dictionary learning via matrix volume optimization,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 36, 2024.   \n[11] N. Gillis, \u201cThe why and how of nonnegative matrix factorization,\u201d Regularization, optimization, kernels, and support vector machines, vol. 12, no. 257, pp. 257\u2013291, 2014.   \n[12] J. Sun, Q. Qu, and J. Wright, \u201cComplete dictionary recovery over the sphere i: Overview and the geometric picture,\u201d IEEE Transactions on Information Theory, vol. 63, no. 2, pp. 853\u2013884, 2016.   \n[13] A. T. Erdogan, \u201cA class of bounded component analysis algorithms for the separation of both independent and dependent sources,\u201d IEEE Transactions on Signal Processing, vol. 61, no. 22, pp. 5730\u20135743, 2013.   \n[14] G. Tatli and A. T. Erdogan, \u201cPolytopic matrix factorization: Determinant maximization based criterion and identifiability,\u201d IEEE Transactions on Signal Processing, vol. 69, pp. 5431\u20135447, 2021.   \n[15] F. R. Bach and M. I. Jordan, \u201cA probabilistic interpretation of canonical correlation analysis,\u201d 2005.   \n[16] H. Richard, P. Ablin, B. Thirion, A. Gramfort, and A. Hyvarinen, \u201cShared independent component analysis for multi-subject neuroimaging,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 34, 2021, pp. 29 962\u201329 971.   \n[17] Y.-O. Li, T. Adali, W. Wang, and V. D. Calhoun, \u201cJoint blind source separation by multiset canonical correlation analysis,\u201d IEEE Transactions on Signal Processing, vol. 57, no. 10, pp. 3918\u20133929, 2009.   \n[18] M. Anderson, G.-S. Fu, R. Phlypo, and T. Adal\u0131, \u201cIndependent vector analysis: Identification conditions and performance bounds,\u201d IEEE Transactions on Signal Processing, vol. 62, no. 17, pp. 4399\u20134410, 2014.   \n[19] P. A. Karakasis and N. D. Sidiropoulos, \u201cRevisiting deep generalized canonical correlation analysis,\u201d IEEE Transactions on Signal Processing, vol. 71, pp. 4392\u20134406, 2023.   \n[20] G. Lample, A. Conneau, L. Denoyer, and M. Ranzato, \u201cUnsupervised machine translation using monolingual corpora only,\u201d in Proc. International Conference on Learning Representations (ICLR), 2018.   \n[21] G. Lample, A. Conneau, M. Ranzato, L. Denoyer, and H. J\u00e9gou, \u201cWord translation without parallel data,\u201d in Proc. International Conference on Learning Representations (ICLR), 2018.   \n[22] E. Grave, A. Joulin, and Q. Berthet, \u201cUnsupervised alignment of embeddings with wasserstein procrustes,\u201d in Proc. International Conference on Artificial Intelligence and Statistics (AISTATS). PMLR, 2019, pp. 1880\u20131890.   \n[23] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, \u201cAnalysis of representations for domain adaptation,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 19, 2006.   \n[24] M. Long, Y. Cao, J. Wang, and M. Jordan, \u201cLearning transferable features with deep adaptation networks,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2015, pp. 97\u2013105.   \n[25] Y. Ganin and V. Lempitsky, \u201cUnsupervised domain adaptation by backpropagation,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2015, pp. 1180\u20131189.   \n[26] S. Waaijenborg, P. C. Verselewel de Witt Hamer, and A. H. Zwinderman, \u201cQuantifying the association between gene expressions and dna-markers by penalized canonical correlation analysis,\u201d Statistical applications in genetics and molecular biology, vol. 7, no. 1, 2008.   \n[27] K. D. Yang, A. Belyaeva, S. Venkatachalapathy, K. Damodaran, A. Katcoff, A. Radhakrishnan, G. Shivashankar, and C. Uhler, \u201cMulti-domain translation between single-cell imaging and sequencing data using autoencoders,\u201d Nature communications, vol. 12, no. 1, p. 31, 2021.   \n[28] I. Gulrajani and T. Hashimoto, \u201cIdentifiability conditions for domain adaptation,\u201d in Proc. International Conference on Machine Learning (ICML), 2022, pp. 7982\u20137997.   \n[29] S. Shrestha and X. Fu, \u201cTowards identifiable unsupervised domain translation: A diversified distribution matching approach,\u201d in Proc. International Conference on Learning Representations (ICLR), 2024.   \n[30] Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha, \u201cStarGAN v2: Diverse image synthesis for multiple domains,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 8188\u20138197.   \n[31] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, \u201cMultimodal unsupervised image-to-image translation,\u201d in Proc. European Conference on Computer Vision (ECCV), 2018, pp. 172\u2013189.   \n[32] S. Xie, L. Kong, M. Gong, and K. Zhang, \u201cMulti-domain image generation and translation with identifiability guarantees,\u201d in Proc. International Conference on Learning Representations (ICLR), 2023.   \n[33] P. Comon, \u201cIndependent component analysis, a new concept?\u201d Signal processing, vol. 36, no. 3, pp. 287\u2013314, 1994.   \n[34] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.   \n[35] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable visual models from natural language supervision,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2021, pp. 8748\u20138763.   \n[36] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, \u201cStarGAN: Unified generative adversarial networks for multi-domain image-to-image translation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8789\u20138797.   \n[37] X. Fu, K. Huang, N. D. Sidiropoulos, and W.-K. Ma, \u201cNonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications,\u201d IEEE Signal Processing Magazine, vol. 36, no. 2, pp. 59\u201380, 2019.   \n[38] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola, \u201cA kernel two-sample test,\u201d The Journal of Machine Learning Research, vol. 13, no. 1, pp. 723\u2013773, 2012.   \n[39] C. Villani et al., Optimal transport: old and new. Springer, 2009, vol. 338.   \n[40] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial networks,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2014.   \n[41] L. Kong, S. Xie, W. Yao, Y. Zheng, G. Chen, P. Stojanov, V. Akinwande, and K. Zhang, \u201cPartial disentanglement for domain adaptation,\u201d in Proc. International Conference on Machine Learning (ICML), 2022, pp. 11 455\u201311 472.   \n[42] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2020, pp. 1597\u20131607.   \n[43] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., \u201cMatching networks for one shot learning,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.   \n[44] E. Triantafillou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. Swersky, P.-A. Manzagol et al., \u201cMeta-dataset: A dataset of datasets for learning to learn from few examples,\u201d in Proc. International Conference on Learning Representations (ICLR), 2019.   \n[45] S. Wold, K. Esbensen, and P. Geladi, \u201cPrincipal component analysis,\u201d Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp. 37\u201352, 1987.   \n[46] J. Li and J. M. Bioucas-Dias, \u201cMinimum volume simplex analysis: A fast algorithm to unmix hyperspectral data,\u201d in IEEE International Geoscience and Remote Sensing Symposium, vol. 3, 2008, pp. III\u2013250.   \n[47] X. Fu, K. Huang, B. Yang, W.-K. Ma, and N. D. Sidiropoulos, \u201cRobust volume minimizationbased matrix factorization for remote sensing and document clustering,\u201d IEEE Transactions on Signal Processing, vol. 64, no. 23, pp. 6254\u20136268, 2016.   \n[48] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu, \u201cA practical algorithm for topic modeling with provable guarantees,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2013, pp. 280\u2013288.   \n[49] K. Huang, X. Fu, and N. D. Sidiropoulos, \u201cAnchor-free correlated topic modeling: Identifiability and algorithm,\u201d Advances in Neural Information Processing Systems, vol. 29, 2016.   \n[50] X. Mao, P. Sarkar, and D. Chakrabarti, \u201cEstimating mixed memberships with sharp eigenvector deviations,\u201d Journal of the American Statistical Association, vol. 116, no. 536, pp. 1928\u20131940, 2021.   \n[51] L. Gresele, P. K. Rubenstein, A. Mehrjou, F. Locatello, and B. Sch\u00f6lkopf, \u201cThe incomplete Rosetta stone problem: Identifiability results for multi-view nonlinear ICA,\u201d in Proc. Uncertainty in Artificial Intelligence, 2020, pp. 217\u2013227.   \n[52] C. Eastwood, J. von K\u00fcgelgen, L. Ericsson, D. Bouchacourt, P. Vincent, M. Ibrahim, and B. Sch\u00f6lkopf, \u201cSelf-supervised disentanglement by leveraging structure in data augmentations,\u201d in Causal Representation Learning Workshop at NeurIPS 2023, 2023.   \n[53] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \u201cUnpaired image-to-image translation using cycleconsistent adversarial networks,\u201d in Proc. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2223\u20132232.   \n[54] M. Long, H. Zhu, J. Wang, and M. I. Jordan, \u201cDeep transfer learning with joint adaptation networks,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2017, pp. 2208\u20132217.   \n[55] W. Wang, H. Zhang, Z. Yuan, and C. Wang, \u201cUnsupervised real-world super-resolution: A domain adaptation perspective,\u201d in Proc. IEEE International Conference on Computer Vision (ICCV), 2021, pp. 4318\u20134327.   \n[56] T. Galanti, L. Wolf, and S. Benaim, \u201cThe role of minimal complexity functions in unsupervised learning of semantic mappings,\u201d in Proc. International Conference on Learning Representations (ICLR), 2018.   \n[57] N. Moriakov, J. Adler, and J. Teuwen, \u201cKernel of CycleGAN as a principle homogeneous space,\u201d in Proc. International Conference on Learning Representations (ICLR), 2020.   \n[58] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, \u201cAdapting visual category models to new domains,\u201d in Proc. European Conference on Computer Vision (ECCV). Springer, 2010, pp. 213\u2013226.   \n[59] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan, \u201cDeep hashing network for unsupervised domain adaptation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5018\u20135027.   \n[60] Y. Zhang, T. Liu, M. Long, and M. Jordan, \u201cBridging theory and algorithm for domain adaptation,\u201d in Proc. International Conference on Machine Learning (ICML), 2019.   \n[61] Y. Jin, X. Wang, M. Long, and J. Wang, \u201cMinimum class confusion for versatile domain adaptation,\u201d in Proc. IEEE International Conference on Computer Vision (ICCV). Springer, 2020, pp. 464\u2013480.   \n[62] H. Rangwani, S. K. Aithal, M. Mishra, A. Jain, and V. B. Radhakrishnan, \u201cA closer look at smoothness in domain adversarial training,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2022, pp. 18 378\u201318 399.   \n[63] Y. Zhang, J. Liang, Z. Zhang, L. Wang, R. Jin, T. Tan et al., \u201cFree lunch for domain adversarial training: Environment label smoothing,\u201d in Proc. International Conference on Learning Representations (ICLR), 2023.   \n[64] L. Eyring, D. Klein, T. Uscidda, G. Palla, N. Kilbertus, Z. Akata, and F. Theis, \u201cUnbalancedness in neural monge maps improves unpaired domain translation,\u201d arXiv preprint arXiv:2311.15100, 2023.   \n[65] M. Amodio and S. Krishnaswamy, \u201cMAGAN: Aligning biological manifolds,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2018, pp. 215\u2013223.   \n[66] J. Cao, D. A. Cusanovich, V. Ramani, D. Aghamirzaie, H. A. Pliner, A. J. Hill, R. M. Daza, J. L. McFaline-Figueroa, J. S. Packer, L. Christiansen et al., \u201cJoint profiling of chromatin accessibility and gene expression in thousands of single cells,\u201d Science, vol. 361, no. 6409, pp. 1380\u20131385, 2018.   \n[67] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, and T. Mikolov, \u201cFasttext.zip: Compressing text classification models,\u201d arXiv preprint arXiv:1612.03651, 2016.   \n[68] K. B. Athreya and S. N. Lahiri, Measure theory and probability theory. Springer, 2006, vol. 19.   \n[69] J.-F. Cardoso, \u201cSuper-symmetric decomposition of the fourth-order cumulant tensor. blind identification of more sources than sensors.\u201d in Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP), vol. 91, 1991, pp. 3109\u20133112.   \n[70] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR, 2015.   \n[71] A. L. Maas, A. Y. Hannun, and A. Y. Ng, \u201cRectifier nonlinearities improve neural network acoustic models,\u201d in Proc. International Conference on Machine Learning (ICML), vol. 30, no. 1, 2013, p. 3.   \n[72] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang, \u201cDeep clustering via joint convolutional autoencoder embedding and relative entropy minimization,\u201d in Proc. the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 5736\u20135745.   \n[73] J. Xie, R. Girshick, and A. Farhadi, \u201cUnsupervised deep embedding for clustering analysis,\u201d in Proc. International Conference on Machine Learning (ICML). PMLR, 2016, pp. 478\u2013487.   \n[74] W. Wang, X. Yan, H. Lee, and K. Livescu, \u201cDeep variational canonical correlation analysis,\u201d arXiv preprint arXiv:1610.03454, 2016.   \n[75] A. Gretton, K. Fukumizu, C. Teo, L. Song, B. Sch\u00f6lkopf, and A. Smola, \u201cA kernel statistical test of independence,\u201d in Advances in Neural Information Processing Systems (NeurIPS), vol. 20, 2007.   \n[76] T. M. Cover, Elements of information theory. John Wiley & Sons, 1999. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material of \u201cIdentifiable Shared Component Analysis of Unpaired Multimodal Mixtures\u201d ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The notations used throughout the paper are summarized in the Table 4.: ", "page_idx": 14}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/02895fce3cf10c564ddb6d23d830549f342026bdb795894649005165f31a560e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We restate the theorem here: ", "page_idx": 15}, {"type": "text", "text": "Theorem 1 Under Assumption 1 and the generative model in (1), denote any solution of (6) as $\\widehat{\\boldsymbol{Q}}^{(q)}\\boldsymbol{q}=1,2$ . Then, if the mixing matrices $A^{(q)}$ are full column ranks and $\\mathbb{E}[c c^{\\top}]\\}$ is full rank, w e have $\\widehat{\\boldsymbol{Q}}^{(q)}\\boldsymbol{\\mathbf{x}}^{(q)}=\\boldsymbol{\\Theta}^{(q)}\\boldsymbol{c}$ . In addition, assume that either of the following is satisfied: ", "page_idx": 15}, {"type": "text", "text": "(a) The individual elements of the content components are statistically independent and (d) (d) non-Gaussian. In addition, $c_{i}\\;\\neq\\;k c_{j},\\forall i\\;\\neq\\;j,\\forall k\\;\\in\\;\\mathbb{R}$ and $c_{i}\\ \\neq\\ -c_{i},\\forall i$ , i.e., the marginal distributions of the content elements cannot be matched with each other by mere scaling. ", "page_idx": 15}, {"type": "text", "text": "(b) The support $\\mathcal{C}$ is a hyper-rectangle, i.e., $\\mathcal{C}=[-a_{1},a_{1}]\\times\\cdot\\cdot\\cdot\\times[-a_{d_{\\mathrm{C}}},a_{d_{\\mathrm{C}}}]$ . Further, $c_{i}\\overset{(\\mathsf{d})}{\\neq}k c_{j},\\forall i\\neq j,\\forall k\\in\\mathbb{R}$ (d) suppose that and $c_{i}\\neq-c_{i},\\forall i$ . ", "page_idx": 15}, {"type": "text", "text": "Then, we have $\\widehat{\\pmb{Q}}^{(q)}{\\pmb{x}}^{(q)}=\\pmb{\\Theta}c$ , i.e., $\\Theta^{(q)}=\\Theta$ for all $q=1,2$ , where $\\Theta^{(q)}$ . ", "page_idx": 15}, {"type": "text", "text": "We will prove the theorem in following two steps. For the first step we will prove $\\widehat{\\boldsymbol{Q}}^{(q)}\\boldsymbol{\\mathbf{x}}^{(q)}=\\boldsymbol{\\Theta}^{(q)}\\boldsymbol{c}$ and for second step we will employ either assumption (a) or (b) to prove that $\\Theta^{(q)}=\\Theta,\\;\\forall q=1,2.$ . ", "page_idx": 15}, {"type": "text", "text": "B.1 Linearly transformed content identification ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let us define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{H}^{(q)}=\\pmb{Q}^{(q)}\\pmb{A}^{(q)}\\in\\mathbb{R}^{d_{\\mathbf{C}}\\times(d_{\\mathbf{C}}+d_{\\mathbf{P}}^{(q)})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We want to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Null}(H^{(q)})=\\mathbf{0}\\times\\mathbb{R}^{d_{\\mathrm{P}}^{(q)}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since this will imply that that $H^{(q)}$ does not depend upon the style component. Combined with the fact that $\\mathrm{rank}(H^{(q)})\\,=\\,d_{\\mathrm{C}}$ , this will imply that $\\bar{H^{(q)}}$ is an invertible function of the content component. To that end, consider the following line of arguments. ", "page_idx": 15}, {"type": "text", "text": "Since the objective in (6) matches the distribution for latent random variables $\\widehat{\\pmb{c}}^{(1)}={\\pmb Q}^{(1)}{\\pmb x}^{(1)}$ and $\\widehat{\\pmb{c}}^{(2)}={\\pmb Q}^{(2)}\\dot{\\pmb x}^{(2)}$ , the following holds for any $\\mathcal{R}_{c}\\subseteq\\mathbb{R}^{d_{\\mathrm{C}}},\\forall k\\in\\mathbb{R}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}_{\\widehat{c}^{(1)}}[k\\mathcal{R}_{c}]=\\mathbb{P}_{\\widehat{c}^{(2)}}[k\\mathcal{R}_{c}],\\ \\ \\ \\ \\ }\\\\ &{\\Longleftrightarrow}&{\\mathbb{P}_{z^{(1)}}[H_{\\mathrm{PreImg}}^{(1)}(k\\mathcal{R}_{c})]=\\mathbb{P}_{z^{(2)}}[H_{\\mathrm{PreImg}}^{(2)}(k\\mathcal{R}_{c})]\\ \\ \\ }\\\\ &{\\Longleftrightarrow}&{\\mathbb{P}_{z^{(1)}}[k H_{\\mathrm{PreImg}}^{(1)}(\\mathcal{R}_{c})]=\\mathbb{P}_{z^{(2)}}[k H_{\\mathrm{PreImg}}^{(2)}(\\mathcal{R}_{c})],\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where, $H_{\\mathrm{PreImg}}^{(q)}(\\mathcal{R}_{c}):=\\{z^{(q)}\\ |\\ H^{(q)}z^{(q)}\\in\\mathcal{R}_{c}\\}$ is the pre-image of $H^{(q)}$ . $(a)$ follows because $\\mathbb{P}_{\\widehat{c}^{(q)}}[k\\mathcal{R}_{c}]=\\mathbb{P}_{H^{(q)}z^{(q)}}[k\\mathcal{R}_{c}]=\\mathbb{P}_{z^{(q)}}[H_{\\mathrm{PreImg}}^{(q)}(k\\mathcal{R}_{c})]$ [68, Section 2.2]. $(b)$ follows because $H^{(q)}$ is a linear operator. ", "page_idx": 15}, {"type": "text", "text": "Although (11) holds for any $\\mathcal{R}_{c}$ , we will see that it is sufficient to consider a special $\\mathcal{R}_{c}$ to prove (10). To that end, take $\\mathcal{R}_{c}=\\mathrm{conv}\\{\\mathbf{0},\\mathbf{a}_{1},\\dots,\\mathbf{a}_{d_{\\mathrm{C}}}\\}$ , where $\\mathbf{a}_{i}\\in\\mathbb{R}^{d_{\\mathrm{C}}}$ such that $\\mathbb{P}_{\\widehat{\\mathbf{c}}^{\\left(q\\right)}}\\left[\\mathcal{R}_{c}\\right]>0$ . Let us take $y_{i}^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)}}$ , such that $\\pmb{H}^{(q)}\\pmb{y}_{i}^{(q)}=\\pmb{a}_{i}$ . For reasons that will be clear lat er, we hope to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\mathrm{PreImg}}^{(q)}(\\mathcal{R}_{c})=\\mathrm{conv}\\{\\mathbf{0},y_{1}^{(q)},\\dots,y_{d_{\\mathrm{C}}}^{(q)}\\}+\\mathrm{Null}(H^{(q)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To that end, observe that for any $\\boldsymbol{r}\\in\\mathcal{R}_{c}$ , we can represent $\\pmb{r}$ as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{r}=\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{a}_{i},\\;\\mathrm{for}\\;\\mathrm{some}\\;\\{w_{i}\\}_{i=1}^{d_{\\mathrm{C}}}\\;\\mathit{s.t.}\\;\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\leq1,\\;\\forall i.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For both view $q=1,2$ , we get, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbf{\\displaystyler}=\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{H}^{(q)}\\pmb{y}_{i}^{(q)}}\\\\ {\\implies r=\\displaystyle\\pmb{H}^{(q)}\\left(\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{y}_{i}^{(q)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nH_{\\mathrm{PreImg}}^{(q)}\\left(\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{a}_{i}\\right)=\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{y}_{i}^{(q)}+\\mathrm{Null}(\\pmb{H}^{(q)})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can write, ", "page_idx": 16}, {"type": "equation", "text": "$$\nH_{\\mathrm{PreImg}}^{(q)}(\\mathcal{R}_{c})=\\mathrm{conv}\\{\\mathbf{0},y_{1}^{(q)},\\dots,y_{d_{\\mathrm{C}}}^{(q)}\\}+\\mathrm{Null}(H^{(q)})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have that $\\mathrm{Null}(H^{(q)})\\subset\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)}}$ is a linear subspace with $\\dim(\\operatorname{Null}(H^{(q)}))=d_{\\mathrm{P}}^{(q)}$ . Let $A^{(q)}=$ $H_{\\mathrm{PreImg}}^{(q)}(\\mathcal{R}_{c})$ . Note that $\\mathbb{P}_{z^{(1)}}[k\\pmb{A}^{(1)}]=\\mathbb{P}_{z^{(2)}}[k\\pmb{A}^{(2)}],\\forall k\\in\\mathbb{R}$ (from (11), and $\\mathbb{P}_{\\pmb{z}^{(q)}}[\\pmb{A}^{(q)}]>0$ (by the construction of $\\mathcal{R}_{c.}$ ). Further, the set $\\boldsymbol{\\mathcal{A}}^{(q)}$ is of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{conv}\\{\\mathbf{0},\\pmb{y}_{1}^{(q)},\\dots,\\pmb{y}_{d_{\\mathrm{C}}}^{(q)}\\}+\\mathcal{P}^{(q)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "because $\\mathrm{Null}(H^{(q)})$ is a subspace of dimension $d_{\\mathrm{P}}^{(q)}$ , hence it satisfies the definition of $\\mathcal{P}^{(q)}$ . Hence, Assumption 1 implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Null}(H^{(q)})=\\mathbf{0}\\times\\mathbb{R}^{d_{\\mathrm{P}}^{(q)}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Denoting the $N$ th to $M$ th columns of $H^{(q)}$ by $H^{(q)}(N:M)$ , the above is equivalent to saying ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{H}^{(q)}(d_{C}+1:d_{C}+d_{P}^{(q)})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Denote, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Theta^{(q)}=H^{(q)}(1:d_{\\cal{C}})\\,\\forall\\,q=1,2.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we can write, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{Q}^{(q)}\\boldsymbol{\\mathbf{x}}^{(q)}=\\boldsymbol{\\Theta}^{(q)}\\boldsymbol{\\mathbf{c}},\\;\\forall\\,q=1,2.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we use Assumption (a) or (b) to show that $\\Theta^{(1)}\\,=\\,\\Theta^{(2)}\\,=\\,\\Theta$ . To that end, note that the distribution matching constraint implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Theta^{(1)}c\\,\\frac{(\\mathsf{d})}{}\\,\\Theta^{(2)}c}}\\\\ {{\\Longrightarrow c\\,\\frac{(\\mathsf{d})}{}\\,(\\Theta^{(1)})^{-1}\\Theta^{(2)}c.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence $M=(\\Theta^{(1)})^{-1}\\Theta^{(2)}$ is an invertible matrix such that $c\\,{\\stackrel{\\mathrm{~(d)~}}{=}}\\,M c$ . However, in the following, we will show that if either Assumption (a) or (b) is satisfied, then $M=I$ , and thus $\\Theta^{(1)}=\\Theta^{(2)}$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Considering Assumption (a) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We want to show that when Assumption (a) is satisfied, if $M c\\overset{\\vartriangle}{=}c$ for any invertible $_M$ , then $M=I$ . ", "page_idx": 16}, {"type": "text", "text": "Note that $M c=[\\pmb{m}_{1}\\ldots\\pmb{m}_{d_{\\mathrm{C}}}]\\,\\left[\\begin{array}{c}{c_{1}}\\\\ {\\vdots}\\\\ {c_{d_{\\mathrm{C}}}}\\end{array}\\right].$ By Assumption (a), we have that the components of content are statistically independent $c_{i}\\ \\perp\\!\\!\\!\\perp c_{j}$ , $i\\ne j$ , non-Gaussian, and has non-zero kurtosis. Then, according to cumulant multilinearity and additivity properties, the fourth order cumulant tensor $\\operatorname{Cum}(M\\boldsymbol{c})$ of $M c$ has the following unique decomposition [69], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Cum}(M\\pmb{c})=\\sum_{i=1}^{d_{\\mathrm{C}}}\\kappa_{i}\\pmb{m}_{i}\\circ\\pmb{m}_{i}\\circ\\pmb{m}_{i}\\circ\\pmb{m}_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\circ$ is the outer product, $\\kappa_{i}$ is the kurtosis of component $c_{i}$ , and $m_{i},\\,\\,i\\in[d_{\\mathrm{C}}]$ are the columns of $_M$ . ", "page_idx": 17}, {"type": "text", "text": "Since $M c\\overset{\\vartriangle}{=}c$ , the following should hold ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Cum}(M c)=\\mathrm{Cum}(c)=\\mathrm{Cum}(I c)\\,}\\\\ {\\displaystyle\\implies\\sum_{d=1}^{d_{\\mathrm{C}}}\\kappa_{d}\\,\\pmb{m}_{d}\\circ\\pmb{m}_{d}\\circ\\pmb{m}_{d}=\\sum_{d=1}^{d_{\\mathrm{C}}}\\kappa_{d}\\,\\pmb{e}_{d}\\circ\\pmb{e}_{d}\\circ\\pmb{e}_{d}\\circ\\pmb{e}_{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$e_{i}$ is the ith column of identity matrix $\\boldsymbol{\\mathit{I}}$ . ", "page_idx": 17}, {"type": "text", "text": "Because of statistical independence of components of $^c$ , the CP-decomposition of $\\mathrm{Cum}(M{\\boldsymbol{c}})=$ $\\mathrm{Cum}(I c)$ is unique [69] upto permutation and scaling ambiguities, i.e., $_M$ should be a permutation scaling matrix. ", "page_idx": 17}, {"type": "text", "text": "Let $M=\\Pi\\Sigma$ where, $\\mathbf{I}\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times d_{\\mathrm{C}}}$ is a permutation matrix and $\\Sigma=\\mathrm{Diag}(r_{1},\\mathbf{\\Omega},\\mathbf{\\Omega}..\\mathbf{\\Omega}.r_{d_{\\mathrm{C}}})\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times d_{\\mathrm{C}}}$ is a diagonal scaling matrix. ", "page_idx": 17}, {"type": "text", "text": "Finally, since $c_{i}\\neq\\frac{\\langle{\\mathsf{d}}\\rangle}{2}\\,k c_{j},\\forall i\\neq j,\\forall k\\in\\mathbb{R},M$ has to to be identity matrix. To see the reason, for the sake of contradiction, suppose that either (i) there exist $i,j\\in[d_{\\mathrm{C}}]^{'}\\times[d_{\\mathrm{C}}]$ and $k\\in\\mathbb{R}$ , with $i\\neq j$ such that $[M\\pmb{c}]_{i}=k c_{j}$ , or (ii) $\\exists i\\in[d_{\\mathrm{C}}]$ such that $[M\\pmb{c}]_{i}=k c_{i}$ for some $k\\in\\mathbb{R},k\\neq1$ . ", "page_idx": 17}, {"type": "text", "text": "For case (i), since $M c\\overset{\\vartriangle}{=}c$ , $[M\\mathbf{c}]_{i}\\overset{(\\mathsf{d})}{=}c_{i},\\forall i$ . Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{[M\\pmb{c}]_{i}=k c_{j}}}\\\\ {{\\Longrightarrow[M\\pmb{c}]_{i}\\stackrel{(\\mathrm{d})}{=}k c_{j}}}\\\\ {{\\Longrightarrow c_{i}\\stackrel{(\\mathrm{d})}{=}k c_{j},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is a contradiction to the assumption $c_{i}\\neq k c_{j}$ . ", "page_idx": 17}, {"type": "text", "text": "For case (ii), $[M\\pmb{c}]_{i}=k c_{j}$ implies that $c_{i}\\overset{\\vartriangle}{=}k c_{j}$ . First, $k\\neq\\pm1$ , cannot hold because it will mean that $\\mathrm{var}(c_{i})=k^{2}\\mathrm{var}(c_{i})$ which cannot hold for $k\\neq\\pm1$ since $\\mathrm{var}(c_{i})>0$ . Hence, the only possible (d) option is $k=-1$ , which is already ruled out by the assumption that $c_{i}\\ \\neq\\ -c_{i}$ . Hence $_M$ is an identity matrix. This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "B.3 Considering Assumption (b) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{e}_{i}=[0,0,\\,\\dots,\\underbrace{1}_{i\\mathrm{th\\,location}},0,0]\\in\\mathbb{R}^{d_{\\mathrm{C}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "denote the standard basis vector in $\\mathbb{R}^{d_{\\mathrm{C}}}$ . Let vertex of hyper-rectangle $\\pmb{v}_{i}=a_{i}\\pmb{e}_{i}=\\pmb{\\Lambda}\\pmb{e}_{i}$ , where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\mathrm{Diag}([a_{1},\\ldots,a_{d_{\\mathrm{C}}}]^{T}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathrm{Diag}(\\cdot)$ represents the diagonal matrix formed by the given vector. ", "page_idx": 17}, {"type": "text", "text": "If Mc=(d=)= c, then the supports of Mc and c should match, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\nM({\\mathcal{C}})={\\mathcal{C}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $M(\\mathcal{C})=\\left\\{M c\\,|\\,c\\in\\mathcal{C}\\right\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Note that $\\forall c\\in{\\mathcal{C}}$ , the set of points $\\pmb{v}_{i}$ satisfy the following property ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{}}&{{c=\\displaystyle\\sum_{i=1}^{d_{\\mathrm{C}}}\\alpha_{i}{\\pmb v}_{i},\\qquad\\mathrm{for\\;some\\;-1\\leq\\alpha_{i}\\leq1}}}\\\\ {{}}&{{}}&{{}}\\\\ {{\\implies M c=\\displaystyle\\sum_{i=1}^{d_{\\mathrm{C}}}\\alpha_{i}(M{\\pmb v}_{i}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since the support of $M c$ is $\\mathcal{C}$ , this implies that $\\forall c\\in{\\mathcal{C}}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nc=\\sum_{i=1}^{d_{\\mathrm{C}}}\\alpha_{i}(M v_{i}),\\qquad\\mathrm{for~some~}-1\\leq\\alpha_{i}\\leq1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last equation implies that the set of points $M v_{i},\\forall i\\in[d_{\\mathrm{C}}]$ also satisfy property (19). Hence, for each $i\\in[d_{\\mathrm{C}}]$ , $M v_{i}=\\pm v_{j}$ for some unique $j\\in[d_{\\mathrm{C}}]$ . Note that $j$ should be unique for each $i$ because $_M$ is invertible, hence $_M$ cannot map two orthogonal vectors $\\pmb{v}_{i}$ and $\\pmb{v}_{k}$ , $i\\neq k$ , to the same vector $\\pm\\pmb{v}_{j}$ with same or different signs. ", "page_idx": 18}, {"type": "text", "text": "Let $V=[\\pmb{v}_{1},\\dots,\\pmb{v}_{d_{\\mathrm{C}}}]^{T}$ . Then one can write ", "page_idx": 18}, {"type": "equation", "text": "$$\nM V=V\\Sigma\\Pi,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Sigma$ is some diagonal matrix with diagonal entries from $\\{+1,-1\\}$ and $\\mathbf{\\delta}\\pi$ is a permutation matrix. Then the above implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M\\Lambda I=\\Lambda I\\Sigma\\Pi}\\\\ &{\\implies M=\\Lambda\\Sigma\\Pi\\Lambda^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence $_M$ is a permutation and scaling matrix. ", "page_idx": 18}, {"type": "text", "text": "Finally, by the same argument presented in last paragraph of Sec. B.2 (i.e., proof with Assumption (a)), we conclude that $_M$ is an identity matrix. ", "page_idx": 18}, {"type": "text", "text": "C Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We restate the theorem here: ", "page_idx": 18}, {"type": "text", "text": "Theorem 2 Consider the mixture model in (8). Assume that $\\operatorname{rank}(A)\\;=\\;d_{\\mathrm{C}}\\,+\\,d_{\\mathrm{P}}$ and $\\mathrm{rank}(\\mathbb{E}[c c^{\\top}])=d_{\\mathrm{C}}$ , and that Assumption 2 holds. Denote $\\widehat{\\pmb{Q}}$ as any solution of (6) by constraining $Q=Q^{(1)}=Q^{(2)}$ . Then, we have $\\widehat{\\mathbf{Q}}\\mathbf{x}^{\\left(q\\right)}=\\mathbf{\\Theta}\\mathbf{c}$ . ", "page_idx": 18}, {"type": "text", "text": "One can follow the same argument as in the step 1 of proof in $\\mathbf{B}$ ", "page_idx": 18}, {"type": "text", "text": "Let us define ", "page_idx": 18}, {"type": "equation", "text": "$$\nH=Q A\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times(d_{\\mathrm{C}}+d_{\\mathrm{P}})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We want to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Null}(H)=\\mathbf{0}\\times\\mathbb{R}^{d_{\\mathrm{P}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since this will imply that that $\\pmb{H}$ does not depend upon the style component. Combined with the fact that $\\operatorname{rank}(H)=d_{\\mathrm{C}}$ , this will imply that $\\pmb{H}$ is an invertible function of the content component. To that end, consider the following line of arguments. ", "page_idx": 18}, {"type": "text", "text": "Since the objective in (6) matches the distribution for latent random variables $\\widehat{\\pmb{c}}^{(1)}=Q\\pmb{x}^{(1)}$ and $\\widehat{\\pmb{c}}^{(2)}=Q\\pmb{x}^{(2)}$ , the following holds for any $\\mathcal{R}_{c}\\subseteq\\mathbb{R}^{d_{\\mathrm{C}}},\\exists\\,k\\in\\mathbb{R}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}_{\\widehat{c}^{(1)}}[k\\mathcal{R}_{c}]=\\mathbb{P}_{\\widehat{c}^{(2)}}[k\\mathcal{R}_{c}],\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\iff}&{\\mathbb{P}_{z^{(1)}}[H_{\\mathrm{PreImg}}(k\\mathcal{R}_{c})]=\\mathbb{P}_{z^{(2)}}[H_{\\mathrm{PreImg}}(k\\mathcal{R}_{c})]}\\\\ &{\\overset{(b)}{\\iff}\\mathbb{P}_{z^{(1)}}[k H_{\\mathrm{PreImg}}(\\mathcal{R}_{c})]=\\mathbb{P}_{z^{(2)}}[k H_{\\mathrm{PreImg}}(\\mathcal{R}_{c})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where, $H_{\\mathrm{PreImg}}(\\mathcal{R}_{c}):=\\left\\{z\\mid H z\\in\\mathcal{R}_{c}\\right\\}$ is the pre-image of $H.\\ (a)$ follows because $\\mathbb{P}_{\\widehat{\\mathbf{c}}^{}(q)}\\left[k\\mathcal{R}_{c}\\right]=$ $\\mathbb{P}_{H z^{(q)}}[k\\mathcal{R}_{c}]=\\mathbb{P}_{z^{(q)}}[H_{\\mathrm{PreImg}}(k\\mathcal{R}_{c})]$ [68, Section 2.2]. $(b)$ follows because $\\pmb{H}$ is a line ar operation. ", "page_idx": 18}, {"type": "text", "text": "Although (21) holds for any $\\mathcal{R}_{c}$ , we will see that it is sufficient to consider a special $\\mathcal{R}_{c}$ to prove (20). To that end, take $\\mathcal{R}_{c}=\\mathrm{conv}\\{\\mathbf{0},\\mathbf{a}_{1},\\dots,\\mathbf{a}_{d_{\\mathrm{C}}}\\}$ , where $\\mathbf{a}_{i}\\in\\mathbb{R}^{d_{\\mathrm{C}}}$ such that $\\mathbb{P}_{\\widehat{\\mathbf{c}}^{}(q)}\\left[\\mathcal{R}_{c}\\right]>0$ . Let us take $y_{i}\\in\\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}}$ , such that $\\pmb{H}\\pmb{y}_{i}=\\pmb{a}_{i}$ . For reasons that will be clear later, we  hope to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\mathrm{PreImg}}(\\mathcal{R}_{c})=\\operatorname{conv}\\{\\mathbf{0},y_{1},\\dots,y_{d_{\\mathrm{C}}}\\}+\\mathrm{Null}(H).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To that end, observe that for any $\\pmb{r}\\in\\mathcal{R}_{c}$ , we can represent $\\pmb{r}$ as, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{r}=\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{a}_{i},\\;\\mathrm{for}\\;\\mathrm{some}\\;\\{w_{i}\\}_{i=1}^{d_{\\mathrm{C}}}\\;\\mathit{s.t.}\\;\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\leq1,\\;\\forall i.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For both view $q=1,2$ , we get, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{\\displaystyle{\\mathbf{\\boldsymbol{r}}=\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{\\cal H}\\boldsymbol{y}_{i}}}\\\\ {\\displaystyle{\\implies r=\\pmb{\\cal H}\\left(\\frac{1}{d_{\\mathrm{C}}+1}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{\\mathscr{y}}_{i}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{\\mathrm{PreImg}}\\left({\\frac{1}{d_{\\mathrm{C}}+1}}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{a}_{i}\\right)={\\frac{1}{d_{\\mathrm{C}}+1}}\\sum_{i=1}^{d_{\\mathrm{C}}}w_{i}\\pmb{y}_{i}+\\mathrm{N}{\\mathrm{ull}}(\\pmb{H})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can write, ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{\\mathrm{PreImg}}(\\mathcal{R}_{c})=\\operatorname{conv}\\{\\mathbf{0},y_{1},\\hdots,y_{d_{\\mathrm{C}}}\\}+\\operatorname{Null}(H)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have that $\\mathrm{Null}(H)\\ \\subset\\ \\mathbb{R}^{d_{\\mathrm{C}}+d_{\\mathrm{P}}}$ is a linear subspace with $\\dim(\\operatorname{Null}(H))\\ =\\ d_{\\operatorname{P}}$ . Let ${\\mathcal{A}}=$ $H_{\\mathrm{PreImg}}(\\mathcal{R}_{c})$ . Note that $\\mathbb{P}_{z^{(1)}}[k\\pmb{A}]\\,=\\,\\mathbb{P}_{z^{(2)}}[k\\pmb{A}],\\forall k\\,\\in\\,\\mathbb{R}$ (from (21), and $\\mathbb{P}_{\\pmb{z}^{(q)}}[\\mathcal{A}]>0$ (by the construction of $\\mathcal{R}_{c.}$ ). Further, the set $\\boldsymbol{\\mathcal{A}}$ is of the form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{conv}\\{\\mathbf{0},y_{1},\\dots,y_{d_{\\mathrm{C}}}\\}+\\mathcal{P},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "because $\\operatorname{Null}(H)$ is a subspace of dimension $d_{\\mathrm{P}}$ , hence it satisfies the definition of $\\mathcal{P}$ . Hence, Assumption 2 implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{Null}(H)=\\mathbf{0}\\times\\mathbb{R}^{d_{\\mathrm{P}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denoting the $N$ th to $M$ th columns of $\\pmb{H}$ by $H(N:M)$ , the above is equivalent to saying ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(d_{C}+1:d_{C}+d_{P})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denote, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Theta=H(1:d_{C})\\,\\forall\\,v=1,2.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can write, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q\\pmb{x}^{(q)}=\\pmb{\\Theta}\\pmb{c},\\;\\forall\\;v=1,2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "D Proof of Theorem 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We restate the theorem here: ", "page_idx": 19}, {"type": "text", "text": "Theorem 3 Assume that Assumption 1 is satisfied, that $|{\\mathcal{L}}|\\geq d_{\\mathrm{C}}$ paired samples $(x_{\\ell}^{(1)},x_{\\ell}^{(2)})$ are available, that , have full column rank, and that $\\mathbb{P}_{c}$ is absolutely continuous. Denote $(\\widehat{\\boldsymbol{Q}}^{(1)},\\widehat{\\boldsymbol{Q}}^{(2)})$ as any optimal solution of (6) under the constraint (9). Then, we have $\\widehat{\\pmb{Q}}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}c$ . ", "page_idx": 19}, {"type": "text", "text": "From our objective in (6), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{Q}^{(1)}\\pmb{x}^{(1)}\\pmb{\\frac{(\\mathsf{d})}{2}}\\pmb{Q}^{(2)}\\pmb{x}^{(2)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Assumption 1 and following the proof of step 1 in Theorem B, we can obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{Q}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}^{(q)}\\pmb{c},\\;\\forall q=1,2,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some invertible matrices $\\Theta^{(q)},\\forall q$ . Hence, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta^{(1)}c\\,{\\stackrel{\\mathrm{(d)}}{=}}\\,\\Theta^{(2)}c}\\\\ &{\\implies c\\,{\\stackrel{\\mathrm{(d)}}{=}}\\,(\\Theta^{(1)})^{-1}\\Theta^{(2)}c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence we can have linear transformation $M:=(\\Theta^{(1)})^{-1}\\Theta^{(2)}$ which has same probability density as $\\mathbb{P}_{c}$ . However, the sample matching constraint (9), for $\\ell-$ th sample implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad Q^{(1)}x_{\\ell}^{(1)}=Q^{(2)}x_{\\ell}^{(2)}}\\\\ &{\\implies\\Theta^{(1)}c_{\\ell}=\\Theta^{(2)}c_{\\ell}}\\\\ &{\\quad\\implies c_{\\ell}=(\\Theta^{(1)})^{-1}\\Theta^{(2)}c_{\\ell}}\\\\ &{\\implies c_{\\ell}=M c_{\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $C=[c_{1}\\ldots c_{N_{p}}]$ . Then the above implies: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C=M C}}\\\\ {{\\implies(M-I)C={\\bf{0}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we show that $_{C}$ is a full row rank matrix, which implies that $M-I=\\mathbf{0}\\implies M=I$ . To that end, note that random variables $\\pmb{x}_{i}^{(1)}$ and $\\pmb{x}_{i}^{(2)}$ being i.i.d implies that $\\boldsymbol{c}^{(i)}$ are i.i.d from $\\mathbb{P}_{c}$ . This implies that for any $1\\leq i\\leq|{\\mathcal{L}}|$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[c_{i}\\in\\operatorname{span}(\\{c_{n_{1}},\\dots,c_{n_{d_{\\mathrm{C}}-1}}\\})]=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This is because $\\operatorname{span}(\\left\\{c_{n_{1}},\\dotsc,c_{n_{d_{\\mathrm{C}}-1}}\\right\\})$ for $n_{j}\\in[|\\mathcal{L}|]$ , is a lower dimensional subspace in $\\mathbb{R}^{d_{\\mathrm{C}}}$ , which has zero probability under absolutely continuous distribution $\\mathbb{P}_{c}$ . Hence any $d_{\\mathrm{C}}$ out of $|\\mathcal{L}|$ column vectors in $_{C}$ are linearly independent with probability 1. ", "page_idx": 20}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "E Detailed Identifiability Conditions of Existing Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Identifiability of CCA ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 4 (Identifiability of Aligned SCA via CCA [1]). Under (1), assume that every aligned pair $(\\pmb{x}^{(1)},\\pmb{x}^{(2)})$ share the same $c$ , and that $A^{(q)}$ has full column rank. Also assume that there exists an $N$ -sample set $\\{\\ell_{1},\\ldots,\\ell_{N}\\}$ such that $[C^{\\top},(P^{(1)})^{\\top},(P^{(2)})^{\\top}]\\in\\mathbb{R}^{N\\times(d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(1)}+d_{\\mathrm{P}}^{(2)})}$ has full column rank, where $\\pmb{C}=[\\pmb{c}_{\\ell_{1}},\\dots\\pmb{c}_{\\ell_{N}}]\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times N}$ and $\\begin{array}{r}{\\pmb{P}^{\\left(q\\right)}=[\\pmb{p}_{\\ell_{1}}^{\\left(q\\right)}\\cdot\\cdot\\cdot\\pmb{p}_{\\ell_{N}}^{\\left(q\\right)}]\\in\\mathbb{R}^{d_{\\mathrm{P}}^{\\left(q\\right)}\\times N}}\\end{array}$ for $q=1,2$ . Denote $(\\hat{\\boldsymbol{Q}}^{(1)},\\hat{\\boldsymbol{Q}}^{(2)})$ as an optimal solution of the CCA formulation. Then, we we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{Q}}^{(q)}\\pmb{x}^{(q)}=\\pmb{\\Theta}c,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Theta$ is nonsingular. ", "page_idx": 20}, {"type": "text", "text": "In the above theorem, one can see that $N\\geq(d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(1)}+d_{\\mathrm{P}}^{(2)})$ is a necessary condition for the identifiability of $\\Theta c$ . Hence, CCA needs at least $d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(1)}+d_{\\mathrm{P}}^{(2)}$ paired samples for identifiability. ", "page_idx": 20}, {"type": "text", "text": "E.2 Identifiability of Unaligned SCA in [8] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We summarize the result in [8] in the following ", "page_idx": 20}, {"type": "text", "text": "Theorem 5 (Identifiability of Unaligned SCA via ICA [8]). Under (1), assume that the following are met: $(i)$ The conditions for ICA identifiability $[33]$ is met by each modality, including that the components of $z^{(q)}=[c^{\\top},^{\\ast}(p^{(q)})^{\\top}]^{\\top}$ are mutually statistically independent and contain at most one Gaussian variable. In addition, each zi(q)has unit variance; (ii) Pz(q) \u0338= Pz(q ), Pz(q) \u0338= ${\\mathbb P}_{-z_{j}^{(q)}}\\;\\forall i,j\\in[d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)}],\\;i\\;;$ $i\\neq j$ . Then, assume that $(i_{m},j_{m})$ are obtained by ICA followed by cross domain matching (see the part on Unaligned SCA in Section 2 ) for $m=1,\\dots,d_{\\mathrm{C}}$ . ", "page_idx": 20}, {"type": "text", "text": "Denote $\\widehat{c}_{m}^{(1)}=e_{i_{m}}^{\\top}\\widehat{z}^{(1)}$ and $\\widehat{c}_{m}^{(2)}=e_{j_{m}}^{\\top}\\widehat{z}^{(2)}$ . We have the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{c}_{m}^{(q)}=k c_{\\pi(m)}^{(q)},\\;m\\in[d_{\\mathrm{C}}],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $k\\in\\{+1,-1\\}$ and $\\pi$ is a permutation of $\\{1,\\dots,d_{\\mathrm{C}}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "F Additional Synthetic Data Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Hyperparameter Settings: We use Adam optimizer [70] to solve (7) and learn matrices $Q^{(q)},\\;q=$ $1,2$ and the discriminator $f$ . We set the initial learning rate of matrix and discriminator to be 0.009 and 0.00008 respectively. We set the $\\lambda=0.1$ in (7) to enforce (6c). For weak supervision experiment in F, we set $\\beta\\:=\\:0.01$ in (9). We generate total of 100,000 samples in each domain. For our experiment we set the batch size to be 1,000 and run (7) for 50 epochs. Our discriminator is a 6-layer multilayer perceptron (MLP) with hidden units $\\{\\ 1024,521,512,256,128,64\\ \\}$ in each layer. All the layers use leaky ReLU activation functions [71] with a slope of 0.2 except for the last layer which has sigmoid activations. We include a label smoothing coefficient of 0.2 in the discriminator predictions as suggested in [40]. ", "page_idx": 21}, {"type": "text", "text": "Additional Details for Validation of Theorem 1 in Sec. 3: Here we explain the data generation details of the result shown in Fig. 3. For the result in top row, we sample $c_{1}$ from a Gaussian mixture with three Gaussian components. Each component follows a normal distribution $\\mathcal{N}(\\mu,\\mathrm{~2)~}$ where $\\mu\\ \\sim\\ N(0,\\ 10)$ . The second component, i.e., $c_{2}$ , is independently sampled from the gamma distribution ${\\mathfrak{a m m a}}(1,\\ 3)$ . The private components are sampled from $p^{(1)}~\\sim\\mathtt{L a p l a c e(1.0,~6.5)}$ and $p^{(2)}~\\sim\\tt U n i f o r m[-10,~10]$ ], both only having one dimension. In the bottom row, we sample $\\pmb{c}\\in\\mathbb{R}^{2}\\sim\\textsf{V o n M i s e s}(2.5,\\ 2.0)$ distribution. The private components satisfy $p^{(1)}\\sim\\mathtt{L a p l a c e}(1.0,\\ 6.5)$ and $\\boldsymbol{p}^{(2)}~\\sim\\mathtt{G a m m a}(0.5,\\ 3.0)$ . Each element of mixing matrices are sampled from $A_{i j}^{(q)}\\sim\\mathcal{N}(0,1)$ , $q=1,2$ . The readers are referred to Table 4 for the definition of notations used for distributions. ", "page_idx": 21}, {"type": "text", "text": "Validation of Theorem 1 under different sample sizes and imbalanced data: Here we observe the shared component identification performance of the proposed method numerically. We conducted two experiments in different settings. First, we vary the sample sizes in both modalities, but the two modalities have the same sample size. Second, we only vary the sample size of modality 2 while keeping the sample size of modality 1 fixed. This way, we create the data imbalance between modalities. Note that the shared components are identified if the following two conditions are met: ", "page_idx": 21}, {"type": "text", "text": "Therefore, we use the above as our performance evaluation metrics. For the following experiments (Table 5 and 6), we generate the data for the two modalities by sampling a two-dimensional content $_c\\sim$ VonMises(2.5, 2.0) and private components from $p^{(\\mathrm{i})}\\,\\sim\\,\\mathtt{L a p l a c e}(1.0,\\ 6.5)$ and $p^{(2)}~\\sim$ $\\mathtt{G a m m a}(0.5,\\ 3.0)$ . The elements of the mixing matrices are sampled as $A_{i j}^{(q)}\\sim\\mathcal{N}(0,1)$ , $q=1,2$ . We report the mean and standard deviation of $||\\widehat{\\Theta}^{(1)}(1:d_{\\mathrm{C}})-\\widehat{\\Theta}^{(2)}(1:d_{\\mathrm{C}})||_{\\mathrm{F}}^{\\circ}$ and $\\begin{array}{r}{{1}/{2}\\sum_{q=1}^{2}\\|\\widehat{\\Theta}^{(q)}(d_{\\mathrm{C}}:}\\end{array}$ $d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)})\\vert\\vert_{\\mathrm{F}}$ obtained from 5 different run s. ", "page_idx": 21}, {"type": "text", "text": "Table 5 shows the performance of SCA and CCA under different sample sizes (i.e., $N$ ). One can see that the proposed method (SCA) clearly identifies the shared components even when only 100 samples are available. The performance starts to deteriorate when $N\\leq50$ , probably because the min-max optimization problem is difficult to solve with very few samples. CCA does not really work under this setting as it needs aligned cross-domain samples. ", "page_idx": 21}, {"type": "text", "text": "Table 6 shows the performance of SCA in the cases where two modalities have unbalanced data sizes. The number of samples in the first modality is fixed to 100,000 while the second modality\u2019s data size varies from 10,000 to 10 samples. The data generation process remains the same as in the previous experiment. One can see that even under obvious cross-domain data size imbalance (e.g., 100,000 to 1,000), the proposed method performs reasonably well in terms of shared component identification. ", "page_idx": 21}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/64def192da770b08184ef29216cb596b3b3d50c7bdb1fc0678f072f8e27a61b7.jpg", "table_caption": ["Table 5: Shared component identification performance over different $N$ "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/bfa228244e0d12b0b1cb57c3c7daeee66af540ee551af212cac3e515a02cc3bf.jpg", "table_caption": ["Table 6: Shared component identification performance under imbalanced multi-modal data sizes. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Validation of Theorem 3. Fig. 5 presents numerical validation for Theorem 3. ", "page_idx": 22}, {"type": "text", "text": "Data Generation: We set $d_{\\mathrm{C}}\\,=\\,3$ and $d_{\\mathrm{P}}^{(q)}\\,=\\,1\\$ for $q\\,=\\,1,2$ . We sample each component of shared component $c_{i}\\sim$ Laplace(0.0, 6.5) $i\\,=\\,1,2,3$ , $p^{(1)}\\,\\in\\,\\mathbb{R}^{1}\\mathrm{~\\Omega~}\\sim\\mathrm{~Unif~orm}[-10,\\ 10]\\$ and $\\boldsymbol{p}^{(2)}\\sim\\mathtt{G a m m a}(0.5,\\ 3.0)$ . Although $^c$ satisfies component-wise independence assumption, it does not satisfy the condition that $c_{i}\\neq\\,k c_{j},\\forall i\\neq j$ because $c_{i}\\ {\\stackrel{({\\mathsf{d}})}{=}}\\ c_{j},\\forall i,j\\in|$ [3]. Therefore, Theorem 1 does not cover this case. Nonetheless, this case falls under the jurisdiction of Theorem 3. ", "page_idx": 22}, {"type": "text", "text": "Result: Fig. 5 corroborates with our Theorem 3. That is, one needs at least $|{\\mathcal{L}}|\\ge d_{\\mathrm{C}}=3$ pairs of \u201canchors\u201d (i.e., aligned cross domain pairs) to ensure identifiability of $\\widehat{\\pmb{c}}^{(q)}=\\pmb{\\Theta}\\boldsymbol{c}$ for $q=1,2$ . ", "page_idx": 22}, {"type": "image", "img_path": "ivCX2cjwcT/tmp/c31e3b3aac3eda3c810c6e559772c58d9d0f68f1c51f6184e522f64b88a4443b.jpg", "img_caption": ["Figure 5: Validation of Theorem 3 $d\\mathrm{C}=3$ and $d_{\\mathrm{P}}^{\\mathrm{(1)}}=1$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "G Real Data Experiment Settings and Additional Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Domain Adaptation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Hyperparameter Settings: The domain adaptation task follows the hyperparameter settings described in Table. 7. ", "page_idx": 22}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/1fd9f83e72565f7de2518e91134900603dd047d48aa91be30333a763b553212f.jpg", "table_caption": ["Table 7: Hyperparameter settings for domain adaptation. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/c0ef515ff45b822b3d2fc771f0d62a29a25a6bbfe925d31ede25ef706db7d367.jpg", "table_caption": ["Table 8: Classification accuracy on the target domain of office-31 dataset using CLIP embeddings. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Baselines and Training Setup: The baselines are representative DA methods, namely, DANN [25], MDD [60], MCC [61], SDAT [62], and ELS [63]. We use the implementations of DANN, MDD, and MCC from the https://github.com/thuml/Transfer-Learning-Library, while SDAT and ELS are taken from https://github.com/yfzhang114/Environment-Label-Smoothing. In all the baselines, the classifier is jointly optimized with the feature extractor $Q$ which arguably regularizes towards more classification-friendly geometry of the shared features; see [72, 73]. Following their training strategy, we also append a cross-entropy (CE) based classifier training module to our loss in (7) (which learns our feature extractor $Q$ ). The CE part uses $\\pmb{Q}\\pmb{x}^{(1)}$ and the labels of the sources as inputs to learn the classifier, i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CE}}=-\\gamma\\,\\sum_{\\ell=1}^{N}\\sum_{k=1}^{K}\\mathbb{I}[y_{\\ell}=k]\\log r_{\\theta}([Q x_{\\ell}^{(1)}]_{k}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\boldsymbol{r}_{\\theta}(\\cdot):\\mathbb{R}^{d_{\\mathrm{C}}}\\,\\rightarrow\\,\\mathbb{R}^{K}$ is the classifier that aims to map the learned feature vector ${\\pmb Q}x_{\\ell}^{(1)}$ to a $K$ -dimensional probability mass function (i.e., the distribution of the ground-truth label over $K$ classes), $y_{\\ell}\\in[K]$ represents the label of the \u2113th sample in source domain, and the indicator function $\\mathbb{I}[y_{\\ell}=k]=1$ only when the event $y_{\\ell}=k$ happens (other wise $\\mathbb{I}[y_{\\ell}=k]=0$ . The $\\gamma\\geq0$ is the tunable parameter. The joint loss is still differentiable, and thus we still use the Adam optimizer to jointly optimize $Q$ and $\\pmb{\\theta}$ . ", "page_idx": 23}, {"type": "text", "text": "Additional domain adaptation experiment using CLIP features: In this experiment, we use CLIP as an image encoder as it learns informative and transferable features from very large datasets [35]. Table 8 and Table 9 show the results on Office-31 and Office-Home datasets, respectively, using CLIP embeddings. Compared to the results on ResNet50 embeddings in Table 1 and Table 2, one can observe that all the methods, including proposed method, gains an advantage. This is likely because CLIP was trained on a large and diverse dataset [35], which may have include similar content to the Office-31 and Office-Home datasets. ", "page_idx": 23}, {"type": "text", "text": "The results show that, as a foundation model, CLIP can already unify the embeddings of the source and target domains to a reasonable extent. In addition, our model and algorithm when combined with regularization techniques like MCC, can still further enhance performance, even with simple post-processing of CLIP embeddings. ", "page_idx": 23}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/814534624525ed3a4faaac4944c1480ef2c7bdecfc777b984a01726576f02245.jpg", "table_caption": ["Table 9: Classification accuracy on the target domain of office-Home dataset using CLIP embeddings. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Visualization Result: Fig. 6 shows the 2-dimensional visualization of the CLIP-learned features $d=256)$ from two domains, namely, DSLR and Amazon images (Office-31), using t-SNE. One can see that CLIP could roughly group the same classes from the two domains together. But the proposed method can further pull the circles and the triangle markers together\u2014meaning that the $Q$ really learns shared representations of the same data in the DSLR and Amazon domains. ", "page_idx": 24}, {"type": "image", "img_path": "ivCX2cjwcT/tmp/7a726e77b05cf3a5cd559b29898bd86d078a3ff0cef4a4fdef76d61db41c55ad.jpg", "img_caption": [], "img_footnote": ["Figure 6: Office-31 dataset: DSLR images features represented as circle markers, Amazon images features represented as triangle markers. Different color represent different classes. "], "page_idx": 24}, {"type": "text", "text": "G.2 Single-cell Sequence Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Hyperparameter Settings: The hyperparameter settings for single-cell sequence analysis is presented in Table. 10. ", "page_idx": 24}, {"type": "text", "text": "Baseline: For more details on baseline refer to the implementation in https://github.com/ uhlerlab/cross-modal-autoencoders. ", "page_idx": 24}, {"type": "text", "text": "G.3 Multi-lingual Information Retrieval ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Hyperparameter Settings: The hyperparameter settings for multi-lingual information retrieval is described in the Table. 11. ", "page_idx": 24}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/844205c1d310c5729eb3a3d239255f2bfb0839b361c6fb5b30189810b3053334.jpg", "table_caption": ["Table 10: Hyperparameter settings for single-cell sequence analysis. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/9f67f76f3102232f9a5ae9a3b5273cb3816006b90508c44d088add4ac8e00875.jpg", "table_caption": ["Table 11: Hyperparameter settings for multi-lingual information retrieval. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Additional Results: Table 12 reports the $\\operatorname{P}(\\!\\omega\\!\\,5\\!\\$ and $\\mathrm{P}@10$ scores over the test data, calculated for different source and target language pairs. It can be observed that the proposed method achieves higher precision than Adv in most of the translation tasks (e.g., by at least $1\\%$ in the $\\scriptstyle\\mathbf{en\\toes}$ and $\\mathbf{es\\longrightarrowen}$ tasks) when considering both $\\operatorname{P}(\\varnothing5\\$ and $\\mathrm{P}@10$ scores. ", "page_idx": 25}, {"type": "text", "text": "G.4 Computation resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "All the experiments were run on Nvidia H100 GPU. The approximate runtime for a single run of the algorithm is 20 minutes for multi-lingual information retrieval, 15 minutes for domain adaptation, and 3 minutes for single-cell sequence analysis. ", "page_idx": 25}, {"type": "text", "text": "Complexity Analysis: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Since the proposed objective is tackled using stochastic gradient (SG)-based first-order iterative method, the computational complexity of the proposed algorithm depends upon the per-iteration complexity. ", "page_idx": 25}, {"type": "text", "text": "For each sample, the per-iteration complexity is composed of a forward pass and a backward pass. ", "page_idx": 25}, {"type": "text", "text": "Note that the problem size depends upon $d^{(q)}$ (the data dimension), $d_{\\mathrm{C}}$ , and the batch size $B$ . We assume that the network architecture of $\\boldsymbol{\\textbf{\\textit{f}}}$ (the number of layers and hidden units in each layer) is fixed, represented by ${\\pmb f}={\\pmb\\sigma}\\circ{\\pmb F}_{L}\\circ\\cdot\\cdot\\cdot\\circ{\\pmb\\sigma}\\circ{\\pmb F}_{1}$ , where $F_{\\ell}$ and $\\pmb{\\sigma}$ are the linear layer (matrix) and activation function corresponding to the $\\ell$ th layer. Only the input dimension $d_{\\mathrm{C}}$ of first matrix $F_{1}$ , varies with the problem size. ", "page_idx": 25}, {"type": "text", "text": "The forward pass involves computing $\\widehat{\\pmb{c}}^{(q)}={\\pmb Q}^{(q)}{\\pmb x}^{(q)}$ and $\\pmb{f}^{(q)}(\\widehat{\\pmb{c}}^{(q)})$ , both of which scale linearly with $d_{\\mathrm{C}},d^{(q)}$ and the batch size $B$ . Hen  ce, the forward pass time co mplexity is $O(B d_{\\mathrm{C}}(d^{(1)}+d^{(2)}))$ . Similarly, the backward pass requires computing of $\\frac{\\partial L}{\\partial\\widehat{\\mathbf{c}}_{i}^{(q)}},\\forall i\\in[d_{\\mathrm{C}}]$ and $\\frac{\\partial\\widehat{\\pmb{c}}_{i}^{(q)}}{\\partial\\pmb{Q}^{(q)}{}_{j k}},\\forall i\\in[d_{\\mathrm{C}}],j,k\\in$ $[d_{\\mathrm{C}}\\times d^{(q)}]$ , where $L$ is the loss function. The first gradient computation is linear in $B d_{\\mathrm{C}}$ , while the second gradient computation has a complexity of $O(B d_{\\mathrm{C}}(d^{(1)}+d^{(2)}))$ . Hence the computational complexity of our method is $O(B d_{\\mathrm{C}}(d^{(1)}+d^{(2)}))$ . ", "page_idx": 25}, {"type": "table", "img_path": "ivCX2cjwcT/tmp/36944ea29e552b9783dd0bc93ac923995e84a5776788100ac35e4af831c366eb.jpg", "table_caption": ["Table 12: Average precision $\\mathrm{P}@\\mathbf{k}$ of cross-language information retrieval "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "The memory complexity involves storing the network parameters and the aforementioned gradients. Hence, only the size of $Q^{(q)}$ , $\\pmb{F}_{1}$ , and $c^{(q)}$ changes with the problem dimension. The size of $Q^{(q)}$ , $F_{1}$ , and $c^{(q)}$ are $d_{\\mathrm{C}}d^{(q)}$ , $O(d_{\\mathrm{C}})$ and $d_{\\mathrm{C}}$ , respectively. Therefore, the space complexity is $O(B d_{\\mathrm{C}}(d^{(1)}+d^{(2)}))$ . ", "page_idx": 26}, {"type": "text", "text": "In summary, both the memory and computational complexities of the proposed method scales linearly with $d_{\\mathrm{C}}$ . ", "page_idx": 26}, {"type": "text", "text": "H Extension: Private Component Identification ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorems 1-3 are concerned with learning the shared component $^c$ . The goal, there, was to ensure that $Q_{\\mathrm{C}}^{(q)}\\pmb{x}^{(q)}\\pmb{\\Theta}\\pmb{c},\\forall q$ . In some cases, the private components $\\pmb{p}^{(q)}$ is also of interest [6, 31, 74]. To learn $\\pmb{p}^{(q)}$ , we propose to solve the following learning criterion: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{find}\\quad Q_{\\mathrm{C}}^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{C}}\\times d^{(q)}},Q_{\\mathrm{P}}^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{P}}^{(q)}\\times d^{(q)}}\\;q=1,2,}\\\\ {\\mathrm{subject~to}\\quad Q_{\\mathrm{C}}^{(1)}x^{(1)}\\stackrel{\\mathrm{(d)}}{=}Q_{\\mathrm{C}}^{(2)}x^{(2)},}\\\\ &{Q_{\\mathrm{C}}^{(q)}x^{(q)}\\stackrel{\\mathrm{\\scriptsize~.~.~}}{\\mathrm{\\scriptsize~.~.~}}Q_{\\mathrm{P}}^{(q)}x^{(q)}\\quad q=1,2,}\\\\ &{Q_{\\mathrm{C}}^{(q)}\\mathbb{E}\\left[x^{(q)}(x^{(q)})^{\\top}\\right](Q_{\\mathrm{C}}^{(q)})^{\\top}=I\\quad q=1,2,}\\\\ &{Q_{\\mathrm{P}}^{(q)}\\mathbb{E}\\left[x^{(q)}(x^{(q)})^{\\top}\\right](Q_{\\mathrm{P}}^{(q)})^{\\top}=I\\quad q=1,2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\textbf{\\em u}$ \u22a5\u22a5 $\\pmb{v}$ means that the random vectors $\\textbf{\\em u}$ and $\\pmb{v}$ are independent with each other. ", "page_idx": 26}, {"type": "text", "text": "For implementation we use following criterion, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{Q_{\\mathrm{C}}^{(1)},Q_{\\mathrm{C}}^{\\smash{(2)}}Q_{\\mathrm{P}}^{(1)},Q_{\\mathrm{P}}^{\\smash{(1)}}}\\operatorname*{max}_{f}\\mathbb{E}_{\\mathbf{x}^{(1)}}\\log\\Big(f(Q_{\\mathrm{C}}^{\\smash{(1)}}\\mathbf{x}^{(1)})\\Big)+\\mathbb{E}_{\\mathbf{x}^{(2)}}\\log\\Big(1-f(Q_{\\mathrm{C}}^{\\smash{(2)}}\\mathbf{x}^{(2)})\\Big)}\\\\ &{\\displaystyle+\\lambda\\sum_{q=1}^{2}\\mathcal{R}\\left(Q_{\\mathrm{C}}^{\\smash{((q))}}\\right)+\\omega\\displaystyle\\sum_{q=1}^{2}\\mathcal{R}\\left(Q_{\\mathrm{P}}^{\\smash{((q))}}\\right)+\\rho\\displaystyle\\sum_{q=1}^{2}\\mathrm{HSIC}(Q_{\\mathrm{C}}^{(q)}\\mathbf{x}^{(q)},Q_{\\mathrm{P}}^{(q)}\\mathbf{x}^{(q)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where, first two term are adversarial loss for distribution matching. The constraint on (33d) and (33e) are enforced as $\\mathcal{R}(Q_{\\mathrm{C}}^{\\mathbf{\\Gamma}}{}^{(q)})$ and $\\mathcal{R}(Q_{\\mathrm{C}}{}^{(q)})$ respectively, where $\\begin{array}{r l}{\\mathcal{R}(Q^{(q)})}&{{}=}\\end{array}$ $\\|\\boldsymbol{Q}^{(q)}\\mathbb{E}[\\pmb{x}^{(q)}(\\pmb{x}^{(q)})^{\\top}](\\pmb{Q}^{(q)})^{\\top}-\\pmb{I}\\|_{\\mathrm{F}}^{2}$ . The constraint on (33c) is realized with Hilbert-Schmidt Independence Criterion (HSIC) [75]. HSIC measures the independence between two distribution. So, we minimize HSIC between estimated shared component and estimated private component to promote independence between shared and private components. ", "page_idx": 26}, {"type": "text", "text": "We show that under some reasonable conditions the block $\\pmb{p}^{(q)}$ can also be learned up to a matrix multiplication: ", "page_idx": 26}, {"type": "text", "text": "Theorem 6. Assume that the blocks c, $\\pmb{p}^{(1)}$ and $\\pmb{p}^{(2)}$ are statistically independent, i.e., $p(\\pmb{c},\\pmb{p}^{(1)},\\pmb{p}^{(2)})=p(\\pmb{c})p(\\pmb{p}^{(1)})p(\\pmb{p}^{(2)})$ . Then, if one of the following holds: ", "page_idx": 27}, {"type": "text", "text": "(i) Assumption $^{\\,I}$ and assumptions in Theorem $^{\\,l}$ are satisfied, and (33) is solved yielding solutions $\\widehat{Q}_{\\mathrm{C}}^{(q)}$ and $\\widehat{Q}_{\\mathrm{P}}^{(q)}$   \n(ii) Assumption 2 is satisfied and has same mixing matrix $A^{(q)}\\ =\\ A$ and (33) with $Q_{\\mathrm{P}}^{(q)}=Q_{\\mathrm{P}}$ and $Q_{\\mathrm{C}}^{(q)}=Q_{\\mathrm{C}}$ is solved yielding $\\bar{Q}_{\\mathrm{C}}^{(q)}$ and $\\widehat{Q}_{\\mathrm{P}}^{(q)}$ as the solutions.   \n(iii) sAuspsuermvipstiioonn) $^{\\,l}$ a insd  sdaetinsoftiee d $d_{\\mathrm{C}}$ eads  stahem psloelsu t $(x_{\\ell}^{(1)},x_{\\ell}^{(2)})$ o lavrien ga (v3a3il).able (weak $\\widehat{Q}_{\\mathrm{C}}^{(q)}$ $\\widehat{Q}_{\\mathrm{P}}^{(q)}$ ", "page_idx": 27}, {"type": "text", "text": "Then, we have $\\widehat{\\pmb{Q}}_{\\mathrm{C}}^{(q)}\\pmb{x}^{(q)}\\,=\\,\\pmb{\\Theta}c$ and $\\widehat{\\pmb{Q}}_{\\mathrm{P}}^{(q)}\\pmb{x}^{(q)}=\\Xi^{(q)}\\pmb{p}^{(q)}$ , for some invertible $\\Xi^{(q)}$ for all $q=1,2$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. For each case in Theorem. 6 (i) - (iii), we can prove ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{c}}^{(q)}=\\widehat{\\pmb{Q}}_{\\mathrm{C}}^{(q)}{\\pmb{x}}^{(q)}=\\Theta{\\pmb{c}},\\;q=1,2\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "using Theorems 1-3. The proofs are referred to Appendix B-D. ", "page_idx": 27}, {"type": "text", "text": "Let us denote ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{p}}^{(q)}=\\widehat{\\pmb{Q}}_{\\mathrm{P}}^{(q)}\\pmb{x}^{(q)}=\\widehat{\\pmb{Q}}_{\\mathrm{P}}^{(q)}\\pmb{A}^{(q)}\\left[\\pmb{p}^{(q)}\\right]=\\pmb{H}^{(q)}\\left[\\pmb{p}^{(q)}\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\pmb{H}^{(q)}=\\widehat{\\pmb{Q}}_{\\mathrm{P}}^{(q)}\\pmb{A}^{(q)}\\in\\mathbb{R}^{d_{\\mathrm{P}}^{(q)}\\times(d_{\\mathrm{C}}+d_{\\mathrm{P}}^{(q)})}$ . Note that the constraint (33c) implies that the mutual information bet w een $\\widehat{\\pmb{p}}^{(q)}$ and $\\widehat{\\mathbf{c}}^{\\left(q\\right)}$ is zero, i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\nI(\\widehat{\\pmb{p}}^{(q)};\\widehat{\\pmb{c}}^{(q)})=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that ${\\widehat{\\pmb{p}}}^{(q)}\\rightarrow{\\widehat{\\pmb{c}}}^{(q)}\\rightarrow\\pmb{\\Theta}^{-1}{\\widehat{\\pmb{c}}}^{(q)}=c$ is a  Mar k ov chain. This is because when conditioned on $\\widehat{\\mathbf{c}}^{\\left(q\\right)}$ , $\\Theta^{-1}\\widehat{\\mathbf{c}}^{(q)}$ b ecomes  constant, ma king it independent of $\\widehat{\\pmb{p}}^{(q)}$ . This allows us to use the data proces s ing inequ ality [76, Theorem 2.8.1], which results in the f ol lowing: ", "page_idx": 27}, {"type": "equation", "text": "$$\nI(\\widehat{\\pmb{p}}^{(q)};\\widehat{\\pmb{c}}^{(q)})\\geq I(\\widehat{\\pmb{p}}^{(q)};\\pmb{\\Theta}^{-1}\\widehat{\\pmb{c}}^{(q)})=I(\\widehat{\\pmb{p}}^{(q)};\\pmb{c})).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since mutual information is always non-negative, the above implies that $I(\\widehat{\\pmb{p}}^{(q)};\\pmb{c})=0$ . This implies that $\\widehat{\\pmb{p}}^{(q)}={\\pmb{H}}^{(q)}\\left[\\pmb{\\mathscr{c}}_{}^{}\\right]$ is independent of $^c$ . Hence, $H^{(q)}[1:d_{\\mathrm{C}}]=0,\\forall q$ . ", "page_idx": 27}, {"type": "text", "text": "Therefore $\\widehat{\\pmb{p}}^{(q)}\\,=\\,\\pmb{H}^{(q)}[d_{\\mathrm{C}}\\,+\\,1\\,:\\,d_{\\mathrm{C}}\\,+\\,d_{\\mathrm{P}}^{(q)}]\\pmb{p}^{(q)}\\,=\\,\\Xi^{(q)}\\pmb{p}^{(q)},\\forall q$ , where $\\Xi^{(q)}\\,=\\,H^{(q)}[d_{\\mathrm{C}}\\,+\\,1\\,:$ $d_{\\mathrm{C}}\\!+\\!d_{\\mathrm{P}}^{(q)}]$ .   Note that $H^{(q)}$ is full row-rank because of constraint (33e). This implies that $\\Xi^{(q)},q=1,2$ are invertible matrices. ", "page_idx": 27}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "H.1 Validation of Theorem 6 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Fig. 7 presents numerical validation for Theorem 6. ", "page_idx": 27}, {"type": "text", "text": "Hyperparameter Setting The hyperparameter setting is the same as mentioned in Appendix. F. We solve (34) to obtain $\\bar{Q}_{\\mathrm{C}}^{(q)}$ and $\\widehat{Q}_{\\mathrm{P}}^{(q)}$ to recover the shared and private components, respectively. For learning $Q_{\\mathrm{P}}^{(q)}$ , we us e  Adam o p timizer and set initial learning rates to be 0.001. Also we set the regularization parameter $\\omega=10.0$ and $\\rho=50.0$ . ", "page_idx": 27}, {"type": "text", "text": "Data Generation: We set $d_{\\mathrm{C}}\\,=\\,2$ and $d_{\\mathrm{P}}^{(q)}\\,=\\,1\\$ as in the previous synthetic experiments. We sample $c\\sim\\mathtt{V o n M i s e s}(2.5,\\ 2.0)$ The private components are sampled from $p^{(1)}\\sim\\mathtt{B e t a}(1.0,\\;3.0)$ and $\\boldsymbol{p}^{(2)}\\,\\sim\\,\\mathtt{G a m m a}(0.5,\\ 3.0)$ distributions. Each element of mixing matrices are sampled from $A_{i j}^{(q)}\\sim\\mathcal{N}(0,1),\\;q=1,2.$ ", "page_idx": 28}, {"type": "text", "text": "Result: Fig. 7 shows the result for proposed method for private component identification. The first column shows the data domain, the second column shows the true and extracted shared component, and the third and fourth columns shows the true and extracted private components. Especially, the last row of the third and fourth columns shows the plot of ground truth $\\bar{\\pmb{p}^{(q)}}$ on $x-$ axis and $\\widehat{\\pmb{p}}^{(q)}$ on the y-axis. The plot is approximately a straight line which indicates that the estimated pri v ate components ${\\widehat{p}}^{\\left(q\\right)}$ are scaled version (i.e., invertible linear transformations) of ground truth private components.   This verifies our Theorem 6. ", "page_idx": 28}, {"type": "image", "img_path": "ivCX2cjwcT/tmp/a1367c82829d3fba9037f1af6cabdcfdcb09db8b71b8a1d82060ee0eac54abd9.jpg", "img_caption": ["Figure 7: Validation of Theorem 6 $d_{\\mathrm{C}}=2$ and $d_{\\mathrm{P}}^{\\mathrm{(1)}}=1$ . "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Section 3, 4 and 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See Section 7. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Appendix B, C, D and H. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: See Appendix F and G. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes the code is provided in the supplemental material. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See Appendix F and G. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have provided the error bars for the experiment that is computationally less demanding. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Appendix G.4. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have followed the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The contribution of this paper is on theoretical aspects of machine learning.   \nWe don\u2019t foresee any immediate societal impacts. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Theoretical paper. So not applicable in our case. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix G. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We donot release any new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Not Applicable. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]