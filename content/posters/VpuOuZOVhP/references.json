{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides the technical details of GPT-4, a large language model leveraged by LLM-AutoDA for automatic data augmentation strategy generation."}, {"fullname_first_author": "Sumyeong Ahn", "paper_title": "CUDA: Curriculum of data augmentation for long-tailed recognition", "publication_date": "2023-00-00", "reason": "CUDA is a state-of-the-art data augmentation method for long-tailed learning, directly compared against in the paper's experimental evaluation."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This foundational paper established the effectiveness of large language models as few-shot learners, a key capability exploited by LLM-AutoDA."}, {"fullname_first_author": "Mateusz Buda", "paper_title": "A systematic study of the class imbalance problem in convolutional neural networks", "publication_date": "2018-00-00", "reason": "This paper provides a comprehensive overview of the class imbalance problem, which is the central problem that LLM-AutoDA aims to address."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-00-00", "reason": "This highly influential paper introduced the ResNet architecture, which is widely used in image classification tasks and serves as a baseline in the paper's experiments."}]}