[{"figure_path": "L8Q21Qrjmd/figures/figures_1_1.jpg", "caption": "Figure 1: Flow matching for observed trajectories. (a) The task aims to reach the terminal state with a reward-proportional probability from the initial state, by incrementing one coordinate as a random action. The black line indicates the two observed trajectories for each terminal state. (b-c) The arrow (\u2192) length indicates the amount of the backward or forward flow. In (b), the flow matching (~) between the observed backward and forward flows underestimates the high-reward object due to the low observed backward flow. In (c), PBP-GFN succeeds with the observed backward flow that fully represents the true rewards.", "description": "This figure illustrates how the conventional flow matching in GFlowNets may underestimate the high-reward objects due to insufficient observed trajectories. Panel (a) shows a simple grid-world environment to explain the concept. Panel (b) shows that conventional flow matching underestimates the high-reward object because the observed backward flow is low. Panel (c) demonstrates that the proposed method, PBP-GFN, successfully aligns the observed backward flow with the true reward, leading to a better estimation of high-reward objects.", "section": "3.1 Motivation: under-exploitation of objects with partially observed trajectories"}, {"figure_path": "L8Q21Qrjmd/figures/figures_3_1.jpg", "caption": "Figure 2: Under-exploitation of objects with partially observed trajectories. The reward R(x) consists of (1) observed backward flow RB(x) and (2) unobserved backward flow R(x) \u2013 RB(x). (a) Conventional flow matching may assign a higher probability to the lower-reward object as the observed forward flow is aligned only with a small amount of observed backward flow. This fails to assign the accurate probability proportional to the reward. (b) PBP-GFN assigns more accurate probability proportional to the reward, by increasing the proportion of observed flow.", "description": "This figure illustrates the under-exploitation problem in conventional flow matching and how the proposed pessimistic backward policy (PBP-GFN) addresses it.  In (a), conventional flow matching, with limited observed trajectories, assigns higher probability to a lower-reward object because it has more observed backward flow. (b) shows that PBP-GFN corrects this by maximizing the observed backward flow, leading to a more accurate probability distribution aligned with the true rewards, thus resolving the under-exploitation problem.", "section": "3.1 Motivation: under-exploitation of objects with partially observed trajectories"}, {"figure_path": "L8Q21Qrjmd/figures/figures_4_1.jpg", "caption": "Figure 3: Pessimistic backward policy for GFlowNets (PBP-GFN). The portion of the circle indicates the amount of flow, e.g.,  indicates the flow of 1, and  indicates the half flow of, i.e., the flow of 0.5. Additionally, the color of the flow indicates the flow inducing the same-colored reward, and the black and gray lines indicate the observed and unobserved trajectories, respectively. (a) Flow matching succeeds with the entire trajectories. One can observe that the true reward of x1 is 1 and the reward of x2 is 0.5 by the amount of flow. (b) Flow matching fails with partially observed trajectories. (c) PBP-GFN assigns high probabilities to the backward transitions of observed trajectories to keep a high probability to high-reward objects.", "description": "This figure shows how the proposed pessimistic backward policy (PBP-GFN) for GFlowNets addresses the under-exploitation problem.  Panel (a) demonstrates that with complete trajectory information, the conventional flow matching correctly assigns probabilities proportional to the true reward. Panel (b) shows that with only partially observed trajectories, the conventional method underestimates the high-reward object due to insufficient backward flow. Panel (c) illustrates how PBP-GFN maximizes the observed backward flow, thereby aligning the observed flow with the true reward and improving the object sampling.", "section": "3.2 Pessimistic backward policy for GFlowNets"}, {"figure_path": "L8Q21Qrjmd/figures/figures_6_1.jpg", "caption": "Figure 4: The target distribution and empirical distributions of each model trained with 10<sup>5</sup> trajectories. The empirical distributions are computed as rescaled products of the distribution over three runs. Our method (PBP-GFN) consistently discovers all modes over three runs and learns the target Boltzmann distribution correctly within the relatively small number of trajectories.", "description": "This figure compares the target Boltzmann distribution with the empirical distributions generated by five different models (TB, Uniform, MaxEnt, and PBP-GFN) after training with 100,000 trajectories.  The visualization uses 3D density plots to show the distribution of objects in a 16x16x16 hypergrid.  The results demonstrate that the proposed PBP-GFN method accurately learns the target distribution, unlike the other methods which show some discrepancies.", "section": "5 Experiment"}, {"figure_path": "L8Q21Qrjmd/figures/figures_6_2.jpg", "caption": "Figure 5: The performance comparison with the prior backward policy design methods. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority in generating diverse high-reward objects, compared to the considered baselines for designing the backward policy.", "description": "This figure compares the performance of PBP-GFN against baseline methods for backward policy design in two synthetic tasks: bag generation and maximum independent set problem.  The plots show the number of high-reward objects generated and the average reward of the top 100 generated objects over training rounds or epochs. PBP-GFN consistently outperforms the baselines, demonstrating its effectiveness in generating diverse high-reward objects.", "section": "5 Experiment"}, {"figure_path": "L8Q21Qrjmd/figures/figures_7_1.jpg", "caption": "Figure 7: The performance on molecular generation. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority compared to the baselines in generating diverse high reward molecules while yielding similar Tanimoto similarities compared to other baselines with prior backward policy designs.", "description": "The figure compares the performance of PBP-GFN against other baselines for molecular generation.  It shows PBP-GFN excels at generating diverse high-reward molecules while maintaining similar Tanimoto similarity scores to other methods. The plots illustrate the number of modes (high-reward molecules), the average top-100 performance, and the Tanimoto similarity, demonstrating the advantages of PBP-GFN in terms of diversity and reward.", "section": "5.2 Molecular generation"}, {"figure_path": "L8Q21Qrjmd/figures/figures_7_2.jpg", "caption": "Figure 4: The target distribution and empirical distributions of each model trained with 10<sup>5</sup> trajectories. The empirical distributions are computed as rescaled products of the distribution over three runs. Our method (PBP-GFN) consistently discovers all modes over three runs and learns the target Boltzmann distribution correctly within the relatively small number of trajectories.", "description": "This figure compares the target Boltzmann distribution with the empirical distributions generated by different models after training with 100,000 trajectories.  The target distribution represents the ideal distribution the models aim to learn. The empirical distributions show how well each model approximates this target distribution. The results demonstrate that PBP-GFN effectively learns the target distribution, accurately capturing all high-probability regions (modes).", "section": "5 Experiment"}, {"figure_path": "L8Q21Qrjmd/figures/figures_8_1.jpg", "caption": "Figure 7: The performance on molecular generation. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority compared to the baselines in generating diverse high reward molecules while yielding similar Tanimoto similarities compared to other baselines with prior backward policy designs.", "description": "This figure compares the performance of PBP-GFN with other baseline methods for molecular generation.  The results show that PBP-GFN achieves superior performance in terms of the number of high-reward molecules generated and the average top-100 performance, while maintaining comparable Tanimoto similarity scores, indicating a similar level of chemical diversity.", "section": "5.2 Molecular generation"}, {"figure_path": "L8Q21Qrjmd/figures/figures_8_2.jpg", "caption": "Figure 9: The number of 2-hamming ball modes discovered during training. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority compared to the baselines in discovering diverse distinct modes.", "description": "This figure compares the number of 2-Hamming ball modes discovered during training across different methods for RNA sequence generation.  The x-axis represents the training progress (active rounds), and the y-axis represents the number of modes.  PBP-GFN consistently discovers more modes than other methods, indicating improved diversity in the generated RNA sequences. Error bars (standard deviation) are included for each method, showing PBP-GFN's consistent superiority.", "section": "5.3 Sequence generation"}, {"figure_path": "L8Q21Qrjmd/figures/figures_9_1.jpg", "caption": "Figure 10: The relative mean error comparison. The solid line and shaded region represent the mean and standard deviation, respectively. Our PBP-GFN yields the closest error to zero.", "description": "This figure compares the relative mean error of different methods for RNA sequence generation tasks across four benchmarks. The relative mean error measures the difference between the empirical distribution generated by each method and the target Boltzmann distribution. The results show that PBP-GFN consistently achieves the lowest relative mean error, indicating its superior performance in learning the target distribution.", "section": "5.3 Sequence generation"}, {"figure_path": "L8Q21Qrjmd/figures/figures_9_2.jpg", "caption": "Figure 5: The performance comparison with the prior backward policy design methods. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority in generating diverse high-reward objects, compared to the considered baselines for designing the backward policy.", "description": "The figure compares the performance of the proposed PBP-GFN method with existing methods for designing backward policies in GFlowNets.  The y-axis shows metrics reflecting the ability of the model to generate high-reward objects.  The x-axis represents the training progress. The results demonstrate that PBP-GFN consistently outperforms baselines in both generating high-reward objects and generating a diverse set of such objects.", "section": "5 Experiment"}, {"figure_path": "L8Q21Qrjmd/figures/figures_9_3.jpg", "caption": "Figure 5: The performance comparison with the prior backward policy design methods. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority in generating diverse high-reward objects, compared to the considered baselines for designing the backward policy.", "description": "This figure compares the performance of the proposed PBP-GFN method against existing backward policy design methods (TB, DB, Uniform, MaxEnt) across two different tasks: bag generation and maximum independent set problem. The results illustrate that PBP-GFN consistently outperforms other methods in generating a diverse set of high-reward objects, demonstrating its effectiveness in addressing the under-exploitation problem of GFlowNets.", "section": "5 Experiment"}, {"figure_path": "L8Q21Qrjmd/figures/figures_16_1.jpg", "caption": "Figure 3: Pessimistic backward policy for GFlowNets (PBP-GFN). The portion of the circle indicates the amount of flow, e.g., indicates the flow of 1, and indicates the half flow of, i.e., the flow of 0.5. Additionally, the color of the flow indicates the flow inducing the same-colored reward, and the black and gray lines indicate the observed and unobserved trajectories, respectively. (a) Flow matching succeeds with the entire trajectories. One can observe that the true reward of x1 is 1 and the reward of x2 is 0.5 by the amount of flow. (b) Flow matching fails with partially observed trajectories. (c) PBP-GFN assigns high probabilities to the backward transitions of observed trajectories to keep a high probability to high-reward objects.", "description": "This figure illustrates how PBP-GFN addresses the under-exploitation problem in GFlowNets.  Panel (a) shows that with complete trajectory observation, flow matching accurately reflects the reward distribution.  Panel (b) demonstrates that with partial observation, conventional flow matching underestimates the reward of high-reward objects due to insufficient trajectory samples. Panel (c) shows that PBP-GFN improves the estimation by maximizing observed backward flow to better match the true reward, thus improving the selection of high-reward objects.", "section": "3.2 Pessimistic backward policy for GFlowNets"}]