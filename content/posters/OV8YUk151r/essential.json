{"importance": "This paper is crucial for researchers working on citation prediction and related fields because it introduces a novel approach to improve accuracy and scalability, addressing limitations of existing methods.  **The proposed HLM-Cite workflow, combining embedding and generative language models, offers significant performance improvements and the ability to handle vastly larger datasets.** This opens new avenues for research in citation analysis, knowledge graph construction, and computational social science.", "summary": "HLM-Cite: A hybrid language model workflow boosts scientific citation prediction accuracy by 17.6% and scales to 100K candidate papers, surpassing existing methods.", "takeaways": ["HLM-Cite, a novel hybrid workflow, significantly improves scientific citation prediction accuracy.", "The concept of 'core citations' provides a more nuanced understanding of citation roles.", "HLM-Cite scales to datasets far exceeding the size handled by previous methods, enabling analysis of entire research domains."], "tldr": "Current citation prediction struggles with the varying importance of citations and the massive scale of available papers.  Existing methods often rely on simple binary classification, failing to distinguish between foundational and superficial citations.  This limits accuracy and scalability.\nHLM-Cite tackles these challenges using a two-stage hybrid approach. First, it uses a fine-tuned embedding model for efficient retrieval of high-likelihood citations.  Then, an LLM-driven workflow ranks these papers, discerning implicit relationships for improved accuracy.  **This approach achieves a 17.6% performance gain compared to state-of-the-art methods and scales to 100K candidate papers.**", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "OV8YUk151r/podcast.wav"}