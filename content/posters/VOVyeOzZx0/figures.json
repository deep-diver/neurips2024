[{"figure_path": "VOVyeOzZx0/figures/figures_6_1.jpg", "caption": "Figure 1: We apply our method to bound test metrics such as accuracy and F1 score (in green) when no true labels are used to estimate performance. In the first row (\"Oracle\"), we use true labels to estimate the conditional distribution P<sub>Y</sub>|<sub>Z</sub>, thus approximating a scenario in which the label model is reasonably specified. On the second row (\"Snorkel\"), we use a label model to estimate P<sub>Y</sub>|<sub>Z</sub> without access to any true labels. Despite potential misspecification in Snorkel's label model, it performs comparably to using labels to estimate P<sub>Y</sub>|<sub>Z</sub>, giving approximate but meaningful bounds.", "description": "This figure compares the performance of the proposed method for estimating Fr\u00e9chet bounds on test accuracy and F1-score in two scenarios: one where the true labels are available (\"Oracle\") and one where only a label model is available (\"Snorkel\").  The results demonstrate that even with potential misspecification in the label model, the proposed method provides reasonably accurate bounds on the performance metrics, highlighting its applicability in weak supervision settings.", "section": "4.1 Bounding the performance of weakly supervised classifiers"}, {"figure_path": "VOVyeOzZx0/figures/figures_7_1.jpg", "caption": "Figure 1: We apply our method to bound test metrics such as accuracy and F1 score (in green) when no true labels are used to estimate performance. In the first row (\"Oracle\"), we use true labels to estimate the conditional distribution P<sub>Y|Z</sub>, thus approximating a scenario in which the label model is reasonably specified. On the second row (\"Snorkel\"), we use a label model to estimate P<sub>Y|Z</sub> without access to any true labels. Despite potential misspecification in Snorkel's label model, it performs comparably to using labels to estimate P<sub>Y|Z</sub>, giving approximate but meaningful bounds.", "description": "The figure shows the results of applying the proposed method to estimate the Fr\u00e9chet bounds for accuracy and F1 score on several datasets. The first row uses true labels to estimate the conditional distribution, while the second row uses a label model. Despite potential misspecifications in the label model, the bounds are still reasonably accurate, demonstrating the robustness of the method.", "section": "4.1 Bounding the performance of weakly supervised classifiers"}, {"figure_path": "VOVyeOzZx0/figures/figures_8_1.jpg", "caption": "Figure 3: Performance bounds for classifiers on the YouTube dataset, initially relying solely on few-shot weak labels obtained via prompts to the LLM Llama-2-13b-chat-hf. The progression of plots illustrates the comparative impact of integrating \\\"high-quality\\\" labels from Wrench versus synthetically generated \\\"low-quality\\\" labels. Evidently, the addition of \\\"high-quality\\\" labels significantly enhances the bounds, underscoring their superior utility over \\\"low-quality\\\" labels for optimal classification of SPAM and HAM comments.", "description": "This figure shows the performance bounds for classifiers trained on the YouTube dataset using three different sets of weak labels: 1) only few-shot labels from the LLM Llama-2-13b-chat-hf, 2) few-shot labels + extra low-quality synthetic labels, and 3) few-shot labels + extra high-quality labels from the Wrench dataset. The figure demonstrates that adding high-quality labels significantly improves the accuracy of the performance bounds, highlighting their importance for reliable model evaluation.", "section": "4 Experiments"}, {"figure_path": "VOVyeOzZx0/figures/figures_28_1.jpg", "caption": "Figure 1: We apply our method to bound test metrics such as accuracy and F1 score (in green) when no true labels are used to estimate performance. In the first row (\"Oracle\"), we use true labels to estimate the conditional distribution P<sub>Y|Z</sub>, thus approximating a scenario in which the label model is reasonably specified. On the second row (\"Snorkel\"), we use a label model to estimate P<sub>Y|Z</sub> without access to any true labels. Despite potential misspecification in Snorkel's label model, it performs comparably to using labels to estimate P<sub>Y|Z</sub>, giving approximate but meaningful bounds.", "description": "The figure shows the effectiveness of the proposed method in estimating the bounds of test metrics (accuracy and F1 score) for various classification thresholds, even when true labels are unavailable.  The \"Oracle\" row uses true labels to estimate the conditional distribution, serving as a benchmark. The \"Snorkel\" row uses a label model, demonstrating that even with potential model misspecification, the method provides meaningful bounds.", "section": "4.1 Bounding the performance of weakly supervised classifiers"}, {"figure_path": "VOVyeOzZx0/figures/figures_28_2.jpg", "caption": "Figure 1: We apply our method to bound test metrics such as accuracy and F1 score (in green) when no true labels are used to estimate performance. In the first row (\"Oracle\"), we use true labels to estimate the conditional distribution P<sub>Y|Z</sub>, thus approximating a scenario in which the label model is reasonably specified. On the second row (\"Snorkel\"), we use a label model to estimate P<sub>Y|Z</sub> without access to any true labels. Despite potential misspecification in Snorkel's label model, it performs comparably to using labels to estimate P<sub>Y|Z</sub>, giving approximate but meaningful bounds.", "description": "This figure shows the effectiveness of the proposed method in estimating the bounds of test accuracy and F1 score for different classification thresholds, even without access to ground truth labels.  The top row uses true labels to estimate the conditional probability distribution P<sub>Y|Z</sub>, serving as an \"oracle\" baseline. The bottom row uses a label model to estimate P<sub>Y|Z</sub>, simulating a realistic weak supervision scenario.  The results demonstrate that the method produces meaningful and relatively accurate bounds even with label model misspecification.", "section": "4 Experiments"}, {"figure_path": "VOVyeOzZx0/figures/figures_29_1.jpg", "caption": "Figure 1: We apply our method to bound test metrics such as accuracy and F1 score (in green) when no true labels are used to estimate performance. In the first row (\"Oracle\"), we use true labels to estimate the conditional distribution P<sub>Y|Z</sub>, thus approximating a scenario in which the label model is reasonably specified. On the second row (\"Snorkel\"), we use a label model to estimate P<sub>Y|Z</sub> without access to any true labels. Despite potential misspecification in Snorkel\u2019s label model, it performs comparably to using labels to estimate P<sub>Y|Z</sub>, giving approximate but meaningful bounds.", "description": "The figure shows the results of applying the proposed method to estimate the upper and lower bounds of accuracy and F1 score for several datasets. The first row uses true labels to estimate the conditional distribution of Y given Z (P<sub>Y|Z</sub>). The second row uses Snorkel's label model to estimate P<sub>Y|Z</sub>, showing that the proposed approach provides reliable bounds even with a misspecified label model. The x-axis represents the classification threshold, and the y-axis represents the accuracy and F1 score.", "section": "4.1 Bounding the performance of weakly supervised classifiers"}]