[{"type": "text", "text": "Weak Supervision Performance Evaluation via Partial Identification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Felipe Maia Polo\\* Department of Statistics University of Michigan ", "page_idx": 0}, {"type": "text", "text": "Subha Maity\\* Department of Statistics and Actuarial Science University of Waterloo ", "page_idx": 0}, {"type": "text", "text": "Mikhail Yurochkin Moulinath Banerjee Yuekai Sun MIT-IBM Watson AI Lab Department of Statistics Department of Statistics University of Michigan University of Michigan ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Programmatic Weak Supervision (PWS) enables supervised model training without direct access to ground truth labels, utilizing weak labels from heuristics, crowdsourcing, or pre-trained models. However, the absence of ground truth complicates model evaluation, as traditional metrics such as accuracy, precision, and recall cannot be directly calculated. In this work, we present a novel method to address this challenge by framing model evaluation as a partial identification problem and estimating performance bounds using Fr\u00e9chet bounds. Our approach derives reliable bounds on key metrics without requiring labeled data, overcoming core limitations in current weak supervision evaluation techniques. Through scalable convex optimization, we obtain accurate and computationally efficient bounds for metrics including accuracy, precision, recall, and F1-score, even in high-dimensional settings. This framework offers a robust approach to assessing model quality without ground truth labels, enhancing the practicality of weakly supervised learning for real-world applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Programmatic weak supervision (PwS) is a modern learning paradigm that allows practitioners to train their supervised models without the immediate need for ground truth labels $Y$ [50,48,47, 49, 58, 64]. In PWS, practitioners first acquire cheap and abundant weak labels $Z$ through heuristics, crowdsourcing, external APIs, and pretrained models, which serve as proxies for $Y$ . Then, they fit a label model, i.e., a graphical model for $P_{Y,Z}$ [50, 49, 22, 17], which, under appropriate modeling assumptions, can be fitted without requiring $Y$ 's. Finally, a predictor $h:\\mathcal{X}\\to\\mathcal{Y}$ is trained using samples $(X_{i},Z_{i})$ 's and a noise-aware loss constructed using this fitted label model [50]. ", "page_idx": 0}, {"type": "text", "text": "One major unsolved issue with the weak supervision approach is that even if we knew $P_{Y,Z}$ ,evaluation metrics such as accuracy, recall, precision, or $F_{1}$ cannot be estimated for model validation without any ground truth labels. In fact, these quantities are not identifiable (not uniquely determined) since we only have partial information about the joint distribution $P_{X,Y}$ through the marginals $P_{X,Z}$ and $P_{Y,Z}$ . As a consequence, any performance metric based on $h$ cannot be estimated without making extra strong assumptions, e.g., $X\\perp\\!\\!\\!\\perp Y\\mid Z$ . Unfortunately, these conditions are unlikely to arise in many situations. A recent work [66] investigated the role and importance of ground truth labels on model evaluation in the weak supervision literature. They determined that, under the current situation, the good performance and applicability of weakly supervised classifiers heavily rely on the presence of at least some high-quality labels,which undermines the purpose of using weak supervision since models can be directly fine-tuned on those labels and achieve similar performance. Therefore, in this work, we develop new evaluation methods that can be used without any ground truth labels and show that the performance of weakly supervised models can be accurately estimated in many cases, even permitting successful model selection. Our solution relies on partial identification by estimating Fr\u00e9chet bounds for bounding performance metrics such as accuracy, precision, recall, and $F_{1}$ scoreof classifiers trained with weak supervision. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Fr\u00e9chet bounds: Consider a random vector $(X,Y,Z)\\in\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Z}$ is drawn from an unknown distribution $P$ . We assume $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is arbitrary while $\\boldsymbol{\\wp}$ and $\\mathcal{Z}$ are finite. In this work, we develop and analyze the statistical properties of a method for estimating Fr\u00e9chet bounds [53, 54] of the form ", "page_idx": 1}, {"type": "equation", "text": "$$\nL\\triangleq\\operatorname*{inf}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}[g(X,Y,Z)]{\\mathrm{~and~}}U\\triangleq\\operatorname*{sup}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}[g(X,Y,Z)]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "when $g$ is a fixed bounded function, with $\\Pi$ being the set of distributions $\\pi$ for $\\left(X,Y,Z\\right)$ such that the marginal $\\pi_{\\boldsymbol{X},Z}$ (resp. $\\pi_{Y,Z})$ is identical to the prescribed marginal $P_{X,Z}$ (resp. $P_{Y,Z})$ Our proposed method can efficiently obtain estimates for the bounds by solving convex programs, with the significant advantage that the computational complexity of our algorithm does not scale with the dimensionality of $X$ , making it well-suited for applications dealing with high-dimensional data. In previous work, for example, Fr\u00e9chet bounds were studied in the financial context (e.g., see Rischendorf [55], Bartl et al. [7]). However, our focus is on applying our methods of estimating Fr\u00e9chet bounds to the problem of assessing predictors trained using programmatic weak supervision (PWS). For example, the upper and lower bounds for the accuracy of a classifier $h$ can be estimated using our method simply by letting $g(x,y,z)=\\mathbb{1}[h(x)=y]$ in (1.1). At a high level, our method replaces $P_{Y,Z}$ with the fitted label model, and $P_{X,Z}$ with its empirical version in the Fr\u00e9chet bounds in (1.1), and reformulates the problem in terms of a convex optimization problem. ", "page_idx": 1}, {"type": "text", "text": "Contributions: Our contributions are ", "page_idx": 1}, {"type": "text", "text": "1. Developing a practical algorithm for estimating the Fr\u00e9chet bounds in (1.1). Our algorithm can be summarized as solving convex programs and is scalable to high-dimensional distributions.   \n2. Quantifying the uncertainty in the computed bounds due to uncertainty in the prescribed marginals by deriving the asymptotic distribution for our estimators.   \n3. Applying our method to bounding the accuracy, precision, recall, and $F_{1}$ score of classifiers trained with weak supervision. This enables practitioners to evaluate classifiers in weak supervision settingswithout access to ground truth labels. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Weak supervision: With the emergence of data-hungry models, the lack of properly labeled datasets has become a major bottleneck in the development of supervised models. One approach to overcome this problem is using programmatic weak supervision (PwS) to train predictors in the absence of high-quality labels $Y$ [50, 48, 47, 49, 58, 64]. PWS has shown the potential to solve a variety of tasks in different fields with satisfactory performance. For example, some works have applied weak supervision to named-entity recognition [32, 21, 57], video frame classification [22], bone and breast tumor classification [61]. More recently, Smith et al. [59] proposed a new approach to integrating weak supervision and pre-trained large language models (LLMs). Rather than applying LLMs in the usual zero/few-shot fashion, they treat those large models as weak labelers that can be used through prompting to obtain weak signals instead of using hand-crafted heuristics. Recently, Zhu et al. [66] showed that in many situations, the success of weakly supervised classifiers depends on the availability of ground truth validation samples, undermining the purpose of weak supervision. Then, we develop a new method for model evaluation that does not depend on the availability of any ground truth labels. ", "page_idx": 1}, {"type": "text", "text": "A relevant line of research within the realm of weak supervision that is closely related to this work is adversarial learning [4, 5, 42, 41]. Often, adversarial learning aims to learn predictors that perform well in worst-case scenarios. For example, Mazzetto et al. [41] develops a method to learn weakly supervised classifiers in the absence of a good label model. In their work, the authors use a small set of labeled data points to constrain the space of possible data distributions and then find a predictor that performs well in the worst-case scenario. Our work relates to this literature in the sense that we are interested in the worst and best-case scenarios over a set of distributions. However, we focus on developing an evaluation method instead of another adversarial learning strategy. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Partial identification: It is often the case that the distributions of interest cannot be fully observed, which is generally due to missing or noisy data [43, 23]. In cases where practitioners can only observe some aspects of those distributions, e.g., marginal distributions or moments, parameters of interest may not be identifiable without strong assumptions due to ambiguity in the observable data. Partial identification deals with the problem without imposing extra assumptions. This framework allows estimating a set of potential values for the parameters of interest (usually given by non-trivial bounds) and has been frequently considered in many areas such as microeconometrics [38-40, 43], causal inference [25, 23], algorithmic fairness [20, 46]. Our work is most related to Ruschendorf [53, 54, 55], Bartl et al. [7], which study bounds for the uncertainty of a quantity of interest for a joint distribution that is only partially identified through its marginals, i.e., Fr\u00e9chet bounds. Compared to the aforementioned works, the novelty of our contribution is proposing a convex optimization algorithm that accurately estimates the Fr\u00e9chet bounds with proven performance guarantees in a setup that is realized in numerous weak-supervision applications. ", "page_idx": 2}, {"type": "text", "text": "1.2Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We write $\\mathbb{E}_{Q}$ and $\\operatorname{Var}_{Q}$ for the expectation and variance of statistics computed using i.i.d. copies of a random vector $W\\sim Q$ . Consequently, $\\mathbb{P}_{Q}(A)=\\mathbb{E}_{Q}\\mathbb{1}_{A}$ , where ${\\mathbb I}_{A}$ is the indicator of an event $A$ .If the distribution is clear by the context, we omit the subscript. If $(a_{m})_{m\\in\\mathbb{N}}$ and $(b_{m})_{m\\in\\mathbb{N}}$ are sequences of scalars, then $a_{m}=o(b_{m})$ is equivalent to $a_{m}/b_{m}\\to0$ as $m\\rightarrow\\infty$ and $a_{m}=b_{m}+o(1)$ means $a_{m}\\!-\\!b_{m}=o(1)$ . If $(V^{(m)})_{m\\in\\mathbb{N}}$ is a sequence of random variables, then (i) $V^{(m)}=o_{P}(1)$ means that for every $\\varepsilon>0$ we have $\\mathbb{P}(|V^{(m)}|>\\varepsilon)\\to0$ as $m\\rightarrow\\infty$ (i) $V^{(m)}=\\mathcal{O}_{P}(1)$ means that for every $\\varepsilon>0$ there exists a $M>0$ such that $\\operatorname*{sup}_{m\\in\\mathbb{N}}\\mathbb{P}(|V^{(m)}|>M)<\\varepsilon$ (i) $V^{(m)}=a_{m}+o_{P}(1)$ means $V^{(m)}-a_{m}=o_{P}(1),$ (iv) $V^{(m)}\\,=\\,o_{P}\\bigl(a_{m}\\bigr)$ means $V^{(m)}/a_{m}\\,=\\,o_{P}(1)$ , and (v) $V^{(m)}=\\mathcal{O}_{P}(a_{m})$ means $V^{(m)}/a_{m}=\\mathcal{O}_{P}(1)$ ", "page_idx": 2}, {"type": "text", "text": "2   Estimating Fr\u00e9chet bounds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A roadmap to our approach follows. We first reformulate the Fr\u00e9chet bounds in (1.1) into their dual problems, which we discuss in (2.1). Then, we replace the non-smooth dual problems with their appropriate smooth approximations, as discussed in (2.2). Finally, we propose estimators for the smooth approximations (2.3) and derive their asymptotic distributions in Theorem 2.5. ", "page_idx": 2}, {"type": "text", "text": "2.1Dual formulations of the bounds and their approximations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents a result that allows us to efficiently solve the optimization problems in (1.1) by deriving their dual formulations as finite-dimensional convex programs. Before we dive into the result, let us define a family of matrices denoted by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathcal{A}}\\triangleq\\big\\{\\boldsymbol{a}\\in\\mathbb{R}^{|\\mathcal{Y}|\\times|\\mathcal{Z}|}\\ :\\ \\sum_{y\\in\\mathcal{Y}}a_{y z}=0\\mathrm{~for~every~}z\\in\\mathcal{Z}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "With this definition in place, we introduce the dual formulation in Theorem 2.1. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1. Let $g:\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Z}\\to\\mathbb{R}$ be a bounded measurable function. Then, ", "page_idx": 2}, {"type": "equation", "text": "$$\nL=\\operatorname*{sup}_{a\\in{\\mathcal{A}}}\\mathbb{E}[f_{l}(X,Z,a)]\\,a n d\\,U=\\operatorname*{inf}_{a\\in{\\mathcal{A}}}\\mathbb{E}[f_{u}(X,Z,a)]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{l}(x,z,a)\\triangleq\\!\\!\\operatorname*{min}_{\\bar{y}\\in\\mathcal{Y}}\\left[g(x,\\bar{y},z)+a_{\\bar{y}z}\\right]-\\mathbb{E}_{P_{Y\\mid z}}\\left[a_{Y z}\\middle|Z=z\\right]}}\\\\ {{f_{u}(x,z,a)\\triangleq\\!\\!\\operatorname*{max}_{\\bar{y}\\in\\mathcal{Y}}\\left[g(x,\\bar{y},z)+a_{\\bar{y}z}\\right]-\\mathbb{E}_{P_{Y\\mid z}}\\left[a_{Y z}\\middle|Z=z\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, $L$ and $U$ are attained by some optimizers in $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 remains valid if we maximize/minimize over $\\mathbb{R}^{|\\mathcal{D}|\\times|\\mathcal{Z}|}$ instead of $\\boldsymbol{\\mathcal{A}}$ However, this is not necessary because the values of $f_{l}$ and $f_{u}$ remain identical for the following shifts in $a$ $\\iota\\colon a._{z}\\leftarrow a._{z}\\!+\\!b_{z}$ where $b_{z}\\in\\mathbb{R}$ . By constraining the set of optimizers to $\\boldsymbol{\\mathcal{A}}$ , we eliminate the possibility of having multiple optimal points. The proof of Theorem 2.1 is placed in Appendix B and is inspired by ideas from Optimal Transport; see Appendix A. ", "page_idx": 2}, {"type": "text", "text": "The computation of these bounds entails finding a minimum or maximum over a discrete set, meaning that straightforward application of their empirical versions could result in optimizing non-smooth functions, which is often challenging. To mitigate this, we consider a smooth approximation of the problem that is found to be useful in handling non-smooth optimization problems [3, 6]. We approximate the max and min operators with their \u201csoft\" counterparts: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{oftmin}\\{b_{1},\\cdots,b_{K}\\}\\triangleq-\\varepsilon\\log[\\frac{1}{K}\\sum_{k}\\exp(\\frac{-b_{k}}{\\varepsilon})],\\:\\:\\mathrm{softmax}\\{b_{1},\\cdots,b_{K}\\}\\triangleq\\varepsilon\\log[\\frac{1}{K}\\sum_{k}\\exp(\\frac{b_{k}}{\\varepsilon})]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\varepsilon>0$ is a small constant that dictates the level of smoothness. As $\\varepsilon$ nears zero,these soft versions of max and min converge to their original non-smooth forms. Using these approximations, we reformulate our dual optimization in (2.1) into smooth optimization problems: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\varepsilon}\\triangleq\\operatorname*{sup}_{a\\in\\mathcal{A}}\\mathbb{E}[f_{l,\\varepsilon}(X,Z,a)]\\mathrm{~and~}U_{\\varepsilon}\\triangleq\\operatorname*{inf}_{a\\in\\mathcal{A}}\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{l,\\varepsilon}(x,z,a)\\triangleq-\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in\\mathcal{y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{-\\varepsilon}\\right)\\right]-\\mathbb{E}_{P_{Y\\mid Z}}\\left[a_{Y z}\\mid Z=z\\right]}\\\\ &{f_{u,\\varepsilon}(x,z,a)\\triangleq\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{\\varepsilon}\\right)\\right]-\\mathbb{E}_{P_{Y\\mid Z}}\\left[a_{Y z}\\mid Z=z\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\varepsilon>0$ is kept fixed at an appropriate value. As a consequence of Lemma 5 of An et al. [3], we know that $L_{\\varepsilon}$ and $U_{\\varepsilon}$ are no more than $\\varepsilon\\log\\left|\\mathcal{V}\\right|$ units from $L$ and $U$ . Thus, that distance can be regulated by adjusting $\\varepsilon$ . For example, if we are comfortable with an approximation error of $10^{-2}$ units when $|\\mathcal{Y}|=2$ , we will set $\\varepsilon=10^{-2}/\\log(2)\\approx.014$ ", "page_idx": 3}, {"type": "text", "text": "2.2  Estimating the bounds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, it is not usually possible to solve the optimization problems in (2.2), because we may not have direct access to the distributions $P_{X,Z}$ and $P_{Y\\mid Z}$ . We overcome this problem by assuming that we can estimate the distributions using an available dataset. ", "page_idx": 3}, {"type": "text", "text": "To this end, let us assume that we have a sample $\\{(X_{i},Z_{i})\\}_{i=1}^{n}\\stackrel{\\mathrm{iid}}{\\sim}P_{X,Z}$ , and thus we replace the relevant expectations with $P_{X,Z}$ by its empirical version. Additionally, we have a sequence $\\{\\hat{P}_{Y|Z}^{(m)},m\\in\\mathbb{N}\\}$ that estimates $P_{Y\\mid Z}$ with greater precision as $m$ increases. Here, $m$ can be viewed as the size of a sample to estimate $P_{Y\\mid Z}$ . Although the exact procedure for estimating the conditional distribution is not relevant to this section, we have discussed in our introductory section that this can be estimated using a label model [49, 22] in applications with weak supervision or in a variety of other ways for applications beyond weak supervision. Later in this section, we will formalize the precision required fo the estiates To simplify our notation, we omithe superscrip m in Ppe whenever it is convenient to do so. ", "page_idx": 3}, {"type": "text", "text": "Thus, the Fr\u00e9chet bounds are estimated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{L}_{\\varepsilon}=\\underset{a\\in A}{\\operatorname*{sup}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\hat{f}_{l,\\varepsilon}(X_{i},Z_{i},a)\\;\\;\\mathrm{and}\\;\\hat{U}_{\\varepsilon}=\\underset{a\\in A}{\\operatorname*{inf}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\hat{f}_{u,\\varepsilon}(X_{i},Z_{i},a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{f}_{l,\\varepsilon}(x,z,a)\\triangleq-\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in\\mathcal{y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{-\\varepsilon}\\right)\\right]-\\mathbb{E}_{\\hat{P}_{Y\\mid Z}}\\left[a_{Y z}\\mid Z=z\\right]}\\\\ &{\\widehat{f}_{u,\\varepsilon}(x,z,a)\\triangleq\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{\\varepsilon}\\right)\\right]-\\mathbb{E}_{\\hat{P}_{Y\\mid Z}}\\left[a_{Y z}\\mid Z=z\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In our practical implementations we eliminate the constraint that $\\textstyle\\sum_{y}a_{y z}=0$ for all $z\\in{\\mathcal{Z}}$ by adding a penalty term $\\begin{array}{r}{\\sum_{z\\in\\mathcal{Z}}(\\sum_{y\\in\\mathcal{y}}a_{y z})^{2}}\\end{array}$ to $\\hat{U}_{\\varepsilon}$ (and its negative to $\\hat{L}_{\\varepsilon}$ ) and then solve unconstrained convex programs using the L-BFGS algorithm [33]. Since the penalty term vanishes only when $\\textstyle\\sum_{y}a_{y z}=0$ for all $z\\in{\\mathcal{Z}}$ , we guarantee that the optimal solution is in $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 3}, {"type": "text", "text": "2.3  Asymptotic properties of the estimated bounds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the following, we state the assumptions required for our asymptotic analysis of $\\hat{L}_{\\varepsilon}$ and $\\hat{U}_{\\varepsilon}$ .We start with some regularity assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2. $L_{\\varepsilon}$ and $U_{\\varepsilon}$ are attained by some optimizers in $\\boldsymbol{\\mathcal{A}}$ (2.2). ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3.Let $\\hat{a}$ represent the optimizer for any problem in (2.3), which is assumed to exist. Suppose $\\|\\hat{a}\\|_{\\infty}=\\mathcal{O}_{P}(1)$ Gas $m\\rightarrow\\infty$ ", "page_idx": 3}, {"type": "text", "text": "We show in Lemmas C.4, C.5, and C.6 that Assumptions 2.2 and 2.3 can be derived in the binary classification case $(|\\mathcal{Y}|=2)$ if $\\mathbb{P}(Y=y\\mid Z=z)$ is bounded away from both zero and one, i.e. $\\kappa<\\mathbb{P}(Y=y\\mid Z=z)<1-\\kappa$ for some $\\kappa>0$ for every $y\\in\\mathcal{V}$ and $z\\in{\\mathcal{Z}}$ ", "page_idx": 4}, {"type": "text", "text": "In our next asumption, w formalize thedgreeof pecsion frthe sequence $\\{\\hat{P}_{Y|Z}^{(m)},m\\in\\mathbb{N}\\}$ estimators that we require for desired performances of the bound estimates. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.4. Denote the total variation distance $(T V)$ between probability measures as $d_{\\mathrm{TV}}$ .For every $z\\in{\\mathcal{Z}}$ for some $\\lambda>0$ we have hat $d_{\\mathrm{TV}}\\big(\\hat{P}_{Y|Z=z}^{(m)},P_{Y|Z=z}\\big)=\\mathcal{O}_{P}(m^{-\\lambda})$ ", "page_idx": 4}, {"type": "text", "text": "From Ratner et al. [49]'s Theorem 2 and a Lipschitz property of the label model, we can conclude $\\lambda=1/2$ for a popular label model used in the PWS literature. The asymptotic distributions for the estimated bounds follow. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.5. Assume 2.2, 2.3, and 2.4, and let n be a function of $m$ suchthat $n~\\rightarrow~\\infty$ and $n=o(m^{2\\lambda})$ when $m\\rightarrow\\infty$ Then,as $m\\rightarrow\\infty$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{L}_{\\varepsilon}-L_{\\varepsilon})\\Rightarrow N(0,\\sigma_{l,\\varepsilon}^{2})\\;a n d\\;\\sqrt{n}(\\hat{U}_{\\varepsilon}-U_{\\varepsilon})\\Rightarrow N(0,\\sigma_{u,\\varepsilon}^{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma_{l,\\varepsilon}^{2}\\triangleq\\operatorname{Var}f_{l,\\varepsilon}(X,Z,a_{l,\\varepsilon}^{*}),\\;\\sigma_{u,\\varepsilon}^{2}\\,\\triangleq\\,\\operatorname{Var}f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})$ and $a_{l,\\varepsilon}^{*}$ and $a_{u,\\varepsilon}^{*}$ are the unique optimizers to attain $L_{\\varepsilon}$ and $U_{\\varepsilon}$ (2.2). ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.5 tells us that, if the label model is consistent (Assumption 2.4), under some mild regularity conditions (Assumption 2.2 and 2.3), our estimators and will be asymptotically Gaussian with means $L_{\\varepsilon}$ and $U_{\\varepsilon}$ and variances $\\sigma_{l,\\varepsilon}^{2}/n$ and $\\sigma_{u,\\varepsilon}^{2}/n$ .The above theorem requires $m^{2\\lambda}$ to grow faster than $n$ implying that, through assumption 2.4, $P_{Y\\mid Z}$ is estimated with a precision greater than the approximation error when we replace $P_{X,Z}$ with $\\begin{array}{r}{\\frac{1}{n}\\sum_{i}\\delta_{X_{i},Z_{i}}}\\end{array}$ . In the case which $\\lambda=1/2$ , this condition translates to $n/m\\rightarrow0$ as $n\\to\\infty$ This allows us to derive the asymptotic distribution when combined with classical results from M-estimation (see proof in Appendix C). ", "page_idx": 4}, {"type": "text", "text": "Construction of confidence bounds: One interesting use of Theorem 2.5 is that we can construct an approximate confidence interval for the estimates of the bounds. For example, an approximate $1-\\gamma$ confidenceintervalfor $L_{\\varepsilon}$ canisconstructed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{I}=\\Big[\\hat{L}_{\\varepsilon}-\\frac{\\tau_{\\gamma}\\hat{\\sigma}_{l,\\varepsilon}}{\\sqrt{n}},\\ \\hat{L}_{\\varepsilon}+\\frac{\\tau_{\\gamma}\\hat{\\sigma}_{l,\\varepsilon}}{\\sqrt{n}}\\Big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau_{\\gamma}=\\Phi^{-1}(1-\\gamma/2)$ and $\\hat{\\sigma}_{l,\\varepsilon}$ is the empirical standard deviation of $f_{l,\\varepsilon}(X,Z,\\cdot)$ , substituting the estimate $\\hat{a}$ (solution for the problem in 2.3). For such interval, it holds $\\mathbb{P}\\big(L_{\\varepsilon}\\in\\hat{I}\\big)\\approx1-\\gamma.$ i.e., with approximately $1-\\gamma$ confidence we can say that the true $L_{\\varepsilon}$ is in the interval above. An interval for $U_{\\varepsilon}$ can be constructed similarly. ", "page_idx": 4}, {"type": "text", "text": "3  Evaluation of model performance in weak supervision ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we describe how to use the ideas presented in Section 2 to estimate non-trivial bounds for the evaluation metrics of a weakly supervised classifier $h$ when no high-quality labels are available. In the standard weak supervision setup, only unlabeled data $(X)$ is available, but the practitioner can extract weak labels $(Z)$ from the available data. More specifically, we assume access to the dataset $\\{(X_{i},Z_{i})\\}_{i=1}^{m}$ , i.d. with distribution $P_{X,Z}$ , used in its entirety to estimate a label model $\\hat{P}_{Y\\mid Z}$ [49, 22] and where part of it, e.g., a random subset of size $n$ , is used to estimate bounds4. To simplify the exposition, we assume the classifier $h$ is fixed5. ", "page_idx": 4}, {"type": "text", "text": "3.1 Risk and accuracy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $\\ell$ be a generic classification loss function. The risk of a classifier $h$ is defined as $R(h)\\;=\\;$ $\\mathbb{E}[\\ell(h(X),\\bar{Y})]$ , which cannot be promptly estimated in a weak supervision problem, where we do not observe any $Y$ . In this situation, we can make use of our bound estimators in Section 2.2, where we set $g(x,y,\\dot{z})=\\ell(h(x),y)$ to obtain bounds for $R(h)$ . Furthermore, we can estimate an uncertainty set for the accuracy of the classification simply by letting $g(x,y,z)=\\mathbb{1}[h(x)=y]$ ", "page_idx": 4}, {"type": "text", "text": "3.2 Precision, recall, and $F_{1}$ score ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For a binary classification problem, where ${\\mathcal{D}}\\,=\\,\\{0,1\\}$ , the precision, recall, and $F_{1}$ score of a classifier $h$ are defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\triangleq\\mathbb{P}(Y=1\\mid h(X)=1)=\\frac{\\mathbb{P}(h(X)=1,Y=1)}{\\mathbb{P}(h(X)=1)},\\;\\;r\\triangleq\\mathbb{P}(h(X)=1\\mid Y=1)=\\frac{\\mathbb{P}(h(X)=1,Y=1)}{\\mathbb{P}(Y=1)},}\\\\ &{F\\triangleq\\frac{2}{r^{-1}+p^{-1}}=\\frac{2\\mathbb{P}(h(X)=1,Y=1)}{\\mathbb{P}(h(X)=1)+\\mathbb{P}(Y=1)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The quantities $\\mathbb{P}(h(X)=1)$ and $\\mathbb{P}(Y=1)$ in the above definitions are identified, since the marginals $P_{X,Z}$ and $P_{Y,Z}$ are specified in the Fr\u00e9chet problem in (1.1). The $\\mathbb{P}(h(X)=1)$ can be estimated from the flldataset $\\{(X_{i},Z_{i})\\}_{i=1}^{m}$ simply using $\\begin{array}{r}{\\hat{\\mathbb{P}}(h(X)=1)\\triangleq\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}[h(X_{i})=1]}\\end{array}$ On the other hand, in most weak supervision applications, $\\mathbb{P}(Y=1)$ is assumed to be known from some prior knowledge or can be estimated from an auxiliary dataset, e.g., using the method described in the appendix of Ratner et al. [49]. Estimating or knowing $\\mathbb{P}(Y=1)$ is required to fit the label model [49, 22] in the first place, so it is beyond our scope of discussion. Then, we assume we have an accurate estimate $\\hat{\\mathbb{P}}(Y=1)$ ", "page_idx": 5}, {"type": "text", "text": "Theprobability $\\mathbb{P}(h(X)=1,Y=1)$ , which is the final ingredient in the definition of precision, recall, and F1 score is not identifiable as $P_{X,Y}$ is unknown. The uncertainty bounds for this quantity can be estimated using our method simply by letting $g(x,y,z)=\\mathbb{1}[h(x)=1$ and $y=1]$ . Let $\\hat{L}_{\\varepsilon}$ and $\\hat{U}_{\\varepsilon}$ denote the estimated lower and upper bounds for $\\mathbb{P}(h(X)=1,Y=1)$ obtained using (2.3). Naturally, the lower bound estimators for precision, recall, and $F_{1}$ score are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{p}_{l,\\varepsilon}\\triangleq\\frac{\\hat{L}_{\\varepsilon}}{\\hat{\\mathbb{P}}(h(X)=1)},\\,\\hat{r}_{l,\\varepsilon}\\triangleq\\frac{\\hat{L}_{\\varepsilon}}{\\hat{\\mathbb{P}}(Y=1)},\\,\\mathrm{and}\\,\\,\\hat{F}_{l,\\varepsilon}\\triangleq\\frac{2\\hat{L}_{\\varepsilon}}{\\hat{\\mathbb{P}}(h(X)=1)+\\hat{\\mathbb{P}}(Y=1)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "while the upper bound estimators $\\hat{p}_{u,\\varepsilon},\\hat{r}_{u,\\varepsilon}$ and $\\hat{F}_{u,\\varepsilon}$ are given by substituting $\\hat{L}_{\\varepsilon}$ by $\\hat{U}_{\\varepsilon}$ above.In the following corollary, we show that the bounds converge asymptotically to normal distributions, which we use for calculating their coverage bounds presented in our applications. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.1. Let n be a function of m such that $n\\to\\infty$ and $n=o\\left(m^{(2\\lambda)\\wedge1}\\right)$ when $m\\rightarrow\\infty$ Assume the conditions of Theorem 2.5 hold. Then as $m\\rightarrow\\infty$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\circ\\sqrt{n}\\big(\\hat{p}_{l,\\varepsilon}-p_{l,\\varepsilon}\\big)\\Rightarrow N(0,\\sigma_{p,l,\\varepsilon}^{2})\\;w i t h\\;p_{l,\\varepsilon}=\\frac{L_{\\varepsilon}}{\\mathbb{P}(h(X)=1)},\\;\\sigma_{p,l,\\varepsilon}^{2}\\triangleq\\frac{\\sigma_{l,\\varepsilon}^{2}}{\\mathbb{P}(h(X)=1)^{2}},}\\\\ &{\\circ\\sqrt{n}\\big(\\hat{r}_{l,\\varepsilon}-r_{l,\\varepsilon}\\big)\\Rightarrow N(0,\\sigma_{r,l,\\varepsilon}^{2})\\;w i t h\\;r_{l,\\varepsilon}=\\frac{L_{\\varepsilon}}{\\mathbb{P}(Y=1)},\\;\\sigma_{r,l,\\varepsilon}^{2}\\triangleq\\frac{\\sigma_{l,\\varepsilon}^{2}}{\\mathbb{P}(Y=1)^{2}},}\\\\ &{\\circ\\sqrt{n}\\big(\\hat{F}_{l,\\varepsilon}-F_{l,\\varepsilon}\\big)\\Rightarrow N(0,\\sigma_{F,l,\\varepsilon}^{2})\\;w i t h\\;F_{l,\\varepsilon}=\\frac{2L_{\\varepsilon}}{\\mathbb{P}(h(X)=1)+\\mathbb{P}(Y=1)}\\;\\&\\;\\sigma_{F,l,\\varepsilon}^{2}\\triangleq\\frac{4\\sigma_{l,\\varepsilon}^{2}}{\\mathbb{P}(h(X)=1)+\\mathbb{P}(Y=1)\\big]^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "e\uff0c $L_{\\varepsilon}$ $\\sigma_{l,\\varepsilon}^{2}$ $\\sqrt{n}\\big(\\hat{p}_{u,\\varepsilon}-p_{u,\\varepsilon}\\big)$ $\\sqrt{n}(\\hat{r}_{u,\\varepsilon}-$$r_{u,\\varepsilon})$ $\\sqrt{n}\\big(\\hat{F}_{u,\\varepsilon}-F_{u,\\varepsilon}\\big)$ $L_{\\varepsilon}$ $U_{\\varepsilon}$ $\\sigma_{l,\\varepsilon}^{2}$ $\\sigma_{u,\\varepsilon}^{2}$", "page_idx": 5}, {"type": "text", "text": "Reiterating our discussion in the final paragraph in Section 2.2, asymptotic distributions are important for constructing confidence intervals for the bounds, which can be done in a similar manner. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "All experiments are structured to emulate conditions where high-quality labels are inaccessible during training, validation, and testing phases, and all weakly-supervised classifiers are trained using the noise-aware loss [50]. To fit the label models, we assume $P_{Y}$ is known (computed using the training set). Unless stated, we use $l_{2}$ -regularized logistic regressors as classifiers, where the regularization strength is determined according to the validation noise-aware loss. ", "page_idx": 5}, {"type": "text", "text": "Wrench datasets: To carry out realistic experiments within the weak supervision setup and study accuracy/F1 score estimation, we utilize datasets incorporated in Wrench (Weak Supervision Benchmark) [63]. This standardized benchmark platform features real-world datasets and pregenerated weak labels for evaluating weak supervision methodologies. Most of Wrench's datasets are designed for classification tasks, encompassing diverse data types such as tabular, text, and image; all contain their pre-computed weak labels. Specifically, we utilize Census [27], YouTube [1], SMS [2], IMDB [37], Yelp [65], AGNews [65], TREC [31], Spouse [12], SemEval [24], CDR [14], ChemProt [29], Commercial [22], Tennis Rally [22], Basketball [22]. For text datasets, we employ the paraphrase-MiniLM-L6-v2 model from the sentence-transformers\u00b0 library for feature extraction [51]. Features were extracted for the image datasets before their inclusion in Wrench. ", "page_idx": 5}, {"type": "image", "img_path": "VOVyeOzZx0/tmp/75acde18cb11914ddb57032f8021fcbf4fc66861d4668dc4fad4f94fdb1c6f4d.jpg", "img_caption": ["Figure 1: We apply our method to bound test metrics such as accuracy and F1 score (in green) when no true labels are used to estimate performance. In the first row (\\*\"Oracle\"), we use true labels to estimate the conditional distribution $P_{Y\\mid Z}$ , thus approximating a scenario in which the label model is reasonably specified. On the second row (\\*Snorkel\"'), we use a label model to estimate $P_{Y\\mid Z}$ without access to any true labels. Despite potential misspecification in Snorkel's label model, it performs comparably to using labels to estimate $P_{Y\\mid Z}$ ,giving approximate but meaningful bounds. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Hate Speech Dataset [15]: This dataset contains sentence-level annotations for hate speech in English, sourced from posts from white supremacy forums. It encompasses thousands of sentences classified into either Hate (1) or noHate (O) categories. This dataset provides an ideal ground for examining recall and precision estimation. Social media moderators aim to maximize the filtering of hate posts, i.e., increasing recall, while ensuring that non-hate content is rarely misclassified as offensive, maintaining high precision. Analogously to the Wrench text datasets, we utilize paraphrase-MiniLM-L6-v2 for feature extraction. ", "page_idx": 6}, {"type": "text", "text": "4.1 Bounding the performance of weakly supervised classifiers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct an empirical study using some of the Wrench and Hate Speech datasets to verify the validity and usefulness of our methodology. We compare results for which $P_{Y\\mid Z}$ is estimated using the true labels $Y$ (\"Oracle\") and those derived using Snorkel's [48, 47] default label model with no hyperparameter tuning and a thousand epochs. Such a comparison facilitates an evaluation of our method's effcacy, especially in cases where the label model could be incorrectly specified. Results for other Wrench datasets and one extra label model (FlyingSquid, [22]) are presented in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "In Figure 1, we demonstrate our approaches for bounding test metrics, such as accuracy and F1 score (shown in green), when no true labels are available to estimate performance at various classification thresholds for binary classification tasks on Wrench datasets. In the first row (\"Oracle\"), true labels are used to estimate the conditional distribution $P_{Y\\mid Z}$ representing a (close to) ideal scenario with a ", "page_idx": 6}, {"type": "table", "img_path": "VOVyeOzZx0/tmp/c6ff5f77473c28deefd9fab77f426ebeef1dda70eb5b3883eddadc9fee9034fe.jpg", "table_caption": ["Table 1: Bounding accuracy in multinomial classification "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "well-specified label model. In the second row (\\*\"Snorkel\"), however, we use a label model to estimate $P_{Y\\mid Z}$ without relying on any true labels. Despite potential inaccuracies in Snorkel's label model, it achieves results close to those obtained using true labels to estimate $P_{Y\\mid Z}$ ,yielding approximate but useful bounds. This indicates that even if Snorkel's label model is imperfectly specified, its effectiveness in estimating bounds remains similar to that of the \u201cOracle\u201d approach, underscoring the value of bounding metrics regardless of label model accuracy. Delving deeper into Figure 1, results for \u201cyoutube\", \u201ccommercial\", and \u201ctennis\u201d highlight that our uncertainty about out-of-sample performance is small, even without labeled samples. However, there is a noticeable increase in uncertainty for \u201cimdb\" and \u201ccdr\", making weakly supervised models deployment riskier without additional validation. Yet, the bounds retain their informative nature. For instance, for those willing to accept the risk, the \u201cimdb' classifier's ideal threshold stands at .5. This is deduced from the flat worst-case and peaking best-case accuracy at this threshold. Table 1 presents some results for \"agnews\" (4 classes) and \u201csemeval\" (9 classes). From Table 1, we can see that both \u201cOracle\u201d and \"Snorkel\" approaches produce valid bounds. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Now, we present bounds on the classifiers' precision and recall across different classification thresholds for the hate speech dataset. This dataset did not provide weak labels, so we needed to generate them. We employed four distinct weak labelers. The initial weak labeler functions are based on keywords and terms. Should words or phrases match those identified as hate speech in the lexicon created by Davidson et al. [13], we categorize the sentence as 1; if not, it's designated O. The second weak labeler is based on TextBlob's sentiment analyzer [36]: a negative text polarity results in a 1 classification, while other cases are labeled 0. Our final pair of weak labelers are language ", "page_idx": 7}, {"type": "image", "img_path": "VOVyeOzZx0/tmp/bb11adedae59cce5d2640082f3a75da0d602d8e2bc590f2798c4c54a73c812d9.jpg", "img_caption": ["Figure 2: Precision and recall bounds for hate speech detection. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "models, specifically BERT [16] and RoBERTa [34], that have undergone fine-tuning for detecting toxic language or hate speech [35, 28]. Figure 2 presents both recall and precision bounds and test estimates for the weakly-supervised hate speech classifier. Mirroring observations from Figure 1, Snorkel's standard label model gives valuable bounds analogous to scenarios where we employ labels toestimate $P_{Y\\mid Z}$ . If used by practitioners, Figure 2 could help trade-off recall and precision by choosing an appropriate classification threshold in the absence of high-quality labels. ", "page_idx": 7}, {"type": "text", "text": "4.2 Choosing a set of weak labels ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, we examine how our approach performs under the influence of highly informative weak labels as opposed to scenarios with less informative weak labels. Using the YouTube dataset provided by Wrench, we attempt to classify YouTube comments into categories of SPAM or HAM, leveraging Snorkel to estimate $P_{Y\\mid Z}$ . Inspired by Smith et al. [59], we craft three few-shot weak labelers by prompting7 the large language model (LLM) Llama-2-13b-chat-hf [60]. For each dataset entry, we pose three distinct queries to the LLM. Initially, we inquire if the comment is SPAM or HAM. Next, we provide clear definitions of SPAM and HAM, then seek the classification from LLM. In the third prompt, leveraging in-context learning ideas [17], we provide five representative comments labeled as SPAM/HAM prior to requesting the LLM's verdict on the comment in question. In cases where LLM's response diverges from SPAM or HAM, we interpret it as LLM's abstention. ", "page_idx": 7}, {"type": "text", "text": "After obtaining this triad of weak labels, we analyze two situations. Initially, we integrate the top five8 weak labels (\\*high-quality\" labels) from Wrench. In the subsequent scenario, we synthetically generate weak labels (\\*low-quality\" labels) that do not correlate with $Y$ . The first plot in Figure 3 depicts the bounds of our classifier based solely on weak few-shot labels, which unfortunately do not provide substantial insights. Enhancing the bounds requires the inclusion of additional weak labels. Yet, as indicated by the subsequent pair of plots, it becomes evident that only the incorporation of \u201chigh-quality\u201d weak labels results in significant shrinkage and upward shift of the bounds. As confirmed by the test accuracy, if a practitioner had used our method to select the set of weak labels, that would have led to a significant boost in performance. ", "page_idx": 7}, {"type": "text", "text": "4.3 Model selection strategies using the Fr\u00e9chet bounds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Sections 4.1 and 4.2, we implicitly touched on the topic of model selection when discussing the classification threshold and weak label selection. Here, we explicitly discuss the use of our Fr\u00e9chet bounds for model selection purposes. Consider a set of possible models $\\mathcal{H}\\triangleq\\{h_{1},\\cdot\\cdot\\cdot,h_{K}\\}$ from which we wish to find the best model according to a specific metric, e.g., accuracy, or F1 score. We consider three approaches for model selection using the Fr\u00e9chet bounds: choosing the model with the best possible (i) lower bound, (ii) upper bound, and (i) average of lower and upper bounds on the metric of interest. Strategy (i) works well for the worst-case scenario and can be seen as the distributionally robust optimization (DRO) [9] solution when the uncertainty set is given by $\\Pi$ in (1.1), while (i) is suitable for an optimistic scenario, and (i) is suggested when one wants to balance between the worst- and best-case scenarios. Please check Appendix E for more details. ", "page_idx": 7}, {"type": "image", "img_path": "VOVyeOzZx0/tmp/6ea5de516c1b6069999a01a8c9d5e24885d3a05980d2117d33b322db6ccdbe82.jpg", "img_caption": ["Figure 3: Performance bounds for classifiers on the YouTube dataset, initially relying solely on few-shot weak labels obtained via prompts to the LLM Llama-2-13b-chat -hf. The progression of plots illustrates the comparative impact of integrating \u201chigh-quality\u201d labels from Wrench versus synthetically generated \u201clow-quality\" labels. Evidently, the addition of \"high-quality\" labels significantly enhances the bounds, underscoring their superior utility over \u201clow-quality\" labels for optimal classification of SPAM and HAM comments. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In this experiment, we select multilayer-perceptrons (MLPs)._The considered MLPs have one hidden layer with a possible number of neurons in $\\{50,100\\}$ . Training is carried out with Adam [26], with possible learning rates in $\\{.1,.001\\}$ and weight decay $l_{2}$ regularization parameter) in $\\{.1,.001\\}$ . For those datasets that use the F1 score as the evaluation metric, we also tune the classification threshold in $\\{.2,.4,.5,.6,.8\\}$ (otherwise, they return the most probable class as a prediction). In total, $\\mathcal{H}$ is composed of 8 trained models when evaluating accuracy and 40 models when evaluating the F1 score. We also consider directly using the label model (Snorkel [47]) to select models. For example, when the metric considered is accuracy, i.e., we use select the model arg $\\begin{array}{r}{\\operatorname*{max}_{h_{k}\\in\\mathcal{H}}\\!\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\hat{P}_{Y|Z}}^{\\texttt{+}}\\mathbb{1}[h_{k}(X)=Y\\mid Z=Z_{i}]}\\end{array}$ which is a natural choice when $X\\perp\\!\\!\\!\\!\\perp Y\\mid Z$ As baselines, we consider having a few labeled samples. ", "page_idx": 8}, {"type": "table", "img_path": "VOVyeOzZx0/tmp/40cbe3ce8bd72d95af8e819a62f98c446ae4d85520ec3f8cc9e31a28264f2f05.jpg", "table_caption": ["Table 2: Performance of selected models "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In Table 2, we report a subset of our results (please check Appendix E for the full set of results). In this table, we report the average test scores of the chosen models over 10 repetitions for different random seeds (standard deviation re", "page_idx": 8}, {"type": "text", "text": "port as subscript). We can extract some lessons from the table. First, using metrics derived from the Fr\u00e9chet bounds is most useful when our uncertainty about the model performance is low, e.g., \"'commercial\"' and \u201c'tennis\"' in Figure 1. In those cases, using our metrics for model selection gives better results even when compared to a labeled validation set of size $n=100$ .Moreover,oncethe practitioner knows that the uncertainty is low, using the label model approach also does well. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Extensions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "An extension we do not address in the main text is the evaluation of end-to-end weak supervision methods [62, 56, 52], where the separation between the label model and the final predictor is less clear than in our primary setting. Our approach remains compatible with these methods as long as we can fit a label model (e.g., Snorkel) separately and utilize it solely for the evaluation step. Another possible extension is the application of the ideas presented in this work in different fields of machine learning or statistics. One could consider applying our ideas to the problem of \u201cstatistical matching\" (SM) [18, 10, 19, 30, 11], for example. The classic formulation of SM involves observing two distinct datasets that contain replications of $(X,Z)$ and $(Y,Z)$ , but the triplet $\\left(X,Y,Z\\right)$ is never observed. The primary goal is to make inferences about the relationship between $X$ and $Y$ . For instance, if our focus is on bounding $\\mathbb{P}((X,Y)\\in B)=\\mathbb{E}[\\mathbb{1}_{B}(X,Y)]$ for a certain event $B$ , we could define $g(x,y,z)=\\mathbb{1}_{B}(x,y)$ and apply our method. ", "page_idx": 8}, {"type": "text", "text": "5.2  Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We discuss some limitations of our methods. Firstly, our method and theoretical results are only applicable to cases where $\\boldsymbol{\\wp}$ and $\\mathcal{Z}$ are finite sets, such as in classification problems. Extending the dual formulation in Theorem 2.1 to general $\\boldsymbol{\\wp}$ and $\\mathcal{Z}$ is possible but would require optimizing over function spaces, which is computationally and theoretically challenging. Additionally, if $|\\mathcal{Z}|$ is large, convergence may be slow, necessitating a large unlabeled dataset for accurate bounds. Using a smaller, curated set of weak labels, may be more effective for bounds estimation and performance. We end this subsection with two other limitations related to misspecification in the label model and informativeness of the bounds. The proofs of the results introduced in this section are placed in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "Label model misspecification: In our asymptotic results we assumed that the label models are well-specifed, i.e,the estimates $\\{\\hat{P}_{Y|Z}^{(m)},m\\in\\mathbb{N}\\}$ converge to the true labelmdel $P_{Y\\mid Z}$ $m\\rightarrow$ $\\infty$ . To understand the qualities of our bound when this assumption is violated, we introduce the mispecication: $\\hat{P}_{Y|Z}^{(m)}\\,\\hat{\\,}\\!\\to Q_{Y|Z}$ and $Q_{Y|Z}\\ne P_{Y|Z}$ In ourinvetigation on the mispecication of the label model, we control the level of misspecification as $d_{\\mathrm{TV}}(Q_{Y|Z=z},P_{Y|Z=z})\\leq\\delta$ and then study the subsequent errors in our Fr\u00e9chet bounds. The following theorem formalizes the result. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1. Recall from equation (2.2) that $L_{\\epsilon}$ and $U_{\\epsilon}$ are the smoothened upper and lower Frechet bounds with the true $P_{Y\\mid Z=z}$ . Additionally, let us define similar $\\check{L}_{\\epsilon}$ and $\\check{U}_{\\epsilon}$ bounds, but with a misspecified $Q_{Y\\mid Z=z}$ , i.e. ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\check{L}_{\\epsilon}\\triangleq\\underset{a\\in A}{\\operatorname*{sup}}\\mathbb{E}[\\check{f}_{l,\\varepsilon}(X,Z,a)]\\quad a n d\\quad\\check{U}_{\\epsilon}\\triangleq\\underset{a\\in A}{\\operatorname*{inf}}\\,\\mathbb{E}[\\check{f}_{u,\\varepsilon}(X,Z,a)]\\,,}\\\\ &{\\check{f}_{l,\\epsilon}(x,z,a)\\triangleq-\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{Y}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{-\\varepsilon}\\right)\\right]-\\mathbb{E}_{Q_{Y\\mid Z}}\\left[a_{Y z}\\mid Z=z\\right]}\\\\ &{\\check{f}_{u,\\varepsilon}(x,z,a)\\triangleq\\,\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{Y}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{\\varepsilon}\\right)\\right]-\\mathbb{E}_{Q_{Y\\mid Z}}\\left[a_{Y z}\\mid Z=z\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Assume $Q_{Y\\mid Z}$ is in a set of conditional distributions such that the optimizers for (5.1), which are assumed to exist,are uniformly bounded. If $d_{\\mathrm{TV}}\\big(Q_{Y|Z=z},P_{Y|Z=z}\\big)\\leq\\delta,$ .thenfor some $C>0$ which isindependentof $\\delta>0$ wehave ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(|\\check{L}_{\\epsilon}-L_{\\epsilon}|,|\\check{U}_{\\epsilon}-U_{\\epsilon}|\\right)\\leq C\\delta\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The above theorem reveals how misspecification translates to the errors in subsequent Fr\u00e9chet bounds. In an ideal scenario, with access to an $\\{(Y_{i},Z_{i})\\}_{i=1}^{m}$ sample we can consistently estimate $P_{Y\\mid Z}$ \uff0c leading to $\\delta=0$ . In situations when this $\\{(Y_{i},Z_{i})\\}_{i=1}^{m}$ sample is not accessible and we have to rely on a label model, we require the misspecification in this model to be small. ", "page_idx": 9}, {"type": "text", "text": "Informativeness of the bounds: The bounds $L$ and $U$ are especially useful when their difference $U-L$ is small because in that case, we obtain a tight bound for $\\mathbb{E}[g(X,Y,Z)]$ and narrow it down with high precision even if the joint random vector $\\left(X,Y,Z\\right)$ is never observed. But when is this bound small? In the next theorem, we provide an upper bound on this difference. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.2. Let $L$ and $U$ be defined as in equation (1.1). Then ", "page_idx": 9}, {"type": "equation", "text": "$$\nU-L\\leq\\sqrt{8\\left\\|g\\right\\|_{\\infty}^{2}\\operatorname*{min}\\{H(X\\mid Z),H(Y\\mid Z)\\}}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $H(X\\mid Z)$ (resp. $H(Y\\mid Z),$ denotes the conditional entropy of $X$ (resp. $Y$ )given $Z$ ", "page_idx": 9}, {"type": "text", "text": "To understand the result better, recall our setting: we do not observe the joint distribution $P_{X,Y,Z}$ and only observe the marginals $P_{Y,Z}$ and $P_{X,Z}$ . The only way we can infer about the joint distribution is by connecting these two marginals through $Z$ . So, readers can guess that it is more favorable when $Z$ is informative for either $X$ or $Y$ . For example, take the extreme case when $Y=h(Z)$ for a function $h:\\mathcal{V}\\to\\mathcal{Z}$ . In this case the $P_{X,Y,Z}=P_{X,g(Z),Z}$ is precisely known from the $P_{X,Z}$ and we can exactly pinpoint the $\\mathbb{E}[g(X,Y,Z)]$ as $\\mathbb{E}[g(X,h(Z),Z)]$ . In this case, $H(Y\\mid Z)=0$ , leading to $U-L=0$ , i.e., its Fr\u00e9chet bounds can precisely pinpoint it as well; ideally at least one of the $H(X\\mid Z)$ and $H(Y\\mid Z)$ is small. ", "page_idx": 9}, {"type": "text", "text": "6Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper is based upon work supported by the National Science Foundation (NSF) under grants no.   \n1916271,2027737, 2113373, and 2113364. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  T.C. Alberto and J.V. Lochter. YouTube Spam Collection. UCI Machine Learning Repository, 2015. D0I: https://doi.org/10.24432/C58885.   \n[2]  Tiago Almeida and Jos Hidalgo. SMS Spam Collction. UCI Machine Learning Repository, 2011. DOI: https://doi.org/10.24432/C5CC84.   \n[3] Dongsheng An, Na Lei, Xiaoyin Xu, and Xianfeng Gu. Effcient optimal transport algorithm by accelerated gradint descentIn Proceedings of the AAAlConference on ArtifcialIntelligence, volume 36, pages 10119-10128, 2022.   \n[4]  Chidubem Arachie and Bert Huang. Adversarial label learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3183-3190, 2019.   \n[5] Chidubem Arachie and Bert Huang. A general framework for adversarial label learning. J. Mach. Learn. Res., 22:118-1, 2021.   \n[6]  Azam Asl and Michael L Overton. Behavior of limited memory bfgs when applied to nonsmooth functions and their nesterov smoothings. In Numerical Analysis and Optimization: NAO-V, Muscat, Oman, January 2020 V, pages 25-55. Springer, 2021.   \n[7] Daniel Bartl, Michael Kupper, Thibaut Lux, Antonis Papapantoleon, and Stephan Eckstein. Marginal and dependence uncertainty: bounds, optimal transport, and sharpness. SIAM Journal on Control and Optimization, 60(1):410-434, 2022.   \n[8]  Mathias Beiglbock and Walter Schachermayer. Duality for borel measurable cost functions. Transactions of the American Mathematical Society, 363(8):4203-4224, 2011.   \n[9]  Ruidi Chen, Ioannis Ch Paschalidis, et al. Distributionally robust learning. Foundations and Trends? in Optimization, 4(1-2): 1-243, 2020.   \n[10]  Pier Luigi Conti, Daniela Marella, and Mauro Scanu. Statistical matching analysis for complex survey data with applications. Journal of the American Statistical Association, 111(516): 1715-1725, 2016.   \n[11]  Pier Luigi Conti, Daniela Marella, and Mauro Scanu. An overview on uncertainty and estimation in statistical matching. Analysis of Integrated Data, pages 73-100, 2019.   \n[12] David PA Corney, Dya Albakour, Miguel Martinez-Alvarez, and Samir Moussa. What do a million news articles look like? In NewsIR@ ECIR, pages 42-47, 2016.   \n[13]  Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAI conference on web and social media, volume 11, pages 512-515, 2017.   \n[14] Allan Peter Davis, Cynthia J Grondin, Robin J Johnson, Daniela Sciaky, Benjamin L King, Roy McMorran, Jolene Wiegers, Thomas C Wiegers, and Carolyn J Mattingly. The comparative toxicogenomics database: update 2017. Nucleic acids research, 45(D1):D972-D978, 2017.   \n[15]  Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and Montse Cuadros. Hate Speech Dataset from a White Supremacy Forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11-20, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5102. URL https: //www .aclweb.org/anthology/ W18-5102.   \n[16] Jacob Devlin. Bert: Pre-training of deep bidirectional ransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[17] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[18] Marcelo D'Orazio, Marco Di Zio, and Mauro Scanu. Statistical matching: Theory and practice. John Wiley & Sons, 2006.   \n[19] Marcello D'Orazio. Statistical learning in official statistics: the case of statistical matching. Statistical Journal of the IA0S, 35(3):435-441, 2019.   \n[20] Riccardo Fogliato, Alexandra Chouldechova, and Max G'Sell. Fairness evaluation in presence ofbiasednoisy labels In International conference on arifcial inteligence and statistics, pages 2325-2336. PMLR, 2020.   \n[21] Jason A Fries, Ethan Steinberg, Saelig Khattar, Scott L Fleming, Jose Posada, Alison Callahan, and Nigam H Shah. Ontology-driven weak supervision for clinical entity classification in electronic health records. Nature communications, 12(1):2017, 2021.   \n[22] Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher Re. Fast and three-rious: Speeding up weak supervision with triplet methods. In International conference on machine learning, pages 3280-3291. PMLR, 2020.   \n[23] Wenshuo Guo, Mingzhang Yin, Yixin Wang, and Michael Jordan. Partial identification with noisy covariates: A robust optimization approach. In Conference on Causal Learning and Reasoning, pages 318-335. PMLR, 2022.   \n[24] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Seaghdha, Sebastian Pad6, Marco Pennacchiott, Lorenza Romano, and Stan Szpakowicz. SemEval2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Katrin Erk and Carlo Strapparava, editors, Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33-38, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL https: //aclanthology org/S10-1006.   \n[25] Nathan Kallus. What's the harm? sharp bounds on the fraction negatively affected by treatment. Advances in Neural Information Processing Systems, 35:15996-16009, 2022.   \n[26]  Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[27] Ron Kohavi. . Census Income. UCI Machine Learning Repository, 1996.  DOI: https://doi.org/10.24432/C5GP7S.   \n[28] Petra Kralj Novak, Teresa Scantamburlo, Andraz Pelicon, Matteo Cinelli, Igor Mozeti, and Fabiana Zollo.Handing disagreement in hate speech modeling. In Intenational Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, pages 681-695. Springer, 2022.   \n[29] Martin Krallinger, Obdulia Rabal, Saber A Akhondi, Martin Perez Perez, Jesus Santamaria, Gael Perez Rodriguez, Georgios Tsatsaronis, Ander Intxaurrondo, Jose Antonio Lopez, Umesh Nandal, et al. Overview of the biocreative vi chemical-protein interaction track. In Proceedings of the sixth BioCreative challenge evaluation workshop, volume 1, pages 141-146, 2017.   \n[30] Israa Lewaa, Mai Sherif Hafez, and Mohamed Ali Ismail. Data integration using statistical matching techniques: A review. Statistical Journal of the IA0S, 37(4):1391-1410, 2021.   \n[31] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002.   \n[32] Pierre Lison, Aliaksandr Hubin, Jeremy Barnes, and Samia Touileb. Named entity recognition without labelled data: A weak supervision approach. arXiv preprint arXiv:2004.14723, 2020.   \n[33]  Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503-528, 1989.   \n[34] Yinhan Liu. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[35] Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Danil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko. ParaDetox: Detoxification with parallel data. In Proceedings of the 6Oth Annual Meeting of the Association for Computational Linguistics (Volme 1: Long Papers), pages 68046818,Dublin, Ireland, May 2022. Associatin for Computational Linguistics. URL https: //aclanthology . org/2022.acl-1ong.469. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[36] Steven Loria et al. textblob documentation. Release 0.15, 2(8):269, 2018. ", "page_idx": 12}, {"type": "text", "text": "[37] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies -Volume 1, HLT \\*11, page 142-150, USA, 2011. Association for Computational Linguistics. ISBN 9781932432879.   \n[38] Charles F Manski. Anatomy of the selection problem. Journal of Human resources, pages 343-360, 1989.   \n[39] Charles F Manski. Nonparametric bounds on treatment effects. The American Economic Review, 80(2):319-323, 1990.   \n[40] Charles F Manski. Partial identifcation of probability distributions, volume 5. Springer, 2003.   \n[41]  Alessio Mazzetto, Cyrus Cousins, Dylan Sam, Stephen H Bach, and Eli Upfal. Adversarial multi class learning under weak supervision with performance guarantees. In International Conference on Machine Learning, pages 7534-7543. PMLR, 2021.   \n[42] Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal, and Stephen Bach. Semi-supervised aggregation of dependent weak supervision sources with performance guarantees. In International Conference on Artificial Intelligence and Statistics, pages 3196-3204. PMLR, 2021.   \n[43] Francesca Molinari. Microeconometrics with partial identification. Handbook of econometrics, 7:355-486, 2020.   \n[44]  Wojciech Niemiro. Asymptotics for m-estimators defined by convex minimization. The Annals of Statistics, pages 1514-1533, 1992.   \n[45]  Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends@ in Machine Learning, 11(5-6):355-607, 2019.   \n[46] Flavien Prost, Pranjal Awasthi, Nick Blumm, Aditee Kumthekar, Trevor Potter, Li Wei, Xuezhi Wang, Ed H Chi, Jjiin Chen, and Alex Beutel. Measuring model fairness under noisy covariats: A theoretical perspective. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 873-883, 2021.   \n[47]  Alex Ratner, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Re. Snorkel metal: Weak supervision for multi-task learning. In Proceedings ofthe Second Workshop on Data Management for End-To-End Machine Learning, pages 1-4, 2018.   \n[48]  Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re. Snorkel: Rapid training data creation with weak supervision. In Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases, volume 11, page 269. NIH Public Access, 2017.   \n[49]  Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher Re. Training complex models with multi-task weak supervision. In Proceedings of the AAA1 Conference on Artificial Intelligence, volume 33, pages 4763-4771, 2019.   \n[50]  Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Re. Data programming: Creating large training sets, quickly. Advances in neural information processing systems, 29, 2016.   \n[51]  Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Association for Computational Linguistics, 2019.   \n[52] Salva Ruhling Cachay, Benedikt Boecking, and Artur Dubrawski. End-to-end weak supervision. Advances in Neural Information Processing Systems, 34:1845-1857, 2021.   \n[53] Ludger Ruischendorf. Bounds for distributions with multivariate marginals. Lecture NotesMonograph Series, pages 285-310, 1991.   \n[54]  Ludger Rischendorf.  Fr\u00e9chet-bounds and their applications.  In Advances in Probability Distributions with Given Marginals: beyond the copulas, pages 151-187. Springer, 1991.   \n[55]  Ludger Rischendorf.  Risk bounds and partial dependence information.  From Statistics to Mathematical Finance: Festschrift in Honour of Winfried Stute, pages 345-366, 2017.   \n[56]  Dylan Sam and J Zico Kolter. Losses over labels: Weakly supervised learning via direct loss construction. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 9695-9703,2023.   \n[57]  Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. Finer: Financial named entity recognition dataset and weak-supervision model. arXiv preprint arXiv:2302.11157, 2023.   \n[58] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic Sala. Universalizing weak supervision. arXiv preprint arXiv:2112.03865, 2021.   \n[59] Ryan Smith, Jason A Fries, Braden Hancock, and Stephen H Bach. Language models in the loop: Incorporating prompting into weak supervision. arXiv preprint arXiv:2205.02318, 2022.   \n[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[61] Paroma Varma, Bryan D He, Payal Bajaj, Nishith Khandwala, Imon Banerjee, Daniel Rubin, and Christopher Re. Inferring generative model structure with static analysis. Advances in neural information processing systems, 30, 2017.   \n[62] Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. Fine-tuning pre-trained language model with weak supervision: A contrastive-regularized self-training approach. arXiv preprint arXiv:2010.07835, 2020.   \n[63] Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. Wrench: A comprehensive benchmark for weak supervision. arXiv preprint arXiv:2109.11377, 2021.   \n[64] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision. arXiv preprint arXiv:2202.05433, 2022.   \n[65] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015.   \n[66] Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. Weaker than you think: A critical look atweakly supervised learning. arXiv preprint arXiv:2305.17442, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A  Connection to optimal transport ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The optimizations in the Frechet bounds (1.1) can be connected to an optimization problem [45]. We only explain this connection for the lower bound, but the connection to the upper bound is quite similar. The optimization lower bound is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{\\pi_{X,Z}=P_{X,Z}}\\mathbb{E}_{\\pi}[g(X,Y,Z)]=}\\\\ &{\\quad\\quad\\pi_{Y\\mid Z}\\!=\\!P_{Y\\mid Z}}\\\\ &{=\\operatorname*{inf}_{\\pi_{X,Z}=P_{X,Z}}\\sum_{z}\\mathbb{P}(Z=z)\\mathbb{E}_{\\pi}[g(X,Y,Z)\\mid Z=z]}\\\\ &{\\quad\\quad\\quad\\pi_{Y\\mid Z}\\!=\\!P_{Y\\mid Z}}\\\\ &{=\\sum_{z}\\mathbb{P}(Z=z)\\left\\{\\operatorname*{inf}_{\\pi_{X\\mid Z=z}=P_{X\\mid Z=z}}\\mathbb{E}_{\\pi_{X,Y\\mid Z=z}}[g(X,Y,z)]\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we notice that the inner minimization is an optimal transport problem between the probability distributions $P_{X\\mid Z=z}$ and $P_{Y|Z=z}$ with the cost function $d_{z}(x,y)=g(x,y,z)$ ", "page_idx": 14}, {"type": "text", "text": "B  A proof for the duality result ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 2.1. We start proving the result for $L$ .See that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L=\\underset{\\pi\\in\\Pi}{\\operatorname*{inf}}\\ \\mathbb{E}_{\\pi}[g(X,Y,Z)]}\\\\ &{\\quad=\\underset{\\{\\pi_{z}\\in\\Pi_{z}\\}_{z}\\in\\mathcal{Z}}{\\operatorname*{inf}}\\sum_{z\\in\\mathcal{Z}}\\mathbb{P}(Z=z)\\cdot\\mathbb{E}_{\\pi_{z}}[g(X,Y,Z)\\mid Z=z]}\\\\ &{\\quad=\\displaystyle\\sum_{z\\in\\mathcal{Z}}\\mathbb{P}(Z=z)\\cdot\\operatorname*{inf}_{\\pi_{z}\\in\\Pi_{z}}\\mathbb{E}_{\\pi_{z}}[g(X,Y,Z)\\mid Z=z]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $\\Pi_{z}$ , is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pi_{z}\\triangleq\\{\\pi_{z}\\in\\Delta(\\mathcal{X}\\times\\mathcal{Y}):\\pi_{z}\\circ\\rho_{X}^{-1}=P_{X|Z=z}\\mathrm{~and~}\\pi_{z}\\circ\\rho_{Y}^{-1}=P_{Y|Z=z}\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is, for each $z\\in{\\mathcal{Z}}$ \uff0c $\\Pi_{z}$ represents the set of couplings such that marginals are given by $P_{X\\mid Z=z}$ and $P_{Y\\mid Z=z}$ .We can represent the problem in this way since the marginal distribution of $Z$ is fixed and, given that distribution, $\\{\\Pi_{z}\\}$ specifies the same set of distributions as $\\Pi$ ", "page_idx": 15}, {"type": "text", "text": "Realize that we have broken down our initial maximization problem in $|\\mathcal{Z}|$ smaller minimization problems. Each of those minimization problems can be treated as an optimal transportation problem Consequently, by Beiglbock and Schachermayer [8, Theorem 1], for each $z\\in{\\mathcal{Z}}$ ,we get thefollowing dualityresult ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\pi_{z}\\in\\Pi_{z}}\\mathbb{E}_{\\pi_{z}}[g(X,Y,Z)\\mid Z=z]=\\operatorname*{sup}_{(\\beta_{z},\\alpha_{z})\\in\\Psi_{z}}\\mathbb{E}[\\beta_{z}(X)+\\alpha_{z}(Y)\\mid Z=z]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Psi_{z}\\triangleq\\left\\{(\\beta_{z},\\alpha_{z}):\\begin{array}{l}{\\beta_{z}:X\\to[-\\infty,\\infty),\\alpha_{z}:Y\\to[-\\infty,\\infty)}\\\\ {\\mathbb{E}[|\\beta_{z}(X)|\\mid Z=z]<\\infty,\\mathbb{E}[|\\alpha_{z}(Y)|\\mid Z=z]<\\infty}\\\\ {\\beta_{z}(x)+\\alpha_{z}(y)\\leq g(x,y,z)\\;\\mathrm{for\\;all}\\;(x,y)\\in X\\times Y}\\end{array}\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, partially optimizing on $\\beta_{z}(x)$ , we can set $\\begin{array}{r}{\\beta_{z}^{*}(x)=\\operatorname*{min}_{y\\in\\mathcal{y}}[g(x,y,z)-\\alpha_{z}(y)]}\\end{array}$ and then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{\\pi_{z}\\in\\Pi_{z}}\\mathbb{E}_{\\pi_{z}}[g(X,Y,Z)\\mid Z=z]=}\\\\ &{=\\operatorname*{sup}_{\\alpha_{z}}\\mathbb{E}\\left[\\operatorname*{min}_{y\\in\\mathcal{Y}}[g(X,y,Z)+\\alpha_{z}(y)]-\\alpha_{z}(Y)\\mid Z=z\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha_{z}$ is a simple function taking values in the real line. ", "page_idx": 15}, {"type": "text", "text": "Consequently, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L=\\displaystyle\\sum_{z\\in\\mathcal{Z}}\\mathbb{P}(Z=z)\\cdot\\operatorname*{sup}_{\\alpha_{z}}\\mathbb{E}\\left[\\operatorname*{min}_{y\\in\\mathcal{Y}}[g(X,y,Z)+\\alpha_{z}(y)]-\\alpha_{z}(Y)\\mid Z=z\\right]}\\\\ &{\\quad=\\displaystyle\\operatorname*{sup}_{\\{\\alpha_{z}\\}_{z\\in\\mathcal{Z}}}\\sum_{z\\in\\mathcal{Z}}\\mathbb{P}(Z=z)\\cdot\\mathbb{E}\\left[\\operatorname*{min}_{y\\in\\mathcal{Y}}[g(X,y,Z)+\\alpha_{z}(y)]-\\alpha_{z}(Y)\\mid Z=z\\right]}\\\\ &{\\quad=\\displaystyle\\operatorname*{sup}_{\\{\\alpha_{z}\\}_{z\\in\\mathcal{Z}}}\\mathbb{E}\\left[\\operatorname*{min}_{y\\in\\mathcal{Y}}[g(X,y,Z)+\\alpha_{Z}(y)]-\\alpha_{Z}(Y)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because each $\\alpha_{z}$ is a function assuming at most $|\\mathcal{V}|$ values and we have $|\\mathcal{Z}|$ functions (one for each value of $z$ ), we can equivalently solve an optimization problem on $\\mathbb{R}^{|\\mathcal{N}|\\times|\\mathcal{Z}|}$ . Adjusting the notation, ", "page_idx": 15}, {"type": "equation", "text": "$$\nL=\\operatorname*{sup}_{a\\in\\mathbb{R}^{|\\mathcal{Y}|\\times|\\mathcal{Z}|}}\\mathbb{E}\\Bigl[\\operatorname*{min}_{\\bar{y}\\in\\mathcal{Y}}\\left[g(X,\\bar{y},Z)+a_{\\bar{y}Z}\\right]\\Bigr]-\\mathbb{E}\\left[a_{Y Z}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From Beiglbock and Schachermayer [8, Theorem 2], we know that the maximum is attained by some $a^{*}\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{Z}|}$ . To show that there is a maximizer in $\\boldsymbol{\\mathcal{A}}$ we need to update the solution ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{\\cdot z}^{*}\\gets a_{\\cdot z}^{*}-\\sum_{y\\in\\mathcal{y}}a_{y z}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for every $z\\in{\\mathcal{Z}}$ . The objective function is not affected by such translations. ", "page_idx": 15}, {"type": "text", "text": "To prove the result for $U$ , first realize that because $g$ is bounded, with no loss of generality, we can assume its range is a subset of $[0,1]$ .Define $c(x,y,z)=1-g(x,y,z)$ and see that ", "page_idx": 15}, {"type": "equation", "text": "$$\nU=\\operatorname*{sup}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}[1-c(X,Y,Z)]=1-\\operatorname*{inf}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}[c(X,Y,Z)]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proceeding as before, we can obtain the final result by finding the dual formulation for $\\operatorname{inf}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}[c(X,Y,Z)]$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Proofs for the estimation results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We will analyze the estimator for $U$ (results for the estimator of $L$ can be obtained analogously). For the next results, we define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{f}_{u,\\varepsilon}(x,z,a)\\triangleq f_{u,\\varepsilon}(x,z,a)+\\sum_{z^{\\prime}\\in\\mathcal{Z}}\\left(\\sum_{y\\in\\mathcal{Y}}a_{y z^{\\prime}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 2.5. By Assumption 2.2 and Lemmas C.1 and C.2, we can_ guarantee that $\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a)]$ is minimized by a unique $a_{u,\\varepsilon}^{*}$ (equalling it to $U_{\\varepsilon}$ ) and that $\\nabla_{a}^{2}\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}^{-}(X,Z,a_{u,\\varepsilon}^{*})]$ is positive definite. Also, from the proof of Lemma C.2, we can see that $\\tilde{f}_{u,\\varepsilon}$ is convex in $a$ (because its Hessian is positive semidefinite). It is also true that the second moment of $\\tilde{f}_{u,\\varepsilon}(X,Z,a)$ is well defined (exists and finite) for each $a$ since $g$ is bounded. Define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{U}_{\\varepsilon}\\triangleq\\underset{a\\in\\mathbb{R}^{|\\mathcal{y}|}\\times\\mathinner{|{\\mathcal{z}}|}}{\\operatorname*{inf}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and let $\\tilde{a}_{\\varepsilon}$ denote a value that attains that minimum; from Niemiro [44, Theorem 4] and the conditions discussed above, we know that $\\sqrt{n}(\\tilde{a}_{\\varepsilon}-a_{u,\\varepsilon}^{*})=\\mathcal{O}_{P}(1)$ .The existence of $\\tilde{a}_{\\varepsilon}$ is discussed by Niemiro [44]. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}(\\Tilde{U}_{\\varepsilon}-U_{\\varepsilon})}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{n}}\\left(\\sum_{i}\\Tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},\\Tilde{a}_{\\varepsilon})-\\sum_{i}\\Tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},a_{u,\\varepsilon}^{*})\\right)+\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i}\\Tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},a_{u,\\varepsilon}^{*})-U_{\\varepsilon}\\right)}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{n}}\\left([\\sqrt{n}(\\Tilde{a}_{\\varepsilon}-a_{u,\\varepsilon}^{*})]^{\\top}[\\frac{1}{n}\\sum_{i}\\nabla_{a}^{2}\\Tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},\\Tilde{a})][\\sqrt{n}(\\Tilde{a}_{\\varepsilon}-a_{u,\\varepsilon}^{*})]\\right)+}\\\\ &{\\quad+\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i}\\Tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},a_{u,\\varepsilon}^{*})-U_{\\varepsilon}\\right)+o_{P}(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first term is obtained by a second-order Taylor expansion of the summing functions around $\\tilde{a}_{\\varepsilon}$ $\\mathit{\\Pi}\\overline{{a}}$ is some random vector). Also, from the standard central limit theorem, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i}\\tilde{f}_{u,\\varepsilon}(X_{i},Z_{i},a_{u,\\varepsilon}^{*})-U_{\\varepsilon}\\right)\\Rightarrow N(0,\\mathrm{Var}\\tilde{f}_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given that $\\sqrt{n}(\\tilde{a}_{\\varepsilon}-a_{u,\\varepsilon}^{*})=\\mathcal{O}_{P}(1)$ and that the Hessian has bounded entries (then $\\mathcal{O}_{P}(1)$ as well), the first term in C.1 is $o_{P}(1)$ . Because $a_{u,\\varepsilon}^{*}\\in A$ , we have that $f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})=\\tilde{f}_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})$ and then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\tilde{U}_{\\varepsilon}-U_{\\varepsilon})\\Rightarrow N(0,\\mathrm{Var}f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*}))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Slutsky's theorem. Since ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{U}_{\\varepsilon}-U)=\\sqrt{n}(\\hat{U}_{\\varepsilon}-\\tilde{U}_{\\varepsilon})+\\sqrt{n}(\\tilde{U}_{\\varepsilon}-U_{\\varepsilon}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if we can show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\tilde{U}_{\\varepsilon}-\\hat{U}_{\\varepsilon})=o_{P}(1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we are done. ", "page_idx": 16}, {"type": "text", "text": "Let $\\hat{a}$ be solution for the problem in 2.3 and see that ", "page_idx": 17}, {"type": "text", "text": "[U-Uel =   \n$\\begin{array}{r l}&{\\quad=\\left|\\frac{1}{2}\\sum_{i=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})-\\frac{1}{2}\\sum_{i=1}^{n}f_{i}\\left[\\frac{1}{\\beta}\\sum_{j\\in\\mathcal{N}_{j}}\\exp\\left(\\frac{(X_{i},Z_{i},\\hat{z}_{i})-\\beta(z_{i})}{z}\\right)\\right]+\\mathbb{E}_{\\hat{P}_{D}\\cap\\mathcal{P}_{D}}[\\log\\chi_{Z}]\\right.}\\\\ &{\\quad\\le\\left|\\frac{1}{2}\\sum_{i=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})-\\frac{1}{2}\\sum_{i=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})\\right|+}\\\\ &{\\quad\\quad+\\left|\\frac{1}{2}\\sum_{i=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})-\\frac{1}{2}\\sum_{j=1}^{n}f_{i}\\left[\\frac{1}{\\beta}\\sum_{j\\in\\mathcal{N}_{j}}\\exp\\left(\\frac{(X_{i},Z_{j},\\hat{z}_{i})-\\beta(z_{i})}{z}\\right)\\right]+\\mathbb{E}_{\\hat{P}_{D}\\cap\\mathcal{P}_{D}}[\\log\\chi_{Z}]\\right.}\\\\ &{\\quad=\\left(\\frac{1}{2}\\sum_{i=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})-\\frac{1}{2}\\sum_{k=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})\\right)}\\\\ &{\\quad\\quad+\\left|\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\mathbb{E}[\\nabla^{\\prime}\\left(\\nabla^{\\prime}\\left(Z_{i},Z_{j}\\right)-\\hat{\\nabla}^{\\prime}\\left(\\nabla^{\\prime}\\left(Z_{i},Z_{i}\\right)\\right)\\right)]+\\mathbb{E}_{\\hat{P}_{D}\\cap\\mathcal{P}_{D}}\\left[\\log\\frac{1}{2}\\right]\\right|}\\\\ &{\\le\\left(\\frac{1}{2}\\sum_{i=1}^{n}f_{i}(X_{i},Z_{i},\\hat{z}_{i})-\\frac{1}{2}\\sum_{i=1}^{n}f_{i}\\left[\\frac{1 $   \n\u2264 2 llall\u2211y|P(Y =y| Z=2) -(Y =y| Z = z)|+ + llaell \u2211\u2211yIP(Y =y| Z =z) -P(Y =y| Z=z)|   \n\u22644lll = dv (Pr|z=z, Pr|z==) + 2llaell0 =dv (Pr|z=z, Pr|z=)   \n= Op(m-x) ", "page_idx": 17}, {"type": "text", "text": "where the last equality is obtained using Assumptions 2.3 and 2.4, and the fact that $\\|\\tilde{a}_{\\varepsilon}\\|_{\\infty}$ is tight (derived from $\\sqrt{n}\\big(\\tilde{a}_{\\varepsilon}-a_{u,\\varepsilon}^{*}\\big)=\\mathcal{O}_{P}(1))$ . Consequently, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\tilde{U}_{\\varepsilon}-\\hat{U}_{\\varepsilon})=\\sqrt{n}\\mathcal{O}_{P}(m^{-\\lambda})=o(m^{\\lambda})\\mathcal{O}_{P}(m^{-\\lambda})=o_{P}(1)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, using Slutsky's theorem, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{U}_{\\varepsilon}-U)=\\sqrt{n}(\\hat{U}_{\\varepsilon}-\\tilde{U}_{\\varepsilon})+\\sqrt{n}(\\tilde{U}_{\\varepsilon}-U)\\Rightarrow N(0,\\mathrm{Var}\\,f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Corollary 3.1. We prove the asymptotic distribution of $\\sqrt{n}\\big(\\hat{p}_{u,\\varepsilon}-p_{u,\\varepsilon}\\big)$ . The result for the lower bound can be obtained analogously. ", "page_idx": 17}, {"type": "text", "text": "First, note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbb{P}}(h(X)=1)-\\mathbb{P}(h(X)=1)=\\mathcal{O}_{P}(m^{-1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by the standard central limit theorem. ", "page_idx": 17}, {"type": "text", "text": "Next, see that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\rho}\\left(\\hat{\\rho}_{u,c}-p_{u,c}\\right)=}\\\\ &{=\\sqrt{n}\\left(\\frac{\\hat{U}_{c}}{\\hat{V}(\\Lambda(\\Lambda)=1)}-\\frac{U_{c}}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\sqrt{n}\\left(\\hat{U}_{c}-\\frac{\\hat{P}(\\Lambda(\\Lambda)=1)}{\\hat{V}(\\Lambda(\\Lambda)=1)}U_{c}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\sqrt{n}\\left(\\hat{U}_{c}-U_{c}\\right)+\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\sqrt{n}\\left(U_{c}-\\frac{\\hat{P}(h(\\Lambda)=1)}{\\hat{V}(\\Lambda(\\Lambda)=1)}U_{c}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\sqrt{n}\\left(\\hat{U}_{c}-U_{c}\\right)+U_{c}\\sqrt{n}\\left(\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}-\\frac{1}{\\mathbb{F}(h(\\Lambda)=1)}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\sqrt{n}\\left(\\hat{U}_{c}-U_{c}\\right)+U_{c}\\sqrt{n}\\left(\\hat{\\mathbb{P}}(h(\\Lambda)=1)-\\mathbb{F}(h(\\Lambda)=1)\\right)\\left(\\frac{-1}{\\hat{V}(\\Lambda(\\Lambda)=1)^{2}}\\right)+o_{P}\\left(m^{-1/2}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(\\Lambda(\\Lambda)=1)}\\sqrt{n}\\left(\\hat{U}_{c}-U_{c}\\right)+o_{P}\\left(1\\right)}\\\\ &{\\Rightarrow N(0,\\sigma_{m,u}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the (i) fifth and sixth lines equality is obtained using Taylor's theorem, (ii) sixth and seventh lines equality is obtained using observation C.2 and the fact that $n=o(m^{(2\\lambda)\\wedge1})$ , and (i) seventh to eighth lines equality is obtained using observation C.2, Theorem 2.5, and Slutsky's theorem. ", "page_idx": 18}, {"type": "text", "text": "We prove the asymptotic distribution of $\\sqrt{n}\\big(\\hat{r}_{u,\\varepsilon}-r_{u,\\varepsilon}\\big)$ . The result for the lower bound can be obtained analogously. From Lemma C.3, we know that there is an estimator $\\hat{\\mathbb{P}}(Y=1)$ Suchthat $\\hat{\\mathbb{P}}(Y=1)-\\mathbb{P}(Y=1)=\\mathcal{O}_{P}(m^{-(\\lambda\\wedge1/2)})$ , i.e., it has enough precision. We use that estimator. ", "page_idx": 18}, {"type": "text", "text": "Next, see that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\left(\\hat{\\psi}_{u,\\varepsilon}-r_{u,\\varepsilon}\\right)=}\\\\ &{=\\sqrt{n}\\left(\\frac{\\hat{U}_{\\varepsilon}}{\\hat{V}(Y=1)}-\\frac{U_{\\varepsilon}}{\\hat{V}(Y=1)}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(Y=1)}\\sqrt{n}\\left(\\hat{U}_{\\varepsilon}-\\frac{\\hat{P}(Y=1)}{\\hat{V}(Y=1)}U_{\\varepsilon}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(Y=1)}\\sqrt{n}\\left(\\hat{U}_{\\varepsilon}-U_{\\varepsilon}\\right)+\\frac{1}{\\hat{V}(Y=1)}\\sqrt{n}\\left(U_{\\varepsilon}-\\frac{\\hat{P}(Y=1)}{\\hat{V}(Y=1)}U_{\\varepsilon}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(Y=1)}\\sqrt{n}\\left(\\hat{U}_{\\varepsilon}-U_{\\varepsilon}\\right)+U_{\\varepsilon}\\sqrt{n}\\left(\\frac{1}{\\hat{V}(Y=1)}-\\frac{1}{\\hat{V}(Y=1)}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(Y=1)}\\sqrt{n}\\left(\\hat{U}_{\\varepsilon}-U_{\\varepsilon}\\right)+U_{\\varepsilon}\\sqrt{n}\\left(\\hat{\\mathfrak{P}}(Y=1)-\\mathbb{P}(Y=1)\\right)\\left(\\frac{-1}{\\hat{V}(Y=1)^{2}}\\right)+o_{P}\\left(m^{-1/2}\\right)}\\\\ &{=\\frac{1}{\\hat{V}(Y=1)}\\sqrt{n}\\left(\\hat{U}_{\\varepsilon}-U_{\\varepsilon}\\right)+o_{P}\\left(1\\right)}\\\\ &{\\Rightarrow N(0,\\sigma_{W,u}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we prove the asymptotic distribution of $\\sqrt{n}\\big(\\hat{F}_{u,\\varepsilon}-F_{u,\\varepsilon}\\big)$ . The result for the lower bound can be obtained analogously. From the facts stated above, we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbb{P}}(h(X)=1)+\\hat{\\mathbb{P}}(Y=1)-[\\mathbb{P}(h(X)=1)+\\mathbb{P}(Y=1)]=}\\\\ &{\\;=[\\hat{\\mathbb{P}}(h(X)=1)-\\mathbb{P}(h(X)=1)]+[\\hat{\\mathbb{P}}(Y=1)-\\mathbb{P}(Y=1)]=}\\\\ &{\\;=\\mathcal{O}_{P}(m^{-(\\lambda\\wedge1/2)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\ln}\\left(\\widehat{F}_{\\bar{t}(M),\\varepsilon}-F_{u,\\varepsilon}\\right)=}\\\\ &{=\\sqrt{\\frac{2\\mathcal{G}_{\\varepsilon}}{\\left(\\|\\bar{F}(M)-1\\right)+\\frac{2\\mathcal{G}(\\gamma-1)}{1+\\theta(\\gamma-1)}-\\frac{2\\mathcal{G}_{\\varepsilon}}{\\|\\mathcal{F}(M)-1\\right)+\\mathcal{F}(Y=1)}}}\\right)}\\\\ &{=\\frac{2}{\\|\\mathcal{F}(\\bar{t}(M)-1)+\\frac{2}{\\theta(\\gamma-1)}\\|\\sqrt{n}}\\left(\\widehat{F}_{\\varepsilon}-\\frac{\\left\\|\\bar{F}(M(X)=1)+\\frac{\\gamma}{\\theta(Y=1)}\\right\\|}{\\|\\mathcal{F}(M(X)=1)+\\mathcal{F}(Y=1)\\|}F_{\\varepsilon}\\right)}\\\\ &{=\\frac{2}{\\|\\mathcal{F}(M(X)=1)-1+\\widetilde{F}(Y=1)\\|}\\sqrt{n}\\left(\\widehat{F}_{\\varepsilon}-U_{\\varepsilon}\\right)+\\frac{2}{\\|\\mathcal{F}(M(X)=1)-1+\\widetilde{F}(Y=1)\\|}\\sqrt{n}\\left(2U_{\\varepsilon}-\\frac{\\|\\bar{F}(M(X)=1)+\\bar{F}(Y=1)\\|}{\\|\\mathcal{F}(M(X)=1)+\\mathbb{F}(Y=1)+\\mathbb{I}(Y=1)}\\right)}\\\\ &{=\\frac{2}{\\|\\mathcal{F}(M(X)=1)-1+\\widetilde{F}(Y=1)\\|}\\sqrt{n}\\left(\\widehat{F}_{\\varepsilon}-U_{\\varepsilon}\\right)+2U_{\\varepsilon}\\sqrt{n}\\left(\\frac{1}{\\|\\bar{F}(M(X)=1)+\\bar{F}(Y=1)\\|}-\\frac{1}{\\|\\bar{F}(M(X)=1)+\\bar{F}(Y=1)\\|}\\right)}\\\\ &{=\\frac{2}{\\|\\mathcal{F}(M(X)=1)+\\bar{F}(Y=1)\\|}\\sqrt{n}\\left(\\widehat{F}_{\\varepsilon}-U_{\\varepsilon}\\right)+}\\\\ &{\\quad+2U_{\\varepsilon}\\sqrt{n}\\left(\\frac{\\|\\bar{F}(M(X)=1)+\\bar{F}(Y=1)\\|}{\\|\\mathcal{F}(M(X)=1)+\\bar{F}(Y=1)\\|-\\left\\|\\mathbb{F}(M(X)=1)+\\mathbb{F}(Y=1)\\right\\|}\\right)\\left(\\frac{-1}{\\|\\mathcal{F}(M(X)=1)+\\bar{F}(Y=1)\\|^{2} \n$$\u2265 N(0,02,u,e) ", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where all the steps are justified as before. ", "page_idx": 19}, {"type": "text", "text": "C.1 Auxiliary lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma C.1. Define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{f}_{u,\\varepsilon}(x,z,a)\\triangleq f_{u,\\varepsilon}(x,z,a)+\\sum_{z^{\\prime}\\in\\mathcal{Z}}\\left(\\sum_{y\\in\\mathcal{Y}}a_{y z^{\\prime}}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{a\\in\\mathbb{R}^{|\\mathcal{Y}|\\times|\\mathcal{Z}|}}\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a)]=\\operatorname*{inf}_{a\\in\\mathcal{A}}\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. First, see that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a)]\\ge\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Assumption 2.2, we know that there exists some $a_{u,\\varepsilon}^{*}\\in\\mathbb{R}^{|\\mathcal{D}|\\times|\\mathcal{Z}|}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{a\\in\\mathcal{A}}\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)]=\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For that specific $a_{u,\\varepsilon}^{*}$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})]=\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{a\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{Z}|}}\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a)]=\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})]=\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a_{u,\\varepsilon}^{*})]=\\operatorname*{inf}_{a\\in\\mathcal{A}}\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma C.2. The function of a given by $\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a)]$ has positive definite Hessian, i.e. ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{\\varepsilon}(a)=\\nabla_{a}^{2}\\mathbb{E}[\\tilde{f}_{u,\\varepsilon}(X,Z,a)]\\succ0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and, consequently, it is strictly convex. ", "page_idx": 19}, {"type": "text", "text": "Proof. See that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\varepsilon}(a)=\\nabla_{a}^{2}\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)]+\\nabla_{a}^{2}\\left[\\sum_{z^{\\prime}\\in\\mathcal{Z}}\\left(\\sum_{y\\in\\mathcal{Y}}a_{y z^{\\prime}}\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We start computing the first term in the sum. ", "page_idx": 20}, {"type": "text", "text": "First, for an arbitrary pair $(k,l)\\in\\mathcal{V}\\times\\mathcal{Z}$ , define ", "page_idx": 20}, {"type": "equation", "text": "$$\ns_{k l}(x)\\triangleq\\frac{\\exp\\left(\\frac{g(x,k,l)+a_{k l}}{\\varepsilon}\\right)}{\\sum_{y}\\exp\\left(\\frac{g\\left(x,y,l\\right)+a_{y l}}{\\varepsilon}\\right)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, see that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial}{\\partial a_{k l}}f_{u,\\varepsilon}(x,z,a)=\\mathbb{1}_{\\{l\\}}(z)\\Bigg[s_{k l}(x)-\\mathbb{P}(Y=k\\mid Z=l)\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial^{2}}{\\partial a_{p l}\\partial a_{k l}}f_{u,\\varepsilon}(x,z,a)=\\frac{1}{\\varepsilon}\\mathbb{1}_{\\{l\\}}(z)\\Bigg[\\mathbb{1}_{\\{k\\}}(p)s_{k l}(x)-s_{k l}(x)s_{p l}(x)\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Se that $\\begin{array}{r}{\\frac{\\partial^{2}}{\\partial a_{p b}\\partial a_{k l}}f_{u,\\varepsilon}(x,z,a)\\,=\\,0}\\end{array}$ $b\\neq l$ Consequently the Hessan $\\nabla_{a}^{2}\\ f_{u,\\varepsilon}(x,z,a)$ is bock diagonal. ", "page_idx": 20}, {"type": "text", "text": "Consequently, because the second derivatives are bounded, we can push them inside the expectations and get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial^{2}}{\\partial a_{p l}\\partial a_{k l}}\\mathbb{E}\\left[f_{u,\\varepsilon}(X,Z,a)\\right]=}\\\\ &{=\\mathbb{E}\\left[\\frac{\\partial^{2}}{\\partial a_{p l}\\partial a_{k l}}f_{u,\\varepsilon}(X,Z,a)\\right]}\\\\ &{=\\frac{1}{\\varepsilon}\\mathbb{E}\\left[\\mathbb{1}_{\\left\\{l\\right\\}}(Z)\\Big[\\mathbb{1}_{\\left\\{k\\right\\}}(p)s_{k l}(X)-s_{k l}(X)s_{p l}(X)\\Big]\\right]}\\\\ &{=\\frac{1}{\\varepsilon}\\mathbb{1}_{\\left\\{k\\right\\}}(p)\\cdot\\mathbb{E}\\left[\\mathbb{P}(Z=l\\mid X)s_{k l}(X)\\right]-\\frac{1}{\\varepsilon}\\mathbb{E}\\left[\\mathbb{P}(Z=l\\mid X)s_{k l}(X)s_{p l}(X)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because $\\nabla_{a}^{2}\\ f_{u,\\varepsilon}(x,z,a)$ is block diagonal, we know that the Hessian ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{a}^{2}\\operatorname{\\mathbb{E}}\\left[f_{u,\\varepsilon}(X,Z,a)\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is block diagonal (one block for each segment $a._{z}$ of the vector $a$ ). Now, realize that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{a}^{2}\\left[\\sum_{z^{\\prime}\\in\\mathcal{Z}}\\left(\\sum_{y\\in\\mathcal{Y}}a_{y z^{\\prime}}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is also block diagonal, with each block being matrices of ones. In this case, we also have one block for each segment $a._{z}$ of the vector $a$ . Consequently, $H_{\\varepsilon}(a)$ is block diagonal, and it is positive definite if and only if all of its blocks are positive definite. Let us analyse an arbitrary block of $H_{\\varepsilon}(a)$ , e.g., $\\nabla_{a.\\,l}^{2}\\textrm{\\ensuremath{\\mathbb{E}}}\\biggl[\\tilde{f}_{u,\\varepsilon}(X,Z,a)\\biggr].$ Let ${\\bf s}_{\\cdot l}(x)$ be the vector composed of $s_{k l}(x)$ for all $k$ If $\\mathbb{1}\\in\\mathbb{R}^{|\\mathcal{D}|}$ denotes a vector of ones, then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{a,\\,\\ell}^{2}\\left[\\tilde{f}_{\\boldsymbol{u},\\varepsilon}(X,Z,a)\\right]=}\\\\ &{=\\frac{1}{\\varepsilon}\\mathtt{d i a g}\\Big(\\mathbb{E}\\left[\\mathbb{P}(Z=l\\mid X)\\mathbf{s}.\\iota(X)\\right]\\Big)-\\frac{1}{\\varepsilon}\\mathbb{E}\\left[\\mathbb{P}(Z=l\\mid X)\\mathbf{s}.\\iota(X)\\mathbf{s}.\\iota(X)^{\\top}\\right]+\\mathbb{1}\\mathbb{1}^{\\top}}\\\\ &{=\\mathbb{E}\\Bigg\\{\\frac{1}{\\varepsilon}\\mathbb{P}(Z=l\\mid X)\\Big[\\mathtt{d i a g}\\big(\\mathbf{s}.\\iota(X)\\big)-\\mathbf{s}.\\iota(X)\\mathbf{s}.\\iota(X)^{\\top}+\\frac{\\varepsilon}{\\mathbb{P}(Z=l)}\\mathbb{1}\\mathbb{1}^{\\top}\\Big]\\Bigg\\}}\\\\ &{=\\mathbb{E}\\Bigg\\{\\frac{1}{\\varepsilon}\\mathbb{P}(Z=l\\mid X)\\Big[\\mathtt{d i a g}\\big(\\mathbf{s}.\\iota(X)\\big)-\\mathbf{s}.\\iota(X)\\mathbf{s}.\\iota(X)^{\\top}+\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}\\Big]\\Bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\tilde{\\mathbb{I}}=\\sqrt{\\frac{\\varepsilon}{\\mathbb{P}(Z{=}l)}}\\mathbb{1}$ . See that $\\mathrm{diag}\\big(\\mathbf{s}._{l}(x)\\big)$ has rank $|\\mathcal{V}|$ (full rank) while s ${\\mathbf{\\nabla}}_{l}(x){\\mathbf{s}}_{\\cdot l}(x)^{\\top}$ is rank one for every $x\\in\\mathbb{R}^{d_{X}}$ . Consequently, the rank of the difference ", "page_idx": 20}, {"type": "equation", "text": "$$\nD(x)=\\mathrm{diag}\\big(\\mathbf{s}._{l}(x)\\big)-\\mathbf{s}._{l}(x)\\mathbf{s}._{l}(x)^{\\top}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is greater or equal $|\\mathcal{V}|-1$ . It is the case that ranl $\\boldsymbol{\\mathfrak{x}}(D(\\boldsymbol{x}))=|\\mathcal{D}|-1$ because $\\tilde{\\mathbb{1}}$ is in the null space of $D$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D(x)\\tilde{\\mathbb{I}}=\\left[\\mathrm{diag}\\big(\\mathbf{s}.\\iota(x)\\big)-\\mathbf{s}.\\iota(x)\\mathbf{s}.\\iota(x)^{\\top}\\right]\\tilde{\\mathbb{I}}=\\sqrt{\\frac{\\varepsilon}{\\mathbb{P}(Z=l)}}\\big(\\mathbf{s}.\\iota(x)-\\mathbf{s}.\\iota(x)\\big)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, the range of $D(x)$ and $\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}$ are orthogonal. For any two vectors $\\mathbf{v},\\mathbf{u}\\in\\mathbb{R}^{|\\mathcal{D}|}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(D(x)\\mathbf{v})^{\\top}(\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}\\mathbf{u})=\\mathbf{v}^{\\top}D(x)^{\\top}\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}\\mathbf{u}=\\mathbf{v}^{\\top}D(x)\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}\\mathbf{u}=\\mathbf{v}^{\\top}0\\tilde{\\mathbb{1}}^{\\top}\\mathbf{u}=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "That implies $D(x)+\\tilde{\\mathbf{l}}\\tilde{\\mathbf{l}}^{\\top}$ is full rank. To see that, let $\\mathbf{v}\\in\\mathbb{R}^{|\\mathcal{V}|}$ be arbitrary and see ", "page_idx": 21}, {"type": "equation", "text": "$$\n(D(x)+\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top})\\mathbf{v}=0\\Rightarrow D(x)\\mathbf{v}=\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}\\mathbf{v}=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Because $D(x)\\mathbf{v}=0$ , it means that $\\mathbf{v}=\\theta\\tilde{\\mathbb{1}}$ for some constant $\\theta$ If $\\theta\\neq0$ , then $\\tilde{\\mathbb{I}}\\tilde{\\mathbb{I}}^{\\top}\\mathbf{v}=\\theta|\\mathcal{V}|\\tilde{\\mathbb{I}}\\neq0$ Therefore, $\\theta=0$ and $\\mathbf{v}=0$ ", "page_idx": 21}, {"type": "text", "text": "Now, let $\\mathbf{u}\\in\\mathbb{R}^{|\\mathcal{V}|}$ be arbitrary non-null vector and see ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{u}^{\\top}D(x)\\mathbf{u}=\\mathbf{u}^{\\top}\\mathrm{diag}\\big(\\mathbf{s}.\\iota(x)\\big)\\mathbf{u}-\\mathbf{u}^{\\top}\\mathbf{s}.\\iota(x)\\mathbf{s}.\\iota(x)^{\\top}\\mathbf{u}}\\\\ &{\\qquad\\qquad=\\sum_{y}u_{y}^{2}s_{y l}(x)-\\left(\\sum_{y}u_{y}s_{y l}(x)\\right)^{2}}\\\\ &{\\qquad\\quad=\\left(\\sum_{y}s_{y l}(x)\\right)\\left(\\sum_{y}u_{y}^{2}s_{y l}(x)\\right)-\\left(\\sum_{y}u_{y}s_{y l}(x)\\right)^{2}}\\\\ &{\\qquad\\quad=\\left(\\sum_{y}\\sqrt{s_{y l}(x)}\\sqrt{s_{y l}(x)}\\right)\\left(\\sum_{y}\\sqrt{u_{y}^{2}s_{y l}(x)}\\sqrt{u_{y}^{2}s_{y l}(x)}\\right)-\\left(\\sum_{y}u_{y}s_{y l}(x)\\right)^{2}}\\\\ &{\\qquad\\quad\\geq\\left(\\sum_{y}u_{y}s_{y l}(x)\\right)^{2}-\\left(\\sum_{y}u_{y}s_{y l}(x)\\right)^{2}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by the Cauchy-Schwarz inequality. Then, $D(x)$ is positive semidefinite, and because $\\tilde{\\mathbb{1}}\\tilde{\\mathbb{1}}^{\\top}$ is also positive semidefinite, their sum needs to be positive definite for all $x$ (that matrix is full rank). Each block of $H_{\\varepsilon}(a)$ is positive definite; consequently, $H_{\\varepsilon}(a)$ is positive definite. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma C.3. Assume Assumption 2.4 holds. Let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbb{P}}(Y=1)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\hat{P}_{Y\\mid Z}}[Y\\mid Z=Z_{i}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{P}}(Y=1)-\\mathbb{P}(Y=1)=\\mathcal{O}_{P}(m^{-(\\lambda\\wedge1/2)})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. To derive this result, see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbb{P}}(Y=1)-\\mathbb{P}(Y=1)|=}\\\\ &{=\\Big|\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{P\\backslash Z}\\big[Y\\mid Z=Z_{i}\\big]-\\mathbb{P}(Y=1)+\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{P\\backslash Z}\\big[Y\\mid Z=Z_{i}\\big]-\\mathbb{E}_{P\\backslash Z}\\big[Y\\mid Z=Z_{i}\\big]}\\\\ &{\\leq\\Big|\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}_{P\\backslash Z}\\big[Y\\mid Z=Z_{i}\\big]-\\mathbb{P}(Y=1)\\Big|+\\frac{1}{m}\\sum_{i=1}^{m}\\Big|\\mathbb{E}_{P\\backslash Z}\\big[Y\\mid Z=Z_{i}\\big]-\\mathbb{E}_{P\\backslash Z}\\big[Y\\mid Z=Z_{i}\\big]}\\\\ &{=O_{P}(m^{-1/2})+\\frac{1}{m}\\sum_{i=1}^{m}\\Big|\\mathbb{P}_{P\\backslash Z}\\big[Y=1\\mid Z=Z_{i}\\big]-\\mathbb{P}_{P\\backslash Z}\\big[Y=1\\mid Z=Z_{i}\\big]\\Big|}\\\\ &{\\leq O_{P}(m^{-1/2})+\\sum_{\\tau\\leq Z}\\Big|\\mathbb{P}_{P\\backslash Z}\\big[Y=1\\mid Z=z\\big]-\\mathbb{P}_{P\\backslash Z}\\big[Y=1\\mid Z=z\\big]\\Big|}\\\\ &{\\leq O_{P}(m^{-1/2})+\\sum_{\\tau\\in\\{0,1\\}}\\sum_{\\tau\\in\\mathcal{Z}}\\Big|\\mathbb{P}_{\\hat{Y}_{1}[Z=y]}\\big[Y=y\\mid Z=z\\big]-\\mathbb{P}_{P\\backslash Z}\\big[Z=z\\big]\\Big|}\\\\ &{=O_{P}(m^{-1/2})+2\\sum_{\\tau\\leq Z}d_{\\tau}\\Big(\\hat{T}_{b\\mid Z=z},P_{Y\\mid Z=z}\\Big)}\\\\ &{=O_{P}(m^{-1/2})+O_{P}(m^{-3})}\\\\ &{=O_{P}(m^{-(\\lambda+1/2)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the standard central limit theorem obtains the third step, the sixth step is obtained by the formula of the total variation distance for discrete measures, and the seventh step is obtained by Assumption 2.4. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma C.4. For some $\\kappa>0$ and any $y\\in\\mathcal{V}$ assume that $\\kappa\\le p_{y}\\le1-\\kappa$ Define $A=\\{a_{y}\\in\\mathbb{R}$ $\\textstyle\\sum_{y}a_{y}=0\\}$ .Then the optima of the following problems ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{a\\in A}f_{u}(a),\\ f_{u}(a)\\triangleq{\\ensuremath{\\mathbb E}}\\Big[\\epsilon\\log\\Big\\{\\frac{1}{|\\mathcal{V}|}\\sum_{y}\\exp\\bigl(\\frac{g(X,y)+a_{y}}{\\epsilon}\\bigr)\\Big\\}\\Big]-\\sum_{y}p_{y}a_{y}\\,,}\\\\ &{\\operatorname*{sup}_{a\\in A}f_{l}(a),\\ \\ f_{l}(a)\\triangleq{\\ensuremath{\\mathbb E}}\\Big[-\\epsilon\\log\\Big\\{\\frac{1}{|\\mathcal{V}|}\\sum_{y}\\exp\\bigl(\\frac{g(X,y)+a_{y}}{-\\epsilon}\\bigr)\\Big\\}\\Big]-\\sum_{y}p_{y}a_{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "are attained in a compact set $K(\\kappa,L)\\subset\\mathcal{A}$ where $\\|g\\|_{\\infty}\\leq L$ ", "page_idx": 22}, {"type": "text", "text": "Proof of lemma C.4. We shall only prove this for the minimization problem. The conclusion for the maximization problem follows in a similar way. ", "page_idx": 22}, {"type": "text", "text": "Strict convexity:  The second derivation of $f_{u}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{a_{y},a_{y^{\\prime}}}f_{u}(a)=\\frac{1}{\\epsilon}\\mathbb{E}\\bigl[p(X,y,a)\\{\\delta_{y,y^{\\prime}}-p(X,y^{\\prime},a)\\}\\bigr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{p(x,y,a)\\triangleq\\frac{\\exp\\left(\\frac{g(X,y)+a_{y}}{\\epsilon}\\right)}{\\sum_{i\\in\\mathcal{Y}}\\exp\\left(\\frac{g(X,i)+a_{i}}{\\epsilon}\\right)}}\\end{array}$ For any $u\\in\\mathbb{R}^{\\mathcal{D}}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{\\top}\\nabla^{2}f_{u}(a)u=\\sum_{i,j\\in\\mathcal{Y}}u_{i}u_{j}\\nabla_{a_{i},a_{j}}f_{u}(a)}\\\\ &{\\phantom{u^{\\top}\\nabla^{2}f_{u}(a)u}=\\frac{1}{\\epsilon}\\sum_{i,j\\in\\mathcal{Y}}u_{i}u_{j}\\mathbb{E}\\bigl[p(X,i,a)\\{\\delta_{i,j}-p(X,j,a)\\}\\bigr]}\\\\ &{\\phantom{u^{\\top}\\nabla^{2}f_{u}(a)u}=\\frac{1}{\\epsilon}\\mathbb{E}\\bigl[\\sum_{i}u_{i}^{2}p(X,i,a)-\\big\\{\\sum_{i}u_{i}p(X,i,a)\\big\\}^{2}\\bigr]=\\frac{1}{\\epsilon}\\mathbb{E}\\bigl[\\sigma^{2}(X,u,a)\\bigr]\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\sigma^{2}(x,u,a)$ is the variance of a categorical random variable taking the value $u_{i}$ with probability $p(x,i,a)$ . This leads to the conclusion that the function is convex. ", "page_idx": 22}, {"type": "text", "text": "To establish strict convexity, we fix $a\\in A$ and notice that $0<p(x,y,a)<1$ (because $\\|g\\|_{\\infty}\\leq L)$ Therefore, the $\\sigma^{2}(x,u,a)$ can be zero only when the $u_{i}$ 's are all equal. Since $\\boldsymbol{A}\\,=\\,\\{a\\,\\in\\,\\mathbb{R}^{\\mathcal{V}}$ $\\textstyle\\sum_{y}a_{y}=0\\}$ , such $u$ belongs to the space $\\boldsymbol{\\mathcal{A}}$ in the unique case $u_{i}=0$ . This leads to the conclusion that for any $u\\in A$ and $u\\ne0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u^{\\top}\\nabla^{2}f_{u}(a)u=\\frac{1}{\\epsilon}\\mathbb E[\\sigma^{2}(X,u,a)]>0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, $f_{u}$ is strictly convex and has a unique minimizer in $\\boldsymbol{\\mathcal{A}}$ . In the remaining part of the proof, we focus on the first-order condition. ", "page_idx": 22}, {"type": "text", "text": "First order condition: The first-order condition is ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\nh(a,y)=p_{y}{\\mathrm{~for~all~}}y\\in\\mathcal{Y},{\\mathrm{~where~}}h(a,y)\\triangleq\\mathbb{E}[p(X,y,a)]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To show that (C.6) has a solution in a compact ball $K(\\epsilon,\\kappa,L)\\triangleq\\{a\\in\\mathcal{A}:\\|a\\|_{2}\\leq M(\\epsilon,\\kappa,L)\\}$ we construct an $M(\\epsilon,\\kappa,L)>0$ such that $\\mathrm{min}_{y}\\,h(a,y)<\\kappa$ whenever $\\|a\\|_{2}>M(\\epsilon,\\kappa,L)$ . Since $\\kappa\\le p_{y}\\le1-\\kappa$ for all $y\\in\\mathcal{V}$ this concludes that the solution to (C.6) must be inside the $K(\\epsilon,\\kappa,L)$ ", "page_idx": 22}, {"type": "text", "text": "Construction of the compact set: Let us define $y_{\\mathrm{min}}\\triangleq\\arg\\operatorname*{min}_{y}a_{y}$ and $y_{\\mathrm{max}}\\triangleq\\arg\\operatorname*{max}_{y}a_{y}$ To construct $M(\\kappa,L,\\epsilon)$ (we shall write it simply as $M$ whenever it is convenient to do so) we notice that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(a,y_{\\mathrm{min}})=\\mathbb{E}[p(X,y_{\\mathrm{min}},a)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\Bigl[\\frac{\\exp\\bigl(\\frac{g(X,1)+a_{y_{\\mathrm{min}}}}{\\varepsilon}\\bigr)}{\\sum_{i\\in y}\\exp\\bigl(\\frac{g(X,\\cdot)+a_{i}}{\\varepsilon}\\bigr)}\\Bigr]}\\\\ &{\\qquad\\qquad\\le\\frac{\\exp\\bigl(\\frac{L+a_{y_{\\mathrm{min}}}}{\\varepsilon}\\bigr)}{\\exp\\bigl(\\frac{L+a_{y_{\\mathrm{min}}}}{\\varepsilon}\\bigr)+\\sum_{i\\ge2}\\exp\\bigl(\\frac{-L+a_{i}}{\\varepsilon}\\bigr)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\exp\\bigl(\\frac{2L}{\\varepsilon}\\bigr)}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\bigl(\\frac{a_{i}}{\\varepsilon}-\\sigma_{y_{\\mathrm{min}}}\\bigr)}{\\exp\\bigl(\\frac{2L}{\\varepsilon}\\bigr)+\\sum_{i\\ge2}\\exp\\bigl(\\frac{a_{i}-a_{y_{\\mathrm{min}}}}{\\varepsilon}\\bigr)}}\\\\ &{\\qquad\\qquad\\le\\frac{\\exp\\bigl(\\frac{L}{\\varepsilon}\\bigr)}{\\exp\\bigl(\\frac{2L}{\\varepsilon}\\bigr)+\\exp\\bigl(\\frac{a_{i}}{\\varepsilon}\\bigr)}\\le\\frac{\\exp\\bigl(\\frac{2L}{\\varepsilon}\\bigr)}{\\exp\\bigl(\\frac{2L}{\\varepsilon}\\bigr)+\\exp\\bigl(\\frac{R}{\\varepsilon}\\bigr)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R\\triangleq\\operatorname*{min}\\{\\operatorname*{max}_{y}a_{y}-\\operatorname*{min}_{y}a_{y}:\\sum_{i}a_{i}=0,\\sum_{i}a_{i}^{2}>M^{2}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We rewrite the constraints of the optimization as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R\\triangleq\\operatorname*{min}\\left\\{\\,\\operatorname*{max}_{y}a_{y}-\\operatorname*{min}_{y}a_{y}:\\operatorname{mean}\\{a_{i}\\}=0,\\operatorname{var}\\{a_{i}\\}>\\frac{M^{2}}{|y|}\\,\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we use the Popoviciu's inequality on variance to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{M^{2}}{|\\mathcal{V}|}<\\mathrm{var}\\{a_{i}\\}\\le\\frac{(\\operatorname*{max}_{y}a_{y}-\\operatorname*{min}_{y}a_{y})^{2}}4,\\ \\mathrm{or}\\ \\operatorname*{max}_{y}a_{y}-\\operatorname*{min}_{y}a_{y}>\\frac{2M}{\\sqrt{|y|}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus R\u2265 whenever $\\|a\\|_{2}>M$ . We use this inequality in (C.7) and obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(a,y_{\\mathrm{min}})\\le\\frac{\\exp\\left(\\frac{2L}{\\epsilon}\\right)}{\\exp\\left(\\frac{2L}{\\epsilon}\\right)+\\exp\\left(\\frac{R}{\\epsilon}\\right)}\\le\\frac{\\exp\\left(\\frac{2L}{\\epsilon}\\right)}{\\exp\\left(\\frac{2L}{\\epsilon}\\right)+\\exp\\left(\\frac{2M}{\\epsilon\\sqrt{|y|}}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, we choose $M=M(\\epsilon,\\kappa,L)>0$ large enough such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(a,y_{\\mathrm{min}})\\le\\frac{\\exp\\left(\\frac{2L}{\\epsilon}\\right)}{\\exp\\left(\\frac{2L}{\\epsilon}\\right)+\\exp\\left(\\frac{2M}{\\epsilon\\sqrt{|y|}}\\right)}<\\kappa\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For such an $M$ we concludes that $h(a,y_{\\mathrm{min}})<\\kappa$ whenever $\\|a\\|_{2}>M$ ", "page_idx": 23}, {"type": "text", "text": "Lemma C.5. $L_{\\varepsilon}$ and $U_{\\varepsilon}$ are attained by some optimizers in a compact set $K(\\epsilon,\\kappa_{z},z\\in\\mathcal{Z};L)\\supset\\mathcal{A}$ (2.2), where $\\kappa_{z}=\\operatorname*{min}\\{p_{y|z},1-p_{y|z}:y\\in\\mathcal{Y}\\}$ ", "page_idx": 23}, {"type": "text", "text": "Proof of lemma C.5. We shall only prove the case of the minimization problem. The proof for the maximization problem uses a similar argument. ", "page_idx": 23}, {"type": "text", "text": "A decomposition of the minimization problem according to the values of $Z$ follows. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}_{a\\in A}\\mathbb{E}\\bigg[\\epsilon\\log\\bigg\\{\\frac{1}{|\\mathcal{P}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\bigl(\\frac{g(X,y,Z)+a_{Y,Z}}{\\epsilon}\\bigr)\\bigg\\}-\\mathbb{E}\\left[a_{Y,Z}\\mid Z\\right]\\bigg]}\\\\ &{=\\sum_{z}p_{z}\\left\\{\\begin{array}{l l}{\\underset{a_{,\\cdot},\\in\\mathbb{R}^{\\mathcal{Y}}}{\\operatorname*{min}}~\\mathbb{E}\\bigg[\\epsilon\\log\\bigg\\{\\frac{1}{|\\mathcal{P}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\bigl(\\frac{g(X,y,z)+a_{Y,z}}{\\epsilon}\\bigr)\\bigg\\}\\mid Z=z\\bigg]-\\mathbb{E}[a_{Y,z}\\mid Z=z]\\bigg\\}}\\\\ {\\sum_{y}a_{y,z=0}}\\end{array}\\right.}\\\\ &{=\\sum_{z}p_{z}\\left\\{\\begin{array}{l l}{\\operatorname*{min}_{\\scriptstyle\\atop{\\scriptstyle\\sum_{y}a_{y,z}=0}}\\mathbb{E}\\bigg[\\epsilon\\log\\bigg\\{\\frac{1}{|\\mathcal{P}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\bigl(\\frac{g(X,y,z)+a_{Y,z}}{\\epsilon}\\bigr)\\bigg\\}\\mid Z=z\\bigg]-\\sum_{y}a_{y,z}p_{y\\mid z}\\right\\}}\\\\ {\\qquad}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $p_{z}=P(Z=z)$ and $p_{y|z}=P(Y=y\\mid Z=z)$ . We fix a $z$ and consider the corresponding optimization problem in the decomposition. Then according to lemma C.4 the optimal point is in a compact set $K(\\epsilon,\\kappa_{z},L)$ . Thus the optimal point of the full problem is in the Cartesian product $\\prod_{z}K(\\epsilon,\\kappa_{z},L)\\subset\\mathcal{A}$ , which a compact set. Thus we let $\\begin{array}{r}{K(\\epsilon,\\bar{\\kappa_{z}},z\\in\\mathcal{Z};L)=\\prod_{z}K(\\epsilon,\\kappa_{z};L)}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Lemma C.6. The optimizers for the problems in (2.3) are tight with respect to $m,n\\to\\infty$ ", "page_idx": 23}, {"type": "text", "text": "Proof of the lemma C.6. Let $p_{y\\mid z}=P(Y=y\\mid Z=z)$ and $\\hat{p}_{y\\vert z}^{(m)}=\\hat{P}_{Y\\vert Z=z}^{(m)}(y)$ PYIz=\u2265(g). Since ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\sum_{y}\\left|\\hat{p}_{y\\mid z}^{(m)}-p_{y\\mid z}\\right|=d_{\\mathrm{TV}}\\left(\\hat{P}_{Y\\mid Z=z}^{(m)},P_{Y\\mid Z=z}\\right)=\\mathcal{O}_{P}(m^{-\\lambda})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "according to the assumption 2.4, for sufficiently large $m$ with high probability $p(m)\\;(p(m)\\rightarrow1$ as m\u2192)< $\\begin{array}{r}{\\frac{\\kappa_{z}}{2}\\leq\\hat{p}_{y\\vert z}^{(m)}\\leq1-\\frac{\\kappa_{z}}{2}}\\end{array}$ for all $y$ . Fix such an $m$ and 1p(m2  In lemma C.5 we replace Px,z With1 Xxuz and plz with ge)t to reach the conclusion that the optimizer is in the compact set $K(\\epsilon,\\kappa_{z}/2,z\\in\\mathcal{Z};L)$ . Thus, for suficiently large $m$ and any $n$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\mathrm{optimizer\\,is\\,in}\\,K(\\epsilon,\\kappa_{z}/2,z\\in\\mathcal{Z};L)\\big)\\geq p(m)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This establishes that the optimizers are tight. ", "page_idx": 23}, {"type": "text", "text": "D  Proof of extra results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 5.1. We only prove the theorem for upper Fr\u00e9chet bound. The proof of lower bound is similar. ", "page_idx": 24}, {"type": "text", "text": "First, notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\check{f}}_{u,\\varepsilon}(x,z,a)-f_{u,\\epsilon}(x,z,a)=}\\\\ &{=\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{\\varepsilon}\\right)\\right]-\\mathbb{E}_{Q_{Y|Z}}[a_{Y z}\\left|\\;Z=z\\right|}\\\\ &{\\;-\\varepsilon\\log\\left[\\frac{1}{|\\mathcal{V}|}\\sum_{y\\in\\mathcal{Y}}\\exp\\left(\\frac{g(x,y,z)+a_{y z}}{\\varepsilon}\\right)\\right]+\\mathbb{E}_{P_{Y|Z}}\\left[a_{Y z}\\left|\\;Z=z\\right|\\right]}\\\\ &{=\\mathbb{E}_{P_{Y|Z}}\\left[a_{Y z}\\left|\\;Z=z\\right|-\\mathbb{E}_{Q_{Y|Z}}[a_{Y z}\\left|\\;Z=z\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{|\\check{f}_{u,\\varepsilon}(x,z,a)-f_{u,\\epsilon}(x,z,a)|}&&{}\\\\ &{=\\left|\\mathbb{E}_{P_{Y}\\mid z}\\left[a_{Y\\,z}\\mid Z=z\\right]-\\mathbb{E}_{Q_{Y\\,|\\,Z}}[a_{Y\\,z}\\mid Z=z]\\right|}\\\\ &{\\leq\\|a\\|_{\\infty}\\times2d_{\\mathrm{TV}}\\big(Q_{Y\\mid Z=z},P_{Y\\mid Z=z}\\big)}&&{(\\mathrm{using~}a^{\\top}b\\leq\\|a\\|_{\\infty}\\|b\\|_{1})}\\\\ &{\\leq2\\|a\\|_{\\infty}\\delta\\,.}&&{(\\mathrm{by~assumption})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Defining ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a^{\\star}\\triangleq\\arg\\operatorname*{min}_{a}\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a)],\\,\\,\\,\\tilde{a}\\triangleq\\arg\\operatorname*{min}_{a}\\mathbb{E}[\\check{f}_{u,\\varepsilon}(X,Z,a)]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we now establish that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\check{U}_{\\epsilon}-U_{\\epsilon}|\\leq2\\delta\\operatorname*{max}(\\|a^{\\star}\\|_{\\infty},\\|\\check{a}\\|_{\\infty})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This is easily established from the following arguments: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\check{U}_{\\epsilon}=\\mathbb{E}[\\check{f}_{u,\\varepsilon}(X,Z,\\check{a})]}\\\\ &{\\quad\\leq\\mathbb{E}[\\check{f}_{u,\\varepsilon}(X,Z,a^{\\star})]}&{(\\check{a}\\mathrm{~is~the~minimizer})}\\\\ &{\\quad\\leq\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a^{\\star})]+2\\|a^{\\star}\\|_{\\infty}\\delta}&{(\\mathrm{using~eq.~(D.4)})}\\\\ &{\\quad=U_{\\epsilon}+2\\|a^{\\star}\\|_{\\infty}\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and similarly ", "page_idx": 24}, {"type": "equation", "text": "$$\nU_{\\epsilon}=\\mathbb{E}[f_{u,\\varepsilon}(X,Z,a^{\\star})]\\leq\\mathbb{E}[f_{u,\\varepsilon}(X,Z,\\check{a})]\\leq\\mathbb{E}[\\check{f}_{u,\\varepsilon}(X,Z,\\check{a})]+2\\|\\check{a}\\|_{\\infty}\\delta=\\check{U}_{\\epsilon}+2\\|a^{\\star}\\|_{\\infty}\\delta\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By assumption, $a^{\\star}$ and $\\check{a}$ are bounded. Thus, we can find a $C>0$ , which is independent of $\\delta>0$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n2\\operatorname*{max}(\\|a^{\\star}\\|_{\\infty},\\|\\check{a}\\|_{\\infty})\\le C\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the theorem holds for this $C$ ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 5.2. For clarity, we shall assume $X$ is a continuous random variable in this proof, even though this is not strictly needed. ", "page_idx": 24}, {"type": "text", "text": "Let $Q_{X,Y,Z}^{(1)}$ and $Q_{X,Y,Z}^{(2)}$ $\\Pi$ $P_{X,Z}$ and $P_{Y,Z}$ Let the densies of $Q_{X,Y,Z}^{(1)}$ and $Q_{X,Y,Z}^{(2)}$ $q_{X,Y,Z}^{(1)}$ z and qx,y,z: . Then, see that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{Z}}\\rho\\mathrm{d}t\\mathcal{A}_{N,\\mathbb{Z}}^{\\prime}\\int_{\\mathbb{Z}}d t\\mathcal{A}_{N,\\mathbb{Z}}^{\\prime}\\int_{\\mathbb{Z}}d t=}\\\\ &{=\\left|\\int_{\\mathbb{Z}}\\sum_{\\rho_{1},\\rho_{1}}\\langle g(\\mathbf{r},\\mathbf{\\hat{z}}_{N}),g(\\mathbf{r},\\mathbf{\\hat{z}}_{N},\\mathbf{z}_{I})-q_{N,\\mathbb{Z}}^{(2)},c(\\mathbf{r},\\mathbf{\\hat{z}}_{N},\\mathbf{z}_{I})\\rangle\\mathrm{d}\\mathbf{z}\\right|}\\\\ &{\\leq\\int\\sum_{\\rho_{1},\\rho_{1}}\\langle\\mathbf{r},\\frac{\\mathbf{\\hat{z}}_{N}}{2}\\rangle\\cdot|\\eta(\\mathbf{r},\\mathbf{\\hat{z}}_{N})-g(\\mathbf{r},\\mathbf{\\hat{z}}_{N}^{\\prime},\\mathbf{z}_{I})-g(\\mathbf{r},\\mathbf{\\hat{z}}_{I}^{\\prime})|d\\mathbf{z}}\\\\ &{\\leq2\\left\\|\\mathbf{\\hat{g}}\\right\\|_{N}\\sum_{\\rho}p_{2}(\\mathbf{r})\\hat{\\textbf{J}}\\sum_{\\rho_{1}}\\sum_{\\rho_{1},\\rho_{1}^{\\prime}}|\\langle\\mathbf{r},\\mathbf{\\hat{z}}_{N}^{\\prime}\\rangle|\\cdot\\eta(\\mathbf{r},\\mathbf{\\hat{z}}_{N}^{\\prime},\\mathbf{z}_{I}^{\\prime})|d\\mathbf{z}}\\\\ &{=2\\left\\|g\\right\\|_{N}\\sum\\left[\\overline{{\\left(\\mathbf{z}_{N}^{\\prime}(\\mathbf{\\hat{z}}_{N}^{\\prime},\\mathbf{r})\\mathbf{z}_{N}^{\\prime},\\mathbf{z}_{I}^{\\prime}\\right)}}\\right]}\\\\ &{\\leq2\\left\\|g\\right\\|_{N}\\left\\{\\mathbb{E}\\left[\\langle\\mathbf{z}_{N}^{\\prime}(\\mathbf{r}_{N}^{\\prime},\\mathbf{r},\\mathbf{\\hat{z}}_{N}^{\\prime},\\mathbf{z}_{I}^{\\prime})\\rangle\\right]+\\mathbb{E}\\left[\\langle\\mathbf{z}_{N}^{\\prime}(\\mathbf{z}_{N}^{\\prime2},\\mathbf{r},\\mathbf{\\hat{z}}_{N}^{\\prime},\\mathbf{r}_{I})\\rangle\\right]\\right\\}}\\\\ &{\\leq2\\left\\|g\\right\\|_{N}\\left\\{\\mathbb{E}\\left[\\\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where step D.9 is justified by triangle inequality, step D.10 is justified by Pinsker's inequality, step D.11 is justified by Jensen's inequality, and step D.12 is justified by the fact that both expected KL terms are conditional mutual information terms, which can be bounded by the conditional entropy. Followingthe sameida we can show that $\\begin{array}{r}{\\left|\\int g\\mathrm{d}Q_{X,Y,Z}^{(1)}-\\int g\\mathrm{d}Q_{X,Y,Z}^{(2)}\\right|\\,\\leq\\,\\sqrt{8\\left\\|g\\right\\|_{\\infty}^{2}H(Y\\mid Z)}}\\end{array}$ Consequently, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\int g\\mathrm{d}Q_{X,Y,Z}^{(1)}-\\int g\\mathrm{d}Q_{X,Y,Z}^{(2)}\\right|\\leq\\operatorname*{min}\\left(\\sqrt{8\\left\\|g\\right\\|_{\\infty}^{2}H(X\\mid Z)},\\sqrt{8\\left\\|g\\right\\|_{\\infty}^{2}H(Y\\mid Z)}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sqrt{8\\left\\|g\\right\\|_{\\infty}^{2}\\operatorname*{min}\\left(H(X\\mid Z),H(Y\\mid Z)\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Beausehstaesaldforydist $Q_{X,Y,Z}^{(1)}$ and $Q_{X,Y,Z}^{(2)}$ .m $\\Pi$ we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\nU-L\\leq{\\sqrt{8\\left\\|g\\right\\|_{\\infty}^{2}\\operatorname*{min}\\left(H(X\\mid Z),H(Y\\mid Z)\\right)}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E  Model selection using performance bounds ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we propose and empirically evaluate three strategies for model selection using our estimated bounds in Equation 2.3. ", "page_idx": 26}, {"type": "text", "text": "E.0.1  Introducing model selection strategies ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Assume, for example, $g(x,y,z)\\,=\\,\\Im[h(x)\\,=\\,y]$ for a given classifier $h$ , i.e., we conduct model selection based on accuracy, even though we can easily extend the same idea to different choices of metrics, such as F1 score. The model selection problem consists of choosing the best model from a set $\\mathcal{H}\\triangleq\\{h_{1},\\cdot\\cdot\\cdot,h_{K}\\}$ in order to maximize out-of-sample accuracy. Define $\\hat{L}_{\\varepsilon}(h)$ and $\\hat{U}_{\\varepsilon}(h)$ as the estimated accuracy lower and upper bounds for a certain model $h$ The first strategy is to choose the model with highest accuracy lower bound, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\nh_{\\mathrm{lower}}^{*}=\\arg\\operatorname*{max}_{h_{k}\\in\\mathcal{H}}\\hat{L}_{\\varepsilon}(h_{k})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Maximizing the accuracy lower bound approximates the distributionally robust optimization (DRO) [9] solution when the uncertainty set is given by $\\Pi$ in 1.1. That is, we optimize for the worst-case distribution in the uncertainty set. Analogously, we can choose the model that optimizes the best-case scenario ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{\\mathrm{upper}}^{*}=\\arg\\operatorname*{max}_{h_{k}\\in\\mathcal{H}}\\hat{U}_{\\varepsilon}(h_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, if we want to guarantee that both worst and best-case scenarios are not bad, we can optimize the average of upper and lower bounds, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{\\mathrm{avg}}^{*}=\\arg\\operatorname*{max}_{h_{k}\\in\\mathcal{H}}\\frac{\\hat{L}_{\\varepsilon}(h_{k})+\\hat{U}_{\\varepsilon}(h_{k})}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.0.2  Experiment setup ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this experiment, we select multilayer-perceptrons (MLPs). The considered MLPs have one hidden layer with a possible number of neurons in $\\lbrace50,100\\rbrace$ . Training is carried out with Adam [26], with possible learning rates in $\\{.1,.001\\}$ and weight decay ( $l_{2}$ regularization parameter) in $\\{.1,.001\\}$ . For those datasets that use the F1 score as the evaluation metric, we also tune the classification threshold in $\\{.2,.4,.5,.6,.8\\}$ (otherwise, they return the most probable class as a prediction). In total, $\\mathcal{H}$ is composed of 8 trained models when evaluating accuracy and 40 models when evaluating the F1 score. We also consider directly using the label model (Snorkel [47]) to select models. For example, when the metric considered is accuracy, i.e., we use ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{\\mathrm{label\\_model}}^{*}=\\arg\\operatorname*{max}_{h_{k}\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\hat{P}_{Y|Z}}\\mathbb{1}[h_{k}(X)=Y\\mid Z=Z_{i}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is a natural choice when $X\\perp\\!\\!\\!\\!\\perp Y\\mid Z$ . As benchmarks, we consider having a few labeled samples. ", "page_idx": 26}, {"type": "text", "text": "In Table 3, we report the average test scores of the chosen models over 10 repetitions for different random seeds (standard deviation report as subscript). The main message here is that, for those datasets in which our uncertainty about the score (given by the different upper and lower bounds) is small, e.g., \"commercial' and \u201ctennis\", using our approaches leads to much better results when compared to using small sample sizes. ", "page_idx": 26}, {"type": "text", "text": "Now, we explore a different way of comparing models. Instead of making an explicit model selection, this experiment uses the proposed metrics, i.e., accuracy lower/upper bounds, bounds average, to rank MLP classifiers. We rank the models using both the test set accuracy/F1 score (depending on Zhang et al. [63]) and alternative metrics, i.e., accuracy/F1 score lower/upper bounds, bounds average, label model, and small labeled sample sizes. Then, we calculate a Pearson correlation between rankings and display numbers in Table 4. If the numbers are higher, it means that the proposed selection method is capable of distinguishing good from bad models. Table 4 shows that the bounds average and label model methods usually return the best results when no labels are used. Moreover, in some cases using a small labeled sample for model selection can relatively hurt performance (when the validation set is small, there is a chance all models will have the same or similar performances, leading to smaller or null rank correlations). ", "page_idx": 26}, {"type": "table", "img_path": "VOVyeOzZx0/tmp/363f80d088161a79b79627505d0bda1cf89cf511033bb2b13e6a15e8a7b93ed6.jpg", "table_caption": ["Table 3: Performance of selected models "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "VOVyeOzZx0/tmp/dd0bed810530701c3dd89e906beef2e93115d9a308ecc05e9b17e087f5054d1d.jpg", "table_caption": ["Table 4: Ranking correlation when ranking using test set and alternative metric "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F More on experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "F.1  Extra results for the Wrench experiment ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Extra results for binary classification datasets can be found in Figures 4 and 5. In Table 5, we can see the results for multinomial classification datasets. ", "page_idx": 27}, {"type": "image", "img_path": "VOVyeOzZx0/tmp/ad4d34513e6d1a029d0ea6ac03793a4b9c87f26017e3f8c670c0c19283ecb8cd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 4: Bounds on classifier accuracies across classification thresholds for the Wrench datasets. Despite potential misspecification in Snorkel's and FlyingSquid's label model, it performs comparably to using labels to estimate $P_{Y\\mid Z}$ , giving approximate but meaningful bounds. . ", "page_idx": 28}, {"type": "image", "img_path": "VOVyeOzZx0/tmp/da69264b188382451450f5d4302378bedcd9952bddeea880930fd7f45b35c60a.jpg", "img_caption": ["Figure 5: Bounds on classifier accuracies and F1 scores across classification thresholds for the Wrench datasets (using the full set of weak labels). Despite potential misspecification in Snorkel's and FlyingSquid's label model, it performs comparably to using labels to estimate $P_{Y\\mid Z}$ , giving approximate but meaningful bounds. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.2  Extra plots for the hate speech detection experiment ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Table 6, we can see the same results already in the main text plus the results for FlyingSquid. ", "page_idx": 28}, {"type": "text", "text": "G Computing resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "All experiments were conducted using a virtual machine with 32 cores. The experiments are not computationally intensive and everything can be run within a few hours. ", "page_idx": 28}, {"type": "table", "img_path": "VOVyeOzZx0/tmp/1844a5a3d0adcdb4ae75a46dfd7fa83b061c79c0b87156cf3208c2cca46944f1.jpg", "table_caption": ["Table 5: Bounding the accuracy of classifiers in multinomial classification "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "VOVyeOzZx0/tmp/2a5e9d0cd887138bca0dadcc97ac30d9dbce5b766c53b5ba8749e484e61db345.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 6: Precision and recall bounds for hate speech detection. These plots guide practitioners to trade off recall and precision in the absence of high-quality labels. ", "page_idx": 29}, {"type": "text", "text": "G.1  Examples of prompts used in Section 4.2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Prompt 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "You should classify the target sentence as \"spam\" or \"ham\". If definitions or examples are introduced, you should consider them when classifying sentences. Respond with \"spam\" or \"ham\". ", "page_idx": 29}, {"type": "text", "text": "Target sentence: if your like drones, plz subscribe to Kamal Tayara. He takes videos with his drone that are absolutely beautiful\uff0e -- Response: ", "page_idx": 29}, {"type": "text", "text": "Prompt 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "You should classify the target sentence as \"spam\" or \"ham\". If definitions or examples are introduced\uff0c you should consider them when classifying sentences. Respond with \"spam\" or \"ham\" ", "page_idx": 29}, {"type": "text", "text": "Definition of spam: spam is a term referencing a broad category of postings which abuse web-based   \nforms to post unsolicited advertisements as comments on forums, blogs \uff0c wikis and online guestbook. ", "page_idx": 30}, {"type": "text", "text": "Definition of ham: texts that are not spam. ", "page_idx": 30}, {"type": "text", "text": "Target sentence: if your like drones\uff0c plz subscribe to Kamal Tayara. He takes videos with  his drone that are absolutely beautiful. -- Response: ", "page_idx": 30}, {"type": "text", "text": "Prompt 3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "You should classify the target sentence as \"spam\" or \"ham\". If definitions or examples are introduced,   \nyou should consider   \nthem when classifying sentences .   \nRespond with \"spam\" or \"ham\". ", "page_idx": 30}, {"type": "text", "text": "Example 0: 860,000,0oo 1ets make it first female to reach one billion!! Share it and replay it! -- Response: ham ", "page_idx": 30}, {"type": "text", "text": "Example 1: Waka waka eh eh -- Response: ham ", "page_idx": 30}, {"type": "text", "text": "Example 2: You guys should check out this EXTRAORDINARY website called ZONEPA.COM . You can make money online and start working from home today as I am! I am making over $^{\\mathbb{S}3,000+}$ per month at ZONEPA.COM ! Visit Zonepa.com and check it out! How does the mother approve the axiomatic insurance? The fear appoints the roll. When does the space prepare the historical shame? -- Response: spam ", "page_idx": 30}, {"type": "text", "text": "Example 3: Check out these Irish guys cover of Avicii&#39;s  Wake Me Up! Just search... &quot;wake me up Fiddle Me Silly&quot; Worth a listen for the gorgeous fiddle player! -- Response: spam ", "page_idx": 30}, {"type": "text", "text": "Example 4: if you want to win money at hopme click here   \n<ahref $=$ \"https : //www. paidverts .com/ ref / sihaa $\\mathrm{m}01\\,^{\\prime\\prime}\\mathrm{>}$   \nhttps : //www. paidverts .com/ ref / sihaam $01</{\\mathrm a}>$ it&#39;s work 100/100 -- Response: spam ", "page_idx": 30}, {"type": "text", "text": "Target sentence: if your like drones, plz subscribe to Kamal Tayara He takes videos with his drone that are absolutely beautiful. --Response : ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the abstract and Introduction section, we summarize the contributions and the scope of thepaper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have included a section to discuss limitations in the conclusion ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that infuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The assumptions are discussed in the statements of the theorems and the proofs are provided in the Appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a general setting of the experiments in the paper and details are provided in the code itself submitted as supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g.. to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide simulation settings that are accessible and reproducible through the submitted zip file. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips . cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We do specify those parameters in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We provide errorbars for the plots. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 33}, {"type": "text", "text": "\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We include a section in the appendix about this Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we followed the NeurIPS Code of Ethics ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work may have potential societal consequences, none of which we feel must be specifically highlighted here. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, Or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We make citations when needed. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]