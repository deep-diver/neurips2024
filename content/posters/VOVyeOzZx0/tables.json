[{"figure_path": "VOVyeOzZx0/tables/tables_6_1.jpg", "caption": "Table 1: Bounding accuracy in multinomial classification", "description": "This table presents the results of bounding accuracy for multinomial classification using two different label models: Oracle (using true labels) and Snorkel (using a learned label model).  The table shows the lower bound, upper bound, and test accuracy for each model on the agnews and semeval datasets. The results demonstrate that the proposed method can provide reasonable bounds on the performance of weakly supervised models, even when the label model is not perfectly specified.", "section": "4.1 Bounding the performance of weakly supervised classifiers"}, {"figure_path": "VOVyeOzZx0/tables/tables_8_1.jpg", "caption": "Table 2: Performance of selected models", "description": "This table presents the performance of various models selected using different strategies: using the lower bound, average of bounds, label model, and a labeled dataset with 100 samples.  The performance is measured using accuracy and F1 scores for different datasets (agnews, imdb, yelp, tennis, commercial).  The results highlight that the proposed methods using Fr\u00e9chet bounds perform better than the label model and are comparable to using a small labeled dataset. ", "section": "4.3 Model selection strategies using the Fr\u00e9chet bounds"}, {"figure_path": "VOVyeOzZx0/tables/tables_27_1.jpg", "caption": "Table 3: Performance of selected models", "description": "This table presents the performance of different models on various datasets, comparing various metrics like accuracy and F1 score. It shows the lower and upper bounds of these metrics, the average of the bounds, results using only the label model, and results using a small set of labeled data (n = 10, 25, 50, 100). This helps to assess the impact of different model selection strategies on accuracy and F1 scores in the context of weak supervision.", "section": "4.3 Model selection strategies using the Fr\u00e9chet bounds"}, {"figure_path": "VOVyeOzZx0/tables/tables_27_2.jpg", "caption": "Table 2: Performance of selected models", "description": "This table presents the performance of various models selected using different strategies based on the Fr\u00e9chet bounds.  The models were evaluated on several datasets from the Wrench benchmark, measuring accuracy or F1 score, depending on the dataset.  The results are compared against the performance of models selected using a traditional approach (label model) and models trained with a small set of labeled data (Labeled (n=10), Labeled (n=25), Labeled (n=50), Labeled (n=100)).  The table demonstrates the effectiveness of using Fr\u00e9chet bounds for model selection, especially when uncertainty around model performance is low.", "section": "4.3 Model selection strategies using the Fr\u00e9chet bounds"}, {"figure_path": "VOVyeOzZx0/tables/tables_29_1.jpg", "caption": "Table 1: Bounding accuracy in multinomial classification.", "description": "This table presents the results of bounding accuracy for multinomial classification using three different label models: Oracle, Snorkel, and FlyingSquid.  The Oracle model uses true labels, while Snorkel and FlyingSquid are weak supervision methods. The table shows the lower bound, upper bound, and test accuracy for each label model on four different datasets: agnews, trec, semeval, and chemprot.  The results demonstrate the ability of the proposed method to estimate reliable performance bounds even without access to ground truth labels.", "section": "4.1 Bounding the performance of weakly supervised classifiers"}]