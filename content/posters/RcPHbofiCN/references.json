{"references": [{"fullname_first_author": "Chenxin An", "paper_title": "L-eval: Instituting standardized evaluation for long context language models", "publication_date": "2023-07-11", "reason": "This paper introduces a benchmark for evaluating LLMs' long context capabilities, providing a standardized framework for comparing the performance of different models, which is relevant to the study's evaluation methodology."}, {"fullname_first_author": "Yuhan Chen", "paper_title": "Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use", "publication_date": "2024-08-01", "reason": "This paper directly addresses the issue of LLMs' uneven awareness of different contextual positions and proposes a solution that is directly compared against in the current research."}, {"fullname_first_author": "Zhenyu Zhang", "paper_title": "Found in the middle: How language models use long contexts better via plug-and-play positional encoding", "publication_date": "2024-03-04", "reason": "This paper also addresses the issue of LLMs' uneven awareness of different contextual positions and proposes a novel method that is compared against in the current research."}, {"fullname_first_author": "Nelson F. Liu", "paper_title": "Lost in the middle: How language models use long contexts", "publication_date": "2023-01-01", "reason": "This paper identifies the 'lost-in-the-middle' phenomenon, a critical observation about LLMs' contextual awareness limitations, which is directly addressed in the current research."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This foundational paper introduces the Transformer architecture, the basis for many modern LLMs, making it a crucial reference for understanding the architecture that the current research builds upon."}]}