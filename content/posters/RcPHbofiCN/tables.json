[{"figure_path": "RcPHbofiCN/tables/tables_6_1.jpg", "caption": "Table 1: Experimental results on the L-Eval Benchmark [1]. Applying to various models, MOICE demonstrate superior performance compared to previous competitive approaches. We emphasize the highest score in bold.", "description": "This table presents the results of experiments conducted on the L-Eval benchmark, comparing the performance of MOICE against several baselines across various tasks.  The tasks are categorized into closed-ended (requiring factual answers) and open-ended (requiring more creative or complex generation).  Three different large language models (LLMs) were used: Llama2-7B-Chat, Mistral-7B-Instruct-8k, and Qwen1.5-7B-Chat. The table shows that MOICE consistently outperforms the baselines across all three LLMs and both task types, demonstrating its effectiveness in enhancing long-context understanding and generation.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_6_2.jpg", "caption": "Table 2: Practical inference time (in minutes) / GPU memory costs (GB) on a single A800-80G GPU for each method applied to Llama2-7B-Chat (top) and Mistral-7B-Instruct-8k (bottom), respectively. Due to out-of-memory issues, AB can not accomplish many tasks, denoted as OOM in the table.", "description": "This table presents a comparison of the practical inference time and GPU memory consumption for different methods used to enhance LLMs' context awareness.  The methods compared are Attention Buckets (AB), Multi-Scale Positional Embedding (Ms-PoE), and Mixture of In-Context Experts (MoICE). The table shows the resource usage for each method on two different LLMs (Llama2-7B-Chat and Mistral-7B-Instruct-8k) for various tasks (Coursera, QUALITY, TOEFL, SFiction, Open-Ended). Note that the Attention Buckets (AB) method resulted in out-of-memory (OOM) errors for several tasks due to high resource demands.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_7_1.jpg", "caption": "Table 3: The experiment results on the MDQA task. MoICE achieve superior average performance compared to previous competitive approaches. We emphasize the highest score in bold.", "description": "This table presents the results of experiments conducted on the MDQA task, a benchmark for evaluating the effectiveness of different methods in enhancing LLMs' context awareness within the context of retrieval-augmented generation (RAG).  The table compares the performance of MoICE against several baseline methods (Ms-PoE, AB) across various positions of the relevant document within the context (for Llama2-7B-Chat, positions 1, 3, 5, 7, and 10; for Mistral-7B-Instruct-8k, positions 1, 8, 15, 23, and 30).  The 'Gap' column represents the difference between the highest and lowest average performance across all positions for each method. The highest average scores are highlighted in bold, showcasing the superior performance of MOICE.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_8_1.jpg", "caption": "Table 4: The performance of Llama-2-7B-chat enhanced by MoICE with N in-context experts. We show results marked with color to emphasize the improvements over the original model.", "description": "This table presents the performance of the Llama-2-7B-chat language model enhanced with the MoICE method, using different numbers (N) of in-context experts. The results show how increasing the number of experts improves the model's performance on various aspects, highlighting improvements over the original model without MoICE.  The table's data is organized by different numbers of experts (N), and their effect on performance metrics (Coursera, QUALITY, TOEFL, SFiction, and Avg.) is shown.", "section": "5.1 The effect of expert total numbers N"}, {"figure_path": "RcPHbofiCN/tables/tables_8_2.jpg", "caption": "Table 5: The improvement of context awareness of Llama-2-7B-chat by MoICE, wherein each head dynamically selects diverse K experts (N=7). We show results marked with color to emphasize the improvements over the original model.", "description": "This table presents the performance of Llama-2-7B-chat enhanced with MoICE, varying the number of selected experts (K) while keeping the total number of experts (N) fixed at 7.  It shows the average scores across four metrics (Coursera, QUALITY, TOEFL, SFiction) for different values of K (1, 3, 5, 7), as well as for scenarios with equal or random weights assigned to the selected experts.  The purpose is to demonstrate the effect of the number of selected experts on the model's context awareness.", "section": "5.2 The effect of selected experts number K"}, {"figure_path": "RcPHbofiCN/tables/tables_8_3.jpg", "caption": "Table 6: The improvement of context awareness of Llama-2-7B-chat by MoICE trained on various data.", "description": "This table presents the results of experiments evaluating the impact of different training datasets on the performance of Llama-2-7B-chat enhanced with the MoICE method.  The table shows the average scores achieved on four different tasks (Coursera, QuALITY, TOEFL, and SFiction) for four different training datasets (OpenHermes, Airoboros, Long-Alpaca, and LongAlign). The results demonstrate the robustness of the MoICE method to various training data.", "section": "5.3 MOICE is robust to training data"}, {"figure_path": "RcPHbofiCN/tables/tables_9_1.jpg", "caption": "Table 3: The experiment results on the MDQA task. MoICE achieve superior average performance compared to previous competitive approaches. We emphasize the highest score in bold.", "description": "This table presents the results of experiments conducted on the MDQA (Multi-Document Question Answering) task.  It compares the performance of MoICE against several baseline methods (Ms-PoE, AB) across different LLMs (Llama-2-7B-Chat and Mistral-7B-Instruct-8k).  The results show the average performance across various positions of the relevant document within the context.  The \"Gap\" column represents the difference between the highest and lowest average performance across positions for each model and LLM.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_14_1.jpg", "caption": "Table 1: Experimental results on the L-Eval Benchmark [1]. Applying to various models, MOICE demonstrate superior performance compared to previous competitive approaches. We emphasize the highest score in bold.", "description": "This table presents the results of experiments conducted on the L-Eval benchmark to evaluate the performance of the proposed MoICE method and several competitive baselines.  The benchmark consists of tasks categorized into two groups: closed-ended and open-ended. Closed-ended tasks focus on the capacity for understanding and reasoning within long contexts, while open-ended tasks include summarization generation and open-format question-answering. The table compares the performance across multiple open-source LLMs (Llama2-7B-Chat, Mistral-7B-Instruct-8k, and Qwen1.5-7B-Chat), highlighting the superior performance achieved by MOICE in both task categories and across various models.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_15_1.jpg", "caption": "Table 1: Experimental results on the L-Eval Benchmark [1]. Applying to various models, MOICE demonstrate superior performance compared to previous competitive approaches. We emphasize the highest score in bold.", "description": "This table presents the results of experiments conducted on the L-Eval benchmark, comparing the performance of the proposed MoICE method against several baselines and other state-of-the-art methods for enhancing long-context understanding and generation in LLMs.  It shows the performance across different tasks (Coursera, QUALITY, TOEFL, SFiction) for both closed-ended and open-ended questions, indicating superior performance of MoICE across multiple models (Llama2-7B-Chat, Mistral-7B-Instruct-8k, Qwen1.5-7B-Chat).  The highest scores for each category are highlighted in bold, showcasing the effectiveness of MOICE.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_16_1.jpg", "caption": "Table 1: Experimental results on the L-Eval Benchmark [1]. Applying to various models, MOICE demonstrate superior performance compared to previous competitive approaches. We emphasize the highest score in bold.", "description": "This table presents the results of experiments conducted on the L-Eval benchmark to evaluate the performance of MoICE against several baselines and other state-of-the-art methods for enhancing the context awareness of LLMs. The benchmark consists of closed-ended and open-ended tasks, evaluating long context understanding and generation. Results show MoICE's superior performance across various LLMs, and the best results are highlighted.", "section": "4 Experiment"}, {"figure_path": "RcPHbofiCN/tables/tables_17_1.jpg", "caption": "Table 11: The ablation study on the auxiliary loss of MoICE. To assess the impact of this loss term, we perform an ablation experiment on two LLMs by removing it from Eq. 10. The results show a significant drop in performance, highlighting the positive impact of the auxiliary loss.", "description": "This table presents the ablation study results on the auxiliary loss used in the MoICE model. By removing the auxiliary loss (Laux) from the overall training objective (Eq. 10), the impact on the model's performance is evaluated on two different LLMs (Llama2-7B-chat and Mistral-7B-Instruct-8k). The results demonstrate that removing the auxiliary loss leads to a significant decrease in performance across various metrics (Coursera, QUALITY, TOEFL, SFiction), highlighting its crucial role in enhancing the performance of the MoICE model.", "section": "5.3 MOICE is robust to training data"}]