[{"figure_path": "UfLH4T676K/figures/figures_9_1.jpg", "caption": "Figure 1: Comparison of the generalization error rates between vanilla gradient descent and over-parameterized gradient descent (OpGD). We set p = 1 and q = 2 for the truth parameter \u03b8*, and \u03b3 = 1.5 for the left column and \u03b3 = 3 for the right column. For each n, we repeat 64 times and plot the mean and the standard deviation.", "description": "This figure compares the generalization error rates of vanilla gradient descent and over-parameterized gradient descent (OpGD) for different sample sizes (n). Two different eigenvalue decay rates (\u03b3 = 1.5 and \u03b3 = 3) are considered.  The results demonstrate that OpGD achieves a better generalization performance compared to the vanilla gradient descent method, especially when the misalignment between the eigenvalues and the true signal is more severe (larger q). The error bars represent the standard deviation across multiple runs.", "section": "Numerical Experiments"}, {"figure_path": "UfLH4T676K/figures/figures_9_2.jpg", "caption": "Figure 4: The generalization error as well as the evolution of the eigenvalue terms a<sub>j</sub>(t)b(t) over the time t. The first row shows the generalization error of three parameterizations D = 0,1,3 with respect to the training time t. The rest of the rows show the evolution of the eigenvalue terms a<sub>j</sub>(t)b(t) over the time t. For presentation, we select the index j = 100 to 200. The blue line shows the eigenvalue terms and the black marks show the non-zero signals scaled according to Proposition 3.4. For the settings, we set p = 1, q = 2 and y = 2.", "description": "This figure shows the generalization error for three different over-parameterized models (D=0, 1, and 3) across various training times (t).  It also illustrates how the trainable eigenvalues (aj(t)b(t)) evolve over time for a subset of components (j=100 to 200). The plot highlights the adaptive nature of the eigenvalues, showing how they adjust to both signal and noise components according to the theoretical proposition 3.4. The settings used for this experiment are p=1, q=2, and \u03b3=2.", "section": "Numerical Experiments"}, {"figure_path": "UfLH4T676K/figures/figures_20_1.jpg", "caption": "Figure 1: Comparison of the generalization error rates between vanilla gradient descent and over-parameterized gradient descent (OpGD). We set p = 1 and q = 2 for the truth parameter \u03b8*, and \u03b3 = 1.5 for the left column and \u03b3 = 3 for the right column. For each n, we repeat 64 times and plot the mean and the standard deviation.", "description": "This figure compares the generalization error rates of vanilla gradient descent and the over-parameterized gradient descent method proposed in the paper.  Two different values of \u03b3 (1.5 and 3) are used, resulting in four subplots.  Each subplot shows the generalization error (log scale) versus the sample size (log scale) for each method.  Error bars representing standard deviations are included for 64 repetitions per data point.  The results demonstrate that the over-parameterized method consistently achieves a lower generalization error across different \u03b3 values.", "section": "Numerical Experiments"}, {"figure_path": "UfLH4T676K/figures/figures_22_1.jpg", "caption": "Figure 4: The generalization error as well as the evolution of the eigenvalue terms a<sub>j</sub>(t)b(t)<sup>D</sup> over the time t. The first row shows the generalization error of three parameterizations D = 0,1,3 with respect to the training time t. The rest of the rows show the evolution of the eigenvalue terms a<sub>j</sub>(t)b(t)<sup>D</sup> over the time t. For presentation, we select the index j = 100 to 200. The blue line shows the eigenvalue terms and the black marks show the non-zero signals scaled according to Proposition 3.4. For the settings, we set p = 1, q = 2 and y = 2.", "description": "This figure shows the generalization error and eigenvalue evolution for different depths of over-parameterization (D=0, 1, 3). The top row displays the generalization error over time for each depth, while the subsequent rows illustrate the evolution of trainable eigenvalues (aj(t)b(t)D) over time for a subset of components (j=100 to 200).  Significant components are highlighted.  The results demonstrate how deeper over-parameterization improves generalization and adapts eigenvalues to the signal structure.", "section": "Numerical Experiments"}, {"figure_path": "UfLH4T676K/figures/figures_23_1.jpg", "caption": "Figure 5: Comparison of the generalization error between the fixed kernel gradient method and the diagonal adaptive kernel method. The left figure shows the generalization error curve of a single trial. The right figure shows the generalization error rates with respect to the sample size n.", "description": "This figure compares the generalization performance of two methods: the fixed kernel gradient descent and the diagonal adaptive kernel method. The left panel shows the generalization error curves obtained from a single trial of the two methods, illustrating how the diagonal adaptive kernel method achieves a lower generalization error with increased training time. The right panel provides a more comprehensive comparison by plotting the generalization error rate against the sample size (n) for both methods, confirming the superior performance of the diagonal adaptive kernel method in terms of convergence rate.", "section": "C Detailed Numerical Experiments"}, {"figure_path": "UfLH4T676K/figures/figures_24_1.jpg", "caption": "Figure 6: The empirical coefficients of the regression function over the Fourier basis for the \\\"California Housing\\\" dataset (upper) and \\\"Concrete Compressive Strength\\\" dataset (lower). Note that we take different numbers of Fourier basis functions for the two datasets for better visualization.", "description": "This figure shows the empirical coefficients of the regression function projected onto the Fourier basis for the \\\"California Housing\\\" and \\\"Concrete Compressive Strength\\\" datasets. The top two subfigures correspond to the \\\"California Housing\\\" dataset, while the bottom two correspond to the \\\"Concrete Compressive Strength\\\" dataset.  In each pair of subfigures, the left one displays the coefficients in the kernel's order, while the right one shows them sorted in descending order. The plots reveal the distribution of coefficients, highlighting the presence of spikes or large values and a potential mismatch between the kernel eigenvalue order and the order of significance of the coefficients.", "section": "C Detailed Numerical Experiments"}]