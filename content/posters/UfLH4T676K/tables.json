[{"figure_path": "UfLH4T676K/tables/tables_21_1.jpg", "caption": "Table 1: Convergence rates of the over-parameterized gradient descent (8) under different settings of the truth parameter p, q and the eigenvalue decay rate \u03b3, where r* is the ideal convergence rate. The convergence rate is estimated by the logarithmic least-squares fitting of the generalization error with n ranging from 2000, 2200,..., 4000, where the generalization error is the mean of 256 repetitions.", "description": "This table presents the convergence rates of the over-parameterized gradient descent method for different settings of parameters p, q, and \u03b3.  The convergence rate, estimated via logarithmic least-squares fitting, shows the relationship between the generalization error and the sample size (n). The ideal convergence rate (r*) is also provided for comparison.  Each rate is an average of 256 repetitions.", "section": "Detailed Numerical Experiments"}, {"figure_path": "UfLH4T676K/tables/tables_21_2.jpg", "caption": "Table 1: Convergence rates of the over-parameterized gradient descent (8) under different settings of the truth parameter p, q and the eigenvalue decay rate \u03b3, where r* is the ideal convergence rate. The convergence rate is estimated by the logarithmic least-squares fitting of the generalization error with n ranging from 2000, 2200,..., 4000, where the generalization error is the mean of 256 repetitions.", "description": "This table presents the convergence rates of the over-parameterized gradient descent method (equation 8 in the paper) under various conditions.  It shows how the convergence rate (estimated using logarithmic least-squares fitting of the generalization error across different sample sizes) changes depending on the values of parameters p, q, and \u03b3, which represent aspects of the target signal's structure and the kernel's properties. The ideal convergence rate (r*) provides a baseline for comparison.", "section": "Detailed Numerical Experiments"}]