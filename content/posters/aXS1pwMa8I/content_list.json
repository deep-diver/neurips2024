[{"type": "text", "text": "Learning 3D Equivariant Implicit Function with Patch-Level Pose-Invariant Representation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xin $\\mathbf{H}\\mathbf{u}^{\\mathbf{1}}$ , Xiaole Tang1, Ruixuan $\\mathrm{Y}\\mathbf{u}^{2}$ , Jian $\\mathbf{Sun}(\\boxtimes)^{1,3}$ ", "page_idx": 0}, {"type": "text", "text": "1 Xi\u2019an Jiaotong University, Xi\u2019an, China 2 Shandong University, Weihai, China   \n3 Pazhou Laboratory (Huangpu), Guangzhou, China {huxin7020,tangxl}@stu.xjtu.edu.cn,   \nyuruixuan@sdu.edu.cn, jiansun@xjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Implicit neural representation gains popularity in modeling the continuous 3D surface for 3D representation and reconstruction. In this work, we are motivated by the fact that the local 3D patches repeatedly appear on 3D shapes/surfaces if the factor of poses is removed. Based on this observation, we propose the 3D patch-level equivariant implicit function (PEIF) based on the 3D patch-level pose-invariant representation, allowing us to reconstruct 3D surfaces by estimating equivariant displacement vector fields for query points. Specifically, our model is based on the pose-normalized query/patch pairs and enhanced by the proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch geometry feature by learnable multi-head memory banks. Extensive experiments show that our model achieves state-of-the-art performance on multiple surface reconstruction datasets, and also exhibits better generalization to crossdataset shapes and robustness to arbitrary rotations. Our code will be available at https://github.com/mathXin112/PEIF.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Surface reconstruction aims at generating continuous surfaces from discrete point clouds. It is a fundamental and challenging task in current robotics and vision applications [1, 2, 3]. Recently, deep learning-based implicit neural representations (INRs) have emerged as a powerful tool for this task, such as signed distance fields (SDFs) [4, 5], unsigned distance fields (UDFs) [6, 7, 8], and neural vector fields (NVF) [9]. INRs benefti from its continuity, and the ability to handle complicated topology, showing promising performance on surface reconstruction. ", "page_idx": 0}, {"type": "text", "text": "Although current INRs-based methods have achieved promising performance in reconstructing surfaces, they suffer from two main challenges. First, most methods [9, 10, 11] deal with the distinct local regions as geometry elements to estimate the query point values, e.g., signed/unsigned distance. However, different local regions may exhibit different poses but with similar intrinsic geometry. The extrinsic poses of these 3D patches prevent the models from capturing the intrinsic geometry of 3D shape patches. Second, INRs [5, 6, 7, 9, 12] without considering equivalence commonly learn the representation of the points using a fixed coordinate frame, implying that if the input points are rotated, the original coordinate mapping may no longer accurately predict the desired output, leading to distortions or inaccuracies. These properties of INRs hinder their applicability to complex 3D scenarios, in particular with regard to their cross-domain generalization ability and robustness to arbitrary transformations like rotations. ", "page_idx": 0}, {"type": "text", "text": "To tackle these challenges, we try to eliminate the redundant factor of poses and more focus on the learning of the intrinsic geometric representation of local regions, yielding a patch-level poseinvariant representation (PPIR) of 3D objects. Based on this representation, we develop a patch-level equivariant implicit function (PEIF), allowing us to achieve the equivariance patch-wisely while effectively encoding arbitrary topology. Specifically, in the PEIF framework, the query/patch pairs are first normalized via a unique pose normalization. Then the query/patch features are extracted and processed via learnable multi-head memory banks to acquire the intrinsic patch geometry representation, which is aggregated with the spatial relation representation, resulting in the patchlevel pose-invariant representation. PPIR is then utilized for displacement prediction, which can be proven to be equivariant for $S E(3)$ transformations. These designs enhance the expressive power of INRs with PPIR and enable the PEIF to flexibly adapt to 3D domain gaps as well as arbitrary $S E(3)$ transformations. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows. First, we propose a patch-based equivariant implicit function based on the pose-invariant feature learning, facilitating 3D reconstruction robust to 3D shapes $S E(3)$ transformations. Second, we design an intrinsic patch geometry representation module encoding rich patch-level pose-invariant features leveraging similar geometric patches. Third, the effectiveness of PEIF for surface reconstruction is demonstrated on four datasets including two CAD object datasets, a synthetic scene-level dataset, and a real scan dataset. Experiments show that our method outperforms baseline methods and can effectively reconstruct fine geometric structures, particularly performing well in cross-dataset generalization and the robustness to arbitrary rotations. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Implicit Representation for 3D Shape Reconstruction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Deep learning-based implicit representations have achieved significant advancements, due to their continuity and ability to handle complex geometry structures. Implicit representation for 3D surface reconstruction commonly learns to assign specific values for query points in 3D space. For example, occupancy field (occ) based methods [13, 14, 15, 16, 17, 18] enable the 3D reconstruction as a binary classification problem. The Occupancy Network [14] introduces predictions of spatial point occupancy, while advancements like ConvONet [19] and POCO [10] integrate grid-oriented convolutional or transformer frameworks to enhance performance. Recently, ALTO [20] iteratively refines features from both points and grids, deploying attention-driven interpolation from adjacent grids to decode occupancy values for query points. GridFormer [11] introduces transformer architecture to integrate the advantages of both points and grids for the prediction of occupancy. ", "page_idx": 1}, {"type": "text", "text": "SDF/UDF provides a continuous value to each spatial point, indicating the corresponding signed or unsigned distance to the surface. UDF with unsigned distance overcomes the limitations of SDF in handling non-watertight geometries. DeepSDF [4] leverages Multi-Layer Perceptron (MLP) to globally model SDF for entire 3D shape, while DeepLS [5], Instant-NGP [21] and NKSR [22] design more detailed operations to predict the SDF / UDF locally or hierarchically with MLP, kernel function or transformers, etc. GIFS [12] represents general shapes with multi-layer surfaces based on the spatial relationship between points. CAP-UDF [8] employs a field consistency constraint to get consistency-aware UDF. GeoUDF [7] adaptively approximates the UDF and its gradient of a point cloud by leveraging local geometry in a decoupled manner. However, separate learning of UDF values and gradients for points may result in accurate UDF but with the inverted direction problem. To address this issue, NVF [9] proposes an explicit approach to learning implicit representations based on displacement vectors, which ensures both accuracy and correct directional information. In this paper, we adopt this representation, predicting a displacement vector for each point in 3D space. Compared to NVF [9], we design our PEIF over the pose-normalized 3D patches and obtain the $S E(3)$ -equivariant implicit function. ", "page_idx": 1}, {"type": "text", "text": "2.2 $S E(3)$ -Equivariant Network ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "$S E(3)$ -equivariance has been extensively studied in both 2D images [23] and 3D point clouds [24, 25, 26, 27, 28]. Given 3D point cloud $X$ and transformation $\\forall\\zeta\\in S E(3)$ , a model $f$ is said to be $S E(3)$ - equivariant when it satisfies $f\\circ\\zeta(X)=\\zeta\\circ f(X)$ . Various works have been proposed to achieve $\\bar{S E}(3)$ -equivariance based on PCA [29, 30, 25], spherical harmonics [31, 32, 33], equivariant message passing [34, 35, 36], or Vector Neuron [26, 27, 28]. $S E(3)$ -equivariant networks are particularly useful for 3D point analysis tasks, such as molecular property or trajectory modeling [34, 35, 36, 37], protein structure prediction [38, 39, 40], 3D shape recognition [26, 27, 28], and robotics [41, 42, 43]. ", "page_idx": 1}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/f79798b4fd60260e9fe833ea61ee1296935035a2f48891f694c54ce5ed0c0def.jpg", "img_caption": ["Figure 1: Local 3D patches may exhibit geometric similarity, but with different poses. When the pose is removed, these local regions appear repeatedly. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Introducing $S E(3)$ -equivariance to build an orientation-robust implicit field is one of the motivations of this work. There are few works involving equivariance in the implicit field. EFEM [44] uses Vector Neuron [27] to learn equivariant shape representations before shape segmentation. E-GraphONet [24] utilizes basic Vector Neuron [27] layers to design graph networks, achieving locally $S O(3)$ -invariant features for implicit function learning. E-GraphONet [24] is the most related work to ours, which extends neurons from 1D scalars to 3D vectors for each point. In comparison, our PEIF employs lightweight PCA to achieve pose-invariant patch-level representation and leverages a multi-head memory bank for intrinsic geometry representation, achieving state-of-the-art 3D reconstruction performance. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Statement for Equivariant Neural Vector Field ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first introduce the implicit representation, namely the neural vector fields (NVF) [9], and then introduce the equivariant implicit function of this representation. ", "page_idx": 2}, {"type": "text", "text": "Given a sparse point cloud $X\\in\\mathbb{R}^{N_{x}\\times3}$ sampled on a shape $\\mathcal{X}$ , and a query set $Q\\in\\mathbb{R}^{N_{q}\\times3}$ sampled near the surface of $\\mathcal{X}$ , where $N_{x}$ and $N_{q}$ represent the number of input points and query points respectively. A shape $\\mathcal{X}$ is defined as the zero displacement of the implicit function $\\mathcal{F}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\left\\{x\\in\\mathbb{R}^{3}\\left|\\mathcal{F}(x)=\\vec{0}\\right.\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x$ is a point in point cloud $X$ , containing its spatial coordinate. $\\vec{0}$ represents the zero displacement of point $x$ . For a query point $q\\in\\mathbb{R}^{3}$ , the implicit function $\\mathcal{F}$ is formulated by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}(q)=\\Delta q=\\hat{x}-q,\\quad\\mathrm{where}\\;\\hat{x}=\\mathrm{argmin}_{x\\in\\mathcal{X}}\\lVert x-q\\rVert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $\\hat{x}$ is the nearest point of query $q$ on the $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Equivariant Implicit Function). Given an abstract group $G$ , the implicit function $\\mathcal{F}$ based on NVF is equivariant with regard to $G,\\,i f$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\zeta\\circ q)=\\zeta\\circ(\\mathcal{F}(q))=\\Delta q,\\quad\\forall\\zeta\\in G,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $q$ is a query point near or on the surface of shape $\\mathcal{X}$ . In this work, the group $G$ is $S E(3)$ . ", "page_idx": 2}, {"type": "text", "text": "4 Equivariant Neural Implicit Function ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we aim to develop an equivariant implicit function model grounded on neural vector field representations. We achieve this goal by firstly learning patch-level pose-invariant representation (PPIR), and then designing shape-level equivariant implicit representation. The overview of our method is introduced in Section 4.1, with the detailed designs presented in Sections 4.2 and 4.3. ", "page_idx": 2}, {"type": "text", "text": "4.1 Overview of the Basic Idea ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given point cloud $X$ , implicit representations conventionally involve sampling a set of query points $Q$ and employing an implicit function $\\mathcal{F}$ to compute their associated implicit values. Typically, the depiction of a query point $q\\in Q$ depends on its K-nearest neighbors (KNN) in $X$ . As shown in Figure 1, it is observed that some local KNN patches exhibit identical geometric structures if ignoring their pose variations in $S E(3)$ , and the local patches across 3D objects also repeatedly appear. Based on this observation, we design an equivariant implicit function based on patch-level pose-invariant representation, capturing recurring geometric patterns invariant to pose transformation. ", "page_idx": 2}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/a541977f69ebe5d4c19ce35475c8c7e8c56d1accbdb36fbc1a291334fff6fa97.jpg", "img_caption": ["Figure 2: Overview of the proposed PEIF. Given query points, the local patches are selected using KNN. The query/patch pairs are normalized by pose transformations $\\tau$ . The displacements of query points to the surface are predicted by displacement predictor $D$ . The implicit function is equivariant under the $S E(3)$ transformations of the input. Finally, the mesh is generated by marching cubes [1] algorithm. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Before delving into the specific details of our approach, we present the overall framework as shown in Figure 2. Given query set $Q\\ =\\ \\{q_{i}\\}$ , the corresponding patch for $q_{i}$ on point cloud $X$ is $P_{i}=\\bar{\\{p_{i,k}\\}}_{k=0}^{K}$ , i.e., the KNN of $q_{i}$ based on Euclidean distance. The point patch $P_{i}$ and query point $q_{i}$ are firstly normalized by patch-based pose normalization $\\tau_{i}$ , achieving invariant ones under $S E(3)$ transformation of patch $P_{i}$ . We then feed $\\{\\tau_{i}(P_{i}),\\tau_{i}(q_{i})\\}$ to the displacement predictor $D$ for $S E(3)$ -invariant representation learning and displacement prediction. Finally, this predicted displacement is transformed back to the pose of $P_{i}$ with $\\tau_{i}^{-1}$ . The overall displacement prediction can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta q_{i}=\\mathcal{F}(q_{i})=\\tau_{i}^{-1}\\circ D\\circ\\{\\tau_{i}(P_{i}),\\tau_{i}(q_{i})\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This framework is $S E(3)$ -equivariant for patch $P_{i}$ and point cloud $X$ . The detailed design of the patch-based pose-normalization $\\tau$ and displacement predictor $D$ are presented in the following Sections 4.2 and 4.3 respectively. We remove index $i$ for brevity and denote the query point, point patch, and pose-normalization as $q\\in\\mathbb{R}^{3}$ , $P\\in\\mathbb{R}^{K\\times3}$ and $\\tau$ respectively in the following paragraphs. ", "page_idx": 3}, {"type": "text", "text": "4.2 Pose Normalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Geometrically identical patches are expected to maintain consistency across various pose transformations, enabling their representations to complement and reinforce each other. Accordingly, we employ Principal Component Analysis (PCA) to extract the pose-invariant information for patch $P$ . ", "page_idx": 3}, {"type": "text", "text": "We first decenter the patch $P$ by subtracting the points center $\\mu$ , then obtain the rotation matrix $U$ by computing the Singular Value Decomposition (SVD) [45] over the covariance matrix $(P\\!-\\!\\mu)^{\\top}(P\\!-\\!\\mu)$ . The pose-normalized patch $\\bar{P}$ and query point $\\bar{q}$ are derived as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{P}\\triangleq\\tau(P)=(P-\\mu)U,\\quad\\bar{q}\\triangleq\\tau(q)=(q-\\mu)U.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The pose-normalized patch $\\bar{P}$ and query point $\\bar{q}$ are invariant under $S E(3)$ transformation of $P$ , and we take them as input to our displacement predictor $D$ . The prediction $D\\circ\\{\\tau(P),\\tau(q)\\}$ is also invariant as proven in following Lemma 1. Note that we uniquely determine $U$ as [46] to solve the direction uncertainty problem brought by PCA. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 With $\\tau$ as our pose normalization, and $D$ as displacement predictor, $D\\circ\\{\\tau(P),\\tau(q)\\}$ is invariant under $S E(3)$ transformation of $P$ . ", "page_idx": 3}, {"type": "text", "text": "Please refer to the Appendix for proof. The displacement predictor will be introduced as follows. ", "page_idx": 4}, {"type": "text", "text": "4.3 Displacement Predictor Design on Normalized Patches ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Taking the pose-normalized patch $\\bar{P}$ and query $\\bar{q}$ as input, the displacement predictor $D$ is designed to predict the displacement $\\Delta\\bar{q}$ . As shown in Figure 2, predictor $D$ comprises a pose-invariant feature extractor $\\Phi$ and a MLP $\\gamma_{\\theta_{d}}$ . Specifically, the feature extractor $\\Phi$ is composed of three modules: the Spatial Relation Module (SRM) for query point feature learning, which models the spatial relative relationship between $\\bar{q}$ and $\\bar{P}$ ; the Patch Feature Extraction Module (PFEM) for patch feature learning, which extracts patch feature leveraging correlation in feature space; the Intrinsic Patch Geometry Extractor (IPGE), which learns memory-augmented patch representation. ", "page_idx": 4}, {"type": "text", "text": "Spatial Relation Module. We design SRM to learn query point features based on spatial relation within query $\\bar{q}$ and patch $\\bar{P}=\\{\\bar{p}_{i}\\}_{i=1}^{K}$ . Specifically, the point-wise representation $z_{i}$ of point $\\bar{p}_{i}\\in P$ is firstly computed as ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{i}=\\gamma_{\\theta_{s}}(\\bar{p}_{i},\\bar{p}_{i}-\\bar{q}),\\quad i=1,2,\\ldots,K,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\gamma_{\\theta_{s}}(\\cdot)$ is set as MLP. Taking query point position and relative offset as inputs, $z_{i}$ is expected to directly capture the geometric patterns. Then we aggregate $z_{i}$ with simple concatenation operator $\\oplus$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{\\bar{q}}=z_{1}\\oplus\\cdot\\cdot\\oplus z_{K}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Feature $h_{\\bar{q}}\\in\\mathbb{R}^{K\\times D}$ contains the relative feature of query point $\\bar{q}$ to the patch $\\bar{P}$ . We take it as a representation for the query point $\\bar{q}$ . ", "page_idx": 4}, {"type": "text", "text": "Patch Feature Extraction Module. We further design PFEM to learn the patch feature for $\\bar{P}$ . Taking the point positions of $\\bar{q}$ and $\\bar{P}=\\{\\bar{p}_{i}\\}_{i=1}^{K}$ as inputs, we first lift them from Euclidean space to feature space via two MLPs $\\gamma_{\\theta_{p}}(\\cdot)$ and $\\gamma_{\\theta_{q}}(\\cdot)$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\bar{q}}=\\gamma_{\\theta_{q}}(\\bar{q}),\\quad f_{\\bar{p}_{i}}=\\gamma_{\\theta_{p}}(\\bar{p}_{i}),\\quad i=1,2,\\ldots,K,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{\\bar{q}},f_{\\bar{p}_{i}}\\in\\mathbb{R}^{1\\times D}$ are the learned point-wise features. Then, a transformer is designed to obtain the patch feature, by encoding the feature attention between point $\\bar{p}_{i}$ and query point $\\bar{q}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\bar{P}_{w}}\\triangleq\\sum_{i=1}^{K}a_{i}\\cdot\\left(f_{\\bar{p}_{i}}W_{V}\\right),\\quad\\mathrm{where~}\\{a_{i}\\}_{i=1}^{K}=\\mathrm{Softmax}\\left(\\{(f_{\\bar{q}}W_{Q})(f_{\\bar{p}_{i}}W_{O})^{\\top}\\}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $a_{i}$ represents the attention score between query point $\\bar{q}$ and patch points $\\bar{p}_{i}$ . The matrices $W_{Q},W_{O},\\dot{W}_{V}\\in\\mathbb{R}^{D\\times D}$ are learnable parameters. Patch feature $f_{\\bar{P}_{w}}$ is aggregated from all the points features in patch $P$ , while different patches may have diverse point distributions. To mitigate the effects of point density in patches, we further design the importance-aware patch feature $f_{\\bar{P}_{s}}$ by selecting the top- $K_{d}$ important points, and aggregating their features as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\bar{P}_{s}}\\triangleq\\sum_{i=1}^{K_{d}}b_{i}f_{\\bar{p}_{i}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "twhihse rPe $\\{b_{i}\\}_{i=1}^{K_{d}}$ is the selected top- $K_{d}$ attention scores from $\\{a_{i}\\}_{i=1}^{K}$ . The final patch feature $f_{\\bar{P}}$ from ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\bar{P}}=\\lambda_{1}f_{\\bar{P}_{w}}+\\lambda_{2}f_{\\bar{P}_{s}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{1},\\lambda_{2}$ are learnable combination coefficients. ", "page_idx": 4}, {"type": "text", "text": "Intrinsic Patch Geometry Extractor. As discussed in Section 4.1, the normalized point patches can be grouped into different geometric patterns across patches or shapes. To learn the intrinsic features hidden behind those geometric patterns, we propose IPGE to enhance the patch features. Specifically, a learnable multi-head memory bank $\\mathcal{M}=\\{\\mathcal{M}_{i}\\}_{i=1}^{N_{M}}$ is constructed, and it is shared across the whole dataset to implicitly model the patch patterns. Then the patch ", "page_idx": 4}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/4de67de08b5881476d6ea9951be4a278af964e2ce3daecef8a20856c0d314eca.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Feature enhancement with IPGE. feature $f_{\\bar{P}}$ is enhanced by querying and aggregating each memory item as ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{\\bar{P}}=\\sum_{i=1}^{N_{M}}w_{i}\\mathcal{M}_{i},\\quad w_{i}=\\mathrm{Softmax}(f_{\\bar{P}}\\mathcal{M}_{i}^{\\top}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The memory weight $w_{i}$ is defined as softmax-normalized similarity vectors between query feature $f_{\\bar{P}}$ and the entries of $\\bar{\\mathcal{M}}_{i}\\in\\mathbb{R}^{C\\times D}$ . Figure 3 illustrates the procedures of this module. ", "page_idx": 5}, {"type": "text", "text": "Displacement Prediction. Based on query point features $\\{h_{\\bar{q}},f_{\\bar{q}}\\}$ , point-wise patch feature $\\{f_{\\bar{p}_{i}}\\}_{i=1}^{K}$ enhanced patch feature $g_{\\bar{P}}$ , the patch-level pose-invariant (PPIR) representation is achieved by ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{P P I R}=\\gamma_{\\theta_{a}}\\left(h_{\\bar{q}}\\oplus f_{\\bar{q}}\\oplus\\{f_{\\bar{p}_{i}}\\}\\right)\\oplus g_{\\bar{P}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then the displacement for query point $\\bar{q}$ can then be derived by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\bar{q}=\\gamma_{\\theta_{d}}\\left(f_{P P I R}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Both $\\gamma_{\\theta_{d}}$ and $\\gamma_{\\theta_{a}}$ are set as MLPs. Finally, $\\Delta\\bar{q}$ is transformed back with pose denormalization, i.e., the inverse transformation of $\\tau$ , achieving the final $S E(3)$ -equivariant displacement estimation $\\Delta q\\,=\\,\\tau^{-1}(\\Delta\\bar{q})$ . The $S E(3)$ -invariance of $f_{P P I R}$ can be found in Lemma 1, and the $S E(3)$ - equivariance of learned implicit representation can be found in the following Theorem 1, please refer to Appendix for proof. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 Given query point $q$ and patch $P$ , implicit function $\\mathcal{F}(\\boldsymbol{q})$ is $S E(3)$ -equivariant. ", "page_idx": 5}, {"type": "text", "text": "4.4 Network Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Sections 4.2 and 4.3 illustrate how to obtain the displacement for a query point, where the trained parameters include the parameters $\\theta_{s},\\theta_{p},\\theta_{q},\\theta_{a},$ $\\theta_{d}$ of five MLPs, the multi-head memory bank $\\mathcal{M}$ and parameters $\\lambda_{1},\\lambda_{2}$ . To optimize the implicit function $\\mathcal{F}$ , we design a joint loss function over the query set $Q$ to train our method in an end-to-end manner. ", "page_idx": 5}, {"type": "text", "text": "Displacement Optimization Loss. We compute $L_{1}$ -loss between the predicted displacement $\\Delta q_{i}$ for each query point $q_{i}\\in Q$ and its ground-truth displacement $\\Delta\\hat{q}_{i}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d}=\\frac{1}{N_{q}}\\sum_{i=1}^{N_{q}}|\\Delta q_{i}-\\Delta\\hat{q}_{i}|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Patch Discrimination Loss. The items in memory should be apart from each other to enhance the representativeness of the memory items. To ensure this, we design the patch discrimination loss as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}=\\sum_{i=1}^{N_{M}}\\sum_{m\\neq m^{\\prime}}^{N_{M_{i}}}\\frac{\\operatorname*{max}(\\langle\\mathcal{M}_{i}[m],\\mathcal{M}_{i}[m^{'}]\\rangle,0)}{N_{M_{i}}(N_{M_{i}}-1)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is similar to cosine embedding loss [47] with a margin set to 0. $N_{M_{i}}$ is the number of the items in memory bank $\\mathcal{M}_{i}$ . The overall loss function is finally written as $\\dot{\\mathcal{L}}=\\dot{\\mathcal{L}}_{d}+\\beta\\dot{\\mathcal{L}}_{m}$ , where $\\beta$ is a hyper-parameter for balancing the two terms. ", "page_idx": 5}, {"type": "text", "text": "3D Reconstruction in Inference Stage. We employ the Marching Cubes (MC) algorithm proposed by MeshUDF [48], which can reconstruct surfaces on UDFs. We first discretize the 3D volume into a 3D grid with a resolution of $N_{R}$ , resulting in $N_{R}^{3}$ grid points as the query set $Q$ . Then, we use the implicit function $\\mathcal{F}$ to predict the displacement $\\Delta q$ of each query point $q$ . Similar to [9], we get the UDF value and gradient of q as d = \u2225\u2206q\u22252 and \u2207q =\u2225\u2206\u2206qq\u22252 . Based on $d$ and $\\nabla_{q}$ , the MC in MeshUDF [48] can reconstruct the surface of the input point cloud as mesh. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Implementation Details. We implement our PEIF in Pytorch [49] using Adam optimizer [50]. The learning rate is $8\\times10^{-4}$ . For each query point, the size of the neighborhood is set as $K=32$ for ShapeNet [51] and ABC [52] datasets, $K=54$ for Synthetic Rooms [19] dataset. We set $\\beta=0.1$ in the training loss and $N_{m}=4$ for the memory bank. Please refer to the Appendix for details on the structures of involved MLPs, and the effect of different values of $\\beta$ . We conducted all experiments on one NVIDIA RTX 4090 GPU. ", "page_idx": 5}, {"type": "text", "text": "Datasets. We experiment on four datasets including ShapeNet [51], ABC [52], Synthetic Rooms [19], MGN [53]. (1) ShapeNet [51], as pre-processed by [7], contains watertight meshes of shapes in ", "page_idx": 5}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/130858582beaf0af2a62e2e8b90c067d0b3d6c479f43e2e1959955a9a3217deb.jpg", "table_caption": ["Table 1: The reconstruction results of ShapeNet [51]. All models are trained on the base classes and evaluated on both the base classes and novel classes. Note that E-GraphONet is the equivariant version of GraphONet. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 4: The qualitative results of ShapeNet [51] dataset. The object is selected from meshes used for class-unseen reconstruction (novel classes in Table 1). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "13 classes. Following the experimental setting in [9], we select cars, chairs, planes, and tables as base classes in Table 1, and speakers, bench, lamps, and watercraft as novel classes in Table 1 for category-unseen reconstruction, only for testing. (2) ABC [52] has one million CAD models, mainly mechanical objects. We use the splits from [54] and select watertight meshes for experiments: 3599/883/98 shapes for training/validation/testing. (3) Synthetic Rooms [19] contains $5\\mathrm{k}$ synthetic room scenes composed of random walls, floors, and ShapeNet objects. We adopt the same train/validation/test division in [19]. (4) MGN [53] is a real scanned dataset containing 5 clothing categories. To generate watertight surfaces, we employ the method [55] for preprocessing. Specifically, we sample $3\\mathbf{k}$ points on the surface as input points for ShapeNet and ABC datasets, while 10k input points for Synthetic Rooms. Then 2048 query points are sampled near the surface for ShapeNet, ABC, and Synthetic Rooms. All experiments are tested on $10\\mathbf{k}$ points. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We use the Chamfer-L1 distance (CD, $\\times10^{-2}$ ), Earth Mover Distance (EMD, $\\times10^{-2})$ ), Normal Consistency (NC), and F-Score (with threshold value $1\\%$ ) metrics for our evaluation. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To evaluate the effectiveness of our methods, baselines used for comparison include the equivariant network E-GraphONet [24], and six non-equivariant networks, including POCO [10], GIFS [12], ALTO [20], NVF [9], GeoUDF [7], GridFormer[11]. For fairness, we trained these networks from scratch under the same training/validation/testing dataset splitting. ", "page_idx": 6}, {"type": "text", "text": "5.1 Results and Comparisons ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3D Object Datasets Reconstruction. We first report the results of the 3D object reconstruction on the object datasets: ShapeNet [51] and ABC [52]. The quantitative results on base and novel classes of ShapeNet [51] are shown in Table 1, our PEIF achieves better results both in base and novel classes, especially in terms of the CD and F-Score metrics. Qualitative comparisons are provided in Figure 4. Compared with other competitors, our method can capture fine-grained details, and the overall topology of the shape is more consistent. Additional instances are provided in the Appendix (Figure 8). We further evaluate the compared methods on the ABC [52] dataset. The quantitative results in Table 2 demonstrate that our PEIF achieves competitive performance compared to both equivariant and non-equivariant methods. Visualizations are provided in Figure 9 of the Appendix. ", "page_idx": 6}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/4018dd1e02b6dbd7365dca292a4a284cf7e333b65ff961f1f37dec3374e12d27.jpg", "table_caption": ["Table 2: Comparison of different methods on ABC [52] and Synthetic Rooms [19] datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3D Scene Datasets Reconstruction. Table 2 shows the quantitative results on the Synthetic Rooms dataset. Our PEIF shows state-of-the-art performance under all quantitative metrics. The competitive competitors such as GridFormer [11] and POCO [10] produce smooth but incomplete surfaces. Other methods like GeoUDF [7] and NVF [9] produce results with rough surfaces. In contrast, our method reconstructs relatively smooth surfaces with fewer issues of completeness and consistency. Visualizations are provided in Figure 10 of the Appendix. ", "page_idx": 7}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/20c21c9bb96efb4a3910a300fe3ec5bbbe39e18e32859befbd3ff8ab9fb869a5.jpg", "img_caption": ["Figure 5: The visual example of cross-domain evaluation on the real scanned dataset MGN [53], where the model is pre-trained on Synthetic Rooms dataset [19]. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Cross-domain Evaluation on Realworld Dataset. We test and compare our method with the state-ofthe-art methods NVF, GoeUDF, and GridFormer on MGN [53] dataset using the trained models on Synthetic Rooms [19]. Table 3 shows that the compared methods generally exhibit a declined performance in the pres", "page_idx": 7}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/2bd76265fdf4dc9f996e536bc234a8920d257ee3ea280839fb0d0fae7f3e48a2.jpg", "table_caption": ["Table 3: The cross-domain evaluation on MNG dataset [53]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "ence of a synthetic-real domain gap. However, in the presence of such a domain gap, our PEIF still achieves notable performance under all metrics. Figure 5 displays the visual comparison. The competitors either produce a rough surface or suffer from shape incompleteness. As a comparison, our PEIF reconstructs a complete surface with fine-grained details. These results show that our PEIF trained on the synthetic data can be well generalized to real scenarios. The reason might be that the pose-normalized patches are the basic elements for composing different shapes, and our PEIF is based on the pose-invariant patch representations. ", "page_idx": 7}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/0d1f9d6b806ce47739cc90626b9405cf403901a23954a68368fe3eaf9dfb1cd0.jpg", "img_caption": ["Figure 6: Visual results before (top) and after (down) arbitrary rotations. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/ab85527f2d15203a6ecdaa1969fe17ea1bf8228959060d532c9c5f4ed2118804.jpg", "table_caption": ["Table 4: Performance under Arbitrary $S O(3)$ rotations. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/0e9e4a8c3d47166741bf65e6e238c6d743a84ad34d0f2feaf46fa91e193bb2df.jpg", "table_caption": ["Table 5: Ablation study on ABC [52] dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/0f1ce90e7a77c5d1c3a55e96b88554ecf1076aa54f1e3ae324433ce4459ffa9d.jpg", "table_caption": ["Table 6: Model size and inference time. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Robustness to rotations. We compare the robustness of two equivariant networks (PEIF and EGraphONet), and two non-equivariant networks (NVF and GeoUDF) to arbitrary rotations. All methods are trained with canonical pose and tested with arbitrary rotations on the ABC [52] dataset. The quantitative and qualitative results are reported in Table 4 and Figure 5, respectively. In Table 4, \"w/ rotation\" and \"w/o rotation\" represent that the testing input point cloud is with and without arbitrary rotation, respectively. The visual results presented in Figure 5 illustrate that our PEIF can retain stable performance under arbitrary rotations, which is consistent with the numerical results presented in Table 4. These results justify the robustness of our PEIF to arbitrary rotations. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Study and Model Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies and present the model size and inference time on the ABC [52] dataset. ", "page_idx": 8}, {"type": "text", "text": "Effect of Pose Normalization. As shown in the 2nd row in Table 5, after removing the patch-level pose normalization, all metrics decline. Particularly, the NC metric is notably affected. ", "page_idx": 8}, {"type": "text", "text": "Effect of the Multi-head Memory Bank. As demonstrated in Table 5, the performance of our PEIF deteriorates significantly when the multi-head memory bank, i.e., the intrinsic patch geometry extractor, is removed. The model performs better with $N_{M}$ increase from 1 to 4. When $N_{M}=5$ , the performance of the model starts to deteriorate. ", "page_idx": 8}, {"type": "text", "text": "Number of Neighbour Points $K$ . The size of KNN determines the number of points in each patch. We report the performance of our PEIF with different patch sizes in Table 5. The results show that our method is relatively stable to the size of KNN. ", "page_idx": 8}, {"type": "text", "text": "Model Size and Computational Time. We compare model size and inference time on the ABC [52] dataset. In Table 6, our network is comparable to other methods in the number of parameters. The computation time of our approach for 3D reconstruction of one point cloud is lower than the state-ofthe-art models NVF, and GeoUDF, but higher than GrridFormer and E-GraphONet. However, our method achieves the best accuracy for 3D reconstruction as shown in experiments. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we propose a patch-level equivariant implicit function, based on the patch-level pose invariant feature representation over the pose-normalized query points and corresponding neighboring patches. The proposed representation achieves promising results in 3D reconstruction both quantitatively and qualitatively, and generates shapes with better geometry details and robustness to $S E(3)$ transforms. Due to the flexibility of the patch-based representation, in the future, we plan to extend this approach to larger-scale 3D reconstruction of real scans. Additionally, our patch-based pose invariant representation can be taken as a foundation network for pre-training, followed by fine-tuning on few-shot examples. ", "page_idx": 8}, {"type": "text", "text": "Limitation. As an implicit network, one limitation is that PEIF relies on the query points and estimating the displacement point-wisely. For scaling up to a larger scale, we plan to utilize a multi-scale technique and importance sampling of query points for efficient displacement field estimation. ", "page_idx": 8}, {"type": "text", "text": "Impact Statement. This work aims to advance the field of equivariant deep learning with applications in 3D surface reconstruction. It may be valuable to the research of equivariant implicit neural representation and geometric modeling of 3D shapes and has no ethical concerns as far as we know. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Key RD Program 2021YFA1003002, NSFC 12125104, U20B2075, 12326615, 62306167, Shandong Province Natural Science Foundation ZR2024QA161. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347\u2013353. 1998.   \n[2] Hugues Hoppe. Poisson surface reconstruction and its applications. In Proceedings of the 2008 ACM symposium on Solid and physical modeling, pages 10\u201310, 2008.   \n[3] Philipp Mittendorfer and Gordon Cheng. 3d surface reconstruction for robotic body parts with artificial skins. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4505\u20134510. IEEE, 2012.   \n[4] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 165\u2013174, 2019.   \n[5] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and Richard Newcombe. Deep local shapes: Learning local sdf priors for detailed 3d reconstruction. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIX 16, pages 608\u2013625, 2020.   \n[6] Julian Chibane, Gerard Pons-Moll, et al. Neural unsigned distance fields for implicit function learning. Advances in Neural Information Processing Systems, 33:21638\u201321652, 2020.   \n[7] Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, and Wenping Wang. Geoudf: Surface reconstruction from 3d point clouds via geometry-guided distance representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14214\u201314224, 2023.   \n[8] Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Yi Fang, and Zhizhong Han. Learning consistencyaware unsigned distance functions progressively from raw point clouds. Advances in Neural Information Processing Systems, 35:16481\u201316494, 2022.   \n[9] Xianghui Yang, Guosheng Lin, Zhenghao Chen, and Luping Zhou. Neural vector fields: Implicit representation by explicit learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16727\u201316738, 2023.   \n[10] Alexandre Boulch and Renaud Marlet. Poco: Point convolution for surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6302\u20136314, 2022.   \n[11] Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, and Ming Gu. Gridformer: Point-grid transformer for surface reconstruction. arXiv preprint arXiv:2401.02292, 2024.   \n[12] Jianglong Ye, Yuntao Chen, Naiyan Wang, and Xiaolong Wang. Gifs: Neural implicit function for general shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12829\u201312839, 2022.   \n[13] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5939\u20135948, 2019.   \n[14] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4460\u20134470, 2019.   \n[15] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for vision-based 3d semantic occupancy prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9223\u20139232, 2023.   \n[16] Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao, Jose M Alvarez, Sanja Fidler, Chen Feng, and Anima Anandkumar. Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9087\u20139098, 2023.   \n[17] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, and Jiwen Lu. Surroundocc: Multi-camera 3d occupancy prediction for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21729\u201321740, 2023.   \n[18] Yunpeng Zhang, Zheng Zhu, and Dalong Du. Occformer: Dual-path transformer for visionbased 3d semantic occupancy prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9433\u20139443, 2023.   \n[19] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages 523\u2013540. Springer, 2020.   \n[20] Zhen Wang, Shijie Zhou, Jeong Joon Park, Despoina Paschalidou, Suya You, Gordon Wetzstein, Leonidas Guibas, and Achuta Kadambi. Alto: Alternating latent topologies for implicit 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 259\u2013270, 2023.   \n[21] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1\u2013 15, 2022.   \n[22] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4369\u20134379, 2023.   \n[23] Jongmin Lee, Byungjin Kim, Seungwook Kim, and Minsu Cho. Learning rotation-equivariant features for visual correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21887\u201321897, 2023.   \n[24] Yunlu Chen, Basura Fernando, Hakan Bilen, Matthias Nie\u00dfner, and Efstratios Gavves. 3d equivariant graph implicit functions. In European Conference on Computer Vision, pages 485\u2013502. Springer, 2022.   \n[25] Ruixuan Yu and Jian Sun. Pose-transformed equivariant network for 3d point trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[26] Yiyang Chen, Lunhao Duan, Shanshan Zhao, Changxing Ding, and Dacheng Tao. Localconsistent transformation learning for rotation-invariant point cloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[27] Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12200\u201312209, 2021.   \n[28] Chunghyun Park, Seungwook Kim, Jaesik Park, and Minsu Cho. Learning so(3)-invariant correspondence via point-wise local shape transform. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[29] Alexandre Agm Duval, Victor Schmidt, Alex Hern\u00e1ndez-Garc\u0131a, Santiago Miret, Fragkiskos D Malliaros, Yoshua Bengio, and David Rolnick. Faenet: Frame averaging equivariant gnn for materials modeling. In International Conference on Machine Learning, pages 9013\u20139033. PMLR, 2023.   \n[30] Omri Puny, Matan Atzmon, Edward J Smith, Ishan Misra, Aditya Grover, Heli Ben-Hamu, and Yaron Lipman. Frame averaging for invariant and equivariant network design. In International Conference on Learning Representations, 2021.   \n[31] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so (3) equivariant representations with spherical cnns. In Proceedings of the European Conference on Computer Vision (ECCV), pages 52\u201368, 2018.   \n[32] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.   \n[33] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970\u20131981, 2020.   \n[34] V\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International Conference on Machine Learning, pages 9323\u20139332, 2021.   \n[35] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equivariant graph mechanics networks with constraints. In International Conference on Learning Representations, 2021.   \n[36] Kristof Sch\u00fctt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377\u20139388, 2021.   \n[37] Fang Wu and Stan Z Li. Diffmd: a geometric diffusion model for molecular dynamics simulations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5321\u20135329, 2023.   \n[38] Yangtian Zhang, Huiyu Cai, Chence Shi, and Jian Tang. E3bind: An end-to-end equivariant network for protein-ligand docking. In The Eleventh International Conference on Learning Representations, 2022.   \n[39] Chen Chen, Xiao Chen, Alex Morehead, Tianqi Wu, and Jianlin Cheng. 3d-equivariant graph neural networks for protein model quality assessment. Bioinformatics, 39(1):btad030, 2023.   \n[40] Rahmatullah Roche, Bernard Moussad, Md Hossain Shuvo, and Debswapna Bhattacharya. E (3) equivariant graph neural networks for robust and accurate protein-protein interaction site prediction. PLoS Computational Biology, 19(8):e1011435, 2023.   \n[41] Haojie Huang, Dian Wang, Robin Walters, and Robert Platt. Equivariant transporter network. In Robotics: Science and Systems, 2022.   \n[42] Dian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, and Robert Platt. On-robot learning with equivariant models. In Conference on Robot Learning, pages 1345\u20131354. PMLR, 2023.   \n[43] Xupeng Zhu, Dian Wang, Guanang Su, Ondrej Biza, Robin Walters, and Robert Platt. On robot grasp learning using equivariant models. Autonomous Robots, 47(8):1175\u20131193, 2023.   \n[44] Jiahui Lei, Congyue Deng, Karl Schmeckpeper, Leonidas Guibas, and Kostas Daniilidis. Efem: Equivariant neural field expectation maximization for 3d object segmentation without scene supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4902\u20134912, 2023.   \n[45] Gene H Golub and Christian Reinsch. Singular value decomposition and least squares solutions. In Handbook for Automatic Computation: Volume II: Linear Algebra, pages 134\u2013151. Springer, 1971.   \n[46] Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo Cao, and Xin Li. Rotation invariant point cloud analysis: Where local geometry meets global topology. Pattern Recognition, 127:108626, 2022.   \n[47] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5265\u20135274, 2018.   \n[48] Benoit Guillard, Federico Stella, and Pascal Fua. Meshudf: Fast and differentiable meshing of unsigned distance field networks. In European Conference on Computer Vision, pages 576\u2013592. Springer, 2022.   \n[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.   \n[50] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[51] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.   \n[52] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9601\u20139611, 2019.   \n[53] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-garment net: Learning to dress 3d people from images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5420\u20135430, 2019.   \n[54] Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu. Aro-net: Learning implicit fields from anchored radial observations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3572\u20133581, 2023.   \n[55] Jingwei Huang, Hao Su, and Leonidas Guibas. Robust watertight manifold surface generation method for shapenet models. arXiv preprint arXiv:1802.01698, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 1. With $\\tau$ as our pose normalization, and $D$ as displacement predictor, $D\\circ\\{\\tau(P),\\,\\tau(q)\\}$ is invariant under $S E(3)$ transformation of $P$ . ", "page_idx": 13}, {"type": "text", "text": "Assume that $P$ , $Y\\ \\in\\mathbb{R}^{K\\times3}$ are two point patches with $Y\\,=\\,\\zeta(P),\\;\\zeta\\,\\in\\,S E(3)$ , and $\\zeta(P)\\;=$ $P R+T$ is the $S E(3)$ transformation of $P$ with rotation matrix $R$ and translation vector $T$ . Denote the patch centers of $P,Y$ are $\\mu,\\nu$ respectively, and their corresponding PCA-normalization are $\\tau_{P}({\\dot{P}})=(P-\\mu)U$ , $\\tau_{Y}(Y)=(Y-\\nu)V$ . To prove Lemma 1, we firstly prove the uniqueness of pose-normalization, then prove the $S E(3)$ -invariance of $D\\circ\\{\\tau(P),\\,\\tau(q)\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Step 1. Uniqueness of $P C A$ -normalization $\\tau$ . According to the definition of $\\tau_{P}(P)=(P-\\mu)U$ , the patch center $\\mu$ is uniquely computed as patch centroid, while rotation matrix $U$ is computed by SVD over the covariance matrix $(\\dot{P^{\\mathrm{~\\,~}}}-\\mu)^{\\top}(\\dot{P}-\\mu)$ . The vector elements of matrix $U=\\{u_{i}\\}_{i=1}^{3}$ may change their directions and result in eight rotation matrix $\\hat{U}=\\{\\pm u_{i}\\}_{i=1}^{3}$ , which bring uncertainty for pose-normalization $\\tau_{P}$ . To uniquely determine $U$ , we follow [46] to determine a single direction for every $\\{u_{i}\\}_{i=1}^{3}$ by estimating their angle with a predefined anchor point $y$ (the vector from the farthest point of the patch to the patch center). The direction of $u_{i}$ should be flipped if the corresponding angle is larger than $90^{\\circ}$ . Specifically, if $\\langle u_{i},\\;y\\rangle>0$ , we take $u_{i}$ as one vector of $U$ . If $\\langle u_{i},\\ y\\rangle<0$ , we take $-u_{i}$ instead. If $\\langle u_{i},\\ y\\rangle=0$ , we take another point $y^{\\prime}$ (e.g., the second farthest point from patch center) that satisfies $\\langle u_{i},\\;y^{\\prime}\\rangle\\ne0$ as a new anchor point to determine the $U$ . By this strategy, the rotation matrix $U$ is uniquely determined and the PCA-normalized $\\tau_{P}(P)$ is uniquely determined. ", "page_idx": 13}, {"type": "text", "text": "Step 2. $S E(3)$ -invariance of $D\\circ\\{\\tau_{P}(P),\\ \\tau_{P}(q)\\}.$ . For point patch $P\\;=\\;\\{p_{i}\\}$ and its $S E(3)$ transformed patch $Y=\\{y_{i}|\\;y_{i}=p_{i}\\dot{R}+\\dot{T}\\}$ , with their corresponding centers $\\mu,\\nu$ computed by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu=\\frac{1}{K}\\sum_{i=1}^{K}p_{i},\\quad\\nu=\\frac{1}{K}\\sum_{i=1}^{K}y_{i},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nu=\\!\\frac{1}{K}\\sum_{i=1}^{K}y_{i}=\\frac{1}{K}\\sum_{i=1}^{K}(p_{i}R+T)=\\left(\\frac{1}{K}\\sum_{i=1}^{K}p_{i}\\right)R+T=\\mu R+T.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then it is obvious ", "page_idx": 13}, {"type": "equation", "text": "$$\nY-\\nu=P R+T-(\\mu R+T)=P R-\\mu R=(P-\\mu)R,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which means $Y-\\nu$ is an orthogonal transformation of $P-\\mu$ , thus $Y-\\nu$ has same singular values as $P-\\mu$ when we conduct SVD on their corresponding covariance matrices, $i.e.$ ., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U\\Lambda U^{\\top}=(P-\\mu)^{\\top}(P-\\mu),}\\\\ &{V\\Lambda V^{\\top}=(Y-\\nu)^{\\top}(Y-\\nu)}\\\\ &{\\phantom{U\\Lambda U^{\\top}=\\;}=[(P-\\mu)R]^{\\top}[(P-\\mu)R]}\\\\ &{\\phantom{U\\Lambda U^{\\top}=\\;}=R^{\\top}[(P-\\mu)^{\\top}(P-\\mu)]R}\\\\ &{\\phantom{U\\Lambda U^{\\top}=\\;}=R^{\\top}(U\\Lambda U^{\\top})R}\\\\ &{\\phantom{U\\Sigma U^{\\top}=\\;}=(R^{\\top}U)\\Lambda(R^{\\top}U)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Eqn. (20) holds for any rotation matrix $R$ , and recalling that we uniquely conduct SVD as proved in Step 1, we derive ", "page_idx": 13}, {"type": "equation", "text": "$$\nV=R^{\\top}U.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, according to the definition of our PCA-normalization and Eqns. (19,21), it is obvious that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tau_{Y}(Y)=(Y-\\nu)V=(P-\\mu)R R^{\\top}U=(P-\\mu)U=\\tau_{P}(P),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which means $\\tau_{P}(P)$ is $S E(3)$ -invariant. The $S E(3)$ -invariance of $\\tau_{P}(q)$ can be proven similarly. Taking the $S E(3)$ -invariant $\\tau_{P}(P)$ , $\\tau_{P}(q)$ as input, our displacement predictor $D$ will produce $S E(3)$ -invariant output, i.e., $D\\circ\\{\\tau_{P}(P)$ $\\{\\bar{\\tau_{P}}(P),\\,\\bar{\\tau_{P}}(q)\\}\\stackrel{\\cdot}{=}D\\circ\\{\\tau_{\\zeta(\\bar{P})}(\\zeta(P)),\\,\\bar{\\tau_{\\zeta(P)}}(\\zeta(q))\\},\\forall\\zeta\\in S E(3)$ ", "page_idx": 13}, {"type": "text", "text": "Table 7: The time to process 10,000 points on the ABC [52] dataset using one NVIDIA 4090 GPU. ", "page_idx": 14}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/0bfce6524c91db4a5de4239fdf97a1872229ba325084c377e9b97c6e4ed7eed4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 1 Given query point q and patch $P$ , implicit function $\\mathcal{F}(\\boldsymbol{q})$ is $S E(3)$ -equivariant. ", "page_idx": 14}, {"type": "text", "text": "Following the denotations in the proof of Lemma 1, we denote the query point of patch $P,\\;Y$ as $p_{1},p_{2}$ , and we further denote the predicted $S E(3)$ -invariant displacements for query point $p_{1},p_{2}$ as $p_{1}^{*},p_{2}^{*}$ , and the final prediction of $p_{1},p_{2}$ are respectively ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{F}(p_{1})=\\tau_{p_{1}}^{-1}(p_{1}^{*})=p_{1}^{*}U^{\\top}+\\mu,\\;\\;\\;\\mathcal{F}(p_{2})=\\tau_{p_{2}}^{-1}(p_{2}^{*})=p_{2}^{*}V^{\\top}+\\nu.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Eqns. (19,21), we can derive ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}({p_{2}})={p_{2}^{*}}V^{\\top}+\\nu}\\\\ &{\\quad\\quad\\quad=p_{1}^{*}(R^{\\top}U)^{\\top}+(\\mu R+T)}\\\\ &{\\quad\\quad\\quad=(p_{1}^{*}U^{\\top}+\\mu)R+T}\\\\ &{\\quad\\quad\\quad=\\mathcal{F}(p_{1})R+T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means $\\mathcal{F}(p_{1})$ is equivariant under $S E(3)$ transformation of point patch $P$ . ", "page_idx": 14}, {"type": "text", "text": "B Architecture Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "MLPs. $\\gamma_{\\theta_{s}}$ in Eqn. ( 7) consists of four $1\\times1$ convolution layers with 6, 32, 64, and 128 hidden units. $\\gamma_{\\theta_{q}}$ and $\\gamma_{\\theta_{q}}$ in Eqn. (8) consists of $1\\times1$ convolution layers with 3, 32, 64, and 128 hidden units while these for $\\gamma_{\\theta_{p}}$ are 3, 32, 64, and 128. For $\\gamma_{\\theta_{a}}$ in Eqn. ( 14), the unit numbers are 256, 512, 256, and 384. For $\\gamma_{\\theta_{d}}$ , the unit numbers are 256, 256, 256, and 256. All feature dimensions are 128. For the multi-head memory bank $\\mathcal{M}$ , the number of the memory bank is set as $N_{M}=4$ , with each memory bank containing 596 items. ", "page_idx": 14}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 The visualization of the learned memory bank ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Figure 7, we provided two approaches to visualize the learned memory bank. ", "page_idx": 14}, {"type": "text", "text": "(1) We visualize the set of point patches with the highest weights to the corresponding element of the memory bank (the weights are computed by Eqn. (12). These patches are highlighted by colors in these examples. It shows that the patches with high weights to each element of memory have similar geometry structures. ", "page_idx": 14}, {"type": "text", "text": "(2) We further visualize (by t-SNE) the features of point patches with the highest weights (Eqn. (12)) to different elements of the learned memory bank, rendered by different colors. It shows that the patches with high weights assigned to different elements of the learned memory bank have clustered features in the feature space. ", "page_idx": 14}, {"type": "text", "text": "C.2 A detailed analysis of the inference time ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 7, we report the time consumption of each operator in PEIF to process 10,000 query points. Specifically, the operations include SVD (Singular Value Decomposition), PE (Point-wise Feature Extraction), SRM (Spatial Relation Module), PFEM (Patch Feature Extraction Module), IPGE (Intrinsic Patch Geometry Extractor) and Others (other Conv layers). ", "page_idx": 14}, {"type": "text", "text": "C.3 Computational cost ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report the computational cost in Table 8, including the training time per epoch, training memory, testing time per 3D shape, and testing memory cost on the ABC dataset. Methods of GeoUDF and ", "page_idx": 14}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/47520c9950195a93bacef16cc1430cb31f94d05ceed06b3b5f5283733f4ddbfc.jpg", "img_caption": ["Figure 7: The visualization of the learned memory bank. (a)-(c) Visualization of patches corresponding to the same memory item with the highest weights (Different colors represent different items). (d) t-SNE visualization of patch features. The patches with the highest weights to different memory items are highlighted in different colors. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/4df02840a5dc223c8b270440447bef58d0a27fa0f64c782428df021b8dcd7a46.jpg", "table_caption": ["Table 8: The comparison of computational cost. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "GridFormer include two stages of upsampling/reconstruction and reconstruction/refinement. We report the computation cost of them in each table cell with two values (denoted as $\\cdot+\\cdot)$ , respectively representing the costs for each stage. ", "page_idx": 15}, {"type": "text", "text": "C.4 Additional results on degraded data ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We evaluate the performance of our PEIF on different data degradations (sparse, noisy, and partial) on the ABC [52] dataset. In the following experiments, the test input point clouds with different degradations, and the results are reported in Tables 9-11. ", "page_idx": 15}, {"type": "text", "text": "Sparse point cloud. In previous experiments, all the compared methods use the same number of input points (10k) for each shape in testing, as NVF. We randomly select a subset of input points as input, and the results are in Table 9. ", "page_idx": 15}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/7afab01e62f0f8450357ded80cacf0738e4172ac061e4a9e98daa751dadd39aa.jpg", "table_caption": ["Table 9: The reconstruction results of sparse data on the ABC [52] dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Noisy point cloud. We plugged Gaussian noise with standard deviation $(\\sigma)$ as 0.005 and 0.01 to the input points. The results are reported in Table 10. ", "page_idx": 16}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/f7362b580a2bf9d6a204c9a63ef7c2a3e1c38a33ef107ec5bb65007e79e02987.jpg", "table_caption": ["Table 10: The reconstruction results from noisy input on the ABC [52] dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Partial point cloud. We remove a fraction (with ratio $p$ ) of the input points to form a partial point cloud. Specifically, we use the farthest point sampling to select a set of center points and remove their KNN points to ensure the sampling fraction. The results are reported in Table 11. ", "page_idx": 16}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/cc1536b2c48e5235e57bb3fb58fc7267ab4bf4e5c1e02aeb37ca9c8481c4acd6.jpg", "table_caption": ["Table 11: The reconstruction results from partial points on the ABC [52] dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.5 Additional ablation study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Ablation study in ABC [52]. In Table 13, we present the additional results of our ablation studies with respect to the coefficient $\\beta$ of the joint loss function in Section 4.4, the impact of keypoint-boosted feature representation $f_{\\bar{P}_{s}}$ in Section 4.3. ", "page_idx": 16}, {"type": "text", "text": "Ablation study in MGN [53]. In Table 12, we present the additional ablation results of $K$ on MGN dataset. The testing results in Table 3 on the MNG dataset using the trained model with $K\\,=\\,54$ on the Synthetic Rooms dataset. As shown ", "page_idx": 16}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/78674a6f78aae230ad7880b15c6dfbaf49e3879138c4bf192cc3938c7a899237.jpg", "table_caption": ["Table 12: The impact of $K$ when training on Synthetic Rooms and testing on MGN [53]. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "in Table 12, when changing $K$ to 48 and 32, the test results using the corresponding $K$ on MGN are stable. ", "page_idx": 16}, {"type": "text", "text": "C.6 Additional Visualizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide more visual results of our PEIF across four datasets: ShapeNet, ABC, Synthetic Rooms, and MGN. ", "page_idx": 16}, {"type": "text", "text": "ShapeNet [51]. We present three examples to further illustrate the performance of our PEIF. The 1st row in Figure 8 are results of examples selected from base classes. In contrast, results in the 2nd and 3rd rows come from the novel classes in this dataset. The visualization results show that our PEIF not only preserves local structures but also demonstrates a remarkable generalization capability. ", "page_idx": 16}, {"type": "text", "text": "ABC [52]. We validate the efficacy of our multi-head memory bank on this dataset, with visualizations depicted as shown in Figure 9. By using the intrinsic patch geometry extractor with a multi-head memory bank, our PEIF reconstructs better 3D surfaces which are more smooth and complete. ", "page_idx": 16}, {"type": "text", "text": "Synthetic Rooms [19]. Beyond the reconstruction of CAD objects, we also validate the performance of our PEIF on a synthesized scene dataset, thereby extending its applicability to more complex environmental contexts. The results in Figure 10 demonstrate that our PEIF achieves competitive results in terms of structural completeness and surface smoothness. ", "page_idx": 16}, {"type": "table", "img_path": "aXS1pwMa8I/tmp/631a51d379fdc3d336e45eda61952b6bbbc147c06202f6fcb9307a025e29000f.jpg", "table_caption": ["Table 13: Additional ablation study in ABC [52]. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/942ac7dbd3074d1f201ee90d7da70c94ef788c6adf303259563f0bbffc796c1c.jpg", "img_caption": ["Figure 8: Results on ShapeNet [51] dataset. The results from the 1st row are selected from base classes in Table 1. Objects in the 2nd to 3rd rows are selected from meshes used for class-unseen reconstruction (novel classes in Table 1). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/03629c567e798a0c0e2540ca6ca339385b5f91112b91e32601b546b6128940d5.jpg", "img_caption": ["Figure 9: Examples of results on ABC [52] dataset with (below) and without (top) intrinsic patch geometry extractor using multi-head memory bank. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "MGN [53]. To evaluate the generalization of competing methods, we conducted tests on the real scanned data, and the corresponding results are presented in Figure 11. The results indicate that our PEIF yields reconstruction results with better completeness and surface smoothness. ", "page_idx": 17}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/2c4121e3a978fb369a7ed0d1bf1b7d59117b8e28cff85a3736fa68da79c55995.jpg", "img_caption": ["Figure 10: Examples of results on Synthetic Room dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "aXS1pwMa8I/tmp/cefc7129ba28a8d1f2cabc0ef1765814c855de25c0bc2b8dcce6c79504dcbeda.jpg", "img_caption": ["Figure 11: The visual example of cross-domain evaluation on the real scanned dataset MGN [53], where the model is trained on Synthetic Room dataset [19]. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 19}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 19}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 19}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 19}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 19}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See section Conlusion. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See the implementation details of section Experiments. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We will release the codes at https://github.com/mathXin112/PEIF.git ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See section Experiments. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See section Experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have read the code of ethics carefully and ensure there is no violation. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See section Conclusion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There are no contents concerning safeguards. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We were unable to find the license for the datasets we used. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There are no new assets introduced in the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no Crowdsourcing and Research with Human Subjects in this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There are no potential risks as far as we are concerned. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]