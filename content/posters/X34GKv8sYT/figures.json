[{"figure_path": "X34GKv8sYT/figures/figures_2_1.jpg", "caption": "Figure 1: Schematic view of the data-analysis workflow in high-energy physics. Measurements (top) are processed in parallel with simulated data (bottom); their comparison is ultimately the basis for most scientific conclusions. In orange, we show how the three applications of L-GATr we experiment with in this paper fit into this workflow. The architecture is also applicable in several other stages, including reconstruction and inference.", "description": "This figure shows a schematic of the data analysis workflow in high-energy physics.  It illustrates the process from initial particle interactions to final discoveries and measurements, highlighting the role of simulation and theory predictions. The three applications of the Lorentz Geometric Algebra Transformer (L-GATr) discussed in the paper are shown in orange, illustrating where they fit within the broader pipeline.", "section": "1 Introduction"}, {"figure_path": "X34GKv8sYT/figures/figures_5_1.jpg", "caption": "Figure 2: Target vector field for Riemannian flow matching. Our choice of metric space guarantees that the generative model respects phase-space boundaries (red circle).", "description": "This figure shows the target vector field used in Riemannian flow matching for training the generative model.  The green lines represent probability paths from a base distribution to the target data distribution.  The red circle highlights a phase-space boundary (p<sub>T</sub> < p<sub>T,min</sub>); the model is designed to respect this boundary, ensuring generated particles are physically realistic.", "section": "3.2 Lorentz-equivariant flow matching"}, {"figure_path": "X34GKv8sYT/figures/figures_6_1.jpg", "caption": "Figure 3: Amplitude surrogates. Left: Surrogate error for processes of increasing particle multiplicity and complexity, training on the full dataset of 4 \u00b7 105 samples. L-GATr outperforms the baselines, especially at more complex processes. Right: Surrogate error as a function of the training dataset size.", "description": "This figure shows the performance of L-GATr and other methods on the task of creating surrogate models for quantum field theory amplitudes.  The left panel compares the mean squared error (MSE) of different methods for processes with increasing numbers of particles.  L-GATr consistently outperforms other models, especially for more complex processes. The right panel shows how the MSE changes as the size of the training dataset is varied, demonstrating that L-GATr is data efficient.", "section": "4.1 Surrogates for QFT amplitudes"}, {"figure_path": "X34GKv8sYT/figures/figures_8_1.jpg", "caption": "Figure 4: Generative modelling: Marginal distributions of reconstructed particles in the pp \u2192 tt + 4 jets process. We compare the ground-truth distribution (black) to three generative models: continuous normalizing flows based on a Transformer, MLP, or our L-GATr network. The three marginals shown represent kinematic features that are known to be challenging. The L-GATr flow describes them most accurately.", "description": "This figure compares the marginal distributions of reconstructed particles from ground truth data and three different generative models: a continuous normalizing flow based on a Transformer, an MLP, and the proposed L-GATr network.  The three marginals shown (pT,j, \u0394Rj,j, mt) represent challenging kinematic features often difficult for generative models to capture accurately. The figure demonstrates that the L-GATr generative model produces distributions that closely match the ground truth.", "section": "4.3 Generative modelling"}, {"figure_path": "X34GKv8sYT/figures/figures_9_1.jpg", "caption": "Figure 5: Generative modelling: negative log likelihood on the test set (lower is better). Left: For different processes. Right: As a function of the training dataset size. We show the mean and standard deviation of three random seeds. The L-GATr conditional flow matching (CFM) model outperforms all other CFM models as well as the autoregressive transformer JetGPT, across all processes and all training set sizes.", "description": "This figure compares the performance of different generative models, including L-GATr, in terms of negative log-likelihood on a test dataset. The left panel shows the performance across different processes (varying jet multiplicities), while the right panel shows how the performance changes as the amount of training data increases. Error bars represent the standard deviation over three different random seeds. L-GATr consistently outperforms other models, demonstrating its effectiveness in generative modeling of particle physics data.", "section": "4.3 Generative modelling"}, {"figure_path": "X34GKv8sYT/figures/figures_20_1.jpg", "caption": "Figure 6: Generative modelling: classifier two-sample tests. We show how well a classifier can discriminate model samples from test samples, measured through the area under the ROC curve (lower is better, 0.5 is ideal). Left: For different processes. Right: As a function of the training dataset size. We show the mean and standard deviation of three random seeds. The L-GATr flow outperforms the baselines in all processes and all training set sizes.", "description": "This figure presents the results of a classifier two-sample test, evaluating the quality of samples generated by various generative models.  The left panel shows the performance for different processes (varying jet multiplicities), while the right panel shows performance as a function of the amount of training data used. The area under the ROC curve (AUC) is used as the metric, with a lower AUC indicating better performance (an ideal AUC is 0.5).  The L-GATr flow model consistently outperforms other models.", "section": "4.3 Generative modelling"}, {"figure_path": "X34GKv8sYT/figures/figures_21_1.jpg", "caption": "Figure 7: Inference cost (wall-time per forward pass) as a function of the number of particles. We compare L-GATr, a Transformer, and a message-passing graph neural network (we use CGENN [68] but expect similar results for other architectures). The latter runs out of memory when evaluating more than a thousand particles. While we do our best to find comparable settings, such comparisons depend on a lot of choices and should be interpreted with care. Nevertheless, we believe they illustrate that L-GATr scales to large systems like a Transformer, thanks to it being based on dot-product attention.", "description": "This figure compares the inference time (in milliseconds) of three different network architectures: L-GATr, Transformer, and a Graph Neural Network (GNN) as a function of the number of particles. The GNN runs out of memory above 1000 particles, whereas the other two scale well.", "section": "4.4 Computational cost and scalability"}]