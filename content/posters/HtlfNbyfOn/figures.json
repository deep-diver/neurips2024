[{"figure_path": "HtlfNbyfOn/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of the reconstruction task. a. A signal in spacetime generates discrete photons through a Poisson process. Real detectors can only count one photon at a time. The discrete nature of photons and the discrete counting process introduce shot noise, resulting in a sparse binary map. Our goal is to predict the underlying signal from this information-sparse data. b. Real SPAD raw data captured by a detector. The highlighted box indicates a zoomed-in region, revealing sparse photon detection events. To the right is a cross-section of the time-height dimensions, showing a similar binary noisy pattern. c. Our method produces the video from the data in b at the original spatiotemporal resolution (Video S1). d. Left: effect of accumulating raw data frames directly, showing shot noise and motion artifacts. Right: additional keyframe pairs are provided for reference.", "description": "This figure visualizes the reconstruction task of the bit2bit method. It demonstrates how a signal in spacetime generates discrete photons, which are then detected by sensors as a sparse binary map due to shot noise. The figure displays real SPAD raw data, showing the sparse photon detection events, and compares the result of the proposed method with a simple binning approach.", "section": "1 Introduction"}, {"figure_path": "HtlfNbyfOn/figures/figures_2_1.jpg", "caption": "Figure 2: Example Results from Our Method Using Real SPAD Data The top row displays raw SPAD data. The middle row shows the corresponding reconstructions using our method. CPU Fan + motion: Imaged under camera motion. Additional paired raw data and reconstruction keyframes are shown below. H&E slide: Moving under a microscope. Sonicating bubbles: Humidifier generates bubbles, water droplets, and mist. USAF 1951 + drill: Resolution target spinning on a drill. Plasma ball: Firing plasma. A color-coded accumulation of 50 frames is shown on the right. [More in supp]", "description": "This figure showcases the results of the bit2bit method applied to various real-world SPAD datasets.  The top row shows the raw, sparse binary SPAD data for each scene. The middle row displays the corresponding reconstructions generated by the bit2bit method, highlighting its ability to reconstruct high-quality images from noisy, sparse input.  Each column represents a different scene: a CPU fan in motion, a histology slide viewed under a microscope, sonicating bubbles, a USAF 1951 resolution target spinning on a drill, a plasma ball, and a color-temporal coded sequence. The bottom section includes additional examples of raw data and reconstructed keyframes for the CPU fan and H&E slide scenarios, further demonstrating the efficacy of bit2bit.", "section": "2 Related Work"}, {"figure_path": "HtlfNbyfOn/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of the sampling/masking strategy. The raw data is processed in 3D to use space and time similarly. Data pairs are created by random 3D crop from the raw data, then randomly split the positive values into an input or a target matrix. The split ratio is controlled by a parameter p. A mask is created by flipping the bits in the input image, which prevents gradient back-propagation from locations of 1s in the input. This process is repeated indefinitely, each time creating a new pair of data equivalent to independent observations from the underlying signal.", "description": "This figure illustrates the data sampling and masking strategy used in the bit2bit method.  The raw 3D SPAD data is randomly cropped. Then, a voxel-wise random split assigns each photon detection event to either an input or target matrix, controlled by parameter *p*. A mask, created by inverting the input, prevents the network from learning the deterministic relationship between input and target matrices (i.e., if a pixel is 1 in the input, it cannot be 1 in the target). This process repeats to generate numerous training pairs from a single data sample.", "section": "Theories and methods"}, {"figure_path": "HtlfNbyfOn/figures/figures_4_1.jpg", "caption": "Figure 4: Real data examples of photon splitting and the effect of the masked loss a. Example of splitting a randomly selected quanta image raw data frame. The Raw data consists of only binary pixels indicating the location of the photon counting event. The Split indicates the Input (black) and Target (white) of the split. The Mask is calculated by inverting the Input and is applied to the loss. b. Comparison of the training results with unmasked and masked loss. Without the masked loss, the network learns that whenever a pixel location has a photon in the input, it never has a photon in the target. The deterministic relationship leads to the artifacts. The pixel locations where the input is 1 appear dark in the network output. The masked loss effectively addresses the problem.", "description": "This figure demonstrates the effect of the proposed masked loss function on real SPAD data.  Part (a) shows the process of splitting a raw data frame into input and target data, highlighting the creation of a mask to prevent overfitting. Part (b) compares the results of training with and without the mask.  The masked loss successfully avoids artifacts caused by deterministic relationships between input and target images, producing significantly better results.", "section": "Splitting of 1-bit quanta data"}, {"figure_path": "HtlfNbyfOn/figures/figures_8_1.jpg", "caption": "Figure 5: Results of ablation studies. a. Group normalization substantially improved the PSNR. b. The choice of lower and c. upper bound of the thinning probability p affects the reconstruction quality. (.9x6: 1 \u2013 10\u00ba, etc.) d. Fixed large p led to performance degradation despite the proposed single photon prediction suggested in GAP. e. Large model size could negatively impact PSNR. Rome numbers indicate the corresponding images in Fig. S3. Numerical values in Table S2-6.", "description": "This figure presents the results of ablation studies conducted to analyze the impact of different hyperparameters and design choices on the performance of the proposed model.  Specifically, it explores the effect of group normalization, the range of the thinning probability (p), and the model size on the PSNR (Peak Signal-to-Noise Ratio) metric. Each subplot shows the results for a specific hyperparameter or design choice, with error bars indicating the standard deviation.  The Roman numerals in the figure refer to corresponding images in Figure S3, which presumably show example reconstructions under the tested conditions. The numerical values are detailed in supplementary tables S2-6.", "section": "Experiments"}, {"figure_path": "HtlfNbyfOn/figures/figures_9_1.jpg", "caption": "Figure 6: Comparison of our method to QBP with real SPAD data presented in the paper. a) Different rendering of the data. The data is from the original QBP, indicating a dynamic scene with a person playing guitar. Our result is shown on the right. b) Height-time slicing of the raw data and our reconstruction. Top: raw data. Middle: our reconstruction, showing the top 3 strings. Bottom: the difference between adjacent frames, indicating sub-pixel movements of the string.", "description": "This figure compares the results of the proposed bit2bit method with the state-of-the-art Quanta Burst Photography (QBP) method using real SPAD data.  Subfigure (a) shows different visual representations of the raw data and the reconstructed videos, highlighting the improved visual quality of bit2bit. Subfigure (b) provides a detailed analysis of a height-time slice through the video data, comparing the raw data with the bit2bit reconstruction and showing the ability of bit2bit to capture sub-pixel movements that are lost in the QBP method.", "section": "Results and discussions"}, {"figure_path": "HtlfNbyfOn/figures/figures_19_1.jpg", "caption": "Figure 3: Overview of the sampling/masking strategy. The raw data is processed in 3D to use space and time similarly. Data pairs are created by random 3D crop from the raw data, then randomly split the positive values into an input or a target matrix. The split ratio is controlled by a parameter p. A mask is created by flipping the bits in the input image, which prevents gradient back-propagation from locations of 1s in the input. This process is repeated indefinitely, each time creating a new pair of data equivalent to independent observations from the underlying signal.", "description": "This figure illustrates the self-supervised sampling and masking strategy used in the bit2bit method.  The raw 3D data (space and time) is randomly cropped, then split into input and target matrices. A masking step prevents learning from the input's '1' values, addressing the issue of complementary dependency in training pairs.  This process creates numerous training pairs from limited data, enhancing performance and mitigating overfitting.", "section": "3 Theories and methods"}, {"figure_path": "HtlfNbyfOn/figures/figures_20_1.jpg", "caption": "Figure 4: Real data examples of photon splitting and the effect of the masked loss a. Example of splitting a randomly selected quanta image raw data frame. The Raw data consists of only binary pixels indicating the location of the photon counting event. The Split indicates the Input (black) and Target (white) of the split. The Mask is calculated by inverting the Input and is applied to the loss. b. Comparison of the training results with unmasked and masked loss. Without the masked loss, the network learns that whenever a pixel location has a photon in the input, it never has a photon in the target. The deterministic relationship leads to the artifacts. The pixel locations where the input is 1 appear dark in the network output. The masked loss effectively addresses the problem.", "description": "This figure demonstrates the process of photon splitting and the effect of using a masked loss function. (a) shows the steps involved in splitting a raw data frame into input and target matrices, along with the creation of a mask. The mask is used in training to prevent the network from learning deterministic relationships between input and target pixels, reducing artifacts. (b) compares results with and without the masked loss, showing how it effectively improves reconstruction quality by mitigating the artifacts generated by correlated input and target images.", "section": "Splitting of 1-bit quanta data"}, {"figure_path": "HtlfNbyfOn/figures/figures_21_1.jpg", "caption": "Figure 3: Overview of the sampling/masking strategy. The raw data is processed in 3D to use space and time similarly. Data pairs are created by random 3D crop from the raw data, then randomly split the positive values into an input or a target matrix. The split ratio is controlled by a parameter p. A mask is created by flipping the bits in the input image, which prevents gradient back-propagation from locations of 1s in the input. This process is repeated indefinitely, each time creating a new pair of data equivalent to independent observations from the underlying signal.", "description": "This figure illustrates the self-supervised sampling and masking strategy used in the bit2bit method.  The raw 3D SPAD data is cropped randomly, then split voxel-wise into input and target matrices. The split ratio is adjustable via parameter 'p'. A mask inverts the input, preventing gradient updates from locations of '1s', ensuring independence and creating numerous training pairs from limited data. This strategy effectively addresses the problem of complementary dependency between the input and target pairs in 1-bit quanta data. This process is iteratively performed to generate an unlimited number of training pairs.", "section": "Theories and methods"}, {"figure_path": "HtlfNbyfOn/figures/figures_22_1.jpg", "caption": "Figure 4: Real data examples of photon splitting and the effect of the masked loss. a. Example of splitting a randomly selected quanta image raw data frame. The Raw data consists of only binary pixels indicating the location of the photon counting event. The Split indicates the Input (black) and Target (white) of the split. The Mask is calculated by inverting the Input and is applied to the loss. b. Comparison of the training results with unmasked and masked loss. Without the masked loss, the network learns that whenever a pixel location has a photon in the input, it never has a photon in the target. The deterministic relationship leads to the artifacts. The pixel locations where the input is 1 appear dark in the network output. The masked loss effectively addresses the problem.", "description": "This figure demonstrates the effect of the proposed masked loss function in addressing the correlation between input and target images created by the photon splitting process.  The left panel (a) shows an example of how a raw quanta image is split into input and target matrices, along with the generated mask.  The right panel (b) compares the results of training with and without the masked loss, highlighting the significant improvement in image quality achieved by masking.", "section": "Splitting of 1-bit quanta data"}, {"figure_path": "HtlfNbyfOn/figures/figures_23_1.jpg", "caption": "Figure 5: Results of ablation studies. a. Group normalization substantially improved the PSNR. b. The choice of lower and c. upper bound of the thinning probability p affects the reconstruction quality. (.9x6:1 \u2013 10\u00ba, etc.) d. Fixed large p led to performance degradation despite the proposed single photon prediction suggested in GAP. e. Large model size could negatively impact PSNR. Rome numbers indicate the corresponding images in Fig. S3. Numerical values in Table S2-6.", "description": "This figure presents the results of ablation studies conducted to analyze the impact of different hyperparameters on the performance of the proposed method.  Specifically, it shows how changes in group normalization, the range of the thinning probability (p), fixed large p values, and model size affect the peak signal-to-noise ratio (PSNR).  The results are presented graphically, showing trends for PSNR across different parameter settings.  References to supplemental tables and figures are provided for more detailed numerical results.", "section": "Experiments"}, {"figure_path": "HtlfNbyfOn/figures/figures_24_1.jpg", "caption": "Figure S6: Example real SPAD data and reconstruction: Image rotating on the CPU fan. This scene is shown in Fig 1, showing a sticker of a mandrill image rotating on a CPU fan acquired in a dark room. The scene is in low light. The camera is static. Part of the scene is rotating. The speed of the CPU fan is 1500 RPM. The light intensity of the scene is in the order of 1-10 lux. This is a scene in a dark room with the room light turned off, with the only light source the computer monitor pointing towards the wall and some small LEDs on the motherboard. It is worth noting that even at this low light condition, the imaging is not photon-limited because the hardware is highly sensitive. We had to reduce the aperture of the camera lens to ensure the sensor was not over-saturated. Top left: The raw data. Top right: the reconstruction. Middle: 50 frames of raw data, skipping 50 frames. Bottom: the corresponding reconstruction.", "description": "This figure shows example real SPAD data and its reconstruction. The scene is a sticker of a mandrill image rotating on a CPU fan in a dark room with low light.  The figure displays the raw SPAD data, the reconstruction result, 50 frames of raw data with 50 frames skipped, and the corresponding reconstruction frames. The data demonstrates the effectiveness of the method in handling low light and motion conditions.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_25_1.jpg", "caption": "Figure S7: Example real SPAD data and reconstruction: Plasma ball. This scene shows a plasma ball, which produces plasma in a vacuum sphere. Plasma is triggered by high voltage generated through a buck converter circuit, with a measured frequency of 28 kHz. The plasma is released in a pulsed fashion at this frequency, following similar paths between adjacent events. The camera is triggered at a similar frequency to capture each image, representing a 6 ns snapshot of the event. Adjacent frames indicate the flow of the plasma, capturing this photo-sparse scene with extremely high-speed events. Top left: The raw data. Top right: the reconstruction. Middle: 50 frames of raw data, skipping 500 frames. Bottom: the corresponding reconstruction.", "description": "This figure shows a real SPAD data and its reconstruction of plasma ball.  The data was acquired using a high-speed camera triggered at a similar frequency to the plasma release, capturing a 6ns snapshot of the event. The reconstruction shows the flow of plasma in a series of frames, illustrating the ability of the method to reconstruct high-speed events from sparse binary data. ", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_26_1.jpg", "caption": "Figure S8: Example real SPAD data and reconstruction: Sonicator and bubbles. This scene shows a piezo transducer sending a 3 MHz sound wave through a detergent liquid, creating bubbles, mist, and water droplets. It is a highly dynamic, complex, and chaotic scene with random high-speed movement. The high background signals from the mist pose a challenge. Despite this, our method demonstrated reasonable performance. Top left: The raw data. Top right: the reconstruction. Middle: 50 frames of raw data, skipping 500 frames. Bottom: the corresponding reconstruction.", "description": "This figure shows the results of applying the proposed method to real SPAD data from a sonicator and bubbles experiment. The top row displays the raw SPAD data, which shows a chaotic scene with many small, rapidly moving bubbles. The bottom row shows the reconstructed images, which are significantly clearer and more interpretable than the raw data. The middle row shows 50 frames of the raw data, which highlight the dynamic nature of the scene and the challenges involved in reconstructing a clear image from such data. The results demonstrate that the proposed method is capable of reconstructing high-quality images from extremely noisy and sparse SPAD data, even in challenging imaging conditions.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_27_1.jpg", "caption": "Figure S14: Example results from photon-sparse confocal 3D volume. The data was acquired using a Leica SP8 confocal microscope in photon counting mode, showing the auto-fluorescence of a mouse brain tissue. Input data, high SNR reference, the result from the original GAP open source code, and our results are shown on the top row for qualitative comparison. The high SNR reference is scanned separately in a different imaging session. There are small differences between the images due to repositioning. Thus, no numerical comparison is available. Raw data and our reconstruction of key z-slices through the volume are shown below.", "description": "This figure shows a comparison of different methods for reconstructing a 3D volume from photon-sparse confocal microscopy data.  The top row displays the input data, a high SNR reference image, results using the original GAP method, and the results from the proposed bit2bit method. The bottom rows show raw data and reconstructions from key z-slices.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_28_1.jpg", "caption": "Figure S14: Example results from photon-sparse confocal 3D volume. The data was acquired using a Leica SP8 confocal microscope in photon counting mode, showing the auto-fluorescence of a mouse brain tissue. Input data, high SNR reference, the result from the original GAP open source code, and our results are shown on the top row for qualitative comparison. The high SNR reference is scanned separately in a different imaging session. There are small differences between the images due to repositioning. Thus, no numerical comparison is available. Raw data and our reconstruction of key z-slices through the volume are shown below.", "description": "This figure shows a comparison of different methods for reconstructing a 3D confocal volume from photon-sparse data. The top row shows the input data, a high SNR reference, the result from the original GAP method, and the results from the proposed method. The bottom rows show the raw data and reconstruction for key z-slices.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_29_1.jpg", "caption": "Figure S11: Example real SPAD data and reconstruction: CPU fan with camera motion. This scene was taken in a dark room, showing a running CPU fan. The exposure was 6 ns for each image. The camera is moving up and down. Individual fan blades and the characters on the fan were both resolved after reconstruction. This is a very low-light scene with dynamic motion on both the object and the camera. Top left: The raw data. Top right: the reconstruction. Middle: 50 frames of raw data, skipping 500 frames. Bottom: the corresponding reconstruction.", "description": "This figure shows the reconstruction of a video of a CPU fan with camera motion using the proposed method.  The top row displays the raw SPAD data (left) and the corresponding reconstruction (right). The middle row shows 50 frames of the raw data, and the bottom row shows the reconstructed frames. This experiment showcases the method's ability to reconstruct high-quality video from noisy, sparse data under dynamic conditions.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_30_1.jpg", "caption": "Figure 2: Example Results from Our Method Using Real SPAD Data The top row displays raw SPAD data. The middle row shows the corresponding reconstructions using our method. CPU Fan + motion: Imaged under camera motion. Additional paired raw data and reconstruction keyframes are shown below. H&E slide: Moving under a microscope. Sonicating bubbles: Humidifier generates bubbles, water droplets, and mist. USAF 1951 + drill: Resolution target spinning on a drill. Plasma ball: Firing plasma. A color-coded accumulation of 50 frames is shown on the right. [More in supp]", "description": "This figure displays example results obtained using real SPAD data. The top row shows the raw SPAD data for different scenarios (CPU fan with motion, H&E slide, sonicating bubbles, USAF 1951 target with drill, and plasma ball). The middle row presents the corresponding reconstructions generated by the proposed method.  The bottom shows additional keyframes.  The rightmost image shows a color-coded accumulation of 50 frames.", "section": "2 Related Work"}, {"figure_path": "HtlfNbyfOn/figures/figures_30_2.jpg", "caption": "Figure 2: Example Results from Our Method Using Real SPAD Data The top row displays raw SPAD data. The middle row shows the corresponding reconstructions using our method. CPU Fan + motion: Imaged under camera motion. Additional paired raw data and reconstruction keyframes are shown below. H&E slide: Moving under a microscope. Sonicating bubbles: Humidifier generates bubbles, water droplets, and mist. USAF 1951 + drill: Resolution target spinning on a drill. Plasma ball: Firing plasma. A color-coded accumulation of 50 frames is shown on the right. [More in supp]", "description": "This figure showcases the results of applying the proposed bit2bit method to various real-world SPAD datasets.  The top row presents the raw, noisy SPAD data from different scenes (CPU fan with motion, H&E stained slide, sonicating bubbles, USAF 1951 resolution target with drill, and plasma ball). The middle row displays the corresponding reconstructions generated by the bit2bit method. Notably, the reconstruction achieves high-quality images at the original spatiotemporal resolution even with extremely sparse photon data.  The figure highlights the capability of the method to handle various challenging imaging conditions (strong/weak ambient light, strong motion, and ultra-fast events).", "section": "2 Related Work"}, {"figure_path": "HtlfNbyfOn/figures/figures_31_1.jpg", "caption": "Figure 6: Comparison of our method to QBP with real SPAD data presented in the paper. a) Different rendering of the data. The data is from the original QBP, indicating a dynamic scene with a person playing guitar. Our result is shown on the right. b) Height-time slicing of the raw data and our reconstruction. Top: raw data. Middle: our reconstruction, showing the top 3 strings. Bottom: the difference between adjacent frames, indicating sub-pixel movements of the string.", "description": "This figure compares the results of the proposed method with those of Quanta Burst Photography (QBP) using real SPAD data showing a person playing a guitar.  Subfigure (a) shows a comparison of raw data, QBP reconstruction, and the proposed method's reconstruction. Subfigure (b) shows height-time slices of the raw data and the method's reconstruction, highlighting the temporal resolution improvement.  The differences between adjacent frames in the reconstruction demonstrate the capture of sub-pixel movements.", "section": "5 Results and discussions"}, {"figure_path": "HtlfNbyfOn/figures/figures_32_1.jpg", "caption": "Figure S14: Example results from photon-sparse confocal 3D volume. The data was acquired using a Leica SP8 confocal microscope in photon counting mode, showing the auto-fluorescence of a mouse brain tissue. Input data, high SNR reference, the result from the original GAP open source code, and our results are shown on the top row for qualitative comparison. The high SNR reference is scanned separately in a different imaging session. There are small differences between the images due to repositioning. Thus, no numerical comparison is available. Raw data and our reconstruction of key z-slices through the volume are shown below.", "description": "This figure displays a comparison of different methods for reconstructing images from photon-sparse confocal 3D data.  The top row shows the input data (very low SNR), a high SNR reference image, and reconstructions using the GAP 2D method and the proposed 'ours' method. The following rows show the raw data and reconstructed data for several z-slices through the 3D volume. The results highlight that the proposed method improves the quality of the reconstruction compared to the GAP 2D method, although quantitative metrics (PSNR/SSIM) are not provided due to variations in acquisition conditions.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_33_1.jpg", "caption": "Figure S15: Example results from simulated photon-sparse confocal 3D volume. The data was acquired using a Leica SP8 confocal microscope in normal mode, showing the DAPI-stained cell nuclei in mouse heart tissue. The data was then sampled into different levels of photon counts and truncated at 1 to simulate the SPAD data. PSNR and SSIM are computed for different input photon levels.", "description": "This figure presents a comparison of results from a simulated photon-sparse confocal 3D volume.  The top row shows the ground truth, the restored images (using the method described in the paper), and zoomed-in views. The subsequent rows show the results of resampling the data at different photon counts (0.0599, 0.0306, and 0.00777 photons per pixel) to mimic the sparsity of SPAD data and demonstrate how the method performs at lower photon counts.  The PSNR and SSIM values are provided for each level of resampling. This shows the ability of the method to reconstruct images despite the extremely sparse nature of the input data.", "section": "Appendix / Supplementary figures"}, {"figure_path": "HtlfNbyfOn/figures/figures_34_1.jpg", "caption": "Figure S16: Example learning curve from N2N training. The validation loss increases since a very early stage of the training. The X-axis indicates training steps.", "description": "This figure shows the validation loss curve during the training of a Noise2Noise (N2N) model. The x-axis represents the training steps, while the y-axis shows the validation loss.  The plot reveals that the validation loss starts increasing from a very early stage of training.  This indicates a potential problem with the model's ability to generalize well to unseen data, suggesting overfitting to the training data.", "section": "Appendix / Supplementary figures"}]