[{"figure_path": "nblJSwtdrJ/tables/tables_3_1.jpg", "caption": "Table 1: Main results across different datasets and models. The best results are in bold.", "description": "This table presents the main experimental results, comparing the performance of Tina against several baseline methods (Generic Model, Classifier Selection, and TAPER-Mixer) across three different datasets (Mini-ImageNet, CIFAR-100, and Caltech-101) and two model architectures (CNN and ResNet).  It shows the accuracy (p-Acc) for both in-distribution and out-of-distribution personalization scenarios.  The best accuracy for each dataset, model, and scenario is highlighted in bold, demonstrating Tina's superior performance in both settings.", "section": "3.2 Results under Different Datasets"}, {"figure_path": "nblJSwtdrJ/tables/tables_5_1.jpg", "caption": "Table 1: Main results across different datasets and models. The best results are in bold.", "description": "This table presents the performance comparison of different methods for in-distribution and out-of-distribution personalization across three datasets: Mini-ImageNet, CIFAR-100, and Caltech-101.  The methods compared include a generic model (without personalization), classifier selection, TAPER-Mixer, and the proposed Tina model.  Results are shown for two model architectures, CNN and ResNet, and performance is measured using accuracy (p-Acc). The best results for each dataset and model combination are highlighted in bold.", "section": "3.2 Results under Different Datasets"}, {"figure_path": "nblJSwtdrJ/tables/tables_7_1.jpg", "caption": "Table 2: Zero-shot transfer of Tina to unseen classes. We test the generalization capability of Tina to unseen classes that have similar textual similarity with the seen ones.", "description": "This table presents the results of a zero-shot transfer experiment to evaluate Tina's generalization capability on unseen classes.  The experiment tests Tina's performance on unseen classification tasks where the unseen classes share textual similarity with the seen classes in the training data. The table shows the accuracy of Tina and TAPER-Mixer across different percentages of unseen tasks (0%, 20%, 40%, 60%, and 100%). This demonstrates how well each model generalizes to novel, but semantically similar, classes.", "section": "3.3 In-depth Analysis of Tina"}, {"figure_path": "nblJSwtdrJ/tables/tables_8_1.jpg", "caption": "Table 1: Main results across different datasets and models. The best results are in bold.", "description": "This table presents the main results of the experiments conducted on three different datasets (Mini-ImageNet, CIFAR-100, and Caltech-101) using various models for image classification, focusing on both in-distribution and out-of-distribution personalization. The models compared include a generic model (baseline), Classifier Selection, TAPER-Mixer (a baseline), and the proposed Tina model. The table shows the accuracy (p-Acc) of each model for both in-distribution and out-of-distribution personalization settings, with the best results highlighted in bold.", "section": "3.2 Results under Different Datasets"}, {"figure_path": "nblJSwtdrJ/tables/tables_13_1.jpg", "caption": "Table 1: Main results across different datasets and models. The best results are in bold.", "description": "This table presents the main results of the experiments conducted on three different datasets (Mini-ImageNet, CIFAR-100, and Caltech-101) using four different models (Generic Model, Classifier Selection, TAPER-Mixer, and Tina).  The table compares the performance of these models on two different tasks: in-distribution personalization (where the model is tested on tasks similar to those seen during training) and out-of-distribution personalization (where the model is tested on unseen tasks).  The best performance for each setting (dataset and task) is highlighted in bold.  The results show that Tina consistently outperforms the other models on both in-distribution and out-of-distribution tasks.", "section": "3.2 Results under Different Datasets"}]