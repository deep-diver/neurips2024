[{"type": "text", "text": "Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Generative artificial intelligence (GenAI) has made significant progress in under  \n2 standing world knowledge and generating content from human languages across   \n3 various modalities, like text-to-text large language models, text-to-image stable dif  \n4 fusion, and text-to-video Sora. While in this paper, we investigate the capability of   \n5 GenAI for text-to-model generation, to see whether GenAI can comprehend hyper  \n6 level knowledge embedded within AI itself parameters. Specifically, we study a   \n7 practical scenario termed train-once-for-all personalization, aiming to generate per  \n8 sonalized models for diverse end-users and tasks using text prompts. Inspired by the   \n9 recent emergence of neural network diffusion, we present Tina, a text-conditioned   \n0 neural network diffusion for train-once-for-all personalization. Tina leverages a   \n11 diffusion transformer model conditioned on task descriptions embedded using a   \n2 CLIP model. Despite the astronomical number of potential personalized tasks (e.g.,   \n3 $1.73\\times10^{13}$ ), by our design, Tina demonstrates remarkable in-distribution and out  \n4 of-distribution generalization even trained on small datasets $(\\sim1000)$ . We further   \n15 verify whether and how Tina understands world knowledge by analyzing its capa  \n16 bilities under zero-shot/few-shot image prompts, different numbers of personalized   \n17 classes, prompts of natural language descriptions, and predicting unseen entities. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Generative artificial intelligence (GenAI) has   \n20 been flourishing in different aspects of human   \n21 life, and people can simply generate content   \n22 from natural language text prompts [1, 2, 3, 4].   \n23 Large language models [1, 5], like GPT-4, have   \n24 especially shown emergent intelligence [6] in   \n25 the knowledge of language through text-to  \n26 text transformation [7, 8, 1, 5]. Besides, re  \n27 cent progress in text-to-image (e.g., stable dif  \n28 fusion) [9, 4, 2, 10] and text-to-video (e.g.,   \n29 Sora) [3, 11] diffusion models has shown the   \n30 great power of AI in understanding the physical   \n31 world and generating high-quality images and   \n32 videos that are virtually indistinguishable from   \n33 reality [12, 3]. The text-prompted GenAI maps   \n34 the human languages\u2019 semantics to the world   \n35 knowledge in different forms in language and vision. One step further, in this paper, we propose   \n36 and study whether the GenAI can understand hyper-level knowledge\u2014the knowledge inherently   \n37 resides in the AI itself models\u2019 parameters. Specifically, we study text-to-model generation; akin to   \n38 text-to-text, text-to-image, and text-to-video, text-to-model targets whether the GenAI models can   \n39 directly generate the model parameters given the human\u2019s text prompts to meet the personalization   \n40 demand of diverse end users.   \n41 We focus on a practical scenario called train-once-for-all personalization [13], which means that   \n42 the generic model is trained just once and can later be customized into a condensed model on the   \n43 fly for different end-users and requests, given their task descriptions. For example, the CIFAR-100   \n44 dataset [14] contains 100 classes, but an end user may just need a personalized model with a certain   \n45 10 classes according to a specific scenario (e.g., classifying items in the kitchen). In other words,   \n46 train-once-for-all personalization targets that train the model once and customize the model to be   \n47 well performed in a sub-distribution when deployed, and an example is in Figure 1. But there are   \n48 tremendous sub-distributions, for the CIFAR-100 example, the number of personalized 10-way tasks   \n49 is $\\binom{100}{10}=1.73\\times10^{13}$ , even not taking permutations into consideration, so it is challenging for the   \n50 GenAI model to generalize. Inspired by recent progress in neural network diffusion [15, 16], we   \n51 propose Tina, a Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization.   \n52 Tina is trained on model parameters with the models\u2019 task descriptions, and it can be generalized to   \n53 unseen tasks, or even unseen classes (entities), given the text prompts.   \n54 In Tina, a CLIP model [17] is used to embed the users\u2019 task descriptions into the diffusion model as   \n55 the conditions. The diffusion model of Tina is the diffusion transformer (DiT) [12] that is shown to   \n56 have high expressive power under scaling law in the fields of image [12] and video generation [3].   \n57 We demonstrate that DiT\u2019s scaling law applies to model parameter generation as well: increasing   \n58 the number of parameters and data sizes enhances the model\u2019s capability to generalize across more   \n59 challenging tasks that involve scaling the dimension of generated models. However, it is surprising to   \n60 find that even though the number of personalized tasks is astronomical (e.g., $1.73\\times10^{13}$ for 10-way   \n61 tasks), by our designs, Tina can generalize on extremely small datasets $\\sim1000$ data points) and   \n62 support different lengths of classification tasks (5-way or 8-way tasks, etc.) in training once. Our   \n63 analysis shows that Tina can reach both in-distribution and out-of-distribution personalization of   \n64 generated models. Thanks to the vision-language alignment of CLIP, Tina can also take images   \n65 as prompts and generalize under few-shot or even zero-shot settings. We also verify whether Tina   \n66 understands world knowledge by testing its abilities under prompts of natural language descriptions   \n67 and predicting unseen entities. Our contributions are as follows:   \n68 \u2022 We explore the potential of GenAI in generating personalized models followed by users\u2019   \n69 text prompts, i.e., text-to-model generation. We open more applications of neural network   \n70 diffusion; to the best of our knowledge, it is the first paper that takes the text prompts as   \n71 conditions for neural network diffusion.   \n72 \u2022 We propose Tina, a well-performed text-conditioned neural network diffusion framework   \n73 for train-once-for-all personalization. Tina can generalize on unseen tasks and entities even   \n74 given small model datasets.   \n75 \u2022 In addition, we analyze the abilities and the boundaries of Tina and gain insights about   \n76 whether and how it generalizes and understands world knowledge. ", "page_idx": 0}, {"type": "image", "img_path": "nblJSwtdrJ/tmp/f2ad9baa2993effb72479c3289681271a86c2e569a9230edcdb7bbb580688a78.jpg", "img_caption": ["Figure 1: Demonstration of train-once-for-all personalization scenario. Users have text descriptions of the desired personalized models. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "77 2 Methodology ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "78 2.1 Problem Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "79 2.1.1 Definition of Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "80 Following [13], we consider image classification for train-once-for-all personalization due to the   \n81 natural personalization requirements of image classification. We note that our method is not limited   \n82 to classification tasks and can be extended to other tasks for personalization. Define a task $k$ as   \n83 classification over a subset of classes $\\mathcal{V}_{k}\\subset\\mathcal{V}$ . The goal of personalization is to learn a neural   \n84 network predictor $f_{\\theta_{k}}:\\mathcal{X}\\mapsto\\mathcal{Y}_{k}$ , parameterized by $\\theta_{k}$ . To handle many tasks at the same time, we   \n85 further assume we have the task description natural text $t_{k}$ for $\\mathcal{V}_{k}$ , and it is generally the description   \n86 of the classes and styles of $\\mathcal{V}_{k}$ . We want to build a neural network generator $G(t_{k})$ where given $t_{k}$ ,   \n87 it will output the model parameters $\\theta_{k}$ . Specifically, consider using a large-scale dataset with many   \n88 classes covering $\\boldsymbol{\\wp}$ to learn the personalized-friendly function $f_{\\theta_{k}}=G_{\\phi}(t_{k})$ parameterized by $\\phi$ . $G_{\\phi}$   \n89 is learned on the large dataset to generate any personalized model directly from the task descriptions,   \n90 and the setup is called train-once-for-all personalization [13]. Train-once-for-all personalization has   \n91 wide applications in a server-user system, where the model generator $G_{\\phi}$ is learned on the server for   \n92 personalized cloud services to many future users. We refer to [13] for more detailed advantages and   \n93 usages of train-once-for-all personalization.   \n95 Classifier Selection. For a generic network $f_{\\theta}$ , we consider that it consists of a feature extractor   \n96 parameterized by $\\psi$ with a linear classifier $\\textbf{w}=~\\left[\\mathbf{w}^{(1)},\\dots,\\mathbf{w}^{(|\\mathcal{V}|)}\\right]$ of $|\\mathcal{V}|$ vectors for output   \n97 predictions over all classes in $\\boldsymbol{\\wp}$ . The generic model is trained on the large dataset, and we want   \n98 to personalize it into a few-way classification task $k$ . One effective method is to build a personalized   \n99 classifier $\\mathbf{w}_{k}$ by selecting only the row vectors in w for the relevant classes. Therefore, the   \n100 personalized model for task $k$ are $\\theta_{k}\\,=\\,\\{\\psi,{\\bf w}_{k}\\}$ , and this approach is called classifier selection,   \n101 which serves as a strong baseline [13].   \n102 TAPER. We briefly introduce TAPER [13] proposed by the original paper on train-once-for-all per  \n103 sonalization and discuss its limitations. The main idea of TAPER is to train several experts (bases) and   \n104 learn a mixture network to fuse these experts into a personalized model. It has three stages as follows.   \n105 \u2022 Stage 1: train a generic model on the large dataset.   \n106 \u2022 Stage 2: divide the dataset into several shards and finetune the generic model on each shard   \n107 respectively for specification. Each finetuned model can be seen as a domain expert.   \n108 \u2022 Stage 3: For a given personalized task, learn an MLP mixer (i.e., the generator $G$ ) whose   \n109 input is the text embedding of the task description and the output is the aggregation weights of   \n10 the expert models. Then, weighted aggregation is conducted to merge several expert models   \n11 into a personalized one. Also, the expert models can be finetuned during personalization.   \n112 TAPER requires finetuning the expert models on the target task, so it is not applicable to unseen tasks   \n113 without having task-specific data. Also, the MLP mixer only generates the aggregation weights instead   \n114 of the parameters, so it has limited generalization and expressiveness. While in our design of Tina,   \n115 we try to construct an end-to-end text-to-model system that can understand the hyper-knowledge   \n116 residing parameters and can generalize to unseen tasks, even unseen classes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "117 2.1.3 Dataset Preparation and Description ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "118 We introduce how to conduct datasets for training Tina and elaborate on the differences in training   \n119 and inference between Tina and TAPER.   \n120 Training data preparation for Tina. Tina takes the personalized model parameters as training data   \n121 for diffusion training, and the dataset is conducted in two stages. i) Stage 1: Similar to TAPER, we   \n122 train a generic model on the large dataset to let the model have a generic capability on all classes. ii)   \n123 Stage 2: We craft the personalized tasks and finetune the generic model on the personalized tasks to   \n124 obtain the personalized models (p-Models) for Tina training. For each personalized task $k$ , we select   \n125 the corresponding $|\\mathcal{D}_{k}|$ classes out of $|\\mathcal{V}|$ classes to craft the data for p-Model, and then finetune to   \n126 get a p-Model as a data sample for Tina. Each data sample for Tina contains the \u201c(task description,   \n127 p-Model)\u201d pair.   \n128 Testing data preparation. The overall demon  \n129 stration of data partitions can be found in Fig  \n130 ure 2. The blue blocks refer to the training data,   \n131 and the green blocks are the testing data. For   \n132 testing, there are two kinds of evaluation metrics:   \n133 i) In-distribution (ID, the light green blocks):   \n134 the personalized tasks are seen during training   \n135 of the generative model $G$ , and $G$ generates the   \n136 p-Models tested on the testset of each seen task.   \n137 ii) Out-of-distribution (OOD, the dark green   \n138 blocks): the tasks are unseen during the gener  \n139 ator $G$ \u2019s training, and $G$ directly generates the   \n140 p-Models from the task prompts (the text de  \n141 scriptions). We note that the original TAPER   \n142 cannot be tested on the OOD tasks since it re  \n143 quires the target personalized training data for   \n144 finetuning the expert models. To remedy this, we derive TAPER-Mixer to only train the mixer without   \n145 finetuning the experts and verify its OOD generalization on unseen tasks. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "nblJSwtdrJ/tmp/c2ee79f04cdbd98c7d96761c39852fe15da76d66152f9f1ea18bcdb14a8ff2e1.jpg", "img_caption": ["Figure 2: Description of the training and testing data for Tina. p-Model is short for personalized models. The blue blocks are for training, and the green blocks are for testing. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "nblJSwtdrJ/tmp/df2748656513418eb66ba2246ea0fd8702c1afddfb5519c825bf709ba1d42b24.jpg", "img_caption": ["Figure 3: Framework overview of Tina. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "146 2.2 Proposed Tina: Text-conditioned Neural Network Diffusion Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "147 2.2.1 Framework Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "148 We present Tina, a text-conditioned neural network diffusion model for train-once-for-all   \n149 personalization. The framework overview is in Figure 3. Generally, Tina consists of DiT and CLIP   \n150 encoders for generating personalized models from text prompts. During training, we use the CLIP   \n151 text encoder for encoding texts, and due to the alignment of image and text in CLIP, during inference,   \n152 Tina can also take images as prompts by utilizing the CLIP image encoder. Additionally, we devise   \n153 an effective data augmentation approach to enable training Tina under limited samples. We also   \n154 propose a classification sequence padding strategy to enable Tina can generate models with different   \n155 lengths of classes for further personalization. ", "page_idx": 3}, {"type": "text", "text": "156 2.2.2 Architecture and Training Objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "157 We use diffusion models as the generative model   \n158 and follow the main architecture of G.pt [16]   \n159 that uses a diffusion transformer as the back  \n160 bone. Analogous to the optimization process   \n161 that takes random initialization as inputs and   \n162 outputs the trained models, the diffusion process   \n163 takes the noise as inputs and gradually denoises   \n164 to recover the original distributions. Previous   \n165 works have shown the rationale of neural net  \n166 work diffusion [16, 15, 18]. We choose DiT as   \n167 the backbone because it can be easily scaled up   \n168 and is shown to have great generalization and   \n169 expressiveness. We use signal prediction for   \n170 the diffusion process and inherit the architecture   \n171 of GPT-2 [8] as the transformer. The used text   \n172 encoder is the pretrained ViT-B/32 in CLIP [17].   \n173 Training objective. Denote the training set   \n174 of Tina as $\\kappa$ , where each piece of data is a   \n175 (task description, p-Model) tuple, notated as   \n176 $(t_{k},\\theta_{k})$ for task $k\\,\\in\\,\\kappa$ . We denote the CLIP   \n177 text encoder as $T$ , and given the task description ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "nblJSwtdrJ/tmp/e108484119965b8941aceae68249a9fa78d77b1927b0725024593188149151ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "$t_{k}$ , the text embedding is $T(t_{k})$ . The text encoder 178 is frozen during training. ", "page_idx": 3}, {"type": "text", "text": "179 Our DiT model $G_{\\phi}$ takes two vectors as input: the text embedding $T(t_{k})$ as conditions and the noised   \n180 p-Model parameter vector $\\theta_{k}^{j}$ , where $j\\in[J]$ denotes the timestep in the diffusion forward noising   \n181 process. The learning objective of diffusion is to minimize the simplified variational lower bound,   \n182 which reduces to predicting the denoised $\\mathfrak{p}$ -Model parameters: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\mathcal{L}(\\phi)=\\sum_{k\\in K}\\sum_{j\\in\\mathcal{I}}||\\theta_{k}-G_{\\phi}(T(t_{k}),\\theta_{k}^{j},j)||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 where the timestep $j$ is embedded in DiT by frequency-based encoding [19]. The detailed training   \n184 procedure is in Algorithm 1. We use DDPM sampling [9]; add Gaussian noise depicted by the $\\bar{\\alpha}$ to   \n185 $\\theta_{k}$ and gradually denoising it. ", "page_idx": 4}, {"type": "text", "text": "186 2.2.3 Design Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "187 We elaborate the design details of Tina. ", "page_idx": 4}, {"type": "text", "text": "188 Parameter tokenization. For a $\\mathbf{p}$ -Model\u2019s parameters $\\theta_{k}$ , we first flatten all the parameters into   \n189 a 1-D vector and chunk/tokenize the parameters within each layer. If the chunk size is $M$ and the   \n190 number of parameters in a certain layer is $N$ , so for this layer, there will be ceil $[N/M)$ tokens. For   \n191 some layers smaller than $M$ , the whole layer is a token.   \n192 Text embedding. Assume the personalized task is a classification task that has $c=|\\mathcal{V}_{k}|$ classes. The   \n193 task description $t_{k}$ is an ordered list of the classes\u2019 text descriptions, of which the simplest form is   \n194 the class entity, e.g., \"telephone\" and \"rabbit\". The generated ${\\mathrm{p}}\\cdot$ -Model is expected to have the correct   \n195 predictions in the same order with $t_{k}$ . In other words, we need Tina to learn the correct classifier   \n196 orders as the text prompts, which is sequence-to-sequence modeling. Therefore, unlike TAPER,   \n197 which averages the class embeddings into one, we make every class description as a token by CLIP   \n198 text encoder and concatenate them in order with position encoding.   \n199 Encoding and decoding of tokens. We use linear layers as encoders for mapping the parameter   \n200 tokens and text embedding tokens to the hidden size of DiT. Each token has a different linear   \n201 layer without weight sharing. The decoders are similar to encoders, which use linear layers, and   \n202 the encoders transform the transformer\u2019s hidden size back to the p-Model\u2019s parameter dimension.   \n203 Between the encoders and decoders, there are transformer attention layers akin to GPT-2.   \n204 Data augmentation. In [16], the permutation invariance property [20, 21, 22] is utilized for data   \n205 augmentation by randomly permuting the neurons without changing the function. However, in our   \n206 scenario, we find this augmentation will even impede training. We hypothesize that the personalized   \n207 models are finetuned from the same generic model, so they may lie in the same or close loss landscape   \n208 basins; as a result, permutation augmentation will disturb network representations and impair Tina   \n209 training. Further, we develop an effective classifier augmentation strategy to speed up Tina training   \n210 under limited data by randomly permuting the order of classes in the task description and also   \n211 the order of corresponding classifier vectors during training. This data augmentation improves   \n212 sample diversity and helps the DiT better learn the description-to-classifier sequence modeling in a   \n213 position-aware manner.   \n214 Parameter inheritance. In [16], the authors release a pretrained checkpoint of G.pt, which is also   \n215 DiT for parameter generation. G.pt is pretrained on large datasets of optimization checkpoints;   \n216 though it has different conditions, designs, and scenarios from ours, we explore whether we can   \n217 inherit some parameters from the pretrained checkpoints to speed up and boost training. Considering   \n218 the model sizes and architectures are different, we use a strategy similar to bert2BERT [23, 24, 25]   \n219 for inheriting parameters.   \n220 Classification sequence padding. We study how to incorporate more personalized settings where   \n221 diverse users request for tasks with different numbers of classes. In language models [26, 5], padding   \n222 is used to enable sequence-to-sequence learning with different input and output lengths. Inspired   \n223 by this, we use the padding technique to enable the description-to-classifier sequence of different   \n224 classification lengths. Specifically, if the user\u2019s number of classes is smaller than the maximal length,   \n225 we pad missing classes with tokens $\\epsilon_{<->},$ in the task description list and mask the corresponding   \n226 classifier vectors with zero-like tensors. We denote this strategy as classification sequence padding,   \n227 and Tina can learn to adapt to any number of classes within the maximal length. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "228 3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "229 3.1 Experimental Setups ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "230 Datasets and p-Models. We use three datasets to conduct experiments: Mini-ImageNet [27, 28],   \n231 CIFAR-100 [14], and Caltech-101 [29]. Mini-ImageNet is a subset of the ImageNet dataset, primarily   \n232 used for few-shot learning tasks. CIFAR-100 is a popular benchmark dataset for image classification   \n233 tasks. Each class contains 600 images, divided evenly into 20 superclasses and 100 classes. Caltech  \n234 101: A dataset for object recognition featuring diverse images with varied resolutions and quality.   \n235 It includes 101 categories, each containing 40 to 800 images, offering a wide range of objects and   \n236 scenes compared to CIFAR-100 and Mini-ImageNet. For the images with different resolutions, we   \n237 resize them into $32\\times32$ for unified modeling. The personalized tasks are crafted by selecting 10   \n238 classes out of the 100/101 total classes. If not mentioned otherwise, the number of p-Models (i.e.,   \n239 personalized tasks) for training Tina is 1000.   \n240 We use two architectures for personalized models: a simple CNN (dubbed as CNN) and ResNet-20   \n241 (dubbed as ResNet). The CNN architecture follows [16], which consists of 2 layers, and the number   \n242 of parameters is approximately 5K. We take all the parameters of CNN as the input and output of   \n243 Tina. But for ResNet-20, the number of parameters is nearly 272k, which is too large for Tina\u2019s   \n244 generation. Thus, we explore partial parameter generation following [15]. We only personalize the   \n245 5 classifier layers for parameter generation, nearly 640 parameters.   \nFor more details about data preparation and p-Models, please refer to Appendix A in the appendix.   \n47 Compared baselines. We follow the baselines used in the original paper of train-once-for-all   \n48 personalization [13]. As described in subsection 2.1.3, we use the generic model trained in stage   \n49 1 as a baseline, showing the performance without any personalization. Further, we compare the   \n50 classifier selection method described in subsection 2.1.2, which serves as a strong baseline for   \n51 personalization [13]. The vanilla TAPER [13] requires finetuning the expert models on the target   \n52 tasks and cannot generalize on out-of-distribution personalization where only target text descriptions   \n53 are available. For fair comparisons, we adopt TAPER-Mixer, which adopts the mixer of TAPER for   \n54 generating the aggregation weights, and the MLP-based mixer can generalize on unseen tasks.   \n255 Evaluation metrics. For Table 1, we compare in-distribution personalization and out-of-distribution   \n256 personalization as elaborated in subsection 2.1.3. For other tables and figures, we report the out-of  \n257 distribution personalization as p-Acc. ", "page_idx": 4}, {"type": "table", "img_path": "nblJSwtdrJ/tmp/355e7f6f875098f5765032b73ddba689b26ca5afca805ad4ca3ab7babfdd1376.jpg", "table_caption": ["Table 1: Main results across different datasets and models. The best results are in bold "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "258 Hyperparameters. The detailed hyperparameters can be found in subsection A.5 in the appendix. ", "page_idx": 5}, {"type": "text", "text": "259 3.2 Results under Different Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "260 In Table 1, we evaluate the performance of our proposed method, Tina, against several baseline   \n261 methods including Generic Model, Classifier Selection, and TAPER-Mixer across various datasets   \n262 and model architectures for the task of train-once-for-all personalization. It is found that the Generic   \n263 Model has inadequate performance, validating the need for personalization techniques. For the   \n264 personalization methods, the results demonstrate that Tina consistently outperforms all baseline   \n265 methods across both in-distribution and out-of-distribution personalization scenarios. Though Tina is   \n266 a text-to-model foundation model, it is worth noting that Tina shows intelligence of personalization   \n267 under limited data (nearly 1000 samples). Specifically, for in-distribution personalization, Tina   \n268 achieves significant improvements with an average score of 79.94, surpassing the next best method,   \n269 Classifier Selection, by a margin of 3.19. Similarly, for out-of-distribution personalization, Tina leads   \n270 with an average score of 80.55, which is a notable increase over the second-best performing method   \n271 by 2.78. It is notable that TAPER-Mixer shows performance gains over Classifier Selection in CNN   \n272 but has marginal results in ResNet. Also, TAPER-Mixer has inferior performance compared with   \n273 Tina, showing the advantages of Tina as a generative model in parameter generation. TAPER-Mixer   \n274 only learns to merge the expert models, while Tina learns to directly generate the parameters. ", "page_idx": 5}, {"type": "image", "img_path": "nblJSwtdrJ/tmp/09a79b0c1368a338547b12304e064b44d35a2a40e762e1a09b2b3dbe40828c76.jpg", "img_caption": ["(a) Scaling the parameters of DiT. (b) Parameter inheritance. (c) Training images as prompts. ", "Figure 4: Tina capability analysis w.r.t. different parameterization and training schemes. (a) Scaling the parameters of DiT in Tina. CNN-5K (14K) means the p-Model is a CNN with 5K (14K) parameters. From 152M (hidden size 32) to 789M (hidden size 2048), scaling helps in the emergence of intelligence. (b) Parameter inheritance from pretrained G.pt helps speed up training in the early. (c) Training Tina with image-prompted data versus text-prompted data. The text-prompted has faster convergence. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "275 3.3 In-depth Analysis of Tina ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "276 Tina shows great potential for text-to-model generation for personalization. We have made several   \n277 in-depth analyses to better understand the capabilities and boundaries of Tina, and we will show   \n278 insights into how Tina learns hyper-level world knowledge as well as its limitations for future   \n279 research. If not mentioned otherwise, we use CIFAR-100 as the dataset for analyses.   \n280 Scaling studies for Tina. Scaling law was found for   \n281 transformer-based foundation models that scaling the pa  \n282 rameters, data, computes can bring intelligence emergence.   \n283 In Figure 4 (a), we scale the parameters of Tina by chang  \n284 ing the hidden sizes ranging from 32 (152M parameters)   \n285 to 2048 (789M), and we test two sizes of p-Model. It is   \n286 found that when Tina is small, it fails to generalize, espe  \n287 cially when the p-Model has a higher parameter dimension.   \n288 The intelligence emerges when scaling Tina at large sizes   \n289 (e.g., 1024 or 2048 hidden sizes), but the scaling effect   \n290 is saturated if reaching the upper bound performance of   \n291 personalization. We also scale the input, also the gener  \n292 ated, dimensions (i.e., p-Model sizes) and the training data   \n293 in Figure 5. It is found that a larger input dimension is   \n294 harder to learn and requires larger sizes of training data   \n295 to converge and generalize. The generalization of Tina   \n296 can benefit from larger training data, but it has diminishing marginal returns. Generally, larger   \n297 p-Models, larger training samples, and larger model sizes make Tina reach higher p-Acc, and it   \n298 demonstrates the increasing expressive power of Tina by scaling, which is consistent with previous   \n299 DiT works [12, 16, 3]. The scaling property indicates the great potential of Tina for more complex   \n300 and challenging text-to-model scenarios.   \n301 Parameter inheritance. We verify whether Tina can benefti from pretrained parameters. We inherit   \n302 the parameters from G.pt\u2019s [16] checkpoints by the bert2BERT-like method [24]. From Figure 4 (b),   \n303 it is found that parameter inheritance from pretrained models can help Tina to converge faster, but   \n304 the final p-Accs are similar.   \n305 Training images as prompts. In the original design of Tina, the texts are used for the prompts   \n306 encoded by the CLIP text encoder. We train a Tina with image prompts using CLIP image encoder,   \n307 and the results are in Figure 4 (c). For each class, we randomly select one single image as the prompts.   \n308 It is found that text-prompted Tina converges faster than the image-prompted, though the final p-Accs   \n309 are similar. This is intuitive to understand since texts are known to have higher knowledge density   \n310 than images [30, 17], that the class text has richer knowledge representations than a single image.   \n11 Testing images as prompts. We train text-prompted Tina and verify its zero-shot and few-shot   \n2 abilities on image prompts, and the results are in Figure 6 (a). Due to the alignment of texts and   \n3 images in CLIP, Tina shows zero-shot ability on image prompts. By few-shot finetuning on image   \n4 prompts, Tina can reach comparable performances to the text-prompted model. We note that the   \n5 image-prompted ability is important in practical personalization scenarios, because some users may   \n6 have few images and want a personalized model for those. The images are too few to train a model   \n17 from scratch, but thanks to the generative power of Tina, we can generate a p-Model given image   \n18 prompts by utilizing Tina\u2019s vision-language-parameter-aligned knowledge. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "nblJSwtdrJ/tmp/52b354c85791a81aa6dd01a0a79fe2ef26bb1aa0943a49668800363a340acf48.jpg", "img_caption": ["Figure 5: Scaling the input dimensions and training data for Tina. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "nblJSwtdrJ/tmp/92c99c182394247a38f2805819b82794b7286fa0f61d541776f7906f6062dd46.jpg", "img_caption": ["Figure 6: Tina capability analysis w.r.t. different prompt schemes. (a) Train text-prompted Tina and verify the zero-shot and few-shot abilities of using images as prompts. (b) The accuracies of $\\mathbf{p}$ -Models generated by Tina vary with different numbers of classes. Classification sequence padding is used, and the maximal sequence length is 10. (c) Train class-name-conditioned Tina and verify its zero-shot ability on the natural language descriptions generated by GPT-4. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "nblJSwtdrJ/tmp/20e23136a09d8110db0a8b0f983dc607f72c06912ce048e164955a6792e0b12c.jpg", "table_caption": ["Table 2: Zero-shot transfer of Tina to unseen classes. We test the generalization capability of Tina to unseen classes that have similar textual similarity with the seen ones. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Varying the number of personalized classes. Without changing architecture, Tina can adapt to any personalized classes within the maximal supported length due to the padding design. In Figure 6 (b), we test the p-Models with different numbers of classes, generated by one Tina. The maximal classification length is 10. It is shown that the generated $\\mathbf{p}$ -Models reach higher p-Accs when the number of classes is fewer, which is consistent with common sense that fewer classes are easier to personalize. ", "page_idx": 7}, {"type": "text", "text": "24 How Tina understands world knowledge I: natural language descriptions as prompts. In our   \n5 implementation of Tina, we adopt a simple prompting that uses the class names as the text prompts.   \n6 We verify whether Tina actually learns the knowledge in the case where the prompts are replaced by the natural language descriptions at test time. We generate the language descriptions of classes   \n8 with the assistance of GPT-4 [31], and we make sure that the descriptions do not include the original   \n9 class entities. The exemplars are in Table 4 of the appendix. From Figure 6 (c), the results reveal that Tina has zero-shot generalization ability when the prompts are unseen language descriptions, though the p-Accs are lower than the ones of the class-named prompts. It shows that Tina is not   \n2 just memorizing the class names but also generalizing and understanding the knowledge behind the names and the nuances inherent in the text semantics. ", "page_idx": 7}, {"type": "text", "text": "334 How Tina understands world knowledge II: generalization to unseen classes/entities. We divide   \n335 the CIFAR-100 dataset into two disjoint shards of classes and train a Tina on one shard, then verify its   \n336 generalization on the unseen classes of another shard. Results in Table 2 showcase that Tina has the   \n337 intelligence to generalize on unseen classes, while TAPER-Mixer fails when meeting $100\\%$ unseen   \n338 classes. As a generative model, Tina can understand the hyper-level world knowledge embedded in   \n339 model parameters as well as text semantics and generate models for predicting unseen entities. ", "page_idx": 7}, {"type": "text", "text": "340 3.4 Ablation of Design Choices of Tina ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "341 We make an ablation study for different design choices of Tina. The ablated designs are the ones dif  \n342 ferent from previous literature, such as our design of classifier augmentation, G.pt\u2019s design of permu  \n343 tation augmentation [16], and TAPER\u2019s design of merge text embedding as one [13]. The results are   \n344 in Table 3. Our classifier augmentation can boost the performance even under small training datasets.   \n345 Permutation augmentation has neg  \n346 ative effects on generating person  \n347 alized models, and we hypothesize   \n348 that for Tina\u2019s training data, the   \n349 p-Models finetuned from the same   \n350 generic model are located in a com  \n351 mon loss basin, where permutations   \n352 will disturb the shared representations.   \n353 In addition, merging the text embeddings into one will hinder the DiT from learning the sequential   \n354 classifications, making Tina bad in generalization. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "nblJSwtdrJ/tmp/e0ce4a996de6115ed6d41ef14348d94516e00dae97fdd68a0506d61d3f220122.jpg", "table_caption": ["Table 3: Ablation study for different design choices of Tina. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "355 4 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "356 Diffusion models. Originating from non-equilibrium thermodynamics [32, 33], diffusion models   \n357 have evolved significantly. DDPM and DDIM pioneered forward-and-reverse processes in text-to  \n358 image generation [9, 34]. Guided-based diffusion models [35] surpassed GAN-based methods in   \n359 image generation quality. Subsequent models like GLIDE [36], Imagen [37], DALL\u00b7E 2 [2], and   \n360 stable diffusion [4] further advanced image generation and art creation. The diffusion transformer   \n361 (DiT) [12] introduced a scaling law, with OpenAI\u2019s Sora [3] being a notable application in text-to  \n362 video generation, employing DiT architecture at a billion-scale.   \n363 Parameter generation. Learning to optimize explores neural networks learning update rules for   \n364 others [38, 39, 40, 41]. Hypernetwork [42] is a meta learning approach that uses networks to modify   \n365 neural network parameters, differing from our approach of mapping language space directly to   \n366 parameter space. Hypernetworks are used in federated learning [43], few-shot learning [44], and   \n367 model editing [45]. A concurrent work ModelGPT [46] customizes models by large language   \n368 models and hypernetworks, while Tina uses conditional neural network diffusion for a different   \n369 task\u2014train-once-for-all personalization. Neural network diffusion [16, 15] is recently proposed to   \n370 mimic optimization rules via diffusion for parameter generation, but previous works haven\u2019t explored   \n371 sufficient use cases of such techniques. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "372 For more detailed related works (e.g., the works about personalization), please refer to Appendix B. ", "page_idx": 8}, {"type": "text", "text": "373 5 Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "374 Limitations. Despite the merits of Tina, it has some current limitations. One bottleneck is the input   \n375 dimension; due to our computation limits, Tina currently supports lightweight models as inputs, and   \n376 it requires huge computation resources to fully generate large models with millions of parameters. On   \n377 the one hand, a larger input dimension needs exponentially larger Tina parameters, so more GPUs.   \n378 On the other hand, a larger input dimension needs more data to converge or generalize, requiring   \n379 more compute hours. As a remedy, we tried to train a variational autoencoder (VAE) for encoding   \n380 the p-Model parameters into a low-dimension latent space as in [15], but the VAE cannot generalize,   \n381 suggesting more advanced techniques are needed. Another limitation is the generality of Tina, that   \n382 one single Tina cannot generate personalized models across different sizes and different modalities;   \n383 in the future, large-scaling pretraining for Tina may be promising to reach this goal.   \n384 Broader impacts. Tina is the preliminary work of text-to-model generation and will have broader   \n385 impacts on the machine learning community, especially in the field of generative AI and model person  \n386 alization. Though in this initial version of Tina, we only showcase its great potential in image classi  \n387 fication tasks, Tina is prospective in a wide range of applications and tasks, such as natural language   \n388 processing, audio recognition, and recommender system. Also, Tina has opened more potential direc  \n389 tions for neural network diffusion, and we believe it can inspire more interesting works in the future. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "390 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "391 In this paper, we present Tina, a text-to-model neural network diffusion model for train-once-for-all   \n392 personalization. Tina has shown its great capability in generating personalized models from text   \n393 prompts, and it can generalize to in-distribution as well as out-of-distribution tasks, zero-shot/few-shot   \n394 image prompts, natural language prompts, and unseen classes. Tina also supports personalization   \n395 under different numbers of classes. This paper explores the potential of text-to-model generative AI   \n396 and opens new applications for neural network diffusion in end-user personalization. ", "page_idx": 8}, {"type": "text", "text": "397 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "398 [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n399 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n400 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n401 [2] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical   \n402 text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3,   \n403 2022.   \n404 [3] OpenAI. Creating video from text. https://openai.com/sora, February 15 2024.   \n405 [4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High  \n406 resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF   \n407 conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n408 [5] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n409 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open   \n410 foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n411 [6] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece   \n412 Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general   \n413 intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n414 [7] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language   \n415 understanding by generative pre-training.   \n416 [8] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n417 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n418 [9] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic   \n419 models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n420 [10] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image   \n421 diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer   \n422 Vision, pages 3836\u20133847, 2023.   \n423 [11] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,   \n424 Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without   \n425 text-video data. In The Eleventh International Conference on Learning Representations, 2023.   \n426 [12] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings   \n427 of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n428 [13] Hong-You Chen, Yandong Li, Yin Cui, Mingda Zhang, Wei-Lun Chao, and Li Zhang. Train  \n429 once-for-all personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n430 and Pattern Recognition, pages 11818\u201311827, 2023.   \n431 [14] Alex Krizhevsky. Learning multiple layers of features from tiny images. In Technical report,   \n432 University of Toronto, 2009.   \n433 [15] Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, and Yang You.   \n434 Neural network diffusion. arXiv preprint arXiv:2402.13144, 2024.   \n435 [16] William Peebles, Ilija Radosavovic, Tim Brooks, Alexei A Efros, and Jitendra Malik. Learning   \n436 to learn with generative models of neural network checkpoints. arXiv preprint arXiv:2209.12892,   \n437 2022.   \n438 [17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,   \n439 Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual   \n440 models from natural language supervision. In International conference on machine learning,   \n441 pages 8748\u20138763. PMLR, 2021.   \n442 [18] Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, and Yong Li. Spatio-temporal few-shot   \n443 learning via diffusive neural network generation. arXiv preprint arXiv:2402.11922, 2024.   \n444 [19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoor  \n445 thi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis.   \n446 Communications of the ACM, 65(1):99\u2013106, 2021.   \n447 [20] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation   \n448 invariance in linear mode connectivity of neural networks. In International Conference on   \n449 Learning Representations, 2022.   \n450 [21] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models   \n451 modulo permutation symmetries. In The Eleventh International Conference on Learning   \n452 Representations, 2023.   \n453 [22] Zexi Li, Zhiqi Li, Jie Lin, Tao Shen, Tao Lin, and Chao Wu. Training-time neuron alignment   \n454 through permutation subspace for improving linear mode connectivity and model fusion. arXiv   \n455 preprint arXiv:2402.01342, 2024.   \n456 [23] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowl  \n457 edge transfer. arXiv preprint arXiv:1511.05641, 2015.   \n458 [24] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao   \n459 Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models.   \n460 [25] Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan   \n461 Liu, Peng Li, Maosong Sun, et al. Knowledge inheritance for pre-trained language models.   \n462 In Proceedings of the 2022 Conference of the North American Chapter of the Association for   \n463 Computational Linguistics: Human Language Technologies, pages 3921\u20133937, 2022.   \n464 [26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of   \n465 deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,   \n466 2018.   \n467 [27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large  \n468 scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern   \n469 Recognition, pages 248\u2013255, 2009.   \n470 [28] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra.   \n471 Matching networks for one shot learning. Advances in Neural Information Processing Systems,   \n472 2016.   \n473 [29] Li Fei-Fei, Rob Fergus, and Pietro Perona. Caltech-101: Object categories and the localized   \n474 features. Technical report, California Institute of Technology, 2004.   \n475 [30] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan   \n476 Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning   \n477 with noisy text supervision. In International conference on machine learning, pages 4904\u20134916.   \n478 PMLR, 2021.   \n479 [31] OpenAI and the co authors. Gpt-4 technical report, 2024.   \n480 [32] Christopher Jarzynski. Equilibrium free-energy differences from nonequilibrium measurements:   \n481 A master-equation approach. Physical Review E, 56(5):5018, 1997.   \n482 [33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper  \n483 vised learning using nonequilibrium thermodynamics. In International conference on machine   \n484 learning, pages 2256\u20132265. PMLR, 2015.   \n485 [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv   \n486 preprint arXiv:2010.02502, 2020.   \n487 [35] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.   \n488 Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n489 [36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,   \n490 Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing   \n491 with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n492 [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,   \n493 Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.   \n494 Photorealistic text-to-image diffusion models with deep language understanding. Advances in   \n495 neural information processing systems, 35:36479\u201336494, 2022.   \n496 [38] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom   \n497 Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by   \n498 gradient descent. Advances in neural information processing systems, 29, 2016.   \n499 [39] Brandon Amos. Tutorial on amortized optimization for learning to optimize over continuous   \n500 domains. arXiv e-prints, pages arXiv\u20132202, 2022.   \n501 [40] Luke Metz, James Harrison, C Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury,   \n502 Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, et al. Velo: Training versatile   \n503 learned optimizers by scaling up. arXiv preprint arXiv:2211.09760, 2022.   \n504 [41] Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, and Erik Meijer. Gradient descent: The   \n505 ultimate optimizer. Advances in Neural Information Processing Systems, 35:8214\u20138225, 2022.   \n506 [42] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106,   \n507 2016.   \n508 [43] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning   \n509 using hypernetworks. In International Conference on Machine Learning, pages 9489\u20139502.   \n510 PMLR, 2021.   \n511 [44] Andrey Zhmoginov, Mark Sandler, and Maksym Vladymyrov. Hypertransformer: Model   \n512 generation for supervised and semi-supervised few-shot learning. In International Conference   \n513 on Machine Learning, pages 27075\u201327098. PMLR, 2022.   \n514 [45] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast   \n515 model editing at scale. In International Conference on Learning Representations, 2022.   \n516 [46] Zihao Tang, Zheqi Lv, Shengyu Zhang, Fei Wu, and Kun Kuang. Modelgpt: Unleashing llm\u2019s   \n517 capabilities for tailored model generation. arXiv preprint arXiv:2402.12408, 2024.   \n518 [47] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil   \n519 Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural   \n520 information processing systems, 27, 2014.   \n521 [48] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil   \n522 Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications   \n523 of the ACM, 63(11):139\u2013144, 2020.   \n524 [49] Jeremy Goecks, Vahid Jalili, Laura M Heiser, and Joe W Gray. How machine learning will   \n525 transform biomedicine. Cell, 181(1):92\u2013101, 2020.   \n526 [50] Jamilu Awwalu, Ali Garba Garba, Anahita Ghazvini, and Rose Atuah. Artificial intelligence in   \n527 personalized medicine application of ai algorithms in solving personalized medicine problems.   \n528 International Journal of Computer Theory and Engineering, 7(6):439, 2015.   \n529 [51] Zexi Li, Feng Mao, and Chao Wu. Can we share models if sharing data is not an option?   \n530 Patterns, 3(11), 2022.   \n531 [52] Sang Hyun Choi, Sungmin Kang, and Young Jun Jeon. Personalized recommendation system   \n532 based on product specification values. Expert Systems with Applications, 31(3):607\u2013616, 2006.   \n533 [53] Zhihua Cui, Xianghua Xu, XUE Fei, Xingjuan Cai, Yang Cao, Wensheng Zhang, and Jinjun   \n534 Chen. Personalized recommendation system based on collaborative filtering for iot scenarios.   \n535 IEEE Transactions on Services Computing, 13(4):685\u2013695, 2020.   \n536 [54] Hannah Rose Kirk, Bertie Vidgen, Paul R\u00f6ttger, and Scott A Hale. The benefits, risks and   \n537 bounds of personalizing the alignment of large language models to individuals. Nature Machine   \n538 Intelligence, pages 1\u201310, 2024.   \n539 [55] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu,   \n540 Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the   \n541 capability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024.   \n542 [56] Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning   \n543 for image classification. In International Conference on Learning Representations, 2022.   \n544 [57] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated   \n545 learning through personalization. In International Conference on Machine Learning, pages   \n546 6357\u20136368. PMLR, 2021.   \n547 [58] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia   \n548 Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning   \n549 and Systems, 2:429\u2013450, 2020.   \n550 [59] Lingzhi Gao, Zexi Li, Yang Lu, and Chao Wu. Fedios: Decoupling orthogonal subspaces for   \n551 personalization in feature-skew federated learning. arXiv preprint arXiv:2311.18559, 2023.   \n552 [60] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework   \n553 for clustered federated learning. Advances in Neural Information Processing Systems, 33:19586\u2013   \n554 19597, 2020.   \n555 [61] Zexi Li, Jiaxun Lu, Shuang Luo, Didi Zhu, Yunfeng Shao, Yinchuan Li, Zhimeng Zhang,   \n556 Yongheng Wang, and Chao Wu. Towards effective clustered federated learning: A peer-to-peer   \n557 framework with adaptive neighbor matching. IEEE Transactions on Big Data, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "559 A Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "560 A.1 Dataset Preparation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "561 Mini-ImageNet. The Mini-ImageNet dataset [28] is a sub-dataset of ImageNet [27], which is   \n562 widely used in few-shot learning. It selects 100 categories from ImageNet1K. The trainset contains   \n563 600 labeled images for each category, a total 60,000 images, and the testset contains 100 labeled   \n564 images for each category, a total of 10,000 pieces.   \n565 CIFAR-100. Each image in CIFAR-100 [14] has two labels: superclass and subclass. There are   \n566 500 training images and 100 testing images per subclass. CIFAR-100 has 20 superclasses, and each   \n567 superclass has 5 subclasses.   \n568 Caltech-101. Caltech-101 [29] is an objects image dataset with 101 categories. Approximately 40   \n569 to 800 images per category, most categories have around 50 images, 8677 images in total. We divide   \n570 it into a trainset and a testset according to the ratio of 8:2.   \n571 When creating the p-Model datasets, we strive to maintain a consistent frequency of occurrences   \n572 for each class, while simultaneously varying the combinations of different classes in various orders.   \n573 For each dataset, we randomly permute the order of all classes, divide them into ten classes, and   \n574 train on the respective classes to construct p-Models. This approach allows us to generate 10 distinct   \n575 class models for each dataset. We utilize various random seeds to control the generation of class   \n576 combinations, ensuring we acquire sufficient ${\\bf p}$ -Models. We randomly selected 150 data from the   \n577 original training data as the out-of-distribution testset.   \n578 For CIFAR-100, it has two classification methods: superclass and subclass. In order to increase the   \n579 diversity and semantics of p-Model data, we use a more complex way to set up the classes included   \n580 in each model. (1) The classes trained by each model come from different superclasses. This ensures   \n581 a wide range of semantic variations. (2) Part of the classes trained by each model come from the   \n582 same superclass. The selection of these classes is done randomly. (3) The classes trained by each   \n583 model only come from two different superclasses. In the trainset and testset, we distribute these three   \n584 division methods in quantity according to 3:2:1. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "585 A.2 Example of class description from GPT-4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "586 For the word of each class, we use GPT-4 to provide a more detailed and standardized description   \n587 and definition. Some examples are shown in Table 4. ", "page_idx": 13}, {"type": "table", "img_path": "nblJSwtdrJ/tmp/ab9b19d7388dd1b891828d0acd1695cbbf64e8154e50f74e7306d04080a501e5.jpg", "table_caption": ["Table 4: Natural language descriptions of the class names from GPT4. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "588 A.3 Data Preparation for Experiments of Unseen Classes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "589 We divide the 100 classes in CIFAR-100 evenly into two groups/shards. The classes belonging to one   \n590 group serve as the training model data, while the classes in the other group are intentionally excluded   \n591 from appearing during the training process. When making these divisions, we take care to distribute   \n592 categories with similar characteristics into separate groups. For instance, we separate the apple and   \n593 the orange, both being common fruits, into different groups. Similarly, the bear and the lion, both   \n594 large carnivorous mammals, are divided, and the boy and the man, both representing the male gender,   \n595 are also separated accordingly. ", "page_idx": 13}, {"type": "text", "text": "596 A.4 Detailed Implementations of Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "597 We first train the model on the entire dataset for 50 epochs to obtain a stage-one model. ", "page_idx": 13}, {"type": "text", "text": "598 Classifier Selection: Based on the stage-one model, for each classification task, we only retain the vec  \n599 tor representing the corresponding class on the classifier and set the vectors for all other classes to zero.   \n600 TAPER-Mixer: We set up two base models and split the dataset into two shards based on the   \n601 classification labels. Each base model is initialized using the parameters of the stage-one model   \n602 and fine-tuned on one of the sharded datasets for 5 epochs. In stage 3, we use the class order of   \n603 the p-Model in the trainset to train the mixer for 5 epochs, and during the testing phase, the mixer   \n604 remains frozen.   \n605 Tina: For each p-Model data, we initialize it using the parameters of the stage-one generic model as   \n606 a starting point. At the same time, each class is sequentially reorganized as labels ranging from 0 to 9   \n607 for training. We fine-tune the generic model for 10 epochs to obtain the p-Models. For ResNet-20, we   \n608 only fine-tune the parameters of the classifier, while keeping the remaining network parameters frozen. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "609 A.5 Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "610 In all experiments, we use the same hyperparameters for training. For the model structure, we set the   \n611 hidden size to 2048, and the number of the encoder and decoder is 1. Each encoder and decoder has   \n612 12 layers, and each self-attention layer has 16 attention heads. For the training process, we divide   \n613 the model parameters into chunks by layer, and the size of each chunk is 576. We set batch size 64,   \n614 learning rate $4e^{-4}$ , and the gradient clipping coefficient to 0.1. ", "page_idx": 14}, {"type": "text", "text": "615 A.6 Environments and Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "616 All our experiments are conducted on CPU Intel(R) Xeon(R) Silver 4210 CPU $\\textcircled{a}\\ 2.20\\mathrm{GHZ}$ . We   \n617 employ two Quadro RTX 8000 for data-parallel distributed training. When Tina generates a CNN   \n618 neural network with 5,000 parameters, each GPU requires 20,000MB of memory, and training for   \n619 300 epochs takes approximately 5 hours. ", "page_idx": 14}, {"type": "text", "text": "620 B Detailed Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "621 Diffusion models The origin of diffusion models is the study of non-equilibrium thermodynam  \n622 ics [32, 33]. In recent years, DDPM [9] and DDIM [34] have refined diffusion models to a higher level   \n623 by transforming the paradigm into forward-and-reverse processes in text-to-image generation. Later   \n624 on, guided-based diffusion models [35] found a better architecture to improve the image generation   \n625 quality that could beat the GAN-based methods [47, 48]. Then, GLIDE [36], Imagen [37], DALL\u00b7E   \n626 2 [2], and stable diffusion [4] emerged and flourished in the field of image generation and art creation.   \n627 In the work of diffusion transformer (DiT) [12], the authors found that if the basic architecture of   \n628 diffusion models is changed to transformers, the scaling law emerges, that scaling the number of   \n629 parameters can reach the increasing quality of image generation. Based on DiT, in Feb 2024, OpenAI   \n630 launched Sora [3], a text-to-video model that can understand and simulate the physical world in   \n631 motion. In Sora, the DiT architecture is used and scaled to the billions level.   \n632 Parameter generation The field of learning to optimize studies how one neural network can learn   \n633 the update rules (gradients) for optimizing another network [38, 39, 40, 41]. Besides, the studies   \n634 of hypernetworks [42] focus on how to directly output or modify neural networks\u2019 parameters by a   \n635 hypernetwork. Hypernetworks usually take models\u2019 parameters as input and generate parameters [43,   \n636 45], which is different from our paper, which directly maps language space into the parameter   \n637 space. Hypernetworks were used to generate local models for federated learning [43], edge-cloud   \n638 collaboration, few-shot learning [44], and model editing [45]. A concurrent work ModelGPT [46] also   \n639 uses text prompts to generate customized models. However, ModelGPT didn\u2019t target the train-once  \n640 for-all personalization scenario, and it uses conventional hypernetwork and meta learning methods   \n641 while our Tina adopts novel conditional neural network diffusion. Recently, empowered by the   \n642 strong expressiveness of diffusion models, neural network diffusion [16, 15] was proposed to mimic   \n643 the optimization rule by diffusion for generating the model parameters. The first paper is G.pt [16],   \n644 which uses DiT to learn to generate the model given a targeted loss or accuracy, and it mimics the   \n645 optimization process while achieving faster inference compared with vanilla optimization. However,   \n646 G.pt may have limited use cases; it can only generate the models for the training tasks (i.e., the   \n647 in-distribution in our paper\u2019s terminology), and the accuracies are upper-bounded by the accuracies   \n648 of checkpoint models in the training datasets. p-diff [15] formally formulates the neural network   \n649 diffusion problem and proposes to diffuse and generate the batch normalization layers for better   \n650 accuracies, but the improvement may be marginal, and the diffusion design is not conditioned. It also   \n651 meets the dilemma of G.pt, which lacks a specific scenario and use case. Recently, GPD [18] uses the   \n652 diffusion model for few-shot learning in smart city applications, which showcases the applications of   \n653 neural network diffusion. However, GPD takes the smart city\u2019s knowledge graphs as prompts and   \n654 is tailored for the specific smart city application that cannot be easily extended to other fields. Our   \n655 Tina takes language texts as prompts, which is more flexible and can be extended to a wider range of   \n656 applications for the personalization of user demands.   \n657 Personalization Instead of training a generic model to provide many users with the same model   \n658 service, personalization of deep learning models acknowledges users\u2019 characteristics and diversity and   \n659 learns each a customized model. Personalization techniques were introduced in medical AI [49, 50,   \n660 51], recommendation systems [52, 53], large language models [54, 55], and especially federated learn  \n661 ing [56, 57]. Personalized federated learning studies how to exploit the common knowledge of users   \n662 and then use it to explore further personalization on users\u2019 local datasets under privacy constraints [56],   \n663 and techniques like proximal descent [58, 57], network decoupling [56, 59], and clustering [60, 61]   \n664 are used. Recently, the scenario of train-once-for-all personalization [13] was proposed to bridge the   \n665 gap between edge-side and server-side personalization. Train-once-for-all personalization aims to   \n666 utilize server-side computation and generic models for fast and effective personalized adaptation to   \n667 meet the edge users\u2019 demands. The original method TAPER [13] finetunes the generic model into   \n668 several base models and learns MLP-based hypernetworks as mixers to fuse the base models into the   \n669 personalized one given users\u2019 task descriptions. However, the MLP mixer has limited generalization   \n670 capability, and it cannot be applied to unseen tasks, whereas our Tina learns the text-to-model world   \n671 knowledge and can be generalized to out-of-distribution samples, modalities, and domains. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "672 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "673 1. Claims   \n674 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n675 paper\u2019s contributions and scope?   \n676 Answer: [Yes]   \n677 Justification: See the paper for details.   \n678 Guidelines:   \n679 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n680 made in the paper.   \n681 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n682 contributions made in the paper and important assumptions and limitations. A No or   \n683 NA answer to this question will not be perceived well by the reviewers.   \n684 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n685 much the results can be expected to generalize to other settings.   \n686 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n687 are not attained by the paper.   \n688 2. Limitations   \n689 Question: Does the paper discuss the limitations of the work performed by the authors?   \n690 Answer: [Yes]   \n691 Justification: See section 5.   \n692 Guidelines:   \n693 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n694 the paper has limitations, but those are not discussed in the paper.   \n695 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n696 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n697 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n698 model well-specification, asymptotic approximations only holding locally). The authors   \n699 should reflect on how these assumptions might be violated in practice and what the   \n700 implications would be.   \n701 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n702 only tested on a few datasets or with a few runs. In general, empirical results often   \n703 depend on implicit assumptions, which should be articulated.   \n704 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n705 For example, a facial recognition algorithm may perform poorly when image resolution   \n706 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n707 used reliably to provide closed captions for online lectures because it fails to handle   \n708 technical jargon.   \n709 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n710 and how they scale with dataset size.   \n711 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n712 address problems of privacy and fairness.   \n713 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n714 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n715 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n716 judgment and recognize that individual actions in favor of transparency play an impor  \n717 tant role in developing norms that preserve the integrity of the community. Reviewers   \n718 will be specifically instructed to not penalize honesty concerning limitations.   \n719 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n720   \n721 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "25 \u2022 The answer NA means that the paper does not include theoretical results.   \n26 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n27 referenced.   \n28 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n29 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n30 they appear in the supplemental material, the authors are encouraged to provide a short   \n31 proof sketch to provide intuition.   \n32 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n33 by formal proofs provided in appendix or supplemental material.   \n34 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "735 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "736 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n737 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n738 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have provided the implementation details and the hyperparameters in Appendix A. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "775 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n776 tions to faithfully reproduce the main experimental results, as described in supplemental   \n777 material?   \n778 Answer: [Yes]   \n779 Justification: The datasets we used are all public datasets that are available to anyone. We   \n780 will release the codes upon acceptance.   \n781 Guidelines:   \n782 \u2022 The answer NA means that paper does not include experiments requiring code.   \n783 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n784 public/guides/CodeSubmissionPolicy) for more details.   \n785 \u2022 While we encourage the release of code and data, we understand that this might not be   \n786 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n787 including code, unless this is central to the contribution (e.g., for a new open-source   \n788 benchmark).   \n789 \u2022 The instructions should contain the exact command and environment needed to run to   \n790 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n791 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n792 \u2022 The authors should provide instructions on data access and preparation, including how   \n793 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n794 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n795 proposed method and baselines. If only a subset of experiments are reproducible, they   \n796 should state which ones are omitted from the script and why.   \n797 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n798 versions (if applicable).   \n799 \u2022 Providing as much information as possible in supplemental material (appended to the   \n800 paper) is recommended, but including URLs to data and code is permitted.   \n801 6. Experimental Setting/Details   \n802 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n803 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n804 results?   \n805 Answer: [Yes]   \n806 Justification: We have provided the implementation details and the hyperparameters in   \n807 Appendix A and the corresponding captions.   \n808 Guidelines:   \n809 \u2022 The answer NA means that the paper does not include experiments.   \n810 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n811 that is necessary to appreciate the results and make sense of them.   \n812 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n813 material.   \n814 7. Experiment Statistical Significance   \n815 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n816 information about the statistical significance of the experiments?   \n817 Answer: [Yes]   \n818 Justification: We have provided experiments on several datasets and models to support the   \n819 statistical significance.   \n820 Guidelines:   \n821 \u2022 The answer NA means that the paper does not include experiments.   \n822 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n823 dence intervals, or statistical significance tests, at least for the experiments that support   \n824 the main claims of the paper.   \n825 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n826 example, train/test split, initialization, random drawing of some parameter, or overall   \n827 run with given experimental conditions).   \n828 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n829 call to a library function, bootstrap, etc.)   \n830 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n831 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n832 of the mean.   \n833 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n834 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n835 of Normality of errors is not verified.   \n836 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n837 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n838 error rates).   \n839 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n840 they were calculated and reference the corresponding figures or tables in the text.   \n841 8. Experiments Compute Resources   \n842 Question: For each experiment, does the paper provide sufficient information on the com  \n843 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n844 the experiments?   \n845 Answer: [Yes]   \n846 Justification: See Appendix A for details.   \n847 Guidelines:   \n848 \u2022 The answer NA means that the paper does not include experiments.   \n849 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n850 or cloud provider, including relevant memory and storage.   \n851 \u2022 The paper should provide the amount of compute required for each of the individual   \n852 experimental runs as well as estimate the total compute.   \n853 \u2022 The paper should disclose whether the full research project required more compute   \n854 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n855 didn\u2019t make it into the paper).   \n856 9. Code Of Ethics   \n857 Question: Does the research conducted in the paper conform, in every respect, with the   \n858 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \nAnswer: [Yes]   \n60 Justification: As it is.   \n861 Guidelines:   \n62 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n63 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n64 deviation from the Code of Ethics.   \n865 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n66 eration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "867 10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "868 Question: Does the paper discuss both potential positive societal impacts and negative   \n869 societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 19}, {"type": "text", "text": "880 \u2022 The conference expects that many papers will be foundational research and not tied   \n881 to particular applications, let alone deployments. However, if there is a direct path to   \n882 any negative applications, the authors should point it out. For example, it is legitimate   \n883 to point out that an improvement in the quality of generative models could be used to   \n884 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n885 that a generic algorithm for optimizing neural networks could enable people to train   \n886 models that generate Deepfakes faster.   \n887 \u2022 The authors should consider possible harms that could arise when the technology is   \n888 being used as intended and functioning correctly, harms that could arise when the   \n889 technology is being used as intended but gives incorrect results, and harms following   \n890 from (intentional or unintentional) misuse of the technology.   \n891 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n892 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n893 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n894 feedback over time, improving the efficiency and accessibility of ML).   \n895 11. Safeguards   \n896 Question: Does the paper describe safeguards that have been put in place for responsible   \n897 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n898 image generators, or scraped datasets)?   \n899 Answer: [NA]   \n900 Justification: NA   \n901 Guidelines:   \n902 \u2022 The answer NA means that the paper poses no such risks.   \n903 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n904 necessary safeguards to allow for controlled use of the model, for example by requiring   \n905 that users adhere to usage guidelines or restrictions to access the model or implementing   \n906 safety filters.   \n907 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n908 should describe how they avoided releasing unsafe images.   \n909 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n910 not require this, but we encourage authors to take this into account and make a best   \n911 faith effort.   \n912 12. Licenses for existing assets   \n913 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n914 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n915 properly respected?   \n916 Answer: [Yes]   \n917 Justification: As it is.   \n918 Guidelines:   \n919 \u2022 The answer NA means that the paper does not use existing assets.   \n920 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n921 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n922 URL.   \n923 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n924 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n925 service of that source should be provided.   \n926 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n927 package should be provided. For popular datasets, paperswithcode.com/datasets   \n928 has curated licenses for some datasets. Their licensing guide can help determine the   \n929 license of a dataset.   \n930 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n931 the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to ", "page_idx": 21}, {"type": "text", "text": "933 the asset\u2019s creators.   \n934 13. New Assets   \n935 Question: Are new assets introduced in the paper well documented and is the documentation   \n936 provided alongside the assets?   \n937 Answer: [NA]   \n938 Justification: NA   \n939 Guidelines:   \n940 \u2022 The answer NA means that the paper does not release new assets.   \n941 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n942 submissions via structured templates. This includes details about training, license,   \n943 limitations, etc.   \n944 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n945 asset is used.   \n946 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n947 create an anonymized URL or include an anonymized zip file.   \n948 14. Crowdsourcing and Research with Human Subjects   \n949 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n950 include the full text of instructions given to participants and screenshots, if applicable, as   \n951 well as details about compensation (if any)?   \n952 Answer: [NA]   \n953 Justification: NA   \n954 Guidelines:   \n955 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n956 human subjects.   \n957 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n958 tion of the paper involves human subjects, then as much detail as possible should be   \n959 included in the main paper.   \n960 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n961 or other labor should be paid at least the minimum wage in the country of the data   \n962 collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "65 Question: Does the paper describe potential risks incurred by study participants, whether   \n66 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n67 approvals (or an equivalent approval/review based on the requirements of your country or   \n68 institution) were obtained?   \n69 Answer: [NA]   \n70 Justification: NA   \n71 Guidelines:   \n72 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n973 human subjects.   \n974 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n975 may be required for any human subjects research. If you obtained IRB approval, you   \n76 should clearly state this in the paper.   \n77 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n78 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n79 guidelines for their institution.   \n980 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n81 applicable), such as the institution conducting the review. ", "page_idx": 21}]