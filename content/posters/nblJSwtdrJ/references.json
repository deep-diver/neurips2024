{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational for large language models and their ability to perform well on various tasks with limited training data, a concept central to the current research."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with clip latents", "publication_date": "2022-00-00", "reason": "This paper details a key advance in text-to-image generation, showcasing the power of combining CLIP with latent diffusion models, providing inspiration for text-to-model generation."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper is a seminal work on high-resolution image generation using diffusion models, an important building block in the train-once-for-all personalization approach."}, {"fullname_first_author": "Alexander Quinn Nichol", "paper_title": "Improved denoising diffusion probabilistic models", "publication_date": "2021-00-00", "reason": "This paper significantly advanced the denoising diffusion probabilistic models, improving its efficiency and performance, which underpins several models used in this research."}, {"fullname_first_author": "Hong-You Chen", "paper_title": "Train-once-for-all personalization", "publication_date": "2023-00-00", "reason": "This paper introduced the train-once-for-all personalization concept, providing the core problem and setup that the current research addresses and expands upon."}]}