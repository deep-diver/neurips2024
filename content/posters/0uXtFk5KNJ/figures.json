[{"figure_path": "0uXtFk5KNJ/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the proposed BAdam, which is based on the block coordinate descent framework. Colors represent the states of the partitioned blocks in one block-epoch, including the active block, non-updated inactive blocks, and updated inactive blocks.", "description": "This figure illustrates the BAdam optimization method, which is based on block coordinate descent (BCD).  It shows how the algorithm iteratively updates blocks of parameters. Each color represents the status of a block: the active block is being updated using K Adam steps, and inactive blocks are either not yet updated in this block-epoch, or already updated in a previous step within this block-epoch. The subproblem for updating the active block is shown, along with the concrete implementation using K Adam steps. The figure visually demonstrates how BAdam divides the model into D blocks and efficiently updates each one sequentially.", "section": "2 The BAdam Method"}, {"figure_path": "0uXtFk5KNJ/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of the proposed BAdam, which is based on the block coordinate descent framework. Colors represent the states of the partitioned blocks in one block-epoch, including the active block, non-updated inactive blocks, and updated inactive blocks.", "description": "This figure illustrates the BAdam optimization method, which uses a block coordinate descent approach.  The diagram shows how the model parameters are partitioned into blocks. In each block-epoch, only one block (the active block) is updated using Adam's update rule, while the other blocks remain unchanged (inactive blocks).  The colors visually represent the different states of the blocks in each step of the process (active, non-updated inactive, and updated inactive).  This visual representation clarifies the memory efficiency of the method, as only one block needs to be stored and updated in memory at any given time.", "section": "2 The BAdam Method"}, {"figure_path": "0uXtFk5KNJ/figures/figures_6_1.jpg", "caption": "Figure 2: Optimization capability of BAdam for finetuning Llama 3-8B on Alpaca-GPT4 dataset. Left: Online training loss of LoRA and BAdam. Middle: Cumulative explained variance of BAdam's learned perturbation to the 25th layer's up-proj matrix. Right: Effective rank of Adam's and BAdam's learned perturbations.", "description": "This figure shows the optimization capability of BAdam by comparing it with LoRA. The left panel shows the training loss curves for both methods, indicating BAdam's faster convergence. The middle panel displays the cumulative explained variance of BAdam's learned perturbation, suggesting a high-rank update. The right panel compares the effective rank of the learned perturbations for both Adam and BAdam, demonstrating that BAdam achieves a similar high-rank update to Adam.", "section": "3.2 Optimization Capability"}, {"figure_path": "0uXtFk5KNJ/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation study for BCD variants and their full counterparts for finetuning Llama 3-8B on Alpaca-GPT4 dataset. Left and middle: Convergence behavior. Right: MT-bench scores.", "description": "This figure presents an ablation study comparing the performance of BAdam and BSGD (block coordinate descent with SGD) against their full counterparts, Adam and SGD, respectively.  The left and middle panels show the convergence behavior of the four optimization methods during the training process, plotting training loss against the number of data passes and training time. The right panel shows the MT-bench scores achieved by each method after training. The results illustrate the effectiveness of the BCD approach in LLM finetuning, even when using SGD instead of Adam.", "section": "3.4 Ablation Study: SGD's Update Rule"}, {"figure_path": "0uXtFk5KNJ/figures/figures_20_1.jpg", "caption": "Figure 2: Optimization capability of BAdam for finetuning Llama 3-8B on Alpaca-GPT4 dataset. Left: Online training loss of LoRA and BAdam. Middle: Cumulative explained variance of BAdam's learned perturbation to the 25th layer's up-proj matrix. Right: Effective rank of Adam's and BAdam's learned perturbations.", "description": "This figure shows the optimization capability of the BAdam model by comparing its performance with LoRA model in finetuning Llama 3-8B on the Alpaca-GPT4 dataset. The left panel displays the online training loss of both models, showing that BAdam converges faster than LoRA. The middle panel shows the cumulative explained variance of BAdam's learned perturbation to the 25th layer's up-proj matrix, indicating that BAdam learns high-rank updates rather than low-rank ones. The right panel displays the effective rank of the learned perturbations by both Adam and BAdam across different layers, showing that BAdam has similar high-rank update with Adam.", "section": "3.2 Optimization Capability"}, {"figure_path": "0uXtFk5KNJ/figures/figures_21_1.jpg", "caption": "Figure 2: Optimization capability of BAdam for finetuning Llama 3-8B on Alpaca-GPT4 dataset. Left: Online training loss of LoRA and BAdam. Middle: Cumulative explained variance of BAdam's learned perturbation to the 25th layer's up-proj matrix. Right: Effective rank of Adam's and BAdam's learned perturbations.", "description": "This figure shows the optimization capability of BAdam by comparing it to LoRA and Adam. The left panel shows the online training loss curves of BAdam and LoRA, demonstrating that BAdam converges faster initially but eventually aligns with LoRA's convergence. The middle panel shows the cumulative explained variance of BAdam's learned perturbation matrix, indicating a heavy-tailed singular value distribution and hence a high-rank update. The right panel displays the effective rank of the learned perturbation matrices of BAdam and Adam across different transformer layers, showing BAdam achieves almost the same high rank update as Adam.", "section": "3.2 Optimization Capability"}]