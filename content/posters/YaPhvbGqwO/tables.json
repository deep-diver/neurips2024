[{"figure_path": "YaPhvbGqwO/tables/tables_31_1.jpg", "caption": "Table 1: Hyperparameters swept across all algorithms. Rows labelled with A-discrepancy are hyperparameters swept specific to our algorithm.", "description": "This table shows the hyperparameters used in the experiments for the different algorithms.  The hyperparameters were step size, lambda 1, lambda 2 (only for the A-discrepancy algorithm), and beta (only for the A-discrepancy algorithm). Each hyperparameter had a range of values which were tested.", "section": "I.4 Experimental and Hyperparameter Details"}, {"figure_path": "YaPhvbGqwO/tables/tables_31_2.jpg", "caption": "Table 2: Best hyperparameters for each environment and each algorithm. Hyperparameters were found using 5 seeds, and taking the maximum AUC.", "description": "This table shows the best hyperparameters found for each of the four environments (Battleship, PacMan, RockSample (11, 11), and RockSample (15, 15)) across three different algorithms: A-discrepancy augmented recurrent PPO, recurrent PPO, and memoryless PPO.  The hyperparameters were determined using a hyperparameter sweep across 5 random seeds, selecting the values that produced the maximum area under the learning curve (AUC).  Each row represents an environment, and the columns list the step size, \u03bb1, \u03bb2 (used for calculating the \u03bb-discrepancy), and \u03b2 (the weighting factor for the \u03bb-discrepancy loss).", "section": "5.1 Combining the X-Discrepancy with PPO"}, {"figure_path": "YaPhvbGqwO/tables/tables_31_3.jpg", "caption": "Table 2: Best hyperparameters for each environment and each algorithm. Hyperparameters were found using 5 seeds, and taking the maximum AUC.", "description": "This table shows the best hyperparameters found for each environment using five seeds and taking the maximum area under the learning curve (AUC).  It lists the step size, lambda 1 value, lambda 2 value, and beta value for the A-discrepancy augmented recurrent PPO, recurrent PPO baseline, and memoryless PPO baseline. These values were used in the experiments described in the paper.", "section": "I.4 Experimental and Hyperparameter Details"}, {"figure_path": "YaPhvbGqwO/tables/tables_32_1.jpg", "caption": "Table 3: Environment-specific hyperparameters, set across all algorithms. We set the entropy coefficient to a higher value in RockSample because the environment requires more exploration.", "description": "This table shows the hyperparameter settings used for the different environments in the experiments.  It specifies the latent size of the neural networks and the entropy coefficient (CEnt) used in the training process. The entropy coefficient is a hyperparameter that controls the exploration-exploitation balance during training.", "section": "1.4 Experimental and Hyperparameter Details"}]