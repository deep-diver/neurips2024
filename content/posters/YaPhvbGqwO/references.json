{"references": [{"fullname_first_author": "Richard S. Sutton", "paper_title": "Learning to predict by the methods of temporal differences", "publication_date": "1988-01-01", "reason": "This paper introduced the temporal difference learning algorithm, which is fundamental to reinforcement learning and directly relevant to the paper's core methodology."}, {"fullname_first_author": "Leslie P. Kaelbling", "paper_title": "Planning and acting in partially observable stochastic domains", "publication_date": "1998-01-01", "reason": "This foundational paper on partially observable Markov decision processes (POMDPs) provides the theoretical background for addressing the core problem tackled in this work."}, {"fullname_first_author": "Richard S. Sutton", "paper_title": "Reinforcement Learning: An Introduction", "publication_date": "2018-01-01", "reason": "As a comprehensive textbook on reinforcement learning, this work provides the broader context and foundational knowledge for understanding the techniques used in the current research."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper presents the Proximal Policy Optimization (PPO) algorithm, which is a crucial component of the deep reinforcement learning approach used to empirically validate the proposed methodology."}, {"fullname_first_author": "Martin L. Puterman", "paper_title": "Markov Decision Processes", "publication_date": "1994-01-01", "reason": "This work provides essential background on Markov decision processes (MDPs), which is a key theoretical foundation that the current paper builds upon and contrasts with the POMDP setting."}]}