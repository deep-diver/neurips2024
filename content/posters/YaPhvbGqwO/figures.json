[{"figure_path": "YaPhvbGqwO/figures/figures_3_1.jpg", "caption": "Figure 1: T-maze decision process. The agent must remember the initial observation to earn the maximum reward (+1).", "description": "This figure shows a T-maze environment, a classic example of a partially observable Markov decision process (POMDP). The agent starts in one of two possible start states, represented by the color of the initial square (blue or red). The agent can only observe the color of the current square.  The agent must reach the goal state (+1 reward) to obtain maximum reward. To do this, it needs to remember the color of the starting square.  It must navigate the corridor and choose the correct path at the junction to reach the goal, highlighting the need for memory to overcome the partial observability.", "section": "3 Detecting Partial Observability"}, {"figure_path": "YaPhvbGqwO/figures/figures_3_2.jpg", "caption": "Figure 2: A-discrepancy augmented network architecture and training objectives.", "description": "This figure illustrates the architecture of a neural network that uses the lambda discrepancy to mitigate partial observability.  The network consists of an RNN (recurrent neural network) that takes as input the observation and action. The RNN's output is fed into two separate value networks: one that estimates values using TD(\u03bb) and another that estimates values using MC (Monte Carlo). The difference between the two value estimates is the lambda discrepancy, which is used as an auxiliary loss during training. The actor network is trained with the PPO algorithm, and its loss is a combination of the usual PPO loss and the lambda discrepancy loss. The critic network is trained to minimize the combination of the lambda discrepancy loss, and the TD(\u03bb) and MC losses. The overall objective is to simultaneously learn a good state representation and a good policy.", "section": "5 Combining the \u03bb-Discrepancy with PPO"}, {"figure_path": "YaPhvbGqwO/figures/figures_5_1.jpg", "caption": "Figure 3: T-Maze A-discrepancy, mixing between full and partial observability. (Left) MDP observation function Perfect. (Right) Various POMDP observation functions Aliased that produce aliased observations at the corridor states, junction states, or both. State indices correspond to starting states (0, 1), hallway (2-11), junctions (12, 13), and terminal state (14). Brighter squares indicate higher probability. (Center) A-discrepancy has a minimum at zero for full observability and increases with partial observability. We interpolate between perfect observations and aliased ones, where the observation function is \u03a6 = (1 \u2212 p) \u00b7 \u03a6Perfect + p\u00b7 \u03a6Aliased.", "description": "This figure shows how the lambda discrepancy changes depending on the level of partial observability in the T-maze environment.  The left panel shows the observation function for a fully observable MDP (no aliasing). The right panels show observation functions for POMDPs with increasing levels of aliasing (corridor states aliased, junction states aliased, both aliased).  The center panel plots the lambda discrepancy against a mixing parameter (p) which interpolates between the fully observable and partially observable observation functions. As expected, the lambda discrepancy increases with the degree of partial observability (i.e., aliasing).", "section": "3 Detecting Partial Observability"}, {"figure_path": "YaPhvbGqwO/figures/figures_6_1.jpg", "caption": "Figure 4: (Left) The Parity Check environment, a POMDP with zero A-discrepancy for every policy. (Center) Almost any randomly-initialized 1-bit memory function reveals a A-discrepancy. (Right) Memory optimization increases normalized return of subsequent policy learning, whereas memoryless policy optimization fails to beat the uniform random baseline.", "description": "This figure shows three parts. The left part shows the Parity Check environment which is a POMDP with zero lambda discrepancy for any policy. The center part shows the distribution of lambda discrepancies for almost any randomly initialized 1-bit memory function in the Parity Check environment. The right part shows that minimizing lambda discrepancy increases the return of subsequent policy gradient learning, whereas memoryless policy optimization fails to beat the uniform random baseline.", "section": "3 Detecting Partial Observability"}, {"figure_path": "YaPhvbGqwO/figures/figures_7_1.jpg", "caption": "Figure 5: Memory optimization increases normalized return of subsequent policy gradient learning. Performance is calculated as the expected start-state value, and is normalized between a random policy (y = 0) and the optimal belief state policy (y = 1) found with a POMDP solver [Cassandra, 2003]. Error bars are 95% confidence intervals over 30 seeds.", "description": "This figure shows the results of an experiment where a memory function was learned to improve the performance of a policy gradient algorithm on several POMDPs.  The y-axis represents the normalized return, which is a measure of how much better the agent performs than a random policy. The x-axis shows different POMDPs. Each bar shows the results for a different memory size (0, 1, 2, or 3 bits).  Error bars indicate the 95% confidence interval over 30 trials.  The dashed line indicates the performance of an optimal policy.", "section": "4 Memory Learning with the \u03bb-Discrepancy"}, {"figure_path": "YaPhvbGqwO/figures/figures_8_1.jpg", "caption": "Figure 6: (Left) The \u03bb-discrepancy auxiliary objective (LD) improves performance over recurrent (RNN) and memoryless PPO. Learning curves shown are the mean and 95% confidence interval over 30 runs. (Right) PacMan memory visualization. The agent moves within the maze (middle), and we reconstruct the dot locations from the agent\u2019s memory. RNNs (bottom) benefit from the \u03bb-discrepancy auxiliary loss (LD, top).", "description": "This figure presents the experimental results of applying the \u03bb-discrepancy auxiliary objective to four partially observable environments. The left panel shows the learning curves for the four environments. The right panel visualizes the agent's memory in the Pac-Man environment, demonstrating the learned memory function's effectiveness in improving performance.", "section": "5.2 Large Partially Observable Environments"}, {"figure_path": "YaPhvbGqwO/figures/figures_19_1.jpg", "caption": "Figure 7: (Left) A POMDP with six states and two actions, where T\u03a6WI = T\u03a0S. From state s0, both actions transition to states s1, sx, and s2 with the labeled probabilities. State sx produces two observations (equivalent to those of s1 and s2) with equal probability. (Right) An equivalent five-state MDP model that behaves identically to the POMDP for any policy.", "description": "This figure shows a POMDP with six states and an equivalent MDP with five states. The POMDP has non-Markovian observations because two distinct states produce the same observation.  The key takeaway is that although the POMDP appears more complex, its behavior is identical to the simpler MDP; thus, the A-discrepancy is zero. This illustrates a situation where the A-discrepancy may not reliably identify partial observability.", "section": "Proof that matching transition-policies is equivalent to an MDP."}, {"figure_path": "YaPhvbGqwO/figures/figures_23_1.jpg", "caption": "Figure 8: Visualizations of the Tiger POMDP. In the original version (left) the observation function was action-dependent, whereas in our modified version (right) observations only depend on state. The state color for the domain on the right represents the distinct state-dependent observation functions: purple states use the initial observation, while the other states are biased towards either left (blue) or right (red) observations with probability 0.85.", "description": "This figure shows two versions of the Tiger POMDP. The left panel shows the original version, where the observation depends on both the state and the action. The right panel shows a modified version, where the observation only depends on the state. In the modified version, the states are colored to indicate the bias in the observation distribution. Purple states have an unbiased observation, while blue and red states have observations biased towards the left and right, respectively.", "section": "H Environments and Experimental Details for Closed-Form Memory Optimization"}, {"figure_path": "YaPhvbGqwO/figures/figures_26_1.jpg", "caption": "Figure 10: (Left) The Parity Check environment (reproduced from Figure 4). (Right) Minor modifications to the transition dynamics (left) or initial state distribution (right) result in non-zero \u03bb-discrepancy for almost all policies.", "description": "This figure shows that the Parity Check environment, which has zero \u03bb-discrepancy for all policies, is a rare exception.  Minor changes such as altering transition probabilities or initial state distribution result in non-zero \u03bb-discrepancies for almost all policies, demonstrating the robustness of the \u03bb-discrepancy metric in detecting partial observability.", "section": "3.3 What conditions cause the \u03bb-discrepancy to be zero?"}, {"figure_path": "YaPhvbGqwO/figures/figures_30_1.jpg", "caption": "Figure 6: (Left) The \u03bb-discrepancy auxiliary objective (LD) improves performance over recurrent (RNN) and memoryless PPO. Learning curves shown are the mean and 95% confidence interval over 30 runs. (Right) PacMan memory visualization. The agent moves within the maze (middle), and we reconstruct the dot locations from the agent\u2019s memory. RNNs (bottom) benefit from the \u03bb-discrepancy auxiliary loss (LD, top).", "description": "This figure presents the results of the proposed method on four challenging partially observable environments: RockSample (11x11), RockSample (15x15), Battleship (10x10), and Partially Observable Pac-Man. The left panel shows the learning curves for the three algorithms: PPO, PPO+RNN, and PPO+RNN+LD (\u03bb-discrepancy). The PPO+RNN+LD algorithm consistently outperforms the other two algorithms in all environments. The right panel visualizes the agent's memory for Partially Observable Pac-Man. The agent's movement is shown in the middle panel. The bottom panel visualizes the memory of the recurrent neural network (RNN) without the auxiliary loss, while the top panel visualizes the memory with the auxiliary loss.", "section": "5.2 Large Partially Observable Environments"}, {"figure_path": "YaPhvbGqwO/figures/figures_30_2.jpg", "caption": "Figure 6: (Left) The \u03bb-discrepancy auxiliary objective (LD) improves performance over recurrent (RNN) and memoryless PPO. Learning curves shown are the mean and 95% confidence interval over 30 runs. (Right) PacMan memory visualization. The agent moves within the maze (middle), and we reconstruct the dot locations from the agent\u2019s memory. RNNs (bottom) benefit from the \u03bb-discrepancy auxiliary loss (LD, top).", "description": "This figure presents the results of applying the proposed \u03bb-discrepancy augmented recurrent PPO algorithm and compares it with recurrent PPO and memoryless PPO.  The left panel shows learning curves across four different partially observable environments for the three algorithms, demonstrating that the auxiliary objective improves learning performance.  The right panel provides a visualization of the agent's memory in the partially observable Pac-Man environment, indicating how the agent's memory helps in solving the partially observable navigation task.  RNNs are shown to benefit from incorporating the \u03bb-discrepancy as an auxiliary loss.", "section": "5.2 Large Partially Observable Environments"}, {"figure_path": "YaPhvbGqwO/figures/figures_33_1.jpg", "caption": "Figure 6: (Left) The \u03bb-discrepancy auxiliary objective (LD) improves performance over recurrent (RNN) and memoryless PPO. Learning curves shown are the mean and 95% confidence interval over 30 runs. (Right) PacMan memory visualization. The agent moves within the maze (middle), and we reconstruct the dot locations from the agent\u2019s memory. RNNs (bottom) benefit from the \u03bb-discrepancy auxiliary loss (LD, top).", "description": "This figure shows the experimental results for four partially observable environments: RockSample (11x11), RockSample (15x15), Battleship (10x10), and Partially Observable Pac-Man.  The left panel displays learning curves for each environment, comparing the performance of three algorithms: recurrent PPO (RNN), recurrent PPO with the \u03bb-discrepancy auxiliary loss (RNN+LD), and memoryless PPO.  The curves show that the algorithm with the \u03bb-discrepancy auxiliary loss consistently outperforms the other two. The right panel shows a visualization of the agent's memory in the Partially Observable Pac-Man environment. This visualization demonstrates that the agent is able to successfully maintain a representation of the environment and use it to make decisions.", "section": "5.2 Large Partially Observable Environments"}]