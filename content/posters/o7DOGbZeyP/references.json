{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2021-00-00", "reason": "This paper is foundational to the current work as it introduces the Vision Transformer (ViT) architecture, which the current research builds upon and improves."}, {"fullname_first_author": "Mostafa Dehghani", "paper_title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution", "publication_date": "2023-00-00", "reason": "This paper is highly relevant as it explores the issue of image-size extrapolation in ViTs, a key challenge addressed by the current work."}, {"fullname_first_author": "Ofir Press", "paper_title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "publication_date": "2022-00-00", "reason": "This paper directly addresses the problem of sequence length extrapolation in transformers, providing valuable insights into the challenges of handling varying input sizes."}, {"fullname_first_author": "Hangbo Bao", "paper_title": "BEiT: BERT Pre-Training of Image Transformers", "publication_date": "2022-00-00", "reason": "This paper is cited for its innovative approach to pretraining image transformers, a technique that is relevant to the high-resolution training methods considered in this research."}, {"fullname_first_author": "Andreas Peter Steiner", "paper_title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers", "publication_date": "2022-00-00", "reason": "This paper offers valuable insights into the training and hyperparameter optimization techniques for ViTs, which are crucial for the success of high-resolution training strategies."}]}