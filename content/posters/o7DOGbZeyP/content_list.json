[{"type": "text", "text": "LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anthony Fuller, Daniel G. Kyrollos, Yousef Yassin, James R. Green ", "page_idx": 0}, {"type": "text", "text": "Department of Systems and Computer Engineering Carleton University Ottawa, Ontario, Canada \u2217anthony.fuller@carleton.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning \u2014 ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating. We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translationequivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg.\u2191 $1.6\\%$ ), against adversarial attack (avg. $\\uparrow5.4\\%$ ), and decreases calibration error (avg.\u2193 $1.5\\%$ ) \u2014 on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by $21.7\\%$ on ImageNet when trained at $224^{2}$ px and tested at $\\mathrm{1024^{2}}$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There is a decades-long trend in computer vision towards higher-resolution imagery, which contains more detailed scene information. Increasing resolution is a reliable way to improve model accuracy [13, 14, 15, 16, 17, 18, 19, 20], but this comes at a cost; training models for hundreds of epochs on large-scale datasets is expensive, especially at high-resolutions. There are two ways to reduce this cost and still see accuracy benefits from high-resolutions: $\\Lsh$ high-resolution finetuning, which pretrains models at a lower resolution, like $224^{2}$ px, then finetunes them at a higher resolution, like $\\mathrm{{\\dot{3}84^{2}}}$ px; and $\\circledcirc$ extrapolating, which deploys models at a higher resolution, without further training. Of these two options, we should aim for models that can effectively extrapolate, as it presents a zero-cost solution that does not require finetuning at every target resolution. Finetuning costs aside, improvements to extrapolation should benefti high-resolution finetuning since models that are better at extrapolating can adapt to higher resolutions more easily. Although extrapolation is a significant and exciting challenge, state-of-the-art (SoTA) model architectures extrapolate poorly. ", "page_idx": 0}, {"type": "text", "text": "Vision transformers (ViTs [9]) offer SoTA performance on many computer vision tasks. ViTs are simple; they split images into non-overlapping patches, linearly project pixels to form patch embeddings, and process these \u201ctokens\u201d with a stack of architecturally identical transformer layers \u2014 maintaining a constant feature map size throughout. This non-hierarchical design enables learning patch representations, which are useful for dense prediction tasks [21, 22, 23] and are fundamental for vision-language models [24, 25, 26]. The design enables efficient processing of only a subset of patches, known as token dropping [27, 28]. Lastly, it enables model scaling by increasing the embedding size and the layer count [29, 30]. ", "page_idx": 0}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/930c0d39d6295d4065d127991b2490fb282e81c1e9dcb6e7b799f47ae96ce6f5.jpg", "img_caption": ["Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at $224^{2}$ px and tested up to $\\mathrm{1024^{2}}$ px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at $\\mathrm{1024^{2}}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Image-size extrapolation with ViTs can be achieved in three ways: $\\Lsh$ increasing the patch size, which packs more pixels into each patch embedding; $\\circledcirc$ increasing the \u201cpatchification\u201d stride, which skips-over pixels; and $\\circledcirc$ increasing the number of patches. Of these three options, we should aim for models that can effectively ingest more patches \u2014 called \u201csequence length extrapolation\u201d in the natural language processing (NLP) community [31] \u2014 as a greater number of patches presents models with more (uncompressed) information that we hope to leverage into higher accuracy. Furthermore, methods that improve sequence length extrapolation, like our proposed method, can be fused with methods that adjust patch sizes, like FlexiViT [32]. We strongly believe that patch position encoding is a primary cause of the poor sequence length extrapolation ability of ViTs \u2014 like it is in NLP, where significant advancements have been made by improving position encoding [31, 33, 34, 35]. ", "page_idx": 1}, {"type": "text", "text": "Adding learnable or fixed sinusoidal position embeddings to patch embeddings before the first layer is the most common way ViTs encode positions. Recently, the rotary position embeddings (RoPE [36]) used in SoTA language models [37, 38] were extended to ViTs, as 2D-RoPE [7], showing exciting results. RoPE is a different approach to position encoding that injects positional information in each self-attention layer by rotating queries and keys with fixed sinusoidal embeddings. But for these methods to ingest more patches at test time, they must either introduce new position embeddings or modify existing embeddings \u2014 both options create a significant distribution shift. Motivated by these observations and more, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1 LookHere \u2014 We introduce a novel position encoding method for plain ViTs that restricts attention heads to fixed fields of view (FOV) and points them in different directions via 2D masks. This design provides: $\\mathbf{e}$ translation-equivariance, $\\oplus$ attention head diversity, $\\circ$ improved interpretability, and $\\mathbb{o}$ limits the distribution shift that attention heads face when extrapolating. ", "page_idx": 1}, {"type": "text", "text": "$\\circledcirc$ Controlled Experiments \u2014 We perform an apples-to-apples comparison between seven position encoding methods for plain ViTs alongside our three LookHere variants. We demonstrate that LookHere: $\\mathbf{e}$ improves classification, segmentation, adversarial robustness, and model calibration when tested at the training resolution; $\\oplus$ significantly improves performance when tested beyond the training resolution; and $\\circ$ increases its performance advantage after high-resolution finetuning. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "$\\circledcirc$ Extrapolation Insights \u2014 We show that extrapolation: $\\spadesuit$ benefits images with small objects the most, as they occupy more patches at test time; $\\oplus$ produces class-level and dataset-level effects; and $\\circ$ creates distribution shifts that can be visualized via attention maps. ", "page_idx": 2}, {"type": "text", "text": "$\\circ$ ImageNet-HR \u2014 We introduce the first natively high-resolution ImageNet test set $(1024^{2}\\;\\mathrm{px})$ aimed to benchmark classifiers on images that were not upsampled to achieve the target image size. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A ViT splits an image into a grid of non-overlapping patches, flattens the grid into a sequence, and flattens the patches into vectors; i.e., $\\mathbb{R}^{Y\\times X\\times C}\\stackrel{\\sim\\;}{\\rightarrow}\\mathbb{R}^{\\bar{N_{y}^{*}}\\times N_{x}\\times P^{2}\\times C}\\rightarrow\\mathbb{R}^{(\\bar{N_{y}^{*}}\\cdot N_{x})\\times(P^{2}\\cdot C)}$ , where $Y$ is the image-height, $X$ is the image-width, $C$ is the number of channels, $N_{y}$ is the grid-height, $N_{x}$ is the grid-width, $P$ is the patch height and width. A linear layer maps each vector of pixels to a patch embedding; i.e., RP 2\u00b7C \u2192Eipatch , where $D$ is the embedding dimension also known as the transformer width. We define and $(i_{y},i_{x})$ as the sequence position and the 2D position of the $i^{\\mathrm{th}}$ patch, respectively, where $N$ is the total number of patches, equal to $N_{y}\\cdot N_{x}$ , $i\\in\\{1,2,\\dots,N\\}$ , $i_{y}\\in\\{1,2,\\ldots,N_{y}\\}$ , and $i_{x}\\in\\left\\{1,2,\\ldots,N_{x}\\right\\}$ . Finally, sequence length extrapolation occurs when $N_{t e s t}>N_{t r a i n}$ . ", "page_idx": 2}, {"type": "text", "text": "A patch embedding represents the content of a patch, and contains no information representing its original location within the image. Thus, we must encode patch positions to enable spatial reasoning; otherwise, a ViT will operate on a bag of patches. ", "page_idx": 2}, {"type": "text", "text": "We define a \u201cplain ViT\u201d as attention-only and non-hierarchical. Our primary goal is to improve the extrapolation ability \u2014 i.e., generalize to more patches at test time \u2014 of plain ViTs. Our work is motivationally aligned with FlexiViT [32] and NaViT [6], improving the flexibility of plain ViTs. Next, we briefly describe seven position encoding methods and refer the reader to the cited studies for further details; we include them all in our controlled experiments. Another method, iRPE [39], is also compatible with plain ViTs. However, we exclude it because it is more than twice as slow as other methods; nonetheless, we benchmark iRPE with our best training recipe in Appendix A.2.1. ", "page_idx": 2}, {"type": "text", "text": "Input Embeddings. This group leverages learned or fixed position embeddings, $E_{i}^{p o s}\\in\\mathbb{R}^{D}$ , that are added to patch embeddings at the transformer input; i.e., zi = Eipatch pos , where $z$ is the input to the first transformer layer. Position embeddings represent the absolute positions of patches in an image. ", "page_idx": 2}, {"type": "text", "text": "$\\Lsh$ 1D position embeddings [9] (1D-learn for short) map $i$ to learnable embeddings. $\\circledcirc$ 2D sinusoidal embeddings [8] (2D-sincos for short) individually map $i_{y}$ and $i_{x}$ to fixed 1D-sinusoidal embeddings $(E_{i}^{y},E_{i}^{x}\\,\\in\\,\\mathbb{R}^{\\frac{D}{2}})$ ), then concatenate them along the embedding dimension. $\\circledcirc$ Factorized position embeddings [6] (Factorized for short) individually map $i_{y}$ and $i_{x}$ to learnable embeddings $(E_{i}^{y},E_{i}^{x}\\in$ $\\mathbb{R}^{D})$ ), then add them. $\\bullet$ Learnable Fourier features [11] (Fourier for short) map $(i_{y},i_{x})$ to Fourier features [40, 41], then to embeddings with a multi-layer perceptron (MLP). ", "page_idx": 2}, {"type": "text", "text": "Attention Biases. This group leverages learned or fixed operations that encode positions by modifying the pairwise interactions between patches in self-attention without adding position embeddings to patch embeddings. Recall that self-attention first applies three separate linear transformations to project internal patch representations and splits the resultant vectors into $H$ smaller vectors of length $D_{H}$ ; i.e., $\\mathbb{R}^{N\\times\\dot{D}}\\rightarrow\\mathbb{R}^{3\\times N\\times H\\times D_{H}}$ \u2014 creating queries, keys, and values for each attention head. We denote a specific head by $h$ . Next, attention scores $(\\dot{A}\\in\\dot{\\mathbb{R}}^{H\\times N\\times N})$ are calculated by measuring the similarity between all pairs of queries $(q_{h i}\\in\\mathbb{R}^{D_{H}})$ and keys $(k_{h j}\\in\\mathbb{R}^{D_{H}})$ , separately, for each head; i.e., $a_{h i j}\\,=\\,q_{h i}\\cdot k_{h j}/\\sqrt{D_{H}}$ , where $i$ and $j$ are query and key sequence positions, and we define $(i_{y},i_{x})$ and $(j_{y},j_{x})$ as their 2D positions. Attention scores $(a_{h i j})$ represent the amount of information moving from patch position $j$ to $i$ \u2014 whereas values $(v_{h j}\\in\\mathbb{R}^{D_{H}}$ ) represent the content of the moving information. ", "page_idx": 2}, {"type": "text", "text": "$\\circledcirc$ Learnable relative position encoding [10] (RPE-learn for short) biases attention scores by mapping all possible relative positions between queries and keys to learnable embeddings $(B_{i j}\\in\\mathbb{R}^{H})$ ; i.e., biases are a function of $i_{y}-j_{y},i_{x}-j_{x}$ , and $h.\\;\\mathfrak{o}$ A 2D extension of Attention with Linear Biases (ALiBi [31]), 2D-ALiBi [12] penalizes attention scores as a function of the Euclidean distance between $(i_{y},i_{x})$ and $(j_{y},j_{x})$ , and a head-specific scalar, called a slope. Slopes bias attention heads at different rates. $\\bullet$ A 2D extension of rotary position embeddings (RoPE [36]), 2D-RoPE [7] rotates queries and keys as a function of their positions. Each query is rotated by the sinusoidal embedding of $i_{y}$ for half its dimensions and the sinusoidal embedding of $i_{x}$ for the other half of its dimensions; likewise, keys are rotated as a function of $j_{y}$ and $j_{x}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Non-plain ViTs. Many hybrid or hierarchical architectures have been invented that often encode positions differently [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]. Although these architectures may be favored in some circumstances, the plain ViT is the most common single architecture due to its simplicity, flexibility, and scalability. We benchmark many non-plain ViTs and large SoTA ViTs on extrapolation in Appendix A.2.1. ", "page_idx": 3}, {"type": "text", "text": "ViT Extrapolation. Some ViTs have been tested at higher resolutions than they were trained [54, 43, 12, 55]. NaViT [6] benchmarked input embedding methods on extrapolation, none see the gains at higher resolutions that we observe. ", "page_idx": 3}, {"type": "text", "text": "3 LookHere ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Design Motivation. We introduce 2D attention masks that assign each attention head a direction and a FOV, preventing attention outside the head\u2019s FOV. Within a head\u2019s FOV, attention scores are penalized based on relative patch distances. Three ideas motivate this design. $\\Lsh$ Attention head diversity: heads often learn redundant algorithms that can be pruned with little accuracy penalty [56, 57, 58]. Head redundancy has also been observed in NLP [59, 60, 61], where diversity-encouraging loss functions have been leveraged to improve generalization [62, 63, 64, 65]. From a mechanistic point of view, we can think of attention heads as an ensemble of sub-networks that \u201coperate completely in parallel, and each add their output back into the residual stream,\u201d [66] and the residual stream is mapped to logits. Diversity has long been a desirable property of ensembles [67, 68], and constraining attention heads to focus in different directions ensures it. $\\circledcirc$ Attention head consistency: heads often learn interpretable spatial algorithms, like \u201cattend to the area above the query,\u201d which reliably retrieves information from the internal representations above the query; however, we believe these types of spatial algorithms might fail when new or modified position embeddings are introduced to encode new patch positions during extrapolation \u2014 misleading the model about the information above the query, for example. We believe hard-coding both directions and distances (via attention masks and biases) will reduce the need for models to learn their own spatial algorithms. $\\otimes$ Translation-equivariance has long been a desirable property of vision models, contributing to the success of convolutional networks [69, 70, 71]. ViTs are critiqued for weak inductive biases, leading to poor sample efficiency when trained from scratch [72, 73, 74]. We believe that LookHere\u2019s stronger inductive biases, achieved via directional masking and distance penalties, can improve ViT sample efficiency. ", "page_idx": 3}, {"type": "text", "text": "Design Specifics. Let $H$ be the number of heads, $L$ be the number of layers, and $N$ be the number of patches (plus one for the CLS token). We denote the LookHere matrices by $\\boldsymbol{A}_{\\mathrm{FIX}}\\in$ $\\mathbb{R}^{L\\times H\\times(N+1)\\times(N+1)^{\\ast}}$ . We encode positions by subtra\u221acting the LookHere matrix for a layer $l$ , $\\boldsymbol{\\mathcal{A}}_{\\mathrm{FIX}}^{l}$ , from the learned attention matrix, $\\mathcal{A}_{\\mathrm{LRN}}^{l}=Q K^{T}/\\sqrt{D_{H}}$ , before the softmax that normalizes the attention matrix prior to multiplying it by values [75], i.e., $\\mathcal{A}^{l}=\\mathtt{s o f t m a x}(\\mathcal{A}_{\\mathrm{LRN}}^{l}-\\mathcal{A}_{\\mathrm{FIX}}^{l})$ . We do not add position embeddings to patch embeddings. ", "page_idx": 3}, {"type": "text", "text": "Let $i$ and $j$ be query and key sequence positions, respectively, with 2D-coordinates $(i_{y},i_{x})$ and $(j_{y},j_{x})$ . Crucially, $j$ is visible to $i$ if $j$ lies within $i$ \u2019s FOV. This attention masking technique is inspired by the 1D causal masks used in autoregressive transformer decoders used in NLP [75]. When $j$ is visible, we bias the attention score based on the Euclidean distance between $i$ and $j$ to encode the relative distance between patches. We scale distances via a slope function $m:\\mathbb{N}_{L}\\times\\mathbb{N}_{H}\\rightarrow\\mathbb{R}$ , $m(l,h)=s_{l}(l)\\cdot s_{h}(h)\\cdot s_{g}$ that strengthens or weakens the distance penalty as a function of the head $(s_{h}:\\mathbb{N}_{H}\\to\\mathbb{R})$ ) and layer $\\left(s_{l}:\\mathbb{N}_{L}\\to\\mathbb{R}\\right)$ ), scaled by a global slope $s_{g}\\in\\mathbb{R}$ . Finally, the CLS token is visible to all positions. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathrm{LookHere}(l,h,i,j)=\\displaystyle\\left\\{\\!\\!\\begin{array}{l l}{\\displaystyle m(l,h)\\cdot\\mathrm{Distance}(i,j)}&{\\mathrm{if~}j\\mathrm{~is~visible~to~}i}\\\\ {\\displaystyle\\infty}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.}\\\\ {\\mathrm{Distance}(i,j)=\\displaystyle\\sqrt{(i_{y}-j_{y})^{2}+(i_{x}-j_{x})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/07c1f33eefdcbc15fb2e090e993c742eff2f8a456f260460ef1e60d8128fa95d.jpg", "img_caption": ["Figure 2: LookHere masks and biases (center) the learned attention matrix (left, where colors are random). Masked cells are black, encoding directions $\\leftrightarrow$ with a $90^{\\circ}\\,\\mathrm{FOV}$ ); biased cells are shaded bluish-green, encoding relative patch distances. (Right) An example of the FOV of the center query patch. The final attention matrix is computed as Al = softmax $\\mathcal{\\hat{A}}_{\\mathrm{LRN}}^{l}-\\mathcal{A}_{\\mathrm{FIX}}^{l})$ , at each layer $l$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "For example, Figure 2 displays attention matrices of a head that \u201clooks right\u201d with a $90^{\\circ}\\,\\mathrm{FOV}.$ We create three LookHere variants, the first two have FOVs of $180^{\\circ}$ and $90^{\\circ}$ (LH-180 and LH-90). We direct attention heads eight different ways, selecting the four cardinal directions $(\\uparrow,\\downarrow,\\leftarrow,\\rightarrow)$ and the four intercardinal directions $(\\check{\\mathcal{T}},\\check{\\searrow},\\check{\\mathcal{L}},\\stackrel{\\kappa}{\\searrow})$ . ViT-B models have twelve attention heads; we leave the last four attention heads undirected to allow them unrestricted attention over the full image. We create a final variant that cuts the first four LH-90 masks in two, creating eight $45^{\\circ}$ views that cover the full image without overlapping (LH-45). Visualizations of the bias matrices are in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "Design Ablations. We offer four takeaways through extensive ablations (Appendix A.6): $\\Lsh$ LookHere is robust to the choice of slope function. We set our default $s_{l}$ to linearly decrease from 1.5 to 0.5 with increasing depth (inspired by depth-wise attention distance findings [76]). This helped in preliminary experiments, but the benefits disappear in our ablations. We arbitrarily set our default $s_{h}$ to $\\textstyle\\left({\\frac{1}{2}},{\\frac{1}{8}},{\\frac{1}{32}},{\\frac{1}{128}}\\right)$ for the four undirected heads, but distance penalties on undirected heads can be removed entirely. We set $s_{g}\\,=\\,1$ ; LookHere is also robust to the choice of the global slope. We believe precisely tuning slopes is unnecessary because models can learn to scale attention logit magnitudes. $\\circledcirc$ Increasing penalties with the square or square root of the distance harms extrapolation. $\\circledcirc$ Removing all distance penalties harms extrapolation. $\\bullet$ Our main contribution, 2D directional masks, are crucial to retain performance, but our method is robust to many directional configurations. ", "page_idx": 4}, {"type": "text", "text": "Compute. $\\boldsymbol{\\mathcal{A}}_{\\mathrm{FIX}}$ is precomputed and fixed, subtracting it element-wise from the learned attention matrices $\\mathcal{A}_{\\mathrm{LRN}}$ only costs $H\\cdot(N+1)\\cdot(N+1)$ floating point operations (FLOPs) per layer. For a ViT-B/16 model, these subtractions account for $0.016\\%$ of the total FLOPs. LookHere reduces FLOPs by not adding position embeddings to patch embeddings, but this amount is also negligible. Additionally, LookHere matrices offer structured sparsity (up to $7/8$ for a $45^{\\circ}$ FOV) that can speedup attention \u2014 although exciting, this speedup requires custom kernels that we leave for future work. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Deep neural networks \u2014 including ViTs \u2014 can be sensitive to seemingly minor hyperparameter changes when trained from scratch. Dosovitskiy et al. [9] finetuned the original ViT at a higher resolution, reaching $77.9\\%$ top-1 accuracy on ImageNet (we refer to ILSVRC2012 or ImageNet-1k as ImageNet). Steiner et al. [77] searched 28 hyperparameter configurations, achieving best and average runs of $80.0\\%$ and $76.9\\%$ , respectively (average calculation omits runs without data augmentation, as they were poor). Touvron et al. [78] ablated repeat augmentation [79], dropping accuracy by $4.8\\%$ . Touvron et al. [17] replaced cross-entropy loss with binary cross-entropy loss, raising accuracy by $1.3\\%$ . Importantly, these are all ViT-B/16 models trained from scratch for 300 epochs on ImageNet. Informed by these observations and more, we design a controlled experiment: We search 8 hyperparameter configurations for each position encoding method using a single codebase; this offers an apples-to-apples comparison between our three LookHere variants and seven baselines. ", "page_idx": 4}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our 80 training runs result from the following Cartesian product: ", "page_idx": 5}, {"type": "text", "text": "Position encoding: 1D-learn, 2D-sincos, Augmentations: RandAugment(2, 15) [80], 3-Augment [17] Factorized, Fourier, RPE-learn, 2D-ALiBi, Learning rate: $1.5\\cdot10^{-3}$ , $3.0\\cdot10^{-3}$ 2D-RoPE, LH-180, LH-90, LH-45 Weight decay: 0.02, 0.05 ", "page_idx": 5}, {"type": "text", "text": "For each configuration, we train a ViT-B/16 on $99\\%$ of the ImageNet training set, holding the last $1\\%$ as a validation set called \u201cminival\u201d, following [77, 81] (see Appendix A.4.1 for other hyperparameters). We train all models from scratch for 150 epochs on $224^{2}$ px images. Our results are competitive and sometimes surpass ViTs trained for much longer, which validates our setup. The best models (according to minival accuracy), among our 8-run hyperparameter sweep per method, are always trained using 3-Augment [17], a $3.0\\cdot1\\bar{0}^{-3}$ learning rate, and a 0.05 weight decay. ", "page_idx": 5}, {"type": "text", "text": "Test sets. We test all 80 models on six ImageNet test sets. This includes $\\Lsh$ the original \u201cvalidation\u201d set used as a test set (Val for short [1]), $\\circledcirc$ the reassessed labels of the original validation set (ReaL for short [4]), $\\circledcirc$ the independently collected and in-distribution test set (v2 for short [2]), $\\circ$ the natural adversarial test set (-A for short [3]), $\\mathbf{\\odot}$ the ImageNet rendition test set (-R for short [5]), and $\\mathfrak{o}$ the high-resolution test set that we introduce (-HR for short). ", "page_idx": 5}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/dc3812879063e70bdebe6f819ac2c51339ff96f530ca0e462bde6c41cab13240.jpg", "img_caption": ["Figure 3: Images of three classes from ImageNet-HR. (Bottom left is Anthony\u2019s niece Addison.) "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "ImageNet-HR. Since there are no natively high-resolution ImageNet test sets, there are two options to test the extrapolation ability of models trained on ImageNet: $\\Lsh$ upsample existing test sets to higher resolutions, and $\\circledcirc$ collect a high-resolution test set ourselves. However, upsampling lowresolution images introduces another distribution shift (i.e., interpolated pixels) that we may not want to test. Thus, we collect a high-resolution test set to remove this confounding variable from our analysis. We manually collect 5 images for each ImageNet class, resulting in $5\\mathbf{k}$ total images, and manually crop them to $1024^{2}$ px. This is smaller than other test sets (v2 is $30\\mathbf{k}$ images, -A is $7.5\\mathbf{k}$ images). However, we invest considerable resources to ensure its quality with two priorities: annotation accuracy and image diversity. See Appendix A.1 for details. ImageNet-HR can be accessed: https://huggingface.co/datasets/antofuller/ImageNet-HR ", "page_idx": 5}, {"type": "text", "text": "Adversarial Attacks. We perform Fast Gradient Sign Method (FGSM [82]) adversarial attacks with two strengths $\\textstyle(\\frac{1}{255},\\,\\frac{3}{255})$ on all models using Val images. ", "page_idx": 5}, {"type": "text", "text": "Calibration Estimates. We calculate the Expected Calibration Error (ECE [83]) with 15 bins of all models using Val images. ", "page_idx": 5}, {"type": "text", "text": "Higher-Resolution Finetuning. With the best model per method, we continue training on ImageNet for 5 epochs at $384^{2}$ px. We test at $384^{2}$ px without extrapolating. ", "page_idx": 5}, {"type": "text", "text": "Segmentation. With the best model per method, we finetune following the Segmenter protocol with a linear decoder [84]. Additionally, we probe the patches by only training a linear layer to produce a low-resolution logit map which is upsampled to obtain a full resolution segmentation map, following [85]. We run these experiments on ADE20k [86] at $512^{2}$ px and Cityscapes [87] at $768^{2}$ px. ", "page_idx": 6}, {"type": "text", "text": "Patch Logit-lens. Inspired by interpretability research [88], we evaluate the quality of the learned patch representations for models leveraging LookHere compared with other methods. Following prior work [89, 90], we project frozen patch representations onto the learned class embedding space using the MLP classifier head that was learned for the CLS token. We leverage the ImageNet-S dataset [91], which contains partial segmentation maps for $12\\mathrm{k}$ images from Val, covering 919 ImageNet classes. ", "page_idx": 6}, {"type": "text", "text": "Extrapolating. With the best model per method, we test on images larger than $224^{2}$ px, increasing the number of patches and we test on images smaller than $224^{2}$ px, decreasing the number of patches; for both experiments, no further training is performed \u2014 the models are tested on their resolution generalization ability. For 1D-learn and 2D-sincos, we bilinearly interpolate the position embeddings used during training. For Factorized, we linearly interpolate the position embeddings for each axis. Fourier does not require adjustment since fractional positions along each axis are used as input. For RPE-learn, we interpolate the learned relative biases using the official BEiT implementation [10]. 2D-ALiBi does not require adjustment either. However, we tune a parameter on minival that scales the distance penalty at each test resolution. For 2D-RoPE, we tune its base frequency on minival \u2014 this is a SoTA method to extrapolate RoPE used in NLP [33]. Lastly for LookHere, we tune the global slope on minival. The benefits of tuning slopes are minimal, see Appendix A.4.4. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/76e6a8b5a62c58f7d33b2c431f13c71739b7ee6cc394592fa6e605d5b49e5031.jpg", "table_caption": ["Table 1: Top-1 acc. $(\\%)$ for ViT-B models trained on ImageNet for 150 epochs; trained and tested at $224^{2}$ . We report the best and average results across our 8-run hyper-parameter sweep. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "LookHere improves ViT sample efficiency (Table 1). Our three variants outperform the best baseline, 2D-RoPE, under almost all test conditions (the single exception being the best 2D-RoPE model on -R). LookHere further improves gains when considering averaged results \u2014 i.e., when accuracy values are averaged over 8 hyperparameter configurations (please see the Appendix A.5 for individual results). For instance, LH-180 outperforms 2D-RoPE by $\\mathrm{\\bar{0}.93\\%\\ /\\ 1.36\\%}$ on on Val / v2 on our best runs and by $1.64\\%\\,/\\,1.96\\%$ on Val / v2 on our averaged runs \u2014 indicating that LookHere decreases hyperparameter sensitivity. Surprisingly, LH-180 averages $80.01\\%$ on Val, which matches the best run trained for twice as long by Steiner et al. [77]. ", "page_idx": 6}, {"type": "text", "text": "LookHere improves ViT adversarial robustness and model calibration (Tables 2 3); both have been linked to ensemble diversity [92, 93, 94], which we offer as a potential explanation. This is an interesting finding because adversarial robustness and calibration can be at odds with accuracy [95, 96]. We show that LookHere learns more diverse attention heads by measuring the generalized JensenShannon divergence [97] between heads (Figure 4). In the Appendix A.8, we measure more properties of models leveraging different position encoding methods. LookHere significantly outperforms other methods on segmentation linear probing, demonstrating its ability to learn spatiallyaware patch representations. LookHere also performs well with segmentation finetuning, achieving comparable performance to 2D-RoPE (Table 4). ", "page_idx": 6}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/5c8640e60b63ad8599fbb83c6c55673c37d027479e45ddf230fb7f562f5e6f11.jpg", "img_caption": ["Figure 4: LookHere learns more diverse attention heads and prevents attention collapse. Legend follows Figures 1 7. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/c1b637d469d905a773921585e4f53388974a857c9e248e6a5cdca76637c46e47.jpg", "table_caption": ["Table 2: Fast Gradient Sign Method at- Table 3: Expected Table 4: Semantic Segmentation tack [82] $\\%$ top-5 acc. on Val), best and Calibration Error $\\%$ $\\%$ mIoU), linear probing (LP) average runs. [83] (\u2193) on Val, best and finetuning (FT). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "High-resolution finetuning increases the performance advantage of all three LookHere variants over 2D-RoPE (Table 5). This aligns with our intuition that improving extrapolation methods can improve high-resolution finetuning. Lower initial finetuning loss has been linked to better retaining the general representations learned during pretraining [98], and better extrapolating models have lower initial loss at a higherresolution, by definition. ", "page_idx": 7}, {"type": "text", "text": "Using a \u201clogit lens\u201d [88] approach, we project patch representations onto the class embedding space [89]. We observe that LookHere encodes semantic information in its patches faithful to ", "page_idx": 7}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/4a8f9e98532c2dc3df21f7fb8953a798abfdd034b7abf1630175c5ec89001bc1.jpg", "table_caption": ["Table 5: Top-1 acc. $(\\%)$ for models trained at $224^{2}$ px, finetuned and tested at $384^{2}$ px. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "the original patch location; these patch-level predictions act as a segmentation map that can be generated without additional training. The officer in Figure 5 is not a one-off example; using ImageNet-S [91], we see that LookHere outperforms 2D-RoPE by at least $22\\%$ mIoU using this patch-projection method (Figure 5). Our best explanation is that, by restricting attention, LookHere prevents the attention collapse at deeper layers observed in Figure 4 that divorces patch representations from their original patch locations; this collapse has been observed in other ViTs [99, 100]. We also expect that preventing attention collapse will benefit vision-language models, where frozen patch representations are used as \u201cimage tokens\u201d that should represent their original patch locations [24, 25, 26]. More examples and detailed analysis are in Appendix A.7 ", "page_idx": 7}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/4de6db1dee9fcfd22a3e00c9efef9277aa55b715eb888f489b61168f5768e229.jpg", "img_caption": ["Figure 6: ViT-B/16 models trained for 150 epochs on ImageNet at $224^{2}$ px and tested down to $64^{2}$ px. Model architectures are consistent between runs other than position encoding methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "LookHere significantly improves extrapolation ability (Figure 1). Our smallest FOV variant (LH-45) sees improving relative performance as resolution increases. LH-45 outperforms 2D-ALiBi, which is equivalent to LookHere without our 2D directional masks, by $9.5\\%$ on Val at $\\mathrm{1024^{2}}$ px. These two results demonstrate the extrapolation benefits of restricting attention to fixed FOVs. LH-45 gains $1.3\\%$ on Val when extrapolating from $224^{2}$ to $384^{2}\\,\\mathrm{px}$ ; this is the largest gain we find in the literature, including our extensive benchmarking of SoTA models in Appendix A.2. LookHere also outperforms other methods when tested on smaller images, but the advantage narrows (Figure 6). ", "page_idx": 8}, {"type": "text", "text": "Interestingly, smaller objects benefit most from extrapolation (Figure 7), which are distributed over more patches at test time. We believe this effect also explains the $6-8\\%$ that LookHere models gain when extrapolating on ImageNet-A from $224^{2}$ to $448^{2}$ px; by inspection, ImageNet-A seems to have small objects, and other work found zooming-in on center-cropped ImageNet-A images improves performance [102]. Finally, all LookHere variants outperform other methods on ImageNet-HR, indicating better handling of interpolated pixels generated when upsampling lower-resolution imagery is not the reason why LookHere extrapolates better. ", "page_idx": 8}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/da91f5e1a4a2ad3e6acc1979216664864aa5bb27b0329ee5f70e20b6e81a6100.jpg", "img_caption": ["Figure 7: The effect of object size on accuracy gains or losses due to extrapolation. Object size is measured using annotations from Kaggle\u2019s ImageNet Object Localization Challenge [101]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Reducing the distribution shift faced by attention heads during extrapolation is our best explanation for LookHere\u2019s large relative improvement. Figure 8 shows attention maps that are \u201cunflattened\u201d to visualize the image regions to which heads attend, averaged over the same $5\\mathbf{k}$ images. We show one head per model that exhibits similar behavior at a $22\\dot{4}^{2}$ resolution. Models leveraging RPE-learn and 2D-ALiBi learn variants of an algorithm that retrieve information from above the query; however, both models retrieve information elsewhere in the image when extrapolating. LookHere hard-codes this type of algorithm, which it continues to execute when extrapolating. In Appendix A.8 we find more examples of interesting attention head behaviour. ", "page_idx": 9}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/ff11e697549eac8f5269a4625e83c6c9b87f696f2b1f9869107046f749c2b4ff.jpg", "img_caption": ["Figure 8: Attention maps of three attention heads across four resolutions, where the query is in the center. We use the colormap: "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Extrapolation affects different datasets differently; it also affects different classes differently. For example, when extrapolating, all models underpredict certain classes (bakery, church, and tights) and overpredict other classes (mobile home, threshing machine, and sports car). This investigation is inspired by the class-level effects of data augmentation [103]. In Appendix A.9 we find more class-level effects of extrapolation. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Closing ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. The primary limitation of LookHere is it requires hand-designed directional masks and distance penalties. However, our extensive ablations demonstrate that LookHere is robust to the choice of directional masks and distance penalties. The primary limitation of our experiments is we do not scale ViTs to giant sizes. Instead, we select the most common size, the ViT-B/16, and focus our computational resources on a controlled experiment \u2014 that extensively and fairly tunes the appropriate baselines for plain ViTs; this allows us to make confident conclusions based on our thorough experiments. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. LookHere position encoding significantly improves the ability of plain ViTs to make inferences when provided a greater number of patches than seen during training. We thoroughly demonstrate that LookHere outperforms other methods with and without extrapolation on standard image benchmarks and our high-resolution ImageNet test set called ImageNet-HR. We provide new insights into ViT extrapolation by showing object-size, class-level, and dataset-level effects. We believe LookHere will help the vision community transform higher-resolution into higher accuracy. ", "page_idx": 9}, {"type": "text", "text": "Future Work. We are excited to realize the computational gains that LookHere makes available via sparse attention kernels, as well as bring LookHere to video and 3D point-cloud applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. Anthony thanks NSERC\u2019s Postgraduate Scholarships Doctoral program for funding his PhD. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li FeiFei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015.   \n[2] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classifiers Generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019.   \n[3] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural Adversarial Examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[4] Lucas Beyer, Olivier J. H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with ImageNet? arXiv preprint arXiv:2006.07159, 2020.   \n[5] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In International Conference on Computer Vision (ICCV), 2021.   \n[6] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lu\u02c7ci\u00b4c, and Neil Houlsby. Patch n\u2019 Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. In Neural Information Processing Systems (NeurIPS), 2023.   \n[7] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Br\u00e9gier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and J\u00e9r\u00f4me Revaud. CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow. In International Conference on Computer Vision (ICCV), 2023.   \n[8] Zelun Wang and Jyh-Charn Liu. Translating math formula images to LaTeX sequences using deep neural networks with sequence-level training. International Journal on Document Analysis and Recognition, 2021.   \n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR), 2021.   \n[10] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT Pre-Training of Image Transformers. In International Conference on Learning Representations (ICLR), 2022.   \n[11] Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. In Neural Information Processing Systems (NeurIPS), 2021.   \n[12] Anthony Fuller, Koreen Millard, and James R Green. CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders. In Neural Information Processing Systems (NeurIPS), 2023.   \n[13] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy. In Neural Information Processing Systems (NeurIPS), 2019.   \n[14] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism. arXiv preprint arXiv:1811.06965, 2019.   \n[15] Mingxing Tan and Quoc V. Le. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In International Conference on Machine Learning (ICML), 2019.   \n[16] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Herve Jegou. Three things everyone should know about Vision Transformers. arXiv preprint arXiv:2203.09795, 2022.   \n[17] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. DeiT III: Revenge of the ViT. In European Conference on Computer Vision (ECCV), 2022.   \n[18] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, et al. MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training. arXiv preprint arXiv:2403.09611, 2024.   \n[19] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[20] Penghao Wu and Saining Xie. V\\*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[21] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, et al. SegViT: Semantic Segmentation with Plain Vision Transformers. In Neural Information Processing Systems (NeurIPS), 2022.   \n[22] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, et al. Simple Open-Vocabulary Object Detection. In European Conference on Computer Vision (ECCV), 2022.   \n[23] Yanghao Li, Hanzi Mao, Ross Girshick, , and Kaiming He. Exploring Plain Vision Transformer Backbones for Object Detection. In European Conference on Computer Vision (ECCV), 2022.   \n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In Neural Information Processing Systems (NeurIPS), 2023.   \n[25] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N. Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In Neural Information Processing Systems (NeurIPS), 2023.   \n[26] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, et al. Language Is Not All You Need: Aligning Perception with Language Models. In Neural Information Processing Systems (NeurIPS), 2023.   \n[27] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. ATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. In Neural Information Processing Systems (NeurIPS), 2021.   \n[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked Autoencoders Are Scalable Vision Learners. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[29] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling Vision Transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[30] Ibrahim M. Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. In Neural Information Processing Systems (NeurIPS), 2023.   \n[31] Ofir Press, Noah Smith, and Mike Lewis. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations (ICLR), 2022.   \n[32] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One Model for All Patch Sizes. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[33] Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling Laws of RoPEbased Extrapolation. In International Conference on Learning Representations (ICLR), 2024.   \n[34] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The Impact of Positional Encoding on Length Generalization in Transformers. In Neural Information Processing Systems (NeurIPS), 2023.   \n[35] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. In International Conference on Learning Representations (ICLR), 2024.   \n[36] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, , and Yunfeng Liu. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomputing, 2024.   \n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023.   \n[38] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, et al. Mixtral of Experts. arXiv preprint arXiv:2401.04088, 2024.   \n[39] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and Improving Relative Position Encoding for Vision Transformer. In International Conference on Computer Vision (ICCV), 2021.   \n[40] Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In Neural Information Processing Systems (NeurIPS), 2007.   \n[41] Ali Rahimi and Benjamin Recht. Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning. In Neural Information Processing Systems (NeurIPS), 2008.   \n[42] Juhong Min, Yucheng Zhao, Chong Luo, and Minsu Cho. Peripheral Vision Transformer. In Neural Information Processing Systems (NeurIPS), 2022.   \n[43] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional Positional Encodings for Vision Transformers. In International Conference on Learning Representations (ICLR), 2023.   \n[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In International Conference on Computer Vision (ICCV), 2021.   \n[45] Qihang Fan, Huaibo Huang, Xiaoqiang Zhou, and Ran He. Lightweight Vision Transformer with Bidirectional Interaction. In Neural Information Processing Systems (NeurIPS), 2023.   \n[46] Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, and Zengqiang Chen. FViT: A Focal Vision Transformer with Gabor Filter. arXiv preprint arXiv:2402.11303, 2024.   \n[47] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-Scale Conv-Attentional Image Transformers. In International Conference on Computer Vision (ICCV), 2021.   \n[48] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal Self-attention for Local-Global Interactions in Vision Transformers. In Neural Information Processing Systems (NeurIPS), 2021.   \n[49] Minghao Chen, Kan Wu, Bolin Ni, Houwen Peng, Bei Liu, Jianlong Fu, Hongyang Chao, and Haibin Ling. Searching the search space of vision transformer. In Neural Information Processing Systems (NeurIPS), 2021.   \n[50] St\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew L. Leavitt, Ari S. Morcos, Giulio Biroli, and Levent Sagun. ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases. In International Conference on Machine Learning (ICML), 2021.   \n[51] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the Design of Spatial Attention in Vision Transformers. In Neural Information Processing Systems (NeurIPS), 2021.   \n[52] Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang. Metaformer Baselines for Vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[53] Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. XCiT: Cross-Covariance Image Transformers. In Neural Information Processing Systems (NeurIPS), 2021.   \n[54] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin Transformer V2: Scaling Up Capacity and Resolution. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[55] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary Position Embedding for Vision Transformer. arXiv preprint arXiv:2403.13298, 2024.   \n[56] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing Sparsity in Vision Transformers: An End-to-End Exploration. In Neural Information Processing Systems (NeurIPS), 2021.   \n[57] Lu Yu and Wei Xiang. X-Pruner: eXplainable Pruning for Vision Transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[58] Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui. Width & Depth Pruning for Vision Transformers. In AAAI Conference on Artificial Intelligence, 2022.   \n[59] Paul Michel, Omer Levy, and Graham Neubig. Are Sixteen Heads Really Better than One? In Neural Information Processing Systems (NeurIPS), 2019.   \n[60] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multihead self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Association for Computational Linguistics (ACL), 2019.   \n[61] Maximiliana Behnke and Kenneth Heafield. Losing Heads in the Lottery: Pruning Transformer. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.   \n[62] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In International Conference on Learning Representations (ICLR), 2017.   \n[63] Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, and Tong Zhang. Multi-Head Attention with Disagreement Regularization. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.   \n[64] Jian Li, Xing Wang, Zhaopeng Tu, and Michael R. Lyu. On the diversity of multi-head attention. Neurocomputing, 2021.   \n[65] Po-Yao Huang, Xiaojun Chang, and Alexander Hauptmann. Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.   \n[66] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A Mathematical Framework for Transformer Circuit. In Transformer Circuits Thread, 2022.   \n[67] Prem Melville and Raymond J Mooney. Diverse ensembles for active learning. In International Conference on Machine Learning (ICML), 2004.   \n[68] Gavin Brown, Jeremy Wyatt, Rachel Harris, and Xin Yao. Diversity creation methods: a survey and categorisation. Information Fusion, 2005.   \n[69] Kunihiko Fukushima and Sei Miyake. Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. Pattern recognition, 1982.   \n[70] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten Digit Recognition with a Back-Propagation Network. In Neural Information Processing Systems (NeurIPS), 1989.   \n[71] Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. Discrete applied mathematics, 1996.   \n[72] Zhiying Lu, Hongtao Xie, Chuanbin Liu, and Yongdong Zhang. Bridging the gap between vision transformers and convolutional neural networks on small datasets. In Neural Information Processing Systems (NeurIPS), 2022.   \n[73] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. In International Conference on Learning Representations (ICLR), 2022.   \n[74] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. In Neural Information Processing Systems (NeurIPS), 2021.   \n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Neural Information Processing Systems (NeurIPS), 2017.   \n[76] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do Vision Transformers See Like Convolutional Neural Networks? In Neural Information Processing Systems (NeurIPS), 2021.   \n[77] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. Transactions on Machine Learning Research, 2022.   \n[78] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (ICML), 2021.   \n[79] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: better training with larger batches. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.   \n[80] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2020.   \n[81] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain ViT baselines for ImageNet-1k. arXiv preprint arXiv:2205.01580, 2022.   \n[82] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. In International Conference on Learning Representations (ICLR), 2015.   \n[83] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. In AAAI Conference on Artificial Intelligence, 2015.   \n[84] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for Semantic Segmentation. In International Conference on Computer Vision (ICCV), 2021.   \n[85] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[86] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 2019.   \n[87] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \n[88] Nostalgebraist. Interpreting gpt: The logit lens. https://www.lesswrong.com/posts/ AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.   \n[89] Martina G. Vilas, Timothy Schauml\u00f6ffel, and Gemma Roig. Analyzing Vision Transformers for Image Classification in Class Embedding Space. In Neural Information Processing Systems (NeurIPS), 2023.   \n[90] Sonia Joseph. Vit prisma: A mechanistic interpretability library for vision transformers. https://github.com/soniajoseph/vit-prisma, 2023.   \n[91] Shanghua Gao, Zhong-Yu Li, Ming-Hsuan Yang, Ming-Ming Cheng, Junwei Han, and Philip Torr. Large-scale Unsupervised Semantic Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.   \n[92] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Neural Information Processing Systems (NeurIPS), 2017.   \n[93] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. In Neural Information Processing Systems (NeurIPS), 2019.   \n[94] Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. In International Conference on Machine Learning (ICML), 2019.   \n[95] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On Calibration of Modern Neural Networks. In International Conference on Machine Learning (ICML), 2017. [96] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations (ICLR), 2019.   \n[97] Jianhua Lin. Divergence measures based on the Shannon entropy. IEEE Transactions on Information theory, 1991.   \n[98] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations (ICLR), 2022.   \n[99] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What Do Self-Supervised Vision Transformers Learn? In International Conference on Learning Representations (ICLR), 2023.   \n[100] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning (ICML), 2023.   \n[101] Wendy Kan Addison Howard, Eunbyung Park. Imagenet object localization challenge. https: //kaggle.com/competitions/imagenet-object-localization-challenge, 2018.   \n[102] Mohammad Reza Taesiri, Giang Nguyen, Sarra Habchi, Cor-Paul Bezemer, and Anh Nguyen. Imagenet-hard: The hardest images remaining from a study of the power of zoom and spatial biases in image classification. In Neural Information Processing Systems (NeurIPS), 2023.   \n[103] Polina Kirichenko, Mark Ibrahim, Randall Balestriero, Diane Bouchacourt, Shanmukha Ramakrishna Vedantam, Hamed Firooz, and Andrew G Wilson. Understanding the detrimental class-level effects of data augmentation. In Neural Information Processing Systems (NeurIPS), 2023.   \n[104] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.   \n[105] Alexandra Sasha Luccioni and David Rolnick. Bugs in the data: How ImageNet misrepresents biodiversity. In AAAI Conference on Artificial Intelligence, 2023.   \n[106] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.   \n[107] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EVA: Exploring the limits of masked visual representation learning at scale. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[108] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning (ICML), 2021.   \n[109] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019.   \n[110] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018.   \n[111] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In International Conference on Computer Vision (ICCV), 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 ImageNet-HR ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We invest considerable resources to ensure ImageNet-HR\u2019s quality with two priorities. $\\Lsh$ Annotation accuracy \u2014 we only include images for which we are confident of their label; we achieve this by: $\\textbf{\\scriptsize e55}$ rounds of quality control consisting of manually reviewing all cases where models disagreed with our annotations, using a SoTA model (eva02_large_patch14_448.mim_m38m_ft_in22k_in1k from timm [104]) and a weaker model that disagrees more often (tiny_vit_5m_224.dist_in22k_ft_in1k from timm [104]), $\\oplus$ consulting someone with wildlife expertise to limit the annotation errors made by other test sets [105], $\\circ$ using multiple labels where necessary, for example, combining the \u201csunglass\u201d and \u201csunglasses\u201d classes, and labeling a \u201ctusker\u201d as also an \u201cAsian elephant,\u201d if the image of the tusked animal is an Asian elephant. $\\circledcirc$ Image diversity \u2014 when collecting images, we try to maximize the diversity of images belonging to a class. Models achieve high accuracy on ImageNet-HR, likely due to less label ambiguity than other ImageNet test sets. Finally, we manually crop all images to $\\mathrm{i024^{2}}$ px, resulting in the first natively high-resolution ImageNet test set. ", "page_idx": 16}, {"type": "text", "text": "We collect the vast majority of images from flickr and Unsplash. Unsplash images \u201care made to be used freely\u201d for commercial and non-commercial uses. flickr images were selected from the \u201cAll creative commons\u201d license option. However, for some classes, we could not find enough open-access high-resolution images like \u201coil filter\u201d or \u201chand or block plane,\u201d so we used Google search to find more. We estimate that around 50 of 5k images were not collected on filckr or Unsplash. Nine images were taken by an author or his family, with consent of everyone involved. ", "page_idx": 16}, {"type": "text", "text": "A.2 Extrapolation Results ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/a054164d8aab656413753566747964d9bba9eb26668b220d02a987360b329e5b.jpg", "table_caption": ["Table 6: Extrapolation results for ViT-B models trained on ImageNet for 150 epochs; trained at $224^{2}$ and tested at various resolutions. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.2.1 Other Models ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/cf81a4f886f86ea172c556a033e450273ee4e99eb81156717b07a76269b98004.jpg", "table_caption": ["Table 7: Top-1 acc. $(\\%)$ on Val [1] for models outside our controlled experiment, using the timm library [104]. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/eb0884a9a0fdc408eeb2fbd359f329c3581a88ac65302cf2137d2481db9e6a9a.jpg", "table_caption": ["Table 8: Top-1 acc. $(\\%)$ on -HR (ours) for models outside our controlled experiment, using the timm library [104]. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.3 LookHere Bias Matrices ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/62d118b39792047c7f34e8bed378d45301a7300ebbd8e7a6f8c153c7e70febcd.jpg", "img_caption": ["Figure 9: LH-180 bias matrices for query patch (11,8), grid size of 14x14. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/09710142cdc756b7155efcca77d063b169f6e1a25bbf0298c8f40a25ab8b5bcd.jpg", "img_caption": ["Figure 10: LH-90 bias matrices for query patch (11,8), grid size of 14x14. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/018c987522a05c5a556905d56caee1c1226221e599c09301f17d777c1b350c9a.jpg", "img_caption": ["Figure 11: LH-45 bias matrices for query patch (11,8), grid size of 14x14. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.4 Experimental Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A.4.1 Training ViTs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recipe. Our training recipe that is consistent across configurations: ", "page_idx": 21}, {"type": "text", "text": "\u2022 AdamW [109] \u2014 using the default PyTorch implementation that does not fully decouple learning rate and weight decay   \n\u2022 Binary cross-entropy loss \u2014 summing along the class dimension, averaging along the batch dimension   \n\u2022 Linear warm-up for $10\\%$ of steps and cool-down using a cosine decay schedule to a zero learning rate   \n\u2022 Batch size of 2048   \n\u2022 Mixup [110] $\\alpha=0.8$ , cutmix [111] $\\alpha=1$   \n\u2022 CLS token with an MLP classifying head \u2014 final linear layer weights are initialized to 0 and biases to $-6.9$ (so all class probabilities start at $\\textstyle{\\frac{1}{1000}}$   \n\u2022 layer drop rate of 0.1 and MLP dropout of 0   \n\u2022 Train for 150 epochs on the first $99\\%$ of ImageNet-1k \u2014 using Huggingface\u2019s datasets library, i.e., load_dataset(\"imagenet-1k\", split $=$ \"train[:99%]\")   \n\u2022 Choose checkpoint according to the best minival top-1 accuracy (run after each epoch), where minival is the last $1\\%$ of the ImageNet-1k training set, i.e., load_dataset(\"imagenet-1k\", split $=$ \"train[99%:]\") ", "page_idx": 21}, {"type": "text", "text": "A.4.2 Compute ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Training takes around 3 days on an RTX 4090 GPU. Thus, all 80 training runs take around 240 GPU-days. We spend another 54 GPU-days on 18 ablations. Ablations and our iRPE run always use our best training recipe, which is 3-Augment [17] data augmentation, $3\\cdot10^{-3}$ learning rate, and 0.05 weight decay. iRPE [39] takes around 7 days on an RTX 4090 GPU, even with the official custom CUDA kernel. As a result, we exclude it from our apples-to-apples comparisons. ", "page_idx": 21}, {"type": "text", "text": "A.4.3 High-resolution finetuning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Following DEiT III\u2019s finetuning recipe [17], we increase the drop rate to 0.2 and the weight decay to 0.1, and fix the learning rate to $10^{-5}$ with a 512 batch size. ", "page_idx": 21}, {"type": "text", "text": "A.4.4 Extrapolation Tuning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For 2D-ALiBi, 2D-RoPE, and LookHere models, we tune a single parameter at the target resolution on minival (Table 9). LookHere models benefit less from tuning than 2D-ALiBi and 2D-RoPE models. For example at a $512^{2}$ resolution, the difference in top-1 accuracy on minival when using the tuned parameter versus the default value is $2.1\\%$ for 2D-ALiBi, $1.3\\%$ for 2D-RoPE, and $0.15\\bar{\\%}$ for LH-45. Thus, LookHere does not require tuning its global slope value to effectively extrapolate. ", "page_idx": 21}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/66abe4d84d0ee97b41fd3046a15b02f87e27f201e966e9b0cefeddf7191a1dc4.jpg", "table_caption": ["Table 9: Tuned Parameter Values "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.4.5 Segmentation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For both linear probing and full finetuning we use a linear decoder. The linear decoder consists of a linear layer applied to the frozen patch representations which is then upsampled to the original image size. Similar to [85] we add a BatchNorm layer before the linear layer. ", "page_idx": 21}, {"type": "text", "text": "For full finetuning, we followed the Segmenter training recipe [84] exactly. For ADE20k, the base learning rate is $1\\bar{0}^{-3}$ for $160\\mathbf{k}$ iterations with a batch size of 8, at $512^{2}$ px. For Cityscapes, the base learning rate is $10^{-2}$ for $80\\mathbf{k}$ iterations with a batch size of 8, at $384^{2}$ px. We train with SGD. For linear probing, we freeze the backbone and pre-compute the patch representations. We use the AdamW optimizer [109] and sweep the following learning rates: $\\{0.0001,0.0002,0.0005,0.001,0.00\\dot{2},0.005,0.01,0.02,0.05,0.\\dot{1},0.2,0.3,0.5\\}$ . For both ADE20k and Cityscapes we set the batch size to 16 and train the linear decoder for 50 epochs. ", "page_idx": 22}, {"type": "text", "text": "A.5 Full Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/29a89ab7cdf989dafc6f35ddaf513e0ef86a4b56bf24f902e53658f72ec41119.jpg", "table_caption": ["Table 10: First half of our hyper-parameter sweep. ViT-B models trained on ImageNet for 150 epochs; trained and tested at $224^{2}$ . RA is for RandAugment and 3A for 3-Augment. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/23f0bc7c4e45de402e02a4335ac51ae31b33202711f42b80231d2040b902cf8d.jpg", "table_caption": ["Table 11: Second half of our hyper-parameter sweep. ViT-B models trained on ImageNet for 150 epochs; trained and tested at $22\\dot{4}^{2}$ . RA is for RandAugment and 3A for 3-Augment. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "We train 18 models to ablate the LookHere design. Each run uses our best 150 epoch training recipe. We test models without extrapolation at $224^{2}$ px (Table 12) and with extrapolation at $1024^{2}$ px (Table 13). Before running extrapolation tests, we tune the global slope of each model at $\\mathrm{1024^{2}}$ px to fairly compare with our three default variants. To fit in the tables, we use short forms explained here: \u201cundir $\\rightarrow90^{\\circ}$ means replacing the four undirected heads with four $90^{\\circ}$ FOV heads, \u201cundir $\\rightarrow$ no dist\u201d means removing the distance penalties on the four undirected heads, \u201cinvert\u201d means inverting the layer-wise slope pattern such that $s_{l}$ linearly increases from 0.5 to 1.5 with depth, \u201cmask: $\\infty\\rightarrow0^{\\circ}$ means replacing $\\infty$ with 0 in equation 1, and \u201cdist $\\rightarrow$ no dist\u201d means removing the distance penalties on all heads. ", "page_idx": 25}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/2c825fdd691be537ac440d7d8352f2973a5981123e174e91d86e40d39d5f19bd.jpg", "table_caption": ["Table 12: LookHere design ablations without extrapolation. ViT-B models trained on ImageNet for 150 epochs; trained and tested at $224^{2}$ . "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "o7DOGbZeyP/tmp/612b79a7c0d7bf49b52fe9821756d1db013a7246137f2f708e05490f9e02f007.jpg", "table_caption": ["Table 13: LookHere design ablations with extrapolation. ViT-B models trained on ImageNet for 150 epochs; trained at $224^{2}$ and tested at $\\mathrm{1024^{2}}$ . "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "A.7 Logit Lens ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/e9d300025563c91f668afc857342fecb5dde75c916ce9e3fb88026cd4fd3bdac.jpg", "img_caption": ["Figure 12: More examples from ImageNet-S and each model\u2019s logit lens predictions. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/8c58c187e91224cc13cb188cdf3896805df3e7dbee90ee97a8d31e8bd2873166.jpg", "img_caption": ["Figure 13: We plot the average class identifiability [89] across the model layers on 1000 images from Val for the class and patch tokens. This is a measure of how recoverable the correct class is from the class projection of the token. The score ranges from 0 to 1, with 1 denoting that the correct class has the highest logits and 0 the lowest. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/c9f49d1160e15037ff249b9f94b9c2bf27928c9b38ffb73dd7330552b1d9b9d6.jpg", "img_caption": ["Figure 14: Leveraging the semantic segmentation labels from the ImageNet-S, we compared the identifiability rate of class patches (blue) vs non-class tokens [89] across the model layers on 1000 images from Val. LookHere can discriminate between class and non-class patches. Other positional encodings cannot unless they are trained for much longer. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In our paper, we show that LookHere prevents attention collapse measured by JSD (Figure 4). Here, we measure attention diversity using $L_{1}$ and $L_{2}$ distance. We also measure attention distances and patch-wise representational similarity \u2014 both at $224^{2}$ px (Figure 15) and at all resolutions tested (Figure 16 & 17). ", "page_idx": 29}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/969bdb74e4f84bf6e21ef57f4db5bd391ab092243917d0f1f6d328652d3f9778.jpg", "img_caption": ["Figure 15: Measurements of head diversity, attention distance, and patch similarity by layer across position encoding methods. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/7184a2b61cdb8d43e3f5f4986eeb5c5548d8d3d7bf6dcd653f6b50afd4a97a3b.jpg", "img_caption": ["Figure 16: We measure the attention distance per depth, for each model and resolution, by taking the sum of patch distances weighted by attention scores, averaged across heads [(row: 1, col: $1)\\rightarrow(4,1)]$ ; and the head diversity as the generalized JSD of attention matrix rows for each head, averaged over rows $[(4,2)\\rightarrow(7,2)]$ . We report the average over 500 randomly selected images from minival. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/023b5a75d8ed3e5b2fc318f21762aa0ebbeed731c2faf764cd54e537dbc572f0.jpg", "img_caption": ["Figure 17: The patch similarity, for each model and resolution, is measured as the average of pairwise cosine-similarities between patch representations in each layer $[(7,3)\\rightarrow(10,3)]$ ; we report the average over the same 500 minival images. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/1ff761ebad3177cc7c1128f1872f96312ef48ca7c79d3b7007089dde1d9a3050.jpg", "img_caption": ["Figure 18: 1D-learn attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/866005638e285c2b6fe80604abd5493e4fab625b60c2de698f049903a0b9e089.jpg", "img_caption": ["Figure 19: 2D-sincos attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/ee7a9c38cd9d463848cd1301910360ea33ca6d8138f800bf6248c3a85b402620.jpg", "img_caption": ["Figure 20: Factorized attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/1ef4e177f15e233e5c87a22a87fb927ad4d1d909a305f90710add98457845a53.jpg", "img_caption": ["Figure 21: Fourier attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/7a94b6d65ec1fa0970c676fc562ca0d5abe1d846472d5e5417afd222f7c6430a.jpg", "img_caption": ["Figure 22: RPE-learn attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/f68d0adef818c341d0ed4c0c1e3eeabc70db1857edb6a15ecc610aa24c1fbbdd.jpg", "img_caption": ["Figure 23: 2D-ALiBi attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/9b4e09981eba0bbd2ac8162f1c07b175cdd2064472c2d1a8fbf89070c9482c7b.jpg", "img_caption": ["Figure 24: 2D-RoPE attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/b39fa869028d8c31798934b5b12a7a1f3cac908a39f5bbd346f3be1cd56c7ffd.jpg", "img_caption": ["Figure 25: LH-180 attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/c417a5eb7aed92472273aef5cfeaa7e9b8abde35986f5789a3df7a07b00b28c8.jpg", "img_caption": ["Figure 26: LH-90 attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/209e631d3fce5570980b0bd96e4b381b6b6c8885318cb230d964e0ceff5b5bb4.jpg", "img_caption": ["Figure 27: LH-45 attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/95feb9c8c186d4168514d7ff8df62dc669da3fd5e4fa36bae23864e1ee928138.jpg", "img_caption": ["A.9 Accuracy Gaps and Class-level Effects ", "Figure 28: We calculate the difference in accuracy for the first 5k ImageNet [1] examples when extrapolating from $224^{2}$ px for models trained at $\\overline{{224^{2}}}$ px "], "img_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/ff037a6cb7cab8e6c1af6cd66a246e368a36bb63201526d4566ce64eb5ccbe60.jpg", "img_caption": ["LH-180 (ours) LH-90 (ours) LH-45 (ours) Factorized [6] 2D-RoPE [7] ", "Figure 29: We calculate the prediction frequency for each class for the first 5k ImageNet [1], and plot those frequencies for the classes with the largest decrease $[(1,1)\\rightarrow(4,2)],$ increase $[(4,3)\\rightarrow(7,2)]$ , and spread $[(7,3)\\rightarrow(10,3)]$ at $1024^{2}$ px. "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/7f99f8c717844538f664730ee78c261c7a0e02c0e29895522ff52785bd6b63fc.jpg", "img_caption": ["Figure 30: A continuation of increase, and spread plots. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/9ec1757f9dfd55f547aa1270d846dd4c86532dd2883b5a5475f176e9bceda723.jpg", "img_caption": ["LH-180 (ours) LH-90 (ours) LH-45 (ours) Factorized [6] 2D-RoPE [7] 2D-sincos [8] Fourier [11] 2D-ALiBi [12] ", "Figure 31: We calculate the prediction accuracy for each class among the first 5k ImageNet [1] examples, and plot these accuracies for classes with the top five classes with the largest decrease $[(1,1)\\rightarrow(2,3)]$ , increase $[(3,1)\\rightarrow(4,3)]$ , and spread $[(5,1)\\rightarrow(6,3)]$ at $\\mathrm{1024^{2}}$ px, along with the average across all five. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/8a13c54f7a77e075b0e280f7ba36f5fa6d99684af98c942b9a46b626e40f4d24.jpg", "img_caption": ["Figure 32: We find and plot cross-plots of class pairs that confuse models during extrapolation, indicated by a transfer in prediction probabilities. For the subset of ImageNet [1] examples, within the first 5k, with true class $X$ (blue) we select pairs $(X,Y)$ where $P(\\dot{X}|224^{2})\\mathrm{~-~}$ $P(X|1024^{2})+P(\\breve{Y_{\\left|1024\\right.^{2}}})-P(Y|224^{2})$ is maximized. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/18a4ca660b7019206468331df11d0990e065e58fdd12975d9eb97285eae1bfee.jpg", "img_caption": ["Figure 33: Cross-plots continued. "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/5c0fa6a2064542fba8e63b1e48d75a5c4631f7186f64a3c6b5f84c759bc56366.jpg", "img_caption": ["Figure 34: Cross-plots continued. "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "o7DOGbZeyP/tmp/bb6960a517d64c970595c63a0d8d188c637982e625977a530b2397eb7350568d.jpg", "img_caption": ["Figure 35: Cross-plots continued. "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction are supported by thorough experiments in our paper. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 50}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: We discuss limitations in the Closing of our paper. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 50}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: We include no theoretical results. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 51}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our method can be implemented following the details provided in our paper.   \nOur method is quite simple and can easily be integrated into existing ViT codebases. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 51}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We provide the code of our PyTorch implementation with a link, which also contains all images from our introduced dataset, ImageNet-HR. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 52}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: In our main paper, we outline hyperparameters that differ between runs. In our Appendix, we outline hyperparameters that are consistent between runs. We use commonly used benchmarks that are listed in our paper. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 52}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [No] ", "page_idx": 52}, {"type": "text", "text": "Justification: However, we report all experimental results in the Appendix. These can be used to calculate statistical significance if required. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: In our \u201cTraining ViTs\u201d Appendix section we provide details on the required GPU-hours. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 53}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We read the Code of Ethics. In our Appendix, we discuss consent and copyright with respect to the dataset we introduce. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 53}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We discuss broader impacts in the Appendix. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 53}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 54}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: Our dataset consists of 5k images, we manually downloaded them from the internet and reviewed them several times. We do not believe any images pose a safety risk. Our models do not pose any more of a safety risk than other ViT-B models. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 54}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We properly credit creators of datasets and models. We share the websites used to collect images and their licenses. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We share training details in the Appendix and provide code via a zip file. We discuss dataset license in the Appendix and provide all images via a URL. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 55}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: We do not perform research with human subjects nor crowdsource. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 55}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: Our research does not require IRB approvals. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}]