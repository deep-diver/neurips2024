[{"type": "text", "text": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hao Bai1,2\u2217Yifei Zhou1\u2217 Mert Cemri1 Jiayi Pan1 ", "page_idx": 0}, {"type": "text", "text": "Alane Suhr1 Sergey Levine1 Aviral Kumar3,4 ", "page_idx": 0}, {"type": "text", "text": "1UC Berkeley 2UIUC 3CMU 4Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offilne-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a $49.5\\%$ absolute improvement \u2013 from 17.7 to $67.2\\%$ success rate \u2013 over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V ( $8.3\\%$ success rate) and the 17B CogAgent trained with AitW data $(38.5\\%)$ , but also the prior best autonomous RL approach based on filtered behavior cloning $(57.8\\%)$ , thereby establishing a new state-of-the-art for digital agents for in-the-wild device control. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Advances in vision-language models (VLMs), especially in regards to their remarkable commonsense, reasoning, and generalization abilities imply that realizing a fully autonomous digital AI assistant, that can simplify human life by automating day-to-day activities on computer devices via natural language interfaces, is no longer a distant aspiration [16, 45, 56]. An effective device-control AI assistant should be able to complete tasks in-the-wild through Graphical User Interfaces (GUIs) on digital devices: make travel plans; experiment with presentation designs; and operate a mobile device autonomously, all while running amidst stochasticity and distractors on the device, the Internet, and the tools it interacts with. However, enhanced reasoning or common-sense abilities do not directly transfer to intelligent assistant behavior: ultimately we want AI assistants to accomplish tasks, exhibit rational behavior, and recover from their mistakes as opposed to simply producing a plausible completion to a given observation based on the data seen during pre-training. This implies that a mechanism to channel abilities from pre-training into a deployable AI \u201cagent\u201d is lacking. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Even the strongest proprietary VLMs, such as GPT-4V [24] and Gemini 1.5 Pro [7] 2, still struggle to produce the right actions when completing tasks on devices. While general-purpose vision-language abilities help these models still make meaningful abstract deductions about novel scenes when deployed, these deductions do not transfer to accurate reasoning for control [47, 45, 55, 44]. As a result, most prior work for building device agents construct complex wrappers around proprietary VLMs by combining them with prompting, search, or tool use [47, 44, 52, 51, 45]. While building prompting or retrieval wrappers to improve decision-making performance of existing VLMs enhances their performance in the short run, without updating the weights, the effectiveness of the resulting agent is inherently limited by the capabilities of the base model [49, 3]. For example, we found that off-the-shelf VLMs make reasoning failures that derail the agent (e.g., Figure 2 and Figure 17), as direct consequences of inability of the base model to reason with low-level device-control actions. A different solution is to fine-tune the model on demonstrations via imitation learning. However, the dynamic nature of the web and device means that models trained to mimic actions in stale data can result in sub-optimalilty as the eco-system changes [26]. Agents trained in this way struggle to recover from the agents\u2019 own mistakes [8, 12]. ", "page_idx": 1}, {"type": "text", "text": "If we can instead build an interactive approach to train a VLM to directly adapt and learn from its own experience on the device and the Internet, that can be used to build a robust and reliable device-control agent, without needing wrappers on top of proprietary models. However, this learning-based approach must satisfy some desiderata. First, it must make use of online interaction data since static demonstration data would not be representative of the task when the model is deployed: for instance, even in the setting of web navigation alone, dynamic nature of in-the-wild websites means that the agent will frequently encounter website versions that differ significantly from the sce", "page_idx": 1}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/efcde25574a9e81219aaba1d617b1aee4a70b62e8a22a749d1a15f1b592a0188.jpg", "img_caption": ["Figure 1: DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing its performance through online RL and autonomous performance evaluations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "narios seen during training and will need to behave reliably despite changes in visual appearance and distractions. Second, learning on-the-fly means the approach must learn from multi-turn interaction data from the model itself, a large of chunk of which would consist of failures. Proper mechanisms must be designed to automatically pick out the correct actions while filtering the wrong ones. ", "page_idx": 1}, {"type": "text", "text": "To this end, our main contribution is a novel autonomous RL approach, DigiRL (i.e., RL for Digital Agents), for training device control agents, as shown in Figure 1. The resulting agent attains state-of-the-art performance on a number of Android device-control tasks. To train this agent, our approach operates in two phases: an initial offilne RL phase to initialize the agent using existing data, followed by an offline-to-online RL phase, that further fine-tunes the model obtained from offline RL on online rollout data. Online RL training requires access to an environment that the agent can interact with and obtain reliable reward signals, all in a reasonable amount of wall-clock time. To do so, we build a scalable and parallelizable Android learning environment equipped with a robust VLM-based general-purpose evaluator [26] (average error rate $2.8\\%$ against human judgement) that supports running up to 64 real Android emulators at the same time to make online RL real-time. Then, to effectively learn autonomously, we develop an online RL approach that retains the simplicity of supervised learning, but incorporates several key deep RL insights to enable fast fine-tuning. Concretely, our approach is a variant of advantage-weighted regression (AWR) [28], equipped with: (i) an automatic curriculum that uses an instruction-level value function to order tasks so as to extract maximal learning signal, which is inspired by prioritized replay methods [11, 32, 23], and (ii) another step-level value function trained via effective cross-entropy loss [17, 5] to extract low-variance and less-biased learning signal amidst stochasticity and diverse tasks. This RL approach allows us to fine-tune VLMs on their own experience. ", "page_idx": 1}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/29d9180f1df4b63982c44be28d0e7ee7b138b0ceee62b72c1a6b38fe59444dc1.jpg", "img_caption": ["Figure 2: Qualitative comparison between DigiRL and other approaches. AutoUI trained from static human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal (searched \u201clogitech g933bestbuy.com logitech $\\mathrm{g}933^{\\circ}$ in Google instead of bestbuy.com). In contrast, DigiRL can recover from such states and complete complex instruction as requested. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We evaluate our agent trained with DigiRL in carrying out diverse instructions from Android in the Wild dataset [31] on real Android device emulators and find that our agent can achieve a $2\\mathbf{8.7\\%}$ improvement over the existing state-of-the-art agents (from $38.5\\%$ to $67.2\\%$ success rate) 18B CogAgent [9], and over $9\\%$ improvement over the prior best autonomous learning approach based on Filtered Behavior Cloning [18, 26]. The performance of our agent also significantly surpasses wrappers on top of state-of-the-art proprietary VLMs such as GPT-4V [24] and Gemini 1.5 Pro [7] $.17.7\\%$ success rate), despite using a significantly smaller model (with 1.3B parameters). To our knowledge, this is the first work to successfully build an autonomous offline-to-online RL approach to enable state-of-the-art performance on device-control problems. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-modal digital agents. In contrast to language-only agents that largely interact with both text or code inputs and outputs [33, 49, 3, 30, 46, 20, 13], training multi-modal agents capable of controlling devices presents different challenges: first, device control is done directly at the pixellevel and in a coordinate-based action space, instead of natural language [31, 44] that LLM is most familiar with, and second, the ecosystem of a device and the Internet tends to be quite stochastic and unpredictable, which is absent with high-level planning in language only. To handle these challenges, prior work largely builds on strong proprietary VLMs [24, 7], and designs complex rule-based wrappers [47, 51, 45, 52] to enhance the visual grounding capabilities of VLMs in GUI interfaces and convert text output into pixel interactions. However, without any form of fine-tuning, this limits the room for possible performance improvement [44, 47, 49, 3, 50], especially when pre-training corpora only present limited action-labeled data. A separate line of work fine-tunes VLMs with demonstration data [19, 15, 9, 53] via imitation learning, but maximizing single-step accuracy from stale demonstrations without accounting for consequences of these actions in subsequent steps may lead to poor solutions amidst stochasticity [26], as agents trained in such ways will struggle to recover from out-of-distribution states not included in the demonstration data [8, 12]. The third category, and perhaps the closest to us, are works that run filtered imitation learning on autonomously-collected data to directly maximize the episode success rate [26, 18]. In contrast, ours is the first work to scale autonomous, offilne-to-online $R L$ for device control, producing an agent that outperforms prior agents built via imitation. Even when compared to prior work running on-policy RL in simplified web navigation settings (MiniWob $^{++}$ [37, 10]), our approach is $1000\\mathrm{x}$ more sample efficient (around 1e3 trajectories compared to around 1e6 trajectories), and operates in real-world GUI navigation tasks. ", "page_idx": 2}, {"type": "text", "text": "Environments for device control agents. Recent works have introduced simulated environments for building device control agents [48, 56, 16, 54, 4, 44]. However, these environments are primarily designed for evaluation, and present only a limited range of tasks within fully deterministic and stationary settings, infeasible for acquiring a diverse repertoire of skills needed for device control. Alternatively, other works use environments with a greater diversity of tasks [48, 37], but these environments often oversimplify the task complexity, thus failing to transfer to in-the-wild settings. Coversely, our training environment utilizes autonomous evaluation [26] with Gemini 1.5 Pro [7] to support diverse, open-ended tasks on parallel actual Android devices, at full scale unlike prior environments. This also contrasts other prior works that use single-threaded Android emulators [26, 39, 19] and thus inefficient for support online RL at scale. ", "page_idx": 2}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/bb4f329e8fc80eb09dbf9613d50223d84eb6eef2b1ee0de431a83c2f84b4870d.jpg", "img_caption": ["Figure 3: Environment details. Top: actions space and dynamics of the environment. Bottom: examples of the read-world non-stationarity and dynamism of the environment. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Reinforcement learning for LLM/VLMs. The majority of prior research employing RL for foundation models concentrates on tasks that must be solved in a single turn, such as preference optimization [25, 58, 2] or reasoning [27]. However, optimizing for single-turn interaction from expert demonstrations may result in sub-optimal strategies for multi-step problems [57, 38, 42], especially amidst a high degree of stochasticity or non-stationarity. Therefore, we focus on building multi-turn RL algorithms that can learn from sub-optimal, online interaction data in this work. While prior works have developed value-based RL algorithms for LLMs [42, 38, 1, 57, 50], they typically require maintaining multiple models such as Q-networks, value-networks, and policy networks, along with their delayed target counterparts, and can be subjective to slow convergence and sensitivity to choices of hyper-parameters. In contrast, we focus on identifying the key design choices for instantiating a simple yet effective RL algorithm for practitioners to incorporate to substantially improve full-scale Android device control. Our approach can serve as a base model for future research. ", "page_idx": 3}, {"type": "text", "text": "3 Problem Setup and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Problem formulation. We are interested in pixel-based interaction with virtual devices. We scope our study in the control of Android devices: this is already significantly more challenging and more general than previous learning-based environments that focus solely on web navigation [16, 56, 4], where the web browser itself is merely one application within our broader environment, and link-based device controls [47, 51] are inadequate for tasks like games that do not support link inputs. ", "page_idx": 3}, {"type": "text", "text": "Each episode begins with the emulator initialized to the home screen. Subsequently, a task is selected from a predefined set of language instructions, some examples of which are shown in Appendix A.1. An agent is then tasked with manipulating the emulator to fulfill this instruction. At each time step, the agent receives a screenshot of the current screen as the observation. Following the action space in prior literature [31], the available actions include tapping and sliding based on normalized $\\bar{(x,y)}$ coordinates (ranging from 0 to 1 relative to the screen dimensions), typing text strings of variable length, and pressing special buttons such as HOME, BACK, and ENTER, as illustrated in Figure 3. Our train and test instructions comes from General and Web Shopping subsets in AitW [31]. These tasks consist of information-gathering tasks like \u201cWhat\u2019s on the menu of In-n-Out?\u201d, and shopping tasks on the web like \u201cGo to newegg.com, search for razer kraken, and select the first entry\u201d. ", "page_idx": 3}, {"type": "text", "text": "Challenges of stochasticity. Real-world device contrl presents unique challenges of stochasticity absent in simulated environments [56, 37] such as: (1) the non-stationarity of websites and applications, which undergo frequent updates, causing the online observations to be different from stale offilne data, (2) various unpredictable distractors such as pop-up advertisements, login requests, and the stochastic order of search results. (3) technical challenges and glitches such as incomplete webpage loading or temporary access restrictions to certain sites. Examples of scenarios with such stochasticity from our experiments are shown in Figure 3. We observe that these stochastic elements pose significant challenges for pre-trained VLMs, including even those fine-tuned on device control data. As a concrete example, Figure 4 shows an experiment result that illustrates the necessity of continuously adapting the models to the non-stationarity of websites and applications. After obtaining a good checkpoint using our approach (DigiRL), that we will introduce in the next section, with autonomous data from June.1 to June.3, we compare the performance of a frozen policy and a continuously updating policy using fresh autonomous data from June.7 to June.11. We find that indeed the the performance of the frozen policy gradually degrades over time due to the changes on websites and applications, while continuous online updates plays a key role in preventing this degradation. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Setup for reliable and scalable online RL. As autonomous RL interleaves data collection and training, to maximize learning amidst stochasticity, it is crucial to have a real-time data collection pipeline to collect enough experience for gradient updates. While this is not possible in single-thread Android emulator environments [26, 39] due to latency, we parallelize our Android emulator using appropriate error handling as discussed in Appendix A.1. In addition, the environment must provide a reward signal by judging whether the current observation indicates the agent has successfully completed the task. To generalize our evaluator to support a wide range of tasks, we extend Pan et al. [26]\u2019s end-to-end autonomous evaluator that does not require accessing the internal states of the emulator or human-written rules for each task. This contrasts previous works that manually write execution functions to verify the functional com", "page_idx": 4}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/282f0d4cffbe6a732f8c3fb21132443f8129e88bcae6e669148e90e983246e43.jpg", "img_caption": ["Figure 4: Performance of our approach (DigiRL) in different training modes on the Webshop subset. When utilizing a stale checkpoint, i.e., \u201cfrozen\u201d (black $^{+}$ blue curve) performance generally begins to degrade as time evolves, whereas autonomous online training (black+red curve) via DigiRL allows us to retain performance despite non-stationarity and stochasticity. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "pleteness of each task [16, 48, 37, 44]. We adopt Gemini 1.5 Pro [6, 7] as the backbone of the autonomous evaluator. We seed this model with few-shot rollouts and the associated human-labeled success indicators to guide evaluation of novel queries. This pipeline enables a single evaluator that can evaluate all AiTW tasks. The evaluator is highly aligned with human annotations (average error rate $2.8\\%$ ), validated in Figure 8. ", "page_idx": 4}, {"type": "text", "text": "4 DigiRL: Autonomous RL for Building a Strong Device-Control Agent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present our autonomous RL framework for training device agents. We pose the device control problem as a Markov decision process (MDP) and develop RL methods for this MDP. The core of our approach is based on a simple and scalable off-policy RL method, advantage-weighted regression (AWR) [29], but we make crucial modifications to handle stochasticity and highly-variable task difficulty, through the use of value functions trained with appropriate losses, and an automatic curriculum, induced by an instruction-level value function to maximize learning. ", "page_idx": 4}, {"type": "text", "text": "Device control and GUI navigation as a MDP. We conceptualize device control guided by natural language instructions as a finite horizon Markov Decision Process (MDP) represented by $\\mathcal{M}=\\{\\bar{S},\\bar{A},T,\\mu_{0},\\mathcal{R},H\\}$ and run policy gradient to solve this MDP. At the beginning, an initial state $s_{0}$ and a natural language instruction $c$ are sampled from the initial state distribution $\\mu_{0}$ . A reward of 1 is given at the end if the agent successfully fulfills the task per the evaluator, otherwise a reward of 0 is given. The trajectory terminates either when the agent accomplishes the task or when the maximum allowed number of interactions $H$ is exceeded. States are represented using the last two screenshots. To explain our approach in detail, we also include several standard definitions used in reinforcement learning (RL). The $\\mathrm{^Q}$ function for a policy $\\pi$ represents the expected longterm return from taking a specific action at the current step and then following policy $\\pi$ thereafter: $\\begin{array}{r}{Q^{\\pi}(s_{h},a_{h},c)=\\mathbb{E}_{\\pi}\\left[\\stackrel{{\\displaystyle\\cdot}}{\\sum_{t=h}^{H}r}(s_{t},a_{t},c)\\right]\\!.}\\end{array}$ . The value function $V^{\\pi}(s_{h},c)$ is calculated by averaging the Q-value, $Q^{\\pi}(s_{h},\\bar{a}_{h},c)$ , over actions $a_{h}$ drawn from the policy $\\pi$ . The advantage $A^{\\pi}(s_{h},a_{h},c)$ for a state-action pair is computed by subtracting the state\u2019s value under the policy from its $\\mathrm{{Q}}.$ -value: $A^{\\pi}(s_{h},a_{h},c)=\\bar{Q}^{\\pi}(s_{h},a_{h},\\bar{c})-V^{\\bar{\\pi}}(s_{h},c)$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Backbone of Our Approach: Off-Policy RL via Advantage-Weighted Regression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The starting point we choose to build our approach on is the advantage-weighted regression (AWR) algorithm [29], which says that we can improve the policy reliably by regressing the policy towards exponentiated advantages induced by the reward function, as a proxy for optimizing the policy gradient while staying close to the previous policy [14, 35, 34]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{\\nu}\\left[\\log\\pi(a|s,c)\\cdot\\exp\\left(A(s,a,c)/\\beta\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some positive parameter $\\beta$ and the distribution of past experience $\\nu$ , and $A(s,a,c)$ denotes the advantage of a state-action pair $(s,a)$ given a context $c$ . To avoid tuning the hyperparameter $\\beta$ , we consider an alternative that does \u201chard filtering\u201d on the advantages instead of computing $\\exp(A)$ , similar to prior works [22, 43]. This leads to the following loss function for fine-tuning the model: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pi)=-\\mathbb{E}_{\\mathrm{fllter}(\\nu)}[\\log\\pi(a|s,c)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Typically, these advantages are computed by running Monte-Carlo (MC) rollouts in the environment to estimate the value of a given state-action pair, and subtracting from it an estimate of the value of the state given by a learned value estimator alone. However, this approach is likely to produce high-variance advantages given the stochasticity of the device eco-system that affects MC rollouts. ", "page_idx": 5}, {"type": "text", "text": "4.2 Obtaining Reliable Advantage Estimates from Doubly-Robust Estimators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To reliably identify advantageous actions given significant environment stochasticity, we construct a per-step advantage estimator, inspired by doubly-robust estimators [40, 36]: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA^{\\mathrm{sep}}(s_{h},a_{h},c):=\\lambda^{H-h}r(s_{H},a_{H},c)+(1-\\lambda^{H-h}r(s_{H},a_{H},c))(V^{\\mathrm{sep}}(s_{h+1},c)+r(s_{h},a_{h},c)-V^{\\mathrm{sep}}(s_{h},c)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is a weighting hyper-parameter. This construction of the advantage estimator is a simplified version of Generalized Advantage Estimation (GAE) [36] using only the next-step advantage estimator and final-step advantage estimator as there are no intermediate rewards in our problem. This construction balances an advantage estimator with higher variance Monte-Carlo estimates $\\lambda^{H-h}r(s_{H},a_{H},c)$ (due to stochasticity) and an estimator with higher bias $V^{\\mathrm{step}}(s_{h+1},c)+r(s_{h},a_{h},c)-V^{\\mathrm{step}}(s_{h},c)$ (due to imperfect ftiting of the value function). We observed that combining both high-variance and high-bias estimators gave us a sweet-spot in terms of performance. To implement the step-level hard flitering, we simply threshold this doubly robust estimator as $A^{\\mathrm{step}}(s_{h},a_{h},\\bar{c})>1/H$ to decide which actions progress towards the goal. ", "page_idx": 5}, {"type": "text", "text": "4.3 Automatic Curriculum using an Instruction-Level Value Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While the AWR update (Equation 4.1) coupled with a robust advantage estimator (Equation 4.3) is likely sufficient on standard RL tasks, we did not find it to be effective enough for device control in preliminary experiments. Often this was the case because the task set presents tasks with highlyvariable difficulties that collecting more data on tasks that the agent was already proficient at affected sample efficieny negatively. In contrast, maximal learning signal can be derived by experiencing the most informative tasks for the agent during training. To this end, we design an instruction-level value function $V^{\\mathrm{instruct}}(c)$ to evaluate if a given rollout can provide an effective learning signal: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A^{\\mathrm{instuct}}(s_{h},a_{h},c):=\\sum_{t=h}^{H}\\!r(s_{t},a_{t},c)-V^{\\mathrm{instuct}}(c)=r(s_{H},a_{H},c)-V^{\\mathrm{instuct}}(c),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where H $\\textstyle\\sum_{t=h}^{H}r(s_{t},a_{t},c)$ is a Monte-Carlo estimator of $Q(s_{h},a_{h},c)$ . The equality holds because the MDP formulation only provides rewards at the end of a rollout. Intuitively, if a rollout attains a high value of $A^{\\mathrm{instruct}}(s_{h},a_{h},c)$ , it means the value function $V^{\\mathrm{instruct}}$ is small. Therefore, this rollout represents a valuable experience of the agent accomplishing a difficult task, and thus should be prioritized, akin to ideas pertaining to prioritized experience [32] or level replay [11]. When training the actor with a buffer of historical off-policy data, we first perform a filtering step to identify the top- $\\cdot p$ datapoints with highest $A^{\\mathrm{instruct}}(s_{h},a_{h},c)$ . Then, we use it for AWR (Equation 4.1) with the doubly-robust advantage estimator (Equation 4.3). ", "page_idx": 5}, {"type": "text", "text": "Implementation details. Inspired by the findings in some recent works [5, 17] that modern deep learning architectures like transformers [41] are better trained with cross-entropy losses instead of mean-squared losses, we utilize a cross-entropy objective based on the Monte-Carlo estimate of the trajectory reward for training both of our value functions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(V^{\\mathrm{traj}})=-\\mathbb{E}_{\\nu}[r(s_{H},a_{H},c)\\log V^{\\mathrm{traj}}(c)+(1-r(s_{H},a_{H},c))\\log(1-V^{\\mathrm{traj}}(c))],}\\\\ &{\\mathcal{L}(V^{\\mathrm{step}})=-\\mathbb{E}_{\\nu}[r(s_{H},a_{H},c)\\log V^{\\mathrm{step}}(s_{h},a_{h},c)+(1-r(s_{H},a_{H},c))\\log(1-V^{\\mathrm{step}}(s_{h},a_{h},c))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/a57965bc64762d2eb4225ba60ce48b8684a87b0dd4e4d5e9292e7faa0064219f.jpg", "img_caption": ["Figure 5: Algorithm visualization. The two value function are first trained with original distribution of collected trajectories according to Equation (4.5) and Equation (4.6), then used to filter the trajectories for training the actor. We use the MLE loss (Maximum Likelihood Estimation loss) to train the actor. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Final algorithm. The final practical algorithm is shown in Figure 5. The instruction-level value function estimates the values of the trajectories, which is trained with loss shown in Equation (4.5). The step-level value function estimates the values of states, which is trained with loss shown in Equation (4.6). When training the actor, we first fliter out trajectories and states using the value functions as shown in Equation (4.4) and Equation (4.3), then train the actor with the MLE loss shown in Equation (4.2) on the filtered data. ", "page_idx": 6}, {"type": "text", "text": "5 Experimental Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The goal of our experiments is to evaluate the performance of DigiRL on challenging Android device control problems. Specifically, we are interested in understanding if DigiRL can produce agents that can effectively learn from autonomous interaction, while still being able to utilize offline data for learning. To this end, we perform a comparative analysis of DigiRL against several prior approaches, including state-of-the-art agents in Section 5.1. We also perform several ablation experiments to understand the necessity and sufficiency of various components of our approach in Section 5.2. ", "page_idx": 6}, {"type": "text", "text": "Baselines and comparisons. We compare DigiRL with: (a) state-of-the-art agents built around proprietary VLMs, with the use of several prompting and retrieval-style techniques; (b) running imitation learning on static human demonstrations with the same instruction distribution, and (c)a flitered BC approach [26]. For proprietary VLMs, we evaluate GPT-4V [24] and Gemini 1.5 Pro [7] both zero-shot and when augmented with carefully-designed prompts. For the zero-shot setting, we use the prompt from Yang et al. [47] and augment the observation with Set-of-Marks [55]. Set-ofMarks overlays a number for each interactable element over the screenshot, so that a VLM can directly output the number of the element to interact with in plain text instead of attempting to calculate pixel coordinates, which is typically significantly harder. We also compare with AppAgent [47], which first prompts the VLM to explore the environment, and appends the experience collected to the test-time prompt. We also compare with two state-of-the-art fine-tuning methods for Android device control: AutoUI (specifically AutoUI-Base [53]) and CogAgent [9]. AutoUI-Base uses an LM with 200M parameters, and a a vision encoder with 1.1B parameters. CogAgent has 11B parameters for its vision encoder and 7B for its LM. The supervised training corpus for both AutoUI-Base and CogAgent contains AitW, including the instruction set and the emulator configuration we use. ", "page_idx": 6}, {"type": "text", "text": "Base VLM and offilne dataset. Both Filtered BC and DigiRL use trained AutoUI-Base checkpoints with the image encoder frozen. The instruction and step-level value functions for DigiRL employ this same frozen image encoder. The visual features output from the encoder are concatenated with instruction features derived from RoBERTa [21]. A two-layer MLP is then used to predict the value function. In the offilne phase, the offilne dataset is collected by rolling out the initial AutoUI-Base supervised trained checkpoint as policy. For fair comparisons, we keep the number of offline data collected in the pure offline training roughly the same as the total number of data collected in the offline-to-online training. Due to the dynamic nature of the Internet-device eco-system, our offilne data was stale by the time we were able to run our offline-to-online experiments, and this presented additional challenge in offilne-to-online learning. In both General and Web Shopping subsets, offilne experiments make use of around 1500 trajectories while offline-to-online experiments start with around 500 offline trajectories and update with another 1000 online trajectories. In the offilne phase, DigiRL skips instruction-level flitering and instead trains the actor with all successful trajectories to make full use of the offline data. See a detailed breakdown of our dataset in Appendix A.1. ", "page_idx": 6}, {"type": "table", "img_path": "4XTvXMSZPO/tmp/15f7b34123bfa02d5c373e24e7df75b7019e8e36e316f0324f28007953be972e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/9a58e624913866b70ac10974bd921f58c14277f004aceeb6af79cbbae09adb35.jpg", "img_caption": ["Table 1: Main comparisons of different agents across various settings. Each offilne experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set. Correlation of our correlation and human judgements can be found in Figure 8. ", "Figure 6: Offline-to-online training curves for Filtered BC and DigiRL. Curves are smoothed with exponential weighting over the $\\mathbf{X}$ -axis. Left: AitW General. Right: AitW Web Shopping. Two runs for each model are started on two different dates with at least two days apart. Observe that DigiRL is able to improve faster with a fewer number of samples. Since the data collection frequency is the bottleneck, these performance trends directly reflect performance trends against wall-clock time as well. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.1 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our main results are summarized in Table 1 and Figure 6. We find that on both AitW General and AitW Web Shopping subsets, the agent trained via DigiRL significantly outperforms prior state-of-the-art methods based on prompting and retrieval (AppAgent $^+$ GPT-4V/Gemini $1.5\\;\\mathrm{Pro}\\!\\!\\!$ ) or training on static demonstrations (CogAgent and AutoUI), by a large margin with more than $49.5\\%$ absolute improvement (from $17.7\\%$ to $71.9\\%$ on the General subset and from $17.7\\%$ to $67.2\\%$ on the Web Shopping subset). Notably, this improvement from DigiRL is realized fully autonomously without making use of human supervision (e.g. manually labeled rollouts or hand-written verifiers). ", "page_idx": 7}, {"type": "text", "text": "Are inference-time prompting and retrieval techniques or supervised training enough for device control? Delving into Table 1, we observe that off-the-shelf proprietary VLMs, even when supplemented with the set-of-marks mechanism, do not attain satisfactory performance: both GPT-4V and Gemini 1.5 Pro achieve success rates under $20\\%$ . One possible cause could be the underrepresentation of Android device data in the pre-training data. Moreover, inference-time adaptation strategies such as AppAgent [47] show minimal improvement, with gains not exceeding $5\\%$ for either model. All this evidence suggests a limited scope for improvement without fine-tuning of some sort. As illustrated in Figure 7, the primary failures of these VLMs stem from hallucinatory reasoning that lead the VLMs to land on a relevant but wrong page. This suggests that while state-of-the-art VLMs excel at reasoning problems in code and math, their reliability in less-familiar domains, such as device control, remains inadequate. For example, for the instruction \u201cGo to newegg.com, search for alienware area 51, and select the first entry\u201d, a GPT-4V based agent erroneously searched \u201calien area 51 ebay\u201d in Google.com and decided that it had made progress towards the task (Figure 17). ", "page_idx": 7}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/11942bcd6a185c3718d0270f86fffff2cc87a08ba0e0f92981519d7a3e87aaa8.jpg", "img_caption": ["Figure 7: Failure modes for each approach on both the AiTW General and Web Shopping subsets. We found that the failure mode RL training is most effective at reducing compared to model supervised trained on human data is \u201cFail to recover from mistakes\u201d. A more fine-grained decomposition can be found in Appendix D. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Training on domain-specific human demonstrations, however, does boost performance, allowing the smaller, specialized VLM, AutoUI with 1.5 billion parameters, to match or surpass the larger, generalist VLMs like GPT-4V and Gemini $1.5\\;\\mathrm{Pro}$ . Nonetheless, this supervised imitation learning approach still fall short, with success rates on both subsets remaining below $20\\%$ . This shortcoming is not fundamentally addressed via enhancements in model scale or architecture, as evidenced by CogAgent [9], with 18 billion parameters still achieving performances below $40\\%$ success rate. As depicted in Figure 7, a predominant failure mode for these agents is an inability to rectify their own errors. An example trajectory that we observed is that for the instruction \u201cwhat\u2019s on the menu of In-n-Out\u201d, the agent accidentally activated the voice input button, and failed to quit that page until the step limit. In contrast, DigiRL is able to recover from the errors more efficiently( Appendix C.2). ", "page_idx": 8}, {"type": "text", "text": "Comparison of different RL approaches. In Table 1 and Figure 6, we present a comparative analysis of various autonomous approaches. Notably, both offilne and offilne-to-online configurations demonstrate that our RL approach, when augmented with a continuous stream of autonomous interaction data and reward feedback, substantially improves performance. This improvement is evident from an increase in the success rate from under $20\\%$ to over $40\\%$ , as the agent learns to adapt to stochastic and non-stationary device interfaces. Moreover, although the total sample sizes for offilne and offilne-to-online settings are equivalent, the top-performing offilne-to-online algorithm markedly surpasses its offilne counterpart ( $75\\%$ versus $62.8\\%$ on the General subset). This highlights the efficacy of autonomous environment interaction, and establishes the efficacy of DigiRL in learning from such uncurated, sub-optimal data. Lastly, DigiRL consistently outperforms the state-of-the-art alternative, Filtered BC, across both the General and Web Shopping subsets, improving from $61.5\\%$ to $71.9\\%$ and $57.8\\%$ to $61.4\\%$ , respectively, highlighting DigiRL\u2019s performance and efficiency. ", "page_idx": 8}, {"type": "text", "text": "5.2 Analysis and Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Failure modes analysis. We conduct an additional user study to annotate the failure modes for each agent as shown in Figure 7, and a more fine-grained breakdown can be found in Appendix D. At a high level, we classify the major failure modes of all agents into the following three categories: (1) Failure to recover from mistakes refers to the scenario where the agent made a mistake that led it to states from which it failed to quickly recover and resume the task, such as a wrong search page. (2) Getting stuck midway refers to the failure mode where the agent gets distracted on the right track to completing the instruction and as a result fails to accomplish the task. For example, failing to click on the right link or failing to search after typing the key words. (3) Arriving at wrong goal refers to the failure mode where the agent arrives at a wrong page and mistakenly thinks that it had completed the task. For e.g, the agent finds a macbook on costco.com instead of finding a macbook on ebay.com. ", "page_idx": 8}, {"type": "text", "text": "While all the types of failure modes benefti from offline and offline-to-online RL training as shown in Figure 7, the most consistent and significant reduction is probably for the failure mode of failing to recover from mistakes. This is because while pre-trained models, generating plausible future tokens, can get distracted by the dynamic nature of the environment and, as a result, encounter at never-before-seen states. With no clue of how to escape such states, these methods are unable to recover and fail to solve the task. In contrast, by training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training. ", "page_idx": 8}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/3dbc58db696ca64003d0abc890f18b119b0fa71ca18fd84eb12dff69d55ce815.jpg", "img_caption": ["Figure 8: Correlation between our autonomous evaluator and human judgements for all policy models on General and Web Shopping subsets. For repeated offilne and online runs, we report the correlation results for the run with the highest autonomous evaluation success rate. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Ablation study of each component in DigiRL. We conduct an ablation study on different components of DigiRL in Figure 9. We find that all the components used by our approach are necessary: (1) using cross-entropy for training the value functions boosts performance by around $12\\%$ (compare Ours and Ours w/ Regression); (2) using step-level advantages improves efficiency by $12\\%$ (comparing Ours and Ours w/o step-level advantage); (3) the use of automatic curriculum improves the speed of learning by around $25\\%$ (comparing Ours w/o step-level advantage and Filtered BC); (4) Ours outperforms vanilla AWR that does not employ a doubly-robust advantage estimator or curriculum. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we also observe no degradation in performance as a result of \u201chard-filtering\u201d, as show by nearly comparable performance of our approach and the best run of exponential filtering obtained via an extensive tuning of the temperature hyperparameter $\\tau$ in na\u00efve AWR (comparing Ours and Ours w/ vanilla AWR reweighting), despite simplicity of implementation in the hard filtering approach. Putting together, these choices result in a new state-of-the-art RL approach for device control. ", "page_idx": 9}, {"type": "text", "text": "Evaluation of our autonomous evaluator. In Figure 8, we present the findings from a user study aimed at assessing the accuracy of our autonomous evaluator. Our results indicate that ", "page_idx": 9}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/379c8bce1ad029e64dafc9acf9384103ebb6466073c55dd644900e29b652047c.jpg", "img_caption": ["Figure 9: Ablation study results on the AitW Web Shopping subset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "the success rates reported by our automatic evaluator are remarkably consistent with those assessed by human evaluators across almost all models, with differences less than $3\\%$ . Furthermore, we observed that evaluations on the Web Shopping subset are more precise compared to those on the General subset. This increased accuracy likely stems from the fact that tasks in the General subset are formulated in free-form language, which can introduce ambiguity, whereas the Web Shopping subset features a narrower range of language expressions, reducing potential variability. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel autonomous RL approach, DigiRL, for training in-the-wild, multimodal, device-control agents that establish a new state-of-the-art performance on a number of Android control tasks from Android-in-the-Wild dataset [31]. To achieve this, we first build a scalable and parallelizable Android environment with a robust VLM-based general-purpose evaluator that supports fast online data collection. We then develop a system for offline RL pre-training, followed by autonomous RL fine-tuning to learn via interaction, admist the stochasticity of the real-world Internet and device eco-system. Our agent achieves a $280\\%$ improvement over the previous state-of-the-art agents (from $17.7\\%$ to $68.2\\%$ in terms of task success rate), including AppAgent based on GPT-4V and Gemini $1.5\\;\\mathrm{Pro}$ , and supervised trained models such as AutoUI and CogAgent. ", "page_idx": 9}, {"type": "text", "text": "Due to computational limitations, and despite the fact that the parallel emulator and autonomous evaluator can be easily extended to complicated tasks, our agent is trained only with tasks from AitW instead of a all possible tasks on the device. Our design of the DigiRL algorithm aims for maximal implementation simplicity, so we hope that our approach to serve as a base algorithm for future research to build on, including algorithmic research as well as expanding the space of tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Yi Su, Izzedin Gur, Xinyang Geng, and Sandra Faust for feedback on an earlier version of this paper and for informative discussions. This work is supported by NSF IIS-2246811 and ONR N00014-21-1-2838, and Gemini $1.5\\,\\mathrm{Pro}$ credit donations for academic use and cloud resources from Google Cloud. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023. ", "page_idx": 10}, {"type": "text", "text": "[2] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Rapha\u00ebl Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem B\u0131y\u0131k, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023.   \n[3] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. ArXiv, abs/2310.05915, 2023. URL https: //api.semanticscholar.org/CorpusID:263829338.   \n[4] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, L\u00e9o Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024.   \n[5] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Ta\u00efga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, and Rishabh Agarwal. Stop regressing: Training value functions via classification for scalable deep rl, 2024.   \n[6] 2023 Gemini Team. Gemini: A family of highly capable multimodal models, 2024.   \n[7] 2024 Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.   \n[8] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability. NeurIPS, 2021.   \n[9] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.   \n[10] Peter C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap. A data-driven approach for learning to control computers, 2022.   \n[11] Minqi Jiang, Edward Grefenstette, and Tim Rockt\u00e4schel. Prioritized level replay. CoRR, abs/2010.03934, 2020. URL https://arxiv.org/abs/2010.03934.   \n[12] Yiding Jiang, J Zico Kolter, and Roberta Raileanu. On the importance of exploration for generalization in reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[13] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024.   \n[14] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, 2002. URL https://api. semanticscholar.org/CorpusID:31442909.   \n[15] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web, 2024.   \n[16] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.   \n[17] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline q-learning on diverse multi-task data both scales and generalizes, 2023.   \n[18] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent, 2024.   \n[19] Juyong Lee, Taywon Min, Minyong An, Changyeon Kim, and Kimin Lee. Benchmarking mobile device control agents across diverse configurations, 2024.   \n[20] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023.   \n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907. 11692.   \n[22] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets. CoRR, abs/2006.09359, 2020. URL https: //arxiv.org/abs/2006.09359.   \n[23] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik\u2019s cube with a robot hand, 2019.   \n[24] 2023 OpenAI Team. Gpt-4 technical report, 2023.   \n[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/ CorpusID:246426909.   \n[26] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.   \n[27] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024.   \n[28] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019. URL http://arxiv.org/abs/1910.00177.   \n[29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019.   \n[30] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master $16000+$ real-world apis, 2023.   \n[31] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088, 2023.   \n[32] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay, 2016.   \n[33] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023.   \n[34] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/ 1502.05477.   \n[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.   \n[36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation, 2018.   \n[37] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3135\u20133144. PMLR, 06\u201311 Aug 2017. URL https://proceedings.mlr.press/v70/shi17a.html.   \n[38] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning, 2023.   \n[39] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021.   \n[40] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. CoRR, abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.   \n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.   \n[42] Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning, 2022.   \n[43] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression, 2021.   \n[44] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024.   \n[45] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation, 2023.   \n[46] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback, 2023.   \n[47] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.   \n[48] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023.   \n[49] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms, 2023.   \n[50] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292, 2024.   \n[51] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024.   \n[52] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents, 2024.   \n[53] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents, 2023.   \n[54] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop multimodal internet agents. arXiv preprint arXiv:2404.09992, 2024.   \n[55] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded, 2024.   \n[56] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. URL https: //api.semanticscholar.org/CorpusID:260164780.   \n[57] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.   \n[58] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019. URL http://arxiv.org/abs/1909.08593. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Environment details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Post-processing of AitW ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Android in the Wild (AiTW) task set is a large-scale dataset for android device control, containing five subsets: GoogleApps, Install, Web Shopping, General, and Single, where we select the General and Web Shopping subsets. Single subset is not considered here because all tasks in Single can be completed within one step and thus this subset fails to examine the multi-step challenges that we are interested in this paper. Install and GoogleApps are not considered due to security reasons as those tasks require an active Google account and parallel emulations can flag security concerns. ", "page_idx": 14}, {"type": "text", "text": "General. The General set focuses on searching for information and basic application usage. For example, it contains searching for latest news in Chile, search for flights from NYC to Sydney, opening Gmail, etc. We use all 545 tasks in the training set for training and the first 96 tasks in the test set for testing due to computational and budget constraints. The maximum allowed number of steps for this subset is 10. Offilne data is collected by rolling our the initial AutoUI policy with tasks from the training set. The offline data used for the offline-to-online setting contains 608 trajectories while the offilne data used for the offilne setting contains 1552 trajectories. Some task examples are shown in Table 3. ", "page_idx": 14}, {"type": "table", "img_path": "4XTvXMSZPO/tmp/3caa22beb962a78e5592f3110ff44f23df76448982e3bac21bf6366af5fac459.jpg", "table_caption": ["Table 2: Examples of task descriptions in the AiTW General task set. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Web Shopping. The Web Shopping subset comprises search instructions on various shopping websites, like searching for razer blader on ebay. As some websites (e.g. Amazon) and operations (e.g. adding items to cart) frequently require captcha verifications, we post-process the Web Shopping subset to exclude such operations and websites and also make the task easy to evaluate for our autonomous evaluator. The resulting task set involves navigating through five websites (costco.com, bestbuy.com, target.com, walmart.com, newegg.com) and three basic operations (go to website, search in the website, and select items from the searched results). Our post-processed training set contains 438 tasks and our testing set contains 96 tasks. Example tasks after post-processing can be found in Table 3. The maximum allowed number of steps for this subset is 20. Offline data is collected by rolling our the initial AutoUI policy with tasks from the training set. The offline data used for the offline-to-online setting contains 528 trajectories while the offline data used for the offline setting contains 1296 trajectories. ", "page_idx": 14}, {"type": "table", "img_path": "4XTvXMSZPO/tmp/d83cad082961053be30e6dc4c9e6949ad2c65a2942211e123d9fa30a2e08e8cb.jpg", "table_caption": ["Table 3: Examples of task descriptions in the AiTW Webshopping task set. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "4XTvXMSZPO/tmp/e81e8e831db722a51498d39c69b2b9975e4d974d00229b0226879eeb117cb179.jpg", "table_caption": [], "table_footnote": ["Table 4: Average rollout length of the DigiRL agent compared to filtered BC. Darker green means shorter rollout length. On both AitW General and AitW Web Shopping test subsets, we find that DigiRL consistently produces shorter length rollouts than filtered BC. "], "page_idx": 15}, {"type": "text", "text": "B Other Quantitative Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Curriculum Learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "When running experiments on the AitW Web Shopping subset, we find solving easier tasks helps improve solving harder tasks, where the difficulty is identified in Table 3. By specifying the difficulty DigiRL-Run1 in Figure 6, we empirically show the success rates of each difficulty across the online learning process in Figure 12, we observe that a significant increase of success rate of tasks of difficulty 1 leads to increasing success rate of difficulty 2, and the same pattern for difficulty 2 and 3, demonstrating effective curriculum learning. ", "page_idx": 15}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/7471697b44663dafea8d6fcc5a38a92ac6421d1ae53ae416cdaafcb9155fdf1e.jpg", "img_caption": ["Figure 10: Left: Success rate under different difficulties for the AiTW Webshopping task set. Right: Success rate under different methods with different horizon length $(H\\in\\{10,20\\})$ on the AiTW Google Search task set. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Learning Method ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We ablate on the learning method, i.e. online learning or offline-to-online learning. We find that offline-to-online learning converges faster than online learning, and is not necessarily worse than online learning in terms of final performance, as shown in Figure 11. ", "page_idx": 15}, {"type": "text", "text": "B.3 Horizon Limit ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We investigate the horizon limit of filtered BC and DigiRL on the AitW General subset. As most tasks can be effectively solved within 10 steps, we specify two horizon limits: a sufficient horizont $H=10$ , and a redundant horizon $H=20$ . Results in Figure 12 show that a redundant horizon introduces significantly faster learning speed for both filtered BC and DigiRL, presumbaly because longer horizon means more opportunity to try in a single trajectory. In both horizon settings, we observe the DigiRL offers a significant speedup of around 100 trajectories over Filtered BC. ", "page_idx": 15}, {"type": "text", "text": "B.4 Trajectory Length ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We investigate the rollout length of DigiRL compared to flitered BC. Results in Table 4 demonstrate that DigiRL consistently achieves shorter average rollout lengths compared to flitered BC across both subsets. This observation holds true whether considering all rollouts for computing this correlation or only investigating this correlation on rollouts that eventually succeed. This indicates the capability of DigiRL to solve tasks in a more efficient and directed manner. Qualitative examples can be found in Figure 16. ", "page_idx": 15}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/7cd79e6da391cd8478bcab6bd04cc5905de0e9bb3a048c556f9fccf45e209951.jpg", "img_caption": ["Figure 11: Success rate with pure online learning or offline-to-online learning w.r.t. the number of online trajectories trained on the AitW General dataset. The starting points of curves in this figure look different from the main results figure because the starting points of the main results figure is smoothed at the average performance of the offline trajectories collected for the offline-to-online learning. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/35b892962af9aca5d5efc911f89796f40033c1366327b9accc873d83bd136a12.jpg", "img_caption": ["Figure 12: Success rate with different horizon length $\\langle H\\in\\{10,20\\}\\rangle$ )under different methods on the AiTW Google Search task set. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "C Qualitative Examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Random sample of trajectories for different agents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figures 13 and 14, we provide trajectories of DigiRL, AutoUI, and GPT-4V randomly sampled from our test set to offer a qualitative understanding of the agents\u2019 performance. As shown in these examples, DigiRLcan efficiently carry out in-the-wild device control tasks and less likely to get stuck or get to a wrong page compared to AutoUI and GPT-4V. ", "page_idx": 16}, {"type": "text", "text": "C.2 Error Recovery ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We observe that DigiRL is able to recover from its own mistakes. As shown in Figure 15, we find that DigiRL explores ways to get back to the original screen in order to perform a search. As a comparison, AutoUI fails to reset to the original screen and gets stuck at the diverged screen. Under the hood, we find DigiRL trying to maximize the state value, which usually induces it to reset to the original screen (that has a large value to success). ", "page_idx": 16}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/29aaa0960a20861ec3a826af94592f1f047f493e5d66f9a4112f0765f0ef89d5.jpg", "img_caption": ["Figure 13: Agents\u2019 trajectory on two randomly sampled tasks on the General split of AitW. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/63b426926bbdab83b3039de17937ebbd4f0836d00f1bb90348304db753473811.jpg", "img_caption": ["Figure 14: Agents\u2019 trajectory on two randomly sampled tasks on the WebShop split of AitW. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/71c50c26f8f41cacac3b4a3da488ecfaf8417f61bd315696f9f1c203de30519f.jpg", "img_caption": ["Figure 15: Error recovery cases. In bestbuy.com, we systematically find DigiRL able to recover from its own mistakes, while AutoUI fails to do so. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 Trajectory Length ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Qualitative example on the number of steps in trajectories of DigiRL and filtered BC are shown in Figure 16. We find consistent cases where DigiRL has shorter trajectory length than filtere BC. ", "page_idx": 19}, {"type": "text", "text": "C.4 Reasoning failure of GPT-4V ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The performance of GPT-4V failed on AiTW tasks predominantly due to not being able to carry out control actions as it plans on a high level, and then not being able to recover from these mistakes. Moreover, one of the main reasons why it is not able to recover from a mistake is that it might hallucinate and make itself believe that it is a wrong app or website. Indeed, GPT-4V constructs a plan of further actions when provided a task from either Web Shopping or General dataset of AiTW. Then, when it makes a misclick and fails to successfully proceed in an intermediate step, it might think that it actually solved that intermediate step and is in the correct app or website to execute further actions, causing the overall trajectory to fail. An example of this is provided in Figure 17. Here, we ask the model to search for an item in a webshopping website, in particular in \u201cnewegg.com\u201d. However, the model fails to proceed to that website due to not being able to precisely locating the search button. Then, instead of trying to go to that website again, the model thinks it is already in that webshopping website, and mistakes the search bar of Google with the search bar of \u201cnewegg.com\u201d. Hence, the rest of the trajectory also fails. Another slightly different phenomenon is illustrated in Figure 18. Here, the model is able to proceed to the correct website and search for an item, but this time it fails to tap on the search button on the website and clicks to an advertisement instead. Consequently, the model fools itself to think it successfully searched the item, and scrolls the page hoping to find that item, but it cannot do so because in reality it views the results of the advertisement. The primary reason of these failures is the challenge of grounding the control actions in GUI interfaces to realize the intermediary goals laid out by GPT-4V model\u2019s thoughts. As an example, we provide an illustration of trying to set up an alarm task in Figure 19. Here, in the last frame, it fails to execute the precise movements in the necessary amount of rounds to correctly set up the alarm to the desired time, and in the last frame we see that the action taken does not align with the thought process of the model. ", "page_idx": 19}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/5d6a90430836f584e214ca13aa49bae3aa776b17cf4d4fdac2397dd4581774c3.jpg", "img_caption": ["Figure 16: Examples where DigiRL has shorter trajectory length than online filtered BC. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Fine-grained failure modes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Figure 20, we present a more fine-grained breakdown for all six failure modes provided in the user study. Those failure modes include: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Failure to recover from mistakes refers to the scenario where the agent made a mistake that led it to states from which it failed to quickly recover and resume the task, such as a wrong google search page.   \n\u2022 Failure to click on the right link or failure to click refers to the failure mode where the agent either fails to locate the element that it tries to click on and keeps clicking on the nearby region, or fails to start typing in the string when it is supposed to do so.   \n\u2022 Failure to take reasonable attempts at all refers to the failure mode where there is no clear reason that the agent fails to complete the task and does not seem to be on the right track throughout the trajectory.   \n\u2022 Quit or press HOME early refers to the failure mode where the agent decided to finish the task or press HOME to start over before the task is actually finished.   \n\u2022 Stops at wrong but relevant page refers to the failure mode where the agent arrives at a wrong page and mistakenly thinks that it had completed the task. For example, the agent finds a macbook on costco.com while the instruction asked it to find a macbook on ebay.com.   \n\u2022 Technical issues refer to the failure mode that either the task is impossible (e.g. the tasks asks to open Amazon app but this app is not installed) or the agent is temporarily blocked from a certain website due to frequent visits. ", "page_idx": 20}, {"type": "text", "text": "The translation between fine-grained failure modes and coarse-grained failure modes is presented in Table 5. ", "page_idx": 20}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/5e14fbc14755f9b8164b02a49117965509476cd1be09e314142f210026708b20.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 17: Failure of GPT-4V, with its thoughts and link-based actions given. A typical cause of failure is that it cannot tap on the correct \u201csearch\u201d button after entering a query and mistakenly tapped onto the $\\mathbf{\\hat{\\omega}_{X}}^{\\bullet,}$ symbol in the search bar as the \u201csearch\u201d button. Here the goal is: Go to newegg.com, search for \u201calienware area $51^{\\,\\cdot}$ and select the first entry. As seen in red emboldened actions, it fails to press search button and deletes the query instead. Also, as seen in red highlighted parts in thoughts, it thinks it is in \u201cnewegg.com\u201d website even though it is not. ", "page_idx": 21}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/d5f8693dcad0c9c9697767bcfd1e6128b002a0b9bfdc1cb41b216e42a003e758.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 18: Failure of GPT-4V, with its thoughts and link-based actions given. This time the reason for failure is misclick on the wrong button. The task is \u201cGo to costco.com, search for \u201cacer predator\u201d, and select the first entry\u201d. Notice that up until the fourth frame in this Figure, the trajectory goes correct. But then it clicks on the generic advertisements on the Costco.com website, and it cannot recover back. It continues to scroll the page and takes wrong actions thereafter. ", "page_idx": 21}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/6ccd92a64d8881abc3398a236c91489e0e21ec1b260d873c2c5b6a15eec13f30.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 19: Failure of GPT-4V, with an example task on the AiTW general test set. The task is \u201cSet an alarm for $4\\mathrm{pm}^{\\circ}$ . Here, GPT-4V is able to successfully navigate to the clock app, and the alarm settings of that app. However, it cannot take the correct precise actions to set the alarm quickly enough, and it fails due to maximum rounds reached. In the last round, notice that the action of tap(1) contradict with its own thought process of setting minutes to ${}^{\\bullet}00^{\\circ}$ . ", "page_idx": 22}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/c4a11c388f7d81506ec26ad1334242d991df7a69f1e36284e5d35eb1a541d9a9.jpg", "img_caption": ["Figure 20: Failure modes decomposition for each policy model for both General and Web Shopping subsets. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "4XTvXMSZPO/tmp/4cd6b57e99e4af8672912bbbae2cbd4c27b6bab41394afa885b4337e4e8c5b37.jpg", "table_caption": [], "table_footnote": ["Table 5: Examples of task descriptions in the AiTW Webshopping task set. "], "page_idx": 22}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/0685fd4f5ef22131e9bc43bde7951c6f167ad8fa19c60f57037674992a85244c.jpg", "img_caption": ["Figure 21: Multi-machine parallel emulator execution. The host machine is equipped with GPU accelerators and the worker machines are equipped only with CPUs. The policy update is executed on the worker machine and the trajectory collections are executed distributedly on the worker machines and aggregated by the host machine. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Experiment machines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our main experiments are conducted on VM instances from Google Cloud Platform. Each VM instance comes with 1x Tesla T4 GPU and 16x Intel(R) Xeon(R) CPU. ", "page_idx": 23}, {"type": "text", "text": "F Setup for parallel environment ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Running multiple emulators in parallel can be challenging due to the inefficiency in thread synchronization and frequent fault propagation when one emulator runs into an unknown error. To address this challenge, we set up a server-client system where all emulator processes are running in independent server processes. Each emulator process communicates with the main training process through different UIAutomotor servers. The main training process sends high-level instructions to UIAutomotor servers (such as reset and step), while UIAutomotor servers parse high-level instructions into low-level UI commands (such as typing a character and tapping at a coordinate) and such UI commands are executed by the emulator processes. When an exception is thrown in the emulator, the UIAutomotor examines if it is recoverable (e.g. an UI command takes too long to execute in the emulator) and reset the emulator process if it is not. When an exception is thrown in the UIAutomotor server, the main training process stops and resets the UIAutomotor server to ensure data correctness. ", "page_idx": 23}, {"type": "text", "text": "This design can easily be scaled up to a multi-machine setting. As illustrated in Figure 21, one host machine equipped with GPU accelerator has a local copy of the current policy $\\pi_{t}$ , and distributes the policy to all worker machines equipped with only one GPU and multiple CPUs. Each worker machine will then collect trajectories of different tasks using $\\pi_{t}$ . After all collection processes are synchronized, the host machine gathers all the trajectories together to update the policy to $\\pi_{t+1}$ . This process keeps iterating until the policy converges. ", "page_idx": 23}, {"type": "text", "text": "Speedup of emulation parallel. The performance boost with respect to the number of worker machines is nearly linear, as demonstrated in Figure 22 (right), where we conduct experiments that examine the scaling performance of our parallel emulator. Our distributed emulator that runs emulations across multiple servers can reliably collect data with up to 64 parallel emulators on 128 CPUs with near-linear speedup. In contrast, a naive baseline that runs all parallel emulations on the same server achieves much inferior performance (0.74 compared to 1.74 trajs/min using 64 CPUs). ", "page_idx": 23}, {"type": "text", "text": "G Autonomous evaluator details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our autonomous evaluator gives a reward to each observation we get. The observation is composed of the current screenshot of device and the task. The evaluator gives a reward of 1 if the screenshot shows a completion of the task, and will terminate the POMDP as a result result. ", "page_idx": 23}, {"type": "text", "text": "The optimized prompt is shown in Figure 23 and Figure 24 for General and Web Shopping subsets respectively. ", "page_idx": 23}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/a7b5540da060d414a9bd07977c1030e1d709d7ca4f64a117f978a050c31ace89.jpg", "img_caption": ["Figure 22: Emulation speed w.r.t number of CPUs used. The upper bound can only achieved when there is no communication and error handling cost. Our design of distributed emulator can significantly improve the efficiency of emulation compaared to the vanilla method of running all emulations over the same instance. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Zero-shot Baseline Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Figure 25 shows the prompt that we used for testing the Set-of-Marks performance for GPT-4V and Gemini $1.5\\;\\mathrm{Pro}$ . This prompt is directly taken from Yang et al. [47]. ", "page_idx": 24}, {"type": "text", "text": "I Hyperparameters ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Hyperparameters for both Filtered BC and DigiRL are carefully tuned through binary search on the training set of General and Web Shopping subsets. The final choice of hyperparameters for both methods can be found in Table 6. As shown in the table, the only hyperparameters introduced by DigiRL are supervised training hyperparameters for the value function and instruction value function (including number of iterations and learning rate) and GAE $\\lambda$ . ", "page_idx": 24}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "You're an expert in evaluating whether the Screenshot successfully completes the Task ", "page_idx": 25}, {"type": "text", "text": "=====Examples===== ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Screenshot: (train_1.png?   \nTask: Open the settings.   \nQ: What should I expect to see on the screenshot if I've opened the settings?   \nA: I should expect to see I'm in the settings app. The screenshot shows the home screen of a mobile device, with various app icons displayed, including the settings app icon, but the setings app is not opened.   \nStatus: failure Screenshot: {train_2.png?   \nTask: Find hotels in washington dc   \nQ: What should I expect to see on the screenshot if I've searched for hotels in Washington, DC?   \nA: I should expect to see I'm in a search results page for hotels in Washington, DC. The screenshot shows a Google search page with the search field populated with the query \"hotels in washington dc\" and a list of suggested searches related to hotels in Washington, DC, but it does not show any search results for hotels in Washington, DC. Status: failure   \nScreenshot: (train_3.png?   \nTask: What's a good restaurant in Portland?   \nQ: What should I expect to see on the screenshot if I've searched for a good restaurant in Portland?   \nA: I should expect to see I'm in a search results page for a good restaurant in Portland. The screenshot shows a   \nGoogle search page with a search input field for\"good restaurant in portland\" and a map results preview showing   \nbusiness locations near Portland, like \"Li Pigeon\",\"Portland City Gril\", and \"Higgins\"   \nStatus: success ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": ".. (more cases) ", "page_idx": 25}, {"type": "text", "text": "$\\scriptstyle======$ Your Turn====: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Screenshot: {test.png}   \nTask: {task_this_traj}   \nRespond in this format:   \nQ: What should I expect to see on the screenshot if I've <repeat the task>?   \nA: I should expect to see <first expectation, then what's in the given screenshot.>   \nStatus: success or failure (don't return anything else)   \nStart with \"Q:\" ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Response ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Q: What should I expect to see on the screenshot if I've searched for the price of a 12' ladder at Home Depot? A: I should expect to see the price of a 12'ladder at Home Depot; the screenshot shows a search result page for the p rice of a 12' ladder, with some product advertisements showing prices from Home Depot. Status: success ", "page_idx": 25}, {"type": "text", "text": "Image Sources ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/dced59df2c36e6946350f62c20f5145f873e82130c81a6d0ad0b24bbb74bcb18.jpg", "img_caption": ["Figure 23: Prompt for our autonomous evaluator for tasks in AitW General subset. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "You're an expert in evaluating whether the Screenshot successfully completes the Task ", "page_idx": 26}, {"type": "text", "text": "=====Examples==== ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Screenshot: {train_1.png?   \nTask: Go to bestbuy.com   \nQ: What should I expect to see on the screenshot if I've gone to bestbuy.com?   \nA: I should expect to see I'm in the Best Buy website, which usually shows the best buy logo with some featured prod ucts and categories. The screenshot shows I'm searching for \"bestbuy.com\" in the Google search (with some search s uggestions) instead of being in the Best Buy website.   \nStatus: failure ", "page_idx": 26}, {"type": "text", "text": "Screenshot: (train_2.png? ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Task: Go to ebay.com, search for \"corsair k70\"   \nQ: What should I expect to see on the screenshot if I've gone to ebay.com AND searched for \"corsair k70\"? A: I should expect to see I'm in the eBay website and search results for \"corsair k70\". The screenshot shows I'm in the eBay ebay website with some search suggestions for \"corsair k70\", but it does not show search results of the product, which usually includes price and the product details.   \nStatus: failure ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Screenshot: {train_3.png}   \nTask: Go to ebay.com, search for \"lenovo thinkpad\"   \nQ: What should I expect to see on the screenshot if Ive gone to ebay.com AND searched for \"lenovo thinkpad\"? A: I should expect to see I'm in the eBay website and search results for \"lenovo thinkpad\". The screenshot shows I'm in the eBay website and have several search results for \"lenovo thinkpad\".   \nStatus: success ", "page_idx": 26}, {"type": "text", "text": "... (more cases) ", "page_idx": 26}, {"type": "text", "text": "=====Your Turn====: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Screenshot: {test.png}   \nTask: {task_this_traj}   \nRespond in this format:   \nQ: What should I expect to see on the screenshot if I've <repeat the task>? A: I should expect to see <first expectation, then what's in the given screenshot.> Status: success or failure (don't return anything else)   \nStart with \"Q:\". ", "page_idx": 26}, {"type": "text", "text": "Response ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Q: What should I expect to see on the screenshot if I've searched for the price of a 12' ladder at Home Depot? A: I should expect to see the price of a $12^{\\prime}$ ladder at Home Depot; the screenshot shows a search result page for the p rice of a 12' ladder, with some product advertisements showing prices from Home Depot. Status: success ", "page_idx": 26}, {"type": "text", "text": "Image Sources ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "4XTvXMSZPO/tmp/988aaf41e7e63e68f7ad32516a9a57d84d93def8de28e4cc9e5d005467eb3ff7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 24: Prompt for our autonomous evaluator for tasks in AitW Web Shopping subset. ", "page_idx": 26}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\"You are an agent that is trained to perform some basic tasks on a smartphone. You will be given a \\nsmartphone screenshot. The interactive UI elements on the screenshot are labeled with numeric tags starting from 1. The nnumeric tag of each interactive element is located in the center of the element $\\scriptstyle\\mathrm{\\left|n\\right|nYou}$ can call the following functions to control the smartphone $\\backslash\\mathfrak{n}\\backslash\\mathfrak{n}\\mathtt{l}$ . tap(element: int)nThis function is used to tap an UI element shown on the smartphone screen.In\\\"element\" is a numeric tag assigned to an UI element shown on the smartphone screen. $\\backslash\\mathtt{n A}$ simple use case can be tap(5), which taps the UI element labeled with the number. ${\\mathfrak{z}}.{\\mathfrak{n}}{\\mathfrak{n}}2$ . text(text_input: str)\\nThis function is used to insert text input in an input field/box. text_input is the string you want to insert and must (nbe wrapped with double quotation marks. A simple use case can be text(\"Hello, world!\\\"), which inserts the string n\\\"Hello, world!\" into the input area on the smartphone screen. This function is usually callable when you see a keyboard nshowing in the lower half of the screen. $|\\mathfrak{n}\\backslash\\mathfrak{n}3.$ long-press(element: int)\\nThis function is used to long press an UI element shown on the smartphone screen.n\\\"element\\\" is a numeric tag assigned to an UI element shown on the smartphone screen. $\\backslash\\mathtt{n A}$ simple use case can be long-press(5), which long presses the UI element labeled with the number $5.|\\mathfrak{n}|\\mathfrak{n}4$ . swipe(element: int, direction: str, dist: str)nThis function is used to swipe an UI element shown on the smartphone screen, usually a scroll view or a slide bar.(n\\\"element\\\" is a numeric tag assigned to an UI element shown on the smartphone screen. \\\"direction\\\" is a string that \\nrepresents one of the four directions: up, down, left, right. \\\"direction\\\" must be wrapped with double quotation \\nmarks. \\\"dist\\\" determines the distance of the swipe and can be one of the three options: short, medium, long. You should \\nchoose the appropriate distance option according to your need $\\backslash\\mathfrak{n}\\mathtt{A}$ simple use case can be swipe(21, \\\"up\\\", \\\"medium\\\"), which swipes up the UI element labeled with the number 21 for a \\nmedium distance $|\\mathfrak{n}\\backslash\\mathfrak{n}5.$ \u00b7 gridO\\nYou should call this function when you find the element you want to interact with is not labeled with a numeric tag and \\nother elements with numeric tags cannot help with the task. The function will bring up a grid overlay to divide the nsmartphone screen into small areas and this will give you more freedom to choose any part of the screen to tap, long npress, or swipe. ", "page_idx": 27}, {"type": "text", "text": "The task you need to complete is to How much does a 2 bedroom apartment rent for in Denver?. ", "page_idx": 27}, {"type": "text", "text": "Your past actions to proceed with this task are summarized as follows: None ", "page_idx": 27}, {"type": "text", "text": "Now, given the documentation and the following labeled screenshot, you need to think and call the function needed   \nto proceed with the task. Your output should include three parts in the given format:   \nObservation: <Describe what you observe in the image>   \nThought: <To complete the given task, what is the next step I should do>   \nAction: <The function call with the correct parameters to proceed with the task. When you are certain that the task   \nis successfully done and the goal is reached as of the current observation, you should output FINISH. You cannot   \noutput anything else except a function call or FINISH \\nin this field.>   \nSummary: <Summarize your past actions along with your latest action in one or two sentences. Do not include the   \nnumeric \\ntag in your summary>\\nYou can only take one action at a time, so please directly call the function.\" ", "page_idx": 27}, {"type": "text", "text": "Figure 25: Set-of-Marks prompting. The boldened inputs can be changed according to our goal. The task changes for every different task. The past actions change as we take actions (it is None now since this is the prompt for the first round). ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction explicitly state the contributions of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 27}, {"type": "table", "img_path": "4XTvXMSZPO/tmp/ddf9aa2c3f9bb570caafd185e9bddf059cf251e476f2a4912397ef3fd489e28c.jpg", "table_caption": ["Table 6: Hyperparameters for All Experiments "], "table_footnote": ["Table 7: Hyperparameters for DigiRL and Filtered BC on both General and Web Shopping subset of AitW.. "], "page_idx": 28}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Limitations are discussed in the last section of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not provide theoretical results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All loss functions and implementation details are provided in Section 4. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We are still actively cleaning the code and make the environment more accessible to a broader audience. Once we are done with that, we will open-source the code along with the release of the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Dataset details are provided in Appendix A.1 and hyperparameters are provided in Appendix I. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Repeated experiments are carried out with their means and standard deviations reported in Table 1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: This information is provided in Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeuIPS code of Etics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The positive societal impacts are discussed in the Introduction while the negative societal impacts are discussed in Section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The capability of the model that we will be releasing is limited to simple tasks in Android in the Wild dataset, and therefore does not have a high risk for misuse. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have properly cited the assets that we are using. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This submission does not include new assets. New assets including opensourced code, model checkpoints, and model trajectories will be released with documentation when we release the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This research does not involve crowdsourcing or human subjects. Annotations of trajectories in Figure 7 and Figure 8 are carried out by authors alone. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This research does not involve crowdsourcing or human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]