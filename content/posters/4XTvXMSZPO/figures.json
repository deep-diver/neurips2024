[{"figure_path": "4XTvXMSZPO/figures/figures_1_1.jpg", "caption": "Figure 1: DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing its performance through online RL and autonomous performance evaluations.", "description": "This figure provides a high-level overview of the DigiRL approach.  It starts by describing the use of a pre-trained Vision-Language Model (VLM) as the foundation. The first stage involves offline reinforcement learning (RL) to fine-tune the VLM using existing trajectory data. This helps the model learn initial goal-oriented behavior. Then, the second stage introduces online RL where the agent interacts with real-world GUIs. The agent's performance is continuously improved by the online RL process, guided by an automated evaluation system that provides rewards based on the agent's performance.", "section": "1 Introduction"}, {"figure_path": "4XTvXMSZPO/figures/figures_2_1.jpg", "caption": "Figure 2: Qualitative comparison between DigiRL and other approaches. AutoUI trained from static human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal (searched \"logitech g933bestbuy.com logitech g933\" in Google instead of bestbuy.com). In contrast, DigiRL can recover from such states and complete complex instruction as requested.", "description": "This figure compares the performance of DigiRL, AutoUI, and GPT-4V on two example tasks.  AutoUI, trained with static demonstrations, frequently gets stuck in unexpected situations.  GPT-4V sometimes pursues the wrong goal, demonstrating a lack of robust reasoning and action selection. In contrast, DigiRL successfully completes both tasks, showcasing its ability to recover from errors and handle unexpected situations.", "section": "Introduction"}, {"figure_path": "4XTvXMSZPO/figures/figures_3_1.jpg", "caption": "Figure 3: Environment details. Top: actions space and dynamics of the environment. Bottom: examples of the read-world non-stationarity and dynamism of the environment.", "description": "This figure illustrates the environment used for training the digital agents in the DigiRL approach. The top part shows the action space (the actions available to the agent, such as clicking, sliding, typing, and using home, back, or enter buttons) and the dynamics of the environment (how the agent interacts with the real-world environment, a model of the agent, and an open-ended evaluator). The bottom part shows examples of real-world non-stationarity and dynamism, highlighting the challenges encountered such as non-stationary websites (websites that change frequently), pop-ups, ads, unpredictable order of elements on a screen, loading times, and identity checks. These elements contribute to the stochasticity and non-stationarity of the environment which DigiRL aims to address.", "section": "3 Problem Setup and Preliminaries"}, {"figure_path": "4XTvXMSZPO/figures/figures_4_1.jpg", "caption": "Figure 4: Performance of our approach (DigiRL) in different training modes on the Webshop subset. When utilizing a stale checkpoint, i.e., \"frozen\" (black+blue curve) performance generally begins to degrade as time evolves, whereas autonomous online training (black+red curve) via DigiRL allows us to retain performance despite non-stationarity and stochasticity.", "description": "This figure compares the performance of DigiRL in two different training modes on the Webshop subset of the Android in the Wild (AitW) dataset.  The \"Frozen (Online)\" line shows the performance of a model trained using data from June 1-3 and then tested on new data from June 7-11 without further training.  The \"Learning (Online)\" line depicts the performance of a model that continuously updates itself using the newer data. The graph shows that the frozen model's performance degrades over time while the model that continuously learns maintains its performance, highlighting the importance of continuous online adaptation for dealing with real-world non-stationarity.", "section": "Setup for reliable and scalable online RL"}, {"figure_path": "4XTvXMSZPO/figures/figures_6_1.jpg", "caption": "Figure 5: Algorithm visualization. The two value function are first trained with original distribution of collected trajectories according to Equation (4.5) and Equation (4.6), then used to filter the trajectories for training the actor. We use the MLE loss (Maximum Likelihood Estimation loss) to train the actor.", "description": "This figure illustrates the DigiRL algorithm's workflow.  It starts with two value functions, instruction-level and step-level, trained using maximum likelihood estimation (MLE) loss on the collected trajectories.  These functions then filter the trajectories, retaining only those deemed most informative. Finally, an actor is trained using MLE loss on this filtered data, leading to a refined agent.", "section": "4 DigiRL: Autonomous RL for Building a Strong Device-Control Agent"}, {"figure_path": "4XTvXMSZPO/figures/figures_7_1.jpg", "caption": "Figure 6: Offline-to-online training curves for Filtered BC and DigiRL. Curves are smoothed with exponential weighting over the x-axis. Left: AitW General. Right: AitW Web Shopping. Two runs for each model are started on two different dates with at least two days apart. Observe that DigiRL is able to improve faster with a fewer number of samples. Since the data collection frequency is the bottleneck, these performance trends directly reflect performance trends against wall-clock time as well.", "description": "This figure shows the learning curves for Filtered Behavior Cloning (Filtered BC) and DigiRL, comparing their success rates over the number of trajectories.  Two runs for each method are included, performed on different dates to account for the dynamic nature of the data. DigiRL demonstrates faster improvement with fewer samples, and this speed advantage is also reflected in wall-clock time due to the data collection bottleneck.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/figures/figures_8_1.jpg", "caption": "Figure 7: Failure modes for each approach on both the AiTW General and Web Shopping subsets. We found that the failure mode RL training is most effective at reducing compared to model supervised trained on human data is \"Fail to recover from mistakes\". A more fine-grained decomposition can be found in Appendix D.", "description": "This figure shows a comparison of failure modes across different methods for device control tasks on two subsets of the Android-in-the-Wild (AiTW) dataset: General and Web Shopping.  The three main failure modes are categorized: inability to recover from mistakes, getting stuck mid-task, and reaching the wrong goal. The bar chart visually represents the proportion of each failure mode for each method, illustrating DigiRL's superior performance in reducing failure rates compared to other methods.", "section": "5.2 Analysis and Ablations"}, {"figure_path": "4XTvXMSZPO/figures/figures_9_1.jpg", "caption": "Figure 8: Correlation between our autonomous evaluator and human judgements for all policy models on General and Web Shopping subsets. For repeated offline and online runs, we report the correlation results for the run with the highest autonomous evaluation success rate.", "description": "This figure displays bar charts illustrating the correlation between the success rates determined by the autonomous evaluator and human judgements for different policy models (Set-of-Marks GPT-4V, Set-of-Marks Gemini-1.5-Pro, AppAgent GPT-4V, AppAgent Gemini-1.5-Pro, AutoUI, CogAgent, Filtered BC Offline, DigiRL Offline, Filtered BC Online, DigiRL Online).  The results are shown separately for the General and Web Shopping subsets of the AiTW dataset.  The higher the bars, the stronger the agreement between human and automated evaluation.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/figures/figures_9_2.jpg", "caption": "Figure 9: Ablation study results on the AitW Web Shopping subset.", "description": "This figure shows the ablation study results on the AitW Web Shopping subset. It compares the performance of DigiRL with several variants, including removing the regression, removing the step-level advantage, using vanilla AWR, and using AWR with reweighting.  The x-axis represents the number of trajectories used for training, and the y-axis represents the success rate. The results demonstrate that all components of DigiRL contribute to its superior performance.  Specifically, using a cross-entropy loss for value functions and incorporating an automatic curriculum significantly improves the learning efficiency.", "section": "5.2 Analysis and Ablations"}, {"figure_path": "4XTvXMSZPO/figures/figures_15_1.jpg", "caption": "Figure 12: Success rate under different difficulties for the AiTW Webshopping task set. Right: Success rate under different methods with different horizon length (H \u2208 {10,20}) on the AiTW Google Search task set.", "description": "The left plot shows the success rate for tasks of different difficulty levels in the Web Shopping subset of the Android-in-the-Wild (AiTW) dataset.  The difficulty is determined by the complexity of the task instructions (see Table 3).  The plot illustrates the learning process for the DigiRL model, showing how success rates on easier tasks improve performance on more difficult ones. The right plot compares the learning curves of DigiRL and Filtered Behavior Cloning (Filtered BC) for the Google Search task, varying the maximum number of steps (H) allowed to complete each task (horizon length).  The plot demonstrates that DigiRL generally achieves higher success rates than Filtered BC and that increasing the horizon (from 10 to 20) can potentially improve the model's learning performance.  The success rate of GPT-4V is also included as a baseline for comparison.", "section": "B Other Quantitative Experiments"}, {"figure_path": "4XTvXMSZPO/figures/figures_16_1.jpg", "caption": "Figure 11: Success rate with pure online learning or offline-to-online learning w.r.t. the number of online trajectories trained on the AitW General dataset. The starting points of curves in this figure look different from the main results figure because the starting points of the main results figure is smoothed at the average performance of the offline trajectories collected for the offline-to-online learning.", "description": "This figure compares the success rate of two training methods: pure online learning and offline-to-online learning, on the AitW General dataset. The offline-to-online approach starts with some pre-trained data from offline RL, and then fine-tunes the model with online data. The pure online approach trains the model only on online data. The x-axis represents the number of online trajectories used for training and the y-axis represents the success rate. The figure shows that offline-to-online learning converges faster, achieving a higher success rate with fewer trajectories, though the final success rates of both methods are comparable.", "section": "5.2 Analysis and Ablations"}, {"figure_path": "4XTvXMSZPO/figures/figures_16_2.jpg", "caption": "Figure 6: Offline-to-online training curves for Filtered BC and DigiRL. Curves are smoothed with exponential weighting over the x-axis. Left: AitW General. Right: AitW Web Shopping. Two runs for each model are started on two different dates with at least two days apart. Observe that DigiRL is able to improve faster with a fewer number of samples. Since the data collection frequency is the bottleneck, these performance trends directly reflect performance trends against wall-clock time as well.", "description": "This figure compares the offline-to-online learning curves of DigiRL and Filtered BC on two subsets of the Android-in-the-Wild (AiTW) dataset: General and Web Shopping.  The x-axis represents the number of trajectories used for training, while the y-axis shows the success rate.  The curves are smoothed to highlight trends.  DigiRL demonstrates faster improvement with fewer samples than Filtered BC, likely due to its more efficient learning strategy and ability to adapt to the changing nature of the environment. The data collection frequency is the limiting factor, so the learning curves reflect the wall-clock training time.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/figures/figures_17_1.jpg", "caption": "Figure 13: Agents\u2019 trajectory on two randomly sampled tasks on the General split of AitW.", "description": "This figure shows the qualitative comparison of DigiRL, AutoUI, and GPT-4V on two randomly sampled tasks from the General split of the Android-in-the-Wild (AitW) dataset.  It visually demonstrates the differences in how each agent approaches and completes the tasks. DigiRL showcases a more robust and successful approach compared to AutoUI and GPT-4V, which encounter issues like getting stuck or arriving at the wrong goal.", "section": "5 Experimental Evaluation"}, {"figure_path": "4XTvXMSZPO/figures/figures_18_1.jpg", "caption": "Figure 13: Agents\u2019 trajectory on two randomly sampled tasks on the General split of AitW.", "description": "This figure shows example trajectories of DigiRL, AutoUI, and GPT-4V on two randomly selected tasks from the General subset of the Android-in-the-Wild (AiTW) dataset.  The figure qualitatively demonstrates the differences in the agents' abilities to perform device control tasks. DigiRL showcases efficient and effective task completion, while AutoUI and GPT-4V encounter difficulties such as getting stuck or reaching an incorrect destination.", "section": "C Qualitative Examples"}, {"figure_path": "4XTvXMSZPO/figures/figures_19_1.jpg", "caption": "Figure 15: Agents\u2019 trajectory on two randomly sampled tasks on the General split of AitW.", "description": "This figure shows two examples of agent's trajectories on the General split of the Android-in-the-Wild (AiTW) dataset.  The top example shows the trajectory for the task \"What are the new products by Samsung?\", and the bottom example shows the trajectory for the task \"Show me some nice wallpapers for my tablet\".  Each row represents the trajectory of a different agent: DigiRL, AutoUI, and GPT-4V. The screenshots in each row illustrate the sequence of actions taken by the agent.  The figure highlights the differences in how each agent approaches the task, demonstrating the superior ability of DigiRL to complete these tasks without getting stuck or making significant errors compared to AutoUI and GPT-4V. ", "section": "5 Experimental Evaluation"}, {"figure_path": "4XTvXMSZPO/figures/figures_20_1.jpg", "caption": "Figure 16: Examples where DigiRL has shorter trajectory length than online filtered BC.", "description": "This figure provides a visual comparison of the trajectory lengths (number of steps) between DigiRL and filtered BC for two sample tasks from the Android in the Wild (AiTW) dataset. It shows that DigiRL consistently requires fewer steps to complete the tasks compared to filtered BC, highlighting the efficiency improvement achieved by DigiRL.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/figures/figures_21_1.jpg", "caption": "Figure 1: DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing its performance through online RL and autonomous performance evaluations.", "description": "This figure illustrates the DigiRL architecture, a two-stage training process for a Vision-Language Model (VLM)-based agent.  Stage 1 involves offline reinforcement learning (RL) to fine-tune the pre-trained VLM using existing trajectory data, which helps the agent learn to exhibit goal-oriented behaviors. In Stage 2, offline-to-online RL is employed, where the agent interacts with real-world graphical user interfaces (GUIs).  The agent's performance is continuously improved through online RL using feedback from an autonomous evaluation system. This system assesses the agent's actions and provides reward signals based on whether the tasks were successfully completed.", "section": "1 Introduction"}, {"figure_path": "4XTvXMSZPO/figures/figures_21_2.jpg", "caption": "Figure 1: DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing its performance through online RL and autonomous performance evaluations.", "description": "This figure presents an overview of the DigiRL approach.  It begins with a pre-trained vision-language model (VLM) that is further fine-tuned using offline reinforcement learning (RL) on existing trajectory data.  This offline stage aims to establish basic goal-oriented behavior.  The second stage involves online RL where the agent interacts with real-world graphical user interfaces (GUIs), continuously learning and improving its performance through autonomous evaluation of its actions.  The figure visually depicts the two-stage process, highlighting the use of offline and online RL, the integration of a VLM-based evaluator, and the iterative refinement of the agent's performance.", "section": "1 Introduction"}, {"figure_path": "4XTvXMSZPO/figures/figures_22_1.jpg", "caption": "Figure 13: Agents\u2019 trajectory on two randomly sampled tasks on the General split of AitW.", "description": "This figure shows two example tasks from the General subset of the Android-in-the-Wild (AiTW) dataset.  For each task, it displays the sequences of screenshots showing the agent's actions for three different approaches: DigiRL, AutoUI, and GPT-4V.  The figure visually demonstrates the differences in how these agents approach and complete (or fail to complete) the tasks, highlighting DigiRL's superior performance and ability to handle complex instructions and recover from mistakes, unlike the other agents.", "section": "5 Experimental Evaluation"}, {"figure_path": "4XTvXMSZPO/figures/figures_22_2.jpg", "caption": "Figure 8: Correlation between our autonomous evaluator and human judgements for all policy models on General and Web Shopping subsets. For repeated offline and online runs, we report the correlation results for the run with the highest autonomous evaluation success rate.", "description": "This figure presents the correlation between the results from the autonomous evaluator and human evaluations across various policy models for both the General and Web Shopping subsets of the dataset.  The plot shows how well the automated assessment aligns with human judgement on the success rate of different agents.  Repeated offline and online experiments were conducted, and the correlation for the run with the best evaluation score is reported.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/figures/figures_23_1.jpg", "caption": "Figure 21: Multi-machine parallel emulator execution. The host machine is equipped with GPU accelerators and the worker machines are equipped only with CPUs. The policy update is executed on the worker machine and the trajectory collections are executed distributedly on the worker machines and aggregated by the host machine.", "description": "This figure illustrates the parallel computing architecture used for training the DigiRL agent. A host machine with GPUs handles the policy updates, while multiple worker machines with CPUs concurrently run Android emulators.  Each worker machine independently collects trajectories, which are then aggregated by the host machine for efficient model training. This distributed approach scales training to a larger number of emulators and speeds up the overall process.", "section": "Setup for parallel environment"}, {"figure_path": "4XTvXMSZPO/figures/figures_24_1.jpg", "caption": "Figure 22: Emulation speed w.r.t number of CPUs used. The upper bound can only achieved when there is no communication and error handling cost. Our design of distributed emulator can significantly improve the efficiency of emulation compaared to the vanilla method of running all emulations over the same instance.", "description": "This figure shows the emulation speed comparison between the vanilla emulator and the distributed emulator with respect to the number of CPUs used. The upper bound represents the ideal scenario without communication and error handling overhead. The results demonstrate that the distributed emulator design significantly improves the emulation efficiency compared to the vanilla approach, especially as the number of CPUs increases.", "section": "Setup for parallel environment"}, {"figure_path": "4XTvXMSZPO/figures/figures_25_1.jpg", "caption": "Figure 2: Qualitative comparison between DigiRL and other approaches. AutoUI trained from static human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal (searched \"logitech g933bestbuy.com logitech g933\" in Google instead of bestbuy.com). In contrast, DigiRL can recover from such states and complete complex instruction as requested.", "description": "This figure compares the performance of DigiRL with AutoUI and GPT-4V on two example tasks.  AutoUI, trained with static demonstrations, struggles with out-of-distribution scenarios, frequently getting stuck. GPT-4V, despite its strong capabilities, sometimes focuses on the wrong goal, as seen in the example task where it incorrectly searches on Google instead of BestBuy. In contrast, DigiRL demonstrates its robustness by successfully completing the tasks, showcasing its ability to adapt to real-world stochasticity and recover from mistakes.", "section": "Qualitative comparison"}, {"figure_path": "4XTvXMSZPO/figures/figures_26_1.jpg", "caption": "Figure 2: Qualitative comparison between DigiRL and other approaches. AutoUI trained from static human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal (searched \"logitech g933bestbuy.com logitech g933\" in Google instead of bestbuy.com). In contrast, DigiRL can recover from such states and complete complex instruction as requested.", "description": "This figure qualitatively compares DigiRL's performance against AutoUI and GPT-4V on two example tasks.  AutoUI, trained with static demonstrations, frequently gets stuck in unexpected situations (out-of-distribution states). GPT-4V, while capable of abstract reasoning, often fails to translate this into correct actions, taking the user down an incorrect path. In contrast, DigiRL successfully completes both tasks and demonstrates its ability to recover from errors and handle real-world stochasticity.", "section": "Qualitative comparison"}]