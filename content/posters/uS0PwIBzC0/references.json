{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022", "reason": "This paper introduces LoRA, a foundational parameter-efficient fine-tuning method that the current paper builds upon and improves."}, {"fullname_first_author": "Shih-Yang Liu", "paper_title": "DoRA: Weight-decomposed low-rank adaptation", "publication_date": "2024", "reason": "This paper introduces DoRA, another significant parameter-efficient fine-tuning method which is directly compared against in the current paper"}, {"fullname_first_author": "Weiyang Liu", "paper_title": "Parameter-efficient orthogonal finetuning via butterfly factorization", "publication_date": "2024", "reason": "This paper introduces BOFT, a competing parameter-efficient fine-tuning method that is directly compared against in the current paper."}, {"fullname_first_author": "Dawid Jan Kopiczko", "paper_title": "ELORA: Efficient low-rank adaptation with random matrices", "publication_date": "2024", "reason": "This paper introduces VeRA, another competing parameter-efficient fine-tuning method that is directly compared against in the current paper."}, {"fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "publication_date": "2019", "reason": "This paper provides early background on parameter-efficient transfer learning, a concept that the current paper expands upon."}]}