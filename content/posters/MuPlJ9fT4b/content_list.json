[{"type": "text", "text": "Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wuyang Chen\u2217 Jialin Song\u2217 Pu Ren Simon Fraser University Simon Fraser University Lawrence Berkeley National Laboratory ", "page_idx": 0}, {"type": "text", "text": "Shashank Subramanian Dmitriy Morozov Lawrence Berkeley National Laboratory Lawrence Berkeley National Laboratory ", "page_idx": 0}, {"type": "text", "text": "Michael W. Mahoney   \nInternational Computer Science Institute   \nLawrence Berkeley National Laboratory University of California, Berkeley ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions, and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/data_efficient_nopt. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in machine learning methodology have shown promise in solving partial differential equations (PDEs) [24, 67, 38, 45, 44, 50, 39, 25]. A significant development in this area is the concept of operator learning for PDEs. This approach differs from traditional neural network methods, which are restricted to fixed-dimension input and output, since neural operators focus on learning mappings between function spaces [38, 45, 44]. Like other neural network methods, neural operators are recognized to be universal approximators for any continuous operator [46, 38], enabling them to approximate any physical operator, including solution operators for various parametric PDE families. A solution operator is defined as a function that maps physical inputs to output solutions. Previous work has shown that in simple settings, neural operators can effectively capture complex, multi-scale dynamic processes [45, 46, 74, 73]. ", "page_idx": 0}, {"type": "text", "text": "However, neural operators tend to suffer from a problem common to other deep networks, namely the need for enormous quantities of data. Limited availability of data is common in science and engineering. High-fidelity numerical simulations are computationally costly or even infeasible for many applications [71]. For example, an extreme-scale simulation of magnitude 7.0 earthquake at frequencies up to $10\\,\\mathrm{Hz}$ in San Francisco requires 3600 Summit GPU nodes and 42.7 hour [53]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Motivated by this data-efficiency challenge, recent works, particularly in natural language processing (NLP) and computer vision (CV), have focused on unsupervised (or self-supervised) pretraining2 to reduce the cost of collecting or generating labeled data. Such pretrained models have been shown to be highly data-efficient in downstream fine-tuning [7, 29, 8], and they can even become few-shot learners without any downstream data [3]. In CV, researchers collect large amounts of natural images without any manual labels, and then pretrain visual encoders with proxy tasks, such as Noise-Contrastive Estimation (NCE) [59], masked reconstruction [28], rotation and jigsaw prediction [58, 20]. In NLP, people typically pretrain models via next-word prediction or masked tokens [3, 12]. ", "page_idx": 1}, {"type": "text", "text": "However, unsupervised pretraining is still largely underexplored in Scientific Machine Learning (SciML). Therefore, our core question is: How can we design unsupervised pretraining for operator learning to reduce the data simulation costs? ", "page_idx": 1}, {"type": "text", "text": "In this work, we resort to unsupervised pretraining for neural operator learning to achieve data efficiency in SciML. We overview our framework in Figure 1. First, we define unlabeled data for PDEs, which avoids heavy computation costs for simulating PDE solutions. We propose two physics-inspired reconstruction-based proxy tasks, and we pretrain neural operators on unlabeled PDE data. We demonstrate that with unsupervised pretraining, our neural operators not only improve counterparts trained with more simulated data, but also they outperform off-the-shelf pretrained checkpoints from other popular domains (such as CV) that are ready for fine-tuning. Then, to further improve the data efficiency during out-ofdistribution (OOD) inference, we design a similarity-based method that learns in-context examples [3, 49, 80, 81, 47]. This approach introduces zero overhead during training: one just maintains the standard training pipeline, and it can be seamlessly plugged in for OOD inference, without further fine-tuning. In more detail, we summarize our main contributions: ", "page_idx": 1}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/8c6dfaa76b3d6b8e4d905351c5aa645020d24f45ec3cd5bb3054acf21ce84608.jpg", "img_caption": ["Figure 1: Overview of our framework for data-efficient neural operator learning (with our contributions highlighted in red). Stage 1: Unsupervised pretraining only on unlabeled PDE data. Stage 2: Fine-tuning with reduced simulation costs of PDE data. Stage 3: Test-time in-context examples can improve the neural operator\u2019s out-of-distribution performance, without additional training costs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1. We introduce unlabeled PDE data and unsupervised pretraining for data-efficient neural operator learning. We show that our method can achieve better performance than models trained with more simulated PDE solutions, or fine-tuned from public checkpoints pretrained on other benchmarks, demonstrating the importance of unsupervised pretraining on domain-specific PDE data. 2. We propose a similarity-based method to improve the OOD generalization of neural operators, which is flexible and can scale up to a large number of unseen in-context examples (\u201cdemos\u201d). 3. We provide detailed empirical evaluations on both diverse PDE benchmarks and also several realworld scenarios, demonstrating that we can achieve both strong forward modeling performance and significant savings in PDE simulations. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Machine Learning for Scientific Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There has been a long history of using learning-based methods to model physical scientific phenomena [41, 42, 6, 5]. A representative line of work is so-called physics-informed neural networks (PINNs) [67, 87, 19, 18, 69], which try to incorporate physics in neural networks by including the differential form of the PDE as an additional physics loss regularization term. However, this paradigm is confined to specific PDE scenarios (e.g., fixed PDE coefficients), instead of being more physics-agnostic. Moreover, recent work has highlighted several fundamental \u201cissues\u201d with PINNbased methods [40, 15]. On the other hand, operator learning methods, including Fourier Neural Operators [45, 44, 38] and Deep Operator Network [50], have achieved progress in approximating the solution operators of PDEs. Although these data-driven approaches show promise in learning PDE solutions, they (like many neural network-based methods) rely on vast quantities of high-fidelity labeled data. For operator learning, such data are usually computationally expensive to simulate [67, 2, 84]. More recently, people have tried to generate synthetic PDE solutions to train SciML models [27]. In contrast, our method works on unlabeled PDE data. This approach is distinct from the generation of synthetic PDE data, and it could be further combined as a semi-supervised learning strategy. ", "page_idx": 2}, {"type": "text", "text": "2.2 Unsupervised Pretraining and Foundation Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unsupervised (or self-supervised) pretraining is a key method in CV and NLP to achieve meaningful representations [7], data-efficient fine-tuning [31], and foundation models [1]. In CV, contrastive learning learns meaningful features by distinguishing between similar (positive) and different (negative) samples [59, 76, 7, 56, 29]. Masked Autoencoder (MAE) [28] uses a reconstructive approach where parts of the input are masked and the model learns to predict masked parts. In NLP, among the most prominent works are large language models (LLMs) such as GPT [3, 64, 65] and BERT [12, 66], which leverage token predictions for pretraining. Similar directions also show progress in SciML. For example, [2] and [54] propose to create augmented views in the solution space via Lie Symmetries; [73] study the scaling behavior of supervised pretraining and OOD generalization, charting directions for foundational models for SciML; [43] target learning astronomical foundation models with crossmodal contrastive learning; and [52] build large task-agnostic models with a broad understanding of common physical behavior to serve as foundation models for SciML. ", "page_idx": 2}, {"type": "text", "text": "2.3 In-Context Learning (ICL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In-context learning (ICL) is a promising paradigm that helps deep networks generalize to unseen domains with a few in-context examples. Early works in CV seek to learn feature-level correspondence between the target and a few \u201cshots,\u201d such that models can generalize to open-set unseen objects [17, 83]. In NLP, people find LLMs are naturally few-shot learners [3], and thus tuning or optimizing prompts becomes extremely important to improve the in-context learning performance of LLMs [85, 72]. More recently, within SciML, a different operator learning strategy, termed \u201cin-context operator learning,\u201d has been proposed [80, 81, 47]. During both training and inference, the neural operator is asked to make predictions by explicitly leveraging a predefined number of so-called \u201cdemo\u201d examples (pairs of physical parameters and simulated solutions). This approach provides a balance between model generalization and addressing data scarcity in the scenario of OOD testing. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce our framework (outlined in Figure 1). We propose first to pretrain the model with unsupervised pretraining (Sec. 3.1), which will contribute to the data efficiency and reduced PDE simulation costs during standard training of neural operators. When we move to OOD scenarios during inference, we test our models with in-context examples (Sec. 3.2) to avoid further fine-tuning costs. ", "page_idx": 2}, {"type": "text", "text": "3.1 Unsupervised Pretraining ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The core idea in unsupervised (or self-supervised) pretraining is to train a neural network with properly designed proxy tasks. These proxy tasks do not require labeled data, but they are designed to be highly related to the supervised learning objectives of interest. While popular in CV and NLP, unsupervised pretraining on unlabeled data has never been explored in PDE operator learning in SciML. This is due to two unresolved questions: 1) What kinds of unlabeled data can we use to train neural operators? 2) How to design proxy tasks for PDE data? We address these questions below. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1.1 Unlabeled PDE Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "How to Define Unlabeled PDE Data? In general, when a neural operator is trained on PDE datasets [45, 74, 73], it learns to map inputs (physical parameters, coordinates, forcing functions, initial conditions, etc.) to PDE solutions. Therefore, given a set of PDE data (collected via simulations or observations), its unlabeled version is defined as the one without PDE solutions. Our unlabeled PDE data is a broader concept of related inputs in modeling PDE systems. Let us consider the second-order linear differential equation as a general example. It is formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{i,j=1}^{n}a_{i j}(x)u_{x_{i}x_{j}}+\\sum_{i=1}^{n}b_{i}(x)u_{x_{i}}+c(x)u=f(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x\\in\\mathbb{R}^{n}$ represents physical space that varies in different systems (e.g. $n=3$ for 2D timedependent PDEs); the coefficients (or physical parameters) $a_{i j},b_{i},c$ are known from the physical process; $u$ is the target solution; and $f$ denotes an external forcing function [57]. We can consider two situations where solutions are unavailable: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Time-independent equations: following [45, 73], our unlabeled PDE data include physical parameters $(a_{i j},b_{i},c)$ , forcing functions $(f)$ , and coordinates (grids of the discrete physical space). \u2022 Time-dependent equations (e.g., forecasting problems [74, 52]): without simulating the temporal dynamics, our unlabeled PDE data only include initial snapshot $u_{0}(x)$ that defines PDE systems. Note that collecting snapshots with temporal dynamics in large-scale scenes is more complex than capturing individual snapshots. For example, weather forecasting [30] and smoke dispersion [14] require continuous monitoring and multiple sensors, whereas single measurements are simpler and less resource-intensive. Long-term data collection often involves extensive networks and processing, unlike one-time measurements. ", "page_idx": 3}, {"type": "text", "text": "For concrete examples of different PDEs and unlabeled data that we will study, see Appendix A.   \nThere are two main reasons for pretraining only on unlabeled PDE data, as discussed below. ", "page_idx": 3}, {"type": "text", "text": "Cheap Generation of Unlabeled PDE Data. One critical reason that leads to expensive computational costs when collecting PDE data is the time marching scheme [23, 22] in numerical simulation. However, only generating unlabeled PDE data and snapshots without temporal dynamics will be much cheaper than simulating solutions (Table 4), making our unsupervised pretraining highly feasible in practice. Our pretraining strategy will be very data-efficient, and can avoid the heavy computational cost of simulating complex time-dependent equations for massive high-fidelity labeled solutions. ", "page_idx": 3}, {"type": "text", "text": "Benefits of Pretraining on Unlabeled PDE Data. Beyond the cheap generation, pretraining on unlabeled PDE data has the following benefits. First, regularization against overfitting. Unsupervised pretraining can strongly regularize the model towards better generalization. Second, faster convergence. Pretraining on unlabeled PDE data will provide neural operators with domain-adapted initializations and can accelerate training speed. Third, meaningful representations. Pretraining on unlabeled PDE data can help models extract useful representations for subsequent operator learning. We defer our results and experimental details in Figure 4 in Sec. 4.1. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Proxy Tasks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To illustrate our general approach of constructing proxy tasks, we choose two variants of reconstruction as our core proxy tasks. In particular, we will input unlabeled PDE data to our neural operators, and after a decoder network, we will force the output to be close to the input. We consider two perturbation variants (or augmented views). They are inspired by real-world settings when people collect scientific data, and they are important invariances we need to introduce to SciML models. ", "page_idx": 3}, {"type": "text", "text": "Masked Autoencoder. Masked autoencoders (MAEs) have been shown to be scalable selfsupervised learners [28]. The method is conceptually simple: remove a portion of the input data, and learn to predict the removed content. These methods enable training in NLP and CV of generalizable models containing over one hundred billion parameters [12, 3]. Here, we investigate the potential of MAE for scientific modeling. Unsupervised Pretraining ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Motivation: PDE dynamics are invariant to sparse sensing of the full field scientific data. It is very common [4, 16, 32] that scientific data need to be collected from sparse sensors, and that people need to reconstruct or generate data for domains without sensors. We enforce our model to learn sensor invariance via random masking, and we extract the invariant features over distorted views of the same unlabeled PDE data. The invariance to sparse sensing of scientific data will facilitate the robustness of the representations from MAE. ", "page_idx": 4}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/dc6a383810246432b8e6957411cc34ffa9b1d72e18fd0636a8ab952c654f4bab.jpg", "img_caption": ["Figure 2: Overview: unsupervised pretraining via MAE and super-resolution. During pre-training, in the input unlabeled PDE data, a random subset (e.g., $70\\%$ ) of spatial locations are masked, followed by a Gaussian blur. After the encoder and decoder, the full set of input is required to be reconstructed. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Therefore, we consider MAE as a proxy task. Specifically, our MAE is a straightforward autoencoding approach that reconstructs the original signal, given its partial observation. Like all autoencoders, our approach has an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the original signal from the latent representation. We randomly sample masks according to a certain masking ratio (i.e., the ratio of removed areas), and the values of masked areas of unlabeled PDE data are set to zero. Our loss function computes the mean squared error (MSE) between the reconstructed and original input. We compute the loss only on masked areas; if no mask is applied (i.e., a vanilla autoencoder), the MSE loss will be applied to all spatial areas. ", "page_idx": 4}, {"type": "text", "text": "Super-resolution. Super-resolution (SR) techniques have emerged as powerful tools for enhancing data resolution, improving the overall quality and fidelity of data representation, and retrieving fine-scale structures. SR is a task that involves recovering fine-scale data from corresponding coarsegrained data. It is also a popular task on PDE learning [9, 68], which is to train SciML models to preserve the inherent physical properties of scientific data. ", "page_idx": 4}, {"type": "text", "text": "Motivation: Numerical solutions of PDEs are expected to exhibit invariance to filtering blur or different resolutions of inputs [37, 79]. For instance, in turbulence simulations, the traditional numerical methods always fail to model the expected physical phenomenon with low-resolution meshes due to substantial numerical errors. SR has emerged as a powerful tool for subgrid modeling of PDE dynamics, especially helping to capture the critical patterns of turbulence [51, 37]. Given a specific input distribution, after ftiting the SR objective, neural operators are expected to preserve the inherent physical properties and exhibit invariance to filtering blur. ", "page_idx": 4}, {"type": "text", "text": "Therefore, we introduce SR as another proxy task. Our objective shares the same motivation as recent SciML works for SR [68]. Specifically, we enforce the model to learn invariant features of unlabeled PDE data that are immune to resolution and blur. Blurry snapshots often occur when the resolution is too low to accurately represent the details in the original content. To do so, we apply a Gaussian fliter to blur the unlabeled PDE data, and the autoencoder will reconstruct the high-resolution input with fine-grained details. Instead of applying a fixed blurring, we randomly sample the variance of the Gaussian filter from a certain range as augmentations. ", "page_idx": 4}, {"type": "text", "text": "3.1.3 PDEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After pretraining on unlabeled PDE data, we fine-tune neural operators on simulated solutions of PDEs. We study two time-independent PDEs (Poisson, Helmholtz) and two time-dependent PDEs (Reaction-Diffusion, Navier-Stokes). We include details of these PDEs in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3.1.4 Model Architectures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider two popular architectures for fair comparisons with previous works. These are encoderdecoder architectures designed to reconstruct the original input given partial observations, where the encoder maps observed unlabeled PDE data to a latent space, and the decoder reconstructs the original conditions. We include visualizations of these architectures in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "Fourier Neural Operator. Fourier Neural Operator (FNO) targets learning PDE data in the Fourier space. The original model backbone (encoder) employs Fourier transform and learns lower Fourier modes with linear transforms. The FNO backbone outputs features back to the spatial domain (i.e., the embeddings are on the pixel level). We refer readers to the original paper for details [45, 46]. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Pretraining: We build the decoder to be identical to the encoder (except for the input/output dimension). Unlabeled PDE data are randomly masked at the pixel level. \u2022 Fine-tuning: After the pretraining, we discard the decoder, and we follow the original design to append two fully-connected layers (with ReLU activations) to predict final spatial-wise solutions. ", "page_idx": 5}, {"type": "text", "text": "Transformer. Transformers, which mainly employ self-attention and linear transform blocks, have shown promise in both NLP and CV [78, 13]. Different from FNO, which directly operates on grids, transformers tokenize and group grids into patches, i.e., each tokenized patch embeds a local neighborhood of subgrids. We follow the 3D transformer architecture of Video-MAE [77, 52]. Our encoder embeds patches by a linear projection with added positional embeddings (just as in a standard ViT), and it then processes the resulting set via a series of Transformer blocks. For the transformer, the unlabeled PDE data are randomly masked at the patch level. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Pretraining: For the transformer encoder, we only apply it on the subset of tokens that are visible (i.e., unmasked patches), and masked patches are removed. This allows us to train encoders efficiently. The input to the MAE decoder is the full set of tokens consisting of (i) encoded visible patches, and (ii) the mask token. The mask token is a shared and learned vector that indicates the presence of a missing patch to be predicted. We add positional embeddings to all tokens in this full set. Without this, mask tokens would have no information about their location in the input. Following [28, 77], we adopt an asymmetric design where the decoder is more lightweight (shallower and narrower) than the encoder.   \n\u2022 Fine-tuning: After the pretraining, the decoder is preserved during fine-tuning, since we need to reconstruct the tokenized patches back to the input. ", "page_idx": 5}, {"type": "text", "text": "3.2 Similarity-based Mining of In-Context Examples ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Out-of-distribution (OOD) generalization is a critical technical challenge, not only in SciML but also across multiple domains in AI for science [84]. To improve the OOD generalizability of neural operators and to reduce the extra effort of downstream fine-tuning, the following inference paradigm has been proposed: given a query input, the model is also provided with a few supporting examples (dubbed \u201cdemos\u201d), together with their ground-truth solutions, to make the final prediction. This approach enables the \u201copen-set\u201d generalization of the model to make predictions on unseen samples. Originally, in the literature on few-shot learning [82, 83, 55, 70, 35, 48, 61], people developed delicate architectures to find a correspondence between the input and the supporting examples. The purpose of this extra architecture/training design is twofold: first, find similarities between the target input and supporting examples; and second, aggregate labels of supporting examples for the final predictions. Recent ICL works [80, 81, 47] on learning PDE data also adopt this strategy, with transformers and cross-attention layers. ", "page_idx": 5}, {"type": "text", "text": "However, the ICL (in-context learning) of LLMs (large language models) enables a different strategy. The pretraining is still standard and simple (next/masked token prediction), without additional training costs. During inference, LLMs can auto-regressively take any number of few-shot examples, finding similarities between tokens in few-shot examples and those in the target query (via self-attention), and then generate responses by aggregating embeddings of tokens in few-shot examples. This ICL strategy used in LLM is highly scalable and training-efficient. ", "page_idx": 5}, {"type": "text", "text": "Motivated by this, we propose to leverage in-context examples via two steps (Algorithm 1). ", "page_idx": 5}, {"type": "text", "text": "Similarity by Prediction. We find spatial-wise and temporal-wise similar demos by calculating their distance in the output space. That means, for two input locations over the spatial and temporal domains, if we find their outputs of the trained neural operator similar, then we treat them as similar samples. Following [80, 81], we assume demos share the same distribution of physical parameters with the query. ", "page_idx": 5}, {"type": "text", "text": "Aggregation. For each spatial-temporal location of the query, after finding its similar samples in demos, we aggregate and average their solutions as the prediction. ", "page_idx": 5}, {"type": "text", "text": "1 Data resolution: $\\mathbf{X}$ -axis $(W)$ , y-axis $(H)$ , output temporal steps $(T)$ , output channel dimensions for the solution $(C_{o u t})$ .   \n2 Input: Query input $(x)$ . Paired unlabeled PDE data $(X)$ and solutions $(Y\\in\\mathbb{R}^{J\\times H\\times W\\times T\\times C_{o u t}})$ as $_J$ demos. Trained Neural Operator Model $\\mathcal{M}$ . TopK $(k)$ demo solutions to aggregate.   \n3 $\\hat{y}=\\mathcal{M}(x)$ $\\triangleright$ Shape: $H\\times W\\times T\\times C_{o u t}$   \n4 $\\hat{Y}=\\mathcal{M}(X)$ \u25b7Shape: $J\\times H\\times W\\times T\\times C_{o u t}$   \n5 $\\hat{\\delta}=\\hat{y}.\\mathrm{reshape}(-1,1,C_{o u t})-\\hat{Y}.\\mathrm{reshape}(1,-1,C_{o u t}$ ) \u25b7Query-Demo Distance. Shape: $H\\times W\\times T\\times(J\\cdot H\\cdot W\\cdot T)\\times C_{o u t}$   \n6 $\\hat{\\delta}=\\mathrm{absolute}(\\hat{\\delta}).\\mathrm{sum}(-1)$   \n7 index $=\\operatorname{argsort}(\\hat{\\delta},-1)[:,:,:,:k]\\triangleright\\mathsf{S}$ patial-wise and temporal-wise selection of demos similar to the query.   \n8 $\\hat{y}_{i c l}=\\mathrm{take\\_along\\_dim}(Y.\\mathrm{reshape}(-1,C_{o u t}).$ , index) $\\triangleright$ Spatial-wise and temporal-wise aggregation of solutions from similar demos. Shape: $H\\times W\\times T\\times C_{o u t}\\times k$ . ", "page_idx": 6}, {"type": "text", "text": "9 Return: $\\hat{y}_{i c l}.\\mathrm{mean}(-1)$ ", "page_idx": 6}, {"type": "text", "text": "4 Empirical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To illustrate the beneftis of our approach, we perform empirical evaluations on both PDE benchmarks and real-world observations. Most importantly, our unsupervised pretraining (on unlabeled PDE data, followed by fine-tuning) outperforms neural operators trained from scratch, while requiring fewer PDE simulations (Sec. 4.1 and Sec. 4.2). Moreover, our in-context examples can help the model generalize better to OOD cases (Sec. 4.3). In our experiments, we trained models three times with different random seeds for statistical significance. For more experimental details, see Appendix B. We also include ablation studies about pretraining hyperparameters in Appendix J. For visualizations of our pretraining, see Appendix K. ", "page_idx": 6}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/6c2c54a5a60818c0b7ce9edc142d2df6e6c5934e8072277aaf88a7c83c6138d3.jpg", "img_caption": ["Figure 3: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (a), Helmholtz (b), Reaction-Diffusion (c), and Navier-Stokes (d and e, with relative errors at different unrolled steps shown on f). \u201crandom init.\u201d: models are trained from scratch with random initialization. \u201cvision pretrained (SSv2)\u201d: fine-tuning from the publicly available checkpoint for Video-MAE (pretrained on computer vision dataset SSV2 [21] for video understanding). Savings of the number of simulated PDE data (when \u201crandom init.\u201d achieves the best test error) are shown in red. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Unsupervised Pretraining Enables Data-Efficient Operator Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Data Efficiency. We first demonstrate that, by leveraging unsupervised pretraining, neural operators can achieve improved errors with less simulated data. In Figure 3, on each PDE, compared with directly training from scratch (\u201crandom init.\u201d), pretraining on unlabeled PDE data can help neural operators achieve better performance, which can further help reduce the amount of simulated data. Specifically, in our experiments, when we target achieving the best test error of the baseline (\u201crandom init.\u201d), our method can save $5\\times10^{2}\\sim8\\times\\bar{1}0^{5}$ simulated solutions across diverse PDE systems. ", "page_idx": 7}, {"type": "text", "text": "Among all PDEs, we find that Helmholtz (Figure 3 (b)) is the most challenging. Both two curves failed to improve the error until we increased the number of simulated data points to over 1024. Meanwhile, the generalization gaps remain high (Figure 12 (b)), indicating low training errors. We suspect that learning on the Helmholtz equation may be exhibiting the grokking issue [63, 86], where the network quickly memorizes the training data, but the improvement in generalizability is delayed. ", "page_idx": 7}, {"type": "text", "text": "Unsupervised Pretraining Outperforms Off-the-Shelf Pretrained Checkpoints. Pretraining on unlabeled PDE data is not the only way to save simulation costs. As pretraining is widely adopted in CV, pretrained checkpoints on vision data become publicly available and are ready for fine-tuning. We choose to compare with Video-MAE [77] for state-of-the-art video understanding pretrained on SSV2 [21]. As shown in Figure 3 (e), vision-pretrained Video-MAE can only outperform the random initialization with high volumes of simulated data, while its performance suffers when fine-tuned with limited simulations. In contrast, our unsupervised pretraining on unlabeled PDE data can save a significant amount of simulated data. ", "page_idx": 7}, {"type": "text", "text": "As errors during testing may quickly accumulate with further timestamps, we also report results with more unrolled steps. We use checkpoints trained with the largest amount of simulated data from above for this study. As shown in Figure 3 (f), at each rollout step, our unsupervised pretraining achieves much better performance. Similarly, vision-pretrained Video-MAE is eventually outperformed by the random initialization at the long rollout step. ", "page_idx": 7}, {"type": "text", "text": "Benefits of Pretraining on Unlabeled PDE Data. Pretraining on unlabeled PDE data is beneficial beyond achieving better performance with reduced simulations. First, when training on extremely low volumes of data, neural operators tend to overfti, resulting in poor generalization. In Figure 4-left, pretraining on unlabeled PDE data can reduce the generalization gap (testing error \u2212training error). We further show that better generalization gaps persist across all PDEs we studied (see Figure 12). Second, pretraining on unlabeled PDE data can lead to faster convergence during fine-tuning. Unlike standard random initializations from Gaussian distributions, pretraining on unlabeled PDE data will provide neural operators with domain-adapted initializations and facilitate a much faster convergence rate, as shown in Figure 4-middle3. Third, unsupervised pretraining can also help models extract useful representations for subsequent operator learning. From Figure 4-right, we find that even with pre-extracted features (i.e., fixed encoder and only fine-tuned decoder), our neural operators can still outperform the baselines where both the encoder and decoder are updated during fine-tuning. ", "page_idx": 7}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/85c9de10d638705ba0f17ec2ea98669428a4213c46a52bde0f5e2d4fca5b5479.jpg", "img_caption": ["Figure 4: Benefits of our unsupervised pretraining. Reduced overfitting (left): our method consistently leads to smaller generalization gaps (test error \u2212training error) across all PDEs we studied (Fig. 12). Faster convergence (middle): our unsupervised pretraining can accelerate model convergence than both random initialization and vision-pretrained checkpoint. Meaningful representations (right): fine-tuning Video-MAE with fixed encoder (pretrained on unlabeled PDE data, red line) can extract meaningful features and outperform the baseline and the vision-pretrained model (both encoder and decoder are updated during fine-tuning). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 More Comprehensive Experiments on Real-World Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now move to a broader range of benchmarks. We will study real-world and noisy data instead of toy datasets, providing even more comprehensive experiments. These benchmarks are widely studied in previous works [60, 62, 52]. ", "page_idx": 8}, {"type": "text", "text": "Datasets. We brief the background, with more details in Appendix L and visualizations in Fig. 15. ", "page_idx": 8}, {"type": "text", "text": "\u2022 ECMWF Reanalysis v5 (ERA5) [30] is a public extensive dataset, which delivers hourly data on multiple atmospheric variables spanning from 1979 to today. ERA5 represents a type of atmospheric reanalysis dataset [34], integrating observations from a variety of measurement sources with numerical models through data assimilation [33]. Essentially, it reconstructs the most accurate estimation of the Earth\u2019s atmospheric conditions over time. This dataset has been extensively utilized in prior SciML studies [60, 68]. We focus on forecasting the important and challenging temperature atmospheric variable. \u2022 ScalarFlow [14] is a reconstruction of real-world smoke plumes. It assembles the first large-scale dataset of realistic turbulent flows. The availability of a large, volumetric data set opens up a wide range of applications, including re-simulations, novel visualization, and metric evaluations. The dataset contains 104 real-world smoke flow density reconstructions. Each reconstruction is captured from five different viewpoints for 150 temporal frames spanning 2.5 seconds. \u2022 Airfoil [75] is a large-scale dataset that contains the pressure and velocity simulations of the flow around real airfoils. This dataset has 53880 samples for training and 90 samples for testing. Each sample contains 6 channels. The 3 input channels include the binary spatial mask of the airfoil and the initial velocity of the freestream condition in the x and y directions. The output channels include the pressure and the x and y velocity components from the simulation at the steady state. ", "page_idx": 8}, {"type": "text", "text": "Model and Training. For time-dependent ERA5 (Sec. L.1) and ScalarFlow (Sec. L.2) datasets, we adopt the same VideoMAE architecture [77] used in Sec. 4.1. We use 15 consecutive temporal snapshots to forecast the next time step. We train VideoMAE with Adam, with other hyperparameters the same as in Table 3 column \u201cN.S. (PDEBench)\u201d. For the time-independent steady-state Airfoil dataset (Sec. L.3), we adopt the 2D-FNO architecture. We train 2D-FNO with Adam, with other hyperparameters the same as in Table 3 column \u201cPoisson\u201d. ", "page_idx": 8}, {"type": "text", "text": "Results. As shown in Figure 5, compared with directly training operators from scratch (\u201crandom init.\u201d), pretraining on unlabeled data (2D snapshots of ERA5/ScalarFlow without temporal dynamics, or freestream velocities of Airfoil) can help neural operators achieve better performance on both temperature/flow forecasting and predictions of the steady-state pressure and velocity around airfoils. ", "page_idx": 8}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/afcc2636d95d3a4190a997518d74333fb161893df2b9e2ab6017269638ba58d4.jpg", "img_caption": ["Figure 5: For real-world scientific problems, pretraining neural operators on unlabeled PDE data improves its performance and data efficiency. We study VideoMAE [77] pretrained with unlabeled snapshots (no temporal dynamics), and then fine-tune across different numbers of temporal snapshots on ERA5 (left) and ScalarFlow (middle). We also pretrain 2D-FNO [45] on freestream velocities and fine-tune on time-independent steady-state airflow pressure and velocities (right). \u201crandom init.\u201d: models are trained from scratch with random initialization. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 In-Context Examples Enable Data-Efficient OOD Generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now move to OOD settings, where models will be tested on PDE data simulated with physical parameters unseen during fine-tuning/training. We include how to simulate OOD samples in ", "page_idx": 8}, {"type": "text", "text": "Appendix B.2. Neural operators suffer from poor OOD generalization [73, 80]. Traditionally, improvements heavily depend on further fine-tuning on simulated data, which requires extra simulation and training costs. We study the benefits of leveraging test-time in-context examples. As shown in Figure 6, when we flexibly scale up the number of demos, we can keep improving FNO\u2019s OOD generalization on diverse PDEs. We follow [80, 81] that demos are randomly sampled from the same distribution used to generate the OOD test set. When the number of demos is 0, we have the baseline in the OOD setting. Notably, we introduce zero training overhead: we keep the standard training pipeline, and our mining of in-context examples can be seamlessly plugged in during OOD inference. ", "page_idx": 9}, {"type": "text", "text": "We further provide a baseline, which uses features extracted by the backbone of the neural operator (high-dimensional features before the final output layer) to find similar samples. As we can see, this baseline is worse than our method (both performance and confidence), indicating that the final output of the neural operator can more accurately indicate true similar samples. ", "page_idx": 9}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/cf509a0fc6130c3ebad01d966e15f33461d5cd17132fae6e38778a5771cab455.jpg", "table_caption": ["See Appendix M for further discussions about the benefits of leveraging in-context examples, and Appendix N for visualizations. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focus on improving the data efficiency of solving partial differential equations (PDEs) using deep learning, with a particular emphasis on unsupervised pretraining and in-context learning (ICL) methods. Our key contributions include introducing unsupervised pretraining for operator learning and a flexible ICL approach that enhances out-of-distribution (OOD) generalization without increasing training costs. Through extensive evaluations, we demonstrate that our method is not only more data-efficient, but it also achieves greater generalizability compared to existing approaches. By improving the data efficiency of neural operators for solving PDEs, our approach can significantly reduce the computational costs and energy demands of high-fidelity numerical PDE simulations. Additionally, by making advanced PDE solutions more accessible through efficient pretraining, our method has the potential to accelerate scientific and engineering progress across various fields, ultimately beneftiing society. We hope our work will inspire the scientific machine learning (SciML) community to further address the high simulation costs and limited OOD generalization of neural operators, contributing to advancements that support both scientific innovation and environmental sustainability. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Current limitations of our work: 1) We could design more physics-inspired proxy tasks and data augmentation methods for scientific data; 2) We could study more PDE systems in our unsupervised pretraining and in-context learning; 3) We could consider more different neural operator architectures. We expect that addressing these limitations will lead to broader impacts in future works. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[2] Johannes Brandstetter, Max Welling, and Daniel E Worrall. Lie point symmetry data augmentation for neural pde solvers. In International Conference on Machine Learning, pages 2241\u20132256. PMLR, 2022.   \n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics, 52:477\u2013508, 2020.   \n[5] Tianping Chen and Hong Chen. Approximation capability to functions of several variables, nonlinear functionals, and operators by radial basis function neural networks. IEEE Transactions on Neural Networks, 6(4):904\u2013910, 1995.   \n[6] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE transactions on neural networks, 6(4):911\u2013917, 1995.   \n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[8] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.   \n[9] Wai Tong Chung, Bassem Akoush, Pushan Sharma, Alex Tamkin, Ki Sung Jung, Jacqueline Chen, Jack Guo, Davy Brouzet, Mohsen Talei, Bruno Savard, et al. Turbulence in focus: Benchmarking scaling behavior of 3d volumetric super-resolution with blastnet 2.0 data. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[10] Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. arXiv preprint arXiv:2301.07733, 2023.   \n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[14] Marie-Lena Eckert, Kiwon Um, and Nils Thuerey. Scalarflow: a large-scale volumetric data set of real-world scalar transport flows for computer animation and machine learning. ACM Transactions on Graphics (TOG), 38(6):1\u201316, 2019.   \n[15] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65(5):27\u201329, 2022.   \n[16] N Benjamin Erichson, Lionel Mathelin, Zhewei Yao, Steven L Brunton, Michael W Mahoney, and J Nathan Kutz. Shallow neural networks for fluid flow reconstruction with limited sensors. Proceedings of the Royal Society A, 476(2238):20200097, 2020.   \n[17] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594\u2013611, 2006.   \n[18] Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain. Journal of Computational Physics, 428:110079, 2021.   \n[19] Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physicsconstrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020.   \n[20] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.   \n[21] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 5842\u2013 5850, 2017.   \n[22] E. Hairer, C. Lubich, and G. Wanner. Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations (Springer Series in Computational Mathematics), volume 31. Springer, 2006.   \n[23] E Hairer, S P N\u00f8rsett, and G Wanner. Solving ordinary differential equations I. nonstiff problems, volume 29. Springer, 1987.   \n[24] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505\u2013 8510, 2018.   \n[25] D. Hansen, D. C. Maddix, S. Alizadeh, G. Gupta, and M. W. Mahoney. Learning physical models that can respect conservation laws. Physica D: Nonlinear Phenomena, 457:133952, 2024.   \n[26] Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training. arXiv preprint arXiv:2403.03542, 2024.   \n[27] Erisa Hasani and Rachel A Ward. Generating synthetic data for neural operators. arXiv preprint arXiv:2401.02398, 2024.   \n[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[30] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00e1s Hor\u00e1nyi, Joaqu\u00edn Mu\u00f1ozSabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly Journal of the Royal Meteorological Society, 146(730):1999\u20132049, 2020.   \n[31] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790\u20132799. PMLR, 2019.   \n[32] Balaji Jayaraman and SM Abdullah Al Mamun. On data-driven sparse sensing and linear estimation of fluid flows. Sensors, 20(13):3752, 2020.   \n[33] Eugenia Kalnay. Atmospheric modeling, data assimilation and predictability. Cambridge university press, 2003.   \n[34] Eugenia Kalnay, Masao Kanamitsu, Robert Kistler, William Collins, Dennis Deaven, Lev Gandin, Mark Iredell, Suranjana Saha, Glenn White, John Woollen, et al. The ncep/ncar 40-year reanalysis project. In Renewable energy, pages Vol1_146\u2013Vol1_194. Routledge, 2018.   \n[35] Dahyun Kang and Minsu Cho. Integrative few-shot learning for classification and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9979\u20139990, 2022.   \n[36] Gene A Klaasen and William C Troy. Stationary wave solutions of a system of reaction-diffusion equations derived from the ftizhugh\u2013nagumo equations. SIAM Journal on Applied Mathematics, 44(1):96\u2013110, 1984.   \n[37] Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine learning\u2013accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21):e2101784118, 2021.   \n[38] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to PDEs. Journal of Machine Learning Research, 24(89):1\u201397, 2023.   \n[39] A. S. Krishnapriyan, A. F. Queiruga, N. B. Erichson, and M. W. Mahoney. Learning continuous models for continuous physics. Communications Physics, 6:319, 2023.   \n[40] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34:26548\u201326560, 2021.   \n[41] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u2013 1000, 1998.   \n[42] Isaac E Lagaris, Aristidis C Likas, and Dimitris G Papageorgiou. Neural-network methods for boundary value problems with irregular boundaries. IEEE Transactions on Neural Networks, 11(5):1041\u20131049, 2000.   \n[43] Francois Lanusse, Liam Parker, Siavash Golkar, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Geraud Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, et al. Astroclip: Cross-modal pre-training for astronomical foundation models. arXiv preprint arXiv:2310.03024, 2023.   \n[44] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations. Advances in Neural Information Processing Systems, 33:6755\u20136766, 2020.   \n[45] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021.   \n[46] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. arXiv preprint arXiv:2111.03794, 2021.   \n[47] Jerry Weihong Liu, N Benjamin Erichson, Kush Bhatia, Michael W Mahoney, and Christopher Re. Does in-context operator learning generalize to domain-shifted settings? In The Symbiosis of Deep Learning and Differential Equations III, 2023.   \n[48] Jie Liu, Yanqi Bao, Guo-Sen Xie, Huan Xiong, Jan-Jakob Sonke, and Efstratios Gavves. Dynamic prototype convolution network for few-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11553\u201311562, 2022.   \n[49] Robert L Logan IV, Ivana Bala\u017eevic\u00b4, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. Cutting down on prompts and parameters: Simple few-shot learning with language models. arXiv preprint arXiv:2106.13353, 2021.   \n[50] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193, 2019.   \n[51] Romit Maulik, Omer San, Adil Rasheed, and Prakash Vedula. Subgrid modelling for twodimensional turbulence using neural networks. Journal of Fluid Mechanics, 858:122\u2013144, 2019.   \n[52] Michael McCabe, Bruno R\u00e9galdo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, et al. Multiple physics pretraining for physical surrogate models. arXiv preprint arXiv:2310.02994, 2023.   \n[53] David McCallen, Anders Petersson, Arthur Rodgers, Arben Pitarka, Mamun Miah, Floriana Petrone, Bjorn Sjogreen, Norman Abrahamson, and Houjun Tang. Eqsim\u2014a multidisciplinary framework for fault-to-structure earthquake simulations on exascale computers part i: Computational models and workflow. Earthquake Spectra, 37(2):707\u2013735, 2021.   \n[54] Gr\u00e9goire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak T Kiani. Self-supervised learning with lie symmetries for partial differential equations. arXiv preprint arXiv:2307.05432, 2023.   \n[55] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6941\u20136952, 2021.   \n[56] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6707\u20136717, 2020.   \n[57] AK Nandakumaran and PS Datti. Partial differential equations: classical theory with a modern touch. Cambridge University Press, 2020.   \n[58] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pages 69\u201384. Springer, 2016.   \n[59] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[60] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.   \n[61] Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang, Shu Liu, Jingyong Su, and Jiaya Jia. Hierarchical dense correlation distillation for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23641\u201323651, 2023.   \n[62] Michael Poli, Stefano Massaroli, Federico Berto, Jinkyoo Park, Tri Dao, Christopher R\u00e9, and Stefano Ermon. Transform once: Efficient operator learning in frequency domain. Advances in Neural Information Processing Systems, 35:7947\u20137959, 2022.   \n[63] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overftiting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.   \n[64] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI, 2018.   \n[65] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[67] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686\u2013707, 2019.   \n[68] Pu Ren, N Benjamin Erichson, Shashank Subramanian, Omer San, Zarija Lukic, and Michael W Mahoney. Superbench: A super-resolution benchmark dataset for scientific machine learning. arXiv preprint arXiv:2306.14070, 2023.   \n[69] Pu Ren, Chengping Rao, Yang Liu, Jian-Xun Wang, and Hao Sun. Phycrnet: Physics-informed convolutional-recurrent network for solving spatiotemporal pdes. Computer Methods in Applied Mechanics and Engineering, 389:114399, 2022.   \n[70] Xinyu Shi, Dong Wei, Yu Zhang, Donghuan Lu, Munan Ning, Jiashun Chen, Kai Ma, and Yefeng Zheng. Dense cross-query-and-support attention weighted mask aggregation for fewshot segmentation. In European Conference on Computer Vision, pages 151\u2013168. Springer, 2022.   \n[71] Justin Sirignano, Jonathan F MacArt, and Jonathan B Freund. Dpm: A deep learning pde augmentation method with application to large-eddy simulation. Journal of Computational Physics, 423:109811, 2020.   \n[72] Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. Deep language networks: Joint prompt training of stacked llms using variational inference. arXiv preprint arXiv:2306.12509, 2023.   \n[73] Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael W Mahoney, and Amir Gholami. Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior. arXiv preprint arXiv:2306.00258, 2023.   \n[74] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pfl\u00fcger, and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. Advances in Neural Information Processing Systems, 35:1596\u20131611, 2022.   \n[75] Nils Thuerey, Konstantin Wei\u00dfenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for reynolds-averaged navier\u2013stokes simulations of airfoil flows. AIAA Journal, 58(1):25\u201336, 2020.   \n[76] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n[77] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:10078\u201310093, 2022.   \n[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[79] Ricardo Vinuesa and Steven L Brunton. Enhancing computational fluid dynamics with machine learning. Nature Computational Science, 2(6):358\u2013366, 2022.   \n[80] Liu Yang, Siting Liu, Tingwei Meng, and Stanley J Osher. In-context operator learning with data prompts for differential equation problems. Proceedings of the National Academy of Sciences, 120(39):e2310142120, 2023.   \n[81] Liu Yang, Tingwei Meng, Siting Liu, and Stanley J Osher. Prompting in-context operator learning with sensor data, equations, and natural language. arXiv preprint arXiv:2308.05061, 2023.   \n[82] Xiaolin Zhang, Yunchao Wei, Guoliang Kang, Yi Yang, and Thomas Huang. Self-produced guidance for weakly-supervised object localization. In Proceedings of the European conference on computer vision (ECCV), pages 597\u2013613, 2018.   \n[83] Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S Huang. Sg-one: Similarity guidance network for one-shot semantic segmentation. IEEE transactions on cybernetics, 50(9):3855\u2013 3865, 2020.   \n[84] Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023.   \n[85] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.   \n[86] Xuekai Zhu, Yao Fu, Bowen Zhou, and Zhouhan Lin. Critical data size of language models from a grokking perspective. arXiv preprint arXiv:2401.10463, 2024.   \n[87] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physicsconstrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56\u201381, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A PDEs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider two time-independent PDEs (Poisson, Helmholtz) and two time-dependent PDEs (Reaction-Diffusion, Navier-Stokes), as described below. We also describe types of unlabeled data for per-PDE examples. For a general definition of unlabeled PDE data, please refer to Section 3.1.1. ", "page_idx": 16}, {"type": "text", "text": "1. Poisson: We consider a two-dimensional (2D) elliptic PDE that arises in various physical situations, with periodic boundary conditions within a spatial domain $\\Omega=[0,1]^{2}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\mathrm{div}\\,K\\nabla u=f\\quad,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $u(x)$ represents the solution, $f(x)$ acts as the source (e.g., forcing) function, and $x$ denotes spatial coordinate. The diffusion coefficient tensor, denoted by $\\kappa$ , is employed to measure the physical properties of this system. ", "page_idx": 16}, {"type": "text", "text": "Unlabeled PDE Data: In [73], inputs to neural operators have four channels: the source function, and three diffusion coefficients (two diagonal elements and one off-diagonal element in the symmetric diffusion coefficient tensor) expanded to the whole spatial domain. We use this input as the unlabeled data to pretrain the neural operator. ", "page_idx": 16}, {"type": "text", "text": "2. Helmholtz: We consider the 2D inhomogeneous Helmholtz equation, which is a time-independent form of the wave equation, with periodic boundary conditions and a spatial domain $\\Omega=[0,1]^{2}$ . The governing equation is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\Delta u+\\omega u=f\\quad\\mathrm{in~}\\Omega,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $u(x)$ denotes the solution, $f(x)$ represents the source function, and $\\omega>0$ is the wavenumber (that defines the dynamic properties of the Helmholtz equation). This system produces high-frequency large wavenumber oscillatory patterns, which poses challenges in terms of generalization. ", "page_idx": 16}, {"type": "text", "text": "Unlabeled PDE Data: In [73], inputs to neural operators have two channels: the source function, and the wavenumber $\\omega$ expanded to the whole spatial domain. We use this input as the unlabeled data to pretrain the neural operator. ", "page_idx": 16}, {"type": "text", "text": "3. Reaction-Diffusion: The 2D Reaction-Diffusion (RD) equation involves the interaction between two nonlinear coupled variables, i.e., the activator $u(t,x,y)$ and the inhibitor $v(t,x,y)$ . The RD equation is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}u=D_{u}\\big(\\partial_{x x}u+\\partial_{y y}u\\big)+R_{u},}\\\\ &{\\partial_{t}v=D_{v}\\big(\\partial_{x x}v+\\partial_{y y}v\\big)+R_{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\{D_{u},D_{v}\\}$ are diffusion coefficients for $u$ and $v$ , respectively, and where $R_{u}$ and $R_{v}$ are reaction functions, which are defined as the Fitzhugh-Nagumo equation [36]: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R_{u}=u-u^{3}-k-v,}}\\\\ {{R_{v}=u-v.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The spatial domain considered for the 2D RD equation is $\\Omega=[-1,1]^{2}$ , and the time duration is $t\\in(0,5]$ . ", "page_idx": 16}, {"type": "text", "text": "Unlabeled PDE Data: In PDEBench [74], we forecast the activator $u$ and the inhibitor $v$ (i.e. two input channels) with $T=10$ . Since each individual snapshot can serve as an initial condition for forecasting, during our unsupervised pretraining we discard the temporal dynamics and use randomly shuffled snapshots of $u$ and $v$ with $T=1$ as unlabeled PDE data. That means, during pretraining the model only has access to single and individual snapshots without any awareness of temporal dynamics. ", "page_idx": 16}, {"type": "text", "text": "4. Navier-Stokes Equation: Lastly, we consider the 2D incompressible Navier-Stokes equation in vorticity form on the unit torus, which is formulated as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\nabla\\cdot\\mathbf{v}=0}\\\\ {\\rho\\left(\\partial_{t}\\mathbf{v}+\\mathbf{v}\\cdot\\nabla\\mathbf{v}\\right)=-\\nabla p+\\nu\\nabla^{2}\\mathbf{v}+\\mathbf{f}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, the velocity $\\mathbf{v}$ is defined within the time duration $[0,T]$ and the spatial domain $[0,1]^{2}$ (the vorticity can be formulated as $w\\,=\\,\\nabla\\,\\times\\,{\\bf v})$ ). Moreover, $\\rho$ is the density, and the coefficient $\\nu$ signifies the viscosity of a fluid, and f is the external forcing function. Dirichlet boundary conditions are employed in this system. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Unlabeled PDE Data: We follow the training settings from previous works. 1) For FNO [45], we focus on mapping the vorticity from the initial condition to the full solution $\\left[w_{0}\\mapsto w\\right|_{[0,T)}$ , with $T=33$ ). The input includes four channels: the vorticity (which is the initial condition, duplicated for $T$ times), and three (xy-spatial and temporal) linear mesh position embedding channels. Thus, similar to Reaction-Diffusion above, during pretraining, the model will take individual snapshots of vorticity and position embeddings as unlabeled data. 2) For PDEBench [74], we forecast both xy velocities and pressure (i.e. three input channels), with $T=15$ . The model will take individual snapshots of vorticity and xy velocities as unlabeled data. ", "page_idx": 17}, {"type": "text", "text": "We summarize detailed inputs and outputs in Table 1. ", "page_idx": 17}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/c4b14713ee8a597b7c8a8057e63ed4ef4543af45989ebaa4ff75a04376a88637.jpg", "table_caption": ["Table 1: Inputs and outputs for learning different PDEs. See Table 3 for resolutions. \u201cNS\u201d: Navier-Stokes. \u201cRD\u201d: Reaction-Diffusion. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Detailed Experiment Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Distributions of Unlabeled PDE Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Table 2, for the purpose of OOD testing, we summarize the distribution of our unlabeled PDE data during pretraining, fine-tuning, and inference with in-context examples. Ranges of these physical parameters are inspired by [73]. During pretraining, we consider a wide distribution of unlabeled PDE data. When training (fine-tuning) our model, we consider in-distribution unlabeled PDE data. Finally, we test our similarity-based method that learns in-context examples in OOD settings.4 For Helmholtz OOD, we choose a narrow range of coefficients ([15, 20]) mainly because its solution is very sensitive to the wavenumber, and FNO\u2019s performance significantly drops when we move to more extreme OOD settings. ", "page_idx": 17}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/7ce79a72ddd5334111d7d29163c20ba2a8cf13108022cc3979e35a9387b3677b.jpg", "table_caption": ["Table 2: Ranges of physical parameters (integers) for unsupervised pretraining, training (fine-tuning), and out-of-distribution (OOD) inference. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Data Generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Unlabeled PDE Data. We generate data for Poisson and Helmholtz [73], Reaction-Diffusion on PDE-Bench [74] and 2D incompressible Navier-Stokes on PINO Dataset [46] following the procedure mentioned in the paper. For unlabeled data generation, we bypass the computation of solvers, which expedites the generation speed, as shown in Table 4. ", "page_idx": 17}, {"type": "text", "text": "OOD Samples. The OOD data generation procedure is similar to the unlabeled data, except for the changes in the physical parameters coefficients. For Poisson and Helmholtz, we consider changing the range of diffusion eigenvalue and waver number respectively. For Navier-Stokes equation, we change the Reynolds number. We list the coefficients in Table 2. ", "page_idx": 17}, {"type": "text", "text": "B.3 Training Hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We summarize our hyperparameters used during pretraining and fine-tuning/training in Table 3. These hyperparameters strictly follow previous works [45, 73, 74, 46, 52]. We conducted our experiments on four A100 GPUs, each with 40GB of memory. ", "page_idx": 18}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/69452e4bf0663696675016bd56a2c90411946b5b6615ab12382111ab02821354.jpg", "table_caption": ["Table 3: Hyperparameters for pretraining and training/fine-tuning. \u201cN.S.\u201d: 2D Incompressible Navier-Stokes. \u201cDAdapt\u201d: adaptive learning rate by D-adaptation [10]. \u201cns\u201d: total number of simulated training samples. A batch size of \u201cmin(32, ns)\u201d is because the total number of training samples might be fewer than 32. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Examples of Simulation Costs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 4, we demonstrate the cheap simulation of only unlabeled PDE data, versus simulating both unlabeled PDE data and solutions, on 2D incompressible Navier-Stoke on PINO Dataset [46] and Reaction-Diffusion on PDE-Bench [74]. We can see that unlabeled PDE data are extremely cheap to simulate. Therefore, our pretraining method can boost the performance and meanwhile save the heavy cost of data simulations. ", "page_idx": 18}, {"type": "text", "text": "Table 4: Simulation time costs on 2D Incompressible Navier-Stokes (\u201cN.S.\u201d) on PINO Dataset [46] and ReactionDiffusion (\u201cR.D.\u201d) on PDE-Bench [74]. \u201c $\\bar{\\,}R\\bar{e}^{,\\ast}$ : Reynolds number. \u201c $D_{u}$ , $D_{v}\\,^{\\,^{\\circ}}$ : diffusion coefficients. $N$ : number of samples. $T$ : temporal resolution. $H\\times W$ : spatial resolution. $C$ : input channels (1 for the vorticity in N.S., 2 for velocities $u,v$ in R.D.). ", "page_idx": 18}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/8dd16577ed043762f9a2feef28297a0633693bfe21985eb9c7540dd520a9bfca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Model Architectures ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 7, We show visualizations of architectures described in Sec. 3.1.4. Specifically, the FNO has 67.1M parameters, and the Video-MAE has 23.4M. During pretraining, FNO costs 20 GPU hours, and Video-MAE costs 18 GPU hours. During fine-tuning, FNO costs 4 GPU hours, and Video-MAE costs 6 GPU hours. ", "page_idx": 18}, {"type": "text", "text": "E Comparison with Contrastive Learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Contrastive learning is an important self-supervised pretraining technique studied in computer vision. For a fair comparison, we directly compare with MoCo v2 [8], a highly-cited self-supervised learning method also originally implemented in PyTorch, whose core method is closely related to SimCLR [7] (originally implemented in TensorFlow). ", "page_idx": 18}, {"type": "text", "text": "We compare MoCo v2 with our method on a broader real-world benchmark ERA5 [30]. As shown in Figure 8, our unsupervised learning method can largely outperform MoCo v2. This extra comprehensive result demonstrates that our method can be widely adopted in real-world problems, outperforming previous unsupervised learning methods. ", "page_idx": 18}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/8078e4e4323d8262d408afc5c782538dffe9fd28fb0dbf51a5b1c097196e8f57.jpg", "img_caption": ["Figure 7: Visualizations of architectures we studied. Left: FNO [46]. Right: VideoMAE [77]. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/8fbbf31df61267ade2bffbf60ebf0054785d490c1771d4dfeba83d230980d756.jpg", "img_caption": ["Figure 8: Comparison between our unsupervised pretraining method versus MoCo v2 [8]. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F More Comparisons with Vision Pretrained Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Beyond Figure 3(e) for the transformer on time-dependent PDE (Navier-Stokes), we further verify that pretraining on unsupervised PDE data makes FNO outperform its off-the-shelf vision-pretrained checkpoints. Specifically, we first pretrain the 2D FNO model on ImageNet [11], then fine-tune on the downstream PDE simulation data. As shown in Figure 9, vision-pretrained FNO performs worse during downstream fine-tuning on time-independent PDEs (including both Poisson and Helmholtz equations), confirming that domain-specific pretraining, even on unsupervised PDE data, is more beneficial than conventional checkpoints pretrained on unrelated domains like computer vision. ", "page_idx": 19}, {"type": "text", "text": "G Joint Pretraining Further Improves Performance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We also study if joint pretraining on unlabeled multiple PDE data can bring extra benefits. We combine all 46,080 unlabeled Poisson samples and all 46,080 unlabeled Helmholtz samples (see Table 3). We choose this setting because recent works on SciML foundation models [52, 26] also use all samples from each PDE for pretraining. We do zero-paddings for mismatched channels. From Figure 10, we can see that joint pretraining can further improve the performance of fine-tuning on different PDEs. ", "page_idx": 19}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/a474cdb544388f88bcffda114a449356f916d6b4f8d448cd9039344ce8556051.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 9: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (left), Helmholtz (right). \u201crandom init.\u201d: models are trained from scratch with random initialization. \u201cvision pretrained\u201d: fine-tuning from the checkpoint pretrained on computer vision dataset ImageNet [11]. ", "page_idx": 20}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/7f05e47bc1277909a4f6c29d41151920a82c73111985885d27d1b97b63fdbc87.jpg", "img_caption": ["Figure 10: Joint unsupervised pretraining on multiple PDEs (green solid curve) further improves the data efficiency of neural operators when fine-tuning on Poisson (left), Helmholtz (middle), Reaction-Diffusion (right). \u201crandom init.\u201d: models are trained from scratch with random initialization. \u201cunsupervised\u201d: models are pretrained on a single unsupervised PDE data. \u201cunsupervised joint\u201d: models are pretrained on a joint of multiple unsupervised PDE datasets. \u201cNS\u201d: Navier Stokes. \u201cRD\u201d: Reaction-Diffusion. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "H Fine-tuning on Unseen PDEs is Challenging ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We also try to fine-tune neural operators on unseen PDEs (i.e. PDEs different from pretraining). Mismatched channels are padded with zeros. We find this will lead to worse performance compared with models pretrained on the same PDE, as shown in Figure 11. ", "page_idx": 20}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/2fecdd66ab204515f8e7c66449e3c63b5688960d0865ddbe7e794b082724b1cd.jpg", "img_caption": ["Figure 11: Fine-tuning FNO (pretrained on Poisson) on unseen samples from Helmholtz. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Consistently Improved Generalization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Beyond the generation gap we have shown in Figure 4 (left) for the Navier Stokes equation from PDEBench, we further collect generation gaps of our models learned on other PDEs. As shown in Figure 12, on diverse PDE systems, our method can contribute to universally reduced overfitting. ", "page_idx": 20}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/840c2184f2b8335b0d1e8317a14e86ff3cfbdcc8a72671977a1bee10043a3cc8.jpg", "img_caption": ["Figure 12: Universally reduced overfitting (i.e. smaller generalization gaps) on diverse PDEs (a to d). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "J More Ablation Studies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "J.1 Magnitude of Perturbations during Pretraining. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We study the optimal magnitude of perturbations during pretraining: \u201cMask Ratio\u201d (1 indicates no visible grids, and 0 means no masks); and \u201cBlur Sigma\u201d for the variance of Gaussian kernel for blur (larger the more degradation). We show our results in Table 5, 6, 7, 8. For example, on 2D incompressible Navier-Stokes with FNO, as shown in Table 8, we can find that when training with a low volume of data, we should use much stronger perturbations (high masking ratios and strong blur), whereas a high volume of data only requires mild perturbations. ", "page_idx": 21}, {"type": "text", "text": "Table 5: Best choice of mask ratio and blur sigma for pretraining on Poisson equation. ", "page_idx": 21}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/318e3717d152ab29a11273fa543885b20d426599462593d32d7b7f1fe9136a5f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 6: Best choice of mask ratio and blur sigma for pretraining on Helmholtz equation. ", "page_idx": 21}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/673e1f5a83ef4b8751a692e43f9c623cce425660ba2dbad5c8c2140b87d4e042.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "J.2 Ablation of the Number of Pretraining Samples. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As shown in Table 9, the more unlabeled PDE data we use for pretraining, the better quality the pretrained model will be. ", "page_idx": 21}, {"type": "text", "text": "Table 7: Best choice of mask ratio and blur sigma for pretraining on 2D Diffusion-Reaction equation. ", "page_idx": 22}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/6051f82747316d9b2d537a80dba2710ece8d023d571ff0611d83be27fdf0725e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 8: Best choice of mask ratio and blur sigma for pretraining on 2D incompressible Navier-Stokes. ", "page_idx": 22}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/c2cb5536f936b903f59e27f0d335863cfac6a6590f295abdeb47cf342b82ab95.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 9: More unlabeled PDE data improve the quality of pretraining. FNO on 2D incompressible Navier-Stokes, pretrained with mask ratio as 0.7. ", "page_idx": 22}, {"type": "table", "img_path": "MuPlJ9fT4b/tmp/8acabeb2ad4e8fe300ae5c3c0a41bed3d0cd78412d62befbc4c7b26d48e46a28.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/d6e8d92cc3910360892ac1e7be4d75bb7ed49ea05fa4474281a95d6aadc746ce.jpg", "img_caption": ["Figure 13: Visualization of FNO reconstructions of unlabeled PDE data on the Poisson (\u201cPois.\u201d), Helmholtz (\u201cHelm.\u201d), 2D Diffusion-Reaction (\u201cD.R.\u201d), and 2D incompressible Navier-Stokes (\u201cN.S.\u201d) equations during MAE pretraining. (Mask ratio: 0.1 for Poisson, Helmholtz, and 2D Diffusion-Reaction equations; 0.7 for incompressible Navier-Stokes.) In masks, only white areas are visible to the model during pretraining. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "K Visualization of MAE Pretraining ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To demonstrate the efficacy of our MAE-based pretraining, we show the unlabeled PDE data and its reconstructed version in Figure 13 (MAE pretraining on different PDEs) and Figure 14 (MAE pretraining on 2D incompressible Navier-Stokes with varying mask ratios). We can see that all inputs are accurately reconstructed with low errors and similar patterns. ", "page_idx": 23}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/37ebedea5953cacfec9366e6f1fad26670c16d659da196bfa663b9ff8ff16d90.jpg", "img_caption": ["Figure 14: Visualization of FNO reconstructions of unlabeled PDE data on the 2D incompressible Navier-Stokes equations during MAE pertaining with mask ratio from 0.1 to 0.9. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "L Details and Visualizations of Real-World Data ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "L.1 ECMWF Reanalysis v5 (ERA5) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We utilize data from 2006 to 2015, with the snapshots taken every 6 hours and a spatial resolution of $360\\times360$ . The total number of snapshots is 14600. We apply the mean-standard deviation normalization to the data and downsample the snapshots to a spatial resolution of $180\\times180$ . We split $75\\%$ of the data for pretrain and $25\\%$ for finetune. For each split, we further separate $80\\%$ of the data for training, $10\\%$ for validation, and $10\\%$ for testing. ", "page_idx": 23}, {"type": "text", "text": "L.2 ScalarFlow ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The original spatial resolution is $1062\\times600$ . We crop the snapshots to $900\\times600$ to remove the padding and background. We remove the first 15 timeframes for each simulation to avoid the initial transient phase at the beginning of the smoke flow generation. We then downscale the snapshots to $180\\times120$ and apply mean-standard deviation normalization to the data. With a total of 70200 snapshots, we split $80\\%$ of the data for pretrain and $20\\%$ for finetune. For each split, we further separate $80\\%$ of the data for training, $10\\%$ for validation, and $10\\%$ for testing. ", "page_idx": 24}, {"type": "text", "text": "L.3 Airfoil ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We split $80\\%$ of the training data for pretrain and $20\\%$ for finetune. The original spatial resolution of each sample is $128\\times128$ . We crop the snapshots to $96\\times45$ to remove the background. We then apply min-max normalization channel-wise to the sample. ", "page_idx": 24}, {"type": "text", "text": "We show visualizations of real-world scientific data in Figure 15. ", "page_idx": 24}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/c8a4371af7c7e3adc6730f692dbd2686e13322b5e8094173ea6be58751479dc6.jpg", "img_caption": ["Figure 15: We show snapshot examples from ERA5 temperature [30] (a, b) and ScalarFlow [14] $(\\mathbf{c},\\,\\mathbf{d})$ at different temporal steps; and also an example of Airfoil mask, velocities, and pressure [75] (e-j). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "M Benefits of In-context Examples ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We try to understand the benefit of leveraging in-context examples by decomposing the relative MSE error into \u201cscale\u201d and \u201cshape.\u201d \u201cScale\u201d is the slope of a linear regression between targets and predictions (closer to 1 the better), indicating the alignment of the range of model outputs with targets. \u201cShape\u201d is the normalized relative MSE (i.e., model outputs or targets are normalized by their own largest magnitude before MSE), indicating the alignment of scale-invariant spatial/temporal structures. We show results in Figure 16. We find that the benefit of in-context examples lies in that the scale of the model\u2019s output keeps being better calibrated (\u201cscale\u201d being closer to 1) when adding more demos. ", "page_idx": 24}, {"type": "text", "text": "In numerical simulations or predictions of PDEs, there are settings where the scale or magnitude of solutions is more important than the exact shape/pattern: ", "page_idx": 24}, {"type": "text", "text": "1. Heat Transfer: In large-scale systems, the focus might be on overall temperature and extreme values. For instance, in evaluating a cooling system, the key concern might be the peak temperature rather than the detailed temperature distribution.   \n2. Fluid Dynamics: For applications like aerodynamics, the overall drag or lift force on an object is often more critical than capturing every detail of the flow pattern, such as in airfoil design.   \n3. Environmental Modeling: The concentration of pollutants at specific locations or total pollutant transport is often more crucial than the exact distribution, such as in groundwater flow studies. ", "page_idx": 24}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/cfbd3bdab191ec172c5ccc8cd294d40b0a4fabe6cf349353570d823ad67ba728.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 16: Beneftis of in-context examples. To analyze the benefti of in-context examples for complicated PDE systems, we decompose the relative MSE error into \u201cScale\u201d and \u201cShape\u201d. \u201cScale\u201d indicates the alignment of the range of model outputs with targets (closer to 1 the better), via the slope of a linear regression. \u201cShape\u201d indicates the alignment of scale-invariant spatial/temporal structures via normalized relative MSE (i.e. model outputs or targets are normalized by their own largest magnitude before MSE). We find that the benefit of in-context examples lies in that the scale of the model\u2019s output keeps being calibrated (red line being closer to 1) when adding more demos. ", "page_idx": 25}, {"type": "text", "text": "N Visualizations with In-Context Examples ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We show visualizations of our similarity-based mining of in-context examples in Figure 17. In this visualization, we find that the range of numerical solutions (e.g., values in colorbars) predicted with in-context examples becomes closer to the target. Meanwhile, based on this visualization, we conclude that OOD generalization is challenging for neural operators because of: 1) Significantly different patterns of solutions under different physical parameters; 2) Different value ranges of solutions under different physical parameters. ", "page_idx": 25}, {"type": "image", "img_path": "MuPlJ9fT4b/tmp/2ac35b4a618201226f66d35ee8528d73fc1d673a808708cb4bcaa43e87cf66a8.jpg", "img_caption": ["Figure 17: Visualizations of mining in-context examples for FNO in OOD testing. Ranges of solutions predicted with in-context examples (min/max of each snapshot, reflected in colorbars) become closer to the target. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 26}, {"type": "text", "text": "Justification: All claims stated in the abstract and introduction are evaluated in our experiments (Sec. 4). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discussed our limitations in a separate Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 27}, {"type": "text", "text": "Justification: We include instructions for reproducibility in the Appendix, including choices of unlabeled PDE data (Appendix A), data generation and training hyperparameters B, and model structures D. We also attached our code in supplement. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 28}, {"type": "text", "text": "Justification: We attached our code in the supplement. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 28}, {"type": "text", "text": "Justification: We include our experimental settings in Appendix B. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All our operator pretraining experiments are reproduced for three times. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 29}, {"type": "text", "text": "Justification: We discussed the compute resources we used in Appendix B.3. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not involve any potential harm, societal impact, or violation. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 29}, {"type": "text", "text": "Justification: This work is about foundational research on data augmentations. It does not lead to any societal impact because: 1) our motivations, goals, and results are too distant from social applications to make any social impact; 2) we mainly use public data and benchmarks; and 3) we use public model architectures and follow previous works which have open access. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks because all datasets studied in our work are from public benchmarks, and we have clearly cited original papers. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 30}, {"type": "text", "text": "Justification: All datasets studied in our work are from public benchmarks, and we have clearly cited original papers. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve any potential risk regarding research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]