[{"figure_path": "jb5qN3212b/tables/tables_5_1.jpg", "caption": "Table 1: Summary statistics of the datasets: size of the training set |V<sub>i</sub>|, test ID set |V<sub>uid</sub>|, test OOD set |V<sub>uood</sub>|; number of ID classes C, scale of the dataset, and whether the graph is homophily.", "description": "This table presents key statistics for ten real-world datasets used in the experiments.  Each row represents a dataset, providing information on the number of training nodes (|V<sub>i</sub>|), the number of in-distribution test nodes (|V<sub>uid</sub>|), the number of out-of-distribution test nodes (|V<sub>uood</sub>|), the number of classes (C), the overall scale of the dataset (SM for small, LG for large), and whether the dataset exhibits homophily (a tendency for nodes of the same class to be connected). This information is crucial for understanding the characteristics of the datasets and how they impact the performance of the out-of-distribution detection methods.", "section": "5 Experiments"}, {"figure_path": "jb5qN3212b/tables/tables_6_1.jpg", "caption": "Table 2: Main results on common benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained GCN. We take the average values that are percentages over 5 independently trained backbones.\u2191(\u2193) indicates larger (smaller) values are better.", "description": "This table compares the performance of the proposed GRASP method with several other out-of-distribution detection methods on five common benchmark datasets using pre-trained Graph Convolutional Networks (GCNs).  It shows the False Positive Rate (FPR) at 95% recall (FPR95) and the Area Under the Receiver Operating Characteristic curve (AUROC) for each method. Lower FPR95 and higher AUROC indicate better performance.  The average performance across all datasets is also presented.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_7_1.jpg", "caption": "Table 3: Main results on large-scale benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained method GCN. We take the average values that are percentages over 5 independently trained backbones. OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. \u2191 (\u2193) indicates larger (smaller) values are better.", "description": "This table presents the results of the proposed GRASP method and several baseline methods for out-of-distribution (OOD) detection on five large-scale graph datasets.  It compares performance using AUROC and FPR95 metrics, averaging results across five independently trained GCN models. The table also notes instances where methods ran out of memory (OOM) or did not produce results after 48 hours (OOT).", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study on OOD detection performance by different augmentation policy. We report averaged AUROC over 5 independently pre-trained GCN models.", "description": "This table presents the results of an ablation study on the effect of different augmentation policies on OOD detection performance.  The study compares several methods of augmenting the training data, including using a greedy approach based on the proposed h(i) score (Top 50%), randomly selecting training nodes, selecting the bottom 50% of nodes according to h(i), and using a standard graph augmentation technique (GAug).  TestAug represents an alternative augmentation strategy which directly applies augmentations to the test dataset. The results show that the proposed strategy (Top 50%) significantly outperforms the other methods across multiple datasets, demonstrating its effectiveness in enhancing OOD detection.", "section": "5.2 A Comprehensive Analysis of GRASP"}, {"figure_path": "jb5qN3212b/tables/tables_8_2.jpg", "caption": "Table 5: GRASP consistently enhances the OOD detection performance of nodes connected by both inter-edges and intra-edges on datasets characterized by a strong degree of heterophily (datasets highlighted in bold in the table). However, naive propagation tends to compromise the performance of these nodes.", "description": "This table compares the performance of three methods for out-of-distribution (OOD) detection on nodes with different edge types (intra-edges and inter-edges). The methods are: MSP (Maximum Softmax Probability), MSP+prop (MSP with naive score propagation), and MSP+GRASP (MSP with the proposed graph augmentation and score propagation).  The results show that GRASP significantly improves OOD detection, especially on heterophilic graphs where naive propagation is less effective. The datasets with strong heterophily are highlighted in bold.", "section": "5.2 A Comprehensive Analysis of GRASP"}, {"figure_path": "jb5qN3212b/tables/tables_8_3.jpg", "caption": "Table 6: GRASP is compatible with different OOD scoring functions. We compare OOD detection methods and the performance after the simple propagation in Equation 1 (denoted by \"+ prop\") and with GRASP respectively. We report AUROC results that are averaged over 5 independent pre-trained GCN models.", "description": "This table compares the performance of different OOD scoring functions (MSP, Energy, KNN) with and without score propagation and with the proposed GRASP method.  It shows that GRASP consistently improves the AUROC (Area Under the Receiver Operating Characteristic curve) across all datasets and scoring functions, demonstrating its effectiveness as a general enhancement to OOD detection.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_18_1.jpg", "caption": "Table 7: Statistics of all the graph datasets. #C is the total number of distinct node classes.", "description": "This table presents the statistics of ten real-world datasets used in the experiments.  For each dataset, it shows the number of nodes, edges, features, number of classes (C), the domain the data comes from, whether the graph is homophilic or heterophilic, the range of indices representing the out-of-distribution (OOD) class, and the range of indices for the in-distribution (ID) class.", "section": "5 Experiments"}, {"figure_path": "jb5qN3212b/tables/tables_19_1.jpg", "caption": "Table 8: ID ACCs of six pre-trained methods on common benchmarks. For each pre-trained method, we take the average values that are percentages over 5 independently trained backbones.", "description": "This table shows the ID accuracy (ACC) for six different pre-trained graph neural network (GNN) models across five common benchmark datasets.  The average ACC is reported, calculated from five independent training runs for each model on each dataset, providing a more robust measure of performance.", "section": "D.1 Graph ID Classification Details"}, {"figure_path": "jb5qN3212b/tables/tables_19_2.jpg", "caption": "Table 9: ID ACCs of pre-trained GCN models on five large-scale datasets. We report the average values that are percentages over 5 independently trained backbones.", "description": "This table shows the In-distribution accuracy (ID ACC) of Graph Convolutional Network (GCN) models pretrained on five large-scale datasets.  The average ID ACC across five independently trained GCN backbones is reported for each dataset.  The results demonstrate the performance of the pre-trained models on these large datasets before the OOD detection task is applied.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_19_3.jpg", "caption": "Table 10: ID ACCs of three training-based ood detection methods on five small-scale datasets. For each method, we take the average values that are percentages over 5 independently runs.", "description": "This table presents the In-distribution accuracy results of three training-based out-of-distribution detection methods on five small-scale benchmark datasets.  The results are averaged over five independent runs for each method, showing the ID ACC (In-distribution accuracy) with standard error for each method on each dataset.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_19_4.jpg", "caption": "Table 11: ID ACCs of three training-based ood detection methods on five large-scale datasets, where OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. All the training-based methods only have results on the moderately sized arXiv-year dataset. For each method, we take the average values that are percentages over 5 independently runs.", "description": "This table shows the ID accuracy (ACC) of three training-based out-of-distribution (OOD) detection methods on five large-scale graph datasets.  The results highlight the limited applicability of training-based methods to large-scale datasets due to the high computational cost and memory requirements. Only the arXiv-year dataset produced results for all three methods. The table reports the average ACC across five independent runs for each method.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_20_1.jpg", "caption": "Table 2: Main results on common benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained GCN. We take the average values that are percentages over 5 independently trained backbones. \u2191(\u2193) indicates larger (smaller) values are better.", "description": "This table presents the main results of the proposed GRASP method and various baseline methods on five common graph datasets (Cora, Amazon-Photo, Coauthor-CS, Chameleon, Squirrel). It shows the performance comparison in terms of AUROC and FPR95. The results are averaged over five independently trained GCN backbones.  The table highlights the consistent superior performance of GRASP across various datasets and baselines, showcasing its effectiveness as a graph OOD detection method.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_20_2.jpg", "caption": "Table 3: Main results on large-scale benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained method GCN. We take the average values that are percentages over 5 independently trained backbones. OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. \u2191 (\u2193) indicates larger (smaller) values are better.", "description": "This table presents the performance comparison of the proposed method (GRASP) with other state-of-the-art methods for out-of-distribution detection on five large-scale graph datasets.  The results show the average values and standard errors over five independently trained GCN backbones.  'OOM' indicates that the method ran out of memory, and 'OOT' indicates that no results were obtained after running for 48 hours.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_21_1.jpg", "caption": "Table 14: Results of various GNN pretrained backbones on common benchmarks.", "description": "This table presents the results of applying various pre-trained Graph Neural Network (GNN) architectures to the task of Out-of-Distribution (OOD) detection.  It shows the performance of different OOD detection methods across multiple datasets using various GNN backbones, allowing for a comparison of the methods' effectiveness across different architectures and data. The table shows False Positive Rate (FPR) and Area Under the Receiver Operating Characteristic Curve (AUROC) metrics to quantify performance.", "section": "D.3 Graph OOD Detection Results on Various Backbones"}, {"figure_path": "jb5qN3212b/tables/tables_23_1.jpg", "caption": "Table 15: Ratios of intra-edges Nintra and respective OOD detection performance on all benchmarks before and after employing graph augmentation.", "description": "This table presents the ratios of intra-edges (Nintra) and the area under the receiver operating characteristic curve (AUROC) scores for OOD detection before and after applying graph augmentation on several benchmark datasets.  It demonstrates the impact of increasing intra-edges (by augmentation) on the OOD detection performance, showcasing the effectiveness of the proposed augmentation strategy in improving the results.", "section": "5.2 A Comprehensive Analysis of GRASP"}, {"figure_path": "jb5qN3212b/tables/tables_24_1.jpg", "caption": "Table 2: Main results on common benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained GCN. We take the average values that are percentages over 5 independently trained backbones. \u2191(\u2193) indicates larger (smaller) values are better.", "description": "This table presents the main results of the proposed method (GRASP) and several competitive out-of-distribution (OOD) detection methods on five common benchmark datasets.  The results are averaged over five independently trained graph convolutional network (GCN) backbones to ensure robustness.  The table compares various metrics, including False Positive Rate (FPR) and Area Under the Receiver Operating Characteristic Curve (AUROC), to evaluate the performance of each method in detecting OOD nodes.", "section": "5.1 Main Results"}, {"figure_path": "jb5qN3212b/tables/tables_25_1.jpg", "caption": "Table 17: Time (s) and Memory (M) costs of all algorithms with backbone GCN on large-scale benchmarks.", "description": "This table compares the runtime and memory usage of various algorithms (including GRASP) on five large-scale graph datasets using the GCN backbone.  The results show the time in seconds and memory consumption in megabytes for each method on each dataset, illustrating efficiency differences between the methods.", "section": "5 Experiments"}, {"figure_path": "jb5qN3212b/tables/tables_25_2.jpg", "caption": "Table 18: Various classical propagation mechanisms on each dataset.", "description": "This table compares the AUROC (Area Under the Receiver Operating Characteristic curve) scores achieved by several classical propagation methods and GRASP on four different datasets.  The methods compared include Personalized PageRank (PPR), Heat Kernel Diffusion (GraphHeat), Graph Diffusion Convolution (GDC), Mixing Higher-Order Propagation (MixHop), and Generalized PageRank (GPR).  The table shows that GRASP consistently outperforms these other methods, achieving substantially higher AUROC scores on each dataset.", "section": "D.7 Explore How Different Propagation Mechanisms Impact the Findings"}, {"figure_path": "jb5qN3212b/tables/tables_25_3.jpg", "caption": "Table 19: Various classical propagation mechanisms on each dataset.", "description": "This table presents the AUROC results of several classical graph propagation methods (PPR, GraphHeat, GDC, MixHop, GPR) and the proposed GRASP method on four large-scale benchmark datasets (Squirrel, arXiv-year, snap-patents, wiki).  It shows the performance variation across different propagation methods, highlighting the consistent superior performance of GRASP compared to the other methods.", "section": "D.7 Explore How Different Propagation Mechanisms Impact the Findings"}]