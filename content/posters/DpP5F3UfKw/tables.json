[{"figure_path": "DpP5F3UfKw/tables/tables_4_1.jpg", "caption": "Table 1: Top 10 hypotheses generated from the best layer of GPT-2 XL on the Harry Potter dataset", "description": "This table presents the top ten hypotheses generated by an LLM-based hypothesis proposer to explain the discrepancies between GPT-2 XL's predictions and actual MEG responses for the Harry Potter dataset.  The hypotheses are ranked by their validity (a measure of how well they distinguish between sentences where the model's predictions were accurate and sentences where they were not), and a p-value indicating the statistical significance of the difference in validity. The hypotheses highlight aspects of language that the model struggles to capture, such as high levels of emotional intensity, complex sentence structures, and nuanced emotional language.", "section": "3 Identifying Phenomena of Interest"}, {"figure_path": "DpP5F3UfKw/tables/tables_4_2.jpg", "caption": "Table 2: Top 10 hypotheses generated from the best layer of GPT-2 XL on the Moth dataset", "description": "This table presents the top 10 hypotheses generated by an LLM-based hypothesis proposer to explain the discrepancies between human brain responses and GPT-2 XL predictions for the Moth dataset.  The hypotheses are ranked by their validity scores (a measure of how well they distinguish between sentences where the model's predictions accurately reflect brain activity and sentences where they do not), and p-values are provided, indicating statistical significance after correcting for multiple comparisons.  These hypotheses shed light on aspects of language processing which the model is not capturing well.", "section": "3 Identifying Phenomena of Interest"}, {"figure_path": "DpP5F3UfKw/tables/tables_7_1.jpg", "caption": "Table 3: Datasets for Fine-Tuning with Sample Questions and Answers (Correct Answer in Bold)", "description": "This table presents two datasets used for fine-tuning the language models: Social IQa (Social/Emotion) and PIQA (Physical).  For each dataset, it lists the type, the number of training examples, the number of answer options per question, a sample question, and the corresponding sample answer choices. The correct answer is bolded for each example.", "section": "5.1 Datasets"}, {"figure_path": "DpP5F3UfKw/tables/tables_17_1.jpg", "caption": "Table 1: Top 10 hypotheses generated from the best layer of GPT-2 XL on the Harry Potter dataset", "description": "This table presents the top ten hypotheses generated by an LLM to explain divergences between human brain responses and GPT-2 XL model predictions on the Harry Potter dataset. The hypotheses are ranked by their validity (how well they distinguish between the sentences that the model predicted poorly versus well) and accompanied by their p-values. These hypotheses highlight aspects of language that are not well-captured by the model such as high levels of emotional intensity, complex sentence structures, emotional language, and characters interacting with their environment.", "section": "3 Identifying Phenomena of Interest"}, {"figure_path": "DpP5F3UfKw/tables/tables_17_2.jpg", "caption": "Table 5: Top 10 hypotheses generated from the best layer of Llama-2 on the Harry Potter dataset", "description": "This table presents the top 10 hypotheses generated by an LLM to explain the discrepancies between human brain responses and the language model's predictions for the Harry Potter dataset.  The hypotheses focus on aspects related to physical actions, events, objects, and locations within the wizarding world, reflecting the strengths and limitations of the model in capturing different aspects of language understanding.", "section": "3 Identifying Phenomena of Interest"}, {"figure_path": "DpP5F3UfKw/tables/tables_19_1.jpg", "caption": "Table 6: Contingency Table for Human Responses", "description": "This table presents the results of a human evaluation experiment designed to validate the hypotheses generated by the LLM.  Participants were given a hypothesis and two sentences, one from each of the sentence groups (divergent and convergent), and were asked to indicate which sentence best aligned with the hypothesis. The table shows the number of responses in each of three categories (Prefer Divergent, Equal, Prefer Convergent) for the top 10 and bottom 10 hypotheses.", "section": "3.3 Manual Hypothesis Verification"}, {"figure_path": "DpP5F3UfKw/tables/tables_20_1.jpg", "caption": "Table 7: Summary of model performance", "description": "This table presents the best epoch and accuracy achieved by fine-tuned language models on the Social IQa and PiQA datasets, compared against baseline (random) accuracy.  It shows the performance of the models after fine-tuning on specific datasets for social/emotional intelligence and physical common sense.", "section": "5.3 Comparing Fine-tuned Models with the Base Model"}, {"figure_path": "DpP5F3UfKw/tables/tables_24_1.jpg", "caption": "Table 8: Summary of language-modeling loss across cross-validation folds for models on the remaining chapters of Harry Potter.", "description": "This table presents the average loss and standard deviation across three cross-validation folds for different language models trained on the remaining chapters of Harry Potter.  The models include a base model and models fine-tuned on either social or physical datasets. The loss values indicate the model's performance on a language modeling task, with lower values reflecting better performance.", "section": "5.3 Comparing Fine-tuned Models with the Base Model"}]