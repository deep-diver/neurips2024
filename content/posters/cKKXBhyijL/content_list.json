[{"type": "text", "text": "No-Regret Bandit Exploration based on Soft Tree Ensemble Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shogo Iwazaki Shinya Suzumura LY Corporation LY Corporation Tokyo, Japan Tokyo, Japan siwazaki@lycorp.co.jp ssuzumur@lycorp.co.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a novel stochastic bandit algorithm that employs reward estimates using a tree ensemble model. Specifically, our focus is on a soft tree model, a variant of the conventional decision tree that has undergone both practical and theoretical scrutiny in recent years. By deriving several non-trivial properties of soft trees, we extend the existing analytical techniques used for neural bandit algorithms to our soft tree-based algorithm. We demonstrate that our algorithm achieves a smaller cumulative regret compared to the existing ReLU-based neural bandit algorithms. We also show that this advantage comes with a trade-off: the hypothesis space of the soft tree ensemble model is more constrained than that of a ReLU-based neural network. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The stochastic bandit framework is a powerful tool for addressing sequential decision-making tasks in uncertain environments. A significant challenge in applying_ stochastic bandits is managing large action spaces. For example, in recommendation systems, there is often a vast action space generated by various combinations of users and items [38]. Standard algorithms designed for finitearmed bandits are inadequate in these scenarios. Consequently, numerous studies have focused on structurally modeling the reward process and using limited observed data to estimate rewards for unobserved actions. These approaches include algorithms that employ estimation methods such as linear models [3, 5, 12], kernel regression [11, 32], and neural networks [30, 41], which are referred to as linear bandit (LB), kernel bandit (KB), and neural bandit (NB) respectively. The effectiveness of these algorithms largely depends on the accuracy of the underlying reward models. Therefore, developing the bandit algorithms that leverage suitable reward estimation models is crucial. ", "page_idx": 0}, {"type": "text", "text": "Motivated by these considerations, this paper explores the stochastic bandit algorithm using tree ensembles, a model type that has gained popularity following neural networks but remains relatively underexplored in the bandit context. Specifically, we focus on the soft tree ensemble model, which has recently been the subject of both practical and theoretical investigations and has demonstrated strong empirical performance on tabular data [18, 21, 22, 25, 28]. Unlike hard trees, which update decision rules greedily and sequentially, soft trees employ gradient descent to update decision rules for the entire tree. This characteristic of soft trees facilitates the extension of existing analyses of NB and ensures a no-regret performance under suitable assumptions. ", "page_idx": 0}, {"type": "text", "text": "Related works.  In the field of stochastic bandits, prior research has established various structural assumptions about underlying rewards. For instance, the assumption of Lipschitz continuity of rewards is explored in Lipschitz bandits [8], linearity of rewards is examined in LB [3, 5, 12], and more generally, the assumption that rewards lie in a known reproducing kernel Hilbert space (RKHS) is studied in KB [11, 32]. ", "page_idx": 0}, {"type": "text", "text": "Our paper studies a type of bandit algorithm that employs a tree structure model, a topic with limited prior exploration. F\u00e9raud et al. [15] proposed a bandit algorithm using random forests, but the theory of their algorithm exhibits linear dependence on the number of actions, making it unsuitable for large action spaces. Elmachtoub et al. [14] introduced a Thompson sampling-style algorithm utilizing decision trees; however, their algorithm's construction relies on heuristics and does not provide a regretguarantee. ", "page_idx": 1}, {"type": "text", "text": "Additionally, our theory is closely related to NB. Zhou et al. [41] proposed an upper confidence bound (UCB) algorithm using a deep neural net (DNN) regressor, and Zhang et al. [40] extended this analysis to Thompson sampling. Their analysis yields a regret upper bound of $\\tilde{\\mathcal{O}}(\\tilde{d}\\sqrt{T})$ \uff0c where $\\tilde{d}$ denotes the effective dimension of the problem, and $\\tilde{\\mathcal{O}}(\\cdot)$ represents an order notation that ignores logarithmic dependence. However, generally, DNNs employing ReLU activation functions lead to $\\tilde{d}\\ =\\ \\tilde{O}(T^{(d-\\bar{1})/d})$ , resulting in super-linear growth of $\\mathcal{O}(\\tilde{d}\\sqrt{T})$ regret, which becomes meaningless [23]. Several studies address this issue by employing algorithms in the form of a sup-variant of UCB [37] or phased elimination-style algorithms [7, 26], proving a regret upper bound of $\\tilde{\\mathcal{O}}(T^{(2d-1)/(2d)})$ [23, 24, 30]. These studies combine theoretical analysis via the neural tangent kernel (NTK) [4, 19] for DNN regression with regret analysis techniques from KB, constructing algorithms and performing regret analysis. Our proposed algorithm can be seen as a generalization of NB theory using a soft-tree regressor from DNN. ", "page_idx": 1}, {"type": "text", "text": "Contributions.  Our contributions are as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u00b7 In Sec. 3.1, we introduce a new UCB-based algorithm: soft tree-based upper confidence bound (ST-UCB), which leverages the soft tree ensemble model. This algorithm can be considered an extension of the existing NN-UCB algorithm [41], incorporating the theory of the tree neural tangent kernel (TNTK) in soft trees [21, 22]. To our knowledge, this paper represents the first effort to extend the theory of NB to a tree-based structural model.   \n\u00b7 In Sec. 3.2, we derive several non-trivial properties of the soft tree ensemble model. These include the decay rates of eigenvalues of the TNTK (Lemma 3.1), concentration properties of TNTK (Lemma 3.2), and upper bounds on the spectral norm of the Hessian matrix (Lemma 3.3). Leveraging these results, we demonstrate that the ST-UCB algorithm achieves aregretof $\\tilde{\\mathcal{O}}(\\sqrt{T})$ under appropriate regularity conditions.   \n\u00b7 In Sec. 4, we elucidate the distinctions in properties and assumptions between the existing NN-UCB and ST-UCB algorithms. Specifically, while NN-UCB generally lacks a no-regret guarantee in general action (or context) spaces, ST-UCB consistently offers a no-regret guarantee across general action spaces. Additionally, we examine the relation between the hypothesis spaces induced by the TNTK and those induced by the NTK using ReLU activation. This comparison reveals that the hypothesis space derived from soft trees, although more constrained, may lead to lower regret. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Problem setting. We consider a sequential decision-making problem whose goal is to maximize the total reward under bandit feedback. Let $f:\\mathcal{X}\\to\\mathbb{R}$ be an unknown reward function, where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is a finite set of action candidates. At each time step $t$ , the environment reveals an action set $\\mathcal X_{t}\\subset\\mathcal X$ thereafter, the learner chooses an action $\\pmb{x}_{t}$ and receives the corresponding reward $y_{t}=f(\\pmb{x}_{t})+\\epsilon_{t}$ where $\\epsilon_{t}$ is a noise random variable whose mean is zero. As a performance metric, we adopt the pseudo cumulative regret $\\begin{array}{r}{R_{T}:=\\sum_{t=1}^{T}\\left[f(\\pmb{x}_{t}^{*})-f(\\pmb{x}_{t})\\right]}\\end{array}$ where $\\pmb{x}_{t}^{*}\\in$ arg $\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}_{t}}f(\\pmb{x})$ . In our problem setup, the action set $\\textstyle{\\mathcal{X}}_{t}$ is allowed to change at each step $t$ . In addition to the standard bandit setup that assumes $\\mathcal X_{t}=\\mathcal X$ , this formulation includes a contextual bandit setup by seting $\\mathcal{X}_{t}=\\{(c_{t},\\mathbf{\\bar{a}})\\mid a\\in\\mathcal{A}(c_{t})\\}$ , where $\\mathbf{}c_{t}$ is a context vector at step $t$ and $\\boldsymbol{A}(\\boldsymbol{c}_{t})$ is the corresponding action set. ", "page_idx": 1}, {"type": "text", "text": "Soft tree ensemble.At each time step $t$ , our algorithm constructs a soft tree-based estimator of the reward function $f$ . We describe the definition of soft trees based on Kanoh and Sugiyama [21]. Now, let us consider $M\\in\\ensuremath{\\mathbb{N}}_{+}$ perfect binary trees whose depths are $\\mathcal{D}\\in\\mathbb{N}_{+}$ . Note that each tree has ${\\mathcal{N}}:=2^{\\mathcal{D}}-1$ internal nodes and $\\mathcal{L}:=2^{\\mathcal{D}}$ leaf nodes. Furthermore, for technical reasons, we assume that $M$ is an even number Let $\\pmb{w}_{n}^{(m)}\\in\\mathbb{R}^{d}$ and $\\pi_{l}^{(m)}\\in\\mathbb{R}$ be te parametersof the $n$ th intermal nd -th leaf node of the -th tree, respectively. We index these parameters according to breadth-first ordering, as described in the left plot of Fig. 1. Moreover, we also denote all internal and leaf node parameters as s(m) = (w(m)T $\\pmb{w}^{(m)}:=(\\pmb{w}_{1}^{(m)\\top},\\dots,\\pmb{w}_{N}^{(m)\\top})^{\\top}\\in\\mathbb{R}^{N d}$ (m))T \u2208 IRNd and \u03c0(m) :=(\u03c0(m).,)i (m)T \u2208 R<. The output of a standard decision tree is obtained as the parameter of some leaf node, which is chosen deterministically based on the hard-splitting rules of internal nodes. On the other hand, the output of the soft tree is given by replacing the hard-splitting operation of the standard decision tree with a probabilistic one. Specifically, given parameters $\\pmb{\\theta}^{\\stackrel{\\vee}{(m)}}:=(\\pmb{w}^{(m)\\top},\\pmb{\\pi}^{(m)\\top})^{\\top}$ and any input $x\\in\\mathcal{X}$ \uff0c the corresponding output $\\tilde{h}(x;\\pmb\\theta^{(m)})$ of the $m$ -th soft tree is defined as ", "page_idx": 1}, {"type": "image", "img_path": "cKKXBhyijL/tmp/76f5db0b197348dbc9e98a1828faa74a78bbf357abed1e230f6ff3f5bda0f2ff.jpg", "img_caption": ["Figure 1: An illustrative image of a soft tree structure with $\\mathcal{D}\\,=\\,3$ . As shown in the left plot, we have $\\mathcal{N}:=\\,2^{\\mathcal{D}}\\,-\\,1$ internal nodes (green) and ${\\mathcal{L}}\\,:=\\,2^{D}$ leaf nodes (orange), indexed using breadth-first ordering. The right plot shows an illustrative example where a soft tree calculates the weight probabilities $p_{l}(\\cdot)$ for the leaf nodes. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{h}(\\mathbf{x};\\theta^{(m)})=\\sum_{l=1}^{\\mathcal{L}}\\pi_{l}^{(m)}p_{l}(\\mathbf{x};w^{(m)}),\\;\\mathrm{~where~}\\;p_{l}(\\mathbf{x};w)=\\prod_{n=1}^{\\mathcal{N}}\\sigma(w_{n}^{\\top}x)^{\\|_{l<r^{n}}}\\left[1-\\sigma(w_{n}^{\\top}x)\\right]^{\\|_{n\\setminus\\mathcal{N}}|}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\mathbb{I}_{l\\times\\mathit{r}}{}_{n}$ and $\\mathbb{I}_{n\\setminus{l}}$ are indicator functions. If the $l$ -th leaf node belongs to the left (resp. right) sub-tree whose root is the $n$ -th internal node, $\\mathbb{I}_{l\\times\\mathscr{n}}$ (resp. $\\mathbb{I}_{n\\setminus l})$ is one; otherwise, zero. Furthermore, $\\sigma(\\cdot):\\mathbb{R}\\rightarrow[0,1]$ is a soft decision function. The right plot of Fig. 1 shows an illustrative image of the calculation of $p_{l}(\\cdot)$ Aswithutale $\\begin{array}{r}{\\sigma({\\pmb w}_{n}^{\\top}{\\pmb x}):=\\frac{1}{2}{\\operatorname{erf}}(\\alpha{\\pmb w}_{n}^{\\top}{\\pmb x})+\\frac{1}{2}}\\end{array}$ with some pre-secifed scaling paramte $\\alpha\\geq0$ where $\\begin{array}{r}{\\operatorname{erf}(b)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{b}\\exp(-z^{2}){\\bf d}z}\\end{array}$ for any $b\\in\\mathbb{R}$ By aggregating $M$ soft trees, the whole output $h(x;\\theta)$ of the soft tree ensemble model is defined as $\\begin{array}{r}{h(\\pmb{x};\\pmb{\\theta})=\\sum_{m=1}^{M}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)})/\\sqrt{M}}\\end{array}$ where $\\pmb{\\theta}:=(\\pmb{\\theta}^{(1)\\top},\\dots,\\pmb{\\theta}^{(M)\\top})^{\\top}\\in\\mathbb{R}^{M(d N+\\mathcal{L})}$ Under the model structures as described above, the training of the model parameters $\\pmb{\\theta}$ is conducted based on the gradient descent optimizer, which aims to minimize some pre-specified loss functions. In our algorithm, we adopt a regularized square loss, whose detailed definition is given in Sec. 3.1. ", "page_idx": 2}, {"type": "text", "text": "Neural tangent kernel theory for overparameterized model. The neural tangent kernel (NTK) [19] is an effective theoretical tool for understanding the learning properties of overparameterized neural networks. Let $h_{\\mathrm{NN}}(\\cdot;\\pmb{\\theta})\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ be a feed-forward neural network with a ReLU activation function, $L$ hidden layers whose width is $M$ , and network parameters $\\pmb{\\theta}$ Given any fixed inputs $\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}}\\,\\in\\,\\mathbb{R}^{d}$ , and $\\tilde{\\pmb{\\theta}}_{0}\\,\\sim\\,\\mathcal{N}({\\bf0},{\\bf I})$ , it has been shown that the inner product $\\langle\\nabla_{\\theta}h_{\\mathrm{NN}}(\\mathbf{\\boldsymbol{x}};\\tilde{\\theta}_{0}),\\nabla_{\\theta}h_{\\mathrm{NN}}(\\tilde{\\mathbf{\\boldsymbol{x}}};\\tilde{\\theta}_{0})\\rangle$ of gradients converges to a fixed kernel function $k_{\\mathrm{NTK}}(\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}})$ (i.e., $\\langle\\nabla_{\\theta}h_{\\mathrm{NN}}(x;\\tilde{\\theta}_{0}),\\nabla_{\\theta}h_{\\mathrm{NN}}(\\tilde{x};\\tilde{\\theta}_{0})\\rangle\\;\\stackrel{p}{\\to}\\;k_{\\mathrm{NTK}}(x,\\tilde{x})$ as $M\\to\\infty)$ 0. The kernel function $k_{\\mathrm{NTK}}$ is called the NTK. Moreover, in the overparameterized regime, $h_{\\mathrm{NN}}(x;\\theta)$ trained with gradient descent with an infinitesimally small learning rate coincides with the kernel ridge-less regressor $h_{\\mathrm{NTK}}(x)$ \uff0c whose kernel function is $k_{\\mathrm{NTK}}$ [4]. This property motivates us to analyze NB problems by bridging original NB to KB problems whose underlying kernel function is the NTK. Indeed, some existing works [23, 24, 30, 41] show the regret upper bound of NB problems by carefully combining NTK theory with existing theoretical tools of KB. In our paper, we consider soft tree variants of these existing works. ", "page_idx": 2}, {"type": "text", "text": "Recently, Kanoh and Sugiyama [21] generalized the NTK theory to the soft tree ensemble model. Let $g(x,\\pmb\\theta):=\\nabla_{\\pmb\\theta}h(\\pmb x;\\pmb\\theta)\\in\\mathbb{R}^{p}$ be the gradient vector of the soft tree ensemble model at parameter ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 The soft tree-based upper confidence bound (ST-UCB) algorithm ", "page_idx": 3}, {"type": "text", "text": "Input: $\\mathcal{X}\\subset\\mathbb{S}^{d-1}$ \uff0c $\\mathcal{D}\\in\\mathbb{N}_{+}$ \uff0c $J\\in\\mathbb{N}_{+}$ \uff0c $\\eta>0$ \uff0c $\\rho>0$ \uff0c $\\alpha>0$ $M\\in\\mathbb{N}_{+}$ \uff0c $T\\in\\mathbb{N}_{+},\\beta>0$   \n1: Initialize $\\theta_{0}$ randomly as described in Sec. 3.1.   \n2: Define $G_{0}=\\mathbf{0}\\in\\mathbb{R}^{p}$   \n3: for $t=1,\\dots,T$ do   \n4:Obtain $\\mathbf{\\mathcal{X}}_{t}$   \n5:Calculate $\\tilde{\\sigma}_{t-1}^{2}(\\pmb{x}):=\\pmb{g}(\\pmb{x};\\pmb{\\theta}_{0})^{\\top}\\left(\\pmb{I}_{p}+\\rho^{-1}\\pmb{G}_{t-1}\\pmb{G}_{t-1}^{\\top}\\right)^{-1}\\pmb{g}(\\pmb{x};\\pmb{\\theta}_{0})$ on $\\scriptstyle{\\mathcal{X}}_{t}$   \n6: $\\mathbf{\\omega}_{t}\\gets\\arg\\operatorname*{max}\\ \\left[h(\\pmb{x};\\pmb{\\theta}_{t-1})+\\beta\\tilde{\\sigma}_{t-1}(\\pmb{x})\\right]$   \n$\\mathbf{\\boldsymbol{x}}{\\in}\\mathbf{\\boldsymbol{\\mathcal{X}}}_{t}$   \n7: Obtain $y_{t}=f(\\pmb{x}_{t})+\\epsilon_{t}$   \n8: $\\theta_{t}\\gets\\mathrm{TrainST}(t,\\theta_{0},(\\mathbf{x}_{i},y_{i})_{i\\in[t]},J,\\eta,\\rho,\\mathcal{D},\\alpha,m).$   \n9: Define $G_{t}=[{\\pmb g}({\\pmb x}_{1};{\\pmb\\theta}_{0}),\\dots,{\\pmb g}({\\pmb x}_{t};{\\pmb\\theta}_{0})]\\in\\mathbb{R}^{p\\times t}$   \n10: end for ", "page_idx": 3}, {"type": "text", "text": "Algorithm 2 TrainST $(t,\\pmb{\\theta}_{0},(\\pmb{x}_{i},y_{i})_{i\\in[t]},J,\\eta,\\mathcal{D},\\alpha,M)$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: $\\pmb{\\theta}_{t;0}\\gets\\pmb{\\theta}_{0}$   \n2: for $j=1,\\dots,J$ do   \n3:  Calculate gradient of $\\begin{array}{r}{L_{t}(\\pmb{\\theta}_{t;j-1}):=\\sum_{i=1}^{t}\\left[h(\\pmb{x}_{i};\\pmb{\\theta}_{t;j-1})-y_{i}\\right]^{2}+\\rho\\|\\pmb{\\theta}_{t;j-1}-\\pmb{\\theta}_{0}\\|_{2}^{2}.}\\end{array}$   \n4: Update parameter: $\\pmb{\\theta}_{t;j}\\leftarrow\\pmb{\\theta}_{t;j-1}-\\eta\\nabla_{\\pmb{\\theta}}L_{t}(\\pmb{\\theta}_{t;j-1})$ \uff1a   \n5: end for   \n6: return $\\pmb{\\theta}_{t;J}$ ", "page_idx": 3}, {"type": "text", "text": "$\\pmb{\\theta}\\in\\mathbb{R}^{p}$ ,where $p\\,:=\\,M(d{\\mathcal N}+{\\mathcal L})$ denotes the total number of parameters. Then, given fixed inputs $\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}}\\in\\mathcal{X}$ and $\\tilde{\\pmb{\\theta}}_{0}\\sim\\mathcal{N}(0,\\pmb{I}_{p})$ , the inner product $\\langle g(x,\\tilde{\\pmb{\\theta}}_{0}),g(\\tilde{x},\\tilde{\\pmb{\\theta}}_{0})\\rangle$ has also been shown to converge in probability to some kernel function $k_{\\mathrm{TNTK}}(x,\\tilde{\\pmb{x}})$ as the number of ensemble models $M$ grows infinitely (see Theorem 1 in [21]). This limiting kernel $k_{\\mathrm{TNTK}}$ is called the tree neural tangent kernel (TNTK) as an analogy to the NTK and is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nk_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})=2^{\\mathcal{D}}\\pmb{x}^{\\top}\\tilde{\\pmb{x}}(\\mathcal{T}(\\pmb{x},\\tilde{\\pmb{x}}))^{\\mathcal{D}-1}\\dot{\\mathcal{T}}(\\pmb{x},\\tilde{\\pmb{x}})+(2\\mathcal{T}(\\pmb{x},\\tilde{\\pmb{x}}))^{\\mathcal{D}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle{T(x,\\tilde{x})=\\frac{1}{2\\pi}\\arcsin\\left(\\frac{\\alpha^{2}x^{\\top}\\tilde{x}}{\\sqrt{(\\alpha^{2}x^{\\top}x+0.5)(\\alpha^{2}\\tilde{x}^{\\top}\\tilde{x}+0.5)}}\\right)+\\frac{1}{4},}}\\\\ {\\displaystyle{\\dot{T}(x,\\tilde{x})=\\frac{\\alpha^{2}}{\\pi}\\frac{1}{\\sqrt{(1+2\\alpha^{2}x^{\\top}x)(1+2\\alpha^{2}\\tilde{x}^{\\top}\\tilde{x})-4\\alpha^{4}(x^{\\top}\\tilde{x})^{2}}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It should be noted that even if we follow the existing NTK-based techniques of NB, generalizing the result of Kanoh and Sugiyama [21] to the analysis of sequential decision-making tasks is non-trivial. Specifically, the existing analysis of NB heavily relies on the following results of ReLU-based NTK: i) non-asymptotic bounds of NTK [4], i) the spectral properties of the Hessian matrix around the initial model parameters [27], and ii) the upper bounds of maximum information gain (MIG) of NTK [35], which measure the complexity of the KB problem depending on the underlying kernel. These results are unique to DNN architectures with a ReLU-based activation function and are not applicable to the soft tree ensemble model. ", "page_idx": 3}, {"type": "text", "text": "3   UCB strategy based on soft tree ensemble model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1  Proposed algorithm: ST-UCB ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The pseudo-code of our proposed algorithm, soft tree-based UCB (ST-UCB), is shown in Algorithm 1. ST-UCB is interpreted as the soft tree-based variant of NN-UCB [41]. We summarize each part of ST-UCBbelow. ", "page_idx": 3}, {"type": "text", "text": "Initialization. ST-UCB first chooses the initial parameter $\\pmb{\\theta}_{0}~\\in~\\mathbb{R}^{p}$ for the gradient descent method as follows. Let $\\theta_{\\mathrm{base}}\\,\\sim\\,\\mathcal{N}(0,I_{p/2})$ be a base initial parameter, with $\\boldsymbol{p}\\,\\stackrel{-}{=}\\,M(d{\\cal N}+{\\cal{L}})$ Using $\\theta_{\\mathrm{base}}$ , we set the initial parameters $\\pmb{\\theta}_{0}$ as $\\pmb{\\theta}_{0}\\;\\;=\\;\\;(\\pmb{\\theta}_{0+}^{\\top},\\pmb{\\theta}_{0-}^{\\top})^{\\top}$ ,where $\\pmb{\\theta}_{0+}\\in\\mathbb{R}^{p/2}$ and $\\theta_{0-}\\ \\in\\ \\mathbb{R}^{p/2}$ are defined as $\\pmb{\\theta}_{0+}\\;=\\;(\\pmb{w}_{\\mathrm{base}}^{(1)\\top},\\pmb{\\pi}_{\\mathrm{base}}^{(1)\\top},\\dots,\\pmb{w}_{\\mathrm{base}}^{(M/2)\\top},\\pmb{\\pi}_{\\mathrm{base}}^{(M/2)\\top})^{\\top}$ (M/2)T , \u03c0(M/2)T)T and 0o- = $({\\pmb w}_{\\mathrm{base}}^{(M/2+1)\\top},-{\\pmb\\pi}_{\\mathrm{base}}^{(M/2+1)\\top},\\dots,{\\pmb w}_{\\mathrm{base}}^{(M)\\top},-{\\pmb\\pi}_{\\mathrm{base}}^{(M)\\top})^{\\top}$ , respectively. This initialization procedure ensures that the initial model output is O (i.e., $\\tilde{h(\\mathbf{x};\\theta_{0})}=0$ for all $\\pmb{x}\\in\\mathcal{X}$ ), which is essential for our theoretical analysis. ", "page_idx": 4}, {"type": "text", "text": "Learning.  At each step $t$ , ST-UCB learns the model parameter $\\pmb{\\theta}_{t}$ based on a regularized squared loss $\\begin{array}{r}{L_{t}(\\pmb\\theta):=\\sum_{i=1}^{t}(h(\\pmb x_{i};\\pmb\\theta)-y_{i})^{2}+\\rho\\|\\pmb\\theta-\\pmb\\theta_{0}\\|_{2}^{2}}\\end{array}$ where $\\rho>0$ is a regularization parameter. ", "page_idx": 4}, {"type": "text", "text": "UCB-based selection of $\\pmb{x}_{t}$ .At each step $t$ , ST-UCB selects $\\pmb{x}_{t}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}\\in\\arg\\operatorname*{max}_{\\pmb{x}_{t}}[h(\\pmb{x};\\pmb{\\theta}_{t-1})+\\beta\\tilde{\\sigma}_{t-1}(\\pmb{x})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\sigma}_{t-1}^{2}(\\pmb{x})=\\pmb{g}(\\pmb{x};\\pmb{\\theta}_{0})^{\\top}\\left(\\pmb{I}_{p}+\\rho^{-1}\\pmb{G}_{t-1}\\pmb{G}_{t-1}^{\\top}\\right)^{-1}\\pmb{g}(\\pmb{x};\\pmb{\\theta}_{0})$ with $g(x;\\pmb\\theta):=\\nabla_{\\pmb\\theta}h(\\pmb x;\\pmb\\theta)\\in\\mathbb{R}^{p}$ and $G_{t-1}:=(\\pmb{g}(\\pmb{x}_{1};\\pmb{\\theta}_{0}),\\dots,\\pmb{g}(\\pmb{x}_{t-1};\\pmb{\\theta}_{0}))\\in\\mathbb{R}^{p\\times t}$ In ST-UCB, the quantity $\\tilde{\\sigma}_{t-1}^{2}(x)$ quantifies the uncertainty of the model output $h(x;\\pmb\\theta_{t})$ and is essential for the construction of confidence bounds. Furthermore, the quantity $\\tilde{\\sigma}_{t-1}^{2}(\\dot{\\pmb x})$ is interpreted as the predictive variance of a Bayesian linear regression whose feature map is the gradient of the initial model output $h(x;\\theta_{0})$ .We note that a similar quantity is leveraged in existing NB algorithms [23, 30, 41]. ", "page_idx": 4}, {"type": "text", "text": "3.2  Theory of ST-UCB ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Assumptions for theoretical analysis. We make the following assumptions for our theory: ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1. (i) The output noise $\\epsilon_{t}$ is conditionally $\\sigma$ -sub-Gaussian for some $\\sigma>0$ Specifically, $\\mathbb{E}[\\exp(\\lambda\\epsilon_{t})\\mid\\mathcal{H}_{t-1}]\\le\\exp(\\lambda^{2}\\sigma^{2}/2)$ holds for any $t\\in[T]:=\\{1,\\ldots,T\\}$ and any history $\\mathcal{H}_{t-1}:=$ $(\\pmb{x}_{1},y_{1},\\dots,\\pmb{x}_{t-1},y_{t-1})$ . (i) The input space $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is a subset of the hyper-sphere ${\\mathbb S}^{d-1}:=\\{{\\pmb x}\\in$ $\\mathbb{R}^{d}\\mid\\|\\pmb{x}\\|_{2}=1\\}$ (ii) The underlying reward function $f$ is an element of the RKHS corresponding to $k_{T N T K}$ where krnTk is the TNTK induced by the same soft tree structure used in ST-UCB. $(i\\nu)$ The RKHS norm of $f$ is bounded by a known constant $B<\\infty$ . That is, $\\|f\\|_{\\mathrm{TNTK}}\\le B$ holds, where $\\|\\cdot\\|_{\\mathrm{TNTK}}$ denotes the RKHS norm corresponding to $k_{T N T K}$ ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. In Assumption 3.1, (i) is the standard assumption for the stochastic bandit problem and is quite mild.For example,Bernoulli, Gaussian,and any bounded reward models are included in this assumption.Assumption $(i i)$ is often assumed in existing NB literature [23, 24, 30, 40, 41] and holds without loss of generality by transforming the original input space through a bijection map. For example, given any original input space $\\tilde{\\mathcal{X}}\\subset\\mathbb{R}^{d}$ we can construct a new input space $\\mathcal{X}$ on the hyper sphere $\\mathbb{S}^{d}$ as $\\mathcal{X}\\,=\\,\\left\\{\\left(\\bar{l}^{-1}\\tilde{\\pmb{x}}^{\\top},(1-\\|\\tilde{\\pmb{x}}\\|_{2}^{2}\\bar{l}^{-2})^{1/2}\\right)^{\\top}\\,|\\,\\tilde{\\pmb{x}}\\in\\tilde{\\mathcal{X}}\\right\\}\\,\\subset\\,\\mathbb{S}^{d}$ , where $\\bar{l}\\,=\\,\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in\\tilde{\\mathcal{X}}}\\,\\|\\tilde{\\mathbf{x}}\\|_{2}$ Assumptions (ii) and $(i\\nu)$ are similar to those in existing NB works [23, 24, 30]. The only difference is that we use TNTK instead of NTK to define the hypothesis space (RKHS) to which $f$ belongs. We omit the basic definition and properties of RKHS; see, e.g., [20] for details. In Sec. 4, we further discuss therelationship between theRKHSs corresponding toNTK and TNTK. ", "page_idx": 4}, {"type": "text", "text": "Similar to NB with ReLU, our theoretical guarantees rely on two crucial tools in the context of KB. The first is the maximum information gain (MIG) [32], which quantifies the complexity of the problem in the context of kernel-based sequential decision-making tasks. MIGs depend on the underlying kernels, and their upper bounds have been provided when using well-known kernels, including the NTK corresponding to NNs with ReLU [23, 35, 36]. We show the upper bound of MIG when the underlying kernel is TNTK. The second tool is the confidence bound. Constructing valid confidence bounds is crucial for obtaining meaningful regret bounds in stochastic bandit algorithms. These two elements are not only essential for the theoretical analysis of ST-UCB but also of independent interest in general sequential decision-making problems. Hereafter, we present our MIG and confidence bounds results for our ST-UCB algorithm, concluding with the regret upper bound for ST-UCB. ", "page_idx": 4}, {"type": "text", "text": "Maximum information gain (MIG) of TNTK. Let us define the quantity $\\gamma_{T}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma_{T}=\\frac{1}{2}\\operatorname*{max}_{x_{1},\\ldots,x_{T}\\in\\mathcal{X}}\\ln\\operatorname*{det}\\left(I_{T}+\\rho^{-1}K_{T}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $K_{T}$ is the $T\\times T$ kernel matrix whose $(i,j)$ -th entry is $k_{\\mathrm{TNTK}}(\\pmb{x}_{i},\\pmb{x}_{j})$ . This $\\gamma_{T}$ is called the maximum information gain (MIG) since the quantity $0.5\\ln\\operatorname*{det}(I_{T}+\\rho^{-1}K_{T})$ is equal to the information gain from $T$ observations in a Gaussian process regression model, characterized by the covariance function $k_{\\mathrm{TNTK}}$ and the noise variance parameter $\\rho$ [32]. The following Theorem 3.1 is our main result about MIG, which shows that $\\gamma_{T}$ grows logarithmically. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Upper bound of MIG of TNTK). Fix any $\\alpha\\in(0,\\infty)$ $d\\geq2$ $\\mathcal{D}\\in\\mathbb{N}_{+}$ and $\\mathcal{X}\\subset\\mathbb{S}^{d-1}$ Then, $\\gamma_{T}=\\mathcal{O}(\\ln^{d}T)$ . Here, the implied constant depends on $d,\\,\\alpha,$ and $\\mathcal{D}$ ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 3.1 is given in Appendix A.2. The analysis of MIG is well-studied in existing KB literature [32, 36]. The key component to quantify the upper bound of MIG is the decaying rate of the eigenvalues of the underlying kernel. The following lemma gives the decay rate of TNTK eigenvalues, which plays a central role in the proof of Theorem 3.1. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1 (Eigendecomposition of TNTK). Fix any $d\\geq2$ $\\alpha\\in(0,\\infty)$ , and $\\mathcal{D}\\in\\mathbb{N}_{+}$ .Furthermore, let us define $N_{d,n}$ $\\begin{array}{r}{N_{d,n}\\,=\\,\\frac{2n+d-2}{n}\\,\\binom{n+d-3}{d-2}}\\end{array}$ for any $n\\,\\in\\,\\mathbb{N},$ where $\\begin{array}{r}{\\binom{a}{b}:=\\frac{a!}{b!(a-b)!}}\\end{array}$ b!(a-b)! is a binomial coefficient. Then, for any $\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ , the TNTK corresponding to $\\alpha$ and $\\mathcal{D}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nk_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})=\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}\\lambda_{n}Y_{n,j}(\\pmb{x})Y_{n,j}(\\tilde{\\pmb{x}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(\\lambda_{n})_{n\\in\\mathbb{N}}$ and $(Y_{n,j})_{n\\in\\mathbb{N},j\\in[N_{d,n}]}$ areeigenvaluesandeigenfuctionsof theintegralpratr of) TNTK that satisfy $\\lambda_{0}\\geq\\lambda_{1}\\geq\\cdots\\geq0.$ In addition, for any $n\\in\\mathbb{N}$ the eigenvalue $\\lambda_{n}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{n}\\leq C_{\\alpha,{\\cal D}}^{(1)}\\exp\\left(-n{\\cal D}\\ln\\left(1+\\frac{1}{4\\alpha^{2}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where C(1) $C_{\\alpha,{\\cal D}}^{(1)}>0$ is a constant, which depends on $\\alpha$ and $\\mathcal{D}$ ", "page_idx": 5}, {"type": "text", "text": "Remark 3.2. The eigenfunctions $(Y_{n,j})_{j\\in[N_{d,n}]}$ are known as spherical harmonics of degree n with multiplicity $N_{d,n}$ (see, e.g., [13]). Furthermore, on the hyper-sphere $\\mathbb{S}^{d-1}$ ,thekernelsthathave rotationally invariant form can be represented in the form of Eq. (6). TNTK and NTK with ReLU activation function areincluded in the rotationally invariant class of kernels; therefore,NTK can also be decomposed as Eq. (6) [35], while corresponding eigenvalues differ from those of TNTK. ", "page_idx": 5}, {"type": "text", "text": "The proof of Lemma 3.1 is given in Appendix A.1. Lemma 3.1 demonstrates the exponential eigenvalue decay of TNTK, in contrast to the polynomial eigenvalue decay of NTK with ReLU activation [6, 35]. This difference leads to faster convergence of ST-UCB compared to NN-UCB, albeit with a smaller corresponding RKHS of TNTK. We discuss more details in Sec. 4. ", "page_idx": 5}, {"type": "text", "text": "Confidence bound. The following shows the confidence bounds for the soft tree-based model. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Confidence bounds based on the soft tree ensemble model). Suppose Assumption 3.1 holds. Fix any $\\delta\\in(0,1)$ \uff0c $\\rho>0$ $\\alpha\\geq1$ and $\\mathcal{D}\\geq2$ Let $K_{\\mathrm{TNTK}}(\\chi):=[k_{\\mathrm{TNTK}}(x,\\tilde{x})]_{x,\\tilde{x}\\in\\mathcal{X}}\\in$ $\\mathbb{R}^{|\\mathcal{X}|\\times|\\mathcal{X}|}$ and $\\lambda_{0}\\,=\\,\\lambda_{\\mathrm{min}}(K_{\\mathrm{TNTK}}(\\chi))>0$ be thekernelmatrix over $\\mathcal X\\times\\mathcal X$ and the minimum eigenvalue of $K_{\\mathrm{TNTK}}(\\mathcal{X})$ ,respectively.If the number of soft tree ensemblemodels $M$ is sufficiently large to satisfy $M\\ge\\mathrm{Poly}(T,\\rho^{-1},B,\\alpha,2^{\\mathcal{D}},\\lambda_{0}^{-1},|\\mathcal{X}|,\\ln(1/\\delta))$ andthelearningrate $\\eta$ satisfies $\\eta\\leq\\mathcal{O}((T^{2}2^{4\\mathcal{D}}\\alpha^{2}\\ln(M/\\delta)+\\rho)^{-1})$ ,then, the following event holds with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall t\\in[T],\\forall x\\in\\mathcal{X},|f(x)-h(x;\\pmb{\\theta}_{t-1})|\\leq\\mathcal{O}\\left(\\frac{T^{2}(\\ln T)^{2}(\\ln M)}{\\sqrt{M}}\\right)+\\beta\\widetilde{\\sigma}_{t-1}(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta=\\mathcal{O}\\left(\\sqrt{\\gamma_{T}+\\frac{T^{3/2}}{M^{1/2}}}+\\frac{T^{3}(\\ln T)(\\ln M^{3/2})}{\\sqrt{M}}+T^{3/2}(\\ln T)(\\ln M)(1-2\\eta\\rho)^{J/2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 3.3. The minimum eigenvalue $\\lambda_{0}$ of the kernel matrix of TNTK is guaranteed to be strictly positive if $\\mathcal{X}\\subset\\mathbb{S}^{d-1}$ SeeProposition $I$ in [21]. ", "page_idx": 5}, {"type": "text", "text": "We provide the proof of Theorem 3.2 in Appendix B.3 with the precise conditions about $M$ and the dependence of constant factors. Our proof strategy for Theorem 3.2 follows the existing analysis of confidence bounds in NB works; however, the application of their proof techniques to the soft tree regressor is not straightforward. Specifically, the existing proof of the confidence bounds in NB depends on the concentration results of NTK (Theorem 3.1 in [4]), and the spectral norm bounds of the Hessian matrix of NN (Theorem 3.2 in [27]). To prove Theorem 3.2, we provide the following soft tree versions of their results. ", "page_idx": 6}, {"type": "text", "text": "Lemma2 Concentraton to TTK) Fi ay $\\textbf{\\em x}$ $\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ \uff0c $\\delta\\in(0,1)$ and $\\epsilon\\in(0,C_{\\alpha,\\mathcal{D}}^{(2)})$ with $C_{\\alpha,{\\cal D}}^{(2)}=2^{2{\\cal D}+2}\\alpha^{2}C$ f $M\\ge\\tilde{C}\\operatorname*{max}\\{C_{\\alpha,{\\cal D}}^{(2)2},2^{2\\mathcal{D}}\\}\\epsilon^{-2}\\ln(16/\\delta)$ then, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(|k_{\\mathrm{TNTK}}(\\pmb{x},\\pmb{\\tilde{x}})-\\langle\\pmb{g}(\\pmb{x},\\pmb{\\theta}_{0}),\\pmb{g}(\\pmb{\\tilde{x}},\\pmb{\\theta}_{0})\\rangle\\,|\\le4\\epsilon)\\ge1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\theta_{0}$ is the initial parameter of ST-UCB, and $C$ $\\tilde{C}>0$ are absolute constants. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.3 (Spectral norm upper bound). For any $\\delta\\in(0,1)$ and $\\alpha\\geq1$ , with probability at least $1-\\delta$ the following holds for any $R>0$ $\\pmb\\theta\\in\\mathbb{R}^{p}$ and $\\pmb{x}\\in\\mathbb{S}^{d-1}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}-\\pmb{\\theta}_{0}\\|_{2}\\leq R\\Rightarrow\\|\\pmb{H}(\\pmb{x},\\pmb{\\theta})\\|\\leq\\frac{C_{\\alpha,\\pmb{D}}^{(3)}(R+\\sqrt{2})^{2}}{\\sqrt{M}}\\ln\\frac{2^{\\pmb{D}+2}M}{\\delta},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where H(x,0) := V2h(x;0) E Rpxp is the Hessian matrix of the model output, and C ${\\sqrt{6}}\\alpha^{2}2^{2{\\cal D}}$ .Furthermore,for any $A\\in\\mathbb{R}^{p\\times p}$ $\\begin{array}{r}{\\lceil{\\boldsymbol{\\mathsf{A}}}\\rceil\\|:=\\operatorname*{max}_{z\\in\\mathbb{S}^{p-1}}\\|{\\boldsymbol{\\cal{A}}}z\\|_{2}}\\end{array}$ denotes the spectral norm. ", "page_idx": 6}, {"type": "text", "text": "The proofs of Lemma 3.2 and Lemma 3.3 are given in Appendix B. By carefully combining Lemma 3.2 and Lemma 3.3 with the existing proof strategy of NB, we derive Theorem 3.2. The overview of the proof is summarized in Appendix B.3.1. ", "page_idx": 6}, {"type": "text", "text": "Regret upper bound of ST-UCB.  By combining Theorem 3.1 and Theorem 3.2 with the standard proof technique of the kernelized UCB algorithm, we obtain the $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret upper bound for ST-UCB as stated in the following theorem. The proof is provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3. Suppose that Assumption 3.1 holds. Fix any $\\delta\\in(0,1)$ $\\alpha\\geq1,$ $\\rho>0$ and $\\mathcal{D}\\geq2$ Furthermore, assume that the confidence width parameter $\\beta$ satisfies Eq. (9). If the number of soft tree ensemble models $M$ and the total step size $J$ of the gradient descent are sufficiently large to satisfy $M\\ge\\mathrm{Poly}(T,\\rho^{-1},B,\\alpha,2^{\\mathcal{D}},\\lambda_{0}^{-1},|\\mathcal{X}|,\\ln(\\bar{1}/\\delta))$ and the learning rate $\\eta$ satisfies $\\eta\\leq\\mathcal{O}((T^{2}2^{4\\mathcal{D}}\\alpha^{2}\\ln(M/\\delta)+\\rho)^{-1})$ , then, the following holds with probability at least $1-\\delta$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}\\leq1+\\left(\\sqrt{2}B+1+\\frac{\\sigma}{\\sqrt{\\rho}}\\sqrt{2\\left(\\gamma_{T}+1+\\ln\\frac{6}{\\delta}\\right)}\\right)\\sqrt{\\frac{8T(\\gamma_{T}+1)}{\\ln(1+\\rho^{-2})}}=\\mathcal{O}\\left(\\sqrt{T}\\ln^{d}T\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4  Comparison of NN-UCB and ST-UCB ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparison of regret. In the existing NN-UCB algorithm [41], a regret upper bound of $\\mathcal{O}(\\tilde{d}\\sqrt{T})$ is provided, where d represents the effective dimension of ReLU-based NTK. It is generally known that the worst-case bound of the effective dimension and MIG are equivalent up to logarithmic dependencies [37]. Considering the upper bound on MIG of NTK, $\\bar{\\gamma}_{T}^{(\\mathrm{NTK})}\\,=\\,\\bar{\\tilde{\\mathcal{O}}}(T^{(d-\\bar{1})/d})$ [23, 35trUCm $\\tilde{\\mathcal{O}}(T^{(d-1)/d+1/2})(=\\tilde{\\mathcal{O}}(\\gamma_{T}^{(\\mathrm{NTK})}\\sqrt{T}))$ This resuts i a superlinear regret, and meaningful guarantees for NN-UCB are not achievable without further restricted assumptions on the input set $\\scriptstyle{\\mathcal{X}}_{t}$ (e.g., see the discussion in Appendix $\\mathrm{D}$ in [40]). To address these issues in a general setting, it is necessary to construct more complex algorithms that incorporate concepts such as a sup-variant of UCB [23, 30] or phased elimination [24], yielding a regret upper bound of O(VT $\\mathcal{O}(\\sqrt{\\gamma_{T}^{\\mathrm{(NTK)}}T})$ NTK)T). In contrast, due to Theorem 3.1, the MIG of TNTK \\~(TNTK) diverges on a logarithmic scale. Therefore, ST-UCB achieves a regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ without requiring additional assumptions on the input set $\\scriptstyle{\\mathcal{X}}_{t}$ , maintaining a simple UCB-style algorithmic structure. ", "page_idx": 6}, {"type": "text", "text": "Comparison of hypothesis space. In our analysis, we assume in Assumption 3.1 that the reward function $f$ belongs to the RKHS $\\mathcal{H}_{\\mathrm{TNTK}}$ associated with TNTK. Conversely, in existing NB research, it is assumed that $f$ belongs to the RKHS $\\mathcal{H}_{\\mathrm{NTK}}$ associated with NTK. By combining Lemma 3.1 with the well-known Mercer's representation theorem (e.g., Theorem 4.51 in [33]), we derive the following lemma, which describes the relationship between $\\mathcal{H}_{\\mathrm{TNTK}}$ and $\\mathcal{H}_{\\mathrm{NTK}}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.1. Fix any $\\alpha~\\geq~0$ and $\\mathcal{D}\\in\\mathrm{~N_{+}}$ and define the corresponding TNTK as $k_{\\mathrm{TNTK}}~:$ $\\mathbb{S}^{d-1}\\times\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}.$ Let $k_{\\mathrm{NTK}}:\\mathbb{S}^{d-1}\\times\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}$ be an NTK corresponding to a ReLU-based $L$ -layer neural network structure,where $L$ is any natural number. Then, $\\mathcal{H}_{\\mathrm{TNTK}}\\subset\\mathcal{H}_{\\mathrm{NTK}}\\;h o l d s$ where $\\mathcal{H}_{\\mathrm{NTK}}$ and $\\mathcal{H}_{\\mathrm{TNTK}}$ are RKHSs corresponding to $k_{\\mathrm{NTK}}$ and $k_{\\mathrm{TNTK}}$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "The proof of Lemma 4.1 is provided in Appendix D. Lemma 4.1 indicates that the regret upper bound of ST-UCB is guaranteed in a more constrained hypothesis space compared to NN-UCB. While NN-UCB generally does not guarantee a no-regret property, the $\\tilde{\\mathcal{O}}(\\sqrt{T})$ guarantee in ST-UCB can be interpreted as being due to focusing on a more constrained hypothesis space. ", "page_idx": 7}, {"type": "text", "text": "It should be noted that whether this property is specific to the tree structure of the model or depends on the choice of the soft-decision function is unknown. We constructed and analyzed our algorithm based on the definition of soft trees from [21]; however, we conjecture that by using a more non-smooth soft decision function, although the regret may degrade to a level similar to NN-UCB, we can align the hypothesis spaces used in NN-UCB and ST-UCB to be almost the same. We leave the detailed analysis tofuturework. ", "page_idx": 7}, {"type": "text", "text": "5  Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we compare ST-UCB and NN-UCB to empirically demonstrate the usefulness of the tree-based model. Additionally, to evaluate the characteristics of UCB-based algorithms, we include $\\epsilon$ -greedy based ST-greedy and NN-greedy as comparative methods. ", "page_idx": 7}, {"type": "text", "text": "Real-world dataset.  We use Energy Effciency dataset [34] registered in UCI Machine Learning Repository [1]. This dataset provides the load required to maintain comfortable indoor air conditions for each of the 768 residential buildings - two types of data are provided as non-negative real values: heating load (HL) and cooling load (CL). For each building, eight types of context are included as explanatory variables. We randomly sample residential buildings without replacement to create a dataset of $\\tilde{K}\\,\\leq\\,768$ arms, where $\\tilde{K}$ is a hyperparameter. The inputs are denoted as $\\mathbf{\\Delta}\\pmb{x}\\;=\\;(\\tilde{\\pmb{x}}_{\\mathrm{building}},\\tilde{\\pmb{x}})\\;\\in\\;\\mathcal{X}$ , where $\\tilde{\\pmb{x}}_{\\mathrm{building}}$ is a $\\tilde{K}$ -dimensional one-hot vector used to identify the arms, and $\\tilde{\\pmb{x}}$ is a vector that aggregates the eight types of context. In most real-world data, the rewards depend not only on the observable context $\\tilde{\\pmb{x}}$ but also on other information. To account for arm-specific characteristics that cannot be represented by $\\tilde{\\pmb{x}}$ alone, we use $\\tilde{\\pmb{x}}_{\\mathrm{building}}$ as part of the input. ", "page_idx": 7}, {"type": "text", "text": "We consider each arm of the multi-armed bandit problem as an individual residential building, and we define the reward of the arm selected in each round as $f_{t}=-(\\mathrm{HL}_{t}+\\mathrm{CL}_{t})$ .Additionally,we standardize the rewards across $\\tilde{K}$ arms to have a mean of O and a standard deviation of 1. ", "page_idx": 7}, {"type": "text", "text": "Synthetic dataset. We evaluate the algorithms using synthetic data similar to that used in [41]. Here, the number of arms is set to 20, and the dimension of the input vector $\\textbf{\\em x}$ for each arm is set to 50. Additionally, the input vectors are chosen uniformly at random from the unit ball. We consider the three reward functions: (i) $f^{(1)}(\\pmb{x})=10(\\pmb{x}^{\\top}\\pmb{a})^{2}$ ,(ii) $f^{(2)}({\\pmb x})={\\pmb x}^{\\top}A^{\\top}A{\\pmb x}$ , and (ii) $f^{(3)}({\\pmb x})=\\cos(3{\\pmb x}^{\\top}{\\pmb a})$ where $\\pmb{a}\\in\\mathbb{R}^{50}$ is randomly generated from uniform distribution over unit ball, and each entry of $\\pmb{A}\\in\\mathbb{R}^{50\\times50}$ is randomly generated from standard normal distribution. Similar to the real-world dataset, we standardize the rewards across all arms. ", "page_idx": 7}, {"type": "text", "text": "Setup.. We define the cumulative regret up to round $T$ as $\\begin{array}{r}{R_{T}=\\sum_{t=1}^{T}f^{*}-f_{t}}\\end{array}$ where $f^{*}$ represents the maximum reward among all arms. We assume that the response used for training the machine learning model is generated from $y_{t}=f_{t}+\\epsilon_{t}$ where $\\epsilon_{t}$ is randomly drawn from a normal distribution with mean O and standard deviation $\\sigma_{\\mathrm{noise}}=0.2$ . Since the rewards are standardized, this setting of $\\sigma_{\\mathrm{noise}}$ effectively acts as noise. ", "page_idx": 7}, {"type": "text", "text": "In this experiment, we will use an $\\epsilon$ -greedy based algorithm as an additional comparative method; In each round, an arm is selected randomly with a probability of $\\epsilon$ , while the arm with the highest predicted value from the machine learning model is selected with a probability of $1-\\epsilon$ .Here, we will perform a grid search to choose the value of $\\epsilon$ from the three candidates $\\epsilon\\in\\lbrace0.05,0.1,0.2\\rbrace$ . Meanwhile, in UCB-based algorithms, $\\beta$ is provided as a parameter to control the degree of exploration. We use a grid search to select the value of $\\beta$ from the three candidates $\\beta\\in\\{0.0\\bar{1},0.1,1\\}$ ", "page_idx": 7}, {"type": "image", "img_path": "cKKXBhyijL/tmp/711376b3a7114dbae4422425adbd072f4a476e9f47223d1353394add0d4e50c2.jpg", "img_caption": ["Figure 2: The average cumulative regret with one standard error. The experiment was conducted over 10 episodes with different initial parameters for the model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We employ a fully connected neural network model with two intermediate layers. Including the input and output layers, the total number of layers is four. Each of the two intermediate layers contains 33 units, one of which is a bias term. As for the tree-based model, we consider an ensemble of four soft-trees, the depth of each soft-tree is three. The regularization coefficient $\\lambda$ for theparameters is fixed at $10^{-4}$ , regardless of the machine learning model. Supplementary details related to the implementation of the algorithms are summarized in Appendix F.1. ", "page_idx": 8}, {"type": "text", "text": "Results. The results for each algorithm are shown in Fig. 2. In real-world dataset, three different numbers of arms were considered, with $\\tilde{K}$ being one of $\\{20,\\,40,\\,60\\}$ . These experiments were conducted over 10 episodes with different initial parameters $\\pmb{\\theta}_{0}$ for the model. Additional results without the grid search for $\\epsilon,\\beta$ are summarized in Appendix F.2. ", "page_idx": 8}, {"type": "text", "text": "In all settings of real-world dataset, the regret of ST-UCB was not smaller in the early rounds, but the increase in the cumulative regret became more gradual as the rounds progressed. For example, in the setting of $\\tilde{K}=60$ , after round 150, there was no change in the cumulative regret of ST-UCB. However, from round 1 to 70, the regret of ST-UCB was relatively high compared to other methods. In our experiment, UCB-based policies (NN-UCB, ST-UCB) tended to actively select arms that had not been chosen before in the early rounds. As the rounds increased, exploratory behavior was suppressed, and there was a stronger tendency to select only arms with high rewards. On the other hand, in policies based on $\\epsilon_{\\mathrm{:}}$ -greedy (NN-greedy, ST-greedy), the exploration rate is kept at $\\epsilon$ across all rounds. Therefore, the regret continues to accumulate gradually as the rounds increase, raising concerns about worsening cumulative regret over extended long rounds. In the $f^{(1)}$ and $f^{(2)}$ settings of synthetic dataset, ST-UCB outperformed the other policies, and the convergence stability of cumulative regret in $f^{(3)}$ was comparable between ST-UCB and NN-UCB. ", "page_idx": 8}, {"type": "text", "text": "6  Conclusion and future direction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we propose a new regret-minimization algorithm based on a soft tree ensemble model. Our analysis extends the theoretical framework of existing neural bandit (NB) approaches to the soft tree ensemble model, demonstrating, under appropriate assumptions, the achievement of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret. To our knowledge, this is the first application of NB theory to models other than neural networks; we believe that our work marks an important first step toward developing exploration and exploitation theory using various complex models beyond neural nets. ", "page_idx": 8}, {"type": "text", "text": "Our future research directions are outlined below. Firstly, it is important to study the extension when employing hard decision trees. In this paper, as the scale parameter $\\alpha$ approaches infinity, the soft tree regressor approaches that of a hard tree. We conjecture that our algorithm also works in this regime; however, since our regret analysis assumes a fixed $\\alpha$ , our proposed method is not guaranteed to maintain the no-regret property with a varying scale parameter $\\alpha$ . Hence, a more careful theoretical treatment is needed for this extension. Secondly, we plan to generalize the theory to encompass more common learning methods of the ensemble tree model. Specifically, learning algorithms using hard trees often utilize optimization methods in a greedy format rather than gradient descent. Therefore, developing theoretical foundations for ensemble tree learning methods that are more practically applicable is crucial. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  UCI Machine Learning Repository \u2014- archive.ics.uci.edu. https://archive.ics.uci.edu/. [Accessed 25-04-2024].   \n[2]  Yasin Abbasi- Yadkori. Online learning for linearly parametrized control problems. 2013.   \n[3]  Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. Proc. Neural Information Processing Systems (NeurIPS), 2011.   \n[4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[5]  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 2002.   \n[6]  Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In Proc. International Conference on Learning Representations (ICLR), 2021.   \n[7]  Ilija Bogunovic and Andreas Krause. Misspecified Gaussian process bandit optimization. In Proc. Neural Information Processing Systems (NeurIPS), 2021.   \n[8]  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed bandits. Journal of Machine Learning Research, 12(5), 2011. [9]  Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Second-order kernel online convex optimization with adaptive sketching. In Proc. International Conference on Machine Learning (ICML), 2017.   \n[10] Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, and Lorenzo Rosasco. Gaussian process optimization with adaptive sketching: Scalable and no regret. In Proc. Conference on Learning Theory (COLT), 2019.   \n[11]  Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Proc. International Conference on Machine Learning (ICML), 2017.   \n[12] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In Proc. Conference on Learning Theory (COLT), 2008.   \n[13] Costas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World Scientific, 2014.   \n[14] Adam N Elmachtoub, Ryan McNellis, Sechan Oh, and Marek Petrik. A practical method for solving contextual bandit problems using decision trees. In Conference on Uncertainty in Artificial Intelligence (UAl), 2017.   \n[15]  Raphael F\u00e9raud, Robin Allesiardo, Tanguy Urvoy, and Fabrice Cl\u00e9rot. Random forest for the contextual bandit problem. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), 2016.   \n[16]  Guillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic) gradient methods. arXiv preprint arXiv:2301.11235, 2023.   \n[17]  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.   \n[18] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The tree ensemble layer: Differentiability meets conditional computation. In Proc. International Conference on Machine Learning (ICML), 2020.   \n[19]  Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Proc. Neural Information Processing Systems (NeurIPS), 31, 2018.   \n[20] Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaussian processes and kernel methods: A review on connections and equivalences. arXiv preprint arXiv:1807.02582, 2018.   \n[21] Ryuichi Kanoh and Mahito Sugiyama. A neural tangent kernel perspective of infinite tree ensembles. In Proc. International Conference on Learning Representations (ICLR), 2021.   \n[22]  Ryuichi Kanoh and Mahito Sugiyama. Analyzing tree architectures in ensembles via neural tangent kernel. In Proc. International Conference on Learning Representations (ICLR), 2022.   \n[23] Parnian Kassraie and Andreas Krause. Neural contextual bandits without regret. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.   \n[24] Parnian Kassraie, Andreas Krause, and lija Bogunovic. Graph neural network bandits. In Proc. Neural Information Processing Systems (NeurIPS), December 2022.   \n[25] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. Deep neural decision forests. In Proceedings of the IEEE international conference on computer vision, 2015.   \n[26]  Zihan Li and Jonathan Scarlett. Gaussian process bandit optimization with few batches. In Proc. International Conference on Artijficial Intelligence and Statistics (AISTATS), 2022.   \n[27]  Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Proc. Neural Information Processing Systems (NeurIPS), 2020.   \n[28] Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning on tabular data. Proc. International Conference on Learning Representations (ICLR), 2020.   \n[29]  Sayak Ray Chowdhury and Aditya Gopalan. Bayesian optimization under heavy-tailed payoffs. In Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[30]  Sudeep Salgia. Provably and practically efficient neural contextual bandits. In Proc. International Conference on Machine Learning (ICML), 2023.   \n[31] Meyer Scetbon and Zaid Harchaoui. A spectral analysis of dot-product kernels. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), 2021.   \n[32] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proc. International Conference on Machine Learning (ICML), 2010.   \n[33] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.   \n[34]  Athanasios Tsanas and Angeliki Xifara. Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools. Energy and buildings, 49: 560-567, 2012.   \n[35] Sattar Vakili, Michael Bromberg, Jezabel Garcia, Da-shan Shiu, and Alberto Bernacchia. Uniform generalization bounds for overparameterized neural networks. arXiv preprint arXiv:2109.06099, 2021.   \n[36] Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in Gaussian process bandits. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), 2021.   \n[37] Michal Valko, Nathaniel Korda, R\u00e9mi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time analysis of kernelised contextual bandits. In Conference on Uncertainty in Artificial Intelligence (UAI),2013.   \n[38] Hastagiri P Vanchinathan, Isidor Nikolic, Fabio De Bona, and Andreas Krause. Exploreexploit in top-n recommender systems via Gaussian processes. In Proceedings of the 8th ACM Conference on Recommender systems, 2014.   \n[39] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[40] Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural Thompson sampling. In Proc. International Conference on Learning Representations (ICLR), 2021.   \n[41] Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration. In Proc. International Conference on Machine Learning (ICML), 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A   Information gain of TNTK ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Firstly, we formally define the dot product kernel on the sphere. ", "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Dot product kernel on the sphere [31]). Let $d\\geq2$ and $\\mathbb{S}^{d-1}$ be the unit sphere of $\\mathbb{R}^{d}$ . Then, a kernel $k\\ {\\stackrel{\\cdot}{:}}\\ \\mathbb{S}^{d-1}\\times\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}$ of the following form is called $a$ dot product kernel on the sphere $\\mathbb{S}^{d-1}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nk(\\pmb{x},\\tilde{\\pmb{x}})=\\sum_{n=0}^{\\infty}b_{n}(\\pmb{x}^{\\top}\\tilde{\\pmb{x}})^{n}\\;\\;\\mathrm{for}\\;\\mathrm{all}\\;\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(b_{n})_{n\\in\\mathbb{N}}$ is an absolutely summable sequence.Furthermore,ij $\\ 'b_{n}\\geq0$ forany $n\\in\\mathbb{N},$ $k$ $a$ continuous positive semi-definite kernel on the sphere $\\mathbb{S}^{d-1}$ ", "page_idx": 12}, {"type": "text", "text": "As described in Sec. 2 in [31], continuous positive semi-definite dot-product kernels are decomposed as Eq. (6) by using spherical harmonics $\\left(Y_{n,j}\\right)$ ", "page_idx": 12}, {"type": "text", "text": "The following lemma shows the eigendecay of dot product kernels depending on coefficients $(b_{n})_{n\\in\\mathbb{N}}$ ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Proposition 2.3 in [31]). Let $d\\geq2$ and $(Y_{n,j})_{j\\in[N_{d,n}]}$ be the spherical harmonics of degree $n$ Furthermore,let $\\begin{array}{r}{k(\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}}):=\\sum_{n=1}^{\\infty}b_{n}(\\mathbf{\\boldsymbol{x}}^{\\top}\\tilde{\\mathbf{\\boldsymbol{x}}})^{n}}\\end{array}$ be a continuous positive semi-definite dot-product kernel on $\\mathbb{S}^{d-1}$ . Here, if there exist $r\\in(0,1)$ and $c>0$ such that $b_{n}\\leq c r^{n}$ holds for any $n\\,\\in\\,\\mathbb{N}$ , then, there exists constant $C>0$ and $(\\lambda_{n})_{n\\in\\mathbb{N}}$ such that $\\lambda_{n}\\leq C r^{n}$ and $k(\\pmb{x},\\tilde{\\pmb{x}})=$ $\\begin{array}{r}{\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}\\lambda_{n}Y_{n,j}(\\pmb{x})Y_{n,j}(\\tilde{\\pmb{x}})}\\end{array}$ hold for all $\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ and $n\\in\\mathbb N$ ", "page_idx": 12}, {"type": "text", "text": "To prove Lemma 3.1, we consider the Maclaurin series expansion of TNTK; then, Lemma 3.1 is given from Lemma A.1. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 3.1. First, we respectively define functions $f_{1}:[-1,1]\\to\\mathbb{R}$ and $f_{2}:[-1,1]\\to\\mathbb{R}$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{1}(a)=\\displaystyle\\frac{1}{2\\pi}\\arcsin{\\left(\\frac{\\alpha^{2}a}{\\alpha^{2}+0.5}\\right)}+\\frac{1}{4},}\\\\ {f_{2}(a)=\\displaystyle\\frac{\\alpha^{2}}{\\pi}\\frac{1}{\\sqrt{(1+2\\alpha^{2})^{2}-4\\alpha^{4}a^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, since $\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ , the following holds directly from the analytical expression of TNTK [21]: ", "page_idx": 12}, {"type": "equation", "text": "$$\nk_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})=2^{\\mathcal{D}}\\mathcal{D}(\\pmb{x}^{\\top}\\tilde{\\pmb{x}})f_{1}(\\pmb{x}^{\\top}\\tilde{\\pmb{x}})^{\\mathcal{D}-1}f_{2}(\\pmb{x}^{\\top}\\tilde{\\pmb{x}})+2^{\\mathcal{D}}f_{1}(\\pmb{x}^{\\top}\\tilde{\\pmb{x}})^{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here, since $\\begin{array}{r}{-1<\\frac{\\alpha^{2}a}{\\alpha^{2}+0.5}<1}\\end{array}$ holds for any $a\\in[-1,1]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f_{1}(a)=\\frac{1}{2\\pi}\\mathrm{arcsin}\\left(\\frac{\\alpha^{2}a}{\\alpha^{2}+0.5}\\right)+\\frac{1}{4}}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{2\\pi}\\sum_{n=0}^{\\infty}\\frac{(2n)!}{4^{n}(n!)^{2}(2n+1)}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{2n+1}a^{2n+1}+\\frac{1}{4},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "from the Maclaurin series expansion of the inverse sine function. Furthermore, since $-1\\ <$ $\\begin{array}{r}{\\left(\\frac{2\\alpha^{2}a}{1+2\\alpha^{2}}\\right)^{2}<1}\\end{array}$ holds for any $a\\in[-1,1]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{2}(a)=\\displaystyle\\frac{\\alpha^{2}}{\\pi}\\frac{1}{\\sqrt{(1+2\\alpha^{2})^{2}-4\\alpha^{4}a^{2}}}}}\\\\ {{\\phantom{f_{2}(a)=\\displaystyle\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}\\frac{1}{\\sqrt{1-\\left(\\frac{2\\alpha^{2}}{1+2\\alpha^{2}}\\right)^{2}a^{2}}}}}\\\\ {{\\phantom{f_{2}(a)=\\displaystyle\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}\\sum_{n=0}^{\\infty}(-1)^{n}\\left(\\!\\!\\!\\begin{array}{c}{{\\!\\!\\!-0.5\\!}}\\\\ {{\\!\\!\\!n}}\\end{array}\\!\\!\\!\\right)\\left(\\!\\!\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\!\\!\\right)^{2n}a^{2n},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last line follows from the fact that (1 + \u03b1) = =0 $\\begin{array}{r}{(1+x)^{c}=\\sum_{n=0}^{\\infty}\\binom{c}{n}\\,x^{n}}\\end{array}$ holds for any $c\\in\\mathbb R$ and $x\\in(-1,1)$ . Here, $\\binom{c}{n}$ denotes a generalized binomial coefficient, which is defined as ${\\binom{c}{n}}=1$ if $n=0$ ; otherwise, $\\binom{c}{n}={\\frac{c(c-1)\\cdots(c-n+1)}{n!}}$ e(c-1)-(-+1) By rearanging Eq 18 and Eq (21), f and 2 can respectively be rewritten as $\\begin{array}{r}{f_{1}(a)=\\sum_{i=1}^{\\infty}b_{i}^{(1)}a^{i}}\\end{array}$ and $\\begin{array}{r}{f_{2}(a)=\\sum_{i=1}^{\\infty}b_{i}^{(2)}a^{i}}\\end{array}$ , where the coefficients $b_{i}^{(1)}$ and $b_{i}^{(2)}$ are defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{i}^{(1)}=\\left\\{\\begin{array}{l l}{\\frac{1}{4}}&{\\mathrm{~if~}i=0,}\\\\ {\\frac{\\left(i-1\\right)!}{\\left(2\\pi\\right)^{2(i-1)}i\\left(\\left(\\left(i-1\\right)/2\\right)!\\right)^{2}}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i}}&{\\mathrm{~if~}\\exists n\\in\\mathbb{N},\\;i=2n+1,\\;}\\\\ {0}&{\\mathrm{~otherwise},}\\end{array}\\right.}\\\\ &{b_{i}^{(2)}=\\left\\{\\begin{array}{l l}{\\frac{\\alpha^{2}}{\\pi\\left(1+2\\alpha^{2}\\right)}}&{\\mathrm{~if~}i=0,}\\\\ {\\frac{\\alpha^{2}}{\\pi\\left(1+2\\alpha^{2}\\right)}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i}\\frac{1}{\\left(i/2\\right)!}\\left[0.5\\cdot1.5\\cdot\\cdots\\left(0.5+0.5i-1\\right)\\right]}&{\\mathrm{~if~}\\exists n\\in\\mathbb{N},\\;i=2n,}\\\\ {0}&{\\mathrm{~otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From the Stirling's inequality: $e(n/e)^{n}\\leq n!\\leq e n(n/e)^{n}$ , for any $i$ such that $i=2n+1$ holds, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(i-1)!}{(2\\pi)2^{i-1}i(((i-1)/2)!)^{2}}=\\frac{(2n)!}{(2\\pi)2^{2n}(2n+1)(n!)^{2}}}\\\\ &{\\phantom{m m m m m m}\\leq\\frac{2e n(2n/e)^{2n}}{(2\\pi)2^{2n}(2n+1)e^{2}(n/e)^{2n}}}\\\\ &{\\phantom{m m m m m m}\\leq\\frac{2n}{(2\\pi)(2n+1)e}}\\\\ &{\\leq\\frac{1}{(2\\pi)e}}\\\\ &{\\leq\\frac{1}{(2\\pi)e}}\\\\ &{\\leq\\frac{1}{e}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{0\\le b_{i}^{(1)}\\le e^{-1}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i}}\\end{array}$ holds for any $i\\in\\mathbb N$ . Furthermore, for any $i$ such that $i=2n$ holds, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}\\cfrac{1}{(i/2)!}\\left[0.5\\cdot1.5\\cdot\\cdots\\left(0.5+0.5i-1\\right)\\right]}\\\\ &{=\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}\\cfrac{1}{n!}\\left[0.5\\cdot1.5\\cdot\\cdots\\left(0.5+n-1\\right)\\right]}\\\\ &{\\leq\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}\\cfrac{1}{n!}\\left(1\\cdot2\\cdot\\cdots n\\right)}\\\\ &{=\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{0\\le b_{i}^{(2)}\\le\\frac{\\alpha^{2}}{\\pi(1+2\\alpha^{2})}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i}}\\end{array}$ holds for any $i\\in\\mathbb N$ Now, we rewrite Eq. (16) by using the multiple Cauchy product formula as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nk_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})=\\sum_{i=0}^{\\infty}b_{i}(\\pmb{x}^{\\top}\\tilde{\\pmb{x}})^{i},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{b_{i}=2^{D}\\mathcal{D}\\displaystyle\\sum_{i_{2}=0}^{i-1}\\sum_{i_{3}=0}^{i_{2}}\\cdots\\sum_{i_{\\mathcal{D}-1}=0}^{i_{\\mathcal{D}-2}}\\sum_{i_{\\mathcal{D}=0}}^{i_{\\mathcal{D}-1}}\\left(b_{i-i_{2}}^{(1)}b_{i_{2}-i_{3}}^{(1)}\\cdot\\cdot\\cdot b_{i_{\\mathcal{D}-1}-i_{\\mathcal{D}}}^{(1)}b_{i_{\\mathcal{D}}}^{(2)}\\right)}}\\\\ {{+\\,2^{D}\\displaystyle\\sum_{i_{2}=0}^{i}\\sum_{i_{3}=0}^{i_{2}}\\cdots\\sum_{i_{\\mathcal{D}-1}=0}^{i_{\\mathcal{D}-2}}\\sum_{i_{\\mathcal{D}=0}}^{i_{\\mathcal{D}-1}}\\left(b_{i-i_{2}}^{(1)}b_{i_{2}-i_{3}}^{(1)}\\cdot\\cdot\\cdot b_{i_{\\mathcal{D}-1}-i_{\\mathcal{D}}}^{(1)}b_{i_{\\mathcal{D}}}^{(1)}\\right)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By combining Eq. (34) with the upper bounds of $b_{i}^{(1)}$ and $b_{i}^{(2)}$ \uff0c ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{i}\\leq2^{D}D\\left(\\frac{1}{e}\\right)^{D-1}\\frac{\\alpha^{2}}{\\pi\\left(1+2\\alpha^{2}\\right)}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i D}\\sum_{\\underline{{i}}=0}^{i-1}\\underset{i_{2}=0}{\\overset{i_{2}}{\\sum}}\\cdots\\sum_{i_{p-1}=0}^{i_{p-2}}\\underset{i_{2}=0}{\\overset{i_{p-1}}{\\sum}}1}\\\\ &{\\quad+\\,2^{D}\\left(\\frac{1}{e}\\right)^{D}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i D}\\underset{i_{2}=0}{\\overset{i_{1}}{\\sum}}\\underset{i_{3}=0}{\\overset{i_{2}}{\\sum}}\\cdots\\underset{i_{p-1}=0}{\\overset{i_{p-1}}{\\sum}}\\sum_{i_{p}=0}^{i_{p-1}}1}\\\\ &{\\quad\\leq\\,2^{D}D\\left(\\frac{1}{e}\\right)^{D-1}\\frac{\\alpha^{2}}{\\pi\\left(1+2\\alpha^{2}\\right)}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i D}\\left(i-1\\right)^{D}+\\,2^{D}\\left(\\frac{1}{e}\\right)^{D}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i D}i^{D}}\\\\ &{\\quad\\leq\\,\\left[2^{p}D\\left(\\frac{1}{e}\\right)^{D-1}\\frac{\\alpha^{2}}{\\pi\\left(1+2\\alpha^{2}\\right)}+2^{p}\\left(\\frac{1}{e}\\right)^{D}\\right]\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.5}\\right)^{i D}i^{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, there exist constant $\\tilde{C}_{\\alpha,\\mathcal{D}}>0$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{i}\\leq\\tilde{C}_{\\alpha,{\\cal D}}\\left(\\frac{\\alpha^{2}}{\\alpha^{2}+0.25}\\right)^{i{\\cal D}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds for any $i\\in\\mathbb N$ . By applying Lemma A.1 with Eq. (38), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{i}\\leq C_{\\alpha,\\mathcal{D}}^{(1)}\\left(\\cfrac{\\alpha^{2}}{\\alpha^{2}+0.25}\\right)^{i\\mathcal{D}}}\\\\ &{\\quad=C_{\\alpha,\\mathcal{D}}^{(1)}\\exp\\left(i\\mathcal{D}\\ln\\left(\\cfrac{\\alpha^{2}}{\\alpha^{2}+0.25}\\right)\\right)}\\\\ &{\\quad=C_{\\alpha,\\mathcal{D}}^{(1)}\\exp\\left(-i\\mathcal{D}\\ln\\left(1+\\cfrac{1}{4\\alpha^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some constant C.) $C_{\\alpha,{\\cal D}}^{(1)}>0$ ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our proof strategy of Theorem 3.1 is adapted from [23, 35]. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 3.1. Fix any deterministic sequence $x_{1},\\ldots,x_{t}\\in\\mathcal{X}\\subset\\mathbb{S}^{d-1}$ .For any $M\\in\\ensuremath{\\mathbb{N}}+$ $k_{\\mathrm{TNTK}}^{(M)}$ $\\tilde{k}_{\\mathrm{TNTK}}^{(M)}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{\\mathrm{TNTK}}^{(M)}(\\pmb{x},\\tilde{\\pmb{x}})=\\displaystyle\\sum_{n=0}^{M}\\sum_{j=1}^{N_{d,n}}\\lambda_{n}Y_{n,j}(\\pmb{x})Y_{n,j}(\\tilde{\\pmb{x}}),}\\\\ &{\\tilde{k}_{\\mathrm{TNTK}}^{(M)}(\\pmb{x},\\tilde{\\pmb{x}})=\\displaystyle\\sum_{n=M+1}^{\\infty}\\sum_{j=1}^{N_{d,n}}\\lambda_{n}Y_{n,j}(\\pmb{x})Y_{n,j}(\\tilde{\\pmb{x}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "${K_{\\mathrm{TNTK}}^{\\left(M\\right)}}$ $\\tilde{K}_{\\mathrm{TNTK}}^{(M)}$ b $t\\times t$ $(i,j)$ $k_{\\mathrm{TNTK}}^{(M)}(\\pmb{x}_{i},\\pmb{x}_{j})$ and $\\tilde{k}_{\\mathrm{TNTK}}^{(M)}(\\pmb{x}_{i},\\pmb{x}_{j})$ espectivlys with th profofhormn [3] wha thefolwin decomposition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}\\right)}\\\\ &{=\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}^{(M)}\\right)+\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}^{(M)}\\right)^{-1}\\tilde{K}_{\\mathrm{TNTK}}^{(M)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By following the same argument as the proof of Theorem 2 in [35], the first term of Eq. (44) is bounded from above as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac12\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}^{(M)}\\right)\\le\\frac{N_{M}}{2}\\ln\\left(1+\\frac{\\overline{{k}}t}{\\rho N_{M}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r}{N_{M}\\,=\\,\\sum_{n=1}^{M}N_{d,n}}\\end{array}$ $\\overline{{k}}\\,=\\,\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}\\,k_{\\mathrm{TNTK}}(\\pmb{x},\\pmb{x})$ follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}^{(M)}\\right)^{-1}\\tilde{K}_{\\mathrm{TNTK}}^{(M)}\\right)}\\\\ &{\\leq\\frac{t}{2}\\ln\\left(1+\\frac{\\rho^{-1}\\mathrm{tr}\\left(\\tilde{K}_{\\mathrm{TNTK}}^{(M)}\\right)}{t}\\right)}\\\\ &{\\leq\\frac{t}{2}\\ln\\left(1+\\rho^{-1}\\sum_{n=M+1}^{\\infty}\\lambda_{n}N_{d,n}\\right)}\\\\ &{\\leq\\frac{t}{2\\rho}\\displaystyle\\sum_{n=M+1}^{\\infty}\\lambda_{n}N_{d,n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, from Lemma 3.1, there exists some constants $C>0$ and $C_{\\alpha,d}>0$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{n=M+1}^{\\infty}\\lambda_{n}N_{d,n}\\leq\\sum_{n=M+1}^{\\infty}C_{\\alpha,\\mathcal{D}}^{(1)}C\\exp\\left(-C_{\\alpha}D n\\right)n^{d-2}}}\\\\ &{}&{\\leq\\displaystyle\\sum_{n=M+1}^{\\infty}C_{\\alpha,\\mathcal{D}}^{(1)}C C_{\\alpha,d}\\exp\\left(-0.5C_{\\alpha}D n\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we set $C_{\\alpha}$ as $C_{\\alpha}=\\ln(1+1/(4\\alpha^{2}))$ . Furthermore, Eq. (50) follows from $N_{d,n}=\\Theta(n^{d-2})$ (see, e.g., [23]). Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{n=M+1}^{\\infty}\\lambda_{n}N_{d,n}\\leq C_{\\alpha,\\mathcal{D}}^{(1)}C C_{\\alpha,d}\\int_{M}^{\\infty}\\exp\\left(-0.5C_{\\alpha}D x\\right)\\mathrm{d}x}}\\\\ &{}&{\\leq\\tilde{C}_{\\alpha,\\mathcal{D},d}\\exp\\left(-\\frac{C_{\\alpha}D M}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Wwhere we set $\\tilde{C}_{\\alpha,\\ensuremath{\\mathcal{D}_{\\!\\circ}},d}$ $\\tilde{C}_{\\alpha,\\mathcal{D},d}=C_{\\alpha,\\mathcal{D}}^{(1)}C C_{\\alpha,d}$ Now,bynoting $N_{M}={\\mathcal{O}}(M^{d-1})$ the constant $\\tilde{C}>0$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}\\right)\\leq\\frac{N_{M}}{2}\\ln\\left(1+\\frac{\\displaystyle\\overline{{k}}t}{\\rho N_{M}}\\right)+\\frac{t}{2\\rho}\\sum_{n=M+1}^{\\infty}\\lambda_{n}N_{d,n}}&{}\\\\ {\\leq\\frac{\\displaystyle\\tilde{C}M^{d-1}}{2}\\ln\\left(1+\\frac{\\displaystyle\\overline{{k}}t}{\\rho}\\right)+\\frac{\\displaystyle\\tilde{C}_{\\alpha,\\mathcal{D},d}t}{2\\rho}\\exp\\left(-\\frac{C_{\\alpha}D M}{2}\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By choosing $M$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nM=\\left\\lceil2C_{\\alpha}^{-1}\\mathcal{D}^{-1}\\ln\\left(\\tilde{C}_{\\alpha,\\mathcal{D},d}t\\rho^{-1}\\tilde{C}^{-1}\\left[\\ln\\left(1+\\frac{\\overline{{k}}t}{\\rho}\\right)\\right]^{-1}\\right)\\right\\rceil,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$M\\geq1$ for sufficiently large $t$ , and we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M=\\left[2C_{a}^{-1}D^{-1}\\ln\\left(\\bar{C}_{a,D,d}t\\rho^{-1}\\bar{C}^{-1}\\left[\\ln\\left(1+\\frac{\\bar{K}_{H}}{\\rho}\\right)\\right]^{-1}\\right)\\right]}\\\\ &{\\Rightarrow M\\geq2C_{a}^{-1}D^{-1}\\ln\\left(\\bar{C}_{a,D,d}\\rho^{-1}\\bar{C}^{-1}\\left[\\ln\\left(1+\\frac{\\bar{K}_{H}}{\\rho}\\right)\\right]^{-1}\\right)}\\\\ &{\\Rightarrow\\exp\\left(\\frac{C_{a}D M}{2}\\right)\\geq\\bar{C}_{a,D,d}\\mu^{-1}\\hat{C}^{-1}\\left[\\ln\\left(1+\\frac{\\bar{K}_{H}}{\\rho}\\right)\\right]^{-1}}\\\\ &{\\Leftrightarrow1\\geq\\bar{C}_{a,D,d}\\mu^{-1}\\hat{C}^{-1}\\left[\\ln\\left(1+\\frac{\\bar{K}_{H}}{\\rho}\\right)\\right]^{-1}\\exp\\left(-\\frac{C_{a}\\mathcal{D}M}{2}\\right)}\\\\ &{\\Rightarrow M^{d-1}\\geq\\bar{C}_{a,D,d}\\mu^{-1}\\hat{C}^{-1}\\left[\\ln\\left(1+\\frac{\\bar{K}_{H}}{\\rho}\\right)\\right]^{-1}\\exp\\left(-\\frac{C_{a}\\mathcal{D}M}{2}\\right)}\\\\ &{\\Rightarrow\\frac{\\bar{C}M^{d-1}}{2}\\ln\\left(1+\\frac{\\bar{K}_{H}}{\\rho}\\right)\\geq\\frac{\\bar{C}_{a,D,d}\\ell}{2}\\exp\\left(-\\frac{C_{a}\\mathcal{D}M}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}K_{\\mathrm{TNTK}}\\right)}\\\\ &{\\leq\\tilde{C}M^{d-1}\\ln\\left(1+\\frac{\\overline{{k}}t}{\\rho}\\right)}\\\\ &{\\leq\\left[2C_{\\alpha}^{-1}\\mathcal{D}^{-1}\\ln\\left(\\tilde{C}_{\\alpha,\\mathcal{D},d}t\\rho^{-1}\\tilde{C}^{-1}\\left[\\ln\\left(1+\\frac{\\overline{{k}}t}{\\rho}\\right)\\right]^{-1}\\right)\\right]^{d-1}\\tilde{C}\\ln\\left(1+\\frac{\\overline{{k}}t}{\\rho}\\right)}\\\\ &{=\\mathcal{O}\\left(\\ln^{d}t\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The above inequality holds for any choice of $\\pmb{x}_{1},\\dots,\\pmb{x}_{t}$ ; hence, the proof is completed. ", "page_idx": 16}, {"type": "text", "text": "B   Confidence bounds of soft trees ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To prevent the subscript from becoming redundant hereafter, unless specifically stated otherwise, we denote the initial parameter $\\pmb\\theta_{0}$ by $\\overline{{\\theta}}:=\\theta_{0}$ , and the initial parameters of the $m$ -th tree are denoted \u7684 $\\overline{{\\pmb{\\theta}}}^{(m)}$ Moreove, the intal parameter vectors coresponding t the intermal nodes and ea odes for g(m) are denoted by $\\overline{{\\pmb{w}}}^{(m)}$ and $\\overline{{\\pi}}^{(m)}$ respectively. First, following [21], we decompose the finite sample approximation of the TNTK as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{\\theta_{0}}h(x;\\theta_{0}),\\nabla_{\\theta_{0}}h(\\bar{x};\\theta_{0})\\rangle}\\\\ &{=\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{w}}^{(m)}},\\mathcal{\\tau}^{\\mathit{h}^{(m)}}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{w}}^{(m)}},\\mathcal{\\tau}^{\\mathit{h}^{(m)}}\\left(\\hat{x};\\theta_{0}^{(m)}\\right)\\right\\rangle}\\\\ &{+\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{w}}^{(m)}},\\mathcal{\\tau}^{\\mathit{h}^{(m)}}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{w}}^{(m)}},\\mathcal{\\tau}^{\\mathit{h}^{(m)}}\\left(\\hat{x};\\theta_{0}^{(m)}\\right)\\right\\rangle}\\\\ &{+\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{w}}^{(m)}},\\mathcal{\\alpha}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{w}}^{(m)}},\\mathcal{\\alpha}h^{(m)}\\left(\\hat{x};\\theta_{0}^{(m)}\\right)\\right\\rangle}\\\\ &{+\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(\\hat{x};\\theta_{0}^{(m)}\\right)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $\\overline{{{\\pmb w}}}^{(m),(T)},\\overline{{{\\pmb w}}}^{(m),(L)}$ and ${\\overline{{\\pmb{w}}}}^{(m),(R)}$ represent the parameters of the root (top) node, ll internal nodes of the left subtree, and all internal nodes of the right subtree of the $m$ -th tree at the initial values, respectively. Now, we define $k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}}),\\,k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}}),\\,k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})$ $k_{\\mathrm{TNTK}}^{(B)}(\\pmb{x},\\tilde{\\pmb{x}})$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})=\\mathbb{E}\\left[\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),}}(\\tau)h^{(m)}\\left(\\pmb{x};\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),}}(\\tau)h^{(m)}\\left(\\tilde{\\pmb{x}};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right],}\\\\ &{k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}})=\\mathbb{E}\\left[\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),}}(\\pmb{x})h^{(m)}\\left(\\pmb{x};\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),}}(\\pmb{x})h^{(m)}\\left(\\tilde{\\pmb{x}};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right],}\\\\ &{k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})=\\mathbb{E}\\left[\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),}}(\\kappa)\\left(\\pmb{x};\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),}}(\\kappa)h^{(m)}\\left(\\tilde{\\pmb{x}};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right],}\\\\ &{k_{\\mathrm{TNTK}}^{(B)}(\\pmb{x},\\tilde{\\pmb{x}})=\\mathbb{E}\\left[\\left\\langle\\nabla_{\\overline{{\\mp}}^{(m)}}h^{(m)}\\left(\\pmb{x};\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m)}}h^{(m)}\\left(\\tilde{\\pmb{x}};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that, since the initial parameters of each tree follow the same distribution, the definitions mentioned above do not depend on the choice of $m$ .Now, assuming that the initial parameters follow a multivariate normal distribution independent across dimensions, by using the law of lrge numbers, Eqgs. (68), (69), and (70) converge in probability, respectively, to $k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})$ $k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}})+k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})$ and $k_{\\mathrm{TNTK}}^{(B)}(\\pmb{x},\\tilde{\\pmb{x}})$ Fromthe cntiusmapingtheorem,ifo $\\begin{array}{r}{k_{\\mathrm{TNTK}}(\\overline{{\\pmb{x}\\,\\tilde{\\pmb{x}}}})=k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\overbar{\\pmb{x}})+k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}})+k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})+k_{\\mathrm{TNTK}}^{(B)}(\\pmb{x},\\tilde{\\pmb{x}})}\\end{array}$ cean be expressed [21]. Note that the convergence to the above TNTK also holds for the initialization strategy of ST-UCB. Actually, regarding Eq. (68), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{w}}^{(m)},(T)}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{w}}^{(m)},(T)}h^{(m)}\\left(\\tilde{x};\\theta_{0}^{(m)}\\right)\\right\\rangle}}\\\\ &{=\\frac{1}{2}\\Big[\\displaystyle\\frac{2}{M}\\sum_{m=1}^{M/2}\\left\\langle\\nabla_{\\overline{{w}}^{(m)},(T)}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{w}}^{(m)},(T)}h^{(m)}\\left(\\tilde{x};\\theta_{0}^{(m)}\\right)\\right\\rangle}\\\\ &{\\ +\\frac{2}{M}\\displaystyle\\sum_{m=1}^{M/2}\\left\\langle\\nabla_{\\overline{{w}}^{(M/2+m)},(T)}h^{(M/2+m)}\\left(x;\\theta_{0}^{(M/2+m)}\\right),\\nabla_{\\overline{{w}}^{(M/2+m)},(T)}h^{(m)}\\left(\\tilde{x};\\theta_{0}^{(M/2+m)}\\right)\\right\\rangle\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first and second terms correspond to the inner products of gradients when initializing $M/2$ soft $k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})$ There fore, by the continuous mapping theorem, Eq. (75) converges in probability to $k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})$ Similar arguments apply to $k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}})\\,+\\,k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})$ k(r,a) and k(BT TNTk(\u03b1, &), indicating that in the initialization strategy of ST-UCB, $\\langle\\nabla\\pmb{\\theta}_{0}h(\\pmb{x};\\pmb{\\theta}_{0}^{-}),\\nabla\\pmb{\\theta}_{0}h\\left(\\pmb{x};\\pmb{\\theta}_{0}\\right)\\rangle$ also converges in probability to $k_{\\mathrm{TNTK}}(\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}})$ . The following three lemmas each evaluate the concentration to $k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})$ $k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}})+k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})$ and $k_{\\mathrm{TNTK}}^{(B)}(\\pmb{x},\\tilde{\\pmb{x}})$ for Eegs. (68),(6), and (70), respectively. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.1. For any $\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ and $\\epsilon\\geq0$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\pmb{\\tilde{x}})-\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),(T)}}h^{(m)}\\left(\\pmb{x};\\pmb{\\theta}_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),(T)}}h^{(m)}\\left(\\pmb{\\tilde{x}};\\pmb{\\theta}_{0}^{(m)}\\right)\\right\\rangle\\right|\\leq\\epsilon\\right)}\\\\ &{\\geq1-4\\exp\\left(-c\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{K^{2}},\\frac{\\epsilon}{K}\\right\\}M\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $K=4\\alpha^{2}C\\mathcal{L}^{2}$ .Furthermore, $C,c>0$ are absolute constants. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2. For any $\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ \uff0c $\\epsilon\\geq0$ and $\\mathcal{D}\\geq2$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|k_{\\mathrm{TNTK}}^{(L)}(\\pmb{x},\\tilde{\\pmb{x}})-\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),(L)}}h^{(m)}\\left(\\pmb{x};\\pmb{\\theta}_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),(L)}}h^{(m)}\\left(\\tilde{\\pmb{x}};\\pmb{\\theta}_{0}^{(m)}\\right)\\right\\rangle\\right|\\leq\\epsilon\\right)}\\\\ &{\\geq1-4\\exp\\left(-c\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{K^{2}},\\frac{\\epsilon}{K}\\right\\}M\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|k_{\\mathrm{TNTK}}^{(R)}(\\pmb{x},\\tilde{\\pmb{x}})-\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),(R)}}h^{(m)}\\left(\\pmb{x};\\pmb{\\theta}_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),(R)}}h^{(m)}\\left(\\tilde{\\pmb{x}};\\pmb{\\theta}_{0}^{(m)}\\right)\\right\\rangle\\right|\\leq\\epsilon\\right)}\\\\ &{\\geq1-4\\exp\\left(-c\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{K^{2}},\\frac{\\epsilon}{K}\\right\\}M\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma B.3. For any $\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathbb{S}^{d-1}$ and $\\epsilon\\geq0$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|k_{\\mathrm{TNTK}}^{(B)}(\\pmb{x},\\tilde{\\pmb{x}})-\\frac{1}{M}\\sum_{m=1}^{M}\\left\\langle\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(\\pmb{x};\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(\\tilde{\\pmb{x}};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right|\\leq\\epsilon\\right)}\\\\ &{\\ \\geq1-4\\exp\\left(-\\frac{\\tilde{c}\\epsilon^{2}M}{\\mathcal{L}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $\\tilde{c}>0$ is an absolute constant. ", "page_idx": 18}, {"type": "text", "text": "In proving the above lemmas, following [21], we denote a single soft tree of depth $\\tilde{\\mathcal{D}}$ determined by the internal node parameters $\\pmb{w}\\in\\mathbb{R}^{d(2^{\\tilde{D}}-1)}$ and leaf node parameters $\\pi\\in\\mathbb{R}^{2^{\\tilde{D}}}$ $h_{\\tilde{D}}(\\cdot,w,\\pi)$ ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma B.1. Fix any $\\tilde{\\mathcal{D}}\\leq\\mathcal{D}$ $\\pmb{w}\\in\\mathbb{R}^{d(2^{\\tilde{D}}-1)}$ , and $\\pi\\in\\mathbb{R}^{2^{\\tilde{D}}}$ . From the definition of the soft tree, the following recursive formula holds [21]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\tilde{D}}({\\pmb x},{\\pmb w},\\pi)}\\\\ &{\\ =\\sigma\\left({\\pmb w}^{(T)\\top}{\\pmb x}\\right)h_{\\tilde{D}-1}\\left({\\pmb x},{\\pmb w}^{(L)},{\\pmb\\pi}^{(L)}\\right)+\\left[1-\\sigma\\left({\\pmb w}^{(T)\\top}{\\pmb x}\\right)\\right]h_{\\tilde{D}-1}\\left({\\pmb x},{\\pmb w}^{(R)},{\\pmb\\pi}^{(R)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $h^{(m)}(\\pmb{x};\\pmb{\\theta}^{(m)})=h_{\\mathcal{D}}\\left(\\pmb{x},\\pmb{w}^{(m)},\\pmb{\\pi}^{(m)}\\right)$ .Here, $\\pi^{(L)},\\pi^{(R)}$ represent the parameters of the leaves belonging to the left and right subtrees, respectively. From Eq. (81), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}^{(T)}}h_{\\widetilde{D}}(x,\\mathbf{w},\\pi)=x\\dot{\\sigma}\\left(w^{(T)\\top}x\\right)\\left[h_{\\widetilde{D}-1}\\left(x,w^{(L)},\\pi^{(L)}\\right)-h_{\\widetilde{D}-1}\\left(x,w^{(R)},\\pi^{(R)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\dot{\\sigma}(b):=\\alpha\\exp(-\\alpha^{2}b^{2})/\\sqrt{\\pi}$ is the derivative of $\\sigma(\\cdot)$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{w^{(T)}}h_{\\bar{\\mathcal{D}}}(\\mathbf{x},w,\\pi),\\nabla_{w^{(T)}}h_{\\bar{\\mathcal{D}}}(\\tilde{x},w,\\pi)\\rangle}\\\\ &{=x^{\\top}\\tilde{x}\\dot{\\sigma}\\left(w^{(T)\\top}x\\right)\\dot{\\sigma}\\left(w^{(T)\\top}\\tilde{x}\\right)\\left[h_{\\bar{\\mathcal{D}}-1}\\left(x,w^{(L)},\\pi^{(L)}\\right)h_{\\bar{\\mathcal{D}}-1}\\left(\\tilde{x},w^{(L)},\\pi^{(L)}\\right)\\right.}\\\\ &{\\phantom{=}\\left.-h_{\\bar{\\mathcal{D}}-1}\\left(x,w^{(L)},\\pi^{(L)}\\right)h_{\\bar{\\mathcal{D}}-1}\\left(\\tilde{x},w^{(R)},\\pi^{(R)}\\right)\\right.}\\\\ &{\\phantom{=}\\left.-h_{\\bar{\\mathcal{D}}-1}\\left(x,w^{(R)},\\pi^{(R)}\\right)h_{\\bar{\\mathcal{D}}-1}\\left(\\tilde{x},w^{(L)},\\pi^{(L)}\\right)\\right.}\\\\ &{\\phantom{=}\\left.+h_{\\bar{\\mathcal{D}}-1}\\left(x,w^{(R)},\\pi^{(R)}\\right)h_{\\bar{\\mathcal{D}}-1}\\left(\\tilde{x},w^{(R)},\\pi^{(R)}\\right)\\right]\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, let us define $\\begin{array}{r}{p_{\\tilde{\\mathcal{D}},l}(\\pmb{x},\\pmb{w}):=\\prod_{n=1}^{2^{\\tilde{\\mathcal{D}}}-1}\\sigma\\left(\\pmb{w}_{n}^{\\top}\\pmb{x}\\right)^{\\mathbb{I}_{l\\times\\tilde{n}}}\\left[1-\\sigma\\left(\\pmb{w}_{n}^{\\top}\\pmb{x}\\right)\\right]^{\\mathbb{I}_{l\\setminus n}}}\\end{array}$ as the weight probability function of leaf $l$ in a soft tree of depth $\\tilde{\\mathcal{D}}$ ; then, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{\\tilde{\\mathcal{D}}-1}\\left(\\pmb{x},\\pmb{w}^{(L)},\\pmb{\\pi}^{(L)}\\right)=\\sum_{l=1}^{2^{\\tilde{\\mathcal{D}}-1}}\\pi_{l}^{(L)}p_{\\tilde{\\mathcal{D}}-1,l}\\left(\\pmb{x},\\pmb{w}^{(L)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the sub-Gaussian norm of the normal distribution is bounded from above by a constant multiple of its standard deviation (see, e.g., Example 2.5.6 in [39]), for any $m\\in[M]$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|h_{\\mathcal{D}-1}\\left(\\pmb{x},\\overline{{\\pmb{w}}}^{(m),(L)},\\overline{{\\pmb{\\pi}}}^{(m),(L)}\\right)\\right\\|_{\\psi_{2}}=\\left\\|\\displaystyle\\sum_{l=1}^{2^{D-1}}\\overline{{\\pi}}_{l}^{(m),(L)}p_{\\mathcal{D}-1,l}\\left(\\pmb{x},\\overline{{\\pmb{w}}}^{(m),(L)}\\right)\\right\\|_{\\psi_{2}}}\\\\ {\\leq\\left\\|\\displaystyle\\sum_{l=1}^{2^{D-1}}\\overline{{\\pi}}_{l}^{(m),(L)}\\right\\|_{\\psi_{2}}}\\\\ {\\leq C\\mathcal{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality follows from $\\left|p_{\\mathcal{D}-1,l}\\left(\\pmb{x},\\overline{{\\pmb{w}}}^{(m),(L)}\\right)\\right|\\leq1$ Similarly, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|h_{\\mathcal{D}-1}\\left(\\pmb{x},\\overline{{\\pmb{w}}}^{(m),(R)},\\overline{{\\pmb{\\pi}}}^{(m),(R)}\\right)\\right\\|_{\\psi_{2}}\\leq C\\mathcal{L}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Due to $\\|\\dot{\\sigma}(\\cdot)\\|_{\\infty}\\leq\\alpha/\\sqrt{\\pi},\\|x^{\\top}\\tilde{x}\\|\\leq1$ and Lemma E.4, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\left\\langle\\nabla_{\\overline{{\\mathbf{w}}}^{(T)}}h^{(m)}\\left(x;\\overline{{\\pmb{\\theta}}}^{(m)}\\right),\\nabla_{\\overline{{\\mathbf{w}}}^{(T)}}h^{(m)}\\left(\\tilde{x};\\overline{{\\pmb{\\theta}}}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}\\leq\\frac{4C^{2}\\mathcal{L}^{2}\\alpha^{2}}{\\pi}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From the centering lemma (Lemma E.3), there exists an absolute constant $\\tilde{C}>0$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})-\\left\\langle\\nabla_{\\overline{{\\mathbf{w}}}^{(T)}}h^{(m)}\\left(\\pmb{x};\\overline{{\\theta}}^{(m)}\\right),\\nabla_{\\overline{{\\mathbf{w}}}^{(T)}}h^{(m)}\\left(\\tilde{\\pmb{x}};\\overline{{\\theta}}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}\\leq\\frac{4\\tilde{C}C^{2}\\mathcal{L}^{2}\\alpha^{2}}{\\pi}}\\\\ {\\leq4\\tilde{C}C^{2}\\mathcal{L}^{2}\\alpha^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, taking $\\tilde{C}C^{2}$ as a new absolute constant $C$ and using the independence of parameters for each $m\\in[M/2]$ , the application of Bernstein's inequality (Lemma E.2) yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|k_{\\mathrm{TNTK}}^{(T)}(\\pmb{x},\\tilde{\\pmb{x}})-\\frac{2}{M}\\sum_{m=1}^{M/2}\\left\\langle\\nabla_{\\overline{{\\pmb{w}}}^{(m),(T)}}h^{(m)}\\left(\\pmb{x};\\pmb{\\theta}_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pmb{w}}}^{(m),(T)}}h^{(m)}\\left(\\tilde{\\pmb{x}};\\pmb{\\theta}_{0}^{(m)}\\right)\\right\\rangle\\right|\\geq\\epsilon\\right)}\\\\ &{\\leq2\\exp\\left(-c\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{2K^{2}},\\frac{\\epsilon}{2K}\\right\\}M\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the similar inequality also holds for $m\\in[M]\\setminus[M/2]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\scriptstyle\\>\\left(\\left|k_{\\mathrm{TNTK}}^{(T)}(\\boldsymbol{x},\\tilde{\\boldsymbol{x}})-\\frac{2}{M}\\sum_{m=M/2+1}^{M}\\left\\langle\\nabla_{\\overline{{\\boldsymbol{w}}}^{(m)},(T)}h^{(m)}\\left(\\boldsymbol{x};\\boldsymbol{\\theta}_{0}^{(m)}\\right),\\nabla_{\\overline{{\\boldsymbol{w}}}^{(m)},(T)}h^{(m)}\\left(\\boldsymbol{\\tilde{x}};\\boldsymbol{\\theta}_{0}^{(m)}\\right)\\right\\rangle\\right|\\geq\\epsilon\\right)}\\\\ &{\\scriptstyle\\leq2\\exp\\left(-c\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{2K^{2}},\\frac{\\epsilon}{2K}\\right\\}M\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By taking union bound in Eqs. (92) and (93) and taking $c/2$ as an new absolute constant $c$ ,we obtain the desired result. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma B.2. We only show Eq. (78) for simplicity. Fix any $\\pmb{w}\\ \\in\\ \\mathbb{R}^{d\\mathcal{N}}$ and $\\pi\\,\\in\\,\\mathbb{R}^{\\mathcal{L}}$ corresponding to the parameters of a soft tree of depth $\\mathcal{D}$ . Furthermore, let $\\pmb{w}_{i}$ : and $\\pi_{i}$ $(1\\leq i\\leq N)$ represent the internal node parameter vectors and the leaf node parameter vectors, respectively, for the subtree rooted at the $i$ -th internal node (note that the parameter indices are assigned in breadth-first order, hence by definition, $w_{2:}=w^{(L)}$ \uff0c ${\\pmb w}_{3:}={\\pmb w}^{(R)}$ ). From Eq. (81), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{w}^{(L)}}h_{\\mathcal{D}}(\\pmb{x},\\pmb{w},\\pmb{\\pi})=\\sigma\\left(\\pmb{w}^{(T)^{\\top}}\\pmb{x}\\right)\\nabla_{\\pmb{w}^{(L)}}h_{\\mathcal{D}-1}\\left(\\pmb{x},\\pmb{w}^{(L)},\\pmb{\\pi}^{(L)}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given that $\\|\\sigma(\\cdot)\\|_{\\infty}\\leq1$ , for any $m\\in[M]$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\langle\\nabla_{\\overline{{w}}^{(m),(L)}}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{w^{(m),(L)}}h^{(m)}\\left(\\tilde{x};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\leq\\left\\|\\left\\langle\\nabla_{\\overline{{w}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(x,\\overline{{w}}^{(m),(L)},\\overline{{\\pi}}^{(m),(L)}\\right),\\nabla_{\\overline{{w}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(\\tilde{x},\\overline{{w}}^{(m),(L)},\\overline{{\\pi}}^{(m),(L)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, decomposing the gradient of the subtree rooted at the left child of the root node, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla_{w^{(L)}}h_{{\\mathcal D}-1}\\left(\\pmb{x},\\pmb{w}^{(L)},\\pmb{\\pi}^{(L)}\\right),\\nabla_{w^{(L)}}h_{{\\mathcal D}-1}\\left(\\tilde{\\pmb{x}},\\pmb{w}^{(L)},\\pmb{\\pi}^{(L)}\\right)\\right\\rangle}\\\\ &{=\\left\\langle\\nabla_{w_{2:}}h_{{\\mathcal D}-1}\\left(\\pmb{x},w_{2:},\\pi_{2:}\\right),\\nabla_{w_{2:}}h_{{\\mathcal D}-1}\\left(\\tilde{\\pmb{x}},w_{2:},\\pi_{2:}\\right)\\right\\rangle}\\\\ &{=\\left\\langle\\nabla_{w_{2:}^{(T)}}h_{{\\mathcal D}-1}\\left(\\pmb{x},w_{2:},\\pi_{2:}\\right),\\nabla_{w_{2:}^{(T)}}h_{{\\mathcal D}-1}\\left(\\tilde{\\pmb{x}},w_{2:},\\pi_{2:}\\right)\\right\\rangle}\\\\ &{+\\left\\langle\\nabla_{w_{2:}^{(L)}}h_{{\\mathcal D}-1}\\left(\\pmb{x},w_{2:},\\pi_{2:}\\right),\\nabla_{w_{2:}^{(L)}}h_{{\\mathcal D}-1}\\left(\\tilde{\\pmb{x}},w_{2:},\\pi_{2:}\\right)\\right\\rangle}\\\\ &{+\\left\\langle\\nabla_{w_{2:}^{(R)}}h_{{\\mathcal D}-1}\\left(\\pmb{x},w_{2:},\\pi_{2:}\\right),\\nabla_{w_{2:}^{(R)}}h_{{\\mathcal D}-1}\\left(\\tilde{\\pmb{x}},w_{2:},\\pi_{2:}\\right)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Considering that $\\pmb{w}_{2}$ : are parameters for a soft tree with $\\mathcal{L}/2$ leaves, similar to the proof of Lemma B.1, there exists an absolute constant $C$ such that for any $m\\in[M]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\langle\\nabla_{\\overline{{\\mathbf{w}}}_{2;}^{(m)}}(r)h_{\\mathcal{D}-1}\\left(x,\\overline{{\\mathbf{w}}}_{2;}^{(m)},\\overline{{\\mathbf{\\pi}}}_{2:}^{(m)}\\right),\\nabla_{\\overline{{\\mathbf{w}}}_{2;}^{(m)}}(r)h_{\\mathcal{D}-1}\\left(\\widetilde{x},\\overline{{\\mathbf{w}}}_{2;}^{(m)},\\overline{{\\mathbf{\\pi}}}_{\\overline{{\\pi}}_{2:}^{(m)}}^{(m)}\\right)\\right\\rangle\\right|_{\\psi_{1}}\\leq4\\alpha^{2}C\\pi^{-1}(\\mathcal{L}/2)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly to Eq. (95), we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\langle\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(x,\\overline{{\\mathbf w}}_{2:}^{(m)},\\overline{{\\pi}}_{2:}^{(m)}\\right),\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(\\widetilde{x},\\overline{{\\mathbf w}}_{2:}^{(m)},\\overline{{\\pi}}_{2:}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\leq\\left\\|\\left\\langle\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(L)}}h_{\\mathcal{D}-2}\\left(x,\\overline{{\\mathbf w}}_{2:}^{(m),(L)},\\overline{{\\pi}}_{2:}^{(m),(L)}\\right),\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(L)}}h_{\\mathcal{D}-2}\\left(\\widetilde{x},\\overline{{\\mathbf w}}_{2:}^{(m),(L)},\\overline{{\\pi}}_{2:}^{(m),(L)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, for the right subtree: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\langle\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(R)}}h_{\\mathcal{D}-1}\\left(x,\\overline{{\\mathbf w}}_{2:}^{(m)},\\overline{{\\pi}}_{2:}^{(m)}\\right),\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(R)}}h_{\\mathcal{D}-1}\\left(\\widetilde{x},\\overline{{\\mathbf w}}_{2:}^{(m)},\\overline{{\\pi}}_{2:}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\leq\\left\\|\\left\\langle\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(R)}}h_{\\mathcal{D}-2}\\left(x,\\overline{{\\mathbf w}}_{2:}^{(m),(R)},\\overline{{\\pi}}_{2:}^{(m),(R)}\\right),\\nabla_{\\overline{{\\mathbf w}}_{2:}^{(m),(R)}}h_{\\mathcal{D}-2}\\left(\\widetilde{x},\\overline{{\\mathbf w}}_{2:}^{(m),(R)},\\overline{{\\pi}}_{2:}^{(m),(R)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\langle\\nabla_{\\overline{{w}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(x,\\overline{{w}}^{(m),(L)},\\overline{{\\pi}}^{(m),(L)}\\right),\\nabla_{\\overline{{w}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(\\tilde{x},\\overline{{w}}^{(m),(L)},\\overline{{\\pi}}^{(m),(L)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\leq4\\alpha^{2}C\\pi^{-1}(\\mathcal{L}/2)^{2}}\\\\ &{\\quad+\\left\\|\\left\\langle\\nabla_{\\overline{{w}}_{2;}^{(m),(L)}}h_{\\mathcal{D}-2}\\left(x,\\overline{{w}}_{2;}^{(m),(L)},\\overline{{\\pi}}_{2;}^{(m),(L)}\\right),\\nabla_{\\overline{{w}}_{2;}^{(m),(L)}}h_{\\mathcal{D}-2}\\left(\\tilde{x},\\overline{{w}}_{2;}^{(m),(L)},\\overline{{\\pi}}_{2;}^{(m),(L)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\quad+\\left\\|\\left\\langle\\nabla_{\\overline{{w}}_{2;}^{(m),(R)}}h_{\\mathcal{D}-2}\\left(x,\\overline{{w}}_{2;}^{(m),(R)},\\overline{{\\pi}}_{2;}^{(m),(R)}\\right),\\nabla_{\\overline{{w}}_{2;}^{(m),(R)}}h_{\\mathcal{D}-2}\\left(\\tilde{x},\\overline{{w}}_{2;}^{(m),(R)},\\overline{{\\pi}}_{2;}^{(m),(R)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By repeating the above described argument, we can further decompose the second and third term of Eq. (102) as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left\\langle\\nabla_{\\overline{{\\mathbf{w}}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(x,\\overline{{\\mathbf{w}}}^{(m),(L)},\\overline{{\\mathbf{\\pi}}}^{(m),(L)}\\right),\\nabla_{\\overline{{\\mathbf{w}}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(\\widetilde{x},\\overline{{\\mathbf{w}}}^{(m),(L)},\\overline{{\\mathbf{\\pi}}}^{(m),(L)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq4\\alpha^{2}C\\pi^{-1}(C/2)^{2}}\\\\ &{\\quad+\\left.2\\times4\\alpha^{2}C\\pi^{-1}(C/4)^{2}\\right.}\\\\ &{\\quad+\\left.\\left\\|\\left\\langle\\nabla_{\\overline{{w}}_{\\mathrm{s}}^{(m)}}h_{{D}-3}\\left(\\mathbf{x},\\overline{{w}}_{\\mathrm{s}}^{(m)},\\overline{{\\pi}}_{\\mathrm{s}}^{(m)}\\right),\\nabla_{\\overline{{w}}_{\\mathrm{s}}^{(m)}}h_{{D}-3}\\left(\\widetilde{x},\\overline{{w}}_{\\mathrm{s}}^{(m)},\\overline{{\\pi}}_{\\mathrm{s}}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\quad+\\left.\\left\\|\\left\\langle\\nabla_{\\overline{{w}}_{\\mathrm{s}}^{(m)}}h_{{D}-3}\\left(\\mathbf{x},\\overline{{w}}_{\\mathrm{p}_{\\mathrm{s}}^{(m)}}^{(m)},\\overline{{\\pi}}_{\\mathrm{p}_{\\mathrm{s}}^{(m)}}^{(m)}\\right),\\nabla_{\\overline{{w}}_{\\mathrm{p}}^{(m)}}h_{{D}-3}\\left(\\widetilde{x},\\overline{{w}}_{\\mathrm{p}_{\\mathrm{s}}^{(m)}}^{(m)},\\overline{{\\pi}}_{\\mathrm{p}_{\\mathrm{s}}^{(m)}}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\quad+\\left.\\left\\|\\left\\langle\\nabla_{\\overline{{w}}_{\\mathrm{in}}^{(m)}}h_{{D}-3}\\left(\\mathbf{x},\\overline{{w}}_{\\mathrm{in}}^{(m)},\\overline{{\\pi}}_{\\mathrm{in}}^{(m)}\\right),\\nabla_{\\overline{{w}}_{\\mathrm{in}}^{(m)}}h_{{D}-3}\\left(\\widetilde{x},\\overline{{w}}_{\\mathrm{in}}^{(m)},\\overline{{\\pi}}_{\\mathrm{in}}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\quad+\\left.\\left\\|\\left\\langle\\nabla_{\\overline{{w}}_{\\mathrm{in}}^{(m)}}h_{{D}-3}\\left(\\mathbf{x},\\overline{{w}}_{\\mathrm{in}}^{(m)},\\overline{{\\pi}}_{\\mathrm{in}}^{(m\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By recursively applying the above discussion until reaching the leaves of the tree, we find: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\langle\\nabla_{\\overline{{w}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(x,\\overline{{w}}^{(m),(L)},\\overline{{\\pi}}^{(m),(L)}\\right),\\nabla_{\\overline{{w}}^{(m),(L)}}h_{\\mathcal{D}-1}\\left(\\tilde{x},\\overline{{w}}^{(m),(L)},\\overline{{\\pi}}^{(m),(L)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\leq4\\alpha^{2}C\\pi^{-1}\\left(\\displaystyle\\frac{\\mathcal{L}}{2}\\right)^{2}+2\\times4\\alpha^{2}C\\pi^{-1}\\left(\\displaystyle\\frac{\\mathcal{L}}{4}\\right)^{2}+\\cdot\\cdot\\cdot+2^{{\\mathcal{D}}-2}\\times4\\alpha^{2}C\\pi^{-1}\\left(\\displaystyle\\frac{\\mathcal{L}}{2^{\\mathcal{D}-1}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we conclude: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\langle\\nabla_{\\overline{{w}}^{(m),(L)}}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{w^{(m),(L)}}h^{(m)}\\left(\\tilde{x};\\theta_{0}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{1}}}\\\\ &{\\leq\\frac{4\\alpha^{2}C}{\\pi}\\sum_{i=1}^{p-1}\\frac{\\mathcal{L}^{2}}{2^{2i}}}\\\\ &{\\leq\\frac{2\\alpha^{2}C\\mathcal{L}^{2}}{\\pi}\\sum_{i=1}^{p-1}2^{-i}}\\\\ &{\\leq\\frac{2\\alpha^{2}C\\mathcal{L}^{2}}{\\pi}}\\\\ &{\\leq\\frac{2\\alpha^{2}C\\mathcal{L}^{2}}{\\pi}}\\\\ &{\\leq4\\alpha^{2}C\\mathcal{L}^{2}}\\\\ &{=K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, by applying centering lemma (Lemma E.3), Bernstein's inequality (Lemma E.2), and the union bound, we obtain the desired result. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma B.3. From the definition of $h^{(m)}$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(\\pmb{x};\\pmb{\\theta}_{0}^{(m)}\\right)=\\left(p_{1}\\left(\\pmb{x};\\overline{{\\pi}}^{(m)}\\right),\\dots,p_{\\mathcal{L}}\\left(\\pmb{x};\\overline{{\\pi}}^{(m)}\\right)\\right)^{\\top}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noting that $|p_{l}\\left(\\pmb{x};\\pmb{w}\\right)|\\leq1$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left\\langle\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right),\\nabla_{\\overline{{\\pi}}^{(m)}}h^{(m)}\\left(x;\\theta_{0}^{(m)}\\right)\\right\\rangle\\right\\|_{\\psi_{2}}\\leq\\displaystyle\\sum_{l=1}^{\\mathcal{L}}\\left\\|p_{l}\\left(x;\\overline{{w}}^{(m)}\\right)^{2}\\right\\|_{\\psi_{2}}}\\\\ {\\leq C\\mathcal{L}.\\hfill}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, by applying the centering lemma (Lemma E.3) and the general Hoeffding's inequality (Lemma E.1) with union bounds, the desired result is obtained. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma 3.2 is derived by taking a union bound over the three preceding lemmas and rearranging the entireexpression. ", "page_idx": 21}, {"type": "text", "text": "Prof of Lemma .2. Fix any $\\epsilon>0$ such that $\\epsilon\\leq K$ Then, $\\begin{array}{r}{\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{K^{2}},\\frac{\\epsilon}{K}\\right\\}=\\frac{\\epsilon^{2}}{K^{2}}}\\end{array}$ .Now, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{M\\ge\\displaystyle\\frac{K^{2}}{c\\epsilon^{2}}\\ln\\displaystyle\\frac{16}{\\delta}\\Rightarrow1-4\\exp\\left(-c\\displaystyle\\frac{\\epsilon^{2}}{K^{2}}M\\right)\\ge1-\\displaystyle\\frac{\\delta}{4},}}\\\\ {{M\\ge\\displaystyle\\frac{\\mathcal{L}^{2}}{\\tilde{c}\\epsilon^{2}}\\ln\\displaystyle\\frac{16}{\\delta}\\Rightarrow1-4\\exp\\left(-\\displaystyle\\frac{\\tilde{c}\\epsilon^{2}M}{\\mathcal{L}^{2}}\\right)\\ge1-\\displaystyle\\frac{\\delta}{4}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, from Lemma B.1, Lemma B.2, and Lemma B.3, by applying the union bound, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{M\\geq\\operatorname*{max}\\left\\{\\displaystyle\\frac{K^{2}}{c},\\frac{\\mathcal{L}^{2}}{\\tilde{c}}\\right\\}\\epsilon^{-2}\\ln\\frac{16}{\\delta}}\\\\ {\\Rightarrow\\mathbb{P}\\left(\\left|k_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})-\\langle g(\\pmb{x},\\pmb{\\theta}_{0}),g(\\tilde{\\pmb{x}},\\pmb{\\theta}_{0})\\rangle\\right|\\leq4\\varepsilon\\right)\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, let $\\tilde{C}=\\operatorname*{max}\\{1/c,1/\\tilde{c}\\}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M\\ge\\Tilde{C}\\operatorname*{max}\\left\\{K^{2},\\mathcal{L}^{2}\\right\\}\\epsilon^{-2}\\ln\\frac{16}{\\delta}}\\\\ &{\\Rightarrow M\\ge\\operatorname*{max}\\left\\{\\frac{K^{2}}{c},\\frac{\\mathcal{L}^{2}}{\\tilde{c}}\\right\\}\\epsilon^{-2}\\ln\\frac{16}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By defining C2) as C(2) $C_{\\alpha,\\mathcal{D}}^{(2)}=K$ , the desired result is obtained. ", "page_idx": 22}, {"type": "text", "text": "B.2Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof scketch  Since the parameters of the different soft trees are independent, we can confirm that theHessian $H(x,\\theta)$ is given as the block diagonal matrix. Since we know the fact that the spectral norm of the block diagonal matrix equals the maximum over the spectral norms of the block matrix, the remaining interest is the upper bound of the spectral norm of each block matrix. Then, we obtain Lemma 3.3 by carefully evaluating the upper bound of the spectral norm of each block matrix with its Frobenius norm. ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 3.3. Define $H^{(m)}\\left(\\pmb{x},\\pmb{\\theta}^{(m)}\\right)=\\nabla_{\\pmb{\\theta}^{(m)}}^{2}h^{(m)}\\left(\\pmb{x};\\pmb{\\theta}^{(m)}\\right)\\in\\mathbb{R}^{\\tilde{p}\\times\\tilde{p}}$ where $\\tilde{p}=d\\mathcal{N}\\!+\\!\\mathcal{L}$ Then, $H(x,\\theta)$ is represented by the following block diagonal matrix: ", "page_idx": 22}, {"type": "equation", "text": "$$\nH(\\pmb{x},\\pmb{\\theta})=\\frac{1}{\\sqrt{M}}\\left(\\begin{array}{c c c c}{H^{(1)}\\left(\\pmb{x},\\pmb{\\theta}^{(1)}\\right)}&{\\mathbf{0}_{\\tilde{p}\\times\\tilde{p}}}&{\\cdot\\cdot\\cdot}&{\\mathbf{0}_{\\tilde{p}\\times\\tilde{p}}}\\\\ {\\mathbf{0}_{\\tilde{p}\\times\\tilde{p}}}&{H^{(2)}\\left(\\pmb{x},\\pmb{\\theta}^{(2)}\\right)}&{\\cdot\\cdot\\cdot}&{\\mathbf{0}_{\\tilde{p}\\times\\tilde{p}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}_{\\tilde{p}\\times\\tilde{p}}}&{\\mathbf{0}_{\\tilde{p}\\times\\tilde{p}}}&{\\cdot\\cdot}&{H^{(M)}\\left(\\pmb{x},\\pmb{\\theta}^{(M)}\\right)}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{0}_{\\widetilde{p}\\times\\widetilde{p}}$ represents a $\\tilde{p}\\times\\tilde{p}$ zero matrix. Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|H(x,\\pmb\\theta)\\right\\|=\\frac{1}{\\sqrt{M}}\\operatorname*{max}_{m\\in[M]}\\left\\|H_{m}\\left(\\pmb x,\\pmb\\theta^{(m)}\\right)\\right\\|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, assume the following event holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall m\\in[M],\\,\\forall l\\in[{\\mathcal{L}}],\\,\\forall n\\in[N],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\overline{{\\pi}}_{l}^{(m)}\\right|\\leq\\sqrt{2\\ln\\frac{2M(\\mathscr{L}+\\mathscr{N})}{\\delta}}\\mathrm{~and~}\\left|\\overline{{\\pmb{w}}}_{n}^{(m)\\top}\\pmb{x}\\right|\\leq\\sqrt{2\\ln\\frac{2M(\\mathscr{L}+\\mathscr{N})}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\pmb{\\theta}_{0}$ is initialized by a standard normal distribution, by the union bound, the above event occurs with probability at least $1-\\delta$ . Therefore, it is sufficient to show that Eq. (11) holds under the event (128). ", "page_idx": 22}, {"type": "text", "text": "Now, the derivatives of $h^{(m)}(\\pmb{x};\\pmb{\\theta}^{(m)})$ up to the second order are given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\displaystyle\\frac{\\partial^{2}h^{(m)}\\left(\\pmb x;\\pmb\\theta^{(m)}\\right)}{\\partial\\pmb w_{n}^{(m)}\\partial\\pmb w_{\\tilde{n}}^{(m)}}=\\sum_{l=1}^{\\mathcal{L}}\\pi_{l}^{(m)}\\frac{\\partial^{2}p_{l}(\\pmb x;\\pmb w^{(m)})}{\\partial\\pmb w_{n}^{(m)}\\partial\\pmb w_{\\tilde{n}}^{(m)}},}&\\\\ &{\\displaystyle\\frac{\\partial^{2}h^{(m)}\\left(\\pmb x;\\pmb\\theta^{(m)}\\right)}{\\partial\\pmb w_{n}^{(m)}\\partial\\pi_{l}^{(m)}}=\\frac{\\partial p_{l}(\\pmb x;\\pmb w^{(m)})}{\\partial\\pmb w_{n}^{(m)}},}&\\\\ &{\\displaystyle\\frac{\\partial^{2}h^{(m)}\\left(\\pmb x;\\pmb\\theta^{(m)}\\right)}{\\partial\\pi_{l}^{(m)}\\partial\\pi_{\\tilde{l}}^{(m)}}=0.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From the definition of $p_{l}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial p_{\\mathrm{P}}(x,\\mathrm{e}^{\\boldsymbol{\\pi}(n)})}{\\partial x_{\\mathrm{P}}(n)}=}&{\\left[\\mathbf{1}_{i\\nearrow\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}\\mathbf{\\Lambda}_{\\mathbf{x}}^{\\mathrm{o}}\\right]=\\left[\\mathbf{1}_{i\\nearrow\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}\\mathbf{\\Lambda}_{\\mathbf{x}}^{\\mathrm{o}}\\right]=\\mathbf{1}_{i\\nearrow\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}\\mathbf{\\Lambda}_{\\mathbf{x}}^{\\mathrm{o}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\times\\prod_{j=i}^{n}\\sigma\\left(w_{i\\setminus i}^{(n)\\pi}\\mathbf{\\Lambda}_{j}^{\\mathrm{t}}\\right)^{\\mathbf{1}_{i\\ne j}}\\left[1-\\sigma\\left(w_{i\\setminus i}^{(n)\\pi}\\mathbf{\\Lambda}_{x}^{\\mathrm{o}}\\right)\\right]^{\\mathbf{1}_{i\\searrow\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}},}\\\\ {\\frac{\\partial^{2}p_{\\mathrm{P}}(x,\\mathrm{e}^{\\boldsymbol{\\pi}(n)})}{\\partial w_{\\mathrm{P}}^{(n)}\\partial x_{\\mathrm{P}}^{(n)}}=}&{\\left[\\mathbf{1}_{i\\nearrow\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}\\mathbf{\\Lambda}_{\\mathbf{x}}^{\\mathrm{o}\\pi}\\right]^{-}\\left[\\mathbf{w}_{i\\setminus i}^{(n)\\pi}\\mathbf{\\Lambda}_{\\mathbf{x}}^{\\mathrm{o}}\\right]^{-}\\mathbf{1}_{i\\setminus\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}\\mathbf{\\Lambda}_{j}^{\\mathrm{o}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\times\\prod_{j=i}^{n}\\left(w_{i\\setminus i}^{(n)\\pi}\\mathbf{\\Lambda}_{j}^{\\mathrm{o}}\\right)^{\\mathbf{1}_{i\\ne j,\\mathrm{e}^{\\boldsymbol{\\pi}(n)}}}\\left[1-\\sigma\\left(w_{i\\setminus i}^{(n)\\pi}\\mathbf{\\Lambda}_{x}^{\\mathrm{o}}\\right)\\right]^{\\mathbf{1}_{i\\setminus i}},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial^{2}p_{\\mathrm{P}}(x,\\mathrm{e}^{\\boldsymbol{\\pi}(n)})}{\\partial w_{\\mathrm{P}}^{(n)}\\partial x_{\\mathrm{P}}^\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the third equation, it was assumed that $n\\not=\\hat{n}$ . Now, let us evaluate the upper bound of the above expressions. First, from the definition of $\\sigma(\\cdot)$ , we know that $\\|\\sigma(\\cdot)\\|_{\\infty}\\leq1$ and $\\lVert\\dot{\\sigma}(\\cdot)\\rVert_{\\infty}\\leq\\alpha/\\sqrt{\\pi}$ Additionally, for any $a\\in\\mathbb{R}$ $|\\ddot{\\sigma}(a)|\\leq2|a|\\alpha^{2}/\\sqrt{\\pi}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\left\\vert\\frac{\\partial^{2}p_{l}}{\\partial w_{n}^{(m)}\\partial w_{n}^{(m)}}\\right\\vert\\right\\vert_{F}\\leq\\left\\vert\\vec{\\sigma}\\left({\\pmb w}_{n}^{(m)\\top}{\\pmb x}\\right)\\right\\vert\\left\\vert\\left\\vert\\mathbb{I}_{l<n}x{\\pmb x}^{\\top}-\\mathbb{I}_{n\\setminus{l}}x{\\pmb x}^{\\top}\\right\\vert\\right\\vert_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\left\\vert{\\pmb w}_{n}^{(m)\\top}{\\pmb x}\\right\\vert\\frac{\\alpha^{2}}{\\sqrt{\\pi}}\\left\\|{\\pmb x}x^{\\top}\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\left(\\left\\vert{\\pmb w}_{n}^{(m)\\top}x-{\\pmb w}_{n}^{(m)\\top}x\\right\\vert+\\left\\vert{\\pmb w}_{n}^{(m)\\top}x\\right\\vert\\right)\\frac{\\alpha^{2}}{\\sqrt{\\pi}}\\left\\|{\\pmb x}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\left(R+\\sqrt{2\\ln\\frac{2M(\\mathscr{L}+{N})}{\\delta}}\\right)\\frac{\\alpha^{2}}{\\sqrt{\\pi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, as for $n\\not=\\hat{n}$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left|\\left.\\left|\\frac{\\partial^{2}p_{j}\\left(\\boldsymbol{x};\\boldsymbol{w}^{(m)}\\right)}{\\partial\\boldsymbol{w}_{n}^{(m)}\\partial\\boldsymbol{w}_{\\hat{n}}^{(m)}}\\right|\\right|_{\\boldsymbol{F}}}&{\\quad(13\\boldsymbol{\\xi})}\\\\ &{\\le\\left|\\left[\\mathbb{I}_{l\\times\\tau^{h}}\\boldsymbol{x}\\hat{\\sigma}\\left(\\boldsymbol{w}_{n}^{(m)\\top}\\boldsymbol{x}\\right)-\\mathbb{I}_{n\\setminus\\tau^{\\lambda}}\\boldsymbol{x}\\hat{\\sigma}\\left(\\boldsymbol{w}_{n}^{(m)\\top}\\boldsymbol{x}\\right)\\right]\\right|}&{\\quad(14\\boldsymbol{\\xi})}\\\\ &{\\quad\\cdot\\left[\\mathbb{I}_{l\\times\\hat{\\tau}^{\\Lambda}}\\boldsymbol{x}\\hat{\\sigma}\\left(\\boldsymbol{w}_{\\hat{n}}^{(m)\\top}\\boldsymbol{x}\\right)-\\mathbb{I}_{\\hat{\\tau}\\setminus\\tau^{\\lambda}}\\boldsymbol{x}\\hat{\\sigma}\\left(\\boldsymbol{w}_{\\hat{n}}^{(m)\\top}\\boldsymbol{x}\\right)\\right]^{\\top}\\right|_{\\boldsymbol{F}}}&{\\quad(14\\boldsymbol{\\xi})}\\\\ &{=\\left|\\left\\|\\mathbb{I}_{l\\times\\tau^{h}}\\mathbb{I}_{l\\times\\hat{\\tau}^{\\hat{n}}}\\left(\\boldsymbol{w}_{n}^{(m)\\top}\\boldsymbol{x}\\right)\\hat{\\sigma}\\left(\\boldsymbol{w}_{\\hat{n}}^{(m)\\top}\\boldsymbol{x}\\right)\\boldsymbol{x}\\boldsymbol{x}^{\\top}-\\mathbb{I}_{n\\setminus\\tau^{\\mathbb{I}}}\\mathbb{I}_{l\\times\\hat{\\tau}^{\\hat{n}}}\\left(\\boldsymbol{w}_{n}^{(m)\\top}\\boldsymbol{x}\\right)\\hat{\\sigma}\\left(\\boldsymbol{w}_{\\hat{n}}^{(m)\\top}\\boldsymbol{x}\\right)\\boldsymbol{x}\\boldsymbol{x}^{\\top}\\right.}\\\\ &{\\quad\\left.-\\mathbb{I}_{l\\times\\tau^{h}}\\mathbb{I}_{\\hat{\\tau}\\setminus\\tau^{\\hat{n}}}\\left(\\boldsymbol{w}_{n}^{(m)\\top}\\boldsymbol{x}\\right)\\hat{\\sigma}\\left(\\boldsymbol{w}_{\\hat{n}}^{(m)\\top}\\boldsymbol{x}\\right)\\boldsymbol{x}\\boldsymbol{x}^{\\top}+\\mathbb{I}_{n\\setminus\\tau^{\\mathbb{I}}}\\mathbb{I}_{\\hat{\\tau}\\setminus\\tau^{\\hat{n}}}\\left(\\boldsymbol{w}_{n}^{(m)\\top}\\boldsymbol{x}\\right)\\hat{\\sigma\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\le\\|\\dot{{\\boldsymbol\\sigma}}(\\cdot)\\|_{\\infty}^{2}\\|{\\boldsymbol x}{\\boldsymbol x}^{\\top}\\|_{F}}\\\\ {\\le\\displaystyle\\frac{\\alpha^{2}}{\\pi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\frac{\\partial^{2}h^{(m)}}{\\partial{w_{n}^{(m)}}\\partial{w_{\\hat{n}}^{(m)}}}\\right\\|_{F}\\leq\\displaystyle\\sum_{l=1}^{\\ell}\\left|\\pi_{l}^{(m)}\\right|\\left\\|\\frac{\\partial^{2}p_{l}\\left(\\mathbf{x};{w^{(m)}}\\right)}{\\partial{w_{n}^{(m)}}\\partial{w_{\\hat{n}}^{(m)}}}\\right\\|_{F}}}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{\\ell}\\left(\\left|\\pi_{l}^{(m)}-\\overline{{\\pi}}_{l}^{(m)}\\right|+\\left|\\overline{{\\pi}}_{l}^{(m)}\\right|\\right)\\left\\|\\frac{\\partial^{2}p_{l}\\left(\\mathbf{x};{w^{(m)}}\\right)}{\\partial{w_{n}^{(m)}}\\partial{w_{\\hat{n}}^{(m)}}}\\right\\|_{F}}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{\\ell}\\left(R+\\sqrt{2\\ln\\frac{2M(\\mathcal{L}+\\mathcal{N})}{\\delta}}\\right)\\left\\|\\frac{\\partial^{2}p_{l}\\left(\\mathbf{x};{w^{(m)}}\\right)}{\\partial{w_{n}^{(m)}}\\partial{w_{\\hat{n}}^{(m)}}}\\right\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{N}\\left\\{\\begin{array}{l}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{R}^{(i)})^{\\top}}\\\\ {\\vdots}\\\\ {\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{R}^{(i)})^{\\top}}\\end{array}\\right\\}^{l}}}\\\\ &{=\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left(\\frac{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{R}^{(i)})^{\\top}}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}\\right)_{j}^{l}+\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left\\{\\frac{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})^{\\top}}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}\\right\\}^{l}}\\\\ &{\\leq\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left(\\frac{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})^{\\top}}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}\\right)^{l}}\\\\ &{\\leq\\frac{\\rho}{2}\\left[\\frac{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})^{\\top}}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}\\right]^{l},\\quad\\leq\\frac{\\rho}{2}\\left[\\frac{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})^{\\top}}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}\\right]^{l}+\\frac{\\rho}{2}\\frac{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})^{\\top}}{\\rho(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}}\\\\ &{\\leq\\frac{\\rho}{2}\\left[\\sum_{i=1}^{N}\\left(\\alpha+\\sqrt{2\\pi\\frac{2(\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)}\\cdot\\mathbf{r}^{(i)})}{\\rho(\\mathbf{r}^{(i)\n$$$$\n\\begin{array}{r l}&{\\leq\\frac{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{K}\\left(R+\\sqrt{2\\ln\\frac{2M(C+X)}{\\delta}}\\right)\\Bigg|\\frac{\\partial\\rho_{\\mathbb{P}}(x,\\ln\\sigma(x))}{\\partial x^{\\prime}\\partial y^{\\prime}\\partial z^{\\prime}}\\Bigg|\\right]^{2}}\\\\ &{~~~+\\frac{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{K}\\left(R+\\sqrt{2\\ln\\frac{2M(C+X)}{\\delta}}\\right)\\Bigg|\\frac{\\partial\\rho_{\\mathbb{P}}(x,\\ln\\sigma(x))}{\\partial y^{\\prime}\\partial z^{\\prime}}\\Bigg|\\right]}{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{K}\\left(\\ln\\frac{2M(C+X)}{\\delta}\\right)\\right]^{2}}\\Bigg|\\frac{\\partial\\rho_{\\mathbb{P}}(x,\\ln\\sigma(x))}{\\partial x^{\\prime}\\partial y^{\\prime}\\partial z^{\\prime}}\\Bigg|\\Bigg|_{\\mathbb{R}_{\\geq}}^{2}}\\\\ &{~~~+\\frac{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{K}\\left(\\ln\\frac{2M(C+X)}{\\delta}\\right)\\right]^{2}}{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{K}\\left(\\ln\\frac{2M(C+X)}{\\delta}\\right)\\right]^{2}}\\Bigg|_{\\mathbb{R}_{>}}^{4}+\\sum_{i=1}^{K}\\sigma_{\\mathbb{P}}^{\\prime}\\left(R+\\sqrt{2\\ln\\frac{2M(C+X)}{\\delta}}\\right)^{2}\\frac{\\sigma_{\\mathbb{P}}^{2}}{\\sigma_{\\mathbb{P}}^{2}}}\\\\ &{~~~~+2\\delta^{2}C_{\\mathbb{P}}^{2}}\\\\ &{~~~~4D C_{\\mathbb{P}}^{2}\\left(R+\\sqrt{2\\ln\\frac{2M(C+X)}{\\delta}}\\right)^{4}\\frac{\\sigma_{\\mathbb{P}}^{4}}{\\sigma_{\\mathbb{P}}}+2\\delta^{2}C_{\\mathbb{P}}^{2}}\\\\ &{~~~~\\delta\\ln\\sigma(x)\\geq\\left(R+\\sqrt{2\\ln\\frac{2M(C+X)}{\\delta}}\\right)^{2}+\\delta^{2}\\Delta^{2}C_{\\mathbb{P}}^{2}\\alpha^{\\prime}}\\\\ &{~~~~\\delta\\ln\\sigma(x)\\geq\\left(R+\\sqrt{2\\ln\\frac{2M(C+X)}{\\delta}}\\right)^{2}+\\delta^{2}\\Delta^{2 \n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where: ", "page_idx": 24}, {"type": "text", "text": "\u00b7Eq. (150) follows from Eq. (131).   \n\u00b7 Eq. (151) follows from Eqs. (146) and (130). ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The first and second term of Eq. (152) follows from Eq. (143) and Eq. (138), re$\\left\\|\\frac{\\partial p_{l}\\Big(\\pmb{x};\\pmb{w}^{(j)}\\Big)}{\\partial\\pmb{w}_{n}^{(j)}}\\right\\|_{F}\\,\\leq$ $\\begin{array}{r}{\\left\\|\\mathbb{I}_{l\\times\\mathcal{n}}\\pmb{x}\\dot{\\sigma}\\left(\\pmb{w}_{n}^{(j)\\top}\\pmb{x}\\right)-\\mathbb{I}_{n\\setminus\\mathcal{l}}\\pmb{x}\\dot{\\sigma}\\left(\\pmb{w}_{n}^{(j)\\top}\\pmb{x}\\right)\\right\\|_{F}\\le\\|\\dot{\\sigma}(\\cdot)\\|_{\\infty}\\le\\frac{\\alpha}{\\sqrt{\\pi}}.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "\u00b7Eq. (154) follows from $1/\\pi\\leq1$ and $\\alpha\\geq1$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{follows\\;from}\\left(R+\\sqrt{2\\ln\\frac{2M(\\mathcal{L}+\\mathcal{N})}{\\delta}}\\right)^{4}\\geq(\\sqrt{2\\ln2})^{4}\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By combining Eq. (155) with Eq. (127), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|H(x,\\theta)\\|\\leq\\frac{\\sqrt{6}\\alpha^{2}\\!\\mathcal{N}\\mathcal{L}}{\\sqrt{M}}\\left(R+\\sqrt{2\\ln\\frac{2M(\\mathcal{L}+\\mathcal{N})}{\\delta}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\sqrt{6}\\alpha^{2}2^{2D}}{\\sqrt{M}}\\left(R+\\sqrt{2\\ln\\frac{2M(\\mathcal{L}+\\mathcal{N})}{\\delta}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, from the denitionf ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{6}\\alpha^{2}2^{2D}(R+\\sqrt{2})^{2}=C_{\\alpha,D}^{(3)}(R+\\sqrt{2})^{2}}\\\\ &{\\Rightarrow\\sqrt{6}\\alpha^{2}2^{2D}\\left(\\cfrac{R}{\\sqrt{\\ln\\frac{2M(C+N)}{\\delta}}}+\\sqrt{2}\\right)^{2}\\leq C_{\\alpha,D}^{(3)}(R+\\sqrt{2})^{2}}\\\\ &{\\Leftrightarrow\\frac{\\sqrt{6}\\alpha^{2}2^{2D}}{\\sqrt{M}}\\left(R+\\sqrt{2\\ln\\frac{2M(C+N)}{\\delta}}\\right)^{2}\\leq\\cfrac{C_{\\alpha,D}^{(3)}(R+\\sqrt{2})^{2}}{\\sqrt{M}}\\ln\\frac{2M(C+N)}{\\delta}}\\\\ &{\\Leftrightarrow\\frac{\\sqrt{6}\\alpha^{2}2^{2D}}{\\sqrt{M}}\\left(R+\\sqrt{2\\ln\\frac{2M(C+N)}{\\delta}}\\right)^{2}\\leq\\cfrac{C_{\\alpha,D}^{(3)}(R+\\sqrt{2})^{2}}{\\sqrt{M}}\\ln\\frac{2^{D+2}M}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where Eq. (159) follows from $\\ln(2M(N+\\mathcal{L})/\\delta)\\geq\\ln6\\geq1$ Furthermore, Eq. (161) follows from $\\mathcal{L}+\\mathcal{N}\\,\\dot{\\leq}\\,2^{\\ensuremath{\\mathcal{D}}+1}$ . By combining Eq. (161) with Eq. (157), we obtain the desired result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "B.3Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Instead of showing Theorem 3.2 directly, we show the proof of the following detailed version of Theorem3.2. ", "page_idx": 25}, {"type": "text", "text": "Theorem B.1 (Detailed version of Theorem 3.2). Suppose that Assumption 3.1 holds. Fix any $\\delta\\,\\in\\,(0,1),\\,\\alpha\\,\\geq\\,1,\\,\\rho\\,>\\,0$ and $\\mathcal{D}\\geq2$ Furthermore, suppose that the number of ensemble $M$ is sufficiently large to satisfy thefollowing four conditions: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M\\ge64C_{\\alpha,\\mathcal{D}}^{(6)}|\\mathcal{X}|^{2}\\lambda_{0}^{-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta},}\\\\ &{M\\ge C_{\\alpha,\\mathcal{D}}^{(6)}C_{\\alpha,\\mathcal{D}}^{(2)-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta},}\\\\ &{\\tilde{R}^{4}(\\tilde{R}+2)^{4}\\le\\frac{3\\eta^{2}M\\rho^{2}}{56C_{\\alpha,\\mathcal{D}}^{(3)2}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}\\right)^{-2},}\\\\ &{\\frac{C_{\\alpha,\\mathcal{D},T}^{(7)}}{\\sqrt{M}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\left(\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}\\right)\\sqrt{\\ln\\frac{6M}{\\delta}}\\le1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\overline{{R}}=\\tilde{R}+\\displaystyle\\frac{1}{2\\rho}\\left[(2\\tilde{R}+2^{D})\\sqrt{T C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3T}+\\tilde{R}\\right],}\\\\ {\\tilde{R}=2\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{\\frac{T}{\\rho}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\eta$ $\\begin{array}{r}{\\eta\\leq4^{-1}\\left(\\rho+2(2\\tilde{R}+2^{\\mathcal{D}}\\hat{C})^{2}T C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}\\right)^{-1}}\\end{array}$ at least $1-\\delta,$ .the following inequality holds for any $t\\in[T]$ and $\\pmb{x}\\in\\mathcal{X}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n|f(x)-h(x;\\theta_{t-1})|\\leq\\frac{72T^{2}C_{\\alpha,\\mathcal{D}}^{(3)}}{\\sqrt{M}\\rho^{2}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{4}\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}+\\beta\\tilde{\\sigma}_{t-1}(x),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta=\\left(\\sqrt{2}B+\\frac{\\sigma}{\\sqrt{\\rho}}\\sqrt{2\\left(\\gamma_{T}+\\frac{T\\sqrt{T C_{\\alpha,D}^{(6)}\\ln(96|\\mathcal{X}|^{2}/\\delta)}}{\\rho\\sqrt{M}}+\\ln\\frac{6}{\\delta}\\right)}\\right)}\\\\ &{\\quad+\\,\\rho^{-1}\\sqrt{\\overline{{k}}^{2}+4C_{\\alpha,D}^{(2)}}\\left[\\frac{C_{\\alpha,D,T}^{(7)}}{\\sqrt{M}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\,\\cdot\\,2^{\\overline{{D}}+2}M}{\\delta}\\right)\\sqrt{\\ln\\frac{6M}{\\delta}}\\right.}\\\\ &{\\quad\\left.+\\,(1-2\\eta\\rho)^{J/2}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\sqrt{\\frac{T}{\\rho}}\\right]\\left(\\rho+T C_{\\alpha,D}^{(4)}2^{2D}\\hat{C}^{2}\\ln\\frac{6M}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, $C>0$ $\\hat{C}>0$ $C_{\\alpha,{\\cal D}}^{(4)}>0,C_{\\alpha,{\\cal D}}^{(5)}>0,$ $C_{\\alpha,{\\cal D}}^{(6)}>0$ are constants that depend on $\\alpha$ and $\\mathcal{D}$ Moreover, $\\begin{array}{r}{\\overline{{k}}:=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\sqrt{k_{\\mathrm{TNTK}}(\\mathbf{x},\\mathbf{x})}}\\end{array}$ is the square root of the maximum value of TNTK, and CD $C_{\\alpha,D,T}^{(7)}=\\mathcal{O}(T^{3})$ is the constant that depends on $\\alpha,\\,\\mathcal{D}$ and $T$ ", "page_idx": 26}, {"type": "text", "text": "B.3.1 Proof overview ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we briefly summarize the overview of our proof. We first define the following six events: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ast\\mathcal{E}_{1}=\\Bigg\\{\\forall\\theta,x,R,\\lVert\\theta-\\theta_{0}\\rVert_{2}\\leq R\\Rightarrow\\lVert H(x,\\theta)\\rVert_{2}\\leq\\frac{C_{\\alpha,D}^{(4)}(R+2)^{2}}{\\sqrt{M}}\\ln\\frac{62^{D+2}M}{\\delta}\\Bigg\\},}\\\\ &{\\ast\\mathcal{E}_{2}=\\Big\\{\\forall\\theta,x,R,\\lVert\\theta-\\theta_{0}\\rVert_{2}\\leq R\\Rightarrow\\lVert g(x;\\theta)\\rVert_{2}^{2}\\leq C_{\\alpha,D}^{(4)}(2R+2^{D}\\hat{C})^{2}\\ln\\frac{6M}{\\delta}\\Bigg\\},}\\\\ &{\\ast\\mathcal{E}_{3}=\\Bigg\\{\\forall\\theta,x,R,\\lVert\\theta-\\theta_{0}\\rVert_{2}\\leq R\\Rightarrow\\lVert g(x;\\theta)-g(x;\\theta_{0})\\rVert_{2}^{2}\\leq\\frac{C_{\\alpha,D}^{(6)}R^{2}}{\\sqrt{M}}\\ln\\frac{6M}{\\delta}\\Bigg\\},}\\\\ &{\\ast\\mathcal{E}_{4}=\\Big\\{\\forall t\\in[T],\\lVert y_{t}\\rVert_{2}\\leq\\Big(\\Bar{k}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\Big)\\,\\sqrt{t}\\Big\\},}\\\\ &{\\ast\\mathcal{E}_{5}=\\Bigg\\{\\forall x,\\Bar{x},\\lVert k_{\\mathrm{rNTK}}(x,\\Bar{x})-\\Bar{k}(x,\\Bar{x})\\rvert\\leq\\operatorname*{min}\\left\\{\\frac{\\lambda_{0}}{2^{|\\mathcal{S}|}},\\sqrt{\\frac{4C_{\\alpha,D}^{(6)}}{M}}\\ln\\frac{96|\\mathcal{X}|^{2}}{\\delta},4C_{\\alpha,D}^{(2)}\\right\\}\\Bigg\\},}\\\\ &{\\ast\\mathcal{E}_{6}=\\Big\\{\\forall t\\in\\mathbb{N}_{+},\\forall x\\in\\mathcal{X},\\lVert f(x)-\\Bar{\\mu}_{t-1}(x)\\rVert\\leq\\Big(\\sqrt{2}B+\\frac{\\sigma}{\\sqrt{\\delta}}\\sqrt{2(\\Bar{\\gamma}_{t}+\\ln\\frac{6}{\\delta})}\\Big)\\,\\Tilde{\\sigma}_{t-1}(x)\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The quantities k, CD, CD, C(), O.D, .D, OD, P\u03bct-1, k, and /t are defined in Lemma B.4-B.7. Only the above six events require probabilistic arguments in our proof. Actually, from Lemma 3.3 and Lemma $\\mathbf{B.4-}$ B.7, which we will show later, we can confirm the events $\\mathcal{E}_{1},\\ldots,\\mathcal{E}_{6}$ simultaneously holds with probability at least $1-\\delta$ for sufficiently large $M$ by taking union bound; therefore, it is enough to show Eq. (168) under the event $\\bigcap_{i\\in[6]}\\mathcal{E}_{i}$ . Hereafter, we show Theorem B.1 in the following steps: ", "page_idx": 26}, {"type": "text", "text": "1. For sufficiently large $M$ , we show that each of the events $\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ \uff0c ${\\mathcal{E}}_{4}$ , and ${\\mathcal{E}}_{5}$ holds with probability at least $1-\\delta/6$ in Lemma B.4, Lemma B.5, and Lemma B.6, respectively. Furthermore, as shown in Lemma B.7, the event ${\\mathcal{E}}_{6}$ holds with probability at least $1-\\delta/{\\dot{3}}$ Since we already know the event ${\\mathcal{E}}_{1}$ holds with probability at least $1-\\delta/6$ from Lemma 3.3, we can show $\\begin{array}{r}{\\mathbb{P}\\left(\\bigcap_{i\\in[6]}\\mathcal{E}_{i}\\right)\\geq1-\\delta}\\end{array}$ in this step by applying the union bound. ", "page_idx": 27}, {"type": "text", "text": "2. As with the proof of Salgia [30], the error term $|f(\\pmb{x})-h(\\pmb{x};\\pmb{\\theta}_{t})|$ is decomposed as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f(x)-h(x;\\theta_{t})|}\\\\ &{\\leq|f(x)-\\tilde{\\mu}_{t}(x)|+|\\tilde{\\mu}_{t}(x)-\\langle g(x;\\theta_{0}),\\theta_{t}-\\theta_{0}\\rangle|+|\\langle g(x;\\theta_{0}),\\theta_{t}-\\theta_{0}\\rangle-h(x_{t};\\theta_{t})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Based on the above decomposition, we derive the upper bound of each term under the event $\\bigcap_{i\\in[6]}\\mathcal{E}_{i}$ with sufficiently large $M$ . The frst term of the above inequality is bounded from above by combining the event ${\\mathcal{E}}_{6}$ with Lemma B.12. The second term is bounded by resorting to the arguments from [41], which is based on the optimization error of the gradient descent of the linearized squared loss (Lemma B.9 and Lemma B.11). The upper bound of the third term is obtained by combining the event ${\\mathcal{E}}_{1}$ with the fact that the error of the first-order Taylor approximation can be characterized by the spectral norm of the Hessian. ", "page_idx": 27}, {"type": "text", "text": "B.3.2  Lemmas for the events $\\mathcal{E}_{2}{-}\\mathcal{E}_{6}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma B.4 (Gradient norm bounds). Let $\\delta\\in(0,1)$ $M\\geq3,$ and $\\alpha\\geq1$ Furthermore,let $\\pmb{\\theta}_{0}$ be an initial parameter of ST-UCB. Then, with probability at least $1-\\delta,$ for any $\\pmb{x}\\in\\mathbb{S}^{d-1}$ $R\\geq0$ and $\\pmb\\theta\\in\\mathbb{R}^{p}$ such that $\\lVert{\\pmb{\\theta}}-{\\pmb{\\theta}}_{0}\\rVert_{2}\\leq R$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|g({\\boldsymbol x};\\theta)\\|_{2}^{2}\\leq C_{\\alpha,\\mathcal{D}}^{(4)}(2R+2^{\\mathcal{D}}\\hat{C})^{2}\\ln\\frac{M}{\\delta},}\\\\ {\\displaystyle\\|g({\\boldsymbol x};\\theta)-g({\\boldsymbol x};\\theta_{0})\\|_{2}^{2}\\leq\\frac{C_{\\alpha,\\mathcal{D}}^{(5)}R^{2}}{M}\\ln\\frac{M}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\hat{C}>0$ is an absolute constant. Moreover, $C_{\\alpha,{\\cal D}}^{(4)}=2^{{\\cal D}+2}\\alpha^{2}$ and $C_{\\alpha,{\\cal D}}^{(5)}=7\\cdot2^{3\\mathcal{D}}\\hat{C}\\alpha^{2}$ ", "page_idx": 27}, {"type": "text", "text": "Proof. Suppose there exists $u\\geq1$ such that the following event holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall m\\in[M],\\sum_{l=1}^{\\mathcal{L}}|\\overline{{\\pi}}_{l}^{(m)}|\\leq u.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Following the proof of Lemma 8 in [21], we can derive that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|g(\\mathbf{\\boldsymbol{x}};\\boldsymbol{\\theta})|_{2}^{2}\\leq\\mathcal{N}(R+u)^{2}\\alpha^{2}+\\mathcal{L},}\\\\ {\\displaystyle|g(\\mathbf{\\boldsymbol{x}};\\boldsymbol{\\theta})-g(\\mathbf{\\boldsymbol{x}};\\boldsymbol{\\theta}_{0})||_{2}^{2}}\\\\ {\\displaystyle\\leq\\frac{1}{M}\\sum_{m=1}^{M}\\left[\\sum_{n=1}^{N}\\left(\\alpha\\sum_{l=1}^{\\mathcal{L}}|\\pi_{l}^{(m)}-\\overline{{\\pi}}_{l}^{(m)}|+2\\alpha u\\sum_{\\widetilde{n}=1}^{N}\\|w_{\\widetilde{n}}^{(m)}-\\overline{{\\pmb{w}}}_{\\widetilde{n}}^{(m)}\\|_{2}\\right)^{2}\\right.}\\\\ {\\displaystyle\\left.\\quad+\\sum_{l=1}^{\\mathcal{L}}\\left(\\sum_{n=1}^{N}\\|{\\pmb{w}}_{n}^{(m)}-\\overline{{\\pmb{w}}}_{n}^{(m)}\\|_{2}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Furthermore, in the second inequality, we obtain the following upper bound from the Schwarz's inequality: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{M}\\displaystyle\\frac{M}{\\displaystyle\\sum_{i=1}^{M}\\left(\\displaystyle\\sum_{m=1}^{K}\\left(\\eta_{i}^{\\alpha\\beta}-\\overline{{\\eta}}_{i}^{(m)}\\right)+2\\alpha\\alpha\\displaystyle\\sum_{i=1}^{K}[\\eta_{i}^{(m)}-\\overline{{\\eta}}_{i}^{(m)}]\\right)^{2}}}\\\\ &{\\phantom{2p c}+\\displaystyle\\sum_{i=1}^{S}\\left(\\displaystyle\\sum_{s=1}^{N}|w_{i}^{(m)}-\\overline{{w}}_{s}^{(m)}|\\right)_{2}\\right)^{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left[\\sum_{s=1}^{N}\\left(2\\alpha^{2}\\mathbb{E}|\\eta^{(m)}-\\overline{{w}}^{(m)}|\\mathbb{Z}_{2}^{0}+4\\alpha^{2}\\alpha^{2}N^{2}\\|w_{m}^{(m)}-\\overline{{w}}_{m}^{(m)}\\|_{2}^{2}\\right)\\right.}\\\\ &{\\quad+\\displaystyle\\sum_{i=1}^{S}N|w_{i}^{(m)}-\\overline{{w}}_{s}^{(m)}|\\mathbb{Z}_{2}^{0}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{M}\\displaystyle\\frac{M}{\\displaystyle\\sum_{i=1}^{M}N}\\left(2\\alpha^{2}\\epsilon+4\\alpha^{2}\\alpha^{2}N^{2}+C\\right)\\left\\|\\theta^{(m)}-\\overline{{\\theta}}^{(m)}\\right\\|_{2}^{2}}\\\\ &{=\\displaystyle\\frac{N(2\\epsilon)^{2}+4\\alpha^{2}\\gamma^{2}+4\\alpha^{2}\\gamma}{16}|\\theta-\\overline{{\\theta}}_{1}^{(m)}|}\\\\ &{\\leq\\frac{\\displaystyle\\frac{\\alpha^{2}\\alpha\\gamma\\epsilon^{2}N^{2}}{4}\\beta^{2}}{16},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, using the general Hoeffding's inequality (Lemma E.1) and the union bound, the event (173) holds with probability at least $1-\\delta$ when $u=\\sqrt{\\hat{C}\\mathcal{L}\\ln(M/\\delta)}$ where $\\hat{C}\\geq1$ is an absolute constant. Therefore, with probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|g(x;\\theta)\\|_{2}^{2}\\leq\\mathcal{N}\\left(R+\\sqrt{\\hat{C}\\mathcal{L}\\ln\\displaystyle\\frac{M}{\\delta}}\\right)^{2}\\alpha^{2}+\\mathcal{L},}\\\\ &{\\|g(x;\\theta)-g(x;\\theta_{0})\\|_{2}^{2}\\leq\\frac{C_{\\alpha,\\mathcal{D}}^{(5)}R^{2}}{M}\\ln\\displaystyle\\frac{M}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finall, from the denitionf C ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2^{D+2}(2R+\\mathcal{L}\\hat{C})^{2}\\alpha^{2}=C_{\\alpha,D}^{4}(2R+\\mathcal{L}\\hat{C})^{2}}\\\\ &{\\Leftrightarrow2^{D+1}(2R+\\mathcal{L}\\hat{C})^{2}\\alpha^{2}+2(2R+\\mathcal{L}\\hat{C})^{2}\\alpha^{2}=C_{\\alpha,D}^{4}(2R+\\mathcal{L}\\hat{C})^{2}}\\\\ &{\\Rightarrow2^{D}(2R+\\mathcal{L}\\hat{C})^{2}\\alpha^{2}+2\\mathcal{L}\\le C_{\\alpha,D}^{4}(2R+\\mathcal{L}\\hat{C})^{2}}\\\\ &{\\Rightarrow X\\Big(2R+\\sqrt{\\hat{C}\\hat{C}}\\Big)^{2}\\alpha^{2}+2\\mathcal{L}\\le C_{D,\\alpha}^{4}(2R+\\mathcal{L}\\hat{C})^{2}}\\\\ &{\\Rightarrow X\\bigg(\\displaystyle\\frac{R}{\\sqrt{\\sin2}}+\\sqrt{\\hat{C}\\hat{C}}\\bigg)^{2}\\alpha^{2}+\\displaystyle\\frac{C}{\\ln2}\\le C_{D,\\alpha}^{4}(2R+\\mathcal{L}\\hat{C})^{2}}\\\\ &{\\Rightarrow X\\bigg(\\displaystyle\\frac{R}{\\sqrt{\\ln(M/\\delta)}}+\\sqrt{\\hat{C}\\hat{C}}\\bigg)^{2}\\alpha^{2}+\\displaystyle\\frac{\\mathcal{L}}{\\ln(M/\\delta)}\\le C_{D,\\alpha}^{4}(2R+\\mathcal{L}\\hat{C})^{2}}\\\\ &{\\Leftrightarrow X\\bigg(R+\\sqrt{\\hat{C}\\hat{C}}\\ln\\frac{\\mathcal{M}}{\\delta}\\bigg)^{2}\\alpha^{2}+\\mathcal{L}\\le C_{D,\\alpha}^{4}(2R+\\mathcal{L}\\hat{C})^{2}\\ln\\frac{\\mathcal{M}}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Eq. (188) follows from $(2R+{\\mathcal{L}}{\\hat{C}})^{2}\\alpha^{2}\\geq{\\mathcal{L}}$ since $\\alpha\\geq1,{\\hat{C}}\\geq1$ , and $R\\geq0$ ", "page_idx": 28}, {"type": "text", "text": "\u00b7Eq. (189) follows from ${\\mathcal{N}}\\leq2^{D}$ and $\\mathcal{L}\\hat{C}\\geq1$ ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Eq. (190) follows from $\\sqrt{\\ln{2}}\\ge\\ln{2}\\ge0.5$ ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Eq. (191) follows from the fact that $\\ln(M/\\delta)\\geq\\ln2$ holds under $M\\geq2$ ", "page_idx": 29}, {"type": "text", "text": "Lemma B.5. Fix any $\\delta\\in(0,1)$ and $f\\in\\mathcal{H}_{\\mathrm{TNTK}}\\,\\nu$ vith $\\|f\\|_{\\mathrm{TNTK}}\\le B$ Furthermore, suppose that $\\epsilon_{t}$ is a $\\sigma$ -sub-Gauss random variable for any $t\\,\\in\\,[T]$ Then, with probability at least $1-\\delta$ the following inequality holds for any $t\\in[T]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|y_{t}\\|_{2}\\leq\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{2T}{\\delta}}\\right)\\sqrt{t},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{k}}=\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}\\sqrt{k_{\\mathrm{TNTK}}(\\pmb{x},\\pmb{x})}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. From the reproducing property of RKHS and Schwarzs inequality, for any $\\pmb{x}\\in\\mathcal{X}$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\pmb{x})=\\langle f,k_{\\mathrm{TNTK}}(\\pmb{x},\\cdot)\\rangle_{\\mathcal{H}_{\\mathrm{TNTK}}}}\\\\ &{\\quad\\quad=\\|f\\|_{\\mathrm{TNTK}}\\|k_{\\mathrm{TNTK}}(\\pmb{x},\\cdot)\\|_{\\mathrm{TNTK}}}\\\\ &{\\quad\\quad=\\|f\\|_{\\mathrm{TNTK}}\\sqrt{k_{\\mathrm{TNTK}}(\\pmb{x},\\pmb{x})}}\\\\ &{\\quad\\quad\\leq B\\overline{{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\pmb{y}_{t}\\|_{2}^{2}=\\sum_{i=1}^{t}[\\pmb{f}(\\pmb{x}_{i})+\\epsilon_{i}]^{2}}}\\\\ &{}&{\\leq\\displaystyle\\sum_{i=1}^{t}\\left(B\\overline{{k}}+|\\epsilon_{i}|\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By using the concentration property of $\\sigma$ -sub-Gauss random variable, for any $t\\in[T]$ and $\\tilde{\\delta}\\in(0,1)$ \uff0c ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\epsilon_{i}|\\le\\sigma\\sqrt{2\\ln\\frac{2}{\\tilde{\\delta}}}\\right)\\ge1-\\tilde{\\delta}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By setting $\\tilde{\\delta}$ as $\\tilde{\\delta}=\\delta/T$ and taking the union bound, we complete the proof. ", "page_idx": 29}, {"type": "text", "text": "Lemma B.6. Let $\\delta\\ \\in\\ (0,1)$ \uff0c $\\mathcal{D}\\ \\geq\\ 2$ and $\\mathcal{X}~\\subset~\\mathbb{S}^{d-1}$ .Furthermore, let $K_{\\mathrm{TNTK}}(\\mathcal{X})\\;:=\\;$ $[k_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})]_{\\pmb{x},\\tilde{\\pmb{x}}\\in\\mathcal{X}}\\in\\mathbb{R}^{|\\mathcal{X}|\\times|\\mathcal{X}|}$ and $\\lambda_{0}=\\lambda_{\\mathrm{min}}(K_{\\mathrm{TNTK}}(\\chi))>0$ be kernel matrix over $\\mathcal X\\times\\mathcal X$ and theminimum eigenvalue of ${\\cal K}_{\\mathrm{TNTK}}(\\lambda)$ ,respectively.Moreover, assume that ", "page_idx": 29}, {"type": "equation", "text": "$$\nM\\ge64C_{\\alpha,\\mathscr{D}}^{(6)}|\\mathscr{X}|^{2}\\lambda_{0}^{-2}\\ln\\frac{16|\\mathscr{X}|^{2}}{\\delta}\\;\\;\\mathrm{and}\\;\\;M\\ge C_{\\alpha,\\mathscr{D}}^{(6)}C_{\\alpha,\\mathscr{D}}^{(2)-2}\\ln\\frac{16|\\mathscr{X}|^{2}}{\\delta}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "hold where $C_{\\alpha,{\\cal D}}^{(6)}=\\tilde{C}\\operatorname*{max}\\{C_{\\alpha,{\\cal D}}^{(2)2},2^{2{\\cal D}}\\}$ Here, $\\tilde{C}$ and C are definedin Lemma 3.2. Then, with probability at least $1-\\delta$ the following inequality holds for any $\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}}\\in\\mathcal{X}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lvert k_{\\mathrm{TNTK}}(\\pmb{x},\\tilde{\\pmb{x}})-\\tilde{k}(\\pmb{x},\\tilde{\\pmb{x}})\\rvert\\leq\\operatorname*{min}\\left\\{\\frac{\\lambda_{0}}{2\\lvert\\mathcal{X}\\rvert},\\sqrt{\\frac{4C_{\\alpha,\\mathcal{D}}^{(6)}}{M}\\ln\\frac{16\\lvert\\mathcal{X}\\rvert^{2}}{\\delta}},4C_{\\alpha,\\mathcal{D}}^{(2)}\\right\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\tilde{k}(\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}})=\\langle\\mathbf{\\boldsymbol{g}}(\\mathbf{\\boldsymbol{x}};\\theta_{0}),\\mathbf{\\boldsymbol{g}}(\\tilde{\\mathbf{\\boldsymbol{x}}};\\theta_{0})\\rangle$ ", "page_idx": 29}, {"type": "text", "text": "PrfFf $\\varepsilon\\in(0,C_{\\alpha,\\mathcal{D}}^{(2)})$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M\\ge C_{\\alpha,\\mathcal{D}}^{(6)}\\varepsilon^{-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}}\\\\ &{\\Rightarrow\\mathbb{P}(\\forall x,\\tilde{x}\\in\\mathcal{X},|k_{\\mathrm{TNTK}}(x,\\tilde{x})-\\langle g(x,\\theta_{0}),g(\\tilde{x},\\theta_{0})\\rangle|\\le4\\varepsilon)\\ge1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, we st $\\varepsilon$ $\\varepsilon\\;=\\;\\operatorname*{min}\\{\\lambda_{0}/(8|\\mathcal{X}|),\\sqrt{C_{\\alpha,\\mathcal{D}}^{(6)}\\ln({16}|\\mathcal{X}|^{2}/\\delta)/M},C_{\\alpha,\\mathcal{D}}^{(2)}\\}$ then, $\\varepsilon\\ \\in\\ (0,C_{\\alpha,\\mathcal{D}}^{(2)})$ Therefore, by using Eq. (203), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{M\\ge C_{\\alpha,D}^{(6)}\\operatorname*{min}\\left\\{\\frac{\\lambda_{0}}{8|\\mathcal{X}|},\\sqrt{\\frac{C_{\\alpha,D}^{(6)}}{M}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}},C_{\\alpha,D}^{(2)}\\right\\}^{-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(20,15)}\\\\ &{}&{\\Rightarrow\\mathbb{P}\\left(\\forall x,\\tilde{x}\\in\\mathcal{X},|k_{\\mathrm{TNTK}}(x,\\tilde{x})-\\tilde{k}(x,\\tilde{x})|\\le\\operatorname*{min}\\left\\{\\frac{\\lambda_{0}}{2|\\mathcal{X}|},\\sqrt{\\frac{4C_{\\alpha,D}^{(6)}}{M}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}},4C_{\\alpha,D}^{(2)}\\right\\}\\right)}\\\\ &{}&{>1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Furthermore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M\\geq64C_{\\alpha,\\mathcal{D}}^{(6)}|\\mathcal{X}|^{2}\\lambda_{0}^{-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}\\,\\,\\,\\mathrm{and}\\,\\,\\,M\\geq C_{\\alpha,\\mathcal{D}}^{(6)}C_{\\alpha,\\mathcal{D}}^{(2)-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}}\\\\ &{\\Rightarrow M\\geq C_{\\alpha,\\mathcal{D}}^{(6)}\\operatorname*{min}\\left\\{\\frac{\\lambda_{0}}{8|\\mathcal{X}|},\\sqrt{\\frac{C_{\\alpha,\\mathcal{D}}^{(6)}}{M}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}},C_{\\alpha,\\mathcal{D}}^{(2)}\\right\\}^{-2}\\ln\\frac{16|\\mathcal{X}|^{2}}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By combining the above implication with Eq. (203), we complete the proof. ", "page_idx": 30}, {"type": "text", "text": "Lemma B.7. Fix any $\\delta\\in(0,1)$ and $f\\in\\mathcal{H}_{\\mathrm{TNTK}}\\,\\nu$ vith $\\|f\\|_{\\mathrm{TNTK}}\\le B$ Let us define $\\tilde{k}$ as $\\tilde{k}(x,\\tilde{{\\boldsymbol{x}}})=$ $\\langle g(x;\\pmb{\\theta}_{0}),g(\\tilde{x};\\pmb{\\theta}_{0})\\rangle$ Furthermore, suppose that $(\\epsilon_{t})_{t\\in\\mathbb{N}_{+}}$ are conditionally $\\sigma$ -sub-Gaussian random variables. Then, under the event $\\mathcal{E}_{5}$ ,with probability at least $1-\\delta$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall t\\in\\mathbb{N}_{+},\\forall x\\in\\mathcal{X},|f(x)-\\tilde{\\mu}_{t-1}(x)|\\leq\\left(\\sqrt{2}B+\\frac{\\sigma}{\\sqrt{\\rho}}\\sqrt{2\\left(\\tilde{\\gamma}_{t}+\\ln\\frac{1}{\\delta}\\right)}\\right)\\tilde{\\sigma}_{t-1}(x).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, we respectively define $\\tilde{\\mu}_{t-1}(\\mathbf{\\boldsymbol{x}})$ and $\\tilde{\\gamma}_{t}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mu}_{t}(\\pmb{x})=\\tilde{\\pmb{k}}_{t}^{\\top}(\\pmb{x})\\left(\\tilde{\\pmb{K}}_{t}+\\rho\\pmb{I}_{t}\\right)^{-1}\\pmb{y}_{t},}\\\\ &{\\qquad\\tilde{\\gamma}_{t}=\\frac{1}{2}\\operatorname*{max}_{\\pmb{x}_{1},\\ldots,\\pmb{x}_{t}}\\ln\\operatorname*{det}\\left(\\pmb{I}_{t}+\\rho^{-1}\\tilde{\\pmb{K}}_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\tilde{\\pmb{k}}_{t}(\\pmb{x})~=~[\\tilde{k}(\\pmb{x},\\pmb{x}_{i})]_{i\\in[t]}~\\in~\\mathbb{R}^{t}$ and $\\tilde{\\pmb{K}}_{t}\\ =\\ [\\tilde{k}(\\pmb{x}_{i},\\pmb{x}_{j})]_{i,j\\in[t]}\\ \\in\\ \\mathbb{R}^{t\\times t}$ with $\\tilde{k}({\\pmb x},\\tilde{{\\pmb x}})\\;=\\;$ $\\langle g(x;\\pmb{\\theta}_{0}),g(\\tilde{x};\\pmb{\\theta}_{0})\\rangle$ ", "page_idx": 30}, {"type": "text", "text": "Proof. From the definition of ${\\mathcal{E}}_{5}$ , we have $|k_{\\mathrm{TNTK}}({\\pmb x},\\tilde{\\pmb x})-\\langle{\\pmb g}({\\pmb x},{\\pmb\\theta}_{0}),{\\pmb g}(\\tilde{\\pmb x},{\\pmb\\theta}_{0})\\rangle|\\leq\\lambda_{0}/(2|\\mathcal{X}|)$ for any $\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}}\\in\\mathcal{X}$ . Therefore, $\\begin{array}{r}{\\sqrt{\\sum_{\\mathbf{x},\\tilde{\\mathbf{x}}\\in\\mathcal{X}}|k_{\\mathrm{TNTK}}(\\mathbf{\\boldsymbol{x}},\\tilde{\\mathbf{\\boldsymbol{x}}})-\\langle\\boldsymbol{g}(\\mathbf{\\boldsymbol{x}},\\theta_{0}),\\mathbf{\\boldsymbol{g}}(\\tilde{\\mathbf{\\boldsymbol{x}}},\\theta_{0})\\rangle|^{2}}\\,\\leq\\,\\lambda_{0}/2}\\end{array}$ Here, by combining this inequality with the arguments of the proof of Lemma C.5 in [24], under the event $\\mathcal{E}_{5}$ we have $f\\in\\mathcal{H}_{\\tilde{k}}$ with $\\|f\\|_{\\tilde{k}}\\le\\sqrt{2}B$ . Therefore, since ${\\tilde{\\mu}}_{t}$ and $\\tilde{\\sigma}_{t}$ are defined as the posterior mean and the posterior variance of Gaussian process characterized by the kernel function $\\tilde{k}$ , we obtain the desired result by applying Lemma 3.11 in [2]. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma B.8. Fix any $\\delta\\in(0,1)$ ; then, $\\mathbb{P}(\\cap_{i\\in[6]}\\mathcal{E}_{i})\\geq1-\\delta$ holds. ", "page_idx": 30}, {"type": "text", "text": "Proof. From Lemma 3.3, B.5, and B.6, we have $\\mathbb{P}(\\mathcal{E}_{i}^{c})\\le\\delta/6$ for any $i\\in[5]/\\{2,3\\}$ . In addition, from Lemma B.4, we have $\\mathbb{P}(\\mathcal{E}_{2}^{c}\\cup\\mathcal{E}_{3}^{c})\\le\\delta/6$ . Here, from Lemma B.6 and Lemma B.7, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E}_{6}^{c})=\\mathbb{P}(\\mathcal{E}_{6}^{c}\\mid\\mathcal{E}_{5})\\mathbb{P}(\\mathcal{E}_{5})+\\mathbb{P}(\\mathcal{E}_{6}^{c}\\mid\\mathcal{E}_{5}^{c})\\mathbb{P}(\\mathcal{E}_{5}^{c})}\\\\ &{\\quad\\quad\\quad\\leq\\frac{\\delta}{6}+\\frac{\\delta}{6}}\\\\ &{\\quad\\quad=\\frac{\\delta}{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, by taking the union bound, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big(\\cap_{i\\in[6]}\\mathcal{E}_{i}\\big)=1-\\mathbb{P}\\big(\\cup_{i\\in[6]}\\mathcal{E}_{i}^{c}\\big)}\\\\ &{\\phantom{\\mathbb{P}\\big(\\cap_{i\\in[6]}\\mathcal{E}_{i}\\big)}\\ge1-\\big[\\mathbb{P}\\big(\\mathcal{E}_{1}^{c}\\big)+\\mathbb{P}\\big(\\mathcal{E}_{2}^{c}\\cup\\mathcal{E}_{3}^{c}\\big)+\\mathbb{P}\\big(\\mathcal{E}_{4}^{c}\\big)+\\mathbb{P}\\big(\\mathcal{E}_{5}^{c}\\big)+\\mathbb{P}\\big(\\mathcal{E}_{6}^{c}\\big)\\big]}\\\\ &{\\phantom{\\mathbb{P}\\big(\\cap_{i\\in[6]}\\mathcal{E}_{i}\\big)}\\ge1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "B.4 Lemmas for the upper bounds of Eq. (170) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Definition B.1. Define $\\tilde{L}_{t}(\\pmb\\theta)$ for any $t\\in\\mathbb{N}_{+}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{L}_{t}(\\pmb{\\theta})=\\left\\|\\pmb{G}_{t}^{\\top}\\left(\\pmb{\\theta}-\\pmb{\\theta}_{0}\\right)-\\pmb{y}_{t}\\right\\|_{2}^{2}+\\rho\\left\\|\\pmb{\\theta}-\\pmb{\\theta}_{0}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Furthermore, let us define $\\tilde{\\pmb{\\theta}}_{t;1},\\dots,\\tilde{\\pmb{\\theta}}_{t;J}$ as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\theta}_{t;j}=\\tilde{\\theta}_{t;j-1}-\\eta\\left\\{2G_{t}\\left[G_{t}^{\\top}\\left(\\tilde{\\theta}_{t;j-1}-\\theta_{0}\\right)-y_{t}\\right]+2\\rho\\left(\\tilde{\\theta}_{t;j-1}-\\theta_{0}\\right)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\tilde{\\pmb{\\theta}}_{t;0}=\\pmb{\\theta}_{0}$ ", "page_idx": 31}, {"type": "text", "text": "Lemma B.9 (Adapted from Lemma C.4 in [41]). Suppose that the events $\\mathcal{E}_{2}$ and ${\\mathcal{E}}_{4}$ simultaneously hold. Fuhemore, ssm tt $\\begin{array}{r}{\\eta\\;\\le\\;2^{-1}\\left(T\\hat{C}^{2}2^{2D}C_{\\alpha,\\mathcal{D}}^{(4)}\\ln(6M/\\delta)+\\rho\\right)^{-1}}\\end{array}$ holds. Then, the following inequalities hold for any $t\\in[T]$ and $j\\in[J]$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\tilde{\\theta}_{t;j}-\\theta_{0}\\right\\|_{2}\\le\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{\\frac{t}{\\rho}},}\\\\ &{\\left\\|\\tilde{\\theta}_{t;j}-\\theta_{0}-\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)^{-1}G_{t}y_{t}\\right\\|_{2}\\le\\left(1-2\\eta\\rho\\right)^{j/2}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{\\frac{t}{\\rho}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Wwhere $\\overline{{k}}$ isd $\\hat{C}$ and $C_{\\alpha,\\mathcal{D}}^{(4)}$ are defned in Lema B.4. Proof. From the definition of $\\tilde{L}_{t}(\\pmb\\theta)$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}^{2}\\tilde{L}_{t}(\\theta)=2G_{t}G_{t}^{\\top}+2\\rho I_{p}}\\\\ &{\\qquad\\qquad\\quad\\preceq2\\left(\\|G_{t}\\|_{F}^{2}+\\rho\\right)I_{p}}\\\\ &{\\qquad\\qquad\\quad\\preceq2\\left(t\\hat{C}^{2}2^{2D}C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}+\\rho\\right)I_{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where Eq. (22) fllows from Lemma B.13. Therefore, $\\tilde{L}_{t}(\\pmb\\theta)$ .s $\\begin{array}{r}{2\\left(t\\hat{C}^{2}2^{2\\mathcal{D}}C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}+\\rho\\right)}\\end{array}$ smooth function. Furthermore, $\\tilde{L}_{t}(\\pmb\\theta)$ is $2\\rho$ -strong convex because $\\nabla_{\\theta}^{2}\\tilde{L}_{t}(\\theta)\\,\\succeq\\,2\\rho I_{p}$ holds. By combining the definition of $\\eta$ with the standard result of gradient descent for the strongly convex and smooth objective function (e.g., Theorem 3.6 in [16]), $\\tilde{L}_{t}(\\tilde{\\pmb{\\theta}}_{t;j})\\geq\\tilde{L}_{t}(\\tilde{\\pmb{\\theta}}_{t;j-1})$ holds for any $j\\in[J]$ Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho\\left\\|\\tilde{\\theta}_{t,J}-\\theta_{0}\\right\\|_{2}^{2}\\leq\\left\\|G_{t}^{\\top}\\left(\\tilde{\\theta}_{t,J}-\\theta_{0}\\right)-y_{t}\\right\\|_{2}^{2}+\\rho\\left\\|\\tilde{\\theta}_{t,J}-\\theta_{0}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|G_{t}^{\\top}\\left(\\tilde{\\theta}_{t;0}-\\theta_{0}\\right)-y_{t}\\right\\|_{2}^{2}+\\rho\\left\\|\\tilde{\\theta}_{t;0}-\\theta_{0}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|y_{t}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where Eq. (226) follows from the event ${\\mathcal{E}}_{4}$ . Furthermore, since the unique minimum of $\\tilde{L}_{t}(\\pmb\\theta)$ is given as $\\pmb{\\theta}^{*}:=\\pmb{\\theta}_{0}+\\left(\\rho\\pmb{I}_{p}+\\pmb{G}_{t}\\pmb{G}_{t}^{\\top}\\right)^{-1}\\pmb{G}_{t}\\pmb{y}_{t}$ , we have the following inequalities from Theorem 3.6 ", "page_idx": 31}, {"type": "text", "text": "in [16]: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\bar{\\theta}_{t,j}-\\theta_{0}-\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)^{-1}G_{t}y_{t}\\right\\|_{2}^{2}}\\\\ &{\\leq(1-2\\eta\\rho)^{j}\\left\\|\\theta_{0}-\\theta^{*}\\right\\|_{2}^{2}}\\\\ &{\\leq(1-2\\eta\\rho)^{j}\\frac{1}{\\rho}\\left[\\bar{L}_{t}(\\theta_{0})-\\bar{L}_{t}(\\theta^{*})\\right]}\\\\ &{\\leq(1-2\\eta\\rho)^{j}\\frac{\\bar{L}_{t}(\\theta_{0})}{\\rho}}\\\\ &{\\leq(1-2\\eta\\rho)^{j}\\frac{\\|y_{t}\\|_{2}^{2}}{\\rho}}\\\\ &{\\leq(1-2\\eta\\rho)^{j}\\frac{\\left(\\bar{L}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t}{\\rho},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 Eq. (229) follows from $\\nabla\\tilde{L}_{t}(\\pmb{\\theta}^{*})=\\mathbf{0}$ and the fact that ${\\tilde{L}}_{t}$ is the $2\\rho$ strong convex function. \u00b7 Eq. (230) follows from $\\tilde{L}_{t}(\\pmb{\\theta}^{*})\\geq0$   \n\u00b7 Eq. (232) follows from the event ${\\mathcal{E}}_{4}$ ", "page_idx": 32}, {"type": "text", "text": "Lemma B.10 (Adapted from Lemma C.3 in [41]). Fix any $R\\geq0,$ $j\\in[J]$ and $t\\in[T]$ . Suppose thathelearmin raie $\\eta$ satisies $\\begin{array}{r}{\\eta\\leq4^{-1}\\left(\\rho+2(2R+2^{\\mathcal{D}}\\hat{C})^{2}T C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}\\right)^{-1}}\\end{array}$ and $\\pmb\\theta_{t;\\tilde{j}}$ satisfes $\\|\\pmb{\\theta}_{t;\\tilde{j}}-\\pmb{\\theta}_{0}\\|_{2}\\leq R$ for all $\\tilde{j}\\in[j]$ .Furthermore,assumethat $R$ satisfiesthefollowinginequality: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\overline{{{R}}}^{4}(\\overline{{{R}}}+2)^{4}\\leq\\frac{3\\eta^{2}M\\rho^{2}}{56C_{\\alpha,\\overline{{{\\mathcal{D}}}}}^{(3)2}}\\left(\\overline{{{k}}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\cdot2^{\\overline{{{D}}}+2}M}{\\delta}\\right)^{-2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\overline{{{R}}}=R+\\frac{1}{2\\rho}\\left[(2R+2^{D})\\sqrt{t C_{\\alpha,{D}}^{(4)}\\ln\\frac{6M}{\\delta}}\\left(\\overline{{{k}}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}+R\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, under the events $\\mathcal{E}_{2}$ and ${\\mathcal{E}}_{4}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|h_{t;j+1}-y_{t}\\|_{2}\\leq\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{h}_{t;j}=[h(\\pmb{x}_{1};\\pmb{\\theta}_{t;j}),\\dots,h(\\pmb{x}_{t};\\pmb{\\theta}_{t;j})]^{\\top}\\in\\mathbb{R}^{t}.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Prof. We firs asume that $\\begin{array}{r}{\\|h_{t;j}-y_{t}\\|\\leq\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}}\\end{array}$ holds. Here by resoring the same arguments as the proof of Lemma C.3 in [41], for any $\\pmb{\\theta},\\pmb{\\theta}^{\\prime}\\in\\mathbb{R}^{d}$ such that $\\|\\pmb{\\theta}-\\pmb{\\theta}_{0}\\|\\leq R$ ,we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\left.\\frac{\\|\\nabla_{\\theta}L_{t}(\\theta)\\|_{2}^{2}}{\\rho}-2\\|h_{t}(\\theta)-y_{t}\\|_{2}\\|e(\\theta^{\\prime},\\theta)\\|_{2}\\right.}\\\\ &{\\leq L_{t}(\\theta^{\\prime})-L_{t}(\\theta)}\\\\ &{\\leq2\\langle\\nabla_{\\theta}L_{t}(\\theta),\\theta^{\\prime}-\\theta\\rangle+2\\|h_{t}(\\theta)-y_{t}\\|_{2}\\|e(\\theta^{\\prime},\\theta)\\|_{2}}\\\\ &{\\quad+\\left.2\\left[(2R+2^{D}\\hat{C})\\sqrt{t C_{\\alpha,D}^{(4)}\\ln\\frac{6M}{\\delta}}\\right]^{2}\\|\\theta^{\\prime}-\\theta\\|_{2}^{2}+2\\|e(\\theta^{\\prime},\\theta)\\|_{2}^{2}+\\rho\\|\\theta^{\\prime}-\\theta\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $e(\\theta^{\\prime},\\theta)=h_{t}(\\theta^{\\prime})-h_{t}(\\theta)-G_{t}(\\theta)^{\\top}(\\theta^{\\prime}-\\theta)$ with $h_{t}(\\pmb\\theta)=(h(\\pmb x_{1};\\pmb\\theta),\\dots,h(\\pmb x_{t};\\pmb\\theta))^{\\top}\\in\\mathbb{R}^{t}$ and $G_{t}(\\pmb\\theta)\\,=\\,(\\pmb g(\\pmb x_{1};\\pmb\\theta),\\dots,\\pmb g(\\pmb x_{t};\\pmb\\theta))^{\\top}\\,\\in\\,\\mathbb{R}^{p\\times t}$ . From the upper bound of ${\\cal L}_{t}(\\pmb\\theta^{\\prime})\\mathrm{~-~}{\\cal L}_{t}(\\pmb\\theta)$ , by setting $\\pmb{\\theta}^{\\prime}\\in\\mathbb{R}^{p}$ as $\\pmb{\\theta}^{\\prime}=\\pmb{\\theta}-\\eta\\nabla_{\\pmb{\\theta}}L_{t}(\\pmb{\\theta})$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{t}(\\pmb{\\theta}-\\eta\\nabla_{\\pmb{\\theta}}L_{t}(\\pmb{\\theta}))-L_{t}(\\pmb{\\theta})}\\\\ &{\\leq-2c\\eta\\|\\nabla_{\\pmb{\\theta}}L_{t}(\\pmb{\\theta})\\|_{2}^{2}}\\\\ &{\\phantom{\\leq}+2\\|h_{t}(\\pmb{\\theta})-y_{t}\\|_{2}\\|e(\\pmb{\\theta}-\\eta\\nabla_{\\pmb{\\theta}}L_{t}(\\pmb{\\theta}),\\pmb{\\theta})\\|_{2}+2\\|e(\\pmb{\\theta}-\\eta\\nabla_{\\pmb{\\theta}}L_{t}(\\pmb{\\theta}),\\pmb{\\theta})\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{c=\\left\\{1-\\eta\\left[\\rho+2(2R+2^{\\mathcal{D}}\\hat{C})^{2}t C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}\\right]\\right\\}\\in(0,1)}\\end{array}$ Furthermore, for any $\\pmb{\\theta}^{\\prime}\\in\\mathbb{R}^{p}$ we obtain the following inequality by combining the lower bound of ${\\cal L}_{t}(\\pmb\\theta^{\\prime})-{\\cal L}_{t}(\\pmb\\theta)$ with the above inequality, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{t}(\\theta-\\eta\\nabla_{\\theta}G L_{t}(\\theta))-L_{t}(\\theta)}\\\\ &{\\le2c\\eta\\rho\\left[L_{t}(\\theta^{\\prime})-L_{t}(\\theta)+2\\|h_{t}(\\theta)-y_{t}\\|_{2}\\|e(\\theta^{\\prime},\\theta)\\|_{2}\\right]}\\\\ &{\\quad+2\\|h_{t}(\\theta)-y_{t}\\|_{2}\\|e(\\theta-\\eta\\nabla_{\\theta}L_{t}(\\theta),\\theta)\\|_{2}+2\\|e(\\theta-\\eta\\nabla_{\\theta}L_{t}(\\theta),\\theta)\\|_{2}^{2}}\\\\ &{\\le2c\\eta\\rho\\left[L_{t}(\\theta^{\\prime})-L_{t}(\\theta)+\\frac{1}{4}\\|h_{t}(\\theta)-y_{t}\\|_{2}^{2}+4\\|e(\\theta^{\\prime},\\theta)\\|_{2}^{2}\\right]}\\\\ &{\\quad+2c\\eta\\rho\\frac{1}{4}\\|h_{t}(\\theta)-y_{t}\\|_{2}^{2}+\\frac{4}{2c\\eta\\rho}\\|e(\\theta-\\eta\\nabla_{\\theta}L_{t}(\\theta),\\theta)\\|_{2}^{2}+2\\|e(\\theta-\\eta\\nabla_{\\theta}L_{t}(\\theta),\\theta)\\|_{2}^{2}}\\\\ &{\\le2c\\eta\\rho\\left[L_{t}(\\theta^{\\prime})-\\frac{1}{2}L_{t}(\\theta)\\right]+8c\\eta\\rho\\|e(\\theta^{\\prime},\\theta)\\|_{2}^{2}}\\\\ &{\\quad+\\frac{2}{c\\eta\\rho}\\|e(\\theta-\\eta\\nabla_{\\theta}L_{t}(\\theta),\\theta)\\|_{2}^{2}+2\\|e(\\theta-\\eta\\nabla_{\\theta}L_{t}(\\theta),\\theta)\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the second inequality follows from the Peter-Paul inequality, and the last inequality follows from $\\lVert h_{t}(\\pmb{\\theta})-y_{t}\\rVert_{2}^{2}\\leq L_{t}(\\dot{\\pmb{\\theta}})$ . Rearranging the above inequality with $\\pmb\\theta=\\pmb\\theta_{t;j}$ and ${\\pmb\\theta}^{\\prime}={\\pmb\\theta}_{0}$ ,wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{t}(\\theta_{t;j+1})-L_{t}(\\theta_{0})}\\\\ &{\\le(1-c\\eta\\rho)\\left[L_{t}(\\theta_{t;j})-L_{t}(\\theta_{0})\\right]+c\\eta\\rho L_{t}(\\theta_{0})}\\\\ &{\\quad+\\left.8c\\eta\\rho\\|e(\\theta_{0},\\theta_{t;j})\\|_{2}^{2}+\\frac{2}{c\\eta\\rho}\\|e(\\theta_{t;j+1},\\theta_{t;j})\\|_{2}^{2}+2\\|e(\\theta_{t;j+1},\\theta_{t;j})\\|_{2}^{2}\\right.}\\\\ &{\\quad\\le(1-c\\eta\\rho)\\left[L_{t}(\\theta_{t;j})-L_{t}(\\theta_{0})\\right]+c\\eta\\rho\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t}\\\\ &{\\quad\\left.+\\,8c\\eta\\rho\\|e(\\theta_{0},\\theta_{t;j})\\|_{2}^{2}+\\frac{2}{c\\eta\\rho}\\|e(\\theta_{t;j+1},\\theta_{t;j})\\|_{2}^{2}+2\\|e(\\theta_{t;j+1},\\theta_{t;j})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, from Lemma B.15, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|e(\\theta_{0},\\theta_{t;j})\\|_{2}^{2}=\\big\\|h_{t}(\\theta_{0})-h_{t}(\\theta_{t;j})-G_{t}(\\theta_{t;j})^{\\top}(\\theta_{0}-\\theta_{t;j})\\big\\|_{2}^{2}}&{}\\\\ {=\\displaystyle\\sum_{i=1}^{t}\\big(h(x_{i};\\theta_{0})-h(x_{i};\\theta_{t;j})-\\langle g(x_{i};\\theta_{t;j}),\\theta_{0}-\\theta_{t;j}\\rangle\\big)^{2}}&{}\\\\ {\\le\\displaystyle\\frac{4t R^{4}(R+2)^{4}C_{\\alpha,D}^{(3)2}}{M}\\left(\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}\\right)^{2}}&{}\\\\ {\\le\\displaystyle\\frac{4t\\overline{R}^{4}(\\overline{R}+2)^{4}C_{\\alpha,D}^{(3)2}}{M}\\left(\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}\\right)^{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore, from $\\begin{array}{r}{\\|h_{t;j}-y_{t}\\|_{2}\\leq\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}}\\end{array}$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{t;j+1}-\\theta_{t;j}\\|=\\eta\\|\\nabla_{\\theta_{t;j}}L_{t}(\\theta_{t;j})\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{4\\rho}\\|2G_{t;j}(h_{t;j}-y_{t})+2(\\theta_{t;j}-\\theta_{0})\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2\\rho}\\left(\\|G_{t;j}\\|\\|h_{t;j}-y_{t}\\|_{2}+\\|\\theta_{t;j}-\\theta_{0}\\|_{2}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2\\rho}\\left[(2R+2^{D})\\sqrt{t C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}+R\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\Rightarrow\\|\\theta_{t;j+1}-\\theta_{0}\\|\\leq\\overline{{R}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the above inequality with Lemma B.15, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|e(\\pmb{\\theta}_{t;j+1},\\pmb{\\theta}_{t;j})\\|_{2}^{2}\\leq\\frac{4t\\overline{{R}}^{4}(\\overline{{R}}+2)^{4}C_{\\alpha,\\mathscr{D}}^{(3)2}}{M}\\left(\\ln\\frac{6\\cdot2^{{D+2}}M}{\\delta}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{8c\\eta\\rho\\|e(\\theta_{0},\\theta_{t;j})\\|_{2}^{2}+\\displaystyle\\frac{2}{c\\eta\\rho}\\|e(\\theta_{t;j+1},\\theta_{t;j})\\|_{2}^{2}+2\\|e(\\theta_{t;j+1},\\theta_{t;j})\\|_{2}^{2}}\\\\ &{\\leq\\left(8c\\eta\\rho+\\displaystyle\\frac{2}{c\\eta\\rho}+2\\right)\\frac{4t\\overline{{R}}^{4}(\\overline{{R}}+2)^{4}C_{\\alpha,\\overline{{D}}}^{(3)2}}{M}\\left(\\ln\\frac{6\\cdot2^{\\overline{{D}}+2}M}{\\delta}\\right)^{2}}\\\\ &{\\leq\\displaystyle\\frac{3}{c\\eta\\rho}\\frac{4t\\overline{{R}}^{4}(\\overline{{R}}+2)^{4}C_{\\alpha,\\overline{{D}}}^{(3)2}}{M}\\left(\\ln\\frac{6\\cdot2^{\\overline{{D}}+2}M}{\\delta}\\right)^{2}}\\\\ &{\\leq c\\eta\\rho\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the second line follows from the fact that $8c\\eta\\rho+2\\leq1/(c\\eta\\rho)$ holds due to $\\eta\\rho\\leq1/4$ and $c\\in(0,1)$ . Furthermore, the last line follows from the condition (233) with $c\\ge3/4$ .By combining Eq. (245) with Eq. (261), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert h_{t;j+1}-y_{t}\\Vert_{2}^{2}-\\Vert y_{t}\\Vert_{2}^{2}}\\\\ &{=L_{t}(\\theta_{t;j+1})-L_{t}(\\theta_{0})}\\\\ &{\\leq(1-c\\eta\\rho)[L_{t}(\\theta_{t;j})-L_{t}(\\theta_{0})]+2c\\eta\\rho\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t}\\\\ &{\\leq\\frac{2c\\eta\\rho\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t}{1-\\left(1-c\\eta\\rho\\right)}}\\\\ &{=2\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By combining the event ${\\mathcal{E}}_{4}$ with the above inequality, we obtain the desired inequality. ", "page_idx": 34}, {"type": "text", "text": "Fially we checktheasumption $\\lVert h_{t;j}\\,-\\,y_{t}\\rVert_{2}\\,\\leq\\,\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}$ $\\tilde{\\boldsymbol{j}}\\,=\\,0,\\,\\|\\boldsymbol{h}_{t;\\tilde{\\boldsymbol{j}}}\\,-$ $\\begin{array}{r}{y_{t}\\vert\\vert_{2}\\leq\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}}\\end{array}$ clearlyholds from thevent $\\mathcal{E}_{2}$ and $\\mathbf{}h_{t;0}=\\mathbf{0}$ Here, by applying the aforementioned arguments, we can also verify $\\lVert h_{t;\\tilde{j}}-y_{t}\\rVert_{2}\\,\\leq\\,\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3t}$ for $\\tilde{j}=1$ .Repeating the same arguments for $\\widetilde{j}=2,3,\\dots,j$ weobtain the inequality $\\|h_{t;j}-y_{t}\\|_{2}\\leq$ $\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln{\\frac{12T}{\\delta}}}\\right)\\sqrt{3t}.$ ", "page_idx": 34}, {"type": "text", "text": "Lemma B.11 (Adapted from Lemma B.2 in [41]). Suppose the following inequalities hold: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{R}}^{4}(\\overline{{R}}+2)^{4}\\leq\\frac{3\\eta^{2}M\\rho^{2}}{56C_{\\alpha,\\mathcal{D}}^{(3)2}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}\\right)^{-2},}\\\\ &{\\frac{C_{\\alpha,\\mathcal{D},T}^{(7)}}{\\sqrt{M}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\left(\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}\\right)\\sqrt{\\ln\\frac{6M}{\\delta}}\\leq1,}\\\\ &{\\eta\\leq4^{-1}\\left(\\rho+2(2\\tilde{R}+2^{\\mathcal{D}}\\hat{C})^{2}T C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}\\right)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\overline{{{R}}}=\\tilde{R}+\\displaystyle\\frac{1}{2\\rho}\\left[(2\\tilde{R}+2^{D})\\sqrt{T C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}}\\left(\\overline{{{k}}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{3T}+\\tilde{R}\\right],}}\\\\ {{\\tilde{R}=2\\left(\\overline{{{k}}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{\\frac{T}{\\rho}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthermore, we set C \u03b1,D,T as ", "page_idx": 35}, {"type": "equation", "text": "$$\nC_{\\alpha,\\mathcal{D},T}^{(7)}=2\\sqrt{3}T^{3/2}\\rho^{-3/2}\\sqrt{C_{\\alpha,\\mathcal{D}}^{(5)}}+16T^{2}\\rho^{-2}(\\tilde{R}+2)^{2}(2+2^{D}\\hat{C})C_{\\alpha,\\mathcal{D}}^{(3)}\\sqrt{C_{\\alpha,\\mathcal{D}}^{(4)}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, under the events $\\mathcal{E}_{2},\\,\\mathcal{E}_{3}$ and ${\\mathcal{E}}_{4}$ , the following inequalities hold for any $t\\in[T]$ and $j\\in[J]$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\theta_{t;j}-\\theta_{0}\\right\\|_{2}\\leq2\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{\\frac{T}{\\rho}},}\\\\ &{\\left\\|\\theta_{t;j}-\\tilde{\\theta}_{t;j}\\right\\|_{2}\\leq\\frac{C_{\\alpha,\\mathcal{D},T}^{(7)}}{\\sqrt{M}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}\\right)\\sqrt{\\ln\\frac{6M}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. We show by induction. Let us define $G_{t;j}$ and $h_{t;j}$ as $G_{t;j}=[g(x_{1};\\pmb{\\theta}_{t;j}),\\dots,g(x_{t};\\pmb{\\theta}_{t;j})]\\in$ $\\mathbb{R}^{p\\times i}$ and $\\begin{array}{r}{\\pmb{h}_{t;j}=[h(\\pmb{x}_{1};\\pmb{\\theta}_{t;j}),\\dots,h(\\pmb{x}_{t};\\pmb{\\theta}_{t;j})]\\stackrel{\\leftarrow}{\\in}\\mathbb{R}^{t}}\\end{array}$ , respectively. First, Eqs. (273) and (274) clearly hold if $j=0$ . Next, fix any $j\\in[J]$ , and suppose that Eqs. (273) and (274) hold for any $\\tilde{j}<j$ . Then, as with Lemma B.2 in [41], we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\theta_{t;j}-\\tilde{\\theta}_{t;j}\\right\\|_{2}\\leq\\left\\|\\left[I_{p}-2\\eta\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)\\right]\\left(\\theta_{t;j-1}-\\tilde{\\theta}_{t;j-1}\\right)\\right\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left.2\\eta\\right\\|\\left(G_{t;j-1}-G_{t}\\right)\\left(h_{t;j-1}-y_{t}\\right)\\right\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left.2\\eta\\right\\|G_{t}\\left[h_{t;j-1}-G_{t}^{\\top}\\left(\\theta_{t;j-1}-\\theta_{0}\\right)\\right]\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By resorting the similar argument of the proof of Lemma B.2 in [41], the first term is bounded from aboveasfollows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big\\|\\big[I_{p}-2\\eta\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)\\big]\\left(\\theta_{t;j-1}-\\tilde{\\theta}_{t;j-1}\\right)\\Big\\|_{2}\\le(1-2\\eta\\rho)\\Big\\|\\theta_{t;j-1}-\\tilde{\\theta}_{t;j-1}\\Big\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As for the second term of Eq. (275), from Lemma B.10 and Lemma B.15, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\left(G_{t;j-1}-G_{t}\\right)\\left(h_{t;j-1}-y_{t}\\right)\\right\\|_{2}}}\\\\ &{\\leq\\left\\|G_{t;j-1}-G_{t}\\right\\|\\left\\|h_{t;j-1}-y_{t}\\right\\|_{2}}\\\\ &{\\leq\\left\\|G_{t;j-1}-G_{t}\\right\\|_{F}\\left\\|h_{t;j-1}-y_{t}\\right\\|_{2}}\\\\ &{\\leq\\sqrt{\\displaystyle\\sum_{i=1}^{t}\\left\\|g(x_{i};\\theta_{t;j-1})-g(x_{i};\\theta_{0})\\right\\|_{2}^{2}\\sqrt{3t}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)}}\\\\ &{\\leq\\sqrt{3}T\\tilde{R}M^{-1/2}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{C_{\\alpha,D}^{(5)}\\ln\\frac{6M}{\\delta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Eq. (280) follows from Lemma B.10, the condition (267), and the induction hypothesis. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 Eq. (281) follows from the event $\\mathcal{E}_{3}$ and the induction hypothesis. ", "page_idx": 36}, {"type": "text", "text": "Furthermore, from Lemma B.13 and Lemma B.15, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|G_{t}\\left[h_{t;j-1}-G_{t}^{\\top}\\left(\\theta_{t;j-1}-\\theta_{0}\\right)\\right]\\right\\|_{2}}\\\\ &{\\leq\\|G_{t}\\|\\left\\|h_{t;j-1}-G_{t}^{\\top}\\left(\\theta_{t;j-1}-\\theta_{0}\\right)\\right\\|_{2}}\\\\ &{\\leq\\frac{2T\\tilde{R}^{2}(\\tilde{R}+2)^{2}(2+2^{D}\\hat{C})C_{\\alpha,\\mathcal{D}}^{(3)}\\left(\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}\\right)\\sqrt{C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}}}{\\sqrt{M}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By combining Eqs. (276), (281), and (284) with Eq. (275), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\theta_{t,\\theta_{l}}-\\theta_{l,\\theta_{l}}\\right|\\right|_{\\theta_{l}\\rightarrow-1}}\\\\ &{\\leq(1-2\\nu_{0})\\|\\theta_{l}\\rightarrow-\\theta_{l-1}\\|_{\\theta_{l}}}\\\\ &{\\quad+2\\nu_{0}T\\hat{H}\\theta_{l}\\rightarrow-\\left(\\frac{\\imath+1}{2}\\nu_{0}^{\\top}\\sqrt{2}\\nu_{1}^{\\top}\\right)\\sqrt{\\epsilon_{\\theta_{l},\\theta_{l}}^{(1)\\rightarrow1}}\\frac{\\,\\mathrm{d}U}{\\hat{H}}}\\\\ &{\\quad+\\frac{4\\nu_{0}T\\hat{H}}{2}(\\hbar\\hat{H}+2\\hat{H})^{2}(2\\hat{H}-2\\nu_{0}^{\\top}\\sqrt{2}\\nu_{0}^{\\top}\\left(8\\frac{\\imath+2\\nu_{1}}{\\hat{H}}\\right)\\sqrt{\\epsilon_{\\theta_{l},\\theta_{l}}^{(1)\\rightarrow1}}\\frac{\\,\\mathrm{d}U}{\\hat{H}}}\\\\ &{\\leq\\sqrt{\\pi}\\hbar^{-1}\\hat{H}\\rho_{0}^{-1}\\hat{H}\\left(\\hbar\\hat{H}+\\sqrt{2}\\right)\\prod_{s=1}^{\\nu_{0}}\\sqrt{\\epsilon_{\\theta_{l},\\theta_{l}}^{(1)\\rightarrow1}}\\frac{\\,\\mathrm{d}U}{\\hat{H}}}\\\\ &{\\quad+\\left.4T\\eta^{-1}\\hat{H}^{-1/2}\\hat{H}\\hat{H}(\\hat{H}+2)^{2}(2\\hat{\\pi}^{\\top}\\hat{H}\\coth^{2}\\left(1\\frac{\\theta_{l}+2\\sqrt{3}+12\\nu_{1}}{\\hat{H}}\\right)\\sqrt{\\epsilon_{\\theta_{l},\\theta_{l}}^{(1)\\rightarrow1}}\\frac{\\,\\mathrm{d}\\hat{H}}{\\hat{H}}}\\\\ &{\\leq\\hbar^{-1/2}\\left(\\hbar\\hat{H}+\\sqrt{2}\\right)\\left(\\frac{1-2\\nu_{1}}{\\hbar}\\frac{\\mathrm{d}U}{\\hat{H}}\\right)^{2}\\left[2\\hat{\\pi}\\hat{H}^{-1/2}\\nu_{0}^{\\top}\\sqrt{\\epsilon_{\\theta_{l},\\theta_{l}}^{(1)\\rightarrow1}}\\frac{\\,\\mathrm{d}\\hat{H}}{\\hat{H}}\\right.}\\\\ &{\\quad+\\left.16T\\hat{H}^{-1/2}\\left(1+2 \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where Eq. (289) follows from the condition (268). From the triangle inequality and Lemma B.9, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert\\theta_{t;j}-\\theta_{0}\\Vert_{2}\\leq\\left\\Vert\\tilde{\\theta}_{t;j}-\\theta_{0}\\right\\Vert_{2}+\\left\\Vert\\tilde{\\theta}_{t;j}-\\theta_{t;j}\\right\\Vert_{2}}\\\\ {\\leq2\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)\\sqrt{\\frac{T}{\\rho}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma B.12 (Adapted from Lemma C.1 in [24]). Under the event $\\mathcal{E}_{5}$ ,thefollowing inequality holds forany $t\\in\\mathbb{N}_{+}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{\\gamma}_{t}\\leq\\gamma_{t}+\\frac{t\\sqrt{t C_{\\alpha,\\cal D}^{(6)}\\ln(96|\\lambda^{\\gamma}|^{2}/\\delta)}}{\\rho\\sqrt{M}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the constant CD is defined in Lemma B.6. ", "page_idx": 36}, {"type": "text", "text": "Proof. Fix any $\\pmb{x}_{1},\\ldots,\\pmb{x}_{t}\\in\\mathcal{X}$ . Then, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}G_{t}^{\\top}G_{t}\\right)}\\\\ &{=\\frac{1}{2}\\ln\\operatorname*{det}\\left[I_{t}+\\rho^{-1}K_{t}+\\rho^{-1}\\left(G_{t}^{\\top}G_{t}-K_{t}\\right)\\right]}\\\\ &{\\leq\\frac{1}{2}\\ln\\operatorname*{det}\\left(I_{t}+\\rho^{-1}K_{t}\\right)+\\frac{1}{2\\rho}\\Bigl\\langle\\left(I_{t}+\\rho^{-1}K_{t}\\right)^{-1},G_{t}^{\\top}G_{t}-K_{t}\\Bigr\\rangle}\\\\ &{\\leq\\gamma_{t}+\\frac{1}{2\\rho}\\left\\|\\left(I_{t}+\\rho^{-1}K_{t}\\right)^{-1}\\right\\|_{F}\\left\\|G_{t}^{\\top}G_{t}-K_{t}\\right\\|_{F}}\\\\ &{\\leq\\gamma_{t}+\\frac{\\sqrt{t}}{2\\rho}\\sqrt{\\underset{s,\\hat{d}\\in\\{\\pi_{1},\\ldots,\\pi_{k}\\}}{\\sum}|k_{\\mathrm{TNTK}}(\\mathbf{x},\\bar{\\mathbf{x}})-\\langle g(\\mathbf{x},\\theta_{0}),g(\\bar{\\mathbf{x}},\\theta_{0})\\rangle|^{2}}}\\\\ &{\\leq\\gamma_{t}+\\frac{t\\sqrt{t}G_{0}^{\\top}\\ln(96|\\mathcal{X}|^{2}/\\delta)}{\\rho\\sqrt{M}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 Eq. (295) follows from the concavity of $\\ln\\operatorname*{det}(\\cdot)$ and the fact that $\\nabla_{X}$ ln det $X=X^{-1}$ holds for any symmetric matrix $\\mathbf{\\deltaX}$ . In Eq. (295), $\\langle\\cdot,\\cdot\\rangle$ represents the matrix inner product. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 Eq. (296) follows from the definition of $\\gamma_{t}$ ", "page_idx": 37}, {"type": "text", "text": "\u00b7 Eq. (298) follows from the event ${\\mathcal{E}}_{5}$ ", "page_idx": 37}, {"type": "text", "text": "Lemma B.13. Let us define $G_{t}(\\pmb\\theta)$ as $G_{t}(\\pmb\\theta)=(\\pmb g(\\pmb x_{1};\\pmb\\theta),\\pmb\\ldots,\\pmb g(\\pmb x_{t};\\pmb\\theta))^{\\top}\\in\\mathbb{R}^{p\\times t}$ . Then, under the event $\\mathcal{E}_{2}$ ,the following inequality holdsforany $R\\geq0$ $t\\in\\mathbb{N}_{+}$ ,and $\\pmb\\theta\\in\\mathbb{R}^{p}$ such that $\\begin{array}{r}{\\|\\pmb{\\theta}-\\pmb{\\theta}_{0}\\|_{2}\\leq R}\\end{array}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|G_{t}(\\pmb\\theta)\\|_{F}\\leq(2R+2^{2D}\\hat{C})\\sqrt{t C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the constants C and CD are defined in Lemma B.4. ", "page_idx": 37}, {"type": "text", "text": "Proof. From the definition of $\\|\\cdot\\|_{F}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|G_{t}(\\pmb{\\theta})\\|_{F}^{2}=\\sum_{i=1}^{t}\\|\\pmb{g}(\\pmb{x}_{i};\\pmb{\\theta})\\|_{2}^{2}}}\\\\ &{}&{\\leq t C_{\\alpha,D}^{(4)}(2R+2^{D}\\hat{C})^{2}\\ln\\frac{6M}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Here, the last inequality follows from the condition $\\mathcal{E}_{2}$ ", "page_idx": 37}, {"type": "text", "text": "Lemma B.14. Under the event $\\mathcal{E}_{2}$ , the following inequality holds for any $t\\in\\mathbb{N}_{+}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|\\rho I_{p}+G_{t}G_{t}^{\\top}\\right\\|\\leq\\rho+t C_{\\alpha,\\mathcal{D}}^{(4)}\\hat{C}^{2}2^{\\mathcal{D}}\\ln\\frac{6M}{\\delta},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the constants C and CD are defined in Lemma B.4. ", "page_idx": 37}, {"type": "text", "text": "Proof. Under the event $\\mathcal{E}_{2}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\rho I_{p}+G_{t}G_{t}^{\\top}\\|=\\rho+\\|G_{t}G_{t}^{\\top}\\|}\\\\ &{\\leq\\rho+\\displaystyle\\sum_{i=1}^{t}\\|g(x_{i};\\theta_{0})g(x_{i};\\theta_{0})^{\\top}\\|}\\\\ &{\\leq\\rho+\\displaystyle\\sum_{i=1}^{t}\\|g(x_{i};\\theta_{0})g(x_{i};\\theta_{0})^{\\top}\\|_{F}}\\\\ &{=\\rho+\\displaystyle\\sum_{i=1}^{t}\\|g(x_{i};\\theta_{0})\\|_{2}^{2}}\\\\ &{\\leq\\rho+t C_{\\alpha,p}^{(4)}2^{p}\\hat{C}^{2}\\ln\\frac{6M}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 Eq. (304) follows from $\\begin{array}{r}{G_{t}G_{t}^{\\top}=\\sum_{i=1}^{t}g(\\pmb{x}_{i};\\pmb{\\theta}_{0})g(\\pmb{x}_{i};\\pmb{\\theta}_{0})^{\\top}}\\end{array}$ and the triangle inequality. \u00b7 Eq. (306) follows from the fact that $\\|\\pmb{x}\\pmb{x}^{\\top}\\|_{F}=\\|\\pmb{x}\\|_{2}^{2}$ holds for any $\\textbf{\\em x}$ \u00b7 Eq. (307) follows from the event $\\mathcal{E}_{2}$ ", "page_idx": 38}, {"type": "text", "text": "Lemma B.15. Under the event ${\\mathcal{E}}_{1}$ the following inequality holds for any $\\pmb{x}\\in\\mathbb{S}^{d-1}$ \uff0c $R\\geq0,$ and $\\widetilde{\\pmb{\\theta}},\\hat{\\pmb{\\theta}}\\in\\mathbb{R}^{p}$ such that $\\lVert\\tilde{{\\boldsymbol{\\theta}}}-{\\boldsymbol{\\theta}}_{0}\\rVert_{2}\\leq R$ and $\\lVert\\widehat{\\pmb{\\theta}}-\\pmb{\\theta}_{0}\\rVert_{2}\\leq R$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n|h(\\pmb{x};\\tilde{\\pmb{\\theta}})-h(\\pmb{x};\\hat{\\pmb{\\theta}})-\\langle\\pmb{g}(\\pmb{x};\\hat{\\pmb{\\theta}}),\\tilde{\\pmb{\\theta}}-\\hat{\\pmb{\\theta}}\\rangle|\\leq\\frac{2R^{2}C_{\\alpha,\\mathcal{D}}^{(3)}(R+2)^{2}}{\\sqrt{M}}\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the constant CD is defined in Lemma 3.3. ", "page_idx": 38}, {"type": "text", "text": "Proof. From Taylor's theorem, there exists $a\\in[0,1]$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|h(\\pmb{x};\\tilde{\\theta})-h(\\pmb{x};\\hat{\\theta})-\\langle g(\\pmb{x};\\hat{\\theta}),\\tilde{\\theta}-\\hat{\\theta}\\rangle\\right|}\\\\ &{=\\frac{1}{2}\\left|(\\tilde{\\theta}-\\hat{\\theta})^{\\top}H\\left(\\pmb{x},a\\tilde{\\theta}+(1-a)\\hat{\\theta}\\right)(\\tilde{\\theta}-\\hat{\\theta})\\right|}\\\\ &{\\leq\\frac{1}{2}\\left\\|\\tilde{\\theta}-\\hat{\\theta}\\right\\|_{2}\\left\\|H\\left(\\pmb{x},a\\tilde{\\theta}+(1-a)\\hat{\\theta}\\right)(\\tilde{\\theta}-\\hat{\\theta})\\right\\|_{2}}\\\\ &{\\leq\\frac{1}{2}\\left\\|\\tilde{\\theta}-\\hat{\\theta}\\right\\|_{2}^{2}\\left\\|H\\left(\\pmb{x},a\\tilde{\\theta}+(1-a)\\hat{\\theta}\\right)\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here, from the conditions of $\\tilde{\\pmb{\\theta}}$ and $\\hat{\\pmb\\theta}$ wehave ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|\\tilde{\\pmb{\\theta}}-\\hat{\\pmb{\\theta}}\\right\\|_{2}\\leq\\left\\|\\tilde{\\pmb{\\theta}}-\\pmb{\\theta}_{0}\\right\\|_{2}+\\left\\|\\pmb{\\theta}_{0}-\\hat{\\pmb{\\theta}}\\right\\|_{2}\\leq2R\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|a\\tilde{\\pmb{\\theta}}+(1-a)\\hat{\\pmb{\\theta}}-\\pmb{\\theta}_{0}\\right\\|_{2}\\leq a\\left\\|\\tilde{\\pmb{\\theta}}-\\pmb{\\theta}_{0}\\right\\|_{2}+(1-a)\\left\\|\\pmb{\\theta}_{0}-\\hat{\\pmb{\\theta}}\\right\\|_{2}\\leq R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, from the event ${\\mathcal{E}}_{1}$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|h({\\pmb x};\\tilde{{\\pmb\\theta}})-h({\\pmb x};\\hat{{\\pmb\\theta}})-\\langle{\\pmb g}({\\pmb x};\\hat{{\\pmb\\theta}}),\\tilde{{\\pmb\\theta}}-\\hat{{\\pmb\\theta}}\\rangle\\right|\\leq\\frac{1}{2}\\left\\|\\tilde{{\\pmb\\theta}}-\\hat{{\\pmb\\theta}}\\right\\|_{2}^{2}\\left\\|{\\pmb H}\\left({\\pmb x},a\\tilde{{\\pmb\\theta}}+(1-a)\\hat{{\\pmb\\theta}}\\right)\\right\\|}\\\\ &{}&{\\leq\\frac{2R^{2}C_{\\alpha,D}^{(3)}(R+2)^{2}}{\\sqrt{M}}\\ln\\frac{6\\cdot2^{{\\mathcal D}+2}M}{\\delta}.~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof of Theorem B.1. Suppose that the events $\\mathcal{E}_{1}{-}\\mathcal{E}_{6}$ hold. As proposed in [30], we decompose the errorterm $|f(\\pmb{x})-h(\\pmb{x};\\pmb{\\theta_{t}})|$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f(x)-h(x;\\theta_{t})|}\\\\ &{\\leq|f(x)-\\tilde{\\mu}_{t}(x)|+|\\tilde{\\mu}_{t}(x)-\\langle g(x;\\theta_{0}),\\theta_{t}-\\theta_{0}\\rangle|+|\\langle g(x;\\theta_{0}),\\theta_{t}-\\theta_{0}\\rangle-h(x_{t};\\theta_{t})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By combining the event ${\\mathcal{E}}_{6}$ with Lemma B.12, the first term of Eq. (317) is bounded from above as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f(x)-\\tilde{\\mu}_{t}(x)|}\\\\ &{\\leq\\left(\\sqrt{2}B+\\displaystyle\\frac{\\sigma}{\\sqrt{\\rho}}\\sqrt{2\\left(\\tilde{\\gamma}_{t}+\\ln\\displaystyle\\frac{6}{\\delta}\\right)}\\right)\\tilde{\\sigma}_{t-1}(x)}\\\\ &{\\leq\\left(\\sqrt{2}B+\\displaystyle\\frac{\\sigma}{\\sqrt{\\rho}}\\sqrt{2\\left(\\gamma_{t}+\\displaystyle\\frac{t\\sqrt{t C_{\\alpha,D}^{(6)}\\ln(96|\\mathcal{X}|^{2}/\\delta)}}{\\rho\\sqrt{M}}+\\ln\\displaystyle\\frac{6}{\\delta}\\right)}\\right)\\tilde{\\sigma}_{t-1}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Furthermore, we obtain the following inequalities for the second term: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{\\mu}_{t}(x)-\\langle g(x;\\theta_{0}),\\theta_{t}\\rangle-\\theta_{0}\\rangle\\right|}\\\\ &{=\\left|g(x;\\theta_{0})^{\\top}\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)^{-1}G_{t}y_{t}-\\langle g(x;\\theta_{0}),\\theta_{t}-\\theta_{0}\\rangle\\right|}\\\\ &{\\leq\\rho^{-1}\\left\\|\\theta_{t}-\\theta_{0}-\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)^{-1}G_{t}y_{t}\\right\\|_{2}\\left\\|\\rho I_{p}+G_{t}G_{t}^{\\top}\\right\\|\\tilde{\\sigma}_{t}^{2}(x)}\\\\ &{\\leq\\rho^{-1}\\sqrt{\\tilde{k}^{2}+4C_{a,D}^{(2)}}\\left\\|\\theta_{t}-\\theta_{0}-\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)^{-1}G_{t}y_{t}\\right\\|_{2}\\left\\|\\rho I_{p}+G_{t}G_{t}^{\\top}\\right\\|\\tilde{\\sigma}_{t}(x)}\\\\ &{\\leq\\rho^{-1}\\sqrt{\\tilde{k}^{2}+4C_{a,D}^{(2)}}\\left[\\frac{C_{a,D}^{\\top}}{\\sqrt{M}}\\left(\\frac{\\lambda\\rho}{\\sqrt{M}}+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}\\right)\\sqrt{\\ln\\frac{6M}{\\delta}}\\right.}\\\\ &{\\quad+\\left.(1-2\\eta\\rho)^{\\prime/2}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\sqrt{\\frac{T}{\\rho}}\\right]}\\\\ &{\\quad\\times\\left(\\rho+T C_{a,D}^{(4)}2^{2D}\\tilde{C}^{2}\\ln\\frac{6M}{\\delta}\\right)\\tilde{\\sigma}_{t}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Eq. (322) follows from the feature space representation of ${\\tilde{\\mu}}_{t}$ .Actually,wehave $\\tilde{\\mu}_{t}(\\pmb{x})=$ $g(x;\\theta_{0})^{\\top}G_{t}(\\rho I_{t}+G_{t}^{\\top}G_{t})^{-1}y_{t}\\,=\\,g(x;\\theta_{0})^{\\top}\\left(\\rho I_{p}+G_{t}G_{t}^{\\top}\\right)^{-1}G_{t}y_{t}$ ,where the last equality follows from the matrix identity $\\mathbf{G}_{t}(\\rho I_{t}+G_{t}^{\\top}G_{t})^{-1}\\,=\\,\\bigl(\\rho I_{p}+G_{t}G_{t}^{\\top}\\bigr)^{-1}\\,G_{t}$ (e.g., Lemma 3 in [29]). ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Eq. (323) follows from the fact that $\\begin{array}{r l r}{\\langle z_{1},z_{2}\\rangle}&{{}\\leq}&{(z_{1}^{\\top}A^{-1}z_{1})}\\end{array}$ $\\begin{array}{r l}{(z_{2}^{\\top}A z_{2})}&{{}\\leq}\\end{array}$ $(z_{1}^{\\top}A^{-1}z_{1})\\|A\\|_{2}\\|z_{2}\\|_{2}$ holds for any positive definite matrix $A\\in\\mathbb{R}^{p\\times p}$ and $z_{1},z_{2}\\in\\mathbb{R}^{p}$ \uff1a \u00b7 Eq. (324) follows from ot(\u00b1) \u2264 \u221ak(x,\u03b1) \u2264 \u00b2 + 40 , where the last inequality follows from the event ${\\mathcal{E}}_{5}$ ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Eq. (325) follows from Lemma B.9 and Lemma B.11. ", "page_idx": 39}, {"type": "text", "text": "By using Taylor's theorem for the third term, there exist $a\\in[0,1]$ such that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad h(x;\\theta_{t})-\\langle g(x;\\theta_{0}),\\theta_{t}-\\theta_{0}\\rangle|}\\\\ &{=\\frac{1}{2}\\left|(\\theta_{t}-\\theta_{0})^{\\top}H(a\\theta_{t}+(1-a)\\theta_{0})(\\theta_{t}-\\theta_{0})\\right|}\\\\ &{\\le\\frac{1}{2}\\|\\theta_{t}-\\theta_{0}\\|_{2}\\|H(a\\theta_{t}+(1-a)\\theta_{0})(\\theta_{t}-\\theta_{0})\\|_{2}}\\\\ &{\\le\\frac{1}{2}\\|\\theta_{t}-\\theta_{0}\\|_{2}^{2}\\|H(a\\theta_{t}+(1-a)\\theta_{0})\\|}\\\\ &{\\le\\frac{72T^{2}C_{\\alpha,D}^{(3)}}{\\sqrt{M}\\rho^{2}}\\bigg(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\bigg)^{4}\\ln\\frac{6\\cdot2^{D+2}M}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where Eq. (327) follows from $h(\\pmb{x};\\pmb{\\theta}_{0})\\,=\\,0$ holds for any $\\textbf{\\em x}\\in\\mathcal{X}$ , and Eq. (330) follows from Lemma B.11 and the event ${\\mathcal{E}}_{1}$ . Finally, since the events $\\mathcal{E}_{1}{-}\\mathcal{E}_{6}$ holds with probability at least $1-\\delta$ from Lemma B.8, we obtain the desired result by aggregating Eqs. (320), (325), and (330). ", "page_idx": 40}, {"type": "text", "text": "C Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Theorem C.1 (Detailed version of Theorem 3.3). Suppose that Assumption 3.1 holds. Fix any $\\delta\\ \\in\\ (0,1),\\ \\alpha\\ \\geq\\ 1,\\ \\rho\\ >\\ 0,$ .and $\\mathcal{D}\\ \\geq\\ 2$ Furthermore,suppose that thenumber of ensemble $M$ is suffciently large to satisfy Eqs. (162)-(165). Then, if the learning rate $\\eta$ satisfy $\\begin{array}{r}{\\eta\\leq4^{-1}\\left(\\rho+2(2\\tilde{R}+2^{\\mathcal{D}}\\hat{C})^{2}T C_{\\alpha,\\mathcal{D}}^{(4)}\\ln\\frac{6M}{\\delta}\\right)^{-1}}\\end{array}$ $1-\\delta$ equality holds: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R_{T}}\\leq\\frac{{144T^{3}C_{\\alpha,\\mathcal{D}}^{(3)}}}{{\\sqrt{M}\\rho}}\\left({\\overline{{k}}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{4}\\ln\\frac{6\\cdot2^{\\mathcal{D}+2}M}{\\delta}}\\\\ &{\\quad+\\,\\beta\\sqrt{\\frac{{8T}}{{\\ln(1+\\rho^{-2})}}\\left({\\gamma_{T}}+\\frac{T\\sqrt{T C_{\\alpha,\\mathcal{D}}^{(6)}\\ln(96|\\mathcal{X}|^{2}/\\delta)}}{\\rho\\sqrt{M}}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\beta$ is defined in Theorem B.1. Furthermore, if $M$ and $J$ is sufficiently large to satisfy the following additional three conditions: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{144T^{3}C_{\\alpha,D}^{(3)}}{\\sqrt{M}\\rho}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{4}\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}\\leq1,}\\\\ &{\\frac{T\\sqrt{T C_{\\alpha,D}^{(6)}\\ln(96|X|^{2}/\\delta)}}{\\rho\\sqrt{M}}\\leq1,}\\\\ &{\\rho^{-1}\\sqrt{\\overline{{k}}^{2}+4C_{\\alpha,D}^{(2)}}\\left[\\frac{C_{\\alpha,D,T}^{(7)}}{\\sqrt{M}}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\left(\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}\\right)\\sqrt{\\ln\\frac{6M}{\\delta}}\\right.}\\\\ &{\\left.\\quad+\\left(1-2\\eta\\rho\\right)^{J/2}\\left(\\overline{{k}}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{2}\\sqrt{\\frac{T}{\\rho}}\\right]\\left(\\rho+T C_{\\alpha,D}^{(4)}2^{2D}\\hat{C}^{2}\\ln\\frac{6M}{\\delta}\\right)\\leq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then, ", "page_idx": 40}, {"type": "equation", "text": "$$\nR_{T}\\leq1+\\left(\\sqrt{2}B+1+\\frac{\\sigma}{\\sqrt{\\rho}}\\sqrt{2\\left(\\gamma_{T}+1+\\ln\\frac{6}{\\delta}\\right)}\\right)\\sqrt{\\frac{8T(\\gamma_{T}+1)}{\\ln(1+\\rho^{-2})}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The proof of Theorem C.1 leverages the following lemma, which describe the relation between the sumof $\\tilde{\\sigma}_{t-1}(\\pmb{x}_{t})$ and MIG. ", "page_idx": 40}, {"type": "text", "text": "Lemma C.1 (Lemma 5.3 and Lemma 5.4 in [32]). Fix any $T\\,\\in\\,\\mathbb{N}_{+}$ .Then,for anysequence $\\boldsymbol{x}_{1},\\ldots,\\boldsymbol{x}_{t}$ thefollowinginequalityholds: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\tilde{\\sigma}_{t-1}(x_{t})\\leq\\sqrt{\\frac{8T\\tilde{\\gamma}_{T}}{\\ln(1+\\rho^{-2})}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Theorem C.1. From Theorem B.1, with probability at least $1-\\delta$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R_{T}=\\sum_{i=1}^{T}[f(\\mathbf{x}_{t}^{*})-f(\\mathbf{x}_{t})]}}\\\\ &{\\leq\\frac{144T^{3}C_{\\alpha,D}^{(3)}}{\\sqrt{M_{\\rho}}}\\left(\\bar{k}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{4}\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}+2\\beta\\sum_{i=1}^{T}(\\bar{x}_{t-1}(x_{i})}\\\\ &{\\leq\\frac{144T^{3}C_{\\alpha,D}^{(3)}}{\\sqrt{M_{\\rho}}}\\left(\\bar{k}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{4}\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}+\\beta\\sqrt{\\frac{X\\cdot T}{\\ln(1+\\rho^{-2})}}}\\\\ &{\\leq\\frac{144T^{3}C_{\\alpha,D}^{(3)}}{\\sqrt{M_{\\rho}}}\\left(\\bar{k}B+\\sigma\\sqrt{2\\ln\\frac{12T}{\\delta}}\\right)^{4}\\ln\\frac{6\\cdot2^{D+2}M}{\\delta}}\\\\ &{+\\beta\\sqrt{\\frac{8T}{\\ln(1+\\rho^{-2})}}\\left(\\gamma\\ r+\\frac{T\\sqrt{T\\sigma_{0}^{(6)}}\\ln(96|X|^{2}/\\delta)}{\\rho\\sqrt{M}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where Eq. (339) follows from the definition of $\\pmb{x}_{t}$ , and Eq. (340) follows from Lemma C.1. Furthermore, Eq. (341) follows from Lemma B.12. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "D Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proof. Fix any function $f\\,\\in\\,\\mathcal{H}_{\\mathrm{TNTK}}$ . According to Mercer's representation theorem (see, e.g.. Theorem4.5.1in [331),tere exists asequence (wn.) suchthat $\\begin{array}{r}{\\sum_{n=0}^{\\bar{\\infty}}\\sum_{j=1}^{N_{d,n}}w_{n,j}^{2}<\\infty}\\end{array}$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\boldsymbol{f}(\\cdot)=\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}w_{n,j}\\lambda_{n}^{1/2}Y_{n,j}(\\cdot).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Furthermore, the RKHS norm $\\|f\\|_{\\mathrm{TNTK}}$ is obtained as $\\begin{array}{r}{\\|f\\|_{\\mathrm{TNTK}}^{2}=\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}w_{n,j}^{2}}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Note that, similar to the TNTK, the ReLU-based NTK can be expanded using spherical harmonics $(Y_{n,j})$ as follows (see, e.g., [35]): ", "page_idx": 41}, {"type": "equation", "text": "$$\nk_{\\mathrm{NTK}}(\\pmb{x},\\tilde{\\pmb{x}})=\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}\\tilde{\\lambda}_{n}Y_{n,j}(\\pmb{x})Y_{n,j}(\\tilde{\\pmb{x}}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $(\\Tilde{\\lambda}_{n})_{n\\in\\mathbb{N}}$ are the eigenvalues of the NTK. Here,the function $f$ can be written as ", "page_idx": 41}, {"type": "equation", "text": "$$\nf(\\cdot)=\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}w_{n,j}\\left(\\sqrt{\\frac{\\lambda_{n}}{\\tilde{\\lambda}_{n}}}\\right)\\lambda_{n}^{1/2}Y_{n,j}(\\cdot).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By noting both TNTK and NTK can be expanded by $\\left(Y_{n,j}\\right)$ , the following equation holds from Mercer's representation theorem: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|f\\|_{\\mathrm{NTK}}^{2}=\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}w_{n,j}^{2}\\frac{\\lambda_{n}}{\\tilde{\\lambda}_{n}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "According to Bietti and Bach [6], $\\tilde{\\lambda}_{n}=\\Theta(n^{-d})$ and there exists a constant $C_{d,L}\\,>\\,0$ such that $C_{d,L}n^{-d}\\leq\\tilde{\\lambda}_{n}$ . Combining this with Lemma 3.1, we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|f\\|_{\\mathrm{NTK}}^{2}\\leq\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}w_{n,j}^{2}C_{\\alpha,\\mathcal{D}}^{(1)}C_{d,L}^{-1}n^{d}\\exp\\left(-\\ln\\left(1+\\frac{1}{4\\alpha^{2}}\\right)\\mathcal{D}n\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since $\\begin{array}{r}{n^{d}\\exp\\left(-\\ln\\left(1+\\frac{1}{4\\alpha^{2}}\\right)\\mathcal{D}n\\right)\\to0}\\end{array}$ (as $n\\rightarrow\\infty,$ ), there exists a constant $C_{\\alpha,d}>0$ such that $\\begin{array}{r}{n^{d}\\exp\\left(-\\ln\\left(1+\\frac{1}{4\\alpha^{2}}\\right)\\mathcal{D}n\\right)\\le C_{\\alpha,d}}\\end{array}$ holds for any $n\\in\\mathbb N$ Thus, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|f\\|_{\\mathrm{NTK}}^{2}\\leq C_{\\alpha,\\mathcal{D}}^{(1)}C_{d,L}^{-1}C_{\\alpha,d}\\sum_{n=0}^{\\infty}\\sum_{j=1}^{N_{d,n}}w_{n,j}^{2}=C_{\\alpha,\\mathcal{D}}^{(1)}C_{d,L}^{-1}C_{\\alpha,d}\\|f\\|_{\\mathrm{TNTK}}^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From the above, it follows that $\\mathcal{H}_{\\mathrm{TNTK}}\\subset\\mathcal{H}_{\\mathrm{NTK}}$ ", "page_idx": 42}, {"type": "text", "text": "E Helper Lemmas ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Definition E.1 (Sub-Gaussian norm, Definition 2.5.6 in [39]). Let $X$ beareal-valuedrandom variable. Then, the following quantity $\\|X\\|_{\\psi_{2}}$ is called the sub-Gaussian norm of $X$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|X\\|_{\\psi_{2}}=\\operatorname*{inf}\\left\\{t\\geq0~\\bigg|\\;\\mathbb{E}\\left[\\exp\\left(\\frac{X^{2}}{t^{2}}\\right)\\right]\\leq2\\right\\}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Moreover, if $\\|X\\|_{\\psi_{2}}<\\infty$ holds, we call the random variable $X$ a sub-Gaussian random variable. ", "page_idx": 42}, {"type": "text", "text": "Definition E.2 (Sub-exponential norm, Definition 2.7.5 in [39]). Let $X$ beareal-valuedrandom variable. Then, the following quantity $\\|X\\|_{\\psi_{1}}$ is called the sub-exponential norm of $X$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|X\\|_{\\psi_{1}}=\\operatorname*{inf}\\left\\{t\\geq0~\\bigg|\\;\\mathbb{E}\\left[\\exp\\left(\\frac{|X|}{t}\\right)\\right]\\leq2\\right\\}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Moreover,if $\\|X\\|_{\\psi_{1}}<\\infty$ holds, we call the random variable $X$ a sub-exponential random variable. ", "page_idx": 42}, {"type": "text", "text": "Lemma E.1 (General Hoeffding's inequality, Theorem 2.6.2 in [39]). Let $X_{1},\\ldots,X_{N}$ beindependent, mean-zero, sub-Gaussian random variables. Then, for every $t\\geq0$ thefollowingholds: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i=1}^{N}X_{i}\\right|\\geq t\\right)\\leq2\\exp\\left(-\\frac{c t^{2}}{\\sum_{i=1}^{N}\\|X\\|_{\\psi_{2}}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $c>0$ is an absoluteconstant. ", "page_idx": 42}, {"type": "text", "text": "Lemma E.2 (Bernstain's inequality, Theorem 2.8.1 in [39]). Let $X_{1},\\allowbreak\\cdot\\cdot,X_{N}$ be independent, meanzero, sub-exponential random variables. Then, for every $t\\geq0$ thefollowingholds: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i=1}^{N}X_{i}\\right|\\geq t\\right)\\leq2\\exp\\left(-c\\operatorname*{min}\\left\\{\\frac{t^{2}}{\\sum_{i=1}^{N}\\|X_{i}\\|_{\\psi_{1}}^{2}},\\frac{t}{\\operatorname*{max}_{i\\in[N]}\\|X_{i}\\|_{\\psi_{1}}}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $c>0$ is an absoluteconstant. ", "page_idx": 42}, {"type": "text", "text": "Lemma E.3 (Centering, Lemma 2.6.8 and Exercise 2.7.10 in [39]). For any sub-Gaussian random variable $X$ $,\\,\\,\\|X\\,-\\,\\mathbb{E}[\\bar{X}]\\|_{\\psi_{2}}\\,\\le\\,C\\|X\\|_{\\psi_{2}}$ holds. Furthermore, for any sub-exponential random variable $Y$ $\\|Y-\\mathbb{E}[Y]\\|_{\\psi_{1}}\\le C\\|Y\\|_{\\psi_{1}}$ holds,where $C>0$ is an absolute constant. ", "page_idx": 42}, {"type": "text", "text": "Lemma E.4 (Product of sub-Gaussians is sub-exponential, Lemma 2.7.7 in [39]). Let $X$ and $Y$ besub-Gaussianrandomvariables.Then, $X Y$ is a sub-exponential randomvariablewhosesubexponentialnormsatisfies $\\|X Y\\|_{\\psi_{1}}\\leq\\|X\\|_{\\psi_{2}}\\|Y\\|_{\\psi_{2}}$ ", "page_idx": 42}, {"type": "text", "text": "F  Details of numerical experiments ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "F.1  Our implementation of algorithms ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Here, we provide additional information on the implementation of the ST-UCB and NN-UCB algorithms. Our implementation includes the following three simplifications: ", "page_idx": 42}, {"type": "text", "text": "(a) In the calculation of the gradient in line 5 of Algorithm 1, we use $\\pmb{g}(\\pmb{x};\\pmb{\\theta}_{t-1})$ from the previous round, rather than the initial gradient ${\\pmb g}({\\pmb x};{\\pmb\\theta}_{0})$ ", "page_idx": 43}, {"type": "text", "text": "(b) In the regularization of parameters in line 3 of Algorithm 3, we do not consider the residual from the initial parameters $\\theta_{0}$ . In other words, we apply L2 regularization directly to the parameters themselves.   \n(c) Instead of initializing $\\theta_{0}$ as described in Sec. 3.1, we initialized $\\theta_{0}$ by the Glorot's uniform initializer [17]. ", "page_idx": 43}, {"type": "text", "text": "It should be noted that the simplification (a) is the same implementation as the original NN-UCB, while the other simplifications are for the sake of simplicity in implementation. We train two models (ST, NN) using stochastic gradient descent (SGD) with a momentum term. The learning rate and the momentum are set to 0.01 and 0.9, respectively. When the momentum is greater than zero, past gradients are considered as a weighted average. SGD is performed in all rounds, with a mini-batch size of 64 and 5 epochs, and we do not use early stopping. ", "page_idx": 43}, {"type": "text", "text": "F.2  Parameter sensitivity ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In the results shown in Fig. 2 of the experimental section, we presented the outcomes with optimal hyperparameters of $\\epsilon,\\beta$ . Here, the experimental results for each parameter $\\epsilon\\in\\{0.05,0.1,0.2\\bar{\\},\\beta\\in$ $\\{0.01,0.1,1\\}$ are summarized in Fig. 3 (real-world dataset) and Fig. 4 (synthetic dataset). In most cases with the real-world dataset, as the rounds progressed, ST-UCB demonstrated better performance than NN-UCB, and UCB-based policies outperformed $\\epsilon_{}$ -greedy based policies when $\\beta=0.01$ . In the $f^{(1)}$ setting for the synthetic data, the regret of ST-UCB converged the fastest. On the other hand, in the $f^{(2)}$ and $f^{(3)}$ settings, NN-UCB sometimes performed well, however the trend of cumulative regret over the rounds was comparable between ST-UCB and NN-UCB. ", "page_idx": 43}, {"type": "text", "text": "G  Summary of the existing works ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "G.1 Derivation of TNTK ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Our analysis relies on the TNTK derived by Kanoh and Sugiyama [21]. From the definition of the soft tree ensemble model, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\theta}h(x;\\theta),\\nabla_{\\theta}h(x;\\theta)\\rangle=\\frac{1}{M}\\sum_{m=1}^{M}\\langle\\nabla_{\\theta^{(m)}}\\tilde{h}(x;\\theta^{(m)}),\\nabla_{\\theta^{(m)}}\\tilde{h}(x;\\theta^{(m)})\\rangle.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "If $\\pmb\\theta\\,\\sim\\,\\mathcal{N}(\\mathbf{0},\\pmb I_{p})$ $\\big(\\langle\\nabla_{\\pmb{\\theta}^{(m)}}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)}),\\nabla_{\\pmb{\\theta}^{(m)}}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)})\\rangle\\big)_{m\\in[M]}$ is mutually independent; therefore, from the law of large number, the inner product $\\langle\\nabla_{\\theta}\\bar{h}(\\mathbf{x};\\theta),\\nabla_{\\theta}h(\\mathbf{x};\\theta)\\rangle$ converges to $\\mathbb{E}[\\langle\\nabla_{\\pmb{\\theta}^{(m)}}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)}),\\nabla_{\\pmb{\\theta}^{(m)}}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)})\\rangle]$ in probability as $M\\to\\infty$ . Kanoh and Sugiyama [21] shows that $\\mathbb{E}[\\langle\\nabla_{\\pmb{\\theta}^{(m)}}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)}),\\nabla_{\\pmb{\\theta}^{(m)}}\\tilde{h}(\\pmb{x};\\pmb{\\theta}^{(m)})\\rangle]$ equals the expression in Eq. (1) by relying on the recursive expressions of the soft tree (such as Eq. (81)). ", "page_idx": 43}, {"type": "text", "text": "G.2 MIG and effective dimension ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "As described in Section 3.2, MIG is commonly used as the problem complexity parameter of the kernel-based decision-making problem. On the other hand, instead of MIG, some existing works quantify the problem complexity based on the following effective dimension d [10, 37, 40, 41]: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\tilde{d}=\\mathrm{Tr}(K_{T}(K_{T}+\\rho I_{T})^{-1}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Due to the following inequality [9, 10], the MIG is bounded from above by the worst-case effective dimension up to logarithmic scale: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\ln\\operatorname*{det}(\\rho^{-1}K_{T}+I_{T})\\le\\mathrm{Tr}(K_{T}(K_{T}+\\rho I_{T})^{-1})(1+\\ln(\\rho^{-1}\\|K_{T}\\|+1)).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "image", "img_path": "cKKXBhyijL/tmp/10e785ae71d0658b27bb53b18f090abc1f227ade85901187779f15e2f6b725ec.jpg", "img_caption": ["Figure 3: The average cumulative regret with one standard error in the real-world dataset. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "cKKXBhyijL/tmp/6d6d82e506782989c6acfb33e024f74122fdb48af5cbf7b54f62722b00bd39aa.jpg", "img_caption": ["Figure 4: The average cumulative regret with one standard error in the synthetic dataset. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The details of our contributions summarized in the abstract and introduction are given in Sec. 3.2 and Sec. 4. The sections in which the details of each contribution are described are explicitly stated in the introduction. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The assumptions for our main result are given in Assumption 3.1, and the discussions of their validities are described in Remark 3.1. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The assumptions for our main result is given in Assumption 3.1. The complete proofs of our main results are given in Appendix A, B, C, and D. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The detailed information for our experiment is given in Sec. 5 and Appendix F.1. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [No] ", "page_idx": 47}, {"type": "text", "text": "Justification: We are not yet ready to release the required codes and will do so as soon as our paper is accepted. Furthermore, we believe that the lack of experimental codes is not problematic because our experiment are not too complex, and the information given in Sec. 5 and Appendix F.1 is sufficient to reproduce numerical experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The information of our model hyperparameter is given in Sec. 5 and Appendix F.1. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The error bars, which represent one standard errors, are given in our experimental results. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [No] ", "page_idx": 48}, {"type": "text", "text": "Justification: Since our experiments are limited to simple problem setups and our contributions are primarily theoretical, the computational resources used are not significant. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We confirmed that our paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 48}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper poses no such risks. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper cited the original papers to use UCI-Machine Learning Repository in Sec.5. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}]