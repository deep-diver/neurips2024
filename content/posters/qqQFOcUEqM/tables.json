[{"figure_path": "qqQFOcUEqM/tables/tables_7_1.jpg", "caption": "Table 1: Comparative performance of OOD detection across baseline methods utilizing CLIP ViT-B/16 architecture with ImageNet-1k as ID data. Performance metrics are presented as percentages.", "description": "This table presents a comparison of the Area Under the Receiver Operating Characteristic curve (AUROC) and False Positive Rate at 95% True Positive Rate (FPR95) for various OOD detection methods.  The methods are categorized into traditional visual OOD detection methods and methods leveraging pre-trained vision-language models (VLMs). The table shows the performance of each method across four different OOD datasets (iNaturalist, SUN, Places, and Textures) and an average performance across all four datasets.  The results highlight the improvement achieved by the proposed method (Ours) compared to existing state-of-the-art methods, especially in terms of FPR95.", "section": "5.2 Evaluation on OOD detection benchmarks"}, {"figure_path": "qqQFOcUEqM/tables/tables_8_1.jpg", "caption": "Table 2: OOD detection performance comparison on hard OOD detection tasks.", "description": "This table presents the results of OOD detection experiments conducted on challenging datasets.  The \"hard\" aspect refers to the difficulty in distinguishing out-of-distribution (OOD) samples from in-distribution (ID) samples.  The table compares three methods: MCM, NegLabel, and the proposed method (Ours).  For each method, the AUROC (Area Under the Receiver Operating Characteristic curve) and FPR95 (False Positive Rate at 95% True Positive Rate) metrics are reported across various ID/OOD dataset pairings, where the ID dataset is drawn from ImageNet-10, ImageNet-20, ImageNet-100 or ImageNet-1k and the OOD dataset is drawn from ImageNet-20, ImageNet-10, ImageNet-100, ImageNet-O or Placesbg.", "section": "5.3 Empirical evidence supporting our assertions"}, {"figure_path": "qqQFOcUEqM/tables/tables_8_2.jpg", "caption": "Table 3: Ablation study with CLIP (ViT-B/16) as the backbone on ImageNet-1k as ID.", "description": "This table presents the ablation study, evaluating the impact of different components of the semantic pool on the overall performance.  The rows represent different configurations of the semantic pool, including the original semantic pool, a simple addition of adjective labels, and the conjugated semantic pool (CSP). The columns show the AUROC and FPR95 metrics for four OOD datasets (iNaturalist, SUN, Places, and Textures), as well as an average across these datasets. This ablation study helps illustrate the contribution of the CSP to improving OOD detection performance.", "section": "5.3 Empirical evidence supporting our assertions"}, {"figure_path": "qqQFOcUEqM/tables/tables_9_1.jpg", "caption": "Table 1: Comparative performance of OOD detection across baseline methods utilizing CLIP ViT-B/16 architecture with ImageNet-1k as ID data. Performance metrics are presented as percentages.", "description": "This table compares the performance of various OOD detection methods on the ImageNet-1k benchmark.  It shows the AUROC and FPR95 scores for different methods using the CLIP ViT-B/16 architecture, comparing traditional methods (MSP to VOS) with methods leveraging pre-trained VLMs (ZOC to NegLabel), and finally showing the proposed method.  The comparison uses different OOD datasets (iNaturalist, SUN, Places, and Textures) to provide a comprehensive evaluation.  The results illustrate the performance improvement of the proposed CSP method.", "section": "5.2 Evaluation on OOD detection benchmarks"}, {"figure_path": "qqQFOcUEqM/tables/tables_19_1.jpg", "caption": "Table 5: The expected Softmax scores of a single OOD label, an approximation of q2, from the original semantic pool and the conjugated semantic pool, scaled up by a factor of 1000.", "description": "This table compares the expected softmax scores of a single OOD label from both the original and conjugated semantic pools.  The scores are averaged across four OOD datasets (iNaturalist, SUN, Places, Textures) to approximate q2, representing the expected probability of OOD labels being activated by OOD samples.  Higher scores indicate a greater likelihood of OOD label activation, illustrating the effectiveness of the conjugated semantic pool in improving OOD detection.", "section": "5.3 Empirical evidence supporting our assertions"}, {"figure_path": "qqQFOcUEqM/tables/tables_20_1.jpg", "caption": "Table 6: Mean and standard deviation of OOD detection performance across various random seeds with CLIP-B/16 on ImageNet-1k as ID data. Performance metrics are presented as percentages.", "description": "This table presents the mean and standard deviation of OOD detection performance across various random seeds. The experiment uses CLIP-B/16 on ImageNet-1k as the ID data.  The results are broken down by OOD dataset (iNaturalist, SUN, Places, Textures) and performance metrics (AUROC and FPR95).  This shows the stability and reproducibility of the proposed method across different runs.", "section": "5.2 Evaluation on OOD detection benchmarks"}, {"figure_path": "qqQFOcUEqM/tables/tables_20_2.jpg", "caption": "Table 1: Comparative performance of OOD detection across baseline methods utilizing CLIP ViT-B/16 architecture with ImageNet-1k as ID data. Performance metrics are presented as percentages.", "description": "This table compares the performance of various OOD detection methods on the ImageNet-1k benchmark.  It contrasts traditional methods (MSP, ODIN, Energy, GradNorm, ViM, KNN, VOS) with methods that leverage pre-trained Vision-Language Models (VLMs) like CLIP (ZOC, MCM, NPOS, CoOp, CoCoOp, CLIPN, LSN, NegLabel).  The table presents the Area Under the Receiver Operating Characteristic curve (AUROC) and the False Positive Rate at 95% True Positive Rate (FPR95) for each method across four different OOD datasets: iNaturalist, SUN, Places, and Textures.  Higher AUROC and lower FPR95 values indicate better performance.", "section": "5.2 Evaluation on OOD detection benchmarks"}, {"figure_path": "qqQFOcUEqM/tables/tables_21_1.jpg", "caption": "Table 8: OOD detection performance evaluated by the FPR95 metric with different candidate selection ratios r. The results of our method and the baseline method NegLabel share similar trends.", "description": "This table presents the FPR95 performance of both the proposed method and the NegLabel baseline across different ratios (r) of selected OOD labels. The ratio r represents the proportion of OOD labels selected from the semantic pool. The table shows how the performance of both methods varies with this ratio, illustrating an initial improvement followed by a decline, which aligns with the theoretical analysis in the paper.", "section": "5.3 Empirical evidence supporting our assertions"}, {"figure_path": "qqQFOcUEqM/tables/tables_21_2.jpg", "caption": "Table 1: Comparative performance of OOD detection across baseline methods utilizing CLIP ViT-B/16 architecture with ImageNet-1k as ID data. Performance metrics are presented as percentages.", "description": "This table compares the performance of different OOD detection methods on the ImageNet-1k benchmark.  It specifically focuses on methods using the CLIP ViT-B/16 architecture with ImageNet-1k as the in-distribution (ID) dataset.  The table shows AUROC and FPR95 scores for various out-of-distribution (OOD) datasets (iNaturalist, SUN, Places, Textures), providing a comprehensive comparison of different approaches. Higher AUROC and lower FPR95 values indicate better performance. The methods are categorized into traditional visual OOD detection methods and methods leveraging pre-trained VLMs, highlighting the improvement achieved by incorporating textual information.", "section": "5.2 Evaluation on OOD detection benchmarks"}, {"figure_path": "qqQFOcUEqM/tables/tables_22_1.jpg", "caption": "Table 6: Mean and standard deviation of OOD detection performance across various random seeds with CLIP-B/16 on ImageNet-1k as ID data. Performance metrics are presented as percentages.", "description": "This table shows the mean and standard deviation of the Area Under the Receiver Operating Characteristic curve (AUROC) and the False Positive Rate at 95% true positive rate (FPR95) across 10 different random seeds.  The results are presented for four different OOD datasets (iNaturalist, SUN, Places, Textures) and the average performance across these datasets. The table demonstrates the robustness and stability of the proposed method by showing the small standard deviation across the runs.", "section": "5.2 Evaluation on OOD detection benchmarks"}]