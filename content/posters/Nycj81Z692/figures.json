[{"figure_path": "Nycj81Z692/figures/figures_1_1.jpg", "caption": "Figure 1: Illustrative example of urban relational triplet extraction and knowledge graph completion. (a) The heterogeneous relationship understanding limitation of LLMs can be addressed by injecting prior urban knowledge into instruction. (b) The geospatial computing limitation of LLMs can be alleviated by invoking external geospatial tools.", "description": "This figure shows two examples highlighting the limitations of LLMs in UrbanKGC tasks.  The first (a) demonstrates how LLMs struggle to understand heterogeneous relationships (e.g., spatial, temporal, and functional) in relational triplet extraction without the injection of prior knowledge. The second (b) illustrates how the limited geospatial reasoning capabilities of LLMs hinder accurate knowledge graph completion, emphasizing the need for integrating external geospatial tools for enhanced performance. ", "section": "1 Introduction"}, {"figure_path": "Nycj81Z692/figures/figures_4_1.jpg", "caption": "Figure 3: Quantitative performance analysis of prompting GPT-4 for UrbanKGC tasks. The result is obtained by comparing 50 GPT-4's outputs with the human's annotation.", "description": "This figure quantitatively analyzes the performance of GPT-4 on two UrbanKGC tasks: Relational Triplet Extraction (RTE) and Knowledge Graph Completion (KGC).  It shows that GPT-4 struggles with heterogeneous relationships (spatial, temporal, and functional) in RTE and with geospatial relations (DC, EC, EQ, PO, IN) in KGC, highlighting the limitations of LLMs in these tasks.  The results are based on comparing 50 GPT-4 outputs to human annotations.", "section": "3.2 Quantitative Task Analysis"}, {"figure_path": "Nycj81Z692/figures/figures_5_1.jpg", "caption": "Figure 4: An overview of UrbanKGent Construction.", "description": "The figure shows the overall pipeline of the UrbanKGent framework, which consists of three main modules: (1) Knowledgeable Instruction Generation, which generates instructions for aligning LLMs to UrbanKGC tasks; (2) Tool-augmented Iterative Trajectory Refinement, which enhances and refines the trajectories generated by LLMs by invoking external geospatial tools; and (3) Hybrid Instruction Fine-tuning, which fine-tunes LLMs based on the refined trajectories for cost-effectively completing diverse UrbanKGC tasks.  Each module is illustrated with detailed diagrams showing the steps and components involved.", "section": "4.2 Knowledgeable Instruction Generation"}, {"figure_path": "Nycj81Z692/figures/figures_9_1.jpg", "caption": "Figure 5: The model latency and cost of constructed UrbanKGent-13B and GPT-4 in UrbanKGC. We report the total inference time and cost of 1,000 RTE and KGC tasks.", "description": "This figure compares the latency (in minutes) and cost (in dollars) of using UrbanKGent-13B and GPT-4 to perform 1000 relational triplet extraction (RTE) and knowledge graph completion (KGC) tasks.  It shows that UrbanKGent-13B is significantly more cost-effective than GPT-4 while maintaining comparable performance.", "section": "5.3 Agent Application"}, {"figure_path": "Nycj81Z692/figures/figures_14_1.jpg", "caption": "Figure 7: The geometry range visualization of the head entity and tail entity of four KGC datasets. The horizontal and vertical coordinates are longitude and latitude, respectively. The blue and red polygons stand for entities with the polygon geometry, the purple line string stands for the entities with linestring geometry and the green point is for the coordinate entities.", "description": "This figure visualizes the geographic distribution of head and tail entities in four knowledge graph completion (KGC) datasets.  The datasets are represented by different colors and shapes, showing the spatial extent of each entity type. This visualization helps to understand the spatial distribution of data within the datasets and provides context for the KGC task.", "section": "2 UrbanKGC Data Description"}, {"figure_path": "Nycj81Z692/figures/figures_16_1.jpg", "caption": "Figure 4: An overview of UrbanKGent Construction.", "description": "This figure shows the overall pipeline of the UrbanKGent framework. It consists of three stages: Knowledgeable Instruction Generation, Tool-augmented Iterative Trajectory Refinement, and Hybrid Instruction Fine-tuning.  The Knowledgeable Instruction Generation stage focuses on aligning LLMs to UrbanKGC tasks using heterogeneity-aware and geospatial-infused instruction generation.  The Tool-augmented Iterative Trajectory Refinement stage enhances and refines generated trajectories using geospatial tools and self-refinement. Finally, the Hybrid Instruction Fine-tuning stage fine-tunes LLMs on refined trajectories to improve performance on diverse UrbanKGC tasks.", "section": "4.2 Knowledgeable Instruction Generation"}, {"figure_path": "Nycj81Z692/figures/figures_16_2.jpg", "caption": "Figure 4: An overview of UrbanKGent Construction.", "description": "This figure shows the overall pipeline of the UrbanKGent framework. It includes three main modules: Knowledgeable Instruction Generation, Tool-augmented Iterative Trajectory Refinement, and Hybrid Instruction Fine-tuning.  Each module has several steps, demonstrating how UrbanKGent generates instructions for LLMs, refines the trajectories using external geospatial tools, and fine-tunes LLMs for urban knowledge graph construction tasks.", "section": "4.2 Knowledgeable Instruction Generation"}, {"figure_path": "Nycj81Z692/figures/figures_18_1.jpg", "caption": "Figure 10: The Spearman correlation between the GPT evaluation and human's evaluation under five different LLM backbones (i.e., Llama-2-7B, Llama-2-13B, Llama-2-70B, GPT-3.5 and GPT-4).", "description": "This figure shows the Spearman correlation coefficients between GPT-4 evaluation results and human evaluation results for different LLMs (Llama-2-7B, Llama-2-13B, Llama-2-70B, GPT-3.5, and GPT-4) on two tasks (RTE and KGC) and two datasets (NYC and CHI).  The higher the correlation coefficient, the more consistent the GPT-4 evaluation is with human judgment.", "section": "D.3 Evaluation Consistency"}, {"figure_path": "Nycj81Z692/figures/figures_18_2.jpg", "caption": "Figure 10: The Spearman correlation between the GPT evaluation and human's evaluation under five different LLM backbones (i.e., Llama-2-7B, Llama-2-13B, Llama-2-70B, GPT-3.5 and GPT-4).", "description": "This figure shows the Spearman correlation coefficients between GPT-4 evaluations and human evaluations for the RTE and KGC tasks. The evaluations were performed using five different LLM backbones: Llama-2-7B, Llama-2-13B, Llama-2-70B, GPT-3.5, and GPT-4.  The high correlation coefficients (above 0.8) across all backbones indicate a strong agreement between GPT-4 and human evaluations, suggesting the reliability of using GPT-4 for automated evaluation.", "section": "D.3 Evaluation Consistency"}]