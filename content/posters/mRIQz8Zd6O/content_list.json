[{"type": "text", "text": "AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yao Fu 1\u2217 Dong-Ki Kim 2\u2217 Jaekyeom Kim 2 Sungryull Sohn 2 Lajanugen Logeswaran 2 Kyunghoon Bae 2 Honglak Lee 1,2 1University of Michigan 2LG AI Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AUTOGUIDE, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent\u2019s current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AUTOGUIDE significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in large language models (LLMs) have empowered AI agents to address various sequential decision-making tasks and applications [1, 2]. The foundation of these successes involves the planning and reasoning capabilities of pre-trained LLMs, enabling agents to execute effective policies [3, 4]. The predominant approach to leveraging these (typically closed source) models for sequential decision making tasks is to provide demonstrations in the form of in-context examples. However, direct application of this learning paradigm can be limited, especially in target domains where the LLM has insufficient prior knowledge such as in web navigation, where LLM agents generally achieve low success rates due to diverse and dynamic contents [5\u20138]. Providing all available experiences as demonstrations to an agent can further be unsuccessful due to context length limitations, prompt sensitivity, and difficulty with complex reasoning [9\u201312]. ", "page_idx": 0}, {"type": "text", "text": "On the other hand, LLMs excel in interpreting concise instructions provided as natural language, an ability that is also reinforced in the instruction-tuning phase of LLMs. Inpired by this, we explore data-driven strategies that leverage offilne experiences to extract actionable knowledge to help guide LLM agents. As offline experiences implicitly convey valuable knowledge about desirable and undesirable policies in domains, they promise to serve as a useful resource for improving an LLM agent\u2019s decision-making in situations where the pre-trained LLM lacks understanding. Despite this potential benefti, a critical challenge lies in effectively extracting the implicit information embedded in offline data. ", "page_idx": 0}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/7123a558b394c3c4dfa2b6f8c52ade70626ff840799ab2f712914e8e9fc53bf4.jpg", "img_caption": ["Figure 1: AUTOGUIDE aims to extract the implicit knowledge embedded in offline experiences and help the decision-making process of an LLM agent. Specifically, our method generates a comprehensive set of context-aware guidelines from offline data and explicitly identifies when each guideline is applicable by generating its corresponding context. Our context-aware guidelines enable providing pertinent guidelines at test time by identifying the context of the current trajectory, leading to correct decision-making compared to baselines without context-aware guidelines. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To address the challenge of extracting knowledge from offline data, we propose a novel framework, called AUTOGUIDE. Specifically, AUTOGUIDE automatically derives a comprehensive set of contextaware guidelines from offilne experiences. Our method applies these context-conditional guidelines to enhance the performance of an LLM agent by retrieving guidelines relevant to the agent\u2019s current state and incorporating them into the prompt during testing (see Figure 1). Notably, we generate context-aware guidelines in concise natural language statements, effectively compressing knowledge in offline data. Moreover, context-aware guidelines clearly describe the contexts where they are applicable, so AUTOGUIDE enables an LLM agent to select pertinent guidelines for its current decision-making process. As a result, AUTOGUIDE achieves the highest success rates compared to competitive baselines in complex sequential decision-making benchmark environments. ", "page_idx": 1}, {"type": "text", "text": "Our contribution. In summary, we present the following main contributions in this paper: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Principled method based on context-aware guidelines (Section 3): We develop two modules to automatically generate context-aware guidelines from offilne experiences: the context identification module for identifying the context of a given trajectory, and the guideline extraction module for extracting a desired guideline corresponding to that context. The outcome is a set of domain knowledge in concise natural language that enhances decision-making by providing pertinent information. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Comprehensive evaluation of AUTOGUIDE (Section 4.2): We show AUTOGUIDE\u2019s capability in extracting helpful context-aware guidelines in various interactive benchmark domains, including navigating real-world web domains (e.g., GitHub). Our results highlight the effectiveness of AUTOGUIDE, which significantly outperforms baselines without context-aware guidelines. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Analyses with important perspectives (Section 4.3): We study various aspects of AUTOGUIDE, such as the significance of determining the applicability of each guideline based on generated contexts. We also investigate the generalization ability of context-aware guidelines and demonstrate that our guidelines enhance the performance across out-of-domain tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "LLM-based agents. Language models have recently been shown to possess strong priors for sequential decision-making tasks, which has given rise to LLM-powered agents [1, 2, 13, 14]. Agents ", "page_idx": 1}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/5340c232c45d4ae487052f6e7aad777e8e77d89edca993e14627383d19fb9a91.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Context-aware guideline generation process based on a pair of contrastive trajectories $\\tau_{+}^{i}$ and $\\tau_{-}^{i}$ . In this example, the two trajectories start deviating from each other at $t\\!=\\!0$ . The context identification module generates a description of the context at $t=0$ given $\\tau_{:0}^{i}$ , and the guideline extraction module generates the corresponding guideline for that context. ", "page_idx": 2}, {"type": "text", "text": "need to possess various skills to be effective in practice including planning [3, 15, 16], reasoning [4, 17], tool manipulation [18\u201321], code generation [22, 16], among others. In this work, we focus on building effective agents for web [23, 8] and embodied [24] environments. ", "page_idx": 2}, {"type": "text", "text": "Self-reflection from past experiences. An important capability for agents to succeed is the ability to learn from past experiences and update their behavior based on feedback. Self-feedback [25\u201327] has emerged as an effective technique where a model inspects its own incorrect predictions, reflects on it to identify what went wrong and attempts to improve its prediction. While self-feedback provides intra-task (i.e., per-episode) knowledge within a task based on immediate feedback, our approach offers an orthogonal and complementary aspect of inter-task knowledge (over multiple tasks) by considering multiple train tasks in offilne data. AutoGuide enhances learning efficiency and credit assignment by utilizing detailed feedback from multiple tasks. However, these self-feedback approaches are complementary to AUTOGUIDE and can be used in conjunction with our approach, as shown in our experiments (see Section 4.2). ", "page_idx": 2}, {"type": "text", "text": "Leveraging natural language guidance. Natural Language can be a rich source of information for agents to learn to act efficiently. Prior work has explored the notion of learning from humanwritten text manuals, which describe details about the environment [28\u201330]. Recent work has explored automatically generating such guidance in the form of chain-of-thought reasoning [4, 23], which emulates a thought process or rationale for agent\u2019s predictions. In contrast to approaches which generate such guidance dynamically on the fly by imitating example guidance demonstrations provided by a human, our approach carefully compares trajectories in offline data to generate appropriate guidance and uses these guidelines for predicting better actions. ExpeL [31] proposed a related approach to derive guidelines. In contrast to ExpeL, where all guidelines are provided to an agent as a prompt, our guideline selection process is contextual, where guidelines relevant to the agent\u2019s current state are retrieved and used for prediction. We show that this substantially improves over ExpeL\u2019s non-contextual guideline-based approach. ", "page_idx": 2}, {"type": "text", "text": "3 AUTOGUIDE: Principled Method Based on Context-Aware Guidelines ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work is motivated by the increasing availability of offline experiences that agents or humans naturally accumulate through their interactions with the environment. AUTOGUIDE aims to leverage this offilne data to improve the decision-making of an LLM agent by generating helpful context-aware guidelines. This section details how AUTOGUIDE automatically constructs these guidelines and applies them to guide action generation at test time. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally, AUTOGUIDE is given offilne data $\\mathcal{D}_{\\mathrm{train}}\\!=\\!(\\tau^{1},...,\\tau^{N})$ that consist of $N$ trajectories from training tasks. Each trajectory $\\pmb{\\tau}=(x_{0},a_{0},r_{0},...,r_{T})$ is a sequence of observations, actions, and rewards following the partially observable Markov decision process [32]. The return of a trajectory is defined as the sum of rewards obtained throughout the trajectory: $\\begin{array}{r}{R(\\tau)\\!=\\!\\sum_{t=0}^{T}r_{t}}\\end{array}$ tT=0 rt. The objective of AUTOGUIDE is to distill knowledge from offline experiences into a useful natural language format, such that the extracted information helps to maximize the expected return $\\mathbb{E}_{\\tau}[R(\\tau)]$ during test time. ", "page_idx": 2}, {"type": "text", "text": "Algorithm 2 Applying context-aware guidelines ", "page_idx": 3}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/c684dac225f5ed787ea44ec0ed2c6a83d8b24ea92dd0e3f0acde57021a43d313.jpg", "table_caption": ["Algorithm 1 Extracting context-aware guide"], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Extraction of Context-Aware Guidelines ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "AUTOGUIDE generates a set of context-aware guidelines by utilizing pairs of contrastive trajectories from offline data. Each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context in which the guideline is applicable. Intuitively, contrasting a pair of trajectories with different returns provides important information about when and which actions are effective or ineffective in maximizing expected returns. Building on this insight, we develop two modules for automatically extracting context-aware guidelines (see Figure 2): ", "page_idx": 3}, {"type": "text", "text": "Context identification module. This module is responsible for abstracting the given partial trajectory into its context, a concise natural language description of the agent\u2019s state. More specifically, for a timestep $t$ and the corresponding trajectory $\\bar{\\tau_{:t}^{i}}^{:}{=(x_{0},a_{0},...,x_{t})}$ , we prompt LLMs to clearly describe the agent\u2019s status: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{CONTEXT}\\leftarrow\\mathcal{M}_{\\mathrm{context}}(\\tau_{:t}^{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our prompt templates for the context identification module are shown in Appendix C.1. ", "page_idx": 3}, {"type": "text", "text": "Guideline extraction module. This module aims to generate a desired guideline corresponding to a specific context. Let $\\tau_{+}^{i}$ and $\\tau_{-}^{i}$ represent a contrasting pair of trajectories for the same task $i$ in offline data $\\mathcal{D}_{\\mathrm{train}}$ , where $R(\\tau_{+}^{i})>R(\\tau_{-}^{i})$ . We want to contrast the pair of trajectories to find desired behaviors at an important timestep. To do this, we compare these two trajectories to find the deviation timestep $t$ at which they begin to diverge due to different actions. Then we apply the context identification module to summarize the context for the shared part of the trajectory $\\tau_{:t}^{i}$ . Eventually, we extract a useful natural language guideline by examining the paired contrastive trajectories $\\tau_{+}^{i}$ and $\\tau_{-}^{i}$ with respect to the context: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{GUDELINE}\\!\\gets\\!\\mathcal{M}_{\\mathrm{guideline}}\\big(\\tau_{+}^{i},\\tau_{-}^{i},\\mathrm{CONTEXT}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we refer to Appendix C.2 for our prompt template. As an example, the paired trajectories in Figure 2 deviate from timestep $t=0$ , for which the context is summarized as On the Reddit main page. This module then generates the following context-aware guideline: When on the Reddit main page, if you want to navigate to a specific forum, you should click on \u201clink \u2018Forums\u2019\u201d located right above \u201clink \u2018Wiki\u2019\u201d. ", "page_idx": 3}, {"type": "text", "text": "Construction of context-aware guidelines. We collect context-aware guidelines $\\mathcal{G}$ by iterating through available pairs in the paired offilne data and organize the guidelines in a dictionary format, using the context as the key and the corresponding guidelines as the value (see Algorithm 1). In particular, we observe that the context identification module occasionally produces contexts that describe the same situation but are expressed slightly differently. To minimize redundancy, we employ an LLM to determine if the current context corresponds to any previously identified context. ", "page_idx": 3}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/c87257c8ea62e47e5d28380b12b1f36dda17f67b88771255c16da501d66e5655.jpg", "img_caption": ["Figure 3: Sequential decision-making benchmark domains considered in our work: ALFWorld [24], WebShop [33], WebArena [8], and multi-modal real-world websites. Graphic credit: [34, 33, 8, 35]. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "If a match is found, we reuse the existing context; otherwise, we introduce a new context into our dictionary $\\mathcal{G}$ . The specific prompt template for this context-matching procedure is in Appendix C.3. ", "page_idx": 4}, {"type": "text", "text": "3.3 Applying Context-Aware Guidelines at Test Time ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After extracting a set of context-aware guidelines $\\mathcal{G}$ from offilne experiences, our method employs these guidelines to enhance the decision-making of an LLM agent during testing. At each timestep, AUTOGUIDE identifies the CONTEXT of the current test trajectory $\\tau$ up to timestep $t$ (which represents the agent\u2019s interactions up to the current time-step) using our context identification module $\\mathcal{M}_{\\mathrm{context}}$ Our guideline selection module $\\mathcal{M}_{\\mathrm{select}}$ then selects relevant guidelines for CONTEXT from $\\mathcal{G}$ More specifically, the module applies CONTEXT as the key to fetch a set of possible guidelines $\\mathcal{G}$ [CONTEXT]. If there are more than $k$ guidelines in $\\mathcal{G}$ [CONTEXT], $\\mathcal{M}_{\\mathrm{select}}$ prompts an LLM to choose top- $k$ guidelines for the specific $\\tau$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{RELEVANT~GUIDELINES}\\leftarrow\\mathcal{M}_{\\mathrm{select}}(\\mathrm{CONTEXT},\\tau;\\mathcal{G},k),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where Appendix C.4 details the prompt template for this selection procedure. Subsequently, AUTOGUIDE incorporates both the context and relevant guidelines into the agent\u2019s action generation prompt. Therefore, the agent selects an action by considering the provided context and guidelines (see Figure 1 for an example). This process iterates until the end of the test trajectory (see Algorithm 2). ", "page_idx": 4}, {"type": "text", "text": "Key benefits of AUTOGUIDE. First, the extraction of context-aware guidelines in AUTOGUIDE offers the inherent benefti of providing relevant guidelines for the context of interest. This capability is important since neglecting the specific context in which a guideline applies can confuse the agent\u2019s decision-making process. The second key benefit is the generation of concise natural language guidelines, which can be seamlessly incorporated into any prompt-based LLM agent. Lastly, AUTOGUIDE generates guidelines at the individual context level rather than at the trajectory level. Given that a single incorrect action can lead to a complete failure, it is essential to provide detailed assistance in each action selection process. With these advantages, we demonstrate in the next section that our approach significantly enhances the performance of LLM agents. ", "page_idx": 4}, {"type": "text", "text": "4 Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section demonstrates the efficacy of AUTOGUIDE by conducting experiments on a diverse suite of sequential decision-making benchmark domains. We also perform important analyses about AUTOGUIDE, such as the ablation study of different AUTOGUIDE components, comparison to ", "page_idx": 4}, {"type": "text", "text": "in-context learning, and generalization to out-of-domain tasks. We refer to Appendix B for additional experimental details. ", "page_idx": 5}, {"type": "text", "text": "4.1 Evaluation Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1.1 Sequential Decision-Making Benchmark Domains ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider the following interactive sequential decision-making benchmarks to study various aspects of AUTOGUIDE (see Figure 3): ", "page_idx": 5}, {"type": "text", "text": "\u2022 ALFWorld [24]: In this embodied benchmark, an LLM agent interacts with an environment to carry out household tasks, such as placing a pan on the dining table. Observations and actions are expressed in natural language statements, and the agent must navigate through the space and manipulate objects to successfully complete the tasks. ", "page_idx": 5}, {"type": "text", "text": "\u2022 WebShop [33]: This interactive web environment simulates the task of online shopping on an e-commerce website. The agent\u2019s goal is to understand a text instruction and buy a product that meets specified criteria. This involves querying the website\u2019s search engine, understanding the descriptions and details of each item, and selecting necessary options. ", "page_idx": 5}, {"type": "text", "text": "\u2022 WebArena [8]: This web-based benchmark introduces realistic environments by replicating the functionality and data found in popular web domains (e.g., Gitlab, Reddit, Wikipedia). Compared to WebShop, WebArena presents more challenges and difficulties for an LLM agent due to its large observation and action space, along with tasks that involve longer planning horizons. We focus oe the Reddit domain for the main WebArena experiments. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Real-world multi-modal websites: Finally, we consider evaluating AUTOGUIDE on a variety of real-world website tasks. These span from a collaborative software development platform (e.g., GitHub) to a flight search engine (e.g., Google Flights) and an online education platform (e.g., Coursera). Please refer to Appendix B.4 for example tasks. In particular, in comparison to WebShop and WebArena, we design our tasks to be multi-modal such that the agent must consider both visual (e.g., images) and textual information (e.g., HTML) to complete these tasks. ", "page_idx": 5}, {"type": "text", "text": "4.1.2 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We compare AUTOGUIDE against the following baseline approaches to study the effect of contextaware guidelines (refer to Appendix B for more details): ", "page_idx": 5}, {"type": "text", "text": "\u2022 ReAct [23]: This LLM-based planning method integrates reasoning and acting to address sequential decision-making tasks. However, it does not leverage offline experiences and thus suffers from the limited understanding of pre-trained LLMs in downstream domains. ", "page_idx": 5}, {"type": "text", "text": "\u2022 ExpeL [31]: This method also extracts natural language knowledge from offline data. However, it fails to consider the applicability of guidelines and does not generate context-aware guidelines. Instead, it provides all guidelines to an LLM agent without filtering out irrelevant ones based on the current context. ExpeL has two contributions, the guideline generation and in-context example selection module. Because the latter is orthogonal to our analysis and can be seamlessly combined with our method, we consider ExpeL with guidelines in our experiments. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Reflexion [27]: This approach converts environmental feedback into text statements to assist an LLM agent (e.g., ReAct) in the next trial. The baseline generates valuable feedback about solving a specific test task. We demonstrate how context-aware guidelines derived by AUTOGUIDE can be combined with the feedback. ", "page_idx": 5}, {"type": "text", "text": "4.1.3 Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We collect offline experiences either by running ReAct and Reflexion, or incorporating human demonstrations. We use ReAct with GPT-3.5-turbo as our base LLM agent for WebShop and ALFWorld and GPT-4-turbo for WebArena. For each benchmark, we apply the same GPT model for action generation, context identification, and guideline selection. We extract context-aware guidelines from offline data with GPT-4-turbo and evaluate their effectiveness by applying them to the test set with non-overlapping tasks. We refer to Appendix B for more details. ", "page_idx": 5}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/5ab9ee239d77ea25b7e743eb55c0383ae868b86846d25c3a9043c4f712a7834d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/89f033274163a95a6fec49032bded1c565d5e8fbed559bd79254e7a2a15a0515.jpg", "table_caption": ["Figure 4: Trajectories of ReAct, ExpeL, and AUTOGUIDE from the same test task. ReAct (Left) chose the wrong item, consequently failing the task in the end. ExpeL (middle) was confused by guidelines that were irrelevant to current context, leading to incorrect reasoning and actions. AUTOGUIDE (right) selects relevant guidelines to the agent\u2019s context, enabling the agent to accomplish the task. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Q1. How effective is AUTOGUIDE compared to baselines without context-aware guidelines? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To answer this question, we compare methods on ALFWorld, WebShop, and WebArena benchmarks. The performance on the test datasets is presented in Table 1. There are three notable observations: ", "page_idx": 6}, {"type": "text", "text": "1. Effectiveness of context-aware guidelines. Our approach surpasses baseline performance in both ALFWorld and WebShop, achieving the highest test rewards and success rates in Table 1. These results highlight the effectiveness of employing context-aware guidelines in language-based decisionmaking domains. To further examine the action selection differences among ReAct, ExpeL, and our method, we present their trajectories in Figure 4. We observed that ReAct makes common mistakes such as trying to take soapbar that is not visible, or taking a soapbottle instead of soapbar due to their similar names. Both ExpeL and AUTOGUIDE improve on this by extracting guidelines from similar mistakes in the offline experience. However, ExpeL often erroneously applies incorrect guidelines ", "page_idx": 6}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/5168b0164f28959d44266e900ff1ec7822f961f1d0c278442838345ea7cd6f02.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Test results of AUTOGUIDE on 3 real-world web domains within multi-modal settings. The base agent model runs with GPT-4V and applying context-aware guidelines significantly improves the performance. ", "page_idx": 7}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/ac7298d0ff44e03f49876cbd9afcc626c8baea979ddd0833cfa14843ec45edea.jpg", "table_caption": [], "table_footnote": ["Table 3: Analysis of AUTOGU- Table 4: Ablation study IDE against ReAct with varying of AUTOGUIDE using numbers of in-context examples. various top- $k$ values. "], "page_idx": 7}, {"type": "text", "text": "due to the availability of all guidelines at each timestep. In Figure 4, ExpeL mistakenly attends to the second guideline \u201censure to specify the item\u2019s number and location ...\u201d, leading to wrong reasoning and action. AUTOGUIDE presents relevant guidelines at necessary moments, enabling accurate task completion by avoiding the mistakes seen in ExpeL and ReAct. ", "page_idx": 7}, {"type": "text", "text": "2. Importance of providing pertinent knowledge. ExpeL approach helps ReAct by extracting knowledge from offilne experiences, but its impact is not as significant as AUTOGUIDE. Recall that for ExpeL, the guidelines are neither generated for specific contexts at training time nor selected to only provide context-aware guidelines at test time. As a result, irrelevant guidelines can be introduced to an agent, potentially causing confusion for the agent. Consequently, the result highlights the significance of providing relevant guidelines conditioned on contexts for LLM agents. ", "page_idx": 7}, {"type": "text", "text": "3. Scalability to complex environments. We conduct experiments on WebArena-Reddit, which features more diverse tasks on realistic and complex websites requiring longer action sequences. This domain has a larger observation space and a more complex action space (e.g., scrolling). Table 1 presents the results, where AUTOGUIDE achieves the highest success rate with a significant margin when compared to ReAct and ExpeL. We observe that ReAct scores low task success rate $(8.0\\%)$ in WebArena due to the complex observation and action spaces and longer task horizon. In ExpeL, the issue of presenting all guidelines to an agent is exacerbated in the WebArena compared to simpler environments like ALFWorld and WebShop. WebArena\u2019s wide variety of tasks across different domains requires a larger number of guidelines to cover the knowledge needed for all tasks and domains. This results in either an overload of irrelevant guidelines that could mislead the agent or a lack of crucial information when the number of guidelines is limited, as suggested in ExpeL [31]. In contrast, AUTOGUIDE achieves a more significant performance enhancement $(47.1\\%)$ compared to ExpeL $(21.8\\%)$ by efficiently providing pertinent guidelines and minimizing the burden on context capacity. We refer to Figure 14 for a list of example contexts and guidelines. ", "page_idx": 7}, {"type": "text", "text": "Q2. How does AUTOGUIDE perform when combined with test-time self-feedback approaches? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our context-aware guidelines effectively provide inter-task knowledge by considering multiple tasks in offline data. Meanwhile, self-feedback methods (e.g., Reflexion) offer intra-task knowledge based on environmental feedback during test time. In this question, we explore the effectiveness of integrating both inter-task and intra-task information. The results presented in Table 1 demonstrate that the combination of AUTOGUIDE with Reflexion achieves the highest performance in the WebShop and ALFWorld benchmarks. Hence, we find that our context-aware guidelines positively complement the intra-task knowledge of Reflexion. Another observation from Table 1 is that, while ExpeL $^+$ Reflexion outperforms ExpeL alone, this combination is not as effective as other approaches. This limitation may stem from ExpeL introducing irrelevant knowledge, potentially leading to conflicts with Reflexion\u2019s feedback and having an adverse impact on the decision-making process. ", "page_idx": 7}, {"type": "text", "text": "Q3. Can AUTOGUIDE generate context-aware guidelines for multi-modal inputs? ", "page_idx": 7}, {"type": "text", "text": "Going beyond text-only inputs is an essential step toward building capable agents for solving realworld environments and tasks. We test AUTOGUIDE in a complex multi-modal setting, where each observation includes image and text information. Specifically, we introduce a set of real-world website navigation tasks in 3 domains: GitHub, Google Flights, and Coursera. For these multi-modal tasks, we employ the Set-of-Marks (SoM) agent [36, 5] as our base method. The SoM prompting improves the visual grounding capabilities of large multi-modal models such as GPT-4V by adding visually distinguishable marks to image inputs [36]. We apply AUTOGUIDE with GPT-4V to generate natural language context-aware guidelines from collected trajectories with both image and text observations. ", "page_idx": 7}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/f44c2472d80b07ed1149eb4d5f86618aef72666bf58d16fa32c1b04d40758703.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/5b252caac848c20b86b810e37f94bd28a880fd5524cc6ce505bc410d676fcaf5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Out-of-distribution generalization of context-aware guidelines from WebShop on the 98 WebArena\u2013Shopping tasks that have a product in the intent template. ", "page_idx": 8}, {"type": "text", "text": "Table 6: Ablation study of AUTOGUIDE, analyzing each module\u2019s contribution in WebShop. CI denotes our context identification module, and GES denotes the guideline extraction and selection modules. ", "page_idx": 8}, {"type": "text", "text": "Table 2 shows the effectiveness of AUTOGUIDE, demonstrating its generalization ability to complex real-world multi-modal settings. We refer to Figure 15 for example context-aware guidelines. ", "page_idx": 8}, {"type": "text", "text": "4.3 Analyses of AUTOGUIDE ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Q4. How does AUTOGUIDE compare to ReAct with varying numbers of in-context examples? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 3 shows that, while increasing the number of in-context examples for ReAct gradually improves performance, there is a plateau at a certain number of shots. Additionally, ReAct with more than 6 shots often exceeds the token limit of GPT-3.5-turbo. These results indicate that directly inputting raw trajectories into ReAct for in-context learning is not an effective way to fully leverage offline data. In contrast, AUTOGUIDE extracts knowledge from entire training trajectories by summarizing them into concise context-aware guidelines, making them easy to integrate with prompt-based agents. ", "page_idx": 8}, {"type": "text", "text": "Q5. How does altering the number of top- $k$ guidelines impact the performance of AUTOGUIDE? ", "page_idx": 8}, {"type": "text", "text": "We conducted an ablation study on WebShop using various values of $k$ in Table 4. We find that employing context-aware guidelines consistently outperforms the no-guideline baseline ( $k=0$ ; ReAct). The $k=3$ yields the best performance. The largest $k$ value of 5 can lead an LLM agent to overthink, potentially resulting in a slight decrease in performance. Conversely, a smaller $k$ , like $k\\!=\\!1$ , may cause LLM to overlook additional helpful guidelines, leading to slightly worse performance. ", "page_idx": 8}, {"type": "text", "text": "Q6. How do AUTOGUIDE\u2019s context-aware guidelines generalize to out-of-domain environments? ", "page_idx": 8}, {"type": "text", "text": "We conduct an experiment to further demonstrate AUTOGUIDE\u2019s out-of-domain capability across different domains but relevant tasks. We extract context-aware guidelines from WebShop and apply them to WebArena-Shopping, which is a distinct domain with variations in observation/action spaces, task intentions, and episodic horizons. For this domain adaptation case, we additionally incorporate a grounding module to align the context-aware guidelines from WebShop to WebArena\u2019s observations based on GPT-4-Turbo. As shown in Table 5, the transferred guidelines bring a notable improvement in success rates compared to the ReAct baseline in WebArena Shopping. ", "page_idx": 8}, {"type": "text", "text": "Q7. How does each component of AUTOGUIDE contribute to the final results? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the impact of different components within AUTOGUIDE on its performance in WebShop, as detailed in Table 6. We examine two variants: $\\mathrm{ReAct+CI}$ and $\\mathrm{ReAct+GES}$ . The ReAct+CI, which incorporates contexts into observations without guidelines, shows improvement over ReAct. This suggests that contexts enhance decision-making by verifying the current state before action selection. ReAct+GES, which generates guidelines from trajectories without contexts and employs GPT-3.5-turbo for guideline selection, also enhances performance but is less effective than the full AUTOGUIDE. This indicates that choosing relevant guidelines based on the trajectory alone is more challenging than using contexts. Therefore, integrating both context summaries and guidelines is crucial for maximizing the benefits of AUTOGUIDE. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present AUTOGUIDE, an effective framework for exploiting important domain knowledge from offilne experiences for improving decision-making with pre-trained LLMs. We proposed to generate context-aware guidelines that can be incorporated into prompts for LLM agents. As AUTOGUIDE extracts the guidelines by contrasting trajectories in offilne data, the resulting context-aware guidelines carry critical information for preventing failures in the domains. For inference, it provides the guidelines pertinent to each of the different context that LLM agents encounter, which can make pre-trained LLMs strong decision-making agents in the downstream domains. Empirically, we showed that AUTOGUIDE outperforms strong baselines by a large margin and achieves outstanding performance in decision-making benchmarks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by LG AI Research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.   \n[2] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.   \n[3] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, pages 287\u2013318. PMLR, 2023.   \n[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[5] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.   \n[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.   \n[7] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding HTML with large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2803\u20132821, Singapore, December 2023. Association for Computational Linguistics.   \n[8] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.   \n[9] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In ACL, 2022.   \n[10] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[11] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022.   \n[12] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models, 2023.   \n[13] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.   \n[14] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.   \n[15] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118\u20139147. PMLR, 2022.   \n[16] Lajanugen Logeswaran, Yao Fu, Moontae Lee, and Honglak Lee. Few-shot subgoal planning with language models. In NAACL: HLT, 2022.   \n[17] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764\u201310799. PMLR, 2023.   \n[18] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master $16000+$ real-world apis. arXiv preprint arXiv:2307.16789, 2023.   \n[19] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.   \n[20] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.   \n[21] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\\`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.   \n[22] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. arXiv preprint arXiv:2305.16653, 2023.   \n[23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.   \n[24] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u02c6ot\u00b4e, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In ICLR, 2021.   \n[25] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.   \n[26] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.   \n[27] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[28] SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a monte-carlo framework. Journal of Artificial Intelligence Research, 43:661\u2013704, 2012.   \n[29] Austin W Hanjie, Victor Y Zhong, and Karthik Narasimhan. Grounding language to entities and dynamics for generalization in reinforcement learning. In International Conference on Machine Learning, pages 4051\u20134062. PMLR, 2021.   \n[30] Victor Zhong, Tim Rockta\u00a8schel, and Edward Grefenstette. Rtfm: Generalising to new environment dynamics via reading. In ICLR, pages 1\u201317. ICLR, 2020.   \n[31] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023.   \n[32] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.   \n[33] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757, 2022.   \n[34] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.   \n[35] Google Flights. https://www.google.com/travel/flights. Accessed: 2024-05-21.   \n[36] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Limitation and Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Limitation. The performance of AUTOGUIDE depends on the diversity of offline experiences. As such, one important direction for improvement is to automatically collect diverse offilne experiences through continual learning, where we iteratively generate guidelines and use them to gather more trajectories with high rewards. Another avenue is the need for quantifying the quality of generated contexts and guidelines. Currently, apart from applying context-aware guidelines to ReAct and measuring test time performance, there lacks a standardized method for quantifying the quality of generated contexts and selected guidelines. Introducing a quantifiable metric to approximate the quality could pave the way for new optimization approaches such as reinforcement learning. ", "page_idx": 12}, {"type": "text", "text": "Broader impact. This paper introduces research aimed at enhancing the decision-making capabilities of LLMs. In terms of societal impact, while we develop a generic LLM-based autonomous agent, having biased offilne datasets may lead to making decisions with suboptimal outcomes. Additionally, autonomous agents may be misused for malicious applications. To mitigate these risks, potential solutions would include diversifying datasets, implementing ethical oversight, ensuring transparency and accountability, engaging with stakeholders for a broader perspective, and incorporating security measures to prevent misuse. We believe that the research community, including ourselves, should responsibly advance LLM-based agent research, prioritizing societal well-being and ethical considerations. ", "page_idx": 12}, {"type": "text", "text": "B Evaluation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 ALFWorld [24] ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1.1 Environment details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Each task in ALFWorld starts with a description of the specific environment and the goal to achieve. At each timestep, an agent can choose one of the following actions to interact with the objects and receptacles in the environment: ", "page_idx": 12}, {"type": "text", "text": "\u2022 go to [recep] \u2022 take [object] from [recep] \u2022 put [object] in/on [recep] \u2022 open/close/use [recep] \u2022 clean/heat/cool [object] with [recep] ", "page_idx": 12}, {"type": "text", "text": "Alternatively, the agent can generate a think action for planning and reflection, which helps with decision-making but does not change the environment itself. After one action is performed, the environment returns an observation that describes view changes. ", "page_idx": 12}, {"type": "text", "text": "Following ReAct, we concatenate a list of (observation, action) pairs to show the entire trajectory up to the current timestep for LLM agents to generate the next action. We experiment on 134 unseen test tasks with 6 categories of pick and place, pick clean then place, pick heat then place, pick cool then place, look at obj, and pick two obj. For each task, the agent is allowed to take a maximum of 50 actions. ", "page_idx": 12}, {"type": "text", "text": "B.1.2 Baseline and Models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For ALFWorld tasks, we follow the same setting as ReAct by providing 2 in-context examples for each of the 6 task categories. The original results of ReAct in their paper are produced based on Text-Davinci-002. However, this GPT version is no longer available, so we apply gpt-3.5-turboinstruct instead to generate actions. For ExpeL, we directly take the guidelines from their appendix and append them to the ReAct agent at test time. ", "page_idx": 12}, {"type": "text", "text": "B.1.3 Implementation details ofAUTOGUIDE ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We run the first 100 training tasks of ALFWorld to collect $(\\tau_{+},\\tau_{-})$ pairs with ReAct+Reflexion and extract context-dependant guidelines on the collected data. For context identification, we provide ", "page_idx": 12}, {"type": "text", "text": "2-shot demonstrations for each of the 6 task categories. The corresponding prompt templates can be found in appendix C. All parameter details are shown in table 7. ", "page_idx": 13}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/16039961feca5bd1923cb5518f042faf9a58754df750c191bad77ca64f162293.jpg", "table_caption": ["Table 7: Experiment hyperparameters on ALFWorld. The maximum allowed episode length and n-shots follow the same setup in ReAct. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B.2 WebShop [33] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.2.1 Environment details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "WebShop provides an e-commerce environment, where the objective is to find and buy the product that matches the task-specific Instruction. The agent can select one of the following actions to perform: ", "page_idx": 13}, {"type": "text", "text": "\u2022 search[query] \u2022 click[button] ", "page_idx": 13}, {"type": "text", "text": "Following ReAct, the agent can generate think actions to do planning or reflection. After buying a product, the environment returns a reward showing how well the bought product matches the target one in type, price, buying options, and attributes. The reward is calculated by: ", "page_idx": 13}, {"type": "equation", "text": "$$\nr\\!=\\!r_{t y p e}\\cdot\\frac{|U_{a t t}\\cap Y_{a t t}|\\!+\\!|U_{o p t}\\cap Y_{o p t}|\\!+\\!1\\big[y_{p r i c e}\\leq u_{p r i c e}\\big]}{|U_{a t t}|\\!+\\!|U_{o p t}|\\!+\\!1}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where y is the bought product and $\\mathbf{u}$ is the target product. Same as ALFWorld, for WebShop, the agent takes $(\\mathrm{obs}_{t},\\mathrm{act}_{t})$ ) pairs for every previous timestep $t$ as input to generate the next action. ", "page_idx": 13}, {"type": "text", "text": "B.2.2 Baseline and Models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Following ReAct, experiments are done in a one-shot setting. We apply gpt-3.5-turbo-0613 to generate actions, but when the token number exceeds the token limit (for example, for the n-shot ReAct experiments in table 1), we use the $16\\mathrm{k}$ version of gpt-3.5-turbo-0613 instead. For ExpeL, we could not find how many training tasks the framework used for training. Therefore, we directly apply the guidelines from the appendix of their paper at test time. We only consider ExpeL with guidelines, not ExpeL with in-context example selection in our experiments for a fair comparison. The in-context example selection method is orthogonal to our work and can be easily combined with our method. For Reflexion, as shown in their paper, their 2-shot Reflexion prompt does not work well on WebShop. Therefore, we re-write the prompt and apply gpt-4-1106-preview to generate episode-level reflections for all Reflexion experiments. Following Reflexion and ExpeL, the evaluation is done on 100 test tasks. The maximum number of allowed actions for each task is 15. At the same time, each search action shows the top 3 products for the search query. Please refer to Table 8 for more details about the experiments. ", "page_idx": 13}, {"type": "text", "text": "B.2.3 Implementation details of AUTOGUIDE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We randomly sample 50 training tasks from the training set of WebShop, on which we run ReAct+Reflexion to collect pairs and generate guidelines. The context identification prompt is one-shot, which is shown in Appendix C. At test time, we ask gpt-3.5-turbo-0613 to select each state\u2019s most relevant top 2 guidelines. ", "page_idx": 13}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/2a85778522df0c3622ca2eec501bdbb9615b034f2a7b9a0157640b43adac3047.jpg", "table_caption": [], "table_footnote": ["Table 8: Experiment hyperparameters on WebShop. The maximum allowed episode length, the number of search results per page, and $\\mathbf{n}$ -shots follow the same setup in ReAct. "], "page_idx": 14}, {"type": "text", "text": "B.3 WebArena [8] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.3.1 Environment details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "WebArena provides web-based benchmark environments that closely follow the data and functionality of real-world websites. Unlike other benchmarks like WebShop that provide clean text of website information as observation, WebArena\u2019s webpage content is represented as an accessibility tree, which is a subset of the DOM tree with useful elements of a webpage. For our expeirments, we focus on WebArena Reddit, which simulates the real Reddit websites with users, forums, and posts with abundant text information. ", "page_idx": 14}, {"type": "text", "text": "For each task in WebArena, the agent is expected to achieve a task-specific intent. At each timestep, WebArena provides a list of opened tabs, the accessibility tree of the focused webpage, and the URL of the current page as observation. For WebArena, each observation is long. Therefore, following the baseline in WebArena, at each timestep, we only provide the observation of the current timestep to the agent. We additionally provide up to 5 past actions for the agent to understand what it did in the past. The allowed actions in WebArena include the following: ", "page_idx": 14}, {"type": "text", "text": "\u2022 goto [url]   \n\u2022 click [element id]   \n\u2022 type [element id] [text] [1 for enter or 0 for not enter]   \n\u2022 press [key combination]   \n\u2022 scroll [up or down]   \n\u2022 go back ", "page_idx": 14}, {"type": "text", "text": "The maximum allowed number of actions for a single task is 20. Note that WebArena does not provide training tasks, but the work provides 19 demonstrations for Reddit, each of a different category. Therefore, we set these 19 tasks as the training tasks and then test on the rest 87 tasks. ", "page_idx": 14}, {"type": "text", "text": "B.3.2 Baseline and Models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We directly run the two-shot ReAct-style baseline in the official codebase of WebArena using gpt4- preview-1106. For ExpeL, the original paper does not include experiments on WebArena, therefore we try our best to implement our own version and run on the same training tasks as our method. ", "page_idx": 14}, {"type": "text", "text": "B.3.3 Implementation details of AUTOGUIDE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We directly run ReAct on the tasks with $\\tau_{+}$ to collect $\\tau_{-}$ actions and generate guidelines correspondingly. We provide a 5-shot prompt for the context identification module, which is shown in Figure 7. At test time, the top 2 guidelines at each timestep are selected to guide action generation. ", "page_idx": 14}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/622d4be5fadee8155aad327c6bfbf55d3f16a965c7b4e2b4881fa7e8cf3f179b.jpg", "table_caption": [], "table_footnote": ["Table 9: Experiment hyperparameters on WebArena. The number of shots follows the same setup in ReAct. "], "page_idx": 15}, {"type": "text", "text": "B.4 Real Websites ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.4.1 Environment details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We design a set of real-world website navigation tasks from 3 domains: Software Development (GitHub), Travel (Google Flights), and Education (Coursera), which have 30, 20, and 20 test tasks accordingly. Here are some example tasks: ", "page_idx": 15}, {"type": "text", "text": "\u2022 GitHub \u2013 Navigate to the repository for the Python library Seaborn with the most stars and show me all the open issues labeled with bug. \u2013 Go to the GitHub org for Spotify and open the pinned project with the most stars for me.   \n\u2022 Google Flights \u2013 Find me the one-way filght from Hong Kong to Paris departing on Oct 15th 2024 with the least emmisions. \u2013 Show me the booking options of the one-way filght departing from Auckland on September 11, 2024, and arriving in Rome with the earliest departure time on that day.   \n\u2022 Coursera \u2013 Show me a Cybersecurity course that can finish within 1 month and show me all the reviews for the selected course. \u2013 Find me a Coursera guided project that covers Unity and show me its main page. ", "page_idx": 15}, {"type": "text", "text": "We follow the action space design of Visual WebArena [5], which has the following action types available: ", "page_idx": 15}, {"type": "text", "text": "\u2022 goto [url]   \n\u2022 click [element id]   \n\u2022 hover [element id]   \n\u2022 type [element id] [text] [1 for enter or 0 for not enter]   \n\u2022 press [key combination]   \n\u2022 scroll [up or down]   \n\u2022 tab focus [tab index]   \n\u2022 close tab   \n\u2022 go back   \n\u2022 go forward ", "page_idx": 15}, {"type": "text", "text": "As the real websites are constantly and dynamically changing, we evaluate the completed task with human experts. ", "page_idx": 15}, {"type": "text", "text": "B.4.2 Baseline and Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We directly run the two-shot SoM algorithm in the official codebase of Visual WebArena. The only modification we made from the original codebase is the bounding box detection algorithm, in ", "page_idx": 15}, {"type": "text", "text": "which we further fliter invisible bounding boxes from the list and add the list elements as interactable elements to consider. ", "page_idx": 16}, {"type": "text", "text": "B.4.3 Implementation details of AUTOGUIDE ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide a total of 6 training tasks (3 for GitHub, 2 for Google Travel, and 1 for Coursera), on which we collect human demonstration as $\\tau_{+}$ \u2019s and run SoM to collect $\\tau_{-}$ \u2019s. From the pairs with multi-modal observations we generate text-based guidelines to guide action selection. Both context identification and guideline extraction are done by gpt-4-vision-preview, and we provide a 3-shot prompt for context identification. All parameter details are shown in Table 10. ", "page_idx": 16}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/a8ab3fffd30524d3cf29323da483a554225beda4ff9be04236a0ec5a1bc450b4.jpg", "table_caption": ["Table 10: Experiment hyperparameters on multi-modal real-world website tasks. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Prompt Templates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Context Identification ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present our prompt templates for the context identification module $\\mathcal{M}_{\\mathrm{context}}$ (Equation (1)) for WebShop, ALFWorld, and WebArena in Figures 5 to 7, respectively. In ALFWorld, there exist six categories of tasks, and we use context identification prompting with 2-shot examples for each task, following the practice by [23]. Figure 6 shows one example for the pick and place tasks. ", "page_idx": 16}, {"type": "text", "text": "C.2 Guideline Extraction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figures 8 to 10 detail our prompt templates ${\\mathcal{M}}_{\\mathrm{guideline}}$ for extracting guidelines (Equation (2)) in the WebShop, ALFWorld, and WebArena domains. ", "page_idx": 16}, {"type": "text", "text": "C.3 Context Matching ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 11 shows our prompt template for matching the generated context with one of the existing contexts if there is any similar context, for the construction of the set of context-aware guidelines (Section 3.2) and the retrieval of relevant guidelines during testing (Section 3.3) in all the three domains: WebShop, ALFWorld, and WebArena. ", "page_idx": 16}, {"type": "text", "text": "C.4 Guideline Selection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For selecting only $k$ most relevant guidelines in case there are more corresponding context-aware guidelines during testing (Equation (3)), we use the prompt with Figure 12 for WebShop and ALFWorld and Figure 13 for WebArena. ", "page_idx": 16}, {"type": "text", "text": "D Example Context-Aware Guidelines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figures 14 and 15, we show a list of possible contexts and context-aware guidelines on WebArena and real-world websites, respectively. ", "page_idx": 16}, {"type": "text", "text": "Context Identification Prompt for WebShop ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/ab41d7f45b162aea623c95d16bf4fbae068b270c8983ef4d9d3fedf326280ced.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 5: Our prompt template for context identification (Equation (1)) in the WebShop domain. ", "page_idx": 17}, {"type": "text", "text": "Context Identification Prompt for ALFWorld ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "mRIQz8Zd6O/tmp/0117d72345b9aed5a2841dd828e2f9267d19b89c4df078561ecffa0f4144ae05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: Our prompt template for context identification (Equation (1)) in the ALFWorld domain. ", "page_idx": 18}, {"type": "text", "text": "Context Identification Prompt for WebArena ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/e8752c1659707a0b0f56f3b2112a85f324844eea9667eb0e9b63121eae5dfc67.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: Our prompt template for context identification (Equation (1)) in the WebArena domain. ", "page_idx": 19}, {"type": "text", "text": "Guideline Extraction Prompt for WebShop ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Instruction {Task description}. You will be provided with a desired and undesired trajectory of the same task. What is the first action that differs between the two trajectories? Why do you think it makes one trajectory failed and the other successful? Based on your answer, generate an action guideline to make future task avoid the same mistake. The guideline should specify what to do in what situation in the format of \"When in what status, you should (or should not)...\". On a product's page with product information, strictly refer to the option buttons as 'buying options such as sizes, colors, scents, and flavors', and clearly say that buying options are not subpages like [Description] and [Attributes] when you mention buying options. Your guideline must be general enough for any task, therefore never include any task-specific information, instead, refer to all the requirements as the requierments in Instruction. Strictly follow what the desired trajectory does and never suggest actions that the desired trajectory didn't do. When referring to actions, use the allowed action format. You should make your answer concise, limit your answer within 256 tokens, and put your answer in this format: 'Reasoning: ... Guideline: ...'. Input Desired Trajectory: {Desired trajectory} Undesired Trajectory: {Undesired trajectory} ", "page_idx": 19}, {"type": "text", "text": "Figure 8: Our prompt template for guideline extraction (Equation (2)) in the WebShop domain. ", "page_idx": 19}, {"type": "text", "text": "Guideline Extraction Prompt for ALFWorld ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Instruction {Task description}. You will be provided with a desired and undesired trajectory of the same task. What is the first action that differs between the two trajectories? Why do you think it makes one trajectory failed and the other successful? Based on your answer, generate an action guideline to make future task avoid the same mistake. The guideline should specify what to do in what situation in the format of \"When in what status, (optional: if you want to ...) you should (or should not)... (optional: a short example for demonstration. )\". For the 'When in what status' part, directly use the words in SUMMARIZATION. Here are two examples: Example 1: When looking for an object, if you want to find a kitchen-related object like a spatula, you should start from the most possible locations. Example 2: When looking for an object and found the desired object at the location, You should only take the exact object that you want. Strictly follow what the desired trajectory does and never suggest actions that the desired trajectory didn't do. When referring to actions, use the allowed action format. You should make your answer concise, limit your answer within 128 tokens, and put your answer in this format: 'Reasoning: ... Guideline: ...'. Input Desired Trajectory: {Desired trajectory} Undesired Trajectory: {Undesired trajectory} ", "page_idx": 20}, {"type": "text", "text": "Figure 9: Our prompt template for guideline extraction (Equation (2)) in the ALFWorld domain. ", "page_idx": 20}, {"type": "text", "text": "Guideline Extraction Prompt for WebArena ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/ff4fca167892b36dbcbf3a32462992091843ddf2544a53b53717b2acabdbead5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 10: Our prompt template for guideline extraction (Equation (2)) in the WebArena domain. ", "page_idx": 20}, {"type": "text", "text": "Context Matching Prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Instruction {Task description} A task trajectory can be long. Therefore the assistant summarizes the status of each step. For different task with the same status, the summarization should be the same, therefore please ignore any information about instructions or products. You will be provided with the following: 1. A list of summarizations the assistant saw in the past. 2. A newly generated summarization. Please determine if any summarization from the list matches the exact same status as the newly generated one. If yes, answer the index of the corresponding summarization, for example \u201cAnswer: $2\".$ ; otherwise, \u201cAnswer: None\u201d. Input Seen Summarizations: {List of contexts} ", "page_idx": 21}, {"type": "text", "text": "Figure 11: Our prompt template for context matching (Sections 3.2 and 3.3) in all the WebShop, ALFWorld, and WebArena domains. ", "page_idx": 21}, {"type": "text", "text": "Guideline Selection Prompt for WebShop and ALFWorld ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/100491484c7280115462c2b53d4c61537ceaf60b798b1df3441ef2d17b19d9a7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 12: Our prompt template for selecting the most relevant context-aware guidelines during the test time (Equation (3) from Section 3.3) in the WebShop and ALFWorld domains. ", "page_idx": 21}, {"type": "text", "text": "Guideline Selection Prompt for WebArena ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "mRIQz8Zd6O/tmp/21c71f05dfc6e0eb9049dcba64a04fdebe002bd4f8831f9fab7e82111bb6e132.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 13: Our prompt template for selecting the most relevant context-aware guidelines during the test time (Equation (3)) in the WebArena domain. ", "page_idx": 21}, {"type": "text", "text": "Context: On the Reddit main page. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 When on the Reddit main page, if you want to change your bio, you should click on the \"link 'Profile'\", which is located in the user menu dropdown, right above the \"link 'My account'\". The correct action format to do this is \\`\\`\\`click [profile_link_id]\\`\\`\\`.   \n\u2022 When on the Reddit main page, if you want to navigate to a specific forum, you should click on the \"link 'Forums'\", which is located at the early part of the observation, right above the link 'Wiki'. The correct action format to do this is \\`\\`\\`click [link_id]\\`\\`\\`.   \n\u2022 When on the Reddit main page, if you want to create a new forum, you should click on the \"link 'Forums'\", which is located near the top of the observation, right above the \"link 'Wiki'\". The correct action format to do this is \\`\\`\\`click [link_id]\\`\\`\\`.   \n\u2022 When on the Reddit main page, if you want to like all submissions created by a specific user in a specific subreddit, you can directly navigate to the user's page with the action format \\`\\`\\`goto [url]\\`\\`\\`, replacing [url] with the user's page URL, formatted as http://<platform_domain>/user/<username>. ", "page_idx": 22}, {"type": "text", "text": "Context: On the overview page of a Reddit user. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the overview page of a Reddit user, if you want to interact with submissions from a specific subreddit, you should first navigate to the 'Submissions' tab to filter the content by the user's submissions. The correct action format to do this is \\`\\`\\`click [link_id]\\`\\`\\`, where [link_id] is the ID of the 'Submissions' link in the main content area of the page. ", "page_idx": 22}, {"type": "text", "text": "Context: On the biography edit page of a Reddit user. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the biography edit page of a Reddit user, if you want to change the biography text, you should do these sequentially: 1. Click on the textbox 'Biography' to focus it, located in the main section of the page, with action \\`\\`\\`click [textbox_id]\\`\\`\\`. 2. Select all text inside the textbox using the action \\`\\`\\`press [Meta+a]\\`\\`\\`. 3. Clear the selected text with the action \\`\\`\\`press [Backspace]\\`\\`\\`. 4. Type the new biography content into the textbox 'Biography' with action \\`\\`\\`type [textbox_id] [new_content] [1]\\`\\`\\`. 5. Click on the button 'Save', located below the textbox 'Biography', to submit the changes with action \\`\\`\\`click [button_id]\\`\\`\\`. After these steps, issue a stop action when the task is complete. ", "page_idx": 22}, {"type": "text", "text": "Context: On the page listing all forums on a Reddit-like platform. ", "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the page listing all forums on a Reddit-like platform, if you want to navigate to a specific forum, you should do these sequentially: 1. click on the \"link 'Alphabetical'\", which is located in the main area, to sort forums alphabetically. The correct action format to do this is \\`\\`\\`click [link_id]\\`\\`\\`. 2. After the forums are sorted, click on the specific \"link '<forum_name>'\" that you wish to navigate to. The correct action format to do this is \\`\\`\\`click [link_id]\\`\\`\\`. ", "page_idx": 22}, {"type": "text", "text": "Context: On the Reddit page to create submission. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the Reddit page to create submission, if you have filled in the 'Title' and 'Body' textboxes but do not see the button 'Create submission', you should scroll down to reveal more of the page. The correct action format to do this is \\`\\`\\`scroll [down]\\`\\`\\`. After scrolling, if you want to submit the post, you should click on the button 'Create submission', which is located after the 'Body' textbox. The correct action format to submit the post is \\`\\`\\`click [button_id]\\`\\`\\`. ", "page_idx": 22}, {"type": "text", "text": "Context: On the main page of a Reddit forum. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 When on the main page of a Reddit forum, if you want to find posts related to a specific topic among the top posts, you should first sort the posts by their popularity to ensure you are viewing the most relevant content. To do this, you can: 1. click on the \"button 'Sort by: Hot'\", which is located in the main section of the page, below the forum heading and above the first article. The correct action format to do this is \\`\\`\\`click [id]\\`\\`\\`. 2. After the sorting options have expanded, click on the \"link 'Top'\", which will appear as a new option under the sorting button. This action should be repeated once. The correct action format to do this is \\`\\`\\`click [id]\\`\\`\\`.   \n\u2022 When on the main page of a Reddit forum, if you want to create a new post, you should click on the \"link 'Submit'\", which is located in the early part of the observation, right below the \"link 'Notifications $(0)\"$ . The correct action format to do this is \\`\\`\\`click [link_id]\\`\\`\\`. ", "page_idx": 22}, {"type": "text", "text": "Context: On the submissions page of a Reddit user. ", "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the submissions page of a Reddit user, if you want to perform an action on specific subreddit submissions but do not see them, you should scroll down to reveal more submissions. The correct action format to do this is \\`\\`\\`scroll [down]\\`\\`\\`. After scrolling, if you find a submission from the desired subreddit, such as 'UpliftingNews', and need to downvote it, click on the button 'Downvote' located at the end of the submission's details. The correct action format to downvote is \\`\\`\\`click [downvote_button_id]\\`\\`\\`. Once all required actions are performed on the submissions, issue a stop action with the format \\`\\`\\`stop []\\`\\`\\`. ", "page_idx": 22}, {"type": "text", "text": "Context: On the submissions page of a Reddit forum sorted by top. ", "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the submissions page of a Reddit forum sorted by top, if you want to review posts but see \"There's nothing here\u2026\", you should expand the time range to view more posts. Do this by clicking on the button 'From: Past 24 hours', which is located in the main section of the page, below the heading '/f/books' and above the StaticText \"There's nothing here\u2026\". The correct action format to do this is \\`\\`\\`click [button_id]\\`\\`\\`. After expanding the time range, if you find a post that meets the task criteria, issue a stop action. ", "page_idx": 22}, {"type": "text", "text": "Context: On the submissions page of a Reddit forum sorted by new. ", "page_idx": 22}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 When on the submissions page of a Reddit forum sorted by new, if you want to upvote the newest post and you have already clicked the upvote button for the first article entry, you should issue the stop action. The correct action format to do this is \\`\\`\\`stop\\`\\`\\`. ", "page_idx": 22}, {"type": "text", "text": "Context: On the page of a Reddit post. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Context -Aware Guideline: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When on the page of a Reddit post, if you have already navigated to the 'Submit' link, filled in the image URL and title, and clicked the 'Create submission' button, you should consider the task complete. The correct action format to do this is \\`\\`\\`stop\\`\\`\\`. ", "page_idx": 22}, {"type": "text", "text": "Context: On the main page of GitHub. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "When on the main page of GitHub, if you want to search for a repository, you should type the search query into the search bar at the top of the page, which is visually identifiable and typically labeled with text like 'Search or jump to...'. Do not type into any other input fields. Perform the action as follows: \\`\\`\\`type [search bar id] [search query] [1]\\`\\`\\`. ", "page_idx": 23}, {"type": "text", "text": "Context: On the issues page of a GitHub repository. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "On the issues page of a GitHub repository, if you want to filter issues by a specific label, you can type the label filter in the search input field, which is usually at the top of the current webpage and shown as the first appeared \"[INPUT] []\" in observation. The correct action format to do this is \\`\\`\\`type [id] [label:\"specific_label\"] [1]\\`\\`\\`. After applying the filter: 1. Click on the link that shows the number of closed issues, which is labeled as \"[A] [num Closed]\" in observation, located near the search input field. The correct action format is \\`\\`\\`click [link_id]\\`\\`\\`. ", "page_idx": 23}, {"type": "text", "text": "Context: On the search result page of GitHub. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "On the search result page of GitHub, if you want to filter the search results to find a specific organization, after you have typed the organization's name in the search bar, you can do these sequentially: 1. click on the \"[LI] [Users]\" in the left sidebar, which is visually represented by the blue text \"Users\" and is the first \"[LI] [Users]\" in observation, by action \\`\\`\\`click [li_id]\\`\\`\\` 2. if the user or organization filter is applied, click on the organization's name, which is visually represented by the blue text \"Meta\" under the \"Users\" section, by action \\`\\`\\`click [link_id]\\`\\`\\` ", "page_idx": 23}, {"type": "text", "text": "Context: On the search result page of Coursera. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "On the search result page of Coursera, if you want to select a course, after you have typed the course topic in the search bar, you can do these sequentially: 1. click on the filter for courses, which is represented by \"[INPUT] []\" and visually located in the filter section on the left side of the webpage, by action \\`\\`\\`click [input_id]\\`\\`\\` 2. click on the course link, which is represented by \"[A] [course_name]\" and visually located in the main content area of the webpage, by action \\`\\`\\`click [link_id]\\`\\`\\`. This action should be repeated twice. 3. switch to the new tab that contains the course details by \\`\\`\\`page_focus [1]\\`\\`\\`. ", "page_idx": 23}, {"type": "text", "text": "Context: On the main search page of Google Flights. ", "page_idx": 23}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "On the main search page of Google Flights, if you want to search for a one-way flight with specific departure and destination airports and date, you can do these sequentially: 1. type the departure airport code in \"[INPUT] [Where from?]\", which is the first input box at the top of the search area, by action \\`\\`\\`type [element_id] [airport_code] [1]\\`\\`\\` 2. type the destination airport code in \"[INPUT] [Where to?]\", which is next to \"[INPUT] [Where from?]\", by action \\`\\`\\`type [element_id] [destination_airport_code] [1]\\`\\`\\` 3. click on \"[DIV] [Round trip]\" to change the trip type, located at the top of the search area, by action \\`\\`\\`click [element_id]\\`\\`\\` 4. click on \"[LI] [One way]\" to select the oneway trip option, which appears after clicking \"[DIV] [Round trip]\", by action \\`\\`\\`click [element_id]\\`\\`\\` 5. type the departure date in \"[INPUT] [Departure]\", which is next to \"[INPUT] [Where to?]\", by action \\`\\`\\`type [element_id] [date] [1]\\`\\`\\` 6. click on \"[BUTTON] [Done]\" to confirm the date, which appears after entering the departure date, by action \\`\\`\\`click [element_id]\\`\\`\\` 7. click on \"[BUTTON] [Search]\" to perform the flight search, which is below the search fields, by action \\`\\`\\`click [element_id]\\`\\`\\` ", "page_idx": 23}, {"type": "text", "text": "Context: On the search result page of Google Flights with a list of Best departing flights and Other departing flights. ", "page_idx": 23}, {"type": "text", "text": "Context-Aware Guideline: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 On the search result page of Google Flights with a list of Best departing flights and Other departing flights, if you want to select a flight, you can click on the first flight option under the \"Best flights\" section. The correct action format to do this is \\`\\`\\`click [flight_option_id]\\`\\`\\`, where [flight_option_id] is the id of the [LI] element corresponding to the first flight listed. This [LI] element is visually located at the top of the list of flights and contains the departure and arrival times, airline name, flight duration, and other details. \u2022 On the search result page of Google Flights with a list of Best departing flights and Other departing flights, if you want to locate the flight with the least emissions, you can do these sequentially: 1. Click on the sort options button, which is visually located at the top of the flight list and shown as \"[BUTTON] [Sort by:]\" in observation, by action \\`\\`\\`click [sort_button_id]\\`\\`\\`. 2. Then, click on the emissions sort option, which is visually located in the sort options dropdown and shown as \"[LI] [Emissions]\" in observation, by action \\`\\`\\`click [emissions_option_id]\\`\\`\\`. 3. Finally, click on the first flight listed under the \"Best flights\" section, which is visually located at the top of the list and shown as \"[LI] [flight_details]\" in observation, by action \\`\\`\\`click [first_best_flight_id]\\`\\`\\`. Repeat this action twice. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the abstract and introduction, we clearly outline our main contribution: extracting context-aware guidelines from offline data to enhance the decision-making of LLM-based agents. We support the effectiveness of our method through empirical evaluations in Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We outline potential limitations of AUTOGUIDE in Appendix A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include theoretical contributions. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 4.1 and Appendix B provide detailed information (e.g., model versions, hyperparameters, number of training data) for reproducing our experimental results. Additionally, Appendix C contains our prompt templates. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We intend to release the source code upon acceptance. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We detail our experimental settings in Section 4.1 and Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the average rewards and success rates calculated across test tasks in our results, but we do not include the variance. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our experiments are based on GPT, and we detail the versions of GPT in Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work conforms the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the broader impacts of our work in Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper makes the algorithmic contribution and does not release new data or models. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We reference the original papers and sources throughout the paper. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]