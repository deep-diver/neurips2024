[{"figure_path": "mRIQz8Zd6O/tables/tables_3_1.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table presents the performance comparison of different methods (ReAct, ExpeL, and AUTOGUIDE) across three benchmark tasks: ALFWorld, WebShop, and WebArena.  It shows the success rate and reward achieved by each method, both with and without the addition of Reflexion (a self-reflection technique).  The base language model used varies across the tasks (GPT-3.5-turbo for ALFWorld and WebShop; GPT-4-turbo for WebArena).  Due to token limitations, Reflexion was not tested with WebArena.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_6_1.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table presents the performance comparison of different algorithms across three benchmark datasets: ALFWorld, WebShop, and WebArena.  The success rate and reward are reported for each algorithm,  including the baseline methods (ReAct and ExpeL) and their combinations with the Reflexion method. AUTOGUIDE consistently outperforms the baselines in terms of both success rate and reward. Note that the Reflexion method was not used with WebArena due to token limits.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_7_1.jpg", "caption": "Table 2: Test results of AUTOGUIDE on 3 real-world web domains within multi-modal settings. The base agent model runs with GPT-4V and applying context-aware guidelines significantly improves the performance.", "description": "This table presents the results of AUTOGUIDE on three real-world multi-modal web domains: GitHub, Google Flights, and Coursera.  The success rate (SR) is shown for each domain, comparing the performance of AUTOGUIDE against the SoM baseline.  The results demonstrate a significant improvement in task success rates when using AUTOGUIDE's context-aware guidelines with the GPT-4V model.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_7_2.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table presents the results of experiments conducted on three benchmark datasets (ALFWorld, WebShop, and WebArena) comparing the performance of AUTOGUIDE against several baseline methods.  The success rate and reward are shown for each method, with and without the addition of the Reflexion self-feedback method.  The results demonstrate the superior performance of AUTOGUIDE in all three benchmark environments.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_8_1.jpg", "caption": "Table 5: Out-of-distribution generalization of context-aware guidelines from WebShop on the 98 WebArena-Shopping tasks that have a product in the intent template.", "description": "This table presents the results of applying WebShop's context-aware guidelines to the WebArena-Shopping tasks.  It specifically focuses on the 98 tasks that include a product in their description. The success rate (SR) of both ReAct and AUTOGUIDE are compared, showing AUTOGUIDE's improved performance on out-of-distribution generalization.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_8_2.jpg", "caption": "Table 6: Ablation study of AUTOGUIDE, analyzing each module's contribution in WebShop. CI denotes our context identification module, and GES denotes the guideline extraction and selection modules.", "description": "This ablation study analyzes the contribution of each module of AUTOGUIDE to the overall performance on the WebShop benchmark.  It shows the success rate (SR) of ReAct alone, ReAct with only the context identification module (CI), ReAct with only the guideline extraction and selection module (GES), and AUTOGUIDE with both CI and GES.  The results demonstrate the importance of both modules for achieving the highest performance.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_13_1.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table compares the performance of different methods (ReAct, ExpeL, and AUTOGUIDE) across three benchmark tasks: ALFWorld, WebShop, and WebArena.  It shows success rates and reward scores, illustrating the improvement achieved by AUTOGUIDE, especially when combined with Reflexion (a self-feedback method).  Note that the token limits of the language models prevented using Reflexion on the WebArena tasks.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_14_1.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table presents the results of the experiments performed on three different benchmark domains (ALFWorld, WebShop, and WebArena) using four different approaches: ReAct, ExpeL, AUTOGUIDE, and each of these combined with Reflexion.  For each method and dataset, the success rate and reward are presented.  The table highlights the superior performance of AUTOGUIDE, particularly when combined with Reflexion.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_15_1.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table presents the results of experiments conducted on three different benchmark domains: ALFWorld, WebShop, and WebArena.  The performance of several algorithms (ReAct, ExpeL, and AUTOGUIDE) are compared.  The success rate and reward are presented for each algorithm, both with and without the addition of the Reflexion algorithm (which provides self-feedback to enhance learning). The table also notes which algorithms utilize offline data, highlighting AUTOGUIDE's performance gains when leveraging this information.", "section": "4.2 Main Results"}, {"figure_path": "mRIQz8Zd6O/tables/tables_16_1.jpg", "caption": "Table 10: Experiment hyperparameters on multi-modal real-world website tasks.", "description": "This table shows the hyperparameters used in the experiments conducted on real-world websites, which involve multi-modal inputs (image and text).  The parameters specify the maximum allowed number of actions within an episode, the number of examples provided for in-context learning, the large language models used for various tasks (agent action generation, context identification, guideline selection, and guideline extraction), and the number of top guidelines selected at each step.", "section": "B.4 Real Websites"}, {"figure_path": "mRIQz8Zd6O/tables/tables_18_1.jpg", "caption": "Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.", "description": "This table presents the results of experiments on three benchmark tasks: ALFWorld, WebShop, and WebArena.  It compares the performance of several methods, including ReAct, ExpeL, and AUTOGUIDE, with and without the addition of the Reflexion self-reflection technique. Success rate and reward are shown for each method across the three tasks, highlighting the impact of context-aware guidelines and self-reflection on performance. Note that the token limitations of GPT prevented the use of Reflexion on the WebArena task.", "section": "4.2 Main Results"}]