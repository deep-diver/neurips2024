[{"figure_path": "Ejg4d4FVrs/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of Attention Heatmaps. Elliptical pays attention to more relevant information. DeiT focuses on just a subset of informative features while Elliptical considers a wider set of contextually relevant information, helping to produce more accurate and robust predictions. Attention scores are min-max scaled for visualization purposes.", "description": "The figure shows a comparison of attention heatmaps for DeiT and DeiT-Elliptical models on two example images.  DeiT focuses its attention on a small subset of features, while DeiT-Elliptical distributes attention more broadly across contextually relevant features.  The color intensity represents the attention weight, with brighter colors indicating higher weights. This visual comparison illustrates the key advantage of Elliptical Attention: improved robustness and accuracy by considering a wider context.", "section": "1 Introduction"}, {"figure_path": "Ejg4d4FVrs/figures/figures_2_1.jpg", "caption": "Figure 2: Left: The function does not vary in the x2 axis so we stretch the neighborhood in that direction. Right: The stretched ellipsoidal neighborhood includes 4 more keys.", "description": "This figure illustrates the concept of hyper-ellipsoidal neighborhoods in the context of non-parametric regression, a perspective used to explain the self-attention mechanism.  The left panel shows a 2D function that doesn't vary along the x2-axis. A circular neighborhood (representing standard self-attention) around a query point only includes keys close in Euclidean distance, regardless of direction, missing potentially relevant keys. The right panel demonstrates that stretching the neighborhood into an ellipse along the x2-axis (the direction of low variability) incorporates additional relevant keys into the neighborhood, improving the accuracy of the regression estimate.", "section": "3 Elliptical Attention: Leveraging Hyper-Ellipsoids to Pay More Attention Without Losing Focus"}, {"figure_path": "Ejg4d4FVrs/figures/figures_3_1.jpg", "caption": "Figure 3: Representation Collapse on WikiText-103. Elliptical Attention learns more diverse representations.", "description": "This figure shows a comparison of token cosine similarity over layers for baseline and elliptical attention on the WikiText-103 dataset. The baseline shows increasing similarity, indicating representation collapse, while elliptical attention maintains more diverse representations across layers.", "section": "Experimental Results"}, {"figure_path": "Ejg4d4FVrs/figures/figures_9_1.jpg", "caption": "Figure 4: ImageNet Efficiency: Comparison of throughput and max memory allocated for DeiT, Elliptical, RVT, RKDE, MoM on Tiny, Small, and Base sizes. Elliptical is the most efficient robust model.", "description": "This figure compares the efficiency of different models (DeiT, Elliptical, RVT, RKDE, MoM) across various sizes (Tiny, Small, Base) in terms of average computation speed and maximum GPU memory usage.  It demonstrates that Elliptical Attention achieves high robustness while being the most efficient model among those compared.", "section": "5 Related Work"}, {"figure_path": "Ejg4d4FVrs/figures/figures_23_1.jpg", "caption": "Figure 5: Left: Evolution of mean values of key perturbations over successive layers. Right: Mean key perturbations at different layers after 300 epochs. The figures show that as the number of layers increases, mean key perturbations over layers stabilize around a constant value.", "description": "The left plot shows the evolution of mean values of key perturbations over successive layers during the training process on ImageNet dataset. The right plot shows the mean key perturbation at different layers after 300 training epochs. Both plots show that as the number of layers increases, the mean key perturbation values stabilize around a constant value, indicating the model's robustness.", "section": "3.4 An Efficient Estimator of the Coordinate-wise Variability"}, {"figure_path": "Ejg4d4FVrs/figures/figures_28_1.jpg", "caption": "Figure 6: Additional Representation Collapse Results on ADE20K, WikiText-103 and ImageNet. Elliptical reduces token similarity over layers across a range of modalities", "description": "This figure compares the token cosine similarity over layers for both the baseline transformer and the proposed Elliptical Attention model across three different tasks: ADE20K image segmentation, WikiText-103 language modeling, and ImageNet image recognition.  It visually demonstrates that Elliptical Attention effectively reduces representation collapse by decreasing the similarity of token representations as the number of layers increases, thereby enhancing the model's ability to learn diverse and informative features across various data modalities.  Lower cosine similarity indicates more diverse feature representations.", "section": "F.2 Representation Collapse"}, {"figure_path": "Ejg4d4FVrs/figures/figures_28_2.jpg", "caption": "Figure 1: Comparison of Attention Heatmaps. Elliptical pays attention to more relevant information. DeiT focuses on just a subset of informative features while Elliptical considers a wider set of contextually relevant information, helping to produce more accurate and robust predictions. Attention scores are min-max scaled for visualization purposes.", "description": "The figure compares the attention heatmaps of two models, DeiT and DeiT-Elliptical, on an image classification task.  DeiT, a standard transformer-based model, focuses its attention on a small subset of features.  DeiT-Elliptical, employing the proposed Elliptical Attention, distributes its attention more broadly across contextually relevant features. This broader attention results in more accurate and robust predictions by the DeiT-Elliptical model.", "section": "1 Introduction"}]