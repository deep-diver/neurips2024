{"references": [{"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-MM-DD", "reason": "This paper is foundational for the field, establishing a benchmark for evaluating code LLMs and heavily influencing the approach and methodology of the current work."}, {"fullname_first_author": "Maxwell Nye", "paper_title": "Show your work: Scratchpads for intermediate computation with language models", "publication_date": "2021-MM-DD", "reason": "This paper introduced the concept of scratchpads for intermediate computation in language models, which the current work expands on with its monologue reasoning approach."}, {"fullname_first_author": "Ansong Ni", "paper_title": "Next: Teaching large language models to reason about code execution", "publication_date": "2024-MM-DD", "reason": "This paper tackles a similar problem of enhancing semantic understanding in code LLMs, which makes it relevant for comparison and contrast with the current work's proposed approach."}, {"fullname_first_author": "Yuxiang Wei", "paper_title": "Magicoder: Source code is all you need", "publication_date": "2023-MM-DD", "reason": "This paper presents a strong baseline model for code generation which is used for comparison against SEMCODER in the experimental evaluation."}, {"fullname_first_author": "Baptiste Rozi\u00e8re", "paper_title": "Code Llama: Open foundation models for code", "publication_date": "2024-MM-DD", "reason": "This paper provides another important baseline model for comparison, highlighting the competitive performance of SEMCODER."}]}