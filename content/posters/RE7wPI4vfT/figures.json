[{"figure_path": "RE7wPI4vfT/figures/figures_1_1.jpg", "caption": "Figure 1: We plot the error in the score estimate for an 1D two mode Gaussian example where diffusion dynamics bridge between a Gaussian and a mixture (see Appendix A.3). Regions near the standard forward training data paths have lower error magnitude (light), whereas other areas have higher error magnitude (dark). While sequential samplers adhere as closely as possible to low-error regions, parallel samplers initialize and update the entire sample trajectory (blue trajectories), leading to evaluations in high-error regions. When the sampling trajectory is initialized, most are inevitably in the OOD regions and will update to the low-error regions gradually.", "description": "This figure shows the error in score estimation during the sampling process in a 1D two-mode Gaussian model.  The heatmap represents the error magnitude across different regions of the sample space, with lighter colors indicating lower error and darker colors indicating higher error. Sequential sampling methods tend to stay in low-error areas, while parallel methods often start in high-error regions (OOD), gradually moving towards the low-error areas. This difference is a key factor in the performance gap between sequential and parallel samplers.", "section": "1 Introduction"}, {"figure_path": "RE7wPI4vfT/figures/figures_4_1.jpg", "caption": "Figure 2: The computation graph of Picard iteration for parallel sampling [28]", "description": "This figure illustrates the computation graph of Picard iteration for parallel sampling.  In parallel sampling, the entire reverse process path is randomly initialized, and then all steps in the path are updated in parallel. This is done using Picard iteration, a method for solving ordinary differential equations (ODEs) through fixed-point iteration. The figure shows how each step xk+1 in the path depends on the previous step xk and the drift function s(x, t). The parallel nature of the update is evident in the figure, with each node representing a point in the path at a specific timestep and the arrows showing the dependencies between steps.", "section": "4 Sequential and Parallel Sampling with Diffusion Models"}, {"figure_path": "RE7wPI4vfT/figures/figures_6_1.jpg", "caption": "Figure 4: Parallel DDPM sampler generated Dino data. Comparing to Dino sampled from DDPM loss, CDL-loss sampled Dino has better sample quality and density estimate around hard areas.", "description": "This figure compares the results of using the Contrastive Diffusion Loss (CDL) versus the standard DDPM loss in a parallel DDPM sampler to generate Dino data (a 2D synthetic dataset).  The leftmost panel (a) shows the ground truth Dino data. The middle panel (b) displays Dino samples generated using the CDL, showing a higher quality, denser representation, especially in the more complex areas. The rightmost panel (c) shows samples generated with DDPM loss. The table below shows quantitative results that support the visual comparison, highlighting the CDL's improved MMD score (a measure of the similarity of generated samples to the ground truth), fewer Picard iterations to convergence, and reduced time and sampling cost.", "section": "5.1 Parallel Sampling"}, {"figure_path": "RE7wPI4vfT/figures/figures_7_1.jpg", "caption": "Figure 1: We plot the error in the score estimate for an 1D two mode Gaussian example where diffusion dynamics bridge between a Gaussian and a mixture (see Appendix A.3). Regions near the standard forward training data paths have lower error magnitude (light), whereas other areas have higher error magnitude (dark). While sequential samplers adhere as closely as possible to low-error regions, parallel samplers initialize and update the entire sample trajectory (blue trajectories), leading to evaluations in high-error regions. When the sampling trajectory is initialized, most are inevitably in the OOD regions and will update to the low-error regions gradually.", "description": "This figure compares the error in score estimation between sequential and parallel sampling methods in a 1D two-mode Gaussian mixture.  The shading represents the error magnitude, showing that sequential samplers stay close to low-error regions, while parallel samplers often venture into high-error OOD regions due to their initializations across the whole trajectory. This highlights the challenge that parallel sampling faces in regions where the denoiser is less accurate.", "section": "1 Introduction"}, {"figure_path": "RE7wPI4vfT/figures/figures_8_1.jpg", "caption": "Figure 7: The FID comparison between our CDL and the baselines EDM in the stochastic sampler experiment on CIFAR-10. CDL's performance is strictly better for all Schurn, outperforming the optimal setting of EDM which inflates the standard deviation Schurn of the newly added noise.", "description": "This figure compares the Frechet Inception Distance (FID) scores of three different models on the CIFAR-10 dataset. The models are EDM with Snoise = 1.00, EDM with Snoise = 1.007, and CDL with Snoise = 1.00. The x-axis represents the Schurn parameter, which controls the amount of stochasticity in the sampling process. The y-axis represents the FID score, which measures the quality of the generated samples. The figure shows that CDL consistently outperforms EDM across all values of Schurn, even when EDM uses the optimal setting (Snoise = 1.007). This demonstrates that CDL is more robust to variations in sampling strategy than EDM.", "section": "5.2 Sequential Sampling"}, {"figure_path": "RE7wPI4vfT/figures/figures_15_1.jpg", "caption": "Figure 8: MMD between Dino data and the standard Gaussian. We plots the relationship between the MMD values and the bandwidth parameter used in the kernel function, and pick the bandwidth value with peak MMD score.", "description": "This figure shows the relationship between the Maximum Mean Discrepancy (MMD) values and the bandwidth parameter of the Gaussian kernel used to compute the MMD.  The MMD is a statistical test used to determine if two distributions are different. In this case, the two distributions are the Dino dataset and a standard Gaussian. The plot shows that the MMD score is maximized at a specific bandwidth, indicating that this is the optimal bandwidth for distinguishing between these two distributions.", "section": "B.1 Synthetic Experiment \u2013 Maximum Mean Discrepancy Bandwidth Choice"}, {"figure_path": "RE7wPI4vfT/figures/figures_18_1.jpg", "caption": "Figure 9: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR-10, via parallel DDPM sampler.", "description": "This figure displays sample images generated from a Conditional CIFAR-10 dataset using a parallel DDPM sampler.  The model used was fine-tuned with the Contrastive Diffusion Loss (CDL).  The images showcase the quality of samples produced by the model after training with CDL, highlighting the effectiveness of the proposed method in generating high-quality samples in a parallel setting.", "section": "C Samples Visualization"}, {"figure_path": "RE7wPI4vfT/figures/figures_19_1.jpg", "caption": "Figure 9: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR-10, via parallel DDPM sampler.", "description": "This figure shows samples generated from a Conditional CIFAR-10 dataset using the parallel DDPM sampler and a model fine-tuned with the Contrastive Diffusion Loss (CDL).  The CDL is a novel self-supervised loss function designed to improve the model's performance, particularly in regions outside of the standard training distribution (OOD). The use of CDL aims to enhance the quality and speed of the generated samples. The image showcases a grid of diverse images generated by the model, reflecting its ability to create realistic and varied samples from the specified dataset.", "section": "C Samples Visualization"}, {"figure_path": "RE7wPI4vfT/figures/figures_19_2.jpg", "caption": "Figure 11: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional AFHQ, via parallel DDPM sampler.", "description": "This figure shows samples generated from an unconditional AFHQ dataset using a parallel DDPM sampler and the CDL loss.  The CDL loss is a contrastive diffusion loss that improves the quality of samples generated by diffusion models, particularly in regions outside of the training distribution.  The use of the parallel DDPM sampler makes sample generation faster than with traditional sequential methods.", "section": "C Samples Visualization"}, {"figure_path": "RE7wPI4vfT/figures/figures_20_1.jpg", "caption": "Figure 1: We plot the error in the score estimate for an 1D two mode Gaussian example where diffusion dynamics bridge between a Gaussian and a mixture (see Appendix A.3). Regions near the standard forward training data paths have lower error magnitude (light), whereas other areas have higher error magnitude (dark). While sequential samplers adhere as closely as possible to low-error regions, parallel samplers initialize and update the entire sample trajectory (blue trajectories), leading to evaluations in high-error regions. When the sampling trajectory is initialized, most are inevitably in the OOD regions and will update to the low-error regions gradually.", "description": "The figure shows the error in score estimation for a 1D two-mode Gaussian example.  It highlights that sequential samplers stay in low-error regions while parallel samplers, due to their initialization across the whole trajectory, inevitably sample from high-error, out-of-distribution regions. This illustrates the core challenge of parallel sampling, motivating the need for improved out-of-distribution performance of diffusion models.", "section": "1 Introduction"}, {"figure_path": "RE7wPI4vfT/figures/figures_20_2.jpg", "caption": "Figure 9: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR-10, via parallel DDPM sampler.", "description": "This figure shows samples generated by a parallel DDPM sampler using a model fine-tuned with the Contrastive Diffusion Loss (CDL). The model was trained on the CIFAR-10 dataset, and the images demonstrate the improved sample quality and density estimation achieved using the CDL.", "section": "C Samples Visualization"}, {"figure_path": "RE7wPI4vfT/figures/figures_21_1.jpg", "caption": "Figure 9: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR-10, via parallel DDPM sampler.", "description": "This figure shows samples generated from a Conditional CIFAR-10 dataset using the parallel DDPM sampler. The model was fine-tuned using the Contrastive Diffusion Loss (CDL). The image grid visually demonstrates the quality of generated samples after applying CDL for fine-tuning the EDM checkpoint.", "section": "C Samples Visualization"}, {"figure_path": "RE7wPI4vfT/figures/figures_21_2.jpg", "caption": "Figure 11: The CDL-loss fine-tuned EDM checkpoint generated examples from Unconditional AFHQ, via parallel DDPM sampler.", "description": "This figure shows samples generated from a diffusion model fine-tuned using the Contrastive Diffusion Loss (CDL). The model was trained on the AFHQ dataset (high-quality animal images), and the samples were generated using the parallel DDPM sampler.  The CDL aims to improve the model's performance in regions far from the training data distribution, which is beneficial for parallel sampling methods.", "section": "5.1 Parallel Sampling"}, {"figure_path": "RE7wPI4vfT/figures/figures_22_1.jpg", "caption": "Figure 9: The CDL-loss fine-tuned EDM checkpoint generated examples from Conditional CIFAR-10, via parallel DDPM sampler.", "description": "This figure shows samples generated from a Conditional CIFAR-10 dataset using the parallel DDPM sampler and the CDL-loss fine-tuned EDM checkpoint.  It visually demonstrates the quality of images produced by the model trained with the Contrastive Diffusion Loss (CDL). The parallel DDPM sampler is a method for generating samples more efficiently compared to the traditional sequential approach.", "section": "C Samples Visualization"}]