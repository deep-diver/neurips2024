[{"figure_path": "JD3NYpeQ3R/figures/figures_1_1.jpg", "caption": "Figure 1. The left panel displays the output of GPT-3.5-Turbo for the prompt \"How often is a shingles vaccine required?\" The first filtered output (center) is calibrated using the frequency score (see Appendix E.1) and the marginally valid conformal factuality method of Mohri and Hashimoto [21] at a fixed level of 90%. The second filtered output (right) is calibrated using a score obtained via our conditional boosting procedure (Section 3.3) at a level of 63%, which is chosen and calibrated using our adaptive method (Section 3.2) to approximately ensure that at least 70% of the claims are retained. Both filtered outputs are guaranteed to include no false claims with the stated probability.", "description": "This figure compares three outputs from a large language model (LLM) in response to a question about shingles vaccines. The first is the unfiltered LLM output. The second is filtered using the existing method by Mohri and Hashimoto, which retains only the claims exceeding a fixed 90% confidence threshold.  The third output uses the authors' new method, achieving a 63% adaptive confidence threshold while retaining approximately 70% of the original claims.  This illustrates the trade-off between retaining useful information and maintaining a high probability of factual correctness.", "section": "1 Introduction"}, {"figure_path": "JD3NYpeQ3R/figures/figures_2_1.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of \u03b1 = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the performance of the proposed conditional boosting and level-adaptive methods. The left panel shows the calibration of the methods by comparing the binned nominal probabilities of factuality against the realized probabilities. The right panel compares the claim retention achieved by different methods, showing that conditional boosting and level-adaptive approaches improve claim retention. ", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_8_1.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of \u03b1 = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the performance of the proposed conditional boosting and level-adaptive methods.  The left panel shows calibration results, comparing predicted and actual factuality probabilities across different confidence levels. The right panel shows claim retention rates comparing the three proposed methods, illustrating the benefits of boosting and adaptive level selection.", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_9_1.jpg", "caption": "Figure 4. Comparison of the split conformal calibration method of Mohri and Hashimoto [21] (blue) against our conditional conformal method (orange). The left and right panels displays the miscoverage and percentage of claims retained by the two methods against the number of views received by the Wikipedia pages in January 2023. The displayed boxplots are computed over 200 trials in which we run both methods on a calibration set of 5890 points and evaluate their coverage on a test set of size 2500. The displayed groups correspond to view counts that are binned into the intervals (-\u221e, 8), [1000000, \u221e), [100000, 1000000), [1000, 100000), [100, 1000), [0, 100). The fraction of the data set belonging to each group (in plotted order) is 8.7%, 30.9%, 39.5%, 19.3%, and 1.5%, respectively.", "description": "This figure compares the performance of the split conformal method from Mohri and Hashimoto [21] and the proposed conditional conformal method.  The left panel shows miscoverage (the difference between the nominal and actual coverage probabilities) for different frequency groups of Wikipedia articles (based on the number of views). The right panel displays the percentage of claims retained by each method.  The results are based on 200 trials with a calibration set of 5890 points and a test set of 2500 points.  The Wikipedia articles are grouped by their view counts into six categories, and each category's size relative to the total is indicated.", "section": "Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_15_1.jpg", "caption": "Figure 5. For a hold-out set of size 424, we plot the optimal level threshold, \u03b1\u2217 for claim retention (13) against the number of Wikipedia page views (on the log scale) for the associated person. Letting vi denote the view count for person i, the black line denotes the estimate of the 0.25-quantile of \u03b1 | vi obtained by regressing over the function class given by {\u03b20 + \u03a33=1 \u03b2ivi | \u03b2 \u2208 R4}.", "description": "This figure shows the relationship between the optimal level threshold (\u03b1*) for claim retention and the number of Wikipedia page views for the associated person.  The optimal \u03b1* is determined using a hold-out dataset of 424 data points. The black line represents the estimated 0.25-quantile of \u03b1 given the number of views, obtained by regression analysis using a specific function class.  The figure illustrates how the optimal threshold for claim retention adjusts based on the popularity (number of views) of the biographical subject.", "section": "3.2 Level-adaptive conformal prediction"}, {"figure_path": "JD3NYpeQ3R/figures/figures_16_1.jpg", "caption": "Figure 6. Comparison of the realized and nominal levels of our level-adaptive method for various choices of F. Here, (Xi, Yi) ~ N(0, I2) and we set the conformity score to be simply S(Xi, Yi) = |Yi|. In this experiment, we use our level-adaptive method (Section 3.2 in the main text) to construct prediction sets for Y\u2081 given covariates X\u2081. We set a(X) = \u03c3(X) where \u03c3(\u00b7) denotes the sigmoid function with temperature 1. We then run the level-adaptive method on a calibration set of size n = 1000 and evaluate the method on 1 test point; the plotted points (center, right) are obtained from 500 trials. The left panel shows results for F = {x \u2194 \u03b2: \u03b2\u2208R}, while the right panel displays results for F = {(1,\u03b1(X))\u03a4\u03b2: \u03b2\u2208R2}.", "description": "This figure compares the realized and nominal coverage levels of the level-adaptive conformal prediction method for two different choices of the function class F. The left panel shows results for F = {x \u2194 \u03b2: \u03b2\u2208R} (constant functions), while the right panel shows results for F = {(1,\u03b1(X))\u03a4\u03b2: \u03b2\u2208R2}, which includes the adaptive level function a(X).  The results demonstrate the importance of choosing a sufficiently rich function class F to achieve good calibration.", "section": "3.2 Level-adaptive conformal prediction"}, {"figure_path": "JD3NYpeQ3R/figures/figures_18_1.jpg", "caption": "Figure 7. Comparison of the marginal boosting procedure of Stutz et al. [27] (blue) against our conditional boosting method (orange). The left panel shows the prediction sets produced by each method, while the right panel displays the conditional coverage, P(Yn+1 \u2208 \u0108(Xn+1) | X(1)) against the values of the first feature, X(1). Using the Adam optimizer with learning rate set to 0.001, the plotted scores are boosted for 500 steps on a synthetic dataset of size n = 1000 and evaluated on another test dataset of size n = 2000.", "description": "This figure compares the performance of the authors' conditional boosting method against the marginal boosting method of Stutz et al. [27] on synthetic data.  The left panel visually shows the prediction intervals generated by each method, illustrating the difference in their coverage.  The right panel plots the conditional coverage (the probability that the true value falls within the prediction interval given the value of the first feature, X(1)) against the value of X(1). This demonstrates how the conditional coverage varies across different values of X(1) for both methods, highlighting the superiority of the authors' approach in maintaining consistent conditional coverage.", "section": "D.1 Synthetic data"}, {"figure_path": "JD3NYpeQ3R/figures/figures_18_2.jpg", "caption": "Figure 8. Performance of our level-adaptive method on a synthetic dataset. We use n = 2000 points to estimate the adaptive a() function. We then run the level-adaptive method on a calibration set of size n = 1000 and evaluate the method on 1 test point; the plotted points (center, right) are obtained from 200 trials. The left panel shows the distribution of interval lengths obtained on the test set for fixed values of a \u2208 {0.05, 0.25,0.5,0.75, 0.95}. The center panel displays the interval lengths obtained when the level a(Xn+1) is now chosen adaptively to ensure a maximum prediction set size of at most 500 (red line). Finally, the right panel compares the realized coverage P(Yn+1 \u2208 \u0108(Xn+1) | (Xn+1)) against the nominal level, 1 a(Xn+1) reported by our method with F = {\u03b2\u03bf + \u03a3\u00b2=1 \u03b2\u03b9x\u00b2 + \u03b2\u03b1(x) | \u03b2\u2208R4}.", "description": "This figure shows the result of applying the level-adaptive conformal prediction method on a synthetic dataset.  The left panel illustrates the distribution of interval lengths for various fixed nominal coverage levels. The middle panel demonstrates the effect of adaptively choosing the level to control the maximum interval length. The right panel displays the calibration of the method by comparing the realized and nominal coverage levels.", "section": "D Additional experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_20_1.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of a = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the effectiveness of the proposed conditional boosting and level-adaptive methods on several datasets for medical question answering. The left panel shows the calibration of the model's nominal factuality probabilities against realized probabilities (estimated from 500 test points and 100 calibration-test splits). The right panel compares the claim retention rates of the proposed methods against the baseline method, showcasing the improved performance of the proposed methods.", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_20_2.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of a = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the effectiveness of the proposed conditional boosting and level-adaptive methods.  The left panel shows the calibration of the method, comparing nominal and realized factuality probabilities across different confidence levels. The right panel compares claim retention rates across various methods (unboosted scores, boosted scores, and boosted scores with level adaptation), highlighting the improved claim retention achieved by the proposed methods while maintaining factuality guarantees.", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_21_1.jpg", "caption": "Figure 9. The left panel shows the percentage of claims retained by various methods on the MedLFQA benchmark of Jeong et al. (2024). This benchmark contains many datasets each of which is displayed on the x-axis. Before boosting, we define our claim score by an equally-weighted ensemble of previously published claim scoring functions. We run 100 trials in which we resample a boosting/a()-estimation split (n = 1441), calibration split (n = 2354), and test split (n = 500). We plot 4 filtering methods: the fixed-level (1 \u2013 a = 0.9) method that guarantees conditional validity over the function class given by dataset indicators and prompt metadata (prompt length, response length, as well as the mean and standard deviation of the \"log probability\" claim score over the output) (blue), the same conditionally valid method with adaptive level a(Xn+1) (orange), the conditionally valid method with boosted scores (green), and a combination method that incorporates both conditional-boosting and level-adaptive CP (red). All methods are set-up to ensure that the final output contains 0 false claims. The middle panel shows that boosting allows the level-adaptive procedure to issue guarantees with higher confidence. The right panel verifies that the reported nominal levels 1 \u2013 a(Xn+1) match the empirical error frequencies over equi-spaced bins of width 0.05; these bin indicators are included in the function class used in the level-adaptive procedure.", "description": "This figure compares four different claim filtering methods on the MedLFQA benchmark dataset.  The methods vary in whether they use boosting and/or an adaptive level for the conformal prediction.  The left panel shows the percentage of claims retained by each method, the middle panel demonstrates the impact of boosting on the level-adaptive method's confidence, and the right panel verifies the calibration of the reported probability levels.", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_21_2.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of \u03b1 = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the performance of conditional boosting and level-adaptive methods on six datasets.  The left panel shows the calibration of the method; the nominal probability of factuality closely matches the realized probability. The right panel compares the claim retention across various methods, with conditional boosting showing significant improvement in the percentage of claims retained.", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_21_3.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of a = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the performance of the proposed conditional boosting and level-adaptive methods.  The left panel shows the calibration of the method, comparing the nominal (expected) factuality probabilities to the realized (actual) probabilities across different confidence levels. The right panel compares the claim retention rates (percentage of original claims kept after filtering) of three methods: unboosted scores, boosted scores, and boosted scores with level adaptation.  The results show that the proposed methods improve both the calibration and claim retention compared to the baseline.", "section": "4 Experiments"}, {"figure_path": "JD3NYpeQ3R/figures/figures_22_1.jpg", "caption": "Figure 2. Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with 0 factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using 500 test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are [0.5, 0.55], [0.55, 0.6], ..., [0.8, 0.85]. Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of \u03b1 = 0.1. Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.", "description": "This figure empirically demonstrates the performance of the proposed conditional boosting and level-adaptive conformal prediction methods. The left panel shows the calibration of the method, comparing the predicted probability of factuality to the actual realized probability. The right panel compares the claim retention rate of the proposed methods against a baseline method, highlighting the improvement achieved by incorporating boosting and adaptive level selection.", "section": "Experiments"}]