{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the work on LLM-based agents and their reasoning capabilities, a central element of the proposed forecasting framework."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-14", "reason": "This paper introduces the LLaMa model, which serves as the core LLM for the proposed forecasting framework's agents and forecasting module."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "This paper introduces Llama 2, an improved version of LLaMa which is also used in the study, highlighting advancements in LLMs relevant to the proposed approach."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper details the chain-of-thought prompting technique, crucial for enabling the LLM agents to perform multi-step reasoning and news filtering effectively."}, {"fullname_first_author": "Nate Gruver", "paper_title": "Large language models are zero-shot time series forecasters", "publication_date": "2024-12-01", "reason": "This paper directly addresses the use of LLMs for time series forecasting, providing a theoretical foundation for the approach of transforming time series forecasting into a next-token prediction problem."}]}