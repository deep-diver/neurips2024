[{"figure_path": "bQMevGCYVM/figures/figures_3_1.jpg", "caption": "Figure 1: Framework of our approach. pixel-level segmentation masks M for all frames. Xtxt is a free-form text that particularly emphasizes implicit intent reasoning, world knowledge, and video temporal dynamics.", "description": "This figure illustrates the architecture of VideoLISA, a video-based multimodal large language model. It consists of four main components: 1) a visual tokenizer that converts video frames into visual tokens, 2) a large language model (LLM) that processes both the visual tokens and a language instruction to understand the user's intent and reason about the video content, 3) a vision encoder that extracts visual features from the video frames, and 4) a promptable mask decoder that uses the LLM's output and the visual features to generate temporally consistent segmentation masks.  The model uses a sparse dense sampling strategy to balance computational efficiency and temporal context, and it incorporates a special token (<TRK>) to segment and track objects across multiple frames, enabling one-token-segmentation-of-all frames. ", "section": "3.1 Architecture"}, {"figure_path": "bQMevGCYVM/figures/figures_4_1.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "The figure compares the performance of three different models on a video object segmentation task. The first row shows the input video frames and a box prompt used for the task. The second row shows the result obtained using an image-based model (LISA). The third row shows the result using the proposed video-based model (VideoLISA). The results show that the VideoLISA model produces more temporally consistent segmentation masks compared to the image-based model, and it is able to segment the objects consistently across multiple frames. The main point of the figure is to illustrate the effectiveness of the proposed One-Token-Seg-All approach, which uses a single token to segment and track objects across multiple frames, improving the temporal consistency of the segmentation masks.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_17_1.jpg", "caption": "Figure 3: VideoLISA is a capable model on video object segmentation with versatile language-instructed reasoning abilities. Beyond basic language referring, it enables complex reasoning by leveraging world knowledge and videos temporal dynamics.", "description": "This figure demonstrates VideoLISA's capabilities in video object segmentation using various types of language instructions.  It showcases VideoLISA's ability to handle simple language referring tasks, more complex reasoning tasks leveraging world knowledge, and tasks requiring an understanding of temporal dynamics in the video.  The different rows illustrate different types of reasoning required to complete the segmentation task for the same video segment.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_17_2.jpg", "caption": "Figure 4: Failure cases of VideoLISA.", "description": "This figure showcases examples where VideoLISA fails to correctly segment objects in videos due to challenges such as complex reasoning, object motion, and the limitations of the underlying large language model (LLM).  The examples highlight scenarios where the model struggles with interpreting nuanced instructions or where the visual input is ambiguous or contains elements not fully captured in its training data. This demonstrates that while VideoLISA exhibits impressive reasoning and segmentation capabilities, it is not perfect and still faces limitations common to current large language models and video segmentation technology.", "section": "A.3 Qualitative Results"}, {"figure_path": "bQMevGCYVM/figures/figures_17_3.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "This figure shows a comparison of different prompting methods for video object segmentation. The top row demonstrates using box prompts with the LISA model (image-based), which struggles to segment consistently due to object motion. The second row utilizes the prompt embedding from LISA for multiple frames, showing improved but still inconsistent results.  The bottom row uses VideoLISA's One-Token-Seg-All approach with a single <TRK> token across all frames, achieving consistent segmentation despite object motion, showcasing the effectiveness of the proposed method.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_18_1.jpg", "caption": "Figure 5: ReasonVOS benchmark. The left part shows the statistics of data samples. The right part shows the source of the videos.", "description": "The figure shows the composition of the ReasonVOS benchmark dataset. The left part provides a breakdown of the dataset's composition, including the number of short queries (205), long queries (253), videos (91), seed data (105), and the final number of samples (458).  The right part illustrates a pie chart visualizing the proportion of videos sourced from four different datasets: BURST (37), MeViS (18), MOSE (26), and VIPSeg (10).", "section": "4 Benchmark"}, {"figure_path": "bQMevGCYVM/figures/figures_19_1.jpg", "caption": "Figure 4: Failure cases of VideoLISA.", "description": "This figure showcases several failure cases of the VideoLISA model, highlighting scenarios where the model struggles with complex reasoning, understanding ambiguous language instructions or subtle differences between similar objects in the video. These examples emphasize the limitations of VideoLISA, underscoring areas for improvement and future work.", "section": "A.3 Qualitative Results"}, {"figure_path": "bQMevGCYVM/figures/figures_19_2.jpg", "caption": "Figure 3: VideoLISA is a capable model on video object segmentation with versatile language-instructed reasoning abilities. Beyond basic language referring, it enables complex reasoning by leveraging world knowledge and videos temporal dynamics.", "description": "This figure showcases VideoLISA's capabilities in video object segmentation using various language instructions.  It highlights the model's ability to go beyond simple object identification, demonstrating its complex reasoning abilities by using world knowledge and understanding temporal dynamics within the videos. The examples shown include identifying a specific person, an object moving quickly, and an object that is moving faster than others, all based on natural language prompts.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_19_3.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "This figure compares the performance of different prompting methods for video object segmentation. The first row shows a simple box prompt used in the baseline method (LISA). The second row demonstrates the use of a prompt embedding from LISA, which shows improved robustness to object motion but still fails when the object motion is significant or a distractor object is present. The third row illustrates the proposed One-Token-Seg-All approach using the <TRK> token, demonstrating its effectiveness in segmenting and tracking objects across multiple frames consistently.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_19_4.jpg", "caption": "Figure 1: Framework of our approach. pixel-level segmentation masks M for all frames. Xtxt is a free-form text that particularly emphasizes implicit intent reasoning, world knowledge, and video temporal dynamics.", "description": "The figure shows the architecture of VideoLISA, a multimodal large language model for video object segmentation.  It consists of a visual tokenizer, an LLM, a vision encoder, and a promptable mask decoder. The visual tokenizer converts video frames into visual tokens which are then concatenated with text tokens from a language instruction and fed to the LLM. The LLM processes this information, and its output is used by the mask decoder to produce segmentation masks.  The architecture highlights the integration of sparse dense sampling and one-token-seg-all approach for efficient and temporally consistent segmentation.", "section": "3.1 Architecture"}, {"figure_path": "bQMevGCYVM/figures/figures_19_5.jpg", "caption": "Figure 3: VideoLISA is a capable model on video object segmentation with versatile language-instructed reasoning abilities. Beyond basic language referring, it enables complex reasoning by leveraging world knowledge and videos temporal dynamics.", "description": "This figure showcases VideoLISA's capabilities in handling various types of language-instructed reasoning tasks during video object segmentation.  It demonstrates the model's ability to go beyond simple object identification based on direct language prompts. Instead, it successfully performs complex reasoning involving world knowledge and temporal dynamics. The examples highlight VideoLISA's capacity to understand and act upon nuanced instructions, such as identifying the person on the left, the object moving fastest, and the faster-moving object in a video. This illustrates VideoLISA's advanced reasoning power, which goes beyond basic referring expression capabilities.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_19_6.jpg", "caption": "Figure 7: More qualitative examples of VideoLISA.", "description": "This figure shows several examples of VideoLISA's performance on various video reasoning tasks.  Each example demonstrates VideoLISA's ability to interpret complex language instructions, reason about the video content, and generate temporally consistent segmentations. The examples include identifying specific objects based on contextual clues, such as the \"lead singer\" in a concert or a \"kid who loses the game\", and inferring the intentions or causes of events within the video, such as identifying the material causing a cat to jump. This figure visually showcases VideoLISA's capacity for nuanced reasoning and temporal comprehension.", "section": "A.3 Qualitative Results"}, {"figure_path": "bQMevGCYVM/figures/figures_20_1.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "The figure shows an ablation study comparing different prompting methods for video object segmentation using the Segment Anything Model (SAM).  It demonstrates that a single, specially designed token (<TRK>) can effectively segment and track objects across multiple frames, surpassing the performance of methods using box prompts or prompts derived from an image-only reasoning model (LISA).  This highlights the effectiveness of the proposed One-Token-Seg-All approach in VideoLISA for achieving temporal consistency in video segmentation.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_20_2.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "This figure shows a comparison of different prompting methods for video object segmentation using the Segment Anything Model (SAM).  The first row shows using box prompts, which are sensitive to object movement, resulting in inconsistent segmentation. The second row shows using prompts generated by the image reasoning segmentation model LISA, which demonstrates better resilience to object movement but still struggles when the motion becomes larger or a distractor object appears. The third row shows the proposed One-Token-Seg-All approach using a special <TRK> token, demonstrating consistent and robust segmentation across multiple frames despite significant object movement.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_20_3.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "The figure demonstrates the effectiveness of using a single <TRK> token to segment and track objects across multiple frames. It compares different prompt embeddings, showing how a prompt embedding from LISA, which is an image reasoning model, is more resilient to object motion compared to box prompts. However, even the LISA prompt embedding fails when object motion becomes significant. The proposed VideoLISA model is shown to successfully track objects across multiple frames using a single <TRK> token, demonstrating the effectiveness of the proposed One-Token-Seg-All approach.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_20_4.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "The figure demonstrates the effectiveness of the proposed One-Token-Seg-All approach for temporal consistent segmentation. It compares the results of using a box prompt (LISA), a prompt embedding from LISA, and the proposed <TRK> token (VideoLISA) for segmenting objects across multiple frames.  The <TRK> token shows improved robustness to object movement and distractors compared to the other methods.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_20_5.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "The figure demonstrates the effectiveness of the One-Token-Seg-All approach in VideoLISA. It compares the segmentation results using different prompt embeddings from LISA (image-based) and VideoLISA (video-based) across multiple frames. The results show that VideoLISA\u2019s One-Token-Seg-All approach, using a special <TRK> token, achieves better temporal consistency in segmentation compared to the image-based LISA approach, which fails when object motion is present or when distractor objects appear.", "section": "3.3 One Token Seg All"}, {"figure_path": "bQMevGCYVM/figures/figures_20_6.jpg", "caption": "Figure 2: Exploration of One-Token-Seg-All approach.", "description": "This figure compares the performance of different prompting methods for video object segmentation. The first row shows a box prompt, which only considers the spatial location of the object. This method struggles to segment objects when they move across frames. The second row shows the results of using a prompt embedding from the image reasoning model LISA. The performance is slightly improved but still limited because LISA does not consider the temporal context.  The third row uses our proposed One-Token-Seg-All approach with a specially designed <TRK> token. This method shows significantly better performance and produces temporally consistent segmentation masks.", "section": "3.3 One Token Seg All"}]