[{"figure_path": "VcPtU8e6yK/figures/figures_0_1.jpg", "caption": "Figure 1: Bifr\u00f6st results on various personalized image compositing tasks. Top: Bifr\u00f6st is adept at precise, arbitrary object placement and replacement in a background image with a reference object and a language instruction, and achieves 3D-aware high-fidelity harmonized compositing results; Bottom Left: Given a coarse mask, Bifr\u00f6st can change the pose of the object to follow the shape of the mask; Bottom Right: Our model adapts the identity of the reference image to the target image without changing the pose.", "description": "This figure showcases the capabilities of the Bifr\u00f6st model in various image compositing tasks.  The top row demonstrates precise object placement and replacement based on language instructions, considering 3D aspects like occlusion and depth. The bottom left shows the model's ability to modify object poses according to a given mask, and the bottom right illustrates identity transfer while maintaining original pose.", "section": "1 Introduction"}, {"figure_path": "VcPtU8e6yK/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "This figure illustrates the two-stage inference pipeline of the Bifr\u00f6st model.  Stage 1 involves using a Multi-modal Large Language Model (MLLM) to predict the 2.5D location (bounding box and depth) of an object based on a background image and a text instruction. Stage 2 uses a depth predictor to estimate the depth of the background and reference object, fuses this depth information with the MLLM prediction, and feeds it to a diffusion model along with the masked background image and the reference object image to generate the final composited image. The process ensures that the final image respects the spatial relationships indicated in the text instruction and appears visually realistic.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of the 2.5D counterfactual dataset generation for fine-tuning MLLM. Given a scene image I, one object o was randomly selected as the object we want to predict (e.g., the laptop in this figure). The depth of the object is predicted by a pre-trained depth predictor. The selected object is then removed from the given image using the SAM (i.e. mask the object) followed by an SD-based inpainting model (i.e., inpaint the masked hole). The final data pair consists of a text instruction, a counterfactual image, and a 2.5D location of the selected object o.", "description": "This figure illustrates the process of generating a counterfactual dataset for fine-tuning the Multi-modal Large Language Model (MLLM).  It starts with a scene image, selects an object, predicts its depth using a depth predictor, masks and inpaints the object, and finally pairs the resulting counterfactual image with a text instruction and the object's 2.5D location (bounding box and depth) as training data for the MLLM.", "section": "3.1 Finetuning MLLM to Predict 2.5D Location"}, {"figure_path": "VcPtU8e6yK/figures/figures_4_2.jpg", "caption": "Figure 3: Overview of the 2.5D counterfactual dataset generation for fine-tuning MLLM. Given a scene image I, one object o was randomly selected as the object we want to predict (e.g., the laptop in this figure). The depth of the object is predicted by a pre-trained depth predictor. The selected object is then removed from the given image using the SAM (i.e. mask the object) followed by an SD-based inpainting model (i.e., inpaint the masked hole). The final data pair consists of a text instruction, a counterfactual image, and a 2.5D location of the selected object o.", "description": "This figure illustrates the process of generating a counterfactual dataset used for fine-tuning the Multi-modal Large Language Model (MLLM). It shows how an object is selected, its depth predicted, and then removed from the original image using image inpainting techniques.  The resulting image, along with a textual instruction about the object's location, forms a data point for training the MLLM to predict 2.5D object locations.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_5_1.jpg", "caption": "Figure 5: Overview of training pipeline of Bifr\u00f6st on image compositing stage. A segmentation module is first adopted to get the masked image and object without background, followed by an ID extractor to obtain its identity information. The high-frequency filter is then applied to extract the detail of the object, stitch the result with the scene at the predicted location, and employ a detail extractor to complement the ID extractor with texture details. We then use a depth predictor to estimate the depth of the image and apply a depth extractor to capture the spatial information of the scene. Finally, the ID tokens, detail maps, and depth maps are integrated into a pre-trained diffusion model, enabling the target object to seamlessly blend with its surroundings while preserving complex spatial relationships.", "description": "This figure illustrates the training pipeline of the 3D-aware image compositing module in Bifr\u00f6st. It shows how the model uses segmentation, ID extraction, depth prediction, and detail extraction to generate a high-fidelity composite image that respects spatial relationships and depth cues.", "section": "3D-Aware Image compositing"}, {"figure_path": "VcPtU8e6yK/figures/figures_6_1.jpg", "caption": "Figure 6: Data preparation pipeline of leveraging videos. Given a clip, we first sample two frames, selecting an instance from one frame as the reference object and using the corresponding instance from the other frame as the training supervision.", "description": "This figure illustrates the process of creating training data from video clips for the image compositing model.  Two frames are selected from a video clip. One frame provides the reference object and the other frame serves as the background image.  The corresponding instance (the same object) is identified in both frames. The object from one frame is used as the reference image, while the other frame, after masking the object, forms the target background image. This method leverages the temporal consistency in videos to generate training pairs with varied poses and views of the object within a similar context.", "section": "3.2 3D-Aware Image compositing"}, {"figure_path": "VcPtU8e6yK/figures/figures_7_1.jpg", "caption": "Figure 7: Qualitative comparison with reference-based image generation methods, including Paint-by-Example (Yang et al., 2023), ObjectStitch (Song et al., 2023), and AnyDoor (Chen et al., 2024), where our Bifr\u00f6st better preserves the geometry consistency. Note that all approaches do not fine-tune the model on the test samples.", "description": "This figure shows a qualitative comparison of Bifrost with three other methods for image generation: Paint-by-Example, ObjectStitch, and AnyDoor.  The comparison highlights Bifrost's superior ability to maintain geometric consistency when compositing images. It's important to note that none of the methods were fine-tuned on the test set used for this comparison.", "section": "4.3 Qualitative Evaluation"}, {"figure_path": "VcPtU8e6yK/figures/figures_8_1.jpg", "caption": "Figure 8: Results of other application scenarios of Bifr\u00f6st.", "description": "This figure shows the results of Bifr\u00f6st on various personalized image compositing tasks beyond object placement and replacement.  It demonstrates the model's capabilities in identity-preserved inpainting (changing the object's pose while maintaining its identity), identity transfer (adapting the reference image's identity to match that of the target image while keeping the pose), and more complex scenarios.", "section": "4. Results"}, {"figure_path": "VcPtU8e6yK/figures/figures_8_2.jpg", "caption": "Figure 9: Qualitative ablation study on the core components of Bifr\u00f6st, where the last column is the result of our full model, \u201cHF-Filter\u201d stands for the high-frequency filter in the detail extractor.", "description": "This figure shows an ablation study on the core components of the Bifr\u00f6st model. It demonstrates the effect of each component on the final image generation quality.  The figure shows a series of images, each representing a stage in the generation process: starting with a baseline model, and then adding classifier-free guidance (+ CFG), a high-frequency filter (+ HF filter), and finally depth information (+ Depth). The last image showcases the result of the full Bifr\u00f6st model.  The comparison highlights the contribution of each component to improved visual quality and realism.", "section": "4.2 Quantitative Evaluation"}, {"figure_path": "VcPtU8e6yK/figures/figures_9_1.jpg", "caption": "Figure 10: Ablation study of different depth control from deep to shallow.", "description": "This figure shows the results of an ablation study on the impact of different depth control levels in the image compositing model.  It demonstrates how changing the depth value affects the final generated image, specifically regarding the positioning and visual realism of the inserted object relative to the background.  The study varies the depth value from deep (0.35) to shallow (0.80), showing a corresponding shift in the object's position within the scene.", "section": "4.4 Ablation Study"}, {"figure_path": "VcPtU8e6yK/figures/figures_9_2.jpg", "caption": "Figure 7: Qualitative comparison with reference-based image generation methods, including Paint-by-Example (Yang et al., 2023), ObjectStitch (Song et al., 2023), and AnyDoor (Chen et al., 2024), where our Bifr\u00f6st better preserves the geometry consistency. Note that all approaches do not fine-tune the model on the test samples.", "description": "This figure compares the visual results of Bifrost against three other methods for image generation: Paint-by-Example, ObjectStitch, and AnyDoor.  It highlights that Bifr\u00f6st better maintains geometric consistency when compositing objects into background images. Importantly, none of the compared methods were fine-tuned on the test data.", "section": "4.3 Qualitative Evaluation"}, {"figure_path": "VcPtU8e6yK/figures/figures_14_1.jpg", "caption": "Figure 12: The distribution of differences between center value and three other choices of the depth values, where (a) is the differences between the max value of depth and center depth value; (b) is the differences between the mean value of depth and center depth value; and (c) is the differences between the median value of depth and center depth value", "description": "This figure shows the distribution of differences between the depth value at the center of a bounding box and three other ways of calculating the depth value (maximum, mean, and median). The purpose is to determine the best way to represent the 2.5D location of an object for the image compositing task.  The results show that using the center point depth is reasonable, with most of the differences between it and other depth values being small. Using the mean value is also reasonable, while the median value is shown to not be reliable.", "section": "Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_15_1.jpg", "caption": "Figure 13: Overview of the MLLM fine-tuning.", "description": "The figure shows the architecture of the MLLM fine-tuning process.  It takes an image (Xv) as input, which is processed by a vision encoder. The output of the vision encoder (Hv) is then combined with the text instruction (H\u0442) and fed into the LLM. The LLM then produces the bounding box and depth value (Xq). The process uses a LoRA (Low-Rank Adaptation) approach where only a subset of parameters in the LLM are updated during fine-tuning, making the process more efficient. The figure also displays the instruction used as an example.", "section": "3.1 Finetuning MLLM to Predict 2.5D Location"}, {"figure_path": "VcPtU8e6yK/figures/figures_15_2.jpg", "caption": "Figure 14: Different types of mask used in the image compositing stage. The generation is constrained in the masked area, so the user-provided mask is able to modify the pose, view, and shape of the subject.", "description": "This figure shows different mask types used in the image compositing stage of the Bifr\u00f6st model.  The masks vary in complexity, from a simple bounding box (Mask 1) to more detailed shapes (Masks 2-5) that progressively constrain the generated image to fit more closely within the mask's shape and position. This allows users to control the generated object's pose and shape by defining the desired region.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "The figure illustrates the inference pipeline of the proposed method, Bifr\u00f6st.  It shows how a background image, text instruction, and reference object are processed in two stages. Stage 1 uses a multi-modal large language model (MLLM) to predict the 2.5D location (bounding box and depth) of where the object should be placed. Stage 2 uses a depth predictor to estimate depths and fuses this information with the masked background and object image. Finally, a diffusion model generates the composited image.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "This figure illustrates the two-stage inference pipeline of the Bifrost model.  Stage 1 uses a Multi-modal Large Language Model (MLLM) to predict the 2.5D location (bounding box and depth) of an object within a background image based on a textual instruction. Stage 2 leverages a depth predictor to fuse the predicted depth with the background image depth and then uses this information, along with the object and background images, to perform 3D-aware image compositing via a diffusion model, resulting in a final image that accurately reflects the spatial relationships described in the instruction.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_17_2.jpg", "caption": "Figure 17: More examples of 2.5D counterfactual dataset for Fine-tuning MLLM.", "description": "This figure shows more examples of the 2.5D counterfactual dataset used for fine-tuning the multi-modal large language model (MLLM). Each example includes an original image, its corresponding depth map, and a counterfactual image where the target object has been removed.  The instruction given to the MLLM for each example is also provided along with the predicted bounding box and depth value for the target object, demonstrating the model's ability to predict the 2.5D location of an object within a complex scene based on textual instructions.", "section": "3.1 Finetuning MLLM to Predict 2.5D Location"}, {"figure_path": "VcPtU8e6yK/figures/figures_18_1.jpg", "caption": "Figure 7: Qualitative comparison with reference-based image generation methods, including Paint-by-Example (Yang et al., 2023), ObjectStitch (Song et al., 2023), and AnyDoor (Chen et al., 2024), where our Bifr\u00f6st better preserves the geometry consistency. Note that all approaches do not fine-tune the model on the test samples.", "description": "This figure presents a qualitative comparison of Bifr\u00f6st against three other image generation methods: Paint-by-Example, ObjectStitch, and AnyDoor.  The comparison highlights Bifr\u00f6st's superior ability to maintain geometric consistency when compositing images.  Each row shows a different example, demonstrating how Bifr\u00f6st achieves better visual harmony and accuracy in object placement and background integration compared to the alternative methods. Notably, all methods used in the comparison did not fine-tune their models on the test samples.", "section": "4.3 Qualitative Evaluation"}, {"figure_path": "VcPtU8e6yK/figures/figures_19_1.jpg", "caption": "Figure 19: Examples that both object and background are from the out-of-distribution dataset.", "description": "This figure shows two examples where both the object and the background image are from the out-of-distribution dataset.  The model successfully composited the dog to the right of the piano and the horse in front of the church, demonstrating its ability to generalize to unseen objects and scenes.", "section": "4.4 Ablation Study"}, {"figure_path": "VcPtU8e6yK/figures/figures_19_2.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "The figure illustrates the inference pipeline of the Bifr\u00f6st model. First, a multi-modal large language model (MLLM) predicts the 2.5D location (bounding box and depth) of the object to be composited. A depth predictor then estimates the depth of the background and object. The object's depth is scaled and fused with the background depth. Finally, the masked background, fused depth map, and reference object image are fed into a diffusion model to generate the final composited image.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_1.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "This figure illustrates the two-stage inference process of the Bifrost model.  Stage 1 involves using a Multi-modal Large Language Model (MLLM) to predict the 2.5D location (bounding box and depth) of an object within a background image based on a text instruction. Stage 2 uses this information, along with a depth map of the background image and the reference object image, to generate the final composite image using a diffusion model. The process ensures that the composite image accurately reflects the spatial relationships specified in the text instruction while maintaining visual coherence.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_2.jpg", "caption": "Figure 1: Bifr\u00f6st results on various personalized image compositing tasks. Top: Bifr\u00f6st is adept at precise, arbitrary object placement and replacement in a background image with a reference object and a language instruction, and achieves 3D-aware high-fidelity harmonized compositing results; Bottom Left: Given a coarse mask, Bifr\u00f6st can change the pose of the object to follow the shape of the mask; Bottom Right: Our model adapts the identity of the reference image to the target image without changing the pose.", "description": "This figure showcases the capabilities of the Bifr\u00f6st model in three personalized image compositing tasks. The top row demonstrates precise object placement and replacement guided by language instructions, achieving realistic results with accurate occlusion and lighting. The bottom left shows how Bifr\u00f6st changes the object's pose to match a provided mask. Finally, the bottom right illustrates the model's ability to adapt the object's identity to the target image while maintaining its original pose.", "section": "Introduction"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_3.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "This figure illustrates the inference stages of the Bifr\u00f6st model.  First, a multi-modal large language model (MLLM) takes the background image and text instruction as input to predict the 2.5D location (bounding box and depth) of the object to be added.  Then, a depth predictor estimates the depth of the background and object. The object's depth is adjusted based on the MLLM's prediction and fused with the background depth map. Finally, a diffusion model generates the composited image using the masked background, fused depth map, and the object image. The resulting image accurately reflects the spatial relationships specified in the text instruction and maintains visual coherence.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_4.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "The figure illustrates the two-stage inference process of Bifrost. First, a Multi-modal Large Language Model (MLLM) predicts the 2.5D location (bounding box and depth) of the object to be composited. Then, a diffusion model uses the predicted depth, reference object image, and masked background image to generate the final composited image.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_5.jpg", "caption": "Figure 1: Bifr\u00f6st results on various personalized image compositing tasks. Top: Bifr\u00f6st is adept at precise, arbitrary object placement and replacement in a background image with a reference object and a language instruction, and achieves 3D-aware high-fidelity harmonized compositing results; Bottom Left: Given a coarse mask, Bifr\u00f6st can change the pose of the object to follow the shape of the mask; Bottom Right: Our model adapts the identity of the reference image to the target image without changing the pose.", "description": "This figure showcases the capabilities of the Bifr\u00f6st model in performing personalized image compositing tasks using language instructions. The top row demonstrates precise object placement and replacement, considering 3D spatial relationships and achieving high-fidelity results. The bottom left shows how Bifr\u00f6st can change object poses according to a given mask. The bottom right illustrates how Bifr\u00f6st can adapt the identity of the reference object to the target image.", "section": "Introduction"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_6.jpg", "caption": "Figure 5: Overview of training pipeline of Bifr\u00f6st on image compositing stage. A segmentation module is first adopted to get the masked image and object without background, followed by an ID extractor to obtain its identity information. The high-frequency filter is then applied to extract the detail of the object, stitch the result with the scene at the predicted location, and employ a detail extractor to complement the ID extractor with texture details. We then use a depth predictor to estimate the depth of the image and apply a depth extractor to capture the spatial information of the scene. Finally, the ID tokens, detail maps, and depth maps are integrated into a pre-trained diffusion model, enabling the target object to seamlessly blend with its surroundings while preserving complex spatial relationships.", "description": "This figure illustrates the training pipeline of the 3D-aware image compositing module in Bifr\u00f6st. It details the process of using a segmentation module to extract the object from the background, employing ID and detail extractors to capture identity and texture information, and leveraging a depth predictor to estimate depth for spatial relationships.  These features are then integrated into a diffusion model to generate a composited image.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/figures/figures_20_7.jpg", "caption": "Figure 2: Overview of the inference pipeline of Bifr\u00f6st. Given background image Ibg, and text instruction cr that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images' depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image Iout that satisfies spatial relations in the text instruction cr and appears visually coherent and natural (e.g., with light and shadow that are consistent with the background image).", "description": "This figure illustrates the two-stage inference pipeline of the Bifr\u00f6st model.  Stage 1 uses a multi-modal large language model (MLLM) to predict the 2.5D location (bounding box and depth) of an object to be composited into a background image based on a text instruction. Stage 2 uses a depth predictor to estimate depth maps for both the object and background, fuses them, and then uses a diffusion model to generate the final composited image while considering depth and spatial relationships.", "section": "3 Method"}]