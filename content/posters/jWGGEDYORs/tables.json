[{"figure_path": "jWGGEDYORs/tables/tables_5_1.jpg", "caption": "Table 1: Details of three datasets used in the experiments.", "description": "This table summarizes the key characteristics of the three datasets (KUL, DTU, and MM-AAD) used in the paper's experiments.  For each dataset, it provides the number of subjects, the type of scene (audio-only or audio-visual), the language of the auditory stimuli, the duration of each subject's participation in minutes, and the total duration of the dataset in hours.", "section": "3.1 Dataset"}, {"figure_path": "jWGGEDYORs/tables/tables_6_1.jpg", "caption": "Table 2: Auditory attention detection accuracy(%) comparison on DTU, KUL and MM-AAD dataset. The results annotated by * are taken from [20]. Our experimental setup is consistent with theirs to ensure fairness in comparison. Hence, we directly cited their results.", "description": "This table compares the performance of DARNet with other state-of-the-art models (SSF-CNN, MBSSFCC, BSAnet, DenseNet-3D, EEG-Graph Net, and DBPNet) on three publicly available datasets (KUL, DTU, and MM-AAD) for auditory attention detection. The accuracy is measured using three different decision windows (0.1-second, 1-second, and 2-second) for audio-only and audio-visual scenes, showing DARNet's superior performance in all settings.", "section": "4 Result"}, {"figure_path": "jWGGEDYORs/tables/tables_7_1.jpg", "caption": "Table 3: Ablation Study on KUL, DTU, and MM-AAD dataset.", "description": "This table presents the results of ablation experiments performed on three datasets (KUL, DTU, and MM-AAD) to evaluate the impact of different modules in the DARNet model on auditory attention detection accuracy.  The table shows the performance of the model with various components removed, such as the spatial feature extraction, the temporal feature extraction, and the feature fusion modules. It also shows the performance of a simplified version of DARNet with only a single layer of self-attention. This helps to understand which components are most important to the overall performance of the model.", "section": "4 Result"}, {"figure_path": "jWGGEDYORs/tables/tables_8_1.jpg", "caption": "Table 2: Auditory attention detection accuracy(%) comparison on DTU, KUL and MM-AAD dataset. The results annotated by * are taken from [20]. Our experimental setup is consistent with theirs to ensure fairness in comparison. Hence, we directly cited their results.", "description": "This table compares the auditory attention detection accuracy of the proposed DARNet model against other state-of-the-art models across three different datasets (DTU, KUL, and MM-AAD) under three different decision window lengths (0.1, 1, and 2 seconds).  The results show the mean accuracy and standard deviation for each model and dataset. The asterisk indicates results taken from a previous study, ensuring a fair comparison.", "section": "4 Result"}, {"figure_path": "jWGGEDYORs/tables/tables_9_1.jpg", "caption": "Table 5: The training parameter counts comparison. \"M\" denotes a million.", "description": "This table compares the number of trainable parameters for four different models: SSF-CNN, MBSSFCC, DBPNet, and DARNet.  It highlights that DARNet has significantly fewer parameters (0.08 million) compared to the others, demonstrating its computational efficiency.", "section": "5.3 Computational Cost"}]