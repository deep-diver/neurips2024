[{"figure_path": "4VWnC5unAV/tables/tables_2_1.jpg", "caption": "Table 1: Summary of adaptive learning rates results on the least squares problem. We summarize our results for line search and AdaGrad-Norm under various assumptions on the covariance matrix K. We denote Amin the smallest non-zero eigenvalue of K and Tr(K) the average eigenvalue. Power law(\u03b4, \u03b2) assumes the eigenvalues of K, {\u03bbi}i=1, follow a power law distribution, that is, for 0 < \u03b2 < 1, \u03bbi ~ (1 \u2212 \u03b2)\u03bb\u2212\u03b2i(0,1) for all 1 \u2264 i \u2264 d and \u3008Xo \u2013 X*, wi\u3009\u00b2 ~ \u03bb\u2212\u03b4i where {wi}i=1 are eigenvectors of K (see Prop 4.4). For * (see Prop. 4.2), requires a good initialization on b, \u03b7.", "description": "This table summarizes the convergence rates of different adaptive learning rate algorithms (AdaGrad-Norm, Exact Line Search, and Polyak Stepsize) applied to the least squares problem under different assumptions on the data covariance matrix (specifically, when the smallest eigenvalue is bounded away from zero, and when the eigenvalues follow a power-law distribution).  It shows the limiting learning rate and the convergence rate of the risk function for each algorithm and assumption.  The table highlights the impact of covariance matrix properties on the performance of different adaptive learning rates, showing how different assumptions influence the rate at which the risk converges to its minimum.", "section": "Main contributions"}, {"figure_path": "4VWnC5unAV/tables/tables_4_1.jpg", "caption": "Table 1: Summary of adaptive learning rates results on the least squares problem. We summarize our results for line search and AdaGrad-Norm under various assumptions on the covariance matrix K. We denote Amin the smallest non-zero eigenvalue of K and Tr(K) the average eigenvalue. Power law(\u03b4, \u03b2) assumes the eigenvalues of K, {\u03bbi}i=1, follow a power law distribution, that is, for 0 < \u03b2 < 1, \u03bbi ~ (1 \u2212 \u03b2)\u03bb\u2212\u03b21(0,1) for all 1 \u2264 i \u2264 d and \u3008Xo \u2013 X*, wi\u3009\u00b2 ~ \u03bbi where {wi}i=1 are eigenvectors of K (see Prop 4.4). For * (see Prop. 4.2), requires a good initialization on b, \u03b7.", "description": "This table summarizes the results of the analysis of adaptive learning rates for line search and AdaGrad-Norm algorithms on the least squares problem. It shows the limiting learning rate (\u03b3t) and convergence rate under different assumptions about the covariance matrix K (e.g., Amin > C, power law distribution). The table also specifies the assumptions made for each case and provides the corresponding asymptotic behavior of the learning rate.", "section": "Main contributions"}]