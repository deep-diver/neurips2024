[{"figure_path": "4VWnC5unAV/figures/figures_1_1.jpg", "caption": "Figure 1: Concentration of learning rate and risk for AdaGrad-Norm on least squares with label noise w = 1 (left) and logistic regression with no noise (right). As dimension increases, both risk and learning rate concentrate around a deterministic limit (red) described by our ODE in Theorem 2.1. The initial risk increase (left) suggests the learning rate started too high, but AdaGrad-Norm adapts. Our ODEs predict this behavior. See Sec. H for simulation details.", "description": "This figure shows the convergence of the risk and learning rate for AdaGrad-Norm on least squares (left) and logistic regression (right) problems.  The key observation is that as the dimensionality (d) of the problem increases, both the risk and the learning rate converge to a deterministic limit, which is predicted by a system of ordinary differential equations (ODEs) derived in the paper. The left panel shows that although the learning rate initially starts high, AdaGrad-Norm adapts and the risk decreases to the deterministic limit.  The right panel shows a similar convergence for the logistic regression problem.", "section": "1 Introduction"}, {"figure_path": "4VWnC5unAV/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison for Exact Line Search and Polyak Stepsize on a noiseless least squares problem. The left plot illustrates the convergence of the risk function, while the right plot depicts the convergence of the quotient \u03b3t/\u03bbmin(K) for Polyak stepsize and exact line search. Both plots highlight the implication of equation (13) in high-dimensional settings, where a broader spectrum of K results in \u03bbmin(K)/Tr(K\u00b2) \u00ab 1, indicating slower risk convergence and poorer performance of exact line search (unmarked) as it deviates from the Polyak stepsize (circle markers). The gray shaded region demonstrates that equation (13) is satisfied. See Appendix H for simulation details.", "description": "This figure compares the performance of exact line search and Polyak stepsize in a noiseless least squares problem. The left plot shows the convergence of the risk function for both methods, highlighting how exact line search can converge much slower than Polyak stepsize, especially when the covariance matrix K has a broader spectrum. The right plot visualizes this difference by showing the quotient \u03b3t/\u03bbmin(K) over time.", "section": "Idealized Exact Line Search and Polyak Stepsize"}, {"figure_path": "4VWnC5unAV/figures/figures_8_1.jpg", "caption": "Figure 3: Quantities effecting AdaGrad-Norm learning rate. (left): Effect of noise (w = 1.0) on risk (left axis) and learning rate (right axis). Depicted is learning rate so it approaches 1. (Center, right): Noiseless least squares (w = 0). As predicted in Prop. 4.2, limt\u2192\u221e Yt depends on avg. eig. of K (Tr(K)/d) and ||X0 \u2013 X* ||2 but not \u043a = Xmax/Amin. See Appendix H for simulation details.", "description": "This figure shows the impact of noise and the initial distance from the optimum on the AdaGrad-Norm learning rate. The left panel shows that with noise, the learning rate decays as t\u207b\u00b9\u2044\u00b2, regardless of the covariance structure. The center and right panels show that without noise, the learning rate approaches a constant inversely proportional to the average eigenvalue of the covariance matrix, and scales with the initial distance from the optimum. The simulation results match the theoretical predictions.", "section": "4 AdaGrad-Norm analysis"}, {"figure_path": "4VWnC5unAV/figures/figures_9_1.jpg", "caption": "Figure 4: Power law covariance in AdaGrad Norm on a least squares problem. Ran exact predictions (ODE) for the risk and learning rate (solid lines). Dashed lines give the predictions from Prop. 4.4 which match experimental results exactly. Phase transition as \u03b4 + \u03b2 varies. When \u03b4 + \u03b2 < 1 (green), the learning rate (right) is constant as t\u2192 \u221e. In contrast, when 2 > \u03b4 + \u03b2 > 1 (purple), the learning rate decreases at a rate t\u22121+1/(\u03b2+\u03b4) with \u03b4 + \u03b2 = 1 (white) where the change occurs. Same phase transition occurs in the sublinear rate of the risk decay (left) (see Prop. 4.4).", "description": "This figure shows the phase transition of the risk and learning rate in AdaGrad-Norm algorithm when the eigenvalues follow a power law distribution. The x-axis represents time, and the y-axis represents risk and learning rate. The different colored lines represent different values of \u03b4 + \u03b2. When \u03b4 + \u03b2 < 1, the learning rate is constant, while when 1 < \u03b4 + \u03b2 < 2, the learning rate decays. The risk shows a similar phase transition.", "section": "4 AdaGrad-Norm analysis"}, {"figure_path": "4VWnC5unAV/figures/figures_40_1.jpg", "caption": "Figure 5: Convergence in Exact Line Search on a noiseless least squares problem. The plot on the left illustrates the convergence of the risk function, while the center and right plots depict the convergence of the quotient  and the learning rate  respectively. Further details and formulas for the limiting behavior can be found in the Appendix F.2. See Appendix H for simulation details.", "description": "This figure shows the convergence of exact line search in a noiseless least squares problem. The left plot displays the convergence of risk, the center plot shows the quotient of learning rate over minimum eigenvalue, and the right plot shows the learning rate itself.  The plots illustrate how the algorithm's performance relates to the data's spectral properties, specifically the ratio of minimum to average eigenvalue squared. The figure demonstrates how the exact line search strategy\u2019s convergence rate varies depending on this ratio.", "section": "Idealized Exact Line Search and Polyak Stepsize"}, {"figure_path": "4VWnC5unAV/figures/figures_44_1.jpg", "caption": "Figure 6: Predicting the training dynamics on a real dataset, CIFAR-5m [38], using multi-pass AdaGrad-Norm. This suggests that the theory extends beyond Gaussian data and one-pass. Note that the curves look significantly different for different n; smaller values of n lead to an overparametrized problem, allowing least squares to memorize datapoints, whereas for larger n, least squares must learn a general function mapping images of cars and airplanes to their respective labels.", "description": "This figure empirically validates the theory presented in the paper using real-world data from the CIFAR-5m dataset.  It shows the training dynamics of multi-pass AdaGrad-Norm on a least-squares problem for different dataset sizes (n). The results demonstrate that the theory's predictions hold even with non-Gaussian data and multiple passes, highlighting the model's robustness and generalizability.  The differing curves for varying dataset sizes illustrate a transition from over-parametrization (memorization) at smaller n to generalization at larger n.", "section": "G.2 CIFAR 5m"}]