[{"type": "text", "text": "Learning from Noisy Labels via Conditional Distributionally Robust Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hui Guo Department of Computer Science University of Western Ontario hguo288@uwo.ca ", "page_idx": 0}, {"type": "text", "text": "Grace Y. Yi \\* Department of Statistical and Actuarial Sciences Department of Computer Science University of Western Ontario gyi5@uwo.ca ", "page_idx": 0}, {"type": "text", "text": "Boyu Wang   \nDepartment of Computer Science   \nUniversity of Western Ontario bwang@csd.uwo.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While crowdsourcing has emerged as a practical solution for labeling large datasets, it presents a significant challenge in learning accurate models due to noisy labels from annotators with varying levels of expertise. Existing methods typically estimate the true label posterior, conditioned on the instance and noisy annotations, to infer true labels or adjust loss functions. These estimates, however, often overlook potential misspecification in the true label posterior, which can degrade model performances, especially in high-noise scenarios. To address this issue, we investigate learning from noisy annotations with an estimated true label posterior through the framework of conditional distributionally robust optimization (CDRO). We propose formulating the problem as minimizing the worst-case risk within a distance-based ambiguity set centered around a reference distribution. By examining the strong duality of the formulation, we derive upper bounds for the worst-case risk and develop an analytical solution for the dual robust risk for each data point. This leads to a novel robust pseudo-labeling algorithm that leverages the likelihood ratio test to construct a pseudo-empirical distribution, providing a robust reference probability distribution in CDRO. Moreover, to devise an efficient algorithm for CDRO, we derive a closed-form expression for the empirical robust risk and the optimal Lagrange multiplier of the dual problem, facilitating a principled balance between robustness and model fitting. Our experimental results on both synthetic and real-world datasets demonstrate the superiority of our method. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in supervised learning have spurred a growing demand for large labeled datasets [1, 2]. However, acquiring accurately annotated datasets is typically costly and time-consuming, often requiring a pool of annotators with adequate domain expertise to manually label the data. Crowdsourcing has emerged as an efficient and cost-effective solution for annotating large datasets. On crowdsourcing platforms, multiple annotators with varying levels of labeling skills are employed to gather extensive labeled data. However, this approach introduces a significant challenge: the labels collected through crowdsourcing are often subject to unavoidable noise, especially in fields requiring substantial domain knowledge, such as medical imaging. Consequently, models trained on noisy labels are prone to error, including overfitting, since deep models can memorize vast amounts of data [3]. In addition to statistical research on label noise (often termed response measurement error, e.g., [4-6]), a growing body of recent machine learning literature has focused on developing effective algorithms capable of training accurate classifiers using noisy data, e.g., [7-10]. Many of these methods seek to approximate the posterior distribution of the underlying true labels using the observeddata. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Let $\\mathbf{X}$ be an instance, $\\mathrm{Y}$ denote the unobserved true label for $\\mathbf{X}$ , and $\\widetilde{\\mathbf Y}$ represent a vector of crowdsourced noisy labels for $\\mathbf{X}$ . The data-generating distribution, Pxy, can be factorized in two ways: y/xylx,yorP\\*Px P\\*,withP\\*denting te cndionadistribtioforthevariabl indicated by the corresponding subscripts. These factorizations have inspired research that trains models by estimating the posterior distribution of the true labels, $P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}^{*}$ , in the latter factorization. ", "page_idx": 1}, {"type": "text", "text": "Previous work [9-12] introduced various algorithms for estimating the annotator confusions, also known as noise transition probabilities, which yield an approximated conditional distribution of $\\widetilde{\\mathbf Y}$ given $\\mathrm{Y}$ and $\\mathbf{X}$ denoted $P_{\\widetilde{\\mathbf{y}}|\\mathbf{y},\\mathbf{x}}$ orasfr $P^{*}$ and $P$ to denote thetrue distribution and an approximate distribution for the variables indicated by the corresponding subscripts. Given the observed data $\\mathbf{X}$ and $\\widetilde{\\mathbf Y}$ , along with an approximated conditional distribution $P_{\\widetilde{\\mathbf{y}}|\\mathbf{y},\\mathbf{x}}$ and a prior for Y given $\\mathbf{X}$ denoted $P_{\\mathrm{y|x}}$ thetrue label posterioristhen computed as $P_{\\mathrm{y|x,\\tilde{y}}}\\propto\\dot{P}_{\\mathrm{y|x}}\\cdot P_{\\widetilde{\\mathbf{y}}|\\mathrm{y,x}}$ by Bayes's theorem [10, 13]. This estimated true label posterior is often used to infer the underlying true labels or to weight the loss functions [7, 9, 10, 14]. ", "page_idx": 1}, {"type": "text", "text": "However, accurately computing the posterior of the true label is challenging, and the estimated posterior $P_{\\mathrm{y|x,\\widetilde{y}}}$ may deviate from the underlying true distribution $P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}^{*}$ due to potential misspecifications in the prior belief and the conditional noise transition probabilities [15]. To address this issue, we introduce a robust scheme for handling crowdsourced noisy labels through conditional distributionally robust optimization (CDRO), as discussed in [16]. Specifically, we frame the problem as minimizing the worst-case risk within a distance-based ambiguity set, which constrains the degree of conditional distributional uncertainty around a reference distribution. By leveraging the strong duality in linear programming, we derive the dual form of the robust risk and establish informative upper bounds for the worst-case risk. Additionally, for each data point, we develop an analytical solution to the robust risk minimization problem, which encompasses existing approaches as special cases [9]. This solution is presented in a likelihood ratio format and inspires a robust approach that assigns pseudo-labels only to instances with high confidence, with uncertain data filtered out. These pseudo-labels also enable us to construct a pseudo-empirical distribution that serves as a robust reference probability distribution in CDRO under potential model misspecifications. Moreover, we derive a closed-form expression for the empirical robust risk by identifying the optimal Lagrange multiplier in the dual form. Building on this, we ultimately develop an algorithm for learning from noisy labels via conditional distributionally robust true label posterior with an adaptive Lagrange multiplier (AdaptCDRP). ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: (1) We formulate learning with noisy labels as a CDRO problem and develop its dual form to tackle the challenge of potential misspecification in estimating the true label posterior from noisy data. (2) We derive an analytical solution to the dual problem for each data point, and propose a novel algorithm that constructs a robust reference distribution for this problem. (3) By deriving the optimal Lagrange multiplier for the empirical robust risk, we develop an efficient one-step update method for the Lagrange multiplier, allowing for a principled balance between robustness and model fitting. Code is available at https://github.com/hguo1728/AdaptCDRP. ", "page_idx": 1}, {"type": "text", "text": "Notations. We use $[k]$ to denote $\\{1,\\ldots,k\\}$ for any positive integer $k$ ,_and $\\mathbf{1}(\\cdot)$ to denote the indicator function. For a vector $\\pmb{v}$ \uff0c $v_{j}$ stands for its $j$ th element, and $\\pmb{v}^{\\top}$ denotes its transpose. For $\\pmb{v}\\,=\\,(v_{1},...,v_{p})^{\\top}$ and $q\\,\\in\\,[1,+\\infty]$ the $L^{q}$ norm is defined as $\\|\\pmb{v}\\|_{q}\\,=\\,(\\sum_{j=1}^{p}|v_{j}|^{q})^{1/q}$ if $1\\leq q<\\infty$ , and $\\|\\pmb{v}\\|_{\\infty}=\\operatorname*{max}_{j}|v_{j}|$ if $q=+\\infty$ . For a matrix $V$ , we use $V_{i,j}$ to represent its $(i,j)$ element. Furthermore, let $(\\Omega,{\\bar{\\mathcal{G}}},\\mu)$ denote the measure space under consideration, where $\\Omega$ is a set, $\\mathcal{G}$ is the $\\sigma$ -field of subsets of $\\Omega$ , and $\\mu$ is the associated measure. For $q>0$ , let $L^{q}(\\mu)$ represent the collection of Borel-measurable functions $f:\\Omega\\to\\mathbb{R}$ such that $\\int|f|^{q}d\\mu<\\infty$ . Let $\\mathsf{d}(\\cdot,\\cdot)$ denote a metric on $\\Omega$ . We call $f\\;L$ -Lipschitz with respect to ${\\mathsf{d}}(\\cdot,\\cdot)$ if $|f(\\bar{u_{1}})-f(u_{2})|\\leq L\\cdot\\mathsf{d}(u_{1},u_{2})$ for all $u_{1},u_{2}\\in\\Omega$ , where $L$ is a positive constant. ", "page_idx": 1}, {"type": "text", "text": "2 Proposed Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a classification task with feature space $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and label space $\\boldsymbol{\\wp}$ , where $d$ is the feature dimension. Here $\\boldsymbol{\\wp}$ is taken as $\\{0,1\\}$ for binary classification and $[K]$ for multi-class classification with $K>2$ . Let $\\mathbf{X}\\in\\mathcal{X}$ denote an instance and $\\textrm{Y}\\in\\mathcal{V}$ denote its true label. Let $\\Psi$ denote the considered hypothesis class consisting of functions $\\psi$ defined over $\\mathcal{X}$ , which, for example, can be neural networks that output predicted label probabilities for each $\\mathbf{x}\\in\\mathcal{X}$ . Specifically, for binary classification, $\\psi:\\mathcal{X}\\to[0,1]$ ,with $\\psi(\\mathbf{x})$ representing $P(\\mathbf{Y}=1|\\mathbf{X}=\\mathbf{x})$ , and the classified value is given by ${\\bf1}(\\psi({\\bf x})>0.5)$ . For multi-class classification, $\\psi:\\mathcal{X}\\to\\Delta^{K-1}$ with $K>2$ and $\\Delta^{K-1}$ denoting the $K$ -simplex, where the $j$ th component of $\\psi(\\mathbf{x})$ , denoted $\\psi(\\mathbf{x})_{j}$ , represents the conditional probability $P(\\mathrm{Y}=j|\\mathbf{X}=\\mathbf{x})$ for $j\\in[K]$ , with the classified value defined as arg $\\operatorname*{max}_{j\\in[K]}\\psi(\\mathbf{x})_{j}$ ", "page_idx": 2}, {"type": "text", "text": "In applications, the true label $\\mathrm{Y}$ is often unobserved, and instead, a set of crowdsourced noisy labels $\\widetilde{\\mathbf{Y}}\\triangleq\\{\\widetilde{\\mathbf{Y}}^{(r)}\\}_{r=1}^{R}$ is collected, where $\\widetilde{\\mathrm{Y}}^{(r)}\\in\\mathcal{D}$ denoting the label provided by annotator $r$ out of $R$ annotators. Let $\\overline{{\\cal D}}\\triangleq\\{{\\bf X}_{i},\\widetilde{\\bf Y}_{i}\\}_{i=1}^{n}$ denote the observed data of size $n$ , where $\\widetilde{\\mathbf{Y}}_{i}$ contains noisy labels provided by $R$ annotators for instance $\\mathbf{X}_{i}$ , which may differ from the true label $\\mathrm{Y}_{i}$ for each $\\overline{{\\boldsymbol{i}}}\\in[n]$ Our goal is to train a classifier using $\\mathcal{D}$ to accurately predict the true label for a future instance. ", "page_idx": 2}, {"type": "text", "text": "A common assumption in supervised learning is that the data points $\\{\\mathbf{X}_{i},\\mathrm{Y}_{i},\\widetilde{\\mathbf{Y}}_{i}\\}$ for $i\\in[n]$ are independently drawn from a probability measure $P_{\\mathbf{x},\\mathbf{y},\\widetilde{\\mathbf{y}}}^{*}$ for $\\{\\mathbf{X},\\mathbf{Y},\\widetilde{\\mathbf{Y}}\\}$ , defined over the space $\\mathcal{Z}\\triangleq\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Y}^{R}$ . Under this assumption, many existing methods aim to approximate the posterior distribution of the underlying true label Y, given the observed data $\\mathbf{X}$ and $\\widetilde{\\mathbf Y}$ [7, 9-11]. The estimated true label posterior, denoted $P_{\\mathrm{y|x,\\widetilde{y}}}$ , is then applied to either infer the true labels or to weight the loss functions. For example, [9] utilized $P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}$ as a weight in the loss functions, without considering potential misspecification of the associated model. However, such strategies typically ignore the variability induced in estimating the true label posterior. ", "page_idx": 2}, {"type": "text", "text": "To mitigate the effects of potential misspecifications, we propose a conditional distributionally robust risk optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\psi\\in\\Psi}\\mathsf{R}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}),\\ \\mathrm{with}\\ \\mathsf{R}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\triangleq\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\Big[\\operatorname*{sup}_{Q_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\in\\Gamma_{\\epsilon}(P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})}\\mathbb{E}_{Q_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\left\\{\\ell(\\psi(\\mathbf{X}),\\mathrm{Y})\\right\\}\\Big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell(\\cdot,\\cdot)$ is a loss function, the expectation $\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}$ is taken with respect to the joint distribution of the observed data $\\mathbf{X}$ and $\\widetilde{\\mathbf Y}$ and the expectation $\\mathbb{E}_{Q_{\\mathrm{y}\\,|\\,{\\bf x},\\widetilde{{\\bf y}}}}$ is evaluated under the conditional distribution model, denoted $Q_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}$ , of the true label $\\mathrm{Y}$ , given $\\mathbf{X}$ and $\\widetilde{\\mathbf Y}$ . Here, $\\Gamma_{\\epsilon}(P_{\\mathrm{y|x,\\widetilde{y}}})$ is an ambiguity set of probability measures centered around the reference probability distribution $P_{\\mathrm{y|x,\\widetilde{y}}}$ , indexed by $\\epsilon>0$ [16-18]. For instance, $\\Gamma_{\\epsilon}(P_{\\mathrm{y|x,\\widetilde{y}}})$ can be conceptualized as a \u201cbal\" with $P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}$ at its center and $\\epsilon$ as te radius, where elements in the bll represent possible distribution models for $P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}^{*}$ , and the distance between two points is measured using a standard metric for distributions. Specifically, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Gamma_{\\epsilon}(P_{\\mathrm{y|x,\\widetilde{\\mathbf{y}}}})=\\left\\{Q_{\\mathrm{y|x,\\widetilde{\\mathbf{y}}}}\\in\\mathcal{P}(\\mathcal{Y}):\\mathcal{A}(Q_{\\mathrm{y|x,\\widetilde{\\mathbf{y}}}},P_{\\mathrm{y|x,\\widetilde{\\mathbf{y}}}})\\leq\\epsilon\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{P}(\\mathcal{Y})$ denotes all Borel probability measures on $\\boldsymbol{\\wp}$ and $\\mathcal{d}$ is a discrepancy metric of probability measures. In this paper, we employ the Wasserstein distance in Definition 2.1 to define the ambiguity set. By taking the supremum in (1) over the ambiguity set (2), we aim to minimize the worst-case risk around the reference distribution, thereby mitigating the impact of potential model misspecifications. ", "page_idx": 2}, {"type": "text", "text": "One main obstacle in solving (1) is constructing a reliable reference distribution $P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}$ Which typically depends on an empirical distribution that requires true labels in conventional distributionally robust optimization (DRO). We address this issue by investigating the dual form of the robust risk presented in (1), which enables us to create a robust pseudo-empirical distribution using a likelihood ratio test, as detailed in Section 3.1. An additional advantage of our approach is that it provides informative upper bounds for the worst-case risk in Section 2.3 via the dual formulation. ", "page_idx": 2}, {"type": "text", "text": "Remark 2.1. For simplicity in theoretical presentation, we assume access to all annotations from all $R$ annotators. However, the theoretical framework presented in this paper is applicable to both single-annotator ( $\\!\\,R=1\\!\\,,$ ) and multiple-annotator ( $[R>1]$ ) scenarios. In our experiments in Section 4, we also consider the scenario of sparse labeling, where we generate a total of $R$ annotators and then randomly select one annotation per instance from these $R$ annotators. We also conduct experiments with varying numbers of annotators for a comprehensive analysis. ", "page_idx": 2}, {"type": "text", "text": "2.2  Duality Result and Relaxed Problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To derive the pseudo-label generation algorithm and establish a reliable reference distribution, we analyze the dual form of (1). We first define the Wasserstein distance of order $p$ for $p\\in[1,+\\infty)$ ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 ( $p$ -Wasserstein distance, [17]). For a Polish space $\\boldsymbol{S}$ (i.e., a complete separable metric space) endowed with a metric $c:S\\times S\\to\\mathbb{R}_{\\geq0}$ , also called a cost function, let ${\\mathcal{P}}(S)$ represent the set of all Borel probability measures on $\\boldsymbol{S}$ ,where $\\mathbb{R}_{>0}$ represents the set of all nonnegative real values. For $p\\geq1$ , let $\\mathcal{P}_{p}(S)$ stand for the subset of $\\bar{\\mathcal{P}}(\\mathcal{S})$ with finite $p$ th moments. Then, for $P_{1},P_{2}\\in\\mathcal{P}_{p}(S)$ , the Wasserstein distance of order $p$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{p}(P_{1},P_{2})\\triangleq\\operatorname*{inf}_{\\Pi\\in\\mathrm{Cpl}(P_{1},P_{2})}\\big[\\mathbb{E}_{(S_{1},S_{2})\\sim\\Pi}\\left\\{c^{p}(S_{1},S_{2})\\right\\}\\big]^{1/p},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{Cpl}(P_{1},P_{2})$ comprises all probability measures on the product space $s\\times s$ such that their marginal measures are $P_{1}(\\cdot)$ and $P_{2}(\\cdot)$ . Here, $c^{p}(\\cdot,\\cdot)$ represents $\\{c(\\cdot,\\cdot)\\}^{p}$ ", "page_idx": 3}, {"type": "text", "text": "In (2), we set $d(\\cdot,\\cdot)$ as the $p$ Wasserstein distance and incorporate the constraint $d(Q_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}},P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\leq$ $\\epsilon$ using the Lagrange formulation, and then establish the strong duality result for (1) as follows. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1 (dual problem). Assume that for every given $\\textbf{x}\\in\\mathbf{\\Sigma}\\mathcal{X}$ $\\widetilde{\\textbf{y}}\\in\\mathcal{V}^{R}$ and $\\psi\\;\\in\\;\\Psi$ $\\ell(\\psi(\\mathbf{\\bar{x}}),\\cdot)\\in L^{1}(P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ where $L^{1}(\\cdot)$ is defined in Section $^{\\,l}$ . Consider $d(\\cdot,\\cdot)$ in (2) as the Wasserstein distance of order $p$ Then, for any $\\epsilon>0$ $\\mathsf{R}_{\\epsilon}\\bigl(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\bigr)$ in $(I)$ becomes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{R}_{\\epsilon}(\\psi;P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})=\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\left\\{\\operatorname*{inf}_{\\gamma\\geq0}\\left(\\gamma\\epsilon^{p}+\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\left[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\{\\ell(\\psi(\\mathbf{X}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\}\\right]\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To avoid solving nested optimization problems, we consider an alternative formulation by swapping the infimum and the first expectation operations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\tilde{y}}})\\triangleq\\operatorname*{inf}_{\\gamma\\geq0}\\mathbb{E}_{\\mathbf{x},\\tilde{\\mathbf{y}}}\\left(\\gamma\\epsilon^{p}+\\mathbb{E}_{P_{\\mathrm{y|x,\\tilde{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\{\\ell(\\psi(\\mathbf{X}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\}\\Big]\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is an upper bound of $\\mathsf{R}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ according to Proposition 2.1, and hence, (4) can be regarded as an relaxation of (3). The empirical counterpart of $\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\tilde{y}}})$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{R}}_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\widetilde{y}}})\\triangleq\\operatorname*{inf}_{\\gamma\\geq0}\\mathbb{E}_{P_{\\mathbf{x},\\widetilde{y}}^{(n)}}\\left(\\gamma\\epsilon^{p}+\\mathbb{E}_{P_{\\mathrm{y|x,\\widetilde{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\{\\ell(\\psi(\\mathbf{X}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\}\\Big]\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{P_{\\mathbf{x},\\widetilde{\\mathbf{y}}}^{(n)}\\triangleq\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i}}}\\end{array}$ $(\\mathbf{X},\\widetilde{\\mathbf{Y}})$ $\\mathcal{D}$ defined in Section 2.1. Here, for any $\\mathbf{v}\\in\\mathcal{X}\\times\\mathcal{Y}^{R}$ $\\delta_{\\mathbf{v}}$ represents the Dirac measure on $\\mathcal{X}\\times\\mathcal{Y}^{R}$ , defined as $\\delta_{\\mathbf{v}}(A)\\triangleq\\mathbf{1}\\{\\mathbf{v}\\in A\\}$ for any $A\\subset\\mathcal{X}\\times\\mathcal{Y}^{R}$ ", "page_idx": 3}, {"type": "text", "text": "Remark 2.2. The Lagrange multiplier $\\gamma$ in (4) and (5) captures the trade-off between robustness and model fitting in the presence of label noise and potential model misspecifications. When the solution in $\\gamma$ is large, the inner supremum tends to favor $\\mathrm{y^{\\prime}=Y}$ , thus encouraging the minimization of the natural risk using the reference distribution directly. In contrast, a small solution in $\\gamma$ introduces perturbations to the data, pushing the classifier away from the sample instances weighted by the reference distribution. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.3. When $p=1$ , (5) represents the dual form of the following problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{Q_{\\mathbf{y}\\mid\\mathbf{x},\\tilde{\\mathbf{y}}}\\in\\overline{{\\Gamma}}_{\\epsilon}(P_{\\mathbf{y}\\mid\\mathbf{x},\\tilde{\\mathbf{y}}})}\\mathbb{E}_{P_{\\mathbf{x},\\tilde{\\mathbf{y}}}^{(n)}}\\left[\\mathbb{E}_{Q_{\\mathbf{y}\\mid\\mathbf{x},\\tilde{\\mathbf{y}}}}\\left\\{\\ell(\\psi(\\mathbf{X}),\\mathrm{Y})\\right\\}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\overline{{\\Gamma}}_{\\epsilon}(P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})=\\left\\{Q_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\in\\mathcal{P}(\\mathcal{Y}):\\mathbb{E}_{P_{\\mathbf{x},\\widetilde{\\mathbf{y}}}^{(n)}}d(Q_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}},P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\leq\\epsilon\\right\\}$ The proof of thistatement is deferred to Appendix A.3. This result indicates that the empirical robust risk in the relaxed problem (5) corresponds to the worst-case risk within an ambiguity set that constrains the size of the average conditional distributional uncertainty. ", "page_idx": 3}, {"type": "text", "text": "2.3  Generalization Bounds ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "With the duality result in Proposition 2.1 and the derivations of the alternative formulations (4) and (5), we now characterize the difference between $\\widehat{\\mathfrak{R}}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ and itspopulation counterpart $\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\tilde{y}}})$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.2. Consider the loss function $\\ell(\\cdot,\\cdot)$ in Proposition 2.1, and let the cost function $c(y,y^{\\prime})=$ $\\kappa\\mathbf{1}(y\\neq y^{\\prime})$ for y, $y^{\\prime}\\in\\mathcal{V}$ where $\\kappa$ is a positive constant. Assume that there exists a positive constant $M$ such that $\\ell(\\psi(\\mathbf{x}),\\mathrm{y})\\,\\in\\,[0,M]$ for all $\\mathbf{x}\\in\\mathcal{X},\\;\\mathbf{y}\\in\\mathcal{Y},$ \uff1aand $\\psi\\,\\in\\,\\Psi$ ,and that $\\ell(\\psi({\\bf x}),{\\bf y})$ is $L$ Lipschitz in the second argument with respect to the cost function $c(\\cdot,\\cdot)$ .Then, there exists a positive constant $C_{1}$ such that for any given $\\epsilon>0$ \uff0c $\\psi\\in\\Psi$ and $0<\\eta<1$ with probability at least $1-\\eta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left|\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\widehat{\\mathfrak{R}}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\right|\\leq\\frac{C_{1}L\\kappa^{p}}{\\epsilon^{p-1}\\sqrt{n}}+M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2 sugests that the empirical counterpart, $\\widehat{\\mathfrak{R}}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ , is a useful approximation for the risk function $\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\tilde{y}}})$ , as their disparity is bounded and cannot grow indefinitely large. For a finite sample size $n$ , this disparity is upper bounded by a finite value depending on the characteristics of the cost and loss functions, as reflected by $\\kappa,L$ , and $M$ . As the sample size $n\\to\\infty$ , the difference tends to zero with high probability, and specifically, the difference is of order $O(n^{-1/2})$ ", "page_idx": 4}, {"type": "text", "text": "Next, we establish an informative bound for the empirical robust risk minimizer. For $\\psi_{1},\\psi_{2}\\in\\Psi$ and for any given norm $\\Vert\\cdot\\Vert$ ,let $\\begin{array}{r}{\\|\\psi_{1}-\\psi_{2}\\|_{\\infty}\\triangleq\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\psi_{1}(\\mathbf{x})-\\psi_{2}(\\mathbf{x})\\|}\\end{array}$ . Here, $\\Vert\\cdot\\Vert$ can be taken as any specific norms, including the $L^{q}$ norm with $q\\geq1$ that is defined in Section 1. ", "page_idx": 4}, {"type": "text", "text": "Corollary 2.3 (Empirical Robust Minimizer). Let $\\widehat{\\psi}_{\\epsilon,n}\\in\\operatorname*{inf}_{\\psi\\in\\Psi}\\widehat{\\mathfrak{R}}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ . Under the assumptions in Theorem 2.2, if we further assume that the loss function $\\ell(\\cdot,\\cdot)$ is $L^{\\prime}$ -Lipschitz in terms of the first argument with respect to the supremum metric $\\|\\cdot\\|_{\\infty},$ then there exists a positive constant $C_{2}$ such that for any $\\epsilon>0$ and $0<\\eta<1$ ,with probability at least $1-\\eta_{-}$ the empiricalrobust risk minimizer $\\widehat{\\psi}_{\\epsilon,n}$ satisfies: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R}_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\le\\mathfrak{R}_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})}\\\\ &{\\le\\operatorname*{inf}_{\\psi\\in\\Psi}\\mathfrak{R}_{\\epsilon}(\\psi;P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})+C_{2}\\left\\{\\frac{L\\kappa^{p}}{\\epsilon^{p-1}}+L^{\\prime}\\int_{0}^{\\infty}\\sqrt{\\log N(s;\\Psi,\\|\\cdot\\|_{\\infty})}d s\\right\\}\\cdot\\frac{1}{\\sqrt{n}}+2M\\sqrt{\\frac{\\log(1/\\eta)}{2n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N(s;\\Psi,\\|\\cdot\\|_{\\infty})$ denotes the $s$ -covering number of $\\Psi$ with respect to the supremum metric. ", "page_idx": 4}, {"type": "text", "text": "3  Implementation Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Section 3.1, we derive the analytical solution to the dual robust risk minimization problem in (5), which leads to the development of a novel approach for assigning pseudo-labels using the likelihood ratio test. These pseudo-labels facilitate the construction of a pseudo-empirical distribution, serving as a robust reference distribution in using (5). In Section 3.2, we derive the optimal value in $\\gamma$ for the empirical robust risk (5) and establish its closed-form expression. This analysis provides a principled framework for balancing the trade-off between robustness and model fitting and also motivates an efficient one-step update technique in solving the robust empirical risk minimization problem. ", "page_idx": 4}, {"type": "text", "text": "3.1  Optimal Solution for Single Data Point ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we determine the optimal value of $\\psi(\\mathbf{x})$ in (5) for a single data point $(\\mathbf{x},\\widetilde{\\mathbf{y}})$ .To simplify the analysis, we first focus on the binary classification problem with $\\bar{\\mathcal{D}}=\\{0\\bar{,}1\\}$ and consider a broad family of loss functions of the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(\\psi(\\mathbf{x}),\\mathrm{y})=(1-\\mathrm{y})\\mathcal{T}(1-\\psi(\\mathbf{x}))+\\mathrm{y}\\mathcal{T}(\\psi(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\psi(\\mathbf{x})$ represents the conditional distribution $P(\\mathbf{Y}=1|\\mathbf{X}=\\mathbf{x})$ as described in Section 2.1, $\\tau:[0,1]\\to\\mathbb{Z}$ is a bounded, decreasing, and twice differentiable function, and $\\mathcal{T}$ is a compact subset of $\\mathbb{R}$ ", "page_idx": 4}, {"type": "text", "text": "For any given $\\mathbf{x}\\in\\mathcal{X}$ and $\\widetilde{\\mathbf{y}}\\in\\mathcal{Y}^{R}$ , let $P_{j}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\triangleq P(\\mathrm{Y}=j|\\mathbf{X}=\\mathbf{x},\\widetilde{\\mathbf{Y}}=\\widetilde{\\mathbf{y}})$ for $j=0,1$ . With the loss function in (7) and the metric $c(\\cdot,\\cdot)$ considered in Theorem 2.2, minimizing (5) with respect to $\\psi\\in\\Psi$ becomes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\psi\\in\\Psi}{\\operatorname*{inf}}\\,\\underset{\\gamma\\geq0}{\\operatorname*{inf}}\\,\\big[\\gamma\\epsilon^{p}+P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\operatorname*{max}\\{\\mathcal{T}(1-\\psi(\\mathbf{x})),\\mathcal{T}(\\psi(\\mathbf{x}))-\\gamma\\kappa^{p}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,P_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\operatorname*{max}\\{\\mathcal{T}(1-\\psi(\\mathbf{x}))-\\gamma\\kappa^{p},\\mathcal{T}(\\psi(\\mathbf{x}))\\}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and let $\\psi^{\\star}$ denote the solution of (8). For $\\epsilon$ and $\\kappa$ described in Theorem 2.2, let $\\varrho(\\epsilon)\\triangleq\\epsilon^{p}/\\kappa^{p}$ .The following theorem shows that (8) has a closed-form solution, with its form varying based on whether $\\tau$ is concave or convex. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Optimal Action for Single Data Point: Binary Case). Let $\\mathbf{x}\\in\\mathcal{X}$ and $\\widetilde{\\mathbf{y}}\\in\\mathcal{Y}^{R}$ be given. Then, for a concave function $\\tau$ , the optimal solution for (8) is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi^{\\star}({\\bf x})=\\left\\{\\begin{array}{l l}{{j,\\;i f\\,P_{j}({\\bf x},{\\widetilde{\\bf y}})\\geq\\varrho(\\epsilon)+\\varpi_{1}\\;f o r\\,j=0,1;}}\\\\ {{1/2,\\;o t h e r w i s e,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\varpi_{1}=\\{{\\mathcal{T}}(0)-{\\mathcal{T}}(1/2)\\}/\\{{\\mathcal{T}}(0)-{\\mathcal{T}}(1)\\}\\in(0,1/2],$ : and for a convex function $\\tau$ the optimal solution of (8) is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi^{\\star}(\\mathbf{x})=\\left\\{\\begin{array}{l l}{j,\\ i f P_{j}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\geq\\varrho(\\epsilon)+\\varpi_{2}\\,f o r\\ j=0,1,}\\\\ {t_{j}^{\\ast},\\ i f\\varrho(\\epsilon)+1/2<P_{j}(\\mathbf{x},\\widetilde{\\mathbf{y}})<\\varrho(\\epsilon)+\\varpi_{2}\\,f o r\\ j=0,1,}\\\\ {1/2,\\ o t h e r w i s e,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\varpi_{2}\\;=\\;\\{{\\mathcal T}^{\\prime}(0)\\}/\\{{\\mathcal T}^{\\prime}(0)}\\,+{\\mathcal T}^{\\prime}(1)\\}\\;\\in\\;[1/2,1),$ $t_{0}^{\\ast}$ istheuniquesolutionof $\\{P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})~-$ $\\varrho(\\epsilon)\\}{^{\\mathcal{T}^{\\prime}}}(1\\,{-}\\,t)=\\{P_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})+\\varrho(\\epsilon)\\}{^{\\mathcal{T}^{\\prime}}}(t)\\,f o r\\,t$ $t\\overset{\\cdot}{\\in}(0,\\frac{1}{2})$ and $t_{1}^{*}$ is the unique solution of $\\{P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})+$ $\\varrho(\\epsilon)\\}T^{\\prime}(1-t)=\\{P_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})-\\varrho(\\epsilon)\\}T^{\\prime}(t)$ for $\\textstyle t\\in({\\frac{1}{2}},1)$ ", "page_idx": 5}, {"type": "text", "text": "Remark 3.1. The optimal solution $\\psi^{\\star}({\\bf x})$ in Theorem 3.1 can also be expressed in a likelihood ratio format, which naturally leads to a novel algorithm for assigning robust pseudo-labels. Specifically, when $\\tau$ is concave, the optimal solution can be expressed as: $\\psi^{\\star}(\\mathbf{x})=0$ if $P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})/\\bar{P}_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\stackrel{.}{\\geq}$ $\\mathcal{C}_{1}$ $\\psi^{\\star}(\\mathbf{x})\\,=\\,1$ if $P_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})/P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\,\\geq\\,\\mathcal{C}_{1}$ ; and $\\psi^{\\star}(\\mathbf{x})\\,=\\,1/2$ otherwise, where $\\mathcal{C}_{1}\\,\\triangleq\\,\\left(\\varrho(\\epsilon)\\,+\\right.$ $\\varpi_{1})/\\{1-(\\varrho(\\epsilon)+\\varpi_{1})\\}\\,>\\,1$ serves as a threshold for the likelihood ratio test. Consequently, for a data point $(\\mathbf{x},\\widetilde{\\mathbf{y}})$ if $P_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})/P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\,\\geq\\,\\mathcal{C}_{1}$ , we assign a robust pseudo-label $\\mathrm{y}^{\\star}=1$ ;if $P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}})/\\bar{P}_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\geq\\mathcal{C}_{1}$ , we assign $\\mathrm{y}^{\\star}=0$ . Leveraging the likelihood ratio format also facilitates extending the robust pseudo-label selection method to the multi-class case by considering pairwise comparisons.  Specifically, if $P_{k^{\\star}}(\\mathbf x,\\widetilde{\\mathbf y})/\\operatorname*{max}_{j\\neq k^{\\star}}P_{j}(\\mathbf x,\\widetilde{\\mathbf y})\\ \\geq\\ C_{1}.$ . we assign the pseudo-label $\\mathrm{y}^{\\star}=k^{\\star}$ to the instance. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.2. Existing pseudo-labeling methods [9, 19] typically identify the underlying true label as the one with the highest probability in the approximated true label posterior. In contrast, the proposed approach in Remark 3.1 considers both the highest and second-highest predicted probabilities. A pseudo-label is assigned only if the ratio of these probabilities exceeds a specified threshold. This strategy ensures that pseudo-labels are assigned to instances with high confidence, effectively filtering outuncertaindata. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.3. In the special case where $P_{j}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\propto\\tau_{j}(\\widetilde{\\mathbf{y}};\\mathbf{x})P_{j}(\\mathbf{x})$ , with $\\tau_{j}(\\widetilde{\\mathbf{y}};\\mathbf{x})=P^{*}(\\widetilde{\\mathbf{Y}}=\\widetilde{\\mathbf{y}}|\\mathrm{Y}=$ $j,\\mathbf{X}=\\mathbf{x}]$ ) denoting the noisy label transition probability and $P_{j}(\\mathbf{x})$ representing a proper prior for ${\\mathrm{Y}}=j$ conditional on $\\mathbf{x}$ for $j=0,1$ , previous studies have indicated the existence of a Chernoff information-type bound on the probability of error for robust pseudo-label selection, as described in Remark 3.1 [10, 20]. Specifically, for a fixed instance $\\mathbf{x}$ , let a pseudo-label $Y^{\\star}$ be generated as described in Remark 3.1, which depends on $\\mathbf{x}$ and the corresponding noisy label vector $\\widetilde{\\mathbf Y}$ . Consider the Bayes error, defined as $\\begin{array}{r}{\\Re_{\\mathrm{Bayes}}\\triangleq\\sum_{j=0,1}P_{j}(\\mathbf{x})P^{*}(\\Upsilon^{\\star}\\ne j|\\Upsilon=j,\\mathbf{x})}\\end{array}$ . According to Section 11.9 of [20], $\\Re_{\\mathrm{Bayes}}\\,\\leq\\,\\exp\\{-C(\\tau_{0}(\\cdot;\\mathbf{x}),\\bar{\\tau_{1}}(\\cdot;\\mathbf{x}))\\}$ , where $C(\\cdot,\\cdot)$ represents the Chernoff information between two distributions. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.4. In practice, one can use either uninformative priors, such as a uniform prior for each class, or informative priors derived from pre-trained or concurrently trained models for $P_{j}(\\mathbf{x})$ as discussed in Remark 3.3 [10, 13]. Moreover, the estimation of $P_{j}(\\mathbf{\\dot{x}},\\widetilde{\\mathbf{y}})$ is not limited to Bayes's rule. For example, [11] proposed aggregating data and noisy labei information by maximizing the $f$ -mutual information gain. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.5. Theorem 3.1 is developed based on the assumption that the function $\\tau$ is convex or concave. In our experiments, we use the cross-entropy loss for $\\ell$ , meaning $\\ T(t)=-\\log t$ for $t>0$ To meet the required conditions, we clip its input to $[0.01,1-0.01]$ to ensure $\\tau(\\cdot)$ remains bounded. ", "page_idx": 5}, {"type": "text", "text": "Next, we extend the preceding development for binary classification to multi-class scenarios with $K>2$ . Letting $\\tau(\\cdot)$ in (7) be specified as $\\begin{array}{r}{\\mathcal{T}(t)=1-t}\\end{array}$ , we extend loss function form (7) to facilitate the worst-case misclassification probability in multi-class scenarios: $\\begin{array}{r}{\\ell(\\psi(\\mathbf{x}),\\mathrm{y})\\,=\\,\\sum_{j=1}^{K}\\mathbf{1}(\\mathrm{y}\\,=\\,}\\end{array}$ $j)\\{1-\\psi(\\mathbf{x})_{j}\\}$ . For ease of presentation, we sometimes omit the dependence on $\\mathbf{x}$ and $\\widetilde{\\mathbf{y}}$ in the notation. Specifically, for $j\\in[K]$ ,we let $P_{j}\\triangleq P_{j}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\triangleq P(\\mathrm{Y}=j|\\mathbf{x},\\widetilde{\\mathbf{y}})$ and $\\psi_{j}\\triangleq\\psi(\\mathbf{x})_{j}$ . In a manner similar to deriving (8), given $\\mathbf{x}$ , minimizing (5) with respect to $\\psi(\\mathbf{x})$ can be expressed as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\psi\\in\\Psi}{\\operatorname*{inf}}\\;\\underset{\\gamma\\geq0}{\\operatorname*{inf}}\\;\\Big[\\gamma\\epsilon^{p}+\\displaystyle\\sum_{j=1}^{K}P_{j}\\operatorname*{max}\\{1-\\psi_{1}-\\gamma\\kappa^{p},\\dots,1-\\psi_{j-1}-\\gamma\\kappa^{p},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad1-\\psi_{j},1-\\psi_{j+1}-\\gamma\\kappa^{p},\\dots,1-\\psi_{K}-\\gamma\\kappa^{p}\\}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 (Optimal Action for Single Data Point: Multi-class Case). Let $\\{P_{1},...,P_{K}\\}$ be arranged in decreasing order, denoted $P^{(1)}\\ge\\ldots\\ge P^{(K)}$ , with the associated indexes denoted $\\chi(1),\\ldots,\\chi(K)$ . Let $\\psi^{\\star}$ denote the solution of the outer optimization problem in (9). For $j\\in[K]$ \uff0c let $\\psi^{\\star(j)}$ denote the $\\chi(j)$ -th component of $\\psi(\\mathbf{x})$ corresponding to $P^{(j)}$ Then, the elements of $\\psi^{\\star}$ are given as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(a).~H\\frac{1}{K}\\geq\\frac{1}{k}\\sum_{j=1}^{k}P^{(j)}-\\frac{1}{k}\\varrho(\\epsilon)f o r\\,a l l\\,k\\in[K-1],\\,t h e n\\,\\psi^{\\star(j)}=\\frac{1}{K}\\,f o r\\,a l l\\,j\\in[K].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 3.6. The robust pseudo-labeling method described in Remark 3.1 can also be extended from Theorem 3.2. Specifically, by Theorem 3.2, if $\\begin{array}{r}{P^{(1)}\\ge\\operatorname*{max}\\{\\frac{1}{K}+\\varrho(\\epsilon),P^{(2)}+\\varrho(\\epsilon)\\}}\\end{array}$ , then the optimal solution is: $\\psi^{\\star(1)}=1$ and $\\psi^{\\star(j)}=0$ for $j=2,\\dots,K$ , which can also be expressed in a likelihood ratio format and applied to assign robust pseudo-labels. ", "page_idx": 6}, {"type": "text", "text": "3.2 Closed-Form Robust Risk ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigate the empirical robust risk (5) by examining its closed form expression. For $i\\in[K]$ and $j\\in[K]$ ,welet $P_{i,j}\\triangleq P_{j}(\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i})\\triangleq P(\\mathrm{Y}=j|\\mathbf{X}=\\mathbf{x}_{i},\\widetilde{\\mathbf{Y}}=\\widetilde{\\mathbf{y}}_{i})$ and $\\psi_{i,j}\\triangleq\\psi(\\mathbf{x}_{i})_{j}$ .For simplicity, we denote the Wasserstein robust loss in (5) and the nominal loss respectively as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\widehat{\\mathfrak{R}}_{\\epsilon}=\\operatorname*{inf}_{\\gamma\\geq0}\\left[\\gamma\\epsilon^{p}+\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{K}P_{i,j}\\operatorname*{max}\\left\\{\\mathcal{T}(\\psi_{i,1})-\\gamma\\kappa^{p},\\ldots,\\mathcal{T}(\\psi_{i,j-1})-\\gamma\\kappa^{p}\\right.\\right.}\\\\ {\\displaystyle\\left.\\left.\\mathcal{T}(\\psi_{i,j}),\\mathcal{T}(\\psi_{i,j+1})-\\gamma\\kappa^{p},\\ldots,\\mathcal{T}(\\psi_{i,K})-\\gamma\\kappa^{p}\\right\\}\\right]\\mathrm{for}\\epsilon>0;}\\\\ {\\displaystyle\\widehat{\\mathfrak{R}}=\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{K}P_{i,j}\\mathcal{T}(\\psi_{i,j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For given $\\mathbf{x}_{i}$ , we sort $\\{\\psi_{i,1},\\ldots,\\psi_{i,K}\\}$ in decreasing order, denoted as $\\psi_{i}^{(1)}\\,\\geq\\,.\\,.\\,\\geq\\,\\psi_{i}^{(K)}$ . Let $\\alpha_{i,j}\\ \\triangleq\\ T(\\psi_{i}^{(K)})\\mathrm{~-~}T(\\psi_{i,j})$ for $i~\\in~[n]$ and $j~\\in~[K]$ , and sort $\\{\\alpha_{i,j}\\ :\\ i\\ \\in\\ [n],j\\ \\in\\ [K]\\}$ in decreasing order, denoted as $\\alpha^{(1)}\\;\\geq\\;.\\;.\\;\\geq\\;\\alpha^{(n K)}$ Correspondingly, the $P_{i,j}$ values with the associated indexes are denoted as $P^{(1)},\\dots,P^{(n K)}$ . For any $\\varrho(\\epsilon)$ , define an associated positive intger $s^{*}\\,\\in\\,[n K+1]$ a fllows:if $\\begin{array}{r}{\\frac{1}{n}P^{(1)}\\,<\\,\\varrho(\\epsilon)\\,<\\,\\frac{1}{n}\\sum_{t=1}^{n K}P^{(t)}}\\end{array}$ thn therexists $s^{*}\\,\\in\\,\\{2,\\dots,n K\\}$ such that $\\begin{array}{r}{\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}<\\varrho(\\epsilon)}\\end{array}$ for $s<s^{*}$ , and $\\begin{array}{r}{\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}\\,\\geq\\,\\varrho(\\epsilon)}\\end{array}$ for $s\\geq s^{*}$ ;if $\\varrho(\\epsilon)\\,\\leq\\,\\frac{1}{n}P^{(1)}$ \uff0c then $s^{*}$ is set as 1; if $\\begin{array}{r}{\\varrho(\\epsilon)\\geq\\frac{1}{n}\\sum_{t=1}^{n K}P^{(t)}}\\end{array}$ , then $s^{*}$ is set as $n K+1$ ", "page_idx": 6}, {"type": "text", "text": "Let $\\gamma_{\\psi}^{\\star}$ denote the optimal value of $\\gamma$ in $\\widehat{\\mathfrak{R}}_{\\epsilon}$ in (10), where its dependence on $\\epsilon$ is implicit, but its dependence on $\\psi$ is explicit. The following theorem presents this value, based on which we demonstrate that the Wasserstein robust loss $\\widehat{\\mathfrak{R}}_{\\epsilon}$ can be expressed as the nominal loss plus an aditional term $\\begin{array}{r}{\\frac{1}{n}\\sum_{t=1}^{s^{\\ast}-1}P^{(t)}\\alpha^{(t)}\\mathbf{1}(s^{\\ast}>1)}\\end{array}$ that prevents the lassier from becoming overly etain on the data. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Closed-Form Robust Risk). The optimal value of $\\gamma$ in $(I O)$ is given by $\\gamma_{\\psi}^{\\star}\\triangleq\\alpha^{(s^{\\ast})}/\\kappa^{p}$ and the resulting robust risk is expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{R}}_{\\epsilon}=\\widehat{\\mathfrak{R}}+\\frac{1}{n}\\sum_{t=1}^{s^{*}-1}P^{(t)}\\alpha^{(t)}\\mathbf{1}(s^{*}>1)+O\\left(\\frac{1}{n}\\right)\\alpha^{(s^{*})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 3.7. Theorem 3.3 shows that minimizing the Wasserstein robust loss $\\widehat{\\mathfrak{R}}_{\\epsilon}$ in (10) effectively minimizes the nominal loss $\\widehat{\\mathfrak R}$ in (11) while simultaneously penalizing terms associated with $\\left|\\alpha_{i}\\right|$ values exceeding a certain threshold $\\left|\\alpha^{s^{*}}\\right|$ , weighted by the corresponding reference probability values. This minimization prevents the classifier from becoming overly confident in certain data points, particularly when there are potential misspecifications in the approximated true label posterior. ", "page_idx": 7}, {"type": "text", "text": "Remark 3.8. As suggested by Remark 2.2, Theorem 3.3 provides a guideline for balancing robustness and model fitting by deriving the optimal value for $\\gamma$ in (10). In Section 3.3, we develop a one-step update method for determining $\\gamma_{\\psi}^{\\star}$ ", "page_idx": 7}, {"type": "text", "text": "3.3  Training using Conditional Distributionally Robust True Label Posterior ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we outline the steps for approximating the true label posterior, constructing the pseudo-empirical distribution as the reference distribution for solving the robust risk minimization problem (5), and subsequently training classifiers robustly. The pseudo code for the training process is provided in Algorithm 1 in Appendix B.1. Here we elaborate on the details. ", "page_idx": 7}, {"type": "text", "text": "Approximating noise transitions probabilities. We begin by warming up the classifiers on the noisy training data, denoted $\\breve{\\mathscr D}=\\{\\mathbf x_{i},\\breve{\\mathrm y_{i}}\\}_{i=1}^{n}$ ,where $\\check{\\mathrm{y}}_{i}$ represents the majority vote label for instance $\\mathbf{x}_{i}$ determined by the label that receives the highest number of votes from the annotators. After warming up the classifiers for 20-30 epochs, we sort the dataset by the cross-entropy loss values and collect a subset of size $m$ with the smallest $m$ losses, denoted as $\\mathcal{D}_{0}^{\\star}=\\{\\mathbf{x}_{i},\\check{\\mathbf{y}}_{i}\\}_{i=1}^{m}$ , where $m\\ll n$ .and the ratio of $m$ to $n$ is set to 1 minus the estimated noise rate. Next, we estimate the noise transition probabiesby $\\begin{array}{r}{\\widehat{\\tau}_{j}(\\widetilde{\\mathbf{y}})=\\sum_{i=1}^{m}\\mathbf{1}(\\widetilde{\\mathbf{y}}=\\widetilde{\\mathbf{y}}_{i},\\check{\\mathbf{y}}_{i}=j)/\\sum_{i=1}^{m}\\mathbf{1}(\\check{\\mathbf{y}}_{i}=j)}\\end{array}$ for $\\widetilde{\\mathbf{y}}\\,\\in\\,[K]^{R}$ and $j\\,\\in\\,[K]$ (Line 1 of Algorithm 1). With $\\widehat{\\tau}_{j}(\\widetilde{\\mathbf{y}})$ , we then iteratively update the approximated true label posterior, construct the pseudo-empirical distribution, and robustly train the classifiers (Lines 2-13 of Algorithm 1). Here, we employ the straightforward frequency-counting method for noise transition estimation for simplicity. However, our approach is versatile and can be integrated with various methods for estimating the noise transition matrices or the true label posterior. Additional experimental results using more advanced transition matrix estimation methods are provided in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Constructing a pseudo-empirical distribution. We train two classifiers, $\\psi^{(1)}$ and $\\psi^{(2)}$ , in parallel each serving as an informative prior for the other. In the tth epoch, the approximated true label posterior with prior $\\psi^{(\\iota)}$ is updated as $\\widehat{P}_{j}^{(\\iota)}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\triangleq\\widehat{P}^{(\\iota)}(\\mathrm{Y}=j|\\widetilde{\\mathbf{Y}}=\\widetilde{\\mathbf{y}},\\mathbf{X}=\\mathbf{x})\\propto\\psi_{j}^{(\\iota)}(\\mathbf{x})\\!\\cdot\\!\\widehat{\\tau}_{j}(\\widetilde{\\mathbf{y}})$ (Line 4 of Algorithm 1), where $\\psi_{j}^{(\\iota)}(\\mathbf{x})$ denotes the $j$ th element of the vector-valued function $\\psi^{(\\iota)}(\\mathbf{x})$ for $j\\in$ $[K]$ and $\\iota=1,2$ . As described in Remark 3.1, for $i\\in[n]$ if $\\widehat{P}_{k^{\\star}}^{(\\iota)}(\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i})/\\operatorname*{max}_{j\\neq k^{\\star}}\\widehat{P}_{j}^{(\\iota)}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\geq\\mathcal{C}$ for a pre-specified threshold $\\mathcal{C}>1$ , we assign the robust pseudo-label $\\mathrm{y}_{i}^{\\star}=k^{\\star}$ to the instance and collet it into $\\mathcal{D}_{t,\\iota}^{\\star}$ (Lines 5-7 of Algorithm 1). The pseudo-empirical distribution $P_{t,\\iota}^{\\star}$ is updated based on $\\mathcal{D}_{t,\\iota}^{\\star}$ (Line 8 of Algorithm 1). ", "page_idx": 7}, {"type": "text", "text": "Robustly training the classifiers. For $\\iota=1,2$ , let $\\backslash\\!\\triangleq1$ if $\\iota=2$ ; and $\\backslash\\!\\triangleq2$ if $\\iota=1$ .With the updated pseudo-empirical distribution, the classifier $\\psi^{(\\iota)}$ is then trained by minimizing the empirical robust risk (5) with the reference distribution $P_{t,\\backslash\\iota}^{\\star}$ (Line 9 of Algorithm 1). After updating the classier $\\psi^{(\\iota)}$ with $\\gamma_{t-1}^{(\\iota)}$ from the previous iteration, we tke one steptoupdate te $\\gamma$ value $\\gamma_{t}^{(\\iota)}$ In particular, as suggested by Theorem 3.3, we use $\\gamma_{0,t}=|\\alpha^{(s^{*})}|/\\kappa^{p}$ as a reference value for $\\gamma$ (Lines 11-12 f Algorithm 1). We then update $\\gamma_{t}^{(\\iota)}$ by minimizing $\\begin{array}{r}{\\left[\\gamma\\{\\epsilon^{p}-\\mathbb{E}_{P_{t,\\backslash,\\tau}^{\\star}}c^{p}(y^{\\prime},\\mathrm{Y})\\}+\\frac{\\lambda}{2}(\\gamma-\\gamma_{0})^{2}\\right]}\\end{array}$ (Line 13 of Algorithm 1) with respect to $\\gamma$ , where $y^{\\prime}$ is determined by (5) after updating $\\psi^{(\\iota)}$ , and $\\lambda>0$ is a positive constant that determines the learning rate of $\\gamma$ ", "page_idx": 7}, {"type": "text", "text": "4  Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and model architectures. We evaluate the performance of the proposed AdaptCDRP on two datasets, CIFAR-10 and CIFAR-100 [21], by generating synthetic noisy labels (details provided below), as well as four datasets, CIFAR-10N [22], CIFAR-100N [22], LabeIMe [23, 24], and Animal10N [25], which contain human annotations. For all datasets except LabelMe, we set aside $10\\%$ of the original data, together with the corresponding synthetic or human annotated noisy labels, to validate the model selection procedure. We use the ResNet-18 architecture [26] for CIFAR-10 and ", "page_idx": 7}, {"type": "table", "img_path": "2NKumsITFw/tmp/63fb7ffe3184dd8eb4b6cb9b5cdcf2ea012006c70b9d22ae386ebb5dc5523ae4.jpg", "table_caption": ["Table 1: Average accuracies (with associated standard errors expressed after the $\\pm$ signs) for learning the CIFAR-10 and CIFAR-100 datasets $(R=5)$ "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "CIFAR-10N, and the ResNet-34 architecture [26] for CIFAR-100 and CIFAR-100N. Following [27], we employ a pretrained VGG-16 model with a $50\\%$ dropout rate for the LabelMe dataset. In line with [25], the VGG19-BN architecture [28] is used for the Animal-10N dataset. Further details on the datasets and experimental setup are provided in Appendix B.1. ", "page_idx": 8}, {"type": "text", "text": "Noise generation. We generate synthetic annotations on the CIFAR-10 and CIFAR-100 datasets using Algorithm 2 from [29]. Three groups of annotators, labeled as IDN-LOW, IDN-MID, and IDN-HIGH, are considered, with average labeling error rates of approximately $20\\%$ $35\\%$ ,and $50\\%$ respectively, representing low, intermediate, and high error rates. Each group consists of $R\\,=\\,5$ annotators. To assess the algorithms in an incomplete labeling setting, we randomly select only one annotation per instance from the $R$ annotators for the training dataset rather than using all available annotations [1o]. Further details on noise generation are provided in Appendix B.1. ", "page_idx": 8}, {"type": "text", "text": "Comparison with SOTA methods. We compare our method with a comprehensive set of state-ofthe-art approaches, including: (1) CE (Clean) with clean labels; (2) CE (MV) with majority vote labels; (3) CE (EM) [7]; (4) Co-teaching [30]; (5) Co-teaching $^+$ [31]; (6) CoDis [32]; (7) LogitClip [33]; (8) DoctorNet [34]; (9) MBEM [9]; (10) CrowdLayer [27]; (11) TraceReg [12]; (12) Max-MIG [11]; (13) CoNAL [35]; and (14) CCC [36]. We report the average test accuracy over five repeated experiments, each with a different random seed, on synthetic datasets, CIFAR-10 and CIFAR-100, with instance-dependent label noise introduced at low, intermediate, and high error rates. Standard errors are shown following the plus/minus sign $(\\pm)$ , and the two highest accuraries are highlighted in bold. Table 2 presents evaluation results on four real-world datasets. As shown, our AdaptCDRP consistently outperforms competing methods across all scenarios. To further explore the impact of annotation sparsity, we conduct additional experiments with the number of annotators ranging from 5 to 100, with each instance labeled only once. Figure 1 illustrates the average accuracy across different numbers of annotators on CIFAR-10, highlighting the advantages of the proposed method under diverse settings. The results for the CIFAR-100 dataset are shown in Figure 3 in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "Hyper-parameter analysis. We investigate the impact of the hyperparameter $\\epsilon$ in the empirical robust risk (5). Under our experiment setup, $\\epsilon$ should be chosen within $(0,1/K)$ for a $K$ -class classification problem with $K\\geq2$ as demonstrated in the proof of Theorem 3.3 in Appendix A.8. Hence, we take $\\epsilon\\in(0,0.1)$ for CIFAR-10 and $\\epsilon\\in(0,0.01)$ for CIFAR-100, with the results presented in Figure 2. The results suggest that setting $\\epsilon$ near zero leads to relatively low test accuracies, highlighting the importance of CDRO under model specification when handling noisy labels. Furthermore, continually increasing $\\epsilon$ eventually results in a drop in accuracy due to excessive noise injection into the data. ", "page_idx": 8}, {"type": "text", "text": "Additional experimental results. To further evaluate the performance of the proposed method across various scenarios, we conducted additional experiments, detailed in Appendix B.2. Specifically, we compare different annotation aggregation methods, present average test accuracies and robust pseudo-label accuracies during training, assess sensitivity to the number of warm-up epochs, explore different noise transition estimation methods, and examine the impact of sparse annotation. ", "page_idx": 8}, {"type": "table", "img_path": "2NKumsITFw/tmp/410c0a4fbad775af5ddb185eaa90263a375efa0a9611747879c7f90ba23b6c60.jpg", "table_caption": ["Table 2: Average accuracies (with associated standard errors expressed after the $\\pm$ signs) for learning the CIFAR-10N, CIFAR100N, LabelMe, and Animal-10N datasets. "], "table_footnote": ["Ours (AdaptCDRP) 88.25\u00b10.34 53.42\u00b10.64 83.36\u00b10.68 83.08\u00b10.39 "], "page_idx": 9}, {"type": "image", "img_path": "2NKumsITFw/tmp/e6b82298d89721d023a5b6f31682bd40673c49090045e1b73b8ec8a944cc54d4.jpg", "img_caption": ["Figure 2: Average accuracy on the CIFAR-10 and CIFAR-100 datasets $[R=5]$ for different $\\epsilon$ values. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "2NKumsITFw/tmp/64303e8f0e561fbd62365aa9025addb559dbcf1496ad7b3d4122e7d371acca9a.jpg", "img_caption": ["Figure 1: Average test accuracy on the CIFAR-10 dataset with varying numbers of annotators. The shaded areas are constructed using the associated standard deviations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we address the challenge of learning from noisy annotations by estimating true label posteriors using the CDRO framework. We formulate the problem as minimizing the worst-case risk within a distance-based ambiguity set, which constrains the conditional distributional uncertainty around a reference distribution. By deriving the dual form of the worst-case risk and finding the analytical solution to the robust risk minimization problem for each data point, we propose a novel approach for determining robust pseudo-labels using the likelihood ratio test. This approach further leads to the construction of a pseudo-empirical distribution that serves as a robust reference probability distribution in CDRO. We also derive a closed-form expression of the empirical robust risk and identify the optimal Lagrange multiplier for the dual problem. This leads to a guideline for balancing robustness and model fitting in a principled way and inspires an efficient one-step update method for the Lagrange multiplier. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Extensions.  Our development here does not focus on precisely estimating the noise transition matrix or the true label posterior. Further research may be conducted to address the sparse annotation problem and improve estimates of the true label posterior. This can be accomplished through several approaches: (1) employing regularization techniques to mitigate the impact of small sample sizes by smoothing estimates and reducing sensitivity to outliers; (2) leveraging subgroup structures among annotators to capture additional nuances; and (3) directly modeling the true label posterior by integrating both data and noisy label information, moving beyond the limitation of purely applyingBayes's rule. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yi is the Canada Research Chair in Data Science (Tier 1). Her research was supported by the Canada Research Chairs Program and the Natural Sciences and Engineering Research Council of Canada (NSERC). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT press, 2016.   \n[2]  Juirgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117, 2015.   \n[3]  Devansh Arpit, Stanistaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 233-242, 2017.   \n[4]  Raymond J Carroll, David Ruppert, Leonard A Stefanski, and Ciprian M Crainiceanu. Measurement Error in Nonlinear Models: A Modern Perspective. Chapman and Hall/CRC, 2006.   \n[5]  Grace Y Yi. Statistical Analysis with Measurement Error or Misclassification. Springer, 2017.   \n[6] Grace Y Yi, Aurore Delaigle, and Paul Gustafson. Handbook of Measurement Error Models. CRC Press, 2021.   \n[7]  Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society: Series C, 28(1):20-28, 1979.   \n[8]  Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier Movellan, and Paul Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems, volume 22, pages 2035-2043, 2009.   \n[9]  Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. arXiv preprint arXiv: 1712.04577, 2017.   \n[10]  Hui Guo, Boyu Wang, and Grace Y Yi. Label correction of crowdsourced noisy annotations with an instance-dependent noise transition model. In Advances in Neural Information Processing Systems, volume 36, pages 347-386, 2023.   \n[11]  Peng Cao, Yilun Xu, Yuqing Kong, and Yizhou Wang. Max-mig: an information theoretic approach for joint learning from crowds. arXiv preprint arXiv: 1905.13436, 2019.   \n[12]  Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11244-11253, 2019.   \n[13] Ravid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and Andrew G Wilson. Pre-train your loss: Easy bayesian transfer learning with informative priors. In Advances in Neural Information Processing Systems, volume 35, pages 27706-27715, 2022.   \n[14] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? In Advances in Neural Information Processing Systems, volume 32, pages 6838-6849, 2019.   \n[15] Hisham Husain and Jeremias Knoblauch. Adversarial interpretation of bayesian inference. In Proceedings of The 33rd International Conference on Algorithmic Learning Theory, volume 167, pages 553-572. Proceedings of Machine Learning Research, 2022.   \n[16]  Alexander Shapiro and Alois Pichler. Conditional distributionally robust functionals. Operations Research, 2023.   \n[17]  Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathematics of Operations Research, 44(2):565-600, 2019.   \n[18]  Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance. Mathematics of Operations Research, 48(2):603-655, 2023.   \n[19]  Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5552-5560, 2018.   \n[20]  Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, 2006.   \n[21]  Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.   \n[22]  Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In International Conference on Learning Representations, 2022.   \n[23]  Filipe Rodrigues, Mariana Lourenco, Bernardete Ribeiro, and Francisco C Pereira. Learning supervised topic models for classification and regression from crowds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2409-2422, 2017.   \n[24] Antonio Torralba, Bryan C. Russell and Jenny Yuen. Labelme: Online image annotation and applications. Proceedings of the IEEE, 98(8):1467-1484, 2010.   \n[25]  Hwanjun Song, Minseok Kim, and Jae-Gil Lee. SELFIE: Refurbishing unclean samples for robust deep leaingrocdingft6tIntationalConfrenconMachLeinol97,pa 5907-5915, 2019.   \n[26]  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 77O-778, 2016.   \n[27]  Filipe Rodrigues and Francisco Pereira. Deep learning from crowds. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, pages 1611-1618, 2018.   \n[28]  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.   \n[29] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. In Advances in Neural Information Processing Systems, volume 33, pages 7597-7610, 2020.   \n[30]  Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems, volume 31, pages 8527-8537, 2018.   \n[31]  Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against labelcorruption\uff1fIn Proceedings of the 36th Intenational Conference on Machine Learning, volume 97, pages 7164-7173, 2019.   \n[32]  Xiaobo Xia, Bo Han, Yibing Zhan, Jun Yu, Mingming Gong, Chen Gong, and Tongliang Liu. Combating noisy labels with sample selection by mining high-discrepancy examples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1833-1843, 2023.   \n[33]  Hongxin Wei, Huiping Zhuang, Renchunzi Xie, Lei Feng, Gang Niu, Bo An, and Yixuan Li. Mitigating memorization of noisy labels by clipping the model prediction. In Proceedings of the 4Oth International Conference on Machine Learning, volume 202, pages 36868-36886, 2023.   \n[34]  Melody Guan, Varun Gulshan, Andrew Dai, and Geoffrey Hinton. Who said what: Modeling individual labelers improves classification. In Proceedings of the AAAI conference on artificial intelligence, volume 32, pages 3109-3118, 2018.   \n[35]  Zhendong Chu, Jing Ma, and Hongning Wang. Learning from crowds by modeling common confusions. In Proceedings of the AAA1 Conference on Artificial Intelligence, volume 35, pages 5832-5840, 2021.   \n[36]  Hansong Zhang, Shikun Li, Dan Zeng, Chenggang Yan, and Shiming Ge. Coupled confusion correction: Learning from crowds with sparse annotations. In Proceedings of the AAAl Conference on Artijficial Intelligence, volume 38, pages 16732-16740, 2024.   \n[37]  David G Luenberger and Yinyu Ye. Linear and Nonlinear Programming, volume 2. Springer, 1984.   \n[38]  Stephen P Bradley, Arnoldo C Hax, and Thomas L Magnanti. Applied Mathematical Programming. Addison-Wesley Publishing Company, 1977.   \n[39] Martin J Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge University Press, 2019.   \n[40]  Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In Advances in Neural Information Processing Systems, volume 31, pages 2687-2696, 2018.   \n[41] John Hunter. The supremum and infimum. https: //www.math.ucdavis.edu/^hunter/m125b/ch2. pdf.   \n[42]  Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT Press, 2018.   \n[43]  Diederik P Kingma and Jimmy Ba. Adam:  A method for stochastic optimization.  arXiv preprint arXiv:1412.6980, 2014.   \n[44]  Hongwei Li and Bin Yu. Error rate bounds and iterative weighted majority voting for crowdsourcing. arXiv preprint arXiv:1411.4086, 2014.   \n[45]  Maria Sofia Bucarelli, Lucas Cassano, Federico Siciliano, Amin Mantrach, and Fabrizio Silvestri. Leveraging inter-rater agreement for classification in the presence of noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3439-3448, 2023.   \n[46] Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao Chen. Error-bounded correction of noisy labels. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 11447-11457, 2020.   \n[47]  Shahana Ibrahim, Tri Nguyen, and Xiao Fu. Deep learning from crowdsourced labels: Coupled crossentropy minimization, identifiability, and regularization. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "SUPPLEMENTARY MATERIAL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Technical Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Preliminaries about Linear Programming and Concentration Bounds ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A linear program (LP) is an optimization problem of the form ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\mathbf x}\\in\\mathbb R^{n}}{\\mathrm{max}}\\,{\\mathbf c}^{\\top}{\\mathbf x}}\\\\ &{\\;\\;\\;s.t.\\;\\;{\\mathbf A}{\\mathbf x}\\leq{\\mathbf b}}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;{\\mathbf x}\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{c}\\in\\mathbb{R}^{n}$ and $\\mathbf{b}\\in\\mathbb{R}^{m}$ are given, and A is a specified $m\\times n$ matrix. Here, $\"\\leq\"$ represents elementwise inequality for vectors. The expression $\\mathbf{c}^{\\top}\\mathbf{x}$ is called the objective function, and the set $\\{\\mathbf{x}\\in\\mathbb{R}^{n}:\\mathbf{Ax}\\leq\\mathbf{b},\\mathbf{x}\\geq0\\}$ defines the feasible region of the linear program. By introducing slack variables, any linear program can be converted to the following standard form: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x}\\in\\mathbb{R}^{n}}{\\mathrm{max}}\\,\\mathbf{c}^{\\top}\\mathbf{x}}\\\\ &{\\quad s.t.\\ \\ \\mathbf{Ax}=\\mathbf{b}}\\\\ &{\\quad\\quad\\quad\\mathbf{x}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We begin by introducing the concept of extreme point of related properties. ", "page_idx": 13}, {"type": "text", "text": "Definition A.1 ([37, Chapter 2]). A point $\\mathbf{z}$ in a convex set $\\Theta$ is called an extreme point of $\\Theta$ if there do not exist two distinct points $\\mathbf{z}^{\\prime},\\mathbf{z}^{\\prime\\prime}\\in\\Theta$ and a scalar $\\nu$ with $0<\\nu<1$ such that ${\\bf z}=\\nu{\\bf z}^{\\prime}+(1-\\nu){\\bf z}^{\\prime\\prime}$ ", "page_idx": 13}, {"type": "text", "text": "The following lemma shows that optimal solutions of a linear program are located among the extreme points. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1 ([37, Chapter 2). If a linear programming problem has a finite optimal solution (i.e., a feasible solution that optimizes the objective function), then there is a finite optimal solution that is anextremepoint of theconstraint set. ", "page_idx": 13}, {"type": "text", "text": "The following lemma on strong duality in linear programming will be used in the proof of Proposition 2.1 and Remark 2.3. ", "page_idx": 13}, {"type": "text", "text": "Lemma 2 (Duality Theorem of Linear Programming; [38, Chapter 4]). Let $\\mathbf{c}=(c_{1},\\hdots,c_{n})^{\\top}$ and $\\mathbf{b}=\\left(b_{1},\\ldots,b_{m}\\right)^{\\dagger}$ be given vectors, and let $\\mathbf{A}=\\left[a_{i j}\\right]$ be a given $m\\times n$ matrix with $a_{i j}$ being its $(i,j)$ element. Define the primal problem as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{max}_{x_{1},\\ldots,x_{n}\\in\\mathbb{R}}\\,\\mathsf{P},\\,w i t h\\,\\mathsf{P}\\triangleq\\sum_{j=1}^{n}c_{j}x_{j},}\\\\ {\\displaystyle s.t.\\,\\sum_{j=1}^{n}a_{i j}x_{j}\\leq b_{i}\\,\\,f o r\\,\\,i\\in[m],}\\\\ {\\displaystyle x_{j}\\geq0\\,\\,f o r\\,\\,j\\in[n],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "or equivalently written in compact form: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{max}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\,\\mathsf{P},\\,w i t h\\,\\mathsf{P}\\triangleq\\mathbf{c}^{\\top}\\mathbf{x}\\,a n d\\,\\mathbf{x}=(x_{1},\\ldots,x_{n})^{\\top},}\\\\ {\\mathit{s.t.}\\,\\mathbf{Ax}\\ge\\mathbf{b},}\\\\ {\\mathbf{x}\\ge0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The corresponding dual linear problem is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{y_{1},\\dots,y_{m}\\in\\mathbb{R}}~\\mathsf{D},~w i t h\\textsf{D}\\triangleq\\sum_{i=1}^{m}b_{i}y_{i},}\\\\ {\\displaystyle s.t.~\\sum_{i=1}^{m}a_{i j}y_{i}\\geq c_{j}~f o r~j\\in[n],}\\\\ {\\displaystyle y_{i}\\geq0~f o r~i\\in[m],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "or equivalently, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{\\mathbf{y}\\in\\mathbb{R}^{m}}\\;\\mathsf{D},\\;w i t h\\;\\mathsf{D}\\triangleq\\mathbf{y}^{\\top}\\mathbf{b}\\;a n d\\;\\mathbf{y}=(y_{1},\\ldots,y_{m})^{\\top},}\\\\ {\\displaystyle s.t.\\;\\mathbf{y}^{\\top}\\mathbf{A}\\leq\\mathbf{c}^{\\top},}\\\\ {\\displaystyle\\mathbf{y}\\geq0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If the primal (or dual) problem has a finite optimal solution, then the dual (or primal) problem also has a finite solution, and the optimal values of the primal and dual problems are equal. ", "page_idx": 14}, {"type": "text", "text": "Next we introduce a concentration bound along with associated concepts, which will be used in the proof of Theorem 2.2. ", "page_idx": 14}, {"type": "text", "text": "Let $\\Omega$ denote a subset of $\\mathbb{R}$ and $f:\\Omega^{n}\\to\\mathbb{R}$ . We say that a function $f$ satisfies the bounded difference inequality [39] with parameters $\\{L_{1},\\ldots,L_{n}\\}$ if for any $k\\in[n]$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{s_{1},\\ldots,s_{n},s_{k}^{\\prime}\\in\\Omega}|f(s_{1},\\ldots,s_{k},\\ldots,s_{n})-f(s_{1},\\ldots,s_{k}^{\\prime},\\ldots,s_{n})|\\leq L_{k}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "That is, for any $k\\in[n]$ , if we substitute $s_{k}$ with $s_{k}^{\\prime}$ while keeping other $s_{j}$ fixed for all $j\\neq k$ the function $f$ changes by at most $L_{k}$ ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Bounded Differences Inequality; [39, Corollary 2.21]). Let $\\mathbf{\\calS}~=~(S_{1},\\ldots,S_{n})^{\\top}$ represent a randomvector withindependent components defined on a sample space $\\Omega^{n}$ \uff0cand $f(S)\\ \\triangleq\\ f(S_{1},\\ldots,S_{n})$ for a function $f\\,:\\,\\Omega^{n}\\,\\rightarrow\\,\\mathbb{R}$ Suppose that $f$ satisfies the bounded differencepropertywithparameters $\\{L_{1},\\ldots,L_{n}\\}$ . Then, for any $t\\geq0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[f(\\pmb{S})-\\mathbb{E}\\left\\{f(\\pmb{S})\\right\\}\\ge t\\right]\\le\\exp\\left\\{-\\frac{2t^{2}}{\\sum_{i=1}^{n}L_{i}^{2}}\\right\\};}\\\\ &{\\mathbb{P}\\left[f(\\pmb{S})-\\mathbb{E}\\left\\{f(\\pmb{S})\\right\\}\\le-t\\right]\\le\\exp\\left\\{-\\frac{2t^{2}}{\\sum_{i=1}^{n}L_{i}^{2}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We also introduce the following definitions and lemmas, which will be used to characterize the complexity of the function class. ", "page_idx": 14}, {"type": "text", "text": "Definition A.2 (Covering number; [39, Definition 5.1]). Let $\\Theta$ denote a set and $\\rho$ a metric on $\\Theta$ . For $t>0$ ,a $t$ -cover of $\\Theta$ with respect to $\\rho$ is a set $\\{\\theta_{i}\\in\\Theta:i=1,\\ldots,N\\}$ such that for each $\\theta\\in\\Theta$ there exists some $i\\in[N]$ such that $\\rho(\\theta,\\theta_{i})\\leq t$ .The $t$ -coveringnumber $N(t;\\Theta,\\rho)$ is the cardinality of the smallest $t$ -cover. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Let $\\Theta_{j}$ be a set equipped with ametric $\\rho_{j}$ for $j\\,=\\,1,2$ and define $\\Theta\\,=\\,\\Theta_{\\mathrm{1}}\\times\\Theta_{\\mathrm{2}}$ Given $\\alpha_{1},\\alpha_{2}\\,>\\,0$ define the metric $\\rho$ on $\\Theta$ as $\\rho(\\theta,\\theta^{\\prime})\\,\\triangleq\\,\\alpha_{1}\\rho_{1}(\\theta^{1},\\theta^{1^{\\prime}})+\\alpha_{2}\\rho_{2}(\\theta^{2},\\theta^{2^{\\prime}})$ for any $\\theta\\triangleq(\\theta^{1},\\theta^{2})\\in\\Theta$ and $\\theta^{\\prime}\\triangleq(\\theta^{1^{\\prime}},\\theta^{2^{\\prime}})\\in\\Theta$ Then for $t>0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nN(t;\\Theta,\\rho)\\leq N(t/(2\\alpha_{1});\\Theta_{1},\\rho_{1})\\times N(t/(2\\alpha_{2});\\Theta_{2},\\rho_{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For $j=1,2$ let $\\bar{\\Theta}_{j}\\triangleq\\{\\theta_{1}^{j},\\cdot\\cdot\\cdot,\\theta_{N_{j}}^{j}\\}$ denote the smallest $t/(2\\alpha_{j})$ -cover of $\\Theta_{j}$ with respect to $\\rho_{j}$ . Then, by Definition A.2, $N(t/(2\\alpha_{j});\\Theta_{j},\\rho_{j})=N_{j}$ for $j=1,2$ . For any $\\theta=(\\theta^{1},\\theta^{2})\\in\\Theta$ by definition A.2, there exists $i_{1}~\\in~[N_{1}]$ and $i_{2}~\\in~[N_{2}]$ such that $\\rho_{1}(\\theta^{1},\\theta_{i_{1}}^{1})\\,\\,\\leq\\,t/(2\\alpha_{1})$ and $\\rho_{2}(\\theta^{2},\\theta_{i_{2}}^{2})\\leq t/(2\\alpha_{2})$ Then, $\\theta_{i}\\triangleq(\\theta_{i_{1}}^{1},\\theta_{i_{2}}^{2})\\in\\bar{\\Theta}_{1}\\times\\bar{\\Theta}_{2}\\subset\\Theta$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho(\\theta,\\theta_{i})=\\alpha_{1}\\rho_{1}(\\theta^{1},\\theta_{i_{1}}^{1})+\\alpha_{2}\\rho_{2}(\\theta^{2},\\theta_{i_{2}}^{2})\\leq t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, by Definition A.2, $\\bar{\\Theta}_{1}\\times\\bar{\\Theta}_{2}$ is a $t$ -cover of $\\Theta$ with respect to $\\rho$ and $N(t;\\Theta,\\rho)\\leq|\\bar{\\Theta}_{1}\\times\\bar{\\Theta}_{2}|=$ $N_{1}\\times N_{2}=N(t/(2\\alpha_{1});\\Theta_{1},\\rho_{1})\\times N(t/(2\\alpha_{2});\\Theta_{2},\\rho_{2})$ , where $|\\cdot|$ represents the cardinality of a set. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Let $\\mathcal{T}\\triangleq[a,b]\\subset\\mathbb{R}$ denote a closed interval on $\\mathbb{R}$ with $a<b$ Define the metric $\\rho$ on $\\mathcal{T}$ as $\\rho(x,x^{\\prime})=|x-x^{\\prime}|$ for any $x,x^{\\prime}\\in\\mathcal{T}$ Then, for any $t>0$ $\\begin{array}{r}{N(t;\\mathcal{T},\\rho)\\leq\\frac{b-a}{2t}+1}\\end{array}$ b+1ift< $\\textstyle t<{\\frac{b-a}{2}}$ and $N(t;\\mathcal{T},\\rho)=1$ \u00fc $\\textstyle t\\geq{\\frac{b-a}{2}}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{n_{t}=\\lfloor\\frac{b-a}{2}\\rfloor}\\end{array}$ ,where $\\lfloor\\cdot\\rfloor$ represents the foor function. To prove the first result for $b\\!-\\!a\\geq2t$ we construct the following subset $\\overline{{\\mathcal{I}}}$ of $\\mathcal{T}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathcal{Z}}}\\triangleq\\Big\\{a+t,a+t+2t,a+t+4t,\\textnormal{\\scriptsize+},a+t+(n_{t}-1)\\cdot2t,\\operatorname*{min}(b,a+t+n_{t}\\cdot2t)\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Clearly, $\\mathcal{Z}\\subset\\cup_{k\\in[n_{t}]}[a+2t(k-1),a+2t k]\\cup[a+2t n_{t},b]$ Next, we verify that $\\overline{{\\mathcal{I}}}$ is a $t$ -cover of $\\mathcal{T}$ withrespect tometric $\\rho$ .Specifically,for any $x\\in\\mathbb{Z}$ thereexists $k\\,\\in\\,[n_{t}]$ such that $x\\in$ $[a+2t\\cdot(k-1),a+2t\\cdot k]$ ,or $x\\in[a+2t n_{t},b]$ .For the former case, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho(x,a+t+2t\\cdot(K-1))\\leq t;\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and for the latter case, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho(x,\\operatorname*{min}(b,a+t+n_{t}\\cdot2t))\\leq t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, by Defnition A.2, $\\overline{{\\mathcal{I}}}$ isa $t$ coverof $\\mathcal{T}$ therefore $\\begin{array}{r}{N(t;\\mathcal{T},\\rho)\\leq|\\overline{{\\mathcal{Z}}}|=n_{t}+1\\leq\\frac{b-a}{2t}+1.}\\end{array}$ The first result is then established. ", "page_idx": 15}, {"type": "text", "text": "The second result for $b\\mathrm{~-~}a\\,\\leq\\,2t$ follows from the fact that $\\textstyle\\{a+{\\frac{b-a}{2}}\\}$ is a $t$ -coverof $\\mathcal{T}$ since $\\textstyle\\rho(x,a+{\\frac{b-a}{2}})\\leq{\\frac{b-a}{2}}\\leq t$ for any $x\\in\\mathcal{Z}$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Definition A.3 ([39, Definition 5.16]). A collection of zero-mean random variables $\\{S_{\\theta}:\\theta\\in\\Theta\\}$ is a sub-Gaussian process with respect to a metric $\\rho$ on $\\Theta$ if, for all $\\theta_{1},\\theta_{2}\\in\\Theta$ and $t\\in\\mathbb R$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left\\{t(S_{\\theta_{1}}-S_{\\theta_{2}})\\right\\}\\right]\\leq\\exp\\left\\{{\\frac{t^{2}\\rho^{2}(\\theta_{1},\\theta_{2})}{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 6 (Dudley's Entropy Integral Bound; modified from Theorem 5.22 of [39]). Let $\\{S_{\\theta}:\\theta\\in\\Theta\\}$ be a zero-mean sub-Gaussian process with respect to a metric $\\rho$ on $\\Theta$ Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\operatorname*{sup}_{\\theta\\in\\Theta}S_{\\theta}\\right)\\leq8\\sqrt{2}\\int_{0}^{\\infty}\\sqrt{\\log N(t;\\Theta,\\rho)}d t,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $N(t;\\Theta,\\rho)$ represents the $t$ -covering number of $\\Theta$ with respect to $\\rho$ ", "page_idx": 15}, {"type": "text", "text": "A.2Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Strong duality can be established using Theorem 1 in [18], which applies to general cases. However, by capitalizing the discrete nature of the sample space $\\boldsymbol{\\wp}$ , we can present a more concise result. To see this, here we provide an alternative proof of strong duality using the duality principle in finite-dimensional linear programming, as detailed below. ", "page_idx": 15}, {"type": "text", "text": "For every fixed $\\mathbf{x}\\in\\mathcal{X}$ $\\widetilde{\\mathbf{y}}\\in\\mathcal{Y}^{R}$ $\\psi\\in\\Psi$ , by re-writing the the constraint $Q_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\in\\Gamma_{\\epsilon}(P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ in (1) using Definition 2.1, we re-express the primal problem (1) as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathsf{P}}}_{\\epsilon}\\overset{\\Delta}{=}\\underset{\\Pi\\in\\mathcal{P}(\\mathcal{Y}^{2})}{\\operatorname*{sup}}\\Big\\{\\int\\ell(\\psi(\\mathbf{x}),\\mathbf{y})d\\Pi(\\mathbf{y},\\mathbf{y}^{\\prime}):\\int c^{p}(\\mathbf{y},\\mathbf{y}^{\\prime})d\\Pi(\\mathbf{y},\\mathbf{y}^{\\prime})\\leq\\epsilon^{p},\\,\\Pi(\\mathcal{Y},\\cdot)=P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}(\\cdot)\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given the discrete nature of the sample space ${\\mathcal{V}},\\,{\\overline{{\\mathsf{P}}}}_{\\epsilon}$ can be reformulated as the following finitedimensional linear program: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathsf{P}}}_{\\epsilon}=\\left\\{\\begin{array}{c}{\\displaystyle\\operatorname*{max}_{\\substack{\\pi_{j}\\,k\\in\\mathbb{R}\\,\\mathrm{wih}\\,j,k\\in[K]}}\\,\\bigg\\{\\sum_{j,k\\in[K]}\\ell(\\psi(\\mathbf{x}),j)\\pi_{j k}\\bigg\\},}\\\\ {\\displaystyle s.t.\\,\\sum_{j,k\\in[K]}c^{p}(j,k)\\pi_{j k}\\leq\\epsilon^{p},}\\\\ {\\displaystyle\\sum_{j=1}^{K}\\pi_{j k}=P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}(k)\\,\\mathrm{for}\\,k\\in[K],}\\\\ {\\displaystyle\\pi_{j k}\\geq0\\,\\mathrm{for}\\,j,k\\in[K],}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pi_{j k}=\\Pi(\\mathrm{Y}=j,\\mathrm{Y}^{\\prime}=k)$ for $j,k\\in[K]$ . By Lemma 2, each primal constraint corresponds to a dual variable. Introducing the dual variables $\\gamma$ and $\\tau_{k}$ for $k\\in[K]$ , the dual linear program for $\\overline{{\\mathsf{P}}}_{\\epsilon}$ is then expressed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{\\mathsf{D}}}_{\\epsilon}=\\left\\{\\!\\!\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{\\substack{\\gamma,\\tau_{k}\\in\\mathbb{R}\\,\\mathrm{with}\\,k\\in[K]}}\\,\\Big\\{\\gamma\\epsilon^{p}+\\sum_{k=1}^{K}P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}(k)\\tau_{k}\\Big\\},}\\\\ {\\displaystyle s.t.\\,\\gamma c^{p}(j,k)+\\tau_{k}\\geq\\ell(\\psi(\\mathbf{x}),j)\\,\\,\\mathrm{for}\\,\\,j,k\\in[K],}\\\\ {\\displaystyle\\gamma\\geq0,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where first constraint can be written as $\\tau_{k}\\,\\geq\\,\\mathrm{max}_{j\\in[K]}\\left\\{\\ell(\\psi(\\mathbf{x}),j)-\\gamma c^{p}(j,k)\\right\\}$ . Therefore, to minimize the objective function in (A4), the value of the dual variable $\\tau_{k}$ should be taken as $\\mathrm{max}_{j\\in[K]}\\left\\{\\ell(\\psi(\\mathring{\\mathbf{x}}),j)-\\gamma c^{p}(j,k)\\right\\}$ for each $\\gamma\\geq0$ and $k\\in[K]$ . Hence, the proof is established. ", "page_idx": 16}, {"type": "text", "text": "A.3Proof of Remark 2.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As in the proof of Proposition 2.1, the optimization problem in (6) can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathsf{P}}}_{\\epsilon}\\triangleq\\operatorname*{sup}\\Bigg\\{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\mathbb{E}_{Q_{\\mathbb{y}\\mid\\mathbf{x}_{i},\\tilde{\\mathbf{y}}_{i}}}\\left\\{\\ell(\\psi(\\mathbf{x}_{i}),\\mathrm{Y})\\right\\}\\Big]:\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}d(Q_{\\mathbb{y}\\mid\\mathbf{x}_{i},\\tilde{\\mathbf{y}}_{i}},P_{\\mathbb{y}\\mid\\mathbf{x}_{i},\\tilde{\\mathbf{y}}_{i}})\\le\\epsilon\\Bigg\\}}\\\\ &{\\quad=\\displaystyle\\operatorname*{sup}_{\\Pi^{(i)}\\in\\mathcal{P}(\\mathcal{Y}^{2}),i\\in[n]}\\Bigg\\{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\int\\ell(\\psi(\\mathbf{x}_{i}),\\mathrm{y})d\\Pi^{(i)}(\\mathbf{y},\\mathbf{y}^{\\prime}):\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\int c(\\mathbf{y},\\mathbf{y}^{\\prime})d\\Pi^{(i)}(\\mathbf{y},\\mathbf{y}^{\\prime})\\le\\epsilon,}\\\\ &{\\qquad\\qquad\\qquad\\Pi^{(i)}(\\mathcal{Y},\\cdot)=P_{\\mathbb{y}\\mid\\mathbf{x}_{i},\\tilde{\\mathbf{y}}_{i}}(\\cdot)\\mathrm{~for~}i\\in[n]\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where thefrst spisd t theinft pircaldistibton Py, and the second step holds by re-writing the constraint $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}d(Q_{\\mathrm{y}|\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i}},P_{\\mathrm{y}|\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i}})\\leq\\epsilon}\\end{array}$ using the definition of the the Wasserstein distance $d(\\cdot,\\cdot)$ given in Definition 2.1. ", "page_idx": 16}, {"type": "text", "text": "$\\overline{{\\mathsf{P}}}_{\\epsilon}$ can be further expressed as the finite- dimensional linear program: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{P}}}_{\\epsilon}=\\left\\{\\begin{array}{c}{\\displaystyle\\operatorname*{max}_{\\substack{\\sigma_{j k}^{(i)}\\in\\mathbb{R}\\,\\mathrm{wid}\\,i\\in[n],\\,j,k\\in[K]}}\\,\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j,k\\in[K]}\\ell(\\psi(\\mathbf{x}_{i}),j)\\pi_{j k}^{(i)}\\right\\}}\\\\ {\\displaystyle s.t.\\,\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j,k\\in[K]}c(j,k)\\pi_{j k}^{(i)}\\leq\\epsilon,}\\\\ {\\displaystyle\\sum_{j=1}^{K}\\pi_{j k}^{(i)}=P_{\\mathrm{y}|\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i}}(k)\\,\\mathrm{for}\\,i\\in[n]\\,\\mathrm{and}\\,k\\in[K],}\\\\ {\\displaystyle\\pi_{j k}^{(i)}\\geq0\\,\\mathrm{for}\\,i\\in[n]\\,\\mathrm{and}\\,j,k\\in[K],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\pi_{j k}^{(i)}=\\Pi^{(i)}(\\Upsilon=j,\\Upsilon^{\\prime}=k)$ for $i\\in[n]$ and $j,k\\in[K]$ ", "page_idx": 16}, {"type": "text", "text": "By Lemma 2 and introducing dual variables $\\gamma$ and $\\tau_{k}^{\\left(i\\right)}$ with $i\\,\\in\\,[n]$ and $k\\,\\in\\,[K]$ the dual linar program for $\\overline{{\\mathsf{P}}}_{\\epsilon}$ is expressed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\overline{{\\mathsf{D}}}_{\\epsilon}=\\left\\lbrace\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{\\substack{s\\in[n]\\,\\in\\mathbb{R}^{n}}\\mathrm{~for~}i\\in[n]\\,\\mathrm{and~}k\\in[K]}\\left\\lbrace\\gamma\\epsilon+\\sum_{i=1}^{n}\\sum_{k=1}^{K}P_{\\mathrm{y}|\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i}}(k)\\tau_{k}^{(i)}\\right\\rbrace,}\\\\ {\\displaystyle s\\,\\iota\\cdot\\frac{1}{n}\\,\\gamma c(j,k)+\\tau_{k}^{(i)}\\ge\\frac{1}{n}\\ell(\\psi(\\mathbf{x}_{i}),j)\\mathrm{~for~}i\\in[n]\\,\\mathrm{and~}j,k\\in[K],}\\\\ {\\displaystyle\\gamma\\ge0,}\\end{array}\\right.}\\\\ {\\displaystyle=\\left\\lbrace\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{\\substack{r,\\tau_{k}^{(i)}\\in\\mathbb{R}^{n}\\mathrm{~for~}i\\in[n]\\,\\mathrm{and~}k\\in[K]}}\\left\\lbrace\\gamma\\epsilon+\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K}P_{\\mathrm{y}|\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i}}(k)\\widetilde{\\tau}_{k}^{(i)}\\right\\rbrace,}\\\\ {\\displaystyle s\\,\\iota\\cdot\\gamma c(j,k)+\\widetilde{\\gamma}_{k}^{(i)}\\ge\\ell(\\psi(\\mathbf{x}_{i}),j)\\mathrm{~for~}i\\in[n]\\,\\mathrm{and~}j,k\\in[K],}\\\\ {\\displaystyle\\gamma\\ge0\\,,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we let $\\widetilde{\\tau}_{k}^{(i)}=n\\tau_{k}^{(i)}$ $i\\in[n]$ and $k\\in[K]$ ", "page_idx": 17}, {"type": "text", "text": "A.4Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We begin by demonstrating that, for various choices of the reference distribution, the optimal value for $\\gamma$ in the relaxed dual problem, as stated in (4), is constrained to a compact set. The proof techniques in [40] are used. ", "page_idx": 17}, {"type": "text", "text": "Specifically, for a given $\\epsilon>0$ and $\\psi\\in\\Psi$ , let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma^{*}\\in\\arg\\operatorname*{inf}_{\\gamma\\geq0}\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\bigg(\\gamma\\epsilon^{p}+\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\Big\\{\\ell(\\psi(\\mathbf{X}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\Big]\\Big\\}\\bigg).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Noting that for any $\\textbf{x}\\in\\ \\mathcal{X}$ and $\\mathrm{~y~}\\in\\mathcal{V}$ \uff0c $\\begin{array}{r l}&{\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\left\\lbrace\\ell(\\psi(\\mathbf{x}),y^{\\prime})\\,-\\,\\ell(\\psi(\\mathbf{x}),\\mathrm{y})\\,-\\,\\gamma^{*}c^{p}(y^{\\prime},\\mathrm{y})\\right\\rbrace\\;\\geq}\\end{array}$ $\\left\\{\\ell(\\psi(\\mathbf{x}),y^{\\prime})-\\ell(\\psi(\\mathbf{x}),\\mathrm{y})-\\gamma^{*}c^{p}(y^{\\prime},\\mathrm{y})\\right\\}\\bigr|_{y^{\\prime}=\\mathrm{y}}=0$ , we obtain that for any $\\gamma\\geq0$ \uff0c ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma^{*}\\epsilon^{p}\\leq\\gamma^{*}\\epsilon^{p}+\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\Bigg(\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi(\\mathbf{X}),y^{\\prime})-\\ell(\\psi(\\mathbf{X}),\\mathbf{Y})-\\gamma^{*}c^{p}(y^{\\prime},\\mathbf{Y})\\big\\}\\Big]\\Bigg)}\\\\ &{\\qquad\\leq\\gamma\\epsilon^{p}+\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\Bigg(\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi(\\mathbf{X}),y^{\\prime})-\\ell(\\psi(\\mathbf{X}),\\mathbf{Y})-\\gamma c^{p}(y^{\\prime},\\mathbf{Y})\\big\\}\\Big]\\Bigg)}\\\\ &{\\qquad\\leq\\gamma\\epsilon^{p}+\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\Bigg(\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{L\\cdot c(y^{\\prime},\\mathbf{Y})-\\gamma c^{p}(y^{\\prime},\\mathbf{Y})\\big\\}\\Big]\\Bigg)}\\\\ &{\\qquad\\leq\\gamma\\epsilon^{p}+\\operatorname*{sup}_{t\\geq0}\\Big\\{L t-\\gamma t^{p}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality is due to the definition of $\\gamma^{*}$ ; the third inequality comes from the Lipschitz property in the assumption; and the last inequality holds because $c(y^{\\prime},\\mathrm{Y})=\\kappa\\mathbf{1}(y^{\\prime}\\neq\\mathrm{Y})$ takes values in $\\{0,\\kappa\\}$ , leading to $\\begin{array}{r}{\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\left\\{L\\cdot c(y^{\\prime},\\mathrm{Y})-\\gamma\\cdot c^{p}(y^{\\prime},\\mathrm{Y})\\right\\}=\\operatorname*{sup}_{t\\in\\{0,\\kappa\\}}\\left\\{L t-\\gamma t^{p}\\right\\}}\\end{array}$ $\\operatorname*{sup}_{t\\geq0}\\left\\{L t-\\gamma t^{p}\\right\\}$ , which is a constant. ", "page_idx": 17}, {"type": "text", "text": "Now, we show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma^{*}\\leq L\\epsilon^{-(p-1)}\\triangleq M^{*}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Indeed, when $p=1$ , then taking $\\gamma=L$ in (A5) shows (A6). When $p>1$ , then $L t-\\gamma t^{p}$ in (A5) takes its maximum value at $t^{*}=\\{L/(p\\gamma)\\}^{1/(p-1)}$ , and hence, (A5) yields that for any $\\gamma\\geq0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma^{*}\\epsilon^{p}\\leq\\gamma\\epsilon^{p}+L\\cdot\\{L/(p\\gamma)\\}^{1/(p-1)}-\\gamma\\{L/(p\\gamma)\\}^{p/(p-1)}}\\\\ &{\\qquad=\\gamma\\epsilon^{p}+L^{p/(p-1)}\\gamma^{-1/(p-1)}p^{-p/(p-1)}(p-1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, taking $\\gamma=L/(p\\epsilon^{p-1})$ leads to $\\gamma^{*}\\epsilon^{p}\\leq L\\epsilon$ , i.e., (A6) holds. ", "page_idx": 17}, {"type": "text", "text": "Next, let $\\ell_{\\gamma,\\psi}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\,\\triangleq\\,\\mathbb{E}_{P_{\\mathrm{y}\\mid\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\Big[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi(\\mathbf{x}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\big\\}\\Big]$ for any $(\\mathbf{x},\\widetilde{\\mathbf{y}})\\,\\in\\,\\mathcal{X}\\times\\mathcal{Y}^{R}$ For every $\\psi\\in\\Psi$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\mathfrak{R}_{\\epsilon}(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\widehat{\\mathfrak{R}}_{\\epsilon}(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\right|}\\\\ &{=\\biggr|\\operatorname*{inf}_{\\eta\\leq0}\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\biggr(\\gamma\\epsilon^{p}+\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\biggr[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\{\\ell(\\boldsymbol{\\psi}(\\mathbf{X}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\}\\biggr]\\biggr)}\\\\ &{\\qquad-\\underset{\\gamma\\geq0}{\\operatorname*{inf}}\\mathbb{E}_{P_{\\mathbb{x},\\widetilde{\\mathbf{y}}}^{(n)}}\\biggr(\\gamma\\epsilon^{p}+\\mathbb{E}_{P_{\\mathbb{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\biggr[\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\{\\ell(\\boldsymbol{\\psi}(\\mathbf{X}),y^{\\prime})-\\gamma c^{p}(y^{\\prime},\\mathrm{Y})\\}\\biggr]\\biggr)\\biggr|}\\\\ &{=\\biggr|\\operatorname*{inf}_{0\\leq\\gamma\\leq M^{\\prime}}\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\biggr\\{\\gamma\\epsilon^{p}+\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\biggr\\}-\\underset{0\\leq\\gamma\\leq M^{\\ast}}{\\operatorname*{inf}}\\mathbb{E}_{P_{\\mathbb{x},\\widetilde{\\mathbf{y}}}^{(n)}}\\biggr\\{\\gamma\\epsilon^{p}+\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\biggr\\}}\\\\ &{\\leq\\underset{0\\leq\\gamma\\leq M^{\\ast}}{\\operatorname*{sup}}\\left|\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}-\\mathbb{E}_{P_{\\mathbb{x},\\widetilde{\\mathbf{y}}}^{(n)}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}\\biggr|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first equality comes from (4) and (5), the second equality follows from (A6) and the definition of $\\ell_{\\gamma,\\psi}$ , and the third step is due to the fact that $|\\operatorname*{inf}_{v\\in A}f(v)-\\operatorname*{inf}_{v\\in A}g(v)|\\ \\leq$ $\\operatorname*{sup}_{v\\in A}|f(v)-g(v)|$ for bounded functions $f,g:A\\rightarrow\\mathbb{R}$ [41, Proposition 2.18]. ", "page_idx": 17}, {"type": "text", "text": "In (A7), we use $\\mathcal{D}$ to stress the dependence of $\\mathbb{E}_{P_{\\mathbf{x},\\widetilde{\\mathbf{y}}}^{(n)}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}$ on the observed data of size $n$ ,as defined in Section 2.1, and let $\\Phi$ represent the resulting function mapping from $\\left(\\mathcal{X}\\times\\mathcal{Y}^{R}\\right)^{n}$ to $\\mathbb{R}$ , with $\\Phi({\\mathcal{D}})$ being the value for data $\\mathcal{D}$ , where $\\left(\\mathcal{X}\\times\\mathcal{Y}^{R}\\right)^{n}$ is the Cartesian product of multiplying $\\mathcal{X}\\times\\mathcal{Y}^{R}$ $n$ times. The function $\\Phi:\\left(\\mathcal{X}\\times\\mathcal{Y}^{R}\\right)^{n}\\overset{}{\\rightarrow}\\mathbb{R}$ defined in (A7) satisfies the bounded difference property (A3) with parameters $\\left\\{{\\frac{M}{n}},\\cdot\\cdot\\cdot,{\\frac{M}{n}}\\right\\}$ ,where $M$ represents the upper bound of the lossfunction $\\ell$ .n the assumption of Theorem 2.2. Indeed, for any $k\\in[n]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{\\substack{\\geq\\,1,\\ldots,\\ldots,\\frac{x}{n_{s}}\\leq k\\,x>y}}\\left|\\Phi(z_{1},\\ldots,z_{k},\\ldots,z_{n})-\\Phi(z_{1},\\ldots,z_{k}^{\\prime},\\ldots,z_{n})\\right|}\\\\ {\\displaystyle=}&{\\displaystyle\\operatorname*{sup}_{z_{1},\\ldots,z_{n},\\leq k\\leq X}\\left|\\Phi\\left|_{\\infty,\\geq\\,\\sum\\lambda\\leq B}\\left|\\mathbb{E}_{z\\times\\{\\mathbf{X},\\sqrt{\\mathbf{X}}\\}}\\right|-\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\ell}_{z_{i},\\psi}(z_{i})\\right|}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\cdot\\operatorname*{sup}_{\\substack{\\geq\\,\\frac{x}{n_{s}}\\leq k,\\ldots,\\frac{y}{n_{s}}\\leq\\binom{k}{n_{s}}\\leq\\binom{k}{n_{s}}}}\\left|\\mathbb{E}_{x\\times\\tilde{\\mathcal{Y}}}\\left\\{\\ell_{\\gamma},\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}-\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{\\gamma,\\psi}(z_{i})-\\frac{1}{n}\\mathbb{E}_{z\\times\\{\\mathbf{X}_{\\infty}^{\\prime}\\}}+\\frac{1}{n}\\ell_{\\gamma,\\psi}(z_{k})\\right|\\right|}\\\\ {\\displaystyle\\leq}&{\\displaystyle\\operatorname*{sup}_{z_{1},\\ldots,z_{n},\\leq\\frac{x}{n_{s}}\\leq k\\leq Y}\\mu_{\\mathbb{S}\\times\\mathcal{Y}}(\\mathbf{z},\\psi_{\\mathbb{X}},\\widetilde{\\mathbf{Y}})\\left|\\mathbb{E}_{x\\times\\tilde{\\mathcal{Y}}}\\left\\{\\ell_{\\gamma},\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}-\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{\\gamma,\\psi}(z_{i})\\right|}\\\\ {\\displaystyle}&{\\displaystyle-\\left|\\mathbb{E}_{x\\times\\tilde{\\mathcal{Y}}}\\left\\{\\ell_{\\gamma},\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}-\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{\\gamma,\\psi}(z_{i})-\\frac{1}{n}\\ell_{\\gamma,\\psi}(z_{k}^{\\prime})+\\frac{1}{n}\\ell_{\\gamma,\\psi}(z_{k})\\right|}\\\\ {\\displaystyle\\leq}&{\\displaystyle\\operatorname*{sup}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second step holds since $\\begin{array}{r}{|\\operatorname*{sup}_{v\\in A}f(v)\\!-\\!\\operatorname*{sup}_{v\\in A}g(v)|\\leq\\operatorname*{sup}_{v\\in A}|f(v)\\!-\\!g(v)|}\\end{array}$ forbounded functions $f,g:A\\rightarrow\\mathbb{R}$ [41, Proposition 2.18], the third step is due to the triangle inequality for absolute values, and the last step holds since $\\ell_{\\gamma,\\psi}\\in[0,M]$ by definition. ", "page_idx": 18}, {"type": "text", "text": "Thus, by letting $\\begin{array}{r}{t=M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}}\\end{array}$ o(/m inlemma3, wehave that, with probabilty at least - ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi(\\mathcal{D})\\le\\!\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\left\\{\\Phi(\\mathcal{D})\\right\\}+M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similar to the derivations for (3.8)-(3.13) in the proof of Theorem 3.3 in [42], we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\left\\{\\Phi(\\mathcal{D})\\right\\}\\leq2\\mathbb{E}\\left[\\operatorname*{sup}_{\\gamma\\in[0,M^{*}]}\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}\\ell_{\\gamma,\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i})\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\{\\sigma_{i}\\}_{i=1}^{n}$ are independent random variables chosen from $\\{-1,+1\\}$ with equal probability, and the expectation is taken with respect to all involved random variables. ", "page_idx": 18}, {"type": "text", "text": "Applying (A8) and (A9) to (A7) gives that with probability at least $1-\\eta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big|\\mathfrak{N}_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\widetilde{y}}})-\\mathfrak{N}_{\\epsilon}(\\psi;P_{\\mathrm{y|x,\\widetilde{y}}})\\Big|\\le2\\mathbb{E}\\left[\\operatorname*{sup}_{\\gamma\\in[0,M^{*}]}\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}\\ell_{\\gamma,\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i})\\right]+M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we identify an entropy based upper bound for the right-hand side of (A9) using the proof techniques in [40] and Example 5.24 of [39]. Specifically, for a given $\\psi$ , we define the random process $\\begin{array}{r}{\\left\\{S_{\\gamma}\\triangleq\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\sigma_{i}\\ell_{\\gamma,\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i}):\\gamma\\in[0,M^{*}]\\right\\}}\\end{array}$ By using $\\mathbb{E}(\\sigma_{i})=0$ and theindependence between $\\sigma_{i}$ and $(\\mathbf{X}_{i},\\tilde{\\mathbf{Y}}_{i})$ , we obtain that $\\begin{array}{r}{\\mathbb{E}(S_{\\gamma})=\\frac{\\dot{1}}{\\sqrt{n}}\\sum_{i=1}^{n}\\mathbb{E}(\\sigma_{i})\\mathbb{E}\\big\\{\\ell_{\\gamma,\\psi}({\\bf X}_{i},\\widetilde{\\bf Y}_{i})\\big\\}=0}\\end{array}$ For any ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{1},\\gamma_{2}\\in[0,M^{*}],}\\\\ &{\\qquad\\left|\\ell_{\\gamma_{1},\\psi}(\\mathbf{x},\\mathbf{\\tilde{y}})-\\ell_{\\gamma_{2},\\psi}(\\mathbf{x},\\mathbf{\\tilde{y}})\\right|}\\\\ &{\\qquad=\\left|\\mathbb{E}_{P_{\\mathbb{P}|\\mathbf{x},\\tilde{\\mathbf{y}}}}\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{P}}\\left\\{\\ell(\\psi(\\mathbf{x}),y^{\\prime})-\\gamma_{1}c^{p}(y^{\\prime},\\mathbf{Y})\\right\\}-\\mathbb{E}_{P_{\\mathbb{P}|\\mathbf{x},\\tilde{\\mathbf{y}}}}\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\left\\{\\ell(\\psi(\\mathbf{x}),y^{\\prime})-\\gamma_{2}c^{p}(y^{\\prime},\\mathbf{Y})\\right\\}\\right|}\\\\ &{\\qquad\\leq\\mathbb{E}_{P_{\\mathbb{P}|\\mathbf{x},\\tilde{\\mathbf{y}}}}\\left|\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\left\\{\\ell(\\psi(\\mathbf{x}),y^{\\prime})-\\gamma_{1}c^{p}(y^{\\prime},\\mathbf{Y})\\right\\}-\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\left\\{\\ell(\\psi(\\mathbf{x}),y^{\\prime})-\\gamma_{2}c^{p}(y^{\\prime},\\mathbf{Y})\\right\\}\\right|}\\\\ &{\\qquad\\leq\\mathbb{E}_{P_{\\mathbb{P}|\\mathbf{x},\\tilde{\\mathbf{y}}}}\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\left|\\gamma_{1}c^{p}(y^{\\prime},\\mathbf{Y})-\\gamma_{2}c^{p}(y^{\\prime},\\mathbf{Y})\\right|}\\\\ &{\\qquad=\\kappa^{p}|\\gamma_{1}-\\gamma_{2}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality is due to the fact that $|\\operatorname*{sup}_{A}f-\\operatorname*{sup}_{A}g|\\leq\\operatorname*{sup}_{A}|f-g|$ for bounded functions $f,g:A\\rightarrow\\mathbb{R}$ [41, Proposition 2.18], and the last step is due to the fact that $c(y^{\\prime},\\mathrm{Y})$ can only take values in $\\{0,\\kappa\\}$ ", "page_idx": 19}, {"type": "text", "text": "Hence, for $t\\in\\mathbb R$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{e^{t(S_{\\gamma_{1}}-S_{\\gamma_{2}})}\\right\\}=\\mathbb{E}\\left\\{\\exp\\left[\\frac{t}{\\sqrt{n}}\\sum_{i=1}^{n}\\sigma_{i}\\left\\{\\ell_{\\gamma_{1},\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i})-\\ell_{\\gamma_{2},\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i})\\right\\}\\right]\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\left\\{\\mathbb{E}\\left(\\exp\\left[\\frac{t}{\\sqrt{n}}\\sigma_{1}\\left\\{\\ell_{\\gamma_{1},\\psi}(\\mathbf{X}_{1},\\widetilde{\\mathbf{Y}}_{1})-\\ell_{\\gamma_{2},\\psi}(\\mathbf{X}_{1},\\widetilde{\\mathbf{Y}}_{1})\\right\\}\\right]\\right)\\right\\}^{n}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\exp\\left\\{\\frac{t^{2}\\left(\\kappa^{p}\\left|\\gamma_{1}-\\gamma_{2}\\right|\\right)^{2}}{2}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the inequality is due to Hoeffding's lemma and (A11). Thus, $\\left\\lbrace S_{\\gamma}:\\gamma\\in\\left[0,M^{*}\\right]\\right\\rbrace$ is azeromean sub-Gaussian process with respect to metric $\\rho_{\\gamma}$ ,defined as $\\rho_{\\gamma}(\\gamma_{1},\\gamma_{2})=\\kappa^{p}|\\gamma_{1}-\\gamma_{2}|$ for any $\\gamma_{1},\\gamma_{2}\\in[0,M^{*}]$ ", "page_idx": 19}, {"type": "text", "text": "Therefore, by Lemma 6, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\frac{1}{t+\\eta(t)+\\lambda}\\bigg|\\frac{\\tilde{\\lambda}_{t}}{t+\\eta(t)}v_{t,0}(\\mathbb{R},\\mathbb{R},\\mathbb{T}_{S})\\bigg]}\\\\ &{=\\frac{\\sqrt{1-\\eta(t)}}{\\sqrt{1+\\eta(t)}}\\bigg(\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\int_{0\\leq t\\leq T}v_{t,0}(\\mathbb{R},\\mathbb{T}_{S})\\bigg)}\\\\ &{\\leq\\frac{v_{t,0}^{2}\\tilde{\\lambda}_{t}}{2}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log(\\mathbb{R}\\bigg(\\int_{0}^{T}\\bigg(\\mathbb{R},\\mathbb{R}\\bigg)T_{S}\\bigg))}\\mu}\\\\ &{=\\frac{\\sqrt{1-\\eta(t)}}{\\sqrt{1+\\eta(t)}}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log(\\mathbb{R}\\bigg(\\int_{0}^{T}\\bigg(\\mathbb{R},\\mathbb{R}\\bigg)T_{S}\\bigg))}|\\mu|}\\\\ &{=\\frac{v_{t,0}^{2}\\tilde{\\lambda}_{t}}{\\sqrt{1+\\eta(t)}}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log(\\mathbb{R}\\bigg(\\int_{0}^{T}\\bigg(\\mathbb{R},\\mathbb{R}\\bigg)T_{S}\\bigg))}|\\mu|d t}\\\\ &{\\leq\\frac{v_{t,0}^{2}\\tilde{\\lambda}_{t}}{\\sqrt{1+\\eta(t)}}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log^{2}(\\mathbb{R}\\bigg(\\frac{\\int_{0}^{T}\\bigg(\\mathbb{R},\\mathbb{R}\\bigg)T_{S}\\bigg)}+1\\bigg)}d t}\\\\ &{\\leq\\frac{\\sqrt{1+\\eta(t)}}{\\sqrt{1+\\eta(t)}}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log(\\frac{\\int_{0}^{T}\\bigg(\\mathbb{R}\\bigg(\\int_{0}^{T}\\bigg)d t}{\\eta(t)})}d t}\\\\ &{\\leq\\frac{\\sqrt{1+\\eta(t)}}{\\sqrt{1+\\eta(t)}}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log(1+\\eta(t))}d t}\\\\ &{=\\frac{v_{t,0}^{2}\\tilde{\\lambda}_{t}}{\\sqrt{1+\\eta(t)}}\\int_{0}^{\\mathbb{T}}\\sqrt{\\log(1+\\eta(t))}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the third step comes from the fact that, by the definition of $\\rho_{\\gamma}$ \uff0c $\\rho_{\\gamma}(\\gamma_{1},\\gamma_{2})\\leq t$ if and only if $|\\gamma_{1}-\\gamma_{2}|\\leq t/\\kappa^{p}$ ; the fourth step holds by letting $s=t/\\kappa^{p}$ ; the fifth step follows from Lemma 5; the ", "page_idx": 19}, {"type": "text", "text": "sixth step comes from the fat that $\\begin{array}{r}{1<\\frac{M^{*}}{2s}+1=\\frac{M^{*}+2s}{2s}\\leq\\frac{M^{*}}{s}}\\end{array}$ for $s\\in[0,M^{\\ast}/2]$ ; the penltimate step holds by letting $u=s/M^{*}$ ; and the last step arises from the fact that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{0}^{1/2}\\sqrt{\\log(1/u)}d u=\\int_{0}^{1/2}\\sqrt{-\\log u}d u=\\int_{0}^{1/2}\\int_{0}^{-\\log(u)}\\frac{1}{2\\sqrt{s}}d s d u}\\\\ &{=\\displaystyle\\int_{0}^{\\log2}\\int_{0}^{1/2}\\frac{1}{2\\sqrt{s}}d u d s+\\int_{\\log2}^{\\infty}\\int_{0}^{e^{-s}}\\frac{1}{2\\sqrt{s}}d u d s=\\frac{\\sqrt{\\log2}}{2}+\\int_{\\log2}^{\\infty}\\frac{e^{-s}}{2\\sqrt{s}}d s}\\\\ &{=\\displaystyle\\frac{\\sqrt{\\log2}}{2}+\\int_{\\sqrt{\\log2}}^{\\infty}\\frac{e^{-w^{2}}}{2w}2w d w=\\frac{\\sqrt{\\log2}}{2}+\\frac{\\sqrt{\\pi}}{2}\\mathrm{erfc}(\\sqrt{\\log2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where erfc $\\begin{array}{r}{\\dot{(x)}=\\frac{2}{\\sqrt{\\pi}}\\int_{x}^{\\infty}e^{-w^{2}}d w.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Applying (A13) and (A6) to (A10) gives that with probability at least $1-\\eta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y}\\mid\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\widehat{\\Re}_{\\epsilon}(\\psi;P_{\\mathrm{y}\\mid\\mathbf{x},\\widetilde{\\mathbf{y}}})\\right|\\leq\\!\\frac{8\\sqrt{2}\\left(\\sqrt{\\log2}+\\sqrt{\\pi}\\mathrm{erfc}(\\sqrt{\\log2})\\right)}{\\sqrt{n}}\\cdot\\frac{L\\kappa^{p}}{\\epsilon^{p-1}}+M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\frac{15L\\kappa^{p}}{\\epsilon^{p-1}\\sqrt{n}}+M\\sqrt{\\frac{\\log(1/\\eta)}{2n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step is due to the fact that $8{\\sqrt{2}}({\\sqrt{\\log2}}+{\\sqrt{\\pi}}\\mathrm{erfc}({\\sqrt{\\log2}}))\\approx14.2<15.$ Hence, the proof is completed. ", "page_idx": 20}, {"type": "text", "text": "A.5 Proof of Corollary 2.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By the definition of $\\operatorname*{inf}_{\\psi\\in\\Psi}\\mathfrak{R}_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})$ forany $\\zeta~>~0$ \uff0cthereexists $\\psi_{\\zeta}~\\in~\\Psi$ such that $\\begin{array}{r}{\\Re_{\\epsilon}(\\psi_{\\zeta};P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\leq\\operatorname*{inf}_{\\psi\\in\\Psi}\\Re_{\\epsilon}(\\psi;P_{\\mathrm{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})+\\zeta}\\end{array}$ .Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Re_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\underset{\\psi\\in\\Psi}{\\mathrm{inf}}\\,\\Re_{\\epsilon}(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})}\\\\ &{\\le\\!\\Re_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\Re_{\\epsilon}(\\psi_{\\zeta};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})+\\zeta}\\\\ &{\\le\\!\\Re_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\widehat{\\Re}_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})+\\widehat{\\Re}_{\\epsilon}(\\psi_{\\zeta};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\Re_{\\epsilon}(\\psi_{\\zeta};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})+\\zeta}\\\\ &{\\le\\!2\\underset{\\psi\\in\\Psi}{\\mathrm{sup}}\\left|\\Re_{\\epsilon}(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})-\\widehat{\\Re}_{\\epsilon}(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}})\\right|+\\zeta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the inequality above is true for all $\\zeta>0$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Re_{\\epsilon}\\bigl(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\bigr)-\\underset{\\psi\\in\\Psi}{\\mathrm{inf}}\\ \\Re_{\\epsilon}\\bigl(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\bigr)}\\\\ &{\\leq2\\underset{\\psi\\in\\Psi}{\\mathrm{sup}}\\left|\\Re_{\\epsilon}\\bigl(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\bigr)-\\widehat{\\Re}_{\\epsilon}\\bigl(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\widetilde{\\mathbf{y}}}\\bigr)\\right|}\\\\ &{\\leq2\\underset{\\psi\\in\\Psi,0\\leq\\gamma\\leq M^{*}}{\\mathrm{sup}}\\left|\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}-\\mathbb{E}_{P_{\\mathbf{x},\\widetilde{\\mathbf{y}}}^{(n)}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality arises from (A7). ", "page_idx": 20}, {"type": "text", "text": "Similar to the proof of Theorem 2.2 in Appendix A.4, we can derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\operatorname*{sup}_{\\psi\\in\\Psi,0\\le\\gamma\\le M^{*}}\\left|\\mathbb{E}_{\\mathbf{x},\\widetilde{\\mathbf{y}}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}-\\mathbb{E}_{P_{\\mathbf{x},\\widetilde{\\mathbf{y}}}^{(n)}}\\left\\{\\ell_{\\gamma,\\psi}(\\mathbf{X},\\widetilde{\\mathbf{Y}})\\right\\}\\right|}\\\\ &{}&{\\le2\\mathbb{E}\\left[\\underset{\\gamma\\in[0,M^{*}],\\psi\\in\\Psi}{\\operatorname*{sup}}\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}\\ell_{\\gamma,\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i})\\right]+M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For any $\\gamma_{1},\\gamma_{2}\\in[0,M^{*}]$ and $\\psi_{1},\\psi_{2}\\in\\Psi$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\big|\\ell_{\\gamma_{1},\\psi_{1}}(\\mathbf{x},\\widetilde{\\mathbf{y}})-\\ell_{\\gamma_{2},\\psi_{2}}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\big|}\\\\ &{=\\!\\left|\\mathbb{E}_{P_{y|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi_{1}(\\mathbf{x}),y^{\\prime})-\\gamma_{1}c^{p}(y^{\\prime},\\mathrm{Y})\\right\\}-\\mathbb{E}_{P_{y|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi_{2}(\\mathbf{x}),y^{\\prime})-\\gamma_{2}c^{p}(y^{\\prime},\\mathrm{Y})\\big\\}\\right|}\\\\ &{\\leq\\!\\mathbb{E}_{P_{y|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\left|\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi_{1}(\\mathbf{x}),y^{\\prime})-\\gamma_{1}c^{p}(y^{\\prime},\\mathrm{Y})\\big\\}-\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\big\\{\\ell(\\psi_{2}(\\mathbf{x}),y^{\\prime})-\\gamma_{2}c^{p}(y^{\\prime},\\mathrm{Y})\\big\\}\\right|}\\\\ &{\\leq\\!\\mathbb{E}_{P_{y|\\mathbf{x},\\widetilde{\\mathbf{y}}}}\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{Y}}\\Big\\{\\Big|\\gamma_{1}c^{p}(y^{\\prime},\\mathrm{Y})-\\gamma_{2}c^{p}(y^{\\prime},\\mathrm{Y})\\Big|+\\Big|\\ell\\big(\\psi_{1}(\\mathbf{x}),y^{\\prime})-\\ell\\big(\\psi_{2}(\\mathbf{x}),y^{\\prime}\\big)\\Big|\\Big\\}}\\\\ &{\\leq\\!\\mathbb{K}^{p}|\\gamma_{1}-\\gamma_{2}|+L^{\\prime}|\\|\\psi_{1}-\\psi_{2}\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first step is due to the definition of $\\ell_{\\gamma,\\psi}$ defined after (A6), the second step is due to Jensen's inequality, and the last step is due to the Lipschitz property with respect to the cost function $c(\\cdot,\\cdot)$ defined in Theorem 2.2 in the assumption, and $\\begin{array}{r}{\\|\\psi_{1}-\\psi_{2}\\|_{\\infty}\\triangleq\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\psi_{1}(\\mathbf{x})-\\psi_{2}(\\mathbf{x})\\|}\\end{array}$ for some norm $\\|\\cdot\\|$ ", "page_idx": 21}, {"type": "text", "text": "Allowing $\\psi$ to vary, we modify the discussion for the random process $\\{S_{\\gamma}\\,:\\,\\gamma\\,\\in\\,\\,[0,M^{\\ast}]\\}$ in AppendixA4, andconsiderthe colletion of randm variables $\\begin{array}{r}{\\left\\{S_{\\gamma,\\psi}\\triangleq\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\sigma_{i}\\ell_{\\gamma,\\psi}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i}):\\right.}\\end{array}$ $\\gamma\\in[0,M^{*}],\\psi\\in\\Psi\\Biggr\\}$ Clearly, $\\mathbb{E}(S_{\\gamma,\\psi})=0$ Modifying the metric $\\rho_{\\gamma}(\\gamma_{1},\\gamma_{2})$ in Appendix A.4, we define the metric $\\rho_{\\gamma,\\psi}((\\gamma_{1},\\psi_{1}),(\\gamma_{2},\\psi_{2}))\\triangleq\\kappa^{p}|\\gamma_{1}-\\gamma_{2}|+L^{\\prime}\\|\\psi_{1}-\\psi_{2}\\|_{\\infty}$ for any $\\gamma_{1},\\gamma_{2}\\in[0,M^{*}]$ and $\\psi_{1},\\psi_{2}\\in\\Psi$ . Similar to deriving (A12), we obtain that for $t\\in\\mathbb R$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\{e^{t\\left(S_{\\gamma_{1},\\psi_{1}}-S_{\\gamma_{2},\\psi_{2}}\\right)}\\right\\}\\leq\\exp\\left[\\frac{t^{2}\\,\\left\\{\\rho_{\\gamma,\\psi}\\left(\\left(\\gamma_{1},\\psi_{1}\\right),\\left(\\gamma_{2},\\psi_{2}\\right)\\right)\\right\\}^{2}}{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, $\\left\\{S_{\\gamma,\\psi}:\\gamma\\in[0,M^{*}],\\psi\\in\\Psi\\right\\}$ is a zero-mean sub-Gaussian proces with respect to metric $\\rho_{\\gamma,\\psi}$ ", "page_idx": 21}, {"type": "text", "text": "Let the Cartesian product $[0,M^{\\ast}]\\times\\Psi$ denote the \u201cparameter\u201d\u2019 space of $(\\gamma,\\psi)$ . Then, by Lemma 6, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{\\gamma\\in\\{0,M\\},\\gamma\\in\\mathbb{R}^{n}}{\\operatorname*{sup}}\\frac{1}{n}\\frac{\\sigma}{n}\\sigma_{\\ell,\\gamma_{0}}(\\mathbf{X}_{i},\\widetilde{\\mathbf{Y}}_{i})\\right]}\\\\ &{=\\frac{1}{\\sqrt{n}}\\mathbb{E}\\left(\\underset{\\gamma\\in\\{0,M\\},\\gamma\\in\\mathbb{R}^{n}}{\\operatorname*{sup}}S_{\\gamma,\\sigma_{\\ell}}\\right)}\\\\ &{\\le\\frac{8\\sqrt{2}}{\\sqrt{n}}\\int_{0}^{\\infty}\\sqrt{\\log(\\mathcal{N}(\\ell;[0,M^{\\ast}]\\times\\mathbb{F},\\mathbb{P}_{\\ell},\\gamma_{0}))d t}}\\\\ &{\\le\\frac{8\\sqrt{2}}{\\sqrt{n}}\\int_{0}^{\\infty}\\sqrt{\\log(\\mathcal{N}(W/(2\\ell^{\\alpha})^{\\frac{1}{\\gamma}}[0,M^{\\ast}]\\times|\\mathcal{N}|^{2}(\\mathbb{Z})T)^{\\frac{1}{\\gamma}}|\\cdot|\\mathcal{N}|)}d t}\\\\ &{\\le\\frac{8\\sqrt{2}}{\\sqrt{n}}\\int_{0}^{\\infty}\\sqrt{\\log(\\mathcal{N}(\\ell/(2\\ell^{\\alpha})^{\\frac{1}{\\gamma}}[0,M^{\\ast}]\\times|\\mathcal{N}|^{\\frac{1}{\\gamma}}|)d t+\\frac{8\\sqrt{2}}{\\sqrt{n}}\\int_{0}^{\\infty}\\sqrt{\\log N(i/(2\\ell^{\\gamma})^{\\frac{1}{\\gamma}}\\Psi_{i})[-|\\mathcal{N}|]}d t}}\\\\ &{\\le\\frac{16\\sqrt{2}\\lambda^{\\mu}\\rho^{\\gamma_{0}}\\sqrt{\\log N(\\mathcal{N}(\\cdot|\\mathcal{N}|,\\mathcal{H})^{\\frac{1}{\\gamma}}|\\cdot|\\mathcal{N}|)}d t+\\frac{16\\sqrt{2}\\lambda^{\\gamma}}{\\sqrt{n}}\\int_{0}^{\\infty}\\sqrt{\\log N(\\cdot\\psi_{\\delta},\\cdot|\\cdot|\\infty)}d s}{\\sqrt{n}}}\\\\ &{\\le\\frac{8\\sqrt{2}(\\sqrt{\\log2}+\\sqrt{\\pi}\\sqrt{\\mathrm{erfc}}(\\sqrt{\\log2}))\\lambda\\cdot|\\mathcal{N}|^{p}}{\\sqrt{n}}+\\frac{16\\sqrt{\\log2}}{\\sqrt{n}}\\int_{0}^{\\infty}\\sqrt{\\log N(\\cdot\\psi_{\\delta},\\cdot|\\cdot \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, for any set $\\Omega$ and metric $\\rho$ on $\\Omega$ $N(t;\\Omega,\\rho)$ denotes the $t$ -covering number for $t>0$ as defined in Definition A.2. In the derivation of (Ai6), the third step is due to Lemma 4; the fourth step is due to ${\\sqrt{\\log(a b)}}\\leq{\\sqrt{\\log a}}+{\\sqrt{\\log b}}$ for $a\\ge1$ and $b\\geq1$ ; the fifth step results from a change of variable; and the last line can be similarly proved as (A13). ", "page_idx": 21}, {"type": "text", "text": "By (A14)-(A16), we have that, with probability at least $1-\\eta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Re_{\\epsilon}(\\widehat{\\psi}_{\\epsilon,n};P_{\\mathbf{y}|\\mathbf{x},\\mathbf{\\widetilde{y}}})-\\operatorname*{inf}_{\\psi\\in\\Psi}\\Re_{\\epsilon}(\\psi;P_{\\mathbf{y}|\\mathbf{x},\\mathbf{\\widetilde{y}}})}\\\\ &{\\leq\\Bigl\\{57\\frac{L K^{p}}{\\epsilon^{p-1}}+91L^{\\prime}\\int_{0}^{\\infty}\\sqrt{\\log N(s;\\Psi,\\|\\cdot\\|_{\\infty})}d s\\Bigr\\}\\cdot\\frac{1}{\\sqrt{n}}+2M\\sqrt{\\frac{\\log(1/\\eta)}{2n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the proof is completed. ", "page_idx": 22}, {"type": "text", "text": "A.6Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For ease of presentation, in this proof we omit the dependence on $\\mathbf{x}$ and $\\widetilde{\\mathbf{y}}$ in the notation. In particular, welet $P_{0}\\triangleq P_{0}(\\mathbf{x},\\widetilde{\\mathbf{y}}),P_{1}\\triangleq P_{1}(\\mathbf{x},\\widetilde{\\mathbf{y}})$ and $\\psi\\triangleq\\psi(\\mathbf{x})$ . Let the objective function in (8) be denoted as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(\\gamma;\\psi)\\triangleq\\gamma\\epsilon^{p}+P_{0}\\operatorname*{max}\\{\\mathcal{T}(1-\\psi),\\mathcal{T}(\\psi)-\\gamma\\kappa^{p}\\}+P_{1}\\operatorname*{max}\\{\\mathcal{T}(1-\\psi)-\\gamma\\kappa^{p},\\mathcal{T}(\\psi)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To complete the proof, we begin by investigating the inner optimization problem in (8) by finding the optimal value of $\\gamma,\\gamma_{\\psi}^{\\ast}$ , as defined in Section 3.2, that minimizes $g(\\gamma;\\psi)$ for each given $\\psi$ , and then address the outer optimization problem in (8) by finding the optimal value $\\psi^{\\star}$ that minimizes $g(\\gamma_{\\psi}^{*};\\psi)$ . To this end, we eliminate the max operators in $g(\\gamma;\\psi)$ based on the values of $\\psi$ , and use the assumption that $\\tau$ is a decreasing function in (7). As $\\psi$ takes its value in $[0,1]$ , we re-write the optimization problem (8) as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\operatorname*{inf}\\ \\ \\ \\operatorname*{inf}_{\\L}g(\\gamma;\\psi)}\\\\ &{\\ \\ \\ \\psi\\!\\in\\![0,\\frac{1}{2}]\\cup[\\frac{1}{2},1]\\,\\gamma\\!\\geq\\!0}\\\\ &{\\!\\!=\\!\\operatorname*{min}\\left\\{\\!\\operatorname*{min}_{\\L}\\ \\operatorname*{inf}_{\\L}g(\\gamma;\\psi),\\ \\underset{\\psi\\in[\\frac{1}{2},1]}{\\operatorname*{min}}\\!\\!\\!\\operatorname*{inf}_{\\L}g(\\gamma;\\psi)\\right\\},}\\\\ &{\\!\\!\\triangleq\\!\\operatorname*{min}\\left\\{g(\\gamma_{\\psi_{1}^{*}}^{*};\\psi_{1}^{*}),g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(\\gamma_{\\psi_{1}^{*}}^{*},\\psi_{1}^{*})$ and $(\\gamma_{\\psi_{2}^{*}}^{*},\\psi_{2}^{*})$ are  the  arguments  o $\\begin{array}{r}{\\operatorname*{min}_{\\psi_{1}\\in[0,\\frac{1}{2}]}\\operatorname*{inf}_{\\gamma\\geq0}g(\\gamma;\\psi_{1})}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{min}_{\\psi_{2}\\in[\\frac{1}{2},1]}\\operatorname*{inf}_{\\gamma\\geq0}g(\\gamma;\\psi_{2})}\\end{array}$ , respectively. We complete the proof by considering the following two cases. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma_{0}=\\frac{\\mathcal{T}(\\psi_{1})-\\mathcal{T}(1-\\psi_{1})}{\\kappa^{p}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\ng(\\gamma;\\psi_{1})=\\left\\{\\gamma\\mathfrak{c}^{p}+P_{0}\\{\\mathcal{T}(\\psi_{1})-\\gamma\\kappa^{p}\\}+P_{1}\\mathcal{T}(\\psi_{1})=\\mathcal{T}(\\psi_{1})+\\gamma\\kappa^{p}(\\varrho(\\epsilon)-P_{0}),\\;\\mathrm{if}\\;\\gamma\\leq\\gamma_{0};\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For given $\\psi_{1}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to\\gamma_{0}^{+}}g(\\gamma;\\psi_{1})=\\operatorname*{lim}_{\\gamma\\to\\gamma_{0}^{-}}g(\\gamma;\\psi_{1})=\\varrho(\\epsilon)\\left\\{{\\mathcal T}(\\psi_{1})-{\\mathcal T}(1-\\psi_{1})\\right\\}+P_{0}{\\mathcal T}(1-\\psi_{1})+P_{1}{\\mathcal T}(\\psi_{1}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "showing that $g(\\gamma;\\psi_{1})$ is continuous at $\\gamma_{0}$ . Therefore, for any given $\\psi_{1},\\,g(\\gamma;\\psi_{1})$ is continuous in $\\gamma$ over IR+. ", "page_idx": 22}, {"type": "text", "text": "Then, for any given $\\psi_{1}\\in[0,\\frac{1}{2}]$ , corresponding to the first term in (A18), we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\gamma\\geq0}{\\operatorname*{inf}}g(\\gamma;\\psi_{1})=\\operatorname*{min}\\left\\lbrace\\underset{\\gamma>\\gamma_{0}}{\\operatorname*{inf}}g(\\gamma;\\psi_{1}),\\underset{\\gamma\\in[0,\\gamma_{0}]}{\\operatorname*{inf}}g(\\gamma;\\psi_{1})\\right\\rbrace}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{min}\\left\\lbrace g(\\gamma_{0};\\psi_{1}),\\underset{\\gamma\\in[0,\\gamma_{0}]}{\\operatorname*{min}}g(\\gamma;\\psi_{1})\\right\\rbrace}\\\\ &{\\quad\\quad\\quad=\\underset{\\gamma\\in[0,\\gamma_{0}]}{\\operatorname*{min}}g(\\gamma;\\psi_{1})}\\\\ &{\\quad\\quad\\quad\\triangleq g(\\gamma_{\\psi_{1}}^{*};\\psi_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use the continuity of $g(\\gamma;\\psi_{1})$ in $\\gamma$ the fact that $g(\\gamma;\\psi_{1})$ is increasing in $\\gamma$ when $\\gamma>\\gamma_{0}$ and the fact that a continuous function attains its infimum within any closed and bounded set in $\\mathbb{R}$ Here, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\gamma_{\\psi_{1}}^{\\ast}\\triangleq\\arg\\operatorname*{min}_{\\gamma\\in[0,\\gamma_{0}]}g(\\gamma;\\psi_{1})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any $\\psi_{1}\\in[0,\\frac{1}{2}]$ ", "page_idx": 23}, {"type": "text", "text": "We complete the proof by the following two steps to examine the range of $P_{0}$ ", "page_idx": 23}, {"type": "text", "text": "Step 1: If $P_{0}<\\varrho(\\epsilon)$ . then, by (A19), for any given $\\psi_{1}\\in[0,\\frac{1}{2}]$ \uff0c $g(\\gamma;\\psi_{1})$ is increasing in $\\gamma$ over $[0,\\gamma_{0}]$ showing that the optimal value in (A21 is $\\gamma_{\\psi_{1}}^{*}=0$ . Furthermore, because $g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})={\\cal T}(\\dot{\\psi}_{1})$ for any $\\psi_{1}\\in[0,\\frac{1}{2}]$ , we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\operatorname*{min}_{\\psi_{1}\\in[0,\\frac{1}{2}]}\\operatorname*{inf}_{\\mathbf{\\sigma}}g(\\gamma;\\psi)}\\\\ &{=\\displaystyle\\operatorname*{min}_{\\psi_{1}\\in[0,\\frac{1}{2}]}g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})}\\\\ &{=\\displaystyle\\operatorname*{min}_{\\psi_{1}\\in[0,\\frac{1}{2}]}\\mathcal{T}(\\psi_{1})}\\\\ &{=\\mathcal{T}(1/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consequently, $\\left(0,{\\frac{1}{2}}\\right)$ minimizes $g(\\gamma;\\psi)$ over $[0,\\gamma_{0}]\\times[0,\\textstyle{\\frac{1}{2}}]$ ,i.e., $\\psi_{1}^{*}=\\textstyle{\\frac{1}{2}}$ and $\\gamma_{\\psi_{1}^{*}}^{*}=0$ ", "page_idx": 23}, {"type": "text", "text": "Step 2: If $P_{0}\\,\\geq\\,\\varrho(\\epsilon)$ , then, by (A19), $g(\\gamma;\\psi_{1})$ is decreasing in $\\gamma$ when $\\gamma\\le\\gamma_{0}$ , showing that the optimal value in (A21) is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\gamma_{\\psi_{1}}^{*}=\\gamma_{0},\\;\\mathrm{with}\\;\\gamma_{0}=\\frac{\\mathcal{T}(\\psi_{1})-\\mathcal{T}(1-\\psi_{1})}{\\kappa^{p}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\ng(\\gamma_{\\psi_{1}}^{\\ast};\\psi_{1})=(P_{1}+\\varrho(\\epsilon))\\mathcal{T}(\\psi_{1})+(P_{0}-\\varrho(\\epsilon))\\mathcal{T}(1-\\psi_{1})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consequently, the derivative of $g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ with respect to $\\psi_{1}$ .s ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})=(P_{1}+\\varrho(\\epsilon))7^{\\prime}(\\psi_{1})-(P_{0}-\\varrho(\\epsilon))7^{\\prime}(1-\\psi_{1}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "leading to $\\begin{array}{r}{\\left.g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\right|_{\\psi_{1}=0}=(1-P_{0}+\\varrho(\\epsilon))\\mathcal{T}^{\\prime}(0)-(P_{0}-\\varrho(\\epsilon))\\mathcal{T}^{\\prime}(1)\\left.\\mathrm{and}\\left.g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\right|_{\\psi_{1}=1/2}=}\\end{array}$ $(1+2\\varrho(\\epsilon)-2P_{0})T^{\\prime}({\\frac{1}{2}})$ . Solving ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left.g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\right|_{\\psi_{1}=0}=0\\mathrm{~and~}g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\big|_{\\psi_{1}=1/2}=0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $P_{0}$ leads to solutions ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{0}^{(1)}\\triangleq\\varrho(\\epsilon)+\\frac{T^{\\prime}(0)}{T^{\\prime}(1)+T^{\\prime}(0)}\\mathrm{~and~}P_{0}^{(2)}\\triangleq\\varrho(\\epsilon)+\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "respectively. ", "page_idx": 23}, {"type": "text", "text": "Next, we identify $\\psi_{1}^{*}$ and $\\gamma_{\\psi_{1}^{*}}^{*}$ by examining $P_{0}$ relaiveto $P_{0}^{(1)}$ and $P_{0}^{(2)}$ in combination with the convexity or concavity of function $\\tau$ by the following two steps. ", "page_idx": 23}, {"type": "text", "text": "Step 2.1: Assume $\\tau$ is concave. Then by twice differentiability of $\\tau$ ${\\cal T}^{\\prime\\prime}(\\psi_{1})\\leq0$ for $\\psi_{1}\\in[0,1]$ leading to ${\\cal T}^{\\prime}(1)\\,\\leq\\,{\\cal T}^{\\prime}(0)\\,<\\,0$ and hence $P_{0}^{(1)}\\,\\leq\\,P_{0}^{(2)}$ .Additinay, $g_{\\psi_{1}}^{\\prime\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\,=\\,(P_{1}\\,+$ $\\varrho(\\epsilon))T^{\\prime\\prime}(\\psi_{1})+(P_{0}-\\varrho(\\epsilon))T^{\\prime\\prime}(1-\\psi_{1})\\leq0$ , and thus, $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is non-increasing in $\\psi_{1}$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ ", "page_idx": 23}, {"type": "text", "text": "\u00b7f $\\varrho(\\epsilon)\\leq P_{0}\\leq P_{0}^{(1)}$ , then $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\leq0$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ , and thus, $g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is nonincreasing in $\\psi_{1}$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ . Therefore, $\\begin{array}{r}{\\operatorname*{inf}_{0\\leq\\psi_{1}\\leq1/2}g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})=g(\\gamma_{1/2}^{*};\\frac{1}{2})=\\mathcal{T}(\\frac{1}{2})}\\end{array}$ Thus, $\\psi_{1}^{*}=\\textstyle{\\frac{1}{2}}$ and $\\gamma_{\\psi_{1}^{*}}^{*}=0$ by (A22). ", "page_idx": 23}, {"type": "text", "text": "\u00b7 If $P_{0}\\ge P_{0}^{(2)}$ .then $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\geq0$ showing that $g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is non-dereasin in $\\psi_{1}$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ . Therefore, $\\begin{array}{r}{\\operatorname*{inf}_{0\\leq\\psi_{1}\\leq1/2}g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})=g(\\gamma_{0}^{*};0)=(P_{1}+\\varrho(\\epsilon))\\mathcal{T}(0)+(P_{0}-\\epsilon-\\psi_{1})(-\\varrho(\\epsilon))^{2}}\\end{array}$ $\\varrho(\\epsilon))\\mathcal{T}(1)$ Thus, $\\psi_{1}^{*}=0$ and $\\begin{array}{r}{\\gamma_{\\psi_{1}^{*}}^{*}=\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}}}\\end{array}$ by (A22).   \n$P_{0}^{(1)}\\,<\\,P_{0}\\,<\\,P_{0}^{(2)}$ .then $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is on-inreasing in $\\psi_{1}$ for $\\psi_{1}\\,\\in\\,[0,\\frac{1}{2}]$ Wwith $\\left.g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\right|_{\\psi_{1}=0}>0$ and $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\big|_{\\psi_{1}=1/2}<0$ Therefore, $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})=0$ has a unique solution on $[0,{\\frac{1}{2}}]$ , denoted $\\psi_{1}^{\\circ}$ , and furthermore, $g\\bigl(\\gamma_{\\psi_{1}}^{*};\\psi_{1}\\bigr)$ is increasing in $\\psi_{1}$ for $\\psi_{1}\\in[0,\\psi_{1}^{\\diamond}]$ and decreasing on $[\\psi_{1}^{\\circ},\\frac{1}{2}]$ .Therefore, the infimum of $g_{\\psi_{1}}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ on $\\psi_{1}\\in$ $[0,{\\frac{1}{2}}]$ $\\psi_{1}=0$ $\\begin{array}{r}{\\psi_{1}=\\frac{1}{2}}\\end{array}$ $g(\\gamma_{1/2}^{*};\\frac{1}{2})\\leq g(\\gamma_{0}^{*};0)$ $P_{0}\\le\\varrho(\\epsilon)\\!+\\!\\frac{\\tau(0)\\!-\\!\\mathcal{T}(1/2)}{\\mathcal{T}(0)\\!-\\!\\mathcal{T}(1)}$ \uff1a\uff0c the optimal alue for $\\psi_{1}$ in Case 1 is $\\psi_{1}^{*}=\\textstyle{\\frac{1}{2}}$ with $\\gamma_{\\psi_{1}^{*}}^{*}=0$ otherwise,theoptimal valueis $\\psi_{1}^{*}=0$ with $\\begin{array}{r}{\\gamma_{\\psi_{1}^{*}}^{*}=\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}}}\\end{array}$ by (A22). ", "page_idx": 24}, {"type": "text", "text": "Summarizing the discussion in Step 2.1, we obtain that when $\\psi_{1}\\in[0,\\frac{1}{2}]$ and $\\tau$ isconcave, ", "page_idx": 24}, {"type": "text", "text": "(i $\\begin{array}{r}{P_{0}>\\varrho(\\epsilon)+\\frac{\\mathcal{T}(0)-\\mathcal{T}(1/2)}{\\mathcal{T}(0)-\\mathcal{T}(1)}}\\end{array}$ \uff0c $(\\psi_{1}^{*},\\gamma_{\\psi_{1}^{*}}^{*})$ in (A18) is give by $\\psi_{1}^{*}=0$ $\\begin{array}{r}{\\gamma_{\\psi_{1}^{*}}^{*}=\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}}}\\end{array}$ \uff0c yielding $g(\\gamma_{\\psi_{1}^{*}}^{*};\\psi_{1}^{*})=(P_{1}+\\varrho(\\epsilon))\\mathcal{T}(0)+(P_{0}-\\varrho(\\epsilon))\\mathcal{T}(1)$   \n(i) otherwise, $\\psi_{1}^{*}=\\textstyle{\\frac{1}{2}}$ and $\\gamma_{\\psi_{1}^{*}}^{*}=0$ , yielding $g(\\gamma_{\\psi_{1}^{*}}^{*};\\psi_{1}^{*})=T(\\textstyle{\\frac{1}{2}})$ ", "page_idx": 24}, {"type": "text", "text": "Step 2.2: Assume $\\tau$ is convex. Then by twice differentiability of $\\tau$ $\\mathcal{T}^{\\prime\\prime}(\\psi_{1})\\geq0$ for $\\psi_{1}\\in[0,1]$ lcading to ${\\mathcal T}^{\\prime}(0)\\,\\leq\\,{\\mathcal T}^{\\prime}(1)\\,<\\,0$ and hence $P_{0}^{(2)}\\,\\leq\\,P_{0}^{(1)}$ .Aditionaly, $g_{\\psi_{1}}^{\\prime\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\,=\\,(P_{1}\\,+$ $\\varrho(\\epsilon))T^{\\prime\\prime}(\\psi_{1})+(P_{0}-\\varrho(\\epsilon))T^{\\prime\\prime}(1-\\psi_{1})\\geq0$ , and thus, $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is non-decreasing in $\\psi_{1}$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ ", "page_idx": 24}, {"type": "text", "text": "\u00b7If $P_{0}\\ge P_{0}^{(1)}$ ,then $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\geq0$ $\\psi_{1}\\in[0,\\frac{1}{2}]$ , and thus, $g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is non-decreasing   \nin $\\psi_{1}$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ . Therefore, $\\begin{array}{r}{\\operatorname*{inf}_{0\\leq\\psi_{1}\\leq1/2}g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})=g(\\gamma_{0}^{*};0)=(P_{1}+\\varrho(\\epsilon))\\mathcal{T}(0)+}\\end{array}$   \n(Po - @()T(1). Thus, b =0and \\~\u00b1 $\\begin{array}{r}{\\gamma_{\\psi_{1}^{*}}^{*}=\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}}}\\end{array}$ T()-T(1) by (A22).   \n\u00b7If $\\varrho(\\epsilon)\\leq P_{0}\\leq P_{0}^{(2)}$ then $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\leq0$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ and thus, $g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is non  \nincreasing in $\\psi_{1}$ for $\\psi_{1}\\in[0,\\frac{1}{2}]$ . Therefore, $\\begin{array}{r}{\\operatorname*{inf}_{0\\leq\\psi_{1}\\leq\\frac{1}{2}}g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})=g(\\gamma_{1/2}^{*};\\frac{1}{2})=\\mathcal{T}(\\frac{1}{2})}\\end{array}$   \nThus, $\\psi_{1}^{*}=\\textstyle{\\frac{1}{2}}$ and $\\gamma_{\\psi_{1}^{*}}^{*}=0$ by (A22).   \n\u00b7If $P_{0}^{(2)}\\,<\\,P_{0}\\,<\\,P_{0}^{(1)}$ .then $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$ is non-derasing in $\\psi_{1}$ for $\\psi_{1}\\,\\in\\,[0,\\frac{1}{2}]$ with $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\big|_{\\psi_{1}=0}~<~0$ and $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\big|_{\\psi_{1}=1/2}\\;>\\;0$ .Therefore, $g_{\\psi_{1}}^{\\prime}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\;=\\;0$   \nhas a unique solution on $[0,{\\frac{1}{2}}]$ , denoted $\\psi_{1}^{\\circ}$ , and furthermore, $g\\bigl(\\gamma_{\\psi_{1}}^{*};\\psi_{1}\\bigr)$ is decreasing in $\\psi_{1}$ for $\\psi_{1}\\,\\in\\,[0,\\psi_{1}^{\\diamond}]$ and increasing on $[\\psi_{1}^{\\circ},1/2]$ . Then, the infmum of $g_{\\psi_{1}}(\\gamma_{\\psi_{1}}^{*};\\psi_{1})$   \non $\\psi_{1}\\,\\in\\,[0,\\frac{1}{2}]$ is taken at $\\psi_{1}\\;=\\;\\psi_{1}^{\\diamond}$ , that is, $\\operatorname*{inf}_{0\\leq\\psi_{1}\\leq1/2}g(\\gamma_{\\psi_{1}}^{*};\\psi_{1})\\;=\\;g(\\gamma_{\\psi_{1}^{\\diamond}}^{*};\\psi_{1}^{\\diamond})\\;=$ $(P_{1}+\\varrho(\\epsilon))T(\\psi_{1}^{\\diamond})+(P_{0}-\\varrho(\\epsilon))T(1-\\psi_{1}^{\\diamond})$ . Thus, $\\psi_{1}^{*}=\\psi_{1}^{\\circ}$ with $\\begin{array}{r}{\\gamma_{\\psi_{1}^{*}}^{*}=\\frac{\\mathcal{T}(\\psi_{1}^{\\diamond})-\\mathcal{T}(1-\\psi_{1}^{\\diamond})}{\\kappa^{p}}}\\end{array}$ by (A22). ", "page_idx": 24}, {"type": "text", "text": "Case 2: $\\psi_{2}\\in[\\textstyle{\\frac{1}{2}},1]$ ", "page_idx": 24}, {"type": "text", "text": "In this case, we set $\\overline{{\\psi}}_{2}\\triangleq1-\\psi_{2}$ , yielding $\\overline{{\\psi}}_{2}\\in[0,\\frac{1}{2}]$ , and the objective function $g(\\gamma;\\psi_{2})$ defined in (A17) can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{g(\\gamma;\\psi_{2})=\\gamma\\epsilon^{p}+P_{0}\\operatorname*{max}\\{T(1-\\psi_{2}),T(\\psi_{2})-\\gamma\\kappa^{p}\\}+P_{1}\\operatorname*{max}\\{T(1-\\psi_{2})-\\gamma\\kappa^{p},T(\\psi_{2})\\}}\\\\ &{}&{\\quad=\\gamma\\epsilon^{p}+P_{1}\\operatorname*{max}\\{T(1-\\overline{{\\psi}}_{2}),T(\\overline{{\\psi}}_{2})-\\gamma\\kappa^{p}\\}+P_{0}\\operatorname*{max}\\{T(1-\\overline{{\\psi}}_{2})-\\gamma\\kappa^{p},T(\\overline{{\\psi}}_{2})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, the derivation in Case 1 for any $\\psi_{1}$ in $[0,{\\frac{1}{2}}]$ can be applied to $\\overline{{\\psi}}_{2}$ by modifying the derivations based on the range of $P_{0}$ to be that for $P_{1}$ , as outlined below. ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Step 1: If $P_{1}<\\varrho(\\epsilon)$ , then following the results for $\\psi_{1}^{*}$ and $\\gamma_{\\psi_{1}^{*}}^{*}$ in Case 1, with only $\\psi_{1}^{*},P_{1}$ and $P_{0}$ there replaced by $\\overline{{\\psi}}_{2}^{*}$ \uff0c $P_{0}$ and $P_{1}$ respectively, we obtain that $\\begin{array}{r}{\\overline{{\\psi}}_{2}^{*}=\\frac12}\\end{array}$ and $\\gamma_{\\overline{{{\\psi}}}_{2}^{*}}^{*}=0$ Hence, $\\psi_{2}^{*}$ is taken as $1-\\overline{{\\psi}}_{2}^{*}=\\textstyle{\\frac{1}{2}}$ and $\\gamma_{\\psi_{2}^{*}}^{*}=0$ yieding $\\begin{array}{r}{g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})=g(0;\\frac{1}{2})=\\mathcal{T}(\\frac{1}{2})}\\end{array}$ \u00b7Step 2: If P \u2265 @(e), then, by (A22), : is set as s $\\begin{array}{r l r}{\\gamma_{\\psi_{2}^{*}}^{*}\\;\\;=\\;\\;\\frac{\\mathcal{T}(\\overline{{\\psi}}_{2}^{*})-\\mathcal{T}(1-\\overline{{\\psi}}_{2}^{*})}{\\kappa^{p}}\\;\\;=}&{}\\end{array}$ $\\begin{array}{r}{\\frac{\\mathcal{T}(1-\\psi_{2}^{*})-\\mathcal{T}(\\psi_{2}^{*})}{\\kappa^{p}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\uff0cStep 2.1: Assume $\\tau$ is concave. We can directly derive the following result from the summary in Step 2.1 of Case 1. $\\begin{array}{r}{P_{1}\\leq\\varrho(\\epsilon)+\\frac{T(0)-T(1/2)}{T(0)-T(1)}}\\end{array}$ ten $\\textstyle{\\overline{{\\psi}}}_{2}^{*}={\\frac{1}{2}}$ $\\gamma_{\\psi_{2}^{*}}^{*}=0$ Hence, $\\psi_{2}^{*}=1-\\overline{{\\psi}}_{2}^{*}=\\frac{1}{2}$ and $\\begin{array}{r}{g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})=g(0;\\frac{1}{2})=\\mathcal{T}(\\frac{1}{2})}\\end{array}$ -f $\\begin{array}{r}{P_{1}\\,>\\,\\varrho(\\epsilon)\\,+\\,\\frac{\\mathcal{T}(0)-\\mathcal{T}(1/2)}{\\mathcal{T}(0)-\\mathcal{T}(1)}}\\end{array}$ $\\overline{{\\psi}}_{2}^{*}\\,=\\,0$ $\\begin{array}{r}{\\gamma_{\\psi_{2}^{*}}^{*}\\,=\\,\\frac{{\\mathcal{T}}(0)-{\\mathcal{T}}(1)}{\\kappa^{p}}}\\end{array}$ Hence, $\\psi_{2}^{*}\\;=\\;$ $1\\!-\\!\\overline{{\\psi}}_{2}^{*}=1$ and $\\begin{array}{r}{g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})=g(\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}};1)=(P_{0}+\\varrho(\\epsilon))\\mathcal{T}(0)+(P_{1}-\\varrho(\\epsilon))\\mathcal{T}(1)}\\end{array}$ \u00b7 Step 2.2: Assume $\\tau$ is convex. From the results on $\\psi_{1}^{*}$ and $\\gamma_{\\psi_{1}^{*}}^{*}$ in Step 2.2 of Case 1, we obtain the following conclusion. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u21611 $\\begin{array}{r}{P_{1}\\ge\\varrho(\\epsilon)\\!+\\!\\frac{T^{\\prime}(0)}{T^{\\prime}(1)+T^{\\prime}(0)}}\\end{array}$ $\\overline{{\\psi}}_{2}^{*}=0$ $\\begin{array}{r}{\\gamma_{\\psi_{2}^{*}}^{*}=\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}}}\\end{array}$ T()-T(1) Hence, (\\* = 1-2 = $\\begin{array}{r}{g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})=g(\\frac{\\mathcal{T}(0)-\\mathcal{T}(1)}{\\kappa^{p}};1)=(P_{0}+\\varrho(\\epsilon))\\mathcal{T}(0)+(P_{1}-\\varrho(\\epsilon))\\mathcal{T}(1).}\\end{array}$ -If $\\varrho(\\epsilon)\\leq P_{1}\\leq\\varrho(\\epsilon)+\\textstyle{\\frac{1}{2}}$ then $\\begin{array}{r}{\\overline{{\\psi}}_{2}^{*}=\\frac12}\\end{array}$ and $\\gamma_{\\psi_{2}^{*}}^{*}=0$ Hence, $\\psi_{2}^{*}=1-\\overline{{\\psi}}_{2}^{*}=\\textstyle{\\frac{1}{2}}$ , and $\\begin{array}{r}{g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})=g(0;\\frac{1}{2})=\\mathcal{T}(\\frac{1}{2})}\\end{array}$   \n- If $\\begin{array}{r}{\\varrho(\\epsilon)+\\frac{1}{2}<P_{1}<\\varrho(\\epsilon)+\\frac{7^{\\prime}(0)}{7^{\\prime}(1)+7^{\\prime}(0)}}\\end{array}$ then $\\overline{{\\psi}}_{2}^{*}=\\overline{{\\psi}}_{2}^{\\diamond}$ and $\\begin{array}{r}{\\gamma_{\\psi_{2}^{*}}^{*}=\\frac{\\mathcal{T}(\\overline{{\\psi}}_{2}^{\\diamond})-\\mathcal{T}(1-\\overline{{\\psi}}_{2}^{\\diamond})}{\\kappa^{p}}}\\end{array}$ \uff0c where $\\overline{{\\psi}}_{2}^{\\diamond}$ is the unique solution to $(P_{0}+\\varrho(\\epsilon))\\mathcal{T}^{\\prime}(\\overline{{\\psi}}_{2})-(P_{1}-\\varrho(\\epsilon))\\mathcal{T}^{\\prime}(1-\\overline{{\\psi}}_{2})=0$ on [0, ]. Hence, 2 =  and T(-9)-T(), where 2 = 1 -2 is the unique solution to $-(P_{0}+\\varrho(\\epsilon))\\mathcal{T}^{\\prime}(1-\\psi_{2})+(P_{1}-\\varrho(\\epsilon))\\mathcal{T}^{\\prime}(\\psi_{2})=0$ on $[{\\textstyle{\\frac{1}{2}}},1]$ . Then $g(\\gamma_{\\psi_{2}^{*}}^{*};\\psi_{2}^{*})=(P_{0}+\\varrho(\\epsilon))\\mathcal{T}(1-\\psi_{2}^{\\circ})+(P_{1}-\\varrho(\\epsilon))\\mathcal{T}(\\psi_{2}^{\\circ})$ ", "page_idx": 25}, {"type": "text", "text": "In summary, we present the derived results in Tables 3 and 4 for the scenarios where $\\tau$ isconcave and convex, respectively. ", "page_idx": 25}, {"type": "table", "img_path": "2NKumsITFw/tmp/307e59eb9a51429b8895dfae5c34d6886bb4d8165d4bc86fac4f93c4f6b329f4.jpg", "table_caption": [], "table_footnote": ["Table 3: Summarized results in two cases when $\\overline{{\\tau}}$ is concave. "], "page_idx": 25}, {"type": "table", "img_path": "2NKumsITFw/tmp/a1e9dafc14632a98fbcd8d7856970b2afc79c8bb54d1c7d889d19893b034e5fb.jpg", "table_caption": [], "table_footnote": ["Table 4: Summarized results in two cases when $\\overline{{\\mathcal{T}}}$ is convex. "], "page_idx": 25}, {"type": "text", "text": "Finally, for any given input $\\mathbf{x}$ , applying the preceding results to the optimal solution $(\\gamma_{\\psi^{*}}^{*},\\psi^{*})=$ $\\arg\\operatorname*{inf}_{\\psi\\in[0,1]}\\operatorname*{inf}_{\\gamma\\geq0}g(\\gamma,\\psi)$ in (A18), we obtain that the optimal action on the given instance $\\mathbf{x}$ is given as below: for concave $\\tau$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\psi^{\\star}(\\mathbf{x})=\\left\\{\\begin{array}{l l}{\\displaystyle0\\mathrm{,~if~}P_{0}\\geq\\varrho(\\epsilon)+\\frac{T(0)-T(1/2)}{T(0)-T(1)}\\mathrm{;}}\\\\ {\\displaystyle1\\mathrm{,~if~}P_{1}\\geq\\varrho(\\epsilon)+\\frac{T(0)-T(1/2)}{T(0)-T(1)}\\mathrm{;}}\\\\ {\\displaystyle1/2\\mathrm{,~otherwise;}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and for convex $\\tau$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\psi^{\\star}(\\mathbf{x})=\\left\\{\\begin{array}{l l}{\\displaystyle0\\mathrm{,~if~}P_{0}\\geq\\varrho(\\epsilon)+\\frac{\\mathcal{T}^{\\prime}(0)}{\\mathcal{T}^{\\prime}(0)+\\mathcal{T}^{\\prime}(1)}\\mathrm{;}}\\\\ {\\displaystyle t_{0}^{*}\\mathrm{,~if~}\\varrho(\\epsilon)+1/2<P_{0}<\\varrho(\\epsilon)+\\frac{\\mathcal{T}^{\\prime}(0)}{\\mathcal{T}^{\\prime}(0)+\\mathcal{T}^{\\prime}(1)}\\mathrm{;}}\\\\ {\\displaystyle1\\mathrm{,~if~}P_{1}\\geq~\\varrho(\\epsilon)+\\frac{\\mathcal{T}^{\\prime}(0)}{\\mathcal{T}^{\\prime}(0)+\\mathcal{T}^{\\prime}(1)}\\mathrm{;}}\\\\ {\\displaystyle t_{1}^{*}\\mathrm{,~if~}\\varrho(\\epsilon)+1/2<P_{1}<\\varrho(\\epsilon)+\\frac{\\mathcal{T}^{\\prime}(0)}{\\mathcal{T}^{\\prime}(0)+\\mathcal{T}^{\\prime}(1)}\\mathrm{;}}\\\\ {\\displaystyle1/2\\mathrm{,~otherwise,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $t_{0}^{*}$ is the unique solution of $(P_{0}-\\varrho(\\epsilon))T^{\\prime}(1-t)=(P_{1}+\\varrho(\\epsilon))T^{\\prime}(t)$ on $t\\in(0,\\frac{1}{2})$ , and $t_{1}^{*}$ is the unique solution of $(P_{0}+\\varrho(\\epsilon))T^{\\prime}(1-t)=(P_{1}-\\varrho(\\epsilon))T^{\\prime}(t)$ on $\\textstyle t\\in({\\frac{1}{2}},1)$ . Hence, the proof is established. ", "page_idx": 26}, {"type": "text", "text": "A.7Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For ease of presentation, we omit the dependence on $\\mathbf{x}$ and $\\widetilde{\\mathbf{y}}$ in the notation for now. Specifically, for $j\\in[K]$ , we let $P_{j}\\triangleq P_{j}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\triangleq P(\\mathrm{Y}=j|\\mathbf{x},\\widetilde{\\mathbf{y}})$ and $\\psi_{j}\\triangleq\\psi(\\mathbf{x})_{j}$ . Let the objective function in (9) be denoted as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{g(\\gamma;\\psi)\\triangleq\\gamma\\epsilon^{p}+\\sum_{j=1}^{K}P_{j}\\operatorname*{max}\\{1-\\psi_{1}-\\gamma\\kappa^{p},\\dots,1-\\psi_{j-1}-\\gamma\\kappa^{p},}}}\\\\ &{}&{1-\\psi_{j},1-\\psi_{j+1}-\\gamma\\kappa^{p},\\dots,1-\\psi_{K}-\\gamma\\kappa^{p}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We complete the proof in four steps. In Step 1, for each given $\\psi$ , we investigate the inner optimization problem in (9) by finding the optimal value of $\\gamma$ , defined as $\\begin{array}{r}{\\gamma_{\\psi}^{\\star}\\triangleq\\arg\\operatorname*{min}_{\\gamma\\ge0}g(\\gamma;\\psi)}\\end{array}$ Then, in Step 2, by substituting $\\gamma_{\\psi}^{\\star}$ into $g(\\gamma;\\psi)$ , we find that the outer optimization problem in (9) can be written in a linear programming format under certain transformations. Next, in Step 3, we find the extreme points of the associated linear programming, and finally, in Step 4, we obtain the solution format of the optimal action $\\psi^{\\star}$ ", "page_idx": 26}, {"type": "text", "text": "Step 1: For any $\\psi\\in\\Psi$ , finding the optimal value of $\\gamma$ , defined as $\\begin{array}{r}{\\gamma_{\\psi}^{\\star}\\triangleq\\arg\\operatorname*{min}_{\\gamma\\ge0}g(\\gamma;\\psi).}\\end{array}$ Given $\\psi$ and $\\mathbf{x}$ , we sort $\\{\\psi_{1},\\ldots,\\psi_{K}\\}$ in an decreasing order, denoted $\\psi^{(1)}\\,\\geq\\,.\\,.\\,\\geq\\,\\psi^{(K)}$ , and hence, $1-\\psi^{(1)}\\leq\\ldots\\leq1-\\psi^{(K)}$ . Assume that $\\{\\psi^{(1)},\\ldots,\\psi^{(K)}\\}$ corresponds to $\\{\\psi_{1},\\ldots,\\psi_{K}\\}$ via a permutation $\\chi$ , that is, $\\psi^{(j)}=\\psi_{\\chi(j)}$ for $j\\in[K]$ . Correspondingly, the $P_{j}$ 's with the associated indexes are denoted $P^{(j)}\\triangleq P_{\\chi(j)}$ for $j\\in[K]$ . Then, for the $\\chi(j)$ -th element in the summation of (A23), the maximum is taken between $1-\\psi^{(K)}-\\gamma\\kappa^{p}$ and $1-\\psi^{(j)}$ ", "page_idx": 26}, {"type": "text", "text": "First, for given $\\psi$ , we examine the continuity of $g(\\gamma;\\psi)$ in $\\gamma$ by eliminating the max operators in (A23), which is conducted by comparing $1-\\psi^{(K)}-\\gamma\\kappa^{p}$ and $1-\\psi^{(j)}$ for $j\\in[K]$ as follows. ", "page_idx": 26}, {"type": "text", "text": "f1-(\u22651-\uff08\uff09-ie\u2265 , then $\\begin{array}{r}{1-\\psi_{j}\\geq1-\\psi^{(1)}\\geq1-\\psi^{(K)}-\\gamma\\kappa^{p}\\geq}\\end{array}$ $1-\\psi^{(j^{\\prime})}-\\gamma\\kappa^{p}$ for $j,j^{\\prime}\\in[K]$ and hence, (A23 becomes ", "page_idx": 26}, {"type": "equation", "text": "$$\ng(\\gamma;\\psi)=\\gamma\\epsilon^{p}+\\sum_{j=1}^{K}P_{j}(1-\\psi_{j}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is continuous in $\\gamma$ for $\\gamma\\geq{\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}}$ ", "page_idx": 26}, {"type": "text", "text": "On the other hand, if $1-\\psi^{(1)}<1-\\psi^{(K)}-\\gamma\\kappa^{p}$ i.e, $\\begin{array}{r}{0\\le\\gamma<\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}}\\end{array}$ , then we express the range of $\\gamma$ as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Bigg[0,\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}\\Bigg)=\\cup_{s\\in[K-1]}\\Bigg[\\frac{\\psi^{(s+1)}-\\psi^{(K)}}{\\kappa^{p}},\\frac{\\psi^{(s)}-\\psi^{(K)}}{\\kappa^{p}}\\Bigg).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then we consider $\\gamma$ in each interval $\\Biggl[\\frac{\\psi^{(s+1)}\\!-\\!\\psi^{(K)}}{\\kappa^{p}},\\frac{\\psi^{(s)}\\!-\\!\\psi^{(K)}}{\\kappa^{p}}\\Biggr]$ for $s\\ \\in\\ [K\\mathrm{~-~}1]$ . In this case, $1-\\psi^{(s)}<1-\\psi^{(K)}-\\gamma\\kappa^{p}\\leq1-\\psi^{(s\\bar{+}1)}$ , and (A23) becomes ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle g(\\gamma;\\psi)=\\gamma\\epsilon^{p}+\\sum_{j=1}^{s}P^{(j)}(1-\\psi^{(K)}-\\gamma\\kappa^{p})+\\sum_{j=s+1}^{K}P^{(j)}(1-\\psi^{(j)})}}\\\\ {{\\displaystyle\\qquad=\\sum_{j=1}^{s}P^{(j)}(1-\\psi^{(K)})+\\sum_{j=s+1}^{K}P^{(j)}(1-\\psi^{(j)})+\\gamma\\kappa^{p}\\Big\\{\\varrho(\\epsilon)-\\sum_{j=1}^{s}P^{(j)}\\Big\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last step holds by re-arranging the arguments and using the definition of $\\varrho(\\epsilon)$ given after (8). Consequently, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\sin}{\\gamma-1}(\\psi(v^{(k)})/\\wedge^{p})-\\ell\\big(\\gamma;\\psi\\big)}\\\\ {\\displaystyle=\\sum_{j=1}^{s}P^{(j)}(1-\\psi^{(K)})+\\sum_{j=s+1}^{K}P^{(j)}(1-\\psi^{(j)})+\\big(\\psi^{(s)}-\\psi^{(K)}\\big)\\Big\\{\\varrho(\\epsilon)-\\sum_{j=1}^{s}P^{(j)}\\Big\\}}\\\\ {\\displaystyle=\\sum_{j=1}^{s}P^{(j)}(1-\\psi^{(K)})+\\sum_{j=s+1}^{K}P^{(j)}(1-\\psi^{(j)})+\\big(\\psi^{(s)}-\\psi^{(K)}\\big)\\Big\\{\\varrho(\\epsilon)-\\sum_{j=1}^{s-1}P^{(j)}\\Big\\}}\\\\ {\\displaystyle\\quad-P^{(s)}\\big\\{(1-\\psi^{(K)})-(1-\\psi^{(s)})\\big\\}}\\\\ {\\displaystyle=\\sum_{j=1}^{s-1}P^{(j)}(1-\\psi^{(K)})+\\sum_{j=s}^{K}P^{(j)}(1-\\psi^{(j)})+\\big(\\psi^{(s)}-\\psi^{(K)}\\big)\\Big\\{\\varrho(\\epsilon)-\\sum_{j=1}^{s-1}P^{(j)}\\Big\\}}\\\\ {\\displaystyle=g\\big((\\psi^{(s)}-\\psi^{(K)})\\big/\\wedge^{p};\\psi\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$g(\\gamma;\\psi)$ $\\begin{array}{r}{\\frac{\\psi^{(s)}-\\psi^{(K)}}{\\kappa^{p}}\\leq\\gamma<\\frac{\\psi^{(s-1)}-\\psi^{(K)}}{\\kappa^{p}}}\\end{array}$ Thus, $g(\\gamma;\\psi)$ is continuous in $\\gamma$ for $\\begin{array}{r}{\\gamma\\in\\left[\\frac{\\psi^{(s+1)}-\\psi^{(K)}}{\\kappa^{p}},\\frac{\\psi^{(s)}-\\psi^{(K)}}{\\kappa^{p}}\\right]}\\end{array}$ with $s\\in[K-1]$ . Consequently, $g(\\gamma;\\psi)$ is continuous in $\\gamma$ for $\\begin{array}{r}{0\\le\\gamma\\le\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Therefore, combining the discussion regrading (A24) and(A25), we obtain that given $\\psi$ $,g(\\gamma;\\psi)$ is continuous in $\\gamma$ for $\\gamma\\geq0$ ", "page_idx": 27}, {"type": "text", "text": "Next, for each given $\\psi$ , we examine the monotonicity of $g(\\gamma;\\psi)$ in $\\gamma$ to find the $\\gamma$ that minimizes $g(\\gamma;\\psi)$ . To this end, we consider the following three cases by the values of $\\varrho(\\epsilon)$ ", "page_idx": 27}, {"type": "text", "text": "then there exists an $s^{*}\\in\\{2,\\ldots,K\\}$ such that $\\begin{array}{r}{\\sum_{j=1}^{s^{*}-1}P^{(j)}\\le\\varrho(\\epsilon)\\le\\sum_{j=1}^{s^{*}}P^{(j)}}\\end{array}$ Then,by (A25), 9(; ab) is decreasing in  for / E [0, ()(] and increasing for $\\begin{array}{r}{\\gamma\\in\\big[\\frac{\\psi^{(s^{*})}-\\psi^{(K)}}{\\kappa^{p}},\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}\\big]}\\end{array}$ and by (A24), (;\u03bc)isincreasing infor \u2265(k) . Therefore,\\*  ),(k) ", "page_idx": 27}, {"type": "text", "text": "Case 2: If $\\varrho(\\epsilon)\\le P^{(1)}$ \uff0c", "page_idx": 27}, {"type": "text", "text": "then by (A25), $g(\\gamma;\\psi)$ is dereasing in $\\gamma$ for $\\gamma\\in[0,\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}]$ and by (A24), inereasing n $\\gamma$ for $\\gamma\\geq{\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}}$ Thrfore, $\\begin{array}{r}{\\gamma_{\\psi}^{\\star}=\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Case 3: If $\\begin{array}{r}{\\varrho(\\epsilon)\\ge\\sum_{j=1}^{K}P^{(j)}}\\end{array}$ \uff0c", "page_idx": 27}, {"type": "text", "text": "then by(A25),g(;\u03bc) s increasing infor  E[0, ) ; and by (A24), increasing in $\\gamma$ for )(k) Therefore,  = 0. ", "page_idx": 27}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma_{\\psi}^{\\star}=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\psi^{(1)}-\\psi^{(K)}}{\\kappa^{p}}\\,\\,\\,\\mathrm{if}\\,\\,\\varrho(\\epsilon)\\leq P^{(1)}}\\\\ {\\displaystyle\\frac{\\psi^{(s^{*})}-\\psi^{(K)}}{\\kappa^{p}}\\,\\,\\,\\mathrm{if}\\,\\,\\sum_{j=1}^{s^{*}-1}P^{(j)}\\leq\\varrho(\\epsilon)\\leq\\sum_{j=1}^{s^{*}}P^{(j)}\\mathrm{~with~}s^{*}\\in\\{2,\\dots,K\\}}\\\\ {\\displaystyle0\\,\\,\\,\\mathrm{if}\\,\\,\\varrho(\\epsilon)\\geq\\sum_{j=1}^{K}P^{(j)}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Step 2: Linear programming format. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For eachfixed permutation $\\chi$ , we now find the optimal $\\psi$ that minimizes $g(\\gamma_{\\psi}^{\\star};\\psi)$ by examining the three cases in Step 1. ", "page_idx": 28}, {"type": "text", "text": "In Case 3 of Step 1, $g(\\gamma_{\\psi}^{\\star};\\psi)=g(0;\\psi)=1-\\psi^{(K)}\\geq1-1/K$ . Then the corresponding optimal action is $\\psi^{(1)}=...=\\psi^{(K)}=1/K$ ", "page_idx": 28}, {"type": "text", "text": "Cafrsieapt x,bsubn with $s^{*}\\ \\in$ $\\{2,\\ldots,K\\}$ into (A25), we obtain that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma(\\gamma_{\\psi}^{*};\\psi)=\\sum_{j=1}^{s^{*}-1}P^{(j)}(1-\\psi^{(K)})+\\sum_{j=s^{*}}^{K}P^{(j)}(1-\\psi^{(j)})+(\\psi^{(s^{*})}-\\psi^{(K)})\\Bigl\\{\\theta(\\epsilon)-\\sum_{j=1}^{s^{*}-1}P^{(j)}\\Bigr\\}}\\\\ {\\displaystyle=(1-\\psi^{(K)})\\sum_{j=1}^{s^{*}-1}P^{(j)}+P^{(s^{*})}(1-\\psi^{(s^{*})})+P^{(K)}(1-\\psi^{(K)})+\\sum_{j=s^{*}+1}^{K-1}P^{(j)}(1-\\psi^{(j)})\\mathbf{1}(\\epsilon)}\\\\ {\\displaystyle\\qquad+(1-\\psi^{(K)})\\Bigl\\{\\theta(\\epsilon)-\\sum_{j=1}^{s^{*}-1}P^{(j)}\\Bigr\\}-(1-\\psi^{(s^{*})})\\Bigl\\{\\theta(\\epsilon)-\\sum_{j=1}^{s^{*}-1}P^{(j)}\\Bigr\\}}\\\\ {\\displaystyle=\\Bigl\\{\\sum_{j=1}^{s^{*}-1}P^{(j)}-\\varrho(\\epsilon)\\Bigr\\}(1-\\psi^{(s^{*})})+\\sum_{j=s^{*}+1}^{K-1}P^{(j)}(1-\\psi^{(j)})\\mathbf{1}(s^{*}<K-1)}\\\\ {\\displaystyle\\qquad+\\left\\{P^{(K)}+\\varrho(\\epsilon)\\right\\}(1-\\psi^{(K)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To find the optimal value that minimizes $g(\\gamma_{\\psi}^{\\star};\\psi)$ in (A26), we link it with a linear programming problem. Specifically, for $j\\in[K]$ , let $z_{j}\\triangleq1-\\psi^{(j)}$ and $\\mathbf{z}\\triangleq(z_{1},\\hdots,z_{K})^{\\top}$ .Define ", "page_idx": 28}, {"type": "equation", "text": "$$\na_{j}=\\left\\{\\begin{array}{l l}{\\displaystyle{\\sum_{j=1}^{s^{*}}}P^{(j)}-\\varrho(\\epsilon)\\,\\,\\,\\mathrm{if}\\,\\,j=s^{*}}&\\\\ {P^{(K)}+\\varrho(\\epsilon)}&{\\,\\,\\,\\mathrm{if}\\,\\,j=K}\\\\ {P^{(j)}}&{\\,\\,\\,\\mathrm{if}\\,\\,s^{*}<j<K\\,\\,\\mathrm{when}\\,\\,s^{*}\\not=K-1.}&\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $s^{*}=K-1$ , only the entries for $j\\,=\\,s^{*}\\,=\\,K\\,-\\,1$ and $j\\,=\\,K$ need to be considered. Let $\\begin{array}{r}{\\nabla(\\mathbf{z})\\,=\\,\\sum_{j=s^{*}}^{K}a_{j}z_{j}}\\end{array}$ .Then, the optimal $\\psi$ that minimizes $g(\\gamma_{\\psi}^{*};\\psi)$ in (A26) can be derived by solving the linear programming problem: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{c l}{\\displaystyle\\operatorname*{min}_{z_{1},\\dots,z_{K}}\\mathsf{V}(\\mathbf{z}),}\\\\ {\\displaystyle\\quad s.t.~~\\sum_{j=1}^{K}(1-z_{j})=1,}\\\\ {\\displaystyle~~~~~~~~0\\leq z_{1}\\leq\\ldots\\leq z_{K}\\leq1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the constraint $\\textstyle\\sum_{j=1}^{K}(1-z_{j})=1$ is due to $\\begin{array}{r}{\\sum_{j=1}^{K}(1-z_{j})=\\sum_{j=1}^{K}\\psi^{(j)}=1}\\end{array}$ by the definitions of ${\\bf z}$ and $\\psi$ , and the constraint $0\\le z_{1}\\le\\ldots\\le z_{K}\\le1$ refects the definition of $\\psi^{(j)}$ for $j\\in[K]$ ", "page_idx": 28}, {"type": "text", "text": "Similarlyfbysbs into (A24), we obtain that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g({\\gamma}_{\\psi}^{\\star};\\psi)=\\varrho(\\epsilon)\\{{\\psi}^{(1)}-{\\psi}^{(K)}\\}+\\sum_{j=1}^{K}P_{j}(1-\\psi_{j})}\\\\ {\\displaystyle\\qquad=\\varrho(\\epsilon)\\left[\\{1-{\\psi}^{(K)}\\}-\\{1-{\\psi}^{(1)}\\}\\right]+\\sum_{j=1}^{K}P_{j}(1-\\psi_{j})}\\\\ {\\displaystyle\\qquad=\\{P^{(1)}-\\varrho(\\epsilon)\\}(1-{\\psi}^{(1)})+\\sum_{j=2}^{K-1}P^{(j)}(1-{\\psi}^{(j)})+\\{P^{(K)}+\\varrho(\\epsilon)\\}(1-{\\psi}^{(K)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is a form similar to (A26) if letting $s^{*}$ in (A26) equal 1. Hence, its optimal minimizer can be found through a linear programming problem similar to (A27). Consequently, in the next step, our discussion focuses on (A26) only. ", "page_idx": 29}, {"type": "text", "text": "Step 3: Extreme points. ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The feasible region of (A27), denoted $\\Xi$ , can be expressed as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\triangleq\\biggr\\{\\mathbf z:\\displaystyle\\sum_{j=1}^{K}(1-z_{j})=1,\\;0\\leq z_{1}\\leq\\ldots\\leq z_{K}\\leq1\\biggr\\}}\\\\ &{\\quad=\\biggr\\{\\mathbf z:\\displaystyle\\sum_{j=1}^{K}(1-z_{j})=1,\\;0\\leq z_{1}\\leq\\ldots\\leq z_{K}\\leq1,1-z_{j}\\leq\\frac{1}{j}\\;\\mathrm{for}\\;j\\in[K]\\biggr\\}}\\\\ &{\\quad=\\biggr\\{\\mathbf z:\\displaystyle\\sum_{j=1}^{K}z_{j}=K-1,\\;0\\leq z_{1}\\leq\\ldots\\leq z_{K}\\leq1,z_{j}\\geq1-\\frac{1}{j}\\;\\mathrm{for}\\;j\\in[K]\\biggr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second step holds since, for $\\begin{array}{r}{j\\in[K],j(1-z_{j})=\\sum_{t=1}^{j}(1-z_{j})\\leq\\sum_{t=1}^{j}(1-z_{t})\\leq1}\\end{array}$ as $z_{t}\\le z_{j}$ for $t\\in[j]$ , and the lat step holds by rarranging the equality $\\begin{array}{r}{\\sum_{j=1}^{K}(1-z_{j})=1}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "We next prove that the following $K$ feasible solutions are the only extreme points of (A27): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{1}\\triangleq\\left(0,1,1,\\dots,1,1\\right)^{\\top},}\\\\ &{\\mathbf{z}_{2}\\triangleq\\left(1-\\frac{1}{2},1-\\frac{1}{2},1,\\dots,1,1\\right)^{\\top},}\\\\ &{\\cdots,}\\\\ &{\\mathbf{z}_{j}\\triangleq\\left(\\underbrace{1-\\frac{1}{j},\\cdots,1-\\frac{1}{j}}_{j\\mathrm{~demens~}},\\underbrace{\\mathbf{\\bar{b}}_{j}\\dots,\\mathbf{\\bar{b}}_{j}}_{K-j\\mathrm{~demen}}\\right)^{\\top}}\\\\ &{\\cdots,}\\\\ &{\\mathbf{z}_{K-1}\\triangleq\\left(1-\\frac{1}{K-1},1-\\frac{1}{K-1},1-\\frac{1}{K-1},\\dots,1-\\frac{1}{K-1},1\\right)^{\\top},}\\\\ &{\\mathbf{z}_{K}\\triangleq\\left(1-\\frac{1}{K},1-\\frac{1}{K},1-\\frac{1}{K},\\dots,1-\\frac{1}{K},1-\\frac{1}{K}\\right)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We denote $\\boldsymbol{\\Xi}_{0}\\triangleq\\{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{K}\\}$ ", "page_idx": 29}, {"type": "text", "text": "Firstly, we prove that each data point in $\\Xi_{0}$ is an extreme point of (A27). To this end, consider any $\\mathbf{z}_{j}\\in\\Xi_{0}$ . If there exist $\\nu\\in(0,\\mathbf{\\dot{1}}),\\,\\mathbf{z}^{\\prime}=(z_{1}^{\\prime},\\dots,z_{K}^{\\prime})^{\\top}\\,\\in\\Xi$ , and (2. 2K)T \u2208 , such that $\\mathbf{z}_{j}=\\nu\\mathbf{z}^{\\prime}+(1-\\nu)\\mathbf{z}^{\\prime\\prime}$ , then ${\\bf z}^{\\prime}={\\bf z}^{\\prime\\prime}\\!={\\bf z}_{j}$ , as shown below. Let $z_{j,t},z_{t}^{\\prime}$ , and $z_{t}^{\\prime\\prime}$ represent the tth element of $\\mathbf{z}_{j},\\mathbf{z}^{\\prime}$ , and $\\mathbf{z}^{\\prime\\prime}$ , respectively. ", "page_idx": 29}, {"type": "text", "text": "\u00b7If $t=j+1,\\ldots,K$ : then by $\\nu z_{t}^{\\prime}+(1-\\nu)z_{t}^{\\prime\\prime}=z_{j,t},\\,z_{j,t}=1$ , and $z_{t}^{\\prime},z_{t}^{\\prime\\prime}\\leq1$ , we have that $z_{t}^{\\prime}=z_{t}^{\\prime\\prime}=z_{j,t}=1$ ", "page_idx": 29}, {"type": "text", "text": "\u00b7If $t=j$ : then $\\begin{array}{r}{\\nu z_{j}^{\\prime}+(1-\\nu)z_{j}^{\\prime\\prime}=z_{j,j}=1-\\frac{1}{j}}\\end{array}$ , and $\\begin{array}{r}{z_{j}^{\\prime},z_{j}^{\\prime\\prime}\\geq1-\\frac{1}{j}}\\end{array}$ by (A28). Thus, we obtain that z'\u00a7 = z\" = Zj,j\u00b7   \n\u00b7If $t=1,\\dots,j-1$ then $\\begin{array}{r}{z_{t}^{\\prime}\\le z_{j}^{\\prime}=1-\\frac{1}{j}}\\end{array}$ \uff0c $\\begin{array}{r}{z_{t}^{\\prime\\prime}\\le z_{j}^{\\prime\\prime}=1-\\frac{1}{j}}\\end{array}$ , and $\\nu z_{t}^{\\prime}+(1-\\nu)z_{t}^{\\prime\\prime}=z_{j,t}=$ $\\textstyle1-{\\frac{1}{j}}$ . Thus, we can also obtain that $z_{t}^{\\prime}=z_{t}^{\\prime\\prime}=z_{j,t}$ ", "page_idx": 30}, {"type": "text", "text": "Therefore, $\\mathbf{z}^{\\prime}=\\mathbf{z}^{\\prime\\prime}=\\mathbf{z}_{j}$ , and hence, $\\mathbf{z}_{j}$ is an extreme point of (A27) by Definition A.1. ", "page_idx": 30}, {"type": "text", "text": "Next, for any point $\\widetilde{\\mathbf z}\\triangleq(\\widetilde{z}_{1},\\hdots,\\widetilde{z}_{K})^{\\intercal}\\in\\Xi\\backslash\\Xi_{0}$ we prove that $\\widetilde{\\mathbf z}$ is not an extreme point of (A27) by construction. Specifically, we have the following claims for $\\widetilde{\\mathbf{z}}$ ", "page_idx": 30}, {"type": "text", "text": "Claim 1: $\\begin{array}{r}{\\tilde{z}_{t}>1-\\frac{1}{t}}\\end{array}$ for $t\\in[K]$   \nThis claim can be proved by contradiction. Assume there exists $t_{0}~\\in~[K]$ such that   \n$\\begin{array}{r}{\\widetilde{z}_{t_{0}}\\,\\le\\,1\\,-\\,\\frac{1}{t_{0}}}\\end{array}$ As $\\bar{\\widetilde{\\mathbf{z}}}\\,\\in\\,\\Xi$ by (A28) and the assumption, we have $\\begin{array}{r}{\\widetilde{z}_{t_{0}}\\,=\\,\\mathrm{i}\\,-\\,\\frac{1}{t_{0}}}\\end{array}$ Since   \n$\\widetilde{\\textbf{z}}\\notin\\Xi_{0}$ , one of the following statements must hold: (1) there exists $\\textit{j}<\\ t_{0}$ such that   \n$\\begin{array}{r}{\\widetilde{z}_{j}\\,<\\,1-\\,\\frac{1}{t_{0}}}\\end{array}$ : or (2) there exists $j~>t_{0}$ such that $\\widetilde{z}_{j}\\,<\\,1$ for some $j\\,>\\,t_{0}$ . Therefore,   \n$\\begin{array}{r}{\\sum_{j=1}^{K}\\widetilde{z}_{j}=\\sum_{j=1}^{t_{0}}\\widetilde{z}_{j}+\\sum_{j=t_{0}+1}^{K}\\widetilde{z}_{j}<\\sum_{j=1}^{t_{0}}\\widetilde{z}_{t_{0}}+\\sum_{j=t_{0}+1}^{K}1=\\!t_{0}\\cdot\\widetilde{z}_{t_{0}}+(K-t_{0})\\cdot1=0,\\quad\\widetilde{z}_{t_{0}}>\\sum_{j=1}^{t_{0}}\\widetilde{z}_{t_{0}}.}\\end{array}$   \n$K-1$ since $\\widetilde{z}_{1}\\leq\\dots\\leq\\widetilde{z}_{K}$ and $\\widetilde{z}_{t}\\leq1$ for $t\\in[K]$ by (A28), where the strict inequality   \narises from the fact that either statement (1) or (2) holds. This conclusion contradicts the   \ncondition that $\\widetilde{\\mathbf{z}}\\in\\Xi$ by (A28).   \nClaim 2: There exists $t_{1}\\in[K]$ such that $\\widetilde{z}_{t_{1}-1}<\\widetilde{z}_{t_{1}}<1$   \nThis claim can be proved by contradiction: - On one hand, if there exists $t^{\\prime}\\in[K]$ such that $\\widetilde{z}_{t^{\\prime}-1}<\\widetilde{z}_{t^{\\prime}}$ , then we must have $\\widetilde{z}_{t}=$ $\\widetilde{z}_{t^{\\prime}-1}$ for $t\\leq t^{\\prime}-1$ ; otherwise, by letting $t^{\\prime\\prime}=\\arg\\operatorname*{max}\\{t:\\widetilde{z}_{t}<\\widetilde{z}_{t^{\\prime}-1},t<t^{\\prime}-1\\}$ ,we obtain that $\\widetilde{z}_{t^{\\prime\\prime}}<\\widetilde{z}_{t^{\\prime\\prime}+1}=\\widetilde{z}_{t^{\\prime}}<1$ and hence, $t_{1}$ can be set as $t^{\\prime\\prime}\\,{+}\\,1$ , which contradicts the assumption. Additionally, we have $\\widetilde{z}_{t}=1$ for $t\\geq t^{\\prime}$ ; otherwise, $\\widetilde{z}_{t^{\\prime}-1}<\\widetilde{z}_{t^{\\prime}}<1$ and $t_{1}$ can be set as $t^{\\prime}$ , which contradicts the assumption. Summarizing the discussion for $t\\le t^{\\prime}-1$ and $t\\geq t^{\\prime}$ , we have $\\widetilde{\\mathbf{z}}\\in\\Xi_{0}$ - On the other hand, if $\\widetilde{z}_{t-1}=\\widetilde{z}_{t}$ for all $t\\in[K]$ , then $\\widetilde{\\mathbf{z}}=\\mathbf{z}_{K}\\in\\Xi_{0}$ ", "page_idx": 30}, {"type": "text", "text": "In both cases, $\\widetilde{\\mathbf{z}}\\in\\Xi_{0}$ , contradicting the fact that $\\widetilde{\\mathbf{z}}\\notin\\Xi_{0}$ . Hence, Claim 2 holds. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{1}\\triangleq\\operatorname*{min}\\{\\frac{\\widetilde{z}_{t_{1}}-\\widetilde{z}_{t_{1}-1}}{2},\\widetilde{z}_{t}-(1-\\frac{1}{t})\\mathrm{~for~}t\\le t_{1}-1\\}\\mathrm{~and~}}\\\\ &{c_{2}\\triangleq\\operatorname*{min}\\{\\frac{\\widetilde{z}_{t_{1}}-\\widetilde{z}_{t_{1}-1}}{2},\\widetilde{z}_{t_{1}}-(1-\\frac{1}{t_{1}}),1-\\widetilde{z}_{t}\\mathrm{~for~}t_{1}\\le t\\le t_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Claims 1 and 2, we have that $c_{1}>0$ and $c_{2}>0$ . Let $\\overline{{c}}\\triangleq\\operatorname*{min}\\{(t_{1}-1)c_{1},(t_{2}-t_{1}+1)c_{2}\\}$ $\\overline{{c}}_{1}\\triangleq\\overline{{c}}/(t_{1}-1)$ , and $\\overline{{c}}_{2}\\triangleq\\overline{{c}}/(t_{2}-t_{1}+1)$ . Then we construct two points in $\\Xi$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}^{\\prime}\\triangleq(\\widetilde{z}_{1}+\\overline{{c}}_{1},\\hdots,\\widetilde{z}_{t_{1}-1}+\\overline{{c}}_{1},\\widetilde{z}_{t_{1}}-\\overline{{c}}_{2},\\hdots,\\widetilde{z}_{t_{2}}-\\overline{{c}}_{2},\\hdots,\\widetilde{z}_{K})^{\\top}\\ \\mathrm{and}}\\\\ &{\\mathbf{z}^{\\prime\\prime}\\triangleq(\\widetilde{z}_{1}-\\overline{{c}}_{1},\\hdots,\\widetilde{z}_{t_{1}-1}-\\overline{{c}}_{1},\\widetilde{z}_{t_{1}}+\\overline{{c}}_{2},\\hdots,\\widetilde{z}_{t_{2}}+\\overline{{c}}_{2},\\hdots,\\widetilde{z}_{K})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, $\\widetilde{{\\bf z}}=\\textstyle{\\frac{1}{2}}{\\bf z}^{\\prime}+\\frac{1}{2}{\\bf z}^{\\prime\\prime}$ , and hence, $\\widetilde{\\mathbf{z}}$ is not an extreme point of (A27) ", "page_idx": 30}, {"type": "text", "text": "Step 4: Solution format and optimal action. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "By Steps 2 and 3, we obtain that for each fixed $\\chi$ and $s^{*}$ , the extreme points of the linear programming problem are given in $\\Xi_{0}$ . By Lemma 1, every linear program has an extreme point that is an optimal solution. Hence, by the format of the $K$ extreme points in $\\Xi_{0}$ , we obtain that at least one optimal action of $\\psi$ can be found in the format: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\psi^{(j)}={\\frac{1}{k^{*}}}\\;\\mathrm{for}\\;j\\leq k^{*}\\;\\mathrm{and}\\;\\psi^{(j)}=0\\;\\mathrm{for}\\;j\\geq k^{*}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for some $k^{*}\\in[K]$ ", "page_idx": 30}, {"type": "text", "text": "f $k^{*}=K$ , by (A23), we have that $\\begin{array}{r}{g(\\gamma;\\psi)=\\gamma\\epsilon^{p}+\\sum_{j=1}^{K}P_{j}\\cdot(1-\\frac{1}{K})}\\end{array}$ and hence, the robust risk .is $\\begin{array}{r}{g(\\gamma_{\\psi}^{\\star};\\psi)=1-\\frac{1}{K}}\\end{array}$ by taking $\\gamma_{\\psi}^{\\star}=0$ ", "page_idx": 30}, {"type": "text", "text": "If $k^{*}<K$ , we obtain that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g(\\gamma;\\psi)=\\gamma\\epsilon^{p}+\\displaystyle\\sum_{j=1}^{k^{\\ast}}P^{(j)}\\operatorname*{max}\\left(1-\\gamma\\kappa^{p},1-\\displaystyle\\frac{1}{k^{\\ast}}\\right)+\\displaystyle\\sum_{j=k^{\\ast}+1}^{K}P^{(j)}\\cdot1}}\\\\ {{\\displaystyle=\\left\\{1+\\gamma\\kappa^{p}\\Big\\{\\varrho(\\epsilon)-\\displaystyle\\sum_{j=1}^{k^{\\ast}}P^{(j)}\\Big\\},\\;{\\mathrm{if~}}0\\leq\\gamma\\leq\\displaystyle\\frac{1}{k^{\\ast}\\kappa^{p}};\\right.}}\\\\ {{\\displaystyle\\left.\\gamma\\epsilon^{p}+1-\\displaystyle\\frac{1}{k^{\\ast}}\\sum_{j=1}^{k^{\\ast}}P^{(j)},\\;{\\mathrm{if~}}\\gamma\\geq\\displaystyle\\frac{1}{k^{\\ast}\\kappa^{p}}.\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, for $k^{*}\\ <\\ K$ , the robust risk is the minimum of $g(\\gamma_{\\psi}^{\\star};\\psi)\\;=\\;1$ by taking $\\gamma_{\\psi}^{\\star}\\;=\\;0$ and $\\begin{array}{r}{g(\\gamma_{\\psi}^{\\star};\\psi)=1+\\frac{1}{k^{\\ast}}\\Big\\{\\varrho(\\epsilon)-\\sum_{j=1}^{k^{\\ast}}P^{(j)}\\Big\\}}\\end{array}$ by taking $\\begin{array}{r}{\\gamma_{\\psi}^{\\star}=\\frac{1}{k^{*}\\,\\kappa^{p}}}\\end{array}$ Aditioally, we oerve th w should take the highest $k^{*}$ values of $\\{P_{1},...,P_{K}\\}$ $P^{(1)},\\dots,P^{(k^{*})}$ to minimize $g(\\gamma_{\\psi}^{\\star};\\psi)$ .Hence, we take the permutation $\\chi$ such that $P^{(1)}\\ge\\dots\\ge P^{(K)}$ ", "page_idx": 31}, {"type": "text", "text": "In summary, the optimal action $\\psi^{\\star}$ that minimizes $g(\\gamma_{\\psi}^{\\star};\\psi)$ is given as below. ", "page_idx": 31}, {"type": "text", "text": "\u00b7If $\\begin{array}{r}{\\frac{1}{K}\\,\\ge\\,\\frac{1}{k^{*}}\\sum_{j=1}^{k^{*}}P^{(j)}-\\frac{1}{k^{*}}\\varrho(\\epsilon)}\\end{array}$ for all $k^{*}\\in[K-1]$ , then $\\psi_{j}^{\\star}=\\frac{1}{K}$ for $j\\in[K]$ \u00b7If there xists some $k_{0}\\,\\in\\,[K\\mathrm{~-~}1]$ $\\begin{array}{r}{\\frac{1}{k_{0}}\\sum_{j=1}^{k_{0}}P^{(j)}\\,-\\,\\frac{1}{k_{0}}\\varrho(\\epsilon)\\,>\\,\\frac{1}{K}}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{k_{0}}\\sum_{j=1}^{k_{0}}P^{(j)}-}\\end{array}$ $\\begin{array}{r}{\\frac{1}{k_{0}}\\varrho(\\epsilon)\\ge\\frac{1}{k^{*}}\\sum_{j=1}^{k^{*}}P^{(j)}-\\frac{1}{k^{*}}\\varrho(\\epsilon)}\\end{array}$ for all $k^{*}\\in[K-1]$ then $\\begin{array}{r}{\\psi^{\\star(j)}=\\frac{1}{k_{0}}}\\end{array}$ $j\\in[k_{0}]$ and $\\psi^{\\star(j)}=0$ for $j=k_{0}+1,\\ldots,K$ ", "page_idx": 31}, {"type": "text", "text": "In particular, if $P^{(1)}\\ge\\operatorname*{max}\\{\\frac{1}{K}+\\varrho(\\epsilon),P^{(2)}+\\varrho(\\epsilon)\\}$ , then the optimal action is given as: $\\psi^{\\star(1)}=1$ and $\\psi^{\\star(j)}=0$ for $j=2,\\dots,K$ . Thus,the proof is complete. ", "page_idx": 31}, {"type": "text", "text": "A.8Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For ease of presentation, we omit the dependence on $\\mathbf{x}_{i}$ and $\\widetilde{\\mathbf{y}}_{i}$ in the notation for now. Specifically, for $i\\in[K]$ and $j\\in[K]$ , let $P_{i,j}\\triangleq P_{j}(\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i})\\triangleq P(\\mathrm{Y}=j|\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i})$ and $\\psi_{i,j}\\triangleq\\psi(\\mathbf{x}_{i})_{j}$ . For given $\\mathbf{x}_{i}$ ,we sort the $K$ elements of $\\psi(\\mathbf{x}_{i}),\\,\\{\\psi_{i,1},\\dots,\\psi_{i,K}\\}$ in a decreasing order, denoted $\\bar{\\psi_{i}^{(1)}}\\geq\\ldots\\geq\\psi_{i}^{(K)}$ We first consider the difference between the Wasserstein robust loss (10) and the nominal loss (11): ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\operatorname*{inf}_{\\gamma\\geq0}\\left[\\gamma\\epsilon^{p}+\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{K}P_{i,j}\\operatorname*{max}\\left\\lbrace T(\\psi_{i,1})-T(\\psi_{i,j})-\\gamma\\kappa^{p},\\ldots,T(\\psi_{i,j-1})-T(\\psi_{i,j})-\\gamma\\kappa^{p},0\\right.\\right.}}\\\\ {{\\displaystyle\\left.T(\\psi_{i,j+1})-T(\\psi_{i,j})-\\gamma\\kappa^{p},\\ldots,T(\\psi_{i,K})-T(\\psi_{i,j})-\\gamma\\kappa^{p}\\right\\rbrace\\right]}}\\\\ {{\\displaystyle=\\operatorname*{inf}_{\\gamma\\geq0}\\left[\\gamma\\epsilon^{p}+\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{K}P_{i,j}\\operatorname*{max}\\left\\lbrace T(\\psi_{i}^{(K)})-T(\\psi_{i,j})-\\gamma\\kappa^{p},0\\right\\rbrace\\right]}}\\\\ {{\\displaystyle\\Delta\\;\\cdot\\;e\\;\\;e\\;\\;\\;.\\;}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the second equalty is due to the fat that $\\psi_{i}^{(1)}\\geq\\ldots\\geq\\psi_{i}^{(K)}$ and that $\\tau$ is decreasing. ", "page_idx": 31}, {"type": "text", "text": "By definition, $\\left\\{\\alpha_{i,j}\\triangleq{\\mathcal{T}}(\\psi_{i}^{(K)})-{\\mathcal{T}}(\\psi_{i,j}):i\\in[n],j\\in[K]\\right\\}$ are ordered as $\\alpha^{(1)}\\geq...\\geq\\alpha^{(n K)}$ \uff0c and correspondingly, the $P_{i,j}$ 's with the associated indexes are denoted $P^{(1)},\\dots,P^{(n K)}$ . Consequently, we obtain that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathtt{\\Pi}_{\\mathtt{A}\\left(\\gamma\\right)}=\\gamma\\epsilon^{p}+\\frac1n\\sum_{t=1}^{n K}P^{\\left(t\\right)}(\\alpha^{\\left(t\\right)}-\\gamma\\kappa^{p})\\mathbf{1}(\\alpha^{\\left(t\\right)}>\\gamma\\kappa^{p}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Define $\\alpha^{(n K+1)}=0$ . Then, $\\ell(\\gamma)$ in (A30) can be expressed as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(\\gamma)=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac1n\\sum_{t=1}^{s}P^{(t)}\\alpha^{(t)}+\\gamma\\kappa^{p}\\bigg\\{\\varrho(\\epsilon)-\\frac1n\\sum_{t=1}^{s}P^{(t)}\\bigg\\},\\,\\mathrm{if~}\\alpha^{(s+1)}/\\kappa^{p}\\leq\\gamma<\\alpha^{(s)}/\\kappa^{p}\\,\\mathrm{for~}s}\\\\ {\\displaystyle\\gamma\\epsilon^{p}\\,\\mathrm{if~}\\gamma\\geq\\alpha^{(1)}/\\kappa^{p}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\operatorname*{lim}_{\\gamma\\to(|\\alpha^{0}|)/\\hbar^{p}>0}\\hbar(\\gamma)=\\displaystyle\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}\\alpha^{(t)}+|\\alpha^{(s)}|\\left\\{\\varrho(\\epsilon)-\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}\\right\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{=|\\alpha^{(s)}|\\varrho(\\epsilon)+\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}(|\\alpha^{(t)}|-|\\alpha^{(s)}|)}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle=\\displaystyle\\frac{1}{n}\\sum_{t=1}^{s-1}P^{(t)}|\\alpha^{(t)}|+|\\alpha^{(s)}|\\Big\\{\\varrho(\\epsilon)-\\frac{1}{n}\\sum_{t=1}^{s-1}P^{(t)}\\Big\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $\\ell(\\gamma)$ is right-continuous at $\\gamma\\,=\\,|\\alpha^{(s)}|/\\kappa^{p}$ by definition (A31), so we conclude that $\\ell(\\gamma)$ is continuous at $\\gamma=\\vert\\alpha^{(s)}\\vert/\\kappa^{p}$ for $s\\in[n K]$ . Hence, by (A31), $\\ell(\\gamma)$ is continuous for $\\gamma\\geq0$ ", "page_idx": 32}, {"type": "text", "text": "By (A31), $\\ell(\\gamma)$ is increasing in $\\gamma$ on $\\gamma\\,\\in\\,[\\alpha^{(s+1)}/\\kappa^{p},\\alpha^{(s)}/\\kappa^{p})$ if $\\begin{array}{r}{\\varrho(\\epsilon)\\,-\\,\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}\\,>\\,0}\\end{array}$ and decreasing if $\\begin{array}{r}{\\varrho(\\epsilon)-\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}<0}\\end{array}$ Hence, to examine the monotonicity of $\\ell(\\gamma)$ and find the $\\gamma$ that minimizes $\\ell(\\gamma)$ , we consider the following three cases by the values of $\\varrho(\\epsilon)$ ", "page_idx": 32}, {"type": "text", "text": "Case 1. If $\\begin{array}{r}{\\frac{1}{n}P^{(1)}\\,<\\,\\varrho(\\epsilon)\\,<\\,\\frac{1}{n}\\sum_{t=1}^{n K}P^{(t)}}\\end{array}$ then there xists $s^{*}\\,\\in\\,\\{2,\\dots,n K\\}$ such that $\\varrho(\\epsilon)>$ $\\textstyle{\\frac{1}{n}}\\sum_{t=1}^{s}P^{(t)}$ for $s<s^{*}$ and $\\begin{array}{r}{\\varrho(\\epsilon)\\leq\\frac{1}{n}\\sum_{t=1}^{s}P^{(t)}}\\end{array}$ for $s\\geq s^{*}$ Hence, by (A31), $\\ell(\\gamma)$ is decrasing in $\\gamma$ for $\\gamma\\in[0,\\alpha^{(s^{*})}/\\kappa^{p}]$ and increasing for $\\gamma\\geq\\alpha^{(s^{*})}/\\kappa^{p}$ . Consequently, $\\gamma_{\\psi}^{\\star}=\\alpha^{(s^{\\ast})}/\\kappa^{p}$ , and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{inf}_{\\gamma\\geq0}\\hbar(\\gamma)=\\hbar(\\alpha^{(s^{*})}/\\kappa^{p})=\\frac{1}{n}\\displaystyle\\sum_{t=1}^{s^{*}-1}P^{(t)}|\\alpha^{(t)}|+|\\alpha^{(s^{*})}|\\Big\\{\\varrho(\\epsilon)-\\frac{1}{n}\\displaystyle\\sum_{t=1}^{s^{*}-1}P^{(t)}\\Big\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{t=1}^{s^{*}-1}P^{(t)}|\\alpha^{(t)}|+O\\left(\\frac{1}{n}\\right)|\\alpha^{(s^{*})}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here the last step holds because by the definition of $s^{*}$ \uff0c $\\begin{array}{r}{0<\\varrho(\\epsilon)-\\frac{1}{n}\\sum_{t=1}^{s^{*}-1}P^{(t)}\\leq\\frac{1}{n}\\sum_{t=1}^{s^{*}}P^{(t)}-}\\end{array}$ $\\begin{array}{r}{\\frac1n\\sum_{t=1}^{s^{*}-1}P^{(t)}=\\frac1n P^{(s^{*})}\\le\\frac1n}\\end{array}$ where the las iequality holds sine $P^{(s^{*})}\\in[0,1]$ ", "page_idx": 32}, {"type": "text", "text": "Case 2. If $\\varrho(\\epsilon)\\leq\\textstyle{\\frac{1}{n}}P^{(1)}$ : then, by (A31), $\\beta(\\gamma)$ is decreasing in $\\gamma$ for $\\gamma\\in[0,\\alpha^{(1)}/\\kappa^{p}]$ and increasing for $\\gamma\\geq\\alpha^{(1)}/\\kappa^{p}$ . Therefore, $\\gamma_{\\psi}^{\\star}=\\alpha^{(1)}/\\kappa^{p}$ , and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\geq0}\\mathcal{h}(\\gamma)=\\mathcal{h}(\\alpha^{(1)}/\\kappa^{p})=\\varrho(\\epsilon)|\\alpha^{(1)}|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Case 3. If $\\begin{array}{r}{\\varrho(\\epsilon)\\,\\geq\\,\\frac{1}{n}\\sum_{t=1}^{n K}P^{(t)}}\\end{array}$ : thn, by (A31), $\\beta(\\gamma)$ is increaing in $\\gamma$ for $\\gamma\\geq0$ Therefore, $\\gamma_{\\psi}^{\\star}=0=\\alpha^{(n K+1)}/\\kappa^{p}$ , and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\geq0}\\,\\hbar(\\gamma)=\\hbar(0)=\\frac{1}{n}\\sum_{t=1}^{n K}P^{(t)}|\\alpha^{(t)}|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, summarizing the discussion in the three cases above, we have that $\\gamma_{\\psi}^{\\star}=\\alpha^{(s^{\\ast})}/\\kappa^{p}$ ,and the robust risk (10) is expressed as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{R}}_{\\epsilon}=\\widehat{\\mathfrak{R}}+\\frac{1}{n}\\sum_{t=1}^{s^{*}-1}P^{(t)}\\alpha^{(t)}\\mathbf{1}(s^{*}>1)+O\\left(\\frac{1}{n}\\right)\\alpha^{(s^{*})},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where, to provide a unified expression, we define $s^{*}\\triangleq1$ and $s^{*}\\triangleq n K+1$ in Case 2 and Case 3, respectively. Thus, the proof is completed. ", "page_idx": 33}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "B.1  Implementation Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Datasets. We evaluate the effectiveness of the proposed AdaptCDRP on CIFAR-10 and CIFAR-100 [21] with synthetic annotations, and on four real-world datasets with human annotations: CIFAR10N, CIFAR-100N [22], LabelMe [23, 24], and Animal-10N [25]. CIFAR-10 has 10 classes of $32\\times32\\times3$ color images, with 50,000 training images and 10,000 test images; CIFAR-10N provides three independent human annotated noisy labels per instance, with a majority vote yielding a $9.03\\%$ noise rate. CIFAR-100, with the same number and size of training and test images as CIFAR-10, features 100 fine-grained classes; for each instance in CIFAR-100, CIFAR-100N provides one human annotated noisy label, with a noise rate of $40.20\\%$ . LabelMe is an image classification dataset comprising 10,000 training images, 500 validation images, and 1,188 test images. The training set includes noisy and incomplete labels provided by 59 annotators, with each image being labeled an average of 2.547 times. The Animal-10N dataset contains 10 classes of $64\\times64\\times3$ color animal images; it includes 5 pairs of similar-looking animals, where each pair consists of two animals that are visually alike. The training dataset contains 50,0o0 images and the test dataset contains 5,000 images. The noise rate (mislabeling ratio) of the dataset is about $8\\%$ . For all the datasets except LabelMe, we allocate $10\\%$ of the training data as validation data used for model selection, where we choose the model with the lowest validation accuracy during training. The test data is reserved for final evaluation of the model's performance on unseen data. ", "page_idx": 33}, {"type": "text", "text": "Noise generation. We generate synthetic instance-dependent label noise on the CIFAR-10 and CIFAR-100 datasets using Algorihtm 2 in [29]. Each annotator is classified as an $\\scriptstyle\\mathrm{IDN-\\tau}$ annotator if their mislabeling ratio is upper bounded by $\\tau$ . We simulate $R$ annotators independently, with $R$ taking values from the set $\\{5,10,30,50,100\\}$ . For each instance, one annotation is randomly selected from those provided by the $R$ annotators? contributions, which evaluates methods under incomplete annotator labeling conditions. Additionally, for each $R$ , we consider three groups of annotators with varying expertise levels, characterized by average mislabeling ratios of approximately $20\\%$ $35\\%$ , and $50\\%$ . These groups are referred to as IDN-LOW, IDN-MID, and IDN-HIGH, indicating low, medium, and high error rates, respectively. We manually corrupt the datasets according to the following annotator groups: ", "page_idx": 33}, {"type": "text", "text": "$\\mathbf{R}{=}5;$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "IDN-LOW. 2 IDN-10% annotators, 2 IDN- $20\\%$ annotators, 1 IDN-30% annotator; ", "page_idx": 33}, {"type": "text", "text": "IDN-MID.2IDN- $30\\%$ annotators, 2 IDN-40% annotators, 1 IDN-50% annotator; ", "page_idx": 33}, {"type": "text", "text": "IDN-HIGH. 2 IDN- $50\\%$ annotators, 2 IDN-60% annotators, 1 IDN-70% annotator; ", "page_idx": 33}, {"type": "text", "text": "${\\bf R=10}$ \uff1a", "page_idx": 33}, {"type": "text", "text": "IDN-LOW. 4 IDN-10% annotators, 4 IDN-20% annotators, 2 IDN-30% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-MID.4 IDN-30% annotators, 4 IDN-40% annotators, 2 IDN-50% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-HIGH. 4 IDN-50% annotators, 4 IDN-60% annotators, 2 IDN-70% annotators; ", "page_idx": 33}, {"type": "text", "text": "$\\mathbf{R}{=}30$ \uff1a IDN-LOW. 11 IDN-10% annotators, 11 IDN-20% annotators, 8 IDN-30% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-MID.11IDN- $30\\%$ annotators, 11 IDN-40% annotators, 8 IDN-50% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-HIGH. 11 IDN-50% annotators, 11 IDN-60% annotators, 8 IDN-70% annotators; ", "page_idx": 33}, {"type": "text", "text": "${\\bf R}{=}50$ ", "page_idx": 33}, {"type": "text", "text": "IDN-LOW. 18 IDN-10% annotators, 18 IDN-20% annotators, 14 IDN-30% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-MID. 18 IDN-30% annotators, 18 IDN-40% annotators, 14 IDN-50% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-HIGH. 18 IDN-50% annotators, 18 IDN-60% annotators, 14 IDN-70% annotators; ${\\bf R=100}$ ", "page_idx": 33}, {"type": "text", "text": "IDN-LOW. 35 IDN-10% annotators, 35 IDN-20% annotators, 30 IDN-30% annotators; ", "page_idx": 33}, {"type": "text", "text": "IDN-MID.35IDN- $30\\%$ annotators, 35 IDN-40% annotators, 30 IDN-50% annotators;   \nIDN-HIGH.35IDN- $50\\%$ annotators, 35 IDN-60% annotators, 30 IDN-70% annotators. ", "page_idx": 34}, {"type": "text", "text": "Experiment setup. We employ the ResNet-18 architecture for CIFAR-10 and CIFAR-10N, and the ResNet-34 architecture for CIFAR-100 and CIFAR-100N datasets. Following [27], we use a pretrained VGG-16 model with a $50\\%$ dropout rate as the backbone for the LabelMe dataset. For the Animal-10N dataset, in line with [25], we use the VGG19-BN architecture [28] as the backbone. A batch size of 128 is maintained across all datasets. We use the Adam optimizer [43] with a weight decay of $5\\times10^{-4}$ for CIFAR-10, CIFAR-100, CIFAR-10N, CIFAR-100N, and LabeIMe datasets. The initial learning rate for CIFAR-10, CIFAR-100, CIFAR-10N, and CIFAR-100N is set to $10^{-3}$ with the networks trained for 120, 150, 120, and 150 epochs respectively. The first 30 epochs serve as a warm-up. For the LabelMe dataset, the model is trained for 100 epochs with an initial learning rate of $10^{-2}$ and a 20-epoch warm-up. For the Animal-10N dataset, the network is trained for 100 epochs with an initial learning rate of $\\mathrm{\\dot{1}0^{-1}}$ and a weight decay of $10^{-3}$ . The learning rate is reduced by a factor of 0.1 at the 50th and 75th epochs, with the first 40 epochs designated as the warm-up stage. Training times are approximately 3 hours on CIFAR-10 and 5.5 hours on CIFAR-100 using an NVIDIA V100 GPU. ", "page_idx": 34}, {"type": "text", "text": "Baselines. Our method addresses learning from noisy annotations, particularly when estimated true label posteriors may be misspecified. Thus, we select baselines that either use estimated transition matrices or true label posteriors (MBEM [9], CrowdLayer [27], TraceReg [12], Max-MIG [11], CoNAL [35]). We also include baselines that aggregate labels differently (CE (MV), CE (EM) [7], DoctorNet [34], CCC [36]). Since our theoretical framework applies to both single-annotator and multiple-annotator scenarios, we also include baselines designed for single noisy labels (LogitClip [33]), particularly those employing two networks (Co-teaching [30], Co-teaching $^+$ [31], CoDis [32]), as our method similarly uses two networks that act as priors for each other. Details of the baselines are given as follows. ", "page_idx": 34}, {"type": "text", "text": "(1) CE (Clean): Trains the network using the standard cross-entropy loss on clean datasets; (2) CE (MV): Trains the network using majority voting labels;   \n(3) CE (EM) [7]: Aggregate labels using the EM algorithm; (4) Co-teaching [30]: Trains two networks and cross-trains on instances with small loss values; (5) Co-teaching $^+$ [31]: Combines the \"Update by Disagreement\" with the Co-teaching method;   \n(6) CoDis [32]: Selects possibly clean data that have high-discrepancy prediction probabilities between two networks; (7) LogitClip [33]: Clamps the norm of the logit vector to ensure it is upper bounded by a constant;   \n(8) DoctorNet [34]: Models individual annotators and learns averaging weights by combining them; (9) MBEM [9]: Alternates between estimating annotator quality from disagreements with the current model and updating the model by optimizing a loss function that accounts for the current estimate of worker quality;   \n(10) CrowdLayer [27]: Concatenates the classifier with multiple annotator-specific layers and learns the parameters simultaneously;   \n(11) TraceReg [12]: Uses a loss function similar to CrowdLayer but adds regularization to establish identifiability of the confusion matrices and the classifier;   \n(12) Max-MIG [11]: Jointly aggregates noisy crowdsourced labels and trains the classifier;   \n(13) CoNAL [35]: Decomposes the annotation noise into common and individual confusions;   \n(14) CCC [36]: Simultaneously trains two models to correct the confusion matrices learned by each other via bi-level optimization. ", "page_idx": 34}, {"type": "text", "text": "Among these methods, Co-teaching, Co-teaching $^+$ , CoDis, and LogitClip are strong baselines for handling single noisy labels. We adapt them to the multiple annotations setting by using majority vote labels for loss computation. The results demonstrate the effectiveness of the proposed pseudo-label generation method across various scenarios. Results for CE (Clean), CE (MV), CE (EM), DoctorNet, MBEM, CrowdLayer, Max-MIG, and CoNAL in Table 1 are sourced from [10]. Baselines (1)-(3) ", "page_idx": 34}, {"type": "text", "text": "Algorithm 1: Learning from Noisy Labels via Conditional Distributionally Robust True Label Posterior with an Adaptive Lagrange multiplier (AdaptCDRP) ", "page_idx": 35}, {"type": "text", "text": "Input: $D=\\{\\mathbf{x}_{i},\\tilde{\\mathbf{y}}_{i}\\}_{i=1}^{n}$ \uff0c $\\epsilon\\in(0,\\frac{1}{K})$ \uff0c $\\kappa>0$ ,C>1,x>0 1 Warm up classifers $\\psi^{(1)}$ and $\\psi^{(2)}$ ; Approximate noise transition probabilities $\\widehat{\\tau}_{j}(\\widetilde{\\mathbf{y}})$ for $j\\in[K]$ using small-loss data; 2 for epoch $t=1,...,T$ do 3 // Update the classifiers with pseudo-empirical distribution (Theorem 3.1) 4 Update approximated true label posteriors: $\\widehat{P}_{j}^{(\\iota)}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\propto\\psi_{j}^{(\\iota)}(\\mathbf{x})\\cdot\\widehat{\\tau}_{j}(\\widetilde{\\mathbf{y}})$ for $j\\in[K]$ .\uff0c for each instance $\\mathbf{x}_{i}$ do 6 if $\\widehat{P}_{k^{\\star}}^{(\\iota)}(\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i})/\\operatorname*{max}_{j\\neq k^{\\star}}\\widehat{P}_{j}^{(\\iota)}(\\mathbf{x},\\widetilde{\\mathbf{y}})\\geq\\mathcal{C}$ let $\\mathrm{y}_{i}^{\\star}=k^{\\star}$ and collect $(\\mathbf{x}_{i},\\widetilde{\\mathbf{y}}_{i},\\mathbf{y}_{i}^{\\star})$ into $\\mathcal{D}_{t,\\iota}^{\\star}$ .\uff0c 7 end for 8 Update the pseudo-empirical distribution $P_{t,\\iota}^{\\star}$ based on $\\mathcal{D}_{t,\\iota}^{\\star}$ 9 Update $\\psi^{(\\iota)}$ by minimizing the empirial robst risk () with the referene distribution $P_{t,\\backslash\\iota}^{\\star}$ and the Lagrange multiplier $\\gamma_{t-1}^{(\\iota)}$ .\uff1b 10 // Update the Lagrange multiplier (Theorem 3.3) 11 Compute $\\alpha_{i}^{\\prime}s$ for $i\\in[n K]$ and $s^{*}$ by Theorem 3.3; 12 Compute the reference value for the Lagrange multiplier: $\\gamma_{0,t}=|\\alpha^{(s^{*})}|/\\kappa^{p}$ 13 Update the Lagrange multiplier: $\\begin{array}{r}{\\gamma_{t}^{(\\iota)}=\\gamma_{0,t}-\\frac{1}{\\lambda}\\{\\epsilon^{p}-\\mathbb{E}_{P_{t,\\backslash,\\iota}^{\\star}}c^{p}(y^{\\prime},\\mathrm{Y})\\}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "14 end for ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Output: $\\psi_{1}$ and $\\psi_{2}$ ", "page_idx": 35}, {"type": "text", "text": "are implemented according to their respective algorithms, while for the remaining baseline methods, we adapted the code from the GitHub repositories provided in their original papers, with further modifications to fit our setup. ", "page_idx": 35}, {"type": "text", "text": "Pseudo code for the algorithm. The training process described in Section 3.3 is presented in Algorithm 1. ", "page_idx": 35}, {"type": "text", "text": "B.2  Additional Experimental Results ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Performance on the CIFAR-1o0 dataset with varying numbers of annotators. We conduct additional experiments on the CIFAR-100 dataset, varying the number of annotators from 5 to 100, with each instance labeled only once. Figure 3 presents the average accuracy across different annotator counts, highlighting the advantages of the proposed method across various settings. As the total number of annotators increases, labeling sparsity becomes more pronounced, which may lead to a performance collapse in methods that do not account for this sparsity, especially in datasets with a large number of classes, such as CIFAR-100. ", "page_idx": 35}, {"type": "image", "img_path": "2NKumsITFw/tmp/deadd885a7b4985fef9c398830ac6da55976b8d705f0bb467e833d326031d7fb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 3: Average test accuracy on the CIFAR-100 dataset with varying numbers of annotators. The error bars representing standard deviations are shaded. ", "page_idx": 35}, {"type": "text", "text": "Performance with varying numbers of annotations per instance.  To further evaluate model performance with varying numbers of annotations per instance, we use $R\\,=\\,30$ annotators and randomlyselect $l=1,3,5,7,9$ labelsfrom these $R$ annotators for each instance. The test accuracies of the proposed method and other annotation aggregation methods are shown in Table 5. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "table", "img_path": "2NKumsITFw/tmp/4227949b8db2cd2b0d090471e344bce4231d602ec9a5ea54b284132624df2552.jpg", "table_caption": ["Table 5: Average test accuracies (with associated standard errors expressed after the $\\pm$ signs)for learning the CIFAR-10 dataset with varying numbers of annotations (denoted l) from $R\\:=\\:30$ annotators. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Accuracy of robust pseudo-labels.  To enhance the assessment of the effectiveness of the proposed robust pseudo-label generation method, we present the average accuracy of the robust pseudo-labels on the CIFAR-10 and CIFAR-100 datasets during the training process over 5 random trials, as shown in Figure 4. Additionally, the average accuracy of the robust pseudo-labels with varying numbers of annotators on the CIFAR-10 dataset is shown in Figure 5. ", "page_idx": 36}, {"type": "image", "img_path": "2NKumsITFw/tmp/51221d5018731f1d05f2af0fde3e31d70ceb60af473d8245d48ae90c7a5b4e08.jpg", "img_caption": ["Figure 4: Average accuracy of robust pseudo-labels on the CIFAR-10 and CIFAR-100 datasets $[R=5]$ ) during the training process. ", "", ""], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "2NKumsITFw/tmp/1dcc443fcdb52becc215ae4fbee67d96058de39349fcc00694e95a5fac78a66f.jpg", "img_caption": ["Figure 5: Average accuracy of robust pseudo-labels on the CIFAR-10 dataset with varying number of annotators in the training process. ", "(e) CIFAR-10 ( $[R=100]$ "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Test accuracy during the training process.   To further assess the effectiveness of the proposed method, we present the average test accuracy for the CIFAR-10 and CIFAR-100 datasets during the training process, as shown in Figures 6 and 7, respectively. The results indicate that the model tends to overfit during the warm-up stage, particularly under higher noise rates. This suggests that the results in Table 1 are not obtained with the optimal number of warm-up epochs. However, following the warm-up phase, the test accuracy of our method steadily improves, outperforming baseline methods acrossvariousscenarios. ", "page_idx": 37}, {"type": "image", "img_path": "2NKumsITFw/tmp/8a9522f77ed9c872997177a7890065d3e67418fb981c9bbfb1b5aaa0b8a44700.jpg", "img_caption": ["Figure 6: Average test accuracy on learning the CIFAR-10 dataset ( $\\mathcal{R}\\,=\\,5\\$ ) during the training process. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "2NKumsITFw/tmp/719a171d2708fa313f55d266153301b72aacd8e093cebab503560c19fbbf56c7.jpg", "img_caption": ["Figure 7: Average test accuracy on learning the CIFAR-100 dataset ( $\\mathcal{R}=5\\$ during the training process. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Impact of the number of warm-up epochs. Following [10, 46], we use 30 warm-up epochs for the CIFAR-10 and CIFAR-100 datasets in our experiments. To rigorously assess the impact of the warm-up stage, we conduct additional experiments with varying numbers of warm-up epochs (10, 20, 30, 40) on both our method and baseline approaches that also incorporate warm-up. The results, presented in Figure 8, illustrate how different warm-up durations affect performance. ", "page_idx": 38}, {"type": "image", "img_path": "2NKumsITFw/tmp/fbff1bf109c376e40d2ef2e948f0d8064dd8c38f2598b9da5cac2c8a7aefcbaf.jpg", "img_caption": ["Figure 8: Average test accuracies for learning the CIFAR-10 and CIFAR-100 datasets with varying numbers of warm-up epochs. The error bars representing standard deviation are shaded. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Different transition matrix estimation methods.  Our work does not focus on precise estimation of the noise transition matrix; instead, we use a simple frequency-counting method for noise transition estimation in our experiments. Nevertheless, our approach is versatile and can be integrated with various methods for estimating the noise transition matrix or the true label posterior. Additional experiments using advanced transition matrix estimation methods are presented in Table 6. As demonstrated, integrating these methods with AdaptCDRP significantly improves test accuracies compared to directly using the estimated noise transition matrices. Furthermore, applying advanced noise transition estimation methods enhances the performance of our method on real datasets. These results highlight the robustness and adaptability of our method. ", "page_idx": 38}, {"type": "table", "img_path": "2NKumsITFw/tmp/8c8c129c397758d9a0494618a1aa714ee076109338e45e931373407ec19d2bf4.jpg", "table_caption": ["Table 6: Average test accuracies (with associated standard errors expressed after the $\\pm$ signs)of learning the CIFAR-10 and real datasets with different transition matrix estimation methods. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Impact of sparse annotation. To further address the issue of annotation sparsity, we increase the total number of annotators, $R$ , to 200, and manually corrupt the datasets according to the following annotatorgroups: ", "page_idx": 39}, {"type": "text", "text": "$\\mathbf{R}{=}200{:}$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "IDN-LOW. 70 IDN-10% annotators, $70\\,I D N{-}20\\%$ annotators, 60 IDN-30% annotators;   \nIDN-MID. 70 IDN-30% annotators, $70\\,I D N{-}40\\%$ annotators,60IDN- $50\\%$ annotators; ", "page_idx": 39}, {"type": "text", "text": "IDN-HIGH. 70 IDN-50% annotators, $70\\,I D N{-}60\\%$ annotators, $60\\,I D N{-}70\\%$ annotators. ", "page_idx": 39}, {"type": "text", "text": "The three groups of annotators, labeled as IDN-LOW, IDN-MID, and IDN-HIGH, have average labeling error rates of approximately $26\\%$ $34\\%$ , and $42\\%$ , respectively. In this setup, we incorporate regularization techniques - specifically, GeoCrowdNet (F) and GeoCrowdNet (W) penalties [47] - into our method. We then compare the results against those obtained using the traditional frequencycounting approach for estimating the noise transition matrices. Table 7 presents the performance of our proposed method on the CIFAR10 ( $\\mathit{R}=200$ ) dataset, where different approaches are used to estimate the noise transition matrices. In addition, Figure 9 displays the average accuracies of the robust pseudo-labels generated by our method during the training process. These pseudo-labels play a crucial role in constructing the pseudo-empirical distribution. ", "page_idx": 39}, {"type": "table", "img_path": "2NKumsITFw/tmp/546b213c69012038d99ba232d3180f551f9aff9b10cf7d3315534cb18d4108bd.jpg", "table_caption": ["Table 7: Average test accuracies (with associated standard errors expressed after the $\\pm$ signs)of learning the CIFAR-10 dataset ( $\\left[R=200\\right)$ with different transition matrix estimation methods. "], "table_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "2NKumsITFw/tmp/9619f95a6d6cc230008e5c9c83a0c2af1ca0f3bbf8c69370b6dc05b97d5c7ac8.jpg", "img_caption": ["Figure 9: Average accuracy of robust pseudo-labels on the CIFAR-10 dataset ( $\\.R=\\,200)$ using different transition matrix estimation methods during the training process. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our contributions and scope. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We discuss the limitations of the work performed by the authors in the conclusion. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide the assumptions in the main text, and the proofs can be found in the appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide all the experimental details ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We upload our code and use public datasets. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide all the details and the code. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We provide the error bars. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confdence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We provide the training time on different datasets. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: It's not appropriate for the scope and focus of our paper, and we don't see any direct negative social impacts of our paper. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our model doesn't have a high risk for misuse or dual-use. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We cite the original papers. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 44}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide the data generation details. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: We do not involve human subjects. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: We do not involve human subjects. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]