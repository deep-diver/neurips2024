[{"figure_path": "GMsi9966DR/figures/figures_3_1.jpg", "caption": "Figure 1: Left Panel: Illustration of the do operator and the corresponding graph surgery: (a) The observation graph G; (b) The intervention graph G1 for do(X2 = x2). Right Panel (c): The DeepITE architecture has an inference and a generative model. The inference model uses a three-branch GAT to link endogenous variables x to posterior distributions of intervention indicators Yi, exogenous variables u\u017c, and observation noise precision \u03b6. The generative model then synthesizes x given these latent variables following Eq. (7) plus observation noise \u20ac. ", "description": "This figure illustrates the concept of graph surgery, showing how an intervention on a node in a causal graph modifies the graph structure.  The left panel displays an observation graph and its modified version after an intervention. The right panel depicts the architecture of DeepITE, a variational graph autoencoder.  DeepITE consists of an inference model and a generative model using Graph Attention Networks (GATs) to learn intervention targets from data. The inference model infers intervention indicators, exogenous variables, and observation noise. The generative model reconstructs the observed variables. ", "section": "5 DeepITE"}, {"figure_path": "GMsi9966DR/figures/figures_22_1.jpg", "caption": "Figure 2: The performance of DeepITE as a function of (a) graph sizes, (b) interventions, (c) the mixture proportion of soft and hard interventions, (d) sample size for each graph, (e) the number of mixed graphs.", "description": "This figure visualizes DeepITE's performance under different conditions.  It shows how DeepITE's Recall@1 and Recall@5 metrics change based on graph size, number of interventions (including hard and soft), mixture of hard and soft interventions, number of samples per graph and the number of mixed graphs used in training. This demonstrates DeepITE's robustness and scalability across diverse scenarios.", "section": "Experimental Results"}]