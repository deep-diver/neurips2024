[{"type": "text", "text": "On the Benefits of Public Representations for Private Transfer Learning under Distribution Shift ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pratiksha Thaker Carnegie Mellon University pthaker@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Amrith Setlur Carnegie Mellon University asetlur@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Zhiwei Steven Wu Carnegie Mellon University zstevenwu@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Virginia Smith Carnegie Mellon University smithv@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Public pretraining is a promising approach to improve differentially private model training. However, recent work has noted that many positive research results studying this paradigm only consider in-distribution tasks, and may not apply to settings where there is distribution shift between the pretraining and finetuning data\u2014a scenario that is likely when finetuning private tasks due to the sensitive nature of the data. In this work, we show empirically across three tasks that even in settings with large distribution shift, where both zero-shot performance from public data and training from scratch with private data give unusably weak results, public features can in fact improve private training accuracy by up to $67\\%$ over private training from scratch. We provide a theoretical explanation for this phenomenon, showing that if the public and private data share a low-dimensional representation, public representations can improve the sample complexity of private training even if it is impossible to learn the private task from the public data alone. Altogether, our results provide evidence that public data can indeed make private training practical in realistic settings of extreme distribution shift. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning models from user data can potentially disclose sensitive user information, violating privacy constraints [1\u20133]. Differential privacy is a standard framework that can be used when learning models from sensitive data to mitigate the risk of leaking private information [4]. However, differentially private learning may significantly degrade accuracy, which remains a barrier to adoption [5]. This has motivated recent works to explore the benefits of incorporating publicly available data into private training, e.g., by pretraining a model on public data and then finetuning it using private data. Empirically, this paradigm has been shown to substantially improve performance on private tasks relative to fully-private training [6\u201313]. ", "page_idx": 0}, {"type": "text", "text": "While these results are encouraging, Tram\u00e8r et al. [14] point out that much of the existing work focuses on in-distribution tasks, where the public and private tasks are very similar. For example, many private vision models [15\u201319] use public features pretrained on ImageNet [20], CIFAR-10 or CIFAR-100 [21], but these works also simulate private transfer performance by finetuning on one of these datasets. In fact, Tram\u00e8r et al. [14] point out that \u201cevery single class contained in the CIFAR-10 dataset has an identical class label in the ImageNet dataset!\u201d This is particularly problematic when attempting to understand the utility of public pretraining for private tasks, because in practice the private task is likely to contain sensitive data that is not perfectly represented by public data, such as in applications in medicine [22] or law [23]. Indeed, if data is already well-represented in a public dataset, the zero-shot performance of a model trained only on public data should be good enough that no private \u201ctransfer\u201d learning is required, potentially making these benchmark datasets uninformative for evaluating the benefits of transfer learning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "From a practical perspective, it is particularly important to understand transfer learning in the private setting: if a non-privacy-sensitive task is poorly represented by the pretrained features, one solution might be to simply add the data from that task into the public training dataset and learn a more general set of features for downstream use. But privacy-sensitive data cannot be used to train a public backbone, and individual private datasets often cannot be combined or shared. Thus, the ability to leverage public features to improve the sample dependence of private learning is critical. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this work, we provide evidence to alleviate these concerns, showing theoretically and empirically that public pretraining can be helpful even in settings with realistic and possibly extreme distribution shift between public (training) and private (transfer) tasks. In particular, we focus on concept shift, where the conditional distributions $P(Y\\mid X)$ can vary drastically between public and private tasks. Our results are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "First, we conduct empirical case studies1 on three datasets to show that public features improve private training accuracy even under extreme distribution shift. In particular, we use a pretrained CLIP ViT-B vision model for public features and measure the accuracy of private transfer learning on datasets including the PatchCamelyon (PCam) [24], Functional Map of the World (fMoW) [25], and Remote Sensing Image Scene Classification (RESISC45) [26]. On all three datasets, the pretrained model has unacceptably low zero-shot accuracy (random guessing on both PCam and fMoW), indicating that \u201cperfect privacy\u201d with zero-shot queries is likely hopeless. In comparison, on CIFAR-10, the CLIP ViT-B/32 model achieves $91.3\\%$ zero-shot accuracy [27], making transfer learning performance far less relevant as the zero-shot accuracy is already high. We observe that across all datasets, private finetuning and linear probing using public features outperform differentially training from scratch \u2013 by up to $67\\%$ . In addition, private linear probing consistently outperforms private finetuning. ", "page_idx": 1}, {"type": "text", "text": "Motivated by our empirical results, we provide a stylized theoretical model to understand and explain our findings. We study a simple linear transfer learning model, a common theoretical model in the non-private meta-learning literature [28\u201334], to show the statistical benefit of learning a shared, lowdimensional representation (in our model, a low-rank linear subspace) using public data. Our transfer learningmodelcapturesanextremeformofconceptshiftinthesensethatthetargetmodelonprivatedata is entirely different from those on public data, even though they are all contained in the same subspace. Analogous to the paradigm of public pre-training then private linear probing, we analyze a simple twostage algorithm that (1) first estimates the shared, low-dimensional representation (or subspace) from a diverse set of tasks in public data, and (2) performs private linear regression within the learned subspace. By leveraging the dimensionality reduction, we provide a better sample complexity that scales with the rank of the shared subspace instead of the ambient dimension of the features. To complement this sample complexity bound, we also show a novel lower bound that shows that our bound is tight among algorithms that search for regression parameters within a fixed low-rank subspace estimate. ", "page_idx": 1}, {"type": "text", "text": "In short, our findings provide optimistic insights regarding the concerns raised by Tram\u00e8r et al. [14]. Specifically, Tram\u00e8r et al. [14] suggest that \u201ccurrent methods for large-scale pretraining may be less effective.\u201d In contrast, our results indicate that pretrained features can indeed benefti private learning, even underconceptshift. Additionally, ourfindingsaddressanotherconcernfromTram\u00e8retal.[14]regarding the necessity of uploading private data to cloud services for finetuning large models due to high resource requirements. We demonstrate that training a linear probe privately is more effective, potentially requiring significantly fewer resources (both memory and computation) than finetuning a full model. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Empirical studies of public pretraining for private learning. As Tram\u00e8r et al. [14] point out, existing empirical studies on public pretraining for private learning largely focus on transfer between similar datasets. For example, [15\u201319, 35] pretrain on CIFAR-100 or ImageNet and finetune on CIFAR-10 or STL-10 (a dataset very similar to CIFAR-10). [8] pretrains on Places365 and finetunes on ImageNet. [18, 36] pretrain on JFT and finetune on ImageNet. Finally, [37, 38, 9, 39] pretrain and finetune on publicly available text on the Web. ", "page_idx": 1}, {"type": "text", "text": "All of these works build evidence that pretraining could be beneficial for private learning. Unfortunately, because the public and private tasks are so similar, these results are unlikely to be representative of real-world private training in which the private task requires learning a model on sensitive data with a very different distribution from data available on the Web. ", "page_idx": 2}, {"type": "text", "text": "Recent work [40] evaluates their algorithm on private learning tasks that are out-of-distribution for the feature extractor they use, including the PCam dataset that we also study. However, their algorithm requires access to (nearly) in-distribution public data in order to learn a projection matrix into a low-dimensional space. We argue that this is a strong and unrealistic assumption considering the arguments put forth in Tram\u00e8r et al. [14] that private data, because of its sensitive nature, will not be well-represented by public datasets. Our work instead focuses on understanding the improvements from using off-the-shelf feature extractors, with no in-distribution public data, over fully-private learning. ", "page_idx": 2}, {"type": "text", "text": "Transfer or meta-learning. Our results build on the framework of Tripuraneni et al. [28] for nonprivate transfer learning with a low-dimensional subspace. This linear, low-dimensional subspace assumption has been studied extensively in the nonprivate meta-learning literature as a tractable model for real shared representation learning [28\u201334]. However, none of these works consider the setting of public subspace estimation followed by private transfer learning. PILLAR [40] makes a shared subspace assumption in the private setting, but on the input features rather than on the models. ", "page_idx": 2}, {"type": "text", "text": "Private algorithms that leverage public data. A number of prior works have theoretically studied the benefits of public data in other settings, including mean estimation [41], query release [42\u201344], and optimization when gradients lie in a low-rank subspace [45\u201347]. Kairouz et al. [45] in particular gives a similar analysis using the principal angle error of the subspace, but the analysis does not apply directly as we assume that models, rather than gradients, lie in a shared low-dimensional subspace. As a result, the algorithm in that work requires expensive subspace oracle calls on every iteration and would be computationally suboptimal in our setting. ", "page_idx": 2}, {"type": "text", "text": "Finally, as discussed earlier, pretraining has empirically been shown to be useful in a number of domains, including vision [6\u20138] and NLP [9\u201312]. While our work does not model the complexities of neural networks, we can understand our results as a stylized version of finetuning in which the public network is tuned with linear regression on the last layer, potentially giving insight into these more complex models. ", "page_idx": 2}, {"type": "text", "text": "Theoretical analyses of pretraining for private learning. Ganesh et al. [35] provides a lower bound construction for a related setting in which public data is abundant and the private task is out of distribution, though does not consider the case where the public and private task explicitly share structure. In our setting, learning from the public data alone provides no guarantees on the transfer task, as we do not assume any bounded shift in the data distributions or target parameters between the public tasks to the private tasks; the key information enabling more efficient learning is the shared structure among the tasks. PILLAR [40] incorporates public pretraining, but their analysis focuses on the beneftis of dimensionality reduction using in-distribution public data, rather than transfer from out-ofdistribution public data. Finally, Ke et al. [19] study the tradeoffs between linear probing and finetuning in the private setting. While their empirical results focus on the in-distribution image recognition settings outlined previously, their theoretical results corroborate our findings that even under extreme distribution shift, linear probing is more effective than finetuning under differential privacy. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Throughout the paper, we use lower-case $v$ for vectors, upper-case $V$ for matrices and calligraphic $\\mathcal{V}$ for sets. Generally, we use the \u201chatted\u201d notation $\\hat{B},\\,\\hat{\\alpha}$ to refer to estimates of the underlying population variables. The use of $O,\\Omega,\\Theta$ is standard and $\\Tilde{O},\\Tilde{\\Omega}$ hides polylog factors in quantities we specify separately. We use $\\|\\cdot\\|_{F}$ for Frobenius, ${\\left\\|\\cdot\\right\\|}_{\\mathrm{op}}$ for operator and $\\|\\cdot\\|_{p}$ for $\\ell_{p}$ norms. ", "page_idx": 2}, {"type": "text", "text": "3.1 Differential Privacy ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Differential privacy (DP) is a quantitative constraint on the information gained from a released statistic [48]. Definition 3.1 restates the standard $(\\varepsilon,\\delta)$ -differential privacy introduced in [4]. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 ( $(\\epsilon,\\delta)$ -differential privacy [4]). Given $\\epsilon\\geq0$ , $\\delta\\in[0,1]$ and a neighboring relation $\\sim$ , a randomized mechanism $M\\colon\\mathcal{X}^{n}\\rightarrow\\mathcal{Y}_{.}$ from the set of datasets of size n to an output space $\\boldsymbol{\\wp}$ is ", "page_idx": 2}, {"type": "text", "text": "$(\\epsilon,\\delta)$ -differentially private if for all neighboring datasets $S\\!\\sim\\!S^{\\prime}\\!\\subseteq\\!\\mathcal{X}$ , and all events $E\\!\\subseteq\\!\\mathcal{V}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\mathcal{M}(\\mathcal{S})\\!\\in\\!E]\\leq e^{\\epsilon}.\\operatorname*{Pr}[\\mathcal{M}(\\mathcal{S}^{\\prime})\\!\\in\\!E]\\!+\\!\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, probabilities are taken over the random coins of $\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "The \u201cneighboring\u201d relation differs according to the desired privacy guarantee. In this paper, we will study row-level privacy in which neighboring datasets $S\\!\\sim\\!S^{\\prime}$ differ in a single element. ", "page_idx": 3}, {"type": "text", "text": "3.2 Problem Setting: Leveraging public samples for private transfer learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will study a setting in which the learner first sees $n_{1}$ public samples $\\left({x_{i},y_{i}}\\right)$ , possibly drawn from multiple different underlying tasks (i.e., sample distributions) $P_{1},\\ldots,P_{t}$ , and then sees $n_{2}$ private samples from a new task $P_{t+1}$ . The goal is to learn a predictor $f:\\mathbb{R}^{d}\\!\\rightarrow\\!\\mathcal{D}$ that maps inputs $\\dot{x}\\in\\mathbb{R}^{d}$ to outputs $y\\!\\in\\!\\mathcal{Y}$ with the constraint that $f$ must satisfy $(\\varepsilon,\\delta)$ -differential privacy. We aim to minimize the population loss on the private task: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)\\!=\\!\\mathbb{E}_{(x,y)\\sim P_{t+1}}[\\ell(f(x),\\!y)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The private learner may or may not use the public samples. We assume the samples are drawn i.i.d. conditioned on the task, but make no other assumptions on the task distribution or the number of samples drawn from each task. In Section 5, we develop a theoretical model of the relationship between the public and private tasks that allows the learner to effectively leverage information from the public tasks to improve private learning. ", "page_idx": 3}, {"type": "text", "text": "4 Public Data Improves Out-of-Distribution Private Transfer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by studying three datasets and show empirically that public data can provide benefits for private transfer learning even when the public data alone gives unusable zero-shot results on the private task. Each of the tasks we evaluate on has unusably low zero-shot performance on CLIP [27], indicating that these are highly out-of-distribution relative to the pretraining data. This directly contrasts with existing work: the CLIP model that we use (pretrained with LAION-2B) achieves $66.6\\%$ zero-shot performance on ImageNet and $93.5\\%$ accuracy on CIFAR-10. ", "page_idx": 3}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "PatchCamelyon. The PatchCamelyon (PCam) medical images dataset is a binary lymph node tumor detection task highlighted by [14]. [14] point out that CLIP [27] as well as other similar text-vision models [22] have notably poor zero-shot performance on PCam: CLIP ViT-B/32 achieves $51.2\\%$ , or close to random, in our evaluation. The poor zero-shot performance (relative to tasks like ImageNet or CIFAR) indicates that the task is truly \u201cout of distribution\u201d in comparison to the source (public) data. Moreover, being medical image data, PCam more faithfully represents a highly privacy-sensitive dataset. ", "page_idx": 3}, {"type": "text", "text": "While the next two datasets are not medical image datasets, they are widely studied distribution shift datasets that have poor zero-shot performance on the training data, making them suitable for understanding transfer learning performance. In particular, they are remote sensing datasets; Wang et al. [49] analyze LAION-2B and find that only $\\partial.03\\%$ of samples are remote sensing images, another strong indication that this data is underrepresented in pretraining. ", "page_idx": 3}, {"type": "text", "text": "fMoW. The Functional Map of the World (fMoW) dataset [25, 50] is a 62-class satellite image classification task. The pretrained CLIP ViT-B model achieves only $1.64\\%$ zero-shot accuracy, so \u201cperfect privacy\u201d with zero-shot classification is not possible. ", "page_idx": 3}, {"type": "text", "text": "RESISC45. The Remote Sensing Image Scene Classification dataset [26] is a 45-class satellite image classification task. The pretrained CLIP ViT-B model achieves $56.3\\%$ zero-shot accuracy. ", "page_idx": 3}, {"type": "text", "text": "4.2 Experimental Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We train a ViT-B/32 model [51] on each dataset (which has output dimension 512) with a linear classification head for each task. For models trained from scratch, we use Xavier initialization on the weights, while for pretrained features, we use OpenCLIP [52] models initialized with weights pretrained using LAION-2B (a 2B-sample subset of LAION-5B [53]). We use the Opacus library [54] to implement private training. For each training setting we performed a hyperparameter sweep over learning rate $(\\{1e\\!-\\!6,\\!...,\\!1e\\!-\\!2\\})$ and number of epochs (1-10 for full training and 1000-2500 for linear probing), and for private learning, clipping norm $(\\{0.5,1.0,2.5,5.0\\})$ . For both private and nonprivate models, we evaluate training from scratch, full finetuning, and linear probing. We train private models for $\\varepsilon\\in\\{0.3,0.4,0.5,1.0,2.0,5.0\\}$ for each training setting. For PCam and RESISC45, we use SGD with momentum (parameter 0.9), while for fMoW we found that Adam gave better performance [55]. We use a cosine learning rate schedule for all experiments and a batch size of 32. Each finetuning run is performed on an A100 or A6000 GPU. ", "page_idx": 3}, {"type": "table", "img_path": "E1nBLrEaJo/tmp/268f966229f9dd4ccb128c3f5a7232e16a5642188534bf5a0537f265d7ccc14d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "E1nBLrEaJo/tmp/b25c1ed5f56b5bdc4556f04040ac75c44ea1ee347be31fdeb1971d2bc784add5.jpg", "img_caption": ["Table 1: Test accuracy of nonprivate training on each dataset that we evaluate. ", "Figure 1: Private training on three datasets. (a) PCam is a binary classification task on which private training from scratch achieves relatively high accuracy, but linear probing on the pretrained model still improves accuracy up to $4\\%$ . (b) The fMoW model trained from scratch is unusable at low privacy levels while linear probing achieves close to nonprivate accuracy. (c) On RESISC45, linear probing outperforms full finetuning by over $50\\%$ at all $\\varepsilon$ levels. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We plot our private training results in Figure 1, and also provide nonprivate training and zeroshot CLIP numbers for reference in Table 1. Zeroshot CLIP has random accuracy on PCam (binary) and fMoW (62 classes). On RESISC45, zero-shot CLIP performs better than training from scratch (nonprivately), but finetuning and linear probing have nearly $40\\%$ higher accuracy. As pointed out by Tram\u00e8r et al. [14], if the zeroshot numbers (with no knowledge of the transfer task) matched the best performance of finetuning, then \u201cperfect privacy\u201d with no finetuning would be sufficient. But in each of these settings, the zero-shot performance is considerably worse than what is achievable with finetuning in both the nonprivate and private settings. ", "page_idx": 4}, {"type": "text", "text": "Across all datasets, we find that any type of finetuning significantly outperforms training privately from scratch. This indicates that the pretrained features are indeed contributing to training accuracy. Further, we find across all datasets ", "page_idx": 4}, {"type": "image", "img_path": "E1nBLrEaJo/tmp/c228700d4ddae630440a092c48f2e7969afbcf90fe45241162d6dc7bb037f275.jpg", "img_caption": ["Figure 2: Linear probing results for ViT-B/32 pretrained on a 14M subset of Datacomp-1B and on LAION-2B. (Solid lines are LAION results while dashed lines are Datacomp results.) While the linear probing results in both settings outperform training from scratch, the worse accuracy on the Datacomp pretrained features are reflective of the lower-quality features from the smaller pretraining set. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "that linear probing (fixing the pretrained features) outperforms full finetuning, sometimes by a large margin, as in the case of RESISC45. This finding is consistent with theoretical work [19] that models the benefits of linear probing over finetuning under differential privacy. This is also consistent with earlier empirical findings on (in-distribution) private finetuning [8]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The key takeaway is positive: that features that work well for nonprivate transfer learning also benefti private transfer learning even when the distribution shift is large. While the conclusions are similar, these results are especially important in the private setting: training models from scratch with strong privacy is simply infeasible for many tasks, resulting in only around $10\\%$ test accuracy for fMoW and RESISC at small values of $\\varepsilon$ . ", "page_idx": 5}, {"type": "text", "text": "To further support our results, we additionally evaluate linear probing for all three datasets with features pretrained on a 14M subset of Datacomp-1B [56] in Figure 2. The trends in this setting are the same and linear probing still outperforms private training from scratch on all datasets, but the smaller pretraining dataset leads to lower-quality features that impact the final accuracy of linear probing. ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our empirical results show that even when distribution shift is extreme, public pretraining can indeed improve the accuracy of private training. In order to explain this observation, we study a simplified linear regression setting in which the goal is to estimate regression parameters privately for a single, unseen private task. This setting has been studied extensively in the nonprivate meta-learning literature as a theoretically tractable model to explain results on larger models [28\u201334], and we propose a novel extension to the private setting that helps explain our empirical findings. ", "page_idx": 5}, {"type": "text", "text": "We show that if the regression parameters for the private task lie in a low-dimensional subspace that is shared with the public tasks, the learner can use the public data to efficiently estimate the lowdimensional subspace, project the private data into the subspace, and thus achieve private estimation error rates that match optimal private linear regression rates (up to constant factors) in $k$ dimensions (rather than $d$ dimensions), with an additive term that accounts for the error in estimating the subspace publicly. These results hold even when we make no assumptions on the relationship between the public and private task other than that they share the same low-dimensional subspace. ", "page_idx": 5}, {"type": "text", "text": "We additionally provide a novel lower bound that shows that the algorithm we analyze for our upper bound achieves the optimal rate among \u201ctwo-stage\u201d algorithms that estimate the transfer parameters within a fixed low-dimensional subspace. ", "page_idx": 5}, {"type": "image", "img_path": "E1nBLrEaJo/tmp/b38f4001277f7a46c691744286c51ca5cc0070f5e9bd6ba728b11b667efbf0bc.jpg", "img_caption": ["Figure 3: Eigenspectrum of feature covariance matrix for PCam features extracted from pretrained CLIP ViT-B/32 model. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "How realistic is the shared subspace assumption? As mentioned, the theoretical model we analyze has been previously studied to explain meta-learning results in nonprivate settings. Nevertheless, one might ask how realistic the model is for the particular settings we study, especially the assumption of a low-rank subspace shared by both the training and transfer tasks. ", "page_idx": 5}, {"type": "text", "text": "As a step toward understanding whether this assumption holds in practice, we plotted the eigenspectrum of the feature covariance matrix computed after extracting features of PCam images from the CLIP ViT-B-32 pretrained model (Figure 3). ", "page_idx": 5}, {"type": "text", "text": "From these results, we see that the pretrained features are approximately low-rank for the out", "page_idx": 5}, {"type": "text", "text": "of-distribution task PCam, yet a linear probe over these features achieves good $(83.5\\%)$ performance (Table 1). The fact that the representation still gives good performance when only a linear layer is trained on top suggests that the data does fundamentally lie in or near the low-rank space that is identified by the pretrained model. ", "page_idx": 5}, {"type": "text", "text": "In Appendix A, we plot and see similar results for the fMoW and RESISC45 datasets, where linear probing is similarly successful (relative to full finetuning). ", "page_idx": 5}, {"type": "text", "text": "5.1 Model and preliminaries ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first describe our model of the data distribution for the private task, learning objective, any assumptions we make and results from prior works we use. ", "page_idx": 6}, {"type": "text", "text": "5.1.1 Shared task structure ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider linear regression models in which every observation $\\left(x_{i},y_{i}\\right)$ for a given task is generated according to: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{x_{i}\\sim\\mathcal{N}(0,I_{d}),\\qquad\\eta\\sim\\mathcal{N}(0,1)}}\\\\ {{y_{i}=x_{i}^{\\top}B\\alpha_{t(i)}\\!+\\!\\eta_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The covariates $x_{i}$ and noise $\\eta$ are sampled i.i.d. Here, $B\\in\\mathbb{R}^{d\\times k}$ is an unknown, low rank $(k\\ll d)$ feature matrix with orthonormal columns. The matrix $B$ , and consequently the subspace spanned by its columns, is shared across all tasks in our problem setting. This includes both the public tasks that may be used to derive the initial estimate of $B$ , as well as the private tasks in single-task and multi-task transfer settings. ", "page_idx": 6}, {"type": "text", "text": "The task vectors $\\alpha_{j}$ are all assumed to lie in the true shared subspace $B$ . $t(i)$ indexes the task $\\alpha_{j}$ for the covariate $x_{i}$ : public tasks are in $\\alpha_{1\\ldots t}$ , and the transfer task is $\\alpha_{t+1}$ . Note that the tasks are not random variables and we do not make distributional assumptions on the tasks for our results. In Appendix B we provide details on the requirements for the public tasks $\\alpha_{1\\ldots t}$ (and also refer the reader to Tripuraneni et al. [28]), but for now we simply require that the public tasks are sufficiently \u201cdiverse\u201d within $B$ . ", "page_idx": 6}, {"type": "text", "text": "The learner sees $n_{1}$ samples from the public tasks (in total across all tasks) and $n_{2}$ samples drawn from the private task. ", "page_idx": 6}, {"type": "text", "text": "We are interested in learning $w$ that minimizes the following population risk: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(w)\\!=\\!\\frac{1}{2}\\mathbb{E}_{(x,y)}\\left[({x^{\\top}w\\!-\\!y})^{2}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "on the private task $B\\alpha_{t+1}$ ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Oracle for public subspace estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In stating our main results, we first assume access to an oracle that can output an orthonormal matrix $\\hat{B}\\!\\in\\!\\mathbb{R}^{d\\times k}$ that is \u201cclose to\u201d $B$ . We measure the distance between subspaces in terms of the principal angle distance, denoted $\\mathrm{sin}\\theta(B,\\hat{B})\\!=\\!\\mathrm{sin}\\theta(\\hat{B},\\!B)$ (see supplement and Tripuraneni et al. [28] for more discussion). ", "page_idx": 6}, {"type": "text", "text": "The following identities on $\\sin\\theta$ will be useful: ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.1 (subspace estimation errors). The following inequalities are satisfied for matrices with orthonormal columns $\\boldsymbol{B},\\hat{\\boldsymbol{B}}\\in\\mathbb{R}^{d\\times k}$ (and when $B,{\\hat{B}}$ are swapped): $\\|(I-\\hat{B}\\hat{B}^{\\top})B\\|_{F}\\geq\\|(I-$ $\\begin{array}{r}{\\hat{B}\\hat{B}^{\\top})B\\|_{\\mathrm{op}}\\!=\\!\\sin\\theta\\!\\left(\\hat{B},\\!B\\right)\\!\\geq\\!\\|(I\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\|_{F}\\!\\Big/\\sqrt{k}.}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Instantiating the oracle with public data. The following corollary characterizes the error incurred from estimating the underlying subspace from public data using the method-of-moments estimator from Tripuraneni et al. [28]. We state this bound for use in subsequent results but refer the reader to the supplement for the conditions required on public data in order to achieve this bound. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 ([28], Theorem 3, simplified). Let $A=(\\alpha_{1},\\ldots,\\alpha_{t})^{\\top}$ be the public task matrix, $\\nu=$ $\\sigma_{k}\\left(\\frac{A^{\\top}A}{t}\\right)$ , and $\\begin{array}{r}{\\bar{\\kappa}=\\frac{\\mathrm{tr}(\\frac{A^{\\top}A}{t})}{k\\nu}}\\end{array}$ be the average condition number. If an equal number of samples is generated from each task, and $\\bar{\\kappa}\\leq O(1)$ and $\\nu\\geq\\Omega\\big(\\frac{1}{k}\\big)$ , then the error of the method-of-moments estimator ( [28], Algorithm $^{\\,l}$ ) is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sin\\!\\theta(\\hat{B},\\!B)\\!\\le\\!\\tilde{\\cal O}\\!\\left(\\sqrt{d k^{2}/n_{1}}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability at least $1\\!-\\!O(n_{1}^{-100})$ . ", "page_idx": 6}, {"type": "text", "text": "We will refer to $\\gamma\\geq\\sin\\theta(B,\\hat{B})$ as an upper bound on the error of the subspace estimation oracle. We give upper bounds with respect to $\\gamma$ and also instantiate the bounds with the upper bound from Theorem 5.2. ", "page_idx": 6}, {"type": "text", "text": "Input: $n_{1}$ public samples drawn according to $(x_{i},y_{i}),x_{i}\\!\\sim\\!\\mathcal{N}(0,I_{d}),y_{i}\\!=\\!x_{i}^{\\top}B\\alpha_{t(i)}\\!+\\!\\eta,\\eta_{i}\\!\\sim\\!\\mathcal{N}(0,1)$ and $n_{2}$ private samples where $x_{i}$ , $\\eta_{i}$ have the same distribution, and $y_{i}\\!=\\!x_{i}^{\\top}B\\alpha_{t+1}\\!+\\!\\eta_{i}$   \n1: Use method-of-moments estimator ([28], Algorithm 1) to estimate $\\hat{B}$ using public data   \n2: Project private data $x_{i}$ to $k$ -dimensional subspace: $\\boldsymbol{x}_{i}^{\\prime}\\!=\\!\\boldsymbol{x}_{i}^{\\top}{\\hat{\\boldsymbol{B}}}$   \n3: Use DP-SGD variant of [57] on projected private data to estimate $\\alpha_{t+1}$   \nOutput: Parameter estimate $\\hat{B}\\hat{\\alpha}_{t+1}$ ", "page_idx": 7}, {"type": "text", "text": "5.1.3 Private linear regression in $d$ dimensions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use in our analysis a known upper bound for private linear regression in $d$ -dimensions. Theorem 5.3 states an informal result from [57] that upper bounds the excess risk for a variant of DP-SGD [15] (see Appendix B for more details). Furthermore, results from [58] imply that this upper bound is tight. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3 (Corollary 11 from [57], simplified). Suppose we have $n_{2}$ i.i.d. datapoints $(x_{i},y_{i})$ , where $x_{i}\\!\\sim\\!\\mathcal{N}(0,\\!I_{d})$ and $y_{i}\\!=\\!x_{i}^{\\top}w\\!+\\!\\epsilon_{i}$ , and $\\epsilon_{i}\\!\\sim\\!(0,\\!\\sigma^{2})$ . Given sufficient private samples $n_{2}$ , there exists an $(\\varepsilon,\\delta)$ private estimate $\\hat{w}_{\\mathrm{priv}}$ such that, with high probability: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{w}_{\\mathrm{priv}})\\!-\\!\\mathcal{L}(w)\\lesssim\\frac{d\\sigma^{2}}{n_{2}}\\biggl(1\\!+\\!\\tilde{\\mathcal{O}}\\biggl(\\frac{d\\log(1/\\delta)}{n_{2}\\varepsilon^{2}}\\biggr)\\biggr).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5.2 Private transfer learning for a single task ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Algorithm. Our proposed algorithm (Algorithm 1) first projects $x$ into the estimated subspace $\\hat{B}_{\\mathrm{pub}}$ , i.e., $x\\mapsto\\hat{B}_{\\mathrm{pub}}^{\\top}x$ , and then runs private linear regression in the $k$ -dimensional subspace. This is analogous to linear probing in our experiments, which first uses the public encoder to compute a low-dimensional feature representation of the data and then learns a linear model using the features. While full finetuning of the model is also a common paradigm in the transfer learning literature, we point to [19] which shows that when the feature representation is sufficiently informative, linear probing outperforms finetuning under differential privacy \u2013 a result that supports our empirical findings. ", "page_idx": 7}, {"type": "text", "text": "The following theorem states that Algorithm 1 achieves a rate that matches optimal rates for private linear regression in $k$ -dimensions, up to the subspace estimation error $\\gamma$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.4 (single-task private transfer upper bound). Assume we have access to a subspace estimation oracle that solely uses public samples to provide estimate $\\hat{B}_{\\mathrm{pub}}$ for the unknown subspace $B$ of a private task defined by the pair $(B,\\alpha_{t+1})$ in (2). Further, the estimate satisfies si $_{1}\\theta(\\hat{B}_{\\mathrm{pub}},B)\\!\\leq\\!\\gamma$ . Given $n_{2}$ i.i.d. samples from the distribution of this private task, Algorithm $^{\\,l}$ outputs an estimate $\\hat{B}_{\\mathrm{pub}}\\hat{\\alpha}_{\\mathrm{t}+1}$ that is $(\\varepsilon,\\delta)$ -differentially private, and with high probability incurs a risk of: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}\\big(\\hat{B}_{\\mathrm{pub}}\\hat{\\alpha}_{\\mathrm{t+1}}\\big)\\!-\\!\\mathcal{L}\\big(B\\alpha_{t+1}\\big)}\\\\ &{\\leq\\tilde{O}\\Big(\\|\\alpha_{t+1}\\|_{2}^{2}(\\gamma^{2}\\!+\\!1)\\Big)\\tilde{O}\\Big(\\frac{1}{n_{2}^{100}}\\!+\\!\\frac{k}{n_{2}}\\!+\\!\\frac{k^{2}\\log(1/\\delta)}{n_{2}^{2}\\varepsilon^{2}}\\Big)\\!+\\!\\gamma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof sketch. The proof nearly follows from existing bounds on subspace estimation and private linear regression. The key difficulty is that regression on the input $x\\!\\sim\\!\\mathcal{N}(0,\\!I_{d})$ projected into the estimated subspace $\\hat{B}_{\\mathrm{pub}}$ still leaves the residual that does not lie in $\\hat{B}_{\\mathrm{pub}}$ , which can be treated as a noise term if we can show that the residual is independent of the projected $x$ . We can show this because $\\hat{B}_{\\mathrm{pub}}$ is orthogonal to $\\hat{B}_{\\mathrm{pub}}^{\\perp}$ (spans null space of $\\hat{B}_{\\mathrm{pub}})$ ), so under the i.i.d. Gaussian assumption on $x$ , the residual is independent of the projected $x$ . As a result, we obtain the private linear regression rate in dimensions with a variance of $\\stackrel{\\cdot}{1\\!+\\!\\gamma^{2}}$ rather than 1 and an additive $\\gamma^{2}$ bias. ", "page_idx": 7}, {"type": "text", "text": "Discussion. From Theorem 5.4, we can break down the errors into an unavoidable bias due to the subspace estimation error (dependent only on the number of public samples) and the subsequent linear regression error due to privacy. For a subspace estimation error $\\gamma$ we require $\\begin{array}{r}{n_{1}\\geq\\frac{d k^{2}}{\\gamma^{2}}}\\end{array}$ \u2265d\u03b3k2 . Given this inevitable error we can hope to achieve an accuracy of $\\textstyle\\operatorname{err}+\\gamma^{2}$ where err is the additional linear regression error and $\\sin\\theta\\left(B,\\hat{B}_{\\mathrm{pub}}\\right)\\leq\\gamma.$ . This requires approximately: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\nn_{2}\\geq{\\frac{k}{\\operatorname{err}}}\\!+\\!{\\frac{k}{\\varepsilon{\\sqrt{\\operatorname{err}}}}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "samples. That is, if the subspace estimation error is zero then we achieve the rate of private linear regression in $k$ dimensions, and consequently optimal non-private rates when $\\epsilon\\!\\to\\!\\infty$ . ", "page_idx": 8}, {"type": "text", "text": "5.3 Lower bound for two-phase estimator ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the previous subsection, we proved an upper bound on the single-task transfer for row-level $(\\varepsilon,\\delta)$ -DP private algorithm, when the publicly estimated subspace $\\hat{B}_{\\mathrm{pub}}$ is $\\gamma$ accurate. In this section, we show that our upper bound is tight among algorithms for our problem that search for solutions within a fixed subspace. ", "page_idx": 8}, {"type": "text", "text": "In particular, we analyze the lowest possible transfer error achieved by any $(\\varepsilon,\\delta)$ -DP algorithm that: (i) takes as input private dataset $\\boldsymbol{S}$ of $n_{2}$ i.i.d. samples from task $\\alpha_{t+1}$ , $\\gamma$ -accurate public estimate $\\hat{B}_{\\mathrm{pub}}$ , and (ii) outputs an estimate in the column space of $\\hat{B}_{\\mathrm{pub}}$ . In Theorem 5.5, we present a lower bound on the risk suffered by any algorithm in such a class. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5 (Two-stage single-task private transfer lower bound). Let $M$ be an $(\\varepsilon,\\delta)$ -DP private algorithm where $\\varepsilon\\!\\in\\!(0,1)$ , $\\delta\\!<\\!^{1}\\!/n^{1+\\omega}$ , $\\omega\\!>\\!0$ , that takes as input: $(i)$ publicly estimated subspace $\\hat{B}_{\\mathrm{pub}}$ from an oracle that only uses public samples; and $(i i)\\,a$ dataset $\\boldsymbol{S}$ of $n_{2}$ private samples. For any such $M$ , there exists a private problem instance given by the pair $(B,\\alpha_{t+1})$ where $B\\!\\in\\!\\mathrm{Gr}_{k,d}(\\mathbb{R}),\\alpha_{t+1}\\!\\in\\!\\mathbb{R}^{k}$ , sin $\\theta(B,\\hat{B}_{\\mathrm{pub}})\\leq\\gamma_{\\mathrm{s}}$ , and $\\|B\\alpha_{t+1}\\|_{2}\\leq1$ , such that for $S$ sampled i.i.d. from this instance using the model in (2), we have: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{M}\\mathbb{E}_{S|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}(y-M(S,\\hat{B}_{\\mathrm{pub}})^{\\top}x)^{2}}\\\\ &{=\\,\\Omega\\biggl(\\biggl(\\frac{k^{2}}{n_{2}^{2}\\varepsilon^{2}}\\!+\\!\\frac{k}{n_{2}}\\biggr)(\\sigma^{2}\\!+\\!\\gamma^{2})\\!+\\!\\gamma^{2}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof Sketch. Our proof relies mainly on tracing attacks in [59, 58], but our analysis additionally needs to handle the misspecification of the subspace $B$ which influences the construction of the worst case problem instance. When we project inputs $x\\mapsto\\hat{B}_{\\mathrm{pub}}^{\\top}x$ B\u02c6p\u22a4ubx, we can show that the projected samples can now be treated as i.i.d. samples from a $k$ -dimensional linear regression model with independent noise. For a fixed $\\hat{B}_{\\mathrm{pub}}$ , any choice of $B,\\alpha_{t+1}$ affects both the scaling of the noise $(\\propto\\|(I\\!-\\!\\hat{B_{\\mathrm{pub}}}\\hat{B}_{\\mathrm{pub}}^{\\top})B\\alpha_{t+1}\\|_{2}^{2})$ and the direction of the regression vector, based on how much of the true parameter $B\\alpha_{t+1}$ is captured in given subspace $\\hat{B}_{\\mathrm{pub}}$ . To handle this, we first construct subclasses of the adversary, where each subclass fixes the norm of $\\|\\hat{B}_{\\mathrm{pub}}^{\\top}B\\alpha_{t+1}\\|_{2}$ . Then, we lower bound the minimax risk over this subclass by via a Bayes risk which we further lower bound by constructing a tracing adversary. ", "page_idx": 8}, {"type": "text", "text": "We show that there exists a prior $\\pi$ over $B\\alpha_{t+1}$ where the probability of the intersection of the following two events is very low: (i) small estimation error $\\mathbb{E}_{\\pi}\\mathcal{L}(M(S,\\hat{B}_{\\mathrm{pub}}))$ , and (ii) small success rate for the tracing adversary to infer the membership of some element in $\\boldsymbol{S}$ . Since, $M$ has to be $(\\epsilon,\\delta)$ private, this reults in a Bayes risk lower bound. ", "page_idx": 8}, {"type": "text", "text": "Discussion. Our lower bound for the class of two-stage algorithms matches our upper bound in Theorem 5.4. This implies that our Algorithm 1 is optimal when $\\hat{B}_{\\mathrm{pub}}$ is the estimate given by the optimal subspace estimation oracle over public samples. When we use Al\u221agorithm 1 from [28], the estimation error matches lower bounds (Theorem 5 in [28]) upto a factor of $\\sqrt{k}$ . ", "page_idx": 8}, {"type": "text", "text": "5.4 Simulated results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we complement the results in this section through a simulated empirical study matching the setup described in Section 5.1. ", "page_idx": 8}, {"type": "text", "text": "Setup. We simulate $n_{1}$ samples $\\left(x_{i},y_{i}\\right)$ from $t=100$ public tasks where the true dimension $d\\!=\\!25$ but the underlying subspace $B$ has rank 5. As baselines, we compare against nonprivate linear regression, DP-SGD without a subspace estimate, and DP-SGD initialized with the true subspace $B$ , and compare against DP-SGD initialized with the subspace estimated using the method-of-moments estimator [28]. We use the Google Tensorflow implementation of DP-SGD for private learning [60]. ", "page_idx": 8}, {"type": "text", "text": "We used a grid search of hyperparameters to set the clipping norm to 0.5, learning rate to 0.1, and used 50 epochs of training for DP-SGD. We use the RDP accountant to set $\\varepsilon\\!=\\!1.1$ and $\\delta\\!=\\!1\\mathrm{e}\\!-\\!5$ . ", "page_idx": 9}, {"type": "text", "text": "Our results are shown in Figure 4. We observe that, as expected, private training from scratch has high error, and additional public data $\\it{n}_{1}\\mathrm{=}500$ vs $n_{1}\\!=\\!2000;$ ) improves performance, reducing the $\\ell_{2}$ parameter error close to that of using DP-SGD with the true underlying subspace B (matching our intuition, for example, from Figure 2). However, we also see that when performing private transfer there are diminishing returns for this more precise subspace estimation, as the noise introduced via private learning becomes a dominating factor. ", "page_idx": 9}, {"type": "image", "img_path": "E1nBLrEaJo/tmp/5b206d72c388acc7079bec8a1bfd880b916a77038e721ba10f7922735da2aa4b.jpg", "img_caption": ["Figure 4: Empirical verification of setup described in Section 5.1. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our results answer questions posed by [14] positively. Empirically, we show that across three datasets with significant shift between the public and private tasks, publicly pretrained features do make private learning far more effective, taking models from unusable when trained from scratch to close-to-nonprivate performance when trained privately with linear probing. In addition, we provide a theoretical model to explain our findings, based on models of nonprivate transfer learning. Our model supports our empirical findings, suggesting that public features should indeed reduce private sample complexity under even extreme distribution shift when the public and private tasks share a low-dimensional representation. Altogether, our conclusions are optimistic and provide confidence that public data can indeed support private training even for highly sensitive tasks that cannot and should not be used in public training. However, our linear subspace model has the clear limitation of being a simplified model for the neural network representations used in practice. As this is a limitation shared by literature on nonprivate transfer learning [28\u201334], improvements in this area would contribute to both the private and nonprivate transfer learning literature. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. Thanks to Shengyuan Hu, Tian Li, Qi Pang, and Anirudh Sivaraman for helpful discussions and feedback that improved the writing. This work was supported in part by the National Science Foundation grants IIS2145670 and CCF2107024, and funding from Amazon, Apple, Google, Intel, Meta, and the CyLab Security and Privacy Institute. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies. Z.S.W. was in part supported by NSF Awards #1763786 and #2339775. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322\u20131333, 2015.   \n[2] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 3\u201318. IEEE, 2017.   \n[3] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021.   \n[4] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265\u2013284. Springer, 2006.   \n[5] Rachel Cummings, Damien Desfontaines, David Evans, Roxana Geambasu, Matthew Jagielski, Yangsibo Huang, Peter Kairouz, Gautam Kamath, Sewoong Oh, Olga Ohrimenko, et al. Challenges towards the next frontier in privacy. arXiv preprint arXiv:2304.06929, 2023. [6] Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns, and Stefano Soatto. Mixed differential privacy in computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8376\u20138386, 2022. [7] Zelun Luo, Daniel J Wu, Ehsan Adeli, and Li Fei-Fei. Scalable differential privacy with sparse network finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5059\u20135068, 2021. [8] Alexey Kurakin, Shuang Song, Steve Chien, Roxana Geambasu, Andreas Terzis, and Abhradeep Thakurta. Toward training at imagenet scale with differential privacy. arXiv preprint arXiv:2201.12328, 2022.   \n[9] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. In International Conference on Learning Representations, 2021.   \n[10] Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, and Jiang Bian. Exploring the limits of differentially private deep learning with group-wise clipping. In The Eleventh International Conference on Learning Representations, 2022.   \n[11] Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Differentially private optimization on large model at small cost. In International Conference on Machine Learning, pages 3192\u20133218. PMLR, 2023.   \n[12] Antonio Ginart, Laurens van der Maaten, James Zou, and Chuan Guo. Submix: Practical private prediction for large-scale language models. arXiv preprint arXiv:2201.00971, 2022.   \n[13] Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private SGD with gradient subspace identification. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.   \n[14] Florian Tram\u00e8r, Gautam Kamath, and Nicholas Carlini. Considerations for differentially private learning with large-scale public pretraining. arXiv preprint arXiv:2212.06470, 2022.   \n[15] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[16] Nicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making the shoe fit: Architectures, initializations, and tuning for learning with privacy. 2019.   \n[17] Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data). In International Conference on Learning Representations, 2020.   \n[18] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022.   \n[19] Shuqi Ke, Charlie Hou, Giulia Fanti, and Sewoong Oh. On the convergence of differentiallyprivate fine-tuning: To linearly probe or to fully fine-tune? arXiv preprint arXiv:2402.18905, 2024.   \n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[22] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for zero-shot transfer learning. Neurocomputing, 555:126658, 2023.   \n[23] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlp dataset for legal contract review. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.   \n[24] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. June 2018.   \n[25] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In CVPR, 2018.   \n[26] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017.   \n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[28] Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations. In International Conference on Machine Learning, pages 10434\u201310443. PMLR, 2021.   \n[29] Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[30] Weisen Jiang, James Kwok, and Yu Zhang. Subspace learning for effective meta-learning. In International Conference on Machine Learning, pages 10177\u201310194. PMLR, 2022.   \n[31] Nikunj Saunshi, Arushi Gupta, and Wei Hu. A representation learning perspective on the importance of train-validation splitting in meta-learning. In International Conference on Machine Learning, pages 9333\u20139343. PMLR, 2021.   \n[32] Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Why does maml outperform erm? an optimization perspective. arXiv preprint arXiv:2010.14672, page 6, 2020.   \n[33] Parker Knight and Rui Duan. Multi-task learning with summary statistics. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022.   \n[35] Arun Ganesh, Mahdi Haghifam, Milad Nasr, Sewoong Oh, Thomas Steinke, Om Thakkar, Abhradeep Guha Thakurta, and Lun Wang. Why is public pretraining necessary for private model training? In International Conference on Machine Learning, pages 10611\u201310627. PMLR, 2023.   \n[36] Harsh Mehta, Abhradeep Thakurta, Alexey Kurakin, and Ashok Cutkosky. Large scale transfer learning for differentially private image classification. arXiv preprint arXiv:2205.02973, 2022.   \n[37] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via lowrank reparametrization. In International Conference on Machine Learning, pages 12208\u201312218. PMLR, 2021.   \n[38] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In International Conference on Learning Representations, 2021.   \n[39] Simran Arora and Christopher R\u00e9. Can foundation models help us achieve perfect secrecy? arXiv preprint arXiv:2205.13722, 2022.   \n[40] Francesco Pinto, Yaxi Hu, Fanny Yang, and Amartya Sanyal. Pillar: How to make semi-private learning more effective. arXiv preprint arXiv:2306.03962, 2023.   \n[41] Brendan Avent, Yatharth Dubey, and Aleksandra Korolova. The power of the hybrid model for mean estimation. Proceedings on Privacy Enhancing Technologies, 4:48\u201368, 2020.   \n[42] Terrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, and Steven Wu. Leveraging public data for practical private query release. In International Conference on Machine Learning, pages 6968\u20136977. PMLR, 2021.   \n[43] Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and Steven Wu. Private query release assisted by public data. In International Conference on Machine Learning, pages 695\u2013703. PMLR, 2020.   \n[44] Miguel Fuentes, Brett C Mullins, Ryan McKenna, Gerome Miklau, and Daniel Sheldon. Joint selection: Adaptively incorporating public information for private synthetic data. In International Conference on Artificial Intelligence and Statistics, pages 2404\u20132412. PMLR, 2024.   \n[45] Peter Kairouz, Monica Ribero Diaz, Keith Rush, and Abhradeep Thakurta. (nearly) dimension independent private erm with adagrad via publicly estimated subspaces. In Conference on Learning Theory, pages 2717\u20132746. PMLR, 2021.   \n[46] Ehsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas Steinke, Vinith M Suriyakumar, Om Thakkar, and Abhradeep Thakurta. Public data-assisted mirror descent for private model training. In International Conference on Machine Learning, pages 517\u2013535. PMLR, 2022.   \n[47] Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. In International Conference on Learning Representations, 2020.   \n[48] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n[49] Zhecheng Wang, Rajanie Prabha, Tianyuan Huang, Jiajun Wu, and Ram Rajagopal. Skyscript: A large and semantically diverse vision-language dataset for remote sensing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 5805\u20135813, 2024.   \n[50] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International conference on machine learning, pages 5637\u20135664. PMLR, 2021.   \n[51] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[52] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. If you use this software, please cite it as below.   \n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[54] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021.   \n[55] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models, 2022.   \n[56] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.   \n[57] Prateek Varshney, Abhradeep Thakurta, and Prateek Jain. (nearly) optimal private linear regression for sub-gaussian data via adaptive clipping. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 1126\u20131166. PMLR, 02\u201305 Jul 2022. URL https://proceedings.mlr.press/v178/varshney22a.html.   \n[58] T Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. The Annals of Statistics, 49(5):2825\u20132850, 2021.   \n[59] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate differential privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 1\u201310, 2014.   \n[60] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.   \n[61] Camille Jordan. Essai sur la g\u00e9om\u00e9trie \u00e0 $n$ dimensions. Bulletin de la Soci\u00e9t\u00e9 math\u00e9matique de France, 3:103\u2013174, 1875.   \n[62] Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory. 1990.   \n[63] John C Duchi, Vitaly Feldman, Lunjia Hu, and Kunal Talwar. Subspace recovery from heterogeneous data with non-isotropic noise. Advances in Neural Information Processing Systems, 35: 5854\u20135866, 2022.   \n[64] Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 552\u2013563. IEEE, 2017.   \n[65] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning highdimensional distributions. In Conference on Learning Theory, pages 1853\u20131902. PMLR, 2019.   \n[66] Alan Edelman, Tom\u00e1s A Arias, and Steven T Smith. The geometry of algorithms with orthogonality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303\u2013353, 1998.   \n[67] Vladimir Igorevich Bogachev and Maria Aparecida Soares Ruas. Measure theory, volume 1. Springer, 2007. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "E1nBLrEaJo/tmp/0e51770388dbfcb882392e2353a4e68a44b8365142cb0b1b00e624db6085e322.jpg", "img_caption": ["Figure 5: Eigenspectra of feature covariance matrices for features extracted from pretrained CLIP ViT-B/32 model. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "It is natural to ask whether the assumption that public (pretraining) tasks and private tasks truly share a low-dimensional subspace as we model in our theoretical analysis. In order to validate this, in figure 5, we plot the eigenspectra of the feature covariance matrices for each of the datasets we evaluate in Section 4. The key takeaway is that even though the three datasets are out of distribution for the pretraining data, the extracted features are low rank. In addition, these features are effective when used to train a linear probe (in contrast, if the subspace were misspecified, the features may be low-rank but lead to poor results with linear probing). ", "page_idx": 14}, {"type": "text", "text": "B Additional definitions and assumptions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Preliminaries for Tripuraneni et al. [28] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we elaborate on the assumptions required to use algorithms from Tripuraneni et al. [28] for subspace estimation using public data (i.e., instantiating the subspace oracle). ", "page_idx": 14}, {"type": "text", "text": "B.1.1 Principal angles ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our analysis requires a notion of distance between subspaces, for which we use the maximum principal angle [61]. We give a definition here and refer the reader to [62] or [63], Appendix A, for more details. ", "page_idx": 14}, {"type": "text", "text": "Definition B.1 (Maximum principal angle). Let $U,V\\in\\mathbb{R}^{d\\times k}$ be orthogonal matrices. Let U and $\\nu$ be the subspaces spanned by the columns of $U$ and $V$ respectively. The maximum principal angle $\\theta\\!\\in\\![0,\\!\\pi/2]$ between U and $\\nu$ is defined by $\\mathrm{sin}\\theta(U,V)\\!=\\!\\|U\\dot{U}^{\\top}\\!-\\!V\\dot{V}^{\\top}\\|\\!=\\!\\|U^{\\top}V_{\\bot}\\|\\!\\stackrel{!}{=}\\!\\|V^{\\top}\\!U_{\\bot}\\|$ . ", "page_idx": 14}, {"type": "text", "text": "B.1.2 Task diversity assumptions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our model each data point $(x_{i},y_{i})$ is associated with a task $\\alpha_{t(i)}\\in\\mathbb{R}^{k}$ . We do not make distributional assumptions on these tasks, but estimating the subspace accurately requires certain diversity assumptions on the tasks. We inherit the following assumption from [28]: ", "page_idx": 14}, {"type": "text", "text": "Assumption B.2 (Task diversity and normalization). Define $\\textstyle A\\!=\\!(\\alpha_{1},\\ldots,\\alpha_{t})^{\\top}$ and $\\nu\\!=\\!\\sigma_{r}\\!\\left(\\frac{A^{\\top}A}{t}\\right)$ . The $t$ underlying task parameters $\\alpha_{j}$ satisfy $\\|\\alpha_{j}\\|\\!=\\!\\Theta(1)$ for all $j\\in[t]$ . Moreover, we assume $\\nu\\!>\\!0$ . Icno nthdiet ifoonl lnouwminbge,r we will also use t, hteo  afuvretrhaegre c choarnadcittieorinz en tuhme btaesr $\\begin{array}{r}{\\bar{\\kappa}=\\frac{\\mathrm{tr}\\left(\\frac{A\\top A}{t}\\right)}{r\\nu}}\\end{array}$ , and the worst-case $\\kappa\\!=\\!\\sigma_{1}\\!\\left(\\frac{A^{\\top}A}{t}\\right)/\\nu$ ", "page_idx": 15}, {"type": "text", "text": "Then we have: ", "page_idx": 15}, {"type": "text", "text": "Theorem 5.2 ([28], Theorem 3, simplified). Let $A=\\left(\\alpha_{1},\\ldots,\\alpha_{t}\\right)^{\\top}$ be the public task matrix, $\\nu=$ $\\sigma_{k}\\left(\\frac{A^{\\top}A}{t}\\right)$ , and \u03ba\u00af = tr( AtA) be the average condition number. If an equal number of samples is generated from each task, and $\\bar{\\kappa}\\leq O(1)$ and $\\nu\\geq\\Omega\\big(\\frac{1}{k}\\big)$ , then the error of the method-of-moments estimator ( [28], Algorithm 1) is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sin\\!\\theta(\\hat{B},\\!B)\\!\\le\\!\\tilde{\\cal O}\\!\\left(\\sqrt{d k^{2}/n_{1}}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1\\!-\\!O(n_{1}^{-100})$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Estimation error bounds ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We restate the full bound given by [57] for private SGD. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.3 ([57], Corollary 11, simplified). Suppose we have data $(x_{i},y_{i})$ such that $x_{i}\\!\\sim\\!\\mathcal{N}(0,\\!I_{k})$ and $y_{i}\\!=\\!x_{i}^{\\top}w^{*}\\!+\\!\\epsilon_{i}$ , where $\\epsilon_{i}\\!\\sim\\!(0,\\!\\sigma^{2})$ . Then, assuming $\\begin{array}{r}{n_{2}\\!\\ge\\!\\tilde{\\Omega}\\Bigg(k\\big(1\\!+\\!\\frac{\\sqrt{\\log(1/\\delta)}}{\\varepsilon}\\big)\\Bigg)}\\end{array}$ , we have: ", "page_idx": 15}, {"type": "text", "text": "1. Algorithm DP-AMBSSGD ([57], Algorithm 2) with parameters \u03b7=1/4k, \u03b1= 8lo\u03b5g(1/\u03b4)i s $(\\varepsilon,\\delta){-}D P.$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{w}^{p r i v})\\!-\\!\\mathcal{L}(w^{*})\\!\\leq\\!\\frac{\\|w^{*}\\|_{2}^{2}}{n_{2}^{100}}\\!+\\!\\frac{8k\\sigma^{2}}{n_{2}}\\!\\left(1\\!+\\!\\tilde{O}\\!\\left(\\frac{k\\mathrm{log}(1/\\delta)}{n_{2}\\varepsilon^{2}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Technical lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we state or restate key lemmas that we will refer to in the following proofs. ", "page_idx": 15}, {"type": "text", "text": "We will use the following lemma to argue that we can project $x$ into the estimated subspace $\\hat{B}$ and treat the residual (that lies outside of $\\hat{B}$ ) as i.i.d. Gaussian noise. ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1 (Independence of $x$ residual). Consider orthonormal matrices $B$ and $\\hat{B}\\in\\mathbb{R}^{d\\times k}$ and $\\alpha\\in\\mathbb{R}^{k}$ . Let $(x,y)$ be generated according to the model in Equation 2 where $x\\sim\\mathcal{N}(0,I_{d})$ and $y\\!=\\!x^{\\top}B\\alpha\\!+\\!\\eta$ . Then the projection of x into $\\hat{B}$ , $x^{\\top}(\\hat{B}\\hat{B}^{\\top})B\\alpha,$ , is independent of the residual that lies in the complement of $\\hat{B}$ , i.e. $x^{\\top}(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha$ . Moreover, this residual is i.i.d. Gaussian. ", "page_idx": 15}, {"type": "text", "text": "Proof. We can rewrite the distribution of $y\\left|x\\right|$ in terms of the projection of the regression vector $B\\alpha$ on to the column span of $\\hat{B}$ , when the input $x$ is also projected in the following way: x \u2192B\u02c6x: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y=x^{\\top}B\\alpha\\!+\\!\\eta;}\\\\ &{\\quad=x^{\\top}((\\hat{B}\\hat{B}^{\\top})B\\alpha\\!+\\!(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha)\\!+\\!\\eta;}\\\\ &{\\quad=x^{\\top}(\\hat{B}\\hat{B}^{\\top}B\\alpha)\\!+\\!x^{\\top}(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha\\!+\\!\\eta;}\\\\ &{\\quad=(x^{\\top}\\hat{B})\\hat{\\alpha}\\!+\\!x^{\\top}(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha\\!+\\!\\eta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\hat{\\alpha}{:=}\\hat{B}^{\\top}B\\alpha$ is a $k-$ dimensional vector in the column span of the given subspace $\\hat{B}$ . ", "page_idx": 15}, {"type": "text", "text": "Next, we note that the projection of input $x$ in the column span of B\u02c6 and its projection into the corresponding null space are independent, i.e., $x^{\\top}(\\hat{B}\\hat{B}^{\\top})\\perp x^{\\top}(I_{d}\\!-\\hat{B}\\hat{B}^{\\top})$ . This is easy to show since both $x^{\\top}(\\hat{B}\\hat{B}^{\\top})$ and $x^{\\top}(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})$ are jointly Gaussian and are marginally distributed as zero mean Gaussian random variables with their covariance: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(\\hat{B}\\hat{B}^{\\top})x x^{\\top}(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})]}\\\\ &{~~~=\\!(\\hat{B}\\hat{B}^{\\top})\\mathbb{E}[x x^{\\top}](I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})}\\\\ &{~~~=\\!(\\hat{B}\\hat{B}^{\\top})I_{d}(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})}\\\\ &{~~~=\\!\\hat{B}\\hat{B}^{\\top}\\!-\\!\\hat{B}\\hat{B}^{\\top}\\!=\\!0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This implies independence of the two projections. Note that the last step in the above calculation uses the fact that $\\hat{B}^{\\top}\\bar{\\hat{B}}=I_{k}$ . Since the two projections are independent, we can rewrite the conditional distribution $y\\vert x$ as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\boldsymbol{x}}^{\\hat{B}}=:{\\boldsymbol{x}}^{\\top}{\\hat{\\boldsymbol{B}}}}\\\\ {y|{\\boldsymbol{x}}^{\\hat{B}}\\sim{\\mathcal{N}}(({\\boldsymbol{x}}^{\\hat{B}})^{\\top}{\\hat{\\alpha}},{\\hat{\\sigma}}^{2}),\\ \\ \\mathrm{where},{\\hat{\\sigma}}^{2}\\!=\\!\\sigma^{2}\\!+\\!\\|(I_{d}\\!-\\!\\hat{B}{\\hat{B}}^{\\top}){\\boldsymbol{B}}\\alpha\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.2 ([58, 64, 65]). Let $M$ be an $(\\varepsilon,\\delta)$ -differentially private algorithm with $0<\\varepsilon<1$ and $\\delta\\!>\\!0$ . Further, let $A_{i}\\!=\\!A_{\\hat{\\alpha}}((y_{i},\\!x_{i}^{\\hat{B}}),\\!M(\\mathcal{S}))$ and $A_{i}^{\\prime}\\!=\\!A_{\\hat{\\alpha}}((y_{i},\\!x^{\\hat{B}}),\\!M(S_{i}^{\\prime}))$ when $(y_{i},x_{i}^{\\hat{B}})\\!\\in\\!S$ and ${\\mathbf{}}S_{i}^{\\prime}$ replaces $(y_{i},x_{i}^{\\hat{B}})$ with another IID draw from the same distribution. Then, for every $T\\!>\\!0,$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}A_{i}\\leq\\mathbf{E}A_{i}^{\\prime}\\!+\\!2\\varepsilon\\mathbb{E}|A_{i}^{\\prime}|\\!+\\!2\\delta T\\!+\\!\\int_{T}^{\\infty}\\!\\!\\mathbb{P}(|A_{i}|\\!>\\!t).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. let $Z^{+}\\!=\\!\\operatorname*{max}(Z,0)$ and $Z^{-}=-\\operatorname*{min}(Z,0)$ denote the positive and negative parts of random variable $Z$ respectively. We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}A_{i}\\!=\\!\\mathbb{E}A_{i}^{+}\\!-\\!\\mathbb{E}A_{i}^{-}\\!=\\!\\int_{0}^{\\infty}\\!\\mathbb{P}(A_{i}^{+}\\!>\\!t)\\,d t\\!-\\!\\int_{0}^{\\infty}\\!\\mathbb{P}(A_{i}^{-}\\!>\\!t)\\,d t.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the positive part, if $0\\!<\\!T\\!<\\!\\infty$ and $0\\!<\\!\\varepsilon\\!<\\!1$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{0}^{\\infty}\\!\\mathbb{P}(A_{i}^{+}>t)\\,d t=\\int_{0}^{T}\\!\\mathbb{P}(A_{i}^{+}>t)\\,d t+\\int_{T}^{\\infty}\\!\\mathbb{P}(A_{i}^{+}>t)\\,d t}}\\\\ &{\\leq\\displaystyle\\int d t_{0}^{T}\\big(e^{\\varepsilon}\\mathbb{P}(A_{i}^{+}>t)+\\delta\\big)\\,d t+\\int_{T}^{\\infty}\\!\\mathbb{P}(A_{i}^{+}>t)\\,d t}\\\\ &{\\leq\\displaystyle\\int_{0}^{\\infty}\\!\\mathbb{P}({A_{i}^{\\prime}}^{+}>t)\\,d t+2\\varepsilon\\int_{0}^{\\infty}\\!\\mathbb{P}({A_{i}^{\\prime}}^{+}>t)\\,d t+\\delta T\\!+\\!\\int_{T}^{\\infty}\\!\\mathbb{P}(|A_{i}|>t)\\,d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly for the negative part, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{0}^{\\infty}\\mathbb{P}(A_{i}^{-}>t)\\,d t=\\int_{0}^{T}\\mathbb{P}(A_{i}^{-}>t)\\,d t+\\int_{T}^{\\infty}\\mathbb{P}(A_{i}^{-}>t)\\,d t}}\\\\ &{\\geq\\int_{0}^{T}\\left(e^{-\\varepsilon}\\mathbb{P}(A_{i}^{\\prime^{-}}>t)-\\delta\\right)d t+\\int_{T}^{\\infty}\\mathbb{P}(A_{i}^{-}>t)\\,d t}\\\\ &{\\geq\\int_{0}^{T}\\mathbb{P}(A_{i}^{\\prime^{-}}>t)\\,d t-2\\varepsilon\\int_{0}^{T}\\mathbb{P}({A_{i}^{\\prime^{-}}>t})-\\delta T+\\int_{T}^{\\infty}\\!\\mathbb{P}(A_{i}^{-}>t)\\,d t}\\\\ &{\\geq\\int_{0}^{\\infty}\\mathbb{P}({A_{i}^{\\prime^{-}}>t})\\,d t-2\\varepsilon\\int_{0}^{\\infty}\\mathbb{P}({A_{i}^{\\prime^{-}}>t})-\\delta T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It then follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb E}A_{i}\\leq\\displaystyle\\int_{0}^{\\infty}\\!{\\mathbb P}({A_{i}^{\\prime}}^{+}>t)\\,d t-\\displaystyle\\int_{0}^{\\infty}\\!{\\mathbb P}({A_{i}^{\\prime}}^{-}>t)\\,d t+2\\varepsilon\\displaystyle\\int_{0}^{\\infty}\\!{\\mathbb P}(|A_{i}^{\\prime}|>t)\\,d t+2\\delta T\\displaystyle\\int_{T}^{\\infty}\\!{\\mathbb P}(|A_{i}|>t)\\,d t}\\\\ &{\\qquad={\\mathbb E}A_{i}^{\\prime}+2\\varepsilon{\\mathbb E}|A_{i}|+2\\delta T+\\displaystyle\\int_{T}^{\\infty}\\!{\\mathbb P}(|A_{i}|>t)\\,d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.3 (Stein\u2019s Lemma). Let $Z$ be distributed according to some density $p(z)$ that is continuously differentiable with respect to $z$ and let $h\\colon\\ensuremath{\\mathbb{R}}\\!\\to\\ensuremath{\\mathbb{R}}$ be a differentiable function such that ${\\bf E}|h^{\\prime}(Z)|\\!<\\!\\infty$ , then: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}h^{\\prime}(Z)\\!=\\!\\mathbf{E}\\Bigg[\\frac{-h(Z)p^{\\prime}(Z)}{p(Z)}\\Bigg].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Proofs for Section 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Proof of Theorem 5.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we prove the upper bound result on the two-phase algorithm for single-task transfer learning, which first estimates the subspace publicly, projects the inputs into the estimated subspace and then privately performs linear regression on the projected data. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{md}\\mathbf{E}\\big[(\\langle\\hat{\\alpha}^{p r i v},\\hat{B}^{\\top}x\\rangle\\!-\\!y)^{2}\\big]\\!-\\!\\operatorname*{min}_{\\alpha}\\!\\mathbf{E}[(\\langle\\alpha,\\hat{B}^{\\top}x\\rangle\\!-\\!y)^{2}]\\!\\leq\\!g(n_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\hat{\\alpha}\\!=\\!\\operatorname*{min}_{\\alpha}\\mathbf{E}\\big[(\\langle\\alpha,\\hat{B}^{\\top}x\\rangle\\!-\\!y)^{2}\\big]$ (the best $\\alpha$ using $x$ projected into $\\hat{B}$ ), and let $\\hat{\\alpha}^{p r i v}$ be the output of DP-AMBSSGD on $x$ projected into the estimated $\\hat{B}$ . then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\tilde{z}}\\bigg[\\bigg(\\big\\langle\\hat{\\alpha}^{p r i v},\\hat{B}^{\\top}x\\big\\rangle-y\\bigg)^{2}\\bigg]-\\mathbf{E}\\bigg[\\big(\\big\\langle\\alpha^{*},B^{\\top}x\\big\\rangle-y\\big)^{2}\\bigg]}\\\\ &{=\\mathbf{E}\\bigg[\\bigg(\\big\\langle\\hat{\\alpha}^{p r i v},\\hat{B}^{\\top}x\\big\\rangle\\!-\\!y\\bigg)^{2}\\bigg]\\!-\\!\\mathbf{E}\\bigg[\\big(\\big\\langle\\alpha^{*},B^{\\top}x\\big\\rangle\\!-\\!y\\big)^{2}\\bigg]\\!+\\!\\mathbf{E}\\bigg[\\bigg(\\big\\langle\\hat{\\alpha},\\hat{B}^{\\top}x\\big\\rangle\\!-\\!y\\bigg)^{2}\\bigg]\\!-\\!\\mathbf{E}\\bigg[\\Big(\\big\\langle\\hat{\\alpha},\\hat{B}^{\\top}x\\big\\rangle\\!-\\!y\\Big)^{2}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can break this into two parts: we will first bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left(\\langle\\hat{\\alpha}^{p r i v},\\hat{B}^{\\top}x\\rangle\\!-\\!y\\right)^{2}\\right]\\!-\\!\\operatorname*{min}_{\\alpha}\\!\\mathbf{E}\\left[\\left(\\langle\\alpha,\\hat{B}^{\\top}x\\rangle\\!-\\!y\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}\\biggl[\\Bigl(\\langle\\hat{\\alpha},\\hat{B}^{\\top}x\\rangle\\!-\\!y\\Bigr)^{2}\\biggr]\\!-\\!\\mathbf{E}\\Bigl[\\bigl(\\langle\\alpha^{*},B^{\\top}x\\rangle\\!-\\!y\\bigr)^{2}\\Bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We first bound (14). Note that according to the model (2), ", "page_idx": 17}, {"type": "equation", "text": "$$\ny\\!=\\!x^{\\top}B\\alpha^{*}\\!+\\!\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\epsilon$ is $\\mathcal{N}(0,\\!1)$ . ", "page_idx": 17}, {"type": "text", "text": "However, our algorithm first projects $x$ into the space of the estimated $\\hat{B}$ before performing linear regression in the lower-dimensional space, which introduces additional error. ", "page_idx": 17}, {"type": "text", "text": "We can rewrite $y$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\ny\\!=\\!x^{\\top}\\hat{B}\\hat{B}^{\\top}B\\alpha^{*}\\!+\\!x^{\\top}(I\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha^{*}\\!+\\!\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "decomposing the first term into the projection into $\\hat{B}$ and the remaining error due to projection. ", "page_idx": 17}, {"type": "text", "text": "By Lemma C.1, this residual term is independent of the first term (with $x$ projected into $\\hat{B}$ ) and $\\epsilon$ . ", "page_idx": 17}, {"type": "text", "text": "We claim that the variance of the residual is $\\sin(\\theta)^{2}\\!+\\!\\|\\alpha^{*}\\|_{2}^{2}$ : ", "page_idx": 17}, {"type": "text", "text": "Let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\epsilon^{\\prime}\\!=\\!x^{\\top}(I\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha^{*}\\!+\\!\\epsilon}}\\\\ {{=\\!x^{\\top}\\hat{B}_{\\perp}\\hat{B}_{\\perp}^{\\top}B\\alpha^{*}\\!+\\!\\epsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and note that the total variance is the sum of the variances because the terms are independent. Moreover, the first term is a rescaled i.i.d. Gaussian with zero mean. Then the variance of $\\epsilon^{\\prime}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf E}[({x}^{\\top}\\hat{B}_{\\bot}\\hat{B}_{\\bot}^{\\top}B{\\alpha}^{*})^{\\top}{x}^{\\top}\\hat{B}_{\\bot}\\hat{B}_{\\bot}^{\\top}B{\\alpha}^{*}]{+}{\\sigma}^{2}}\\\\ &{{=}{\\mathbf E}[{\\alpha}^{*\\top}B^{\\top}\\hat{B}_{\\bot}\\hat{B}_{\\bot}^{\\top}x x^{\\top}\\hat{B}_{\\bot}\\hat{B}_{\\bot}^{\\top}B{\\alpha}^{*}]{+}{\\sigma}^{2}}\\\\ &{{=}{\\mathbf E}[{\\alpha}^{*\\top}B^{\\top}\\hat{B}_{\\bot}\\hat{B}_{\\bot}^{\\top}B{\\alpha}^{*}]{+}{\\sigma}^{2}}\\\\ &{{=}\\sin({\\theta})^{2}\\|{\\alpha}^{*}\\|_{2}^{2}{+}{\\sigma}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, we assume $\\sigma^{2}\\!=\\!1$ so we have $\\operatorname{var}(\\epsilon^{\\prime})\\!=\\!\\sin(\\theta)^{2}\\|\\alpha^{*}\\|_{2}^{2}\\!+\\!1$ ", "page_idx": 18}, {"type": "text", "text": "Using the rewritten $y$ , we can treat the new private regression problem as estimating $\\hat{B}^{\\top}B\\alpha^{*}$ . Thus we will instantiate $g(n_{2})$ with the linear regression bound from Theorem B.3 with $k$ dimensions and variance $\\sigma^{2}\\!=\\!\\sin(\\theta)^{2}\\|{\\alpha^{*}}\\|_{2}^{2}\\!+\\!1$ . ", "page_idx": 18}, {"type": "text", "text": "Now we bound the second half of the expression, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\bigg[\\Big(\\langle\\hat{\\alpha},\\hat{B}^{\\top}x\\rangle\\!-\\!y\\Big)^{2}\\bigg]\\!-\\!\\mathbf{E}\\Big[\\big(\\langle\\alpha^{*},B^{\\top}x\\rangle\\!-\\!y\\big)^{2}\\Big]}\\\\ &{=\\!\\mathbf{E}\\bigg[\\Big(\\langle\\hat{\\alpha},\\hat{B}^{\\top}x\\rangle\\!-\\!\\langle\\alpha^{*},B^{\\top}x\\rangle\\!+\\!\\langle\\alpha^{*},B^{\\top}x\\rangle\\!-\\!y\\Big)^{2}\\bigg]\\!-\\!\\mathbf{E}\\Big[\\big(\\langle\\alpha^{*},B^{\\top}x\\rangle\\!-\\!y\\big)^{2}\\Big]}\\\\ &{=\\!\\mathbf{E}\\Big[\\big(\\langle\\hat{B}\\hat{\\alpha}-B\\alpha^{*},x\\rangle\\big)^{2}\\Big]\\!=\\!\\|\\hat{B}\\hat{\\alpha}\\!-\\!B\\alpha^{*}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally this leaves us to bound $\\big||\\hat{B}\\hat{\\alpha}\\!-\\!B\\alpha^{*}\\big||_{2}^{2}$ . We will make use of the following lemma: ", "page_idx": 18}, {"type": "text", "text": "Lemma D.1. Let \u03b1\u02c6 be the (public) linear regression estimate of the task \u03b1 on the projected data ${\\hat{B}}^{\\top}x$ .   \nThen $\\begin{array}{r}{\\lVert\\hat{B}\\hat{\\alpha}\\!-\\!B\\alpha^{*}\\rVert_{2}^{2}\\!\\leq\\!(\\!\\sin\\!\\theta(B,\\hat{B}))^{2}\\lVert B\\alpha^{*}\\rVert_{2}^{2}.}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{B}\\hat{\\alpha}-B\\alpha^{*}}\\\\ &{=\\hat{B}(\\mathbf{E}[\\hat{B}^{\\top}x x^{\\top}\\hat{B}]^{-1})\\hat{B}^{\\top}\\mathbf{E}[x x^{\\top}B\\alpha^{*}]-B\\alpha^{*}}\\\\ &{=\\hat{B}(\\mathbf{E}[\\hat{B}^{\\top}x x^{\\top}\\hat{B}]^{-1})\\hat{B}^{\\top}\\mathbf{E}[x x^{\\top}]B\\alpha^{*}-B\\alpha^{*}}\\\\ &{=\\hat{B}(\\mathbf{E}[\\hat{B}^{\\top}x x^{\\top}\\hat{B}]^{-1})\\hat{B}^{\\top}\\mathbf{E}[x x^{\\top}](\\hat{B}\\hat{B}^{\\top}B\\alpha^{*}+(I-\\hat{B}\\hat{B}^{\\top})B\\alpha^{*})-B\\alpha^{*}}\\\\ &{=\\hat{B}(\\mathbf{E}[\\hat{B}^{\\top}x x^{\\top}\\hat{B}]^{-1})\\hat{B}^{\\top}\\mathbf{E}[x x^{\\top}]\\hat{B}\\hat{B}^{\\top}B\\alpha^{*}-B\\alpha^{*}}\\\\ &{=\\hat{B}\\hat{B}^{\\top}B\\alpha^{*}-B\\alpha^{*}}\\\\ &{=\\hat{B}\\hat{B}^{\\top}B\\alpha^{*}-B B^{\\top}B\\alpha^{*}}\\\\ &{=(\\hat{B}\\hat{B}^{\\top}-B B^{\\top})B\\alpha^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{B}\\hat{\\alpha}\\!-\\!B\\alpha^{*}\\|_{2}^{2}}\\\\ &{=(\\hat{B}\\hat{B}^{\\top}\\!-\\!B B^{\\top})B\\alpha^{*}}\\\\ &{\\leq(\\sin\\!\\theta(B,\\hat{B}))^{2}\\|B\\alpha^{*}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we have $\\begin{array}{r}{\\mathbf{E}\\bigg[\\Big(\\langle\\hat{\\alpha},\\hat{B}^{\\top}x\\rangle\\!-\\!y\\Big)^{2}\\bigg]\\!-\\!\\mathbf{E}\\Big[\\big(\\langle\\alpha^{*},B^{\\top}x\\rangle\\!-\\!y\\big)^{2}\\Big]\\!\\leq\\!h(n_{1})^{2}\\|B\\alpha^{*}\\|_{2}^{2}.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Putting these together gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\biggl[\\Bigl(\\langle\\hat{\\alpha}^{p r i v},\\hat{B}^{\\top}x\\rangle\\!-\\!y\\Bigr)^{2}\\biggr]\\!-\\!\\mathbf{E}\\Bigl[\\bigl(\\langle\\alpha^{*},B^{\\top}x\\rangle\\!-\\!y\\bigr)^{2}\\Bigr]}\\\\ &{\\leq\\!g(n_{2})\\!+\\!h(n_{1})^{2}\\|B\\alpha^{*}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the generic result with $\\gamma$ subspace error, we substitute $h(n_{1})\\!=\\!\\gamma$ , or Theorem 5.2 to instantiate the bound with the method of moments estimator [28]. Substituting B.3 for $g(n_{2})$ and taking a union bound over failure probabilities gives the result. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D.2 Proof of Theorem 5.5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we prove our result lower bounding the lowest possible transfer error achieved by any $(\\varepsilon,\\delta)$ -DP algorithm in our single-task transfer setting. We denote the class of two-stage algorithms of interest as $\\mathcal{M}_{\\mathrm{2stg}}(\\epsilon,\\!\\delta,\\!\\gamma)$ . Each algorithm in our class satisfies the following: ", "page_idx": 19}, {"type": "text", "text": "1. The algorithm takes as input private dataset $\\boldsymbol{S}$ of $n_{2}$ i.i.d. samples from task $\\alpha_{t+1}$ , along with a $\\gamma$ -accurate public estimate B\u02c6,   \n2. Projects the input data point $x\\mapsto{\\hat{B}}^{\\top}x$ for any $\\scriptstyle(x,y)$ in the private dataset $\\boldsymbol{S}$ .   \n3. Algorithm outputs an estimate in the column space of $\\hat{B}$ . ", "page_idx": 19}, {"type": "text", "text": "Similarly, we can define a class of misspecified linear regression problem instances. We use $\\mathcal{P}_{\\mathrm{2stg}}(d,k,\\gamma)$ to denote the following class of problem instances for the private task $t\\!+\\!1$ : ", "page_idx": 19}, {"type": "text", "text": "1. The input product distribution over $\\scriptstyle(x,y)$ is given by the model in (2), and additionally the noise $\\bar{\\eta}\\!\\sim\\!\\bar{\\mathcal{N}}(0,\\sigma^{2})$ .   \n2. Let the true regression vector be $B\\alpha_{t+1}$ . A subspace $\\hat{B}$ is known such that: $\\sin\\theta(\\hat{B},\\!B)\\!\\leq\\!\\gamma$ Also, $\\alpha_{t+1}\\!\\in\\!\\mathbb{R}^{k}$ .   \n3. The i.i.d. sampled dataset $\\boldsymbol{S}$ from the above model satisifies: $\\|x\\|_{2}\\!\\leq\\!1$ for every $x\\!\\in\\!S$ . ", "page_idx": 19}, {"type": "text", "text": "In the above, both $B$ and $\\hat{B}$ are $d{\\times}k$ matrices with orthonormal columns, i.e., $B,\\hat{B}\\!\\in\\!\\mathrm{Gr}_{k,d}(\\mathbb{R})$ , where, $\\mathrm{Gr}_{k,d}(\\mathbb{R})$ is the Grassmann manifold [66] and consists of the set of $k$ -dimensional subspaces within an underlying $d$ -dimensional space. Also, for both $\\mathcal{M}_{\\mathrm{2stg}}(\\epsilon,\\!\\delta,\\!\\gamma),\\!\\mathcal{P}_{\\mathrm{2stg}}(d,\\!k,\\!\\gamma)$ we omit the dependence on $\\hat{B}$ , which is fixed. We note here that our proof works for any fixed $\\hat{B}$ . ", "page_idx": 19}, {"type": "text", "text": "Now, we are ready to restate our Theorem 5.5. ", "page_idx": 19}, {"type": "text", "text": "Theorem 5.5 (Two-stage single-task private transfer lower bound). Let $M$ be an $(\\varepsilon,\\delta)$ -DP private algorithm where $\\varepsilon\\!\\in\\!(0,1)$ , $\\delta\\!<\\!^{1}\\!/n^{1+\\omega}$ , $\\omega\\!>\\!0$ , that takes as input: $(i)$ publicly estimated subspace $\\hat{B}_{\\mathrm{pub}}$ from an oracle that only uses public samples; and $(i i)\\,a$ dataset $\\boldsymbol{S}$ of $\\dot{n}_{2}$ private samples. For any such $M$ , there exists a private problem instance given by the pair $(B,\\alpha_{t+1})$ where $B\\!\\in\\!\\mathrm{Gr}_{k,d}(\\mathbb{R}),\\alpha_{t+1}\\!\\in\\!\\mathbb{R}^{k}$ , sin $\\theta(B,\\hat{B}_{\\mathrm{pub}})\\leq\\gamma$ , and $\\|B\\alpha_{t+1}\\|_{2}\\leq1$ , such that for $S$ sampled i.i.d. from this instance using the model in (2), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{M}\\mathbb{E}_{S|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}(y-M(S,\\hat{B}_{\\mathrm{pub}})^{\\top}x)^{2}}\\\\ &{=\\,\\Omega\\biggl(\\biggl(\\displaystyle\\frac{k^{2}}{n_{2}^{2}\\varepsilon^{2}}\\!+\\!\\frac{k}{n_{2}}\\biggr)(\\sigma^{2}\\!+\\!\\gamma^{2})\\!+\\!\\gamma^{2}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Given the estimate $\\hat{B}$ , the goal is to lower bound the following minimax risk: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\mathrm{2stg}}(\\epsilon,\\delta,\\gamma)\\atop B,\\alpha_{t+1}\\in\\mathcal{P}_{\\mathrm{2stg}}(d,k,\\gamma)}~\\mathbb{E}_{M}\\mathbb{E}_{S|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}(y-M(S,\\hat{B})^{\\top}x)^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us begin by defining the class of regression vectors constiuting the set of all possible $B\\alpha_{t+1}$ that can be realized by a problem instance in $\\mathcal{P}_{\\mathrm{2stg}}(d,k,\\gamma)$ . Given some rank $k$ subspace defined by the matrix $\\hat{B}\\!\\in\\!\\operatorname{Gr}_{k,d}(\\mathbb{R})$ , we define the following set of $d$ -dimensional $\\ell_{2}$ norm bounded vectors that are $\\gamma\\!\\le\\!1$ close to given $\\hat{B}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta(B,\\gamma)=:\\left\\{\\theta\\in\\mathbb{R}^{d}:\\theta\\!=\\!B\\alpha_{t+1}\\;\\mathrm{for}\\left(B,\\alpha_{t+1}\\right)\\!\\in\\!\\mathcal{P}_{\\mathrm{2stg}}(d,\\!k,\\gamma)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From the definition of the principal angles and $\\mathcal{P}_{\\mathrm{2stg}}(d,k,\\gamma)$ it follows that for any $\\theta\\!\\in\\!\\theta(B,\\!\\gamma)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\hat{B}\\theta\\|_{2}\\ge\\sqrt{1\\!-\\!\\gamma^{2}}\\iff\\|(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})\\theta\\|_{2}\\!\\le\\!\\gamma\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can break the above set $\\Theta({\\hat{B}},\\gamma)$ into disjoint sets: $\\begin{array}{r}{\\Theta(\\hat{B},\\gamma)\\!=\\!\\mathrm{I}\\big[_{\\rho\\in[\\sqrt{1-\\gamma^{2}},1]}\\Theta_{\\rho}(\\hat{B})}\\end{array}$ , where $\\Theta_{\\rho}(\\hat{B})$ is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta_{\\rho}(\\hat{B})=:\\left\\{\\theta\\in\\Theta(\\hat{B},\\gamma):\\|\\hat{B}\\theta\\|_{2}\\!=\\!\\rho\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The above subclass of regression vectors results in a convenient subclass of problem instances class $\\mathcal{P}_{\\mathrm{2stg}}(\\rho)$ . Just as we did for $\\mathcal{P}_{\\mathrm{2stg}}(d,k,\\gamma)$ , we can define the following minimax risk for $\\mathcal{P}_{\\mathrm{2stg}}(\\rho)$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\mathrm{2stg}}(\\epsilon,\\delta,\\gamma)}~\\operatorname*{sup}_{B,\\alpha_{t+1}\\in\\mathcal{P}_{2\\mathrm{stg}}(\\rho)}~\\mathbb{E}_{S|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}\\left(y\\!-\\!M(\\mathcal{S},\\!\\hat{B})^{\\top}x\\right)^{2}\\!.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Based on the above definitions we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{M\\in\\mathcal{M}_{2^{\\mathrm{st}}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}~\\underset{B,\\alpha_{t+1}\\in\\mathcal{P}_{2^{\\mathrm{st}}}(d,k,\\gamma)}{\\operatorname*{sup}}~\\mathbb{E}_{\\mathcal{S}|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}~(y-M(\\mathcal{S},\\hat{B})^{\\top}x)^{2}}\\\\ &{\\quad=\\underset{M\\in\\mathcal{M}_{2^{\\mathrm{st}}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}~\\underset{\\rho\\in[\\sqrt{1-\\gamma^{2}},1]}{\\operatorname*{sup}}~\\underset{B,\\alpha_{t+1}\\in\\mathcal{P}_{2^{\\mathrm{st}}}(\\rho)}{\\operatorname*{sup}}~\\mathbb{E}_{\\mathcal{S}|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}(y-M(\\mathcal{S})^{\\top}x)^{2}}\\\\ &{\\quad=\\underset{M\\in\\mathcal{M}_{2^{\\mathrm{st}}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}~\\underset{\\rho\\in[\\sqrt{1-\\gamma^{2}},1]}{\\operatorname*{sup}}~\\underset{\\theta\\in\\Theta_{\\rho}(\\hat{B})}{\\operatorname*{sup}}~\\mathbb{E}_{\\mathcal{S}|\\theta=B\\alpha_{t+1}}\\mathbb{E}_{(x,y)|\\theta=B\\alpha_{t+1}}~(y-M(\\mathcal{S})^{\\top}x)^{2}}\\\\ &{\\quad\\geq\\underset{\\rho\\in[\\sqrt{1-\\gamma^{2}},1]}{\\operatorname*{sup}}~\\underset{M\\in\\mathcal{M}_{2^{\\mathrm{st}}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{sup}}~\\underset{\\theta\\in\\Theta_{\\rho}(\\hat{B})}{\\operatorname*{sup}}~(y-M(\\mathcal{S})^{\\top}x)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the final inequality uses inf sup $\\geq$ supinf [67]. We can do this because inf and sup are defined over non-empty sets and the loss function remains bounded over the product space $\\bar{\\mathcal{M}_{\\mathrm{2stg}}}(\\epsilon,\\delta,\\gamma)\\times$ $\\Theta_{\\rho}(\\hat{B})$ . The loss function is bounded because the norm of the regression vector and the input covariates is bounded. Further, the linearly independent noise $\\eta$ in $y$ (2) has finite variance. ", "page_idx": 20}, {"type": "text", "text": "For the next part of the proof, we focus on lower bounding the minimax risk in (23) when the adversary is searching over the set $\\Theta_{\\rho}(\\hat{B})$ . The lower bound for the minimax risk over this subclass is given by two parts: (i) statistical error rate that is suffered by any non-private algorithm for which we lower bound hypothesis testing lower bounds; and (ii) the risk suffered by any $(\\varepsilon,\\delta)$ -DP private estimator which we lower bound by constructing a tracing attack. We will begin the proof for the latter part and then plug in standard statistical risk lower bounds. ", "page_idx": 20}, {"type": "text", "text": "The following Lemma D.2 (proven later) states a lower bound over the class $\\mathcal{P}_{\\mathrm{2stg}}(\\rho)$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2 (Lower bound for $\\mathcal{P}_{\\mathrm{2stg}}(\\rho))$ . For any fixed $\\hat{B}$ and any $(\\varepsilon,\\delta)$ -DP private algorithm $M$ (where $0\\!<\\!\\varepsilon\\!<\\!1$ , $\\delta\\!<\\!^{1}\\!/n^{1+\\omega}$ for some $\\omega\\!>\\!0$ ) that belongs to class $\\mathcal{M}_{\\mathrm{2stg}}(\\epsilon,\\!\\delta,\\!\\gamma)$ , there exists a problem instance for the transfer task in the class ${\\mathcal{P}}_{\\mathrm{2stg}}(\\rho)$ such that for the $B,\\alpha_{t+1}$ given by the problem instance: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M}\\mathbb{E}_{S|B,\\alpha_{t+1}}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}(y-M(\\mathcal{S},\\hat{B})^{\\top}x)^{2}=\\Omega\\biggl(\\biggl(\\frac{k^{2}}{n_{2}^{2}\\varepsilon^{2}}+\\frac{k}{n_{2}}\\biggr)(\\sigma^{2}+1-\\rho^{2})+1-\\rho^{2}\\biggr).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can now come back to (27) and compute the supremum over $\\rho$ after plugging in the lower bound in Lemma D.2. Since $\\rho\\ge\\sqrt{1\\!-\\!\\gamma^{2}}$ , plugging in this value for $\\rho$ in Lemma D.2, and from the minimax risk lower bound on $\\mathcal{P}_{\\mathrm{2stg}}(d,k,\\gamma)$ in (27), we obtain the result in Theorem 5.5. ", "page_idx": 20}, {"type": "text", "text": "D.2.1 Proof of Lemma D.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. The proof for the subclass lower bound relies upon re-parameterizing the problem instance as a $k$ -dimensional linear regression in the problem, but now in the column span of $\\bar{\\hat{B}}$ . ", "page_idx": 20}, {"type": "text", "text": "Let the worst case in instance in ${\\mathcal{P}}_{\\mathrm{2stg}}(\\rho)$ be $\\theta=B\\alpha_{t+1}$ , where $\\|\\hat{B}^{\\top}\\theta\\|_{2}=\\rho$ . We shall derive a low-dimensional linear regression problem posed by the the unknown worst case instance $\\theta$ , and the projected inputs: $x\\mapsto{\\hat{B}}^{\\top}{\\bar{x}}$ . Recall that the joint data distribution for private samples is given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{x\\sim\\mathcal{N}(0,I_{d}),}\\\\ {y\\,|\\,x\\sim\\mathcal{N}(x^{\\top}\\theta,\\sigma^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Additionally, we also recall that the learning algorithm is given $n_{2}$ private i.i.d. samples from the above distribution ${\\cal{S}}=:\\{(x_{i},y_{i})\\}_{i=1}^{n}$ . In addition, it is also given a rank $k$ matrix with orthonormal columns: $\\hat{B}\\!\\in\\!\\operatorname{Gr}_{k,d}(\\mathbb{R})$ that is close to the unknown low rank subspace $B$ , i.e., $\\sin\\theta(\\hat{B},\\!B)\\!\\le\\!\\gamma\\implies$ $\\|(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\|_{2}\\!\\le\\!\\gamma$ . Next, we write each sample in $\\boldsymbol{S}$ in terms of the projection of the regression vector $B\\alpha$ on to the column span of $\\hat{B}$ , when the input $x$ is also projected in the following way: $x\\!\\mapsto\\!{\\hat{B}}x$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z\\sim\\!\\mathcal{N}(0,\\!\\sigma^{2})}\\\\ &{y=x^{\\top}B\\alpha\\!+\\!z;}\\\\ &{~~=\\!x^{\\top}\\!\\big((\\hat{B}\\hat{B}^{\\top})B\\alpha\\!+\\!(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha\\big)\\!+\\!z;}\\\\ &{~~=\\!x^{\\top}\\!\\big(\\hat{B}\\hat{B}^{\\top}B\\alpha\\big)\\!+\\!x^{\\top}\\!(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha\\!+\\!z;}\\\\ &{~~=\\!\\big(x^{\\top}\\hat{B}\\big)\\!\\hat{\\alpha}\\!+\\!x^{\\top}\\!(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha\\!+\\!z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{\\alpha}{:=}\\hat{B}^{\\top}B\\alpha$ is a $k-$ dimensional vector in the column span of the given subspace $\\hat{B}$ . ", "page_idx": 21}, {"type": "text", "text": "For any output $M(\\boldsymbol{\\cal S},\\hat{\\boldsymbol{B}})$ for an algorithm in $\\mathcal{M}_{\\mathrm{2stg}}(\\epsilon,\\!\\delta,\\!\\gamma)$ , from the independence of the two projections: ${\\hat{B}}{\\hat{B}}^{\\top}x$ and $(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})x$ argued in Lemma C.1.: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S|B,\\alpha}\\mathbb{E}_{(x,y)|B,\\alpha_{t+1}}(y-M(\\mathcal{S},\\hat{B})^{\\top}x)^{2}}\\\\ &{\\qquad\\quad=\\mathbb{E}_{S|B,\\alpha}\\mathbb{E}_{x}\\mathbb{E}_{\\eta}(x^{\\top}\\hat{B}\\hat{B}^{\\top}\\theta+x^{\\top}(I_{d}-\\hat{B}\\hat{B}^{\\top})\\theta+\\eta-M(\\mathcal{S},\\hat{B})^{\\top}x)^{2}}\\\\ &{\\qquad\\quad=\\sigma^{2}+\\|(I_{d}-\\hat{B}\\hat{B}^{\\top})\\theta\\|_{2}^{2}+\\mathbb{E}_{S|B,\\alpha}\\mathbb{E}_{x}\\mathbb{E}_{\\eta}(x^{\\top}\\hat{B}\\hat{B}^{\\top}\\theta-M(\\mathcal{S},\\hat{B})^{\\top}x)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since, the norm of $\\theta$ in the nullspace of $\\hat{B}$ can be chosen without affecting the hardness of the above rejection problem, the worst case problem instance will maximize the additive error by picking any component along the null space (note that direction along null space does not impact the regression error) that has the maximum norm of $1-\\rho^{2}$ (recall that for any $\\bar{\\theta}\\in\\mathcal P_{\\mathrm{2stg}}(\\rho)$ , $\\lVert\\theta\\rVert_{2}\\leq1)$ . Now, from (31) it follows that the i.i.d. samples in $\\boldsymbol{S}$ for the worst case instance are drawn from the following low-dimensional linear regression model: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\scriptstyle x\\;\\sim\\;{\\mathcal{N}}(0,I_{d})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From the above model in (33) and equivalence in (32), we have the following equality for the minimax risk over $\\mathcal{P}_{\\mathrm{2stg}}(\\rho)$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{M\\in\\mathcal{M}_{2\\mathrm{stg}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}\\ \\underset{B,\\alpha\\in\\mathcal{P}_{2\\mathrm{stg}}(\\rho)}{\\operatorname*{sup}}\\ \\mathbb{E}_{S|B,\\alpha}\\mathbb{E}_{(x,y)|B,\\alpha}\\left(y\\!-\\!M(\\mathcal{S},\\hat{B})^{\\top}x\\right)^{2}}\\\\ &{\\quad=\\underset{M\\in\\mathcal{M}_{2\\mathrm{stg}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}\\ \\underset{\\hat{\\alpha}=\\hat{\\mathcal{B}}^{\\top}B\\alpha,}{\\operatorname*{sup}}\\ \\underset{\\mathbb{E}_{S|\\hat{\\alpha}}}{\\mathbb{E}_{S|\\hat{\\alpha}}}\\mathbb{E}_{x|\\hat{\\alpha}}\\left(\\hat{\\alpha}^{\\top}x^{\\hat{B}}\\!-\\!M(\\mathcal{S},\\hat{B})^{\\top}x^{\\hat{B}}\\right)\\!+\\!\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}}\\\\ &{\\quad=\\underset{M\\in\\mathcal{M}_{2\\mathrm{stg}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}\\ \\underset{\\hat{\\alpha}=\\hat{\\mathcal{B}}^{\\top}B\\alpha,}{\\operatorname*{sup}}\\ \\underset{\\mathbb{E}_{S|\\hat{\\alpha}}}{\\mathbb{E}_{S|\\hat{\\alpha}}}{\\mathbb{E}_{|\\hat{\\alpha}}}-M(\\mathcal{S},\\hat{B})\\|_{2}^{2}\\!+\\!\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2},}\\\\ &{\\quad=\\underset{M\\in\\mathcal{M}_{2\\mathrm{stg}}(\\epsilon,\\delta,\\gamma)}{\\operatorname*{inf}}\\ \\underset{\\hat{\\alpha}=\\hat{\\mathcal{B}}^{\\top}B\\alpha,}{\\operatorname*{sup}}\\!\\!\\!\\!(\\rho)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $S\\!=\\!\\{(x_{i}^{\\hat{B}},y_{i})\\}_{i=1}^{n_{2}}$ and $y_{i}\\!=\\!\\hat{\\alpha}^{\\top}x_{i}^{\\hat{B}}\\!+\\!z_{i}$ , where $z_{i}\\!\\sim\\!\\mathcal{N}(0,\\!\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})$ ", "page_idx": 21}, {"type": "text", "text": "Our main technique for proving lower bounds for the minimax risk in (34) is based on the tracing adversary technique proposed by [59]. Next, we prove that there exists a prior over the effective regression vector $\\hat{\\alpha}$ , and a tracing attack that is successful in recovering an element of the i.i.d. sampled dataset $\\boldsymbol{S}$ , in expectation over the prior and the dataset. Consider the following tracing attack: ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{\\hat{\\alpha}}((x^{\\hat{B}},y),M(S,\\hat{B}))=(y-(x^{\\hat{B}})^{\\top}\\hat{\\alpha})\\sum_{j=1}^{k-1}(M(S,\\hat{B})_{j}-\\hat{\\alpha}_{j})\\cdot x_{j}^{\\hat{B}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similar to the tracing attacks for mean estimation problems [58], Lemma D.3 proves that the attack $A_{\\hat{\\alpha}}\\big((y,x^{\\hat{B}}),~M(S)\\big)$ takes large value when $(x^{\\hat{B}},y)$ belongs to $\\boldsymbol{S}$ and small value otherwise. We compare the attack success with estimation error, and show that whenever the estimation error is small, the attack has to be fairly successful. Since, we are only searching over private algorithms where the attack takes small values, this yields a lower bound on the estimation error. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The minimax lower bound stated in Lemma D.2, i.e., the lower bound for the minimax risk over subclass $\\mathcal{P}_{\\mathrm{2stg}}(\\rho)$ (for a fixed $\\rho$ ), is given by the summation over two terms: the statistical lower bound and a second term implied by the tracing attack sucess lower bound stated in Lemma D.3 (proven later). ", "page_idx": 22}, {"type": "text", "text": "Lemma D.3. For any fixed $0<\\sigma$ , ${\\sqrt{1\\!-\\!\\gamma^{2}}}\\,\\leq\\,\\rho\\,\\leq\\,1,$ , $(B,\\alpha)$ satisfying sin $\\theta(\\hat{B},B)\\leq\\gamma,$ , and $\\|\\hat{\\alpha}\\|_{2}\\!=\\!\\|\\hat{B}^{\\top}B\\alpha\\|_{2}\\!=\\!\\rho\\!\\le\\!1,$ , let $(x^{\\hat{B}},y)$ be an i.i.d. sample (and $\\boldsymbol{S}$ a dataset of $n_{2}$ i.i.d. samples) drawn from the distribution defined in (33). Then, for every $(\\varepsilon,\\delta)$ -differentially private estimator $M$ that takes as input $^{S,\\hat{B}}$ and satisfies $\\mathbb{E}_{S|B,\\alpha,\\sigma,\\rho}\\|M(S,\\hat{B})\\!-\\!\\hat{\\alpha}\\|_{2}^{2}\\!=\\!o(1),$ , for every $\\hat{\\alpha}$ , the following are true: ", "page_idx": 22}, {"type": "text", "text": "1. For each $i\\in[n],$ , let ${\\mathbf{}}S_{i}^{\\prime}$ denote the data set obtained by replacing $(x_{i}^{\\hat{B}},y_{i})$ in $\\boldsymbol{S}$ with an independent copy from the distribution in (31), then $\\mathbb{E}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(S_{i}^{\\prime}))\\!=\\!0$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,|A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(S_{i}^{\\prime},\\hat{B}))|\\!\\le\\!(\\sqrt{\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}})\\!\\cdot\\!\\sqrt{\\mathbb{E}\\|M(S,\\hat{B})\\!-\\!\\hat{\\alpha}\\|_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "2. There exists a prior distribution of $\\pi\\!=\\!\\pi(\\hat{\\alpha})$ supported over $\\hat{\\alpha}\\!\\in\\!\\mathbb{R}^{k}$ such that ${\\hat{\\alpha}}\\!=\\!\\rho,$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\mathbb{E}_{S|\\hat{\\alpha},\\rho,\\sigma}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(S_{i},\\hat{B}))\\gtrsim(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})\\!\\cdot\\!(k\\!-\\!1).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Lemma C.2 and from the first part of Lemma D.3, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i\\in[n]}\\mathbb{E}_{\\boldsymbol{S}|\\hat{\\alpha}}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(\\boldsymbol{S},\\hat{\\boldsymbol{B}}))\\leq2n_{2}\\varepsilon\\sqrt{\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}}\\sqrt{\\mathbb{E}_{\\boldsymbol{S}|\\hat{\\alpha}}\\|M(\\boldsymbol{S},\\hat{\\boldsymbol{B}})\\!-\\!\\hat{\\alpha}\\|_{2}^{2}}}\\\\ &{}&{+2n_{2}\\delta T\\!+\\!n_{2}\\!\\int_{T}^{\\infty}\\!\\mathbb{P}\\Big(|A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(\\boldsymbol{S},\\boldsymbol{B}))|\\!>\\!t\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the tail probability term, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(|A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(\\mathcal{S},\\hat{B}))|>t\\bigg)=\\mathbb{P}\\bigg(|y_{i}-x_{i}^{\\top}\\hat{\\alpha}|\\displaystyle\\left|\\sum_{j=1}^{k-1}(M(\\mathcal{S},\\hat{B})_{j}-\\hat{\\alpha}_{j})\\cdot x_{j}^{\\hat{B}}\\right|>t\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{P}\\bigg(|y_{i}-x_{i}^{\\top}\\hat{\\alpha}|\\|\\hat{\\alpha}\\|\\|x^{\\hat{B}}\\|>t\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{P}\\bigg(|y_{i}-x_{i}^{\\top}\\hat{\\alpha}|\\sqrt{k}>t\\bigg)\\leq2\\mathrm{exp}\\bigg(\\displaystyle\\frac{-t^{2}}{2k(\\sigma^{2}+1-\\rho^{2})}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By choosing $T\\!=\\!\\sqrt{2(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})k\\!\\log(1/\\delta)},$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[n]}\\mathbb{E}_{{\\cal S}|\\hat{\\alpha}}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M({\\cal S},\\hat{B}))\\,\\lesssim\\,2n_{2}\\varepsilon\\sqrt{\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}}\\sqrt{\\mathbb{E}_{{\\cal S}|\\hat{\\alpha}}\\|M({\\cal S},\\hat{B})\\!-\\!\\hat{\\alpha}\\|_{2}^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\mathcal{O}\\Big(n_{2}\\delta\\sqrt{\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}})k\\mathrm{log}(1/\\delta)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now plugging in the second part of Lemma D.3 gives us ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})k\\leq\\mathbb{E}_{\\pi}\\underset{i\\in[n]}{\\sum}\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\left[\\mathbb{E}_{S|\\hat{\\alpha}}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(\\mathcal{S},\\hat{B}))\\right]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim2n_{2}\\varepsilon\\sqrt{\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}}\\sqrt{\\mathbb{E}_{\\pi}\\mathbb{E}_{S|\\hat{\\alpha}}\\|M(\\mathcal{S})\\!-\\!\\hat{\\alpha}\\|_{2}^{2}}\\!+\\!\\mathcal{O}\\Big(n_{2}\\delta\\sqrt{(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})k\\log(1/\\delta)}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\delta\\!<\\!n^{-(1+\\omega)}$ for $\\omega\\!>\\!0$ , for every $(\\varepsilon,\\delta)$ -differentially private $M$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\mathbb{E}_{S|\\hat{\\alpha}}\\|M(S)\\!-\\!\\hat{\\alpha}\\|_{2}^{2}\\gtrsim(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})\\frac{k^{2}}{n_{2}^{2}\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Adding the statistical lower bound of $\\scriptstyle{\\frac{k(\\sigma^{2}+1-\\rho^{2})}{n_{2}}}$ to the lower bound from (36), and from (34), we complete the proof of Lemma D.2. ", "page_idx": 22}, {"type": "text", "text": "D.2.2 Proof of Lemma D.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. Let us begin by looking at $\\mathbb{E}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(S_{i}^{\\prime},\\hat{B}))$ , where we use the fact that $y_{i}-(x_{i}^{\\hat{B}})^{\\top}\\hat{\\alpha}$ is independent of $x_{i}^{\\hat{B}}$ , and $\\mathbb{E}[y_{i}\\!-\\!(x_{i}^{\\hat{B}})]\\!=\\!0$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(S_{i}^{\\prime},\\hat{B}))}\\\\ &{\\quad=\\mathbb{E}\\left[(y_{i}-(x_{i}^{\\hat{B}})^{\\top}\\hat{\\alpha})\\underset{j=1}{\\overset{k-1}{\\sum}}(M(S_{i}^{'},\\hat{B})_{j}-\\hat{\\alpha}_{j})x_{i,j}^{\\hat{B}})\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[(y_{i}-(x_{i}^{\\hat{B}})^{\\top}\\hat{\\alpha})\\underset{j=1}{\\overset{k-1}{\\sum}}\\mathbb{E}[M(S_{i}^{'},\\hat{B})-\\hat{\\alpha}_{j}]\\mathbb{E}[x_{i,j}^{\\hat{B}}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad=0\\cdot\\underset{j=1}{\\overset{k-1}{\\sum}}\\mathbb{E}[M(S_{i}^{'},\\hat{B})-\\hat{\\alpha}_{j}]\\mathbb{E}[x_{i,j}^{\\hat{B}}]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This proves the first claim about the expected value of the attack when the datapoint is not a part of the training set, i.e., $\\mathbb{E}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),M(S_{i}^{\\prime}))\\!=\\!0.$ . Next, we look at the expected magnitude of the same random variable and upper bound it with a term that scales with the estimation error. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}\\big|A_{4}\\big((x_{i}^{\\beta},\\hat{y}_{i}),M(S_{i}^{\\star},\\hat{y}_{i})\\big)\\big|}\\\\ &{\\quad\\le\\sqrt{\\mathbb{E}\\bigg[\\bigg((\\eta-(x_{i}^{\\beta})^{\\top}\\hat{\\alpha})\\!+\\!(M(S_{i}^{\\star},\\hat{y}_{i}))\\!-\\!\\mathrm{\\Theta}(\\mathrm{Insen}^{\\mathrm{i}}\\!\\cdot\\!\\mathrm{inequality})}}\\\\ &{\\quad\\le\\sqrt{\\mathbb{E}\\bigg[\\bigg((y-(x^{\\beta})^{\\top}\\hat{\\alpha})\\!\\sum_{j=1}^{M}\\!\\!\\!(M(S_{i}^{\\beta}\\hat{B}_{j})\\!-\\!\\hat{\\alpha}_{j})\\!\\cdot\\!x_{j}^{\\beta}\\bigg)^{2}\\bigg]}}\\\\ &{\\quad\\le\\sqrt{\\mathbb{E}\\bigg[\\bigg(\\!\\left(M(S_{i}^{\\prime},\\hat{B})\\!-\\!\\hat{\\alpha},(y_{i}\\!-\\!(x_{i}^{\\beta})^{\\top}\\hat{\\alpha})\\!\\ x_{j}^{\\beta}\\!\\right)\\!\\bigg]}}\\\\ &{\\quad=\\sqrt{\\mathbb{E}\\big[((y\\!-\\!(x_{i}^{\\beta})^{\\top}\\hat{\\alpha})\\!)^{2}\\!\\cdot\\!(M(S_{i}^{\\star},\\hat{B})\\!-\\!\\hat{\\alpha})^{\\top}\\mathbb{E}[(x_{i}^{\\beta})\\!\\rangle^{\\top}](M(S_{i}^{\\star},\\hat{B})\\!-\\!\\hat{\\alpha})\\!\\bigg]}\\quad\\mathrm{(independence)}}\\\\ &{\\quad=\\sqrt{\\mathbb{E}\\bigg[((y\\!-\\!(x_{i}^{\\beta})^{\\top}\\hat{\\alpha})\\!)^{2}\\!\\cdot\\!(M(S_{i}^{\\star},\\hat{B})\\!-\\!\\hat{\\alpha})^{\\top}\\mathbb{I}_{K}(M(S_{i}^{\\star},\\hat{B})\\!-\\!\\hat{\\alpha})\\bigg]}\\quad\\mathrm{(ince~\\hat{B}^{\\top}\\hat{B}=\\cal I_{k})}}\\\\ &{\\quad=\\sqrt{\\sigma^{2}+(1\\!-\\!\\rho^{2})\\!\\cdot\\!\\sqrt{\\mathbb{E}\\|M(S_{i}^{\\star},\\hat{B})\\!-\\!\\hat{\\alpha}\\|})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality uses the following derivation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[((y_{i}-(x_{i}^{\\hat{B}})^{\\top}\\hat{\\alpha}))^{2}\\Big]\\!=\\!\\hat{\\sigma}^{2}\\!=\\!\\sigma^{2}\\!+\\!\\|(I_{d}\\!-\\!\\hat{B}\\hat{B}^{\\top})B\\alpha\\|_{2}^{2}}\\\\ {=\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2}\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof for the first part of the Lemma. For the second part we will begin by constructing a convenient prior for $\\hat{\\alpha}$ . ", "page_idx": 23}, {"type": "text", "text": "Note that $\\hat{\\alpha}$ can take any value in the column span of $\\hat{B}$ if the adversary has complete control over $B$ and $\\alpha$ . Thus, defining a prior over $\\hat{\\alpha}$ would involve defining a prior over the column span of $\\hat{B}$ such that $\\|{\\hat{\\alpha}}\\|_{2}\\!=\\!\\rho$ . We define a sample from the prior $\\pi$ as a multi step procedure: ", "page_idx": 23}, {"type": "text", "text": "1. For all $i\\in[k\\!-\\!1]$ , sample $\\omega_{i}$ from the truncated Gaussian, with mean 0, variance ${\\rho^{2}}/(k\\!-\\!1)$ , and truncation at points $^{-\\rho/\\sqrt{k-1}}$ and $\\scriptstyle\\rho/{\\sqrt{k-1}}$ . ", "page_idx": 23}, {"type": "text", "text": "2. Set $\\begin{array}{r}{\\omega_{k}\\!=\\!\\pm\\sqrt{1\\!-\\!\\sum_{i\\in[k-1]}\\!\\omega_{i}^{2}}}\\end{array}$ with equal probability for either sign. ", "page_idx": 23}, {"type": "text", "text": "3. Now, set $\\begin{array}{r}{B\\alpha\\!=\\!\\sum_{i\\in[k]}\\!\\omega_{i}\\!\\cdot\\!v_{i}}\\end{array}$ , where $v_{i}$ is the $i^{\\mathrm{th}}$ column of $\\hat{B}$ . Consequently, $\\hat{\\alpha}\\!=\\!\\hat{B}^{\\top}B\\alpha\\!=$ $[\\omega_{1},\\omega_{2},...,\\omega_{k}]^{\\top}$ . ", "page_idx": 24}, {"type": "text", "text": "For the second part of the claim we need to lower bound $\\begin{array}{r}{\\sum_{i\\in[n]}\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\mathbb{E}\\left[A_{\\hat{\\alpha}}\\big((y_{i},\\!x_{i}^{\\hat{B}}),M(S)\\big)\\big|\\hat{\\alpha}\\right]}\\end{array}$ which we can decompose over co-ordinates in the following way: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[n]}\\mathbb{E}_{\\alpha\\sim\\pi}\\mathbb{E}\\Big[A_{\\delta}\\big((y_{i},x_{i}^{\\hat{B}}),M({\\cal S},\\hat{B})\\big)\\big|\\hat{\\alpha}\\Big]}\\\\ &{=\\displaystyle\\sum_{j\\in[k-1]}\\mathbb{E}_{\\delta\\sim\\pi}\\mathbb{E}\\Bigg[M({\\cal S},\\hat{B})_{j}\\Bigg(\\sum_{i\\in[n]}(y_{i}-\\hat{\\alpha}^{\\top}x_{i}^{\\hat{B}})x_{i,j}^{\\hat{B}}\\Bigg)\\big|\\hat{\\alpha}\\Bigg]}\\\\ &{=\\displaystyle\\sum_{j\\in[k-1]}\\mathbb{E}_{\\delta\\sim\\pi}\\mathbb{E}\\bigg[M({\\cal S},\\hat{B})_{j}\\frac{\\partial}{\\partial\\hat{\\alpha}_{j}}\\big[\\log p(\\delta|B,\\hat{\\alpha},\\sigma)\\big|(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})\\big|\\hat{\\alpha}\\Big]}\\\\ &{=(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})\\cdot\\displaystyle\\sum_{j\\in[k-1]}\\mathbb{E}_{\\delta\\sim\\pi}\\mathbb{E}\\bigg[M({\\cal S},\\hat{B})_{j}\\frac{\\partial}{\\partial\\hat{\\alpha}_{j}}\\big[\\log p(\\delta|B,\\hat{\\alpha},\\sigma)\\big|\\big|\\hat{\\alpha}\\bigg]}\\\\ &{=(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})\\cdot\\displaystyle\\sum_{j\\in[k-1]}\\mathbb{E}_{\\delta\\sim\\pi}\\frac{\\partial}{\\partial\\hat{\\alpha}_{j}}\\mathbb{E}\\big[M({\\cal S},\\hat{B})_{j}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the final equation uses the log-derivative trick. ", "page_idx": 24}, {"type": "text", "text": "Next, we focus on E\u03b1\u02c6\u223c\u03c0 \u2202\u2202\u03b1\u02c6j E[M(S, B\u02c6)j] for any j \u2208[k \u22121]. Recall, that for any dimension $j~\\in~[k~-~1]$ , the prior $\\pi$ draws a sample from the Gaussian ${\\mathcal{N}}(0,\\,\\rho^{2}/k\\!-\\!1)$ , truncated at $-\\rho/\\sqrt{k-1},\\rho/\\sqrt{k-1}$ independently. We will now apply Stein\u2019s Lemma (see Lemma C.3) for the term $\\mathbb{E}_{{\\hat{\\alpha}}\\sim\\pi}\\left[{\\frac{\\partial}{\\partial{\\hat{\\alpha}}_{j}}}\\mathbb{E}[M(S,{\\hat{B}})_{j}]\\right]$ . ", "page_idx": 24}, {"type": "text", "text": "Denoting $\\hat{\\alpha}_{-j}$ as the set $\\{\\hat{\\alpha}_{j}\\}_{j=1}^{k}\\backslash\\{\\hat{\\alpha}_{j}\\}$ , and $\\pi_{j}$ as the marginal prior over $j^{\\mathrm{th}}$ dimension of $\\hat{\\alpha}_{j}$ , we can lower bound $\\mathbb{E}_{{\\hat{\\alpha}}\\sim\\pi}\\left[{\\frac{\\partial}{\\partial{\\hat{\\alpha}}_{j}}}\\mathbb{E}[M(S)_{j}]\\right]$ in the following way: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\bigg[\\frac{\\partial}{\\partial\\hat{\\alpha}_{j}}\\mathbb{E}[M(\\bar{S},\\hat{B})_{j}]\\bigg]=\\mathbb{E}_{\\hat{\\alpha}_{-j}}\\left[\\mathbb{E}_{\\hat{\\alpha}_{j}}\\frac{\\partial}{\\partial\\hat{\\alpha}_{j}}\\mathbb{E}_{S}[M(\\bar{S},\\hat{B})_{j}]\\mid\\hat{\\alpha}_{-j}\\right]}&{}\\\\ {=\\mathbb{E}_{\\hat{\\alpha}}\\left[-\\frac{\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})}{\\pi_{j}(\\hat{\\alpha}_{j})}\\mathbb{E}_{S}[M(\\bar{S},\\hat{B})_{j}]\\right]}&{}\\\\ {=\\mathbb{E}_{\\hat{\\alpha}}\\bigg[-\\frac{\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})}{\\pi_{j}(\\hat{\\alpha}_{j})}\\mathbb{E}_{S}[M(\\bar{S},\\hat{B})_{j}-\\hat{\\alpha}_{j}+\\hat{\\alpha}_{j}]\\bigg]}&{}\\\\ {\\geq\\mathbb{E}_{\\hat{\\alpha}}\\bigg[-\\hat{\\alpha}_{j}\\frac{\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})}{\\pi_{j}(\\hat{\\alpha}_{j})}\\bigg]-\\mathbb{E}_{\\hat{\\alpha}}\\bigg[|\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})/\\pi_{j}(\\hat{\\alpha}_{j})|\\cdot\\mathbb{E}_{S}[|M(S,\\hat{B})_{j}-\\hat{\\alpha}_{j}|]\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we use the density of the truncated Normal: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi_{j}(\\hat{\\alpha}_{j})\\!=\\!\\frac{\\exp\\!{\\left(-\\frac{(k-1)}{2\\rho^{2}}\\cdot\\hat{\\alpha}_{j}^{2}\\right)}}{\\sqrt{2\\pi}\\rho/\\sqrt{k-1}\\cdot(\\Phi(1)\\!-\\!\\Phi(-1))},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where \u03a6(\u00b7) is the CDF function for a standard Normal distribution. Thus,\u03c0\u03c0jj((\u03b1\u03b1\u02c6\u02c6jj)) $\\begin{array}{r}{\\frac{\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})}{\\pi_{j}(\\hat{\\alpha}_{j})}\\!=\\!-\\frac{(k-1)}{\\rho^{2}}\\hat{\\alpha}_{j}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Substituting the above and applying Cauchy-Schwarz followed by Jensen\u2019s inequality we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{k-1}\\mathbb{E}_{\\hat{\\alpha}}\\left[\\vert\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})/\\pi_{j}(\\hat{\\alpha}_{j})\\vert\\cdot\\mathbb{E}_{\\delta}[\\vert M(S,\\hat{B})_{j}-\\hat{\\alpha}_{j}\\vert]\\right]}&{}\\\\ {\\displaystyle=(k-1)/\\rho^{2}\\cdot\\mathbb{E}_{\\hat{\\alpha}}\\left[\\,\\sum_{j=1}^{k-1}\\vert\\hat{\\alpha}_{j}\\vert\\cdot\\mathbb{E}_{S}[\\vert M(S,\\hat{B})_{j}-\\hat{\\alpha}_{j}\\vert]\\right]}&{}\\\\ {\\displaystyle\\leq(k-1)/\\rho^{2}\\cdot\\sqrt{\\mathbb{E}_{\\hat{\\alpha}}\\left[\\,\\sum_{j\\in[k-1]}\\hat{\\alpha}_{j}^{2}\\right]\\cdot\\mathbb{E}_{\\hat{\\alpha}}\\left[\\,\\sum_{j\\in[k-1]}\\left(\\mathbb{E}_{S}\\left[M(S,\\hat{B})_{j}-\\hat{\\alpha}_{j}\\right]\\right)^{2}\\right]}}&{}\\\\ {\\displaystyle\\leq(k-1)/\\rho^{2}\\cdot\\sqrt{\\mathbb{E}_{\\hat{\\alpha}}\\|\\hat{\\alpha}\\|^{2}}\\cdot\\left\\langle\\mathbb{E}_{\\hat{\\alpha}}\\mathbb{E}_{S}\\|M(S,\\hat{B})_{j}-\\hat{\\alpha}\\|^{2}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From directly applying the density of the truncated Normal distribution we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{k-1}\\!\\mathbb{E}_{\\hat{\\alpha}}\\left[-\\hat{\\alpha}_{j}\\frac{\\pi_{j}^{\\prime}(\\hat{\\alpha}_{j})}{\\pi_{j}(\\hat{\\alpha}_{j})}\\right]=(k\\!-\\!1)\\!/\\rho^{2}\\!\\cdot\\!\\mathbb{E}_{\\hat{\\alpha}}\\sum_{j\\in[k-1]}\\!\\hat{\\alpha}_{j}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging (40), (39) into (38), and using (37) we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[n]}\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\mathbb{E}\\Big[A_{\\hat{\\alpha}}((y_{i},x_{i}^{\\hat{B}}),M(\\mathcal{S}))\\big|\\hat{\\alpha}\\Big]}\\\\ &{\\displaystyle\\geq\\frac{(\\sigma^{2}+1-\\rho^{2})}{\\rho^{2}/(k-1)}\\cdot\\left(\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\underset{j=1}{\\overset{k-1}{\\sum}}\\hat{\\alpha}_{j}^{2}-\\sqrt{\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\mathbb{E}_{\\mathcal{S}|\\hat{\\alpha}}\\|M(\\mathcal{S},\\hat{B})-\\hat{\\alpha}\\|_{2}^{2}}\\sqrt{\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\|\\hat{\\alpha}\\|_{2}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\mathbb{E}_{{\\hat{\\alpha}}\\sim\\pi}{\\sum}_{j=1}^{k-1}{\\hat{\\alpha}}_{j}^{2}\\,{=}\\,\\rho^{2}$ by construction of the prior $\\pi$ and $\\mathbb{E}_{\\hat{\\alpha}\\sim\\pi}\\mathbb{E}_{S|\\hat{\\alpha}}\\|M(S,\\hat{B})-\\hat{\\alpha}\\|_{2}^{2}\\!=\\!o(1)$ by assumption. Thus, $\\begin{array}{r}{\\sum_{i\\in[n]}\\!\\mathbb{E}_{\\pi}\\mathbb{E}_{S|B,\\hat{B},\\alpha,\\sigma}A_{\\hat{\\alpha}}((x_{i}^{\\hat{B}},y_{i}),\\!M(S_{i},\\hat{B}))\\gtrsim(\\sigma^{2}\\!+\\!1\\!-\\!\\rho^{2})\\!\\cdot\\!(k\\!-\\!1)}\\end{array}$ , which completes the proof of the second claim in Lemma D.3. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper claims to provide empirical evidence and a theoretical model for the beneftis of public representations in improving private training in out-of-distribution settings. These claims are clearly outlined in the abstract and introduction and reflect the results in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: See Section 6 for a discussion of limitations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The assumptions for the theorems are provided in Section 5.1, and the proofs are provided in the Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experimental setup is provided in Section 4.2. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not include our experimental code with the submission at this time but plan to release it if the submission is published. All the packages and models we use are open-source. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Details of the training and test setup are provided in Section 4.2. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: Because our experiments involved full training or fine-tuning of models, it was too computationally expensive to generate error bars over multiple runs. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details are provided in Section 4.2. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The pretrained models and datasets we use are currently publicly available and no human subjects have been involved in our experiments. Nevertheless, we acknowledge the blurry line between private and public data on the open web, and the potential for harmful content to reside in publicly available datasets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work contributes to the theory of differentially private learning, which is essential to preserving user privacy under increasing amounts of data collection. Our work has the potential benefits of making private learning more practical by incorporating public data. However, we echo cautions pointed out in recent work [14] that care must be taken to ensure that publicly available data does not contain sensitive information for real deployments, and applying our techniques without such measures could inadvertently leak sensitive information. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The method we evaluate is itself a form of a safeguard (differentially private training). Above that, the methods do not suggest obvious new safeguards necessary. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We use the OpenCLIP [52] library for models and the Opacus [54] library for private training. We provide citations to these as well as the datasets we use in the paper. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have not (yet) released any new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not perform any human subjects experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not perform any human subjects experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]