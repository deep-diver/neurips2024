[{"type": "text", "text": "Discretely Beyond $1/e$ : Guided Combinatorial Algorithms for Submodular Maximization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Yixin Chen, Ankur Nath, Chunli Peng, Alan Kuhnle Department of Computer Science & Engineering Texas A&M University Colloge Station, TX {chen777, anath, chunli.peng, kuhnle}@tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than $1/e$ require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function. For combinatorial algorithms, the best known approximation ratios for both size and matroid constraint are obtained by a simple randomized greedy algorithm of Buchbinder et al. [9]: $1/e\\approx0.367$ for size constraint and 0.281 for the matroid constraint in ${\\mathcal{O}}(k n)$ queries, where $k$ is the rank of the matroid. In this work, we develop the first combinatorial algorithms to break the $1/e$ barrier: we obtain approximation ratio of 0.385 in ${\\mathcal{O}}(k n)$ queries to the submodular set function for size constraint, and 0.305 for a general matroid constraint. These are achieved by guiding the randomized greedy algorithm with a fast local search algorithm. Further, we develop deterministic versions of these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio 0.377. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A nonnegative set function $f:\\,2^{\\mathcal{U}}\\,\\to\\,\\mathbb{R}^{+}$ is submodular iff for all $S\\,\\subseteq\\,T\\,\\subseteq\\,\\mathcal{U}$ , $x\\,\\in\\,\\mathcal{U}\\setminus T$ , $f\\left(S\\cup\\left\\{{\\bar{x}}\\right\\}\\right)-f\\left(S\\right)\\geq f\\left(T\\cup\\left\\{x\\right\\}\\right)-f\\left(T\\right)$ ; and $f$ is monotone iff $f\\left(S\\right)\\leq f\\left(T\\right)$ for all $S\\subseteq T\\subseteq\\mathcal{U}$ . Submodular optimization plays an important role in data science and machine learning [3], particularly in tasks that involve selecting a representative subset of data or features. Its diminishing returns property makes it ideal for scenarios where the incremental benefit of adding an element to a set decreases as the set grows. Applications include sensor placement for environmental monitoring [19, 27], where the goal is to maximize coverage with limited sensors, feature selection [22, 18, 2] in machine learning to improve model performance and reduce overftiting, and data summarization [23, 31] for creating concise and informative summaries of large datasets. Further, many of these applications employ submodular objective functions that are non-monotone, e.g. Mirzasoleiman et al. [23], Tschiatschek et al. [31]. Formally, we study the optimization problem (SM): max $f\\left(S\\right),s.t.\\;S\\in$ $\\mathcal{T}$ , where $f$ is nonnegative, submodular and not necessarily monotone; and $\\mathcal{T}\\subseteq2^{\\mathcal{U}}$ is a family of feasible subsets. Specifically, we consider two cases: when $\\mathcal{T}$ is a size constraint (all sets of size at most $k$ ); and more generally, when $\\mathcal{T}$ is an arbitrary matroid of rank $k$ . ", "page_idx": 0}, {"type": "text", "text": "In this field, algorithms typically assume access to a value oracle for the submodular function $f$ , and the efficiency of an algorithm is measured by the number of queries to the oracle, because evaluation of the submodular function is typically expensive and dominates other parts of the computation. In the general, not necessarily monotone case, the approximability of constrained submodular optimization in the value oracle model is not well understood. For several years, $1/e\\approx0.367$ was conjectured to be the best ratio, as this ratio is obtained by the measured continuous greedy [15] algorithm that also gets the $1-1/e$ ratio in the monotone setting, which is known to be optimal [24]. However, in several landmark works, the $1/e$ barrier was broken: first to 0.371 by Buchbinder et al. [9] (for size constraint only) and subsequently to 0.372 by Ene and Nguyen [13], then 0.385 by Buchbinder and Feldman [6]. Very recently, the best known approximation factor has been improved to 0.401 [7]. On the other hand, the best hardness result is 0.478 [16, 28]. ", "page_idx": 0}, {"type": "table", "img_path": "cgiOX8lfwG/tmp/a9b4d9cd8ac9a7b699965a111c16c0f9b8fdab7925ad6aaf1b15f1ad4373d80e.jpg", "table_caption": ["Table 1: The prior state-of-the-art and the ratios achieved in this paper, in each category: deterministic (det), randomized combinatorial (cmb), and continuous (cts). "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "All of the algorithms improving on the $1/e$ ratio use oracle queries to the multilinear extension of a submodular function and its gradient. The multilinear extension relaxes the submodular set function to allow choosing an element with probability in [0, 1]. Although this is a powerful technique, the multilinear extension must be approximated by polynomially many random samples of the original set function oracle. Unfortunately, this leads to a high query complexity for these algorithms, which we term continuous algorithms; typically, the query complexity to the original submodular function is left uncomputed. As an illustration, we compute in Appendix B that the continuous algorithm of Buchbinder and Feldman [6] achieves ratio of 0.385 with query complexity of $O\\left(n^{11}\\,\\overline{{\\log(n)}}\\right)$ to the set function oracle. Consequently, these algorithms are of mostly theoretical interest \u2013 the time cost of running on tiny instances (say, $n<100)$ ) is already prohibitive, as demonstrated by Chen and Kuhnle [12] where a continuous algorithm required more than $10^{9}$ queries to the set function on an instance with $n=87,k=10$ . ", "page_idx": 1}, {"type": "text", "text": "For size and matroid constraints, the current state-of-the-art approximation ratio for a combinatorial algorithm (i.e. not continuous) is obtained by the RANDOMGREEDY algorithm (Algorithm 1) of Buchbinder et al. [9]. RANDOMGREEDY achieves ratio $1/e\\,\\approx\\,0.367$ for size constraint, and $0.283-\\varepsilon$ ratio for matroid constraint; its query complexity is $O\\left(k n\\right)$ . Thus, there is no known combinatorial algorithm that breaks the $1/e$ barrier; and therefore, no such algorithm is available to be used in practice on any of the applications of SM described above. ", "page_idx": 1}, {"type": "table", "img_path": "cgiOX8lfwG/tmp/81cefe1c9321143b347021f5b250b5d6b08d4a9f531896c1485a7842d95ac5c8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Moreover, closing the gap between ratios achieved by deterministic and randomized algorithms for SM has been the focus of a number of recent works [5, 17, 11, 8]. In addition to theoretical interest, deterministic algorithms are desirable in practice, as a ratio that holds in expectation may fail on any given run with constant probability. Buchbinder and Feldman [5] introduced a linear programming method to derandomize the RANDOMGREEDY algorithm (at the expense of additional time complexity), meaning that the best known ratios for deterministic algorithms are again given by RANDOMGREEDY. There is no known method to derandomize continuous algorithms, as the only known way to approximate the multilinear extension of a general submodular set function relies on random sampling methods. \u00d6zcan et al. [26], however, introduced a deterministic estimation via Taylor series approximation, but this approach is limited to a specific class of submodular functions that can be expressed as weighted compositions of analytic and multilinear functions. Therefore, there is no known deterministic algorithm that breaks the $1/e$ barrier. The best known ratio in each category of continuous, combinatorial, and deterministic algorithms is summarized in Table 1. In this work, we consider the following questions: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Can combinatorial algorithms, and separately, deterministic algorithms, obtain approximation ratios for SM beyond $1/e?$ If so, are the resulting algorithms practical and do they yield empirical improvements in objective value over existing algorithms? ", "page_idx": 2}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we improve the best known ratio for a combinatorial algorithm for size-constrained SM to $0.385-\\varepsilon\\approx1/\\bar{e^{*}}+0.018$ . This is achieved by using the result of a novel local search algorithm to guide the RANDOMGREEDY algorithm. Overall, we obtain query complexity of ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ , which is at worst quadratic in the size of the ground set, since $k\\leq n$ . Thus, this algorithm is practical and can run on moderate instance sizes; the first algorithm with ratio beyond $1/e$ for which this is possible. Further, we extend this algorithm to the matroid constraint, where it improves the best known ratio of a combinatorial algorithm for a general matroid constraint from 0.283 of RANDOMGREEDY to $0.305-\\varepsilon$ . ", "page_idx": 2}, {"type": "text", "text": "Secondly, we obtain these same approximation ratios with deterministic algorithms. The ideas are similar to the randomized case, except we leverage a recently formulated algorithm INTERPOLATEDGREEDY [11] as a replacement for guided RANDOMGREEDY. The analysis of INTERPOLATEDGREEDY has similar recurrences (up to low order terms) and the algorithm can be guided in a similar fashion to RANDOMGREEDY, but is amenable to derandomization. The derandomization only adds a constant factor, albeit one that is exponential in $(1/\\varepsilon)$ . ", "page_idx": 2}, {"type": "text", "text": "Next, we seek to lower the query complexity further, while still improving the $1/e$ ratio. As INTERPOLATEDGREEDY can be sped up to $O_{\\varepsilon}(n\\log k)$ , the bottleneck becomes the local search procedure. Thus, we develop a faster way to produce the guiding set $Z$ by exploiting a run of (unguided) INTERPOLATEDGREEDY and demonstrating that a decent guiding set is produced if the algorithm exhibits nearly worst-case behavior. With this method, we achieve a deterministic algorithm with ratio $0.377\\approx1/e+0.01$ in $O_{\\varepsilon}(n\\log k)$ query complexity, which is nearly linear in the size of the ground set (since $k=O(n)$ ). ", "page_idx": 2}, {"type": "text", "text": "Finally, we demonstrate the practical utility of our combinatorial 0.385-approximation algorithm by implementing it and evaluating in the context of two applications of size-constrained SM on moderate instance sizes (up to $n=10^{4}$ ). We evaluate it with parameters set to enforce a ratio $>1/e$ . It outperforms both the standard greedy algorithm and RANDOMGREEDY by a significant margin in terms of objective value; moreover, it uses about twice the queries of RANDOMGREEDY and is orders of magnitude faster than existing local search algorithms. ", "page_idx": 2}, {"type": "text", "text": "1.2 Additional Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Derandomization. Buchbinder and Feldman [5] introduced a linear programming (LP) method to derandomize the RANDOMGREEDY algorithm, thereby obtaining ratio $1/e$ with a deterministic algorithm. Further, Sun et al. [30] were able to apply this technique to RANDOMGREEDY for matroids. A disadvantage of this approach is an increase in the query complexity over the original randomized algorithm. Moreover, we attempted to use this method to derandomize our guided RANDOMGREEDY algorithm, but were unsuccessful. Instead, we obtained our deterministic algorithms by guiding the INTERPOLATEDGREEDY algorithm instead of RANDOMGREEDY; this algorithm is easier to derandomize, notably without increasing the asymptotic query complexity. ", "page_idx": 2}, {"type": "text", "text": "Relationship to Buchbinder and Feldman [6]. The continuous, 0.385-approximation algorithm of Buchbinder and Feldman [6] guides the measured continuous greedy algorithm using the output of a continuous local search algorithm, in analogous fashion to how we guide RANDOMGREEDY with the output of a combinatorial local search. However, the analysis of RANDOMGREEDY is much different from that of measured continuous greedy, although the resulting approximation factor is the same. ", "page_idx": 2}, {"type": "text", "text": "Specifically, Buchbinder and Feldman [6] obtain their ratio by optimizing a linear program mixing the continous local search and guided measured continous greedy; in contrast, we use submodularity and the output of our fast local search to formulate new recurrences for guided RANDOMGREEDY, which we then solve. ", "page_idx": 3}, {"type": "text", "text": "Local search algorithms. Local search is a technique widely used in combinatorial optimization. Nemhauser et al. [25] introduced a local search algorithm for monotone functions under size constraint; they showed a ratio of $1/2$ , but noted that their algorithm may run in exponential time. Subsequently, local search has been found to be useful, especially for non-monotone functions. Feige et al. [14] proposed a $1/3$ approximation algorithm with ${\\mathcal{O}}\\left({\\bar{n}}^{4}/\\varepsilon\\right)$ queries for the unconstrained submodular maximization problem utilizing local search. Meanwhile, Lee et al. [21] proposed a local search algorithm for general SM with matroid constraint, attaining $1/4-\\varepsilon$ approximation ratio with a query complexity of ${\\mathcal{O}}\\left(k^{5}\\log(k)n/\\varepsilon\\right)$ . We propose our own FASTLS in Section 2.1, yielding a ratio of $1/2$ for monotone cases and $1/4$ for non-monotone cases through repeated applications of FASTLS, while running in ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ queries. ", "page_idx": 3}, {"type": "text", "text": "Fast approximation algorithms. Buchbinder et al. [10] developed a faster version of RANDOMGREEDY for size constraint that reduces the query complexity to ${\\mathcal{O}}_{\\varepsilon}\\left(n\\right)$ with ratio of $1/e\\mathrm{~-~}\\varepsilon$ . Chen and Kuhnle [11] proposed LINEARCARD, the first deterministic, linear-time algorithm with an $1/11.657$ -approximation ratio for size constraints. Also, Han et al. [17] introduced TWINGREEDY, a 0.25-approximation algorithm with a query complexity of $\\mathcal{O}\\left(k n\\right)$ for matroid constraints. These algorithms are fast enough to be used as building blocks for our FASTLS, which requires as an input a constant-factor approximation in $\\mathcal{O}\\left(k n\\right)$ queries. ", "page_idx": 3}, {"type": "text", "text": "Relationship to Tukan et al. [32]. During the submission of this paper, we noticed an independent and parallel work by Tukan et al. [32], which proposed a different 0.385-approximation algorithm. Both papers start from a similar idea-guiding the random greedy algorithm with a fast algorithm to find a local optimum. However, Tukan et al. [32] only considered size constraint and focused on algorithm speedup. They introduced a randomized local search algorithm and used its output to guide the stochastic greedy of Buchbinder et al. [9], achieving a query complexity of $O_{\\varepsilon}\\left(n^{\\star}+k^{2}\\right)$ . On the other hand, we 1) address a more general constraint-matroid constraint; 2) for size constraint, present an asymptotically faster algorithm that uses a novel way of guiding with partial solutions from random greedy itself, which are not local optima, thereby achieving ratio $0.377\\!-\\!\\varepsilon$ with ${\\mathcal{O}}_{\\varepsilon}\\left(n\\log(k)\\right)$ queries; and 3) derandomize these algorithms. ", "page_idx": 3}, {"type": "text", "text": "1.3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation. In this section, we establish the notations employed throughout the paper. We denote the marginal gain of adding $A$ to $B$ by $\\Delta(A|B)=f\\left(A\\cup B\\right)-f\\left(B\\right)$ . For every set $S\\subseteq\\mathcal{U}$ and an element $x\\in\\mathcal{U}$ , we denote $S\\cup\\{x\\}$ by $S+x$ , and $S\\backslash\\{x\\}$ by $S-x$ . Given a constraint and its related feasible sets $\\mathcal{T}$ , let $O\\in$ arg $\\operatorname*{max}_{S\\in{\\mathcal{Z}}}f\\left(S\\right)$ ; that is, $O$ is an optimal solution. To simplify the pseudocode and the analysis, we add $k$ dummy elements into the ground set, where the dummy element serves as a null element with zero marginal gain when added to any set. The symbol $e_{0}$ is utilized to represent a dummy element. ", "page_idx": 3}, {"type": "text", "text": "Submodularity. A set function $f:2^{\\mathcal{U}}\\to\\mathbb{R}^{+}$ is submodular, if $\\Delta(x|S)\\geq\\Delta(x|T)$ for all $S\\subseteq T\\subseteq$ $\\boldsymbol{\\mathcal{U}}$ and $x\\in\\mathcal{U}\\setminus T$ , or equivalently, for all $A,B\\subseteq{\\mathcal{U}}$ , it holds that $f\\left(A\\right)+f\\left(B\\right)\\geq f\\left(A\\cup B\\right)+$ $f\\left(A\\cap B\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "Constraints. In this paper, our focus lies on two constraints: size constraint and matroid constraint. For size constraint, we define the feasible subsets as ${\\mathcal{T}}(k)=\\{S\\subseteq\\mathcal{U}:|S|\\leq k\\}$ , where $k$ is an input parameter. The matroid constraint is defined in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Organization. Our randomized algorithms are described in Section 2, with two subroutines, FASTLS and GUIDEDRG, in Section 2.1 and 2.2, respectively. Due to space constraints, we provide only a sketch of the analysis for size constraint in the main text. The full pseudocodes and formal proofs for both size and matroid constraint are provided in Appendix C. Then, we briefly sketch the deterministic approximation algorithms in Section 2.3, with full details provided in Appendix D. Next, we introduce the nearly linear-time deterministic algorithm in Section 3, with omitted analysis provided in Appendix E. Our empirical evaluation is summarized in Section 4. In Section 5, we discuss limitations and future directions. ", "page_idx": 3}, {"type": "table", "img_path": "cgiOX8lfwG/tmp/a080dbe2766861bf75b53c9b3d3b52ff85a8f4248dad1d47317695d756693280.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "2 A Randomized $\\left(0.385-\\varepsilon\\right)$ -approximation in ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ Queries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our randomized approximation algorithm (Alg. 2) for both size and matroid constraints. This algorithm improves the state-of-the-art, combinatorial approximation ratio to $0.385-\\varepsilon\\approx1/e+0.0{\\bar{1}}8$ for size constraint, and to $0.305-\\varepsilon\\approx0.283+0.022$ for matroid constraint. ", "page_idx": 4}, {"type": "text", "text": "Algorithm Overview. In overview, Alg. 2 consists of two components, which are detailed below. The first component is a local search algorithm, FASTLS (Alg. 4 in Appendix C.1), described in detail in Section 2.1. In brief, the local search algorithm takes an accuracy parameter $\\varepsilon>0$ and a constant-factor, approximate solution $Z_{0}$ as input, which may be produced by any approximation algorithm with better than $O\\left(k n\\right)$ query complexity. The second component is a random greedy algorithm, GUIDEDRG (Alg. 6 in Appendix C.2), that is guided by the output $Z$ of the local search, described in detail in Section 2.2. Also, GUIDEDRG takes a parameter $t\\,\\in\\,[0,1]$ , which is the switching time (as fraction of the budget or rank $k$ ) from guided to unguided behavior. The candidate with best $f$ value from the two subroutines is returned. ", "page_idx": 4}, {"type": "text", "text": "If $f(Z)<\\alpha\\mathrm{OPT}$ (otherwise, there is nothing to show), then the local search set satisfies our definition of $((1+\\varepsilon)\\alpha,\\alpha)$ -guidance set (Def. 2.1 below). Under this guidance, we show that GUIDEDRG produces a superior solution compared to its unguided counterpart. The two components, FASTLS and GUIDEDRG are described in Sections 2.1 and 2.2, respectively. The following theorem is proven in Section 2.2 (size constraint) and Appendix C.2.2 (matroid constraint). ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1. Let $(f,\\mathcal{T})$ be an instance of SM. Let $\\varepsilon>0$ , and $k\\geq1/\\varepsilon$ . Algorithm 2 achieves an expected $(0.385-\\varepsilon)$ -approximation ratio for size constraint with $t=0.372$ , and an expected $(0.305-\\varepsilon)$ -approximation ratio for matroid constraint with $t=0.559$ . The query complexity of the algorithm is ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "2.1 The Fast Local Search Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce FASTLS (Alg. 4), which is the same for size or matroid constraints. There are several innovations in FASTLS that result in ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ time complexity, where $k$ is the maximum size of a feasible set, and $\\varepsilon>0$ is an input accuracy parameter. ", "page_idx": 4}, {"type": "text", "text": "In overview, the algorithm maintains a feasible set $Z$ ; initially, $Z=Z_{0}$ , where $Z_{0}$ is an input set which is a constant approximation to OPT. The value of $Z$ is iteratively improved via swapping, which is done in the following way. For each element $a\\in\\mathcal{U}$ , we compute $\\Delta(a|Z\\setminus a)$ ; if $a\\not\\in Z$ , this is just the gain of $a$ to $Z$ ; this requires ${\\mathcal{O}}\\left(n\\right)$ queries. Then, if $a\\in Z$ and $e\\not\\in Z$ such that $Z\\setminus a+e$ is feasible, and $\\begin{array}{r}{\\Delta(e|Z)-\\Delta(a|\\bar{Z}\\setminus a)\\geq\\frac{\\varepsilon}{k}f\\bar{(}Z)}\\end{array}$ , then $a$ is swapped in favor of $e$ . If no such swap exists, the algorithm terminates. ", "page_idx": 4}, {"type": "text", "text": "One can show that, for each swap, the value of $Z$ increases by at least a multiplicative $\\left(1+\\varepsilon/k\\right)$ factor. Since $f(Z)$ is initialized to a constant fraction of OPT, it follows that we make at most ${\\mathcal{O}}(k/\\varepsilon)$ swaps. Since each swap requires ${\\mathcal{O}}\\left(n\\right)$ queries, this yields the query complexity of the algorithm: ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ . In addition, if $f$ is monotone, FASTLS gets ratio of nearly $1/2$ for FASTLS. A second repetition of FASTLS yields a ratio of $1/4$ in the case of general (non-monotone) $f$ , as shown in Appendix C.1.2. Thus, FASTLS may be of independent interest, as local search obtains good objective values empirically and is commonly used in applications. ", "page_idx": 4}, {"type": "text", "text": "For our purposes, we want to use the output of FASTLS to guide RANDOMGREEDY. Since we will also use another algorithm for a similar purpose in Section 3, we abstract the properties needed for such a guidance set. Intuitively, a set $Z$ is a good guidance set if it has a low $f$ -value and also ensures that the value of its intersection and union with an optimal solution are poor. ", "page_idx": 4}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/7b9d12d4ede0039af215cdd8d60052d7d4c28b5c8aee3098468239d49afdaf81.jpg", "img_caption": ["Figure 1: (a): The evolution of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ and $\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]$ in the worst case of the analysis of RANDOMGREEDY, as the partial solution size increases to $k$ . (b): Illustration of how the degradation of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ changes as we introduce an $(0.385+\\varepsilon,0.385)$ -guidance set. (c): The updated degradation with a switch point $t k$ , where the algorithm starts with guidance and then switches to running without guidance. The dashed curved lines depict the unguided values from (a). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Definition 2.1. A set $Z$ is a $(\\alpha,\\beta)$ -guidance set, if given constants $\\alpha,\\beta\\in(0,0.5)$ and optimum solution $O$ , it holds that: 1) $f\\left(Z\\right)<\\alpha f\\left(O\\right);2$ ) $f\\left(\\bar{O}\\cap Z\\right)\\leq\\alpha f\\left(O\\right);$ 3) $f\\left(O\\cup Z\\right)\\leq\\beta f\\left(O\\right)$ , or alternatively, $3^{\\prime}$ ) $f\\left(O\\cap Z\\right)+f\\left(O\\cup Z\\right)\\leq(\\alpha+\\beta)f\\left(O\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.2 (proved in Appendix C.1.1) implies that for the FASTLS output $Z$ , if $f(Z)<\\alpha\\mathrm{OPT}$ , then $Z$ is $((1+\\varepsilon)\\alpha,\\alpha)$ -guidance set. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.2. Let $\\varepsilon\\ >\\ 0$ , and let $(f,\\mathcal{T}(\\mathcal{M}))$ be an instance of SM. The input set $Z_{0}$ is an $\\alpha_{0^{-}}$ approximate solution to $(f,\\mathcal{T}(\\mathcal{M}))$ . FASTLS (Alg. 4) returns a solution $Z$ with ${\\mathcal O}\\left(k n\\log(1/\\alpha_{0})/\\varepsilon\\right)$ queries such that $f\\left(S\\cup Z\\right)+f\\left(S\\cap Z\\right)<\\left(2+\\varepsilon\\right)f\\left(Z\\right)$ , where $S\\in{\\mathcal{T}}(M)$ . ", "page_idx": 5}, {"type": "text", "text": "2.2 Guiding the RANDOMGREEDY Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we discuss the guided RANDOMGREEDY algorithm (Alg. 6) using an $((1+\\varepsilon)\\alpha,\\alpha)$ - guidance set $Z$ returned by FASTLS. Due to space constraints, we only consider the size constraint in the main text. The ideas for the matroid constraint are similar, although the final recurrences obtained differ. The version for matroid constraints is given in Appendix C.2.2. ", "page_idx": 5}, {"type": "text", "text": "The algorithm GUIDEDRG is simple to describe: it maintains a partial solution $A$ , initially empty. It takes as parameters the switching time $t$ and guidance set $Z$ . While the partial solution satisfies $|A|<t k$ , the algorithm operates as RANDOMGREEDY with ground set $\\mathcal{U}\\setminus Z$ ; after $|A|\\geq t k$ , it operates as RANDOMGREEDY with ground set $\\boldsymbol{\\mathcal{U}}$ . Pseudocode is provided in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "Overview of analysis. For clarity, we first describe the (unguided) RANDOMGREEDY analysis from Buchbinder et al. [9]. There are two recurrences: the first is the greedy gain: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\right]\\geq\\frac{1}{k}\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)-f\\left(A_{i-1}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Intuitively, the gain at iteration $i$ is at least a $1/k$ fraction of the difference between $f(O\\cup A)$ and $A$ , in expectation, where $A$ is the partial solution. If $f$ were monotone, the right hand side would be at least $(\\mathrm{OPT}-f(A))/k$ . However, in the case that $f$ is not monotone, the set $O\\cup A$ may have value smaller than OPT. ", "page_idx": 5}, {"type": "text", "text": "To handle this case, it can be shown that the expected value of $f(O\\cup A)$ satisfies a second recurrence: $\\overline{{\\left[f\\left(O\\cup A_{i}\\right)\\right]\\stackrel{(a)}{\\geq}}}\\,\\left(1-\\frac{1}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]+\\frac{1}{k}\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)\\right]\\stackrel{(b)}{\\geq}\\left(1-\\frac{1}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right],$ where $M_{i}$ is the set of elements with the top $k$ marginal gains at iteration $i$ , (a) is from submodularity, and $(b)$ is from nonnegativity. Thus, this expected value, while initially OPT (since $A_{0}=\\varnothing$ ), may degrade but is bounded. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Both of these recurrences are solved together to prove the expected ratio of $1/e$ for RANDOMGREEDY: the worst-case evolution of the expected values of $f(A_{i}),\\;f(O\\cup A_{i})$ , according to this analysis, is illustrated in Fig. 1(a). Observe that $f(A_{i})$ converges to $\\mathrm{OPT}/e$ (as required for the ratio), and observe that $f(O\\cup A_{i})$ also converges to $O P T/e$ . Thus, very little gain is obtained in the later stages of the algorithm, as illustrated in the plot. The overarching idea of the guided version of the algorithm is to obtain a better degradation of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ , leading to better gains later in the algorithm that improve the worst-case ratio. In the following, we elaborate on this goal, the achievement of which is illustrated in Fig. 1(c). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Stage 1: Recurrences when avoiding $Z$ . Suppose $Z$ is an $(\\alpha,\\beta)$ -guidance set, and that RANDOMGREEDY selects elements as before, but excluding $Z$ from the ground set. Then, the recurrences change as follows. The recurrence for the gain becomes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\right]\\geq\\frac{1}{k}\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)-f\\left(A_{i-1}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $O\\setminus Z$ replaces $O$ since we select elements outside of the set $Z$ . For the second recurrence, we can lower bound the term $\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)\\right]$ using submodularity and the fact that $Z\\cap A_{i-1}=\\emptyset$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\frac{1}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]+\\frac{1}{k}\\mathbb{E}\\left[f\\left(O\\right)-f\\left(O\\cup Z\\right)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, a similar recurrence to (2) also holds for $f\\left((O\\setminus Z)\\cup A_{i}\\right)$ ; both are needed for the analysis. Since $Z$ is a guidance set, by submodularity, $f(O\\setminus Z)\\geq f(O)-f(O\\cap Z)\\geq(1-\\alpha){\\mathrm{OPT}}$ , which ensures that some gain is available by selection outside of $Z$ . And $f(O)-f(O\\cup Z)\\geq(1-\\beta){\\bf O P T},$ , which means that the degradation recurrences are improved. ", "page_idx": 6}, {"type": "text", "text": "The blue line in Figure 1(b) depicts this improved degradation with the size of the partial solution. However, this improvement comes at a cost: a smaller increase in $\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]$ is obtained over the unguided version. Therefore, to obtain an improved ratio we switch back to the regular behavior of RANDOMGREEDY \u2013 intuitively, this shifts the relatively good, earlier behavior of RANDOMGREEDY to later in the algorithm. ", "page_idx": 6}, {"type": "text", "text": "Stage 2: Switching back to selection from whole ground set. After the switch, the recurrences revert back to the original ones, but with different starting values. Since $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ was significantly enhanced in the first stage, in the final analysis we get an overall improvement over the unguided version. The blue line in Figure 1(c) demonstrates the degradation of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ over two stages, while the orange line depicts how the approximation ratio converges to a value $0.3\\bar{8}5>1/e$ . ", "page_idx": 6}, {"type": "text", "text": "The above analysis sketch can be formalized and the resulting recurrences solved: the results are stated in the following lemma, which is formally proven in Appendix C.2.1. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2.3. With an input size constraint $\\mathcal{T}$ and a $((1\\,+\\,\\varepsilon)\\alpha,\\alpha)$ -guidance set $Z$ , GUIDEDRG returns set $A_{k}$ with $O\\left(k n\\right)$ queries, s.t. $\\begin{array}{r}{\\mathbb{E}\\left[f\\left(A_{k}\\right)\\right]\\;\\ge\\;\\left[\\left(2-t-\\frac{1}{k}\\right)\\left(1-\\frac{1}{k}\\right)e^{t-1}-e^{-1}\\right.}\\end{array}$ $\\begin{array}{r}{-(1+\\varepsilon)\\alpha\\left(\\left(1-\\frac{1}{k}\\right)^{2}e^{t-1}-e^{-1}\\right)-\\alpha\\left(\\left(1+\\frac{1-t}{1-\\frac{1}{k}}\\right)e^{t-1}-\\left(2-\\frac{1}{k}\\right)e^{-1}\\right)\\right]f\\left(O\\right).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "From Lemma 2.3, we can directly prove the main result for size constraint. ", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 2.1 under size constraint. Let $(f,\\mathcal{T})$ be an instance of SM, with optimal solution set $O$ . If $f\\left(Z\\right)\\geq\\left(0.385-\\varepsilon\\right)f\\left(O\\right)$ under size constraint, the approximation ratio holds immediately. Otherwise, by Lemma 2.2, FASTLS returns a set $Z$ which is an $((1+\\varepsilon)\\alpha,\\alpha)$ -guidance set, where $\\alpha=0.385-\\varepsilon$ . By Lemma 2.3, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}\\left[f\\left(A_{k}\\right)\\right]\\geq\\left[\\left(2-t-\\varepsilon\\right)\\left(1-\\varepsilon\\right)e^{t-1}-e^{-1}-\\left(0.385-0.615\\varepsilon\\right)\\left(\\left(1-\\varepsilon\\right)^{2}e^{t-1}-e^{-1}\\right)\\right.}\\\\ &{\\left.-(0.385-\\varepsilon)\\left(\\left(1+\\frac{1-t}{1-\\varepsilon}\\right)e^{t-1}-(2-\\varepsilon)e^{-1}\\right)\\right]f\\left(O\\right)}&{\\left(\\forall k\\geq\\frac{1}{\\varepsilon}\\right)}\\\\ &{\\geq(0.385-\\varepsilon)f\\left(O\\right).}&{\\left(t=0.372\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2.3 Deterministic approximation algorithms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we outline the deterministic algorithms, for size and matroid constraints. The main idea is similar, but we replace GUIDEDRG with a deterministic subroutine. For simplicity, we present a randomized version in Appendix D.2 as Alg. 10, which we then derandomize (Alg. 11 in Appendix D.3). Further discussion is provided in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Algorithm overview. Chen and Kuhnle [11] proposed a randomized algorithm, INTERPOLATEDGREEDY, which may be thought of as an interpolation between standard greedy [25] and RANDOMGREEDY [9]. Instead of picking $k$ elements, each randomly chosen from the top $k$ marginal gains, it picks $\\ell=\\mathcal{O}\\left(1/\\varepsilon\\right)$ sets randomly from $O\\left(\\ell\\right)$ candidates. Although it uses only a constant number of rounds, the recurrences for INTERPOLATEDGREEDY are similar to the RANDOMGREEDY ones discussed above, so we can guide it similarly. ", "page_idx": 7}, {"type": "text", "text": "To select the candidate sets in each iteration, we replace INTERLACEGREEDY (the subroutine of INTERPOLATEDGREEDY proposed in Chen and Kuhnle [11]) with a guided version: GUIDEDIG-S (Alg. 9 in Appendix D.1.1) for size constraint, and GUIDEDIG-M (Alg. 8 in Appendix D.1) for matroid constraint. Since only $\\ell$ random choices are made, each from $O\\left(\\ell\\right)$ sets, there are at most $O\\left(\\ell^{O(\\ell)}\\right)$ possible solutions, where $\\ell$ is a constant number depending on $\\varepsilon$ . Notably, we are still able to obtain the same approximation factors as in Section 2. The proof of Theorem 2.4 is provided in Appendices D.2 and D.3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2.4. Let $(f,k)$ be an instance of SM, with the optimal solution set $O$ . Alg. 11 achieves a deterministic $(0.385-\\varepsilon)$ approximation ratio with $t\\,=\\,0.372$ , and a deterministic $(0.305-\\varepsilon)$ approximation ratio with $t=0.559$ . The query complexity of the algorithm is $\\mathcal{O}\\left(k n\\ell^{2\\ell-1}\\right)$ where $\\begin{array}{r}{\\ell=\\frac{10}{9\\varepsilon}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "3 Deterministic Algorithm with Nearly Linear Query Complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we sketch a deterministic algorithm with $\\left(0.377\\mathrm{~-~}\\varepsilon\\right)$ approximation ratio and ${\\mathcal{O}}_{\\varepsilon}\\left(n\\log(k)\\right)$ query complexity for the size constraint. A full pseudocode (Alg. 14) and analysis is provided in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Description of algorithm. Our goal is to improve the asymptotic ${\\mathcal{O}}_{\\varepsilon}(k n)$ query complexity. Recall that in Section 2, we described a deterministic algorithm that employed the output of local search to guide the INTERPOLATEDGREEDY algorithm, which obeys similar recurrences to RANDOMGREEDY. To produce the $\\ell$ candidate sets for each iteration of INTERPOLATEDGREEDY, a greedy algorithm (guided INTERLACEGREEDY) is used. These algorithms can be sped up using a descending thresholds technique. This results in THRESHGUIDEDIG (Alg. 12 in Appendix E.1), which achieves $\\bar{O_{\\varepsilon}}\\left(n\\log k\\right)$ query complexity for the guided part of our algorithm. ", "page_idx": 7}, {"type": "text", "text": "However, the local search FASTLS still requires ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ queries, so we seek to find a guidance set in a faster way. Recall that, in the definition of guidance set $Z$ , the value $f(Z)$ needs to dominate both $f(O\\cap Z)$ and $f(O\\cup Z)$ . To achieve this with faster query complexity, we employ a run of unguided INTERPOLATEDGREEDY. Consider the recurrences plotted in Fig. 1(a) \u2013 if the worst-case degradation occurs, then at some point the value of $f(A_{i})$ becomes close to $f(O\\cup A_{i})$ . On the other hand, if the worst-case degradation does not occur, then the approximation factor of INTERPOLATEDGREEDY is improved (see Fig. 2). Moreover, if we ensure that at every stage, $A_{i}$ contains no elements that contribute a negative gain, then we will also have $f(A_{i})\\geq f(O\\cap A_{i})$ . ", "page_idx": 7}, {"type": "text", "text": "To execute this idea, we run (derandomized, unguided) INTERPOLATEDGREEDY, and consider all $\\dot{\\mathcal{O}}\\left(\\ell^{\\widetilde{\\ell}}\\right)$ intermediate solutions. Each one of these is pruned (by which we mean, any element with negative contribution is discarded until none such remain). Then, the guided part of our algorithm executes with every possible candidate intermediate solution as the guiding set; finally, the feasible set encountered with maximum $f$ value is returned. ", "page_idx": 7}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/f1c24bcdb542c40e4b42047d780b31a51a1404903873648a1adda6235959b1c0.jpg", "img_caption": ["Figure 2: Depiction of how analysis of INTERPOLATEDGREEDY changes if there is no (0.377, 0.46)-guidance set. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/d8cda6b3be960932a0db88edd47bcb5a06e3e255a59d5e87e49aa0af58f15721.jpg", "img_caption": ["(a) Video Summarization,(b) Video Summarization,(c) Maximum Cut (ER),(d) Maximum Cut (ER), solution value queries solution value queries "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: The objective value (higher is better) and the number of queries (log scale, lower is better) are normalized by those of STANDARDGREEDY. Our algorithm (blue star) outperforms every baseline on at least one of these two metrics. ", "page_idx": 8}, {"type": "text", "text": "The tradeoff between the first and second parts of the algorithm is optimized with $\\alpha=0.377$ and $\\beta=0.46$ . That is, if INTERPOLATEDGREEDY produces an $(\\alpha,\\beta)$ -guidance set, the guided part of our algorithm achieves ratio 0.377; otherwise, INTERPOLATEDGREEDY has ratio at least 0.377. We have the following theorem. The algorithms and analysis sketched here are formalized in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.1. Let $(f,k)$ be an instance of SM, with the optimal solution set $O$ . Algorithm 14 achieves a deterministic $\\left(0.377-\\varepsilon\\right)$ approximation ratio with $\\mathcal{O}(n\\log(k){\\ell_{1}}^{2\\ell_{1}}{\\ell_{2}}^{2\\ell_{2}-1})$ queries, where $\\begin{array}{r}{\\ell_{1}=\\frac{10}{3\\varepsilon}}\\end{array}$ and $\\begin{array}{r}{\\ell_{2}=\\frac{5}{\\varepsilon}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "4 Empirical Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, along with Appendix F, we implement and empirically evaluate our randomized $(0.385-\\varepsilon)$ -approximation algorithm (Alg. 2, FASTL $^{\\mathrm{S+}}$ GUIDEDRG) on two applications of sizeconstrained SM, and compare to several baselines in terms of objective value of solution and number of queries to $f$ . In summary, our algorithm uses roughly twice the queries as the standard greedy algorithm, but obtains competitive objective values with an expensive local search that uses one to two orders of magnitude more queries. 1 ", "page_idx": 8}, {"type": "text", "text": "Baselines. 1) STANDARDGREEDY: the classical greedy algorithm [25], which often performs well empirically on non-monotone objectives despite having no theoretical guarantee. 2) RANDOMGREEDY, the current state-of-the-art combinatorial algorithm as discussed extensively above. 3) The local search algorithm of Lee et al. [21], which is the only prior polynomial-time local search algorithm with a theoretical guarantee: ratio $1/4-\\varepsilon$ in $\\mathcal{O}\\left(k^{5}\\log(k)\\dot{n}/\\varepsilon\\right)$ queries. As our emphasis is on theoretical guarantees above $1/e$ , we set $\\varepsilon\\,=\\,0.01$ for our algorithm, which yields ratio at least 0.375 in this evaluation. For Lee et al. [21], we set $\\varepsilon=0.1$ , which is the standard value of the accuracy parameter in the literature \u2013 running their algorithm with $\\varepsilon=0.01$ produced identical results. ", "page_idx": 8}, {"type": "text", "text": "Applications and datasets. For instances of SM upon which to evaluate, we chose video summarization and maximum cut (MC). For video summarization, our objective is to select a subset of frames from a video to create a summary. As in Banihashem et al. [1] , we use a Determinantal Point Process objective function to select a diverse set of elements [20]. Maximum cut is a classical example of a non-monotone, submodular objective function. We run experiments on unweighted Erd\u02ddos-R\u00e9nyi (ER), Barab\u00e1si-Albert (BA) and Watts-Strogatz (WS) graphs which have been used to model many real-world networks. The formal definition of problems, details of datasets, and hyperparameters of graph generation can be found in the Appendix F. In video summarization, there are $n=100$ frames. On all the instances of maximum cut, the number of vertices $n=10000$ . The mean of 20 independent runs is plotted, and the shaded region represents one standard deviation about the mean. ", "page_idx": 8}, {"type": "text", "text": "Results. As shown in Figure 3 in this section, and Figure 5 and 6 in Appendix F, on both applications, FASTLS $^+$ GUIDEDRG produces solutions of higher objective value than STANDARDGREEDY, and also higher than RANDOMGREEDY. The objective values of FASTLS $+\\mathrm{G}$ UIDEDRG often matches with Lee et al. [21] which performs the best; this agrees with the intuition that, empirically, local search is nearly optimal. In terms of queries, our algorithm uses roughly twice the number of queries as STANDARDGREEDY, but we improve on Lee et al. [21] typically by at least a factor of 10 and often by more than a factor of 100. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Prior to this work, the state-of-the-art combinatorial ratios were $1/e\\approx0.367$ and 0.283 for size constrained and matroid constrained SM, respectively, both achieved by the RANDOMGREEDY algorithm. In this work, we show how to guide RANDOMGREEDY with a fast local search algorithm to achieve ratios 0.385 and 0.305, respectively, in ${\\mathcal{O}}\\left(k n/\\varepsilon\\right)$ queries. The resulting algorithm is practical and empirically outperforms both RANDOMGREEDY and standard greedy in objective value on several applications of SM. However, if $k$ is on the order of $n$ , the query complexity is quadratic in $n$ , which is too slow for modern data sizes. Therefore, an interesting question for future work is whether further improvements in the query complexity to achieve these ratios (or better ones) could be made. ", "page_idx": 9}, {"type": "text", "text": "In addition, we achieve the same approximation ratios and asymptotic query complexity with deterministic algorithms, achieved by guiding a different algorithm; moreover, we speed up the deterministic algorithm to $O_{\\varepsilon}(n\\log k)$ by obtaining the guidance set in another way. This result is a partial answer to the limitation in the previous paragraph, as we achieve a ratio beyond $1/e$ in nearly linear query complexity. However, for all of our deterministic algorithms, there is an exponential dependence on $1/\\varepsilon$ , which makes these algorithms impractical and mostly of theoretical interest. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Banihashem, K., Biabani, L., Goudarzi, S., Hajiaghayi, M., Jabbarzade, P., and Monemizadeh, M. (2023). Dynamic non-monotone submodular maximization. Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023.   \n[2] Bao, W., Hang, J., and Zhang, M. (2022). Submodular feature selection for partial label learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2022.   \n[3] Bilmes, J. A. (2022). Submodularity in machine learning and artificial intelligence. arXiv preprint arXiv:2202.00132.   \n[4] Brualdi, R. A. (1969). Comments on bases in dependence structures. Bulletin of the Australian Mathematical Society, 1(2):161\u2013167.   \n[5] Buchbinder, N. and Feldman, M. (2018). Deterministic algorithms for submodular maximization problems. ACM Trans. Algorithms, 14(3):32:1\u201332:20.   \n[6] Buchbinder, N. and Feldman, M. (2019). Constrained submodular maximization via a nonsymmetric technique. Math. Oper. Res., 44(3):988\u20131005.   \n[7] Buchbinder, N. and Feldman, M. (2023). Constrained submodular maximization via new bounds for dr-submodular functions. arXiv preprint arXiv:2311.01129.   \n[8] Buchbinder, N., Feldman, M., and Garg, M. (2023). Deterministic $(1/2+\\epsilon)$ -approximation for submodular maximization over a matroid. SIAM J. Comput., 52(4):945\u2013967.   \n[9] Buchbinder, N., Feldman, M., Naor, J., and Schwartz, R. (2014). Submodular maximization with cardinality constraints. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014.   \n[10] Buchbinder, N., Feldman, M., and Schwartz, R. (2017). Comparing apples and oranges: Query trade-off in submodular maximization. Math. Oper. Res., 42(2):308\u2013329.   \n[11] Chen, Y. and Kuhnle, A. (2023). Approximation algorithms for size-constrained non-monotone submodular maximization in deterministic linear time. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023.   \n[12] Chen, Y. and Kuhnle, A. (2024). Practical and parallelizable algorithms for non-monotone submodular maximization with size constraint. J. Artif. Intell. Res., 79:599\u2013637.   \n[13] Ene, A. and Nguyen, H. L. (2016). Constrained submodular maximization: Beyond 1/e. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016.   \n[14] Feige, U., Mirrokni, V. S., and Vondr\u00e1k, J. (2011). Maximizing non-monotone submodular functions. SIAM J. Comput., 40(4):1133\u20131153.   \n[15] Feldman, M., Naor, J., and Schwartz, R. (2011). A unified continuous greedy algorithm for submodular maximization. In IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS 2011.   \n[16] Gharan, S. O. and Vondr\u00e1k, J. (2011). Submodular maximization by simulated annealing. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011.   \n[17] Han, K., Cao, Z., Cui, S., and Wu, B. (2020). Deterministic approximation for submodular maximization over a matroid in nearly linear time. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020.   \n[18] Khanna, R., Elenberg, E. R., Dimakis, A. G., Negahban, S. N., and Ghosh, J. (2017). Scalable greedy feature selection via weak submodularity. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017.   \n[19] Krause, A., Singh, A. P., and Guestrin, C. (2008). Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. J. Mach. Learn. Res., 9:235\u2013284.   \n[20] Kulesza, A., Taskar, B., et al. (2012). Determinantal point processes for machine learning. Foundations and Trends\u00ae in Machine Learning, 5(2\u20133):123\u2013286.   \n[21] Lee, J., Mirrokni, V. S., Nagarajan, V., and Sviridenko, M. (2009). Non-monotone submodular maximization under matroid and knapsack constraints. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009.   \n[22] Liu, Y., Wei, K., Kirchhoff, K., Song, Y., and Bilmes, J. A. (2013). Submodular feature selection for high-dimensional acoustic score spaces. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013.   \n[23] Mirzasoleiman, B., Jegelka, S., and Krause, A. (2018). Streaming non-monotone submodular maximization: Personalized video summarization on the fly. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI 2018.   \n[24] Nemhauser, G. L. and Wolsey, L. A. (1978). Best algorithms for approximating the maximum of a submodular set function. Math. Oper. Res., 3(3):177\u2013188.   \n[25] Nemhauser, G. L., Wolsey, L. A., and Fisher, M. L. (1978). An analysis of approximations for maximizing submodular set functions - I. Math. Program., 14(1):265\u2013294.   \n[26] \u00d6zcan, G., Moharrer, A., and Ioannidis, S. (2021). Submodular maximization via taylor series approximation. In Demeniconi, C. and Davidson, I., editors, Proceedings of the 2021 SIAM International Conference on Data Mining, SDM 2021, Virtual Event, April 29 - May 1, 2021, pages 423\u2013431. SIAM.   \n[27] Powers, T., Bilmes, J. A., Krout, D. W., and Atlas, L. E. (2016). Constrained robust submodular sensor selection with applications to multistatic sonar arrays. In 19th International Conference on Information Fusion, FUSION 2016.   \n[28] Qi, B. (2022). On maximizing sums of non-monotone submodular and linear functions. In $33r d$ International Symposium on Algorithms and Computation, ISAAC 2022.   \n[29] Roy (2022). Cooking video. https://youtu.be/voUDP4rUKvQ?si=ZUezR4jMVzOz5Kcw. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[30] Sun, X., Zhang, J., Zhang, S., and Zhang, Z. (2022). Improved deterministic algorithms for non-monotone submodular maximization. In Computing and Combinatorics - 28th International Conference, COCOON 2022. [31] Tschiatschek, S., Iyer, R. K., Wei, H., and Bilmes, J. A. (2014). Learning mixtures of submodular functions for image collection summarization. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, NeurIPS 2014. [32] Tukan, M., Mualem, L., and Feldman, M. (2024). Practical 0.385-approximation for submodular maximization subject to a cardinality constraint. arXiv preprint arXiv:2405.13994. ", "page_idx": 11}, {"type": "text", "text": "A Additional Preliminaries ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Constraints ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this paper, our focus lies on two constraints: size constraint and matroid constraint. For size constraint, we define the feasible subsets as ${\\mathcal{T}}(k)=\\{S\\subseteq\\mathcal{U}:|S|\\leq k\\}$ . For matroid constraint, the definition is as follows: ", "page_idx": 12}, {"type": "text", "text": "Definition A.1. A matroid $\\mathcal{M}$ is a pair $(\\mathcal{U},\\mathcal{T})$ , where $\\boldsymbol{\\mathcal{U}}$ is the ground set and $\\mathcal{T}$ is the independent sets with the following properties: (1) $\\varnothing\\in{\\mathcal{T}}$ ; (2) hereditary property: $A\\in{\\mathcal{T}}\\Rightarrow B\\in{\\mathcal{T}},\\forall B\\subseteq A$ ; (3) exchange property: $A,B\\in{\\mathcal{Z}},|A|>|B|\\Rightarrow\\exists x\\in A\\setminus B,s.t.B+x\\in{\\mathcal{Z}}$ . ", "page_idx": 12}, {"type": "text", "text": "Specifically, we use $\\mathcal{T}(\\mathcal{M})$ to represent the independent sets of matroid $\\mathcal{M}$ . A maximal independent set in $\\mathcal{T}(\\mathcal{M})$ is called a basis. Let $k$ be the size of the maximal independent set. ", "page_idx": 12}, {"type": "text", "text": "In the following, we consider an extended matroid with $k$ dummy elements added to the ground set $\\mathcal{M}^{\\prime}=(\\mathcal{U}^{\\prime},\\mathcal{I}^{\\prime})$ . We show that SM on $\\mathcal{M}^{\\prime}$ return the same solution as on $\\mathcal{M}$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Let $\\mathcal{M}=(\\mathcal{U},\\mathcal{T})$ be a matroid, and $\\mathcal{U}^{\\prime}=\\mathcal{U}\\cup E$ , where $E=\\{e_{0}^{1},\\ldots,e_{0}^{k}\\}$ and $e_{0}^{i}$ is a dummy element for each $i\\in[k]$ . Let $\\mathcal{Z}^{\\prime}=\\bigcup_{S\\in\\mathcal{Z}}\\{S,S\\cup\\{e_{0}^{1}\\},\\ldots,S\\cup\\{e_{0}^{1},\\ldots,e_{0}^{k-|S|}\\}\\}$ e0k\u2212|S|}}. Then, $\\mathcal{M}^{\\prime}=(\\mathcal{U}^{\\prime},\\mathcal{I}^{\\prime})$ is also a matroid and $\\mathrm{max}_{S\\in\\mathcal{Z}}\\,f\\left(S\\right)=\\mathrm{max}_{S\\in\\mathcal{Z}^{\\prime}}\\,f\\left(S\\right)$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Firstly, we prove that $\\mathcal{M}^{\\prime}=(\\mathcal{U}^{\\prime},\\mathcal{I}^{\\prime})$ is also a matroid by Definiton A.1. ", "page_idx": 12}, {"type": "text", "text": "Since $\\varnothing\\in{\\mathcal{T}}$ and $\\mathcal{T}\\subseteq\\mathcal{T}^{\\prime}$ , it holds that $\\varnothing\\in{\\mathcal{Z}}^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "Let $A^{\\prime}\\,\\in\\,{\\mathcal{Z}}^{\\prime}$ , and $A\\,=\\,A^{\\prime}\\setminus E$ . Then, $A\\in{\\mathcal{T}}$ . For every $B^{\\prime}\\subseteq A^{\\prime}$ , since $(\\mathcal{U},\\mathcal{T})$ is a matroid, $B=B^{\\prime}\\setminus E\\subseteq A\\in\\mathbb{Z}$ . Since $|B^{\\prime}\\cap E|\\leq|A^{\\prime}\\cap E|\\leq k-|A|\\leq k-|B|$ , it holds that $B\\in{\\mathbb{Z}}^{\\prime}$ by the construction of ${\\mathcal{Z}}^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "Let $A^{\\prime},B^{\\prime}\\in{\\mathcal{T}}^{\\prime}$ and $|A^{\\prime}|>|B^{\\prime}|$ . Let $A=A^{\\prime}\\backslash E$ and $B=B^{\\prime}\\backslash E$ . Then, $A,B\\in{\\mathcal{T}}$ . If $|A|>|B|$ , by Def. A.1, there exists $x\\in A\\setminus B$ s.t. $B+x\\in{\\mathcal{T}}$ . Since $(B^{\\prime}+x)\\setminus(B+x)\\subseteq E$ and $|B^{\\prime}|<|A^{\\prime}|\\leq k$ , $B^{\\prime}\\!+\\!x\\in{\\mathcal{T}}^{\\prime}$ . Otherwise, $|A|<|B|$ , which indicates that $|A^{\\prime}\\cap E|>|B^{\\prime}\\cap E|$ . Since $|B^{\\prime}|<|A^{\\prime}|\\leq k$ , by adding a dummy element $e_{0}\\in{\\dot{A}}^{\\prime}\\cap E\\setminus B^{\\prime}$ to $B^{\\prime}$ , it holds that $B^{\\prime}+e_{0}\\in\\mathcal{T}^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "Thus, by Def. A.1, $\\mathcal{M}^{\\prime}=(\\mathcal{U}^{\\prime},\\mathcal{I}^{\\prime})$ is also a matroid. ", "page_idx": 12}, {"type": "text", "text": "As for SM on ${\\mathcal{Z}}^{\\prime}$ , since dummy element does not contribute to the objective value, it holds that, for every $S\\in{\\mathcal{T}}$ , $f\\left(S\\right)=f\\left(S\\cup E^{\\prime}\\right)$ , where $E^{\\prime}\\subseteq E$ . Then, $\\{f\\left(S\\right):{\\bar{S}}\\in{\\bar{Z}}\\}=\\left\\{f\\left(S\\right):S\\in{\\bar{Z}}^{\\prime}\\right\\}$ . Further, $\\mathrm{max}_{S\\in\\mathcal{Z}}\\,f\\left(S\\right)=\\mathrm{max}_{S\\in\\mathcal{Z}^{\\prime}}\\,f\\left(S\\right)$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "A.2 Technical Lemma ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma A.2. (Brualdi [4]) If $B_{1}$ and $B_{2}$ are finite bases, then there exists a bijection $\\sigma:B_{1}\\setminus B_{2}\\rightarrow$ $B_{2}\\setminus B_{1}$ such that $B_{2}+e-\\sigma(e)$ is a basis for all $e\\in B_{1}\\setminus B_{2}$ ", "page_idx": 12}, {"type": "text", "text": "Lemma A.3. Let $a\\in\\mathbb{R}^{+},b,X_{0}\\in\\mathbb{R}.$ . If $X_{i}\\geq a X_{i-1}+b$ for every $i\\in[k]$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\nX_{k}\\geq a^{k}X_{0}+{\\frac{b\\left(1-a^{k}\\right)}{1-a}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. By repeatedly implementing that $X_{i}\\geq a X_{i-1}+b$ , we can bound $X_{k}$ as follows, ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{X_{k}\\geq a X_{k-1}+b}\\\\ &{\\qquad\\geq a^{2}X_{k-2}+a b+b}\\\\ &{\\qquad\\cdot}\\\\ &{\\qquad\\geq a^{k}X_{0}+b\\left(\\sum_{i=0}^{k-1}a^{i}\\right)}\\\\ &{\\qquad=a^{k}X_{0}+{\\frac{b\\,{\\bigl(}1-a^{k}{\\bigr)}}{1-a}}.\\complement}\\end{array}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.4. ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{1-\\displaystyle\\frac{1}{x}\\leq\\log(x)\\leq x-1,}}&{{\\qquad\\qquad\\qquad\\forall x>0}}\\\\ {{1-\\displaystyle\\frac{1}{x+1}\\geq e^{-\\displaystyle\\frac{1}{x}},}}&{{\\qquad\\qquad\\qquad\\forall x\\in\\mathbb{R}}}\\\\ {{(1-x)^{y-1}\\geq e^{-x y},}}&{{\\forall x y\\leq1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Query Complexity Analysis of the Continuous Algorithm in Buchbinder and Feldman [6] ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/bd59f08066479e050876f34048eb05a969982bffb14fd71a8652719853299ff5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "In this section, we analyze the query complexity of Aided Measured Continuous Greedy (Alg. 3) proposed by Buchbinder and Feldman [6], which is $O\\left(n^{11}\\log(n)\\right)$ . ", "page_idx": 13}, {"type": "text", "text": "In Alg. 3, queries to the oracle $f$ only occur on Line 5. For each element $u$ in the ground set $\\boldsymbol{\\mathcal{U}}$ , $r=\\lceil48n^{6}\\,\\dot{\\log}(2n)\\rceil$ queries are made. These queries correspond to $r$ independent samples of $R(y(t))$ to estimate $\\mathbb{E}\\left[\\Delta(u|R(y(t)))\\right]$ . Therefore, there are $n r=O\\left(n^{7}\\log(n)\\right)$ queries for each iteration of the while loop (Line 3-11). ", "page_idx": 13}, {"type": "text", "text": "Time variable $t$ is increased by $\\bar{\\delta}_{1}=t_{s}\\cdot n^{-4}$ , when $t<t_{s}$ , and is increased by $\\bar{\\delta}_{2}=(1-t_{s})\\cdot n^{-4}$ , when $t\\geq t_{s}$ . Thus, there are a total of $2n^{4}$ iterations within the while loop. In conclusion, the total number of queries made by the algorithm is $O\\left(n^{11}\\log(n)\\right)$ . ", "page_idx": 13}, {"type": "text", "text": "C Analysis of Randomized Approximation Algorithm, Alg. 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a detailed analysis of our randomized approximation algorithm and its two components, FASTLS and GUIDEDRG. This section is organized as follows: Appendix C.1 analyzes the theoretical guarantee of single run of FASTLS (Appendix C.1.1), which is needed to show that it finds a good guidance set. Then, although it is not needed for our results, we show the FASTLS independently achieves an approximation ratio achieved for SMCC under monotone and non-monotone objectives (Appendix C.1.2). ", "page_idx": 13}, {"type": "text", "text": "In Appendix C.2, we provide pseudocode and formally prove the results for GUIDEDRG. Specifically, we solve the recurrences for both size and matroid constraint. ", "page_idx": 13}, {"type": "text", "text": "xity . 1 Procedure FASTLS $(f,\\mathcal{T}(\\mathcal{M}),Z_{0},\\varepsilon)$ : Input: oracle $f$ , matroid constraint $\\mathcal{T}(\\mathcal{M})$ , an approximation result $Z_{0}$ , accuracy parameter $\\varepsilon$ 3 Initialize: $Z\\gets Z_{0}\\qquad\\qquad\\quad/\\star$ add dummy elements to $Z$ until $|Z|=k\\ \\ \\star/$ while $\\exists a\\in Z,e\\in\\mathcal{U}\\setminus Z,s.t.Z-a+e\\in\\mathcal{Z}(\\mathcal{M})$ and $\\begin{array}{r}{\\Delta(e|Z)-\\Delta(a|Z\\setminus a)\\ge\\frac{:}{k}\\dot{f}\\left(Z\\right)}\\end{array}$ 5 do 6 $Z\\gets Z-a+e$ 7 end 8 return Z ", "page_idx": 14}, {"type": "text", "text": "C.1 Analysis of FASTLS (Alg. 4) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Pseudocode for FASTLS is provided in Alg. 4. ", "page_idx": 14}, {"type": "text", "text": "C.1.1 Finding A Good Guidance Set \u2013 Proofs for Lemma 2.2 of Alg. 4 in Section 2.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that FASTLS (Alg. 4) takes a matroid constraint $\\mathcal{T}(\\mathcal{M})$ and an approximation result $Z_{0}$ as inputs, and outputs a local optimum $Z$ . Here, we restate the theoretical guarantees of FASTLS (Lemma 2.2). Using the conclusions drawn from Lemma 2.2, we demonstrate in Corollary C.1 that $Z$ is a $((1+\\varepsilon)\\alpha,\\alpha)$ -guidance set. At the end of this section, we provide the proof for Lemma 2.2. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2.2. Let $\\varepsilon\\ >\\ 0$ , and let $(f,\\mathcal{T}(\\mathcal{M}))$ be an instance of SM. The input set $Z_{0}$ is an $\\alpha_{0}$ - approximate solution to $(f,\\mathcal{T}(\\mathcal{M}))$ . FASTLS (Alg. 4) returns a solution $Z$ with $\\mathcal O\\left(k n\\log(1/\\alpha_{0})/\\varepsilon\\right)$ queries such that $f\\left(S\\cup Z\\right)+f\\left(S\\cap Z\\right)<\\left(2+\\varepsilon\\right)f\\left(Z\\right)$ , where $S\\in{\\mathcal{T}}(M)$ . ", "page_idx": 14}, {"type": "text", "text": "Corollary C.1. Let $Z$ be the solution of FASTLS $(f,\\mathcal{T}(\\mathcal{M}),Z_{0},\\varepsilon)$ . If $f\\left(Z\\right)\\,<\\,\\alpha\\mathrm{OPT}$ , $Z$ is a $((1+\\varepsilon)\\dot{\\alpha_{,}}\\alpha)$ -guidance set. ", "page_idx": 14}, {"type": "text", "text": "Proof of Corollary C.1. By Lemma 2.2, let $S=O$ and $S=O\\cap Z$ respectively, it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)<(2+\\varepsilon)f\\left(Z\\right),}\\\\ &{f\\left(O\\cap Z\\right)<(1+\\varepsilon)f\\left(Z\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $f\\left(Z\\right)<\\alpha\\mathrm{OPT}$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)<(2+\\varepsilon)\\alpha\\mathbf{O}\\mathbf{P}\\mathrm{T},}\\\\ &{f\\left(O\\cap Z\\right)<(1+\\varepsilon)\\alpha\\mathbf{O}\\mathbf{P}\\mathrm{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Definition 2.1, $Z$ is a $((1+\\varepsilon)\\alpha,\\alpha)$ -guidance set. ", "page_idx": 14}, {"type": "text", "text": "In the following, we prove Lemma 2.2 for Alg. 4. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 2.2. Query Complexity. For each successful replacement of elements on Line 6, it holds that $\\begin{array}{r}{\\Delta(e|Z)-\\Delta(a|\\bar{Z}-a)\\stackrel{=}{\\geq}\\frac{\\varepsilon}{k}f^{'}(Z)}\\end{array}$ . By submodularity, ", "page_idx": 14}, {"type": "equation", "text": "$$\nf\\left(Z-a+e\\right)-f\\left(Z\\right)=\\Delta(e|Z-a)-\\Delta(a|Z-a)\\geq\\Delta(e|Z)-\\Delta(a|Z-a)\\geq\\frac{\\varepsilon}{k}f\\left(Z\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, the oracle value $f(Z)$ is increased by a factor of at least $\\left(1+\\varepsilon/k\\right)$ after the swap. Therefore, there are at most log1+\u03b5/k ff((ZO0)) iterations, since otherwise, it would entail $f\\,(Z)\\,>\\,f\\,(O),$ which contradicts the fact that $O$ is the optimal solution. Then, since the algorithm makes at most ${\\mathcal{O}}\\left(n\\right)$ queries at each iteration, the query complexity can be bounded as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\#\\operatorname{queries}\\leq\\mathcal{O}\\left(n\\left\\lceil\\log_{1+\\varepsilon/k}\\left(\\frac{f\\left(O\\right)}{f\\left(Z_{0}\\right)}\\right)\\right\\rceil\\right)\\leq\\mathcal{O}\\left(n\\frac{k}{\\varepsilon}\\log\\left(\\frac{1}{\\alpha}\\right)\\right)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {(f\\left(Z_{0}\\right)\\geq\\alpha f\\left(O\\right);\\operatorname{Lemma}\\,\\operatorname{A}.4)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Objective Value. For any $S\\in{\\mathbb{Z}}({\\mathcal{M}})$ , we consider adding dummy elements into $S$ until $|S|=k$ .   \nBy Lemma A.1, we consider $Z$ and $S$ are bases of matroid $\\mathcal{M}$ with or without dummy elements. ", "page_idx": 14}, {"type": "text", "text": "Then, by Lemma A.2, there exits a bijection $\\sigma\\,:\\,S\\setminus Z\\,\\rightarrow\\,Z\\setminus S$ such that $Z+e\\mathrm{~-~}\\sigma(e)$ is a basis for all $e\\ \\in\\ S\\ \\backslash\\ Z$ . After the algorithm terminates, for every $e\\,\\in\\,S\\setminus Z$ , it holds that, $\\begin{array}{r}{\\Delta(e|Z)-\\Delta(\\sigma(e)|Z-\\dot{\\sigma}(e))<\\frac{\\varepsilon}{k}f\\left(Z\\right)}\\end{array}$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varepsilon f\\left(Z\\right)>\\sum_{e\\in S\\backslash Z}\\Delta(e|Z)-\\sum_{a\\in Z\\backslash S}\\Delta(a|Z-a)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\ell=|S\\setminus Z|=|Z\\setminus S|,S\\setminus Z=\\{e_{1},\\dots,e_{\\ell}\\},Z\\setminus S=\\{a_{1},\\dots,a_{\\ell}\\},Z\\setminus Z,$ . By submodularity, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{e\\in S\\backslash z}\\Delta(e|Z)=(f\\left(Z+e_{1}\\right)-f\\left(Z\\right))+(f\\left(Z+e_{2}\\right)-f\\left(Z\\right))+\\ldots+(f\\left(Z+e_{\\ell}\\right)-f\\left(Z\\right))}}\\\\ &{}&{\\ge(f\\left(Z+e_{1}+e_{2}\\right)-f\\left(Z\\right))+(f\\left(Z+e_{3}\\right)-f\\left(Z\\right))+\\ldots+(f\\left(Z+e_{\\ell}\\right)-f\\left(Z\\right))}\\\\ &{}&{\\ge\\ldots}\\\\ &{}&{\\ge f\\left(Z+e_{1}+\\ldots+e_{\\ell}\\right)-f\\left(Z\\right)=f\\left(S\\cup Z\\right)-f\\left(Z\\right).\\mathrm{Also~by~submodularity,}}\\\\ &{}&{\\ell}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\in Z\\backslash S}\\Delta(a|Z-a)=\\sum_{i=1}^{\\ell}\\Delta(a_{i}|Z-a_{i})\\leq\\sum_{i=1}^{\\ell}\\Delta(a_{i}|Z-a_{1}-\\ldots-a_{i})=f\\left(Z\\right)-f\\left(S\\cap Z\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\varepsilon f\\left(Z\\right)>f\\left(S\\cup Z\\right)-f\\left(Z\\right)-\\left(f\\left(Z\\right)-f\\left(S\\cap Z\\right)\\right)}}\\\\ {{\\Rightarrow(2+\\varepsilon)f\\left(Z\\right)>f\\left(S\\cup Z\\right)+f\\left(S\\cap Z\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.1.2 Approximation Ratio achieved by FASTLS ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we show that FASTLS can be employed independently to achieves approximation ratios of nearly $1/2$ and $1/4$ for both the monotone and non-monotone versions of the problem, respectively. ", "page_idx": 15}, {"type": "text", "text": "Monotone submodular functions. By employing FASTLS once, it returns an $\\frac{1}{2+\\varepsilon}$ -approximation result in monotone cases. ", "page_idx": 15}, {"type": "text", "text": "Theorem C.1. Let $\\varepsilon>0$ , and let $(f,\\mathcal{T}(\\mathcal{M}))$ be an instance of SM, where $f$ is monotone. The input set $Z_{0}$ is an $\\alpha_{0}$ -approximate solution to $(f,\\dot{Z}(\\mathcal{M}))$ . FASTLS (Alg. 4) returns a solution $Z$ such that $f\\left(Z\\right)\\geq f\\left(O\\right)/(2+\\varepsilon)$ with $\\mathcal O\\left(k n\\log(1/\\alpha_{0}\\right)/\\varepsilon\\right)$ queries. ", "page_idx": 15}, {"type": "text", "text": "Proof. By Lemma 2.2, set $S=O$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n(2+\\varepsilon)f\\left(Z\\right)>f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)\\geq f\\left(O\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality follows by monotonicity and non-negativity. ", "page_idx": 15}, {"type": "text", "text": "Non-monotone submodular functions. For the non-monotone problem, 2 repetitions of FASTLS (Alg. 5) yields a ratio of $\\textstyle{\\frac{1}{4+2\\varepsilon}}$ . The theoretical guarantees and the corresponding analysis are provided as follows. We remark that this is a primitive implementation of the guiding idea: the second run of FASTLS avoids the output of the first one. ", "page_idx": 15}, {"type": "table", "img_path": "cgiOX8lfwG/tmp/9f36baabf0b8db34746ba799836dc6a1a757f38c385f9af504ccb6dd91dd1f37.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Theorem C.2. Let $\\varepsilon\\,>\\,0$ , and let $(f,\\mathcal{T}(\\mathcal{M}))$ be an instance of SM, where $f$ is not necessarily monotone. The input set $Z_{0}$ is an $\\alpha_{0}$ -approximate solution to $(f,\\mathcal{T}(\\mathcal{M}))$ . Alg. 5 returns a solution $Z$ such that $f\\left(Z\\right)\\geq f\\left(O\\right)/(4+2\\varepsilon)$ with $\\mathcal O\\left(k n\\log(1/\\alpha_{0})/\\varepsilon\\right)$ queries. ", "page_idx": 15}, {"type": "text", "text": "Proof. By repeated application of Lemma 2.2 for the two calls of FASTLS in Alg. 5, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f\\left(O\\cup Z_{1}\\right)+f\\left(O\\cap Z_{1}\\right)<(2+\\varepsilon)f\\left(Z_{1}\\right)}\\\\ {f\\left((O\\setminus Z_{1})\\cup Z_{2}\\right)+f\\left((O\\setminus Z_{1})\\cap Z_{2}\\right)<(2+\\varepsilon)f\\left(Z_{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By summing up the above two inequalities, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left({4+\\varepsilon}\\right)f\\left(Z\\right)\\geq\\left({2+\\varepsilon}\\right)f\\left({Z_{1}}\\right)+\\left({2+\\varepsilon}\\right)f\\left({Z_{2}}\\right)}\\\\ &{\\qquad\\qquad\\geq f\\left({O\\cup Z_{1}}\\right)+f\\left({O\\cap Z_{1}}\\right)+f\\left({\\left(O\\setminus Z_{1}\\right)\\cup Z_{2}}\\right)+f\\left({\\left(O\\setminus Z_{1}\\right)\\cap Z_{2}}\\right)}\\\\ &{\\qquad\\qquad\\geq f\\left({O\\cup Z_{1}}\\right)+f\\left({\\left(O\\setminus Z_{1}\\right)\\cup Z_{2}}\\right)+f\\left({O\\cap Z_{1}}\\right)}&{\\mathrm{(nonnegativity)}}\\\\ &{\\qquad\\qquad\\geq f\\left({O\\setminus Z_{1}}\\right)+f\\left({O\\cap Z_{1}}\\right)}&{\\mathrm{(submodularity)}}\\\\ &{\\qquad\\qquad\\geq f\\left({O}\\right).}&{\\mathrm{(submodularity)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2 Pseudocode of GUIDEDRG (Alg. 6) and its Analysis ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/fecdc5ddcbe4be2e235eaa1af3b36edab9da9e528f3a660ac14b2e2d905d2daf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In this section, we present the pseudocode for GUIDEDRG as Alg. 6. Then, we provide the detailed proof of Lemma 2.3 in Appendix C.2.1, which addresses size constraints. Finally, we analyze the algorithm under matroid constraints and provide the guarantees and its analysis in Appendix C.2.2. ", "page_idx": 16}, {"type": "text", "text": "C.2.1 GUIDEDRG under Size Constraints ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Sec. 2.2, we introduce the intuition behind GUIDEDRG under size constraint. Below, we reiterate theoretical guarantees achieved by GUIDEDRG under size constraints and provide the detailed analysis. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2.3. With an input size constraint $\\mathcal{T}$ and a $((1\\,+\\,\\varepsilon)\\alpha,\\alpha)$ -guidance set $Z$ , GUIDEDRG returns set $A_{k}$ with $O\\left(k n\\right)$ queries, s.t. $\\begin{array}{r}{\\mathbb{E}\\left[f\\left(A_{k}\\right)\\right]\\ \\ge\\ \\left[\\left(2-t-\\frac{1}{k}\\right)\\left(1-\\frac{1}{k}\\right)e^{t-1}-e^{-1}\\right.}\\end{array}$ $\\begin{array}{r}{-(1+\\varepsilon)\\alpha\\left(\\left(1-\\frac{1}{k}\\right)^{2}e^{t-1}-e^{-1}\\right)-\\alpha\\left(\\left(1+\\frac{1-t}{1-\\frac{1}{k}}\\right)e^{t-1}-\\left(2-\\frac{1}{k}\\right)e^{-1}\\right)\\right]f\\left(O\\right)\\,.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "We provide the recurrence of $f\\left((O\\setminus Z)\\cup A_{i}\\right),f\\left(O\\cup A_{i}\\right)$ and $f\\left(A_{i}\\right)$ in Lemmata C.3 and C.4 and their analysis below to help prove Lemma 2.3 under size constraint. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.3. When $0<i\\leq t\\cdot k$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{k}\\right)\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)\\right]+\\displaystyle\\frac{1}{k}\\left[f\\left(O\\setminus Z\\right)-f\\left(O\\cup Z\\right)\\right],}\\\\ &{\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]+\\displaystyle\\frac{1}{k}\\left[f\\left(O\\right)-f\\left(O\\cup Z\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When $t\\cdot k<i\\leq k$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\frac{1}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. At iteration $i$ , condition on a given $A_{i-1}$ . When $i\\leq t k,A_{i-1}\\cap Z=\\emptyset$ and $M_{i}$ is selected out of $A_{i-1}\\cup Z$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(O\\cup Z)\\cap((O\\setminus Z)\\cup A_{i-1}\\cup M_{i})=O\\setminus Z}}\\\\ {{((O\\cup Z)\\cap(O\\cup A_{i-1}\\cup M_{i})=O.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, ", "page_idx": 17}, {"type": "text", "text": "E [f ((O \\ Z) \u222aAi) | Ai\u22121] = kx\u2208 M f ((O \\ Z) \u222aAi\u22121 \u222a{x}) (selection of next element)   \n\u2265k [(k \u22121)f ((O \\ Z) \u222aAi\u22121) + f ((O \\ Z) \u222aAi\u22121 \u222aMi)] (submodularity)   \n\u2265 [(k \u22121)f ((O \\ Z) \u222aAi\u22121) + f (O \\ Z) + f (O \u222aZ \u222aAi\u22121 \u222aMi) \u2212f (O \u222aZ)] (submodularity)   \n$\\begin{array}{r l}&{\\geq\\displaystyle\\frac{1}{k}\\left[(k-1)f\\left((O\\setminus Z)\\cup A_{i-1}\\right)+f\\left(O\\setminus Z\\right)-f\\left(O\\cup Z\\right)\\right]}\\\\ &{\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\mid A_{i-1}\\right]=\\displaystyle\\frac{1}{k}\\sum_{x\\in M_{i}}f\\left(O\\cup A_{i-1}\\cup\\left\\{x\\right\\}\\right)}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\left[(k-1)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)\\right]}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\left[(k-1)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\right)+f\\left(O\\cup Z\\cup A_{i-1}\\cup M_{i}\\right)-f\\left(O\\cup Z\\right)\\right]}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\left[(k-1)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\right)-f\\left(O\\cup Z\\right)\\right]}\\end{array}$ (nonnegativity) (submodularity) (submodularity) (nonnegativity) ", "page_idx": 17}, {"type": "text", "text": "When $i>t k$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathop{\\mathbb{E}}\\left[f\\left(O\\cup A_{i}\\right)\\mid A_{i-1}\\right]=\\frac{1}{k}\\sum_{x\\in M_{i}}f\\left(O\\cup A_{i-1}\\cup\\{x\\}\\right)}\\\\ &{\\geq\\frac{1}{k}\\left[(k-1)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)\\right]}\\\\ &{\\geq\\left(1-\\frac{1}{k}\\right)f\\left(O\\cup A_{i-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By unconditioning $A_{i-1}$ , the lemma is proved. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.4. When $0<i\\leq t\\cdot k$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]-\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\geq\\frac{1}{k}\\left(\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)\\right]-\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $t\\cdot k<i\\leq k$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]-\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\geq\\frac{1}{k}\\left(\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]-\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Given $A_{i-1}$ at iteration $i$ . When $i\\leq t\\cdot k$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\mid A_{i-1}\\right]=\\displaystyle\\frac{1}{k}\\sum_{x\\in M_{i}}\\Delta(x|A_{i-1})}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\sum_{x\\in O\\backslash(A_{i-1}\\cup Z)}\\Delta(x|A_{i-1})}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\left[f\\left((O\\setminus Z)\\cup A_{i-1}\\right)-f\\left(A_{i-1}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(submodularity) ", "page_idx": 17}, {"type": "text", "text": "When $i>t\\cdot k$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\mid A_{i-1}\\right]=\\displaystyle\\frac{1}{k}\\sum_{x\\in M_{i}}\\Delta(x|A_{i-1})}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\sum_{x\\in O\\backslash A_{i-1}}\\Delta(x|A_{i-1})}\\\\ &{\\geq\\displaystyle\\frac{1}{k}\\left[f\\left(O\\cup A_{i-1}\\right)-f\\left(A_{i-1}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(Line 6 in Alg. 6) ", "page_idx": 18}, {"type": "text", "text": "(submodularity) ", "page_idx": 18}, {"type": "text", "text": "By unconditioning $A_{i-1}$ , the lemma is proved. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 2.3. It follows from Lemma C.3 and the closed form for a recurrence provided in Lemma A.3 that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i}\\right)\\right]\\geq f\\left(O\\setminus Z\\right)-\\left(1-\\left(1-\\displaystyle\\frac{1}{k}\\right)^{i}\\right)f\\left(O\\cup Z\\right),}&{\\forall0<i\\leq t k}\\\\ {\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{k}\\right)^{i-\\lfloor t k\\rfloor}\\left[f\\left(O\\right)-\\left(1-\\left(1-\\displaystyle\\frac{1}{k}\\right)^{\\lfloor t k\\rfloor}\\right)f\\left(O\\cup Z\\right)\\right],}&{\\forall t k<i\\leq k}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With the above inequalities, we can solve the recursion in Lemma C.4 as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\cdot}{2}\\left[f\\left(A_{\\lfloor t k\\rfloor}\\right)\\right]\\stackrel{(a)}{\\geq}\\left(1-\\left(1-\\frac{1}{k}\\right)^{\\lfloor t k\\rfloor}\\right)f\\left({O}\\setminus{Z}\\right)-\\left(1-\\left(1-\\frac{1}{k}\\right)^{\\lfloor t k\\rfloor}-t\\left(1-\\frac{1}{k}\\right)^{\\lfloor t k\\rfloor-1}\\right)f\\left({O}\\right)}\\\\ &{\\quad\\stackrel{(\\cdot)}{\\geq}\\left(1-\\frac{1}{k}\\right)^{k-\\lfloor t k\\rfloor}{\\mathbb{E}}\\left[f\\left(A_{\\lfloor t k\\rfloor}\\right)\\right]+(1-t)\\left(1-\\frac{1}{k}\\right)^{k-\\lfloor t k\\rfloor-1}\\left[f\\left({O}\\right)-\\left(1-\\left(1-\\frac{1}{k}\\right)\\right)^{\\lfloor t k\\rfloor}\\right]}\\\\ &{\\quad\\stackrel{(\\cdot)}{\\geq}\\left(1-t\\right)\\left(1-\\frac{1}{k}\\right)^{k-\\lfloor t k\\rfloor-1}f\\left({O}\\right)+\\left(\\left(1-\\frac{1}{k}\\right)^{k-\\lfloor t k\\rfloor}-\\left(1-\\frac{1}{k}\\right)^{k}\\right)f\\left({O}\\setminus{Z}\\right)}\\\\ &{\\quad\\quad-\\left(\\left(1+\\frac{1-t}{1-\\frac{1}{k}}\\right)\\left(1-\\frac{1}{k}\\right)^{k-\\lfloor t k\\rfloor}-\\left(2-\\frac{1}{k}\\right)\\left(1-\\frac{1}{k}\\right)^{k-1}\\right)f\\left({O}\\cup{Z}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ge(1-t)\\left(1-\\frac1k\\right)e^{t-1}f\\left(O\\right)+\\left(\\left(1-\\frac1k\\right)^{2}e^{t-1}-e^{-1}\\right)f\\left(O\\right)\\le2}\\\\ &{\\qquad-\\left(\\left(1+\\frac1{1-\\frac{t}{1}}\\right)e^{t-1}-\\left(2-\\frac1k\\right)e^{-1}\\right)f\\left(O\\cup Z\\right)\\qquad\\qquad\\mathrm{(nomegaivity.Lemma~A.4)}}\\\\ &{\\ge(1-t)\\left(1-\\frac1k\\right)e^{t-1}f\\left(O\\right)+\\left(\\left(1-\\frac1k\\right)^{2}e^{t-1}-e^{-1}\\right)\\left(f\\left(O\\right)-f\\left(O\\cap Z\\right)\\right)}\\\\ &{\\qquad-\\left(\\left(1+\\frac{1-t}{1-\\frac1k}\\right)e^{t-1}-\\left(2-\\frac1k\\right)e^{-1}\\right)f\\left(O\\cup Z\\right)\\qquad\\qquad\\qquad\\qquad\\mathrm{(submodularity)}}\\\\ &{=\\left(\\left(2-t-\\frac1k\\right)\\left(1-\\frac1k\\right)e^{t-1}-e^{-1}\\right)f\\left(O\\right)-\\left(\\left(1-\\frac1k\\right)^{2}e^{t-1}-e^{-1}\\right)f\\left(O\\cap Z\\right)}\\\\ &{\\qquad-\\left(\\left(1+\\frac{1-t}{1-\\frac1k}\\right)e^{t-1}-\\left(2-\\frac1k\\right)e^{-1}\\right)f\\left(O\\cup Z\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "table", "img_path": "cgiOX8lfwG/tmp/46c2a07a8542dd08a840f7f81fd0efe7e43722f32d3dbb6e293a71080a53c04a.jpg", "table_caption": ["C.2.2 GUIDEDRG under Matroid Constraints "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Discussion about Intuition behind GUIDEDRG under Matroid Constraints. The pseudocode for RANDOMGREEDY under matroid constraints is provided in Alg. 7. To deal with the feasibility for matroid constraints, Alg. 7 starts with an arbitrary basis and builds the solution by randomly swapping the elements in ground set with a candidate basis. The analysis of it proceeds according to two main recurrences. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1)\\;\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\right]\\geq\\displaystyle\\frac{1}{k}\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)-2f\\left(A_{i-1}\\right)\\right],}\\\\ &{2)\\;\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{2}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]+\\displaystyle\\frac{1}{k}\\mathbb{E}\\left[f\\left(O\\right)+f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Fig. 4(a) depicts the worse-case behavior of $\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]$ and $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ . As discussed in Section 2.2, we consider improving the degradation of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ by selecting elements from outside of an $\\left(\\alpha+\\varepsilon,\\alpha\\right)$ -guidance set $Z$ to enhance the lower bound of $\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)\\right]$ . The blue line in Fig. 4(b) illustrated the improvement of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ with an $\\left(\\alpha+\\varepsilon,\\alpha\\right)$ -guidance set. However, restricting the selection only to elements outside $Z$ restricts the increase in $\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]$ to the difference between $\\mathbb{E}\\left[f\\left((O\\setminus Z)\\cup A_{i-1}\\right)\\right]$ and $\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]$ . This restriction is illustrated by the red line in Figure 4(b), indicating degradation in $\\mathbb{E}\\left[f\\left((O\\setminus Z)\\cup A_{i}\\right)\\right]$ . ", "page_idx": 19}, {"type": "text", "text": "To benefit from the improved degradation of $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ , we consider transitioning to selecting elements from the whole ground set at a suitable point. The blue line in Fig. 4(b) illustrates how $\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]$ degrades before and after we switch, and the orange line illustrates the evolution of $\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]$ . Even starting with an inferior selection at the first stage, we still get an overall improvement on the objective value. ", "page_idx": 19}, {"type": "text", "text": "We provide the updated recursions for $f\\left((O\\setminus Z)\\cup A_{i}\\right)$ , $f\\left(O\\cup A_{i}\\right)$ and $f\\left(A_{i}\\right)$ in Lemma C.5 and C.6 below. Then, the closed form of the solution value, derived from these lemmata, is presented in Lemma C.7. After that, we prove the approximation ratio of the randomized algorithm under matroid constraint. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.5. When $0<i\\leq t\\cdot k$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{2}{k}\\right)\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)\\right]+\\displaystyle\\frac{1}{k}\\left[2f\\left(O\\setminus Z\\right)-f\\left(O\\cup Z\\right)\\right],}\\\\ &{\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{2}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]+\\displaystyle\\frac{1}{k}\\left[2f\\left(O\\right)-f\\left(O\\cup Z\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $t\\cdot k<i\\leq k$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\left(1-\\frac{2}{k}\\right)\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]+\\frac{1}{k}f\\left(O\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/a7843d4fb94431d555a4af57b311197b0a7b3a9347f558296ef61fd5049a5948.jpg", "img_caption": ["Figure 4: This set of figures indicates how guiding benefits RANDOMGREEDY under matroid constraints. The figure (a) depicts the evolution of $f\\left(O\\cup A_{i}\\right)$ and $f\\left(A_{i}\\right)$ with RANDOMGREEDY. The figure (b) illustrates how the degradation of $f\\left(O\\cup A_{i}\\right)$ changes as we introduce an $(0.305\\,+$ $\\varepsilon,0.305)$ -guidance set. Additionally, we also need to consider the degradation of $f\\left((O\\setminus Z)\\cup A_{i}\\right)$ , which is the value that the solution approaches with the guidance. The figure (c) shows the updated degradation with a switch point $t k$ , where the algorithm starts with guidance and then switches to running without guidance. It demonstrates that even though the value of $A_{i}$ decreases initially when the selection starts outside of $Z$ , it beneftis from the improved degradation of $f\\left(O\\cup A_{i}\\right)$ upon switching back to the original algorithm. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Proof. When $i\\leq t k$ , it holds that ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{l}{{:[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i}\\right)\\mid A_{i-1}]=\\displaystyle\\frac{1}{k}\\sum_{x\\in M_{i}}f\\left(\\left(O\\setminus Z\\right)\\cup\\left(A_{i-1}+x-\\sigma_{i}(x)\\right)\\right)}}\\\\ {{\\ge\\displaystyle\\frac{1}{k}\\sum_{x\\in M_{i}}[\\Delta(x|(O\\setminus Z)\\cup A_{i-1})+f\\left(\\left(O\\setminus Z\\right)\\cup(A_{i-1}-\\sigma_{i}(x))\\right)]\\qquad\\qquad\\qquad\\mathrm{(submodularity)}}}\\\\ {{\\ge\\displaystyle\\frac{1}{k}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\cup M_{i}\\right)-f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)+\\left(k-1\\right)f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)+f\\left(O\\setminus Z\\right)\\right]}}\\end{array}$ (submodularity)   \n$\\begin{array}{r l r}{\\lefteqn{\\geq\\frac{1}{k}\\left[\\left(k-2\\right)f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)+2f\\left(O\\setminus Z\\right)-f\\left(O\\cup Z\\right)\\right]}}&{\\quad{\\mathrm{(submodularity;~nonnega~}}}\\\\ &{}&{\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\mid A_{i-1}\\right]=\\frac{1}{k}\\sum_{x\\in M_{i}}f\\left(O\\cup\\left(A_{i-1}+x-\\sigma_{i}(x)\\right)\\right)}\\\\ &{}&{\\geq\\frac{1}{k}\\sum_{x\\in M_{i}}\\left[\\Delta(x|O\\cup A_{i-1})+f\\left(O\\cup\\left(A_{i-1}-\\sigma_{i}(x)\\right)\\right)\\right]}\\\\ &{}&{\\geq\\frac{1}{k}\\left[f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)-f\\left(O\\cup A_{i-1}\\right)+(k-1)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\right)\\right]}\\\\ &{}&{\\geq\\frac{1}{k}\\left[\\left(k-2\\right)f\\left(O\\cup A_{i-1}\\right)+2f\\left(O\\cup-f\\left(O\\cup Z\\right)\\right]}\\\\ &{}&{\\mathrm{(submodularity;nonnega}}\\end{array}$ ivity) arity) arity) ivity) ", "page_idx": 20}, {"type": "text", "text": "When $i>t k$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\mid A_{i-1}\\right]=\\frac{1}{k}\\sum_{x\\in M_{i}}f\\left(O\\cup\\left(A_{i-1}+x-\\sigma_{i}(x)\\right)\\right)}\\\\ &{\\displaystyle\\geq\\frac{1}{k}\\sum_{x\\in M_{i}}\\left[\\Delta(x|O\\cup A_{i-1})+f\\left(O\\cup\\left(A_{i-1}-\\sigma_{i}(x)\\right)\\right)\\right]}\\\\ &{\\displaystyle\\geq\\frac{1}{k}\\left[f\\left(O\\cup A_{i-1}\\cup M_{i}\\right)-f\\left(O\\cup A_{i-1}\\right)+(k-1)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\right)\\right]}\\\\ &{\\displaystyle\\geq\\frac{1}{k}\\left[(k-2)f\\left(O\\cup A_{i-1}\\right)+f\\left(O\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By unconditioning $A_{i-1}$ , the lemma is proved. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.6. When $0<i\\leq t\\cdot k$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]-\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\geq\\frac{1}{k}\\left(\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i-1}\\right)\\right]-2\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $t\\cdot k<i\\leq k$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A_{i}\\right)\\right]-\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\geq\\frac{1}{k}\\left(\\mathbb{E}\\left[f\\left(O\\cup A_{i-1}\\right)\\right]-2\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Given $A_{i-1}$ at iteration $i$ . When $i\\leq t\\cdot k$ , since $O$ is a base, $O\\setminus(A_{i-1}\\cup Z)$ with dummy elements is also a base. It holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\mid A_{i-1}\\right]=\\frac{1}{k}\\displaystyle\\sum_{x\\in M_{i}}\\left[f\\left(A_{i-1}+x-\\sigma_{i}(x)\\right)-f\\left(A_{i-1}\\right)\\right]}\\\\ &{\\ge\\frac{1}{k}\\displaystyle\\sum_{x\\in M_{i}}\\left[\\Delta(x|A_{i-1})+f\\left(A_{i-1}-\\sigma_{i}(x)\\right)-f\\left(A_{i-1}\\right)\\right]}&&{\\mathrm{(submodularity)}}\\\\ &{\\ge\\displaystyle\\frac{1}{k}\\displaystyle\\sum_{x\\in O\\backslash\\left(A_{i-1}\\cup Z\\right)}\\Delta(x|A_{i-1})+\\frac{1}{k}\\displaystyle\\sum_{x\\in M_{i}}\\left[f\\left(A_{i-1}-\\sigma_{i}(x)\\right)-f\\left(A_{i-1}\\right)\\right]}&&{\\mathrm{(Line~5in~Alg.\\,6)}}\\\\ &{\\ge\\displaystyle\\frac{1}{k}\\left[f\\left((O\\setminus Z)\\cup A_{i-1}\\right)-f\\left(A_{i-1}\\right)\\right]-\\frac{1}{k}f\\left(A_{i-1}\\right).}&&{\\mathrm{(submodularity)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $i>t\\cdot k$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{E}\\left[f\\left(A_{i}\\right)-f\\left(A_{i-1}\\right)\\mid A_{i-1}\\right]=\\cfrac{1}{k}\\displaystyle\\sum_{x\\in M_{i}}\\left[f\\left(A_{i-1}+x-\\sigma_{i}(x)\\right)-f\\left(A_{i-1}\\right)\\right]}\\\\ &{\\ge\\cfrac{1}{k}\\displaystyle\\sum_{x\\in M_{i}}\\left[\\Delta(x|A_{i-1})+f\\left(A_{i-1}-\\sigma_{i}(x)\\right)-f\\left(A_{i-1}\\right)\\right]}&&{\\mathrm{(submodularity)}}\\\\ &{\\ge\\cfrac{1}{k}\\displaystyle\\sum_{x\\in O}\\Delta(x|A_{i-1})+\\cfrac{1}{k}\\displaystyle\\sum_{x\\in M_{i}}\\left[f\\left(A_{i-1}-\\sigma_{i}(x)\\right)-f\\left(A_{i-1}\\right)\\right]}&&{\\mathrm{(Line~6~in~Alg.~6)}}\\\\ &{\\ge\\cfrac{1}{k}\\left[f\\left(O\\cup A_{i-1}\\right)-f\\left(A_{i-1}\\right)\\right]-\\cfrac{1}{k}f\\left(A_{i-1}\\right).}&&{\\mathrm{(submodularity)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma C.7. With an input matroid constraint $\\mathcal{T}$ and a $((1\\ \\ +\\ \\varepsilon)\\alpha,\\alpha)$ -guidance set $Z$ , GUIDEDRG returns set $A_{k}$ with $O\\left(k n\\right)$ queries, s.t. E [f (Ak)] \u2265 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{r l}&{\\frac{1}{2}\\left(\\frac{1}{2}+\\left(\\frac{3}{2}-t-\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)e^{2(t-1)}-e^{-2}-(1+\\varepsilon)\\alpha\\left(\\left(1-\\frac{2}{k}\\right)^{2}e^{2(t-1)}-e^{-2}\\right)\\right.}\\\\ &{\\left.-\\alpha\\left(\\left(\\frac{1}{2}+\\frac{1-t}{1-\\frac{2}{k}}\\right)e^{2(t-1)}-\\left(\\frac{3}{2}-\\frac{1}{k}\\right)e^{-2}\\right)\\right)f\\left(O\\right).}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. It follows from Lemma C.5 and the closed form for a recurrence provided in Lemma A.3 that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall0<i\\leq t k\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A_{i}\\right)\\right]\\geq f\\left(O\\setminus Z\\right)-\\frac{1}{2}\\left(1-\\left(1-\\frac{2}{k}\\right)^{i}\\right)f\\left(O\\cup Z\\right),}\\\\ &{\\left\\{\\mathbb{E}\\left[f\\left(O\\cup A_{i}\\right)\\right]\\geq\\frac{1}{2}\\left(1+\\left(1-\\frac{2}{k}\\right)^{i-\\lfloor t k\\rfloor}\\right)f\\left(O\\right)-\\frac{1}{2}\\left(\\left(1-\\frac{2}{k}\\right)^{i-\\lfloor t k\\rfloor}-\\left(1-\\frac{2}{k}\\right)^{i}\\right)f\\left(O\\cup Z\\right)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, by sovling the recursion in Lemma C.6 with the above inequalities, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathit{\\Sigma}_{\\mathbf{\\hat{\\tau}}}^{{\\left[f\\left(A_{\\lfloor t k\\rfloor}\\right)\\right]}}\\stackrel{{(a)}}{\\geq}\\frac{1}{2}\\left[1-\\left(1-\\frac{2}{k}\\right)^{\\lfloor t k\\rfloor}\\right]f\\left({O}\\setminus{Z}\\right)-\\left[\\frac{1}{4}-\\frac{1}{4}\\left(1-\\frac{2}{k}\\right)^{\\lfloor t k\\rfloor}-\\frac{t}{2}\\left(1-\\frac{2}{k}\\right)^{\\lfloor t k\\rfloor-1}\\right]f\\left({O}\\right)}}\\\\ {{\\displaystyle{\\mathit{\\Sigma}}_{\\mathbf{\\hat{\\tau}}}^{{\\left[f\\left(A_{k}\\right)\\right]}}\\stackrel{{(b)}}{\\geq}\\left(1-\\frac{2}{k}\\right)^{k-\\lfloor t k\\rfloor}{\\mathbb{E}}\\left[f\\left(A_{\\lfloor t k\\rfloor}\\right)\\right]+\\frac{1}{2}\\left[\\frac{1}{2}+\\left(\\frac{1}{2}-t+\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)^{k-\\lfloor t k\\rfloor-1}\\right]f({O})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle-\\,\\frac{1-t}{2}\\left[\\left(1-\\frac{2}{k}\\right)^{k-\\lfloor t k\\rfloor-1}-\\left(1-\\frac{2}{k}\\right)^{k-1}\\right]f\\left(\\mathcal{O}\\cup\\mathcal{Z}\\right)}}\\\\ {{\\displaystyle\\ge\\frac{1}{2}\\left[\\left(\\left(1-\\frac{2}{k}\\right)^{k-\\lfloor t k\\rfloor}-\\left(1-\\frac{2}{k}\\right)^{k}\\right)f\\left(\\mathcal{O}\\setminus\\mathcal{Z}\\right)+\\left(\\frac{1}{2}+\\left(\\frac{1}{2}-t+\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)^{k-\\lfloor t k\\rfloor-1}\\right)f(\\mathcal{O}\\cup\\mathcal{Z})\\right]}}\\\\ {{\\displaystyle~~~~-\\left(\\left(\\frac{1}{2}+\\frac{1-t}{1-\\frac{2}{k}}\\right)\\left(1-\\frac{2}{k}\\right)^{k-\\lfloor t k\\rfloor}-\\left(\\frac{3}{2}-\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)^{k-1}\\right)\\right]f\\left(\\mathcal{O}\\cup\\mathcal{Z}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\frac{1}{2}\\left[\\left(\\left(1-\\frac{2}{k}\\right)^{(1-\\varepsilon)k+1}-\\left(1-\\frac{2}{k}\\right)^{k}\\right)f\\left(\\mathcal{O}\\setminus\\mathcal{Z}\\right)+\\left(\\frac{1}{2}+\\left(\\frac{1}{2}-t+\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)^{(1-\\varepsilon)k}\\right)f(\\mathcal{O})\\right.}\\\\ &{\\qquad-\\left(\\left(\\frac{1}{2}+\\frac{1}{1-\\frac{\\varepsilon}{2}}\\right)\\left(1-\\frac{2}{k}\\right)^{(1-\\varepsilon)k}-\\left(\\frac{3}{2}-\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)^{k-1}\\right)\\right]f\\left(\\mathcal{O}\\cup\\mathcal{Z}\\right)}\\\\ &{\\qquad\\qquad\\left.\\mathrm{~(}t k-1\\leq\\frac{1}{k}\\right)\\left[\\left(\\left(1-\\frac{2}{k}\\right)^{2}e^{2\\left(t-1\\right)}-e^{-2}\\right)f\\left(\\mathcal{O}\\setminus\\mathcal{Z}\\right)+\\left(\\frac{1}{2}+\\left(\\frac{1}{2}-t+\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)e^{\\left(t-1\\right)}\\right)f(\\mathcal{O})\\right.}\\\\ &{\\qquad\\qquad-\\left(\\left(\\frac{1}{2}+\\frac{1}{1-\\frac{\\varepsilon}{2}}\\right)e^{2\\left(t-1\\right)}-\\left(\\frac{3}{2}-\\frac{1}{k}\\right)e^{-2}\\right)f\\left(\\mathcal{O}\\cup\\mathcal{Z}\\right)\\right]\\qquad\\mathrm{(unomequivity.}\\mathrm{Lemma\\iA})}\\\\ &{\\geq\\frac{1}{2}\\left[\\left(\\frac{1}{2}+\\left(\\frac{3}{2}-t-\\frac{1}{k}\\right)\\left(1-\\frac{2}{k}\\right)e^{2\\left(t-1\\right)}-e^{-2}\\right)f\\left(\\mathcal{O}\\right)-\\left(\\left(1-\\frac{2}{k}\\right)^{2}e^{2\\left(t-1\\right)}-e^{-2}\\right)f\\left(\\mathcal{O}\\cap\\mathcal{Z}\\right)\\right]}\\\\ &{\\qquad\\qquad\\left.-\\left(\\left(\\frac{1}{2}+\\frac{1}{1-\\frac{\\varepsilon}{2}}\\right)e^{2\\left(t-1\\right)}-\\left(\\frac{3}{2}-\\frac{1}{k}\\right)e^{-2}\\right)f\\left(\\mathcal{O}\\cup\\mathcal{Z}\\right)\\right],}\\end{\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where Inequality $(a)$ and $(b)$ follow from Inequality 6, Lemma C.6 and A.3. ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 2.1 under matroid constraint. Let $(f,\\mathcal{T})$ be an instance of SM, with optimal solution set $O$ . If $f\\left(Z\\right)\\geq\\left(0.305-\\varepsilon\\right)f\\left(O\\right)$ under matroid constraint, the approximation ratio holds immediately. Otherwise, by Corollary C.1, FASTLS returns a set $Z$ which is an $((1\\!+\\!\\varepsilon)\\alpha,\\alpha)$ -guidance set, where $\\alpha=0.305-\\varepsilon$ . ", "page_idx": 22}, {"type": "text", "text": "By Lemma C.7 and $Z$ is an $((1+\\varepsilon)\\alpha,\\alpha)$ -guidance set with $\\alpha=0.305-\\varepsilon$ , $\\begin{array}{r l r}&{\\frac{1}{2}[f\\left(A_{k}\\right)]\\geq\\frac{1}{2}\\left[\\frac{1}{2}+\\left(\\frac{3}{2}-t-\\varepsilon\\right)(1-2\\varepsilon)e^{2(t-1)}-e^{-2}-(0.305-0.695\\varepsilon)\\left(\\left(1-2\\varepsilon\\right)^{2}e^{2(t-1)}-1\\right)\\right.}\\\\ &{...\\left.-\\left(0.305-\\varepsilon\\right)\\left(\\left(\\frac{1}{2}+\\frac{1-t}{1-2\\varepsilon}\\right)e^{2(t-1)}-\\left(\\frac{3}{2}-\\varepsilon\\right)e^{-2}\\right)\\right]f\\left(\\boldsymbol{O}\\right)}&{(\\forall k\\ge\\frac{1}{\\varepsilon})}\\\\ &{\\geq(0.305-\\varepsilon)f\\left(\\boldsymbol{O}\\right).\\qquad}&{(t=0.559)}\\end{array}$ e\u22122 ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "D Analysis of Deterministic Approximation Algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we present the pseudocode of deterministic algorithm and its analysis. The organization of this section is as follows: firstly, in Appendix D.1, we introduce the deterministic subroutine, GUIDEDIG-S and GUIDEDIG-M, along with their analysis; then, in Appendix D.2, we provide a randomized version of the deterministic algorithm for analytical purposes; finally, in Appendix D.3, we provide the deterministic algorithm and its theoretical guarantee. ", "page_idx": 22}, {"type": "text", "text": "D.1 Deterministic Subroutines - GUIDEDIG-S and GUIDEDIG-M ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Inspired by INTERLACEGREEDY algorithm, a subroutine of INTERPOLATEDGREEDY, proposed by Chen and Kuhnle [11], we introduce guided versions of it for both size and matroid constraints. ", "page_idx": 22}, {"type": "text", "text": "1 Procedure GUIDEDIG-M $(f,\\mathcal{T}(\\mathcal{M}),Z,G,\\ell)$ :   \n2 Input: oracle $f$ , matroid constraint $\\mathcal{M}$ , guidance set $Z$ , starting set $G$ , set size $\\ell$   \n3 Initialize: $A,A_{1},\\ldots,A_{\\ell}\\leftarrow\\emptyset$   \n4 for $i\\gets1$ to $k$ do   \n5 $X_{i}\\leftarrow\\{x\\in\\mathcal{U}\\setminus(G\\cup A\\cup Z):A+x\\in\\mathcal{Z}(\\mathcal{M})\\}$   \n6 $j_{i}^{*},a_{i}^{*}\\gets\\arg\\operatorname*{max}_{j\\in[\\ell],x\\in X}\\Delta(x|G\\cup A_{j})$   \n7 A \u2190A + ai\u2217 , $A_{j_{i}^{*}}\\gets A_{j_{i}^{*}}+a_{i}^{*}$   \n8 end   \n9 $\\sigma\\gets\\mathbf{a}$ bijection from $G$ to $A$ s.t. $\\textstyle(G\\cup A_{j})\\setminus\\left(\\sum_{x\\in A_{j}}\\sigma^{-1}(x)\\right)$ is a basis   \n0 return $\\begin{array}{r}{\\left\\{(G\\cup A_{j})\\setminus\\left(\\sum_{x\\in A_{j}}\\sigma^{-1}(x)\\right):1\\leq j\\leq\\ell\\right\\}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "The algorithm for the size constraint closely resembles INTERLACEGREEDY in Chen and Kuhnle [11]. Hence, we provide the pseudocode (Alg. 9), guarantees, and analysis in Appendix D.1.1. In this section, we focus on presenting GUIDEDIG-M for matroid constraints as Alg. 8. This algorithm addresses the feasibility issue by incorporating INTERLACEGREEDY into matroid constraints. Moreover, while it compromises the approximation ratio over size constraint to some extent, it no longer has the drawback of low success probability, which the size-constrained version has. ", "page_idx": 23}, {"type": "text", "text": "Algorithm overview. Under size constraints, INTERPOLATEDGREEDY [11] constructs the solution with $\\ell$ iterations, where each iteration involves calling the INTERLACEGREEDY subroutine and adding $k/\\ell$ elements into the solution. However, this approach is not applicable to matroid constaint due to the feasibility problem. Consequently, we propose GUIDEDIG-M for matroid constraints designed as follows: 1) consider adding a basis ( $k$ elements) $A$ to $\\ell$ solution sets, where each addition dominates the gain of a distinct element in $O;2$ ) by exchange property, establish a bijection between the basis $A$ and the starting set $G$ ; 3) delete elements in each solution set that are mapped to by the basis $A$ . This procedure avoids the extensive guessing of GUIDEDIG-S for size constraints and reduces the number of potential solutions from $\\ell(\\ell+1)$ to $\\ell$ . We provide the theoretical guarantees and the detailed analysis below. ", "page_idx": 23}, {"type": "text", "text": "Lemma D.1. Let $O\\in\\mathcal{T}(\\mathcal{M})$ , and suppose GUIDEDIG-M(Alg. 8) is called with $(f,\\mathcal{M},Z,G,\\ell)$ , where $Z\\cap G=\\emptyset$ . Then GUIDEDIG-M outputs $\\ell$ candidate sets with $\\mathcal{O}\\left(\\ell k n\\right)$ queries. Moreover, a randomly selected set $G^{\\prime}$ from the output satisfies that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{)}\\,\\mathbb{E}\\left[f(G^{\\prime})\\right]\\geq\\left(1-\\displaystyle\\frac{2}{\\ell}\\right)f(G)+\\frac{1}{\\ell+1}\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f((O\\setminus Z)\\cup G);}\\\\ &{\\mathrm{)}\\,\\mathbb{E}\\left[f(O\\cup G^{\\prime})\\right]\\geq\\left(1-\\displaystyle\\frac{2}{\\ell}\\right)f(O\\cup G)+\\frac{1}{\\ell}\\left(f(O)+f\\left(O\\cup(Z\\cap G)\\right)-f(O\\cup Z)\\right);}\\\\ &{\\mathrm{)}\\,\\mathbb{E}\\left[f\\left((O\\setminus Z)\\cup G^{\\prime}\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{2}{\\ell}\\right)f((O\\setminus Z)\\cup G)+\\frac{1}{\\ell}\\left(f(O\\setminus Z)+f\\left((O\\setminus Z)\\cup(Z\\cap G)\\right)-f(O\\setminus Z)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. $A=\\{a_{1}^{*},a_{2}^{*},\\ldots,a_{k}^{*}\\}$ be the sequence with the order of elements being added. Since $A$ and $O\\backslash Z$ are basis of $\\mathcal{M}$ (by adding dummy elements into $O\\backslash Z)$ , we can order $O\\backslash Z=\\{o_{1},o_{2},...\\,,o_{k}\\}$ s.t. for any $1\\leq i\\leq k$ , $\\{a_{1}^{*},\\ldots,a_{i-1}^{*},o_{i}\\}$ is an independent set. Thus, $o_{i}\\in X_{i}$ . Let $A_{j}^{(i)}$ be $A_{j}$ after $i$ -th iteration. Therefore, for any $1\\leq j\\leq\\ell$ , by submodularity, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta(o_{i}|G\\cup A_{j})\\leq\\Delta\\Bigl(o_{i}|G\\cup A_{j}^{(i-1)}\\Bigr)\\leq\\Delta\\Bigl(a_{i}^{*}|G\\cup A_{j_{i}^{*}}^{(i-1)}\\Bigr)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Rightarrow f\\left(\\left(O\\setminus Z\\right)\\cup G\\cup A_{j}\\right)-f\\left(G\\cup A_{j}\\right)\\leq\\sum_{i=1}^{k}\\Delta(o_{i}|G\\cup A_{j})\\leq\\sum_{i=1}^{k}\\Delta\\left(a_{i}^{*}|G\\cup A_{j_{i}^{*}}^{(i-1)}\\right)=\\sum_{l=1}^{\\ell}\\Delta\\left(A_{l}|G\\cup A_{j_{l}^{*}}^{(l)}\\right)=\\sum_{l=1}^{k}\\Delta\\left(a_{l}|G\\cup A_{j_{l}^{*}}^{(l)}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By summing up the above inequality with $1\\leq j\\leq\\ell$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\ell+1)\\sum_{j=1}^{\\ell}\\Delta(A_{j}|G)\\geq\\sum_{j=1}^{\\ell}f\\left((O\\setminus Z)\\cup G\\cup A_{j}\\right)-\\ell f(G)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\geq(\\ell-1)f\\left((O\\setminus Z)\\cup G\\right)+f\\left((O\\setminus Z)\\cup G\\cup A\\right)-\\ell f(G)}\\\\ &{\\geq(\\ell-1)f\\left((O\\setminus Z)\\cup G\\right)-\\ell f(G)\\qquad\\qquad\\qquad\\qquad(\\mathrm{nonne}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we can prove the first inequality as follows, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left[f(G^{\\prime})-f(G)\\right]=\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\left(f\\left(G\\setminus\\sigma^{-1}(A_{j})\\cup A_{j}\\right)-f(G)\\right)}\\\\ {\\displaystyle\\geq\\frac{1}{\\ell}\\sum_{j=1}^{\\ell}\\left(\\Delta(A_{j}|G)+f\\left(G\\setminus\\sigma^{-1}(A_{j})\\right)-f(G)\\right)}\\\\ {\\displaystyle\\geq\\frac{1}{\\ell+1}\\left(1-\\frac{1}{\\ell}\\right)f\\left((O\\setminus Z)\\cup G\\right)-\\frac{1}{\\ell+1}f(G)-\\frac{1}{\\ell}f(G)}\\\\ {\\displaystyle\\Rightarrow\\ \\ \\ \\ \\ \\ \\ \\ \\mathbb{E}\\left[f(G^{\\prime})\\right]\\geq\\left(1-\\frac{2}{\\ell}\\right)f(G)+\\frac{1}{\\ell+1}\\left(1-\\frac{1}{\\ell}\\right)f\\left((O\\setminus Z)\\cup G\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By submodularity, nonnegativity, and $Z\\cap A=\\emptyset$ , the last two inequalities can be proved as follows, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[f\\left(O\\cup G^{\\prime}\\right)\\right]=\\cfrac{1}{\\ell}\\cfrac{\\ell}{\\int\\!\\!\\!-1}\\,f\\left(O\\cup\\left(G\\setminus\\sigma^{-1}(A_{j})\\cup A_{j}\\right)\\right)}&{}\\\\ {\\geq\\cfrac{1}{\\ell}\\cfrac{\\ell}{\\int\\!\\!\\!-1}\\,\\left[\\Delta(A_{j}|O\\cup G)+f\\left(O\\cup\\left(G\\setminus\\sigma^{-1}(A_{j})\\right)\\right)\\right]}&{}\\\\ {\\geq\\cfrac{1}{\\ell}\\left(f(O\\cup G\\cup A)-f(O\\cup G)+(\\ell-1)f(O\\cup G)+f(O)\\right)}&{}\\\\ {\\geq\\left(1-\\cfrac{2}{\\ell}\\right)f(O\\cup G)+\\cfrac{1}{\\ell}\\left(f(O)+f\\left(O\\cup(Z\\cap G)\\right)-f(O\\cup Z)\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\div[f\\left(\\left(O\\setminus Z\\right)\\cup G^{\\prime}\\right)]=\\frac{1}{\\ell}\\displaystyle\\sum_{j-1}^{\\ell}f\\left(\\left(O\\setminus Z\\right)\\cup\\left(G\\setminus\\sigma^{-1}(A_{j})\\cup A_{j}\\right)\\right)}&{}\\\\ {\\qquad\\qquad\\qquad\\geq\\frac{1}{\\ell}\\displaystyle\\sum_{j-1}^{\\ell}\\left[\\Delta(A_{j}|(O\\setminus Z)\\cup G)+f\\left(\\left(O\\setminus Z\\right)\\cup\\left(G\\setminus\\sigma^{-1}(A_{j})\\right)\\right)\\right]}&{}\\\\ {\\qquad\\qquad\\geq\\frac{1}{\\ell}\\left(f((O\\setminus Z)\\cup G\\cup A)-f((O\\setminus Z)\\cup G)+(\\ell-1)f((O\\setminus Z)\\cup G)+f((O\\setminus Z)\\cup G)\\right)}&{}\\\\ {\\qquad\\qquad\\geq\\left(1-\\frac{2}{\\ell}\\right)f((O\\setminus Z)\\cup G)+\\frac{1}{\\ell}\\left(f(O\\setminus Z)+f\\left((O\\setminus Z)\\cup(Z\\cap G)\\right)-f(O\\setminus Z)\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.1.1 GUIDEDIG-S and its Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide the pseudocode, guarantees and analysis of GUIDEDIG-S, which highly resembles INTERLACEGREEDY in Chen and Kuhnle [11]. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.2. Let $O\\subseteq\\,\\mathcal{U}$ be any set of size at most $k$ , and suppose GUIDEDIG-S(Alg. 9) is called with $(f,k,Z,G,\\ell)$ . Then GUIDEDIG-S outputs $\\ell(\\ell+1)$ candidate sets with $\\mathcal{O}\\left(\\ell k n\\right)$ queries. Moreover, with a probability of $(\\ell+1)^{-1}$ , a randomly selected set $A$ from the output satisfies that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1)\\;\\mathbb{E}\\left[f\\left(A\\right)\\right]\\geq\\displaystyle\\frac{1}{\\ell+1}\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]+\\displaystyle\\frac{\\ell}{\\ell+1}f\\left(G\\right);}\\\\ &{2)\\;\\mathbb{E}\\left[f(O\\cup A)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f(O\\cup G)+\\displaystyle\\frac{1}{\\ell}\\left(f\\left(O\\cup\\left(Z\\cap G\\right)\\right)-f\\left(O\\cup Z\\right)\\right)}\\\\ &{3)\\;\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}(f\\left(\\left(O\\setminus Z\\right)\\cup\\left(Z\\cap G\\right)\\right)-f(O\\cup Z)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Algorithm 9: A $({\\approx}\\ell)$ -approximation that interlaces $\\ell$ greedy procedures together and uses only $1/\\bar{\\ell}$ fraction of the budget. ", "page_idx": 25}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/5af9cde81a8f2654469991689f69a29edddfcac792e301f369958f9f1fad3346.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Proof. Let $o_{\\operatorname*{max}}\\,=\\,\\arg\\operatorname*{max}_{o\\in O\\setminus(G\\cup Z)}\\Delta(o|G)$ , and let $\\{a_{1},\\ldots,a_{\\ell}\\}$ be the largest $\\ell$ elements of $\\{\\Delta(x|G):x\\in\\mathcal{U}\\setminus(G\\cup Z)\\}$ , as chosen on Line 3. We consider the following two cases. ", "page_idx": 25}, {"type": "text", "text": "Case $(O\\setminus(G\\cup Z))\\cap\\{a_{1},\\dots,a_{\\ell}\\}=\\emptyset.$ . Then, $\\cal O_{\\mathrm{max}}\\not\\in\\{a_{1},\\ldots,a_{\\ell}\\}$ which implies that $\\Delta(a_{u}|G)\\ge$ $\\Delta(o|G)$ , for every $1\\leq u\\leq\\ell$ and $o\\in O\\setminus(G\\cup Z)$ ; and, after the first iteration of the for loop on Line 10 of Alg. 9, none of the elements in $O\\setminus(G\\cup Z)$ is added into any of $\\{A_{0,i}\\}_{i=1}^{\\ell}$ . We will analyze the iteration of the for loop on Line 4 with $u=0$ . ", "page_idx": 25}, {"type": "text", "text": "Since none of the elements in $O\\setminus(G\\cup Z)$ is added into the collection when $j=0$ , we can order $O\\setminus(G\\cup Z)\\,=\\,\\{o_{1},o_{2},..\\,.\\}$ such that the first $\\ell$ elements are not selected in any set before we get to $j\\,=\\,1$ , the next $\\ell$ elements are not selected in any set before we get to $j\\,=\\,2$ , and so on. Let $i\\,\\in\\,\\{1,\\ldots,\\ell\\}$ . Let $A_{0,i}^{j}$ be the value of $A_{0,i}$ after $j$ elements are added into it, and define $A_{0,i}=A_{0,i}^{k/\\ell}$ , the final value. Finally, denote by $\\delta_{j}$ the value $\\Delta\\Big(x_{j,i}|A_{0,i}^{j}\\Big)$ . Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n(G\\subseteq A_{0,i})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f\\left(\\left(O\\setminus Z\\right)\\cup A_{0,i}\\right)-f\\left(A_{0,i}\\right)\\leq\\underset{o\\in\\mathcal{O}\\backslash(A_{0,i}\\cup Z)}{\\sum}\\Delta(o|A_{0,i})}\\\\ &{\\leq\\underset{o\\in\\mathcal{O}\\backslash(G\\cup Z)}{\\sum}\\Delta(o|A_{0,i})}\\\\ &{\\leq\\underset{l=1}{\\overset{\\ell}{\\sum}}\\Delta\\big(o_{l}|A_{0,i}^{0}\\big)+\\underset{l=\\ell+1}{\\overset{2\\ell}{\\sum}}\\Delta\\big(o_{l}|A_{0,i}^{1}\\big)+\\dots}\\\\ &{\\leq\\underset{j=1}{\\overset{k/\\ell}{\\sum}}\\delta_{j}=\\ell(f\\left(A_{0,i}\\right)-f\\left(G\\right)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from the ordering of $O$ and the selection of elements into the sets. By summing up the above inequality with all $1\\leq i\\leq\\ell$ , it holds that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[f(A)\\right]=\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f\\left(A_{0,i}\\right)\\geq\\frac{1}{\\ell(\\ell+1)}\\sum_{i=1}^{\\ell}f((O\\setminus Z)\\cup A_{0,i})+\\frac{\\ell}{\\ell+1}f(G)}\\\\ {\\displaystyle=\\frac{1}{\\ell+1}\\mathbb{E}\\left[f\\left((O\\setminus Z)\\cup A\\right)\\right]+\\frac{\\ell}{\\ell+1}f\\left(G\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $A_{0,i_{1}}\\cap A_{0,i_{2}}=G$ for any $1\\leq i_{1}\\neq i_{2}\\leq\\ell,$ , and each $x_{j,i}$ is selected outside of $Z$ , by repeated application of submodularity, it can be shown that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]=\\displaystyle\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f(\\left(O\\setminus Z\\right)\\cup A_{0,i})}\\\\ &{\\quad\\ge\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}f\\left(\\left(O\\setminus Z\\right)\\cup\\left(\\bigcup_{i=1}^{\\ell}A_{0,i}\\right)\\right)}\\\\ &{\\quad\\ge\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}(f\\left(\\left(O\\setminus Z\\right)\\cup\\left(Z\\cap G\\right))-f(O\\cup Z)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(O\\cup A\\right)\\right]=\\displaystyle\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f(O\\cup A_{0,i})\\geq\\left(1-\\frac{1}{\\ell}\\right)f\\left(O\\cup G\\right)+\\frac{1}{\\ell}f\\left(O\\cup\\left(\\cup_{i=1}^{\\ell}A_{0,i}\\right)\\right)}\\\\ &{\\quad\\geq\\left(1-\\frac{1}{\\ell}\\right)f\\left(O\\cup G\\right)+\\frac{1}{\\ell}\\left(f\\left(O\\cup\\left(Z\\cap G\\right)\\right)-f\\left(O\\cup Z\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, if we select a random set from $\\{A_{0,i}:1\\leq i\\leq\\ell\\}$ , the three inequalities in the Lemma hold and we have probability $1/(\\ell+1)$ of this happening. ", "page_idx": 26}, {"type": "text", "text": "Case $(O\\setminus(G\\cup Z))\\cap\\{a_{1},\\dots,a_{\\ell}\\}\\ \\neq\\ \\emptyset$ . Then $\\sigma_{\\operatorname*{max}}\\ \\in\\ \\{a_{1},\\ldots,a_{\\ell}\\}$ , so $a_{u}\\mathrm{~=~}o_{\\mathrm{max}}$ , for some $u\\in{1,\\ldots,\\ell}$ . We analyze the iteration $u$ of the for loop on Line 4. Similar to the previous case, let $i\\in\\{1,\\ldots,\\ell\\}$ , define $A_{u,i}^{j}$ be the value of $A_{u,i}$ after we add $j$ elements into it, and we will use $A_{u,i}$ for $A_{u,i}^{k/\\ell}$ , Also, let $\\delta_{j}=\\Delta\\Big(x_{j,i}|A_{u,i}^{j-1}\\Big)$ . Finally, let $x_{1,i}=a_{u}$ and observe $A_{u,i}^{1}=G\\cup\\{a_{u}\\}$ , for any $i\\in\\{1,\\ldots,\\ell\\}$ . ", "page_idx": 26}, {"type": "text", "text": "Then, we can order $O\\setminus G=\\{o_{1},o_{2},...\\}$ such that: 1) for the first $\\ell$ elements $\\{o_{l}\\}_{l=1}^{\\ell}$ , $\\Delta(o_{l}|G)\\le$ $\\Delta(o_{\\mathrm{max}}|G)=\\delta_{1};2)$ the next $\\ell$ elements $\\{o_{l}\\}_{l=\\ell+1}^{2\\ell}$ are not selected by any set before we get to $j=2$ , which implies that $\\Delta(o_{l}|A_{u,i}^{1})\\leq\\delta_{2}$ , and so on. Therefore, analagous to the the previous case, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(A\\right)\\right]\\geq{\\frac{1}{\\ell+1}}\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]+{\\frac{\\ell}{\\ell+1}}f\\left(G\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since, $A_{u,i_{1}}\\cap A_{u,i_{2}}=G\\cup\\{a_{u}\\}$ for any $1\\leq i_{1}\\neq i_{2}\\leq\\ell,a_{u}\\in O\\setminus Z$ , and each $x_{j,i}$ is selected outside of $Z$ , by submodularity and nonnegativity of $f$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]=\\displaystyle\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f(\\left(O\\setminus Z\\right)\\cup A_{u,i})}\\\\ &{\\quad\\ge\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}f\\left(\\left(O\\setminus Z\\right)\\cup\\left(\\bigcup_{i=1}^{\\ell}A_{u,i}\\right)\\right)}\\\\ &{\\quad\\ge\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}(f\\left(\\left(O\\setminus Z\\right)\\cup\\left(Z\\cap G\\right))-f(O\\cup Z))}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(O\\cup A\\right)\\right]=\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f(O\\cup A_{u,i})\\geq\\left(1-\\frac{1}{\\ell}\\right)f\\left(O\\cup G\\right)+\\frac{1}{\\ell}f\\left(O\\cup\\left(\\cup_{i=1}^{\\ell}A_{u,i}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\geq\\left(1-{\\frac{1}{\\ell}}\\right)f\\left(O\\cup G\\right)+{\\frac{1}{\\ell}}\\left(f\\left(O\\cup\\left(Z\\cap G\\right)\\right)-f\\left(O\\cup Z\\right)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, if we select a random set from $\\{A_{u,i}:1\\leq i\\leq\\ell\\}$ , the three inequalities in the lemma holds, and this happens with probability $(\\ell+1)^{-1}$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D.2 Randomized Version of our Deterministic Algorithm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide the randomized version (Alg 10) of our deterministic algorithm (Alg. 11, provided in Appendix D). The deterministic version simply evaluates all possible paths and returns the best solution. In the following, we provide the theoretical guarantee and its analysis under different constraints. ", "page_idx": 26}, {"type": "text", "text": "1 Input: oracle $f$ , constraint $\\mathcal{T}$ , an approximation result $Z_{0}$ , switch point $t$ , error rate $\\varepsilon$   \n2 $Z\\gets\\mathrm{FASTLS}(f,\\mathcal{T},Z_{0},\\varepsilon)$   \n3 Initialize $\\begin{array}{r}{\\ell\\gets\\frac{10}{9\\varepsilon},A_{0}\\gets\\emptyset}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "4 if $\\mathcal{T}$ is a size constraint then 5 for $i\\gets1$ to $\\ell$ do 6 if $i\\leq t\\ell$ then $A_{i}\\leftarrow\\mathbf{a}$ random set in GUIDEDIG- $\\mathrm{S}(f,\\mathcal{T},Z,A_{i-1},\\ell)$ 7 $\\mathbf{else}A_{i}\\gets\\mathbf{a}$ random set in GUIDEDIG- $\\mathbf{S}(f,\\mathcal{T},\\emptyset,A_{i-1},\\ell)$   \n8 end   \n9 else   \n10 for $i\\gets1$ to $\\ell$ do   \n11 if $i\\leq t\\ell$ then $A_{i}\\leftarrow\\mathbf{a}$ random set in GUIDEDIG- $\\mathbf{\\nabla}\\cdot\\mathbf{M}(f,\\mathcal{T},Z,A_{i-1},\\ell)$   \n12 else $A_{i}\\leftarrow\\mathbf{a}$ random set in GUIDEDIG- $\\mathsf{M}(f,\\mathcal{T},\\emptyset,A_{i-1},\\ell)$   \n13 end   \n14 end   \n15 return $A^{*}\\gets\\arg\\operatorname*{max}\\{f(Z),f(A_{\\ell})\\}$ ", "page_idx": 27}, {"type": "text", "text": "Theorem D.3. Let $(f,\\mathcal{T})$ be an instance of SM, with the optimal solution set $O$ . Algorithm 10 achieves an expected $(0.385-\\varepsilon)$ approximation ratio with $(\\ell+1)^{-\\ell}$ success probability and input $t\\,=\\,0.372$ under size constraint, where $\\begin{array}{r}{\\ell\\,=\\,\\frac{10}{9\\varepsilon}}\\end{array}$ . Moreover, it achieves an expected $\\left(0.305-\\varepsilon\\right)$ approximation ratio with $t=0.559$ under matroid constraint. The query complexity of the algorithm is ${\\bar{O}}\\left(k n/\\varepsilon\\right)$ . ", "page_idx": 27}, {"type": "text", "text": "D.2.1 Size constraints ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "By Lemma D.2 in Appendix D.1.1 and the closed form for a recurrence provided in Lemma A.3, the following corollary holds, ", "page_idx": 27}, {"type": "text", "text": "Corollary D.1. After iteration $i$ of the for loop in Alg. 10, the following inequalities hold with a probability of $(\\ell+1)^{-i}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{!}{=}\\left[f\\left(A_{i}\\right)\\right]\\geq\\frac{\\ell}{\\ell+1}\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]+\\frac{1}{\\ell+1}\\left(f\\left(O\\setminus Z\\right)-\\left(1-\\left(1-\\frac{1}{\\ell}\\right)^{i}\\right)f\\left(O\\cup Z\\right)\\right),}\\\\ &{\\overset{\\because}{=}\\left[f\\left(A_{i}\\right)\\right]\\geq\\frac{\\ell}{\\ell+1}\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]+\\frac{1}{\\ell+1}\\left(1-\\frac{1}{\\ell}\\right)^{i-\\lfloor\\ell\\ell\\rfloor}\\left(f(O)-\\left(1-\\left(1-\\frac{1}{\\ell}\\right)^{\\lfloor\\ell\\ell\\rfloor}\\right)f\\left(O\\cup Z\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of approximation ratio. If $f\\left(Z\\right)\\ge\\left(0.385-\\varepsilon\\right)f\\left(O\\right)$ , the approximation ratio holds immediately. So, we analyze the case $f\\left(Z\\right)<(0.385-\\varepsilon)f\\left(O\\right)$ in the following. ", "page_idx": 27}, {"type": "text", "text": "Recall in Corollary C.1 that $Z$ is a $(1+\\varepsilon)\\alpha,\\alpha)$ -guidance set, it holds that $f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)<$ $(0.77-1.615\\varepsilon)\\dot{f}\\left(O\\right)$ and $f\\left(O\\cap\\mathcal{Z}\\right)<\\left(0.385-0.615\\varepsilon\\right)f\\left(O\\right)$ . ", "page_idx": 27}, {"type": "text", "text": "By repeatedly implementing Lemma A.3 with the recursion in Corollary D.1, it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle\\mathrm{\\boldmath~\\hat{\\Pi}~}[f\\left(A_{\\lfloor t\\ell\\rfloor}\\right)]\\ge\\left(1-\\left(1-\\frac{1}{\\ell+1}\\right)^{\\lfloor t\\ell\\rfloor}\\right)\\left(f\\left(O\\setminus Z\\right)-f\\left(O\\cup Z\\right)\\right)+\\frac{\\displaystyle\\lfloor t\\ell\\rfloor}{\\displaystyle\\ell+1}\\left(1-\\frac{1}{\\ell}\\right)^{\\lfloor t\\ell\\rfloor}f\\left(O\\cup Z\\right)}\\\\ &{\\ge\\left(1-\\left(1-\\frac{1}{\\ell+1}\\right)^{\\lfloor t\\ell\\rfloor}\\right)\\left(f\\left(O\\right)-f\\left(O\\cap Z\\right)-f\\left(O\\cup Z\\right)\\right)+\\frac{\\displaystyle\\lfloor t\\ell\\rfloor}{\\displaystyle\\ell+1}\\left(1-\\frac{1}{\\ell}\\right)^{\\lfloor t\\ell\\rfloor}f\\left(O\\cup Z\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{-}[f\\left(A_{\\ell}\\right)]\\geq\\left(1-\\frac{1}{\\ell+1}\\right)^{\\ell-\\lfloor t\\ell\\rfloor}\\mathbb{E}\\left[f\\left(A_{\\lfloor t\\ell\\rfloor}\\right)\\right]+\\frac{\\ell-\\lfloor t\\ell\\rfloor}{\\ell+1}\\left(1-\\frac{1}{\\ell}\\right)^{\\ell-\\lfloor t\\ell\\rfloor}\\left(f\\left(O\\right)-\\left(1-\\left(1-\\frac{1}{\\ell}\\right)\\right)\\right)}\\\\ &{\\geq\\left(\\left(1-\\frac{1}{\\ell+1}\\right)^{(1-t)\\ell+1}-\\left(1-\\frac{1}{\\ell+1}\\right)^{\\ell}\\right)\\left(f\\left(O\\right)-f\\left(O\\cap Z\\right)-f\\left(O\\cup Z\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad_{5}\\frac{H}{\\varepsilon+1}\\left(1-\\frac{1}{\\varepsilon+1}\\right)^{1(1-\\varepsilon)+1}\\left(1-\\frac{1}{\\varepsilon}\\right)^{9}f(0,0)\\mathcal{Z}}\\\\ &{\\quad+(1-\\frac{1}{\\varepsilon+1})\\left(1-\\frac{1}{\\varepsilon}\\right)^{1(3-\\varepsilon)+1}\\left(f(0,-1)\\left(1-\\left(1-\\frac{1}{\\varepsilon}\\right)^{9-\\varepsilon}\\right)f(0,0)\\right)}\\\\ &{\\quad+\\bigg((1-\\frac{1}{\\varepsilon+1})\\left(1-\\frac{1}{\\varepsilon+1}\\right)^{1(3-\\varepsilon-2)}\\left(1+\\frac{1}{\\varepsilon}\\right)\\left(1-\\frac{1}{\\varepsilon+1}\\right)^{6+3}\\bigg)\\mathcal{O}(\\varepsilon-1;|u|)\\le\\varepsilon0}\\\\ &{\\quad+\\varepsilon\\cdot\\frac{\\varepsilon}{\\varepsilon+1}\\left(1-\\frac{1}{\\varepsilon+1}\\right)\\bigg(1-\\frac{1}{\\varepsilon+1}\\bigg)^{1(3-\\varepsilon)}\\left(1-\\frac{1}{\\varepsilon}\\right)^{16-3}f(0,0)}\\\\ &{\\quad+(1-\\frac{1}{\\varepsilon})\\bigg(1-\\frac{1}{\\varepsilon+1}\\bigg)^{3}f\\left(1-\\frac{1}{\\varepsilon}\\right)^{1(3-\\varepsilon-1)}f(0,0)}\\\\ &{\\quad+(1-\\frac{1}{\\varepsilon})^{2}\\left(1-\\frac{1}{\\varepsilon}\\right)^{2}\\left((1-\\frac{1}{\\varepsilon})^{10-\\varepsilon}\\right)^{1}f(0,0)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}$ \u222aZ)) $\\begin{array}{r l r}&{}&{\\geq\\left[c(c^{2}(1-t)+1)e^{t-1}-e^{-1}\\right]f\\left(O\\right)-\\left[c(c(1-t)+1)e^{t-1}-(c^{3}+1)e^{-1}\\right]\\left(f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)\\right)}\\\\ &{}&{-\\left[c^{3}e^{-1}-c^{2}(1-t)e^{t-1}\\right]f\\left(O\\cap Z\\right)}&{\\mathrm{(Let}\\,c=1-\\frac{1}{10}=1-\\frac{1}{\\ell}\\right)}\\\\ &{}&{\\geq\\left[c(c^{2}(1-t)+1)e^{t-1}-e^{-1}\\right]f\\left(O\\right)-\\left[c(c(1-t)+1)e^{t-1}-(c^{3}+1)e^{-1}\\right]\\left(0.77-1.615\\varepsilon\\right)f\\left(O\\right)}\\\\ &{}&{-\\left[c^{3}e^{-1}-c^{2}(1-t)e^{t-1}\\right]\\left(0.385-0.6156\\right)f\\left(O\\right)}\\\\ &{}&{(f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)<(0.77-1.615\\varepsilon)f\\left(O\\right);f\\left(O\\cap Z\\right)<(0.385-0.615\\varepsilon)f\\left(O\\right))}\\\\ &{}&{\\geq(0.385-\\varepsilon)f\\left(O\\right)}\\end{array}$ Z)) ) ", "page_idx": 28}, {"type": "text", "text": "D.2.2 Matroid Constraints ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "By Lemma D.1 in Appendix D.1 and the closed form for a recurrence provided in Lemma A.3, the following corollary holds, ", "page_idx": 28}, {"type": "text", "text": "Corollary D.2. After iteration $i$ of the for loop in Alg. 10, the following inequalities hold, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\because[f\\left(A_{i}\\right)]\\geq\\left(1-\\frac2\\ell\\right)\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]}\\\\ &{\\quad+\\frac{1}{\\ell+1}\\left(1-\\frac1\\ell\\right)\\left(f\\left(\\mathcal{O}\\setminus\\mathcal{Z}\\right)-\\frac12\\left(1-\\left(1-\\frac2\\ell\\right)^{i-1}\\right)f\\left(\\mathcal{O}\\cup\\mathcal{Z}\\right)\\right),1\\le i\\le t\\ell}\\\\ &{\\therefore[f\\left(A_{i}\\right)]\\geq\\left(1-\\frac2\\ell\\right)\\mathbb{E}\\left[f\\left(A_{i-1}\\right)\\right]}\\\\ &{\\quad+\\frac{1}{\\ell+1}\\left(1-\\frac1\\ell\\right)\\left(\\frac12\\left(1+\\left(1-\\frac2\\ell\\right)^{i-t\\ell}\\right)f(\\mathcal{O})-\\frac12\\left(\\left(1-\\frac2\\ell\\right)^{i-t\\ell}-\\left(1-\\frac2\\ell\\right)^{i}\\right)f(\\mathcal{O}\\cup\\mathcal{Z})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of approximation ratio. If $f\\left(Z\\right)\\geq\\left(0.305-\\varepsilon\\right)f\\left(O\\right)$ , the approximation ratio holds immediately. So, we analyze the case $f\\left(Z\\right)<(0.305-\\varepsilon)f\\left(O\\right)$ in the following. ", "page_idx": 28}, {"type": "text", "text": "Recall that $Z$ is a $(1+\\varepsilon)\\alpha,\\alpha)$ -guidance set in Corollary C.1, it holds that $f\\left(O\\cup Z\\right)+f\\left(O\\cap Z\\right)<$ $(0.61-1.695\\varepsilon)f\\left(O\\right)$ and $f\\left(\\bar{O}\\cap Z\\right)<(0.305-0.695\\varepsilon)f\\left(O\\right)$ . ", "page_idx": 28}, {"type": "text", "text": "By repeatedly implementing Lemma A.3 with the recursion in Corollary D.2, it holds that ", "page_idx": 29}, {"type": "text", "text": "E f A $\\begin{array}{l}{\\displaystyle\\left\\langle t\\ell\\right\\rangle\\right]\\geq\\frac{\\ell-1}{2(\\ell+1)}\\left[\\left(1-\\left(1-\\frac{2}{\\ell}\\right)^{\\lfloor t\\ell\\rfloor}\\right)\\left(f\\left(O\\setminus Z\\right)-\\frac{1}{2}f\\left(O\\cup Z\\right)\\right)+t\\left(1-\\frac{2}{\\ell}\\right)^{\\lfloor t\\ell\\rfloor-1}f\\left(O\\cup Z\\right)\\right]\\right],}\\\\ {\\displaystyle\\frac{\\ell-1}{2(\\ell+1)}\\left[\\left(1-\\left(1-\\frac{2}{\\ell}\\right)^{\\lfloor t\\ell\\rfloor}\\right)\\left(f\\left(O\\right)-f\\left(O\\cap Z\\right)-\\frac{1}{2}f\\left(O\\cup Z\\right)\\right)+t\\left(1-\\frac{2}{\\ell}\\right)^{\\lfloor t\\ell\\rfloor-1}f\\left(O\\cup Z\\right)\\right]\\right],}\\end{array}$ )   \n\u2265 Z) (submodularity)   \n$\\begin{array}{r l}&{|f(A)|2\\left({n-\\frac{2}{3}}\\right)^{t+\\frac{2}{\\gamma-1}}\\Gamma\\left(f(A_{m})\\right)+\\frac{2}{3}\\frac{n}{{\\gamma}(1+\\frac{3}{3})}\\Bigg[\\left(\\frac{1}{2}+\\left(\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}\\right)\\left((1-\\frac{3}{2})^{t+\\frac{2}{\\gamma-1}}\\right)f\\right)}\\\\ &{\\qquad-(1-\\delta)\\left(\\left(1-\\frac{2}{3}\\right)^{t+\\frac{2}{\\gamma-1}}+\\left(1-\\frac{2}{3}\\right)^{t}\\right)f(\\alpha)\\,\\mathrm{f}\\Bigg]}\\\\ &{\\geq\\frac{\\gamma}{2+1}\\Bigg[\\frac{1}{2}+\\left(\\frac{1}{2}-\\varepsilon\\right)^{\\frac{2}{\\gamma-1}}\\left((1-\\frac{2}{3})^{t-\\frac{2}{\\gamma}}-(1-\\frac{2}{3})^{t}\\right)f(\\alpha)}\\\\ &{\\qquad-\\left((1-\\frac{2}{3})^{t}\\right)^{\\frac{2}{\\gamma-1}}-\\left((1-\\frac{2}{3})^{t}\\right)f(\\alpha)\\,\\mathrm{f}\\Bigg]}\\\\ &{\\qquad-\\left(\\left(\\frac{1}{2}+\\frac{1}{3}-\\varepsilon\\right)\\left(1-\\frac{2}{3}\\right)^{t-\\delta}-\\left(\\frac{3}{2}-\\frac{1}{3}\\right)\\left((1-\\frac{2}{3})^{t}\\right)f(\\alpha)\\,\\mathrm{f}\\Bigg]}\\\\ &{\\geq\\frac{\\gamma}{2+1}\\Bigg[\\left(\\frac{1}{2}+\\left(\\frac{3}{2}-\\varepsilon\\right)\\left(1-\\frac{2}{3}\\right)^{t}-(1-\\frac{2}{3})^{t}\\right)f(\\alpha)-\\left(\\frac{3}{2}-\\frac{1}{3}-\\left(\\frac{1}{3}\\right)^{t}\\right)f(\\alpha)}\\\\ &{\\qquad-\\left(\\frac{1}{2}+\\frac{1}{3}-\\varepsilon\\right)^{\\frac{2}{\\gamma-1}}\\left((3-\\frac{2}{3})^{t-\\delta}-\\left(\\frac{3}{2}-1\\right)^{t}f(\\alpha)\\,\\mathrm{f}\\right)}\\\\ &{\\qquad+\\frac{2}{3}\\frac{n}{2}\\Bigg[\\left(\\frac{1}{2}+\\left(\\frac{3}{2}-\\varepsilon $ (O) (O \u2229Z) 2(t\u22121) f (O \u2229Z)   \n\u22652(\u2113\u2113\u2212+ 11) 12 + 32 \u2212t \u22121\u2113  1 \u22122\u2113 e2(t\u22121) \u2212e\u22122 \u2212(0.305 \u22120.695\u03b5)  21 + 1\u2113 e\u22122 \u2212 12 \u2212t e2(t\u22121)   \n\u2212(0.61 \u22121.695\u03b5)  21 + 11  \u2212\u2212t2 e2(t\u22121) \u2212 23 \u22121\u2113 e\u22122  f (O)   \n\u2265(0.305 \u2212\u03b5)f (O) $\\begin{array}{r}{(\\ell=\\frac{10}{9\\varepsilon};0<\\varepsilon<0.305;t=0.559)}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "D.3 Derandomize Alg. 10 in Section 2.3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we present the deranmized version of Alg. 10, which simply evaluates all possible paths and returns the best solution. We reiterate the guarantee as follows. ", "page_idx": 29}, {"type": "text", "text": "Theorem 2.4. Let $(f,k)$ be an instance of SM, with the optimal solution set $O$ . Alg. 11 achieves a deterministic $(0.385-\\varepsilon)$ approximation ratio with $t\\,=\\,0.372$ , and a deterministic $(0.305-\\varepsilon)$ approximation ratio with $t=0.559$ . The query complexity of the algorithm is $O\\left(k n\\ell^{2\\ell-1}\\right)$ where \u2113= 19\u03b50. ", "page_idx": 29}, {"type": "text", "text": "E Proofs for Nearly Linear Time Deterministic Algorithm in Section 3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we provide the pseudocode and additional analysis of our nearly linear time deterministic algorithm introduced in Section 3. We organize this section as follows: in Appendix E.1, we ", "page_idx": 29}, {"type": "text", "text": "Algorithm 11: Deterministic combinatorial approximation algorithm with the same ratio as Alg. 2 ", "page_idx": 30}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/ec1a80e9a69e63c20dd3352da64290dab8367d22c17f1434e1ad19aed4683c32.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "provide a speedup version of GUIDEDIG-S which queries ${\\mathcal{O}}\\left(n\\log(k)\\right)$ times; in Appendix E.2, we analyze the subroutine, PRUNE; at last, in Appendix E.3, we provide the pseudocode of nearly linear time deterministic algorithm and its analysis. ", "page_idx": 30}, {"type": "text", "text": "E.1 GUIDEDIG-S Speedup ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we provide the algorithm THRESHGUIDEDIG, which combines the guiding and descending threshold greedy techniques with INTERLACEGREEDY [11]. ", "page_idx": 30}, {"type": "text", "text": "Lemma E.1. Let $O\\subseteq\\mathcal{U}$ be any set of size at most $k$ , and suppose THRESHGUIDEDIG(Alg. 12) is called with $(f,k,Z,G,\\ell)$ . Then THRESHGUIDEDIG outputs $\\ell(\\ell+1)$ candidate sets with $O\\left(\\ell^{2}n\\log(k)/\\varepsilon\\right)$ queries. Moreover, with a probability of $(\\ell+1)^{-1}$ , a randomly selected set $A$ from the output satisfies that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1)\\,\\left(\\cfrac{\\ell}{1-\\varepsilon}+1\\right)\\mathbb{E}\\left[f\\left(A\\right)\\right]\\geq\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]+\\cfrac{\\ell}{1-\\varepsilon}\\,f\\left(G\\right)-\\varepsilon f(O);}\\\\ &{2)\\,\\mathbb{E}\\left[f(O\\cup A)\\right]\\geq\\left(1-\\cfrac{1}{\\ell}\\right)\\,f(O\\cup G)+\\cfrac{1}{\\ell}\\left(f\\left(O\\cup\\left(Z\\cap G\\right)\\right)-f\\left(O\\cup Z\\right)\\right)}\\\\ &{3)\\,\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]\\geq\\left(1-\\cfrac{1}{\\ell}\\right)\\,f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\cfrac{1}{\\ell}(f\\left(\\left(O\\setminus Z\\right)\\cup\\left(Z\\cap G\\right)\\right)-f(O\\cup Z)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Let $o_{\\operatorname*{max}}\\,=\\,\\arg\\operatorname*{max}_{o\\in O\\setminus(G\\cup Z)}\\Delta(o|G)$ , and let $\\{a_{1},\\ldots,a_{\\ell}\\}$ be the largest $\\ell$ elements of $\\{\\Delta(x|G):x\\in\\mathcal{U}\\setminus(G\\cup Z)\\}$ , as chosen on Line 13. We consider the following two cases. ", "page_idx": 30}, {"type": "text", "text": "Case $(O\\setminus(G\\cup Z))\\cap\\{a_{1},\\dots,a_{\\ell}\\}=\\emptyset$ . Then, $\\cal O_{\\mathrm{max}}\\not\\in\\{a_{1},\\ldots,a_{\\ell}\\}$ which implies that $\\Delta(a_{u}|G)\\ge$ $\\Delta(o|G)$ , for every $1\\leq u\\leq\\ell$ and $\\ot{o}\\in O\\setminus(G\\cup Z)$ ; and, after the first iteration of the while loop on ", "page_idx": 30}, {"type": "text", "text": "Algorithm 12: A guided INTERLACEGREEDY subroutine with descending threshold technique for size constraints. ", "page_idx": 31}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/4621b474b68ab5782e4cbfcf2e65b6b93a9e92d60bf083dbe50fce3d4dfada9f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Line 13, none of the elements in $O\\setminus(G\\cup Z)$ is added into any of $\\{A_{0,i}\\}_{i=1}^{\\ell}$ . We will analyze the iteration of the for loop on Line 4 with $u=0$ . ", "page_idx": 31}, {"type": "text", "text": "For any $l\\in[\\ell]$ , let $A_{0,l}^{(j)}$ be $A_{0,l}$ after we add $j$ elements into it, $\\tau_{l}^{(j)}$ be $\\tau_{l}$ when we adopt $j$ -th elements into $A_{0,l}$ , and $\\tau_{l}^{(1)}=M$ . By Line 7, it holds that $A_{0,l}^{(1)}=G\\cup\\{a_{l}\\}$ . Since $(O\\setminus(G\\cup Z))\\cup$ $\\{a_{1},\\ldots,a_{\\ell}\\}=\\emptyset$ , and we add elements to each set in turn, we can order $O\\setminus(G\\cup Z)=\\{o_{1},o_{2},...\\}$ such that the first $\\ell$ elements are not selected by any set before we get $A_{0,l}^{(1)}$ , the next $\\ell$ elements are not selected in any set before we get $A_{0,l}^{(2)}$ , and so on. Therefore, for any $j\\,\\leq\\,\\vert A_{0,l}\\setminus G\\vert$ and $\\ell(j-1)+1\\leq i\\leq\\ell j,o$ $o_{i}$ are filtered out by $A_{0,l}$ with threshold $\\tau_{l}^{(j)}/(1-\\varepsilon)$ , which follows that $\\Delta\\Bigl(o_{i}|A_{0,l}^{(j)}\\Bigr)<\\tau_{l}^{(j)}/(1{-}\\varepsilon)\\le(f(A_{0,l}^{(j)}){-}f(A_{0,l}^{(j-1)}))/(1{-}\\varepsilon)$ ; for any $\\ell|A_{0,l}\\backslash G|<i\\leq|O\\backslash(G\\cup Z)|$ , $o_{i}$ are filtered out by $A_{0,l}$ with threshold $\\frac{\\varepsilon M}{k}$ , which follows that $\\Delta(o_{i}|A_{0,l})<\\varepsilon M/k$ . Thus, ", "page_idx": 31}, {"type": "text", "text": "(submodularity) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal f}\\left(\\left({\\cal O}\\setminus{\\cal Z}\\right)\\cup{\\cal A}_{0,l}\\right)-{\\cal f}\\left({\\cal A}_{0,l}\\right)\\leq\\sum_{o\\in{\\cal O}\\setminus{\\cal A}_{0,l}}\\Delta(o|{\\cal A}_{0,l})}}\\\\ {{\\displaystyle\\leq\\sum_{o\\in{\\cal O}\\setminus({\\cal G}\\cup{\\cal Z})}\\Delta(o|{\\cal A}_{0,l})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n(G\\subseteq A_{0,l})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\sum_{i=1}^{\\ell}\\Delta\\Big(o_{i}|A_{0,l}^{(1)}\\Big)+\\displaystyle\\sum_{i=\\ell+1}^{2\\ell}\\Delta\\Big(o_{i}|A_{0,l}^{(2)}\\Big)+...+\\displaystyle\\sum_{i>\\ell|A_{0,l}\\backslash G|}\\Delta(o_{i}|A_{0,l})}\\\\ &{\\leq\\ell\\cdot\\displaystyle\\frac{f\\left(A_{0,l}\\right)-f\\left(G\\right)}{1-\\varepsilon}+\\varepsilon M}\\\\ &{\\leq\\ell\\cdot\\displaystyle\\frac{f\\left(A_{0,l}\\right)-f\\left(G\\right)}{1-\\varepsilon}+\\varepsilon f\\left(O\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By summing up the above inequality with all $1\\leq l\\leq\\ell$ , it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left({\\frac{\\ell}{1-\\varepsilon}}+1\\right)\\operatorname{\\mathbb{E}}\\left[f\\left(A\\right)\\right]\\geq\\operatorname{\\mathbb{E}}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]+{\\frac{\\ell}{1-\\varepsilon}}f\\left(G\\right)-\\varepsilon f\\left(O\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $A_{u,l_{1}}\\,\\cap\\,A_{u,l_{2}}\\;\\,=\\;\\,G$ for any $1\\ \\leq\\ l_{1}\\ \\neq\\ l_{2}\\ \\leq\\ \\ell$ it holds that $((O\\setminus Z)\\cup A_{u,l_{1}})$ \u2229 $((O\\setminus Z)\\cup A_{u,l_{2}})=(O\\setminus Z)\\cup G$ . By repeated application of submodularity, it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\left(O\\setminus Z\\right)\\cup A\\right)\\right]=\\displaystyle\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f(\\left(O\\setminus Z\\right)\\cup A_{0,i})}\\\\ &{\\quad\\ge\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}f\\left(\\left(O\\setminus Z\\right)\\cup\\left(\\bigcup_{i=1}^{\\ell}A_{0,i}\\right)\\right)}\\\\ &{\\quad\\ge\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f\\left(\\left(O\\setminus Z\\right)\\cup G\\right)+\\displaystyle\\frac{1}{\\ell}(f\\left(\\left(O\\setminus Z\\right)\\cup\\left(Z\\cap G\\right))-f(O\\cup Z))}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(O\\cup A\\right)\\right]=\\displaystyle\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}f(O\\cup A_{0,i})\\geq\\left(1-\\frac{1}{\\ell}\\right)f\\left(O\\cup G\\right)+\\frac{1}{\\ell}f\\left(O\\cup\\left(\\cup_{i=1}^{\\ell}A_{0,i}\\right)\\right)}\\\\ &{\\quad\\geq\\left(1-\\frac{1}{\\ell}\\right)f\\left(O\\cup G\\right)+\\frac{1}{\\ell}\\left(f\\left(O\\cup\\left(Z\\cap G\\right)\\right)-f\\left(O\\cup Z\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, the last two inequalities in the theorem hold. ", "page_idx": 32}, {"type": "text", "text": "Case $(O\\setminus(G\\cup Z))\\cap\\{a_{1},\\ldots,a_{\\ell}\\}\\neq\\emptyset.$ . Then $\\cal{o}_{\\mathrm{max}}\\in\\{a_{1},\\ldots,a_{\\ell}\\}$ . Suppose that $a_{u}=o_{\\operatorname*{max}}$ . We analyze that lemma holds with sets $\\{A_{u,l}\\}_{l=1}^{\\ell}$ . ", "page_idx": 32}, {"type": "text", "text": "Similar to the analysis of the previous case, let $A_{0,l}^{(j)}$ be $A_{0,l}$ after we add $j$ elements into it, $\\tau_{l}^{(j)}$ be $\\tau_{l}$ when we adopt $j$ -th elements into $A_{0,l}$ , and $\\tau_{l}^{(1)}=M$ . By Line 10, it holds that $A_{u,l}^{(1)}=G\\cup\\{a_{u}\\}$ Then, we can order $O\\setminus(G\\cup Z\\cup\\{a_{u}\\})=\\{o_{1},o_{2},...\\}$ such that the first $\\ell$ elements are not selected by any set before we get $A_{u,l}^{(1)}$ , the next $\\ell$ elements are not selected in any set before we get $A_{u,l}^{(2)}$ , and so on. Therefore, Inequality 8 also holds in this case, where is a random set from $\\{A_{u,l}\\}_{l=1}^{\\ell}$ . Since, $a_{u}~\\in~O$ , and $A_{u,l_{1}}\\cap A_{u,l_{2}}\\ =\\ G\\cup\\{a_{u}\\}$ for any $1\\ \\leq\\ l_{1}\\ \\neq\\ l_{2}\\ \\leq\\ \\ell\\,$ , it holds that $((O\\setminus Z)\\cup A_{u,i_{1}})\\cap((O\\setminus Z)\\cup A_{u,i_{2}})=O\\cup G$ . Following the proof of case $(O\\setminus(G\\cup Z))\\cap$ $\\{a_{1},\\ldots,a_{\\ell}\\}=\\varnothing$ , the last two inequalities still hold in this case. ", "page_idx": 32}, {"type": "text", "text": "Overall, since either one of the above cases happens, the theorem holds. ", "page_idx": 32}, {"type": "text", "text": "E.2 Pruning Subroutine ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "PRUNE is used in Alg. 14 to help construct the $(\\alpha,\\beta)$ -guidance set, where $\\alpha=0.377$ and $\\beta=0.46$ .   \nIn this section, we provide the pseudocode, guarantee and its analysis. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.2. Suppose PRUNE(Alg. 13) is called with $(f,A)$ and returns the set $\\mathcal{A^{\\prime}}$ . For every $A\\in{\\mathcal{A}}$ and its related output set $A^{\\prime}\\in{\\mathcal{A}}^{\\prime}$ , it holds that, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1)\\,f\\left(A^{\\prime}\\right)\\geq f\\left(A\\right);}\\\\ &{2)\\,f\\left(S\\cup A^{\\prime}\\right)\\geq f\\left(S\\cup A\\right),\\forall S\\subseteq\\mathcal{U};}\\\\ &{3)\\,f\\left(T\\right)\\leq f\\left(A^{\\prime}\\right),\\forall T\\subseteq A^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Algorithm 13: A pruning algorithm which deletes element with negative marginal gain ", "page_idx": 33}, {"type": "text", "text": "1 Procedure PRUNE $(f,A)$ :   \n2 Input: oracle $f$ , a sequence of subsets $\\boldsymbol{\\mathcal{A}}$   \n3 Initialize: $\\mathcal{A}^{\\prime}\\leftarrow\\emptyset$   \n4 for $A\\in{\\mathcal{A}}$ do   \n5 for $x\\in A$ do   \n6 if $\\Delta(x|A-x)<0$ then $A\\leftarrow A-x$   \n7 end   \n8 ${\\mathcal{A}}^{\\prime}\\leftarrow{\\mathcal{A}}^{\\prime}\\cup\\{A\\}$   \n9 end   \n10 return A\u2032 ", "page_idx": 33}, {"type": "text", "text": "Proof. Let $A_{i}$ be $A$ after we delete $i$ -th element $x_{i}$ , $A_{0}$ be the input set $A$ , $A_{m}$ be the output set $A^{\\prime}$ . Since any element $x_{i}$ being deleted follows that $\\Delta(x_{i}|A_{i})<0$ , it holds that $f\\left(A_{i}\\right)>f\\left(\\bar{A_{i}}+x_{i}\\right)=$ $f\\left(A_{i-1}\\right)$ . Therefore, $f\\left({\\bar{A^{\\prime}}}\\right)=f\\left(A_{m}\\right)>...>f\\left({\\dot{A}}_{0}\\right)=f\\left(A\\right)$ . The first inequality holds. ", "page_idx": 33}, {"type": "text", "text": "For any $x_{i}\\in A\\setminus A^{\\prime}$ , it holds that $\\Delta(x_{i}|A_{i})<0$ . By submodularity, ", "page_idx": 33}, {"type": "equation", "text": "$$\nf\\left(S\\cup A\\right)-f\\left(S\\cup A^{\\prime}\\right)=\\sum_{i=1}^{m}\\Delta(x_{i}|S\\cup A_{i})\\leq\\sum_{i=1}^{m}\\Delta(x_{i}|A_{i})<0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second inequality holds. ", "page_idx": 33}, {"type": "text", "text": "For any $y\\in A^{\\prime}\\setminus T$ , since it is not deleted, there exists $0\\leq i_{y}\\leq m$ such that $\\Delta(y|A_{i_{y}})\\geq0$ . By submodularity, ", "page_idx": 33}, {"type": "equation", "text": "$$\nf\\left(A^{\\prime}\\right)-f\\left(T\\right)\\geq\\sum_{y\\in A^{\\prime}\\backslash T}\\Delta(y|A^{\\prime})\\geq\\sum_{y\\in A^{\\prime}\\backslash T}\\Delta\\bigl(y|A_{i_{y}}\\bigr)\\geq0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The third inequality holds. ", "page_idx": 33}, {"type": "text", "text": "E.3 Proofs for Theorem 3.1 of Alg. 14 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Prior to delving into the proof of Theorem 3.1, we provide the following corollary first. It demonstrates the progression of the intermediate solution of THRESHGUIDEDIG, after the pruning process, relying on Lemma E.2 and E.1. ", "page_idx": 33}, {"type": "text", "text": "Corollary E.1. Let $\\begin{array}{r l r}{O}&{{}\\subseteq}&{\\;\\;\\mathcal{U}}\\end{array}$ be any set of size at most $k$ . Then PRUNE(THRESHGUIDEDIG $(f,k,\\emptyset,G,\\ell))$ outputs $\\ell(\\ell+1)$ candidate sets with $O\\left(\\ell^{2}n\\log(k)/\\varepsilon\\right)$ queries. Moreover, with a probability of $(\\ell+1)^{-1}$ , a randomly selected set $A$ from the output satisfies that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1)\\,\\displaystyle\\left(\\frac{\\ell}{1-\\varepsilon}+1\\right)\\mathbb{E}\\left[f\\left(A\\right)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f(O\\cup G)+\\frac{\\ell}{1-\\varepsilon}f\\left(G\\right)-\\varepsilon f(O);}\\\\ &{2)\\,\\mathbb{E}\\left[f(O\\cup A)\\right]\\geq\\left(1-\\displaystyle\\frac{1}{\\ell}\\right)f(O\\cup G);}\\\\ &{3)\\,f(O\\cap A)\\leq f(A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Theorem 3.1. Let $(f,k)$ be an instance of SM, with the optimal solution set $O$ . Algorithm 14 achieves a deterministic $\\left(0.377-\\varepsilon\\right)$ approximation ratio with $\\mathcal{O}(n\\log(k){\\ell_{1}}^{2\\ell_{1}}{\\ell_{2}}^{2\\ell_{2}-1})$ queries, where $\\begin{array}{r}{\\ell_{1}=\\frac{10}{3\\varepsilon}}\\end{array}$ and $\\begin{array}{r}{\\ell_{2}=\\frac{5}{\\varepsilon}}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Following the proof of Theorem 3.1, we consider two cases of the algorithm. ", "page_idx": 33}, {"type": "text", "text": "Case 1. For every $A\\in Z$ , it holds that $f(O\\cup A)\\geq0.46f(O)$ . Then, we prove that $\\operatorname*{max}_{C\\in Z}f\\left(C\\right)\\geq$ $(0.377-\\varepsilon)f\\left(O\\right)$ . ", "page_idx": 33}, {"type": "text", "text": "In the following, we prove the theorem by analyzing the random case of the algorithm, where we randomly select a set from the output of PRUNE (THRESHGUIDEDIG). Suppose that we successfullly select a set where the inequalities in Corollary E.1 hold. Let $A_{i}$ and $A_{i-1}$ be random sets in $Z_{i}$ ", "page_idx": 33}, {"type": "text", "text": "1 Input: oracle $f$ , size constraint $k$ , error rate $\\varepsilon$   \n2 Initialize $\\begin{array}{r}{\\varepsilon^{\\prime}=\\frac{2}{\\varepsilon},\\ell_{1}\\gets\\frac{5}{3\\varepsilon^{\\prime}},\\ell_{2}\\gets\\frac{5}{2\\varepsilon^{\\prime}},t\\gets0.3}\\end{array}$   \n$_{3\\mathrm{~}\\mathrm{~}}\\triangleright$ Create guided sets   \n4 $Z_{0}\\gets\\emptyset$   \n5 for $i\\gets1$ to $\\ell_{1}$ do   \n6 $Z_{i}\\gets\\emptyset$   \n7 for $A_{i-1}\\in Z_{i-1}$ do   \n8 Zi \u2190Zi \u222aPRUNE(THRESHGUIDEDIG $\\cdot(f,k,\\emptyset,A_{i-1},\\ell_{1},\\varepsilon^{\\prime}))$   \n9 end   \n10 end   \n11 $Z\\gets\\cup_{i=1}^{\\ell_{1}}Z_{i}$   \n$^{12\\,\\circ}$ Build solution based on guided sets   \n13 $G\\gets\\emptyset$   \n14 for $A\\in Z$ do   \n15 $G_{0}\\gets\\emptyset$   \n16 for $i\\gets1$ to $\\ell_{2}$ do   \n17 $G_{i}\\gets\\emptyset$   \n18 for $B_{i-1}\\in G_{i-1}$ do   \n19 if $i\\leq t\\ell$ then   \n20 Gi \u2190Gi \u222aTHRESHGUIDEDIG $(f,k,A,B_{i-1},\\ell_{2},\\varepsilon^{\\prime})$   \n21 else   \n22 \u2014 $G_{i}\\leftarrow G_{i}\\cup$ THRESHGUIDEDIG $(f,k,\\emptyset,B_{i-1},\\ell_{2},\\varepsilon^{\\prime})$   \n23 end   \n24 end   \n25 end   \n26 $G\\leftarrow G\\cup G_{\\ell_{2}}$   \n27 end   \n28 return $C^{*}\\gets\\arg\\operatorname*{max}_{C\\in Z\\cup G}f(C)$ ", "page_idx": 34}, {"type": "text", "text": "and $Z_{i-1}$ , respectively. Let $\\begin{array}{r}{\\left(1-\\frac{1}{\\ell_{1}}\\right)^{i^{*}-1}\\;>\\;0.46\\;\\geq\\;\\left(1-\\frac{1}{\\ell_{1}}\\right)^{i^{*}}}\\end{array}$ . Then, by Inequality (2) in Corollary E.1, when $i<i^{*}$ , $\\begin{array}{r}{\\mathbb{E}\\left[f(O\\cup A_{i})\\right]\\ge\\left(1-\\frac{1}{\\ell_{1}}\\right)^{i}f(O)}\\end{array}$ ; when $i\\geq i^{*}$ , $f(O\\cup A_{i})\\geq0.46f(O)$ by assumption. By applying Inequality (1) in Corollary E.1, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle|f(A_{t})|\\geq\\left[\\frac{i^{\\star}}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}\\right]\\left(1\\!-\\!\\frac{1}{\\hat{l}_{t}^{\\star}}\\right)^{t^{\\star}}-\\varepsilon^{\\prime}\\left(1\\!-\\!\\left(1\\!-\\!\\frac{1}{\\frac{1}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}}\\right)^{t^{\\star}}\\right)\\right]f(\\mathcal{O})}\\\\ &{:\\left[f(A_{t})\\right]\\geq\\left(1\\!-\\!\\frac{1}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}\\right)^{t^{\\star}-\\varepsilon^{\\prime}}\\mathbb{E}\\left[f(A_{t})\\right]+\\left(1\\!-\\!\\left(1\\!-\\!\\frac{1}{1\\!-\\!\\frac{1}{\\hat{l}_{t}^{\\star}}}\\right)^{t^{\\star}\\!-\\varepsilon^{\\prime}}\\right)(0.46\\!-\\!\\varepsilon^{\\prime})f(\\mathcal{O})}\\\\ &{\\geq\\left[\\frac{\\varepsilon^{\\prime}}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}\\!+\\!\\left(1\\!-\\!\\frac{1}{\\hat{l}_{t}^{\\star}}\\right)^{t^{\\star}}\\left(1\\!-\\!\\frac{1}{\\frac{1}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}+1}\\right)^{t^{\\star}-\\varepsilon^{\\prime}}+\\left(1\\!-\\!\\left(1\\!-\\!\\frac{1}{\\frac{1}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}+1}\\right)^{t^{\\star}-\\varepsilon^{\\prime}}\\right)0.46\\right.}\\\\ &{\\qquad-\\varepsilon^{\\prime}\\left(1\\!-\\!\\left(1\\!-\\!\\frac{1}{\\frac{1}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}+1}\\right)^{t^{\\star}}\\right)\\Bigg]f(\\mathcal{O})}\\\\ &{\\geq\\left[\\frac{\\log(0.46)}{\\left(\\frac{1}{1\\!-\\!\\frac{\\delta}{\\tau^{\\prime}}}+1\\right)\\log\\left(1-\\frac{1}{\\hat{l}_{t}^{\\star}}\\right)}\\left(1-\\frac{1}{\\ell}\\right)e^{-1}+\\left(1\\!-\\!\\frac{e^{\\varepsilon^{\\prime}-1}}{0.46\\left(1-\\frac{1}{\\frac{1}{1\\!-\\delta}+1}\\right)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\textstyle(\\ell_{1}=\\frac{5}{3\\varepsilon^{\\prime}};\\varepsilon^{\\prime}=\\frac{\\varepsilon}{2};0<\\varepsilon<0.377)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since we return the best solution in $Z$ and $G$ , it holds that $f(C^{*})\\geq\\mathbb{E}\\left[f\\left(A_{\\ell_{1}}\\right)\\right]\\geq(0.377-\\varepsilon)f(O).$ . ", "page_idx": 35}, {"type": "text", "text": "Case 2. There exists $A\\in Z$ , such that $f(O\\cup A)<0.46f(O)$ . Then, we prove that $\\operatorname*{max}_{C\\in G}f\\left(C\\right)\\geq$ $(0.377-\\varepsilon)f\\left(O\\right)$ . ", "page_idx": 35}, {"type": "text", "text": "Suppose that $f(A)<0.377f(O)$ . Otherwise, $f(C^{*})\\geq0.377f(O)$ immediately. By Inequality (3) in Corollary E.2, it holds that $f\\left(O\\cap A\\right)\\leq f\\left(A\\right)<0.377f\\left(O\\right)$ . Let $B_{\\ell_{2}}$ be a randomly selected set in $G_{\\ell_{2}}$ , where we calculate $G_{\\ell_{2}}$ with the guidance set $A$ and inequalities in Theorem D.2 hold successfully. In the following, we also consider that randomized version of the algorithm, where we randomly select a set $B_{i}$ from all the solution set returned by THRESHGUIDEDIG. Suppose that we successfully select a set where the inequalities in Lemma E.1 hold. Then, the recursion of $\\mathbb{E}\\left[f\\left(B_{i}\\right)\\right]$ can be calculated as follows, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{i)\\big\\vert\\ge\\frac{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}}{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}+1}\\mathbb{E}\\left[f\\left(B_{i-1}\\right)\\right]+\\frac{1}{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}+1}\\left(f\\left(O\\setminus A\\right)-\\left(1-\\left(1-\\frac{1}{\\ell_{2}}\\right)^{i}\\right)f\\left(O\\cup A\\right)-\\varepsilon^{\\prime}f\\left(O\\right)\\right),}&\\\\ &{\\qquad\\qquad+\\left.1\\le i\\le t\\ell_{2}\\right.}\\\\ &{i)\\big]\\ge\\frac{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}}{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}+1}\\mathbb{E}\\left[f\\left(B_{i-1}\\right)\\right]+\\frac{1}{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}+1}\\left[\\left(\\left(1-\\frac{1}{\\ell_{2}}\\right)^{i-\\lfloor t\\ell_{2}\\rfloor}-\\varepsilon^{\\prime}\\right)f\\left(O\\right)\\right.}&\\\\ &{\\qquad\\qquad-\\left(\\left(1-\\frac{1}{\\ell_{2}}\\right)^{i-\\lfloor t\\ell_{2}\\rfloor}-\\left(1-\\frac{1}{\\ell_{2}}\\right)^{i}\\right)f\\left(O\\cup A\\right)\\right],}&{\\qquad t\\ell_{2}<i\\le\\ell_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, by solving the above recursion, it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert f(B_{n})\\Vert\\geq\\bigg(1-\\frac{1}{\\ell}\\bigg)^{n+1}\\Vert^{4/2}}\\\\ &{\\qquad\\qquad-\\frac{4}{\\sqrt{2}\\ell}\\frac{1}{n}\\bigg(1-\\left(-\\frac{1}{\\ell}\\right)^{3}\\operatorname*{det}\\bigg)^{1}f(\\ell)}\\\\ &{\\geq\\bigg(\\bigg(1-\\frac{1}{\\ell}\\bigg)^{1+1}\\operatorname*{det}\\bigg(-\\frac{1}{\\ell}\\bigg)^{n+1}\\int_{0}^{1}(\\ell)}\\\\ &{\\qquad+\\frac{4}{\\sqrt{n}}\\frac{\\langle\\mathbf{1}_{0}|\\mathbf{1}_{\\ell}\\rangle}{\\sqrt{n}+1}+\\bigg(-\\frac{1}{\\ell}\\bigg)^{3}f(\\ell)-\\ell\\bigg)}\\\\ &{\\qquad-\\frac{4}{\\sqrt{n}}\\frac{\\langle\\mathbf{1}_{0}|\\mathbf{1}_{\\ell}\\rangle}{n}\\bigg(1-\\frac{1}{\\ell}\\bigg)^{1+2+n+1}f(\\ell)-\\frac{4}{\\sqrt{n}}\\bigg(1-\\theta(\\bigg(1-\\frac{1}{\\ell}\\bigg)^{1+1}\\operatorname*{det}\\bigg)-\\bigg(1-\\frac{1}{\\ell}\\bigg)^{n}f(\\ell)}\\\\ &{\\qquad-\\frac{4}{\\sqrt{n}}\\bigg)^{1}\\bigg(1-\\left(1-\\frac{1}{\\ell}\\right)^{3+3}\\operatorname*{det}\\bigg)\\int_{0}^{1}(\\ell)}\\\\ &{\\geq\\bigg(\\bigg(1-\\frac{1}{\\ell}\\bigg)^{2}+n-\\frac{4}{\\sqrt{n}}\\bigg)^{1}(\\ell)-f(\\ell)\\,\\mathrm{d}\\ell-f(\\ell)}\\\\ &{\\qquad+\\frac{1}{\\sqrt{n}}\\frac{\\langle\\mathbf{1}_{0}|\\mathbf{1}_{\\ell}\\rangle}{n}\\bigg(1-\\frac{1}{\\ell}\\bigg)^{3}f(\\ell)}\\\\ &{\\qquad-\\frac{4}{\\sqrt{n}}\\frac{\\langle\\mathbf{1}_{0}|\\mathbf{1}_{\\ell}\\rangle}{n}-\\frac{4}{\\sqrt{n}}f(\\ell)-\\ell\\bigg)^{3}f(\\ell)}\\\\ &{\\qquad-\\frac{\\nu_{0}\\ell}{\\sqrt{n}}+\\bigg(1-\\frac{1}{\\ell}\\bigg)^{3}f^{(3)}-\\ell\\bigg)\\,\\frac{\\sqrt{n}}{\\sqrt{n}}+\\bigg((1-\\theta)^{n+1}-\\bigg(1- \n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "$\\begin{array}{r l}&{\\geq\\left[\\left(\\frac{\\left(1-t\\right)\\left(\\ell_{2}-1\\right)}{\\frac{\\ell_{2}}{1-\\varepsilon^{\\prime}}+1}+1-\\frac{1}{\\ell_{2}}\\right)\\left(1-\\frac{1}{\\ell_{2}}\\right)e^{t-1}-e^{-1}-\\varepsilon^{\\prime}\\left(1-e^{-1}\\right)\\right.}\\\\ &{\\left.-0.377\\left(e^{t-1}-e^{-1}\\right)-0.46\\left((2-t)e^{t-1}-\\left(2-\\frac{1}{\\ell_{2}}\\right)e^{-1}\\right)\\right]f\\left(O\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(f\\left(O\\cap A\\right)<0.377f\\left(O\\right);f\\left(O\\cup A\\right)<0.46f\\left(O\\right)\\right)}\\\\ &{\\geq\\left(0.377-\\varepsilon\\right)f\\left(O\\right)}\\end{array}$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "F Experiments ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Experimental setup We run all experiments on an Intel Xeon(R) W5-2445 CPU at $3.10\\:\\mathrm{GHz}$ with 20 cores, $\\mathrm{64\\,GB}$ of memory, and one NVIDIA RTX A4000 with $\\mathrm{16\\,GB}$ of memory. For Maximum Cut experiments, we use the standard multiprocessing provided in Python, which takes about 20 minutes to complete, while the video summarization finishes in under a minute. ", "page_idx": 36}, {"type": "text", "text": "F.1 Additional tables and plots ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, you can find the tables and plots omitted in the main paper due to space constraints. In Figure 5, we compare the frames selected by FASTLS $+.$ RANDOMGREEDY and STANDARDGREEDY, and in Figure 6, we report the results for Barab\u2019asi-Albert and Watts-Strogatz models for Maximum Cut. ", "page_idx": 36}, {"type": "text", "text": "F.2 Problem Formulation ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we formally introduce video summarization and Maximum Cut. ", "page_idx": 36}, {"type": "text", "text": "F.2.1 Video summarization ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Formally, given $n$ frames from a video, we present each frame by a $p$ -dimensional vector. Let $X\\in\\mathcal{R}^{\\dot{n}\\times\\bar{n}}$ be the Gramian matrix of the $n$ resulting vectors so $X_{i j}$ quantifies the similarity between two vectors through their inner product. The Determinantal Point Process (DPP) objective function is defined by the determinant function $f:2^{n}\\to{\\mathcal{R}}:f(S)=\\log(\\operatorname*{det}(X_{S})+1)$ , where $X_{S}$ is the principal submatrix of $X$ indexed by $S$ following Banihashem et al. [1] to make the objective function $f$ a non-monotone non-negative submodular function. ", "page_idx": 36}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/c69daaa44406917e2c8ab30c04ba415a366eb955cad831a68bcc853934444aea.jpg", "img_caption": ["Figure 5: Frames selected for Video Summarization "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "F.2.2 Maximum cut ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Given an undirected graph $G(V,E)$ , where $V$ represents the set of vertices, $E$ denotes the set of edges and weights $w(u,v)$ on the edges $(u,v)\\in E$ , the goal of the Maximum Cut problem is to find a subset of nodes $S\\subseteq V$ that maximizes the objective function, $\\begin{array}{r}{f(S)=\\sum_{u\\in S,v\\in V\\setminus S}w(u,v)}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "F.3 Hyperparameters ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "For all experiments, we set the error rate, $\\epsilon$ , to 0.01 for FASTLS $^+$ GUIDEDRG and to 0.1 for Lee et al. [21]. Additionally, for video summarization, we run RANDOMGREEDY 20 times and report the standard deviation of these runs. For all other experiments, we run the algorithms once per instance and report the standard deviation over instances. ", "page_idx": 36}, {"type": "image", "img_path": "cgiOX8lfwG/tmp/f514b8b93dd8e0fa2955a6d695cf318dfb4f48d91d3939f262e4478edfb34b59.jpg", "img_caption": ["Figure 6: The objective value (higher is better) and the number of queries (lower is better) are normalized by those of STANDARDGREEDY. Our algorithm (blue star) outperforms every baseline on at least one of these two metrics. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "F.4 Datasets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The video we select [29] (available under CC BY license) for video summarization lasts for roughly 4 minutes, and we uniformly sample 100 frames from the video to form the ground set. For maximum cut, we run experiments on synthetic random graphs, each distribution consisting of 20 graphs of size $10,000$ vertices generated using the Erd\u02ddos-Renyi (ER), Barabasi-Albert (BA), and Watts-Strogatz (WS) models. The ER graphs are generated with $p=0.001$ , while the WS graphs are created with $p=0.001$ and 10 edges per node. For the BA model, graphs are generated by adding $m=2$ edges in each iteration. Data and code are provided in the supplementary material to regenerate the empirical results provided in the paper. ", "page_idx": 37}, {"type": "text", "text": "F.5 Implementation of FASTLS $^+$ GUIDEDRG ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "For our implementation of FASTLS, we take the solution of STANDARDGREEDY as our initial solution $Z_{0}$ ; the theoretical guarantee is thus $f(Z_{0})>\\mathrm{OPT}/k$ , since $Z_{0}$ has higher $f$ -value than the maximum singleton. This increases the theoretical query complexity of our algorithm as implemented to $\\mathcal{O}\\left(\\frac{k n}{\\varepsilon}\\log(\\frac{k}{\\varepsilon})\\right)$ . Then, for each swap, we find the best candidates to remove from and add to the current solution set (including the dummy element), rather than any pair that satifies the criterion. For guided GUIDEDRG, we implement exactly as in the pseudocode (Alg. 6) \u2013 we remark that we could instead use (a guided version of) the linear-time variant of RANDOMGREEDY [10] to reduce the empirical number of queries further, but for simplicity we did not do this in our evaluation. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction are matched by theorem statements with detailed proofs in the main text and appendices or by empirical evaluation. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: All formal theorem statements include the assumed hypotheses, and theoretical and empirical limitations are discussed explicitly in Section 5. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All assumptions are clearly specified. In the main text, we discuss the intuition behind the analysis and sketch the main arguments, while complete detailed proofs are provided in the appendices. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All parameter settings and implementation choices for the evaluated algorithms are reported. Further, the source code and documentation is provided, with instructions for how to reproduce the results. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The complete data, source code, and instructions to reproduce the experimental results are provided in the supplementary material. Once this manuscript is published, we plan to make this available on an open-source repository. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: No training is involved in the evaluation. All hyperparameters for random graph generation and for the algorithms are provided. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: In out plots, we plot the mean over 20 independent repetitions. The shaded regions of the plots indicate one standard deviation about the mean. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 40}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our experiments are not large-scale, and modern desktop hardware should be sufficient to reproduce all results in a few hours. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The research conforms in every respect to the code of ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: Ours is foundational research, not tied to any particular application or deployment. The applications upon which we implemented and empirically evaluated our algorithms are proof-of-concept and thus our provided implementations are unlikely to lead directly to negative societal impact. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: We do not release any data or models that have a high risk for misuse. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The only pre-existing asset used is the video in the summarization experiment.   \nWe explicitly mention the license in Appendix F and properly respect the terms. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The released source code is well documented. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: ", "page_idx": 43}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]