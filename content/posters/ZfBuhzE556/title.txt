$\beta$-DPO: Direct Preference Optimization with Dynamic $\beta$