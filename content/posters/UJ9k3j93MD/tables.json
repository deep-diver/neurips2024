[{"figure_path": "UJ9k3j93MD/tables/tables_27_1.jpg", "caption": "Table 1: Training settings of experiment. We apply DEQ with width-10 and FNN trained on sawtooth target function with 25 linear regions while keeping FLOPs per iteration similar.", "description": "This table shows the training settings used for comparing DEQ and FNN on a sawtooth function with 25 linear regions.  The goal was to maintain a similar number of FLOPs (floating point operations) per iteration for each model type while varying the network architecture (depth and width).  This allowed the authors to isolate the impact of network architecture on performance rather than differences in computational cost.", "section": "6 Experiments"}]