[{"type": "text", "text": "Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhoutong Wu1 ztwu@stu.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Yimu Zhang2 zym24@stu.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Cong Fang2,3\u2020 fangcong@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zhouchen Lin2,3,4\u2020zlin@pku.edu.cn", "page_idx": 0}, {"type": "text", "text": "1 Academy for Advanced Interdisciplinary Studies, Peking University 2 State Key Lab of General AI, School of Intelligence Science and Technology, Peking University 3 Institute for Artificial Intelligence, Peking University 4 Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The deep equilibrium model (DEQ) generalizes the conventional feedforward neural network by fixing the same weights for each layer block and extending the number of layers to infinity. This novel model directly finds the fixed points of such a forward process as features for prediction. Despite empirical evidence showcasing its efficacy compared to feedforward neural networks, a theoretical understanding for its separation and bias is still limited. In this paper, we take a step by proposing some separations and studying the bias of DEQ in its expressive power and learning dynamics. The results include: (1) A general separation is proposed, showing the existence of a width- $^{m}$ DEQ that any fully connected neural networks (FNNs) with depth $O(m^{\\alpha})$ for $\\alpha\\in(0,1)$ cannot approximate unless its width is sub-exponential in $m$ ; (2) DEQ with polynomially bounded size and magnitude can efficiently approximate certain steep functions (which has very large derivatives) in $L^{\\infty}$ norm, whereas FNN with bounded depth and exponentially bounded width cannot unless its weights magnitudes are exponentially large; (3) The implicit regularization caused by gradient flow from a diagonal linear DEQ is characterized, with specific examples showing the benefits brought by such regularization. From the overall study, a high-level conjecture from our analysis and empirical validations is that DEQ has potential advantages in learning certain high-frequency components. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Implicit deep learning [1], a paradigm that generalizes the recursive principles of traditional explicit models, has gained renewed interest with the advent of novel neural network architectures. Among these, deep equilibrium model (DEQ) [2] stands out as a commonly utilized model. In contrast to explicit neural network that derives features through forward propagation, DEQ computes features directly by solving an equilibrium equation induced by the implicit layer. Since the equilibrium state is also the limit point of the infinitely recursive iterations of the implicit layer, DEQ can be regarded as a new neural network that models the limit of a multi-layer weight-tied neural network with the depth going to infinity. ", "page_idx": 0}, {"type": "text", "text": "Nowadays, DEQ has become a popular and widely studied model in the field of machine learning. On the empirical side, competitive performances against explicit feedforward neural networks have been achieved in various real applications such as natural language processing [2], computer vision [3], image generation [4], and solving inverse problems [5]. On the theoretic side, a main research line is to study the well-posedness of DEQ. This line aims to analyze when unique equilibrium can be guaranteed by DEQ and some weight parameterization and initialization techniques have been proposed to ensure the well-posedness [6, 7, 8]. ", "page_idx": 1}, {"type": "text", "text": "However, despite wide studies on DEQ, an understanding of the basic learning theory for its separation and bias against explicit feedforward neural networks is still limited. For the expressivity, a preliminary study about the connections between DEQ and fully-connected network (FNN) is provided in the seminar work [2], where it is proved that every FNN can be reformulated as a large DEQ under a specific weight re-parameterization, whereas, a deeper study on the provable and quantitative advantage of DEQ in its expression power is still lacking. Besides, there is another research line that studies the learning properties of DEQ using the so-called neural tangent kernel (NTK) view [9], originating from analyzing FNNs [10, 11]. It has been shown that under suitable initialization, the dynamic of over-parameterized DEQ can be approximated by a linear kernel model [12, 13], therefore the global convergence of gradient descent algorithm and possible generalization can be achieved under some regimes. However, in the NTK regime, it is shown that DEQs are almost equivalent to not-so-deep explicit FNNs for high dimensional Guassian mixtures [14], and it is still not known whether DEQs have potential advantages over FNNs in more general settings. A study on the separation and bias of DEQ over FNN can provide us with clear and intuitive suggestions about when DEQ is preferred in practice, thus it is strongly desired. In this paper, we initialize the study by analyzing the expressive power and learning dynamics of DEQs. The main results are sketched as follows. ", "page_idx": 1}, {"type": "text", "text": ". We first propose a general separation showing that there exists a width- $^{m}$ DEQ which cannot be approximated to a constant accuracy by an FNN with depth $O(m^{\\alpha})$ for $\\alpha\\in(0,1)$ unless its width is $\\exp\\bigl(\\Omega(m^{1-\\alpha})\\bigr)$ . This is achieved by comparing the the number of linear regions that the two networks can generate. Based on the result, we further prove that a width- ${}^{m}$ DEQ can generate at most $2^{m}$ linear regions, which has provable advantages than FNNs. ", "page_idx": 1}, {"type": "text", "text": "2. We then propose another separation, where a steep function in $[0,1]^{d}$ being the solution to fixed point equation is considered as the target function. We show that a DEQ with size and magnitude bounded by $O(\\varepsilon^{-1})$ can approximate this function to $O(\\varepsilon)$ -accuracy in $L^{\\infty}$ norm, whereas an FNN with bounded depth and exponentially bounded width cannot unless its weights is $\\exp(\\Omega(d))$ . For the technical contribution, we manage to show that an approximation of the fixed point mapping by the implicit layer can also guarantee the approximation the solution defined by the fixed point equation even if the Lipschitz constant of the fixed point mapping is very close to 1 by a new observation as shown in Lemma 3. 3. Finally, we study the bias of DEQ from the perspective of learning dynamics. We propose a general characterization of regularization for gradient flow in an overparameterized setting. We further analyze the dynamics of both gradient flow and gradient descent, showing that under mild conditions, convergence is guaranteed, and the model tends to produce \u2018dense\u2019 features. Then we offer a concrete example on a specific Out-of-Distribution (OOD) task, demonstrating that this bias can help reduce the OOD error. ", "page_idx": 1}, {"type": "text", "text": "Finally, we conduct experiments to validate our theoretical results. From the overall study, a high-level conjecture is that DEQ has potential advantages in learning certain high-frequency components. ", "page_idx": 1}, {"type": "text", "text": "Notations. We use standard notation $O(\\cdot)$ and $\\Omega(\\cdot)$ to hide constants. We use $\\sigma$ to denote the ReLU function, i.e., $\\sigma(x)=\\operatorname*{max}(0,x)$ , and we use $\\operatorname{sgn}({\\mathord{\\cdot}})$ to denote the sign function. We use $\\mathrm{diag(\\cdot)}$ to transform a vector into a diagonal matrix with the vector\u2019s elements on the diagonal. We denote by $\\|\\cdot\\|_{p}$ the $\\,\\ell^{p}$ vector norm or the subordinate matrix norm, and by $\\|h\\|_{L^{p}(k)}$ the $L^{p}$ -norm of a function $h$ on a compact set $K$ . For a vector or vector-valued function $\\mathbf{v}$ , we denote $v_{i}$ the $i$ -th entry of the vector or the function. For a function $u:\\mathbb{R}\\rightarrow\\mathbb{R}$ , we denote $u^{\\circ n}$ the $n$ -fold composition of $u$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section we briefly review the literature that are most related to us. ", "page_idx": 1}, {"type": "text", "text": "Theoretical Studies on DEQs. Theoretical research on DEQs has primarily focused on ensuring their well-posedness [6, 7]. To guarantee well-posedness, strategies like new parameterizations [6, 7], regularization [15], and special initialization [8] were proposed. Another research line delves into the learning properties of DEQ. The expressivity of DEQ is preliminarily studied in [2]. Regarding learning dynamics, some works [16, 17, 13] couple the dynamics of over-parameterized DEQs with a linear kernel using the NTK method. They manage to prove the global convergence and study the generalization [17]. Other studies examine DEQs in a linear framework or infinite-width limit. For instance, Kawaguchi [16] studies the convergence of linear DEQ, while Gao et al. [18] explore information propagation in DEQ and show its equivalence to a Gaussian process in the infinite-width limit. Nevertheless, an in-depth study on the potential or quantifiable advantage of DEQ over FNN is still lacking. ", "page_idx": 2}, {"type": "text", "text": "Separations on Expressivity of Neural Networks. The separation on expressivity of neural networks is a fundamental study characterizing functions that can be approximated efficiently by one neural architecture but cannot by another. These architectures include FNNs [19, 20], CNNs [21], RNNs [22], etc. Since DEQ can be viewed as an infinitely deep weight-tied network, depth separation [23] is most relevant to our study. A key study by Telgarsky [24] constructs a saw-tooth function with many oscillations to give a separation, which further inspires a series of separations [25, 26, 27]. In addition to depth, some recent works study the separation regrading the overall number of neurons [28] or the magnitude of parameters [29] of the networks. In this paper, the first separation is also inspired by Telgarsky\u2019s construction, whereas we focus on the separation between DEQ and FNN and provide a more refined analysis of networks\u2019 depth. The second separation is new. ", "page_idx": 2}, {"type": "text", "text": "Implicit Bias of Learning Dynamics on Neural Networks. The implicit bias of learning dynamics plays a key role in determining what particular optima can be found by the algorithms when there are multiple optima. A series of papers study the implicit regularization of gradient-based methods, showing that under varying settings, these algorithms bias towards solutions with specific properties [30, 31], such as norm minimization [32], sparsity [33] and low complexity [34, 35, 36]. Due to the theoretical barrier in analyzing nonlinear neural networks [37], most existing works focus on simplified models such as random feature models [38, 32], networks with quadratic activations [39] and diagonal linear networks [33]. This paper follows similar strategies and analyzes the implicit bias of a simplified diagonal linear DEQ from learning dynamics. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries of DEQ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The DEQ is an implicit-depth model [2] that employs the same weights in each layer block of a feedforward neural network and extends the number of layer to infinity. The layer blocks used in DEQ can be fully connected, convolutional, or Transformer blocks, resulting in different variants of deep equilibrium networks. In this paper, we consider a vanilla DEQ with ReLU activation as the generalization of an FNN for the separation result and a simplified linear diagonal DEQ for the bias analysis. Specifically, an $L$ -layer FNN from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{s}$ can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}^{1}=\\mathbf{x};\\quad\\mathbf{z}^{i+1}=\\sigma(\\mathbf{W}_{i}\\mathbf{z}^{i}+\\mathbf{b}_{i}),\\quad1\\leq i\\leq L-2;\\quad\\mathbf{y}=\\mathbf{W}_{L}\\mathbf{z}^{L},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}\\in\\mathbb{R}^{d}$ and $\\mathbf{y}\\in\\mathbb{R}^{s}$ . In DEQ, each $\\mathbf{W}_{i}$ and ${\\bf b}_{i}$ in Eq. (1) is replaced by the same weight W and bias $\\mathbf{b}$ , and a linear transform of the input ${\\bf U}\\,{\\bf x}$ is added to each layer, i.e., $\\mathbf{z}^{l}\\,{\\overset{\\cdot}{=}}\\,\\sigma(\\mathbf{W}\\,\\mathbf{z}^{l-1}\\,{\\overset{\\cdot}{+}}\\,\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b})$ for all $l$ . By extending the layer $l$ to infinity, the feature and the prediction of this DEQ can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}=\\sigma(\\mathbf{W}\\,\\mathbf{z}+\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}),}\\\\ &{\\mathbf{y}=\\mathbf{A}\\,\\mathbf{z},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{W}\\in\\mathbb{R}^{m\\times m},\\mathbf{U}\\in\\mathbb{R}^{m\\times d},b\\in\\mathbb{R}^{m},$ , and $\\mathbf{A}\\in\\mathbb{R}^{s\\times m}$ . We call $\\sigma(\\mathbf{W_{\\deltaZ}}+\\mathbf{U_{\\deltaX}}+\\mathbf{b})$ the implicit layer and $m$ the width of DEQ. In this paper, we mainly consider $s=1$ , i.e., DEQ as a scalar function on $\\mathbb{R}^{d}$ . ", "page_idx": 2}, {"type": "text", "text": "In [2], the authors show that every FNN can be reformulated as a large DEQ with specific weight reparameterization. Specifically, the depth- $L$ FNN described in Eq. (1) is equivalent to a DEQ in the ", "page_idx": 2}, {"type": "text", "text": "form of Eq.(2) with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\left(\\mathbf{0},\\quad\\cdots\\quad,\\mathbf{W}_{L}\\right),\\mathbf{W}=\\left(\\mathbf{W}_{2}^{\\mathbf{0}}\\quad\\mathbf{0}\\\\ &{\\mathbf{W}_{3}}\\quad\\mathbf{0}\\\\ &{\\ddots}\\quad\\ddots}\\\\ &{\\mathbf{W}_{L-1}\\quad\\mathbf{0}}\\end{array}\\right),\\mathbf{U}=\\left(\\begin{array}{l}{\\mathbf{W}_{1}}\\\\ {\\mathbf{0}}\\\\ {\\vdots}\\\\ {\\mathbf{0}}\\end{array}\\right),\\mathbf{b}=\\left(\\begin{array}{l}{\\mathbf{b}_{1}}\\\\ {\\mathbf{b}_{2}}\\\\ {\\vdots}\\\\ {\\mathbf{b}_{L-1}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4 Separation on the Expressivity of DEQ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we focus on the separations on the expressivity of DEQ with ReLU activation. We follow the common ways to compare the expressivity from the actual size (width and depth) of the networks. More explanations on the fairness of the comparison are provided in Appendix A.4. We will show that DEQ is more parameter-efficient in approximating specific target functions than FNN. ", "page_idx": 3}, {"type": "text", "text": "4.1 General Separation over FNNs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The following theorem states a general separation between DEQ and FNN from the size of networks. The motivation behind the theorem is a common observation that functions with many linear pieces are typically hard to be approximated by functions having fewer linear pieces. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $m\\in\\mathbb{N}^{+}$ . Assume that $L\\le m^{\\alpha}$ for some $0<\\alpha<1$ . Then there exists a function $N_{d}:[0,1]^{d}\\to\\mathbb{R}$ computed by a width-m ReLU-DEQ, such that for any function $N_{f}:[0,\\bar{1}]^{d}\\to\\mathbb{R}$ computed by a depth- $L$ ReLU- FNN with width at most $2^{m^{1-\\alpha}-1}$ , it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\int_{[0,1]^{d}}|N_{d}(\\mathbf{x})-N_{f}(\\mathbf{x})|\\mathrm{d}\\,\\mathbf{x}\\geq\\frac{1}{16}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof is provided in Appendix A.1. It involves quantifying the number of linear regions1 generated by a DEQ compared to an FNN. Specifically, we show in the proof that there exists a DEQ producing $2^{m}$ linear pieces whereas no-so-deep FNNs, i.e., FNNs with depth $O(m^{\\alpha})$ cannot generate such a large number of linear regions unless the width is sub-exponentially large. ", "page_idx": 3}, {"type": "text", "text": "Moreover, the example of the hard-to-approximate DEQ enables us to derive an exact bound on the number of linear regions that a DEQ can generate. This result is of independent interest and is stated in the proposition below. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $m>0$ . A width-m ReLU-DEQ has at most $2^{m}$ linear regions in the input space. Moreover, this upper bound is attainable, i.e., there exists a width-m ReLU-DEQ that computes a function with $2^{m}$ linear regions on $\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. As a comparison, the work of [40] analyzes ReLU-FNNs. It shows that for a ReLU-FNN with a total of $\\tilde{N}$ neurons of arbitrary depth, the maximal number of linear regions is bounded above by $2^{\\tilde{N}}$ . To the best of our knowledge, it is yet to be determined whether this bound is achievable. Moreover, there is evidence suggesting that this upper bound is not achievable for FNNs that when the input dimension is 1 (See Lemma 5 in Appendix A.1 for details). Consequently, width-m DEQs can potentially generate a larger number of linear regions compared to FNNs with m neurons, as DEQs have been shown to achieve their upper bound. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 shows that there exists a width- $^m$ DEQ that is hard to be approximated by FNN with depth $O(m^{\\alpha})$ . This theorem along with Proposition 1 reveals that, although DEQ computes features by solving an equilibrium function induced by a shallow implicit layer, its complexity in terms of expressing linear regions of DEQ can be larger than that of not-so-deep FNN. ", "page_idx": 3}, {"type": "text", "text": "4.2 Separation on Certain Steep Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present another separation concerning both the size and parameter magnitude of neural networks, which more explicitly reveals the bias and potential advantages of DEQ on ", "page_idx": 3}, {"type": "text", "text": "expressivity. The separation is based on the observation that the fixed point of a DEQ can be rewritten as the solution to an optimization problem under certain conditions. ", "page_idx": 4}, {"type": "text", "text": "To be specific, consider a simple quadratic optimization problem with the optimization variable $\\mathbf{z}\\in\\mathbb{R}^{m}$ and a parameter $\\mathbf{x}\\in\\mathbb{R}^{d}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{z}}\\ {\\frac{1}{2}}\\mathbf{z}^{T}\\mathbf{A}(\\mathbf{x})\\,\\mathbf{z}+\\mathbf{b}^{T}(\\mathbf{x})\\,\\mathbf{z}+\\mathbf{c},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{A}(\\mathbf{x})$ is a positive definite matrix parameterized by $\\mathbf{x}$ and $\\eta\\mathbf{I}\\ \\succ\\ \\mathbf{A}(\\mathbf{x})\\ \\succ\\ \\mathbf{0}$ for some $\\eta>0$ . Approximating $\\mathbf{z}=\\mathbf{z}(\\mathbf{x})$ , i.e., the optimum as a function of the parameter $\\mathbf{x}$ , serves useful primitives in various applications. Directly approximating $\\mathbf{z}(\\mathbf{x})$ by FNN requires the approximation of $\\mathbf{z}(\\mathbf{x})\\,=\\,-\\mathbf{A}(\\mathbf{x})^{-1}\\,\\mathbf{\\dot{b}}(\\mathbf{x})$ . On the other hand, from the optimality condition, $\\mathbf{z}(\\mathbf{x})$ is implicitly defined through fixed point equation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}=\\mathbf{z}-\\frac{1}{\\eta}\\left(\\mathbf{A}(\\mathbf{x})\\,\\mathbf{z}+\\mathbf{b}(\\mathbf{x})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hence, approximating $\\mathbf{z}(\\mathbf{x})$ by DEQ may only require the approximation of the fixed point mapping $\\begin{array}{r}{{\\mathbf{z}}-\\frac{1}{\\eta}\\left({\\mathbf{A}}\\mathbf{\\dot{(x)}}\\,{\\mathbf{z}}+{\\mathbf{b}}({\\mathbf{x}})\\right)}\\end{array}$ by the implicit layer. To some extent, the approximation problem is \u2018altered\u2019 due to the model difference, which possibly leads to distinctive division in approximation. ", "page_idx": 4}, {"type": "text", "text": "Now, we construct a workable instance. The objective function of our central interest is a special case of Eq.(4) given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z}(1+\\delta-x_{1})z^{2}-\\delta x_{1}z,\\quad\\mathbf{x}\\in[0,1]^{d},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\delta=2^{-d}$ . The solution function is calculated as ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(\\mathbf{x})=\\frac{\\delta x_{1}}{2(1+\\delta-x_{1})},\\quad\\mathbf{x}\\in[0,1]^{d},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and it can also be determined by the following fixed point equation ", "page_idx": 4}, {"type": "equation", "text": "$$\nz=\\tilde{g}(z,\\mathbf{x}):=(x_{1}-\\delta)z+\\frac{1}{2}\\delta x_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $g\\mathbf{(x)}$ has very large derivative when $x_{1}$ is near 1. It can be regarded as a continuous version of the common indicator function of the first entry ${\\scriptstyle{\\frac{1}{2}}}\\mathbf{1}_{x_{1}=1}(\\mathbf{x})$ . The separation is presented as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $g\\mathbf{(x)}$ be defined as in Eq.(6) for $\\mathbf{x}\\in[0,1]^{d}$ and $\\begin{array}{r}{\\frac14\\geq\\varepsilon>0}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "A. For any function $N_{f n n}(x)$ implemented by an FNN with depth $L$ and width $k$ where $L\\leq C$ and $k\\leq2^{\\frac{d}{2C}}$ for some constant $C=O(1)$ . If ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|N_{f n n}(\\mathbf{x})-g(\\mathbf{x})\\|_{L^{\\infty}([0,1]^{d})}\\leq\\frac{1}{16},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then there exists a weight parameter $W_{i j}$ of the FNN for $1\\leq i\\leq L$ and $1\\le j\\le k$ , such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n|W_{i j}|\\geq2^{\\frac{d}{2C}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "B. There exists a function $N_{d e q}$ implemented by a DEQ with width bounded by $5\\varepsilon^{-1}$ and weights bounded $2\\varepsilon^{-1}$ , such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|N_{d e q}(\\mathbf{x})-g(\\mathbf{x})\\|_{L^{\\infty}([0,1]^{d})}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 2. The inapproximability result of FNN in Theorem 2 is stated from the perspective of weight magnitude, which holds practical significance. Exponentially large weight often results in exponential iterations of optimization algorithms in learning with this model, as also noted in [41]. Additionally, neural networks in practice typically have small weights due to techniques such as (standard) small initialization, normalization, and gradient clipping. ", "page_idx": 4}, {"type": "text", "text": "The proof is shown in Appendix A.2. In Theorem 2, the inapproximability of FNNs is relatively simple: Direct calculation shows that the derivative of the target function $g\\mathbf{(x)}$ is exponentially large when $x_{1}>1-\\delta$ . To approximate $g\\mathbf{(x)}$ in $L^{\\infty}$ norm requires FNNs to have large derivative in certain region, resulting in exponentially large weight for FNNs with bounded depth. On the other hand, the proof of the approximability of DEQs is more technical. While $\\tilde{g}$ in Eq. (7) seems more benign, it is not clear how to construct the approximation using the implicit layer in Eq. (2) that resembles an 1-layer FNN with very limited expressive power. Moreover, even if we manage to approximate $\\tilde{g}$ in Eq. (7), it will not necessarily imply a good approximation between the fixed point of DEQ and the solution of $z=\\tilde{g}(z,\\mathbf{x})$ , i.e., the target function due to the Lipschitz constant of $\\tilde{g}$ with respect to $z$ being very close to 1 when $x_{1}$ is around 1 according to Eq. (7). We provide a proof sketch of this result in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Further insights and implications can be gleaned from Theorem 2. First, it suggests that DEQ may excel in approximating functions induced by fixed-point iterations. In other words, DEQ may be better suited for representing algorithms. Second, Theorem 2 implies that functions with large derivative, or high-frequency components, may be approximated more efficiently by DEQ, as the function to be approximated by the implicit layer can have much smaller derivative. ", "page_idx": 5}, {"type": "text", "text": "4.3 Proof Sketch of B. in Theorem 2 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As discussed in Section 4.2, we want to approximate $\\tilde{g}$ using the implicit layer of DEQ. Due to the limited expressive power of the implicit layer, we propose an equivalent reparameterization of DEQ. Lemma 1. Consider a revised DEQ defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}=\\mathbf{V}\\sigma(\\mathbf{W}\\,\\mathbf{z}+\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}),}\\\\ &{\\mathbf{y}=\\mathbf{B}\\,\\mathbf{z},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{x}\\in\\mathbb{R}^{d},\\mathbf{z}\\in\\mathbb{R}^{m},\\mathbf{W}\\in\\mathbb{R}^{q\\times m},\\mathbf{U}\\in\\mathbb{R}^{q\\times d},$ , $\\mathbf{V}\\in\\mathbb{R}^{m\\times q},$ , $b\\in\\mathbb{R}^{q}$ , $\\mathbf{B}\\in\\mathbb{R}^{p\\times m}$ and $\\|\\mathbf{W}\\mathbf{V}\\|_{2}\\leq$ 1. Then any revised $D E Q$ can be represented by a vanilla DEQ defined as in Eq. (2) with width $q$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 enables us to approximate $\\tilde{g}(z,\\mathbf{x})$ using the revised implicit layer, denoted by $\\tilde{h}(z,\\mathbf{x})$ . Then the crux of the proof centered in bounding the error between the equilibria of two fixed-point equations. To begin, for every $\\mathbf{x}$ we denote $\\hat{u}(z)=z-\\tilde{g}(z,\\mathbf{x}),\\,\\hat{v}(z)=z-\\tilde{h}(z,\\mathbf{x})$ and consider $|\\hat{u}^{\\mathrm{e}2}(z)-\\hat{v}^{\\mathrm{e}2}(z)|$ . Suppose that $\\hat{u}(z)$ is $L_{\\hat{u}}$ -Lipschitz, then we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{u}^{\\circ2}(z)-\\hat{v}^{\\circ2}(z)|\\leq|\\hat{u}^{\\circ2}(z)-\\hat{u}\\circ\\hat{v}(z)|+|\\hat{u}\\circ\\hat{v}(z)-\\hat{v}^{\\circ2}(z)|\\leq(L_{\\hat{u}}+1)|\\hat{u}(z)-\\hat{v}(z)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus if $L_{\\hat{u}}<1$ , by recursion, we can bound distance the between the infinitely composition of $\\hat{u}(z)$ and $\\hat{v}(z)$ , from which the error of the two fixed points can be bounded. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Let $\\Omega\\subset\\mathbb{R}$ be a compact set, and $u(z,\\mathbf{x}),v(z,\\mathbf{x}):\\Omega\\times[0,1]^{d}\\to\\Omega$ be two functions. Assume that for all $\\mathbf{x}\\in[0,1]^{d}$ , $u(\\cdot,\\mathbf{x})$ and $v(\\cdot,\\mathbf{x})$ are Lipschitz continuous with Lipschitz constant $L_{u},L_{v}<1$ , respectively. Then for any $\\mathbf{x}\\in[0,1]^{d}$ , it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|z_{u}({\\bf x})-z_{v}({\\bf x})|\\leq\\operatorname*{min}\\{(1-L_{u})^{-1},(1-L_{v})^{-1}\\}\\cdot\\operatorname*{max}_{z\\in\\Omega}|u(z,{\\bf x})-v(z,{\\bf x})|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z_{u}(\\mathbf{x})$ and $z_{v}(\\mathbf{x})$ are the fixed point of $z=u(z,\\mathbf{x})$ and $z=v(z,\\mathbf{x}),$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "In our case, $u(z,\\mathbf{x})$ and $v(z,\\mathbf{x})$ in this Lemma represent $\\tilde{g}(z,\\mathbf{x})$ and $\\tilde{h}(z,\\mathbf{x})$ , respectively. When $x_{1}<1-\\mathrm{poly}(d)^{-1}$ , by calculating $\\frac{\\partial\\tilde{g}(z,\\mathbf{x})}{\\partial z}$ , we have $(1-L_{\\tilde{g}})^{-1}<\\mathrm{poly}(d)$ . Leveraging this and Lemma 2, we just need $\\|\\tilde{h}-\\tilde{g}\\|_{\\infty}\\,\\le\\,\\mathrm{poly}(d)^{-1}$ to achieve a final accuracy of $O(\\varepsilon)$ . However, when $x\\ge1-\\delta$ , we only have $(1-L_{\\tilde{g}})^{-1}<\\exp(\\Omega(d))$ , which may necessitate an exponential width for the implicit layer to achieve $O(\\varepsilon)$ accuracy. In fact, $\\tilde{h}(z,\\mathbf{x})\\,=\\,x_{i}z$ gives an example that even assuming $\\|\\tilde{h}-\\tilde{g}\\|_{\\infty}\\,\\le\\,\\mathrm{exp}(\\Omega(d))^{-1}$ is not sufficient to achieve $O(\\varepsilon)$ accuracy since $\\begin{array}{r}{z_{\\tilde{h}}(1)-z_{\\tilde{g}}(1)=\\frac{1}{2}}\\end{array}$ . So it seems difficult to bound the error without a specific structure of $\\tilde{h}$ . To overcome the issue, we observe a novel property that enables us to effectively bound the error. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3. Let $\\xi>0$ . Under the conditions in Lemma 2, if for any interval $T\\subset\\Omega$ with diam $\\mathbf{\\nabla}(T)>\\xi_{i}$ , $u(z,\\mathbf{x})=v(z,\\mathbf{x})$ has a zero in $T$ for all $\\mathbf{x}$ , then it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|z_{u}(\\mathbf{x})-z_{v}(\\mathbf{x})|\\leq\\xi,\\quad\\forall\\,\\mathbf{x}\\in[0,1]^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The intuition behind Lemma 3 is that if for any x, $z-u(z,\\mathbf{x})$ and $z-v(z,\\mathbf{x})$ as two monotone univariate functions w.r.t. $z$ can take the same value at frequent intervals, then their zeros will also be close to each other. By using this Lemma, it suffices to construct such $\\tilde{h}(z,\\mathbf{x})$ that equals $\\tilde{g}(z,\\mathbf{x})$ at frequent interval of length $O(\\varepsilon)$ for every $\\mathbf{x}$ . ", "page_idx": 5}, {"type": "text", "text": "5 The Bias on Learning Dynamics of DEQ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we study the implicit bias of a simplified linear diagonal DEQ and present a concrete example illustrating how such an implicit bias may improve generalization. Specifically, we focus on the overparameterized setting and our analysis is beyond the lazy training regime. ", "page_idx": 6}, {"type": "text", "text": "To ensure tractability, we follow the common techniques (e.g. see [31, 39]) to reduce a matrix problem to a vector problem by considering only the diagonal elements of the weight matrix. Our primary focus is on the following model: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf(\\mathbf{w},\\mathbf{x})=\\sum_{i=1}^{d}{\\frac{1}{1-w_{i}}}x_{i}:=\\langle{\\boldsymbol{\\beta}},\\mathbf{x}\\rangle,\\quad{\\boldsymbol{\\beta}}_{i}={\\frac{1}{1-w_{i}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The model can be regarded as a diagonal linear DEQ in Eq. (2) with activation $\\sigma={\\mathrm{Id}}$ , weights $\\mathbf{W}=\\mathrm{diag}(w_{1},w_{2},\\cdot\\cdot\\cdot\\cdot,w_{d})$ , $\\mathbf{U}=\\mathbf{I}_{d}$ , $\\mathbf{b}=\\mathbf{0}$ and $\\mathbf{A}=(1,\\bar{1},\\cdots,1)^{T}\\in\\mathbb{R}^{d}$ . Although simplified, this model is essentially a nonlinear model and it retains the implicit nature of DEQ. ", "page_idx": 6}, {"type": "text", "text": "Our primary focus lies in minimizing the expected square loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}L(\\mathbf{w}):=\\frac{1}{2}\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(y-f(\\mathbf{w},\\mathbf{x}))^{2}].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We are given access to a set of i.i.d. training examples $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ , and we denote the (half) square loss on these examples by $\\begin{array}{r}{\\hat{L}(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^{N}(y_{i}-f(\\mathbf{w},\\dot{x_{i}}))^{2}}\\end{array}$ . As mentioned above, we focus on the overparameterized setting that $N\\leq d$ . Moreover, let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X}=(\\mathbf{x}_{1},\\cdot\\cdot\\cdot,\\mathbf{x}_{N})^{T},\\quad\\mu_{\\operatorname*{min}}=\\lambda_{\\operatorname*{min}}(\\mathbf{X}\\mathbf{X}^{T}),\\quad\\mu_{\\operatorname*{max}}=\\lambda_{\\operatorname*{max}}(\\mathbf{X}\\mathbf{X}^{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu_{\\mathrm{min}}>0$ can hold when $N\\leq d$ and the data matrix $\\mathbf{X}$ is of full rank. We mainly consider the dynamics of gradient flow (GF) and gradient descent (GD) with fixed stepsize $\\eta$ on minimizing $\\hat{L}(\\mathbf{w})$ , expressed as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\dot{\\mathbf{w}}(t)=-\\nabla_{\\mathbf{w}}\\hat{L}(\\mathbf{w}(t));\\quad(\\mathrm{GD})\\quad\\mathbf{w}^{k+1}=\\mathbf{w}^{k}-\\eta\\nabla_{\\mathbf{w}}\\hat{L}(\\mathbf{w}^{k}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The main theorem below gives a general characterization of the bias of diagonal linear DEQ in the overparameterized regime. The proof is based on the technique proposed in [31]. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Let $\\beta_{i}$ in Eq. (9) be initialed as $\\beta_{i}(0)>0$ for all i. Suppose that gradient flow for the parameterization problem in Eq. (10) converges to some $\\hat{\\beta}$ satisfying $\\mathbf{X}{\\hat{\\boldsymbol{\\beta}}}=\\mathbf{y}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\beta}=\\underset{\\beta}{\\operatorname{argmin}}\\,Q(\\beta),\\quad s.t.\\ \\mathbf{X}\\beta=\\mathbf{y},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{Q(\\beta)=\\sum_{i=1}^{d}q_{i}(\\beta_{i})}\\end{array}$ and $\\begin{array}{r}{q_{i}(x)=\\frac{1}{2x^{2}}+\\beta_{i}(0)^{-3}x}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Remark 3. In Theorem $^3$ , our proof shows that $\\beta(t)$ remains positive for all entries throughout the training process. Within the space where $\\beta>0$ , $\\overset{\\cdot}{Q}(\\beta)$ is convex and has a unique minimum. The restriction to positive entries arises from our simplification on A in Eq. (9) to be an all-one vector. To accommodate negative entries, one could assign $-1$ to the corresponding entry. In this case, $q_{i}(x)$ becomes $\\begin{array}{r}{q_{i}(x)=\\frac{\\widecheck{1}}{2x^{2}}-\\beta_{i}(0)^{-3}x}\\end{array}$ with $\\beta_{i}(0)<\\bar{0},$ , which is convex for $x<0$ . ", "page_idx": 6}, {"type": "text", "text": "All the proofs in this section are included in Appendix A.3. The theorem implies that the bias of the (simplified) DEQ significantly differs from that of conventional linear models and two-layer linear network which tends to give a minimum $\\ell_{2}$ -norm interpolator [42]. Specifically, the predictor $\\hat{\\beta}$ hardly admits parameters of small magnitude due to the penalty term $\\textstyle{\\frac{1}{2}}\\sum_{i=1}^{d}{\\frac{1}{\\beta_{i}^{2}}}$ . Meanwhile, the predictor can endure parameters of greater magnitude as the penalty $q_{i}(x)$ increase almost linearly when $x$ is large. ", "page_idx": 6}, {"type": "text", "text": "We then study the implicit bias from the learning dynamics of GF and GD. We show that when $\\mu_{\\mathrm{min}}>0$ , under mild conditions, the convergence of both algorithms is guaranteed. Moreover, in this case, a positive lower bound of the $\\ell_{\\infty}$ norm of the iterates can be derived, indicating that the model inclines to produce \u2018dense\u2019 features in learning process. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. Denote by $\\beta_{0}$ the initialization of $\\beta$ of the model in Eq. (9). There exists an optima $\\hat{\\beta}^{*}$ , i.e., $\\mathbf{X}\\,{\\hat{\\boldsymbol{\\beta}}}^{*}=\\mathbf{y}$ and a constant $c>0$ , such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\beta}^{*}\\|_{\\infty}-\\|\\hat{\\beta}^{*}-\\beta_{0}\\|_{2}\\geq c>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let $\\{\\beta(t)\\}$ be the process following $G F$ in Eq. (11) and $\\{\\beta^{k}\\}$ the iterates following GD in $E q$ . (11). Assume that $\\mu_{m i n}>0$ and the initialization $\\beta(0)$ and $\\beta^{0}$ satisfy Assumption $^{\\,l}$ with an optima $\\hat{\\beta}^{*}$ . ", "page_idx": 7}, {"type": "text", "text": "A. $\\{\\beta(t)\\}$ converges to an optima $\\beta_{f}^{\\infty}$ with $\\|\\beta_{f}^{\\infty}\\|_{\\infty}\\geq c$ . Moreover, for any $t\\geq0$ , we have $c\\leq\\|\\beta(t)\\|_{\\infty}\\leq\\|\\hat{\\beta}^{*}\\|_{\\infty}+\\|\\hat{\\beta}^{*}-\\beta_{0}\\|_{2}$ . B. If there exists a constant $C>0$ such that $\\|\\beta^{k}\\|_{\\infty}\\leq C$ for all $k$ , then $\\{\\beta^{k}\\}$ converges to an optima $\\beta_{d}^{\\infty}$ with $\\|\\beta_{d}^{\\infty}\\|_{\\infty}\\geq c.$ . Moreover, for any $k\\geq0$ , we have $c\\leq\\|\\beta^{k}\\|_{\\infty}\\leq C$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4. The assumption in Theorem $^{4}$ that $\\|\\beta^{k}\\|_{\\infty}$ is uniformly bounded can be removed if we manually incorporate a constraint on $\\beta$ and optimize the problem using projected gradient descent. In practice, certain reparameterization tricks $I6,7J$ are proposed to ensure that $\\mathbf{I}-\\mathbf{W}\\succeq m I$ for some $m>0$ , thus corresponding to the aforementioned assumption. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 does not require the updates to stay in a small domain near the initialization, so it is beyond the lazy training regime. Importantly, the \u2018dense\u2019 bias observed in $\\beta$ is not a direct consequence of our model assumption even though we only assume the diagonal elements to be nonzero. In fact, utilizing the diagonal elements can still lead to sparse features (e.g., see [39]). We believe that this bias in DEQs stems essentially from their network architecture. On the other hand, the current implicit bias holds for GF and GD, whereas other optimization algorithms may induce different implicit biases, which we aim to explore in future work. ", "page_idx": 7}, {"type": "text", "text": "Based on our results above, we now provide a concrete example to show the advantages brought by the bias of DEQ in out-of-distribution (OOD) tasks. This is motivated by the fact that diversifying spurious features can improve OOD generalization [43]. Specifically, we focus on generalization on the unseen domain (GOTU) setting [36], a rather strong case of OOD generalization where part of the distribution domain is unseen at training but used at testing. As an example, we here utilize the setting in Theorem 3.11 in [36]. Consider the sample space $\\bar{S^{\\mathrm{}}}=\\{-1,1\\}^{d}$ and a linear boolean function $f:S\\rightarrow\\mathbb{R}$ defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(\\ensuremath{\\mathbf{x}})=\\hat{f}(\\varnothing)+\\sum_{i=1}^{d}\\hat{f}(\\{i\\})x_{i},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\hat{f}(\\{i\\})\\,=\\,\\mathbb{E}_{{\\mathbf{x}}\\sim{\\mathbf{\\boldsymbol{U}}}\\left\\{-1,1\\right\\}^{d}}[x_{i}f(\\mathbf{x})],\\;\\hat{f}(\\emptyset)\\,=\\,\\mathbb{E}_{X\\sim{\\boldsymbol{U}}\\left\\{-1,1\\right\\}^{d}}[f(\\mathbf{x})]$ and ${\\sim}\\boldsymbol{U}^{\\mathcal{U}}$ refers to uniform sampling from $\\boldsymbol{\\mathcal{U}}$ . In training, the $k$ -th component of every accessible sample is fixed as 1, i.e., the unseen domain is $\\mathcal{U}=\\{\\mathbf{x}\\in\\{\\pm1\\}^{d}:x_{k}=-1\\}$ . Denote by $\\tilde{f}_{S\\backslash\\mathcal{U}}$ the function learned on $\\scriptstyle{S\\backslash\\mathcal{U}}$ . The GOTU error is the defined as the generalization completely on the unseen domain, i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\nG O T U(f,\\tilde{f},\\mathcal{U})=\\mathbb{E}_{X\\sim\\upsilon}\\mathcal{U}[l(\\tilde{f}_{S\\backslash\\mathcal{U}}(X),f(X))],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $l$ is the quadratic loss function. It is shown in [36] that learning this function with diagonal linear network results in a GOTU error of $4\\hat{f}(\\{k\\})^{2}+O(\\varepsilon)$ for an infinitesimal $\\varepsilon$ . On the other hand, the following proposition shows that under mild conditions, learning such function with DEQ achieves smaller GOTU error, where we consider DEQ in Eq. (9) with a bias term, i.e., $\\begin{array}{r}{f(\\mathbf{w},\\mathbf{x})=\\sum_{i=1}^{d}\\frac{1}{1-w_{i}}x_{i}+b.}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 2. Let $f(\\mathbf{x})$ be defined as in Eq. (13). Assume that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{f}(\\{i\\})>0,\\quad\\forall1\\le i\\le d,\\quad\\hat{f}(\\{k\\})>1,\\quad|\\hat{f}(\\emptyset)|\\le2|\\hat{f}(\\{k\\})|.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Consider learning $f$ using gradient flow on population loss2 on a linear diagonal DEQ with bias initialized by $w_{i}(0)=b(0)=0$ for all $i$ with unseen domain $\\mathcal{U}=\\{\\mathbf{x}\\in\\{\\pm1\\}^{d}:x_{k}=-1\\}$ . Then the loss converges to 0, and it holds for the generalization error on the unseen that ", "page_idx": 7}, {"type": "equation", "text": "$$\nG O T U\\le4\\left(\\hat{f}(\\{k\\})-\\left(4+3\\hat{f}(\\{k\\})\\right)^{-\\frac{1}{3}}\\right)^{2}<4\\hat{f}(\\{k\\})^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this setting, the function $\\mathbf{x}\\mapsto x_{k}$ has a higher frequency component (i.e., degree) compared to the constant function 1. Consequently, the inductive bias of DEQ enables the model to capture some information about the high-frequency components. We further conduct experiments to study the potential advantages of DEQ in learning high-frequency components in Appendix B.2. ", "page_idx": 7}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/16dffb37940fedf7f16fed10e6a23c176731b82fc2351e03b07347778b7c14d3.jpg", "img_caption": ["Figure 1: Test losses of FNN and DEQ networks with various width $W$ and depth $L$ . (a) and (d) apply Sawtooth function I and $\\mathrm{II}$ with $2^{5}$ and $2^{10}$ linear regions, respectively. (b) and (e) apply function $g\\mathbf{(x)}$ defined in Eq. (5) with $\\delta=2^{-10}$ and $\\delta=2^{-2\\overline{{0}}}$ , respectively. (c) and (f) show the train loss and the GOTU error of FNN and DEQ on the boolean function $f_{1},\\,f_{2}$ with unseen domain given by Eq. (14) and Eq. (15). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct experiments on FNNs and DEQs based on our theoretical results. We first evaluate the expressivity of both networks on the functions proposed in our two separation results. Then we experiment on specific OOD tasks. Several additional experiments on audio representation and mutiscale DEQ are provided in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "Piecewise functions. We first verify the results in Section 4.1. The target function is designed as a saw-tooth function, as defined in Lemma 4 in Appendix A.1, which can be exactly computed by a DEQ. We set the number of linear regions of the saw-tooth function to $2^{5}$ and $2^{10}$ and experiments on other sawtooth functions can be seen in Appendix B.1. According to Proposition 1, a DEQ with width 5 and 10 can compute the above functions. ", "page_idx": 8}, {"type": "text", "text": "Figure 1(a) and Figure 1(d) show that DEQ can achieve nearly zero test loss, demonstrating the saw-tooth function with $2^{m}$ linear regions can be computed by DEQ. On the other hand, a not-so-deep and not-so-wide FNN fails to achieve test loss as low as DEQ, thus verifying the separation results between FNN and DEQ. ", "page_idx": 8}, {"type": "text", "text": "Solution to quadratic optimization problem. We then validate the ability of DEQ to approximate the solution function to certain optimization problems. We empirically demonstrate that DEQ can approximate such function better than an FNN with a similar number of parameters. We consider the objective function $g\\mathbf{(x)}$ defined in Eq. (6), with the input dimension $d\\,10$ and 20, and thus $\\delta$ in target function being $2^{-10}$ and $2^{-20}$ . The input space is $\\mathbf{x}\\in[0,1]^{d}$ with the sampling distribution $\\begin{array}{r}{p(\\Breve{\\mathbf{x}})\\,=\\,\\frac{1}{2(1-\\delta)}}\\end{array}$ for $0\\,<\\,x_{1}\\,<\\,1\\,-\\,\\delta$ and $\\begin{array}{r}{p(\\mathbf{\\dot{x}})=\\dot{\\frac{1}{2\\delta}}}\\end{array}$ for $1-\\delta<x_{1}<1$ . To verify results under different network parameters, we adjust the layer number and hidden dimension of FNN and the layer width of DEQ while keeping the total number of parameters of both networks similar. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 1(b) and Figure 1(e), for different network parameters and target functions, DEQ consistently achieves a lower test loss than FNN, demonstrating the superiority of DEQ to approximate and represent functions as solutions to certain optimization problems. ", "page_idx": 8}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/8d6064a81f5cba281d75ea9ea030f9ba33452552c8a12fa1e3ff745c3b3f6889.jpg", "img_caption": ["Figure 2: The heatmaps of diagonal DEQ, vanilla DEQ and FNN. We dispaly the magnitude of feature $z$ of DEQ and the magnitude of feature before the fully-connented layer of FNN. The $\\mathbf{X}$ -axis represents features 1-20 and darker colors indicate smaller features. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Out-of-Distribution tasks. We further perform experiments on the implicit bias of DEQ to verify the advantage of DEQ on OOD tasks. We consider 2 linear boolean functions $f:S\\to\\mathbb{R}$ in the form of Eq. (13) and unseen domains $\\mathcal{U}\\subset\\{\\pm1\\}^{d}$ . The first function is an example of the mean function and the second function is a part of DTFT. Experiments on other OOD functions can be found in Appendix B.1. ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}(x)=1.25x_{0}+1.25x_{1}+1.25x_{2}+\\cdots+1.25x_{10},\\quad\\mathcal{U}=\\{\\mathbf{x}\\in\\{\\pm1\\}^{10}:x_{2}=-1\\},}\\\\ &{f_{2}(x)=\\displaystyle\\sum_{n=0}^{9}\\sin\\left(\\frac{\\pi n}{10}\\right)x_{n},\\quad\\mathcal{U}=\\{\\mathbf{x}\\in\\{\\pm1\\}^{10}:x_{1}=-1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "For each experiment, we generate all binary sequences in $\\{\\pm1\\}^{d}\\backslash\\mathcal{U}$ for training. We employ AdamW optimizer with $\\ell_{2}$ loss and a cosine annealing scheduler. We can observe in Figure1 (c) that the GOTU error of $f_{1}$ is below the threshold of generalization error based on the Proposition 2. As shown in Figure1 (c) and Figure1 (f), the training loss converges to 0 and the generalization error on the unseen is bounded, which empirically demonstrates the advantage of DEQ on OOD tasks. ", "page_idx": 9}, {"type": "text", "text": "In Figure 2, we display the heatmap of the features of diagonal DEQ, vanilla DEQ and FNN trained on the target function with the unseen domain defined in Eq.(14). As the darker colors indicate smaller magnitude of features, we can confirm that the learned features of DEQ are indeed \u2018denser\u2019 than those of FNN. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we provide two separations of DEQ and FNN and analyze the bias of DEQ through the lens of learning dynamics. Our theoretical results provably show the advantage of DEQ over FNN in specific problems and quantify certain learning properties of DEQ. Overall, we conjecture that DEQ may be advantageous in learning certain high-frequency components. ", "page_idx": 9}, {"type": "text", "text": "There are many directions that remain to study. First, it is expecting to study the expressivity and bias of more general DEQs. Second, extending our bias analysis under gradient methods to stochastic methods is intriguing. Additionally, it remains promising to incorporate the advantages of DEQ in commonly used networks in real applications. Besides, we also aim to explore DEQ\u2019s potential in representing algorithms for reasoning or in-context learning, as evidence suggests the learned networks perform algorithms in some of these tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "C. Fang and Z. Lin were supported by National Key R&D Program of China (2022ZD0160300). C. Fang was also supported by the NSF China (No. 62376008). Z. Lin was also supported and the NSF China (No. 62276004). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. El Ghaoui, F. Gu, B. Travacca, A. Askari, and A. Tsai, \u201cImplicit deep learning,\u201d SIAM Journal on Mathematics of Data Science, 2021.   \n[2] S. Bai, J. Z. Kolter, and V. Koltun, \u201cDeep equilibrium models,\u201d Advances in Neural Information Processing Systems, 2019.   \n[3] S. Bai, V. Koltun, and J. Z. Kolter, \u201cMultiscale deep equilibrium models,\u201d Advances in Neural Information Processing Systems, 2020.   \n[4] A. Pokle, Z. Geng, and J. Z. Kolter, \u201cDeep equilibrium approaches to diffusion models,\u201d Advances in Neural Information Processing Systems, 2022.   \n[5] D. Gilton, G. Ongie, and R. Willett, \u201cDeep equilibrium architectures for inverse problems in imaging,\u201d IEEE Transactions on Computational Imaging, 2021.   \n[6] E. Winston and J. Z. Kolter, \u201cMonotone operator equilibrium networks,\u201d Advances in Neural Information Processing Systems, 2020.   \n[7] M. Revay, R. Wang, and I. R. Manchester, \u201cLipschitz bounded equilibrium networks,\u201d arXiv preprint arXiv:2010.01732, 2020.   \n[8] A. Agarwala and S. S. Schoenholz, \u201cDeep equilibrium networks are sensitive to initialization statistics,\u201d in International Conference on Machine Learning, 2022.   \n[9] A. Jacot, F. Gabriel, and C. Hongler, \u201cNeural tangent kernel: Convergence and generalization in neural networks,\u201d Advances in Neural Information Processing Systems, 2018.   \n[10] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai, \u201cGradient descent finds global minima of deep neural networks,\u201d in International Conference on Machine Learning, 2019.   \n[11] Z. Allen-Zhu, Y. Li, and Z. Song, \u201cA convergence theory for deep learning via overparameterization,\u201d in International Conference on Machine Learning, 2019.   \n[12] T. Gao, H. Liu, J. Liu, H. Rajan, and H. Gao, \u201cA global convergence theory for deep relu implicit networks via over-parameterization,\u201d in International Conference on Learning Representations, 2022.   \n[13] Z. Ling, X. Xie, Q. Wang, Z. Zhang, and Z. Lin, \u201cGlobal convergence of over-parameterized deep equilibrium models,\u201d in International Conference on Artificial Intelligence and Statistics, 2023.   \n[14] Z. Ling, L. Li, Z. Feng, Y. Zhang, F. Zhou, R. C. Qiu, and Z. Liao, \u201cDeep equilibrium models are almost equivalent to not-so-deep explicit models for high-dimensional gaussian mixtures,\u201d in International Conference on Machine Learning, 2024.   \n[15] S. Bai, V. Koltun, and J. Z. Kolter, \u201cStabilizing equilibrium models by jacobian regularization,\u201d in International Conference on Machine Learning, M. Meila and T. Zhang, Eds., 2021.   \n[16] K. Kawaguchi, \u201cOn the theory of implicit deep learning: Global convergence with implicit layers,\u201d in International Conference on Learning Representations, 2021.   \n[17] T. Gao and H. Gao, \u201cOn the optimization and generalization of overparameterized implicit neural networks,\u201d arXiv preprint arXiv:2209.15562, 2022.   \n[18] T. Gao, X. Huo, H. Liu, and H. Gao, \u201cWide neural networks as gaussian processes: Lessons from deep equilibrium models,\u201d Advances in Neural Information Processing Systems, 2023.   \n[19] R. Eldan and O. Shamir, \u201cThe power of depth for feedforward neural networks,\u201d in Conference on Learning Theory, 2016.   \n[20] M. Telgarsky, \u201cRepresentation benefits of deep feedforward networks,\u201d arXiv preprint arXiv:1509.08101, 2015.   \n[21] Z. Wang and L. Wu, \u201cTheoretical analysis of the inductive biases in deep convolutional networks,\u201d Advances in Neural Information Processing Systems, 2024.   \n[22] M. Emami, M. Sahraee-Ardakan, P. Pandit, S. Rangan, and A. K. Fletcher, \u201cImplicit bias of linear rnns,\u201d in International Conference on Machine Learning, 2021.   \n[23] A. Daniely, \u201cDepth separation for neural networks,\u201d in Conference on Learning Theory, 2017.   \n[24] M. Telgarsky, \u201cBeneftis of depth in neural networks,\u201d in Conference on Learning Theory, 2016.   \n[25] I. Safran and O. Shamir, \u201cDepth-width tradeoffs in approximating natural functions with neural networks,\u201d in International Conference on Machine Learning, 2017.   \n[26] V. Chatziafratis, S. G. Nagarajan, and I. Panageas, \u201cBetter depth-width trade-offs for neural networks through the lens of dynamical systems,\u201d in International Conference on Machine Learning, 2020.   \n[27] E. Malach, G. Yehudai, S. Shalev-Schwartz, and O. Shamir, \u201cThe connection between approximation, depth separation and learnability in neural networks,\u201d in Conference on Learning Theory, 2021.   \n[28] G. Vardi, D. Reichman, T. Pitassi, and O. Shamir, \u201cSize and depth separation in approximating benign functions with neural networks,\u201d in Conference on Learning Theory, 2021.   \n[29] G. Vardi and O. Shamir, \u201cNeural networks with small weights and depth-separation barriers,\u201d Advances in Neural Information Processing Systems, 2020.   \n[30] S. Gunasekar, J. Lee, D. Soudry, and N. Srebro, \u201cCharacterizing implicit bias in terms of optimization geometry,\u201d in International Conference on Machine Learning, 2018.   \n[31] B. Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Srebro, \u201cKernel and rich regimes in overparametrized models,\u201d in Conference on Learning Theory, 2020.   \n[32] P. L. Bartlett, A. Montanari, and A. Rakhlin, \u201cDeep learning: a statistical viewpoint,\u201d Acta Numerica, 2021.   \n[33] E. Moroshko, B. E. Woodworth, S. Gunasekar, J. D. Lee, N. Srebro, and D. Soudry, \u201cImplicit bias in deep linear classification: Initialization scale vs training accuracy,\u201d Advances in Neural Information Processing Systems, 2020.   \n[34] Y. Cao, Z. Fang, Y. Wu, D. Zhou, and Q. Gu, \u201cTowards understanding the spectral bias of deep learning,\u201d in International Joint Conference on Artificial Intelligence, Z. Zhou, Ed., 2021.   \n[35] N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht, Y. Bengio, and A. Courville, \u201cOn the spectral bias of neural networks,\u201d in International Conference on Machine Learning. PMLR, 2019.   \n[36] E. Abbe, S. Bengio, A. Lotf,i and K. Rizk, \u201cGeneralization on the unseen, logic reasoning and degree curriculum,\u201d in International Conference on Machine Learning, 2023.   \n[37] G. Vardi and O. Shamir, \u201cImplicit regularization in relu networks with the square loss,\u201d in Conference on Learning Theory, 2021.   \n[38] A. Jacot, B. Simsek, F. Spadaro, C. Hongler, and F. Gabriel, \u201cImplicit regularization of random feature models,\u201d in International Conference on Machine Learning, 2020.   \n[39] Y. Li, T. Ma, and H. Zhang, \u201cAlgorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations,\u201d in Conference On Learning Theory, 2018.   \n[40] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio, \u201cOn the number of linear regions of deep neural networks,\u201d Advances in Neural Information Processing Systems, 2014.   \n[41] G. Yehudai and O. Shamir, \u201cOn the power and limitations of random features for understanding neural networks,\u201d Advances in Neural Information Processing Systems, 2019.   \n[42] A. V. Varre, M.-L. Vladarean, L. Pillaud-Vivien, and N. Flammarion, \u201cOn the spectral bias of two-layer linear networks,\u201d Advances in Neural Information Processing Systems, 2024.   \n[43] Y. Lin, L. Tan, Y. Hao, H. Wong, H. Dong, W. Zhang, Y. Yang, and T. Zhang, \u201cSpurious feature diversification improves out-of-distribution generalization,\u201d arXiv preprint arXiv:2309.17230, 2023.   \n[44] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.   \n[45] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, \u201cGrad-cam: Visual explanations from deep networks via gradient-based localization,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2017.   \n[46] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009.   \n[47] Z. Huang, S. Bai, and J. Z. Kolter, \u201c(Implicit)2: Implicit layers for implicit representations,\u201d Advances in Neural Information Processing Systems, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs and Discussions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proofs in Subsection 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In following technical lemma, we show that there exists a width- $m$ ReLU-DEQ computing a function with $2^{m}$ linear regions. ", "page_idx": 13}, {"type": "text", "text": "Lemma 4. Let $m\\in\\mathbb{N}^{+}$ . For all $m\\geq1$ , consider the following function on $[0,1]^{d}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi^{(m)}(\\mathbf x)=\\left\\{\\begin{array}{l l}{2^{m}x_{1}-2i,}&{x_{1}\\in\\left[\\frac{2i}{2^{m}},\\frac{2i+1}{2^{m}}\\right],\\quad0\\leq i\\leq2^{m-1}-1,}\\\\ {-2^{m}x_{1}+2i+2,}&{x_{1}\\in\\left[\\frac{2i+1}{2^{m}},\\frac{2i+2}{2^{m}}\\right],\\quad0\\leq i\\leq2^{m-1}-1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then there exists a DEQ with width m that exactly computes $-\\phi^{(m)}(\\mathbf{x})+2^{m}x_{1}\\;o n\\;[0,1]^{d}$ . Moreover, the DEQ has $2^{m}$ linear regions on $[0,1]^{d}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Since $2^{m}x_{1}$ is a linear function with respect to $z_{1}$ , by definition, $-\\phi^{(m)}({\\bf x})+2^{m}x_{1}$ has 2 linear regions on $\\textstyle\\left[{\\frac{2i}{2^{m}}},{\\frac{2i+2}{2^{m}}}\\right]\\times[0,1]^{d-1}$ for all $0\\leq i\\leq2^{m-1}-1$ . Thus it has $2^{m}$ linear regions on $[0,1]^{d}$ . It suffices to show that existence of a DEQ computing $-\\phi^{(m)}({\\bf x})+2^{m}x_{1}$ . ", "page_idx": 13}, {"type": "text", "text": "Consider a width- $m$ DEQ with weight matrices as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{A}^{T}=\\left(\\begin{array}{c}{-2^{m}}\\\\ {-2^{m-1}}\\\\ {\\vdots}\\\\ {-2}\\end{array}\\right),\\mathbf{W}=\\left(\\begin{array}{c c c c c}{0}\\\\ {-4}&{0}&&\\\\ {-8}&{-4}&{0}&\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}\\\\ {-2^{m}}&{-2^{m-1}}&{-2^{m-2}}&{\\cdots}&{0}\\end{array}\\right),\\mathbf{U}_{1}=\\left(\\begin{array}{c}{2}\\\\ {4}\\\\ {\\vdots}\\\\ {2^{m}}\\end{array}\\right),\\mathbf{b}=\\left(\\begin{array}{c}{-1}\\\\ {-1}\\\\ {\\vdots}\\\\ {-1}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{U}_{1}$ denotes the first column of $\\mathbf{U}$ and $\\mathbf{U}=\\left(\\mathbf{U}_{1}\\quad\\mathbf{0}\\right)$ . When a DEQ is equipped with the above weight matrices, direct calculations show that the fixed point $\\mathbf{z}$ satisfy ", "page_idx": 13}, {"type": "equation", "text": "$$\nz_{1}(\\mathbf{x})=\\sigma(2x_{1}-1),\\quad z_{t}(\\mathbf{x})=\\sigma\\left(-\\sum_{i=1}^{t-1}2^{t-i+1}z_{i}(\\mathbf{x})+2^{t}x_{1}-1\\right),\\quad\\forall2\\leq t\\leq m.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that $\\{\\phi^{(m)}(\\mathbf{x})\\}$ admits a recursive expression: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi^{(m+1)}(\\mathbf{x})=2\\phi^{(m)}(\\mathbf{x})-2\\sigma(2\\phi^{(m)}(\\mathbf{x})-1),\\quad\\forall m\\geq0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for $\\phi^{(0)}(\\mathbf{x}):=x_{1}$ . We now show by induction that $z_{t}(\\mathbf{x})=\\sigma(2\\phi^{(t-1)}(x)-1)$ for all $1\\leq t\\leq m$ . When $t=1$ , it is true immediately from Eq. (16) and (17). Assume it is true for some $t<m$ , then by Eq. (16) we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{t+1}(\\mathbf x)=\\sigma\\left(\\displaystyle\\sum_{i=1}^{t}-2^{t-i+2}z_{i}(x)+2^{t+1}x_{1}-1\\right)}\\\\ &{\\phantom{z x z x}=\\sigma\\left(\\displaystyle\\sum_{i=1}^{t}-2^{t-i+2}\\sigma(2\\phi^{(i-1)}(x)-1)+2^{t+1}x_{1}-1\\right)}\\\\ &{\\phantom{z x z x}=\\sigma\\left(\\displaystyle\\sum_{i=1}^{t}-2^{t-i+2}\\left(\\phi^{(i-1)}(x)-\\displaystyle\\frac{\\phi^{(i)}(x)}{2}\\right)+2^{t+1}x_{1}-1\\right)}\\\\ &{\\phantom{z x z x}=\\sigma(-2^{t+1}\\phi^{(0)}(x)+2\\phi^{(t)}(x)+2^{t+1}x_{1}-1)=\\sigma(2\\phi^{(t)}(x)-1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we use the induction in the second line, Eq. (17) in the third line, and $\\phi^{(0)}(\\mathbf{x})=x_{1}$ in the last line. Thus the induction holds. ", "page_idx": 13}, {"type": "text", "text": "Using the induction and Eq. (17) for the DEQ, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{A}\\,\\mathbf{z}(\\mathbf{x})=\\sum_{i=1}^{m}-2^{m+1-i}z_{i}(\\mathbf{x})}\\\\ {\\displaystyle=\\sum_{i=1}^{m}-2^{m+1-i}\\sigma(2\\phi^{(i-1)}(\\mathbf{x})-1)}\\\\ {\\displaystyle=\\sum_{i=1}^{m}-2^{m+1-i}\\left(\\phi^{(i-1)}(\\mathbf{x})-\\frac{\\phi^{(i)}(\\mathbf{x})}{2}\\right)}\\\\ {\\displaystyle=-2^{m}\\phi^{(0)}(\\mathbf{x})+\\phi^{(m)}(\\mathbf{x})=\\phi^{(m)}(\\mathbf{x})-2^{m}x_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the lemma follows. ", "page_idx": 14}, {"type": "text", "text": "To prove the theorem, we also need the following lemma which is proved in [20]. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5 (Lemma 2.1 in [20]). Let $k\\in\\mathbb{N}^{+}$ , $L\\ge2$ and $\\rho(x):\\mathbb{R}\\rightarrow\\mathbb{R}$ be a piecewise affine linear function with $p$ pieces. Then every $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ implemented by an FNN with depth $L$ , width $k$ and activation function $\\rho$ has at most $(p k)^{L-1}$ linear regions. ", "page_idx": 14}, {"type": "text", "text": "Note that the in Lemma 4, the function computed by DEQ is a variant of the saw-tooth function that has many linear regions. On the other hand, Lemma 5 provides an upper bound on the number of linear regions generated by FNN. Combining these two lemmas and using a technique similar to that in Theorem 1.1 in [24], we are able to prove Theorem 1. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Let $N_{d}({\\bf x})$ be the DEQ in Lemma 4 that computes $2^{m}x_{1}-\\phi^{(m)}(\\mathbf{x})$ and denote the width of the FNN that computes $N_{f}(x)$ by $k$ . For any $\\mathbf{y}\\in[0,1]^{d-1}$ , define $p_{\\mathbf{y}}(x):[0,1]\\rightarrow$ $[0,1]^{d}$ as $p_{\\mathbf{y}}=(x_{1},\\mathbf{y})$ . Then for $N_{f}\\circ p_{\\mathbf{y}}(x):[0,1]\\to\\mathbb{R}$ , by Lemma 5, the number of linear regions is upper bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n(p k)^{L-1}\\leq2^{\\binom{m^{1-\\alpha}-1}{m^{1-\\alpha}-1}(L-1)}\\leq2^{m-2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $p=2$ denotes the number of linear pieces of ReLU activation function. Therefore, $N_{f}\\circ$ $p_{\\mathbf{y}}(x)-2^{m}x$ has at most $2^{m-2}$ linear regions on $[0,1]$ . ", "page_idx": 14}, {"type": "text", "text": "Note that $\\phi^{(m)}(\\mathbf{x})$ only depends on the first entry of $\\mathbf{x}$ , for simplicity, we define $\\varphi^{(m)}(x):\\mathbb{R}\\to\\mathbb{R}$ as $\\boldsymbol{\\varphi}^{(m)}(\\boldsymbol{x})=\\boldsymbol{\\phi}^{(m)}\\circ p_{\\mathbf{y}}(\\boldsymbol{x})$ . Now we claim that there exists at least $3\\cdot2^{m-3}-2$ small intervals $\\{T_{l}\\}_{l}$ on $[0,1]$ with diam $\\left(\\dot{T}_{l}\\right)=2^{-m}$ , such that for any $\\mathbf{y}$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{sgn}\\left(\\varphi^{(m)}(x)-\\frac{1}{2}\\right)\\neq\\mathrm{sgn}\\left(N_{f}\\circ p_{\\mathbf{y}}(x)-2^{m}x-\\frac{1}{2}\\right),\\quad\\forall x\\in T_{l},\\quad\\forall l.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For simplicity, denote $\\tilde{\\varphi}(x)=\\varphi^{(m)}(x)-\\textstyle\\frac{1}{2}$ and $\\begin{array}{r}{\\tilde{N}_{f}(x)=N_{f}\\circ p_{\\mathbf{y}}(x)-2^{m}x-\\frac{1}{2}}\\end{array}$ . Denote $\\mathcal{P}_{\\phi}$ and $\\mathcal{P}_{N}$ the partitions of $[0,1]$ into intervals so that sgn $\\left(\\tilde{\\varphi}(x)\\right)$ and $\\mathrm{sgn}\\left(\\tilde{N}_{f}(x)\\right)$ remains constant within each interval, respectively. Let $\\mathcal{T}_{\\phi}$ be the set of all intervals partitioned by $\\mathcal{P}_{\\phi}$ and $\\mathcal{T}_{N}$ be the set of all intervals partitioned by $\\mathcal{P}_{N}$ . By definition, $|{\\mathcal{T}}_{\\phi}|=2^{m}+1$ . Since $\\tilde{N}_{f}(x)$ has at most $2^{m-2}$ linear regions, the number of the boundary points of the intervals in $\\mathcal{T}_{N}$ is upper bounded $2^{m-2}+1$ . So there are at least $2^{m}+1-(2^{m-2}\\dot{+}\\dot{1})=3\\cdot2^{m-2}$ intervals in $\\mathcal{T}_{\\phi}$ that do not intersect with any boundary points of intervals, i.e., lie completely in an interval in ${\\dot{\\mathcal{T}}}_{N}$ . Denote this set of intervals by $\\mathcal{T}_{\\phi}^{\\prime}$ . On the other hand, since $\\operatorname{sgn}(\\tilde{N}_{f}(\\bar{x}))$ remains constant on every interval in $\\mathcal{Z}_{N}$ , for every $J\\in\\mathcal{Z}_{N}$ that contains $i_{J}$ intervals in $\\mathcal{T}_{\\phi}^{\\prime}$ , there will be $\\textstyle{\\frac{i_{J}+1}{2}}$ intervals when $i_{J}$ is odd and $\\frac{i_{J}}{2}$ intervals when $i_{J}$ is even, on which $\\operatorname{sgn}(\\tilde{\\varphi}(x))=\\operatorname{sgn}(\\tilde{N_{f}}(x))$ . Note that $\\sum_{J\\in{\\mathcal{Z}}_{N}}i_{J}\\geq3\\cdot2^{m-2}$ . Therefore, among the sets in $\\mathcal{T}_{\\phi}^{\\prime}$ , the number of sets on which $\\mathbf{sgn}(\\tilde{\\varphi}(x))\\neq\\mathbf{sgn}(\\tilde{N_{f}}(x))$ is at least ", "page_idx": 14}, {"type": "equation", "text": "$$\n3\\cdot2^{m-2}-\\sum_{J\\in\\mathbb{Z}_{N}}\\frac{i_{J}+1}{2}\\geq2^{m-3},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use the fact that $|{\\mathcal{I}}_{N}|\\leq2^{m-2}+1$ . Note that except for two intervals, every $T\\in{\\mathcal{Z}}_{\\phi}^{\\prime}$ can be represented as $\\textstyle\\left[{\\frac{4i+1}{2^{m+1}}},{\\frac{4i+3}{2^{m+1}}}\\right]$ or $\\left[{\\frac{4i-1}{2^{m+1}}},{\\frac{4i+1}{2^{m+1}}}\\right]$ for some $i$ , thus dia $\\operatorname{m}(T_{l})=2^{-m}$ , which proves the ", "page_idx": 14}, {"type": "text", "text": "claim. Moreover, on each $T_{l}$ , direct calculations show $\\begin{array}{r}{\\int_{T_{l}}\\left|\\phi^{(m)}(x)-\\frac{1}{2}\\right|\\mathrm{d}x\\geq2^{-m-2}}\\end{array}$ . Therefore, by using the claim, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{[0,1]^{4}}|N_{d}(\\mathbf{x})-N_{f}(\\mathbf{x})||\\,\\mathrm{d}\\mathbf{x}}\\\\ &{=\\displaystyle\\int_{[0,1]^{4-1}}\\int_{[0,1]}\\left|2^{m}x_{1}-\\phi^{(m)}(x_{1})-N_{f}\\circ p_{\\mathbf{y}}(x_{1})\\right|\\mathrm{d}x_{1}\\mathrm{d}\\mathbf{y}}\\\\ &{\\geq\\displaystyle\\int_{[0,1]^{4-1}}\\int_{[0,T]}\\left|2^{m}x_{1}-\\phi^{(m)}(x_{1})-N_{f}\\circ p_{\\mathbf{y}}(x_{1})\\right|\\mathrm{d}x_{1}\\mathrm{d}\\mathbf{y}}\\\\ &{\\geq\\displaystyle\\int_{[0,1]^{4-1}}\\int_{[0,T]}\\left|\\frac{1}{2}-\\phi^{(m)}(x_{1})\\right|\\mathrm{d}x_{1}\\mathrm{d}\\mathbf{y}}\\\\ &{\\geq\\displaystyle\\int_{[0,1]^{4-1}}|T|\\cdot2^{-m-2}\\mathrm{d}\\mathbf{y}}\\\\ &{\\geq(3\\cdot2^{m-3}-2)\\cdot2^{-m-2}\\geq\\frac{1}{16}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next we turn to proof Proposition 1. We use $\\mathrm{Diag}(\\cdot)$ to extract the diagonal elements of a matrix into a vector. The proof of Proposition 1 relies on following explicit expression of ReLU DEQ. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. Let W, U, b be the weights of a DEQ with $\\|\\mathbf{W}\\|_{2}<1$ . Then for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ , there exists a diagonal matrix $\\mathbf{D}\\in\\mathbb{R}^{d\\times d}$ whose diagonal entries are either 1 or $0$ , such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{sgn}(\\mathrm{diag}((\\mathbf{I}-\\mathbf{W}\\mathbf{D})^{-1})(\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}))=\\mathrm{Diag}(\\mathbf{D}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, fix D, for all $\\mathbf{x}$ that Eq. (18) holds, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{z}(\\mathbf{x})=(\\mathbf{I}-\\mathbf{D}\\mathbf{W})^{-1}\\mathbf{D}(\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that the fixed point $\\mathbf{z}(\\mathbf{x})$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{z}=\\sigma(\\mathbf{W}\\,\\mathbf{z}+\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For each $z_{i}$ , if the $i$ -th entry of $(\\mathbf{W_{\\deltaZ}}+\\mathbf{U_{\\deltaX}}+\\mathbf{b})$ is smaller than 0, then $z_{i}=0$ . Without loss of generality, we assume that the first $t$ $(t\\leq m)$ entries of $(\\mathbf{W_{\\deltaZ}}+\\mathbf{U_{\\deltaX}}+\\mathbf{b})$ are greater than 0, and the rest $m-t$ entries are smaller than 0. Denote by $\\mathbf{v}=\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}$ and the corresponding block matrices $\\mathbf{z},\\mathbf{W},\\mathbf{v}$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{z}=\\left(\\begin{array}{l}{\\tilde{\\mathbf{z}}}\\\\ {\\mathbf{0}}\\end{array}\\right),\\mathbf{W}=\\left(\\mathbf{W}_{11}\\quad\\mathbf{W}_{12}\\right),\\mathbf{v}=\\left(\\mathbf{v}_{1}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{\\mathbf{z}}\\in\\mathbb{R}^{t},\\mathbf{W}_{11}\\in\\mathbb{R}^{t\\times t}$ , and $\\mathbf{v}_{1}\\in\\mathbb{R}^{t}$ . Then, Eq.(20) is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{z}}=\\mathbf{W}_{11}\\tilde{\\mathbf{z}}+\\mathbf{v}_{1},\\quad\\mathbf{W}_{21}\\tilde{\\mathbf{z}}+\\mathbf{v}_{2}\\leq0,\\quad\\tilde{\\mathbf{z}}>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we define $\\mathbf{D}={\\left(\\begin{array}{l l}{\\mathbf{I}_{t}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}\\end{array}\\right)}$ and show that it is the desired matrix. Note that $\\|\\mathbf{W}\\|_{2}<1$ and $\\|\\mathbf{D}\\|_{2}=1$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{11}\\|_{2}=\\|\\mathbf{W}\\,\\mathbf{D}\\|_{2}\\leq\\|\\mathbf{W}\\,\\|_{2}\\|\\mathbf{D}\\|_{2}<1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "showing that $\\mathbf{I}_{t}-\\mathbf{W}_{11}$ is invertible. Thus Eq.(22) gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{z}}=(\\mathbf{I}_{t}-\\mathbf{W}_{11})^{-1}\\mathbf{v}_{1}>0,\\quad\\mathbf{W}_{21}(\\mathbf{I}_{t}-\\mathbf{W}_{11})^{-1}\\mathbf{v}_{1}+\\mathbf{v}_{2}\\le0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Additionally, by simple calculation, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{I}-\\mathbf{D}\\mathbf{\\boldsymbol{W}})^{-1}=\\left(\\!\\!\\begin{array}{c c}{(\\mathbf{I}_{t}-\\mathbf{W}_{11})^{-1}}&{(\\mathbf{I}_{t}-\\mathbf{W}_{11})^{-1}\\mathbf{W}_{12}}\\\\ {\\mathbf{0}}&{\\mathbf{I}}\\end{array}\\!\\!\\right),}\\\\ &{\\left(\\mathbf{I}-\\mathbf{W}\\mathbf{D}\\right)^{-1}=\\left(\\!\\!\\begin{array}{c c}{(\\mathbf{I}_{t}-\\mathbf{W}_{11})^{-1}}&{\\mathbf{0}}\\\\ {\\mathbf{W}_{21}(\\mathbf{I}_{t}-\\mathbf{W}_{11})^{-1}}&{\\mathbf{I}}\\end{array}\\!\\!\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining Eq. (21), (23) and (24), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{({\\bf I}-{\\bf W}\\,{\\bf D})^{-1}({\\bf U}\\,{\\bf x}+{\\bf b})=\\left({\\bf W}_{21}({\\bf I}_{t}-{\\bf W}_{11})^{-1}{\\bf v}_{1}+{\\bf v}_{2}\\right),}\\\\ &{{\\bf z}=\\left(({\\bf I}_{t}-{\\bf W}_{11})^{-1}{\\bf v}_{1}\\right)=({\\bf I}-{\\bf D}{\\bf W})^{-1}{\\bf D}({\\bf U}\\,{\\bf x}+{\\bf b}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, since the output of the implicit layer is unique, in the sense of permuting the entries of $\\mathbf{D}$ , there always exists a matrix $\\mathbf{D}$ such that the lemma follows. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Note that there are at most $2^{m}$ diagonal matrix whose diagonal entries are either 1 or 0, the upper bound of the number of linear regions is $2^{m}$ . Thus Proposition 1 follows straightforwardly from Lemma 6 and 4. ", "page_idx": 16}, {"type": "text", "text": "A.2 Proofs in Subsection 4.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.2.1 Inapproximability of FNNs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The goal of this section is to prove the following proposition, which is an extended version of the inapproximability result in Theorem 2. ", "page_idx": 16}, {"type": "text", "text": "Assumption 2. The activation function $\\tilde{\\sigma}$ is of ${\\mathcal{C}}^{0}(\\mathbb{R})$ and continuous differentiable except for at most finitely many points. And there exists an absolute constant $C_{\\tilde{\\sigma}}>0$ , such that $|\\tilde{\\sigma}^{\\prime}(x)|\\leq\\dot{C}_{\\tilde{\\sigma}}$ for all x on which $\\tilde{\\sigma}$ is differentiable. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3 (Inapproximability of FNN). Let $N_{f n n}(x)$ be computed by an FNN with depth $L$ , width $k$ , and an activation function $\\tilde{\\sigma}$ satisfying Assumption 2 on $\\mathbf{x}\\in[0,1]^{d}$ . Let $g\\mathbf{(x)}$ be defined as in Eq.(6), and $\\begin{array}{r}{\\frac14\\geq\\varepsilon>\\grave{0}.}\\end{array}$ . If $\\|N_{f n n}(\\mathbf{x})-g(\\mathbf{x})\\|_{L^{\\infty}([0,1]^{d})}\\leq\\varepsilon,$ , then there exists a weight parameter $W_{i j}$ of the FNN for $1\\leq i\\leq L$ and $1\\le j\\le k$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|W_{i j}|\\geq\\frac{1}{C_{\\tilde{\\sigma}}k}\\cdot2^{\\frac{d-4}{L}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. By assumption, $N_{\\mathrm{fnn}}(\\mathbf{x})$ is of ${\\mathcal{C}}^{0}(\\mathbb{R}^{d})$ and continuous differentiable except for at most finitely many points, then by the intermediate value theorem, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in[0,1]^{d}}\\bigg|\\frac{\\partial N_{\\mathrm{fm}}(\\mathbf{x})}{\\partial x_{1}}\\bigg|\\geq\\bigg|\\frac{g_{1}(1)-g_{1}(1-\\delta)}{\\delta}\\bigg|\\geq\\frac{\\frac{1}{2}-\\frac{\\delta(1-\\delta)}{4\\delta}-2\\cdot\\frac{1}{16}}{\\delta}\\geq\\frac{1}{8\\delta}-1\\geq2^{d-4},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\frac{\\partial N_{\\mathrm{fnn}}(\\mathbf{x})}{\\partial x_{1}}$ refers to the subgradient on the non-differentiable points. Additionally, by definition, ", "page_idx": 16}, {"type": "equation", "text": "$$\nN_{\\mathrm{fnn}}(\\mathbf{x})=\\mathbf{W}_{L}\\tilde{\\sigma}\\left(\\mathbf{W}_{L-1}\\tilde{\\sigma}(\\cdot\\cdot\\cdot\\tilde{\\sigma}(\\mathbf{W}_{1}\\,\\mathbf{x}+\\mathbf{b}_{1})\\cdot\\cdot\\cdot)+\\mathbf{b}_{L-1}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then direct calculation gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla N_{\\mathrm{fnn}}(\\mathbf x)=\\mathbf W_{1}^{T}\\mathbf D_{1}\\cdot\\cdot\\cdot\\mathbf W_{L-1}^{T}\\mathbf D_{L-1}\\mathbf W_{L}^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{D}_{l}=\\mathrm{diag}(\\tilde{\\sigma}^{\\prime}(\\mathbf{W}_{l}\\tilde{\\sigma}(\\cdot\\cdot\\cdot\\tilde{\\sigma}(\\mathbf{W}_{1}\\mathbf{x}+\\mathbf{b}_{1})\\cdot\\cdot\\cdot)+\\mathbf{b}_{l})$ ) for $1\\leq l\\leq L-1$ . By Assumption 2, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}_{l}\\|_{\\infty}\\leq C_{\\tilde{\\sigma}},\\quad\\forall1\\leq l\\leq L-1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then combining Eq. (25) and (26), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n2^{d-4}\\le\\|\\nabla N_{\\mathrm{fmn}}(\\mathbf{x})\\|_{\\infty}\\le\\prod_{i=1}^{L}\\|\\mathbf{D}_{i}\\mathbf{W}_{i}\\|_{\\infty}\\le C_{\\tilde{\\sigma}}^{L}\\cdot\\prod_{i=1}^{L}\\|\\mathbf{W}_{i}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, there exists at least one $\\mathbf{W}_{i}$ for $1\\leq i\\leq L$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{W}_{i}\\rVert_{\\infty}\\geq C_{\\tilde{\\sigma}}^{-1}2^{\\frac{d-4}{L}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, by the definition of $\\|\\cdot\\|_{\\infty}$ , there exists an entry $W_{i j}$ with $1\\le j\\le k$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|W_{i j}|\\geq\\frac{1}{C_{\\tilde{\\sigma}}k}\\cdot2^{\\frac{d-4}{L}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Remark 5. Assumption 2 is mild and one can verify that most commonly used activation functions such as ReLU, GeLU, sigmoid and tanh satisfy the assumption. ", "page_idx": 17}, {"type": "text", "text": "To prove the inapproximability of FNNs in Theorem 2, we take $C_{\\tilde{\\sigma}}=1$ in Proposition 3 as $|\\sigma^{\\prime}(x)|\\le1$ and derive ", "page_idx": 17}, {"type": "equation", "text": "$$\n|W_{i j}|\\geq k^{-1}\\cdot2^{\\frac{d-4}{L}}\\leq2^{-\\frac{d}{2C}+\\frac{d-4}{C}}\\geq2^{-\\frac{d}{2C}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.2.2 Approximability of DEQs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section centers around the approximability result of DEQs. We restate the approximability result of Theorem 2 as the following proposition. ", "page_idx": 17}, {"type": "text", "text": "Proposition 4 (Approximability of DEQ). Let $g\\mathbf{(x)}$ be defined as in Eq.(6) on $[0,1]^{d}.\\ \\forall{\\frac{1}{4}}\\geq\\varepsilon>0,$ , there exists a DEQ $N_{d e q}$ with width bounded by $5\\varepsilon^{-1}$ and weights bounded by $2\\varepsilon^{-1}$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|N_{d e q}(\\mathbf{x})-g(\\mathbf{x})\\|_{L^{\\infty}([0,1]^{d})}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof of the proposition requires some intermediate steps regrading the constructing approximation by DEQ and bounding the fixed-points\u2019 error. For simplicity, in the rest of the section, for any function $f$ which is continuous differentiable except for at most finitely many points, we denote $f^{\\prime}$ the derivative of $f$ on the differentiable points, and the subgradient of $f$ on the non-differentiable points. ", "page_idx": 17}, {"type": "text", "text": "The next lemma considers approximating the square function using a 2-layer FNN. ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. For any $N\\in\\mathbb{N}^{+}$ , there exists a function $\\phi$ implemented by a 2-layer ReLU FNN with width $2N$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\phi(x)-x^{2}|\\leq4N^{-2},\\quad|\\phi^{\\prime}(x)|\\leq2-\\frac{1}{N},\\quad\\forall x\\in[-1,1].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Denote $\\textstyle{\\frac{1}{N}}$ by $t$ for simplicity. Let $\\{x_{i}\\}_{i=1}^{2N+1}$ be $2N+1$ points on $\\mathbb{R}$ be defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{1}=-1,\\;x_{2}=-1+t,\\;\\cdot\\cdot\\cdot\\;,\\;x_{2N}=1-t,\\;x_{2N+1}=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We consider the following function $\\phi(x)$ that interpolates $x^{2}$ on all $\\{x_{i}\\}_{i=1}^{2N+1}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi(x)=\\sigma(t x)+\\sigma(-t x)+\\sum_{i=1}^{N-1}\\sigma(2t x-2i t^{2})+\\sum_{i=1}^{N-1}\\sigma(-2t x+2i t^{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It can be seen that $\\phi(x)$ can be implemented by a 2-layer ReLU FNN with width $2N$ and weight bounded by $2t$ . By the interpolation property of $\\phi(x)$ , on every $[x_{j},x_{j+1}]$ , it holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in[x_{j},x_{j+1}]}\\left|\\phi(x)-x^{2}\\right|=\\phi\\left(\\frac{x_{j}+x_{j+1}}{2}\\right)-\\left(\\frac{x_{j}+x_{j+1}}{2}\\right)^{2}=\\frac{t^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we have $|\\phi(x)-x^{2}|\\leq4N^{-2}$ for all $x\\in[-1,1]$ . Moreover, since $\\phi(x)$ is convex, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\phi^{\\prime}(x)|\\leq{\\frac{1-(1-t)^{2}}{t}}=2-t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now move to prove the equivalence between the revised DEQ and vanilla DEQ. ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . For any $\\hat{\\mathbf{z}}^{0}\\in\\mathbb{R}^{m}$ , we define a sequence $\\{\\hat{\\mathbf{z}}^{k}\\}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{z}}^{k+1}=\\sigma(\\mathbf{W}\\,\\mathbf{V}\\hat{\\mathbf{z}}^{k}+\\mathbf{U}\\mathbf{x}+\\mathbf{b}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\|\\mathbf{W}\\,\\mathbf{V}\\|_{2}\\leq1,\\{\\hat{\\mathbf{z}}^{k}\\}$ converges and the limit $\\hat{\\mathbf{z}}^{\\ast}$ is the fixed point of $\\hat{{\\mathbf{z}}}=\\sigma({\\mathbf{W}}{\\mathbf{V}}\\hat{{\\mathbf{z}}}+{\\mathbf{U}}\\,{\\mathbf{x}}+{\\mathbf{b}})$ . Now we set $\\mathbf z^{0}=\\mathbf V\\mathbf y^{0}$ and define another sequence $\\{\\mathbf{z}^{k}\\}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbf z}^{k+1}={\\mathbf V}\\sigma({\\mathbf W}{\\mathbf z}^{k}+{\\mathbf U}\\,{\\mathbf x}+{\\mathbf b}),\\quad\\forall k\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows immediately by induction that $\\mathbf{z}^{k}=\\mathbf{V}\\hat{\\mathbf{z}}^{k}$ for all $k\\geq0$ . Note that $\\{\\mathbf{z}^{k}\\}$ converges and the limit $\\mathbf{z}^{\\ast}$ is exactly the fixed point of the revised DEQ in Eq. (8). Therefore, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{z}^{*}=\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathbf{z}^{k}=\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathbf{V}\\hat{\\mathbf{z}}^{k}=\\mathbf{V}\\hat{\\mathbf{z}}^{*}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The desired DEQ is constructed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{z}}=\\sigma(\\mathbf{W}\\,\\mathbf{V}\\hat{\\mathbf{z}}+\\mathbf{U}\\,\\mathbf{x}+\\mathbf{b}),}\\\\ &{\\hat{\\mathbf{y}}=(\\mathbf{B}\\mathbf{V})\\hat{\\mathbf{z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the following we turn to bound the error between the equilibria of two fixed-point equations. We start with the proof of Lemma 2. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 2. The existence of $z_{u}$ and $z_{v}$ follows from the fixed point theorem since $u(\\cdot,\\mathbf{x})$ and $v(\\cdot,\\mathbf{x})$ are contraction mappings. For simplicity, we denote $u_{\\mathbf{x}}(z)\\doteq u(z,\\mathbf{x})$ and $v_{\\mathbf{x}}(z)=v(z,\\mathbf{x})$ Note that the range of $u_{\\mathbf{x}}$ and $v_{\\mathbf{x}}$ are in $\\Omega$ . Then $\\forall n\\in\\mathbb{N}^{+}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|u_{\\mathbf{x}}^{o n}-v_{\\mathbf{x}}^{o n}\\|_{\\infty}\\leq\\left\\|u_{\\mathbf{x}}^{o n}-u_{\\mathbf{x}}\\left(v_{\\mathbf{x}}^{o(n-1)}\\right)\\right\\|_{\\infty}+\\left\\|u_{\\mathbf{x}}\\left(v_{\\mathbf{x}}^{o(n-1)}\\right)-v_{\\mathbf{x}}^{o n}\\right\\|_{\\infty}}}\\\\ &{\\leq L_{u}\\left\\|u_{\\mathbf{x}}^{o(n-1)}-v_{\\mathbf{x}}^{o(n-1)}\\right\\|_{\\infty}+\\left\\|u_{\\mathbf{x}}-v_{\\mathbf{x}}\\right\\|_{\\infty}}\\\\ &{\\leq L_{u}\\left(\\left\\|u_{\\mathbf{x}}^{o(n-2)}-v_{\\mathbf{x}}^{o(n-2)}\\right\\|_{\\infty}+\\left\\|u_{\\mathbf{x}}-v_{\\mathbf{x}}\\right\\|_{\\infty}\\right)+\\left\\|u_{\\mathbf{x}}-v_{\\mathbf{x}}\\right\\|_{\\infty}}\\\\ &{\\leq\\cdots}\\\\ &{\\leq(1+L_{u}+\\cdots+L_{u}^{n-1})\\|u_{\\mathbf{x}}-v_{\\mathbf{x}}\\|_{\\infty}}\\\\ &{=\\frac{1-L_{u}^{n}}{1-L_{u}}\\|u_{\\mathbf{x}}-v_{\\mathbf{x}}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By definition, $\\forall(z,\\mathbf{x})\\in\\Omega\\times[0,1]^{d}$ , $z_{u}(\\mathbf{x})=\\operatorname*{lim}_{n\\rightarrow\\infty}u_{\\mathbf{x}}^{\\circ n}(z)$ , and $z_{v}(\\mathbf{x})=\\operatorname*{lim}_{n\\rightarrow\\infty}v_{\\mathbf{x}}^{\\circ n}(z)$ . Hence, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|z_{u}(\\mathbf{x})-z_{v}(\\mathbf{x})|\\leq\\displaystyle\\operatorname*{lim}_{n\\to\\infty}|u_{\\mathbf{x}}^{\\circ n}(z)-v_{\\mathbf{x}}^{\\circ n}(z)|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\frac{1-L_{u}^{n}}{1-L_{u}}\\cdot\\displaystyle\\operatorname*{max}_{z\\in\\Omega}|u(z,\\mathbf{x})-v(z,\\mathbf{x})|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{1-L_{u}}\\cdot\\displaystyle\\operatorname*{max}_{z\\in\\Omega}|u(z,\\mathbf{x})-v(z,\\mathbf{x})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, by the symmetry of $u$ and $v$ , we also have $\\begin{array}{r}{|z_{u}(\\mathbf{x})-z_{v}(\\mathbf{x})|\\,\\le\\,\\frac{1}{1-L_{v}}\\cdot\\operatorname*{max}_{z\\in\\Omega}|u(z,\\mathbf{x})-}\\end{array}$ $v(z,\\mathbf{x})|$ . The proof is finished. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "We also need Lemma 3 to bound the error. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 3. We use the intermediate value theorem to proof the lemma. Define $q(z,\\mathbf{x})=$ $z-v(z,\\mathbf{x})$ . The fixed point $z_{v}(\\mathbf{x})$ is the unique zero of $q(z,\\mathbf{x})=0$ . Since $v(z,\\mathbf{x})$ is $L_{v}$ Lipschitz with respect to $z$ and $L_{v}<1,q(z,{\\bf x})$ is monotonically increasing with respect to $z$ for all $\\mathbf{x}$ . ", "page_idx": 18}, {"type": "text", "text": "Fix $z_{u}$ , the proof proceeds by discussing the following 2 cases: ", "page_idx": 18}, {"type": "text", "text": "\u2022 If $q(z_{u},\\mathbf{x})\\;\\leq\\;0$ , i.e., $u(z_{u},\\mathbf{x})\\;=\\;z_{u}\\;\\leq\\;v(z_{u},\\mathbf{x})$ , we consider $T\\,=\\,[z_{u},z_{u}+\\xi]\\,\\subset\\,\\Omega$ . By assumption, there exists $z^{*}\\in T$ , such that $u(z^{*},\\mathbf{x})\\,=\\,v(z^{*},\\mathbf{x})$ . Note that $q(\\cdot,\\mathbf{x})$ is monotonically increasing, thus we have $q(z^{*},\\mathbf{x})\\geq0$ . By the continuity of $q(z,\\mathbf{x})$ w.r.t. $z$ and the intermediate value theorem, $q(z,\\mathbf{x})$ must have a zero in $[z_{u},z_{0}]\\subset T$ , which is $z_{v}(\\mathbf{x})$ by definition. Hence, it holds that $|z_{u}-z_{v}|\\le\\xi$ . ", "page_idx": 18}, {"type": "text", "text": "\u2022 If $q(z_{u},\\mathbf{x})\\ge0$ , i.e., $u(z_{u},\\mathbf{x})\\,=\\,z_{u}\\,\\geq\\,v(z_{u},\\mathbf{x})$ , we consider $T=[z_{u}-\\xi,z_{u}]\\,\\subset\\,\\Omega$ . It follows from similar deductions that $|z_{u}-z_{v}|\\le\\xi$ in this case. ", "page_idx": 18}, {"type": "text", "text": "With the results above, we begin our formal proof of Proposition 4. The proof is sketched as follows: First, we consider a fixed point equation $z=\\tilde{g}(z,\\mathbf{x})$ that induces the target function $g\\mathbf{(x)}$ . We show that there exists a function $\\tilde{h}(z,\\mathbf{x}):\\mathbb{R}^{d+1}\\rightarrow\\mathbb{R}$ computed by a 2-layer FNN with width $O(\\varepsilon^{-1})$ that can approximate $\\tilde{g}(z,\\mathbf{x})$ in sup-norm to an accuracy of $O(\\varepsilon^{2})$ . Moreover, $z=\\tilde{h}(z,\\mathbf{x})$ is a well-posed fixed point equation and induces a revised DEQ. Second, we bound the error between $g\\mathbf{(x)}$ and $\\mathbf{\\bar{\\alpha}}h(\\mathbf{x})$ , where $h(\\mathbf{x})$ is the fixed point of $z=\\tilde{h}(z,\\mathbf{x})$ . The proof is further divided into two parts: When $\\textstyle1-x_{1}>{\\frac{\\varepsilon}{2}}$ , by using Lemma 2, we can bound the error $\\|h-g\\|$ by $\\varepsilon\\cdot\\|\\tilde{h}-\\tilde{g}\\|$ . When $\\begin{array}{r}{1-x_{1}<\\frac{\\varepsilon}{2}}\\end{array}$ , we show that the conditions of Proposition 4 holds for $\\xi=\\varepsilon$ , thus $\\|h-g\\|$ is upper bounded by $\\varepsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition $^{4}$ . Let $g\\mathbf{(x)}$ be defined as in Eq.(6). Recall that $g\\mathbf{(x)}$ is the fixed point of the fixed point equation ", "page_idx": 19}, {"type": "equation", "text": "$$\nz=\\tilde{g}(z,{\\bf x}):=z x_{1}+\\delta\\left(\\frac{x_{1}}{2}-z\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Approximate $\\tilde{g}$ using 2-layer FNN. By Lemma ${7,\\}/{N}\\in\\mathbb{N}^{+}$ , there exist $\\mathbf{a}\\in\\mathbb{R}^{2N},\\tilde{\\mathbf{b}}\\in\\mathbb{R}^{2N},\\tilde{\\mathbf{W}}\\in$ $\\mathbb{R}^{2N}$ and a function $\\bar{\\phi}(x)=\\mathbf{a}^{T}\\sigma(\\tilde{\\mathbf{W}}x+\\tilde{\\mathbf{b}})$ , such that for all $x\\in[-1,1]$ , it holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\phi_{N}(x)-x^{2}|\\leq4N^{-2},\\quad|\\phi_{N}^{\\prime}(x)|\\leq2-\\frac{1}{N}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{h}(z,\\mathbf{x})=\\frac{1}{2}\\left[\\phi_{N}\\left(z+\\frac{x_{1}}{2}\\right)-\\phi_{N}\\left(z-\\frac{x_{1}}{2}\\right)\\right]+\\delta\\left(\\frac{x_{1}}{2}-z\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "1. $\\tilde{h}(z,\\mathbf{x})$ can be implemented by a 2-layer ReLU FNN with width $4N+2$ for $(z,\\mathbf{x})\\in$ $\\left[-\\delta,\\frac{1}{2}\\right]\\times[0,1]^{d}$ . To see this, when $(z,\\mathbf{x})\\,\\in\\,\\left[-\\delta,{\\frac{1}{2}}\\right]\\,\\times\\,[0,1]^{d}$ , it holds $\\textstyle z+{\\frac{x_{1}}{2}}\\in[0,1]$ , $z-\\frac{x_{1}}{2}\\in[-1,0]$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{\\alpha^{\\tau}}{2}\\quad\\frac{\\alpha^{\\tau}}{2}\\quad-\\delta\\quad\\delta\\right)\\sigma\\left(\\left(\\begin{array}{l l}{\\hat{W}}&{0}\\\\ {0}&{\\hat{W}}\\\\ {0}&{1}\\end{array}\\right)\\left(1,\\begin{array}{l l}{\\frac{1}{2}}&{0}\\\\ {1}&{-\\frac{1}{2}}&{0}\\end{array}\\right)\\left(\\frac{z}{\\kappa_{1}-1}\\right)+\\left(\\frac{\\hat{b}}{0}\\right)\\right)}\\\\ &{=\\left(\\frac{\\alpha^{\\tau}}{2}\\quad\\frac{\\alpha^{\\tau}}{2}\\quad-\\delta\\quad\\delta\\right)\\sigma\\left(\\left(\\begin{array}{l}{\\hat{W}\\left(z+\\frac{x_{1}}{2}\\right)+\\hat{b}}\\\\ {\\hat{W}\\left(z-\\frac{x_{1}}{2}\\right)+\\hat{b}}\\\\ {0}&{\\left(z-\\frac{x_{1}}{2}\\right)}\\end{array}\\right)\\right)}\\\\ &{=\\frac{1}{2}\\alpha^{\\tau}\\sigma\\left(\\tilde{\\Psi}\\left(z+\\frac{x_{1}}{2}\\right)+\\mathbf{b}\\right)+\\frac{1}{2}\\alpha^{\\tau}\\sigma\\left(\\tilde{\\Psi}\\left(z-\\frac{x_{1}}{2}\\right)+\\hat{b}\\right)}\\\\ &{\\quad+\\delta\\left(\\sigma\\left(-z+\\frac{x_{1}}{2}-\\sigma\\left(z-\\frac{x_{1}}{2}\\right)\\right)\\right)}\\\\ &{=\\frac{1}{2}\\left(\\phi_{N}\\left(z+\\frac{x_{1}}{2}\\right)-\\phi\\left(z-\\frac{x_{1}}{2}\\right)\\right)+\\delta\\left(\\frac{x_{1}}{2}-z\\right)=\\tilde{h}(z,\\mathbf{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first line resembles a function implemented by an FNN with width $4N+2$ . ", "page_idx": 19}, {"type": "text", "text": "2. $\\tilde{h}(z,\\mathbf{x})$ approximate $\\tilde{g}(z,\\mathbf{x})$ well on $\\begin{array}{r}{(z,\\mathbf{x})\\in\\left[-\\delta,\\frac{1}{2}\\right]\\times[0,1]^{d}}\\end{array}$ . Since $\\textstyle z+{\\frac{x_{1}}{2}}\\in[0,1]$ and $z-\\frac{x_{1}}{2}\\in[-1,0]$ , from Eq. (28), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\tilde{h}(z,\\mathbf{x})-\\tilde{g}(z,\\mathbf{x})|=\\displaystyle\\frac{1}{2}\\left[\\phi_{N}\\left(z+\\frac{x_{1}}{2}\\right)-\\left(z+\\frac{x_{1}}{2}\\right)^{2}\\right]-\\displaystyle\\frac{1}{2}\\left[\\phi_{N}\\left(z-\\frac{x_{1}}{2}\\right)-\\left(z-\\frac{x_{1}}{2}\\right)^{2}\\right]}\\\\ &{\\phantom{\\leq\\displaystyle\\frac{1}{2}\\left[\\phi_{N}\\left(z+\\frac{x_{1}}{2}\\right)-\\left(z+\\frac{x_{1}}{2}\\right)^{2}\\right]+\\displaystyle\\frac{1}{2}\\left\\vert\\phi_{N}\\left(z-\\frac{x_{1}}{2}\\right)-\\left(z-\\frac{x_{1}}{2}\\right)^{2}\\right\\vert}\\\\ &{\\phantom{\\leq\\displaystyle\\frac{1}{2}\\left[\\phi_{N}^{2}+\\frac{t^{2}}{4}\\right)=\\frac{t^{2}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "3. The fixed point equation $z\\,=\\,\\tilde{h}(z,{\\bf x})$ is well-posed on $\\left[-\\delta,\\frac{1}{2}\\right]\\,\\times\\,[0,1]^{d}$ . for the partial derivative \u2202\u02dch(\u2202zz,x), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left|\\frac{\\partial\\tilde{h}(z,\\mathbf{x})}{\\partial z}\\right|=\\frac{1}{2}\\left(\\phi_{N}^{\\prime}\\left(z+\\frac{x_{1}}{2}\\right)-\\phi_{N}^{\\prime}\\left(z-\\frac{x_{1}}{2}\\right)\\right)-\\delta}}\\\\ &{}&{\\leq\\frac{1}{2}\\left(\\phi_{N}^{\\prime}\\left(z+\\frac{x_{1}}{2}\\right)-\\phi_{N}^{\\prime}\\left(z+\\frac{x_{1}}{2}-1\\right)\\right)-\\delta}\\\\ &{}&{\\leq1-\\delta<1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second line holds because $\\phi^{\\prime}(x)$ is monotonically increasing and $x_{1}<1$ . Therefore, the fixed point equation $z=\\tilde{h}(z,\\mathbf{x})$ has a unique solution for all $\\mathbf{x}$ . ", "page_idx": 20}, {"type": "text", "text": "4. Note that $\\tilde{h}(z,\\mathbf{x})$ can be computed by a revised DEQ defined in Eq. (8) with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{V}=\\left(\\frac{\\mathbf{a}^{T}}{2}\\quad\\frac{\\mathbf{a}^{T}}{2}\\quad-\\delta\\quad\\delta\\right),\\quad\\mathbf{W}=\\left(\\begin{array}{l}{\\tilde{\\mathbf{W}}}\\\\ {\\tilde{\\mathbf{W}}}\\\\ {1}\\\\ {-1}\\end{array}\\right),\\quad\\mathbf{B}=\\mathbf{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "And it can be verified that $\\|\\mathbf{W}\\mathbf{V}\\|_{2}=1-t-2\\delta\\le1$ . By Lemma 1, the fixed point of $z=\\tilde{h}(z,\\mathbf{x})$ can be computed by a DEQ with width $4N+2$ , which we denote by $N_{\\mathrm{deq}}(\\mathbf{x})$ . Further calculations show that the weight of the DEQ is also bounded by $2t$ . ", "page_idx": 20}, {"type": "text", "text": "Approximate $g$ using the induced DEQ. We will bound $\\|N_{\\mathrm{deq}}(\\mathbf{x})-g(\\mathbf{x})\\|_{L^{\\infty}([0,1]^{d})}$ using Lemma 2 and Lemma 3. Let $\\Omega=[-\\delta,\\frac{1}{2}]$ and assume that $t>10\\delta$ . It can be easily verified that both the range of $\\tilde{g}(z,\\mathbf{x})$ and $\\tilde{h}(z,\\mathbf{x})$ are in $\\Omega$ when $(z,\\mathbf{x})\\in\\Omega\\times[0,1]^{d}$ . ", "page_idx": 20}, {"type": "text", "text": "1. When $\\begin{array}{r}{x_{1}\\leq1-\\frac{t}{2}}\\end{array}$ , by definition, the Lipschitz constant of $\\tilde{g}(\\cdot,\\mathbf x)$ is upper bounded by $\\operatorname*{max}\\left|\\frac{\\partial\\tilde{g}(z,\\mathbf{x})}{\\partial z}\\right|$ \u2202g\u02dc(\u2202zz,x)  . Leveraging Lemma 2 and Eq.(30), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|N_{\\mathrm{deq}}(\\mathbf{x})-g(\\mathbf{x})|\\leq\\left|1-\\frac{\\partial\\widetilde{g}(z,\\mathbf{x})}{\\partial z}\\right|^{-1}|\\widetilde{h}(z,\\mathbf{x})-\\widetilde{g}(z,\\mathbf{x})|\\leq\\frac{2}{t}\\cdot\\frac{t^{2}}{4}=\\frac{t}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. When $1>x_{1}>1-\\textstyle{\\frac{t}{2}}$ , if $\\textstyle z+{\\frac{x_{1}}{2}}=n t$ for some $\\begin{array}{r}{\\frac{N}{2}-1\\le n\\le N}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nz-{\\frac{x_{1}}{2}}=n t-{\\frac{x_{1}}{2}}\\in\\left(\\left(n-{\\frac{N}{2}}\\right)t,\\left(n-{\\frac{N}{2}}-1\\right)t\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\phi_{N}(x)>x^{2}$ for all $x\\in[0,1]\\backslash t\\mathbb{N}$ and $\\phi_{N}(x)=x^{2}$ for all $x\\in[0,1]\\cap t\\mathbb{N}$ . Thus, when $\\begin{array}{r}{z=n t-\\frac{x_{1}}{2}}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{h}(z,\\mathbf{x})<\\frac{1}{2}\\left(\\left(z+\\frac{x_{1}}{2}\\right)^{2}-\\left(z-\\frac{x_{1}}{2}\\right)^{2}\\right)+\\delta\\left(\\frac{x_{1}}{2}-z\\right)=\\tilde{g}(z,\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For every $T\\subset\\Omega$ with $\\vert T\\vert\\le t$ , there exists $z_{g}\\in T$ , such that $\\begin{array}{r}{z_{g}=n t-\\frac{x_{1}}{2}}\\end{array}$ . Thus, from the above analysis, we know that $\\tilde{h}(z_{g},\\mathbf{x})<\\tilde{g}(z_{g},\\mathbf{x})$ . ", "page_idx": 20}, {"type": "text", "text": "On the other hand, if $\\begin{array}{r}{z={\\frac{x_{1}}{2}}-k t}\\end{array}$ for some $\\begin{array}{r}{0\\leq k\\leq\\frac{N}{2}-1}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nz+{\\frac{x_{1}}{2}}=k t+{\\frac{x_{1}}{2}}\\in\\left(\\left(-k+{\\frac{N}{2}}-1\\right)t,\\left(-k+{\\frac{N}{2}}\\right)t\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{h}(z,\\mathbf{x})>\\frac{1}{2}\\left(\\left(z+\\frac{x_{1}}{2}\\right)^{2}-\\left(z-\\frac{x_{1}}{2}\\right)^{2}\\right)+\\delta\\left(\\frac{x_{1}}{2}-z\\right)=\\tilde{g}(z,\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For every $T\\subset\\Omega$ with $\\vert T\\vert\\le t$ , there exists $z_{l}\\in T$ , such that $\\begin{array}{r}{z_{l}=\\frac{x_{1}}{2}-k t}\\end{array}$ . Thus we have $\\tilde{h}(z_{g},\\mathbf{x})>\\tilde{g}(z_{g},\\mathbf{x})$ . From the intermediate value theorem, there exists $z^{*}\\in T$ , such that $\\tilde{h}(z^{*},{\\bf x})=\\tilde{g}(z^{*},{\\bf x})$ . Thus it follows from Lemma 3 immediately that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|N_{\\mathrm{deq}}(\\mathbf{x})-g(\\mathbf{x})|\\leq t.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Additionally, when $x_{1}=1$ , by simple calculations, we have $\\begin{array}{r}{\\tilde{h}\\left(\\frac{1}{2},\\mathbf{x}\\right)=\\tilde{g}\\left(\\frac{1}{2},\\mathbf{x}\\right)=\\frac{1}{2}}\\end{array}$ , indicating that $N_{\\mathrm{deq}}(\\mathbf{x})=g(\\mathbf{x})=\\frac{1}{2}$ . Combining all the results above, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n|N_{\\mathrm{deq}}(\\mathbf{x})-g(\\mathbf{x})|\\leq|g(\\mathbf{x})-\\bar{z}^{\\prime}|\\leq t,\\quad\\mathbf{x}\\in[0,1]^{d}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By choosing $t=\\varepsilon$ , we finish the proof. ", "page_idx": 21}, {"type": "text", "text": "A.3 Proofs in Section 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We start with the proof of Theorem 3. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3. Denote $\\{\\beta(t)\\}$ the process that follows the gradient flow dynamics $\\dot{\\mathbf{w}}(t)\\,=$ $-\\nabla_{\\mathbf{w}}\\hat{L}(\\mathbf{w}(t))$ initialized by $\\beta(0)>0$ . Recall that the empirical loss is $\\frac{1}{2}\\|\\mathbf{\\nabla}\\mathbf{X}\\,\\beta-\\mathbf{y}\\parallel_{2}^{2}$ , then the dynamics of $\\{\\beta(t)\\}$ can be computed as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}\\beta(t)}{\\mathrm{d}t}=\\nabla_{\\mathbf{w}}\\beta(t)\\cdot\\frac{\\mathrm{d}w(t)}{\\mathrm{d}t}}\\\\ &{\\quad\\quad=\\nabla_{\\mathbf{w}}\\beta(t)\\cdot\\left(-\\nabla_{\\mathbf{w}}\\left(\\frac{1}{2}\\|\\mathbf{X}\\,\\beta(t)-\\mathbf{y}\\|_{2}^{2}\\right)\\right)}\\\\ &{\\quad\\quad=\\nabla_{\\mathbf{w}}\\beta(t)^{2}\\cdot\\left(-\\nabla_{\\beta}\\left(\\frac{1}{2}\\|\\mathbf{X}\\,\\tilde{\\beta}(t)-\\mathbf{y}\\|_{2}^{2}\\right)\\right)}\\\\ &{\\quad\\quad=-\\left(\\mathbf{X}^{T}\\mathbf{r}(t)\\right)\\odot\\tilde{\\beta}(t)^{\\odot4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{r}(t)=\\mathbf{X}\\,\\beta(t)-\\mathbf{y}$ denotes the residual. For any $t>0$ , it can be verified easily from Eq. (31) that ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\frac{1}{3}\\beta(t)^{\\odot-3}+\\frac{1}{3}\\beta(0)^{\\odot-3}=-\\mathbf{X}^{T}\\int_{0}^{t}\\mathbf{r}(s)\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For simplicity, we denote $\\begin{array}{r}{\\mathbf{v}(t)=\\int_{0}^{t}\\mathbf{r}(s)\\mathrm{d}s}\\end{array}$ . Then from Eq. (32), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\beta(t)=\\left(3{\\bf X}^{T}{\\bf v}(t)+\\beta(0)^{\\odot-3}\\right)^{\\odot-{\\frac{1}{3}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By assumption, $\\beta(t)$ converges to some $\\beta^{\\infty}\\,\\in\\,\\mathbb{R}^{d}$ when $t\\,\\rightarrow\\,\\infty$ , thus $\\mathbf{v}(t)$ converges to some $\\begin{array}{r}{\\mathbf{v}^{\\infty}:=\\int_{0}^{\\infty}\\mathbf{r}(s)\\mathrm{d}s}\\end{array}$ . By letting $t\\to\\infty$ in Eq. (33), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{\\infty}=\\left(3\\mathbf{X}^{T}\\mathbf{v}^{\\infty}+\\beta(0)^{\\odot-3}\\right)^{\\odot-\\frac{1}{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next we want to show that $\\beta^{\\infty}$ satisfies the KKT condition of the optimization problem in Eq. (12). Given access to the expression of $Q(\\beta)$ , the KKT optimality conditions can be expressed as ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\bf X}\\,\\beta^{*}={\\bf y},\\quad\\nabla Q(\\beta^{*})={\\bf X}\\,{\\bf v},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $\\mathbf{v}\\in\\mathbb{R}^{d}$ . By the definition of $Q(\\beta),\\nabla Q(\\beta^{*})=\\mathbf{X}^{T}\\,\\mathbf{v}$ is equivalent to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\mathbf{X}^{T}\\thinspace\\mathbf{v}\\right)_{i}=(\\nabla Q(\\beta^{*}))_{i}=q^{\\prime}(\\beta_{i}^{*})=-(\\beta_{i}^{*})^{-3}+\\beta_{i}(0)^{-3},\\quad\\forall i.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, from Eq. (34), it can be verified that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-(\\beta_{i}^{\\infty})^{-3}+\\beta_{i}(0)^{-3}=-3(\\mathbf{X}^{T}\\mathbf{v}^{\\infty})_{i}-\\beta_{i}(0)^{-3}+\\beta_{i}(0)^{-3}=-3(\\mathbf{X}^{T}\\mathbf{v}^{\\infty})_{i},\\quad\\forall i.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus it holds that $\\begin{array}{r}{\\nabla Q(\\beta^{\\infty})=-\\frac{1}{3}\\,\\mathbf{X}\\,\\mathbf{v}^{\\infty}}\\end{array}$ . Combining this with the assumption that $\\mathbf{X}\\,{\\boldsymbol{\\beta}}^{\\infty}=\\mathbf{y}$ , we derive that $\\beta^{\\infty}$ satisfies the KKT condition. Moreover, it can be observed that $Q(\\beta)$ is convex, which makes $\\beta^{\\infty}$ an optimum of the problem. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 4. Gradient Flow. We first show that the distance between $\\beta(t)$ and $\\hat{\\beta}^{*}$ is bounded. From the dynamic of $\\beta(t)$ shown in Eq. (31), we can derive that the gradient flow of $\\lVert\\beta(t)-\\hat{{\\boldsymbol\\beta}}^{*}\\rVert_{2}^{2}$ as below: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\|\\beta(t)-\\hat{\\beta}^{*}\\|_{2}^{2}=\\left(\\frac{\\mathrm{d}\\beta(t)}{\\mathrm{d}t}\\right)^{T}(\\beta(t)-\\hat{\\beta}^{*})=-\\left\\|\\mathbf{X}(\\beta(t)-\\hat{\\beta}^{*})\\odot\\beta(t)^{\\odot2}\\right\\|_{2}^{2}\\le0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, $\\|\\beta(t)-\\hat{\\beta}^{*}\\|_{2}^{2}$ is monotonically non-increasing and upper bounded by $\\lVert\\beta(0)-\\hat{{\\boldsymbol\\beta}}^{*}\\rVert_{2}^{2}$ for all $t$ . By Assumption 1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\beta(t)\\|_{\\infty}\\leq\\|\\beta(t)-\\hat{\\beta}^{*}\\|_{\\infty}+\\|\\hat{\\beta}^{*}\\|_{\\infty}}&{}\\\\ {\\leq\\|\\beta(t)-\\hat{\\beta}^{*}\\|_{2}+\\|\\hat{\\beta}^{*}\\|_{\\infty}}&{}\\\\ {\\leq\\|\\hat{\\beta}^{*}\\|_{\\infty}+\\|\\hat{\\beta}^{*}-\\beta(0)\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, we also have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\beta(t)\\|_{\\infty}\\geq\\|\\hat{\\beta}^{*}\\|_{\\infty}-\\|\\beta(t)-\\hat{\\beta}^{*}\\|_{2}\\geq\\|\\hat{\\beta}^{*}\\|_{\\infty}-\\|\\hat{\\beta}^{*}-\\beta(0)\\|_{2}\\geq c}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $t$ . To prove the convergence, we denote $\\mathbf{r}(t)=\\mathbf{X}\\,\\beta(t)-\\mathbf{y}$ . The gradient flow of $\\|\\mathbf{r}(t)\\|_{2}^{2}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\|\\mathbf{r}(t)\\|_{2}^{2}=\\left(\\frac{\\mathrm{d}\\beta(t)}{\\mathrm{d}t}\\right)^{T}\\mathbf{X}^{T}\\mathbf{r}(t)=-(\\mathbf{r}(t)^{T}\\,\\mathbf{X}\\,\\mathbf{X}^{T}\\mathbf{r}(t))\\odot\\beta(t)^{\\odot4}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining this with the fact that $\\mu_{\\mathrm{min}}>0$ and the lower boundedness of $\\|\\beta(t)\\|_{\\infty}$ , we then have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\|\\mathbf{r}(t)\\|_{2}^{2}\\leq-c^{4}\\mu_{\\mathrm{min}}\\|\\mathbf{r}(t)\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which proves the convergence of gradient flow. Thus $\\beta(t)$ converges to an optima $\\beta_{f}^{\\infty}$ . By letting $t\\to\\infty$ and using the lower boundedness of $\\|\\beta(t)\\|_{\\infty}$ , we know that $\\|\\beta_{f}^{\\infty}\\|_{\\infty}\\geq c$ . ", "page_idx": 22}, {"type": "text", "text": "Gradient Descent. The proof of gradient descent follows from a similar strategy. We first give an explicit expression of the update on $\\beta^{k}$ . In the following we denote $\\mathbf{r}^{k}=\\mathbf{X}\\,\\beta^{k}\\,\\breve{-}\\,\\mathbf{y}$ . Recall that the gradient descent iterate on $w_{i}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{i}^{k+1}=w_{i}^{k}-\\eta\\frac{1}{(1-w_{i})^{2}}\\mathbf{x}_{i}\\mathbf{r}^{k},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{x}_{i}$ denotes the $i$ -th column of $\\mathbf{X}$ . Then by the definition of $\\beta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\beta_{i}^{k+1}=\\frac{1}{1-w_{i}^{k+1}}=\\left(\\frac{1}{1-w_{i}^{k}}-\\frac{1}{1-w_{i}^{k}+\\eta\\beta_{i}^{2}\\mathbf{x}_{i}^{T}\\mathbf{r}^{k}}\\right)\\frac{1-w_{i}^{k}}{\\eta\\beta_{i}^{2}\\mathbf{x}_{i}^{T}\\mathbf{r}^{k}}}\\\\ {\\displaystyle=\\frac{\\beta_{i}^{k}-\\beta_{i}^{k+1}}{\\eta(\\beta_{i}^{k})^{3}\\mathbf{x}_{i}^{T}\\mathbf{r}^{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Equivalently, the update of $\\beta$ can be expressed as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta^{k+1}=\\beta^{k}-\\eta{\\bf X}^{T}{\\bf r}^{k}\\odot{\\bf u}^{k},\\quad{\\bf u}^{k}:=\\left(\\beta^{k}\\right)^{\\odot3}\\odot\\beta^{k+1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now show that with an appropriate choice of $\\eta$ , the distance between $\\beta^{k}$ and $\\hat{\\beta}^{*}$ is bounded. By Eq. (38), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\beta^{k+1}-\\hat{\\beta}^{*}\\|_{2}^{2}-\\|\\beta^{k}-\\hat{\\beta}^{*}\\|_{2}^{2}=\\|\\beta^{k+1}-\\beta^{k}\\|_{2}^{2}-2(\\beta^{k}-\\hat{\\beta}^{*})^{T}(\\beta^{k+1}-\\beta^{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\eta^{2}\\left\\|\\mathbf{X}^{T}\\mathbf{r}^{k}\\odot\\mathbf{u}^{k}\\right\\|_{2}^{2}-2\\eta\\left\\|\\mathbf{r}^{k}\\odot(\\mathbf{u}^{k})^{\\odot}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mu_{\\operatorname*{max}}\\eta^{2}\\displaystyle\\sum_{i=1}^{n}(r_{i}^{k}u_{i}^{k})^{2}-2\\eta\\displaystyle\\sum_{i=1}^{n}(r_{i}^{k})^{2}u_{i}^{k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{n}\\left(\\mu_{\\operatorname*{max}}\\eta^{2}(u_{i}^{k})^{2}-2\\eta u_{i}^{k}\\right)(r_{i}^{k})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Assume \u03b2k > 0 for all k so that uik > 0 for all i. Now we set \u03b7 <C\u00b51max . With these conditions, it holds for each $i$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu_{\\operatorname*{max}}\\eta^{2}(u_{i}^{k})^{2}-2\\eta u_{i}^{k}\\leq0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining this with Eq. (39), we have $\\|\\beta^{k+1}-\\hat{\\beta}^{*}\\|_{2}^{2}\\,\\le\\,\\|\\beta^{k}-\\hat{\\beta}^{*}\\|_{2}^{2}$ . Similar to Eq. (36), by Assumption 1, it can be shown that $\\|\\beta^{k}\\|_{\\infty}\\geq c>0$ for all $k$ . We then turn to analyze the update of $\\|\\mathbf{r}^{k}\\|_{2}$ . Note that the square loss function is $\\mu_{\\mathrm{max}}$ -smooth w.r.t. $\\beta$ , thus we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{r}^{k+1}\\|_{2}^{2}\\leq\\|\\mathbf{r}^{k}\\|_{2}^{2}+2(\\mathbf{r}^{k})^{T}\\mathbf{X}(\\beta^{k+1}-\\beta^{k})+\\mu_{\\operatorname*{max}}\\|\\beta^{k+1}-\\beta^{k}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting the update of $\\beta^{k}$ in Eq. (38) into the above equation, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\mathbf{r}^{k+1}\\|_{2}^{2}\\leq\\|\\mathbf{r}^{k}\\|_{2}^{2}-2\\eta(\\mathbf{r}^{k})^{T}\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{r}^{k}\\odot(\\beta^{k+1})^{3}\\odot\\beta^{k}\\right)+\\eta^{2}\\mu_{\\operatorname*{max}}\\left\\|\\mathbf{X}^{T}\\mathbf{r}^{k}\\odot(\\beta^{k+1})^{3}\\odot\\beta^{k}\\right\\|_{2}^{2}}\\\\ {\\displaystyle=\\|\\mathbf{r}^{k}\\|_{2}^{2}-2\\eta\\displaystyle\\sum_{i=1}^{n}(l_{i}^{k})^{2}u_{i}^{k}+\\eta^{2}\\mu_{\\operatorname*{max}}\\displaystyle\\sum_{i=1}^{n}(l_{i}^{k}u_{i}^{k})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we denote $\\mathbf{X}^{T}\\mathbf{r}^{k}=1^{k}$ for simplicity. For every fixed $l_{i}^{k}$ , the quadratic function $f(u_{i}^{k})=$ $-2\\eta(l_{i}^{k})^{2}u_{i}^{k}+\\eta^{2}\\mu_{\\operatorname*{max}}(l_{i}^{k}u_{i}^{k})^{2}$ attains its minima at $\\begin{array}{r}{u_{i}^{k}=\\frac{1}{\\eta\\mu_{\\mathrm{max}}}>C}\\end{array}$ , from which we know that $f(u_{i}^{k})$ is monotonically decreasing for $u_{i}^{k}<C$ . Hence, by the fact that $u_{i}^{k}>c$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n-2\\eta(l_{i}^{k})^{2}u_{i}^{k}+\\eta^{2}\\mu_{\\operatorname*{max}}(l_{i}^{k}u_{i}^{k})^{2}\\leq(-2\\eta c+\\eta^{2}\\mu_{\\operatorname*{max}}c^{2})(l_{i}^{k})^{2}\\leq0,\\quad\\forall1\\leq i\\leq n.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\sum_{i=1}^{n}(l_{i}^{k})^{2}=\\|\\mathbf{X}^{T}\\mathbf{r}^{k}\\|_{2}^{2}\\leq\\mu_{\\operatorname*{max}}\\|\\mathbf{r}^{k}\\|_{2}^{2}}\\end{array}$ . Leveraging this and Eq. (40) and Eq. (41), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{r}^{k+1}\\|_{2}^{2}\\leq\\big(1-(-2\\eta c+\\eta^{2}\\mu_{\\operatorname*{max}}c^{2})\\big)\\|\\mathbf{r}^{k}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, to ensure $\\begin{array}{r}{\\beta_{i}^{1}=\\frac{\\beta_{i}^{0}}{1+\\eta(\\beta_{i}^{0})^{3}\\mathbf{x}_{i}^{T}\\mathbf{r}^{0}}\\geq0}\\end{array}$ for all $i$ , we choose ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{\\|\\beta^{0}\\|_{\\infty}^{3}\\mathbf{x}_{i}^{T}\\mathbf{r}^{0}}\\leq\\frac{1}{C^{3}\\|\\mathbf{X}\\|_{2}\\|\\mathbf{r}^{0}\\|_{2}}\\leq\\frac{1}{C^{4}\\mu_{\\operatorname*{max}}\\|\\mathbf{r}^{0}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With this choice of $\\eta$ , we can prove by induction that the assumption $\\beta^{k}>0$ holds for all $k$ . Indeed, $k=0$ follows immediately from assumption. If $\\beta^{t}>0$ holds, then from update of $\\beta^{t+1}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta_{i}^{t+1}=\\frac{\\beta_{i}^{t}}{1+\\eta(\\beta_{i}^{t})^{3}\\mathbf{x}_{i}^{T}\\mathbf{r}^{t}}\\geq\\frac{\\beta_{i}^{t}}{1+\\eta(\\beta_{i}^{t})^{3}\\|\\mathbf{x}_{i}^{T}\\|_{2}\\|\\mathbf{r}^{t}\\|_{2}}\\geq\\frac{\\beta_{i}^{t}}{1+\\eta C^{3}\\|\\mathbf{X}\\|_{2}\\|\\mathbf{r}^{0}\\|_{2}}\\geq0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus the induction holds. Finally, we set n C42\u00b5max ,C4\u00b5ma1x\u2225r0\u22252 , then \u03b2k converges to an optimal $\\beta_{d}^{\\infty}$ . It follows from $\\|\\beta^{k}\\|_{\\infty}\\geq c$ that $\\|\\beta_{d}^{\\infty}\\|_{\\infty}\\geq c$ . We finish the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Next, we move to prove Proposition 2. For completeness, we formally introduce the definition of GOTU in [36] as below. ", "page_idx": 23}, {"type": "text", "text": "Definition 1 (Generalization on the Unseen, [36]). Let $\\ell:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ be a loss function and $\\boldsymbol{S}$ be $a$ given sample space. During training, part of $\\boldsymbol{S}$ is not sampled, which we call the unseen domain $\\boldsymbol{\\mathcal{U}}$ , while in testing, we sample from the full set $\\boldsymbol{S}$ . Let $f$ be the target function and $\\tilde{f}_{S\\backslash\\mathcal{U}}$ the function learned by a learning algorithm on $s\\backslash{\\mathcal{U}}$ . The generalization on the unseen for an algorithm $\\tilde{f}$ and target function $f$ is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\nG O T U(f,\\tilde{f},\\mathcal{U})=\\mathbb{E}_{X\\sim\\upsilon}\\lbrack\\ell(\\tilde{f}_{S\\backslash\\mathcal{U}}(X),f(X))\\rbrack,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ${\\sim}\\boldsymbol{U}^{\\mathcal{U}}$ refers to uniform sampling from $\\boldsymbol{\\mathcal{U}}$ ", "page_idx": 23}, {"type": "text", "text": "Proof of Proposition 2. We first give an explicit expression of the expected loss and gradient flow dynamics. Denote ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\tilde{f}}_{\\beta}(\\mathbf{x})=\\sum_{i=1}^{d}\\beta_{i}x_{i}+b=f(\\mathbf{w},\\mathbf{x})=\\sum_{i=1}^{d}{\\frac{1}{1-w_{i}}}x_{i}+b.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By definition, the half $\\ell_{2}$ loss on any sample $\\mathbf{x}$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ell(\\tilde{f}_{\\beta}(\\mathbf{x}),f(\\mathbf{x}))=\\frac{1}{2}(\\tilde{f}_{\\beta}(\\mathbf{x})-f(\\mathbf{x}))=\\frac{1}{2}\\left(b-\\hat{f}(\\boldsymbol{\\emptyset})+\\sum_{i=1}^{d}\\left(\\beta_{i}-\\hat{f}(\\{i\\})\\right)x_{i}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Denote the distribution on the training set by $U_{-k}^{d-1}$ . Note that $\\{1,x_{1},\\cdot\\cdot\\cdot,x_{d}\\}$ are orthogonal in the Hilbert space $S=\\{\\pm1\\}^{d}$ equipped with the inner product $\\langle g,h\\rangle=\\mathbb{E}_{\\mathbf{x}\\sim\\mathbf{\\boldsymbol{U}}}\\{\\pm1\\}^{d}\\left[g(\\mathbf{x})h(\\mathbf{x})\\right]$ . Denote ", "page_idx": 23}, {"type": "text", "text": "the distribution on the training samples by $U_{-k}^{d-1}$ . By using Parseval\u2019s Theorem, the expected loss on the training set can be expressed as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{U_{-k}^{d-1}}[\\ell(\\tilde{f}_{\\beta}(\\mathbf x),f(\\mathbf x))]=\\displaystyle\\frac12\\mathbb{E}_{U_{-k}^{d-1}}\\left[\\left(b-\\hat{f}(\\boldsymbol\\theta)+\\displaystyle\\sum_{i=1}^{d}\\Big(\\beta_{i}-\\hat{f}(\\{i\\})\\Big)x_{i}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac12\\left(b-\\hat{f}(\\boldsymbol\\theta)+\\beta_{k}-\\hat{f}(\\{k\\})\\right)^{2}+\\frac{1}{2}\\displaystyle\\sum_{i\\neq k}^{d}(\\beta_{i}-\\hat{f}(\\{i\\}))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we can derive the gradient flow for $\\beta_{i}$ and $b$ as below ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\mathrm{d}b(t)}{\\mathrm{d}t}=-(b(t)-\\hat{f}(\\emptyset)+\\beta_{k}(t)-\\hat{f}(\\{k\\})),}\\\\ &{\\frac{\\mathrm{d}\\beta_{k}(t)}{\\mathrm{d}t}=-(b(t)-\\hat{f}(\\emptyset)+\\beta_{k}(t)-\\hat{f}(\\{k\\}))\\beta_{k}(t)^{4},}\\\\ &{\\frac{\\mathrm{d}\\beta_{i}(t)}{\\mathrm{d}t}=-(\\beta_{i}(t)-\\hat{f}(\\{i\\}))\\beta_{i}(t)^{4},\\quad\\forall i\\neq k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For simplicity, denote $B={\\hat{f}}(\\emptyset)+{\\hat{f}}(\\{k\\})$ . Using the above, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}(b(t)+\\beta_{k}(t)-B)^{2}=-2(b(t)+\\beta_{k}(t)-B)^{2}(1+\\beta_{k}(t)^{4}),}\\\\ &{\\quad\\frac{\\mathrm{d}}{\\mathrm{d}t}(\\beta_{i}(t)-\\hat{f}(\\{i\\}))^{2}=-2(\\beta_{i}(t)-\\hat{f}(\\{i\\}))^{2}\\beta_{i}(t)^{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which shows that $|b(t)+\\beta_{k}(t)-B|^{2}$ and $|\\beta_{i}(t)-\\hat{f}(\\{i\\})|^{2}$ is monotonically nonincreasing. Since $\\beta_{i}(0)$ and ${\\hat{f}}(\\{i\\})$ are greater than 0, from the monotonicity we know that $\\beta_{i}(t)~>~0$ for all $t$ Therefore, the convergence of gradient flow follows from Eq. (42) that both $|b(t)+\\beta_{k}(t)-B|^{2}$ and $|\\beta_{i}(t)-\\hat{f}(\\{i\\})|^{2}$ decrease linearly. ", "page_idx": 24}, {"type": "text", "text": "Denote the limit of $\\beta_{i}(t)$ and $b(t)$ by $\\beta_{i}^{\\infty}$ and $b^{\\infty}$ , respectively. We now turn to estimate the GOTU error. ", "page_idx": 24}, {"type": "text", "text": "When $B>1$ , it holds that $b(0)+\\beta_{k}(0)-B<0$ , thus $b(t)$ and $\\beta_{k}(t)$ is monotonically increasing. Using the fact that $\\beta_{k}(t)>\\beta_{k}(0)=1$ , we know that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}(\\beta_{k}(t)-b(t))=-2(b(t)+\\beta_{k}(t)-B)^{2}(\\beta_{k}(t)^{4}-1)<0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combing this with $\\beta_{k}^{\\infty}+b^{\\infty}=B$ , it can be verified that $\\begin{array}{r}{\\beta_{k}^{\\infty}\\geq\\frac{B+1}{2}}\\end{array}$ . Then by definition and Parseval\u2019s theorem, the GOTU loss is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G O T U(f,\\tilde{f}_{\\beta},\\{x_{k}=-1\\})=\\Big(b^{\\infty}-\\hat{f}(\\varnothing)-\\beta_{k}^{\\infty}+\\hat{f}(\\{k\\})\\Big)^{2}+\\displaystyle\\sum_{i\\neq k}^{d}(\\beta_{i}^{\\infty}-\\hat{f}(\\{i\\}))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=4(\\hat{f}(\\{k\\})-\\beta_{k}^{\\infty})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use the convergence of the flow in the second line. Leveraging the bound of $\\beta_{k}^{\\infty}$ , we derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n4(\\hat{f}(\\{k\\})-\\beta_{k}^{\\infty})^{2}\\le4\\left(\\hat{f}(\\{k\\})-\\frac{B+1}{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the assumption that ${\\hat{f}}(\\emptyset)<2{\\hat{f}}(\\{k\\})$ , we have $\\begin{array}{r}{\\frac{B+1}{2}\\prec\\frac{3\\hat{f}(\\{k\\})+1}{2}<2\\hat{f}(\\{k\\})}\\end{array}$ . Leveraging this in Eq. (43), we know that ", "page_idx": 24}, {"type": "equation", "text": "$$\nG O T U(f,\\tilde{f}_{\\beta},\\{x_{k}=-1\\})\\le(\\hat{f}(\\{k\\})+1)^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "2. When $B<1$ , similar to the proof of Theorem 3, we have from the dynamic of $\\beta_{k}(t)$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\beta_{k}(t)^{-3}-1=3\\int_{0}^{t}(b(s)+\\beta_{k}(t)-B)\\mathrm{d}s\\leq3(1-B),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use the monotonicity of $b(s)+\\beta_{k}(t)\\,-\\,B$ and the convergence of the flow. Therefore, it holds that $\\beta_{k}^{\\infty}\\geq(3(1-B)+1)^{-{\\frac{1}{3}}}$ . We can bound the GOTU error as ", "page_idx": 25}, {"type": "equation", "text": "$$\n4(\\hat{f}(\\{k\\})-\\beta_{k}^{\\infty})^{2}\\le4(\\hat{f}(\\{k\\})-(3(1-B)+1)^{-\\frac{1}{3}})^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By using the assumption that ${\\hat{f}}(\\varnothing)>-2{\\hat{f}}(\\{k\\})$ , Eq. (45) gives ", "page_idx": 25}, {"type": "equation", "text": "$$\nG O T U(f,\\tilde{f}_{\\beta},\\{x_{k}=-1\\})\\le4\\left(\\hat{f}(\\{k\\})-\\left(4+3\\hat{f}(\\{k\\})\\right)^{-\\frac{1}{3}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the proposition follows from Eq. (44) and (46). ", "page_idx": 25}, {"type": "text", "text": "A.4 Discussion on Comparison Fairness ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide a detailed discussion on the fairness of the comparison in the separation results between the expressivity on DEQs and FNNs. ", "page_idx": 25}, {"type": "text", "text": "Concerns may arise regarding the fairness of comparing finite-depth FNNs to DEQs, which resemble infinite-depth weight-tied FNNs. However, as stated in Section 4, our separation results are mainly stated from the actual size of the network (Theorem 2 also considers the parameter magnitude). Characterizing expressivity in term of the depth and width of the network is a common approach as these dimensions can be manipulated in practice. From a theoretical perspective, one primary goal for the comparison is to investigate how the DEQ architecture influences the model capacity with the same number of parameters as FNN whose weights are not tied. This analysis provides intuitions for understanding the model bias of DEQ. Note that one conceptual understanding here is no free lunch. Our separation results provide concrete examples of functions that DEQs can approximate more parameter-efficiently. Thus, we believe that it is reasonable to compare the expressivity from the actual size as well as the number of parameters of the networks. ", "page_idx": 25}, {"type": "text", "text": "Another concern is whether our separation remains valid when considering the actual memory and computational costs associated with training DEQs. If a DEQ needs exponentially more computational cost than FNN, it could be justifiable to compare a DEQ to an FNN of exponentially larger depth. Nevertheless, considering these issues is beyond the scope of this paper since they are also determined by the forward and backward algorithms used. For completeness, we provide some comparisons based on the computational cost of the forward propagation of DEQs as follows. ", "page_idx": 25}, {"type": "text", "text": "For the DEQ in Theorem 1, we claim that it needs one step to converge using fixed-point iteration. This is achieved by initializing iteration point ${\\bf z}^{0}$ according to Eq. (16) (While it may seem tricky to initialize $\\mathbf{z}^{0}$ as the fixed point, the logic here is that we first observe such convergence under this initialization that we claim this $\\mathbf{z}^{0}$ to be the fixed point). Then from the definition of $\\mathbf{W},\\mathbf{U}$ and $\\mathbf{b}$ , each entry $t$ of $\\mathbf{z}^{1}$ can be calculated as ", "page_idx": 25}, {"type": "equation", "text": "$$\nz_{t}^{1}=\\sigma\\left(\\sum_{i=1}^{t-1}w_{t i}z_{i}^{0}+u_{t1}x_{1}-b_{t}\\right)=\\sigma\\left(-\\sum_{i=1}^{t-1}2^{t-i+1}z_{i}^{0}+2^{t}x_{1}-1\\right)=z_{t}^{0},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "showing that it converges in one iteration. Additionally, if we set $\\mathbf{z}^{0}=\\mathbf{0}$ , which may be considered less ad hoc, and denote the fixed point by $\\mathbf{z}^{\\ast}$ . Then using the lower-triangularity of $\\mathbf{W}$ , we can show by induction that ${\\mathbf z}_{t}^{k}={\\mathbf z}_{t}^{*}$ for all $1\\leq t\\leq k$ . Thus, the convergence is achieved in at most $m$ iterations, which still remains far from exponential. ", "page_idx": 25}, {"type": "text", "text": "For the DEQ in Theorem 2, we claim that it needs $\\log_{2}(\\varepsilon^{-1})+\\log_{2}(b-a)$ iterations to achieve an $O(\\varepsilon)$ -solution using bisection method. Given the DEQ, we can derive a revised DEQ, i.e., $\\mathbf{z}=$ ${\\bf V}\\sigma({\\bf W}z+{\\bf U}{\\bf x}+{\\bf b})$ based on our Lemma 1 (One can inversely derive the revised DEQ through rankone decomposition, and we admit any version of it). Then for the function $z-\\mathbf{V}\\sigma(\\mathbf{W}z+\\mathbf{U}\\mathbf{x}+\\mathbf{b})$ , we choose some $[a^{0},b^{0}]$ as the initial interval such that this function has opposite signs on $a^{0}$ and $b^{0}$ . Then after $k$ iterations the solution $z^{*}$ will lie within an interval $[a^{k},b^{k}]$ . From the definition of the algorithm, we know that $b^{i+1}-a^{i+1}=2^{-1}(b^{i}-a^{i})$ , leading to $b^{k}\\stackrel{\\cdot}{-}a^{k}\\leq2^{-k}(b^{0}-a^{0})$ . To achieve an $O(\\varepsilon)$ -solution, i.e., $2^{-k}(b^{0}-a^{0})\\leq\\varepsilon$ , we can require $k\\geq\\log_{2}\\varepsilon^{-1}+\\log_{2}(b^{0}-a^{0})$ , which proves our claim. ", "page_idx": 25}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/30655aa51ed6d0e67f34c1afcba93d10ba21ad0b917a71daa7ddf35a5d03417e.jpg", "img_caption": ["Figure 3: Test loss of FNN and DEQ trained on sawtooth functions with $2^{1}$ , $2^{3}$ , $2^{15}$ linear regions. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/6bb10af4f0b782aec3690b0eda8a811bdd62261c2ec1eb3e2b55c9e958fcbcaa.jpg", "img_caption": ["Figure 4: Results on steep target functions applying DEQ and FNN with comparable performance on loss. We plot the infinity norm of the weights of DEQ and FNN with $d=6,8,10,12,14,16,18,20.$ "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "For the memory cost, DEQ\u2019s memory usage is comparable to that of constant-depth FNN. Since the DEQs we consider in our theorems have fewer parameters, they are more advantageous than the corresponding FNNs in memory consumption. Consequently, our comparisons are likely to remain valid even when actual memory and computational costs are taken into account. ", "page_idx": 26}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "B.1 Supplementary Experiments on Separation Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "More experiments on piecewise functions. We first experiment on other sawtooth functions with varying number of linear regions in the setting of Figure 3 to further validate our theoretical results. Figure 3 presents the test loss for sawtooth functions with $2^{1}$ , $2^{3}$ and $2^{15}$ linear regions. For all experiments, we execute our programs on Nvidia GTX 1660 and all the programs occupy less than 10M memory and run for less than 2 minutes. Consistent with our results in Section 6, we observe that DEQ outperforms FNN with similar size of network on every sawtooth function in our experiment and the test loss of DEQ converges closer to zero loss. ", "page_idx": 26}, {"type": "text", "text": "Additionally, we conduct an experiment to investigate the performance of DEQ and FNN on sawtooth function with similar actual computational cost to further strengthen the fairness as discussed in Appendix A.4. Following the standard setting, all models in our experiment are trained using $\\ell_{2}$ loss with AdamW optimizer [44], with a learning rate of 5e-4, weight decay of 1e-4 and a cosine annealing scheduler for 1000 iterations. Table 1 indicates that when training on sawtooth function under similar computational cost, DEQ also performs better than FNN. ", "page_idx": 26}, {"type": "text", "text": "More experiments on steep target functions. To verify the statements of the magnitude of weights in Theorem 2, we add an experiment to examine the infinity norm of the weights of DEQ and FNN when training on steep target functions with varying $d$ , as shown in Figure 4. In this experiment, we train a 3-layer FNN and a 50-width DEQ model using the $\\ell_{2}$ loss. Following standard settings, we employ a mini-batch SGD optimizer with a learning rate of 0.005, weight decay of 1e-4 and a cosine annealing scheduler for all models. The infinity norm of the weights of DEQ remains approximately constant when $d$ increases, while the infinity norm of FNN rises with $d$ , verifying the theoretical claims that DEQ with polynomially bounded size and magnitude can efficiently approximate certain steep functions in infinity norm while FNN cannot. ", "page_idx": 26}, {"type": "table", "img_path": "UJ9k3j93MD/tmp/d5f9bcbdc5c4f0e74bec18cd5b4060246bc550a4734d70bf0fd42f6723f67d68.jpg", "table_caption": [], "table_footnote": ["Table 1: Training settings of experiment. We apply DEQ with width-10 and FNN trained on sawtooth target function with $2^{5}$ linear regions while keeping FLOPs per iteration similar. "], "page_idx": 27}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/0c4935df4eefce71a4ff38986db64590bd9798eab0816c08bd6225ac50846952.jpg", "img_caption": ["Figure 5: Train and test loss of DEQ and FNN trained on OOD tasks $f_{1}$ and $f_{2}$ . "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "B.2 Experiments on the Bias on Learning Dynamics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "More experiments on Out-of-Distribution tasks. We conduct more out-of-distribution (OOD) experiments on the following 2 functions and unseen domains. The first function is a higherdimensional extension of Eq. (14) which is a form of mean function. The second function is the majority function for 3 bits with a maximum degree of 3. The mathematical expressions of these functions are presented below. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}(x)=1.25*x_{0}+1.25*x_{1}+\\cdot\\cdot\\cdot+1.25*x_{20},\\quad\\mathcal{U}=\\{\\mathbf x\\in\\{\\pm1\\}^{10}:x_{1}=-1\\},}\\\\ &{f_{2}(x)=\\cfrac{1}{2}(x_{0}+x_{1}+x_{2}-x_{1}x_{2}x_{3}),\\quad\\mathcal{U}=\\{\\mathbf x\\in\\{\\pm1\\}^{10}:x_{0}x_{1}=-1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For all experiments, we generate all binary sequences in $\\mathcal{U}^{c}=\\{\\pm1\\}^{d}\\backslash\\mathcal{U}$ for training. Figure 5(a) shows that the GOTU error does not increase significantly compared to Figure 1 where the ambient dimension is 10. In consistency with our results in Section 6, we can learn from Figure 5(a) that when learning a linear boolean function on population loss on DEQ, the training loss converges to zero and the generalization error on the unseen is bounded. As shown in Figure 5(b), when learning the unlinear boolean function, DEQ can also achieve nearly zero train loss with smaller GOTU error compared with FNN. ", "page_idx": 27}, {"type": "text", "text": "Experiment on salinecy map. We further provide a visualization experiment to investigate whether the \u2018dense\u2019 bias property still holds for more general DEQs, particularly those with inner structures resembling ResNet or Transformer architectures. Specifically, we conduct an experiment using Grad-CAM [45] to generate the saliency map of Multiscale DEQ (MDEQ [3]), which is a variant of ResNet-DEQ and compare it with that of ResNet-50 trained for image classification on ImageNet [46]. The saliency map highlights the regions that are crucial for the model\u2019s prediction, which can be regarded as the features. It is shown in Figure 6 that MDEQ allocates attention to more features such as the fences and trees in the background, indicating that MDEQ may generate dense features. The intuitive experiment suggests that our bias results may be applicable to a wider array of DEQs with general architectures. ", "page_idx": 27}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/06b4e1c6e4f822d7cad2fdc4989297466e17440846979a2897a6f06013caf6fa.jpg", "img_caption": ["(a) Raw image of horses. (b) MDEQ\u2019s saliency map. (c) ResNet\u2019s saliency map. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/250e47b3bab0c03e0b23db9253a90662281319142849838403e75c5322eb3ab4.jpg", "img_caption": ["Figure 6: The saliency map of Multiscale DEQ (MDEQ) and ResNet. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "UJ9k3j93MD/tmp/27da4fac7a03c59360a5a1f20cd286c780009812c7631a2947d3a892ac580fbc.jpg", "img_caption": ["Figure 7: The reconstruction results with DEQ and FNN and the error computed by subtracting the original signal. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Experiment on audio representation. Inspired by the overall studies, we conduct an experiment on a real task of audio representation to verify the potential advantage of DEQ in learning functions with high-frequency component. We utilize the setting of experiments in [47], where the very-highfrequency audio signals were represented using a conventional explicit network and an (implicit)2 network, which is a variant of DEQ employing a neural block with three layers and specific activation functions such as $\\sin(x)$ Although Huang et al. [47] show that (implicit)2network outperforms conventional explicit networks in audio representation [47], revealing the advantage of DEQ to an extend, it is unclear whether the superiority of the (implicit)2network is attributed solely to the carefully-designed block. In contrast, we apply DEQ and FNN in their basic forms to represent the audio signal in our experiment to further explore the potential advantages of DEQ in real scenarios. ", "page_idx": 28}, {"type": "text", "text": "For our audio representation task, we aim to train a function that can reconstruct an given audio signal over a period of time. The spectrum of an audio signal contains many high-frequency components, making it difficult to reconstruct the audio signal for a not-so-large FNN. ", "page_idx": 28}, {"type": "text", "text": "Following the setting in [47], we train the models to fit a 7-second music piece. We set the width of DEQ to 20, the layer of FNN to 3 and the hidden dimension of FNN to 20. This setting enables the model to exactly fit the audio signal based on our experiments. ", "page_idx": 28}, {"type": "text", "text": "In Figure 7, we show the reconstruction results with DEQ and FNN and the error computed by subtracting the original signal. We observe that DEQ outperforms FNN with a noticeable error, verifying the advantages of DEQ in representing high-frequency components. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Abstract and Introduction, where enumerates the contributions. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: See Related Works and Section 5. We mention that we study a simplified DEQ due to technical issues. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: For assumptions, see Section 5. For proofs, see Appendix A Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See Section 6. We provide details of our experiment. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We will provide the open access to the code after the paper publication. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Section 6 and Appendix B. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not include experiments on large real datasets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix B.1. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper mainly focus on the theory of neural networks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 33}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper focuses on theory. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: See Section 6 and Appendix B. Although experiments in Appendix B.2 utilize another experiment, the code, data and models are created by ourselves. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]