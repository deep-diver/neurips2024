[{"figure_path": "bzGAELYOyL/figures/figures_1_1.jpg", "caption": "Figure 1: Architectures of different RGB-E tracking frameworks. (a) RGB-E tracker based on appearance information. (b) RGB tracker based on scene information propagation. (c) Our proposed CSAM framework.", "description": "This figure compares three different RGB-E tracking approaches. (a) shows a typical approach that focuses on appearance information from RGB and event data, resulting in challenges with associating targets and distractors over time. (b) illustrates another method that propagates scene information across frames, but it can be sensitive to environmental interference.  (c) presents the authors' proposed CSAM framework, which uses both RGB and event data along with a Multi-Object Tracking (MOT) approach to robustly track both targets and distractors.", "section": "1 Introduction"}, {"figure_path": "bzGAELYOyL/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of the proposed RGB-E tracking pipeline.", "description": "The figure shows a detailed overview of the RGB-E tracking pipeline proposed in the paper.  It illustrates the flow of data through various modules, starting from the input RGB and event data.  The appearance model generates initial candidates which are then processed by the candidate encoding module to create appearance and motion embeddings.  These embeddings are fed into a spatial-temporal transformer encoder to model spatial-temporal relationships. A dual-branch transformer decoder then matches candidates with historical tracklets based on appearance and motion information. Finally, the inference step determines the tracking result. The diagram clearly shows the connections between each module and how features are extracted and used in each stage.", "section": "3 Method"}, {"figure_path": "bzGAELYOyL/figures/figures_4_1.jpg", "caption": "Figure 3: Architectures of the proposed Spatial-Temporal Transformer Encoder.", "description": "This figure shows the architecture of the Spatial-Temporal Transformer Encoder (STTE) module.  The STTE processes both appearance and motion embeddings to model spatial and temporal relationships between candidates across multiple frames. It consists of three main components:\n\n1. **Spatial Encoder:** This part independently processes each frame's appearance and motion embeddings to construct spatial relationships between candidates using a Graph Multi-Head Attention mechanism.\n2. **Re-arrangement:** This stage rearranges the spatially encoded features into tracklets across frames to prepare for the temporal processing.\n3. **Temporal Encoder:** This part utilizes a Multi-Head Attention mechanism to model temporal relationships within each tracklet, using the previously re-arranged features. The output of the STTE provides enriched feature representations capturing both spatial and temporal context for each candidate.", "section": "3.3 Spatial-Temporal Transformer Encoder"}, {"figure_path": "bzGAELYOyL/figures/figures_4_2.jpg", "caption": "Figure 3: Architectures of the proposed Spatial-Temporal Transformer Encoder.", "description": "This figure illustrates the architecture of the Spatial-Temporal Transformer Encoder (STTE), a key component of the CSAM RGB-E tracking framework.  The STTE takes as input T sets of appearance embeddings and T sets of motion representations.  It first processes these embeddings through a Spatial Encoder to establish spatial relationships between candidates within each frame. Then, it re-arranges these spatially encoded features to construct N tracklets across T frames and uses a Temporal Encoder to model temporal relationships between these tracklets.  The output is a set of discriminative feature representations for each tracklet.", "section": "3.3 Spatial-Temporal Transformer Encoder"}, {"figure_path": "bzGAELYOyL/figures/figures_5_1.jpg", "caption": "Figure 5: Architectures of the proposed Dual-branch Transformer Decoder.", "description": "The Dual-branch Transformer Decoder (DBTD) consists of two branches: a Spatial-Temporal Decoder and a Motion Decoder. The Spatial-Temporal Decoder takes the spatial-encoded features of the current frame and the encoded features from the previous frame to generate attention weights, which are then used to generate the output tensor. The Motion Decoder takes the motion information from the previous frame to generate another assignment matrix. Finally, the two assignment matrices are added together to obtain the final assignment matrix.", "section": "3.4 Dual-branch Transformer Decoder"}, {"figure_path": "bzGAELYOyL/figures/figures_9_1.jpg", "caption": "Figure 10: Visualization of tracking results on COESOT dataset. Event images are used for visual comparison only.", "description": "This figure shows a visual comparison of the proposed CSAM-B method against four other state-of-the-art RGB-E trackers (OSTrack, HRCEUTrack, KeepTrack, and CEUTrack) on the COESOT dataset. The top row displays the RGB frames, while the bottom row visualizes the event data with tracking results overlaid.  The figure highlights how CSAM-B handles challenging scenarios with distractors and occlusions more effectively than the other methods.", "section": "4 Experiment"}, {"figure_path": "bzGAELYOyL/figures/figures_13_1.jpg", "caption": "Figure 7: Architectures of the appearance model.", "description": "This figure illustrates the architecture of the appearance model used in the paper.  The model processes both RGB images and event data. RGB images are processed using a standard CNN approach, while event data is converted into voxel representations. These two modalities are then processed jointly through a projection layer and a transformer-based backbone network. The output of the backbone network is fed into a head network to produce the final results.", "section": "3.1 Appearance Model"}, {"figure_path": "bzGAELYOyL/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of the proposed RGB-E tracking pipeline.", "description": "This figure shows a detailed overview of the proposed RGB-E tracking pipeline, called CSAM. It illustrates the different modules involved, including the appearance model, candidate encoding module (CEM), spatial-temporal transformer encoder (STTE), dual-branch transformer decoder (DBTD), and the final inference stage. The figure depicts the flow of information from the input RGB and event data through each module, highlighting the process of candidate generation, feature encoding, spatial-temporal relationship modeling, and candidate matching. It visually demonstrates how the CSAM framework integrates motion and appearance information to robustly track targets and distractors.", "section": "3 Method"}, {"figure_path": "bzGAELYOyL/figures/figures_17_1.jpg", "caption": "Figure 9: Candidate feature clustered by t-SNE.", "description": "This figure visualizes the effectiveness of the Spatial-Temporal Transformer Encoder (STTE) in distinguishing between target and distractors.  The left panels (a,b,c) show the input data, encoded features without STTE, and encoded features with STTE, respectively.  The t-SNE plot shows how the STTE module improves the separability of target and distractor features, making it easier to distinguish between them during the candidate matching process. The right panel (d,e) displays match results without STTE and with STTE, indicating improved accuracy when STTE is used.", "section": "C.2 Visualization of Candidate Features"}, {"figure_path": "bzGAELYOyL/figures/figures_17_2.jpg", "caption": "Figure 10: Visualization of tracking results on COESOT dataset. Event images are used for visual comparison only.", "description": "This figure visualizes the tracking results of CSAM-B and four other state-of-the-art trackers (OSTrack, HRCEUTrack, KeepTrack, and CEUTrack) on two sequences from the COESOT dataset.  It showcases how CSAM-B handles challenging scenarios with distractors and complex backgrounds more effectively than other trackers. The top row shows the RGB image and the bottom row shows the event data (a representation of the motion information).\n", "section": "C Experiments"}]