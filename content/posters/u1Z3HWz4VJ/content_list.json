[{"type": "text", "text": "RAMP: Boosting Adversarial Robustness Against Multiple $l_{p}$ Perturbations for Universal Robustness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Enyi Jiang Gagandeep Singh Department of Computer Science Department of Computer Science University of Illinois Urbana-Champaign University of Illinois Urbana-Champaign Urbana, IL 61801 Urbana, IL 61801 enyij2@illinois.edu ggnds@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most existing works focus on improving robustness against adversarial attacks bounded by a single $l_{p}$ norm using adversarial training (AT). However, these AT models\u2019 multiple-norm robustness (union accuracy) is still low, which is crucial since in the real-world an adversary is not necessarily bounded by a single norm. The tradeoffs among robustness against multiple $l_{p}$ perturbations and accuracy/robustness make obtaining good union and clean accuracy challenging. We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts. We connect natural training (NT) with AT via gradient projection, to incorporate useful information from NT into AT, where we empirically and theoretically show it moderates the accuracy/robustness tradeoff. We propose a novel training framework RAMP, to boost the robustness against multiple $l_{p}$ perturbations. RAMP can be easily adapted for robust fine-tuning and full AT. For robust fine-tuning, RAMP obtains a union accuracy up to $53.3\\bar{\\%}$ on CIFAR-10, and $29.1\\%$ on ImageNet. For training from scratch, RAMP achieves a union accuracy of $44.6\\%$ and good clean accuracy of $81.2\\%$ on ResNet-18 against AutoAttack on CIFAR-10. Beyond multi-norm robustness RAMP-trained models achieve superior universal robustness, effectively generalizing against a range of unseen adversaries and natural corruptions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Though deep neural networks (DNNs) demonstrate superior performance in various vision applications, they are vulnerable against adversarial examples [Goodfellow et al., 2014, Kurakin et al., 2018]. Adversarial training (AT) [Tram\u00e8r et al., 2017, Madry et al., 2017] which works by injecting adversarial examples into training for enhanced robustness, is currently the most popular defense. However, most AT methods address only a single type of perturbation [Wang et al., 2020, Wu et al., 2020, Carmon et al., 2019, Gowal et al., 2020, Raghunathan et al., 2020, Zhang et al., 2021, Debenedetti and Troncoso\u2014EPFL, 2022, Peng et al., 2023, Wang et al., 2023]. An $l_{\\infty}$ robust model may not be robust against $l_{p}(p\\neq\\infty)$ attacks. Also, enhancing robustness against one perturbation type can sometimes increase vulnerability to others [Engstrom et al., 2017, Schott et al., 2018]. On the contrary, training a model to be robust against multiple $l_{p}$ perturbations is crucial as it reflects real-world scenarios [Sharif et al., 2016, Eykholt et al., 2018, Song et al., 2018, Athalye et al., 2018] where adversaries can use multiple $l_{p}$ perturbations. We show that multi-norm robustness is the key to improving generalization against other threat models [Croce and Hein, 2022]. For instance, we show it enables robustness against perturbations not easily defined mathematically, such as image corruptions and unseen adversaries [Wong and Kolter, 2020]. ", "page_idx": 0}, {"type": "text", "text": "Two main challenges exist for training models robust against multiple perturbations: (i) tradeoff among robustness against different perturbation models [Tramer and Boneh, 2019] and (ii) tradeoff between accuracy and robustness [Zhang et al., 2019, Raghunathan et al., 2020]. Adversarial examples induce a shift from the original distribution, causing a drop in clean accuracy with AT [Xie et al., 2020, Benz et al., 2021]. The distinct distributions created by $l_{1},l_{2},l_{\\infty}$ adversarial examples make the problem even more challenging. Through a finer analysis of the distribution shifts caused by these adversaries, we propose the RAMP framework to efficiently boost the Robustness Against Multiple Perturbations. RAMP can be used for both fine-tuning and training from scratch. It utilizes a novel logit pairing loss on a certain pair and connects NT with AT via gradient projection [Jiang et al., 2023] to improve union accuracy while maintaining good clean accuracy and training efficiency. ", "page_idx": 1}, {"type": "text", "text": "Logit pairing loss. We visualize the changing of $l_{1},l_{2},l_{\\infty}$ robustness when fine-tuning a $l_{\\infty}$ -AT pre-trained model in Figure 1 using the CIFAR-10 training dataset. The DNN loses substantial robustness against $l_{\\infty}$ attack after only 1 epoch of fine-tuning: $l_{1}$ fine-tuning and E-AT [Croce and Hein, 2022] (red and yellow histograms under Linf category) both lose significant $l_{\\infty}$ robustness (compared with blue histogram under Linf category). Inspired by this observation, we devise a new logit pairing loss for a $l_{q}-l_{r}$ tradeoff pair to attain better union accuracy, which enforces the logit distributions of $l_{q}$ and $l_{r}$ adversarial examples to be close, specifically on the correctly classified $l_{q}$ subsets. In comparison, our method (green histogram under Linf and union categories) preserves more $l_{\\infty}$ and union robustness than others after 1 epoch. We show this technique works on larger models and datasets (Section 5.1). ", "page_idx": 1}, {"type": "text", "text": "Connect natural training (NT) with AT. We explore the connections between NT and AT to obtain a better accuracy/robustness tradeoff. We find that NT can help with adversarial robustness: useful information in natural distribution can be extracted and leveraged to achieve better robustness. To this end, we compare the similarities of model updates of NT and AT layer-wise for each epoch, where we find and incorporate useful NT components into AT via gradient projection (GP), as outlined in Algorithm 2. In Figure 2 and Section 5.1, we empirically and theoretically show this technique strikes a better balance between accuracy and robustness, for both single and multiple $l_{p}$ perturbations. We provide a theoretical analysis of why GP works for adversarial robustness in Theorem A.2 & 4.5. ", "page_idx": 1}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/78a0e220b4252eb62a0b91577bdcdabe6974b38418fe0bbcc738a6aa10216612.jpg", "img_caption": ["Figure 1: Multiple-norm tradeoff with robust finetuning: We observe that fine-tuning on $l_{\\infty}$ -AT model using $l_{1}$ examples drastically reduces $l_{\\infty}$ robustness. RAMP preserves more $l_{\\infty}$ and union robustness. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Main contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We design a new logit pairing loss to mitigate the ${\\mathrm{\\Delta}l}_{q}\\mathrm{\\Delta}-{\\mathrm{\\Delta}l}_{r}$ tradeoff for better union accuracy, by enforcing the logit distributions of $l_{q}$ and $l_{r}$ adversarial examples to be close. \u2022 We empirically and theoretically show that connecting NT with AT via gradient projection better balances the accuracy/robustness tradeoff for $l_{p}$ perturbations, compared with standard AT. \u2022 RAMP achieves good union accuracy, accuracy-robustness tradeoff, and generalizes better to diverse perturbations and corruptions (Section 5.1) achieving superior universal robustness $(75.5\\%$ for common corruption and $26.\\bar{1\\%}$ union accuracy against unseen adversaries). RAMP fine-tuned DNNs achieve union accuracy up to $53.3\\%$ on CIFAR-10, and $29.1\\%$ on ImageNet. RAMP achieves a $44.6\\%$ union accuracy and good clean accuracy on ResNet-18 against AutoAttack on CIFAR-10. Our code is available at https://github.com/uiuc-focal-lab/RAMP. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Adversarial training (AT). Adversarial Training (AT) usually employs gradient descent to discover adversarial examples, incorporating them into training for enhanced adversarial robustness [Tram\u00e8r et al., 2017, Madry et al., 2017]. Numerous works focus on improving robustness by exploring the trade-off between robustness and accuracy [Zhang et al., 2019, Wang et al., 2020], instance reweighting [Zhang et al., 2021], loss landscapes [Wu et al., 2020], wider/larger architectures [Gowal et al., 2020, Debenedetti and Troncoso\u2014EPFL, 2022], data augmentation [Carmon et al., 2019, Raghunathan et al., 2020], and using synthetic data [Peng et al., 2023, Wang et al., 2023]. However, these methods often yield DNNs robust against a single perturbation type while remaining vulnerable to other types. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Robustness against multiple perturbations. Tramer and Boneh [2019], Kang et al. [2019] observe that robustness against $l_{p}$ attacks does not necessarily transfer to other $l_{q}$ attacks $(q\\neq p)$ . Previous studies [Tramer and Boneh, 2019, Maini et al., 2020, Madaan et al., 2021, Croce and Hein, 2022] modified Adversarial Training (AT) to enhance robustness against multiple $l_{p}$ attacks, employing average-case [Tramer and Boneh, 2019], worst-case [Tramer and Boneh, 2019, Maini et al., 2020], and random-sampled [Madaan et al., 2021, Croce and Hein, 2022] defenses. There are also works [Nandy et al., 2020, Liu et al., 2020, Xu et al., 2021, Xiao et al., 2022, Maini et al., 2022] using preprocessing, ensemble methods, mixture of experts, and stability analysis to solve this problem. Ensemble models and preprocessing methods are weakened since their performance heavily relies on correctly classifying or detecting various types of adversarial examples. In certified training, Banerjee et al. [2024], Banerjee and Singh [2024] propose verification/certifiable training methods under different threat models for $l_{p}$ universal adversarial perturbation. However, prior works are hard to scale to larger models and datasets, e.g. ImageNet, due to the efficiency issue. Furthermore, Croce and Hein [2022] devise Extreme norm Adversarial Training (E-AT) and fine-tune a $l_{p}$ robust model on another $l_{q}$ perturbation to quickly make a DNN robust against multiple $l_{p}$ attacks. However, E-AT does not adapt to varying epsilon values. Our work demonstrates that the suboptimal tradeoff observed in prior studies can be improved with our proposed framework. ", "page_idx": 2}, {"type": "text", "text": "Logit pairing in adversarial training. Adversarial logit pairing methods encourage logits for pairs of examples to be similar [Kannan et al., 2018, Engstrom et al., 2018]. People apply this technique to both clean images and their adversarial counterparts, to devise a stronger form of adversarial training. In our work, we devise a novel logit pairing loss to train a DNN originally robust against $l_{p}$ attack to become robust against another $l_{q}(q\\neq p)$ attack on the correctly predicted $l_{p}$ subsets, which helps gain better union accuracy. ", "page_idx": 2}, {"type": "text", "text": "Adversarial versus distributional robustness. Sinha et al. [2018] theoretically studies the AT problem through distributional robust optimization. Mehrabi et al. [2021] establishes a pareto-optimal tradeoff between standard and adversarial risks by perturbing the test distribution. Other works explore the connection between natural and adversarial distribution shifts [Moayeri et al., 2022, Alhamoud et al., 2023], assessing transferability and generalizability of adversarial robustness across datasets. However, little research delves into distribution shifts induced by $l_{1},l_{2},l_{\\infty}$ adversarial examples and their interplay with the robustness-accuracy tradeoff [Zhang et al., 2019, Yang et al., 2020, Rade and Moosavi-Dezfooli, 2021]. Our work, inspired by recent domain adaptation techniques [Jiang, 2023, Jiang et al., 2023], designs a logit pairing loss and utilizes model updates from NT via GP to enhance adversarial robustness. We show that GP adapts to both single and multi-norm scenarios. ", "page_idx": 2}, {"type": "text", "text": "3 AT against Multiple Perturbations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a standard classification task with samples $\\{(x_{i},y_{i})\\}_{i=0}^{N}$ from an empirical data distribution $\\widehat{\\mathcal{D}}_{n}$ ; we have input images $x\\in\\mathbb{R}^{d}$ and corresponding labels $\\boldsymbol{y}\\in\\mathbb{R}^{k}$ . Standard training aims to o btain a classifier $f$ parameterized by $\\theta$ to minimize a loss function $\\mathcal{L}:\\mathbb{R}^{k}\\times\\mathbb{R}^{k}\\rightarrow\\mathbb{R}$ on $\\widehat{\\mathcal{D}}_{n}$ . Adversarial training (AT) [Madry et al., 2017, Tram\u00e8r et al., 2017] aims to find a DNN robust against adversarial examples. It is framed as a min-max problem where a DNN is optimized using the worst-case examples within an adversarial region around each $x_{i}$ . Different types of adversarial regions $B_{p}(x,\\epsilon_{p})\\,\\doteq\\,\\{x^{\\prime}\\in\\mathbb{R}^{d}:\\|x^{\\prime}-x\\|_{p}\\,\\le\\,\\breve{\\epsilon}_{p}\\}$ can be defined around a given image $x$ using various $l_{p}$ -based perturbations. Formally, we can write the optimization problem of AT against a certain $l_{p}$ attack as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{(x,y)\\sim\\widehat{\\mathcal{D}}_{n}}\\left[\\operatorname*{max}_{x^{\\prime}\\in B_{p}(x,\\epsilon_{p})}\\mathcal{L}(f(x^{\\prime}),y)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The above optimization is only for certain $p$ values and is usually vulnerable to other perturbation types. To this end, prior works have proposed several approaches to train the network robust against multiple perturbations $(l_{1},l_{2},l_{\\infty})$ at the same time. We focus on the union threat model $\\Delta=B_{1}(x,\\epsilon_{1})\\cup B_{2}(x,\\epsilon_{2})\\cup B_{\\infty}(x,\\epsilon_{\\infty})$ which requires the DNN to be robust within the $l_{1},l_{2},l_{\\infty}$ adversarial regions simultaneously [Croce and Hein, 2022]. Union accuracy is then defined as the robustness against $\\Delta_{(i)}$ for each $x_{i}$ sampled from $\\mathcal{D}$ . In this paper, similar to the prior works, we use union accuracy as the main metric to evaluate the multiple-norm robustness. Apart from that, we define universal robustness as the generalization ability against a range of unseen adversaries and common corruptions. Specifically, we have average accuracy across five severity levels for common corruption and union accuracy against a range of unseen adversaries used in Laidlaw et al. [2020]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Worst-case defense follows the following min-max optimization problem to train DNNs using the worst-case example from the $l_{1},l_{2},l_{\\infty}$ adversarial regions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{(x,y)\\sim\\widehat{\\mathcal{D}}_{n}}\\left[\\operatorname*{max}_{p\\in\\{1,2,\\infty\\}}\\operatorname*{max}_{x^{\\prime}\\in B_{p}(x,\\epsilon_{p})}\\mathcal{L}(f(x^{\\prime}),y)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "MAX [Tramer and Boneh, 2019] and MSD [Maini et al., 2020] fall into this category. Finding worst-case examples yields a good union accuracy but results in a loss of clean accuracy as the distribution of generated examples is different from the clean data distribution. ", "page_idx": 3}, {"type": "text", "text": "Average-case defense train DNNs using the average of the $l_{1},l_{2},l_{\\infty}$ worst-case examples: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{(x,y)\\sim\\widehat{\\mathcal{D}}_{n}}\\left[\\mathbb{E}_{p\\in\\{1,2,\\infty\\}}\\operatorname*{max}_{x^{\\prime}\\in B_{p}(x,\\epsilon_{p})}\\mathcal{L}(f(x^{\\prime}),y)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "AVG [Tramer and Boneh, 2019] is of this type. This method generally leads to good clean accuracy but suboptimal union accuracy as it does not penalize worst-case behavior within the $l_{1},l_{2},l_{\\infty}$ regions. ", "page_idx": 3}, {"type": "text", "text": "Random-sampled defense. The defenses mentioned above lead to a high training cost as they compute multiple attacks for each sample. SAT [Madaan et al., 2021] and E-AT [Croce and Hein, 2022] randomly sample one attack out of each type at a time, contributing to a similar computational cost as standard AT on a single perturbation model. They achieve a slightly better union accuracy compared with AVG and relatively good clean accuracy. However, they are not better than worst-case defenses for multiple-norm robustness, since they do not consider the strongest attack within the union region all the time. ", "page_idx": 3}, {"type": "text", "text": "4 RAMP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There are two main tradeoffs in achieving better union accuracy while maintaining good accuracy: 1. Among perturbations: there is a tradeoff among different attacks, e.g., a $l_{\\infty}$ pre-trained AT DNN is not robust against $l_{1},l_{2}$ perturbations, which makes the union accuracy harder to attain. Also, we observe there exists a main tradeoff pair of two attacks among the union over $\\mathbf{\\Delta}_{1},\\,l_{2},\\,l_{\\infty}$ attacks. 2. Accuracy and robustness: all defenses lead to degraded clean accuracy. To address these tradeoffs, we study the problem from the lens of distribution shifts. ", "page_idx": 3}, {"type": "text", "text": "Interpreting tradeoffs from the lens of distribution shifts. The adversarial examples with respect to an empirical data distribution $\\widehat{\\mathcal{D}}_{n}$ , adversarial region $B_{p}(x,\\epsilon_{p})$ , and DNN $f_{\\theta}$ generate a new adversarial distribution $\\widehat{\\mathcal{D}}_{a}$ with samples $\\{(x_{i}^{\\prime},y_{i})\\}_{i=0}^{N}$ , that are correlated by adding certain perturbations but different from the original $\\widehat{\\mathcal{D}}_{n}$ . Because of the shifts between $\\widehat{\\mathcal{D}}_{n}$ and $\\widehat{\\mathcal{D}}_{a}$ , DNN decreases performance on $\\widehat{\\mathcal{D}}_{n}$ when we move away from it and towards $\\widehat{\\mathcal{D}}_{a}$ . Also, the distinct distributions created by multi ple perturbations, $\\widehat{D}_{a}^{l_{1}},\\widehat{D}_{a}^{l_{2}},\\widehat{D}_{a}^{l_{\\infty}}$ , contribute to t he tradeoff among $l_{1},l_{2},l_{\\infty}$ attacks. To address the tradeoff among pe rturb ation s while maintaining good efficiency, we focus on the distributional interconnections between $\\widehat{\\mathcal{D}}_{n}$ and $\\widehat{D}_{a}^{l_{1}},\\,\\widehat{D}_{a}^{l_{2}},\\,\\widehat{D}_{a}^{l_{\\infty}}.$ . From the insights we get from above, we propose our framework RAMP, w hich in clude s (i)  logit pairing to improve tradeoffs among multiple perturbations, and (ii) identifying and combining the useful DNN components using the model updates from NT and AT, to obtain a better robustness/accuracy tradeoff. ", "page_idx": 3}, {"type": "text", "text": "Identify the Key Tradeoff Pair. We study the common case with $l_{p}$ norms $\\epsilon_{1}=12,\\epsilon_{2}=0.5,\\epsilon_{\\infty}=$ $\\frac{8}{255}$ on CIFAR-10 [Tramer and Boneh, 2019]. The distributions generated by the two strongest attacks show the largest shifts from $\\widehat{\\mathcal{D}}_{n}$ ; also, they have the largest distribution shifts between each other because of larger and most distinct search areas. Thus, by comparing the single norm robustness of $l_{p}$ adversarially trained models, we select the two $l_{p}$ -AT models with the lowest $l_{p}$ robustness against themselves as the key tradeoff pair. They refer to the strongest attack since their $l_{p}$ robustness is low. The attack with the highest $l_{p}$ robustness is mostly included by the convex hull of the other two stronger attacks [Croce and Hein, 2022]. Here we identify $l_{\\infty}-l_{1}$ as the key tradeoff pair. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.1 Logit Pairing for Multiple Perturbations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Figure 1: Finetuning a $l_{q}$ -AT model on $l_{r}$ examples reduces $l_{q}$ robustness. To get a finer analysis of the $l_{\\infty}-l_{1}$ tradeoff mentioned above, we visualize the changing of $l_{1},l_{2},l_{\\infty}$ robustness of the training dataset when we fine-tune a $l_{\\infty}$ pre-trained model with $l_{1}$ examples for 1 epochs, as shown in Figure 1: $\\mathbf{X}$ -axis represents the robustness against different attacks and y-axis is the accuracy. After 1 epoch of finetuning on $l_{1}$ examples or performing E-AT, we lose much $l_{\\infty}$ robustness since blue/yellow histograms are much lower than the red histogram under the Linf category. RAMP preserves both $l_{\\infty}$ and union robustness more effectively: the green histogram is higher than the red/yellow histogram under Linf and Union categories. Specifically, RAMP maintains $14\\%$ , $28\\%$ more union robustness than E-AT and $l_{1}$ fine-tuning. The above observations indicate the necessity of preserving more $l_{q}$ robustness as we adversarially fine-tune with $l_{r}$ adversarial examples on a $l_{q}$ pre-trained AT model, with $l_{q}-l_{r}$ as the key tradeoff pair, which inspires us to design our loss design with logit pairing. We want to enforce the union predictions between $l_{q}$ and $l_{r}(q\\neq r)$ attacks: bringing the predictions of $l_{q}$ and $l_{r}(q\\neq r)$ close to each other, specifically on the correctly predicted $l_{q}$ subsets. Based on our observations, we design a new logit pairing loss to enforce a DNN robust against one $l_{q}$ attack to be robust against another $l_{r}(q\\neq r)$ attack. ", "page_idx": 4}, {"type": "text", "text": "Enforcing the Union Prediction via Logit Pairing. The $l_{q}-l_{r}(q\\neq r)$ tradeoff leads us to the following principle to improve union accuracy: for a given set of images, when we have a DNN robust against some $l_{q}$ examples, we want it to be robust against $l_{r}$ examples as well. This serves as the main insight for our loss design: we want to enforce the logits predicted by $l_{q}$ and $l_{r}$ adversarial examples to be close, specifically on the correctly predicted $l_{q}$ subsets. To accomplish this, we design a KL-divergence (KL) loss between the predictions from $l_{q}$ and $l_{r}$ perturbations. For each batch of data $(x,y)\\sim\\mathcal{D}$ , we generate $l_{q}$ and $l_{r}$ adversarial examples $x_{g}^{\\prime},x_{r}^{\\prime}$ and their predictions $p_{q},p_{r}$ using APGD [Croce and Hein, 2020]. Then, we select indices $\\gamma$ , which part elements of $p_{q}$ correctly predicts the ground truth $y$ . We denote the size of the indices as $n_{c}$ , and the batch size as $N$ . We compute a KL-divergence loss over this set of samples using $K L(p_{q}[\\gamma]||p_{r}[\\gamma])$ (Eq. 1). For the subset indexed by $\\gamma$ , we want to push its $l_{r}$ logit distribution towards its $l_{q}$ logit distribution, such that we prevent losing more $l_{q}$ robustness when training with $l_{r}$ adversarial examples. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{K L}=\\frac{1}{n_{c}}\\cdot\\sum_{i=1}^{n_{c}}\\sum_{j=0}^{k}p_{q}[\\gamma[i]][j]\\cdot\\log\\left(\\frac{p_{q}[\\gamma[i]][j]}{p_{r}[\\gamma[i]][j]}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To further boost the union accuracy, apart from the KL loss, we add another loss term using a MAX-style approach in Eq. 2: we find the worst-case example between $l_{q}$ and $l_{r}$ adversarial regions by selecting the example with the higher loss. $\\mathcal{L}_{m a x}$ is a cross-entropy loss over the approximated worst-case adversarial examples. Here, we use $\\mathcal{L}_{c e}$ to represent the cross-entropy loss. Our final loss $\\mathcal{L}$ combines $\\mathcal{L}_{K L}$ and $\\mathcal{L}_{m a x}$ , via a hyper-parameter $\\lambda$ in Eq. 3. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m a x}=\\frac{1}{N}\\sum_{i=0}^{N}\\left[\\operatorname*{max}_{p\\in\\{q,r\\}}\\operatorname*{max}_{x_{i}^{\\prime}\\in B_{p}(x,\\epsilon_{p})}\\mathcal{L}_{c e}(f(x_{i}^{\\prime}),y_{i})\\right]\\quad(2)\\quad\\quad\\quad\\mathcal{L}=\\mathcal{L}_{m a x}+\\lambda\\cdot\\mathcal{L}_{K L}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 shows the pseudocode of robust fine-tuning with RAMP that leverages logit pairing. ", "page_idx": 4}, {"type": "text", "text": "4.2 Connecting Natural Training with AT ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To improve the robustness and accuracy tradeoff against multiple perturbations, we explore the connections between AT and NT. Since extracting valuable information in NT aids in improving robustness (Section 4.2), we use gradient projection [Jiang et al., 2023] to compare and integrate natural and adversarial model updates, which yields an improved tradeoff between robustness and accuracy. ", "page_idx": 4}, {"type": "text", "text": "NT can help adversarial robustness. Let us consider two models $f_{1}$ and $f_{2}$ , where $f_{1}$ is randomly initialized and $f_{2}$ undergoes NT on $\\widehat{\\mathcal{D}}_{n}$ for $k$ epochs: $f_{2}$ results in a better decision boundary and higher clean accuracy. Performing AT on $f_{1}$ and $f_{2}$ subsequently, intuitively, $f_{2}$ becomes more robust than $f_{1}$ due to its improved decision boundary, leading to fewer misclassifications of adversarial examples. This effect is empirically shown in Figure 2. For AT (blue), standard AT against $l_{\\infty}$ attack [Madry et al., 2017] is performed, while for AT-pre (red), 50 epochs of pre-training precede the standard AT procedure. AT-pre shows superior clean and robust accuracy on CIFAR-10 against $l_{\\infty}$ PGD-20 attack with $\\epsilon_{\\infty}=0.031$ . Despite $\\widehat{\\mathcal{D}}_{n}$ and $\\widehat{\\mathcal{D}}_{a}$ are different, Figure 2 suggests valuable information in $\\widehat{\\mathcal{D}}_{n}$ that potentially enhances performance on $\\widehat{\\mathcal{D}}_{a}$ . ", "page_idx": 5}, {"type": "text", "text": "AT with Gradient Projection. To connect NT with AT more effectively, we analyze the training procedures on $\\widehat{\\mathcal{D}}_{n}$ and $\\widehat{\\mathcal{D}}_{a}$ . We consider model updates over all samples from $\\widehat{\\mathcal{D}}_{n}$ and $\\widehat{\\mathcal{D}}_{a}$ , with the initial mod el $f^{(r)}$ at  epoch $r$ , and models $f_{n}^{(r)}$ and $f_{a}^{(r)}$ after 1 epoch of natur al and ad versarial training from the same starting point $f^{(r)}$ , respectively. Here, we compare the natural updates $\\widehat{g}_{n}=\\bar{f}_{n}^{(r)}-f^{(r)}$ and adversarial updates $\\widehat{g}_{a}=\\bar{f}_{a}^{(r)}-\\dot{f}^{(r)}$ . Due to distribution shift, an angle exists between them. Our goal is to identify useful components from $g_{n}$ and incorporate them into $g_{a}$ for increased robustness in $\\widehat{\\mathcal{D}}_{a}$ while maintaining accuracy in $\\widehat{\\mathcal{D}}_{n}$ . Inspired by Jiang et al. [2023], we layer-wisely compute the cosine similarity between ${\\widehat{g}}_{n}$ and $\\widehat{g}_{a}$ . For a specific layer $l$ of ${\\widehat{g}}_{n}^{l}$ and $\\widehat{g}_{a}^{l}$ , we preserve a portion of ${\\widehat{g}}_{n}^{l}$ based on their cosine similarity score (Eq.4). Negative scores indicate that ${\\widehat{g}}_{n}^{l}$ is not beneficial for robustness in $\\widehat{\\mathcal{D}}_{a}$ . Therefore, we filter components with similarity score $\\leq0$ . We define the GP (Gradient Projection) operation in Eq.5 by projecting $\\widehat{g}_{a}^{l}$ towards ${\\widehat{g}}_{n}^{l}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cos(\\widehat{g}_{n}^{l},\\widehat{g}_{a}^{l})=\\frac{\\widehat{g}_{n}^{l}\\cdot\\widehat{g}_{a}^{l}}{\\|\\widehat{g}_{n}^{l}\\|\\|\\widehat{g}_{a}^{l}\\|}\\quad\\mathrm{~(4)~}\\quad\\qquad\\mathbf{GP}(\\widehat{g}_{n}^{l},\\widehat{g}_{a}^{l})=\\left\\{\\begin{array}{l l}{\\cos(\\widehat{g}_{n}^{l},\\widehat{g}_{a}^{l})\\cdot\\widehat{g}_{n}^{l},}&{\\cos(\\widehat{g}_{n}^{l},\\widehat{g}_{a}^{l})>0}\\\\ {0,}&{\\cos(\\widehat{g}_{n}^{l},\\widehat{g}_{a}^{l})\\leq0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Therefore, the total projected (useful) model updates $g_{p}$ coming from ${\\widehat{g}}_{n}$ could be computed as Eq. 6. We use $\\mathcal{M}$ to denote all layers of the current model update. Note that $\\cup_{l\\in\\mathcal{M}}$ concatenates all layers\u2019 useful natural model update components. A hyper-parameter $\\beta$ is use d to balance the contributions of $g_{G P}$ and $\\widehat{g}_{a}$ , as shown in Eq. 7. By finding a proper $\\beta$ (0.5 as in Figure $4c$ ), we can obtain better robustness on $\\widehat{\\mathcal{D}}_{a}$ , as shown in Figure 2 and Figure 3. In Figure 2, with $\\beta=0.5$ , AT-GP refers to AT with GP; for A T-GP-pre, we perform 50 epochs of NT before doing AT-GP. We see AT-GP obtains a better accuracy/robustness tradeoff than AT. We observe a similar trend for AT-GP-pre vs. AT-pre. Further, in Figure 3, RN-18 $l_{\\infty}$ -GP achieves good clean accuracy and better robustness than RN-18 $l_{\\infty}$ against AutoAttack [Croce and Hein, 2020]. ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{p}=\\bigcup_{l\\in\\mathcal{M}}\\mathbf{GP}(\\widehat{g}_{n}^{l},\\widehat{g}_{a}^{l})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\nf^{(r+1)}=f^{(r)}+\\beta\\cdot g_{p}+(1-\\beta)\\cdot\\widehat g_{a}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Fine-tuning via Logit Pairing   \n1: Input: model $f$ , input samples $(x,y)$ from distribution $\\widehat{\\mathcal{D}}_{n}$ , fine-tuning rounds $R$ , hyper-paramet er $\\lambda$ , adversarial regions $B_{q},B_{r}$ with size $\\epsilon_{q}$ and $\\epsilon_{r}$ , APGD attack.   \n2: for $r=1,2,...,R$ do   \n3: for $(x,y)\\sim$ training set $\\mathcal{D}$ do   \n4: $x_{q}^{\\prime},p_{q}\\gets\\mathbf{APGD}(B_{q}(x,\\epsilon_{q}),y)$   \n5: $x_{r}^{\\prime},p_{r}\\gets\\mathbf{APGD}(B_{r}(x,\\epsilon_{r}),y)$   \n6: $\\gamma\\leftarrow w h e r e(a r g m a x\\:p_{q}=y)$   \n7: $n_{c}\\gets\\gamma.s i z e()$   \n8: calculate $\\mathcal{L}$ using Eq. 3 and update $f$   \n9: end for   \n10: end for   \n11: Output: model $f$ .   \nAlgorithm 2 Connect AT with NT via GP 1: Input: model $f$ , input images with distribution $\\widehat{\\mathcal{D}}_{n}$ , training rounds $R$ , adversarial region $B_{p}$ and its size $\\epsilon_{p},\\beta.$ , natural training NT and adversarial training AT.   \n2: for $r=1,2,...,R$ do   \n3: $f_{n}\\gets\\bar{\\mathbf{N}}\\bar{\\mathbf{T}}(f^{(r)},\\mathcal{D})$   \n4: $f_{a}\\gets\\mathbf{A}\\mathbf{T}(f^{(r)},B_{p},\\epsilon_{p},\\mathcal{D})$   \n5: compute $\\widehat{g}_{n}\\leftarrow f_{n}-f^{(r)},\\widehat{g}_{a}\\leftarrow f_{a}-$ f (r)   \n6: compute $g_{p}$ using Eq. 6 7: update $f^{(r+1)}$ using Eq. 7 with $\\beta$ and $\\widehat{g}_{a}$   \n8: end for   \n9: Output: model $f$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.3 Theoretical Analysis of GP for Adversarial Robustness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We define $\\boldsymbol{D}_{n}=\\{(x_{i},y_{i})\\}_{i=0}^{\\infty}$ as the ideal data distribution with an infinite cardinality. Here, we consider a classifier $f_{\\theta}$ at epoch $t$ . We define $\\mathcal{D}_{a}$ as the distribution created by $\\{(x_{i}\\!+\\!\\epsilon(f_{\\theta},\\dot{x}_{i},y_{i}),y_{i})\\}_{i=0}^{\\infty}$ where $(x_{i},y_{i})\\sim\\mathcal{D}_{n}$ . $x_{i}+\\epsilon(f_{\\theta},x_{i},y_{i})$ denotes the perturbed image, which could be both single and multiple perturbations based on $f_{\\theta}$ itself. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. We assume $\\widehat{\\mathcal{D}}_{n}$ consists of $N$ i.i.d. samples from the ideal distribution $\\mathcal{D}_{n}$ and $\\widehat{\\mathcal{D}}_{a}=\\{(x_{i}+\\epsilon(f^{\\theta},x_{i},y_{i}),y_{i})\\}_{i=0}^{N}$ where $(x_{i},y_{i})\\sim\\widehat{\\cal D}_{n}$ consists of $N$ i.i.d. samples from $\\mathcal{D}_{a}$ . ", "page_idx": 6}, {"type": "text", "text": "We define the population loss as $\\mathcal{L}_{\\mathcal{D}}(\\theta):=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\mathcal{L}(f(x),y)$ , and let $g_{\\mathcal{D}}(\\theta):=\\nabla\\mathcal{L}_{\\mathcal{D}}(\\theta)$ . For simplification, we use $g_{a}:=\\nabla\\mathcal{L}_{\\mathcal{D}_{a}}(\\theta)$ , $\\widehat{g}_{a}:=\\nabla\\mathcal{L}_{\\widehat{D}_{a}}(\\theta)$ , and $\\widehat{g}_{n}:=\\nabla\\mathcal{L}_{\\widehat{D}_{n}}(\\theta)$ . $g_{G P}=\\beta\\cdot g_{p}+(1-$ $\\beta)\\cdot\\widehat{g}_{a}$ (Definition A.3) is the aggregation using GP. We define the following optimization problem. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.2 (Aggregation for NT and AT). $f_{\\theta}$ is trained by iteratively updating the parameter ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta\\leftarrow\\theta-\\mu\\cdot A g g r(\\widehat{g}_{a},\\widehat{g}_{n}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu$ is the step size. We seek an aggregation rule $A g g r(\\cdot)=\\widehat{g}_{A g g r}$ such that after training, $f_{\\theta}$ minimizes the population loss function $\\mathcal{L}_{\\mathcal{D}_{a}}(\\theta)$ . ", "page_idx": 6}, {"type": "text", "text": "We need $\\widehat{g}_{\\mathtt{A g g r}}$ to be close to $g_{a}$ for each iteration, since $g_{a}$ is the optimal update on $\\mathcal{D}_{a}$ . Thus, we define $L^{\\pi}$ -Norm and delta error to indicate the performance of different aggregation rules. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.3 $L^{\\pi}$ -Norm [Enyi Jiang, 2024]). Given a distribution $\\pi$ on the parameter space $\\theta$ , we define an inner product $\\langle g_{\\mathcal{D}},g_{\\mathcal{D^{\\prime}}}\\rangle_{\\pi}=\\mathbb{E}_{\\theta\\sim\\pi}[\\langle g_{\\mathcal{D}}(\\theta),g_{\\mathcal{D^{\\prime}}}(\\theta)\\rangle]$ . The inner product induces the $L^{\\pi}$ - norm on $g_{\\mathscr D}$ as $\\|g_{\\mathcal{D}}\\|_{\\pi}:=\\sqrt{\\mathbb{E}_{\\theta\\sim\\pi}\\|g_{\\mathcal{D}}(\\theta)\\|^{2}}$ . We use $L^{\\pi}$ -norm to measure the gradient differences under certain $\\mathcal{D}$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 4.4 (Delta Error of an aggregation rule ${\\tt A g g r}(\\cdot))$ . We define the following squared error term to measure the closeness between $\\widehat{g}_{A g g r}$ and $g_{a}$ under $\\widehat{\\mathcal{D}}_{a}^{t}$ (distribution at time step $t$ ), i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta_{A g g r}^{2}:=\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g}_{A g g r}\\|_{\\pi}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Delta errors $\\Delta_{A T}^{2}$ and $\\Delta_{G P}^{2}$ measure the closesness of $g_{G P},\\widehat{g}_{a}$ from $g_{a}$ in $\\widehat{\\mathcal{D}}_{a}$ at each iteration. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5 (Error Analysis of GP). When the model dimension $m\\rightarrow\\infty$ , for an epoch $t$ , we have an approximation of the error difference $\\Delta_{A T}^{2}-\\Delta_{G P}^{2}$ as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta_{A T}^{2}-\\Delta_{G P}^{2}\\approx\\beta(2-\\beta)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g_{n}}\\|_{\\pi}^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\bar{\\tau}^{2}=\\mathbb{E}_{\\pi}[\\tau^{2}]\\in[0,1],$ , where $\\tau(\\theta)$ is the $\\sin(\\cdot)$ value of the angle between ${\\widehat{g}}_{n}$ and $g_{a}-\\widehat{g}_{n}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5 shows $\\Delta_{G P}^{2}$ is generally smaller than $\\Delta_{A T}^{2}$ for a large model dimension during each iteration, as is the case for the models in our evaluation, with $\\beta=0.5$ , since $\\beta(1-\\beta)>\\beta^{2}(\\bar{0}.75>$ 0.25) and the small value of $\\bar{\\tau}$ in practice (see Interpretation of Theorem A.2 in Appendix A, where we show the order of difference is between $1e^{-8}$ and $\\mathrm{i}e^{-12}$ ). Thus, GP achieves better robust accuracy than AT by achieving a smaller delta error; GP also obtains good clean accuracy by combining parts of the model updates from the clean distribution $\\widehat{\\mathcal{D}}_{n}$ . Further, we provide an error analysis of a single gradient step in Theorem A.1 and convergence analysis in Theorem A.2, showing that a smaller Delta error results in better convergence. The full proof of all theorems is in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "We outline the AT-GP method in Algorithm 2 and it can be extended to the multiple-norm scenario. The overhead of this algorithm comes from natural training and GP operation. Their costs are small, and we discuss this more in Section 5.2. Combining logit pairing and gradient projection methods, we provide the RAMP framework which is similar to Algorithm 2, except that we replace line 4 of Algorithm 2 as Algorithm 1 line 3-9. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets, baselines, and models. CIFAR-10 [Krizhevsky et al., 2009] includes 60K images with 50K and 10K images for training and testing respectively. ImageNet has $\\approx14.2\\mathbf{M}$ images and 1K classes, containing $\\approx1.3\\mathrm{M}$ training, 50K validation, and 100K test images [Russakovsky et al., ", "page_idx": 6}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/16de10da57678fc6b5b497af896bf1bf6ff1f73955325a5a7bb8fbfaa8a9e3c3.jpg", "img_caption": ["applied. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "2015]. We compare RAMP with following baselines: 1. SAT [Madaan et al., 2021]: randomly sample one of the $l_{1},l_{2},l_{\\infty}$ attacks. 2. AVG [Tramer and Boneh, 2019]: take the average of $l_{1},l_{2},l_{\\infty}$ examples. 3. MAX [Tramer and Boneh, 2019]: take the worst of $l_{1},l_{2},l_{\\infty}$ attacks. 4. MSD [Maini et al., 2020]: find the worst-case examples over $l_{1},l_{2},l_{\\infty}$ steepest descent directions during each step of inner maximization. 5. E-AT [Croce and Hein, 2022]: randomly sample between $l_{1}$ , $l_{\\infty}$ attacks. For models, we use PreAct-ResNet-18, ResNet-50, WideResNet-34-20, and WideResNet-70-16 for CIFAR-10, as well as ResNet-50 and XCiT-S transformer for ImageNet. ", "page_idx": 7}, {"type": "text", "text": "Implementations and Evaluation. For AT from scratch for CIFAR-10, we train PreAct ResNet18 [He et al., 2016] with a $l r=0.05$ for 70 epochs and 0.005 for 10 more epochs. We set $\\lambda=2$ , $\\beta=0.5$ for training from scratch, and $\\lambda=0.5$ for robust fine-tuning. For all methods, we use 10 steps for the inner maximization in AT. For ImageNet, we perform 1 epoch of fine-tuning and use a learning rate $l r\\,=\\,0.005$ , $\\lambda=0.5$ for ResNet-50 and $l\\bar{r}=1e^{-4}$ , $\\lambda=0.5$ for XCiT-S models. We reduce the rate by a factor of 10 every $\\textstyle{\\frac{1}{3}}$ of the training epoch and set the weight decay to $1e^{-4}$ . We use APGD with 5 steps for $l_{\\infty}$ and $l_{2}$ , 15 steps for $l_{1}$ . Settings are similar to [Croce and Hein, 2022]. We use the standard values of $\\begin{array}{r}{\\epsilon_{1}=1\\bar{2^{\\prime}},\\epsilon_{2}=0.5,\\epsilon_{\\infty}=\\bar{\\frac{8}{255}}}\\end{array}$ for CIFAR-10 and \u03f51 = 255, \u03f52 = 2, \u03f5\u221e=2455 for ImageNet. We focus on $l_{\\infty}$ -AT models for fine-tuning, as Croce and Hein [2022] shows their higher union accuracy for the $\\epsilon$ values in our evaluation. We report the clean accuracy, robust accuracy against $\\{l_{1},l_{2},l_{\\infty}\\}$ attacks, union accuracy, universal robustness against common corruptions and unseen adversaries, as well as runtime for RAMP. The robust accuracy is evaluated using Autoattack [Croce and Hein, 2020]. More implementation details are in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "5.1 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes. ", "page_idx": 7}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/2a96b4fb938adbc06d5022bc73b2cde96cf38407045fa105c7592926d95b57d5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Robust fine-tuning. In Table 2, we apply RAMP to larger models and datasets (ImageNet). However, the implementation of other baselines is not publicly available and Croce and Hein [2022] do not report other baseline results except E-AT on larger models and datasets, so we only compare against E-AT in Table 2, which shows RAMP consistently obtains better union accuracy and accuracyrobustness tradeoff than E-AT. We observe that RAMP improves the performance more as the model becomes larger. We obtain the SOTA union accuracy of $53.3\\%$ on CIFAR-10 and $29.1\\%$ on ImageNet. ", "page_idx": 7}, {"type": "text", "text": "RAMP with varying $\\epsilon_{1},\\epsilon_{2},\\epsilon_{\\infty}$ values. We provide results with 1. $\\begin{array}{r}{(\\epsilon_{1}=12,\\epsilon_{2}=0.5,\\epsilon_{\\infty}=\\frac{2}{255})}\\end{array}$ where $\\epsilon_{\\infty}$ size is small and 2. $\\begin{array}{r}{(\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;1.5,\\epsilon_{\\infty}\\;=\\;\\frac{8}{255})}\\end{array}$ where $\\epsilon_{2}$ size is large, using PreAct ResNet-18 model for CIFAR-10 dataset: these cases have different tradeoff pair compared to ", "page_idx": 7}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/b568a918babe3b24a02f36f9253e282fa5826036610352c7849cdb9cde11c95f.jpg", "table_caption": ["Table 2: Robust fine-tuning on larger models and datasets ( $^*$ uses extra data for pre-training). We evaluate all CIFAR-10 and Imagenet test points. RAMP consistently achieves better union accuracy with significant margins and good accuracy-robustness tradeoff. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1. The pair identified using our heuristic are $l_{1}-l_{2}$ and $l_{2}-l_{\\infty}$ . In Table 1, we observe that RAMP consistently outperforms E-AT and MAX with significant margins in union accuracy, when training from scratch and performing robust fine-tuning. In Table 1, when $l_{2}$ is the bottleneck, E-AT obtains a lower union accuracy as it does not leverage $l_{2}$ examples. Similar observations are made across various epsilon values, with RAMP consistently outperforming other baselines, as detailed in Appendix B.4. Appendix B includes more training details/results, and ablation studies. Results for applying the trades loss to RAMP outperforming E-AT are detailed in Appendix B.6. Appendix B.7 presents robust fine-tuning using ResNet-18, where RAMP achieves the highest union accuracy. ", "page_idx": 8}, {"type": "text", "text": "Adversarial training from random initialization. Table 3 presents the results of AT from random initialization on CIFAR-10 with PreAct ResNet-18. ", "page_idx": 8}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/cd70b0552ccf7231a26b09a44c75852fa8f53b58a469bafeee5bebe54f5f6cfd.jpg", "table_caption": ["Table 3: RN-18 model trained from random initialization on CIFAR-10 over 5 trials: RAMP achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein [2022]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "RAMP has the highest union accuracy with good clean accuracy, which indicates that RAMP can mitigate the tradeoffs among perturbations and robustness/accuracy in this setting. The results for all baselines are from Croce and Hein [2022]. ", "page_idx": 8}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/84083d071db806b35b9c4d45728c837c35deac095e7afb67016fcccec3eedca8.jpg", "table_caption": ["Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Universal Robustness. In Table 4, we report average accuracy against common corruptions and union accuracy against unseen adversaries from Laidlaw et al. [2020] (implementation details are in Appendix B.3). We compare against $l_{p}$ pretrained models, E-AT, MAX, winninghand [Diffenderfer et al., 2021] (a SOTA method for natural corruptions) using WideResNet-28-10 architecture on the CIFAR-10 dataset. Compared to E-AT and MAX, RAMP achieves $4\\%$ higher accuracy for common corruptions with five severity levels and $2{-}4\\%$ better union accuracy against multiple unseen adversaries. Winninghand has high corruption robustness but no adversarial robustness. The results show that RAMP obtains a better robustness and accuracy tradeoff with stronger universal robustness. In Appendix B.3, we evaluate on ResNet-18 to support this fact further. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Study and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Sensitivities of $\\lambda.$ We perform experiments with different $\\lambda$ values in $[0.1,0.5,1.0,1.5,2,3,4,5]$ for robust fine-tuning and [1.5, 2, 3, 4, 5, 6] for AT from scratch using PreAct-ResNet-18 model for CIFAR-10 dataset. In Figure 4, we observe a decreased clean accuracy when $\\lambda$ becomes larger. We pick $\\lambda=2.0$ for training from scratch (Figure 4a) and $\\lambda=0.5$ for robust fine-tuning (Figure 4b) in our main experiments, as these values of $\\lambda$ yield both good clean and union accuracy. ", "page_idx": 8}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/c8386c8ac0801c81c6745c07a68a50f3a7ad23c2e01173214bf6ed8bd8af0b00.jpg", "img_caption": ["Figure 4: Alabtion studies on $\\lambda$ and $\\beta$ hyper-parameters. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Choices of $\\beta$ . Figure 4c shows the performance of RAMP with varying $\\beta$ values on CIFAR-10 ResNet-18 experiments. We pick $\\beta\\,=\\,0.5$ for combining natural training and AT via GP, which achieves comparatively good robustness and clean accuracy. This choice is also based on Theorem 4.5 when $\\beta(2-\\Bar{\\beta})$ has the largest difference from $\\beta^{2}$ (0.75 vs 0.25). ", "page_idx": 9}, {"type": "text", "text": "Fine-tune $l_{p}$ AT models with RAMP. Table 5 shows the robust fine-tuning results using RAMP with $l_{\\infty}$ - AT $\\mathbf{\\Delta}q=\\infty,r=1)$ ), $l_{1}$ -AT $(q=1,r=\\infty)$ ), $l_{2}$ -AT $(q=\\infty,r=1)$ ) RN-18 models for CIFAR-10 dataset. For $l_{\\infty}-l_{1}$ tradeoffs, RAMP on $l_{\\infty}$ -AT pre-trained model achieves the best union accuracy. ", "page_idx": 9}, {"type": "text", "text": "Computational analysis and Limitations. The extra training costs of AT-GP are small, e.g. for each epoch on ResNet-18, the extra NT takes 6 seconds and the ", "page_idx": 9}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/ab5f5e3e865e3e446d68fda0a00d639786fe117941b570fa35b6b27ad9fae7b7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5: RAMP with $l_{\\infty},l_{1},l_{2}$ -RN-18-AT models on CIFAR-10 with standard epsilons. ", "page_idx": 9}, {"type": "text", "text": "standard AT takes 78 seconds using a single NVIDIA A100 GPU, and the GP operation only takes 0.04 seconds on average. RAMP is more expensive than E-AT and less expensive than MAX. We have a complete runtime analysis in Appendix B.2. We notice occasional drops in clean accuracy during fine-tuning with RAMP. In some cases, union accuracy improves slightly but clean accuracy and single $l_{p}$ robustness reduce. Further, we find no negative societal impact from this work. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce RAMP, a framework enhancing multiple-norm robustness and achieving superior universal robustness against corruptions and perturbations by addressing tradeoffs among $l_{p}$ perturbations and accuracy/robustness. We apply a new logit pairing loss and use gradient projection to obtain SOTA union accuracy with favorable accuracy/robustness tradeoffs against common corruptions and other unseen adversaries. Results demonstrate that RAMP surpasses SOTA methods in union accuracy across model architectures on CIFAR-10 and ImageNet. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by NSF Grants No. CCF-2238079, CCF-2316233, CNS-2148583. We would like to thank Jacky Yibo Zhang for the helpful discussions and advice on the proof. Also, we thank anonymous reviewers for their valuable feedback on the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, and Bernard Ghanem. Generalizability of adversarial robustness under distribution shifts. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id= XNFo3dQiCJ. Featured Certification. ", "page_idx": 9}, {"type": "text", "text": "Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In International conference on machine learning, pages 284\u2013293. PMLR, 2018.   \nDebangshu Banerjee and Gagandeep Singh. Relational dnn verification with cross executional bound refinement. arXiv preprint arXiv:2405.10143, 2024.   \nDebangshu Banerjee, Changming Xu, and Gagandeep Singh. Input-relational verification of deep neural networks. Proceedings of the ACM on Programming Languages, 8(PLDI):1\u201327, 2024.   \nPhilipp Benz, Chaoning Zhang, and In So Kweon. Batch normalization increases adversarial vulnerability and decreases adversarial transferability: A non-robust feature perspective. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7818\u20137827, 2021.   \nYair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. Advances in neural information processing systems, 32, 2019.   \nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pages 2206\u20132216. PMLR, 2020.   \nFrancesco Croce and Matthias Hein. Adversarial robustness against multiple and single $l\\_p$ -threat models via quick fine-tuning of robust classifiers. In International Conference on Machine Learning, pages 4436\u20134454. PMLR, 2022.   \nFrancesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020.   \nFrancesco Croce, Maksym Andriushchenko, Naman D Singh, Nicolas Flammarion, and Matthias Hein. Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6437\u20136445, 2022.   \nEdoardo Debenedetti and Carmela Troncoso\u2014EPFL. Adversarially robust vision transformers, 2022.   \nJames Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand: Compressing deep networks can improve out-of-distribution robustness. Advances in neural information processing systems, 34:664\u2013676, 2021.   \nLogan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling cnns with simple transformations. 2017.   \nLogan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness of adversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.   \nLogan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness (python library), 2019. URL https://github.com/MadryLab/robustness.   \nSanmi Koyejo Enyi Jiang, Yibo Jacky Zhang. Principled federated domain adaptation: Gradient projection and auto-weighting. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=6J3ehSUrMU.   \nKevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1625\u20131634, 2018.   \nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \nSven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.   \nEnyi Jiang. Federated domain adaptation for healthcare, 2023.   \nEnyi Jiang, Yibo Jacky Zhang, and Oluwasanmi Koyejo. Federated domain adaptation via gradient projection. arXiv preprint arXiv:2302.05049, 2023.   \nDaniel Kang, Yi Sun, Tom Brown, Dan Hendrycks, and Jacob Steinhardt. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019.   \nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nAlexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In Artificial intelligence safety and security, pages 99\u2013112. Chapman and Hall/CRC, 2018.   \nCassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. arXiv preprint arXiv:2006.12655, 2020.   \nAishan Liu, Shiyu Tang, Xianglong Liu, Xinyun Chen, Lei Huang, Zhuozhuo Tu, Dawn Song, and Dacheng Tao. Towards defending multiple adversarial perturbations via gated batch normalization. arXiv preprint arXiv:2012.01654, 2020.   \nDivyam Madaan, Jinwoo Shin, and Sung Ju Hwang. Learning to generate noise for multi-attack robustness. In International Conference on Machine Learning, pages 7279\u20137289. PMLR, 2021.   \nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \nPratyush Maini, Eric Wong, and Zico Kolter. Adversarial robustness against the union of multiple perturbation models. In International Conference on Machine Learning, pages 6640\u20136650. PMLR, 2020.   \nPratyush Maini, Xinyun Chen, Bo Li, and Dawn Song. Perturbation type categorization for multiple adversarial perturbation robustness. In Uncertainty in Artificial Intelligence, pages 1317\u20131327. PMLR, 2022.   \nMohammad Mehrabi, Adel Javanmard, Ryan A Rossi, Anup Rao, and Tung Mai. Fundamental tradeoffs in distributionally adversarial training. In International Conference on Machine Learning, pages 7544\u20137554. PMLR, 2021.   \nMazda Moayeri, Kiarash Banihashem, and Soheil Feizi. Explicit tradeoffs between adversarial and natural distributional robustness. Advances in Neural Information Processing Systems, 35: 38761\u201338774, 2022.   \nJay Nandy, Wynne Hsu, and Mong Li Lee. Approximate manifold defense against multiple adversarial perturbations. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2020.   \nShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, Jason Martin, and Duen Horng Chau. Robust principles: Architectural design principles for adversarially robust cnns. arXiv preprint arXiv:2308.16258, 2023.   \nRahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In International Conference on Learning Representations, 2021.   \nAditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716, 2020.   \nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \nLukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust neural network model on mnist. arXiv preprint arXiv:1805.09190, 2018.   \nMahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pages 1528\u20131540, 2016.   \nAman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id $\\fallingdotseq$ Hk6kPgZA-.   \nDawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, and Tadayoshi Kohno. Physical adversarial examples for object detectors. In $l2t h$ USENIX workshop on offensive technologies (WOOT 18), 2018.   \nFlorian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. Advances in neural information processing systems, 32, 2019.   \nFlorian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.   \nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In ICLR, 2020.   \nZekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 36246\u201336263. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr. press/v202/wang23ad.html.   \nEric Wong and J Zico Kolter. Learning perturbation sets for robust machine learning. arXiv preprint arXiv:2007.08450, 2020.   \nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33:2958\u20132969, 2020.   \nJiancong Xiao, Zeyu Qin, Yanbo Fan, Baoyuan Wu, Jue Wang, and Zhi-Quan Luo. Adaptive smoothness-weighted adversarial training for multiple perturbations with its stability analysis. arXiv preprint arXiv:2210.00557, 2022.   \nCihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examples improve image recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 819\u2013828, 2020.   \nKaidi Xu, Chenan Wang, Hao Cheng, Bhavya Kailkhura, Xue Lin, and Ryan Goldhahn. Mixture of robust experts (more): A robust denoising method towards multiple perturbations. arXiv preprint arXiv:2104.10586, 2021.   \nYao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. Advances in neural information processing systems, 33:8588\u20138601, 2020.   \nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British Machine Vision Conference 2016. British Machine Vision Association, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pages 7472\u20137482. PMLR, 2019. Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\equiv$ iAX0l6Cz8ub. ", "page_idx": 13}, {"type": "text", "text": "A Proof of Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem A.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first show what happens during one step of optimization, where we highlight the importance of analyzing delta error. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. Consider model parameter $\\theta\\sim\\pi$ and an aggregation rule $A g g r(\\cdot)$ with step size $\\mu>0$ . Define the updated parameter as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta^{+}:=\\theta-\\mu\\widehat{g}_{A g g r}(\\theta).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assuming the gradient $\\nabla{\\mathcal{L}}(\\theta)$ is $\\gamma$ -Lipschitz in $\\theta$ for any input, and let the step size $\\begin{array}{r}{\\mu\\leq\\frac{1}{\\gamma}}\\end{array}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a},\\theta}[\\mathcal{L}_{\\mathcal{D}_{a}}(\\theta^{+})-\\mathcal{L}_{\\mathcal{D}_{a}}(\\theta)]\\leq-\\frac{\\mu}{2}(\\|g_{a}\\|_{\\pi}^{2}-\\Delta_{4g g r}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. The proof is the same as Theorem A.1 in [Enyi Jiang, 2024]. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.2 (Convergence of ${\\tt A g g r}(\\cdot))$ . For any probability measure $\\pi$ over the parameter space, and an aggregation rule $A g g r(\\cdot)$ with step size $\\mu\\,>\\,0$ . We update the parameter for $T$ steps by $\\theta^{t+1}:=\\theta^{\\bar{t}}-\\mu\\widehat g_{A g g r}(\\theta^{t})$ . Assume the gradient $\\nabla{\\mathcal{L}}(\\theta)$ and ${\\widehat{g}}_{A g g r}(\\theta)$ are $\\scriptstyle{\\frac{\\gamma}{2}}$ -Lipschitz in $\\theta$ such that $\\theta^{t}\\rightarrow\\widehat{\\theta}_{A g g r}$ . \u2206Aggr_max is the Delta error at time $t^{\\prime}$ when $\\|\\widehat{g}_{A g g r}(\\widehat{\\theta}_{A g g r})\\ldots\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t^{\\prime}}}(\\widehat{\\theta}_{A g g r})\\|^{2}$ is maximized. Then, given step size $\\begin{array}{r}{\\mu\\leq\\frac{1}{\\gamma}}\\end{array}$ and a small enough $\\epsilon>0$ , with probability at least $1-\\delta$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{T}}(\\theta^{T})\\|^{2}\\leq\\frac{1}{\\delta^{2}}\\left(\\sqrt{C_{\\epsilon}\\cdot\\Delta_{4g g r_{-}m a x}^{2}}+\\mathcal{O}(\\epsilon)\\right)^{2}+\\mathcal{O}\\left(\\frac{1}{T}\\right)+\\mathcal{O}(\\epsilon),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $C_{\\epsilon}=\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t^{\\prime}}}[1/\\pi(B_{\\epsilon}(\\widehat{\\theta}_{A g g r}))]^{2}$ and $B_{\\epsilon}(\\widehat{\\theta}_{A g g r})\\subset\\mathbb{R}^{m}$ is the ball with radius $\\epsilon$ centered at $\\widehat{\\theta}_{A g g r}$ .   \nThe $C_{\\epsilon}$ measures how well $\\pi$ covers where the optimization goes. ", "page_idx": 14}, {"type": "text", "text": "Proof. Denote random function $\\widehat{f}:\\mathbb{R}^{m}\\to\\mathbb{R}_{+}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{f}(\\theta)=\\|\\widehat{g}_{\\mathrm{Aggr}}(\\theta)-\\nabla\\mathcal{L}_{\\mathcal{D}_{a}}(\\theta)\\|,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the randomness comes from $\\widehat{\\mathcal{D}}_{a}$ . Note that $\\widehat{f}$ is $\\gamma$ -Lipschitz by assumption. Now we consider $B_{\\epsilon}(\\widehat{\\theta}_{\\mathrm{Aggr}})\\subset\\mathbb{R}^{m}$ , i.e., the ball with radius $\\epsilon$ centered at $\\widehat{\\theta}_{\\mathrm{Aggr}}$ . Then, by $\\gamma$ -Lipschitzness we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{\\theta\\sim\\pi}\\widehat{f}(\\theta)=\\displaystyle\\int\\widehat{f}(\\theta)\\,\\mathrm{d}\\pi(\\theta)}\\\\ &{}&{\\quad\\quad\\quad\\geq\\displaystyle\\int_{B_{\\epsilon}(\\widehat{\\theta}_{\\mathrm{Aggr}})}(\\widehat{f}(\\widehat{\\theta}_{\\mathrm{Aggr}})-\\gamma\\epsilon)\\,\\mathrm{d}\\pi(\\theta)}\\\\ &{}&{\\quad\\quad=(\\widehat{f}(\\widehat{\\theta}_{\\mathrm{Aggr}})-\\gamma\\epsilon)\\pi(B_{\\epsilon}(\\widehat{\\theta}_{\\mathrm{Aggr}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{f}(\\widehat{\\theta}_{\\mathrm{Aggr}})\\leq\\frac{1}{\\pi(B_{\\epsilon}(\\widehat{\\theta}_{\\mathrm{Aggr}}))}\\cdot\\mathbb{E}_{\\theta\\sim\\pi}\\widehat{f}(\\theta)+\\mathcal{O}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking expectation w.r.t. $\\widehat{\\mathcal{D}}_{a}$ on both sides, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\widehat{\\mathcal{P}}_{a}}\\widehat{f}(\\widehat{\\theta}_{k\\otimes\\mathbf{z}})\\leq\\mathbb{E}_{\\widehat{\\mathcal{P}}_{a}}\\left[\\frac{1}{\\pi\\left(B_{\\epsilon}(\\widehat{\\theta}_{k\\otimes\\mathbf{z}})\\right)}\\cdot\\mathbb{E}_{\\theta\\sim\\pi}\\widehat{f}(\\theta)\\right]+{\\mathcal O}(\\epsilon)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\sqrt{\\mathbb{E}_{\\widehat{\\mathcal{P}}_{a}}\\left[\\frac{1}{\\pi\\left(B_{\\epsilon}(\\widehat{\\theta}_{k\\otimes\\mathbf{z}})\\right)}\\right]^{2}\\cdot\\mathbb{E}_{\\widehat{\\mathcal{P}}_{a}}\\left[\\mathbb{E}_{\\theta\\sim\\pi}\\widehat{f}(\\theta)\\right]^{2}}+{\\mathcal O}(\\epsilon)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ =\\sqrt{C_{\\epsilon}\\cdot\\mathbb{E}_{\\widehat{\\mathcal{P}}_{a}}\\left[\\mathbb{E}_{\\theta\\sim\\pi}\\widehat{f}(\\theta)\\right]^{2}}+{\\mathcal O}(\\epsilon)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\sqrt{C_{\\epsilon}\\cdot\\mathbb{E}_{\\widehat{\\mathcal{P}}_{a}}\\mathbb{E}_{\\theta\\sim\\pi}\\left[\\widehat{f}(\\theta)\\right]^{2}}+{\\mathcal O}(\\epsilon)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ =\\sqrt{C_{\\epsilon}\\cdot\\Delta_{\\mu_{\\mathtt{p}}^{2}}}+{\\mathcal O}(\\epsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Markov\u2019s inequality, with probability at least $1-\\delta$ we have a sampled datasetD a such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{f}(\\widehat{\\theta}_{\\mathrm{Aggr}})\\leq\\frac{1}{\\delta}\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}}\\widehat{f}(\\widehat{\\theta}_{\\mathrm{Aggr}})\\leq\\frac{1}{\\delta}\\sqrt{C_{\\epsilon}\\cdot\\Delta_{\\mathrm{Aggr}}^{2}}+\\mathcal{O}(\\epsilon/\\delta)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Conditioned on such event, we proceed on to the optimization part. ", "page_idx": 15}, {"type": "text", "text": "Note that Theorem A.1 characterizes how the optimization works for one gradient update. We denote $\\mathcal{D}_{a}^{t}$ as the data distribution $\\mathcal{D}_{a}$ at time step $t$ . Therefore, for any time step $t=0,\\dots,T-1$ , we can apply Theorem A.1 which only requires the Lipschitz assumption: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t+1})-\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\leq-\\frac{\\mu}{2}\\left(\\Vert\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\Vert^{2}-\\Vert\\widehat{g}_{\\mathrm{Aggr}}(\\theta^{t})-\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We notice that $\\mathcal{D}_{a}^{t}$ changes based on $\\theta\\mathbf{s}$ of different time steps. On both sides, to sum over $t=$ $0,\\dots,T-1$ , we first consider two terms: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t+1})-\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t}))+(\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})-\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t-1}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To compare $\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})$ and $\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})$ , since $\\mathcal{D}_{a}^{t}$ optimizes one more step than $\\mathcal{D}_{a}^{t-1}$ , we assume $\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\leq\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})$ for $\\forall t$ . Therefore, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t+1})-\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t}))+(\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})-\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t-1}))\\ge(\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t+1})-\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t}))+(\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})-\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t}))\\ge0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Summing up all time steps, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t+1})-\\mathcal{L}_{\\mathcal{D}_{a}^{0}}(\\theta^{0})\\leq\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t+1})-\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})+\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})-\\mathcal{L}_{\\mathcal{D}_{a}^{t-2}}(\\theta^{t-1})+\\dots-\\mathcal{L}_{\\mathcal{D}_{a}^{0}}(\\theta^{0})}\\\\ &{\\qquad\\leq-\\frac{\\mu}{2}\\left(\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})\\|^{2}-\\displaystyle\\sum_{t=0}^{T-1}\\|\\widehat{g}_{\\sf A g x}(\\theta^{t})-\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Dividing both sides by $T$ , and with regular algebraic manipulation we derive ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})\\|^{2}\\leq\\frac{2}{\\mu T}(\\mathcal{L}_{\\mathcal{D}_{a}^{0}}(\\theta^{0})-\\mathcal{L}_{\\mathcal{D}_{a}^{T-1}}(\\theta^{T}))+\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\widehat{g}_{\\mathrm{Aggr}}(\\theta^{t})-\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that we assume the loss function $\\mathcal{L}_{\\mathcal{D}}(\\theta):=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\mathcal{L}(f(x),y)$ , is non-negative. Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})\\|^{2}\\leq\\frac{2\\mathcal{L}_{\\mathcal{D}_{a}^{0}}(\\theta^{0})}{\\mu T}+\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\widehat{g}_{\\mathtt{A g g r}}(\\theta^{t})-\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\theta^{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that we assume given $\\widehat{\\mathcal{D}}_{a}$ we have $\\theta^{t}\\to\\widehat{\\theta}_{\\mathtt{A g g r}}$ . Therefore, for any $\\epsilon>0$ there exist $T_{\\epsilon}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall t>T_{\\epsilon}:\\|\\theta^{t}-\\widehat{\\theta}_{\\mathrm{Aggr}}\\|<\\epsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This implies that $\\forall t>T_{\\epsilon}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu\\Vert\\widehat{g}_{\\mathtt{A g g r}}(\\theta^{t})\\Vert=\\Vert\\theta^{t+1}-\\widehat{\\theta}_{\\mathtt{A g g r}}+\\widehat{\\theta}_{\\mathtt{A g g r}}-\\theta^{t}\\Vert\\leq\\Vert\\theta^{t+1}-\\widehat{\\theta}_{\\mathtt{A g g r}}\\Vert+\\Vert\\widehat{\\theta}_{\\mathtt{A g g r}}-\\theta^{t}\\Vert<2\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, (11) also implies $\\forall t_{1},t_{2}>T_{\\epsilon}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla{\\mathcal L}_{\\mathcal{D}_{a}^{t_{1}}}({\\boldsymbol{\\theta}}^{t_{1}})-\\nabla{\\mathcal L}_{\\mathcal{D}_{a}^{t_{2}}}({\\boldsymbol{\\theta}}^{t_{2}})\\|\\leq\\gamma\\|{\\boldsymbol{\\theta}}^{t_{1}}-{\\boldsymbol{\\theta}}^{t_{2}}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad<2\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{n}|\\nabla c_{p+\\cdot}(\\theta^{t})|^{2}\\leq\\frac{2C_{p+\\cdot}(\\theta^{t})}{\\mu\\Gamma}+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\hat{\\ln}\\omega_{i}(\\theta^{t})-\\nabla c_{p+\\cdot}(\\theta^{i})|^{2}+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\hat{\\ln}\\omega_{i}(\\theta^{i})-\\nabla c_{p-\\cdot}(\\theta^{i})|^{2}}\\\\ {=}&{-\\displaystyle(\\frac{1}{\\Gamma})+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\hat{\\ln}\\omega_{i}(\\theta^{i})-\\nabla c_{p+\\cdot}(\\theta^{i})|^{2}}\\\\ &{-\\displaystyle(\\frac{1}{\\Gamma})+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\hat{\\ln}\\omega_{i}(\\theta^{i})-\\hat{\\ln}\\omega_{i}(\\hat{\\theta}_{i+1})+\\hat{\\ln}\\omega_{i}(\\hat{\\theta}_{i+1})-\\nabla c_{p+\\cdot}(\\theta^{i})|^{2}}\\\\ &{\\leq\\displaystyle(\\frac{1}{\\Gamma})+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\left(\\hat{\\ln}\\omega_{i}(\\theta^{i})-\\hat{\\ln}\\omega_{i}(\\hat{\\theta}_{i+1})+\\hat{\\ln}\\omega_{i}(\\hat{\\theta}_{i+1})-\\nabla c_{p+\\cdot}(\\theta^{i})\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\mathrm{~otangh~in~outing~}\\right)}\\\\ &{=\\displaystyle(\\frac{1}{\\Gamma})+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\hat{\\left(\\hat{\\psi}_{i}(\\theta^{i})+\\hat{\\ln}\\omega_{i}(\\hat{\\theta}_{i+1})-\\nabla c_{p-\\cdot}(\\hat{\\theta}_{i})\\right)^{2}}\\\\ &{=\\displaystyle(\\frac{1}{\\Gamma})+O(\\epsilon)+\\frac{1}{\\Gamma}\\displaystyle\\sum_{i=1}^{n-1}\\left(\\hat{\\ln}\\omega_{i}(\\hat{\\theta}_{i+1})-\\nabla c_{p-\\cdot}(\\hat{\\theta}_{i+1})+\\nabla c_{p-\\cdot}(\\hat{\\theta}_{i+\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Equation 14 bounds the left hand side with the maximum $\\|\\widehat{g}_{\\mathtt{A g g r}}(\\widehat{\\theta}_{\\mathtt{A g g r}})-\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t}}(\\widehat{\\theta}_{\\mathtt{A g g r}})\\|^{2}$ one can get during the optimization steps. Here, we assume at time $t^{\\prime}$ , the largest value is attained. We denote $\\bar{\\Delta}_{\\mathrm{Aggr\\_max}}^{2}$ as the delta error at time step $t^{\\prime}$ . ", "page_idx": 16}, {"type": "text", "text": "Then, we can continue with what we have done at the beginning of the proof of this theorem: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(14)=\\mathcal{O}\\left(\\frac{1}{T}\\right)+\\mathcal{O}(\\epsilon)+f(\\widehat{\\theta}_{\\mathrm{Aggr}})^{2}}\\\\ &{\\quad\\quad\\leq\\mathcal{O}\\left(\\frac{1}{T}\\right)+\\mathcal{O}(\\epsilon)+\\bigg(\\frac{1}{\\delta}\\sqrt{C_{\\epsilon}\\cdot\\Delta_{\\mathrm{Aggr\\_max}}^{2}}+\\mathcal{O}(\\epsilon/\\delta)\\bigg)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, combining the above we finally have: for $\\forall T>T_{\\epsilon}$ with probability at least $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{t-1}}(\\theta^{t})\\|^{2}\\leq\\mathcal{O}\\left(\\frac{1}{T}\\right)+\\mathcal{O}(\\epsilon)+\\frac{1}{\\delta^{2}}\\left(\\sqrt{C_{\\epsilon}\\cdot\\Delta_{\\mathrm{Aggr\\_max}}^{2}}+\\mathcal{O}(\\epsilon)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To complete the proof, let us investigate the left-hand side. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla C_{D_{\\tau_{*}^{-1}}}(\\theta^{t})\\|^{2}=\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\|\\nabla C_{D_{\\tau_{*}^{*}}}(\\theta^{t})\\|^{2}+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=T_{*}}^{T-1}\\|\\nabla C_{D_{\\tau_{*}^{*}}}(\\theta^{t})\\|^{2}}\\\\ {\\displaystyle}&{=\\mathcal{O}\\left(\\displaystyle\\frac{1}{T}\\right)+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=T_{*}}^{T-1}\\|\\nabla C_{D_{\\tau_{*}^{*}}}(\\theta^{t})\\|^{2}}\\\\ &{\\ge\\mathcal{O}\\left(\\displaystyle\\frac{1}{T}\\right)+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=T_{*}}^{T-1}\\left(\\|\\nabla C_{D_{\\tau_{*}^{*}}}(\\theta^{t})-\\nabla C_{D_{\\tau_{*}^{-1}}}(\\theta^{T})\\|-\\|\\nabla C_{D_{\\tau_{*}^{-1}}}(\\theta^{T})\\|\\right)^{2}}\\\\ &{\\quad\\displaystyle\\qquad\\ge\\mathcal{O}\\left(\\displaystyle\\frac{1}{T}\\right)+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=T_{*}}^{T-1}(\\mathcal{O}(\\epsilon)+\\|\\nabla C_{D_{\\tau_{*}^{-1}}}(\\theta^{T})\\|^{2})}\\\\ &{=\\mathcal{O}\\left(\\displaystyle\\frac{1}{T}\\right)+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=T_{*}}^{T-1}\\mathcal{O}(\\epsilon)+\\|\\nabla C_{D_{\\tau_{*}^{-1}}}(\\theta^{T})\\|^{2}}\\\\ &{=\\mathcal{O}\\left(\\displaystyle\\frac{1}{T}\\right)+\\mathcal{O}(\\epsilon)+\\|\\nabla C_{D_{\\tau_{*}^{-1}}}(\\theta^{T})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining (15) and (16), we finally have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}_{\\mathcal{D}_{a}^{T}}(\\theta^{T})\\|^{2}\\leq\\mathcal{O}\\left(\\frac{1}{T}\\right)+\\mathcal{O}(\\epsilon)+\\frac{1}{\\delta^{2}}\\left(\\sqrt{C_{\\epsilon}\\cdot\\Delta_{\\mathrm{Aggr\\_max}}^{2}}+\\mathcal{O}(\\epsilon)\\right)^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.2 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To prove Theorem 4.5, we first use the following definitions and lemmas from [Enyi Jiang, 2024], to get the delta errors of Gradient Projection (GP) and standard adversarial training (AT): ", "page_idx": 17}, {"type": "text", "text": "Definition A.3 (GP Aggregation). Let $\\beta\\in[0,1]$ be the weight that balances between $\\widehat{g}_{a}$ and ${\\widehat{g}}_{n}$ . The $G P$ aggregation operation is ", "page_idx": 17}, {"type": "equation", "text": "$$\nG P(\\widehat{g}_{a},\\widehat{g}_{n})=\\left((1-\\beta)\\widehat{g}_{a}+\\beta P r o j_{+}(\\widehat{g}_{a}|\\widehat{g}_{n})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $P r o j_{+}(\\widehat{g}_{a}|\\widehat{g}_{n})=\\operatorname*{max}\\{\\langle\\widehat{g}_{a},\\widehat{g}_{n}\\rangle,0\\}\\widehat{g}_{n}/\\|\\widehat{g}_{n}\\|^{2}$ is the operation that projects $\\widehat{g}_{a}$ to the positive direction ofgn. ", "page_idx": 17}, {"type": "text", "text": "Definition A.4 (AT Aggregation). The $A T$ aggregation operation is ", "page_idx": 17}, {"type": "equation", "text": "$$\nA T({\\widehat{g}}_{a})={\\widehat{g}}_{a}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "standard AT only leverages the gradient update on $\\widehat{\\mathcal{D}}_{a}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma A.5 (Delta Error of GP). Given distributions $\\widehat{\\mathcal{D}}_{a}$ , $\\mathcal{D}_{a}$ and $\\widehat{\\mathcal{D}}_{n}$ , as well as the model updates $\\widehat{g_{a}},g_{a},\\widehat{g_{n}}$ on these distributions per epoch, we have $\\Delta_{G P}^{2}$ as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{G P}^{2}\\approx\\left((1-\\beta)^{2}+\\frac{2\\beta-\\beta^{2}}{m}\\right)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}}\\|g_{a}-\\widehat{g}_{a}\\|_{\\pi}^{2}+\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g}_{n}\\|_{\\pi}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the above equation, $m$ is the model dimension and $\\bar{\\tau}^{2}=\\mathbb{E}_{\\pi}[\\tau^{2}]\\in[0,1]$ where $\\tau(\\theta)$ is the $\\sin(\\cdot)$ value of the angle between ${\\widehat{g}}_{n}$ and $g_{a}-\\widehat{g}_{n}.\\mathrm{~}\\|\\cdot\\|_{\\pi}$ is the $\\pi$ -norm over the model parameter space. ", "page_idx": 17}, {"type": "text", "text": "Proof. The proof is the same as Theorem 4.4 in Enyi Jiang [2024]. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.6 (Delta Error of AT). Given distributions $\\widehat{\\mathcal{D}}_{a}$ , $\\mathcal{D}_{a}$ and $\\widehat{\\mathcal{D}}_{n}$ , as well as the model updates $\\widehat{g_{a}},g_{a},\\widehat{g_{n}}$ on these distributions per epoch, we have $\\Delta_{A T}^{2}$ as follow s ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{A T}^{2}=\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}}\\left\\lVert g_{a}-\\widehat{g}_{a}\\right\\rVert_{\\pi}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\pi}$ is the $\\pi$ -norm over the model parameter space. ", "page_idx": 17}, {"type": "text", "text": "Then, we prove Theorem 4.5. ", "page_idx": 18}, {"type": "text", "text": "Theorem A.7 (Error Analysis of GP). When the model dimension is large $m\\rightarrow\\infty,$ ) at time step $t$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{A T}^{2}-\\Delta_{G P}^{2}\\approx\\beta(2-\\beta)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g}_{n}\\|_{\\pi}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\bar{\\tau}^{2}=\\mathbb{E}_{\\pi}[\\tau^{2}]\\in[0,1]$ where $\\tau$ is the $\\sin(\\cdot)$ value of the angle between ${\\widehat{g}}_{n}$ and $g_{a}-\\widehat{g}_{n},\\,\\|\\cdot\\|_{\\pi}$ is the $\\pi$ -norm over the model parameter space. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P r o o f.\\ \\Delta_{\\mathtt{k T}}^{2}-\\Delta_{\\mathtt{G P}}^{2}}\\\\ &{\\approx\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\Big((1-\\beta)^{2}+\\frac{2\\beta-\\beta^{2}}{m}\\Big)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g_{n}}\\|_{\\pi}^{2}}\\\\ &{=\\Big(1-\\big((1-\\beta)^{2}+\\frac{2\\beta-\\beta^{2}}{m}\\big)\\Big)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g_{n}}\\|_{\\pi}^{2}}\\\\ &{=(1+\\frac{1}{m})\\beta(2-\\beta)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g_{n}}\\|_{\\pi}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $m\\rightarrow\\infty$ , we have a simplified version of the error difference as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta_{A T}^{2}-\\Delta_{G P}^{2}\\approx\\beta(2-\\beta)\\mathbb{E}_{\\widehat{\\mathcal{D}}_{a}^{t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}-\\beta^{2}\\bar{\\tau}^{2}\\|g_{a}-\\widehat{g_{n}}\\|_{\\pi}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Interpretation. When $\\beta~=~0.5$ , we can usually show $\\Delta_{A T}^{2}\\ >\\ \\Delta_{G P}^{2}$ , because $\\beta(2\\mathrm{~-~}\\beta)\\;>\\;$ $\\beta^{2}\\bar{\\tau}^{2}\\bar{(}0.75\\;>\\;0.25)$ for the coefficients of two terms. We estimate the actual values of terms $E_{\\widehat{D}_{a^{t}}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}$ (variance), $\\|g_{a}-\\widehat{g}_{n}\\|_{\\pi}^{2}$ (bias), and $\\bar{\\tau}$ using the estimation methods in Enyi Jiang [2024]. Table 6 displays the values of those terms as well as the error differences on ResNet18 experiments at epoch 5, 10, 15, 20, 60. We plot the changing of these terms on the ResNet18 experiment in Figure 5. The order of difference is always positive and usually smaller than $1e^{-08}$ and approaches the order of $1e^{-12}$ in the end. ", "page_idx": 18}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/41fa7d53779ad5c4422f619ae8709cb0db591e1571c82a9209e6e5cb3a00c6be.jpg", "table_caption": ["Table 6: Estimations the actual values of terms $E_{\\widehat{D_{a}t}}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}$ (variance), $\\|g_{a}-\\widehat{g}_{n}\\|_{\\pi}^{2}$ (bias), $\\bar{\\tau}$ , and $\\Delta_{A T}^{2}-\\Delta_{G P}^{2}$ (error differences) across different epochs. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B Additional Experiment Information ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide more training details, additional experiment results on the universal robustness of RAMP to common corruptions and unseen adversaries, runtime analysis of RAMP, additional ablation studies on different logit pairing losses, and AT from random initialization results on CIFAR-10 using WideResNet-28-10. ", "page_idx": 18}, {"type": "text", "text": "B.1 More Training Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We set the batch size to 128 for the experiments on ResNet-18 and WideResNet-28-10 architectures. We use an SGD optimizer with 0.9 momentum and $5e^{-4}$ weight decay. For other experiments on ImageNet, we use a batch size of 64 to fit into the GPU memory for larger models. For all training procedures, we select the last checkpoint for the comparison. When the pre-trained model was originally trained with extra data beyond the CIFAR-10 dataset, similar to Croce and Hein [2022], we use the extra $500\\mathbf{k}$ images introduced by Carmon et al. [2019] for fine-tuning, and each batch contains the same amount of standard and extra images. An epoch is completed when the whole standard training set has been used. ", "page_idx": 18}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/6957d8c5d7620a42c8f0b5ad3b5eb7464a9cdcbfa456cd55ac0745ab18bfb3ad.jpg", "img_caption": ["Figure 5: Plot of values of terms $E_{\\widehat{D}_{a}t}\\|g_{a}-\\widehat{g_{a}}\\|_{\\pi}^{2}$ (variance), $\\lVert g_{a}-\\widehat{g}_{n}\\rVert_{\\pi}^{2}$ (bias), $\\bar{\\tau}$ , and $\\Delta_{A T}^{2}-\\Delta_{G P}^{2}$ (error differences). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.2 Runtime Analysis of RAMP ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present runtime analysis results demonstrating the fact that RAMP is more expensive than E-AT and less expensive than MAX in Table 7. These results, recorded in seconds per epoch, were obtained using a single A100 40GB GPU. RAMP consistently supports that fact in all experiments. ", "page_idx": 19}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/0b6d5b7044c358224312d0f6b3bd10ccbfc107461517963823f6ca0a4ce50a95.jpg", "table_caption": ["Table 7: Analysis of time per epoch for RAMP and related baselines. RAMP is more expensive than E-AT and less expensive than MAX. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.3 Additional Results on RAMP Generalizing to Common Corruptions and Unseen Adversaries for Universal Robustness ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we show RAMP can generalize better to other corruptions and unseen adversaries on union accuracy for stronger universal robustness. ", "page_idx": 19}, {"type": "text", "text": "Implementations. For the $l_{0}$ attack, we use Croce et al. [2022] with an epsilon of 9 pixels and $5k$ query points. For common corruptions, we directly use the implementation of RobustBench [Croce et al., 2020] for evaluation across 5 severity levels on all corruption types used in Hendrycks and Dietterich [2019]. For other unseen adversaries, we follow the implementation of Laidlaw et al. [2020], where we set $e p s=12$ for the fog attack, $e p s=0.5$ for the snow attack, $e p s=60$ for the gabor attack, $e p s=0.125$ for the elastic attack, and $e p s=0.125$ for the jpeglinf attack with 100 iterations. For ResNet-18 experiments, we do not compare with Winninghand [Diffenderfer et al., ", "page_idx": 19}, {"type": "text", "text": "2021] since it uses a Wide-ResNet architecture. Also, we select the strongest baselines $\\mathrm{\\bfE}$ -AT and MAX) from the Wide-ResNet experiment results to compare for ResNet-18 experiments on universal robustness. ", "page_idx": 20}, {"type": "text", "text": "Results. For the ResNet-18 training from scratch experiment on CIFAR-10, in Table 8 and 9, we also show RAMP generally outperforms by $0.5\\%$ on common corruptions and $7\\%$ on union accuracy against unseen adversaries compared with E-AT. ", "page_idx": 20}, {"type": "text", "text": "Table 8: Accuracy against common corruptions using ResNet-18 on CIFAR-10 dataset. ", "page_idx": 20}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/c8d803f80c8310b6ce8222b35d09af43365f61fa0d6f995eb8e5a7b317a3e355.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 9: Individual, average, and union accuracy against unseen adversaries using ResNet-18 on CIFAR-10 dataset. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/7f8f20549744c3017992cffaae16a3bdc1cf48c62cbe4bc5a6bd15252b0cb698.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.4 Additional Experiments with Different Epsilon Values ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide additional results with different $\\epsilon_{1},\\epsilon_{2},\\epsilon_{\\infty}$ values. We select $\\epsilon_{\\infty}=$ [ 2255,2455, 21525, 21565], \u03f51 = [6, 9, 12, 15], and \u03f52 = [0.25, 0.75, 1.0, 1.5]. We provide additional RAMP results compared with related baselines with training from scratch and performing robust fine-tuning in Section B.4.1 and Section B.4.2, respectively. We observe that RAMP can surpass E-AT with significant margins as well as a better accuracy-robustness tradeoff for both training from scratch and robust fine-tuning with $\\lambda=2.0$ for training from scratch and $\\lambda=0.5$ for robust fine-tuning in most cases. ", "page_idx": 20}, {"type": "text", "text": "B.4.1 Additional Results with Training from Scratch ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "CRhAaMngPi ncgo $l_{\\infty}$ tpeentrltyu robutaptieorfnosr mwsit hE - $\\begin{array}{r}{\\epsilon_{\\infty}\\,=\\,[\\frac{2}{255},\\,\\frac{4}{255},\\frac{12}{255},\\,\\frac{16}{255}]}\\end{array}$ .2 ] Toanb luen i1o0n  aancdc uTraabclye  w1h1e snh torawi ntihnagt from scratch. ", "page_idx": 20}, {"type": "text", "text": "iTnaitbilael i1za0t:i o $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{2}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{4}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ with random ", "page_idx": 20}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/1c8ed1b8a5efb3bd31284bb69e305cb989a353003547b11902bf9b785ba2b6a1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 11: $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{12}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{16}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ with random initializations. ", "page_idx": 20}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/d386992abc7f873405240265cb3ce49b38d9777b433ac23aa1117028bd35e6b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Changing $l_{1}$ perturbations with $\\epsilon_{1}\\,=\\,[6,9,12,15]$ . Table 12 and Table 13 show that RAMP consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training from scratch. ", "page_idx": 20}, {"type": "text", "text": "Changing $l_{2}$ perturbations with $\\epsilon_{2}=[0.25,0.75,1.0,1.5]$ . Table 14 and Table 15 show that RAMP consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training from scratch. ", "page_idx": 20}, {"type": "text", "text": "tTiaobnlse. 12: $\\begin{array}{r}{(\\epsilon_{\\infty}=\\frac{8}{255},\\epsilon_{1}=6,\\epsilon_{2}=0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}=\\frac{8}{255},\\epsilon_{1}=9,\\epsilon_{2}=0.5)}\\end{array}$ with random initializa", "page_idx": 21}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/d0c910021996b4fc81d61f5ba19348c1d18d4d61d9c388cfad717db352f9bda4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "iTnaitbilael iz1a3t:i o $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{8}{255},\\epsilon_{1}=15,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{8}{255},\\epsilon_{1}=18,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ with random ", "page_idx": 21}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/3a0821a62d86fbd470d30beed7c933d3a2cbb3d99006cc384bf553f520d60fc8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "iTnaitbilael i1za4t:i o $\\begin{array}{r}{(\\epsilon_{\\infty}\\,=\\,\\frac{8}{255},\\epsilon_{1}\\,=\\,12,\\epsilon_{2}=0.25)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\,=\\,\\frac{8}{255},\\epsilon_{1}\\,=\\,12,\\epsilon_{2}=0.75)}\\end{array}$ with random ", "page_idx": 21}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/ea51f2ff81f407ff479c11f05b7dcc70fc82fe3b15fabc766c93b88757cfb779.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "iTnaitbilael iz1a5t:i o $\\begin{array}{r l}{(\\epsilon_{\\infty}\\;=\\;\\frac{8}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}=1.0)}\\end{array}$ and $(\\epsilon_{\\infty}\\;=\\;\\textstyle{\\frac{8}{255}},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}=1.5)$ with random ", "page_idx": 21}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/0bd3c9f73fd7efea3fee00f2d2bfb825d8a39447a2e572569d2feaa3787b12c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.4.2 Additional Results with Robust Fine-tuning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "RChAaMngPi cnog $l_{\\infty}$ tepnetrlty uorubtapteirofonrs mws itEh- $\\begin{array}{r}{\\epsilon_{\\infty}\\,=\\,[\\frac{2}{255},\\frac{4}{255},\\frac{12}{255},\\frac{16}{255}]}\\end{array}$ .o nT aubnlieo n1 a6c caunrda cTya bwleh e1n 7p serhfoowr mtihnagt robust fine-tuning. ", "page_idx": 21}, {"type": "text", "text": "Table 16: $(\\epsilon_{\\infty}\\;=\\;\\textstyle{\\frac{2}{255}},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{4}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ with robust fine-tuning. ", "page_idx": 21}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/fce6ba75cecf77d41301e40492760f4128844199693c6bffc87fab56cc63b398.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 17: $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{12}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{16}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}\\;=\\;0.5)}\\end{array}$ with robust ", "page_idx": 21}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/94a5305f8a62bc92ae668a0a8aefb46b50b1dfbddcf7c9bb5e90e75386a30c72.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Changing $l_{1}$ perturbations with $\\epsilon_{1}\\,=\\,[6,9,12,15]$ . Table 12 and Table 13 show that RAMP consistently outperforms $\\boldsymbol{\\mathrm E}$ -AT [Croce and Hein, 2022] on union accuracy when performing robust fine-tuning. ", "page_idx": 21}, {"type": "text", "text": "Changing $l_{2}$ perturbations with $\\epsilon_{2}=[0.25,0.75,1.0,1.5]$ . Table 14 and Table 15 show that RAMP consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when performing robust fine-tuning. ", "page_idx": 21}, {"type": "text", "text": "Table 18: (\u03f5\u221e= $\\begin{array}{r}{(\\epsilon_{\\infty}=\\frac{8}{255},\\epsilon_{1}=6,\\epsilon_{2}=0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}=\\frac{8}{255},\\epsilon_{1}=9,\\epsilon_{2}=0.5)}\\end{array}$ with robust fine-tuning. ", "page_idx": 22}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/7e44a3a04f6063f74a5664718a03c5b72822ce0ce8ea5d6400cccc664da4db3a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 19: $\\begin{array}{r}{(\\epsilon_{\\infty}\\,=\\,\\frac{8}{255},\\epsilon_{1}=\\mathbf{15},\\epsilon_{2}\\,=\\,0.5)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\,=\\,\\frac{8}{255},\\epsilon_{1}=18,\\epsilon_{2}\\,=\\,0.5)}\\end{array}$ with robust fine", "page_idx": 22}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/b4cff648a221d1fd743bc5b3d00421dd56fa230c3fe7f07d38988daa344e88d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 20: $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{8}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}=0.25)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\;=\\;\\frac{8}{255},\\epsilon_{1}\\;=\\;12,\\epsilon_{2}=0.75)}\\end{array}$ with robust fine-tuning. ", "page_idx": 22}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/48447d55321c7db18f77a1af87382773cc98d0b7e0d372690d9fcc7d58243fca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 21: $\\begin{array}{r}{(\\epsilon_{\\infty}\\,=\\,\\frac{8}{255},\\epsilon_{1}\\,=\\,12,\\epsilon_{2}=1.0)}\\end{array}$ and $\\begin{array}{r}{(\\epsilon_{\\infty}\\,=\\,\\frac{8}{255},\\epsilon_{1}\\,=\\,12,\\epsilon_{2}=1.5)}\\end{array}$ with robust finetuning. ", "page_idx": 22}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/1a309868a47806438c6aaf304ca5496a92f893bc9a1082048add01078ffb9b7f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.5 Different Logit Pairing Methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we test RAMP with robust fine-tuning using two more different logit pairing losses: (1) Mean Squared Error Loss $(\\mathcal{L}_{m s e})$ (Eq. 17), (2) Cosine-Similarity Loss $(\\mathcal{L}_{c o s})$ (Eq. 18). We replace the KL loss we used in the paper using the following losses. We use the same lambda value $\\lambda=1.5$ for both cases. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m s e}=\\frac{1}{n_{c}}\\cdot\\sum_{i=0}^{n_{c}}\\frac{1}{2}\\left(p_{q}[\\gamma[i]]-p_{r}[\\gamma[i]]\\right)^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c o s}=\\frac{1}{n_{c}}\\cdot\\sum_{i=0}^{n_{c}}\\left(1-\\cos(p_{q}[\\gamma[i]],p_{r}[\\gamma[i]])\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Table 22 displays RAMP robust fine-tuning results of different logit pairing losses using PreActResNet-18 on CIFAR-10 with $\\lambda=1.5$ . We see those losses generally improve union accuracy compared with baselines in Table 24. $\\mathcal{L}_{c o s}$ has a better clean accuracy yet slightly worsened union accuracy. $\\mathcal{L}_{m s e}$ has the best union accuracy and the worst clean accuracy. $\\mathcal{L}_{K L}$ is in the middle of the two others. However, we acknowledge the possibility that each logit pairing loss may have its own best-tuned $\\lambda$ value. ", "page_idx": 22}, {"type": "text", "text": "Table 22: RAMP fine-tuning results of different logit pairing losses using PreAct-ResNet-18 on CIFAR-10. ", "page_idx": 22}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/edd7fc3e6e13ca60f6d2a222ff86199fa9f8dbe3a63d7ee6d61ebdaf8e7d0021.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.6 AT from Scratch Using WideResNet-28-10 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Implementations. We use a cyclic learning rate with a maximum rate of 0.1 for 30 epochs and adopt the outer minimization trades loss from Zhang et al. [2019] with the default hyperparameters, same as Croce and Hein [2022]; also, we set $\\lambda=2.0$ and $\\beta=0.5$ for training RAMP. Additionally, we use the WideResNet-28-10 architecture same as Zagoruyko and Komodakis [2016] for our reimplementations on CIFAR-10. ", "page_idx": 23}, {"type": "text", "text": "Results. Since the implementation of experiments on WideResNet-28-10 in Croce and Hein [2022] paper is not public at present, we report our implementation results on E-AT, where our results show that RAMP outperforms E-AT in union accuracy with a significant margin, as shown in Table 23. Also, we experiment with using the trade loss (RAMP w trades) for the outer minimization, we observe that RAMP w trades achieves a better union accuracy at the loss of some clean accuracy. ", "page_idx": 23}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/49f09dc7bd9d23824d2bd1a3480ecf2ab2f0b1b7b36b80925835017256db4995.jpg", "table_caption": ["Table 23: WideResNet-28-10 trained from random initialization on CIFAR-10. RAMP outperforms E-AT on union accuracy with our implementation. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.7 Robust Fine-tuning Using PreAct-ResNet-18 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Implementations. For robust fine-tuning with ResNet-18, we perform 3 epochs on CIFAR-10. We set the learning rate as 0.05 for PreAct-ResNet-18 and 0.01 for other models. We set $\\lambda=0.5$ in this case. Also, we reduce the learning rate by a factor of 10 after completing each epoch. ", "page_idx": 23}, {"type": "text", "text": "Result. Table 24 shows the robust fine-tuning results using PreAct ResNet-18 model on the CIFAR-10 dataset with different methods. The results for all baselines are directly from the E-AT paper [Croce and Hein, 2022] where the authors reimplemented other baselines (e.g., MSD, MAX) to achieve better union accuracy than presented in the original works. RAMP surpasses all other methods on union accuracy. ", "page_idx": 23}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/ff47977705216a064e5d36de08a4d3c510acfdd1b69b0d277c6997f9673bd51b.jpg", "table_caption": ["Table 24: RN-18 $l_{\\infty}$ -AT model fine-tuned for 3 epochs (repeated for 5 seeds). RAMP has the highest union accuracy. Baseline results are from Croce and Hein [2022]. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.8 Robust Fine-tuning with More Epochs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 25, we apply robust fine-tuning on the PreAct ResNet-18 model for the CIFAR-10 dataset with 5, 7, 10, 15 epochs, and compare it with E-AT. RAMP consistently outperforms the baseline on union accuracy, with a larger improvement when we increase the number of epochs. ", "page_idx": 23}, {"type": "text", "text": "C Additional Visualization Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide additional t-SNE visualizations of the multiple-norm tradeoff and robust fine-tuning procedures using different methods. ", "page_idx": 23}, {"type": "text", "text": "Table 25: Fine-tuning with more epochs: RAMP consistently outperforms E-AT on union accuracy.   \nE-AT results are from Croce and Hein [2022]. ", "page_idx": 24}, {"type": "table", "img_path": "u1Z3HWz4VJ/tmp/e1ec7d2c7a400c108b4aa527d07593bf9c8da69b9f104dbfe83e358db4b32eb0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.1 Pre-trained $l_{1},l_{2},l_{\\infty}$ AT models ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Figure 6 shows the robust accuracy of $l_{1},l_{2},l_{\\infty}$ AT models against their respect $l_{1},l_{2},l_{\\infty}$ perturbations, on CIFAR-10 using PreAct-ResNet-18 architecture. Similar to Figure ??, $l_{\\infty}$ -AT model has a low $l_{1}$ robustness and vice versa. In this common choice of epsilons, we further confirm that $l_{\\infty}-l_{1}$ is the key trade-off pair. ", "page_idx": 24}, {"type": "text", "text": "C.2 Robust Fine-tuning for all Epochs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide the complete visualizations of robust fine-tuning for 3 epochs on CIFAR-10 using $l_{1}$ examples, E-AT, and RAMP. Rows in $l_{1}$ fine-tuning (Figure 7), E-AT fine-tuning (Figure 8), and RAMP fine-tuning (Figure 9) show the robust accuracy against $l_{\\infty},l_{1},l_{2}$ attacks individually, of epoch $0,1,2,3$ , respectively. We observe that throughout the procedure, RAMP manages to maintain more $l_{\\infty}$ robustness during the fine-tuning with more points colored in cyan, in comparison with two other methods. This visualization confirms that after we identify a $l_{p}-l_{r}(p\\neq r)$ key tradeoff pair, RAMP successfully preserves more $l_{p}$ robustness when training with some $l_{r}$ examples via enforcing union predictions with the logit pairing loss. ", "page_idx": 24}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/bc85306231231d492c217b91dbb536a0504d56019c71e5c0722be509d7763872.jpg", "img_caption": ["Figure 6: $l_{1},l_{2},l_{\\infty}$ pre-trained RN18 $l_{\\infty}$ -AT models with correct/incorrect predictions against $l_{1},l_{2},l_{\\infty}$ attacks. Correct predictions are colored with cyan and incorrect with magenta. Each row represents $l_{\\infty},l_{1},l_{2}$ AT models, respectively. Each column shows the accuracy concerning a certain $l_{p}$ attack. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/6537b5dbf3c98e8ccdcab88bbf0ea8c1b5608439d622dcae5c9aeac5e49c57f5.jpg", "img_caption": ["Figure 7: Finetune RN18 $l_{\\infty}$ -AT model on $l_{1}$ examples for 3 epochs. Each row represents the prediction results of epoch $0,1,2,3$ respectively. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/1f868d9a44c9b06ce8aeb838e273b2b8debb214ecb67cdec4a794a2980b719ab.jpg", "img_caption": ["Figure 8: Finetune RN18 $l_{\\infty}$ -AT model with E-AT for 3 epochs. Each row represents the prediction results of epoch $0,1,2,3$ respectively. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "u1Z3HWz4VJ/tmp/65767637965097edc5d6747e7f8b63f62e979bba5ac87fd70b6ab10ab5ccd9e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 9: Finetune RN18 $l_{\\infty}$ -AT model with RAMP for 3 epochs. Each row represents the prediction results of epoch $0,1,2,3$ respectively. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The abstract and introduction present the claims made in the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitations of our work in the discussion section of the paper. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We state the assumptions along with theoretical results in the theory part of the paper. The proofs are in the supplemental material. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the implementations of our experiments in both main paper and supplementary materials. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide scripts to reproduce all experimental results for the new proposed method. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The full details are provided with the code and in appendix. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The results are accompanied by confidence intervals. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the information on the computer resources in the discussion section of the paper. ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: We conduct the research with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We mention the societal impacts of the work in the discussion section of the paper. ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package or dataset. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide README and scripts to run for the new asset we introduce. They are well documented. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not include crowdsourcing or research involving human subjects. ", "page_idx": 28}]