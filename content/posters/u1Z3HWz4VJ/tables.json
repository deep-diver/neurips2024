[{"figure_path": "u1Z3HWz4VJ/tables/tables_7_1.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table presents the results of experiments comparing RAMP's performance against E-AT and MAX under different epsilon values for both training from scratch and robust fine-tuning scenarios.  It demonstrates RAMP's consistent superior performance across various settings by showing its higher union accuracy and superior robustness in handling diverse perturbation types.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_8_1.jpg", "caption": "Table 2: Robust fine-tuning on larger models and datasets (* uses extra data for pre-training). We evaluate all CIFAR-10 and Imagenet test points. RAMP consistently achieves better union accuracy with significant margins and good accuracy-robustness tradeoff.", "description": "This table presents the results of robust fine-tuning experiments using various models and datasets (CIFAR-10 and ImageNet). It compares the performance of RAMP against E-AT, highlighting RAMP's superior union accuracy (robustness against multiple perturbation types) while maintaining good accuracy. The table showcases the clean accuracy, robustness against different perturbation types (l\u221e, l2, l1), and the union accuracy for each model and method.  The asterisk (*) indicates that additional data was used for pre-training in those specific cases. The results clearly demonstrate RAMP's improvement in robustness against multiple norms.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_8_2.jpg", "caption": "Table 3: RN-18 model trained from random initialization on CIFAR-10 over 5 trials: RAMP achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein [2022].", "description": "This table presents the results of adversarial training from random initialization on CIFAR-10 using the PreAct ResNet-18 model.  The results are averaged over 5 independent trials.  The table compares the performance of RAMP against several baselines (SAT, AVG, MAX, MSD, E-AT) across multiple metrics: clean accuracy,  and robustness against l\u221e, l2, and l1 attacks, and union accuracy (robustness against all three attacks simultaneously). The results show that RAMP achieves the highest union accuracy while maintaining good clean accuracy, demonstrating its ability to mitigate the trade-offs among different attacks and between accuracy and robustness.", "section": "Adversarial training from random initialization"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_8_3.jpg", "caption": "Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.", "description": "This table presents a comparison of different models' robustness against common image corruptions and unseen adversarial attacks.  The \"Common Corruptions\" column lists various types of image corruptions, with each column showing the accuracy of the model on that specific corruption.  The \"Avg\" column provides the average accuracy across all corruptions. The \"Union\" column shows the union accuracy across multiple unseen adversarial attacks.  The table compares various models, including \u21131-AT, \u21132-AT, \u2113\u221e-AT, Winninghand, E-AT, MAX, and RAMP.  Winninghand is included as a state-of-the-art model for handling natural corruptions.", "section": "Universal Robustness"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_9_1.jpg", "caption": "Table 5: RAMP with l\u221e, l1, l2-RN-18-AT models on CIFAR-10 with standard epsilons.", "description": "This table shows the results of fine-tuning with RAMP using pre-trained ResNet-18 models with different lp-norms (l\u221e, l1, l2) on the CIFAR-10 dataset.  The table presents the clean accuracy and robust accuracy against different lp attacks (l\u221e, l1, l2) as well as the union accuracy, which represents the robustness against all three perturbation types simultaneously.  The results highlight RAMP's effectiveness in improving the robustness against multiple perturbations.", "section": "4.1 Logit Pairing for Multiple Perturbations"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_18_1.jpg", "caption": "Table 6: Estimations the actual values of terms ED ||ga - 9a|| (variance), ||ga \u2013 \u00cen || (bias), \u03c4, and \u2206\u00b2AT \u2013 \u2206\u00b2GP (error differences) across different epochs.", "description": "This table presents the estimated values of variance, bias, and the angle between natural and adversarial gradients, along with the calculated difference between the squared errors of adversarial training and gradient projection at various epochs (5, 10, 15, 20, and 60). These values are used to support the theoretical analysis demonstrating the superiority of gradient projection in balancing accuracy and robustness.", "section": "4.3 Theoretical Analysis of GP for Adversarial Robustness"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_19_1.jpg", "caption": "Table 7: Analysis of time per epoch for RAMP and related baselines. RAMP is more expensive than E-AT and less expensive than MAX.", "description": "This table presents a runtime analysis comparing RAMP against E-AT and MAX.  It shows the time (in seconds) taken per epoch for training different models on CIFAR-10 and ImageNet datasets. The results demonstrate that RAMP's computational cost is higher than E-AT but lower than MAX, offering a balance between efficiency and performance improvements.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_20_1.jpg", "caption": "Table 8: Accuracy against common corruptions using ResNet-18 on CIFAR-10 dataset.", "description": "This table presents the accuracy of three different models (E-AT, MAX, and RAMP) against common corruptions on the CIFAR-10 dataset using the ResNet-18 architecture.  It showcases the models' robustness to image corruptions.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_20_2.jpg", "caption": "Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.", "description": "This table shows the performance of different models (l1-AT, l2-AT, l\u221e-AT, Winninghand, E-AT, MAX, and RAMP) on CIFAR-10 dataset in terms of their accuracy against common corruptions and unseen adversaries.  The common corruptions are evaluated across five severity levels (fog, snow, gabor, elastic, jpeg compression).  Unseen adversaries represent different types of attacks not seen during training. The table provides individual accuracies for each corruption type and adversary, the average accuracy across all corruptions/adversaries, and the union accuracy (which represents robustness against the union of all corruptions and adversaries).", "section": "Universal Robustness"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_20_3.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table shows the results of experiments comparing RAMP with E-AT and MAX on CIFAR-10 using different epsilon values for l1, l2, and l\u221e attacks.  The results demonstrate the consistent superior performance of RAMP across various settings, particularly in achieving higher union accuracy (robustness against multiple perturbation types). The change in the key tradeoff pair (the pair of attacks showing the strongest trade-off) is also highlighted, emphasizing RAMP's adaptability to different adversarial attack scenarios.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_20_4.jpg", "caption": "Table 2: Robust fine-tuning on larger models and datasets (* uses extra data for pre-training). We evaluate all CIFAR-10 and Imagenet test points. RAMP consistently achieves better union accuracy with significant margins and good accuracy-robustness tradeoff.", "description": "This table presents the results of robust fine-tuning on larger models and datasets, comparing the performance of RAMP against other methods.  The models are fine-tuned using additional data for pre-training in some cases.  The table evaluates on all CIFAR-10 and ImageNet test points and shows RAMP achieves better union accuracy and accuracy-robustness tradeoff. Results are given for clean accuracy, robustness against l\u221e, l2, and l1 attacks, and union accuracy.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_21_1.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table compares the performance of RAMP against E-AT and MAX under different epsilon values for both training from scratch and robust fine-tuning scenarios.  It demonstrates RAMP's superior performance in terms of clean accuracy, l\u221e, l2, l1 accuracies, and union accuracy across various epsilon configurations.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_21_2.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table shows the results of experiments conducted using different epsilon values for l1, l2, and l\u221e attacks.  It compares the performance of RAMP against E-AT and MAX methods for training deep neural networks (DNNs) from scratch and robust fine-tuning. The key tradeoff pair refers to the strongest attack among the three types of attacks (l1, l2, l\u221e) that have the lowest robustness against other attack types. The table highlights how RAMP consistently outperforms the other two methods across different scenarios.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_21_3.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table shows the performance comparison of RAMP, E-AT, and MAX under different epsilon values for both training from scratch and robust fine-tuning scenarios.  The results demonstrate that RAMP consistently outperforms the other methods in terms of clean accuracy,  l\u221e, l2, l1 accuracy, and especially union accuracy, which is a key metric for multi-perturbation robustness.  The table highlights the impact of changing the key trade-off pair (the two attacks with the lowest robustness against each other) on model performance.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_21_4.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table presents a comparison of the performance of RAMP, E-AT, and MAX under different epsilon values for both training from scratch and robust fine-tuning. It demonstrates that RAMP consistently outperforms the other two methods across various settings, highlighting its effectiveness in enhancing robustness against multiple perturbations.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_21_5.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table shows the results of the RAMP model compared to E-AT and MAX models for different epsilon values.  It demonstrates that RAMP consistently achieves better performance across various metrics, including clean accuracy, robustness against different attacks, and union accuracy, regardless of the specific epsilon values used or whether the model is trained from scratch or fine-tuned.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_21_6.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table presents the results of experiments comparing the performance of RAMP against E-AT and MAX under different epsilon values for both training from scratch and robust fine-tuning. The key tradeoff pair is identified using a heuristic based on the single-norm robustness of adversarially trained models, where the two models with the lowest robustness against themselves are selected. The results show that RAMP achieves consistently better union accuracy, especially when the key tradeoff pair changes.  The table highlights the superior performance of RAMP across various scenarios.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_22_1.jpg", "caption": "Table 3: RN-18 model trained from random initialization on CIFAR-10 over 5 trials: RAMP achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein [2022].", "description": "This table shows the results of adversarial training from random initialization on CIFAR-10 using the ResNet-18 model.  The table compares the performance of RAMP against several baselines (SAT, AVG, MAX, MSD, E-AT) across multiple metrics including clean accuracy, and robustness against various L-p attacks, culminating in a final union accuracy score.  The results highlight that RAMP achieves the best union accuracy with good clean accuracy, indicating its effectiveness in mitigating the trade-offs between accuracy and robustness against multiple perturbations.", "section": "Adversarial training from random initialization"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_22_2.jpg", "caption": "Table 3: RN-18 model trained from random initialization on CIFAR-10 over 5 trials: RAMP achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein [2022].", "description": "This table presents the results of adversarial training from random initialization on CIFAR-10 using the PreAct ResNet-18 model.  The table compares the performance of RAMP against several baselines (SAT, AVG, MAX, MSD, E-AT) across five trials.  The metrics reported include clean accuracy, robust accuracy against l\u221e, l2, and l1 attacks, and the union accuracy (a measure of robustness against multiple attack types). RAMP demonstrates superior union robustness with good clean accuracy.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_22_3.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table compares the performance of RAMP, E-AT, and MAX across different epsilon values for both training from scratch and robust fine-tuning.  The results show that RAMP achieves higher clean accuracy, individual robustness against different lp-norms, and overall union accuracy, which demonstrates its effectiveness in boosting robustness across multiple perturbation types.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_22_4.jpg", "caption": "Table 1: Different epsilon values: RAMP consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.", "description": "This table presents a comparison of the performance of RAMP, E-AT, and MAX across different settings of epsilon values for L1, L2, and L\u221e attacks.  The results are split into two parts, one for models trained from scratch and another for models that underwent robust fine-tuning.  The key takeaway is that RAMP consistently shows better performance across all scenarios and demonstrates its superiority in handling multiple perturbation types.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_22_5.jpg", "caption": "Table 22: RAMP fine-tuning results of different logit pairing losses using PreAct-ResNet-18 on CIFAR-10.", "description": "This table shows the results of robust fine-tuning experiments using the RAMP framework with three different logit pairing loss functions: KL divergence, Mean Squared Error (MSE), and Cosine Similarity.  The results are compared in terms of clean accuracy and robustness against three different adversarial attack types (l\u221e, l2, l1), along with overall union accuracy.  This helps to analyze the effect of different logit pairing strategies on the model's performance.", "section": "B.5 Different Logit Pairing Methods"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_23_1.jpg", "caption": "Table 23: WideResNet-28-10 trained from random initialization on CIFAR-10. RAMP outperforms E-AT on union accuracy with our implementation.", "description": "This table compares the performance of different methods on the WideResNet-28-10 model trained from random initialization on the CIFAR-10 dataset.  The methods compared include E-AT with and without the trades loss, and RAMP with and without the trades loss. The table shows the clean accuracy,  accuracy against individual attacks ($l_\\infty$, $l_2$, $l_1$), and union accuracy.  The results demonstrate that RAMP, particularly with the trades loss, achieves superior union accuracy compared to E-AT.", "section": "B.6 AT from Scratch Using WideResNet-28-10"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_23_2.jpg", "caption": "Table 3: RN-18 model trained from random initialization on CIFAR-10 over 5 trials: RAMP achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein [2022].", "description": "This table presents the results of adversarial training from random initialization on CIFAR-10 using the PreAct ResNet-18 model.  It compares the performance of RAMP against several baselines (SAT, AVG, MAX, MSD, E-AT) across five trials, evaluating clean accuracy, robustness against l\u221e, l2, and l1 attacks, and union accuracy (robustness against all three attack types simultaneously). The results show that RAMP achieves the best union robustness and good clean accuracy, indicating its ability to mitigate the tradeoffs among perturbations and robustness/accuracy in this scenario.", "section": "5.1 Main Results"}, {"figure_path": "u1Z3HWz4VJ/tables/tables_24_1.jpg", "caption": "Table 25: Fine-tuning with more epochs: RAMP consistently outperforms E-AT on union accuracy. E-AT results are from Croce and Hein [2022].", "description": "This table shows the results of robust fine-tuning experiments using PreAct ResNet-18 model on CIFAR-10 dataset with varying numbers of epochs (5, 7, 10, and 15).  The table compares the performance of RAMP against E-AT (Extreme Adversarial Training) in terms of clean accuracy and union accuracy. Union accuracy is a measure of robustness across multiple adversarial attacks (l1, l2, and l\u221e).  The results demonstrate that RAMP consistently achieves higher union accuracy than E-AT, with the improvement becoming more pronounced as the number of epochs increases.", "section": "B.7 Robust Fine-tuning Using PreAct-ResNet-18"}]