{"references": [{"fullname_first_author": "J. Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper introduces the concept of reinforcement learning from human feedback (RLHF), a crucial technique for aligning language models and a core topic of the current paper."}, {"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper is highly influential in demonstrating the effectiveness of RLHF in improving instruction-following abilities of large language models, a key area the current paper examines and builds upon."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-00", "reason": "This paper presents a more efficient algorithm for RLHF called Direct Preference Optimization (DPO), which is directly compared to and contrasted with in the current paper."}, {"fullname_first_author": "H. Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-00", "reason": "This paper introduces the LLaMA model family, which serves as a basis for many of the open-source LLMs used in the experiments of the current paper."}, {"fullname_first_author": "W-L. Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality", "publication_date": "2023-04-14", "reason": "This paper introduces the Vicuna model, a significant open-source LLM used in the current paper's experiments and is important in setting the background of open-source LLMs."}]}