[{"type": "text", "text": "Learning on Large Graphs using Intersecting Communities ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ben Finkelshtein I\u02d9smail I\u02d9lkan Ceylan University of Oxford University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Michael Bronstein Ron Levie University of Oxford / AITHYRA Technion -\u2013 Israel Institute of Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each node\u2019s representation in an input graph by aggregating messages from the node\u2019s neighbors, which necessitates a memory complexity of the order of the number of graph edges. This complexity might quickly become prohibitive for large graphs provided they are not very sparse. In this paper, we propose a novel approach to alleviate this problem by approximating the input graph as an intersecting community graph (ICG) \u2013 a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph does not depend on the graph size. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the number of nodes (rather than edges). This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The vast majority of graph neural networks (GNNs) learn representations of graphs based on the message passing paradigm [22], where every node representation is synchronously updated based on an aggregate of messages flowing from its neighbors. This mode of operation hence assumes that the set of all graph edges are loaded in the memory, which leads to a memory complexity bottleneck. Specifically, considering a graph with $N$ nodes and $E$ edges, message-passing leads to a memory complexity that is linear in the number of edges, which quickly becomes prohibitive for large graphs especially when $E\\gg N$ . This limitation motivated a body of work with the ultimate goal of making the graph machine learning pipeline amenable to large graphs [24, 12, 13, 7, 65]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we take a drastically different approach to alleviate this problem and design a novel graph machine learning pipeline for learning over large graphs with memory complexity linear in the number of nodes. At the heart of our approach lies a graph approximation result which informally states the following: every undirected graph with node features can be represented by a linear combination of intersecting communities (i.e., cliques) such that the number of communities required to achieve a certain approximation error is independent of the graph size for dense graphs. Intuitively, this results allows us to utilize an \u201capproximating graph\u201d, which we call intersecting community graph (ICG), for learning over large graphs. Based on this insight \u2014 radically departing from the standard message-passing paradigm \u2014 we propose a new class of neural networks acting on the ICG and on the set of nodes from the original input graph, which leads to an algorithm with memory and runtime complexity that is linear in the number nodes. ", "page_idx": 0}, {"type": "text", "text": "Our analysis breaks the traditional dichotomy of approaches that are appropriate either for small and dense graphs or for large and sparse graphs. To our knowledge, we present the first rigorously motivated approach allowing to efficiently handle graphs that are both large and dense. The main advantage lies in being able to process a very large non-sparse graph without excessive memory and runtime requirements. The computation of ICG requires linear time in the number of edges, but this is an offilne pre-computation that needs to be performed once. After constructing the ICG, learning to solve the task on the graph \u2014 the most computation demanding aspect \u2014 is very efficient, since the architecture search and hyper-parameter optimization is linear in the number of nodes. ", "page_idx": 1}, {"type": "text", "text": "Context. The focus of our study is graph-signals: undirected graphs with node features, where the underlying graph is given and fixed. This is arguably the most common learning setup for large graphs, including tasks such as semi-supervised (transductive) node classification. As we allow varying features, our approach is also suitable for spatiotemporal tasks on graphs, where a single graph is given as a fixed domain on which a dynamic process manifests as time-varying node features. Overall, our approach paves the way for carrying out many important learning tasks on very large and relatively dense graphs such as social networks that typically have $10^{8}\\sim10^{9}$ nodes and average node degree of $10^{2}\\stackrel{\\cdot}{\\sim}1\\dot{0}^{3}$ [19] and do not fit into GPU memory. ", "page_idx": 1}, {"type": "text", "text": "Challenges and techniques. There are two fundamental technical challenges to achieve the desired linear complexity, which we discuss and elaborate next. ", "page_idx": 1}, {"type": "text", "text": "How to construct the ICG efficiently with formal approximation guarantees? Our answer to this question rests on an adaptation of the Weak Regularity Lemma [20, 38] which proves the existence of an approximating ICG in cut metric \u2013 a well-known graph similarity meausre1. Differently from the Weak Regularity Lemma, however, we need to construct an approximating ICG. Directly minimizing the cut metric is numerically unstable and hard to solve2. We address this in Theorem 3.1, by proving that optimizing the error in Frobenius norm, a much easier optimization target, guarantees a small cut metric error. Hence, we can approximate uniformly \u201cgranular\u201d adjacency matrices by low-rank ICGs in cut metric. Uniformity here means that the number of communities $K$ of the ICG required for the error tolerance $\\epsilon$ is $K=\\epsilon^{-2}$ , independently of on any property of the graph, not even its size $N$ .3 This result allows us to design an efficient and numerically stable algorithm. Figure 1 shows an adjacency matrix with an inherent intersecting community structure, and our approximated ICG, which captures the statistics of edge densities, ignoring their granularity. ", "page_idx": 1}, {"type": "text", "text": "How to effectively use ICG for efficient graph learning? We present a signal processing framework and deep neural networks based on the ICG representation (ICG-NN). As opposed to message-passing networks that require $\\mathcal{O}(E)$ memory and time complexity, ICG-NNs operate in only $\\mathcal{O}(N)$ complexity. They involve node-wise operations to allow processing the small-scale granular behavior of the node features, and community operations that account for the large-scale structure of the graph. Notably, since communities have large supports, ICG-NN can propagate information between far-apart nodes in a single layer. ", "page_idx": 1}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/fb18af4bcd45205da939c7cf09a00c2049b2aa79d749858ca8bab3c1dfaa315f.jpg", "img_caption": ["Figure 1: Top: adjacency matrix of a simple graph. Bottom: approximating 5 community ICG. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present a novel graph machine learning approach for large and non-sparse graphs which enables an $\\mathcal{O}(N)$ learning algorithm on ICG with one-off $\\mathcal{O}(E)$ pre-processing cost for constructing ICG.   \n\u2022 Technically, we prove a constructive version of Weak Regularity Lemma in Theorem 3.1 to efficiently obtain ICGs, which could be of independent theoretical interest.   \n\u2022 We introduce ICG neural networks as an instance of a signal processing framework, which applies neural networks on nodes and on ICG representations .   \n\u2022 Empirically, we validate our theoretical findings on semi-supervised node classification and on spatio-temporal graph tasks, illustrating the scalability of our approach, which is further supported by competitive performance against strong baseline models. ", "page_idx": 1}, {"type": "text", "text": "2 A primer on graph-signals and the cut-metric ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Graph signals. We are interested in undirected (weighted or simple), featured graphs $G=({\\mathcal{N}},{\\mathcal{E}},\\mathbf{S})$ , where $\\mathcal{N}$ denotes the set of nodes, $\\mathcal{E}$ denotes the set of edges, and S denotes the node feature matrix, also called the signal. We will write $N$ to represent the number of nodes and $E$ to represent the number of edges. For ease of notation, we will assume that the nodes are indexed by $\\left[N\\right]\\;=\\;$ $\\{1,\\ldots,N\\}$ . Using the graph signal processing convention, we represent the graphs in terms of a graph-signal $G=(A,S)$ , where $\\bar{\\pmb{A}}=(a_{i,j})_{i,j=1}^{\\breve{N}}\\in\\mathbb{R}^{N\\times N}$ is the adjacency matrix, and ${\\boldsymbol{S}}=$ $(s_{i,j})_{i\\in[N],j\\in[d]}\\in\\mathbb{R}^{N\\times D}$ is the $D$ -channel signal. We assume that the graph is undirected, meaning that $\\pmb{A}$ is symmetric. The graph may be unweighted (with $a_{i,j}\\in\\{0,1\\})$ or weighted (with $a_{i,j}\\in$ $[0,1])$ . We additionally denote by $s_{n}^{\\top}$ the $n$ -th row of $\\boldsymbol{S}$ , and denote $\\pmb{S}=(\\pmb{s}_{n})_{n=1}^{N}$ . By abuse of notation, we also treat a signal $\\boldsymbol{S}$ as the function $\\boldsymbol{S}:[N]\\rightarrow\\mathbb{R}^{D}$ with $S(n)\\,=\\,\\pmb{s}_{n}$ . We suppose that all signals are standardized to have values in $[0,1]$ . In general, any matrix (or vector) and its entries are denoted by the same letter, and vectors are seen as columns. The $i$ -th row of the matrix $\\pmb{A}$ is denoted by $A_{i,:}$ , and the $j$ -th column by $A_{:,j}$ . The Frobenius norm of a square matrix $M\\,\\in\\,\\mathbb{R}^{N\\times N}$ is defined by $\\begin{array}{r}{\\left\\|M\\right\\|_{\\mathrm{F}}\\,=\\,\\sqrt{\\frac{1}{N^{2}}\\sum_{n,m=1}^{N}\\left|b_{n,m}\\right|^{2}}}\\end{array}$ , and for a signal $\\boldsymbol{S}\\,\\in\\,\\mathbb{R}^{N\\times D}$ by $\\begin{array}{r}{\\left\\|\\boldsymbol{S}\\right\\|_{\\mathrm{F}}=\\sqrt{\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{j=1}^{D}\\left|s_{n,j}\\right|^{2}}}\\end{array}$ . The Frobenius norm of a matrix-signal, with weights $a,b>0$ is defined to be $\\left\\Vert(M,S)\\right\\Vert_{\\mathrm{F}}:=\\sqrt{a\\left\\Vert M\\right\\Vert_{\\mathrm{F}}^{2}+b\\left\\Vert S\\right\\Vert_{\\mathrm{F}}^{2}}$ . We define the degree of a weighted graph as $\\deg(A):=N^{2}\\left\\|A\\right\\|_{\\mathrm{F}}^{2}$ . In the special case of an unweighted graph we get $\\deg(A)=E$ . The pseudoinverse of a full rank matrix $M\\,\\in\\,\\mathbb{R}^{N\\times K}$ , where $N\\,\\geq\\,K$ , is $M^{\\dagger}\\,=\\,(M^{\\top}M)^{-1}M^{\\top}$ . Cut-metric. The cut metric is a graph similarity, based on the cut norm. We show here the definitions for graphs of the same size; the extension for arbitrary pairs of graphs is based on graphons (see Appendix B.2). The definition of matrix cut norm is well-known, and we normalize it by a parameter $E\\in\\mathbb{N}$ that indicates the characteristic number of edges of the graphs of interest, so cut norm remains within a standard range. The definition of cut norm of signals is taken from [33]. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. The matrix cut norm of $M\\in\\mathbb{R}^{N\\times N}$ , normalized by $E_{i}$ , is defined to be ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{M}\\right\\|_{\\Omega}=\\left\\|\\boldsymbol{M}\\right\\|_{\\Omega;N,E}:=\\frac{1}{E}\\operatorname*{sup}_{u,\\nu\\subset[N]}\\bigg|\\sum_{i\\in\\mathcal{U}}\\sum_{j\\in\\mathcal{V}}m_{i,j}\\bigg|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The signal cut norm of $\\boldsymbol{Z}\\in\\mathbb{R}^{N\\times D}$ is defined to be ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\|Z\\right\\|_{\\Omega}=\\left\\|Z\\right\\|_{\\Omega;N}:=\\frac{1}{D N}\\sum_{j=1}^{D}\\operatorname*{sup}_{w\\subset[N]}\\bigg|\\sum_{i\\in w}z_{i,j}\\bigg|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The matrix-signal cut norm of $(M,Z)$ , with weights $\\alpha>0$ and $\\beta>0$ s.t. $\\alpha+\\beta=1$ , is defined to be ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(M,\\pmb{Z})\\|_{\\Omega}=\\|(\\pmb{M},\\pmb{Z})\\|_{\\Omega;N,E}:=\\alpha\\,\\|\\pmb{M}\\|_{\\Omega;N,E}+\\beta\\,\\|\\pmb{Z}\\|_{\\Omega;N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given two graphs $A,A^{\\prime}$ , their distance in cut metric is the cut norm of their difference, namely ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\pmb{A}-\\pmb{A}^{\\prime}\\|_{\\Omega}=\\frac{1}{E}\\operatorname*{sup}_{u,\\nu\\subset[N]}\\Big|\\sum_{i\\in\\mathcal{U}}\\sum_{j\\in\\mathcal{V}}(a_{i,j}-a_{i,j}^{\\prime})\\Big|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The right-hand-side of (4) is interpreted as the difference between the edge densities of $\\pmb{A}$ and $A^{\\prime}$ on the block on which these densities are the most dissimilar. Hence, cut-metric has a probabilistic interpretation. Note that for simple graphs, $A-A^{\\prime}$ is a granular function: it has many jumps between the values $-1,\\,0$ and 1. The fact that the absolute value in (4) is outside the integral, as opposed to being inside like in $\\ell_{1}$ norm, means that cut metric has an averaging effect, which mitigates the granularity of $A-A^{\\prime}$ . The graph-signal cut metric is similarly defined to be $\\|(A,S)-(\\bar{A^{\\prime}},S^{\\prime})\\|_{\\Omega}$ . ", "page_idx": 2}, {"type": "text", "text": "Cut metric in graph machine learning. The cut metric has the following interpretation: two (deterministic) graphs are close to each other in cut metric iff they look like random graphs sampled from the same stochastic block model. The interpretation has a precise formulation in view of the Weak Regularity Lemma [20, 38] discussed below. This makes the cut metric a natural notion of similarity for graph machine learning, where real-life graphs are noisy, and can describe the same underlying phenomenon even if they have different sizes and topologies. Moreover, [33] showed that ", "page_idx": 2}, {"type": "text", "text": "GNNs with normalized sum aggregation cannot separate graph-signals that are close to each other in cut metric. In fact, cut metric can separate any non-isomorphic graphons [37], so it has sufficient discriminative power to serve as a graph similarity measure in graph machine learning problems. As opposed to past works that used the cut norm to derive theory for existing GNNs, e.g., [46, 40, 33], we use cut norm to derive new methods for a class of problems of interest. Namely, we introduce a new theorem about cut norm \u2013 the constructive weak regularity lemma (Theorem 3.1) \u2013 and use it to build new algorithms on large non-sparse graphs. ", "page_idx": 3}, {"type": "text", "text": "3 Approximation of graphs by intersecting communities ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Intersecting community graphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a graph $G$ with $N$ nodes, for any subset of nodes $\\mathcal{U}\\subset[N]$ , denote by $\\mathbb{L}_{\\mathcal{U}}$ its indicator (i.e., $\\bar{\\mathbb{L}}\\bar{\\mathcal{U}}^{(i)}=\\bar{\\mathbb{1}}$ if $i\\in\\mathcal{U}$ and zero otherwise). We define an intersecting community graph-signal (ICG) with $K$ classes ( $K$ -ICG) as a low rank graph-signal $(C,P)$ with adjacency matrix and signals given respectively by ", "page_idx": 3}, {"type": "equation", "text": "$$\nC=\\sum_{j=1}^{K}r_{j}\\mathbb{1}_{\\mathcal{U}_{j}}\\mathbb{1}_{\\mathcal{U}_{j}}^{\\top},\\qquad P=\\sum_{j=1}^{K}\\mathbb{1}_{\\mathcal{U}_{j}}f_{j}^{\\top}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r_{j}\\in\\mathbb{R}$ , $f_{j}\\in\\mathbb{R}^{D}$ , and $\\mathcal{U}_{j}\\subset[N]$ . In order to allow efficient optimization algorithms, we relax the $\\{0,\\ensuremath{\\mathrm{i}}\\}$ -valued hard affliiation functions ${\\mathbb L}_{\\mathcal{U}}$ to functions that can assign soft affiliation values in $\\mathbb{R}$ . Letting $\\chi$ denote the set of all indicator functions of the form $\\mathbb{I}_{\\mathcal{U}}$ , for subsets $\\mathcal{U}\\in[N]$ , we can formally define soft affiliation models as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. A set $\\mathcal{Q}$ of functions $q:[N]\\to\\mathbb{R}$ that contains $\\chi$ is called $a$ soft affiliation model. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2. Let $d\\in\\mathbb{N}$ , and let $\\mathcal{Q}$ be a soft affliiation model. We define $[\\mathcal{Q}]\\subset\\mathbb{R}^{N\\times N}\\times\\mathbb{R}^{N\\times D}\\;t o$ be the set of all elements of the form $(r q q^{\\top},q f^{\\top})$ , with $\\mathbf{\\Delta}q\\in{\\mathcal{Q}},$ , $r\\in\\mathbb{R}$ and $\\pmb{f}\\in\\mathbb{R}^{D}$ . We call $[\\mathcal{Q}]$ the soft rank-1 intersecting community graph (ICG) model corresponding to $\\mathcal{Q}$ . Given $K\\in\\mathbb{N}$ , the subset $[\\mathcal{Q}]_{K}$ of $\\mathbb{R}^{N\\times N}\\times\\breve{\\mathbb{R}}^{N\\times D}$ of all linear combinations of $K$ elements of $[\\mathcal{Q}]$ is called the soft rank- $K$ ICG model corresponding to $\\mathcal{Q}$ . ", "page_idx": 3}, {"type": "text", "text": "ICG models can be written in matrix form as follows. Given $K,D\\in\\mathbb{N}$ , any intersecting community graph-signal $(C,P)\\,\\in\\,\\mathbb{R}^{N\\times N}\\,\\times\\,\\mathbb{R}^{N\\times D}$ in $[\\mathcal{Q}]_{K}$ can be represented by a triplet of community affliiation matrix $Q\\stackrel{^{.}}{\\in}\\mathbb{R}^{N\\times K}$ , community magnitude vector $\\pmb{r}\\in\\mathbb{R}^{K}$ , and community feature matrix F \u2208RK\u00d7D. In the matrix form, (C, P ) \u2208[Q]K if and only if it has the form ", "page_idx": 3}, {"type": "equation", "text": "$$\nC=Q\\operatorname{diag}(r)Q^{\\top}\\quad{\\mathrm{and}}\\quad P=Q F,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{diag}(r)$ is the diagonal matrix in $\\mathbb{R}^{K\\times K}$ with $\\pmb{r}$ as its diagonal elements. ", "page_idx": 3}, {"type": "text", "text": "3.2 The Constructive Graph-Signal Weak Regularity Lemma ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The following theorem is a semi-constructive version of the Weak Regularity Lemma for intersecting communities. It is semi-constructive in the sense that the approximating graphon is not just assumed to exist, but is given as the result of an optimization problem with an \u201ceasy to work with\u201d loss, namely, the Frobenius norm error. The theorem also extends the standard weak regularity lemma by allowing soft affiliations to communities instead of hard affiliations. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Let $(A,S)$ be a $D$ -channel graph-signal of $N$ nodes, where $\\deg(A)\\,=\\,E^{\\prime}$ . Let $K\\;\\in\\;\\mathbb{N},\\;\\delta\\;>\\;0,$ , and $\\mathcal{Q}$ be a soft affiliation model. Consider the matrix-signal cut norm with weights $\\alpha,\\beta\\ge0$ not both zero, and the matrix-signal Frobenius norm with weights $\\|(B,X)\\|_{\\mathrm{F}}:=$ $\\sqrt{\\alpha\\frac{N^{2}}{E}\\left\\|\\boldsymbol{B}\\right\\|_{\\mathrm{F}}^{2}+\\beta\\left\\|\\boldsymbol{X}\\right\\|_{\\mathrm{F}}^{2}}$ . Let m be sampled uniformly from $[K]$ , and let $R\\geq1$ such that $K/R\\in\\mathbb{N}$ . Then, in probability $\\textstyle1-{\\frac{1}{R}}$ (with respect to the choice of $m$ ), for every $(C^{*},P^{*})\\in[\\mathcal{Q}]_{m},$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{i f}&{{}\\|(A,S)-(C^{*},P^{*})\\|_{F}\\leq(1+\\delta)\\operatorname*{min}_{(C,P)\\in[\\mathcal{Q}]_{m}}\\|(A,S)-(C,P)\\|_{\\mathrm{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\nt h e n~~~~\\|({\\cal A},S)-(C^{*},P^{*})\\|_{\\boxed{\\Omega;N,E^{\\prime}}}\\leq\\frac{3N}{2\\sqrt{E^{\\prime}}}\\sqrt{\\frac{R}{K}+\\delta}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof of Theorem 3.1 is given in Appendix B, where the theorem is extended to graphon-signals. Theorem 3.1 means that we need not consider a complicated algorithm for estimating cut distance. Instead, we can optimize the Frobenius norm, which is much more direct (see Section 4). The term $\\delta$ describes the stability of the approximation, where a perturbation in $(C^{*},P^{*})$ that corresponds to a small relative change in the optimal Frobenius error, leads to a small additive error in cut metric. ", "page_idx": 4}, {"type": "text", "text": "3.3 Approximation capabilities of ICGs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "ICGs vs. Message Passing. Until the rest of the paper we suppose for simplicity that the degree of the graph $E^{\\prime}$ is equal to the number of edges $E$ up to a constant scaling factor $0\\,<\\,\\gamma\\,\\leq\\,1$ , namely $E^{\\prime}=\\gamma E$ . Note that this is always true for unweighted graphs, where $\\gamma=1$ . The bound in Theorem 3.1 is closer to be uniform with respect to $N$ the denser the graph is (the closer $E$ is to $N^{2}$ ). Denote the average node degree by $\\mathrm{d}=E/N$ . To get a meaningful bound in (6), we must choose $K>N^{2}/E$ . On the other hand, for signal processing on the ICG to be more efficient than message passing, we require $K<\\mathrm{d}$ . Combining these two bounds, we see that ICG signal processing is guaranteed to be more efficient than message passing for any graph in the semi-dense regime: $\\mathrm{d}^{2}>N$ (or equivalently $E>N^{3/2}$ ). ", "page_idx": 4}, {"type": "text", "text": "Essential tightness of Theorem 3.1. In [2, Theorem 1.2] it was shown that the bound (6) is essentially tight in the following sense4. There exists a universal constant $c$ (greater than $1/2312)$ such that for any $N$ there exists an unweighted graph $A\\in\\mathbb{R}^{N\\times N}$ with $\\deg(A)=E$ , such that any $B\\in\\mathbb{R}^{N\\times N}$ that approximates $\\pmb{A}$ with error $\\|A-B\\|_{\\boxed{\\Omega;N,E}}\\leq16$ must have rank greater than $c\\frac{N^{2}}{E}$ . In words, there are graphs of $E$ edges that we cannot approximate in cut metric by any ICG with $K\\ll\\frac{N^{2}}{E}$ Hence, the requirement $K\\gg N^{2}/E^{\\prime}$ for a small cut metric error in (6) for any graph is tight. ", "page_idx": 4}, {"type": "text", "text": "ICGs approximations of sparse graphs. In practice, many natural sparse graphs are amenable to low rank intersecting community approximations. For example, a simplistic model for how social networks are formed states that people connect according to shared characteristics (i.e. intersecting communities), like hobbies, occupation, age, etc [61]. Moreover, since ICGs can have negative community magnitudes $r_{k}$ , one can also construct heterophilic components (bipartite substructures) of graphs with $\\mathrm{ICGs}^{5}$ . Hence, a social network can be naturally described using intersecting communities, even with $K<N^{2}/E$ . For such graphs, ICG approximations are more accurate than their theoretical bound (6). In addition, Figure 5 in Appendix F.5 shows that in practice the Frobenius local minimizer, which does not give a small Frobenius error, guarantees small cut metric error even if $K<N^{2}/E$ This means that Frobenius minimization is a practical approach for ftiting ICGs also for sparse natural graphs. Moreover, note that in the experiments in Tables 2 and 1 we take $K<N^{2}/E$ and still get competitive performance with SOTA message passing methods. ", "page_idx": 4}, {"type": "text", "text": "4 Fitting ICGs to graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let us fix the soft affiliation model to be all vectors $\\pmb q\\in[0,1]^{N}$ . In this subsection, we propose algorithms for fitting ICGs to graphs based on Theorem 3.1. ", "page_idx": 4}, {"type": "text", "text": "4.1 Fitting an ICG to a graph with gradient descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In view of Theorem 3.1, to fit a soft rank- $\\mathcal{K}$ ICG to a graph in cut metric, we learn a triplet $Q\\in[0,1]^{N\\times K}$ , ${\\pmb r}\\in\\mathbb{R}^{K}$ and $\\pmb{F}\\in\\mathbb{R}^{K\\times D}$ that minimize the Frobenius loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\boldsymbol{Q},\\boldsymbol{r},\\boldsymbol{F})=\\left\\lVert\\boldsymbol{A}-\\boldsymbol{Q}\\,\\mathrm{diag}(\\boldsymbol{r})\\boldsymbol{Q}^{\\top}\\right\\rVert_{\\mathrm{F}}^{2}+\\lambda\\left\\lVert\\boldsymbol{S}-\\boldsymbol{Q}\\boldsymbol{F}\\right\\rVert_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda\\in\\mathbb R$ is a scalar that balances the two loss terms. To implement $Q\\in[0,1]^{N\\times K}$ in practice, we define $Q={\\mathrm{Sigmoid}}(R)$ for a learned matrix ${\\pmb R}\\in\\mathbb{R}^{N\\times K}$ . ", "page_idx": 4}, {"type": "text", "text": "Suppose that $\\pmb{A}$ is sparse with $K^{2}$ , $K\\mathrm{d}\\ll N$ . The loss (7) involves sparse matrices and low rank matrices. Typical time complexity of sparse matrix operations is $\\mathcal{O}(E)$ , and for rank- $K$ matrices it is $\\mathcal{O}(N K^{2})$ . Hence, we would like to derive an optimization procedure that takes $\\mathcal{O}(K^{2}N+E)$ operations. However, the first term of (7) involves a subtraction of the low rank matrix $Q\\,\\mathrm{diag}(r)Q^{\\top}$ from the sparse matrix $\\pmb{A}$ , which gives a matrix that is neither sparse nor low rank. Hence, a na\u00efve optimization procedure would take $\\mathcal{O}(N^{2})$ operations. The next proposition shows that we can write the loss (7) in a form that leads to a $\\mathcal{O}(K^{2}N+K E)$ time and $\\mathcal{O}(K N+E)$ space complexities. Proposition 4.1. Let $\\pmb{A}=(a_{i,j})_{i,j=1}^{N}$ be an adjacency matrix of a weighted graph with $E$ edges. The graph part of the Frobenius loss can be written as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|A-Q\\operatorname{diag}(r)Q^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\displaystyle\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathrm{Tr}\\left((Q^{\\top}Q)\\operatorname{diag}(r)(Q^{\\top}Q)\\operatorname{diag}(r)\\right)+\\left\\|A\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\frac{2}{N^{2}}\\sum_{i=1}^{N}\\sum_{j\\in N(i)}Q_{i,:}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}a_{i,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Computing the right-hand-side and its gradients with respect to $Q$ and $\\pmb{r}$ has a time complexity of $\\mathcal{O}(\\bar{K}^{2}N+K E)$ , and a space complexity of $\\mathcal{O}(K N+E)$ . ", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix C. We optimize $Q,r$ , and $\\pmb{F}$ using gradient descent (GD) on (7) implemented via Proposition 4.1. After convergence, we further refine $\\pmb{F}$ by setting ${\\pmb F}=Q^{\\dagger}{\\pmb S}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Initialization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Appendix $\\mathrm{D}$ we propose a good initialization for the GD minimization of (7). Suppose that $K$ is divisible by 3. Let $\\mathbf{\\bar{\\Phi}}_{K/3}\\ \\bar{\\in}\\ \\mathbb{R}^{N\\times K/3}$ be the matrix consisting of the leading eigenvectors of $\\pmb{A}$ as columns, and $\\mathbf{A}_{K/3}\\,\\in\\,\\mathbb{R}^{K/3\\times K/3}$ the diagonal matrix of the leading eigenvalues. For each eigenvector $\\phi$ with eigenvalue $\\lambda$ , consider the three soft indicators ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi_{1}=\\frac{\\phi_{+}}{\\|\\phi_{+}\\|_{\\infty}},\\quad\\phi_{2}=\\frac{\\phi_{-}}{\\|\\phi_{-}\\|_{\\infty}},\\quad\\phi_{3}=\\frac{\\phi_{+}+\\phi_{-}}{\\|\\phi_{+}+\\phi_{-}\\|_{\\infty}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with community affiliation magnitudes $\\begin{array}{r c l}{r_{1}}&{=}&{2\\lambda\\left\\lVert\\phi_{+}\\right\\rVert_{\\infty}^{2}}\\end{array}$ , $r_{2}\\;\\;=\\;\\;2\\lambda\\left\\|\\phi_{-}\\right\\|_{\\infty}^{2}$ and $r_{3}\\quad=$ $-\\lambda\\left\\|\\phi_{+}+\\phi_{-}\\right\\|_{\\infty}^{2}$ respectively. Here, $\\phi_{\\pm}$ is the positive or negative part of the vector. One can now show that the ICG $_{C}$ based on the $K$ soft affliiations corresponding to the leading $K/3$ eigenvectors approximates $\\pmb{A}$ in cut metric with error $\\mathcal{O}(K^{-1/2})$ . Note that computing the leading eigenvectors is efficient with any variant of the power iteration for sparse matrices, like any variant of Lanczos algorithm, which takes $\\mathcal{O}(E)$ operations per iteration, and requires just a few iteration due to its fast super-exponential convergence [47]. We moreover initialize $\\pmb{F}$ optimally by ${\\pmb F}=Q^{\\dagger}{\\pmb S}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Subgraph SGD for fitting ICGs to large graphs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In many situations, we need to process large graphs for which $E$ is too high to read to the GPU memory, but $N K$ is not. These are the situation where processing graph-signals using ICGs is beneficial. However, to obtain the ICG we first need to optimize the loss (7) using a message-passing type algorithm that requires reading $E$ edges to memory. To allow such processing, we next show that one can read the $E$ edges to shared RAM (or storage), and at each SGD step read to the GPU memory only a random subgraph with $M$ nodes. ", "page_idx": 5}, {"type": "text", "text": "At each interation, we sample $M\\ll N$ random nodes $n:=(n_{m})_{m=1}^{M}$ uniformly and independently from $[N]$ (with repetition). We construct the sub-graph $\\pmb{A}^{(n)}\\in\\mathbb{R}^{M\\times M}$ with entries $a_{i,j}^{(n)}=a_{n_{i},n_{j}}$ and the sub-signal $S^{(n)}\\in\\mathbb{R}^{M\\times K}$ with entries $\\pmb{s}_{i}^{(n)}=\\pmb{s}_{n_{i}}$ . We similarly define the sub-community affiliation matrix $Q^{(n)}\\in[0,1]^{M\\times K}$ with entries $q_{i,j}^{(n)}=q_{n_{i},j}$ . We consider the loss ", "page_idx": 5}, {"type": "equation", "text": "$$\nL^{(n)}(Q^{(n)},r,F)=\\left\\|A^{(n)}-Q^{(n)}\\operatorname{diag}(r)Q^{(n)\\top}\\right\\|_{\\mathrm{F}}^{2}+\\lambda\\left\\|S^{(n)}-Q^{(n)}\\operatorname{diag}(r)F\\right\\|_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which depends on all entries of $\\pmb{F}$ and $\\pmb{r}$ and on the $\\mathbfit{\\Delta}$ entries of $Q$ . Each SGD updates all of the entries of $\\pmb{F}$ and $\\pmb{r}$ , and the $\\mathbfit{\\Delta}$ entries of $Q$ by incrementing them with the respective gradients of $L^{(n)}(Q^{n},r,F)$ . Hence, $\\nabla_{Q}L^{(n)}(Q^{(n)},r,\\mathbf{\\dot{F}})$ may only be nonzero for entries $\\textstyle\\pmb q_{n}$ with $\\textit{n}\\in\\textbf{n}$ Proposition E.1 in Appendix $\\boldsymbol{\\mathrm E}$ shows that the gradients of ${\\boldsymbol{L}}^{(n)}$ approximate the gradients of $L$ More concretely, $\\begin{array}{r}{\\frac{M}{N}\\nabla_{Q^{(n)}}L^{(n)}\\approx\\nabla_{Q^{(n)}}L,\\nabla_{r}L^{(n)}\\approx\\nabla_{r}L}\\end{array}$ , and $\\bar{\\nabla}_{F}L^{(n)}\\approx\\nabla_{F}L.$ . Note that the stochastic gradients with respect to ${Q}^{(n)}$ approximate a scaled version of the full gradients. ", "page_idx": 5}, {"type": "text", "text": "5 Learning with ICG ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given a graph-signal $(A,S)$ and its approximating ICG $(C,P)$ , the corresponding soft affiliations $Q=(q_{k})_{k=1}^{K}$ represent intersecting communities that can describe the adjacency structure $\\pmb{A}$ and can account for the variability of $\\boldsymbol{S}$ . In a deep network architecture, one computes many latent signals, and these signals typically correspond in some sense to the structure of the data $(A,S)$ . It is hence reasonable to put a special emphasis on latent signals that can be described as linear combinations of $(\\pmb{q}_{k})_{k=1}^{K}$ . Next, we develop a signal processing framework for such signals. ", "page_idx": 6}, {"type": "text", "text": "Graph signal processing with ICG. We develop a signal processing approach with ${\\mathcal{O}}(N K)$ complexity for each elemental operation. Let $Q\\in\\mathbb{R}^{\\hat{N}\\times K}$ be a fixed community matrix. We call $\\mathring{\\mathbb{R}}^{N\\times D}$ the node space and $\\mathbb{R}^{K\\times D}$ the community space. We process signals via the following operations: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Synthesis is the mapping $F\\mapsto Q F$ from the community space to the node space, in $\\mathcal{O}(N K D)$ .   \n\u2022 Analysis is the mapping $S\\mapsto Q^{\\dagger}S$ from the node space to the community space, in $\\mathcal{O}(N K D)$ .   \n\u2022 Community processing refers to any operation that manipulates the community feature vector $\\pmb{F}$ (e.g., an MLP in which a linear operator requires $K D\\times K D$ parameters or a Transformer) in $\\bar{\\mathcal{O}(K^{2}D^{2})}$ operations.   \n\u2022 Node processing is any function $\\Theta$ that operates in the node space on nodes independently via $\\Theta(S):=(\\theta(\\pmb{s}_{n}))_{n=1}^{N}$ , where $\\theta:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D^{\\prime}}$ (e.g., an MLP) takes $\\mathcal{O}(D D^{\\prime})$ operations. Node processing has time complexity of $\\mathcal{O}(N D D^{\\prime})$ . ", "page_idx": 6}, {"type": "text", "text": "Analysis and synthesis satisfy the reconstruction formula $Q^{\\dagger}Q F=F$ . Moreover, $Q Q^{\\dagger}S$ is the projection of $\\boldsymbol{S}$ upon the space of linear combinations of communities $\\{Q F\\mid F\\in\\mathbb{R}^{K\\times D}\\}$ . Note that when $\\pmb{A}$ is not low rank, since $Q$ is almost surely full rank when initialized randomly, the optimal configuration of $Q$ would avoid having repetitions of communities, as this would unnecessarily reduce its rank. Therefore, $Q$ is typically full rank, and in this generic case $Q^{\\dagger}=(Q^{\\top}Q)^{-1}Q^{\\top}$ . ", "page_idx": 6}, {"type": "text", "text": "Deep ICG Neural Networks. In the following, we propose deep architectures based on the elements of signal processing with an ICG, which take $\\bar{\\mathcal{O}}(D(\\bar{N}\\bar{K}\\,{+}\\,K^{2}\\bar{D_{}}{+}\\,N D))$ operations at each layer. In comparison, simplified message passing layers (e.g., GCN and GIN), where the message is computed using just the feature of the transmitting node, have a complexity of $\\mathcal{O}(E D+\\bar{N D}^{2})$ . General message passing layers, which define messages via a function applied on the concatenated pair of node features of each edge, take $\\mathcal{O}(E D^{2})$ operations for MLP message functions. Therefore, ICG neural networks are more efficient than MPNNs if $K\\ll D\\mathrm{d}$ , where $\\mathrm{d}$ is the average node degree. ", "page_idx": 6}, {"type": "text", "text": "We denote by $D^{(\\ell)}$ the dimension of the node features at layer $\\ell$ and define the initial node representations $\\dot{H}^{(0)}=S$ and the initial community features $\\pmb{F}^{(0)}=\\pmb{Q}^{\\dagger}\\pmb{S}$ . The node features at layers $0\\leq\\ell\\leq L-1$ are defined by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{H}^{(\\ell+1)}=\\sigma\\left(\\pmb{H}^{(\\ell)}\\pmb{W}_{1}^{(\\ell)}+\\pmb{Q}\\Theta\\left(\\pmb{F}^{(\\ell)}\\right)\\pmb{W}_{2}^{(\\ell)}\\right),\\ \\ \\ \\ \\pmb{F}^{(\\ell+1)}=\\pmb{Q}^{\\dagger}\\pmb{H}^{(\\ell)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Theta$ is a neural network (i.e. multilayer perceptron or MultiHeadAttention), $W_{1}^{(\\ell)},W_{2}^{(\\ell)}\\in$ D(\u2113)\u00d7D(\u2113+1) are learnable matrices acting on signals in the node space, and $\\sigma$ is a non-linearity. We call this method ICG neural network (ICG-NN). The final representations $H^{(L)}$ can be used for predicting node-level properties. ", "page_idx": 6}, {"type": "text", "text": "We propose another deep ICG method, where F (\u2113) \u2208 RK\u00d7D(\u2113) are taken directly as trainable parameters. Namely, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}^{(\\ell+1)}=\\sigma\\left(\\pmb{H}^{(\\ell)}\\pmb{W}_{s}^{(\\ell)}+\\pmb{Q}\\pmb{F}^{(\\ell)}\\pmb{W}^{(\\ell)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We call this method $\\mathrm{ICG_{u}}$ -NN for the unconstrained community features. The motivation is that $\\boldsymbol{Q}\\boldsymbol{F}^{(\\ell)}$ exhausts the space of all signals that are linear combinations of the communities. If the number of communities is not high, then this is a low dimensional space that does not lead to overfitting in typical machine learning problems. Therefore, there is no need to reduce the complexity of $F^{(\\ell)}$ by constraining it to be the output of a neural network $\\Theta$ on latent representations. The term $Q F^{(\\ell)}$ can hence be interpreted as a type of positional encoding that captures the community structure. ", "page_idx": 6}, {"type": "text", "text": "ICG-NNs for spatio-temporal graphs. For a fixed graphs with varying node features, the ICG is ftited to the graph once6. Given that there are $T$ training signals, each learning step with ICG-NN takes $\\mathcal{O}(T N(K+D))$ rather than $\\mathcal{O}(T E D^{2})$ for MPNNs. Note that $T$ does not scale the preprocessing time. Hence, the time dimension amplifies the difference in efficiency between ICG-NNs and MPNNs. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We empirically validate our methods with the following experiments: 7 \u2022 Runtime analysis (Section 6.1): We report the forward pass runtimes of $\\mathrm{ICG_{u}}$ -NN and GCN [30], empirically validating the theoretical advantage of the former. We further extend this analysis in Appendices F.7 and F.8. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u2022 Node classification (Appendix F.1): We evaluate our method on real-world node classification datasets [43, 45, 36], observing that the model performance is competitive with standard approaches.   \n\u2022 Node classification using Subgraph SGD (Section 6.2 and Appendix F.3): We evaluate our subgraph SGD method (Section 4.3) to identify the effect of sampling on the model performance on the tolokers and Flickr datasets [43, 65]. We find the model\u2019s performance to be robust on tolokers and state-of-the-art on Flickr.   \n\u2022 Spatio-temporal tasks (Section 6.3): We evaluate $\\mathrm{ICG_{u}}$ -NN on real-world spatio-temporal tasks [35] and obtain competitive performance to domain-specific baselines.   \n\u2022 Comparison to graph coarsening methods (Appendix F.2): We provide an empirical comparison between ICG-NNs and a variety of graph coarsening methods on the Reddit [23] and Flickr [65] datasets, where ICG-NNs achieve state-of-the-art performance.   \n\u2022 Additional experiments: We perform an ablation study over the number of communities (Appendix F.4) and the choice of initialization in Section 4.2 (Appendix F.6). We moreover experimentally demonstrate a positive correlation between the Frobenius error and cut norm error as hinted by Theorem 3.1 (Appendix F.5), and perform a memory allocation analysis (Appendix F.9). ", "page_idx": 7}, {"type": "text", "text": "6.1 How does the runtime compare to standard GNNs? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. We compare the forward pass runtimes of our signal processing pipeline $\\mathrm{ICG_{u}}$ -NN) and GCN [30] on Erd\u02ddos-R\u00e9nyi $\\mathrm{ER}(n,p(\\bar{n})\\bar{=}\\,0.5)$ graphs with up to $7k$ nodes. Node features are independently drawn from $U[0,1]$ and the initial feature dimension is 128. Both models use a hidden dimension of 128, 3 layers and an output dimension of 5. ", "page_idx": 7}, {"type": "text", "text": "Results. Figure 2 reveals a strong square root relationship between the runtime of $\\mathrm{ICG_{u}}$ -NN and the runtime of GCN. This aligns with our expectations, as the time complexity of GCN is $\\bar{O(E)}$ , while the time complexity of ICG-NNs is $\\dot{\\cal O}(N)$ , highlighting the computational advantage of using ICGs. We complement this analysis with experiments using sparse Erdo\u02dds-R\u00e9nyi graphs in Appendix F.7. ", "page_idx": 7}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/979ad2a542a8c054717fddbca6884b378233605ac20535d0a8de9ce95f911796.jpg", "img_caption": ["Figure 2: Runtime of $\\mathrm{K-ICG_{u}}$ - NN (for $K{=}100$ ) as a function of GCN forward pass duration on graphs $G\\sim\\mathrm{ER}(n,p(n)=0.5)$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Node classification using Subgraph SGD ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. We evaluate $\\mathrm{ICG_{u}}$ -NN on the non-sparse graphs tolokers [43], following the 10 data splits of [43]. We report the mean ROC AUC and standard deviation as a function of the ratio of nodes that are removed from the graph. We compare to the baseline of MLP on the full graph. ", "page_idx": 7}, {"type": "text", "text": "Results. Figure 3 shows a slight degradation of $2.8\\%$ when a small number of nodes is removed from the graph. However, the key insight is that when more than $10\\%$ of the graph is removed, the performance stops degrading. These results further support our Proposition E.1 in Appendix E about the error between ", "page_idx": 7}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/ca7b33b715acd33c8cdb6e3b3f820fffcbb4590d97bb926b6e987f3fbf5f4ad9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: ROC AUC of $\\mathrm{ICG_{u}}$ -NN and an MLP as a function of the $\\%$ nodes removed from the graph. ", "page_idx": 7}, {"type": "text", "text": "subgraph gradients and full-graph gradients of the Frobenius loss, and establish ICG-NNs as a viable option for learning on large graphs. ", "page_idx": 8}, {"type": "text", "text": "6.3 Spatio-temporal graphs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setup & baselines. We evaluate ICG-NN and $\\mathrm{ICG_{u}}$ NN on real-world traffic networks, METR-LA and PEMS-BAY [35] following the methodology described by [15]. We segment the datasets into windows of 12 time steps and train the models to predict the subsequent 12 observations. For all datasets, these windows are divided sequentially into $70\\%$ for training, $10\\%$ for validation, and $20\\%$ for testing. We report the mean absolute error (MAE) and standard deviation averaged over the forecastings. The baselines DCRNN [35], GraphWaveNet [60], AGCRN [8], T&S-IMP, TTS-IMP, T&S-AMP, and TTSAMP [15], are adopted from [15]. We incorporate a GRU to embed the data before inputting it into our ICG-NN models, we symmetrize the graph to enable our method to operate on it, and we disregard the features in the ICG approximation (setting $\\lambda=0$ in (7)). Additionally, we use the Adam optimizer and detail all hyperparameters in Appendix I. ", "page_idx": 8}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/5000081889b691a8db2349b8aa8fbab97739ae97b3a214083694769fef6cc39d.jpg", "table_caption": ["Table 1: Results on dense temporal graphs. Top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Results. Table 1 shows that ICG-NNs achieve competitive performance when compared to methods that are specially tailored for spatio-temporal data such as DCRNN, GraphWaveNet and AGCRN. Despite the small graph size (207 and 325 nodes) and the low ratio of edges (graph densities of $3.54\\cdot10^{-2}$ and $2.{\\bar{2}}4\\cdot{\\bar{1}}0^{-2}$ ), ICG-NNs perform well, corroborating the discussion in Section 3.3. ", "page_idx": 8}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide the main related work in this Section. In Appendix G we give an additional review. ", "page_idx": 8}, {"type": "text", "text": "Intersecting communities and stochastic block models. We express graphs as intersections of cliques, or communities, similarly to classical works in statistics and computer science [1, 62]. Our approach can be interpreted as fitting a stochastic-block-model (SBM) to a graph. As opposed to standard SBM approaches, our method is interpreted as data fitting with norm minimization rather than statistical inference. Similarly to the intersecting community approach of BigCLAM [62], our algorithm takes $\\mathcal{O}(E)$ operations. Unlike BigCLAM, however, we can approximate any graph, as guaranteed by the regularity lemma. This is possible since ICGs are allowed to have negative coefficients, while BigCLAM only uses positive coefficients due to its probabilistic interpretation. To the best of our knowledge, we are the first to propose a SBM fitting algorithm based on the weak regularity lemma. For a survey on SBMs we refer the reader to [32]. ", "page_idx": 8}, {"type": "text", "text": "The Weak Regularity Lemma. The Regularity Lemma is a central result in graph theory with many variants and many proof techniques. One version is called the Weak Regularity Lemma (for graphs [20], graphons [38], or graph-signals and graphon-signals [33]), and has the following interpretation: every graph can be approximated in the cut metric by an ICG, where the error rate $\\epsilon$ is uniformly $\\mathcal{O}(\\dot{K}^{-1/2})^{8}$ . While [20, Theorem 2] proposes an algorithm for finding the approximating ICG, the algorithm takes $2^{\\mathscr{O}(N^{2}K)}$ time to find this minimizer9. This time complexity is too high to be practical in real-world problems. Alternatively, since the cut metric is defined via a maximization process, finding a minimizing ICG by directly optimizing cut metric via a GD approach involves a min-max problem, which appears numerically problematic in practice. Moreover, computing cut norm is NP-hard [3, 44]. Instead, we introduce a way to bypass the need to explicitly compute the cut norm, finding a $K$ community ICG with error $\\mathcal{O}(K^{-1/2})$ in the cut metric by minimizing the Frobenius norm. We note that for the Szemer\u00e9di (non-weak) regularity lemma [49], it was shown by Alon et al. [4] that a regular partition with error $<\\epsilon$ into non-intersecting communities (the analogue to ICG in the Szemer\u00e9di regularity lemma) can be found in polynomial time with respect to $N$ , provided that $N$ is very large $(N\\sim2^{\\epsilon^{-20}}$ ). For more details on the Weak Regularity Lemma, see Appendix A. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Subgraph methods. Learning on large graphs requires sophisticated subgraph sampling techniques for deployment on contemporary processors [24, 12, 13, 7, 65]. After the preprocessing step on the ICG, our approach allows processing very large networks more accurately, avoiding subsampling schemes that can alter the properties of the graph in unintended ways. ", "page_idx": 9}, {"type": "text", "text": "GNNs with local pooling. Local graph pooling methods, e.g., [63, 9], construct a sequence of coarsened versions of the given graph, each time collapsing small sets of neighboring nodes into a single \u201csuper-node.\u201d At each local pooling layer, the signal is projected from the finer graph to the coarser graph. This is related to our ICG-NN approach, where the signal is projected upon the communities. As opposed to local graph pooling methods, our communities intersect and have large-scale supports. Moreover, our method does not lose the granular/high-frequency content of the signal when projecting upon the communities, as we also have node-wise operations at the fine level. In pooling approaches, the graph is partitioned into disjoint, or slightly overlapping communities, each community collapses to a node, and a standard message passing scheme is applied on this coarse graph. In ICG-NNs, each operation on the community features has a global receptive field in the graph. Moreover, ICG-NNs are not of message-passing type: the (flattened) community feature vector $\\pmb{F}$ is symmetryless and is operated upon by a general MLP in the ICG-NN, while MPNNs operate by the same function on all edges. ", "page_idx": 9}, {"type": "text", "text": "In terms of computational efficiency, GNNs with local pooling do not asymptotically improve runtime, as the first layer operates on the full graph, while our method operates solely on the efficient data structure. ", "page_idx": 9}, {"type": "text", "text": "Variational graph autoencoders. Our ICG construction is related to statistical network analysis with latent position graph models [6]. Indeed, the edge weight between any pair of nodes $n,m$ in the ICG is the diagonal Bilinear product between their affliiation vectors, namely, $\\begin{array}{r}{\\sum_{j}r_{j}\\mathbb{1}\\boldsymbol{\\mathcal{U}}_{j}\\left(n\\right)\\mathbb{1}_{\\mathcal{U}_{j}}(m)}\\end{array}$ , which is similar to the inner produce decoder in variational graph autoencoders [29, 6]. However, as opopsed to variational graph autoencoders, we have the coefficients $r_{j}$ , that can be negative and hence allow a more expressive decoding. ", "page_idx": 9}, {"type": "text", "text": "8 Summary ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced an new approach for learning on large non-sparse graphs, using ICGs. We proved a new constructive variant of the weak regularity lemma, which shows that minimizing the Frobenius error between the ICG and the graph leads to a uniformly small error in cut metric. We moreover showed how to optimize the Frobenius error efficiently. We then developed a signal processing setting, operating on the ICG and on the node reprsentations in $\\mathcal{O}(N)$ complexity. The overall pipeline involves precomputing the ICG approximation of the graph in ${\\mathcal{O}}(E)$ operations per iteration, and then solving the task on the graph in $\\mathcal{O}(N)$ operations per iteration. Both fitting an ICG to a large graph, and training a standard subgraph GNNs, require an online subsampling method, reading from slow memory during optimization. However, fitting the ICG is only done once, and does not require an extensive hyperparameter optimization. Then, learning to solve the task on the graph with ICG-NNs is efficient, and can be done directly on the GPU memory. Since the second learning phase is the most demanding part, involving an extensive hyperparameter optimization and architecture tuning, ICG-NN offer a potentially significant advantage over standard subgraph GNNs. This gap between ICG-NNs and MPNNs is further amplified for time series on graphs. ", "page_idx": 9}, {"type": "text", "text": "The main limitation of our method is that the ICG-NN is fitted to a specific ICG, and cannot be na\u00efvely transferred between different ICGs approximating different graphs. Another limitation is the fact that the current ICG construction is limited to undirected graphs, while many graphs, especially spatiotemporal, are directed. One potential avenue for future work is thus to extend the ICG construction to directed graphs. Additional future work will study the expressive power of ICGs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the Israel Science Foundation (grant No. 1937/23), EPSRC Turing AI World-Leading Research Fellowship No. EP/X040062/1, and the EPSRC AI Hub on Mathematical Foundations of Intelligence: An \"Erlangen Programme\" for AI No. EP/Y028872/1. BF is funded by the Clarendon scholarship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Edo M. Airoldi, David Blei, Stephen Fienberg, and Eric Xing. Mixed membership stochastic blockmodels. NeurIPS, 2008.   \n[2] Noga Alon. Approximating sparse binary matrices in the cut-norm. Linear Algebra and its Applications, 2015.   \n[3] Noga Alon and Assaf Naor. Approximating the cut-norm via grothendieck\u2019s inequality. SIAM, 2006.   \n[4] Noga Alon, Richard A Duke, Hanno Lefmann, Vojtech Rodl, and Raphael Yuster. The algorithmic aspects of the regularity lemma. Journal of Algorithms, 1994.   \n[5] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In ICLR, 2021.   \n[6] Avanti Athreya, Minh Tang, Youngser Park, and Carey E Priebe. On estimation and inference in latent structure random graphs. Statistical Science, 2021.   \n[7] Jiyang Bai, Yuxiang Ren, and Jiawei Zhang. Ripple walk training: A subgraph-based training framework for large and deep graph neural network. IJCNN, 2020.   \n[8] Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent network for traffic forecasting. NeurIPS, 2020.   \n[9] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural networks for graph pooling. In ICML, 2020.   \n[10] Deyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. Structural deep clustering network. In WWW, 2020.   \n[11] Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing. Advances in Mathematics, 2008.   \n[12] Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks via importance sampling. In ICLR, 2018.   \n[13] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In KDD, 2019.   \n[14] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. In ICLR, 2020.   \n[15] Andrea Cini, Ivan Marisca, Daniele Zambon, and Cesare Alippi. Taming local effects in graph-based spatiotemporal forecasting. NeurIPS, 2024.   \n[16] Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael Bronstein. Latentgraph learning for disease prediction. In Medical Image Computing and Computer Assisted Intervention, 2020.   \n[17] Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. Slaps: Self-supervision improves structure learning for graph neural networks. In NeurIPS, 2021.   \n[18] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical inter-message passing for learning on molecular graphs. arXiv, 2020.   \n[19] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Ben Chamberlain, Michael Bronstein, and Federico Monti. Sign: Scalable inception graph neural networks. In ICML: Graph Representation Learning and Beyond $(G R L+$ ) Workshop, 2020.   \n[20] Alan M. Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combinatorica, 1999.   \n[21] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. In NeurIPS, 2019.   \n[22] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017.   \n[23] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. NeurIPS, 2017.   \n[24] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, 2017.   \n[25] Zengfeng Huang, Shengzhong Zhang, Chong Xi, Tang Liu, and Min Zhou. Scaling up graph neural networks via graph coarsening. In KDD, 2021.   \n[26] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph condensation for graph neural networks. arXiv, 2021.   \n[27] Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang, and Bing Yin. Condensing graphs via one-step gradient matching. In KDD, 2022.   \n[28] Anees Kazi, Luca Di Cosmo, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael M. Bronstein. Differentiable graph module (dgm) for graph convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.   \n[29] Thomas Kipf and Max Welling. Variational graph auto-encoders. arXiv, 2016.   \n[30] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.   \n[31] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In ICML, 2018.   \n[32] Clement Lee and Darren Wilkinson. A review of stochastic block models and extensions for graph clustering. Applied Network Science, 2019.   \n[33] Ron Levie. A graphon-signal analysis of graph neural networks. In NeurIPS, 2023.   \n[34] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian. Finding global homophily in graph neural networks when meeting heterophily. In ICML, 2022.   \n[35] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In ICLR, 2018.   \n[36] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. NeurIPS, 2021.   \n[37] L\u00e1szl\u00f3 Mikl\u00f3s Lov\u00e1sz. Large networks and graph limits. In volume 60 of Colloquium Publications, 2012.   \n[38] L\u00e1szl\u00f3 Mikl\u00f3s Lov\u00e1sz and Bal\u00e1zs Szegedy. Szemer\u00e9di\u2019s lemma for the analyst. GAFA Geometric And Functional Analysis, 2007.   \n[39] Jiaqi Ma, Weijing Tang, Ji Zhu, and Qiaozhu Mei. A flexible generative framework for graphbased semi-supervised learning. In NeurIPS, 2019.   \n[40] Sohir Maskey, Ron Levie, and Gitta Kutyniok. Transferability of graph neural networks: An extended graphon approach. Applied and Computational Harmonic Analysis, 2023.   \n[41] Raffaele Paolino, Aleksandar Bojchevski, Stephan G\u00fcnnemann, Gitta Kutyniok, and Ron Levie. Unveiling the sampling density in non-uniform geometric graphs. In ICLR, 2023.   \n[42] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. arXiv, 2020.   \n[43] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of GNNs under heterophily: Are we really making progress? In ICLR, 2023.   \n[44] Ji\u02c7r\u00ed Rohn. Computing the norm $\\|a\\|_{\\infty,1}$ is np-hard. Linear and Multilinear Algebra, 2000.   \n[45] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of Complex Networks, 2021.   \n[46] Luana Ruiz, Luiz F. O. Chamon, and Alejandro Ribeiro. Graphon signal processing. IEEE Transactions on Signal Processing, 2021.   \n[47] Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011.   \n[48] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv, 2017.   \n[49] Endre Szemer\u00e9di. Regular partitions of graphs. Stanford University, 1975.   \n[50] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In ICLR, 2022.   \n[51] Lloyd N. Trefethen and David Bau. Numerical Linear Algebra, Twenty-ffith Anniversary Edition. Society for Industrial and Applied Mathematics, 2022.   \n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.   \n[53] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.   \n[54] Petar Velivckovic\u00b4, Lars Buesing, Matthew Overlan, Razvan Pascanu, Oriol Vinyals, and Charles Blundell. Pointer graph networks. In NeurIPS, 2020.   \n[55] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. Attributed graph clustering: A deep attentional embedding approach. In IJCAI, 2019.   \n[56] Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, and Qing Li. Fast graph condensation with structure-based neural tangent kernel. In WWW, 2024.   \n[57] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics, 2019.   \n[58] Max Welling. Herding dynamical weights to learn. In ICML, 2009.   \n[59] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples for graph data: Deep insights into attack and defense. In IJCAI, 2019.   \n[60] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for deep spatial-temporal graph modeling. In IJCAI, 2019.   \n[61] Jaewon Yang and Jure Leskovec. Structure and overlaps of communities in networks. In SNAKDD, 2012.   \n[62] Jaewon Yang and Jure Leskovec. Overlapping community detection at scale: a nonnegative matrix factorization approach. In Association for Computing Machinery, 2013.   \n[63] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In NeurIPS, 2018.   \n[64] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. In NeurIPS, 2019.   \n[65] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. In ICLR, 2019.   \n[66] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. arXiv, 2020.   \n[67] Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen, Xingquan Zhu, and Shirui Pan. Structure-free graph condensation: From large-scale graphs to condensed graph-free data. NeurIPS, 2024.   \n[68] Houquan Zhou, Shenghua Liu, Danai Koutra, Huawei Shen, and Xueqi Cheng. A provable framework of learning graph embeddings via summarization. In AAAI, 2023.   \n[69] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. NeurIPS, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A The weak regularity lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $G=\\{\\mathcal{N},\\mathcal{E}\\}$ be a graph and $\\mathcal{P}=\\{\\mathcal{N}_{1},\\ldots,\\mathcal{N}_{k}\\}$ be a partition of $\\mathcal{N}$ . The partition is called equipartition if $||\\mathcal{N}_{i}|-|\\bar{\\mathcal{N}_{j}}||\\leq1$ for every $1\\leq i,j\\leq k$ . For any two subsets $\\mathcal{U},\\mathscr{S}\\subset\\mathscr{N}$ , the number of edges from $\\boldsymbol{\\mathcal{U}}$ to $\\boldsymbol{S}$ are denoted by $e_{G}(\\mathcal{U},S)$ . Given two node set $\\mathcal{U},\\mathscr{S}\\subset\\mathscr{N}$ , if the edges between ${\\mathcal{N}}_{i}$ and $\\mathcal{N}_{j}$ were distributed randomly, then, the number of edges between $\\boldsymbol{\\mathcal{U}}$ and $\\boldsymbol{S}$ would have been close to the expected value ", "page_idx": 14}, {"type": "equation", "text": "$$\ne_{\\mathcal{P}(\\mathcal{U},S)}:=\\sum_{i=1}^{k}\\sum_{j=1}^{k}\\frac{e_{G}(\\mathcal{N}_{i},\\mathcal{N}_{j})}{|\\mathcal{N}_{i}|\\,|\\mathcal{N}_{j}|}\\,|\\mathcal{N}_{i}\\cap\\mathcal{U}|\\,|\\mathcal{N}_{j}\\cap S|\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the irregularity, which measure of how non-random like the edges between $\\{\\mathcal{N}_{j}\\}$ are, is defined to be ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{irreg}_{G}(\\mathcal{P})=\\operatorname*{max}_{\\mathcal{U},\\mathcal{S}\\subset\\mathcal{N}}\\left|e_{G}(\\mathcal{U},\\mathcal{S})-e_{\\mathcal{P}}(\\mathcal{U},\\mathcal{S})\\right|/\\left|\\mathcal{N}\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem A.1 (Weak Regularity Lemma [20]). For every $\\epsilon>0$ and every graph $G=(\\mathcal{N},\\mathcal{E}),$ , there is an equipartition $\\mathcal{P}=\\{\\mathcal{N}_{1},\\ldots,\\mathcal{N}_{k}\\}$ of $\\mathcal{N}$ into $k\\le2^{c/\\epsilon^{2}}$ classes such that $\\arg_{G}(\\mathcal{P})\\leq\\epsilon.$ . Here, $c$ is a universal constant that does not depend on $G$ and $\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "The weak regularity lemma states that any large graph $G$ can be represented by the weighted graph $G^{\\epsilon}$ with node set $\\dot{\\mathcal{N}}^{\\epsilon}\\,=\\,\\{\\mathcal{N}_{1},\\dots,\\mathcal{N}_{k}\\}$ , where the edge weight between the nodes ${\\mathcal N}_{i}$ and $\\mathcal{N}_{j}$ is $\\frac{e_{G}(\\mathcal{N}_{i},\\mathcal{N}_{j})}{|\\mathcal{N}_{i}|,|\\mathcal{N}_{j}|}$ , which depicts a smaller, coarse-grained version of the large graph. The \u201clarge-scale\u201d structure of $G$ is given by $G^{\\epsilon}$ , and the number of edges between any two subsets of nodes $\\mathcal{U}_{i}\\subset\\mathcal{N}_{i}$ and $\\mathcal{U}_{j}\\subset\\mathcal{N}_{j}$ is close to the \u201cexpected value\u201d $e_{\\mathcal{P}(\\mathcal{U}_{i},\\mathcal{U}_{j})}$ . Hence, the deterministic graph $G$ \u201cbehaves\u201d as if it was randomly sampled from the \u201cstochastic block model\u201d $G^{\\epsilon}$ . ", "page_idx": 14}, {"type": "text", "text": "It can be shown that irregularity coincides with error in cut metric between the graph and its coarsening SBM. Namely, to see how the cut-norm is related to irregularity (10), consider a graphon $W_{G}$ induced by the graph $G=\\{\\mathcal{N},\\mathcal{E}\\}$ . Let $\\mathcal{P}=\\{\\mathcal{N}_{1},\\ldots,\\mathcal{N}_{k}\\}$ be an equipartition of $\\mathcal{N}$ . Consider the graph $G^{\\mathcal{P}}$ defined by the adjacency matrix $A^{\\mathcal{P}}=(a_{i,j}^{\\mathcal{P}})_{i,j=1}^{|\\mathcal{N}|}$ with ", "page_idx": 14}, {"type": "equation", "text": "$$\na_{i,j}^{\\mathcal{P}}=\\frac{e_{G}(\\mathcal{N}_{q_{i}},\\mathcal{N}_{q_{j}})}{|\\mathcal{N}_{q_{i}}|\\;,|\\mathcal{N}_{q_{j}}|},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{N}_{q_{i}}\\in\\mathcal{P}$ is the class that contains node $i$ . Now, it can be verified that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|W_{G}-W_{G^{\\mathcal{P}}}\\|_{\\Omega}=\\operatorname{irreg}_{G}(\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, the weak regularity lemma can be formulated with cut norm instead of irregularity. ", "page_idx": 14}, {"type": "text", "text": "B Graphon-Signal Intersecting Communities and the Constructive Regularity Lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we prove a constructive weak regularity lemma for graphon-signals, where Theorem 3.1 is a special case. We start by defining graphon-signals in Subsection B.1. We define graphon-signal cut norm and metric in Subsection B.2, and graphon-signal intersecting communities in B.3. In Subsections B.4 and B.5 we formulate and prove the constructive graphon-signal weak regularity lemma. Finally, in Subsection B.6 we prove the constructive graph-signal weak regularity lemma as a special case. ", "page_idx": 14}, {"type": "text", "text": "For $m,J\\;\\in\\;\\mathbb{N}$ , the $L_{2}$ norm of a measurable function $f\\;=\\;(f_{i})_{i=1}^{j}\\;:\\;[0,1]^{m}\\;\\rightarrow\\;\\mathbb{R}^{J}.$ , where $f_{i}:[0,1]^{m}\\to\\mathbb{R}$ , is defined to be $\\begin{array}{r}{\\|f\\|_{2}=\\sqrt{\\sum_{j=1}^{J}\\int_{[0,1]^{m}}|f_{j}(\\pmb{x})|^{2}\\,d\\pmb{x}}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "B.1 Graphon-signals ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A graphon [11, 37] can be seen as a weighted graph with a \u201ccontinuous\u201d node set $[0,1]$ . The space of graphons $\\mathcal{W}$ is defined to be the set of all measurable symmetric function $W:[0,\\dot{1}]^{2}\\rightarrow[0,1]$ , $\\bar{W(x,y)}=W(y,x)$ . Each edge weight $W(x,y)\\in[0,1]$ of a graphon $W\\in\\mathcal{W}$ can be seen as the probability of having an edge between nodes $x$ and $y$ . Graphs can be seen as special graphons. Let $\\mathbf{\\dot{Z}}_{m}=\\{I_{1},\\ldots,I_{m}\\}$ be an interval equipartition: a partition of $[0,1]$ into disjoint intervals of equal length. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A graph with an adjacency matrix $A=(a_{i,j})_{i,j=1}^{N}$ induces the graphon $W_{A}$ , defined by $W_{A}(x,y)=$ $a_{[x m],[y m]}$ 10. Note that $W_{A}$ is piecewise constant on the partition ${\\mathcal{T}}_{m}$ . We hence identify graphs with their induced graphons. A graphon can also be seen as a generative model of graphs. Given a graphon $W$ , a corresponding random graph is generated by sampling i.i.d. nodes $\\bar{\\{}u_{n}\\}$ from the graphon domain $[0,1]$ , and connecting each pair $u_{n},u_{m}$ in probability $W(u_{n},u_{m})$ to obtain the edges of the graph. ", "page_idx": 15}, {"type": "text", "text": "The space of grahon-signals is the space of pairs of measurable functions $(W,s)$ of the form $W:$ $[0,1]^{2}\\stackrel{}{\\rightarrow}[0,\\bar{1}]$ and $s:[0,1]\\to[0,1]^{D}$ , where $D\\in\\mathbb{N}$ is the number of signal channels. Note that datasets with signals in value ranges other than $[0,1]$ can always be transformed (by translating and scaling in the value axis of each channel) to be $[0,\\dot{1}]^{D}$ -valued. Given a discrete signal $S:[\\bar{N}]\\rightarrow$ $[0,1]^{\\bar{D}}$ , we define the induced signal $s_{S}$ over $[0,1]$ by $s s(x)\\,=\\,s_{\\lceil x m\\rceil}$ . We define the Frobenius distance between two graphon-signals $(W,s)$ and $(W^{\\prime},s^{\\prime})$ simply as their $L_{2}$ distance, namely, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|(W,s)-(W^{\\prime},s^{\\prime})\\right\\|_{\\mathrm{F}}:=\\sqrt{a\\left\\|W-W^{\\prime}\\right\\|_{2}^{2}+b\\sum_{j=1}^{D}\\left\\|s_{j}-s_{j}^{\\prime}\\right\\|_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some $a,b>0$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Cut-distance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The cut metric is a natural notion of graph similarity, based on the cut norm. The graphon-signal cut norm was defined in [33], extending the standard definition to a graphon with node features. ", "page_idx": 15}, {"type": "text", "text": "Definition B.1. The graphon cut norm of $W\\in L^{2}[0,1]^{2}$ is defined to be ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|W\\|_{\\Omega}=\\operatorname*{sup}_{S,T\\subset[0,1]}\\left|\\int_{S}\\int_{T}W(x,y)d x d y\\right|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The signal cut norm of $s=(s_{1},\\ldots,s_{D})\\in(L^{2}[0,1])^{D}$ is defined to be ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|s\\right\\|_{\\Omega}=\\frac{1}{D}\\sum_{j=1}^{D}\\operatorname*{sup}_{U\\subset[0,1]}\\left|\\int_{U}s_{j}(x)d x\\right|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The graphon-signal cut norm of $(W,s)\\in L^{2}[0,1]^{2}\\times(L^{2}[0,1])^{D}$ , with weights $\\alpha>0$ and $\\beta>0$ , is defined to be ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|(W,s)\\right\\|_{\\square}=\\alpha\\left\\|W\\right\\|_{\\square}+\\beta\\left\\|s\\right\\|_{\\square}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given two graphons $W,W^{\\prime}$ , their distance in cut metric is the cut norm of their difference, namely ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Vert W-W^{\\prime}\\Vert_{\\Omega}=\\operatorname*{sup}_{S,T\\subset[0,1]}\\left|\\int_{S}\\int_{T}\\big(W(x,y)-W^{\\prime}(x,y)\\big)d x d y\\right|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The right-hand-side of (15) is interpreted as the similarity between the adjacencies $W$ and $W^{\\prime}$ on the block on which they are the most dissimilar. The graphon-signal cut metric is similarly defined to be $\\|(W,s)-(W^{\\prime},s^{\\prime})\\|_{\\square}$ . The cut metric between two graph-signals is defined to be the cut metric between their induced graphon-signals. ", "page_idx": 15}, {"type": "text", "text": "In Subsection B.6 we show that the graph-signal cut norm of Definition 2.1 are a special case of graphon-signal cut norm for induced graph-signals. ", "page_idx": 15}, {"type": "text", "text": "B.3 Intersecting community graphons ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we define ICGs for graphons. Denote by $\\chi$ the set of all indicator function of measurable subset of $[0,1]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\chi=\\{\\mathbb{1}_{u}\\mid u\\subset[0,1]\\mathrm{~measurable}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition B.2. A set $\\mathcal{Q}$ of bounded measurable functions $q:[0,1]\\to\\mathbb{R}$ that contains $\\chi$ is called a soft affiliation model. ", "page_idx": 16}, {"type": "text", "text": "Definition B.3. Let $D\\in\\mathbb{N}$ . Given a soft affliiation model $\\mathcal{Q},$ , the subset $[\\mathcal{Q}]$ of $L^{2}[0,1]^{2}\\times(L^{2}[0,1])^{D}$ of all elements of the form $(r q(x)q(y),b q(z))$ , with $q\\in{\\mathcal{Q}}_{*}$ , $r\\in\\mathbb{R}$ and $\\bar{b}\\in\\mathbb{R}^{D}$ , is called the soft rank-1 intersecting community graphon (ICG) model corresponding to $\\mathcal{Q}.$ . Given $K\\in\\mathbb{N}$ , the subset $[\\mathcal{Q}]_{K}$ of $L^{2}[0,1]^{2^{\\underline{{\\sim}}}}\\times(L^{2}[0,1])^{\\bar{D}}$ of all linear combinations of $K$ elements of $[\\mathcal{Q}]$ is called the soft rank- $K$ ICG model corresponding to $\\mathcal{Q}$ . Namely, $(C,p)\\in[Q]_{K}$ if and only if it has the form ", "page_idx": 16}, {"type": "equation", "text": "$$\nC(x,y)=\\sum_{k=1}^{K}r_{k}q_{k}(x)q_{k}(y)\\quad a n d\\quad p(z)=\\sum_{k=1}^{K}b_{k}q_{k}(z)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $(q_{k})_{k=1}^{K}\\in\\mathcal{Q}^{K}$ called the community affliiation functions, $(r_{k})_{k=1}^{K}\\in\\mathbb{R}^{K}$ called the community affiliation magnitudes, and $(b_{k})_{k=1}^{K}\\in\\mathbb{R}^{K\\times D}$ called the community features. ", "page_idx": 16}, {"type": "text", "text": "B.4 A constructive graphon-signal weak regularity lemma ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following theorem is the semi-constructive version of the weak regularity lemma for intersecting communities of graphons. Recall that $\\alpha$ and $\\beta$ denote the weights of the graphon-signal cut norm (14). ", "page_idx": 16}, {"type": "text", "text": "Theorem B.1. Let $D\\in\\mathbb{N}$ . Let $(W,s)$ be a $D$ -channel graphon-signal, $K\\in\\mathbb{N},\\,\\delta>0,$ , and let $\\mathcal{Q}\\,b e$ a soft indicators model. Let m be sampled uniformly from $[K]$ , and let $R\\geq1$ such that $K/R\\in\\mathbb{N}$ . Consider the graphon-signal Frobenius norm with weights $\\begin{array}{r}{\\sqrt{\\alpha\\left\\|\\boldsymbol{W}\\right\\|_{F}^{2}+\\beta\\sum_{j=1}^{D}\\left\\|\\boldsymbol{s}_{j}\\right\\|_{\\mathrm{F}}^{2}}}\\end{array}$ . Then, in probability $\\textstyle1-{\\frac{1}{R}}$ (with respect to the choice of $m$ ), for every $(C^{*},p^{*})\\in[\\mathcal{Q}]_{m}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle i f\\ \\ \\ \\|(W,s)-(C^{*},p^{*})\\|_{F}\\leq(1+\\delta)\\operatorname*{min}_{(C,p)\\in[Q]_{n}}\\|(W,s)-(C,p)\\|_{\\mathrm{F}}}\\\\ &{\\displaystyle t h e n\\ \\ \\ \\|(W,s)-(C^{*},p^{*})\\|_{\\Sigma}\\leq}\\\\ &{\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\Big(\\frac{3}{2}\\sqrt{\\alpha^{2}\\|W\\|_{\\mathrm{F}}^{2}+\\alpha\\beta\\,\\|s\\|_{\\mathrm{F}}}+\\sqrt{\\alpha\\beta\\|W\\|_{\\mathrm{F}}^{2}+\\beta^{2}\\,\\|s\\|_{\\mathrm{F}}}\\Big)\\sqrt{\\frac{R}{K}+\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.5 Proof of the soft constructive weak regularity lemma ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection we prove the constructive weak graphon-signal regularity lemma. ", "page_idx": 16}, {"type": "text", "text": "B.5.1 Constructive regularity lemma in Hilbert spaces ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The classical proof of the weak regularity lemma for graphons is given in [38]. The lemma is a corollary of a regularity lemma in Hilbert spaces. Our goal is to extend this lemma to be constructive. For comparison, let us first write the classical result, from [38, Lemma 4], even though we do not use this result directly. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.2 ([38]). Let $\\begin{array}{r}{{\\cal K}_{1},{\\cal K}_{2},\\ldots.}\\end{array}$ be arbitrary nonempty subsets (not necessarily subspaces) of a real Hilbert space $\\mathcal{H}$ . Then, for every $\\epsilon>0$ and $g\\in\\mathcal H$ there is $m\\leq\\lceil1/\\epsilon^{2}\\rceil$ and $(f_{i}\\in\\boldsymbol{K}_{i})_{i=1}^{m}$ and $(\\gamma_{i}\\in\\mathbb{R})_{i=1}^{m}$ , such that for every $w\\in\\kappa_{m+1}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\left\\langle w,g-(\\sum_{i=1}^{m}\\gamma_{i}f_{i})\\right\\rangle\\right|\\leq\\epsilon\\;\\left\\|w\\right\\|\\left\\|g\\right\\|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we show how to modify the result to be written explicitly with an approximate minimizer of a Hilbert space norm minimization problem. The proof follows the step of [38] with some modifications required for explicitly constructing an approximate optimizer. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. Let $\\{\\kappa_{j}\\}_{j\\in\\mathbb{N}}$ be a sequence of nonemply subsets of a real Hilbert space $\\mathcal{H}$ and let $\\delta\\geq0$ . Let $K>0$ , let $\\dot{R}\\geq1$ such that $K/R\\in\\mathbb{N}$ , and let $g\\in\\mathcal H$ . Let m be randomly uniformly sampled from $[K]$ . Then, in probability $\\textstyle1-{\\frac{1}{R}}$ (with respect to the choice of $m$ ), any vector of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\ng^{*}=\\sum_{j=1}^{m}\\gamma_{j}\\,f_{j}\\,\\quad\\,s u c h\\,t h a t\\,\\quad\\,\\gamma=(\\gamma_{j})_{j=1}^{m}\\in\\mathbb R^{m}\\quad a n d\\quad\\mathbf f=(f_{j})_{j=1}^{m}\\in K_{1}\\times\\ldots\\times K_{m}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that gives a close-to-best Hilbert space approximation of g in the sense that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|g-g^{*}\\|\\leq(1+\\delta)\\operatorname*{inf}_{\\gamma,\\mathbf{f}}\\,\\|g-\\sum_{i=1}^{m}\\gamma_{i}f_{i}\\|,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the infimum is over $\\gamma\\in\\mathbb{R}^{m}$ and f $\\in K_{1}\\times\\ldots\\times K_{m},$ , also satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{K}_{m+1},\\quad|\\langle w,g-g^{*}\\rangle|\\leq\\|w\\|\\,\\|g\\|\\,\\sqrt{\\frac{R}{K}+\\delta}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $K>0$ . Let $R\\geq1$ such that $K/R\\in\\mathbb{N}$ . For every $k$ , let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{k}=(1+\\delta)\\operatorname*{inf}_{\\gamma,\\mathbf{f}}\\,\\|g-\\sum_{i=1}^{k}\\gamma_{i}f_{i}\\|^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the infimum is over $\\pmb{\\gamma}=\\{\\gamma_{1},\\dots,\\gamma_{k}\\}\\in\\mathbb{R}^{k}$ and $\\mathbf{f}=\\{f_{1},\\dots,f_{k}\\}\\in{\\mathcal{K}}_{1}\\times\\dots\\times{\\mathcal{K}}_{k}$ . Note that $\\begin{array}{r}{\\left\\|g\\right\\|^{2}\\geq\\frac{\\eta_{1}}{1+\\delta}\\geq\\frac{\\eta_{2}}{1+\\delta}\\geq...\\geq0}\\end{array}$ . Therefore, there is a subset of at least $(1-\\textstyle{\\frac{1}{R}})K+1$ indices $m$ in $[K]$ such that $\\begin{array}{r}{\\eta_{m}\\leq\\eta_{m+1}+\\frac{R(1+\\delta)}{K}\\left\\|g\\right\\|^{2}}\\end{array}$ . Otherwise, there are $\\frac{K}{R}$ indices $m$ in $[K]$ such that $\\begin{array}{r}{\\eta_{m+1}<\\eta_{m}-\\frac{R(1+\\delta)}{K}\\left\\|g\\right\\|^{2}}\\end{array}$ R(1K+\u03b4)\u2225g\u22252, which means that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{K}<\\eta_{1}-\\frac{K}{R}\\frac{R(1+\\delta)}{K}\\left\\|g\\right\\|^{2}\\leq\\left(1+\\delta\\right)\\left\\|g\\right\\|^{2}-\\left(1+\\delta\\right)\\left\\|g\\right\\|^{2}=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is a contradiction to the fact that $\\eta_{K}\\geq0$ . ", "page_idx": 17}, {"type": "text", "text": "Hence, there is a set $\\mathcal{M}\\subseteq[K]$ of $\\textstyle(1-{\\frac{1}{R}})K$ indices such that for every $m\\in\\mathcal{M}$ , every ", "page_idx": 17}, {"type": "equation", "text": "$$\ng^{*}=\\sum_{j=1}^{m}\\gamma_{j}f_{j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "that satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|g-g^{*}\\right\\|^{2}\\leq\\eta_{m}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "also satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|g-g^{*}\\right\\|^{2}\\leq\\eta_{m+1}+\\frac{(1+\\delta)R}{K}\\left\\|g\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $w\\in\\kappa_{m+1}$ . By the definition of $\\eta_{m+1}$ , we have for every $t\\in\\mathbb R$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|g-(g^{*}+t w)\\right\\|^{2}\\geq\\frac{\\eta_{m+1}}{1+\\delta}\\geq\\frac{\\left\\|g-g^{*}\\right\\|^{2}}{1+\\delta}-\\frac{R}{K}\\left\\|g\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall t\\in\\mathbb{R},\\quad\\left\\|w\\right\\|^{2}t^{2}-2\\left\\langle w,g-g^{*}\\right\\rangle t+\\frac{R}{K}\\left\\|g\\right\\|^{2}+\\left(1-\\frac{1}{1+\\delta}\\right)\\left\\|g-g^{*}\\right\\|^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The discriminant of this quadratic polynomial is ", "page_idx": 17}, {"type": "equation", "text": "$$\n4\\left<w,g-g^{*}\\right>^{2}-4\\left\\|w\\right\\|^{2}\\left(\\frac{R}{K}\\left\\|g\\right\\|^{2}+(1-\\frac{1}{1+\\delta})\\left\\|g-g^{*}\\right\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and it must be non-positive to satisfy the inequality (20), namely ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{4\\left\\langle w,g-g^{*}\\right\\rangle^{2}\\leq4\\left\\|w\\right\\|^{2}\\left(\\displaystyle\\frac{R}{K}\\left\\|g\\right\\|^{2}+(1-\\displaystyle\\frac{1}{1+\\delta})\\left\\|g-g^{*}\\right\\|^{2}\\right)}\\\\ {\\leq4\\left\\|w\\right\\|^{2}\\left(\\displaystyle\\frac{R}{K}\\left\\|g\\right\\|^{2}+(1-\\displaystyle\\frac{1}{1+\\delta})\\eta_{m}\\right)}\\\\ {\\leq4\\left\\|w\\right\\|^{2}\\left(\\displaystyle\\frac{R}{K}\\left\\|g\\right\\|^{2}+(1-\\displaystyle\\frac{1}{1+\\delta})(1+\\delta)\\left\\|g\\right\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which proves ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\langle w,g-g^{*}\\right\\rangle\\leq\\left\\|w\\right\\|\\left\\|g\\right\\|\\sqrt{\\frac{R}{K}+\\delta}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is also true for $-w$ , which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "B.5.2 Proof of the Soft Constructive Weak Regularity Lemma for Graphon-Signals ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For two functions $f,g:[0,1]\\rightarrow\\mathbb{R}$ , we denote by $f\\otimes g:[0,1]^{2}\\to\\mathbb{R}$ the function ", "page_idx": 18}, {"type": "equation", "text": "$$\nf\\otimes g(x,y)=f(x)g(y).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We recall here Theorem B.1 for the convenience of the reader. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.1. Let $(W,s)$ be a graphon-signal, $K\\,\\in\\,\\mathbb{N}$ , $\\delta\\,>\\,0_{\\!\\!\\!\\!\\!\\!_{\\!}}$ , and let $\\mathcal{Q}$ be a soft indicators model. Let m be sampled uniformly from $[K]$ , and let $R\\geq1$ such that $\\underline{{K}}/R\\in\\mathbb{N}$ . Consider the graphon-signal Frobenius norm with weights $\\begin{array}{r}{\\sqrt{\\alpha\\left\\|\\boldsymbol{W}\\right\\|_{F}^{2}+\\beta\\sum_{j=1}^{D}\\left\\|\\boldsymbol{s}_{j}\\right\\|_{\\mathrm{F}}^{2}}}\\end{array}$ . Then, in probability $\\textstyle1-{\\frac{1}{R}}$ (with respect to the choice of $m$ ), for every $(C^{*},p^{*})\\in[\\mathcal{Q}]_{m}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{i f\\quad\\|(W,s)-(C^{*},p^{*})\\|_{F}\\leq(1+\\delta)\\displaystyle\\operatorname*{min}_{(C,p)\\in[Q]_{n}}\\|(W,s)-(C,p)\\|_{\\mathrm{F}}}\\\\ &{t h e n\\quad\\|(W,s)-(C^{*},p^{*})\\|_{\\Omega}\\leq}\\\\ &{\\qquad\\qquad\\qquad\\Big(\\displaystyle\\frac{3}{2}\\sqrt{\\alpha^{2}\\|W\\|_{\\mathrm{F}}^{2}+\\alpha\\beta\\,\\|s\\|_{\\mathrm{F}}}+\\sqrt{\\alpha\\beta\\|W\\|_{\\mathrm{F}}^{2}+\\beta^{2}\\,\\|s\\|_{\\mathrm{F}}}\\Big)\\sqrt{\\displaystyle\\frac{R}{K}+\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof follows the techniques of a part of the proof of the weak graphon regularity lemma from [38], while tracking the approximate minimizer in our formulation of Lemma B.3. This requires a probabilistic setting, and extending to soft indicators models. We note that the weak regularity lemma in [38] is formulated for non-intersecting blocks, but the intersecting community version is an intermediate step in its proof. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let us use Lemma B.3, with $\\mathcal{H}=L^{2}[0,1]^{2}\\times(L^{2}[0,1])^{D}$ with the weighted inner product ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle(W,s),(W^{\\prime},s^{\\prime})\\rangle=\\alpha\\iint_{[0,1]^{2}}W(x,y)W^{\\prime}(x,y)d x d y+\\beta\\sum_{j=1}^{D}\\int_{[0,1]}s_{j}(x)s_{j}^{\\prime}(x)d x,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and corresponding norm $\\begin{array}{r}{\\sqrt{\\alpha\\left\\|\\boldsymbol{W}\\right\\|_{F}^{2}+\\beta\\sum_{j=1}^{D}\\left\\|\\boldsymbol{s}_{j}\\right\\|_{\\mathrm{F}}^{2}}}\\end{array}$ and corresponding weighted inner product, and $K_{j}=[\\mathcal{Q}]$ . Note that the Hilbert space norm is the Frobenius norm in this case. In the setting of the lemma, we take $g=(W,s)$ , and $g^{\\ast}\\in[\\mathcal{Q}]_{m}$ . By the lemma, in the event of probability $1-1/R$ , any approximate Frobenius minimizer $\\left(C^{*},p^{*}\\right)$ , namely, that satisfies $\\lVert(W,s)-(C^{*},p^{*})\\rVert_{\\mathrm{F}}\\,\\leq$ $(1+\\delta)\\operatorname*{min}_{(C,p)\\in[\\mathcal{Q}]_{m}}\\|(W,s)-(C,p)\\|_{\\mathrm{F}}$ , also satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle(T,y),(W,s)-(C^{*},p^{*})\\rangle\\leq\\Vert(T,y)\\Vert_{\\mathrm{F}}\\Vert(W,s)\\Vert_{\\mathrm{F}}{\\sqrt{\\frac{R}{K}+\\delta}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for every $(T,y)\\in[\\mathcal{Q}]$ . Hence, for every choice of measurable subsets $S,\\mathcal{T}\\subset[0,1]$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\omega}\\int_{(\\mathcal{R}^{n}(x,y))}^{\\infty}C^{*}(x,y)|d x d y}\\bigg|}\\\\ &{=\\frac{1}{2}\\left|\\int_{\\omega}\\int_{(\\mathcal{R}^{n}(x,y))}^{\\infty}-C^{*}(x,y)|d x d y\\right|+\\int_{\\mathcal{T}_{2}}\\int_{\\omega}^{\\infty}\\int_{\\mathbb{R}^{n}(x,y)}-C^{*}(x,y)d x d y\\bigg|}\\\\ &{=\\frac{1}{2}\\left|\\int_{\\partial\\mathcal{R}^{n}(\\mathcal{T}^{n}(x,y))}(\\psi(x,y)-C^{*}(x,y))d x d y-\\int_{\\partial\\mathcal{R}}\\int_{(\\mathcal{R}^{n}(x,y))}-C^{*}(x,y))d x d y\\right|}\\\\ &{\\leq\\int_{\\mathcal{T}_{2}}\\int_{(\\mathcal{W}(x,y))}-\\int_{(\\mathcal{R}^{n}(x,y))}-\\int_{\\gamma}\\int_{(\\mathcal{W}(x,y))}-C^{*}(x,y)d x d y\\bigg|}\\\\ &{\\leq\\bigg|\\frac{1}{2\\alpha}\\langle(\\mathrm{1}_{B,\\sigma}\\odot\\mathrm{1}_{B,\\mathcal{T}(0,)},(W,s)-(C^{*},p^{*}))\\bigg|+\\bigg|\\frac{1}{2\\alpha}\\langle(\\mathrm{1}_{\\mathcal{S}^{\\otimes}}\\mathrm{1}_{\\mathcal{S}^{\\otimes}},0),(W,s)-(C^{*},p^{*})\\rangle\\bigg|\\;\\right|}\\\\ &{\\qquad+\\left|\\frac{1}{2\\alpha}\\langle(\\mathrm{1}_{\\mathcal{T}^{\\otimes}}\\mathrm{1}_{\\mathcal{T}^{\\otimes}},0),(W,s)-(C^{*},p^{*})\\rangle\\right|}\\\\ &{\\leq\\frac{1}{2\\alpha}|(W,s)|\\gamma_{*}\\left(\\|(\\mathrm{1}_{\\mathcal{S}^{n}(\\mathcal{T}^{\\otimes})}\\mathrm{1}_{\\mathcal{S}^{n}(\\mathcal{T}^{\\otimes})}\\|\\gamma+\\|(\\mathrm{1}_{\\mathcal{S}^{\\otimes}}\\mathrm{1}_{\\mathcal{S}^{\\otimes}})\\|\\gamma+\\|(\\mathrm{1}_{\\mathcal{T}^{\\otimes}}\\mathrm{1}_{\\mathcal{T}})\\|\\nu\\right)\\bigg|\\overline{{K}}^{\\textnormal{i}}\\delta}\\\\ &{\\leq\\frac{3} \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, taking the supremum over $S,\\mathcal{T}\\subset[0,1]$ , we also have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha\\left\\|W-C^{*}\\right\\|_{\\Omega}\\leq\\frac{3}{2}\\sqrt{\\alpha^{2}\\|W\\|_{\\mathrm{F}}^{2}+\\alpha\\beta\\left\\|s\\right\\|_{\\mathrm{F}}}\\sqrt{\\frac{R}{K}+\\delta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, for every measurable $\\mathcal{T}\\subset[0,1]$ and every standard basis element $\\pmb{b}=(\\delta_{j,i})_{i=1}^{D}$ for any $j\\in[D]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{T}(s_{j}(x)-p_{j}^{*}(x))d x\\bigg|}}\\\\ &{=\\bigg|\\frac{1}{\\beta}\\left\\langle(0,b\\mathbb{1}_{T}),(W,s)-(C^{*},p^{*})\\right\\rangle\\bigg|}\\\\ &{\\leq\\frac{1}{\\beta}\\|(W,s)\\|_{\\mathrm{F}}\\|(0,b\\mathbb{1}_{T})\\|_{\\mathrm{F}}\\sqrt{\\frac{R}{K}+\\delta}}\\\\ &{\\leq\\frac{1}{\\beta}\\sqrt{\\alpha\\|W\\|_{\\mathrm{F}}^{2}+\\beta\\|s\\|_{\\mathrm{F}}}\\sqrt{\\beta}\\sqrt{\\frac{R}{K}+\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so, taking the supremum over $\\tau\\subset[0,1]$ independently for every $j\\ \\in[D]$ , and averaging over $j\\in[D]$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta\\left\\Vert s-p^{*}\\right\\Vert_{\\Omega}\\leq\\sqrt{\\alpha\\beta\\|W\\|_{\\mathrm{F}}^{2}+\\beta^{2}\\left\\Vert s\\right\\Vert_{\\mathrm{F}}}\\sqrt{\\frac{R}{K}+\\delta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Overall, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|(W,s)-(C^{*},p^{*})\\|_{\\Omega}\\leq\\Big(\\frac{3}{2}\\sqrt{\\alpha^{2}\\|W\\|_{\\mathrm{F}}^{2}+\\alpha\\beta\\,\\|s\\|_{\\mathrm{F}}}+\\sqrt{\\alpha\\beta\\|W\\|_{\\mathrm{F}}^{2}+\\beta^{2}\\,\\|s\\|_{\\mathrm{F}}}\\Big)\\sqrt{\\frac{R}{K}+\\delta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.6 Proof of the constructive weak regularity lemma for sparse graph-signals ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now show that Theorem 3.1 is a special case of Theorem B.1, restricted to graphon-signals induced by graph-signals, up to choice of the graphon-signal weights $\\alpha$ and $\\beta$ . Namely, choose $\\alpha\\,=\\,\\alpha^{\\prime}N^{2}\\bar{/}E$ and $\\beta\\,=\\,\\beta^{\\prime}$ with $\\alpha^{\\prime}+\\beta^{\\prime}\\,=\\,1$ as the weights of the graphon-signal cut norm of Theorem B.1, with arbitrary $\\alpha^{\\prime},\\beta^{\\prime}\\geq0$ satisfying $\\alpha^{\\prime}+\\beta^{\\prime}=\\overline{{1}}$ . It is easy to see the following relation between the graphon-signal and graph-signal cut norms ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(W_{\\mathbf{A}},s_{S})-(W_{C^{*}},s_{P^{*}})\\|_{\\Omega}=\\|(\\mathbf{A},S)-(C^{*},P^{*})\\|_{\\Omega,N,E}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\|(\\pmb{A},\\pmb{S})-(\\pmb{C}^{*},\\pmb{P}^{*})\\|_{\\Omega,N,E}$ is based on the weights $\\alpha^{\\prime}$ and $\\beta^{\\prime}$ . Now, since $\\begin{array}{r}{\\mathrm{deg}(A)/N^{2}=\\left\\lVert A\\right\\rVert_{\\mathrm{F}}^{2}=\\frac{E}{N^{2}}}\\end{array}$ , and by $\\|S\\|_{\\mathrm{F}}\\leq1$ , the bound in Theorem B.1 becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|(\\boldsymbol{A},\\boldsymbol{S})-(C^{*},P^{*})\\|_{\\boldsymbol{\\Omega},N,E}\\leq\\left(\\frac{3}{2}\\sqrt{\\alpha^{\\prime2}\\displaystyle\\frac{N^{4}}{E^{2}}\\displaystyle\\frac{E}{N^{2}}+\\alpha^{\\prime}\\beta^{\\prime}\\displaystyle\\frac{N^{2}}{E}}+\\sqrt{\\alpha^{\\prime}\\beta^{\\prime}\\displaystyle\\frac{N^{2}}{E}\\displaystyle\\frac{E}{N^{2}}+\\beta^{\\prime2}}\\right)\\sqrt{\\displaystyle\\frac{R}{K}+\\delta}}}\\\\ &{}&{\\leq C\\displaystyle\\frac{N}{\\sqrt{E}}\\sqrt{\\displaystyle\\frac{R}{K}+\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where, since $\\alpha^{\\prime}+\\beta^{\\prime}=1$ , and by convexity of the square root, ", "page_idx": 19}, {"type": "equation", "text": "$$\nC={\\frac{3}{2}}{\\sqrt{\\alpha^{\\prime}}}+{\\sqrt{\\beta^{\\prime}}}\\leq{\\frac{3}{2}}{\\sqrt{\\alpha^{\\prime}}}+{\\frac{3}{2}}{\\sqrt{\\beta^{\\prime}}}\\leq{\\frac{3}{2}}{\\sqrt{\\alpha^{\\prime}+\\beta^{\\prime}}}={\\frac{3}{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The only thing left to do is to change the notations $\\alpha^{\\prime}\\mapsto\\alpha$ and $\\beta^{\\prime}\\mapsto\\beta$ . ", "page_idx": 19}, {"type": "text", "text": "C Fitting ICGs to graphs efficiently ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we prove Proposition 4.1. For convenience, we recall the proposition. ", "page_idx": 19}, {"type": "text", "text": "Proposition 4.1. Let $\\pmb{A}=(a_{i,j})_{i,j=1}^{N}$ be an adjacency matrix of a weighted graph with $E$ edges. The graph part of the Frobenius loss can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|A-Q\\operatorname{diag}(r)Q^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\displaystyle\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathrm{Tr}\\left((Q^{\\top}Q)\\operatorname{diag}(r)(Q^{\\top}Q)\\operatorname{diag}(r)\\right)+\\left\\|A\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\frac{2}{N^{2}}\\sum_{i=1}^{N}\\sum_{j\\in N(i)}Q_{i,:}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}a_{i,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Computing the right-hand-side and its gradients with respect to $Q$ and $\\pmb{r}$ has a time complexity of $\\mathcal{O}(\\bar{K}^{2}N+K E)$ , and a space complexity of $\\mathcal{O}(K N+E)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The loss can be expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|A-Q\\operatorname{diag}(r)Q^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\frac{1}{N^{2}}\\sum_{i,j=1}^{N}\\left(a_{i,j}-Q_{i,i}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}\\right)^{2}}\\\\ &{\\displaystyle=\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\left(\\sum_{j\\in\\cal N(i)}\\left(a_{i,j}-Q_{i,i}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}\\right)^{2}+\\sum_{j\\notin\\cal N(i)}\\left(-Q_{i,i}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}\\right)^{2}\\right)}\\\\ &{\\displaystyle=\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\left(\\sum_{j\\in\\cal N(i)}\\left(a_{i,j}-Q_{i,i}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}\\right)^{2}+\\right.}\\\\ &{\\displaystyle\\left.+\\sum_{j=1}^{N}\\left(Q_{i,i}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}\\right)^{2}-\\sum_{j\\in\\cal N(i)}\\left(Q_{i,i}\\operatorname{diag}(r)\\left(Q^{\\top}\\right)_{:,j}\\right)^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We expand the quadratic term $\\left(a_{i,j}-Q_{i,:}\\operatorname{diag}(r)\\left(\\boldsymbol{Q}^{\\top}\\right)_{:,j}\\right)^{2}$ , and get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle|A-G\\operatorname{diag}(r)^{2}|_{r}^{2}}\\\\ &{\\displaystyle=\\frac{1}{N^{2}}\\sum_{i,j\\neq N_{0}(0)}^{N}\\left(Q_{i j}\\operatorname*{diag}(r)\\left(Q^{\\top}\\right)_{,j}\\right)^{2}+\\frac{1}{N}\\sum_{i=1;\\,j\\neq N_{0}(0)}^{N}\\left(a_{i,j}^{2}-2Q_{i,i}\\operatorname*{diag}(r)\\left(Q^{\\top}\\right)_{,j}a_{i,j}\\right)}\\\\ &{\\displaystyle=\\lVert\\mathrm{diag}(r)Q^{\\top}\\rVert_{r}^{2}+\\frac{1}{N^{2}}\\sum_{i,j=1}^{N}a_{i,j}^{2}-\\frac{2}{N^{2}}\\sum_{i=1;\\,j\\neq N_{0}(1)}^{N}Q_{i,i}\\operatorname*{diag}(r)\\left(Q^{\\top}\\right)_{,j}a_{i,j}}\\\\ &{\\displaystyle=\\frac{1}{N^{2}}\\mathrm{T}\\left(\\mathrm{diag}(r)Q^{\\top}\\mp\\operatorname*{diag}(r)Q^{\\top}\\right)+\\frac{1}{N^{2}}\\sum_{i,j=1}^{N}a_{i,j}^{2}}\\\\ &{\\displaystyle-\\frac{2}{N^{2}}\\sum_{i=1;\\,j\\neq N_{0}(1)}^{N}Q_{i,i}\\operatorname*{diag}(r)Q^{\\top}\\rangle_{,j}a_{i,j}}\\\\ &{\\displaystyle=\\frac{1}{N^{2}}\\frac{\\mathrm{T}\\left(Q^{\\top}Q\\operatorname*{diag}(r)Q^{\\top}\\right)}{\\mathrm{T}\\left(r\\right)}+\\frac{1}{N^{2}}\\sum_{i=1}^{N}a_{i,j}^{2}}\\\\ &{\\displaystyle-\\frac{2}{N^{2}}\\sum_{i=1}^{N}Q_{i}\\operatorname*{diag}(r)Q^{\\top}Q\\operatorname*{diag}(r)+\\frac{1}{N^{2}}\\sum_{i=1}^{N}a_{i,j}^{2}}\\\\ &{\\displaystyle=\\frac{2}{N^{2}}\\sum_{i=1}^{N}Q_{i}\\operatorname*{diag}(r)Q^{\\top}\\rangle_{,j}a_{i,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last equality follows the identity $\\forall B,D\\,\\in\\,\\mathbb{R}^{N\\times K}\\,:\\,\\,\\operatorname{Tr}(B D^{\\top})\\,=\\,\\operatorname{Tr}(D^{\\top}B)$ , with $B\\,=$ $Q\\,\\mathrm{diag}(r)Q^{\\top}Q\\,\\mathrm{diag}(r)$ and $D=Q^{\\top}$ . ", "page_idx": 20}, {"type": "text", "text": "The middle term in the last line is constant and can thus be omitted in the optimization process. The leftmost term in the last line is calculated by performing matrix multiplication from right to left, or by first computing $Q^{\\top}Q$ and then the rest of the product. Thus, the time complexity is $\\mathcal{O}(K^{2}N)$ and the largest matrix held in memory is of size $\\mathcal{O}(K N)$ . The rightmost term is calculated by message-passing, which has time complexity of ${\\mathcal{O}}(K E)$ . Thus, Computing the right-hand-side and its gradients with respect to $Q$ and $\\pmb{r}$ has a time complexity of $O(\\bar{K}^{2}N^{-}+K E)$ and a space complexity of $\\mathcal{O}(K N+E)$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "D Initializing the optimization with eigenvectors ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Next, we propose a good initialization for the GD minimization of (7). For that, we consider a corollary of the constructive soft weak regularity for grapohns, with the signal weight in the cut norm set to $\\beta=0$ , using all $L^{2}[0,1]$ functions as the soft affliiation model, and taking relative Frobenius error with $\\delta\\,=\\,0$ . In this case, the best rank- $K$ approximation theorem (Eckart\u2013Young\u2013Mirsky Theorem [51, Theorem 5.9]), the minimizer of the Frobenius error is the projection of $W$ upon its leading eigenvectors. This leads to the following corollary. ", "page_idx": 21}, {"type": "text", "text": "Corollary D.1. Let $A\\in[0,1]^{N\\times N}$ be a matrix with $\\mathrm{deg}(\\pmb{A})=N^{2}\\left\\|\\pmb{A}\\right\\|_{\\mathrm{F}}^{2}=E^{\\prime}$ . Let $K\\in\\mathbb{N},$ , let $m$ be sampled uniformly from $[K]$ , and let $R\\geq1$ such that $K/R\\in\\mathbb{N}$ . Let $\\phi_{1},\\ldots,\\phi_{m}$ be the leading eigenvectors of $\\pmb{A}$ , with eigenvalues $\\lambda_{1},\\ldots,\\lambda_{m}$ of highest magnitudes $|\\lambda_{1}|\\geq|\\lambda_{2}|\\geq\\ldots\\geq|\\lambda_{m}|$ , and let $\\begin{array}{r}{C^{*}=\\sum_{k=1}^{m}\\lambda_{k}\\phi_{k}\\boldsymbol{\\phi}_{k}^{\\intercal}}\\end{array}$ . Then, in probability $\\textstyle1-{\\frac{1}{R}}$ (with respect to the choice of $m$ ), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert A-C^{*}\\rVert_{\\Omega;N,E^{\\prime}}\\leq\\frac{3N}{2\\sqrt{E^{\\prime}}}\\sqrt{\\frac{R}{K}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The initialization is based on Corollary D.1 as follows. We consider the leading $K/3$ eigenvectors, assuming $K$ is divisible by 3, for reasons that will become clear shortly. Hence, for $C=\\Phi_{K/3}\\mathbf{\\Lambda}_{K/3}\\Phi_{K/3}^{\\mathrm{T}}$ , where $\\bar{\\Phi_{K/3}}\\in\\mathbb{R}^{N\\times K/3}$ is the matrix consisting of the leading eigenvectors of $\\pmb{A}$ as columns, and $\\mathbf{A}_{K/3}\\in\\mathbb{R}^{K/3\\times K/3}$ the diagonal matrix of the leading eigenvalues, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert A-C\\rVert_{\\boxed{\\Omega;N,E}}<\\frac{3N}{2\\sqrt{E^{\\prime}}}\\sqrt{\\frac{3R}{K}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To obtain soft affiliations with values in $[0,1]$ , we now initialize $Q$ as described in (8). ", "page_idx": 21}, {"type": "text", "text": "E Learning ICG with subgraph SGD ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we prove that the gradients of the subgraph SGD approximate the gradients of the full GD method. ", "page_idx": 21}, {"type": "text", "text": "Proposition E.1. Let $0<p<1$ . Under the above setting, if we restrict all entries of $C,\\,P,\\,Q,\\,$ $\\pmb{r}$ and $\\pmb{F}$ to be in $[0,1]$ , then in probability at least $1-p,$ for every $k\\in[K]$ , $d\\in[D]$ and $m\\in[M]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla_{q_{n m,k}}L-\\displaystyle\\frac{M}{N}\\nabla_{q_{n m,k}}L^{(n)}\\right|\\leq\\displaystyle\\frac{4}{N}\\sqrt{\\frac{2\\log(1/p_{3})+2\\log(N)+2\\log(K)+2\\log(2)}{M}},}\\\\ &{\\qquad\\quad\\left|\\nabla_{r_{k}}L-\\nabla_{r_{k}}L^{(n)}\\right|\\leq4\\sqrt{\\frac{2\\log(1/p_{1})+2\\log(N)+2\\log(K)+2\\log(2)}{M}},}\\\\ &{\\qquad\\quad\\left|\\nabla_{f_{k,d}}L-\\nabla_{f_{k,d}}L^{(n)}\\right|\\leq\\displaystyle\\frac{4\\lambda}{D}\\sqrt{\\frac{2\\log(1/p_{2})+2\\log(K)+2\\log(D)+2\\log(2)}{M}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proposition E.1 means that the gradient with respect to $\\pmb{r}$ and $\\pmb{F}$ computed at each SGD step approximate the full GD gradients. The gradients with respect to $Q$ approximate a scaled version of the full gradients, and only on the sampled nodes, where the unsampled nodes are not updated in the SGD step. This means that when optimizing the ICG with SGD, we need to scale the gradients with resect to $Q$ of the loss by $\\frac{M}{N}$ . To use the same entry-wise learning rate in SGD as in GD one must multiply the loss (9) by $M/N$ . Hence, SGD needs in expectation $N/M$ times the number of steps that GD requires. This means that in SGD we trade the memory complexity (reducing it by a factor $M/N)$ by time complexity (increasing it by a factor $N/M)$ . Note that slower run-time, while not desirable, can be tolerated, while higher memory requirement is often prohibitive due to hardware limitations. ", "page_idx": 21}, {"type": "text", "text": "From here until the end of this section we prove Proposition E.1. For that, we compute the gradients of the full loss $L$ and the sampled loss $L^{(\\Bar{n})}$ . ", "page_idx": 22}, {"type": "text", "text": "To avoid tensor notations, we treat by abuse of notation the gradient of a function as a linear operator. Recall that the differential $\\mathcal{D}_{r}\\pmb{T}=\\dot{\\mathcal{D}}_{r}\\pmb{T}(\\pmb{r}^{\\prime})$ of the function $\\pmb{T}:\\mathbb{R}^{R}\\rightarrow\\mathbb{R}^{U}$ at the point $\\pmb{r}^{\\prime}\\in\\mathbb{R}^{R}$ is the unique linear operator $\\mathbb{R}^{R}\\to\\mathbb{R}^{U}$ that describes the linearization of $\\textbf{\\emph{T}}$ about $\\pmb{r}^{\\prime}$ via ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\pmb T}({\\pmb r}^{\\prime}+\\epsilon{\\pmb r}^{\\prime\\prime})={\\pmb T}({\\pmb r}^{\\prime})+\\epsilon{\\pmb T}_{\\pmb r}{\\pmb T}{\\pmb r}^{\\prime\\prime}+o(\\epsilon)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\epsilon\\in\\mathbb{R}$ . Given an inner product in $\\mathbb{R}^{R}$ , the gradient $\\nabla_{\\boldsymbol{r}}\\mathbf{T}=\\nabla_{\\boldsymbol{r}}T(\\boldsymbol{r^{\\prime}})\\in\\mathbb{R}^{U\\times R}$ of the function $\\pmb{T}:\\mathbb{R}^{R}\\rightarrow\\mathbb{R}^{U}$ at the point $\\pmb{r}^{\\dot{\\prime}}\\in\\mathbb{R}^{R}$ is defined to be the vector (guaranteed to uniquely exist by the Riesz representation theorem) that satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ensuremath{\\mathcal{D}_{r}}\\ensuremath{\\mathbf{T}}\\ensuremath{\\mathbf{r}}^{\\prime\\prime}=\\langle\\ensuremath{\\mathbf{r}}^{\\prime\\prime},\\nabla_{r}T\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for every $\\pmb{r}^{\\prime\\prime}\\in\\mathbb{R}^{R}$ . Here, the inner product if defined row-wise by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle\\pmb{r}^{\\prime\\prime},\\nabla_{\\pmb{r}}\\pmb{T}\\rangle:=\\left(\\,\\langle\\pmb{r}^{\\prime\\prime},\\nabla_{\\pmb{r}}\\pmb{T}[u,:]\\rangle\\,\\right)_{u=1}^{U},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pmb{T}[u,:]$ is the $u$ -th row of $\\nabla_{r}T$ . In our analysis, by abuse of notation, we identify $\\nabla_{r}T$ with $\\mathcal{D}_{r}T$ for a fixed given inner product. ", "page_idx": 22}, {"type": "text", "text": "Define the inner product between matrices ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle B,D\\rangle_{2}=\\sum_{i,j}b_{i,j}d_{i,j}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Denote ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(C,P)=\\left\\|A-C\\right\\|_{\\mathrm{F}}^{2}+\\lambda\\left\\|S-P\\right\\|_{\\mathrm{F}}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, the gradient of $\\mathrm{\\lVert}A-C\\mathrm{\\rVert_{F}^{2}}$ with respect to $_{C}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{C}\\left\\|A-C\\right\\|_{\\mathrm{F}}^{2}=-{\\frac{2}{N^{2}}}(A-C),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is identified by abuse of notation by the linear functional $\\mathbb{R}^{N^{2}}\\rightarrow\\mathbb{R}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nD\\mapsto\\nabla_{C}\\left\\|A-C\\right\\|_{\\mathrm{F}}^{2}\\left(D\\right)=\\left\\langle D,-{\\frac{2}{N^{2}}}(A-C)\\right\\rangle_{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{P}\\left\\|S-P\\right\\|_{\\mathrm{F}}^{2}=-{\\frac{2}{N D}}(S-P):\\mathbb{R}^{N D}\\rightarrow\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Given any parameter $z\\in\\mathbb{R}^{R}$ on which the graph $C=C(z)$ and the signal $P=P(z)$ depend, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{z}L(C,P)=\\frac{2}{N^{2}}(C-A)\\nabla_{z}C+\\lambda\\frac{2}{N D}(P-S)\\nabla_{z}P,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\nabla_{z}C:\\mathbb{R}^{R}\\,\\rightarrow\\,\\mathbb{R}^{N^{2}}$ and $\\nabla_{z}C\\,\\in\\,\\mathbb{R}^{R}\\,\\rightarrow\\,\\mathbb{R}^{N D}$ are linear operators, and the products in (22) are operator composition, namely, in coordinates it is elementwise multiplication (not matrix multiplication). ", "page_idx": 22}, {"type": "text", "text": "Let us now compute the gradients of $\\boldsymbol{C}=\\boldsymbol{Q}\\,\\mathrm{diag}(r)\\boldsymbol{Q}^{\\intercal}$ and $P=Q F$ with respect to $Q,r$ and $\\pmb{F}$ in coordinates. We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{r}_{k}}c_{i,j}=q_{i,k}q_{j,k},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{q_{m,k}}c_{i,j}=r_{k}\\delta_{i-m}q_{j,k}+r_{k}\\delta_{j-m}q_{i,k},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\delta_{l}$ is 1 if $l=1$ and zero otherwise. Moreover, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{f_{k,l}}p_{i,d}=q_{i,k}\\delta_{l-d},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{q_{m,k}}p_{i,d}=f_{k,d}\\delta_{i-m}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{r_{k}}L=\\frac{2}{N^{2}}\\sum_{i,j=1}^{N}(c_{i,j}-a_{i,j})q_{i,k}q_{j,k},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n7_{q_{m,k}}L=\\frac{2}{N^{2}}\\sum_{j=1}^{N}(c_{m,j}-a_{m,j})r_{k}q_{j,k}+\\frac{2}{N^{2}}\\sum_{i=1}^{N}(c_{i,m}-a_{i,m})r_{k}q_{i,k}+\\lambda\\frac{2}{N D}\\sum_{d=1}^{D}(p_{m,d}-s_{m,d})f_{k,d},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{f_{k,l}}L=\\lambda\\frac{2}{N D}\\sum_{i=1}^{N}(p_{i,l}-s_{i,l})q_{i,k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{r}_{k}}L^{(n)}=\\frac{2}{M^{2}}\\sum_{i,j=1}^{M}(c_{n_{i},n_{j}}-a_{n_{i},n_{j}})q_{n_{i},k}q_{n_{j},k},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla_{q_{n_{m},k}}L^{(n)}=\\displaystyle\\frac{2}{M^{2}}\\sum_{j=1}^{M}(c_{n_{m},n_{j}}-a_{n_{m},n_{j}})c_{k}q_{n_{j},k}\\ ~~}}\\\\ {{\\ ~~~~~~~~~~~~~~~+\\displaystyle\\frac{2}{M^{2}}\\sum_{i=1}^{M}(c_{n_{i},n_{m}}-a_{n_{i},n_{m}})c_{k}q_{n_{i},k}+\\lambda\\displaystyle\\frac{2}{M D}\\sum_{d=1}^{D}(p_{n_{m},d}-s_{n_{m},d})f_{k,d},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{f_{k,l}}L^{(n)}=\\lambda\\frac{2}{M D}\\sum_{i=1}^{M}(p_{n_{i},l}-s_{n_{i},l})q_{n_{i},k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We next derive the convergence analysis, based on Hoeffding\u2019s Inequality and two Monte-Carlo approximation lemmas. ", "page_idx": 23}, {"type": "text", "text": "Theorem E.2 (Hoeffding\u2019s Inequality). Let $Y_{1},\\ldots,Y_{M}$ be independent random variables such that $a\\leq Y_{m}\\leq b$ almost surely. Then, for every $k>0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\Big|\\frac{1}{M}\\sum_{m=1}^{M}(Y_{m}-\\mathbb{E}[Y_{m}])\\Big|\\geq k\\Big)\\leq2\\exp\\Big(-\\frac{2k^{2}M}{(b-a)^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The following is a standard Monte Carlo approximation error bound based on Hoeffding\u2019s inequality. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.3. Let $\\lbrace i_{m}\\rbrace_{m=1}^{M}$ be uniform i.i.d in $[N]$ . Let $\\pmb{v}\\in\\mathbb{R}^{N}$ be a vector with entries $v_{n}$ in the set $[-1,1]$ . Then, for every $0<p<1$ , in probability at least $1-p$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{M}\\sum_{m=1}^{M}v_{i_{m}}-\\frac{1}{N}\\sum_{n=1}^{N}v_{n}\\right|\\leq\\sqrt{\\frac{2\\log(1/p)+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. This is a direct result of Hoeffding\u2019s Inequality on the i.i.d. variables $\\{v_{i_{m}}\\}_{m=1}^{M}$ ", "page_idx": 23}, {"type": "text", "text": "The next lemma derives a Monte Carlo approximation error bound based on Hoeffding\u2019s inequality for 2D arrays, in case one samples only 1D independent sample points, and the 2D sample points are all pairs of the 1D points (which are hence no longer independent). ", "page_idx": 23}, {"type": "text", "text": "Lemma E.4. Let $\\{i_{m}\\}_{m=1}^{M}$ be uniform i.i.d in $[N]$ . Let $A\\in\\mathbb{R}^{N\\times N}$ be symmetric with $a_{i,j}\\in[-1,1]$ Then, for every $0<p<1$ , in probability more than $1-p$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N^{2}}\\sum_{j=1}^{N}\\sum_{n=1}^{N}a_{j,n}-\\frac{1}{M^{2}}\\sum_{m=1}^{M}\\sum_{l=1}^{M}a_{i_{m},i_{l}}\\right|\\leq2\\sqrt{\\frac{2\\log(1/p)+2\\log(N)+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $0\\ <\\ p\\ <\\ 1$ . For each fixed $\\textit{n}\\in\\:[N]$ , consider the independent random variables $Y_{m}^{n}=a_{i_{m},n}$ , with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}(Y_{m}^{n})=\\frac{1}{N}\\sum_{j=1}^{N}a_{j,n},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and $-1\\leq Y_{m}\\leq1$ . By Hoeffding\u2019s Inequality, for $\\begin{array}{r}{k=\\sqrt{\\frac{2\\log(1/p)+2\\log(N)+2\\log(2)}{M}}}\\end{array}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{j=1}^{N}a_{j,n}-\\frac{1}{M}\\sum_{m=1}^{M}a_{i_{m},n}\\right|\\leq k\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "in an event ${\\mathcal{E}}_{n}$ of probability more than $1-p/N$ . Intersecting the events $\\{\\mathcal{E}_{n}\\}_{n=1}^{N}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall n\\in[N]:\\quad\\left|\\frac{1}{N}\\sum_{j=1}^{N}a_{j,n}-\\frac{1}{M}\\sum_{m=1}^{M}a_{i_{m},n}\\right|\\leq k\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "in the event $\\mathcal{E}=\\cap_{n}\\mathcal{E}_{n}$ probability at least $1-p$ . Hence, by the triangle inequality, we also have in the event $\\mathcal{E}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N M}\\sum_{l=1}^{M}\\sum_{j=1}^{N}a_{j,i_{l}}-\\frac{1}{M^{2}}\\sum_{l=1}^{M}\\sum_{m=1}^{M}a_{i_{m},i_{l}}\\right|\\leq k,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N^{2}}\\sum_{n=1}^{N}\\sum_{j=1}^{N}a_{j,n}-\\frac{1}{N M}\\sum_{n=1}^{N}\\sum_{m=1}^{M}a_{i_{m},n}\\right|\\leq k.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, by the symmetry of $\\pmb{A}$ and by the triangle inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N^{2}}\\sum_{n=1}^{N}\\sum_{j=1}^{N}a_{j,n}-\\frac{1}{M^{2}}\\sum_{l=1}^{M}\\sum_{m=1}^{M}a_{i_{m},i_{l}}\\right|\\leq2k.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, since all entries of $A,S,C,P,Q,\\i$ $\\pmb{r}$ and $\\pmb{F}$ are in $[0,1]$ , we may use Lemmas E.3 and E.4 to derive approximation errors for the gradients of $L$ . Specifically, for any $0<p_{1}<1$ , for every $k\\in[K]$ there is an event $\\mathcal{A}_{k}$ of probability at least $1-p_{1}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\nabla_{r_{k}}L-\\nabla_{r_{k}}L^{(n)}\\right|\\leq4\\sqrt{\\frac{2\\log(1/p_{1})+2\\log(N)+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, for every $k\\in[K]$ and $j\\in[D]$ , and every $0<p_{2}<1$ there is an event $\\mathcal{C}_{k,j}$ of probability at least $1-p_{2}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\nabla_{f_{k,l}}L-\\nabla_{f_{k,l}}L^{(n)}\\right|\\leq\\frac{4\\lambda}{D}\\sqrt{\\frac{2\\log(1/p_{2})+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the approximation analysis of $\\nabla_{q_{n_{m},l}}L$ , note that the index $n_{m}$ is random, so we derive a uniform convergence analysis for all possible values of $n_{m}$ . For that, for every $n\\in[N]$ and $k\\in[K]$ , define the vector ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\widetilde{\\nabla_{q_{n,k}}L^{(n)}}=\\displaystyle\\frac{2}{M^{2}}\\sum_{j=1}^{M}(c_{n,n_{j}}-a_{n,n_{j}})c_{k}q_{n_{j},k}}}\\\\ {{\\displaystyle\\qquad\\qquad+\\,\\displaystyle\\frac{2}{M^{2}}\\sum_{i=1}^{N}(c_{n_{i},n}-a_{n_{i},n})c_{k}q_{n_{i},k}+{\\lambda\\displaystyle\\frac{2}{M D}\\sum_{d=1}^{D}}\\big(p_{n,d}-s_{n,d}\\big)f_{k,d},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\nabla_{\\boldsymbol{q}_{n,k}}L^{(n)}$ is not a gradient of $L^{(n)}$ (since if $n$ is not a sample from $\\{n_{m}\\}$ the gradient must be zero), but is denoted with $\\nabla$ for its structural similarity to $\\nabla_{\\boldsymbol{q}_{n_{m},k}}L^{(n)}$ . Let $0<p_{3}<1$ . By ", "page_idx": 24}, {"type": "text", "text": "Lemma E.3, for every $k\\in[K]$ there is an event $B_{k}$ of probability at least $1-p_{3}$ such that for every $n\\in[N]$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bigg|\\nabla_{q_{n,k}}L-\\frac{M}{N}\\widetilde{\\nabla_{q_{n,k}}L^{(n)}}\\bigg|\\leq\\frac{4}{N}\\sqrt{\\frac{2\\log(1/p_{3})+2\\log(N)+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This means that in the event $\\boldsymbol{{\\beta}}_{k}$ , for every $m\\in[M]$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\nabla_{q_{n_{m},k}}L-\\frac{M}{N}\\nabla_{q_{n_{m},k}}L^{(n)}\\right|\\leq\\frac{4}{N}\\sqrt{\\frac{2\\log(1/p_{3})+2\\log(N)+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lastly, given $0<p<1$ , choosing $p_{1}=p_{3}=p/3K$ and $p_{2}=p/3K D$ and intersecting all events for all corrdinates gives and event $\\mathcal{E}$ of probability at least $1-p$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\nabla_{r_{k}}L-\\nabla_{r_{k}}L^{(n)}\\right|\\leq4\\sqrt{\\frac{2\\log(1/p_{1})+2\\log(N)+2\\log(K)+2\\log(2)}{M}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\nabla_{f_{k,l}}L-\\nabla_{f_{k,l}}L^{(n)}\\right|\\leq\\frac{4\\lambda}{D}\\sqrt{\\frac{2\\log(1/p_{2})+2\\log(K)+2\\log(D)+2\\log(2)}{M}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\nabla_{q_{n m,k}}L-\\frac{M}{N}\\nabla_{q_{n m,k}}L^{(n)}\\right|\\leq\\frac{4}{N}\\sqrt{\\frac{2\\log(1/p_{3})+2\\log(N)+2\\log(K)+2\\log(2)}{M}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "F Additional experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Node classification ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Setup. We evaluate ICG-NN and $\\mathrm{ICG_{u}}$ -NN on the non-sparse graphs tolokers [43], squirrel [45] and twitch-gamers [36]. We follow the 10 data splits of Platonov et al. [43] for tolokers, the 10 data splits of Pei et al. [42], Li et al. [34] for squirrel and the 5 data splits of Lim et al. [36], Li et al. [34] for twitch-gamers. We report the mean ROC AUC and standard deviation for tolokers and the accuracy and standard deviation for squirrel and twitch-gamers. We also evaluate the relative Frobenius error between the graph and the ICG as a measure of the quality of the ICG approximation. ", "page_idx": 25}, {"type": "text", "text": "The baselines MLP, GCN [30], GAT [53], $\\mathrm{H_{2}G C N}$ [69], GPR-GNN [14], LINKX [36], GloGNN [34] are taken from Platonov et al. [43] for tolokers and from Li et al. [34] for squirrel, twitch-gamers and penn94. We use the Adam optimizer and report all hyperparameters in Appendix I. ", "page_idx": 25}, {"type": "text", "text": "Results. All results are reported in Table 2. Observe that ICG-NNs achieve state-of-the-art results across the board, despite the low ratio of edge (graph densities of $2.41\\cdot10^{-4}$ to $8.02\\cdot10^{-3})$ ). ICGNNs surpass the performance of more complex GNN models such as GT, LINKX and GloGNN, which solidify ICG-NNs as a strong method and a possible contender to message-passing neural networks. ", "page_idx": 25}, {"type": "text", "text": "F.2 Comparison with graph coarsening methods over large graphs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we highlight the main differences and similarities between graph coarsening methods and ICG-NNs, followed by an empirical comparison over large graphs. ", "page_idx": 25}, {"type": "text", "text": "F.2.1 Conceptual comparison ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Graph coarsening methods [18, 25], graph condensation methods [26] and graph summarization methods [68] replace the original graph by one which has fewer nodes, with different features and a different topological structure, while sometimes preserving certain properties, such as the degree distribution [68]. ", "page_idx": 25}, {"type": "text", "text": "Theoretical guarantees. Graph coarsening methods do not typically stem from theoretical guarantees, whereas ICG-NNs provably approximate the graph. ", "page_idx": 25}, {"type": "text", "text": "Computational complexity. Graph coarsening methods usually account for the graph\u2019s structure by apply message-passing on the coarsened graph. Condensation methods require $\\bar{O}(\\bar{E}M)$ operations to construct a smaller graph [26, 67, 56], where $E$ is the number of edges of the original graph and $M$ is the number of nodes of the condensed graph. Conversely, ICGs estimate the ICG with $\\mathcal{O}(K^{2}N+K E)$ operations, where $K$ is typically smaller than $M$ . ", "page_idx": 25}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/db4ec1e51ce6b9dd9e31a013e3b527518abe3019752eae0ebe0476189787ccb2.jpg", "table_caption": ["Table 2: Results on large graphs. Top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/2b7b1d11678bda243f7a8496b8bc4a307eefd3ff5fb220a78f80bfde72163633.jpg", "table_caption": ["Table 3: Comparison with graph coarsening methods over large graphs. Top three models are colored by First, Second, Third. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Node processing. Graph coarsening methods process representations on an iteratively coarsened graph, while ICG-NN also process the fine node information at every layer. ", "page_idx": 26}, {"type": "text", "text": "Graphs larger than memory. ICG-NNs offer a subgraph sampling approach when the original graph cannot fit in memory. In contrast, the aforementioned methods lack a strategy for managing smaller data structures when computing the compressed graph. ", "page_idx": 26}, {"type": "text", "text": "F.2.2 Empirical comparison ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Setup. We evaluate ICG-NN and $\\mathrm{ICG_{u}}$ -NN on the large graph datasets Flickr [65] and Reddit [23].   \nWe follow the splits of [67] and report the accuracy and standard deviation. ", "page_idx": 26}, {"type": "text", "text": "The standard graph coarsening baselines Coarsening [25], Random [25], Herding [58], K-Center [48], One-Step [27] and the graph condensation baselines DC-Graph [66], GCOND [26], SFGC [67] and GC-SNTK [56] are taken from [67, 56]. We use the Adam optimizaer and report all hyperparameters in Appendix I. ", "page_idx": 26}, {"type": "text", "text": "Results. ICG-NNs present state-of-the-art performance in Table 3 when compared to a vareity of both graph coarsening methods and graph condensation methods, further solidifying its effectiveness. ", "page_idx": 26}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/c8c1c1cf468aa51f586d9bab06bfcd3ee7fe187ae59e6443d7f3c77788dc9455.jpg", "img_caption": ["Figure 4: Test ROC AUC of tolokers (left) and test accuracy of squirrel (right) as a function of the number of communities. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "F.3 Node classification on large graphs using Subgraph SGD ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Setup. We evaluate ICG-NN and $\\mathrm{ICG_{u}}$ -NN on the large graph Flickr [65]. We follow the splits of [67] and report the accuracy and standard deviation. We set the ratio of condensation $\\begin{array}{r}{r\\,=\\,\\frac{M}{N}}\\end{array}$ to $1\\%$ , where $N$ and $M$ are the number of nodes in the graph and the number of sampled nodes. ", "page_idx": 27}, {"type": "text", "text": "Table 4: Results on Flickr using $1\\%$ node sampling. Top three models are colored by First, Second, Third. ", "page_idx": 27}, {"type": "text", "text": "The graph coarsening baselines Coarsening [25], Random [25], Herding [58], K-Center [48], One-Step [27] and the graph condensation baselines DC-Graph [66], GCOND [26], SFGC [67] and GC-SNTK [56] are taken from [67, 56]. We use the Adam optimizaer and report all hyperparameters in Appendix I. ", "page_idx": 27}, {"type": "text", "text": "Results. Table 4 shows that $\\mathrm{ICG_{u}}$ -NN with subgraph ICGm with $1\\%$ sampling rate, achieves competitive performance with coarsening and condensation methods that operate on the full graph in memory. ", "page_idx": 27}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/cfa3d9da3fbe68214adf9c7998652dcc28f4d56df493698889a6108388e3e045.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F.4 Ablation study over the number of communities ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Setup. We evaluate ICG-NN and $\\mathrm{ICG_{u}}$ -NN on non-sparse graphs tolokers [43] and squirrel [45]. We follow the 10 data splits of Platonov et al. [43] for tolokers and Pei et al. [42], Li et al. [34] for squirrel. We report the the mean ROC AUC and standard deviation for tolokers and the accuracy and standard deviation for squirrel. ", "page_idx": 27}, {"type": "text", "text": "Results. Figure 4 shows several key trends across both datasets. First, as anticipated, when a very small number of communities is used, the ICG fails to approximate the graph accurately, resulting in degraded performance. Second, the optimal performance is observed when using a relatively small number of communities, specifically 25 for tolokers and 75 for squirrel. Lastly, using a large number of communities does not appear to negatively impact performance. ", "page_idx": 27}, {"type": "text", "text": "F.5 The cut-norm and Frobenius norm ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While the cut norm is the theoretical target of the ICG approximation, it is expensive to estimate directly. However, Theorem 3.1 implies that optimizing the Frobenius norm, which is computationally inexpensive, leads to a small cut norm. ", "page_idx": 27}, {"type": "text", "text": "In this experiment, we first aim to demonstrate the positive correlation between the Frobenius error and the cut norm error. Moreover, we show that even though the Frobenius error cannot be made close to zero in general, it is still correlated with cut norm. We show that when the relative Frobenius error is reduced below a specified threshold (not close to zero in general), the ICG approximation allows ICG-NNs to achieve state-of-the-art results. ", "page_idx": 27}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/d1153d5271b9292804c6ca27024909204dbd19a3889a96001cadee12021a4c9e.jpg", "img_caption": ["Figure 5: Cut-norm as a function of Frobenius norm on the tolokers (left) and squirrel (right) datasets. The number of communities used is indicated next to each point. "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/e133cc5e2c6a22c392a620d86411948e9094bd33899ffe90e7109dac07888f6d.jpg", "table_caption": ["Table 5: Time until convergence in seconds on large graphs. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Setup. We evaluate our ICG approximation on non-sparse graphs tolokers [43] and squirrel [45]. We vary the number of communities used by the approximation and report the relative cut-norm (estimated by [3]) as a function of the relative Frobenius norm. We also present the resulting relative Frobenius error for our node classification experiments in Table 2. ", "page_idx": 28}, {"type": "text", "text": "Results. As expected, the cut-norm and Frobenius norm are positively correlated, both decreasing as more communities are used, as shown in Figure 5. We also observe in Table 2 that ICG-NNs perform best on the squirrel dataset and worst on twitch-gamers, aligning with their relative Frobenius error. The lower the relative Frobenius error, the more accurate the ICG approximation, leading to better performance. Table 2 and Figure 5 also revealed a useful rule of thumb: when the relative Frobenius error is below 0.8, it correlates with a very small cut norm error. This, in turn, leads to an accurate ICG approximation, enabling ICG-NNs to achieve state-of-the-art results. ", "page_idx": 28}, {"type": "text", "text": "F.6 Initialization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We repeated the node classification experiments presented in Table 2, using random initialization when optimizing the ICG, and compared the results to those obtained with eigenvector initialization. ", "page_idx": 28}, {"type": "text", "text": "Setup. We evaluate ICG-NN and $\\mathrm{ICG_{u}}$ -NN on the non-sparse graphs tolokers [43], squirrel [45] and twitch-gamers [36]. We follow the 10 data splits of Platonov et al. [43] for tolokers, the 10 data splits of Pei et al. [42], Li et al. [34] for squirrel and the 5 data splits of Lim et al. [36], Li et al. [34] for twitch-gamers. We report the mean ROC AUC and standard deviation for tolokers and the accuracy and standard deviation for squirrel and twitch-gamers. ", "page_idx": 28}, {"type": "text", "text": "Results. Table 5 indicates that eigenvector initialization generally outperforms random initialization across all datasets. While the improvements are minimal in some cases, such as with the tolokers and squirrel datasets, they are more pronounced in the twitch-gamers dataset. ", "page_idx": 28}, {"type": "text", "text": "Additionally, eigenvector initialization offers a practical advantage in terms of training efficiency. On average, achieving the same loss value requires $5\\%$ more time when using random initialization compared to eigenvector initialization. This efficiency gain further supports the utility of the proposed initialization method. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "F.7 Run-time analysis ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this subsection, we compare the forward pass run times of our ICG approximation process, signal processing pipeline ( $\\mathrm{ICG_{u}}$ -NN) and the GCN [30] architecture. We sample graphs of sizes up to 7,000 from a dense and a sparse Erdo\u02dds-R\u00e9nyi distribution $\\mathrm{ER}(n,p(n)=0.5)$ and $\\begin{array}{r}{\\dot{\\mathrm{ER}}(n,p(n)=\\frac{50}{n})}\\end{array}$ , correspondingly. Node features are independently drawn from $U[0,1]$ and the initial feature dimension is 128. $\\mathrm{ICG_{u}}$ -NN and GCN use a hidden dimension of 128, 3 layers and an output dimension of 5. ", "page_idx": 29}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/15bc1ccd9bb50cbdea4ab1697fd9b0f87f42b34e8b8ad0717ca907c737f49639.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 6: Empirical runtimes: K-ICG approximation process as a function of GCN forward pass duration on the dense (left) and sparse (right) Erd\u02ddos-R\u00e9nyi distribution $\\mathrm{ER}(n,p(n)\\,=\\,0.5)$ and $\\begin{array}{r}{\\mathrm{ER}(n,p(n)=\\frac{50}{n})}\\end{array}$ for $K{=}10$ , 100. ", "page_idx": 29}, {"type": "text", "text": "Results. Figure 6 demonstrates a strong linear relationship between the runtime of our ICG approximation and that of the message-passing neural network GCN for both sparse and dense graphs. Unlike GCN, our ICG approximation is versatile for multiple tasks and requires minimal hyperparameter tuning, which reduces its overall time complexity. Additionally, Appendix F.7 reveals a strong square root relationship between the runtime of $\\mathrm{ICG}_{\\mathrm{u}}{\\cdot}\\mathrm{NN}$ and the runtime of GCN for both sparse and dense graphs. This aligns with our expectations, as the time complexity of GCN is $\\mathcal{O}(E)$ , while the time complexity of ICG-NNs is $\\mathcal{O}(N)$ , highlighting the computational advantage of using ICGs over message-passing neural networks. ", "page_idx": 29}, {"type": "text", "text": "F.8 Time until convergence ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Setup. We measured the average time until convergence in seconds for the GCN and $\\mathrm{ICG_{u}}$ -NN architectures on the non-sparse graphs tolokers [43], squirrel [45] and twitch-gamers [36]. We follow the 10 data splits of Platonov et al. [43] for tolokers, the 10 data splits of Pei et al. [42], Li et al. [34] for squirrel and the 5 data splits of Lim et al. [36], Li et al. [34] for twitch-gamers. We ended the training after 50 epochs in which the validation metric did not improve. We set the hyper-parameters with best validation results. ", "page_idx": 29}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/3c0c4ef0e6630815fcb65e1914b0f4c5b5e333b6086c622d3bef8a3c22446f92.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "", "img_caption": ["Figure 7: Empirical runtimes: $\\mathrm{K-ICG_{u}}$ -NN as a function of GCN forward pass duration on the dense (left) and sparse (right) Erd\u02ddos-R\u00e9nyi distribution $\\mathrm{ER}(n,p(n)=0.5)$ and $\\begin{array}{r}{\\mathrm{ER}(n,p(n)\\,=\\,\\frac{50}{n})}\\end{array}$ for ${\\mathrm{K}}{=}10$ , 100. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/cd6288eb7bf01ff0256f4cad00fa6c61538a0b3d9b9ec3cb6095437a3dff2cd3.jpg", "table_caption": ["Table 6: Time until convergence in seconds on large graphs. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Results. Table 6 shows that $\\mathrm{ICG_{u}}$ -NN consistently converges faster than GCN across all three benchmarks. Specifically, converges approximately 15 times faster on the tolokers dataset, 17 times faster on the squirrel dataset, and 25 times faster on the twitch-gamers dataset compared to GCN, indicating a significant improvement in efficiency. ", "page_idx": 30}, {"type": "text", "text": "F.9 Memory allocation analysis ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this subsection, we compare the memory allocated during the ICG approximation process and during a forward pass of the GCN [30] architecture. We sample graphs of sizes up to 2,000 from a dense Erd\u02ddos-R\u00e9nyi distribution $\\mathrm{ER}(n,p(n)=0.5)$ ). Node features are independently drawn from $U[0,1]$ and the initial feature dimension is 128. $\\mathrm{ICG_{u}}$ -NN and GCN use a hidden dimension of 128, 3 layers and an output dimension of 5. ", "page_idx": 30}, {"type": "image", "img_path": "pGR5X4e1gy/tmp/c334d30457c9303457a85ab5a0f8b5e5e645b22e209f6607f160219de7849b54.jpg", "img_caption": ["Figure 8: Memory allocated for K-ICG approximation $_{\\mathrm{K=10,100}}$ ) as a function of the memory allocated for GCN on graphs $G\\sim\\mathrm{ER}(n,p(\\bar{n})=0.5)$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Results. Appendix F.9 demonstrates a strong linear relationship between the the memory allocated during the ICG approximation process and during a forward pass of the message-passing neural network GCN. This aligns with our expectations: the memory allocation for the GCN architecture is $\\mathcal{O}(E D)$ , and of ICG-NNs $\\mathcal{O}(E K+N K D)$ , where $D$ is the feature dimension and $K$ is the number of communities used for the ICG approximation. ", "page_idx": 30}, {"type": "text", "text": "G Additional related work ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Latent graph GNNs. One way to interpret our ICG-NNs are as latent graph methods, where the ICG approximation of the given graph $G$ is seen as a latent low complexity \u201cclean version\u201d of the observed graph $G$ . ", "page_idx": 31}, {"type": "text", "text": "In latent graph methods, the graph on which the GNN is applied is inferred from the given data graph, or from some raw non-graph data. Two motivations for latent graph methods are (1) the given graph is noisy, and the latent graph is its denoised version, or (2) even if the given graph is \u201caccurate\u201d, the inferred graph improves the efficiency of message passing. Our ICG GNN can be cast in category (2), since we use the latent ICG model to define an efficient \u201cmessage passing\u201d scheme, which has linear run-time with respect to the number of nodes. ", "page_idx": 31}, {"type": "text", "text": "Some examples of latent graph methods are papers like [31, 57, 16, 10, 17] which learn a latent graph from non-graph raw data. Papers like [39, 59, 28, 21] treat the given graph as noisy or partially observed, and learn the \u201cclean\u201d graph. Papers like [54, 50] modify the given graph to improve the efficiency of message passing (e.g., based on the notion of oversquashing [5]), and [41] modifies the weights of the given graph if the nodes were sampled non-uniformly from some latent geometric graph. Additionaly, attention based GNN methods, which dynamically choose edge weights [53, 55, 52, 64], can be interpreted as latent graph approaches. ", "page_idx": 31}, {"type": "text", "text": "H Dataset statistics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The statistics of the real-world node-classification, spatio-temporal and graph coarsening benchmarks used can be found in Tables 7 to 9. ", "page_idx": 31}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/0c276cb64a9471569e7a91e1d00ea40ed17ac94c021dd6bbe32b692bb61f820c.jpg", "table_caption": ["Table 7: Statistics of the node classification benchmarks. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/7782bba86a84975a06ea2e845bd3303c4e3167382d11ae4312a1950f610f0e26.jpg", "table_caption": ["Table 8: Statistics of the spatio-temporal benchmarks. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/bde9b28e3f014cdb0c0a9c3455aa7af422066407bce80df86b5bd680d25e22ed.jpg", "table_caption": ["Table 9: Statistics for graph coarsening benchmarks. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "I Hyperparameters ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Tables 10 to 12, we report the hyper-parameters used in our real-world node-classification, spatiotemporal and graph coarsening benchmarks. ", "page_idx": 32}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/1b464cf22771e2121f40d3ba8a1f322f384d8d8a6fe7fb2d91705906912afa40.jpg", "table_caption": ["Table 10: Hyper-parameters used for the node classification benchmarks. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/6eaf09447e13d9a1c3b6100363cb0c10c373d8b2971128613207b8fef4ac64ce.jpg", "table_caption": ["Table 11: Hyper-parameters used for the spatio-temporal benchmarks. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "pGR5X4e1gy/tmp/781683fce61f3f43101ae14e7e0424dc4725b7ab14589cd39bc2316b3cfbe28a.jpg", "table_caption": ["Table 12: Hyper-parameters used for graph coarsening benchmarks. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "For spatio-temporal dataset, we follow the methodology described in [15], where the time of the day and the one-hot encoding of the day of the week are used as additional features. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 33}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 33}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 33}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 33}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 33}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 33}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We prove our proposed constructive version of the Weak Graph Regularity Lemma. We use the construction to then develop a new pipeline for learning on large non-sparse graphs efficiently, following what is written in the abstract. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the model\u2019s limitations in the conclusions section. Primarily, our approach is constrained to the single, relatively large, undirected graph setting. Consequently, this restricts the number of datasets we can experiment on and, by extension, limits the scope of our experimental analysis. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The appendix contains the complete proofs for all of our theoretical results, along with the full set of assumptions. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The main paper details the models, datasets, data splits, and optimizer used. Additionally, the appendix includes the complete hyperparameter grids tested for each experiment and any other relevant implementation details. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We provide a URL to our reproducible repository, which includes detailed operation instructions. Additionally, the appendix contains the complete hyperparameter grids tested for each experiment and any other relevant implementation details. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We follow the standard train and test splits and optimizer. The train/test splits and optimizer are detailed in Section 6 and are present in our repository. The appendix contains the complete hyperparameter grids tested for each experiment and any other relevant implementation details. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We include standard deviation error bars in our figures and tables. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We use a single NVidia L40 GPU (with a standard memory and time of execution) in all of our experiments. This is reported in the paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper proposes a general purpose computational pipeline for efficient learning on large non-sparse graphs, focusing mainly on theoretical/mathematical results, e.g., a new constructive version of the Weak Graph Regularity Lemma,. We believe there are no societal consequences of our work that require specific highlighting here. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not pose such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We cite the owners of each dataset and state the version and splits that are used in Section 6. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]