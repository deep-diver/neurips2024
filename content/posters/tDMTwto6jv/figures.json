[{"figure_path": "tDMTwto6jv/figures/figures_1_1.jpg", "caption": "Figure 1: (a): Active Learning with Instance Rejection (ALIR). The requester wants to acquire additional labeled samples to improve the ML classifier f. Due to the selective labels problem, only accepted labeling requests are labeled and added to the dataset D. (b): The most informative samples to the current ML classifier may not be the best samples to label in ALIR. The instances on the left of Dpool are more informative to the current classifier yet humans will never label these instances.", "description": "This figure illustrates the Active Learning with Instance Rejection (ALIR) framework.  Panel (a) shows the general ALIR process: a machine learning model selects instances from a pool of unlabeled data (Dpool), a human decides whether to label each instance, and only accepted labeled instances are added to the training dataset (D).  Panel (b) highlights a crucial aspect of ALIR: the most informative instances for improving the model (based on information gain) may not be the ones that humans actually label. Humans might reject some highly informative instances, making the active learning process more challenging.", "section": "1 Introduction"}, {"figure_path": "tDMTwto6jv/figures/figures_6_1.jpg", "caption": "Figure 2: Qualitative Results for Synthetic Data. Figure 11(a) shows the underlying synthetic data distribution. Figure 11(b) shows the initial labels randomly acquired. Figure 2 (c)-(h) show the labeled data the corresponding learned decision boundary after spending a budget of 450. Human decision makers always reject to label when x1 > 0. RANDOM and Naive-BALD spend most of the budget on examination since many candidate applications are in the high-risk region and have low probabilities to be labeled. In contrast, e-BALD selects the samples that are most likely to be labeled according to a noisy estimation of the human discretion model, which overlooks the lower left region which is important for learning the correct decision boundary.", "description": "This figure compares different active learning methods' performance on a synthetic dataset with selective labels.  It visualizes the learned decision boundaries after a fixed budget, highlighting how different methods handle the trade-off between exploring informative samples and accounting for human labeler rejection.", "section": "5.1 Synthetic Data"}, {"figure_path": "tDMTwto6jv/figures/figures_7_1.jpg", "caption": "Figure 5: Synthetic Data. Figure 5(a) shows the underlying synthetic data distribution. Figure 5 (b)-(d) and (e)-(g) shows the labeled data after 3 and 9 steps of active learning. Human decision makers always reject to label when x0 > 6. BALIR selects the samples that are likely to be labeled by the human decision makers and are both informative to the machine learning model and the human discretion model update. In contrast, BALD selects the samples that are informative to the machine learning model but may not be labeled by the human decision makers. RANDOM selects the samples randomly and since many candidate applications are in the high-risk region, it also has a low probability to select the samples that will be labeled by the human decision makers.", "description": "This figure shows a comparison of three active learning methods (Random, Naive-BALD, and Joint-BALD-UCB) on a synthetic dataset.  The key difference is how the methods account for the human's selective labeling behavior.  The figure illustrates the labeled data after 3 and 9 steps, highlighting how Joint-BALD-UCB is more effective at selecting samples that are both informative and likely to be labeled by the human, while the other methods struggle. The dataset simulates a high-risk scenario where the human avoids labeling certain areas.", "section": "5.1 Synthetic Data"}, {"figure_path": "tDMTwto6jv/figures/figures_8_1.jpg", "caption": "Figure 2: Qualitative Results for Synthetic Data. Figure 11(a) shows the underlying synthetic data distribution. Figure 11(b) shows the initial labels randomly acquired. Figure 2 (c)-(h) show the labeled data the corresponding learned decision boundary after spending a budget of 450. Human decision makers always reject to label when x1 > 0. RANDOM and Naive-BALD spend most of the budget on examination since many candidate applications are in the high-risk region and have low probabilities to be labeled. In contrast, e-BALD selects the samples that are most likely to be labeled according to a noisy estimation of the human discretion model, which overlooks the lower left region which is important for learning the correct decision boundary.", "description": "This figure shows a qualitative comparison of different active learning methods on synthetic data with selective labels.  It illustrates how the choice of algorithm affects the selection of samples for labeling, highlighting the trade-off between informative samples and samples likely to be labeled by a human.  The results demonstrate that naive methods may waste resources on samples that are unlikely to be labeled, while the proposed methods are more effective at selecting samples for labeling.", "section": "5.1 Synthetic Data"}, {"figure_path": "tDMTwto6jv/figures/figures_15_1.jpg", "caption": "Figure 5: Synthetic Data. Figure 5(a) shows the underlying synthetic data distribution. Figure 5 (b)-(d) and (e)-(g) shows the labeled data after 3 and 9 steps of active learning. Human decision makers always reject to label when x0 > 6. BALIR selects the samples that are likely to be labeled by the human decision makers and are both informative to the machine learning model and the human discretion model update. In contrast, BALD selects the samples that are informative to the machine learning model but may not be labeled by the human decision makers. RANDOM selects the samples randomly and since many candidate applications are in the high-risk region, it also has a low probability to select the samples that will be labeled by the human decision makers.", "description": "This figure shows the results of three different active learning methods (Random, Naive-BALD, and Joint-BALD-UCB) on a synthetic two-moon dataset.  The dataset simulates a scenario where human labelers selectively reject certain instances, particularly those with x0 > 6.  The figure visually demonstrates how the different methods perform in this scenario, highlighting the effectiveness of Joint-BALD-UCB in selecting samples that are informative and are likely to be labeled by the humans.  The top row shows results after 3 steps and the bottom row after 9 steps of active learning. ", "section": "5 Experiments"}, {"figure_path": "tDMTwto6jv/figures/figures_15_2.jpg", "caption": "Figure 2: Qualitative Results for Synthetic Data. Figure 11(a) shows the underlying synthetic data distribution. Figure 11(b) shows the initial labels randomly acquired. Figure 2 (c)-(h) show the labeled data the corresponding learned decision boundary after spending a budget of 450. Human decision makers always reject to label when x1 > 0. RANDOM and Naive-BALD spend most of the budget on examination since many candidate applications are in the high-risk region and have low probabilities to be labeled. In contrast, e-BALD selects the samples that are most likely to be labeled according to a noisy estimation of the human discretion model, which overlooks the lower left region which is important for learning the correct decision boundary.", "description": "This figure displays a qualitative comparison of different active learning methods on synthetic data with selective labels.  It shows the initial data distribution, initial random labels, and the resulting decision boundaries after applying different algorithms (RANDOM, Naive-BALD, e-BALD, Joint-BALD, Joint-BALD-UCB, Joint-BALD-TS). The visualization highlights how the algorithms handle the selective labeling problem, with some methods being more efficient in selecting informative labels than others, due to the human decision-maker's tendency to reject high-risk instances.", "section": "5.1 Synthetic Data"}, {"figure_path": "tDMTwto6jv/figures/figures_16_1.jpg", "caption": "Figure 7: Experimental Results (Accuracy) for Additional Datasets. We report the accuracy for each cost on different additional datasets. Joint-BALD variants often produce better results than Naive-BALD and RANDOM across different costs. Fashion-MNIST and CIFAR-10 use the same setting as MNIST in the main paper.", "description": "This figure presents the accuracy achieved by different active learning methods (RANDOM, Naive-BALD, e-BALD, Joint-BALD, Joint-BALD-UCB, and Joint-BALD-TS) across various total costs on four different datasets: Fashion MNIST, CIFAR-10, Adult, and Mushroom.  The results show that the Joint-BALD variants generally outperform Naive-BALD and RANDOM, indicating their effectiveness in handling selective labeling scenarios with heterogeneous human behavior.  The similar experimental setting to MNIST for Fashion MNIST and CIFAR-10 provides a comparative analysis.", "section": "5 Experiments"}, {"figure_path": "tDMTwto6jv/figures/figures_16_2.jpg", "caption": "Figure 4: Results for GMC and MNIST. We report the accuracy and number of samples labeled for both datasets. RANDOM and Naive-BALD spend most of the budget on examination since they do not consider human discretion behavior. On both datasets, Joint-BALD-UCB and Joint-BALD-TS demonstrate robust performance across different budgets.", "description": "This figure shows the results of experiments conducted on the Give-Me-Some-Credit (GMC) and MNIST datasets.  The performance of several active learning algorithms is compared, namely RANDOM, Naive-BALD, e-BALD, Joint-BALD, Joint-BALD-UCB, and Joint-BALD-TS.  The plots illustrate the accuracy and the number of labeled samples obtained for each method, across varying budget sizes.  The key takeaway is that Joint-BALD-UCB and Joint-BALD-TS exhibit consistent, strong performance across different budget levels, unlike the other methods which are more sensitive to the human labeling behavior, which is explicitly not taken into account by RANDOM and Naive-BALD.", "section": "5 Experiments"}, {"figure_path": "tDMTwto6jv/figures/figures_17_1.jpg", "caption": "Figure 4: Results for GMC and MNIST. We report the accuracy and number of samples labeled for both datasets. RANDOM and Naive-BALD spend most of the budget on examination since they do not consider human discretion behavior. On both datasets, Joint-BALD-UCB and Joint-BALD-TS demonstrate robust performance across different budgets.", "description": "The figure shows the results of experiments conducted on two datasets: Give Me Some Credit (GMC) and MNIST.  The plots compare the performance of several active learning methods, including RANDOM, Naive-BALD, e-BALD, Joint-BALD, Joint-BALD-UCB, and Joint-BALD-TS.   The x-axis represents the total cost, while the y-axis shows either accuracy or the number of samples labeled. The results demonstrate that Joint-BALD-UCB and Joint-BALD-TS are more robust and achieve better performance across various budget constraints compared to methods that do not explicitly consider human discretion behavior.", "section": "5 Experiments"}, {"figure_path": "tDMTwto6jv/figures/figures_17_2.jpg", "caption": "Figure 11: Extension on other uncertainty-based metric: Qualitative Results for Synthetic Data after spending a budget of 450. Our method can also help improve the performance of other uncertainty-based active learning metric like Entropy.", "description": "Figure 11 shows the results of applying the proposed Joint-Entropy-UCB method and other methods (Entropy, Naive-BALD, Random) to a synthetic dataset. The figure visualizes the decision boundaries learned by each method after a budget of 450 has been spent.  The visualization highlights how Joint-Entropy-UCB, by incorporating both the human discretion model and the entropy uncertainty measure, effectively selects informative samples and achieves a more accurate decision boundary compared to other baselines. This demonstrates the effectiveness of the proposed method for enhancing the performance of uncertainty-based active learning techniques.", "section": "G Experiments with Other Uncertainty Metric"}]