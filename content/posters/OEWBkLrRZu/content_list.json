[{"type": "text", "text": "Towards Stable Representations for Protein Interface Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziqi Gao1,2, Zijing Liu3\\*, $\\mathrm{{\\bfY}}{\\bf u}\\,{\\bf L}{\\bf i}^{3}$ , Jia Li1,2\\* ", "page_idx": 0}, {"type": "text", "text": "1Hong Kong University of Science and Technology 2Hong Kong University of Science and Technology (Guangzhou) 3 International Digital Economy Academy (IDEA) ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The knowledge of protein interactions is crucial but challenging for drug discovery applications. This work focuses on protein interface prediction, which aims to determine whether a pair of residues from different proteins interact. Existing data-driven methods have made significant progress in effectively learning protein structures. Nevertheless, they overlook the conformational changes (i.e., flexibility) within proteins upon binding, leading to poor generalization ability. In this paper, we regard the protein flexibility as an attack on the trained model and aim to defend against it for improved generalization. To fulfill this purpose, we propose ATProt, an adversarial training framework for protein representations to robustly defend against the attack of protein flexibility. ATProt can theoretically guarantee protein representation stability under complicated protein flexibility. Experiments on various benchmarks demonstrate that ATProt consistently improves the performance for protein interface prediction. Moreover, our method demonstrates broad applicability, performing the best even when provided with testing structures from structure prediction models like ESMFold and AlphaFold2. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "OEWBkLrRZu/tmp/6ba8946f1f32a180c00e9b54e4dab2a25556cd8b7b18346ae86e023b6817601b.jpg", "img_caption": ["Figure 1: (A). The task illustration. PIP involves predicting if there is an interaction between two residues from different proteins. (B). The task challenge. During training, the input consists of bound structures of two proteins. However, for testing, one can only access their unbound structures. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Protein-protein interactions are important for understanding biological processes, and for the design of novel therapies [48, 16] and drugs [47]. The protein interface refers to the surface region of a protein where the interaction occurs. It therefore holds the key to revealing the specific interaction mechanism and understanding protein functions. In this work, we tackle the problem of protein interface prediction (PIP) (shown in Figure 1(A)): predicting whether two residues, each from an individual protein, interact with each other, given the separate structures of two proteins. ", "page_idx": 0}, {"type": "image", "img_path": "OEWBkLrRZu/tmp/ba3dfc5a21eeaa0364d193bdddb437f8fb380006ce6f8124efbf90ced9da3655.jpg", "img_caption": ["Figure 2: The impact of flexibility on results with the DB5.5 dataset [45]. (A) The testing results of two baselines (SASNet [43], NEA [15]) and our method. \u2018B-U\u2019 represents the popular formulation, i.e., training with bound structures and testing with unbound ones. \u2018B-B\u2019 refers to the formulation where both training and testing are conducted with bound structures. (B) Loss trends for three method. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For years, data-driven methods based on deep learning (DL) have made significant progress in response to this critical task by effectively learning protein structures using geometric graph neural networks (GNN) [15, 33], 3D CNN [43], etc. Limited by the difficulty in accessing protein structure data, they typically follow a formulation of training on bound (after binding) structures and testing on unbound (before binding) ones, as depicted in Figure 1(B). For training, large-scale datasets like the Database of Interacting Protein Structures (DIPS) [43] typically consist of only bound structures, which are directly extracted from the PDB database[4]. In contrast, in the practical inference scenario, the model cannot access the bound structures but can only be provided with unbound ones [43, 15, 33]. In this paper, we empirically find that this training (bound)-testing (unbound) formulation leaves significant room for performance improvement. In Figure 2, exploratory experiments show that prevailing PIP methods are sensitive to flexibility. Utilizing the bound version structures for testing can greatly boost their performance. Based on these findings, we aim to answer the question in this paper\u2014how to handle the mismatch between bound and unbound structures for PIP? ", "page_idx": 1}, {"type": "text", "text": "Since it is usually impractical to access the protein bound structures for testing, the most intuitive solution is to explicitly learn the mapping relationship from unbound to bound structures of proteins. However, this is challenging due to the following two factors: (1) The amount of pairwise unbound and bound structure data for proteins is extremely limited [10] (to our knowledge, only DB5.5 [45]). (2) A protein\u2019s bound structure is not unique and depends on its binding partner, so diverse training data is necessary. To address this issue, we take a different route. We consider any potentially complicated flexibility in a protein as an attack [29, 24], which can harm the testing performance of a model trained on bound structures. Therefore, our core idea is to enable protein representations with adversarial robustness, which can defend against the attacks of protein flexibility. In simple terms, for a protein with both unbound and bound versions, the model outputs similar (stable) representations. ", "page_idx": 1}, {"type": "text", "text": "In this work, we take an important step forward in mitigating the impact of flexibility on PIP. We propose ATProt, an end-to-end adversarial training (AT) framework for protein representations, to effectively defend against protein flexibility in PIP. Inspired by the recent protein graph representation methods [17, 53, 15], our model comprises graph-based feature extractors (encoders) for protein graphs. Our ATProt framework does not require computationally expensive data augmentation and can be smoothly applied to most existing protein graph encoders. Specifically, we implement differentiable AT regularizations for various protein representation encoders. Importantly, we introduce a novel and expressive graph encoder for protein representations and propose its theoretical regularization form for the first time. ATProt can produce stable representations for the same protein with different structure versions (e.g., bound, unbound, and model-generated ones). Extensive experiments on several protein interaction benchmarks verify that our ATProt method consistently outperforms advanced PIP methods. The results demonstrate the effectiveness of the AT regularization. Furthermore, ATProt maintains excellent performance even when tested with structures generated by AlphaFold2 [25] and ESMFold [32], allowing for user-friendly inference without the need for native structures. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Protein interface prediction. Protein interface prediction (PIP), a well-studied problem, focuses on determining whether there is an interaction between amino acids from two different proteins. Recently, a series of methods based on protein [8, 19, 17, 20, 14] or amino acid representation [39, 46] learning have achieved significant success. NEA [15] pioneers the use of protein graphs to address PIP, where protein structure information is represented and aggregated, followed by the dense layers. SASNet [43] considers embedding the hierarchical structures of proteins, integrating atomic and amino acid information into a 4D-grid data, and employs 3D CNN for learning. To further enhance performance, more fine-grained structure information modeling, specifically surface geometry [46, 39], is introduced to effectively learn amino acid representations. Existing methods have effectively represented proteins from various perspectives of protein information. However, we have observed that protein flexibility, which is overlooked by most methods, poses significant performance bottlenecks for them. We focus on this key issue of mitigating the bound-unbound mismatch in protein structures to improve model generalization. ", "page_idx": 2}, {"type": "text", "text": "Modelling protein flexibility. Recently, pioneer works in biology confirm that protein-protein interaction (PPI) conforms to the \u201cinduced fti\u201d theory [27, 38]. Specifically, proteins undergo structure changes due to residue-level forces, and they adjust structures to achieve the best binding state. More importantly, proteins with PPI typically undergo larger structure changes at the interface compared to non-interface regions [10, 11, 52, 12], which will exacerbate the generalization challenge of the PIP task. Modelling flexibility directly is challenging, whether using traditional computational or deep learning (DL) approaches. Traditional methods often rely on finding the lowest energy state [42, 51] or introducing an induced fti model (specifically, the elastic network model) [12, 3] to guide structure deformations. The optimization space in these methods is vast, making them very time-consuming. DL-based methods [10] struggle to achieve satisfactory accuracy in learning the distribution mapping of bound-unbound states due to limited training data (i.e., 253 complexes in the DB5.5). As directly predicting bound structures is challenging, in the context of the PIP task, we choose to eliminate the influence of different versions of the same protein structure on the task. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, (1) we present the definition of the protein interface prediction (PIP). Then, (2) we verify the importance of representation stability through empirical and mathematical views. Finally, (3) we investigate to ensure protein representation stability within the adversarial training framework. ", "page_idx": 2}, {"type": "text", "text": "Problem definition. We are given as input two proteins $\\mathcal{P}^{1}$ and $\\mathcal{P}^{2}$ , consisting of $M$ and $N$ residues, respectively. The proteins are represented as their residue sequences and 3D structures, which are composed of $\\alpha$ -carbon atom locations of all residues. The goal of the PIP is to classify all possible pairs of residues from separate proteins. More formally, the set of data is $\\{(\\mathcal{P}_{i}^{1},\\mathcal{P}_{j}^{2}),\\dot{y}_{i j}\\}_{1\\le i\\le M,1\\le j\\le N}$ , where $\\mathcal{P}_{i}^{1}$ represents the $i$ -th residue in protein $\\mathcal{P}^{1}$ and $y_{i j}\\in\\{0,1\\}$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 The importance of protein representation stability ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We verify the importance of stable protein representations from both empirical and theoretical perspectives. For clarity, we use the notations $X_{1}^{b},X_{2}^{\\dot{b}}$ to represent the native bound structures of proteins $\\mathcal{P}^{1}$ and $\\mathcal{P}^{2}$ , while $X_{1}^{t},X_{2}^{t}$ represent their structures used for testing. After using a protein graph encoder, we have their representations, denoted as $\\bar{H_{1}^{b}},\\bar{H_{2}^{b}},H_{1}^{t},\\bar{H_{2}^{t}}$ . We denote the protein representation perturbation as $\\|\\delta_{1}\\|_{p}+\\|\\delta_{1}\\|_{p}$ , where $\\delta_{1}=\\pmb{H}_{1}^{t}-\\pmb{H}_{1}^{b},\\delta_{2}=\\pmb{H}_{2}^{t}-\\pmb{H}_{2}^{b}$ . ", "page_idx": 2}, {"type": "text", "text": "From an empirical perspective, in Figure 3, we quantify the test results of the NEA method [15] under flexibility. We gradually increase the structure change of test samples and calculate the representation perturbation. We note that the test performance is negatively correlated with the representation perturbation caused by flexibility. Moreover, testing with bound structures yields the best results. ", "page_idx": 2}, {"type": "image", "img_path": "OEWBkLrRZu/tmp/37eb2e1fab9b257ecd4073ecd9456330fd9fb01ab2d7f431b9df1a9787eb76d9.jpg", "img_caption": ["Figure 3: AUC vs. representation perturbation. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "From a theoretical perspective, we draw the consistent conclusion that stable representations lead to improved PIP results. Following [15], we model PIP as a pairwise classification problem. Specifically, we concatenate the $i$ -th and $j$ -th rows of $\\pmb{H}_{1}^{t}$ and $\\pmb{H}_{2}^{t}$ into a vector embedding, which is then sent to the PIP classifier $f$ . In this case, the following proposition describes the impact of representation perturbations on PIP results. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1. For two proteins with $M$ and $N$ residues respectively, the classification results obtained using bound and unbound structures are the same. This is true if $N\\,\\|\\delta_{1}\\|_{p}^{\\dot{p}}+M\\,\\|\\delta_{2}\\|_{p}^{p}<$ ${\\mathcal{A}}(f,p)$ , where ${\\mathcal{A}}(f,p)$ can be a constant depending only on the PIP classifier $f$ and the norm $\\left\\|\\cdot\\right\\|_{p}$ . ", "page_idx": 3}, {"type": "text", "text": "We detail ${\\mathcal{A}}(f,p)$ and prove Proposition 3.1 in Appendix B.1. This proposition tells that stable protein representations (i.e., smaller $\\left\\|\\delta_{1}\\right\\|_{p}$ and $\\|\\delta_{2}\\|_{p})$ under flexibility are necessary for achieving high performance. Thus, to effectively address the structure mismatch in PIP, an intuitive idea is to perform adversarial training for protein representation learning. ", "page_idx": 3}, {"type": "text", "text": "3.2 Adversarial training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we introduce the concept of adversarial training (AT) [1, 6, 18, 5, 7] and establish a connection between it and the stability of protein representations. We consider a classification task with a given dataset $D=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ , consisting of $K$ classes. We assume that the entire prediction pipeline includes a representation model (e.g., encoder) and a classifier. The concept of adversarial training (AT) requires the entire pipeline to perform well not only on $\\mathcal{D}$ but also on the worst-case distribution near $\\mathcal{D}$ , as determined by a specific distance metric. More concretely, the AT that we primarily focus on in this paper is the $\\ell_{p}$ -robustness. For a given $p$ value and a finite $\\epsilon>0$ , AT aims to train a pipeline that can correctly classifies $(x+\\delta,y)$ for any $\\|\\delta\\|_{p}\\leq\\epsilon$ , where $(x,y)$ belongs to $\\mathcal{D}$ . ", "page_idx": 3}, {"type": "text", "text": "Among all AT methods, Lipschitz neural networks belong to a common and effective category. Specifically, an encoder function is considered to have Lipschitz continuity if a slight perturbation to the input of the encoder does not significantly change its output. ", "page_idx": 3}, {"type": "text", "text": "Formally, the definition of Lipschitz continuity is given by: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. (Lipschitz continuity in adversarial training) An encoder function ENC is said to be $C$ -Lipschitz continuous w.r.t. norm $\\lVert\\cdot\\rVert$ if for any two versions of inputs $\\mathbf{\\boldsymbol{x}}_{1},\\mathbf{\\boldsymbol{x}}_{2}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\operatorname{ENC}(\\pmb{x}_{1})-\\operatorname{ENC}(\\pmb{x}_{2})\\right\\|\\leq C\\left\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\right\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Lipschitz continuity explains the requirements of AT for a general representation learning encoder. In the context of protein graph representation, this can be modified to become the definition below. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2. (Lipschitz continuity for protein representations) $A$ protein representation encoder $\\Psi(\\cdot)$ has $C$ -Lipschitz continuity w.r.t. norm $\\lVert\\cdot\\rVert$ if for any two versions of structure inputs $X^{t},X^{b}$ and the invariant residue feature input $\\pmb{F}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\Psi({\\boldsymbol{F}},{\\boldsymbol{X}}^{t})-\\Psi({\\boldsymbol{F}},{\\boldsymbol{X}}^{b})\\right\\|\\leq C\\left\\|{\\boldsymbol{X}}^{t}-{\\boldsymbol{X}}^{b}\\right\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.3 How to ensure Lipschitz continuity for protein graph representations? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As an expressive representation, graph structured data is widely used for representing input proteins [17, 22, 19], with residues acting as nodes and physical interactions as edges. Let $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ be an undirected graph with nodes $\\mathcal{V}=\\left\\{{1,...,N}\\right\\}$ , edges $\\mathcal{E}\\subset\\mathcal{V}^{2}$ , graph signal $F\\in\\mathbb{R}^{N\\times d}$ and a graph shift operator $\\bar{\\b{L}}\\in\\bar{\\mathbb{R}}^{N\\times N}$ (i.e., node connectivity). We consider any variant of the spectral GNN (e.g., GCN [26, 30], ChebNets [13], BWGNN [41, 40]) that follows the concept of learning filter coefficients for graph convolution. By constructing the filter $\\begin{array}{r}{h({\\cal L}):=\\sum_{k=0}^{K}\\bar{\\theta}_{k}{\\cal L}^{k}}\\end{array}$ ( $\\boldsymbol{\\theta}_{k}$ are learnable parameters), the protein graph representation can be defined as $\\begin{array}{r}{\\pmb{H}=\\sum_{k=0}^{K}\\theta_{k}\\pmb{L}^{k}\\pmb{F}:=h(\\pmb{L})\\pmb{F}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Here, we extend the Definition 3.2 to the scenario of using GNN models. To achieve this, we assume $\\textbf{\\emph{L}}$ is perturbed to become $\\tilde{L}$ due to the protein flexibility, and introduce the key factor (GNN filter stability constant $C_{h}$ ), for achieving GNN-based stable protein representations. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. (GNN filter stability constant) Given a graph spectral filter $h:[0,2]\\mapsto[0,1],$ it is defined as Lipschitz with constant $C_{h}>0$ if for any pair of points $\\lambda_{1},\\lambda_{2}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|h(\\lambda_{1})-h(\\lambda_{2})\\right|\\leq C_{h}\\left|\\lambda_{1}-\\lambda_{2}\\right|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "OEWBkLrRZu/tmp/16d6a9d36cb77123aa8a23e96d2272e84685aa80cc97ebc1426ba7e637355f96.jpg", "img_caption": ["Figure 4: The framework overview with the BernNet encoder. The whole framework contains the stability-regularized graph encoder for stable protein representations, the cross attention layers for communication and the final binary classifier. ATProt takes in two protein graphs as inputs, and extracts features with the pre-defined graph encoder (BernNet is taken as an example here). The PIP results are obtained after the learned representations have passed through the cross attention module and classifier. The $\\mathcal{L}_{S}$ loss for stability regularization and classification loss $\\mathcal{L}_{B C E}$ jointly optimize the model. ", "where ${\\mathcal{A}}(\\Psi)$ is a constant determined by the model (e.g., layer number and feature dimension). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "which introduces our main theorem below. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. (Protein graph stability with GNNs) Let the perturbation to $\\textbf{\\emph{L}}$ is finite such that $\\left\\|{\\tilde{L}}-L\\right\\|_{p}\\leq\\epsilon$ . The protein graph encoder $\\Psi(\\cdot)$ is always stable with a polynomial filter h if for some finite $C_{h}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\Psi(\\boldsymbol{F},L)-\\Psi(\\boldsymbol{F},\\tilde{L})\\right\\|_{p}\\leq\\epsilon C_{h}\\cdot A(\\Psi)\\cdot\\left\\|\\boldsymbol{F}\\right\\|_{p},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Based on Eq. 3 and Eq. 4, the lower bound of the left term in Eq. 4 is solely determined by the maximum of $C_{h}$ (denoted as $C_{h}^{*}$ ). To be more intuitive, $C_{h}^{*}$ is equal to the maximum absolute slope (MAS) of the filter $h$ (the straightforward proof is in the Appendix B.2). ", "page_idx": 4}, {"type": "text", "text": "Therefore, we conclude the core idea for designing our ATProt framework as follows: ", "page_idx": 4}, {"type": "text", "text": "We can enhance the stability of protein representations by decreasing the MAS value of h. ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Overview. We propose ATProt, an end-to-end framework (illustrated in Fig. 4) to boost PIP from a view of representation stability. Specifically, our model inputs two proteins whose structures can be provided in various sources (e.g., native bound, native unbound, AlphaFold2, ESMFold). ATProt incorporates a protein graph representation encoder and a targeted differentiable regularization scheme to theoretically guarantee the representation stability. Our ATProt framework can be implemented with at least four protein graph encoders, and we provide specific examples of these cases. Lastly, for the PIP task, we apply a cross-attention module to facilitate communication between the protein representations and use a simple linear classifier for final prediction. ", "page_idx": 4}, {"type": "text", "text": "Protein representation. We represent a protein as an undirected weighted graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ . Each node $v_{i}\\in\\mathcal{V}$ representing one residue has a $d$ -dimensional feature vector $\\pmb{F}_{i}\\in\\mathbb{R}^{d}$ (i.e., residue type) and a 3D coordinate $\\bar{X_{i}}\\in\\mathbb{R}^{3}$ (i.e., the $\\alpha$ -carbon atom location). Edges $\\mathcal{E}=\\{(i,j)\\}$ are constructed with a $\\mathbf{k}$ -nearest-neighbor (k-NN) graph using Euclidean distance. To distinguish the edges, we follow [17] to construct the SE(3)-invariant edge features $\\{e_{i,j}:\\forall(i,j)\\in\\mathcal{E}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "The goal of ATProt. According to Definition 3.2, we aim to achieve representation stability, which means that the perturbation in the representation caused by protein flexibility is constrained. In addition, it is also important to ensure the commonly encapsulated SE(3)-invariance, which means that the representation of the protein is not affected by its rotation or translation. Formally, we represent a single protein $\\mathcal{P}$ consisting of $N$ residues, with its residue-level feature matrix $\\pmb{F}\\in\\dot{\\mathbb{R}}^{N\\times d}$ and residue-level structure $\\pmb{X}\\in\\mathbb{R}^{N\\times3}$ . We wish our model $\\Psi(\\cdot)$ to ensure the following property. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Given}\\quad Z,H=\\Psi(F,X);Z^{\\prime},H^{\\prime}=\\Psi(F,Q X+g+\\Delta X),}\\\\ &{\\mathrm{we~have}\\quad\\left\\|H-H^{\\prime}\\right\\|\\leq C\\cdot\\|\\Delta X\\|\\,,(C\\mathrm{-Lipschitz~continuity})}\\\\ &{\\forall Q\\in S O(3),\\forall g\\in\\mathbb{R}^{3},\\forall\\Delta X\\in\\mathbb{R}^{3\\times N},\\exists C\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.1 Adversarial training regularizations for various encoders ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Fourier transform is a powerful tool in the representation of both structured [50, 26, 49, 28] and unstructured data [9, 21, 31]. Our main focus is on employing spectral graph encoders for representing protein graphs. These encoders adhere to the principle of utilizing a graph spectral filter $\\begin{array}{r}{h(\\lambda)=\\sum_{k=0}^{K}\\theta_{k}b_{k}(\\lambda)}\\end{array}$ to learn effective protein representations. Here, $\\{b_{k}\\}_{k=0}^{K}$ is the predefined filter  basis and $\\{\\theta_{k}\\}_{k=0}^{K}$ is the learnable parameters. For graph convolution, the filter $h(\\cdot)$ will be applied to the whole Laplacian matrix $\\textbf{\\emph{L}}$ , which is calculated from the protein structure data. Specifically, to construct $\\textbf{\\emph{L}}$ , we first apply a Multi-Layer Perceptron (MLP) to reduce the dimensionality of $\\pmb{E}$ to 1, and incorporate the results into the edge weight matrix $W$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{i,j}=\\left\\{\\overset{\\mathrm{MLP}_{e}(e_{i,j}),\\quad(i,j)\\in\\mathcal{E}}{0,}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then the Laplacian matrix $\\textbf{\\emph{L}}$ is obtained by ${\\cal L}\\,=\\,{\\cal I}-{\\cal D}^{-1/2}W{\\cal D}^{-1/2}$ , where $_{D}$ is the degree matrix, i.e., $\\begin{array}{r}{\\dot{\\boldsymbol{D}}=\\mathrm{diag}(\\sum_{j}W_{1,j},...,\\sum_{j}W_{N,j})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The protein graph representation can be defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nH=\\sum_{k=0}^{K}\\theta_{k}b_{k}(L)F=h(L)F,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the result of $h(L)$ represents the graph spectral response. ", "page_idx": 5}, {"type": "text", "text": "In this paper, we introduce four types of top graph encoders (i.e., Simple GCN [49], Chebynet [13], Low-pass filter [34], and BernNet [23]) along with their corresponding stability regularizations. ", "page_idx": 5}, {"type": "text", "text": "Case 4.1. (Simple GCN encoder [49]) The Simple GCN encoder $(S G C)$ utilizes a spectral filter in the monomial function form: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(\\lambda)=\\lambda^{K}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the spectrum $\\lambda$ lies in $[-1,1]$ , it is clear that the maximum absolute slope (MAS) that $h(\\lambda)$ can reach is $K$ . Therefore, the stability regularization of the SGC encoder does not involve any loss function. We can directly constrain the size of the order $K$ . ", "page_idx": 5}, {"type": "text", "text": "Case 4.2. (Chebynet encoder [13]) The Chebynet encoder utilizes a spectral fliter in the polynomial function form: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(\\lambda)=\\sum_{k=0}^{K}\\theta_{k}\\lambda^{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The stability regularization of Chebynet can be implemented by the loss function $\\begin{array}{r}{\\mathcal{L}_{S}=\\sum_{k=1}^{K}k|\\lambda^{k}|}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Case 4.3. (Low-pass fliter encoder [34]) The Low-pass fliter $(L P F)$ encoder utilizes a spectral fliter in the low-pass filter function form: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(\\lambda)=(1+\\theta\\lambda)^{-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The stability regularization of LPF can be implemented by the loss function $\\mathcal{L}_{S}=\\theta$ . ", "page_idx": 5}, {"type": "text", "text": "Case 4.4. (BernNet encoder [23]) The BernNet encoder utilizes Bernstein basis, the state-of-the-art graph spectral basis, to construct the graph spectral filter: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(\\lambda)=\\sum_{k=0}^{K}\\theta_{k}b_{k}^{K}(\\lambda^{(l+1)})=\\sum_{k=0}^{K}\\theta_{k}\\frac{1}{2^{K}}\\left(\\!\\!\\begin{array}{c}{{K}}\\\\ {{k}}\\end{array}\\!\\!\\right)(2I-\\lambda^{(l+1)})^{K-k}(\\lambda^{(l+1)})^{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "According to Theorem 3.1, the BernNet encoder easily satisfies the representation C-Lipstchiz continuity in Eq. 4 provided that the filter $h(\\lambda)$ always has a finite MAS. However, in Eq. 11, the analytical relationship between MAS and $\\{\\dot{\\theta}_{k}\\}_{k=0}^{K}$ is intractable, suggesting the difficulty of constraining with a gradient descent manner. Thus, our next goal is to discover a differentiable method for constraining the minimum of $C$ in the training process. ", "page_idx": 6}, {"type": "text", "text": "The stability of the BernNet is still unclear in these four cases, prompting us to investigate its regularization form. To the best of our knowledge, it is the first investigation of the Lipstchiz continuity for Bernstein-based spectral filters. ", "page_idx": 6}, {"type": "text", "text": "4.2 Guaranteeing stability of the BernNet encoder ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For clarity, we rewrite the Eq. 4 as $\\left\\|H-H^{\\prime}\\right\\|\\leq C\\cdot\\left\\|\\Delta\\pmb{X}\\right\\|=C_{h}\\cdot\\mathcal{A}(\\Psi)\\cdot\\left\\|\\Delta\\pmb{X}\\right\\|\\cdot\\left\\|\\pmb{F}\\right\\|$ , where, referring to Eq. 3, ${\\mathcal{A}}(\\Psi)$ is determined by the model architecture hyperparameters and remains constant. We say the overall model $\\Psi$ is of $C$ -Lipstchiz continuity and the learned spectral filter $\\begin{array}{r}{h(\\lambda)=\\sum_{k=0}^{K}\\dot{\\theta_{k}}b_{k}^{K}(\\lambda)}\\end{array}$ is of $C_{h}$ -Lipstchiz continuity. We aim to constrain the minimum of constant $C_{h}$ (denoted as $C_{h.}^{*}$ ) with $\\{\\theta_{k}\\}_{k=0}^{K}$ by discovering the underlying relationship between them. Finally, we propose an auxiliary differentiable regularization of $\\{\\theta_{k}\\}_{k=0}^{K}$ to constrain $C_{h}^{*}$ to a controllable bound, for any $\\Delta X$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. Given an arbitrary polynomial function $f(\\lambda)$ on $\\lambda\\in[0,2]$ and suppose its $K$ -order Bernstein polynomial is denoted as $\\begin{array}{r}{h(\\lambda)=\\sum_{k=0}^{K}f(\\frac{2k}{K})\\binom{K}{k}\\left(2-\\lambda\\right)^{K-k}\\lambda^{k}}\\end{array}$ . For any point pair $\\lambda_{1},\\lambda_{2}\\in[0,2]$ , if there exists a constant $C_{f}$ fo $r\\,|f(\\lambda_{1})-f(\\lambda_{2})|\\leq C_{f}|\\lambda_{1}-\\lambda_{2}|$ , then $h(\\lambda)$ always holds $C_{f}$ -Lipstchiz continuity for all $K\\geq1$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|h(\\lambda_{1})-h(\\lambda_{2})\\right|\\leq C_{f}\\left|\\lambda_{1}-\\lambda_{2}\\right|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We introduce Theorem 4.1, which describes the stability relationship between the filter $h(\\lambda)$ and an auxiliary function $f(\\lambda)$ . It tells that with Berstein basis, the MSA of $h(\\lambda)$ will never exceed that of the auxiliary function $f(\\lambda)$ . Due to $\\textstyle f({\\frac{2k}{K}})\\,=\\,\\theta_{k}$ , for all $k\\in[0,K]\\cap\\mathbb{Z},\\,f(\\lambda$ can be any 2-D curve passing through all points of $\\big\\{\\big(\\frac{2k}{K},\\theta_{k}\\big)\\big\\}_{k=0}^{K}$ . Therefore, the MAS of $f(\\lambda)$ is analytically tractable, and we can subsequently provide the bounds for the MAS of $h(\\lambda)$ . We have the following proposition, which is accompanied by a detailed derivation in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "PDreonpotoisnigti tohne  4M.1A. S S(umpipnoisme $h(\\lambda)$ piss cahiptpz rcooxinsmtaatnetd)  owfi aBse ,t eiti nc abna sbies , ui.pep.,e $\\begin{array}{r}{h(\\lambda)=\\sum_{k=0}^{K}\\theta_{k}b_{k}^{K}(\\lambda)}\\end{array}$ $h$ $C_{h}^{*}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nC_{h}^{*}\\leq\\operatorname*{max}_{i\\in[0,K-1]\\cap\\mathbb{Z}}K\\cdot|\\theta_{i}-\\theta_{i+1}|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To summarize, Bernstein basis applied to our ATProt guarantees a finite MAS, and more importantly, we can further enhance the stability of the BernNet encoder with the following objective: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S}=\\operatorname*{max}_{i\\in[0,K-1]\\cap\\mathbb{Z}}|\\theta_{i}-\\theta_{i+1}|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.3 Protein interface prediction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given proteins $\\mathcal{P}_{1},\\mathcal{P}_{2}$ with their initial feature $F_{1},F_{2}$ and coordinates $X_{1},X_{2}$ , ATProt produces their stable representations under respective structure perturbations. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb{H}}_{1}\\in\\mathbb{R}^{M\\times d}=A T P r o t({\\pmb X}_{1},{\\pmb F}_{1});{\\pmb H}_{2}\\in\\mathbb{R}^{M\\times d}=A T P r o t({\\pmb X}_{2},{\\pmb F}_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We apply a cross-attention layer (shown in Appendix D) to enable communication between proteins and obtain their final representations $H_{1}^{\\prime},H_{2}^{\\prime}$ . Next, we aim to predict whether pairs of inter-protein residues belong to the interface, which involves performing pairwise binary classification. Concretely, for each training pair of proteins, we have a set of $10N_{I}$ labeled pairs $\\bar{\\left\\{(([H_{1}^{\\prime}]_{i},[H_{2}^{\\prime}]_{i}),y_{i})\\right\\}_{i=1}^{10N\\bar{I}}}$ where $y_{i}\\in\\{0,1\\}$ , $N^{I}$ is the number of positive residue pairs and $9N^{I}$ negative ones are downsampled. We take the element-wise product of two residue representations and feed it to another MLP with the Sigmoid function to compute the probability $p_{i}$ . Weighted cross-entropy loss is used for training. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{I}=\\frac{1}{|\\mathcal{Y}_{t r a i n}|}\\sum_{y^{k}\\in\\mathcal{Y}_{t r a i n}}\\left(\\sum_{i=0}^{2N_{I}^{k}}-y_{i}^{k}\\mathrm{log}p_{i}^{k}-(1-y_{i}^{k})\\mathrm{log}(1-p_{i}^{k})\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and processing. We evaluate our method on the complexes from Docking Benchmark 5.5 (DB5.5) [45], a gold standard dataset with high-quality, and Database of Interacting Protein Structures (DIPS) [43], which collects 41,876 complexes mined from PDB [4]. DB5.5 only contains 253 complex structures manually curated by domain experts, which cover both native unbound and bound structures. In comparison, DIPS has a significantly larger data size, but it only includes bound structures of proteins. The two datasets are randomly divided into training, validation, and testing sets with the following sizes: 203/25/25 (DB5.5) and 39,937/974/965 (DIPS). ", "page_idx": 7}, {"type": "text", "text": "For both datasets, we test using various versions of protein structures as inputs for PIP. To accomplish this, we prepared three versions of the testing set for DB5.5, including native unbound structures, structures produced by AlphaFold2, and structures produced by ESMFold. We use Native-Bound, Native-Unbound, ESMFold, AlphaFold2 to denote these four settings respectively. ", "page_idx": 7}, {"type": "text", "text": "As for DIPS, since it does not have native unbound structures, we only used ESMFold to prepare unbound structure inputs for its testing set. AlphaFold2 is not considered due to its high computational cost for the entire testing set of DIPS. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We compare our ATProt method with state-of-the-art conventional machine learning method BIPSPI [36], the CNN-based methods Siamese Atomic Surfacelet Network (SASNet) [43], Diffusion-Convolutional Neural Networks (DCNN) [2], differentiable molecular surface interaction fingerprinting (dMaSIF) [39], and a set of GNN-based methods Deep Tensor Neural Networks (DTNN) [37], and NEA [15]. ", "page_idx": 7}, {"type": "text", "text": "Setup and metrics. We consider three experimental setups: (1) performing training and testing both on the DB5.5; (2) performing training and testing both on the DIPS; and (3) performing pre-training on the DIPS, fine-tuning on the DB5.5 and testing on the DB5.5. For our proposed ATProt method, we consider graph encoders of Simple GCN, Chebynet, Low-pass filter and BernNet (denoted as ATProt-SGC, ATProt-Cheby, ATProt-LPF, and ATProt-Bern, respectively). ", "page_idx": 7}, {"type": "text", "text": "For each complex in the testing set, assuming that the two proteins have $M$ and $N$ residues, respectively, we test all its $M\\times N$ binary classification samples and calculate the Area Under the ROC Curve (AUC) value. Following [15], we report the median AUC score (MedAUC) across all complexes as the final evaluation metric. ", "page_idx": 7}, {"type": "table", "img_path": "OEWBkLrRZu/tmp/3d8976ce43df75d8c9877a686d1848b16dc1da0457a8ee843fcc88d56c832d82.jpg", "table_caption": ["Table 1: Training and testing on the DB5.5. Mean and standard deviation values of the MedAUC scores of all baselines, computed from three random seeds. The best performance is in bold and the second best one is underlined. \u2018SR\u2019 means the proposed stable regularization $\\mathcal{L}_{S}$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. Table 1, 2, and Table 3 (shown in Appendix) show the model performance for PIP. We find that our method is competitive and outperforms the majority of baseline methods with native bound structures. Under this Native-Bound setting, although ATProt is slightly less effective than BIPSPI, it demonstrates the ability to learn sufficiently powerful protein representations (especially with BernNet and Chebynet). Notably, all the baselines fail significantly when using non-bound structures as inputs, whereas our method maintains similar performance compared to that under Native-Bound. As can be seen from the results of \u2018ATProt-BernNet w/o SR\u2019, the robustness to structure perturbation is attributed to the proposed SR (stable regularization) strategy. Overall, the experiments on DB5.5 demonstrate that our method does not rely on native bound or unbound structure data for inference, but can directly utilize structure prediction software to achieve satisfactory PIP results. From Table 2, 3, we note that although DIPS has a larger data scale than DB5.5, it is difficult to achieve better results. However, pre-training on DIPS can improve most of the testing results on DB5.5. ", "page_idx": 7}, {"type": "table", "img_path": "OEWBkLrRZu/tmp/54152fbd0195391a415f4945aaeab9d4cb1d1b9cc569123245a7ec31d44c3b55.jpg", "table_caption": ["Table 2: Pre-training on the DIPS, fine-tuning on the DB5.5 and finally testing on the DB5.5. $\\pmb{\\triangle}/\\pmb{\\Psi}$ indicates that the model performs better/worse than without pre-training (i.e., results in Table 1). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In summary, all three tables show consistent performance degradation for all baselines with the non-bound structures. In contrast, our method can defend against this bound-unbound mismatch and achieve performance similar to, or even better than that with Native-Bound. ", "page_idx": 8}, {"type": "text", "text": "Benefits of representation stability. Figure 5 shows the dimensionality reduction visualization of residue representations with t-SNE [44]. We observe that ATProt leads to a \"clustering\" effect in the representations, which is the result of stability regularization. Importantly, this ultimately generates clearer classification boundary in visualization and quantification (i.e., the Silhouette Score [35]). ", "page_idx": 8}, {"type": "image", "img_path": "OEWBkLrRZu/tmp/8e04a4e74acab1879b0a8d8578f6beeb8c12ba632553d23f81641380326e6158.jpg", "img_caption": ["Figure 5: The t-SNE visualization for the last layer representations. The $x$ and $y$ axes of all three subplots are uniformly scaled to (0, 1). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we present ATProt, an end-to-end learning framework for protein interface prediction (PIP). By highlighting the importance of protein representation stability for the PIP task, we tailor the stability regularization for four types of spectral graph encoders, theoretically ensuring that the ATProt framework exhibits Lipschitz continuity properties. Our method demonstrates competitive empirical performance compared to leading deep learning-based baselines, especially when dealing with significant bound-unbound structure gaps. Importantly, ATProt demonstrates the ability to perform inference in native-structure free scenarios. ", "page_idx": 8}, {"type": "text", "text": "Limitations. To present a clear and focused issue, we do not incorporate adversarial training-based classifiers into our framework. However, it is expected that their inclusion could further enhance the performance of ATProt, as they have shown promising results in the field of computer vision. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by HKUST-HKUST(GZ) 20 for 20 Crosscampus Collaborative Research Scheme C019, and Shenzhen Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone under Grant No. HTHZQSWS-KCCYB-2023052. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training. Advances in Neural Information Processing Systems, 33:16048\u201316059, 2020. [2] James Atwood and Don Towsley. Diffusion-convolutional neural networks. Advances in neural information processing systems, 29, 2016.   \n[3] Ahmet Bakan, Lidio M Meireles, and Ivet Bahar. Prody: protein dynamics inferred from theory and experiments. Bioinformatics, 27(11):1575\u20131577, 2011. [4] Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1):235\u2013242, 2000.   \n[5] Aochuan Chen, Peter Lorenz, Yuguang Yao, Pin-Yu Chen, and Sijia Liu. Visual prompting for adversarial robustness. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135, 2023.   \n[6] Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. Understanding and improving visual prompting: A label-mapping perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19133\u201319143, June 2023. [7] Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Jiancheng Liu, Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura, and Sijia Liu. Deepzero: Scaling up zeroth-order optimization for deep model training. arXiv preprint arXiv:2310.02025, 2023.   \n[8] Muhao Chen, Chelsea J-T Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted protein\u2013protein interaction prediction based on siamese residual rcnn. Bioinformatics, 35(14):i305\u2013i314, 2019. [9] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolution. Advances in Neural Information Processing Systems, 33:4479\u20134488, 2020.   \n[10] Lee-Shin Chu, Jeffrey A Ruffolo, Ameya Harmalkar, and Jeffrey J Gray. Flexible protein\u2013 protein docking with a multitrack iterative transformer. Protein Science, 33(2):e4862, 2024.   \n[11] Kelly L Damm and Heather A Carlson. Gaussian-weighted rmsd superposition of proteins: a structural comparison for flexible proteins and predicted protein structures. Biophysical journal, 90(12):4558\u20134573, 2006.   \n[12] Sjoerd J de Vries, Christina EM Schindler, Isaure Chauvot de Beauch\u00eane, and Martin Zacharias. A web interface for easy flexible protein-protein docking with attract. Biophysical journal, 108(3):462\u2013465, 2015.   \n[13] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29, 2016.   \n[14] Tao Feng, Ziqi Gao, Jiaxuan You, Chenyi Zi, Yan Zhou, Chen Zhang, and Jia Li. Deep reinforcement learning for modelling protein complexes. arXiv preprint arXiv:2405.02299, 2024.   \n[15] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. Advances in neural information processing systems, 30, 2017.   \n[16] Paul J Gane and Philip M Dean. Recent advances in structure-based rational drug design. Current opinion in structural biology, 10(4):401\u2013404, 2000.   \n[17] Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein docking. arXiv preprint arXiv:2111.07786, 2021.   \n[18] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1\u201335, 2016.   \n[19] Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. Hierarchical graph learning for protein\u2013protein interaction. Nature Communications, 14(1):1093, 2023.   \n[20] Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, and Jia Li. Protein multimer structure prediction via prompt learning. arXiv preprint arXiv:2402.18813, 2024.   \n[21] Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameter-efficient fine-tuning with discrete fourier transform. arXiv preprint arXiv:2405.03003, 2024.   \n[22] Vladimir Gligorijevi\u00b4c, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based protein function prediction using graph convolutional networks. Nature communications, 12(1):3168, 2021.   \n[23] Mingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021.   \n[24] Yufei Huang, Siyuan Li, Lirong Wu, Jin Su, Haitao Lin, Odin Zhang, Zihan Liu, Zhangyang Gao, Jiangbin Zheng, and Stan Z Li. Protein 3d graph structure learning for robust structure-based protein property prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 12662\u201312670, 2024.   \n[25] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[27] Daniel E Koshland Jr. The key\u2013lock theory and the induced fit theory. Angewandte Chemie International Edition in English, 33(23-24):2375\u20132378, 1995.   \n[28] Jia Li, Jiajin Li, Yang Liu, Jianwei Yu, Yueting Li, and Hong Cheng. Deconvolutional networks on graph data. Advances in Neural Information Processing Systems, 34:21019\u201321030, 2021.   \n[29] Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, and Junzhou Huang. Adversarial attack on community detection by hiding individuals. In Proceedings of The Web Conference 2020, pages 917\u2013927, 2020.   \n[30] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. Zerog: Investigating crossdataset zero-shot transferability in graphs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1725\u20131735, 2024.   \n[31] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.   \n[32] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022.   \n[33] Yi Liu, Hao Yuan, Lei Cai, and Shuiwang Ji. Deep learning of high-order interactions for protein interface prediction. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 679\u2013687, 2020.   \n[34] Raksha Ramakrishna, Hoi-To Wai, and Anna Scaglione. A user guide to low-pass graph signal processing and its applications: Tools and applications. IEEE Signal Processing Magazine, 37(6):74\u201385, 2020.   \n[35] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53\u201365, 1987.   \n[36] Ruben Sanchez-Garcia, Carlos Oscar S\u00e1nchez Sorzano, Jos\u00e9 Mar\u00eda Carazo, and Joan Segura. Bipspi: a method for the prediction of partner-specific protein\u2013protein interfaces. Bioinformatics, 35(3):470\u2013477, 2019.   \n[37] Kristof T Sch\u00fctt, Farhad Arbabzadah, Stefan Chmiela, Klaus R M\u00fcller, and Alexandre Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature communications, 8(1):13890, 2017.   \n[38] Woody Sherman, Tyler Day, Matthew P Jacobson, Richard A Friesner, and Ramy Farid. Novel procedure for modeling ligand/receptor induced fit effects. Journal of medicinal chemistry, 49(2):534\u2013553, 2006.   \n[39] Freyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end learning on protein surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15272\u201315281, 2021.   \n[40] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. Gadbench: Revisiting and benchmarking supervised graph anomaly detection. Advances in Neural Information Processing Systems, 36:29628\u201329653, 2023.   \n[41] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. In International Conference on Machine Learning, pages 21076\u201321089. PMLR, 2022.   \n[42] Mieczyslaw Torchala, Iain H Moal, Raphael AG Chaleil, Juan Fernandez-Recio, and Paul A Bates. Swarmdock: a server for flexible protein\u2013protein docking. Bioinformatics, 29(6):807\u2013 809, 2013.   \n[43] Raphael Townshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-to-end learning on 3d protein structure for interface prediction. Advances in Neural Information Processing Systems, 32, 2019.   \n[44] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(11), 2008.   \n[45] Thom Vreven, Iain H Moal, Anna Vangone, Brian G Pierce, Panagiotis L Kastritis, Mieczyslaw Torchala, Raphael Chaleil, Brian Jim\u00e9nez-Garc\u00eda, Paul A Bates, Juan Fernandez-Recio, et al. Updates to the integrated protein\u2013protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2. Journal of molecular biology, 427(19):3031\u20133041, 2015.   \n[46] Yiqun Wang, Yuning Shen, Shi Chen, Lihao Wang, YE Fei, and Hao Zhou. Learning harmonic molecular representations on riemannian manifold. In The Eleventh International Conference on Learning Representations.   \n[47] Lennart Wirthmueller, Abbas Maqbool, and Mark J Banfield. On the front line: structural insights into plant\u2013pathogen interactions. Nature Reviews Microbiology, 11(11):761\u2013776, 2013.   \n[48] Chyuan-Chuan Wu, Tsai-Kun Li, Lynn Farh, Li-Ying Lin, Te-Sheng Lin, Yu-Jen Yu, Tien-Jui Yen, Chia-Wang Chiang, and Nei-Li Chan. Structural basis of type ii topoisomerase inhibition by the anticancer drug etoposide. Science, 333(6041):459\u2013462, 2011.   \n[49] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR, 2019.   \n[50] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network. arXiv preprint arXiv:1904.07785, 2019.   \n[51] Yumeng Yan, Di Zhang, Pei Zhou, Botong Li, and Sheng-You Huang. Hdock: a web server for protein\u2013protein and protein\u2013dna/rna docking based on a hybrid strategy. Nucleic acids research, 45(W1):W365\u2013W373, 2017.   \n[52] Zheng Yuan, Timothy L Bailey, and Rohan D Teasdale. Prediction of protein b-factor profiles. Proteins: Structure, Function, and Bioinformatics, 58(4):905\u2013912, 2005.   \n[53] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary of \u201cTowards Stable Representations for Protein Interface Prediction\u201d ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Representing proteins with graph data ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ be an undirected graph with nodes $\\mathcal{V}=\\{{1,...,N}\\}$ , edges $\\mathcal{E}\\subset\\mathcal{V}^{2}$ , graph signal $F\\in\\mathbb{R}^{N\\times d}$ and a graph shift operator ${\\bf L}\\in\\mathbb{R}^{N\\times N}$ (i.e., node connectivity). We construct edges with the $\\mathbf{k}\\cdot$ -nearest neighbor ( $\\mathrm{\\bfk}$ -NN) algorithm and each node in $\\mathcal{G}$ is connected to the 10 closest nodes within a physical distance of less than $30\\textup{\\AA}$ . The edge attributes are distances between $\\alpha$ -carbon atoms encoded with Gaussian basis functions. The nodes have two kinds of attributes: the one-hot encoding of amino acid type and the surface-aware features at the residue level. The latter is defined by [17] to distinguish residues closer to the protein surface from those in the interior. Notably, we do not introduce any biochemically related attributes of atoms or amino acids featurization. Instead, protein graphs are constructed solely using their $\\alpha$ -carbon coordinates. ", "page_idx": 13}, {"type": "text", "text": "B Proofs of main propositions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Assuming we model the protein interface prediction as a pairwise classification problem. Basically, this proposition provides a perturbation radius, within which if the perturbations of two protein representations fall, the predicted correspondences of all pairs of inter-protein residues will be robust. ", "page_idx": 13}, {"type": "text", "text": "Formally, we apply the model $\\Psi$ to obtain $d$ -dimensional protein representations $H_{1}=\\Psi(\\mathcal{P}_{1})\\in$ $\\mathbb{R}^{M\\times d},\\dot{\\pmb{H}}_{2}\\,=\\,\\dot{\\Psi}(\\dot{\\mathcal{P}}_{2})\\,\\in\\,\\mathbb{R}^{N\\times d}$ . The $i$ -th row of ${\\cal{H}}_{1}$ and $j$ -th row of ${\\cal{H}}_{2}$ are concatenated into $\\pmb{x}^{(i j)}\\,\\in\\,\\mathbb{R}^{2d}$ , which is the pairwise residue representations. We then use a neural network $f$ to calculate the probability of the presence of residue correspondence, and define the classifier as $g(\\pmb{x}^{(i j)}):=\\mathrm{arg\\;max}_{k}f_{k}\\big(\\pmb{x}^{(i j)}\\big)$ . Assuming that the structure changes of two proteins cause perturbations $\\delta_{1},\\delta_{2}$ on ${\\cal{H}}_{1}$ and $H_{2}$ , respectively, the interface prediction robustness can be determined by the following proposition. First, we rewrite Proposition 3.1 into a more formal expression. ", "page_idx": 13}, {"type": "text", "text": "Proposition B.1. For all $i\\in[1,M]\\cap\\mathbb{Z}$ and $j\\in[1,N]\\cap\\mathbb{Z},$ , the interface classifier g is provably robust for arbitrary $\\pmb{x}^{(i j)}$ (i.e., $g$ is robust for all residue pairs of $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$ ) if $\\left.\\right/N\\left\\|\\delta_{1}\\right\\|_{p}^{p}+M\\left\\|\\delta_{2}\\right\\|_{p}^{p}<$ ${\\mathcal{A}}(f,p)$ , where ${\\mathcal{A}}(f,p)$ can be a constant depending only on $f$ and the norm $\\left\\|\\cdot\\right\\|_{p}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. . We denote $\\textbf{\\em x}$ as a variable for convenience to represent the pairwise residue representation. Let $g(\\pmb{x})=y$ . Suppose there exists a perturbation $\\delta$ such that $g(\\pmb{x}\\mp\\delta)\\neq g(\\pmb{x})$ and $\\bar{f}_{j}({\\pmb x}+{\\pmb\\delta})\\geq$ $f_{y}({\\pmb x}+{\\pmb\\delta})$ for some $j\\neq y$ . We first prove that $\\begin{array}{r}{\\|\\delta\\|_{p}\\geq\\frac{\\ell\\sqrt{2}}{2C}\\cdot\\operatorname*{margin}(f(\\pmb{x}))}\\end{array}$ , where $C$ is the lipschitz constant of $f$ , margin $(f(x))$ is the margin between the largest and second runner-up output logits. Define ${z}^{\\prime}\\,=\\,f({\\pmb x}+{\\pmb\\delta})$ , then $z_{y}^{\\prime}\\,\\le\\,z_{j}^{\\prime}$ . The difference between outputs $_{\\textit{z}}$ and $f(\\pmb{x})$ can be lower bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|z^{\\prime}-f(\\mathbf{x})\\|_{p}\\ge\\big\\|(z_{y}^{\\prime},z_{j}^{\\prime})^{T}-([f_{y}(\\mathbf{x}),f_{j}(\\mathbf{x})])^{T}\\big\\|=(|z_{y}^{\\prime}-f_{y}(\\mathbf{x})|^{p}+|z_{j}^{\\prime}-f_{j}(\\mathbf{x})|^{p})^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the above equation, we utilize the fact that setting certain elements of a vector to zero can only decrease its $p$ -norm. Let us now consider the following optimization problem: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z^{\\prime}}|z_{y}^{\\prime}-f_{y}(x)|^{p}+|z_{j}^{\\prime}-f_{j}(x)|^{p}\\qquad\\mathrm{s.t.~z_{y}^{\\prime}\\leq z_{j}^{\\prime}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When $z_{y}^{\\prime}=z_{j}^{\\prime}=(f_{y}(\\pmb{x})+f_{j}(\\pmb{x}))/2$ , we have the minimum of (2) and the update for (11): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|z^{\\prime}-f(x)\\|_{p}\\geq\\big\\|(z_{y}^{\\prime},z_{j}^{\\prime})^{T}-([f_{y}(x),f_{j}(x)])^{T}\\big\\|\\geq\\frac{\\sqrt{2}}{2}(f_{y}(x)-f_{j}(x))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the Lipschitz constant of $f$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|z^{\\prime}-f(\\pmb{x})\\right\\|_{p}\\leq C\\left\\|\\delta\\right\\|_{p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Considering (3) and (4), we have the initial conclusion as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\delta\\|_{p}^{p}\\geq\\left({\\frac{\\sqrt[{p}]{2}}{2C}}\\cdot\\operatorname{margin}(f(\\pmb{x}))\\right)^{p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we consider the complete residue pair representation set $\\{\\pmb{x}^{(i j)}\\}_{i\\in[1,M]\\cap\\mathbb{Z},j\\in[1,N]\\cap\\mathbb{Z}}.$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{M}\\sum_{j=1}^{N}\\left\\|\\delta^{(i j)}\\right\\|_{p}^{p}\\geq\\sum_{i=1}^{M}\\sum_{j=1}^{N}\\left(\\frac{\\sqrt[p]{2}}{2C}\\cdot\\mathrm{margin}(f(\\pmb{x}^{(i j)}))\\right)^{p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We denote the perturbations on protein representations ${\\cal{H}}_{1}$ and ${\\cal{H}}_{2}$ as $\\delta_{1}$ and $\\delta_{2}$ , respectively. $\\delta^{(i j)}$ is actually the concatenation of the $i$ -th row of $\\delta_{1}$ and the $i$ -th row of $\\delta_{2}$ , i.e., $\\delta^{(i j)}=\\mathrm{AGG}(\\delta_{1},\\delta_{2})$ . Thus the left term of (6) is equal to $N\\left\\|\\delta_{1}\\right\\|_{p}^{p}+M\\left\\|\\delta_{2}\\right\\|_{p}^{p}.$ . Finally, we conclude the proof: ", "page_idx": 14}, {"type": "equation", "text": "$$\nN\\left\\|\\delta_{1}\\right\\|_{p}^{p}+M\\left\\|\\delta_{2}\\right\\|_{p}^{p}\\geq\\sum_{i=1}^{M}\\sum_{j=1}^{N}\\left({\\frac{{\\sqrt{2}}}{2C}}\\cdot{\\mathrm{margin}}(f(\\pmb{x}^{(i j)}))\\right)^{p}=M N\\left({\\frac{{\\sqrt{2}}}{2C}}\\cdot{\\mathrm{margin}}^{*}(f(\\pmb{x}))\\right)^{p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where margin $^*(f(x))$ is the average value of all margins w.r.t the $M N$ concatenated residue representations. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of the statement after Theorem 3.1 that $C_{h}^{*}$ is equal to the MAS. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We prove this based on the definition of graph Lipschitz fliter, which is continuously differentiable due to the polynomial approximation. ", "page_idx": 14}, {"type": "text", "text": "Let $C_{h}$ be the Lipschitz constant of $h$ , and let $u(\\lambda)=|h^{\\prime}(\\lambda)|$ . We want to show that $C_{h}$ is minimized when $C_{h}=\\operatorname*{sup}_{\\lambda}u(\\lambda)$ . ", "page_idx": 14}, {"type": "text", "text": "Suppose there exist $\\lambda_{1}$ and $\\lambda_{2}$ such that $|\\lambda_{1}-\\lambda_{2}|>0$ and $|h(\\lambda_{1})-h(\\lambda_{2})|>C_{h}|\\lambda_{1}-\\lambda_{2}|$ . Then, by the mean value theorem, there exists $\\lambda_{3}$ between $\\lambda_{1}$ and $\\lambda_{2}$ such that $|h^{\\prime}(\\lambda_{3})|>C_{h}$ . But this contradicts the assumption that $C_{h}$ is the Lipschitz constant of $h$ . Therefore, $C_{h}$ must be greater than or equal to $\\operatorname{sup}_{\\lambda}u(\\lambda)$ . ", "page_idx": 14}, {"type": "text", "text": "To show that $C_{h}$ is minimized when $C_{h}=\\operatorname*{sup}_{\\lambda}u(\\lambda)$ , suppose there exists a Lipschitz constant $C_{h}^{\\prime}$ such that $C_{h}^{\\prime}<\\operatorname*{sup}_{\\lambda}u(\\lambda)$ . Then, for any $\\lambda_{1}$ and $\\lambda_{2}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert h(\\lambda_{1})-h(\\lambda_{2})\\vert\\le C_{h}^{\\prime}\\vert\\lambda_{1}-\\lambda_{2}\\vert}&{}\\\\ {<\\operatorname*{sup}_{\\lambda}u(\\lambda)\\vert\\lambda_{1}-\\lambda_{2}\\vert}&{}\\\\ {\\le\\vert h^{\\prime}(\\lambda_{3})\\vert\\vert\\lambda_{1}-\\lambda_{2}\\vert}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda_{3}$ is some point between $\\lambda_{1}$ and $\\lambda_{2}$ . But this contradicts the definition of $u(\\lambda)$ as the maximum of $|h^{\\prime}(\\lambda)|$ . Therefore, $C_{h}$ must be equal to $\\operatorname{sup}_{\\lambda}u(\\lambda)$ . ", "page_idx": 14}, {"type": "text", "text": "Thus, we have shown that $C_{h}$ is minimized when $C_{h}\\,=\\,\\operatorname*{sup}_{\\lambda}u(\\lambda)$ , as desired. For readability, we denote the minimum of $C_{h}$ as $C_{h}^{*}$ and $\\operatorname{sup}_{\\lambda}u(\\lambda)$ as the MAS (maximum absolute slope) of $h$ , respectively. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.3 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We start the analysis with the unique stability-preservation property of the Bernstein basis shown in Theorem 4.1. Simply put, during Bernstein polynomial approximation, the outcome polynomial $h(\\lambda)$ is at least as stable as the target function $f(\\lambda)$ . This is crucial as it tells that the MAS of an exact function $f(\\lambda)$ can always serve as an upper bound for $C_{h}^{*}$ . ", "page_idx": 14}, {"type": "image", "img_path": "OEWBkLrRZu/tmp/0e062f0ff1169ec1545d8db282052c5fccf62be3035f18b17ebcd01327510561.jpg", "img_caption": ["Figure 6: The way to find upper bound of $C_{h}^{*}$ , a case to explain Proposition 4.1. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The relationship between $f(\\lambda)$ and $\\{\\theta_{k}\\}_{k=0}^{K}$ . Setting the polynomial order to $K$ , our model learns a set of $K+1$ weights $\\{\\theta_{k}\\}_{k=0}^{K}$ that acts as the coefficients for polynomial approximation, such that $\\textstyle f({\\frac{2k}{K}})=\\theta_{k}$ , for all $k\\in[0,K]\\cap\\mathbb{Z}$ . Put differently, any 2-D curve passing through all points of $(0/K,\\dot{\\theta_{0}_{0}}),(2/K,\\theta_{1})..,(2K/K,\\theta_{K})$ can be regarded as a possible version of $f(\\lambda)$ for polynomial approximation. Consequently, given any set of $\\{\\theta_{k}\\}_{k=0}^{K}$ , the stability of $h(\\lambda)$ is not worse than the most stable version among all possible $f(\\lambda)$ . ", "page_idx": 15}, {"type": "text", "text": "tFhoeromreatlilcya,l  wupe pnera tbuoraulnlyd  feoxrt $C_{h}^{*}$ otfh $h(\\lambda)$ ,e odrireemct l4y. 1u stion gt hteh ef loellaornwaibnlge  cprooefpfoicsiietinotns , $\\{\\theta_{k}\\}_{k=0}^{K}$ p.rovides a ", "page_idx": 15}, {"type": "text", "text": "As $h$ is at least as stable as the most stable $f$ , the MAS of $h$ can be upper bounded by the minimum throu MAS among all possible points $\\{(k/K,\\theta_{k})\\}_{k=0}^{K}$ $f$ functions, which is infinitely close to the MAS of the broken line passing . Figure 6 helps to better understand Proposition 4.1. For example, let us set $K$ to 1, any curve passing through and can be a version of . Out of all possible $f$ versions, the line segment directly connecting two points has the minimum MAS (i.e., $\\frac{|\\theta_{0}\\!-\\!\\theta_{1}|}{2}\\rangle$ ). ", "page_idx": 15}, {"type": "text", "text": "We have introduced in the main text the relationship between $f(\\lambda)$ and $\\{\\theta\\}_{k=0}^{K}$ . Based on this, the proof of Proposition 3.2 is equivalent to proving the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. Given $K+1$ points $(0,\\theta_{0})$ , $\\textstyle({\\frac{2}{K}},\\theta_{1})$ , . . . , $\\textstyle\\left({\\frac{2K}{K}},\\theta_{K}\\right)$ , suppose there are infinitely many functions $f(\\lambda)$ that pass through these $K+1$ points. The maximum absolute slope (MAS) of any version of a function $f(\\lambda)$ is not less than the MAS of the piecewise linear function passing through these $K+1$ points. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $f_{b r o k e n}(\\lambda)$ be the piecewise linear function passing through the given $K+1$ points. The slope of $f_{b r o k e n}(\\lambda)$ between two consecutive points $\\big(\\frac{2i}{K},\\theta_{i}\\big)$ and $\\textstyle\\big(\\frac{2(i+1)}{K},\\theta_{i+1}\\big)$ is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\nm_{i}=\\frac{\\theta_{i+1}-\\theta_{i}}{\\frac{2(i+1)}{K}-\\frac{2i}{K}}=\\frac{\\theta_{i+1}-\\theta_{i}}{\\frac{2}{K}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The MAS of $f_{b r o k e n}(\\lambda)$ is the maximum of the absolute values of these slopes: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{MAS}_{b r o k e n}=\\operatorname*{max}_{0\\leq i\\leq K-1}|m_{i}|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, consider any function $f(\\lambda)$ that passes through the $K+1$ points. Since $f(\\lambda)$ is differentiable, by the mean value theorem, for each interval 2Ki , 2(iK+1) , there exists a point \u03bbi such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{\\prime}(\\lambda_{i})=\\frac{f\\left(\\frac{2(i+1)}{K}\\right)-f\\left(\\frac{2i}{K}\\right)}{\\frac{2(i+1)}{K}-\\frac{2i}{K}}=\\frac{\\theta_{i+1}-\\theta_{i}}{\\frac{2}{K}}=m_{i}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $f(\\lambda)$ passes through all the given points, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathsf{M A S}_{f}\\geq\\operatorname*{max}_{0\\leq i\\leq K-1}|f^{\\prime}(\\lambda_{i})|=\\operatorname*{max}_{0\\leq i\\leq K-1}|m_{i}|=\\mathsf{M A S}_{b r o k e n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the MAS of any function $f(\\lambda)$ satisfying the conditions is greater than or equal to the MAS of the piecewise linear function $f_{b r o k e n}(\\lambda)$ passing through the points. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Results on DIPS ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 3 shows the results on DIPS. Kindly note that we train and test all methods both on the DIPS dataset. ", "page_idx": 16}, {"type": "table", "img_path": "OEWBkLrRZu/tmp/345c229feb3d073baa6a395ac76b8e312a6904bd66a5aa81bdb2e36988380704.jpg", "table_caption": ["Table 3: Training and testing on DIPS. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D The cross-attention layer used ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\pmb{H}_{1}^{\\prime}=\\mathrm{softmax}\\left(\\frac{(\\pmb{H}_{1}\\pmb{W}_{Q})(\\pmb{H}_{2}\\pmb{W}_{K})^{T}}{\\sqrt{d}}\\right)(\\pmb{H}_{2}\\pmb{W}_{V})\\,,}\\\\ {\\pmb{H}_{2}^{\\prime}=\\mathrm{softmax}\\left(\\frac{(\\pmb{H}_{2}\\pmb{W}_{Q})(\\pmb{H}_{1}\\pmb{W}_{K})^{T}}{\\sqrt{d}}\\right)(\\pmb{H}_{1}\\pmb{W}_{V})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E Hyper-parameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The training process takes around 0.5 hours with 1 Nvidia 4090 GPUs with 24GB RAM. The hyper-parameters used in this paper are listed in the following table. ", "page_idx": 16}, {"type": "table", "img_path": "OEWBkLrRZu/tmp/c734c840c656d4778500a2933983457130f288eb2eb0ed6a641287e7a7ae5091.jpg", "table_caption": [], "table_footnote": ["Table 4: Hyperparameter choices of ATPROT. "], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] Justification: The contributions are included in the abstract and the introduction. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Section B. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] Justification: See Section 5 ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The code is provided in https://github.com/ATProt/ATProt. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In our experiments, we report the mean and standard deviation of three random seeds. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Section Appendix E. ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are some potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper poses no safety risks. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The materials in this paper are used with permission and properly cited. The Database of Interacting Protein Structures (DIPS) is under the MIT License. DB 5.5 is under a Creative Commons Attribution 4.0 International License ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The developed code is provided in https://github.com/ATProt/ATProt. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 18}]