[{"figure_path": "YRemB4naKK/tables/tables_4_1.jpg", "caption": "Algorithm 1 Reduction to RL", "description": "This algorithm reduces the constrained reinforcement learning problem to an unconstrained Markov Decision Process (MDP) problem by augmenting the state space with value demands. The algorithm first transforms the original constrained MDP and cost criterion into a new MDP with a modified cost criterion. Then, it solves the new MDP using a cost-minimizing algorithm. Finally, it checks if the new MDP's cost is within the budget. If the cost is within the budget, then the algorithm outputs a feasible policy; otherwise, it outputs \"infeasible\".", "section": "3 Covering algorithm"}, {"figure_path": "YRemB4naKK/tables/tables_6_1.jpg", "caption": "Algorithm 3 Approx Bellman Update", "description": "This algorithm details the steps for approximating the Bellman update in the context of the covering program. It uses a recursive approach and incorporates value rounding to efficiently compute the update. The algorithm takes as input the current time step, state, value demand, and next-step cost and outputs the optimal action and its associated cost.", "section": "4 Fast Bellman updates"}, {"figure_path": "YRemB4naKK/tables/tables_8_1.jpg", "caption": "Algorithm 5 Approximation Scheme", "description": "This algorithm provides an approximation scheme for solving the constrained optimization problem (CON) by carefully rounding value demands.  It first defines an approximate MDP (Markov Decision Process) using a rounding function and a lower bound function. Then, it solves this approximate MDP using Algorithm 4.  The algorithm returns either \"Infeasible\" if no feasible policy is found, or a near-optimal deterministic policy \u03c0.", "section": "5 Approximation algorithms"}]