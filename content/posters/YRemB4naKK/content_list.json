[{"type": "text", "text": "Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jeremy McMahan University of Wisconsin-Madison jmcmahan@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a novel algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems. Our approach combines three key ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. Our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for any timespace recursive (TSR) cost criteria. A TSR criteria requires the cost of a policy to be computable recursively over both time and (state) space, which includes classical expectation, almost sure, and anytime constraints. Our work answers three open questions spanning two long-standing lines of research: polynomial-time approximability is possible for 1) anytime-constrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Constrained Reinforcement Learning (CRL) traditionally produces stochastic, expectationconstrained policies that can behave undesirably - imagine a self-driving car that randomly changes lanes or runs out of fuel. However, artificial decision-making systems must be predictable, trustworthy, and robust. One approach to ensuring these qualities is to focus on deterministic policies, which are inherently predictable, easily implemented [19], reliable for autonomous vehicles [30, 23], and effective for multi-agent coordination [38]. Similarly, almost sure and anytime constraints [36] provide inherent trustworthiness and robustness, essential for applications in medicine [15, 37, 32], disaster relief [18, 50, 45], and resource management [35, 34, 40, 7]. Despite the advantages of deterministic policies and stricter constraints, even the computation of approximate solutions has remained an open challenge since NP-hardness was proven nearly 25 years ago [19]. Our work addresses this challenge by studying the computational complexity of computing deterministic policies for general constraint criteria. ", "page_idx": 0}, {"type": "text", "text": "Consider a constrained Markov Decision Process (cMDP) denoted by $M$ . Let $C$ represent an arbitrary cost criterion and $B$ be the available budget. We focus on the set of deterministic policies denoted by $\\Pi^{D}$ . Our objective is to compute: $\\operatorname*{max}_{\\pi\\in\\Pi^{D}}$ $V_{M}^{\\pi}$ s.t. $C_{M}^{\\pi}\\le B$ , where $V_{M}^{\\pi}$ is the value and $C_{M}^{\\pi}$ is the cost of $\\pi$ in $M$ . This objective generalizes the example of a self-driving car calculating the fastest fixed route without running out of fuel. Our main question is the following: ", "page_idx": 0}, {"type": "text", "text": "Can near-optimal deterministic policies for constrained reinforcement learning problems be computed in polynomial time? ", "page_idx": 0}, {"type": "text", "text": "Although optimal stochastic policies for expectation-constrained problems are efficiently computable [3], the situation drastically changes when we require deterministic policies and general constraints. Computing optimal deterministic policies is NP-hard for most popular constraints, including expectation [19], chance [51], almost sure, and anytime constraints [36]. This complexity remains even if we relax our goal to finding just one feasible policy, provided that we are dealing with a single chance constraint [51], or at least two of the other mentioned constraints [36]. Beyond these computational challenges, traditional solution methods, such as backward induction [41, 3], fail to apply due to the cyclic dependencies among subproblems: the value of any decision may depend on the costs of both preceding and concurrent decisions, preventing a solution from being computed in a single backward pass. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Past work. Past approaches fail to simultaneously achieve computational efficiency, feasibility, and optimality. Optimal and feasible algorithms, albeit inefficient, utilize Mixed-Integer Linear Programs [17] and Dual-guided heuristic forward searches [29] for expectation-constraints, and cost-augmented MDPs for almost sure [11] and anytime constraints [36]. Conversely, optimal and efficient, though infeasible, algorithms are known for expectation [43], almost sure, and anytime constraints [36]. A fully polynomial-time approximation scheme (FPTAS) [49] is known for expectation constraints, but it requires strong assumptions such as a constant horizon [31]. Balancing computational efficiency, feasibility, and optimality remains the bottleneck to efficient approximation. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. We present an FPTAS for computing deterministic policies under any timespace recursive (TSR) constraint criteria. A TSR criteria requires the cost of a policy to be computable recursively in both time and (state) space, which captures expectation, almost sure, and anytime constraints. Since non-TSR criteria, such as chance constraints [51], are provably inapproximable, TSR seems pivotal for efficient computation. Overall, our general framework answers three open complexity questions spanning two longstanding lines of work: we prove polynomial-time approximability for 1) anytime-constrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies, which have been open for nearly 25 years [19]. ", "page_idx": 1}, {"type": "text", "text": "Our approach breaks down into three main ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. We augment the states with value demands and the actions with future value demands to break cyclic subproblem dependencies, enabling dynamic programming methods. Importantly, we use values because they can be rounded without compromising feasibility [36] and can capture constraints that are not predictable from cumulative costs. However, this results in an exponential action space that makes solving the Bellman operator as hard as the knapsack problem. By exploiting the space-recursive nature of the criterion, we can efficiently approximate the Bellman operator with dynamic programming. Finally, rounding value demands result in approximation errors over both time and space, but carefully controlling these errors ensures provable guarantees. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Approximate packing. Many stochastic packing problems, which generalize the knapsack problem, are captured by our problem. Dean et al. [16], Frieze and Clarke [21] derived optimal approximation ratio algorithms for stochastic packing and integer packing with multiple constraints, respectively. Yang et al. [52], Bhalgat et al. [6] designed efficient approximation algorithms for variations of the stochastic knapsack problem. Then, Halman et al. [27] derived an FPTAS for a general class of stochastic dynamic programs, which was then further improved in [26, 1]. These methods require a single-dimensional state space that captures the constraint. In contrast, our problems have an innate state space in addition to the constraint. Our work forms a similar general dynamic programming framework for the more complex MDP setting. ", "page_idx": 1}, {"type": "text", "text": "Constrained RL. It is known that stochastic expectation-constrained policies are polynomial-time computable via linear programming [3], and many planning and learning algorithms exist for them [39, 46, 8, 28]. Some learning algorithms can even avoid violation during the learning process under certain assumptions [48, 4]. Furthermore, Brantley et al. [10] developed no-regret algorithms for cMDPs and extended their algorithms to the setting with a constraint on the cost accumulated over all episodes, which is called a knapsack constraint [10, 13]. ", "page_idx": 1}, {"type": "text", "text": "Safe RL. The safe RL community [22, 25] has mainly focused on no-violation learning for stochastic expectation-constrained policies [14, 9, 2, 12, 5] and solving chance constraints [47, 53], which capture the probability of entering unsafe states. Performing learning while avoiding dangerous states [53] is a special case of expectation constraints that has also been studied [42, 44] under non-trivial assumptions. In addition, instantaneous constraints, which require the expected cost to be within budget at all times, have also been studied [33, 20, 24]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Cost criteria ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formalize our problem setting. We also define our conditions for cost criteria. ", "page_idx": 2}, {"type": "text", "text": "Constrained Markov decision processes. A (tabular, finite-horizon) Constrained Markov Decision Process (cMDP) is a tuple $M=(S,\\mathcal{A},P,r,c,H)$ , where (i) $\\boldsymbol{S}$ is the finite set of states, (ii) $\\mathcal{A}$ is the finite set of actions, (iii) $P_{h}(s,a)\\in\\Delta(S)$ is the transition distribution, (iv) $r_{h}(s,a)\\in\\mathbb{R}$ is the reward, (v) $c_{h}(s,a)\\in\\mathbb{R}$ is the cost, and (vi) $H\\in\\mathbb{N}$ is the finite time horizon. We let $S:=|S|$ , $A:=|A|$ , $[H]:=\\{1,\\dots,H\\}$ , and $\\mathcal{M}$ denote the set of all cMDPs. We also let $r_{m a x}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname*{max}_{h,s,a}\\left|r_{h}(s,a)\\right|$ denote the maximum magnitude reward, $\\begin{array}{r}{r_{m i n}\\overset{\\mathrm{def}}{=}\\operatorname*{min}_{h,s,a}r_{h}(s,a)}\\end{array}$ denote the true minimum reward, and $\\begin{array}{r}{p_{m i n}\\overset{\\mathrm{def}}{=}\\operatorname*{min}_{h,s,a,s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)}\\end{array}$ denote the minimum transition probability. Since $\\boldsymbol{S}$ is a finite set, we often assume $\\cal S=[S]$ WLOG. Lastly, for any predicate $p$ , we use the Iverson bracket notation $[p]$ to denote 1 if $p$ is true and 0 otherwise, and we let $\\chi_{p}$ denote the characteristic function which evaluates to $0$ if $p$ is true and $\\infty$ otherwise. ", "page_idx": 2}, {"type": "text", "text": "Interaction protocol. The agent interacts with $M$ using a policy $\\pi\\,=\\,(\\pi_{h})_{h=1}^{H}$ . In the fullest generality, $\\pi_{h}:\\mathcal{H}_{h}\\to\\Delta(A)$ is a mapping from the observed history at time $h$ to a distribution of actions. In contrast, a deterministic policy takes the form $\\pi_{h}:\\mathcal{H}_{h}\\rightarrow\\mathcal{A}$ . We let $\\Pi$ denote the set of all possible policies and $\\Pi^{D}$ denote the set of all deterministic policies. The agent starts at the initial state $s_{0}\\in\\mathcal{S}$ with observed history $\\tau_{1}=\\left(s_{0}\\right)$ . For any $h\\in[H]$ , the agent chooses an action $a_{h}\\sim\\pi_{h}(\\tau_{h})$ . Then, the agent receives immediate reward $r_{h}(s_{h},a_{h})$ and cost $c_{h}(s_{h},a_{h})$ . Lastly, $M$ transitions to state $s_{h+1}\\sim P_{h}(s_{h},a_{h})$ and the agent updates the history to $\\tau_{h+1}=(\\tau_{h},a_{h},s_{h+1})$ This process is repeated for $H$ steps; the interaction ends once $s H{+}1$ is reached. ", "page_idx": 2}, {"type": "text", "text": "Objective. For any cost criterion $C:\\mathcal{M}\\times\\Pi\\rightarrow\\mathbb{R}$ and budget $B\\,\\in\\,\\mathbb{R}$ , the agent\u2019s goal is to compute a solution to the following optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{M}^{\\pi}\\left[\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})\\right]\\quad\\mathrm{~s.t.~}\\quad\\left\\{\\!C_{M}^{\\pi}\\leq B\\!\\atop\\pi\\mathrm{~deterministic}\\quad\\right..\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\mathbb{P}_{M}^{\\pi}$ denotes the probability law over histories induced from the interaction of $\\pi$ with $M$ , and $\\mathbb{E}_{M}^{\\pi}$ denotes the expectation defined by this law. We let $\\begin{array}{r}{V_{M}^{\\pi}\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\mathbb{E}}_{M}^{\\pi}\\left[\\sum_{t=1}^{H}r_{t}(s_{t},a_{t})\\right]}\\end{array}$ denote the value of a policy $\\pi$ , and $V_{M}^{*}$ denote the optimal solution value to (CON). ", "page_idx": 2}, {"type": "text", "text": "Cost criteria. We consider a broad family of cost criteria that satisfy a strengthening of the standard policy evaluation equations [41]. This strengthening requires not only the cost of a policy to be computable recursively in the time horizon, but at each time the cost should also break down recursively in (state) space. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (TSR). We call a cost criterion $C$ time-recursive (TR) if for any cMDP $M$ and policy $\\pi\\,\\in\\,\\Pi^{D}$ , $\\pi$ \u2019s cost decomposes recursively into $C_{M}^{\\pi}\\,=\\,C_{1}^{\\pi}(s_{0})$ . Here, $C_{H+1}^{\\pi}(\\cdot)\\,=\\,\\mathbf{0}$ and for any $h\\in[H]$ and $\\tau_{h}\\in\\mathcal{H}_{h}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nC_{h}^{\\pi}(\\tau_{h})=c_{h}(s,a)+f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a),C_{h+1}^{\\pi}\\left(\\tau_{h},a,s^{\\prime}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a)}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s=s_{h}(\\tau_{h})$ , $a=\\pi_{h}(\\tau_{h})$ , and $f$ is a non-decreasing function1 computable in $O(S)$ time. For technical reasons, we also require that $f(x)=\\infty$ whenever $\\infty\\in x$ . ", "page_idx": 2}, {"type": "text", "text": "We further say $C$ is time-space-recursive (TSR) if the $f$ term above is equal to $g_{h}^{\\tau_{h},a}(1)$ . Here, $g_{h}^{\\tau_{h},a}(S+1)=0$ and for any $t\\leq S$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\ng_{h}^{\\tau_{h},a}(t)=\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),C_{h+1}^{\\pi}\\left(\\tau_{h},a,t\\right)\\right),g_{h}^{\\tau_{h},a}(t+1)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha$ is a non-decreasing function, and both $\\alpha,\\beta$ are computable in $O(1)$ time. We also assume that $\\alpha(\\cdot,\\infty)=\\infty$ , and $\\beta$ satisfies $\\alpha(\\beta(0,\\cdot),x)=x$ to match $f$ \u2019s condition. ", "page_idx": 3}, {"type": "text", "text": "Since the TR condition is a slight generalization of traditional policy evaluation, it is easy to see that we can solve for minimum-cost policies using backward induction. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (TR Intuition). If $C$ is TR, then $C$ satisfies the usual optimality equations. Furthermore, arg $\\operatorname*{min}_{\\pi\\in\\Pi^{D}}C$ $C_{M}^{\\pi}$ can be computed using backward induction in $O(H S^{2}A)$ time. ", "page_idx": 3}, {"type": "text", "text": "Although the TR condition is straightforward, the TSR condition is more strict. We will see the utility of the TSR condition in Section 4 when computing Bellman updates. For now, we point out that the TSR condition is not too restrictive: it is satisfied by many popular criteria studied in the literature. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (TSR examples). The following classical constraints can be modeled by a TSR cost constraint. ", "page_idx": 3}, {"type": "text", "text": "1. (Expectation Constraints) are captured by $\\begin{array}{r}{C_{M}^{\\pi}\\overset{d e f}{=}\\mathbb{E}_{M}^{\\pi}\\left[\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})\\right]\\le B}\\end{array}$ . We see $C$ is TSR by defining $\\alpha(x,y)\\stackrel{d e f}{=}x+y$ and $\\beta(x,y)\\stackrel{d e f}{=}x y$ .   \n2. (Almost Sure Constraints) are captured by $\\begin{array}{r}{C_{M}^{\\pi}\\overset{d e f}{=}\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}},\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})\\le B.}\\end{array}$ $\\mathbb{P}_{M}^{\\pi}[\\tau]\\!>\\!0$ see $C$ is TSR by defining $\\alpha(x,y)\\stackrel{d e f}{=}\\operatorname*{max}(x,y)$ and $\\beta(x,y)\\stackrel{d e f}{=}[x>0]y.$ .   \n3. (Anytime Constraints) are captured by $\\begin{array}{r}{C_{M}^{\\pi}\\overset{d e f}{=}\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1},\\;\\sum_{h=1}^{t}c_{h}\\left(s_{h},a_{h}\\right)\\;\\leq}}\\end{array}$ P\u03c0M[\u03c4]>0 B. We see $C$ is TSR by defining $\\alpha(x,y)\\stackrel{d e f}{=}\\operatorname*{max}(0,\\operatorname*{max}(x,y))$ and $\\beta(x,y)\\stackrel{d e f}{=}[x>0]y.$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1 (Extensions). Our methods can also handle stochastic costs and infinite discounting. We defer the details to Appendix F. Moreover, we can handle multiple constraints using vector-valued criteria so long as the comparison operator is a total ordering of the vector space. ", "page_idx": 3}, {"type": "text", "text": "Remark 2 (Inapproximability). Our methods cannot handle chance constraints or more than one of our example constraints. However, this is not a limitation of our framework as the problem becomes provably inapproximable under said constraints [51, 36]. ", "page_idx": 3}, {"type": "text", "text": "3 Covering algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose an algorithm to solve (CON). Our approach relies on converting the original problem into an equivalent covering problem that can be solved using an unconstrained MDP. This covering MDP is derived using the key idea of value augmentation. ", "page_idx": 3}, {"type": "text", "text": "Packing and covering. We can view (CON) as a packing program, which wishes to maximize $V_{M}^{\\pi}$ subject to $C_{M}^{\\pi}\\le B$ . However, we could also tackle the problem by reversing the objective: attempt to minimize $C_{M}^{\\pi}$ subject to $V_{M}^{\\pi}\\geq V_{M}^{*}$ . If (CON) is feasible, then any optimal solution $\\pi$ to this covering program satisfies $V_{M}^{\\pi}\\geq V_{M}^{*}$ and $C_{M}^{\\pi}\\le B$ . Thus, we can solve the original packing program by solving the covering program. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3 (Packing-Covering Reduction). Suppose that $C_{M}^{*}\\stackrel{d e f}{=}\\operatorname*{min}_{\\pi\\in\\Pi^{D}}C_{M}^{\\pi}$ s.t. $V_{M}^{\\pi}\\geq V_{M}^{*}$ . Then, $C_{M}^{*}\\leq B\\iff V_{M}^{*}>-\\infty$ . Furthermore, if $V_{M}^{*}>-\\infty,$ , then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\arg\\operatorname*{min}_{\\pi\\in\\Pi^{D}}C_{M}^{\\pi}\\;\\subseteq\\;\\arg\\operatorname*{max}_{\\pi\\in\\Pi^{D}}V_{M}^{\\pi}.}\\\\ &{}&{V_{M}^{\\pi}\\ge V_{M}^{*}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;C_{M}^{\\pi}\\le B\\;\\;\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, any solution to the covering program is a solution to the packing program. ", "page_idx": 3}, {"type": "text", "text": "We focus on the covering program for several reasons. To optimize the value recursively, we would need to predict the final cost resulting from intermediate decisions to ensure feasibility. Generally, such predictions would require strict assumptions on the cost criteria. By treating the value as the constraint instead, we only need to assume the cost can be optimized efficiently. Moreover, values are well understood in RL and are more amenable to approximation [36]. Thus, the covering program allows us to capture many criteria, ensure feasibility, and compute accurate value approximations. ", "page_idx": 3}, {"type": "table", "img_path": "YRemB4naKK/tmp/1dafdc7198712af7ce6d92f55a971a72dbcfac244db069fc490e136443490af6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Value augmentation. We can solve the covering program by solving a cost-minimizing MDP $\\bar{M}$ . The key idea is to augment the state space with value demands, $(s,v)$ . Then, the agent can recursively reason how to minimize its cost while meeting the current value demand. If the agent starts at $(s_{0},V_{M}^{*})$ , then an optimal policy for $\\bar{M}$ should be a solution to the covering program. ", "page_idx": 4}, {"type": "text", "text": "The key invariant we desire is that any feasible policy $\\pi$ for $\\bar{M}$ should satisfy $\\bar{V}_{h}^{\\pi}(s,v)\\geq v$ . To ensure this invariance, we recall the policy evaluation equations [41]. If $\\pi_{h}(s)=a$ , then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{\\pi}(s,v)=r_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\bar{V}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the value invariant to be satisfied, it suffices for the agent to choose an action $a$ and commit to future value demands $v_{s^{\\prime}}$ satisfying, ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\geq v.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can view choosing future value demands as part of the agent\u2019s augmented actions. Then, at any augmented state $(s,v)$ , the agent\u2019s augmented action space includes all $(a,\\mathbf{v})\\in\\mathcal{A}\\times\\mathbb{R}^{S}$ satisfying (DEM). When $M$ transitions to $s^{\\prime}\\sim\\bar{P}_{h}(s,a)$ , the agent\u2019s new augmented state should consist of the environment\u2019s new state in addition to its chosen demand for that state, $(s^{\\prime},v_{s^{\\prime}})$ . Putting these pieces together yields the definition of the cover MDP, Definition 2. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Cover MDP). The cover MDP $\\bar{M}\\ {\\stackrel{\\mathrm{def}}{=}}\\ (\\bar{S},\\bar{A},\\bar{P},\\bar{c},H)$ where, ", "page_idx": 4}, {"type": "equation", "text": "$\\begin{array}{r}{\\bar{A}_{h}(s,v)\\stackrel{\\mathrm{def}}{=}\\{(a,\\mathbf{v})\\in\\boldsymbol{\\mathcal{A}}\\times\\mathcal{V}^{S}\\mid r_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\geq v\\}.}\\end{array}$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The objective for $\\bar{M}$ is to minimize the cost function $\\bar{C}\\ {\\stackrel{\\mathrm{def}}{=}}\\ C_{\\bar{M}}$ with modified base case $\\bar{C}_{H+1}^{\\pi}(s,v)\\ \\overset{\\mathrm{def}}{=}$ $\\chi_{\\{v\\leq0\\}}$ . ", "page_idx": 4}, {"type": "text", "text": "Covering algorithm. Importantly, the action space definition ensures the value constraint is satisfied. Meanwhile, the minimum cost objective ensures optimal cost. So long as our cost is TR, $\\bar{M}$ can be solved using fast RL methods instead of the brute force computation required for general covering programs. These properties ensure our method, Algorithm 1, is correct. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Reduction). If SOLVE is any finite-time MDP solver, then Algorithm 1 correctly solves (CON) in finite time for any TR cost criterion. ", "page_idx": 4}, {"type": "text", "text": "Remark 3 (Execution). Given a value-augmented policy $\\pi$ output from Algorithm 1, the agent can execute $\\pi$ using Algorithm 2. To compute $V_{M}^{*}$ as the starting value, it suffices for the agent to compute, ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{M}^{*}=\\operatorname*{max}\\left\\{v\\in\\mathcal{V}\\mid\\bar{C}_{1}^{*}(s_{0},v)\\leq B\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This computation can be easily done given $\\bar{C}_{1}^{*}(s_{0},\\cdot)$ in $O(|\\mathcal{V}|)$ time. ", "page_idx": 4}, {"type": "text", "text": "Input: $\\pi$   \n1: $\\bar{s}_{1}=(s_{0},V_{M}^{*})$   \n2: for $h\\leftarrow1$ to $H$ do   \n3: $(a,{\\bf v})\\leftarrow\\pi_{h}(\\bar{s}_{h})$   \n4: $r_{h}=r_{h}(s,a)$ and $s_{h+1}\\sim P_{h}(s_{h},a)$   \n5: $\\bar{s}_{h+1}=(s_{h+1},v_{s_{h+1}})$ ", "page_idx": 5}, {"type": "text", "text": "4 Fast Bellman updates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present an algorithm to solve $\\bar{M}$ from Definition 2 efficiently. Although the Bellman updates can be as hard to solve as the knapsack problem, we use ideas from knapsack approximation algorithms to create an efficient method. Our approach exploits (SR) through approximate dynamic programming on the action space. ", "page_idx": 5}, {"type": "text", "text": "Even if $\\nu$ were small, solving $\\bar{M}$ would still be challenging due to the exponentially large action space. Even a single Bellman update requires the solution of a constrained optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\bar{C}_{h}^{*}(s,v)=\\displaystyle\\operatorname*{min}_{a,\\mathbf{v}}c_{h}(s,a)+f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a),\\bar{C}_{h+1}^{*}\\left(s^{\\prime},v_{s^{\\prime}}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a)}\\right)}\\\\ {\\mathrm{~s.t.~}r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\geq v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Above, we used the fact that $(s^{\\prime},v^{\\prime})\\in\\bar{P}_{h}((s,v),(a,\\mathbf{v}))$ iff $s^{\\prime}\\in P_{h}(s,a)$ and $v^{\\prime}=v_{s^{\\prime}}$ to simplify $f$ \u2019s input. Observe that even when each $v_{s^{\\prime}}$ only takes on two possible values, $\\{0,w_{s^{\\prime}}\\}$ , the optimization above can capture the minimization version of the knapsack problem, implying that it is NP-hard to compute. ", "page_idx": 5}, {"type": "text", "text": "Recursive approach. Fortunately, we can use the connection to the Knapsack problem positively to efficiently approximate the Bellman update. For any fixed $(s,v)\\in\\bar{S}$ and $a\\in A$ , we focus on the inner constrained minimization over v: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}^{S},\\qquad\\qquad f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a),\\bar{C}_{h+1}^{*}\\left(s^{\\prime},v_{s^{\\prime}}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a)}\\right)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We use (SR) to transform this minimization over $\\mathbf{v}$ into a sequential decision-making problem that decides each vs\u2032. As above, we can use the definition of P\u00af to simplify g(hs ,v),(a,v)(t, v\u2032) into a function of $t$ alone: ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{h}^{(s,v),(a,\\mathbf{v})}(t)=\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),g_{h}^{(s,v),(a,\\mathbf{v})}(t+1)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $v$ only constrains the valid $(a,\\mathbf{v})$ pairs, we can discard $v$ and use the simplified notation $g_{h,\\mathbf{v}}^{s,a}(t)$ instead o f g(hs,v),(a,v)(t). It is then clear that we can recursively optimize the value of vt by focusing on $g_{h,\\mathbf{v}}^{s,a}(t)$ . ", "page_idx": 5}, {"type": "text", "text": "$\\textstyle\\sum_{s^{\\prime}=1}^{t-1}P_{h}(s^{\\prime}\\ |\\ s,a)v_{s^{\\prime}}$ tt thhwaeitt  hvw atelh ueh eia nvceeoq nuascatlcriatuiymn d .  csTaohn ef  arfreo.cr omTrahdle  trnhe, cewu rpes aicrotainna il s c vhdaeelcfukine e i $u~=~r_{h}(s,a)~+$ $\\ensuremath{\\mathbf{v}}$ $u\\geq v$ ", "page_idx": 5}, {"type": "text", "text": "Definition 3. For any $h\\in[H]$ , $s\\in S$ , $v\\in\\mathcal{V}$ , and $u\\in\\mathbb R$ , we define, $g_{h,v}^{s,a}(S+1,u)=\\chi_{\\{u\\geq v\\}}$ and for $t\\leq S$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{h,v}^{s,a}(t,u)=\\operatorname*{min}_{v_{t}\\in\\mathcal{V}}\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),g_{h,v}^{s,a}(t+1,u+P_{h}(t\\mid s,a)v_{t})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recursive rounding. This approach can still be slow due to the exponential number of partial values $u$ induced. Similarly to the knapsack problem, the key is to round each input $u$ to ensure fewer subproblems. Unlike the knapsack problem, however, we do not have an easily computable lower bound on the optimal value. Thus, we turn to a more aggressive recursive rounding. Since rounding may cause originally feasible values to violate the demand constraint, we also relax the demand constraint to $u\\geq\\kappa(v)$ for some lower bound function $\\kappa$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm 3 Approx Bellman Update ", "page_idx": 6}, {"type": "table", "img_path": "YRemB4naKK/tmp/425a86370cd3cbdf48a002b655271ffae3b1a3686dc357de38f2a7ddb3f8ba27.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Algorithm 4 Approx Solve ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: $(\\bar{M},\\bar{C})$   \n1: $\\hat{C}_{H+1}^{*}(s,v)\\gets\\chi_{\\{v\\leq0\\}}$ for all $(s,v)\\in\\bar{S}$   \n2: for $h\\leftarrow H$ down to 1 do   \n3: for $(s,v)\\in\\bar{S}$ do   \n4: $\\hat{a},\\hat{C}_{h}^{*}(s,v)\\gets\\mathrm{Algorithm}\\ 3(h,s,v,\\hat{C}_{h+1}^{*})$   \n5: $\\pi_{h}(s,v)\\leftarrow\\hat{a}$   \n6: return \u03c0 and C\u02c6\u2217 ", "page_idx": 6}, {"type": "text", "text": "Definition 4. Fix a rounding function $\\lfloor\\cdot\\rfloor_{\\mathcal{G}}$ and a lower bound function $\\kappa$ . For any $h\\in[H],s\\in S$ , $v\\in\\mathcal{V}$ , and $u\\in\\mathbb{R}$ , we define, $\\hat{g}_{h,v}^{s,a}(S+1,\\bar{u})=\\chi_{\\{u\\geq v\\}}$ and for $t\\leq S$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{g}_{h,v}^{s,a}(t,u)\\overset{\\mathrm{def}}{=}\\operatorname*{min}_{v_{t}\\in\\mathcal{V}}\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),\\hat{g}_{h,v}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)v_{t}\\right)\\rfloor_{\\mathcal{G}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Fortunately, the approximate version behaves similarly to the original. The main difference is the constraint now ensures the rounded sums are at least the value lower bound. This is formalized in Lemma 1. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. For any $t\\in[S+1]$ and $u\\in\\mathbb{R},$ , we have that, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{g}_{h,v}^{s,a}(t,u)=\\underset{\\mathbf{v}\\in\\mathcal{V}^{S-t+1}}{\\operatorname*{min}}\\,g_{h,\\hat{\\mathbf{v}}}^{s,a}(t)\\,}\\\\ {s.t.\\quad\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(t,u)\\geq\\kappa(v),}\\\\ {\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(t,u)\\overset{d e f}{=}\\big\\lfloor[u+P_{h}(t\\mid s,a)v_{t}]_{\\mathcal{G}}+\\ldots+P_{h}(S\\mid s,a)v_{S}\\big\\rfloor_{\\mathcal{G}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To turn this recursion into a usable dynamic programming algorithm, we must also pre-compute the inputs to any sub-computation. Unlike in standard RL, this computation must be done with a forward recursion. The details for the approximate Bellman update are given in Definition 5. ", "page_idx": 6}, {"type": "text", "text": "Definition 5 (Approx Bellman). For any $h\\in[H]$ , $s\\in S$ , and $a\\in{\\mathcal{A}}$ , we define $\\hat{\\mathcal{U}}_{h}^{s,a}(1)\\stackrel{\\mathrm{def}}{=}\\{r_{h}(s,a)\\}$ and for any $t\\in[S]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{U}}_{h}^{s,a}(t+1)\\stackrel{\\mathrm{def}}{=}\\bigcup_{v_{t}\\in\\mathcal{V}}\\bigcup_{u\\in\\hat{\\mathcal{U}}_{h}^{s,a}(t)}\\left\\{\\lfloor u+P_{h}(t\\mid s,a)v_{t}\\rfloor_{\\mathcal{G}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, an approximation to the Bellman update can be computed using Algorithm 3.2 ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. Algorithm 4 runs in $O(H S^{2}A|\\mathcal{V}|^{2}\\hat{U})$ time, where $\\hat{U}\\overset{d e f}{=}\\operatorname*{max}_{h,s,a}|\\hat{U}_{h}^{s,a}|$ . When $\\lfloor\\cdot\\rfloor_{\\mathcal{G}}$ and $\\kappa$ are the identity function, Algorithm 4 outputs an optimal solution to $\\bar{M}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4 (Speedups). The runtime of our methods can be quadratically improved by rounding the differences instead of the sums. We defer the details to Appendix F. ", "page_idx": 6}, {"type": "text", "text": "5 Approximation algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present our approximation algorithms for solving (CON). We carefully round the value demands over both time and space to induce an approximate MDP. Solving this approximate MDP with Algorithm 4 yields our FPTAS. ", "page_idx": 7}, {"type": "text", "text": "Although we can avoid exponential-time Bellman updates, the running time of the approximate Bellman update will still be slow if $\\vert\\nu\\vert$ is large. To reduce the complexity, we instead use a smaller set of approximate values by rounding elements of $\\vert\\nu\\vert$ . By rounding down, we effectively relax the value-demand constraint. More aggressive rounding not only leads to smaller augmented state spaces but also to smaller cost policies. The trade-off is aggressive rounding leads to weaker guarantees on the computed policy\u2019s value. Thus, it is critical to carefully design the rounding and lower bound functions to balance this trade-off. ", "page_idx": 7}, {"type": "text", "text": "Value approximation. Given a rounding down function $\\lfloor\\cdot\\rfloor_{\\mathcal{G}}$ , we would ideally use the rounded set $\\{\\lfloor v\\rfloor_{\\mathcal{G}}\\mid v\\in\\mathcal{V}\\}$ to form our approximate state space. To avoid having to compute $\\mathcal{V}$ explicitly, we instead use the rounded superset $\\left\\{\\lfloor v\\rfloor_{\\mathcal{G}}\\mid v\\in[v_{m i n},v_{m a x}]\\right\\}$ , where $v_{m i n}$ and $v_{m a x}$ are bounds on the extremal values that we specify later. To ensure we can use Algorithm 4 to find solutions efficiently, we must also relax the augmented action space to only include vectors that lead to feasible subproblems for (ADP). From Lemma 1, we know this is exactly the set of $(a,{\\hat{\\mathbf{v}}})$ for which $\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(\\bar{1},r_{h}(s,a))\\geq\\kappa(v)$ . Combining these ideas yields the new approximate MDP, defined in Definition 6. ", "page_idx": 7}, {"type": "text", "text": "Definition 6 (Approximate MDP). Given a rounding function $\\lfloor\\cdot\\rfloor_{\\mathcal{G}}$ and lower bound function $\\kappa$ , the approximate MDP $\\hat{M}\\overset{\\mathrm{def}}{=}(\\hat{S},\\hat{\\mathcal{A}},\\hat{P},\\hat{c},H)$ where, ", "page_idx": 7}, {"type": "text", "text": "The objective for $\\hat{M}$ is to minimize the cost function ${\\hat{C}}\\ {\\stackrel{\\mathrm{def}}{=}}\\ C_{\\hat{M}}$ with modified base case $\\hat{C}_{H+1}^{\\pi}(s,\\hat{v})\\ {\\stackrel{\\mathrm{def}}{=}}$ $\\chi\\{\\hat{v}{\\leq}0\\}$ . ", "page_idx": 7}, {"type": "text", "text": "We can show that rounding down in Definition 6 achieves our goal of producing smaller cost policies.   \nThis ensures feasibility is even easier to achieve. We formalize this observation in Lemma 2. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2 (Optimistic Costs). For our later choices of $\\lfloor\\cdot\\rfloor_{\\mathcal{G}}$ and $\\kappa$ , the following holds: for any $h\\in[H+1]$ and $(s,v)\\in\\bar{S}$ , we have $\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})\\leq\\bar{C}_{h}^{*}(s,v)$ . ", "page_idx": 7}, {"type": "text", "text": "Thus, Algorithm 5 always outputs a policy with better than optimal cost when the instance is feasible, $V_{M}^{*}>-\\infty$ . If the instance is infeasible, all policies have cost larger than $B$ by definition and so Algorithm 5 correctly indicates the instance is infeasible. The remaining question is whether Algorithm 5 outputs policies having near-optimal value. ", "page_idx": 7}, {"type": "text", "text": "Time-Space errors. To assess the optimality gap of Algorithm 5 policies, we must first explore the error accumulated by our rounding approach. Rounding each value naturally accumulates approximation error over time. Rounding the partial values while running Algorithm 3 accumulates additional error over (state) space. Thus, solving $\\hat{M}$ using Algorithm 4 accumulates error over both time and space, unlike other approximate methods in RL. As a result, our rounding and threshold functions will generally depend on both $H$ and $S$ . ", "page_idx": 7}, {"type": "text", "text": "Arithmetic rounding. Our first approach is to round each value down to its closest element in a $\\delta$ -cover. This guarantees that $v-\\delta\\leq\\lfloor v\\rfloor_{\\mathcal{G}}\\leq v$ . Thus, $\\lfloor v\\rfloor_{\\mathcal{G}}$ is an underestimate that is not too far from the true value. By setting $\\delta$ to be inversely proportional to $S H$ , we control the errors over time and space. The lower bound must also be a function of $S$ since it controls the error over space. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 5 Approximation Scheme ", "page_idx": 8}, {"type": "table", "img_path": "YRemB4naKK/tmp/f02d26422456010fd13c124988fe1829173dc6307c03ac9675fc6a867a83e125.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Definition 7 (Additive Approx). Fix $\\epsilon>0$ . We define, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lfloor v\\rfloor_{\\mathcal{G}}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\lfloor{\\frac{v}{\\delta}}\\right\\rfloor\\delta\\ \\mathrm{and}\\ \\kappa(v)\\ {\\stackrel{\\mathrm{def}}{=}}\\ v-\\delta(S+1),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where \u03b4d=ef $\\delta\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{\\epsilon}{H(S\\!+\\!1)\\!+\\!1}}$ H(S+1)+1, vmin $v_{m i n}\\ {\\stackrel{\\mathrm{def}}{=}}\\ -H r_{m a x}$ f \u2212Hrmax, and vmax $v_{m a x}\\ {\\stackrel{\\mathrm{def}}{=}}\\ H r_{m a x}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 (Additive FPTAS). For any $\\epsilon>0$ , Algorithm $5$ using Definition 7 given any cMDP $M$ and TSR criteria $C$ either correctly outputs the instance is infeasible, or produces a policy $\\pi$ satisfying $\\hat{V}^{\\pi}\\ge V_{M}^{*}-\\epsilon$ in $O(H^{7}S^{5}A r_{m a x}^{3}/\\epsilon^{3})$ time. Thus, it is an additive-FPTAS for the class of cMDPs with polynomial-bounded $r_{m a x}$ and TSR criteria. ", "page_idx": 8}, {"type": "text", "text": "Geometric rounding. Since the arithmetic approach can be slow when $r_{m a x}$ is large, we can instead round values down to their closest power of $1/(1-\\delta)$ . This guarantees the number of approximate values needed is upper bounded by a function of $\\log(r_{m a x})$ , which is polynomial in the input size. We choose a geometric scheme satisfying $v(1-\\delta)\\leq\\lfloor v\\rfloor_{\\mathcal{G}}\\leq v$ so that the rounded value is an underestimate and a relative approximation to the true value. To ensure this property, we must now require that all rewards are non-negative. ", "page_idx": 8}, {"type": "text", "text": "Definition 8 (Relative Approx). Fix $\\epsilon>0$ . We define, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lfloor v\\rfloor_{\\mathcal{G}}\\stackrel{\\scriptscriptstyle{\\mathrm{def}}}{=}v^{m i n}\\left(\\frac{1}{1-\\delta}\\right)^{\\left\\lfloor\\log_{\\frac{1}{1-\\delta}}\\frac{v}{v^{m i n}}\\right\\rfloor}\\mathrm{~and~}\\kappa(v)\\stackrel{\\scriptscriptstyle{\\mathrm{def}}}{=}v(1-\\delta)^{S+1},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where \u03b4d=ef $\\delta\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{\\epsilon}{H(S\\!+\\!1)\\!+\\!1}}$ H(S+1)+1, vmin = $v_{m i n}=p_{m i n}^{H}r_{m i n}$ , and $v_{m a x}=H r_{m a x}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 (Relative FPTAS). For $\\epsilon>0$ , Algorithm $^{5}$ using Definition 8 given any cMDP M and TSR criteria $C$ either correctly outputs the instance is infeasible, or produces a policy $\\pi$ satisfying $\\hat{V}^{\\pi}\\ge V_{M}^{*}(1-\\epsilon)$ in $O(H^{7}S^{5}A\\log{(r_{m a x}/r_{m i n}p_{m i n})^{3}}/\\epsilon^{3})$ time. Thus, it is a relative-FPTAS for the class of cMDPs with non-negative rewards and TSR criteria. ", "page_idx": 8}, {"type": "text", "text": "Remark 5 (Assumption Necessity). We also note the mild reward assumptions we made to guarantee efficiency are unavoidable. Without reward bounds, (CON) captures the knapsack problem which does not admit additive approximations. Similarly, without non-negativity, relative approximations for maximization problems are generally not computable. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we studied the computational complexity of computing deterministic policies for CRL problems. Our main contribution was the design of an FPTAS, Algorithm 5, that solves (CON) for any cMPD and TSR criteria under mild reward assumptions. In particular, our method is an additiveFPTAS if the cMDP\u2019s rewards are polynomially bounded, and is a relative-FPTAS if the cMDP\u2019s rewards are non-negative. We note these assumptions are necessary for efficient approximation, so our algorithm achieves the best approximation guarantees possible under worst-case analysis. Moreover, our algorithmic approach, which uses approximate dynamic programming over time and the state space, highlights the importance of the TSR condition in making (CON) tractable. Our work finally resolves the long-standing open questions of polynomial-time approximability for 1) anytimeconstrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies. ", "page_idx": 8}, {"type": "text", "text": "Future work. Several interesting questions remain unanswered. First, it remains unresolved whether an FPTAS exists asymptotically faster than ours. Second, whether our TSR condition is necessary for efficient computation or whether a more general condition could be derived is unclear. Lastly, it is open whether there exist algorithms that can feasibly handle multiple constraints from Proposition 2. Although computing feasible policies for multiple constraints is NP-hard, special cases may be approximable efficiently. Moreover, an average-case or smoothed-case analysis could circumvent this worst-case hardness. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by NSF grant 2023239. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] T. Alon and N. Halman. Automatic generation of fptases for stochastic monotone dynamic programs made easier. SIAM Journal on Discrete Mathematics, 35(4):2679\u20132722, 2021. doi: 10.1137/19M1308633. URL https://doi.org/10.1137/19M1308633.   \n[2] M. Alshiekh, R. Bloem, R. Ehlers, B. K\u00f6nighofer, S. Niekum, and U. Topcu. Safe reinforcement learning via shielding. Proceedings of the AAAI Conference on Artificial Intelligence, 32 (1), Apr. 2018. doi: 10.1609/aaai.v32i1.11797. URL https://ojs.aaai.org/index.php/ AAAI/article/view/11797.   \n[3] E. Altman. Constrained Markov Decision Processes. Chapman and Hall/CRC, 1999. doi: 10.1201/9781315140223.   \n[4] Q. Bai, A. Singh Bedi, and V. Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm. Proceedings of the AAAI Conference on Artificial Intelligence, 37(6):6737\u20136744, 6 2023. doi: 10.1609/aaai. v37i6.25826. URL https://ojs.aaai.org/index.php/AAAI/article/view/25826.   \n[5] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learning with stability guarantees. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf.   \n[6] A. Bhalgat, A. Goel, and S. Khanna. Improved approximation results for stochastic knapsack problems. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201911, page 1647\u20131665, USA, 2011. Society for Industrial and Applied Mathematics.   \n[7] A. Bhatia, P. Varakantham, and A. Kumar. Resource constrained deep reinforcement learning. Proceedings of the International Conference on Automated Planning and Scheduling, 29(1): 610\u2013620, 5 2021. doi: 10.1609/icaps.v29i1.3528. URL https://ojs.aaai.org/index. php/ICAPS/article/view/3528.   \n[8] V. Borkar. An actor-critic algorithm for constrained markov decision processes. Systems & Control Letters, 54(3):207\u2013213, 2005. ISSN 0167-6911. doi: https://doi.org/10.1016/j. sysconle.2004.08.007. URL https://www.sciencedirect.com/science/article/pii/ S0167691104001276.   \n[9] D. M. Bossens and N. Bishop. Explicit explore, exploit, or escape (e4): Near-optimal safetyconstrained reinforcement learning in polynomial time. Mach. Learn., 112(3):817\u2013858, 6 2022. ISSN 0885-6125. doi: 10.1007/s10994-022-06201-z. URL https://doi.org/10.1007/ s10994-022-06201-z.   \n[10] K. Brantley, M. Dud\u00edk, T. Lykouris, S. Miryoosef,i M. Simchowitz, A. Slivkins, and W. Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. In NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ bc6d753857fe3dd4275dff707dedf329-Abstract.html.   \n[11] A. Castellano, H. Min, E. Mallada, and J. A. Bazerque. Reinforcement learning with almost sure constraints. In R. Firoozi, N. Mehr, E. Yel, R. Antonova, J. Bohg, M. Schwager, and M. Kochenderfer, editors, Proceedings of The 4th Annual Learning for Dynamics and Control Conference, volume 168 of Proceedings of Machine Learning Research, pages 559\u2013570. PMLR, 6 2022. URL https://proceedings.mlr.press/v168/castellano22a.html.   \n[12] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3387\u20133395, Jul. 2019. doi: 10.1609/aaai.v33i01. 33013387. URL https://ojs.aaai.org/index.php/AAAI/article/view/4213.   \n[13] W. C. Cheung. Regret minimization for reinforcement learning with vectorial feedback and complex objectives. In Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/ a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf.   \n[14] Y. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/ paper/2018/file/4fe5149039b52765bde64beb9f674940-Paper.pdf.   \n[15] A. Coronato, M. Naeem, G. De Pietro, and G. Paragliola. Reinforcement learning for intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109:101964, 2020. ISSN 0933-3657. doi: https://doi.org/10.1016/j.artmed.2020.101964. URL https://www. sciencedirect.com/science/article/pii/S093336572031229X.   \n[16] B. C. Dean, M. X. Goemans, and J. Vondr\u00e1k. Adaptivity and approximation for stochastic packing problems. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201905, page 395\u2013404, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857.   \n[17] D. A. Dolgov and E. H. Durfee. Stationary deterministic policies for constrained mdps with multiple rewards, costs, and discount factors. In IJCAI, volume 19, pages 1326\u20131331, 2005.   \n[18] C. Fan, C. Zhang, A. Yahja, and A. Mostafavi. Disaster city digital twin: A vision for integrating artificial and human intelligence for disaster management. International Journal of Information Management, 56:102049, 2021. ISSN 0268-4012. doi: https://doi.org/10.1016/ j.ijinfomgt.2019.102049. URL https://www.sciencedirect.com/science/article/ pii/S0268401219302956.   \n[19] E. A. Feinberg. Constrained discounted markov decision processes and hamiltonian cycles. Mathematics of Operations Research, 25(1):130\u2013140, 2000. doi: 10.1287/moor.25.1.130.15210. URL https://doi.org/10.1287/moor.25.1.130.15210.   \n[20] J. F. Fisac, N. F. Lugovoy, V. Rubies-Royo, S. Ghosh, and C. J. Tomlin. Bridging hamiltonjacobi safety analysis and reinforcement learning. In 2019 International Conference on Robotics and Automation (ICRA), page 8550\u20138556. IEEE Press, 2019. doi: 10.1109/ICRA.2019.8794107. URL https://doi.org/10.1109/ICRA.2019.8794107.   \n[21] A. Frieze and M. Clarke. Approximation algorithms for the m-dimensional 0\u20131 knapsack problem: Worst-case and probabilistic analyses. European Journal of Operational Research, 15(1):100\u2013109, 1984. ISSN 0377-2217. doi: https://doi.org/10.1016/0377-2217(84)90053-5. URL https://www.sciencedirect.com/science/article/pii/0377221784900535.   \n[22] J. Garc\u00eda, Fern, and o Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(42):1437\u20131480, 2015. URL http://jmlr.org/ papers/v16/garcia15a.html.   \n[23] F. Gei\u00dfer, G. Pov\u00e9da, F. Trevizan, M. Bondouy, F. Teichteil-K\u00f6nigsbuch, and S. Thi\u00e9baux. Optimal and heuristic approaches for constrained flight planning under weather uncertainty. Proceedings of the International Conference on Automated Planning and Scheduling, 30(1): 384\u2013393, Jun. 2020. doi: 10.1609/icaps.v30i1.6684. URL https://ojs.aaai.org/index. php/ICAPS/article/view/6684.   \n[24] S. Gros, M. Zanon, and A. Bemporad. Safe reinforcement learning via projection on a safe set: How to achieve optimality? IFAC-PapersOnLine, 53(2):8076\u20138081, 2020. ISSN 2405-8963. doi: https://doi.org/10.1016/j.ifacol.2020.12.2276. URL https://www.sciencedirect.com/ science/article/pii/S2405896320329360. 21st IFAC World Congress.   \n[25] S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, and A. Knoll. A review of safe reinforcement learning: Methods, theory and applications, 2024. URL https://arxiv.org/abs/2205. 10330.   \n[26] N. Halman and G. Nannicini. Toward breaking the curse of dimensionality: An fptas for stochastic dynamic programs with multidimensional actions and scalar states. SIAM Journal on Optimization, 29(2):1131\u20131163, 2019. doi: 10.1137/18M1208423. URL https://doi.org/ 10.1137/18M1208423.   \n[27] N. Halman, D. Klabjan, C.-L. Li, J. Orlin, and D. Simchi-Levi. Fully polynomial time approximation schemes for stochastic dynamic programs. SIAM Journal on Discrete Mathematics, 28(4):1725\u20131796, 2014. doi: 10.1137/130925153. URL https://doi.org/10.1137/ 130925153.   \n[28] A. HasanzadeZonuzy, A. Bura, D. Kalathil, and S. Shakkottai. Learning with safety constraints: Sample complexity of reinforcement learning for constrained mdps. Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):7667\u20137674, 5 2021. doi: 10.1609/aaai.v35i9.16937. URL https://ojs.aaai.org/index.php/AAAI/article/view/16937.   \n[29] S. Hong and B. C. Williams. An anytime algorithm for constrained stochastic shortest path problems with deterministic policies. Artificial Intelligence, 316:103846, 2023. ISSN 0004- 3702. doi: https://doi.org/10.1016/j.artint.2022.103846. URL https://www.sciencedirect. com/science/article/pii/S0004370222001862.   \n[30] S. Hong, S. U. Lee, X. Huang, M. Khonji, R. Alyassi, and B. C. Williams. An anytime algorithm for chance constrained stochastic shortest path problems and its application to aircraft routing. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 475\u2013481, 2021. doi: 10.1109/ICRA48506.2021.9561229.   \n[31] M. Khonji, A. Jasour, and B. Williams. Approximability of constant-horizon constrained pomdp. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5583\u20135590. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/775. URL https://doi.org/10.24963/ijcai. 2019/775.   \n[32] P. Kolesar. A markovian model for hospital admission scheduling. Management Science, 16 (6):B384\u2013B396, 1970. ISSN 00251909, 15265501. URL http://www.jstor.org/stable/ 2628725.   \n[33] J. Li, D. Fridovich-Keil, S. Sojoudi, and C. J. Tomlin. Augmented lagrangian method for instantaneously constrained reinforcement learning problems. In 2021 60th IEEE Conference on Decision and Control (CDC), page 2982\u20132989. IEEE Press, 2021. doi: 10.1109/CDC45484. 2021.9683088. URL https://doi.org/10.1109/CDC45484.2021.9683088.   \n[34] R. Li, Z. Zhao, Q. Sun, C.-L. I, C. Yang, X. Chen, M. Zhao, and H. Zhang. Deep reinforcement learning for resource management in network slicing. IEEE Access, 6:74429\u201374441, 2018. doi: 10.1109/ACCESS.2018.2881964.   \n[35] H. Mao, M. Alizadeh, I. Menache, and S. Kandula. Resource management with deep reinforcement learning. In Proceedings of the 15th ACM Workshop on Hot Topics in Networks, HotNets \u201916, page 50\u201356, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450346610. doi: 10.1145/3005745.3005750. URL https: //doi.org/10.1145/3005745.3005750.   \n[36] J. McMahan and X. Zhu. Anytime-constrained reinforcement learning. In S. Dasgupta, S. Mandt, and Y. Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 4321\u20134329. PMLR, 02\u201304 May 2024. URL https://proceedings.mlr.press/v238/mcmahan24a. html.   \n[37] G. Paragliola, A. Coronato, M. Naeem, and G. De Pietro. A reinforcement learning-based approach for the risk management of e-health environments: A case study. In 2018 14th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS), pages 711\u2013716, 2018. doi: 10.1109/SITIS.2018.00114.   \n[38] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Towards a formalization of teamwork with resource constraints. In Third International Joint Conference on Autonomous Agents and Multiagent Systems, United States, 2004. IEEE Computer Society. Place of conference:USA.   \n[39] S. Paternain, L. Chamon, M. Calvo-Fullana, and A. Ribeiro. Constrained reinforcement learning has zero duality gap. In Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/c1aeb6517a1c7f33514f7ff69047e74e-Paper.pdf.   \n[40] H. Peng and X. Shen. Multi-agent reinforcement learning based resource management in mecand uav-assisted vehicular networks. IEEE Journal on Selected Areas in Communications, 39 (1):131\u2013141, 2021. doi: 10.1109/JSAC.2020.3036962.   \n[41] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779.   \n[42] M. Roderick, V. Nagarajan, and Z. Kolter. Provably safe pac-mdp exploration using analogies. In A. Banerjee and K. Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1216\u20131224. PMLR, 4 2021. URL https://proceedings.mlr.press/ v130/roderick21a.html.   \n[43] M. A. Taleghan and T. G. Dietterich. Efficient exploration for constrained mdps. In 2018 AAAI Spring Symposium Series, 2018.   \n[44] G. Thomas, Y. Luo, and T. Ma. Safe reinforcement learning by imagining the near future. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 13859\u201313869. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 73b277c11266681122132d024f53a75b-Paper.pdf.   \n[45] Y. L. Tsai, A. Phatak, P. K. Kitanidis, and C. B. Field. Deep Reinforcement Learning for Disaster Response: Navigating the Dynamic Emergency Vehicle and Rescue Team Dispatch during a Flood. In AGU Fall Meeting Abstracts, volume 2019, pages NH33B\u201314, Dec. 2019.   \n[46] S. Vaswani, L. Yang, and C. Szepesvari. Near-optimal sample complexity bounds for constrained mdps. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3110\u20133122. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/14a5ebc9cd2e507cd811df78c15bf5d7-Paper-Conference.pdf.   \n[47] Y. Wang, S. S. Zhan, R. Jiao, Z. Wang, W. Jin, Z. Yang, Z. Wang, C. Huang, and Q. Zhu. Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 36593\u201336604. PMLR, 7 2023. URL https://proceedings.mlr.press/v202/wang23as.html.   \n[48] H. Wei, X. Liu, and L. Ying. Triple-q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation. In G. Camps-Valls, F. J. R. Ruiz, and I. Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 3274\u20133307. PMLR, 3 2022. URL https://proceedings.mlr.press/v151/wei22a.html.   \n[49] D. P. Williamson and D. B. Shmoys. The Design of Approximation Algorithms. Cambridge University Press, USA, 1st edition, 2011. ISBN 0521195276. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[50] C. Wu, B. Ju, Y. Wu, X. Lin, N. Xiong, G. Xu, H. Li, and X. Liang. Uav autonomous target search based on deep reinforcement learning in complex disaster scene. IEEE Access, 7: 117227\u2013117245, 2019. doi: 10.1109/ACCESS.2019.2933002. ", "page_idx": 13}, {"type": "text", "text": "[51] H. Xu and S. Mannor. Probabilistic goal markov decision processes. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three, IJCAI\u201911, page 2046\u20132052. AAAI Press, 2011. ISBN 9781577355151.   \n[52] S. Yang, S. Khuller, S. Choudhary, S. Mitra, and K. Mahadik. Correlated Stochastic Knapsack with a Submodular Objective. In S. Chechik, G. Navarro, E. Rotenberg, and G. Herman, editors, 30th Annual European Symposium on Algorithms (ESA 2022), volume 244 of Leibniz International Proceedings in Informatics (LIPIcs), pages 91:1\u201391:14, Dagstuhl, Germany, 2022. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik. ISBN 978-3-95977-247-1. doi: 10. 4230/LIPIcs.ESA.2022.91. URL https://drops.dagstuhl.de/opus/volltexte/2022/ 17029.   \n[53] W. Zhao, T. He, R. Chen, T. Wei, and C. Liu. State-wise safe reinforcement learning: a survey. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI \u201923, 2023. ISBN 978-1-956792-03-4. doi: 10.24963/ijcai.2023/763. URL https://doi.org/10.24963/ijcai.2023/763. ", "page_idx": 13}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B Proofs for Section 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proof follows from the standard proof of backward induction [41]. The main ideas for the proof can also be seen in the proof of Lemma 4 and Lemma 5. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 14}, {"type": "text", "text": ". (Expectation Constraints) We claim that $C_{M}^{\\pi}$ captures expectation constraints. This is immediate as an expectation constraint takes the form $\\begin{array}{r}{\\mathbb{E}_{M}^{\\pi}\\left[\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})\\right]\\leq B}\\end{array}$ and by definition $\\begin{array}{r}{C_{M}^{\\pi}\\,=\\,\\mathbb{E}_{M}^{\\pi}\\,\\left[\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})\\right]\\,}\\end{array}$ . Moreover, the standard policy evaluation equations for deterministic policies immediately imply, ", "page_idx": 14}, {"type": "equation", "text": "$$\nC_{h}^{\\pi}(\\tau_{h})=c_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, (TR) holds. It is also easy to see that $\\begin{array}{r}{\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})}\\end{array}$ can be computed recursively state-wise by, ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{h}(1\\mid s,a)C_{h+1}^{\\pi}(\\tau_{h},a,1)+\\sum_{s^{\\prime}=2}^{S}P_{h}(s^{\\prime}\\mid s,a)C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and so (SR) holds. The infinity conditions and non-decreasing requirements are also easy to verify. ", "page_idx": 14}, {"type": "text", "text": "2. (Almost Sure Constraints) We claim that $C_{M}^{\\pi}$ captures almost sure constraints. This is because that for tabular MDPs, $\\begin{array}{r}{\\mathbb{P}_{M}^{\\pi}[\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})\\:\\leq\\:B]\\:=\\:1}\\end{array}$ if and only if for all $\\tau\\,\\in\\,\\mathcal{H}_{H+1}$ with $\\mathbb{P}_{M}^{\\pi}[\\tau]\\,>\\,0$ it holds that $\\begin{array}{r}{\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})\\;\\le\\;B}\\end{array}$ if and only if $C_{M}^{\\pi}\\,=$ $\\begin{array}{r}{\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:\\atop{\\mathbb{P}_{\\times\\tau}^{\\pi}}|\\tau|>0}\\sum_{h=1}^{H}c_{h}\\big(s_{h},a_{h}\\big)\\leq B.}\\end{array}$ . $\\mathbb{P}_{M}^{\\pi}[\\tau]\\!>\\!0$ ", "page_idx": 14}, {"type": "text", "text": "Let $\\begin{array}{r}{c(\\tau)=\\sum_{h=1}^{H}c_{h}(s_{h},a_{h})}\\end{array}$ denote the cost of a full history $\\tau\\in\\mathcal{H}_{H+1}$ and let $c_{h:t}(\\tau)=$ $\\textstyle\\sum_{k=h}^{t}c_{k}{\\left(s_{k},a_{k}\\right)}$ denote the partial cost of $\\tau$ from time $h$ to time $t$ . Our choice of $\\alpha$ and $\\beta$ imply that, ", "page_idx": 14}, {"type": "equation", "text": "$$\nC_{h}^{\\pi}(\\tau_{h})=c_{h}(s,a)+\\operatorname*{max}_{s^{\\prime}\\in P_{h}(s,a)}C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To show that $C_{M}^{\\pi}$ satisfies (TR), we prove for all $h\\in[H+1]$ and all $\\tau_{h}\\in\\mathcal{H}_{h}$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\nC_{h}(\\tau_{h})=\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:\\atop\\mathbb{P}_{M}^{\\pi}[\\tau|\\tau_{h}]>0}c_{h:H}(\\tau).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we see that C1\u03c0 (s0) = max \u03c4\u2208HH+1: $\\begin{array}{r}{c_{1:H}(\\tau)=\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:\\;\\sum_{h=1}^{H}c_{h}\\left(s_{h},a_{h}\\right)}=}\\end{array}$ $\\mathbb{P}_{M}^{\\pi}[\\tau|s_{0}]\\!>\\!0$ P\u03c0M[\u03c4]>0 $C_{M}^{\\pi}$ . Thus, $C_{M}^{\\pi}$ satisfies (TR). Furthermore, it is clear that $\\mathrm{max}_{s^{\\prime}\\in P_{h}(s,a)}\\,C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})$ can be computed state-recursively by, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\operatorname*{max}(C_{h+1}^{\\pi}(\\tau_{h},a,1)[P_{h}(1\\mid s,a)>0],\\underset{s^{\\prime}=2}{\\!\\!\\operatorname*{max}}\\,C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})[P_{h}(s^{\\prime}\\mid s,a)>0]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and so $C_{M}^{\\pi}$ satisfies (SR). The infinity conditions and non-decreasing requirements are also easy to verify. ", "page_idx": 14}, {"type": "text", "text": "We proceed by induction on $h$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 (Base Case) For the base case, we consider $h=H+1$ . Observe that for any history $\\tau$ , we have $c_{H+1:H}(\\tau)=0$ since it is an empty sum. Then, by definition of $C_{M}^{\\pi}$ , we see that $C_{H+1}^{\\pi}(\\tau_{H+1})=0=\\operatorname*{max}_{\\tau}0=\\operatorname*{max}_{\\tau}c_{H+1:H}(\\tau)$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 (Inductive Step) For the inductive step, we consider $h\\,\\leq\\,H$ . Let $s\\,=\\,s_{h}(\\tau_{h})$ and $a=\\pi_{h}(\\tau_{h})$ . For any $\\tau\\in\\mathcal{H}_{H+1}$ for which $\\mathbb{P}_{M}^{\\pi}[\\tau\\mid\\tau_{h}]>0$ , we can decompose its cost by $c_{h:H}(\\tau)=c_{h}(s,a)+c_{h+1:H}(\\tau)$ . Since $a$ is fixed, we can remove $c_{h}(s,a)$ from the optimization to get, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\tau}\\in\\mathcal{H}_{H+1:}\\atop\\mathbb{P}_{M}^{\\pi}[\\boldsymbol{\\tau}|\\tau_{h}]>0}c_{h:H}(\\boldsymbol{\\tau})=c_{h}(s,a)+\\operatorname*{max}_{\\boldsymbol{\\tau}\\in\\mathcal{H}_{H+1:}\\atop\\mathbb{P}_{M}^{\\pi}[\\boldsymbol{\\tau}|\\tau_{h}]>0}c_{h+1:H}(\\boldsymbol{\\tau}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we observe by the Markov property that $\\begin{array}{r l r}{\\mathbb{P}_{M}^{\\pi}[\\tau}&{{}|\\quad\\tau_{h}]}&{=}&{\\sum_{s^{\\prime}}\\mathbb{P}_{M}^{\\pi}[\\tau}\\end{array}|}\\end{array}$ $\\tau_{h},a,s^{\\prime}]P_{h}(s^{\\prime}\\mid s,a)$ . Thus, $\\mathbb{P}_{M}^{\\pi}[\\tau~~|~~\\tau_{h}]~>~0$ if and only if ther e exists some $s^{\\prime}\\in P_{h}(s,a)$ satisfying $\\mathbb{P}_{M}^{\\pi}[\\tau\\mid\\tau_{h}^{\\\"},a,s^{\\prime}]>0$ . This implies that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:\\atop\\mathbb{R}_{M}^{\\pi}[\\tau|\\tau_{h}]>0}c_{h+1:H}(\\tau)=\\operatorname*{max}_{s^{\\prime}\\in P_{h}(s,a)\\atop\\mathbb{R}_{M}^{\\pi}[\\tau|\\tau_{h},a,s^{\\prime}]>0}\\operatorname*{max}_{c_{h+1:H}(\\tau).}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By applying the induction hypothesis, we see that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tau\\in\\mathcal{H}_{H+1}:0}{\\operatorname*{max}}\\quad c_{h:H}(\\tau)=c_{h}(s,a)+\\underset{s^{\\prime}\\in P_{h}(s,a)}{\\operatorname*{max}}\\underset{\\tau\\in\\mathcal{H}_{H+1}:0}{\\operatorname*{max}}c_{h+1:H}(\\tau)}\\\\ &{\\mathbb{P}_{M}^{\\pi}[\\tau|\\tau_{h}]\\!>\\!0}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=c_{h}(s,a)+\\underset{s^{\\prime}\\in P_{h}(s,a)}{\\operatorname*{max}}C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=C_{h}(\\tau_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second line used the induction hypothesis and the third line used the definition of $C_{M}^{\\pi}$ . ", "page_idx": 15}, {"type": "text", "text": "3. (Anytime Constraints) We claim that $C_{M}^{\\pi}$ captures anytime constraints. This is because that for tabular MDPs, $\\mathbb{P}_{M}^{\\pi}[\\forall t\\in[H],\\,\\,\\sum_{h=1}^{t}c_{h}(s_{h},a_{h})\\leq B]=1$ if and only if for all $t\\in[H]$ and $\\tau\\in\\mathcal{H}_{H+1}$ with $\\mathbb{P}_{M}^{\\pi}[\\tau]>0$ it holds that $\\begin{array}{r}{\\sum_{h=1}^{t}c_{h}(s_{h},a_{h})\\le B}\\end{array}$ if and only if $\\begin{array}{r}{C_{M}^{\\pi}=\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:\\mathbb{P}_{M}^{\\pi}[\\tau]>0}\\sum_{h=1}^{t}c_{h}(s_{h},a_{h})\\le B}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Our choice of $\\alpha$ and $\\beta$ imply that, ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{h}^{\\pi}(\\tau_{h})=c_{h}(s,a)+\\operatorname*{max}\\left(0,\\operatorname*{max}_{s^{\\prime}\\in P_{h}(s,a)}C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To show that $C_{M}^{\\pi}$ satisfies (TR), we show that for all $h\\in[H+1]$ and all $\\tau_{h}\\in\\mathcal{H}_{h}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{h}(\\tau_{h})=\\operatorname*{max}_{t\\geq h}\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:\\atop\\mathbb{P}_{M}^{\\pi}[\\tau|\\tau_{h}]>0}c_{h:t}(\\tau).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we see that $C_{1}^{\\pi}(s_{0})\\,=\\,\\operatorname*{max}_{t\\in[H]}$ max $\\tau\\!\\in\\!\\mathcal{H}_{H+1}$ : $\\begin{array}{r}{\\mathbf{\\phi}_{c_{1:t}(\\tau)}=\\operatorname*{max}_{t\\in[H]}\\operatorname*{max}_{\\tau\\in\\mathcal{H}_{H+1}:}}\\\\ {\\mathbb{P}_{\\mathbf{\\phi}_{n}}^{\\pi}[\\tau]\\!>\\!0}\\end{array}$ : $\\mathbb{P}_{M}^{\\pi}[\\tau|s_{0}]\\!>\\!0$ $\\textstyle\\sum_{h=1}^{t}c_{h}(s_{h},a_{h})\\ =\\ C_{M}^{\\pi}$ . Thus, $C_{M}^{\\pi}$ satisfies (TR). Furthermore, it is clear that $\\mathrm{max}(0,\\mathrm{max}_{s^{\\prime}\\in P_{h}(s,a)}\\,C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime}))$ can be computed state-recursively by, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\Big(\\operatorname*{max}(0,C_{h+1}^{\\pi}(\\tau_{h},a,1)[P_{h}(1\\mid s,a)>0]),}\\\\ &{\\qquad\\qquad\\qquad\\operatorname*{max}(0,\\underset{s^{\\prime}=2}{\\operatorname*{max}}C_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})[P_{h}(s^{\\prime}\\mid s,a)>0])\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and so $C_{M}^{\\pi}$ satisfies (SR). The infinity conditions and non-decreasing requirements are also easy to verify. ", "page_idx": 15}, {"type": "text", "text": "We proceed by induction on $h$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 (Base Case) For the base case, we consider $h=H+1$ . Observe that for any history $\\tau$ and $t$ , we have $c_{H+1:t}(\\tau)=0$ since it is an empty sum. Then, by definition of $C_{M}^{\\pi}$ , we see that $\\begin{array}{r}{C_{H+1}^{\\pi}(\\tau_{H+1})=0=\\operatorname*{max}_{t}\\operatorname*{max}_{\\tau}0=\\operatorname*{max}_{t}\\operatorname*{max}_{\\tau}c_{H+1:t}(\\tau)^{3}.}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 (Inductive Step) For the inductive step, we consider $h\\,\\leq\\,H$ . Let $s\\,=\\,s_{h}(\\tau_{h})$ and $a=\\pi_{h}(\\tau_{h})$ . By separately considering the case where $t=h$ and $t\\geq h+1$ in the $\\operatorname*{max}_{t\\geq h}$ , we see that $\\operatorname*{max}_{t\\geq h}$ max $\\tau\\!\\in\\!\\mathcal{H}_{H+1}\\!:\\;\\;c_{h:t}(\\tau)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{=\\mathrm{max}\\left(\\frac{\\eta_{0}\\hbar\\omega_{0}}{\\eta_{0}^{2}\\hbar\\omega_{0}^{2}}\\phi_{0}\\lambda\\left(\\tau\\right),\\frac{\\eta_{0}\\hbar\\omega_{1}}{p_{x}^{2}\\left(\\tau\\right)+\\hbar\\omega_{1}^{2}}\\frac{\\eta_{0}\\hbar\\omega_{0}}{p_{x}^{2}\\left(\\tau\\right)+q_{0}^{2}\\hbar\\omega_{1}^{2}}\\phi_{1}(\\tau)\\right)}\\\\ &{=\\mathrm{nux}\\left(\\eta_{0}(\\delta_{1}),\\phi_{1}(\\omega_{0})+\\frac{\\eta_{0}\\hbar\\omega_{1}}{p_{x}^{2}\\left(\\tau\\right)+q_{0}^{2}\\hbar\\omega_{1}^{2}}\\frac{\\eta_{0}\\hbar\\omega_{0}}{p_{x}^{2}\\left(\\tau\\right)+q_{0}^{2}\\hbar\\omega_{1}^{2}}\\phi_{1}+\\eta_{0}\\right)}\\\\ &{=\\mathrm{c.}(\\delta_{1})+\\mathrm{max}\\left(\\frac{\\eta_{0}\\hbar\\omega_{0}}{q_{0}^{2}\\hbar\\omega_{0}^{2}}\\frac{\\eta_{0}\\hbar\\omega_{1}}{p_{x}^{2}\\left(\\tau\\right)+q_{0}^{2}\\hbar\\omega_{1}^{2}}\\phi_{1}+\\eta_{0}\\right)}\\\\ &{=\\mathrm{c.}(\\delta_{1})+\\mathrm{max}\\left(\\frac{\\eta_{0}\\hbar\\omega_{0}}{q_{0}\\hbar\\omega_{0}^{2}}\\eta_{0}^{2}\\hbar\\omega_{1}^{2}\\phi_{1}+\\eta_{0}\\right)}\\\\ &{=\\mathrm{c.}(\\delta_{1})+\\mathrm{max}\\left(\\frac{\\eta_{0}\\hbar\\omega_{0}}{q_{0}\\hbar\\omega_{0}^{2}}\\frac{\\eta_{0}\\hbar\\omega_{0}}{p_{x}^{2}\\left(\\tau\\right)+q_{0}^{2}\\hbar\\omega_{1}^{2}}\\frac{\\eta_{0}\\hbar\\omega_{0}}{p_{x}^{2}\\left(\\tau\\right)+q_{0}^{2}\\hbar\\omega_{1}^{2}}\\right)}\\\\ &{=\\mathrm \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second line used the fact that $c_{h:h}(\\tau)=c_{h}(s,a)$ and the recursive definition of $c_{h;t}(\\tau)$ . The fourth line used the result proven for the almost sure case above. The sixth line used the induction hypothesis. The last line used the definition of $C_{M}^{\\pi}$ . ", "page_idx": 16}, {"type": "text", "text": "C Proofs for Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Helpful Technical Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we use a different, inductive definition for $\\nu$ then in the main text. However, the following lemma shows they are equivalent. ", "page_idx": 16}, {"type": "text", "text": "Definition 9 (Value Space). For any $s\\in S$ , we define $\\mathcal{V}_{H+1}(s)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{0\\}$ , and for any $h\\in[H]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{V}_{h}(s)\\stackrel{\\mathrm{def}}{=}\\bigcup_{a}\\bigcup_{{\\bf v}\\in\\times_{s^{\\prime}}}\\bigcup_{\\mathcal{V}_{h+1}(s^{\\prime})}\\left\\{r_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We define $\\begin{array}{r}{\\mathcal{V}\\stackrel{\\mathrm{def}}{=}\\bigcup_{h,s}\\mathcal{V}_{h}(s)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 3 (Value Intution). For all $s\\in S$ and $h\\in[H+1]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{h}(s)=\\left\\{v\\in\\mathbb{R}\\mid\\exists\\pi\\in\\Pi^{D},\\tau_{h}\\in\\mathcal{H}_{h},\\;(s=s_{h}(\\tau_{h})\\wedge V_{h}^{\\pi}(\\tau_{h})=v)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $|\\mathcal{V}_{h}(s)|\\leq A^{\\sum_{t=h}^{H}S^{H-t}}$ . Thus, $\\nu$ can be computed in finite time using backward induction. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Cost). For any $h\\in[H+1]$ , $\\tau_{h}\\in\\mathcal{H}_{h}$ , and $v\\in\\mathcal{V}$ , i $f s=s_{h}(\\tau_{h})$ , then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{C}_{h}^{*}(s,v)\\leq\\underset{\\pi\\in\\Pi^{D}}{\\operatorname*{min}}\\;C_{h}^{\\pi}(\\tau_{h})}\\\\ &{\\qquad\\qquad\\qquad s.t.\\;V_{h}^{\\pi}(\\tau_{h})\\geq v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 5 (Value). Suppose that $\\pi\\in\\Pi^{D}$ . For all $h\\in[H+1]$ and $(s,v)\\in\\bar{S},$ , if $\\bar{C}_{h}^{\\pi}(s,v)<\\infty$ , then $\\bar{V}_{h}^{\\pi}(s,v)\\geq v$ . ", "page_idx": 17}, {"type": "text", "text": "Remark 6 (Technical Subtlety). Technically, $V_{h}^{\\pi}(\\tau_{h})$ is only well defined if $\\mathbb{P}_{M}^{\\pi}[\\tau_{h}]>0$ and all of our arguments technically should assume this is the case. However, it is standard in MDP theory to define the policy evaluation equations on non-reachable trajectories using the standard recursion to simplify proofs, as we have done here. Formally, this is equivalent to assuming the process starts initially at $\\tau_{h}$ instead of just conditioning on reaching $\\tau_{h}$ , or defining the values to correspond to policy evaluation equations directly. This is consistent with the usual definition when $\\mathbb{P}_{M}^{\\pi}[\\tau_{h}]>0$ but gives it a defined value also when $\\mathbb{P}_{M}^{\\pi}[\\tau_{h}]=0$ . In either case, this detail only means our recursive definition of $\\nu$ is a superset rather than exactly the set of all values as we defined in the main text. This does not effect the final results since unreachable trajectories do not effect $\\pi$ \u2019s overall value in the MDP anyway, and only effects the interpretations of some intermediate variables. ", "page_idx": 17}, {"type": "text", "text": "C.2 Proof of Proposition 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. By definition of $V_{M}^{*}$ and $C_{M}^{*}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{V_{M}^{*}>-\\infty\\iff\\exists\\pi\\in\\Pi^{D},\\;C_{M}^{\\pi}\\leq B\\wedge V_{M}^{\\pi}\\geq V_{M}^{*}}}\\\\ {{\\iff C_{M}^{*}\\leq B.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the second claim, we observe that if $V_{M}^{*}\\;>\\;-\\infty$ then by the above argument any optimal deterministic policy $\\pi$ for COVER satisfies $\\begin{array}{r}{\\ddot{C}_{M}^{\\pi}=C_{M}^{*}\\leq B}\\end{array}$ and $V_{M}^{\\pi}\\geq V_{M}^{*}$ . Thus, $C\\dot{O}V\\bar{E}R\\subseteq$ $P A C K$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of Lemma 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $s\\in S$ be arbitrary. ", "page_idx": 17}, {"type": "text", "text": "Base Case. For the base case, we consider $h\\,=\\,H\\,+\\,1$ . In this case, we know that for any $\\pi\\,\\in\\,\\Pi^{D}$ and any $\\tau\\in\\mathcal{H}_{H+1}$ , $V_{H+1}^{\\pi}(\\tau_{H+1})\\,=\\,0\\,\\in\\,\\{0\\}\\,=\\,\\mathcal{V}_{H+1}(s)$ by definition. Furthermore, |VH+1(s)| = 1 = A0 = A tH=H+1 St. ", "page_idx": 17}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\leq H$ . In this case, we know that for any $\\pi\\,\\in\\,\\Pi^{D}$ and any $\\tau_{h}\\in\\mathcal{H}_{h}$ , if $s\\,=\\,s_{h}(\\tau_{h})$ and $a=\\pi_{h}(\\tau_{h})$ , then the policy evaluation equations imply, ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{h}^{\\pi}(\\tau_{h})=r_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)V_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We know by the induction hypothesis that $V_{h+1}^{\\pi}(\\tau_{h},a,s^{\\prime})\\,\\in\\,\\mathcal{V}_{h+1}(s^{\\prime})$ . Thus, by (13), $V_{h}^{\\pi}(\\tau_{h})\\in$ $\\mathcal{V}_{h}(s)$ . Lastly, we see by (13) and the induction hypothesis that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\mathcal{V}_{h}(s)|\\le A\\prod_{s^{\\prime}}|\\mathcal{V}_{h+1}(s^{\\prime})|\\le A\\prod_{s^{\\prime}}A^{\\sum_{t=h+1}^{H}S^{H-t}}=A^{1+S\\sum_{t=h+1}^{H}S^{H-t}}=A^{\\sum_{t=h}^{H}S^{H-t}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "C.4 Proof of Lemma 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $\\tau_{h}\\,\\in\\,\\mathcal{H}_{h}$ and $v\\ \\in\\ \\mathcal{V}$ be arbitrary and suppose that $s=s_{h}(\\tau_{h})$ . We let $C_{h}^{*}(\\tau_{h},v)$ denote the minimum for the RHS of (15). ", "page_idx": 17}, {"type": "text", "text": "Base Case. For the base case, we consider $\\textit{h}=\\textit{H}+1$ . Observe that for any $\\pi\\ \\in\\ \\Pi^{D}$ , $V_{H+1}^{\\pi}(\\tau_{H+1})=0$ by definition. Thus, there exists a $\\pi\\in\\Pi^{D}$ satisfying $V_{H+1}^{\\pi}(\\tau_{H+1})\\geq v$ if and only if $v\\leq0$ . We also know by definition that any such policy $\\pi$ satisfies $C_{H+1}^{\\pi}(\\tau_{H+1})=0$ and if no such policy exists $C_{H+1}^{*}(\\tau_{H+1},v)=\\infty$ by convention. Therefore, we see that $C_{H+1}^{*}(\\tau_{H+1},v)=\\chi_{\\{v\\leq0\\}}$ . Then, by definition of the base case for $\\bar{C}$ , it follows that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{C}_{H+1}^{*}(s,v)=\\chi_{\\{v\\leq0\\}}=C_{H+1}^{*}(\\tau_{H+1},v).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\,\\leq\\,H$ . If $C_{h}^{*}(\\tau_{h},v)\\,=\\,\\infty$ , then trivially $\\bar{C}_{h}^{*}(s,v)\\,\\leq\\,C_{h}^{*}(\\tau_{h},v)$ . Instead, suppose that $C_{h}^{*}(\\tau_{h},v)\\,<\\,\\infty$ . Then, there must exist a feasible $\\pi\\in\\Pi^{D}$ satisfying $V_{h}^{\\pi}(\\tau_{h})\\geq v$ . Let $a^{*}=\\pi_{h}(\\tau_{h})$ . By the policy evaluation equations, we know that, ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{h}^{\\pi}(\\tau_{h})=r_{h}(s,a^{*})+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a^{*})V_{h+1}^{\\pi}(\\tau_{h},a^{*},s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For each $s^{\\prime}\\in\\mathcal{S}$ , define $v_{s^{\\prime}}^{\\ast}\\ {\\stackrel{\\mathrm{def}}{=}}\\ V_{h+1}^{\\pi}(\\tau_{h},a^{\\ast},s^{\\prime})$ and observe that $v_{s^{\\prime}}^{*}\\in\\mathcal{V}_{h+1}(s^{\\prime})\\subseteq\\mathcal{V}$ by Lemma 3. Thus, we see that $(a^{*},\\mathbf{v}^{*})\\,\\in\\,\\mathcal{A}\\times\\mathcal{V}^{S}$ and $\\begin{array}{r}{r_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\ge v}\\end{array}$ , which implies $(a^{\\ast},\\mathbf{v}^{\\ast})\\in\\bar{\\mathcal{A}}_{h}(s,v)$ . ", "page_idx": 18}, {"type": "text", "text": "Since $\\pi$ satisfies $V_{h+1}^{\\pi}(\\tau_{h},a^{*},s^{\\prime})\\geq v_{s^{\\prime}}^{*}$ , it is clear that $C_{h+1}^{*}(s^{\\prime},v_{s^{\\prime}}^{*})\\leq C_{h+1}^{\\pi}(\\tau_{h},a^{*},s^{\\prime})$ . Thus, the induction hypothesis implies that $\\bar{\\mathcal{C}}_{h+1}^{*}(s^{\\prime},v_{s^{\\prime}}^{*})\\leq C_{h+1}^{*}(s^{\\prime},v_{s^{\\prime}}^{*})\\leq C_{h+1}^{\\pi}(\\tau_{h},a^{*},s^{\\prime})$ . The optimality equations for $\\bar{M}$ then imply that, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{C}_{h}^{*}(s,v)=\\underset{(a,\\mathbf{v})\\in\\bar{\\mathcal{A}}_{h}(s,v)}{\\operatorname*{min}}\\,c_{h}(s,a)+f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a),\\bar{C}_{h+1}^{*}\\left(s^{\\prime},v_{s^{\\prime}}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a)}\\right)}\\\\ &{\\qquad\\leq c_{h}(s,a^{*})+f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a^{*}),\\bar{C}_{h+1}^{*}\\left(s^{\\prime},v_{s^{\\prime}}^{*}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a^{*})}\\right)}\\\\ &{\\qquad\\leq c_{h}(s,a^{*})+f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a),C_{h+1}^{\\pi}\\left(\\tau_{h},a^{*},s^{\\prime}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a^{*})}\\right)}\\\\ &{\\qquad=C_{h}^{\\pi}(\\tau_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first inequality used the fact that $\\left(a^{\\ast},\\mathbf{v}^{\\ast}\\right)\\in\\bar{\\mathcal{A}}_{h}(s,v)$ . The second inequality relied on $f$ being non-decreasing and the induction hypothesis. The final equality used (TR). ", "page_idx": 18}, {"type": "text", "text": "Since $\\pi$ was an arbitrary feasible policy for the optimization defining $C_{h}^{*}(\\tau_{h},v)$ , we see that $\\bar{C}_{h}^{*}(s,v)\\leq C_{h}^{*}(\\tau_{h},v)$ . This completes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.5 Proof of Lemma 5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $(s,v)\\in\\bar{S}$ be arbitrary. ", "page_idx": 18}, {"type": "text", "text": "Base Case. For the base case, we consider $h=H\\!+\\!1$ . By definition and assumption, $\\bar{C}_{H+1}^{\\pi}(s,v)=$ $\\chi_{\\{v\\leq0\\}}<\\infty$ . Thus, it must be the case that $v\\leq0$ and so by definition $\\bar{V}_{H+1}^{\\pi}(s,v)=0\\stackrel{\\cdot}{\\geq}v$ . ", "page_idx": 18}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\leq H$ . We decompose $\\pi_{h}(s,v)=(a,\\mathbf{v})$ where we know $(a,\\mathbf{v})\\in\\bar{\\mathcal{A}}_{h}(s,v)$ since $\\pi$ has finite cost4. Moreover, it must be the case that for any $s^{\\prime}\\in\\mathcal{S}$ with $P_{h}(s^{\\prime}\\mid s,a)>0$ that $\\bar{C}_{h+1}^{\\pi}\\big(s^{\\prime},v_{s^{\\prime}}\\big)<\\infty$ otherwise the property that $f$ outputs $\\infty$ when inputted an $\\infty$ would imply a contradiction: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{C}_{h}^{\\pi}(s,v)=c_{h}(s,a)+f\\left(\\left(P_{h}(s^{\\prime}\\mid s,a),\\bar{C}_{h+1}^{\\pi}\\left(s^{\\prime},v_{s^{\\prime}}\\right)\\right)_{s^{\\prime}\\in P_{h}(s,a)}\\right)}\\\\ &{\\qquad\\qquad=c_{h}(s,a)+f(\\cdot\\cdot\\cdot,\\infty,\\cdot\\cdot)}\\\\ &{\\qquad\\qquad=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the induction hypothesis implies that $\\bar{V}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}})\\geq v_{s^{\\prime}}$ for any such $s^{\\prime}\\in\\mathcal{S}$ . By the policy evaluation equations, we see that, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{\\pi}(s,v)=r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\bar{V}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}})}\\\\ &{\\qquad\\qquad\\geq r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}}\\\\ &{\\qquad\\qquad\\geq v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The third line uses the definition of $\\bar{\\mathcal{A}}_{h}(s,v)$ . This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "C.6 Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. If $\\bar{C}_{1}^{*}(s_{0},v)>B$ for all $v\\in\\mathcal{V}$ , then $C_{M}^{*}>B$ since otherwise we would have $\\bar{C}_{1}^{*}(s_{0},v)\\leq$ $C_{1}^{*}(s_{0},v)=C_{M}^{*}\\leq B$ by Lemma 4. Thus, if Algorithm 1 outputs \u201cinfeasible\u201d it is correct. ", "page_idx": 19}, {"type": "text", "text": "On the other hand, suppose that there exists some $v\\in\\mathcal{V}$ for which $\\bar{C}_{1}^{*}(s_{0},v)\\leq B$ . By standard MDP theory, we know that since $\\pi\\in\\Pi^{D}$ is a solution to $\\bar{M}$ , it must satisfy the optimality equations. In particular, $\\bar{C}_{1}^{\\pi}(s_{0},v)=\\bar{C}_{1}^{*}(s_{0},v)\\leq B$ . Since $C_{M}^{\\pi}=\\bar{C}_{1}^{\\pi}(s_{0},v)^{5}$ , we see that there exists a $\\pi\\in\\Pi^{D}$ for which $C_{M}^{\\pi}\\le B$ and so $V_{M}^{*}>-\\infty$ . ", "page_idx": 19}, {"type": "text", "text": "Since $V_{M}^{*}$ is the value of some deterministic policy, Lemma 3 implies that $V_{M}^{*}\\in\\mathcal{V}$ . Thus, Lemma 5 implies that $V_{1}^{\\pi}(s_{0},V_{M}^{*})\\geq V_{M}^{*}$ and $C_{1}^{\\pi}(s_{0}^{\\bar{}},V_{M}^{*})\\leq C_{1}^{*}(s_{0},V_{M}^{*})\\leq B$ . Consequently, running $\\pi$ with initial state $\\bar{s}_{0}\\,=\\,(s_{0},V_{M}^{*})$ is an optimal solution to (CON). In either case, Algorithm 1 is correct. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D Proofs for Section 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Definition 10. We define the exact partial sum, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma_{h,{\\bf v}}^{s,a}(t,u)\\ {\\stackrel{\\mathrm{def}}{=}}\\ u+\\sum_{s^{\\prime}=t}^{S}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Observation 1. We observe that both $\\sigma$ and $\\hat{\\sigma}$ can be computed recursively. Specifically, $\\sigma_{h,\\mathbf{v}}^{s,a}(S+$ $1,u)\\;=\\;u$ and $\\sigma_{h,{\\bf v}}^{s,a}(t,u)\\;=\\;\\sigma_{h,{\\bf v}}^{s,a}(t,u\\,+\\,P_{h}(t\\:\\mid\\,s,a)v_{t})$ . Similarly, $\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(S+1,u)\\;=\\;\\stackrel{.}{u}$ and $\\hat{\\sigma}_{h,{\\bf v}}^{s,a}(t,u)=\\sigma_{h,{\\bf v}}^{s,a}(t,\\lfloor u+P_{h}(t\\mid s,a)v_{t}\\rfloor_{\\mathcal{G}})$ . ", "page_idx": 19}, {"type": "text", "text": "For completeness, and to assist with other arguments, we also prove the exact recursion we presented in Definition 3 is correct using Lemma 6. ", "page_idx": 19}, {"type": "text", "text": "Lemma 6. For any $t\\in[S+1]$ and $u\\in\\mathbb{R}$ , we have that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{g_{h,v}^{s,a}(t,u)=\\displaystyle\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}^{S-t+1}}\\,g_{h,\\mathbf{v}}^{s,a}(t)}&{}\\\\ {s.t.\\,\\;}&{u+\\displaystyle\\sum_{s^{\\prime}=t}^{S}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\geq v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, $\\begin{array}{r}{\\bar{C}_{h}^{*}(s,v)=\\operatorname*{min}_{a\\in\\mathcal{A}}c_{h}(s,a)+g_{h,v}^{s,a}(1,r_{h}(s,a)).}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "D.1 Proof of Lemma 6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. We proceed by induction on $t$ . ", "page_idx": 19}, {"type": "text", "text": "tBhae see mCpatsy es.umF, otrh te hceo bnadsitei ocna $t=S+1$ $\\begin{array}{r}{\\sum_{s^{\\prime}=S+1}^{S}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}=0}\\end{array}$ $\\begin{array}{r}{u+\\sum_{s^{\\prime}=S+1}^{S}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\geq v}\\end{array}$ $u\\geq v$ $\\ensuremath{\\mathbf{v}}$ $g_{h,\\mathbf{v}}^{s,a}(S+1)=0$ by definition. Thus, the minimum defining $g_{h,\\mathbf{v}}^{s,a}(S+1,u)$ is 0 when $u\\geq v$ and is $\\infty$ due to infeasibility otherwise. In symbols, $g_{h,v}^{s,a}(S+1,u)=\\chi_{\\{u\\geq v\\}}$ as was to be shown. ", "page_idx": 19}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $t\\leq S$ . We see that $g_{h,v}^{s,a}(t,u)$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{~\\frac{\\partial\\beta_{h}^{h,\\alpha}(t)}{\\partial\\alpha}}\\\\ &{~\\quad=\\underbrace{\\operatorname*{min}(h)}_{\\geq\\nu_{h}(\\cdot)^{p+1}\\alpha}\\nu_{\\alpha},}\\\\ &{~=\\underbrace{\\operatorname*{min}(h)}_{\\geq\\nu_{h}(\\cdot)^{p+1}+\\alpha}\\nu_{\\alpha}\\geq\\nu}\\\\ &{~=\\underbrace{\\operatorname*{min}(h)}_{\\geq\\nu_{h}(\\cdot)^{p+1}+\\alpha}\\nu_{\\alpha}\\geq\\nu}\\\\ &{~=\\underbrace{\\operatorname*{min}(h)}_{\\geq\\nu_{h}(\\cdot)^{p}+\\alpha}\\nu_{\\alpha}^{\\geq\\alpha},\\quad\\geq\\nu}\\\\ &{~=\\underbrace{\\operatorname*{min}}_{\\geq\\nu_{h}(\\cdot)^{p}+\\alpha}\\nu_{\\alpha}^{\\geq\\alpha}+\\underbrace{\\operatorname*{min}(h)}_{\\geq\\nu_{h}^{p+1}+\\alpha}\\nu_{\\alpha},\\geq\\nu}\\\\ &{~=\\underbrace{\\operatorname*{min}}_{\\geq\\nu_{h}(\\cdot)^{p}+\\alpha}\\left(\\beta\\left(P_{h}(t\\mid s,\\alpha),\\tilde{C}_{h+1}^{*}(t,s)\\right),\\ \\underbrace{\\operatorname*{max}_{\\in\\Theta}(\\cdot)}_{\\geq\\nu_{h}(\\cdot)^{p}+\\alpha}\\nu_{\\alpha}^{\\geq\\alpha},\\geq\\nu\\right.}\\\\ &{\\left.=\\underset{\\geq\\nu_{h}(\\cdot)^{p}+\\alpha}{\\operatorname*{min}}\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,\\alpha),\\tilde{C}_{h+1}^{*}(t,v_{t})\\right),\\ \\underbrace{\\operatorname*{min}(\\cdot)}_{\\geq\\nu_{h}^{\\geq\\beta}+\\sum_{\\alpha+1}\\cdot\\operatorname*{min}(s)\\geq\\nu_{h}(\\cdot)s}\\underbrace{g_{h,\\alpha}^{s,\\alpha}(t+1)}_{\\geq\\nu_{h}^{\\geq\\beta}+\\alpha}\\right)}\\\\ &{~=\\underset{\\geq\\nu_{h}(\\cdot)^{p}}{\\operatorname*{min}}\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,\\alpha),\\tilde{C}_{h+1}^{*}(t,v_{t})\\right),\\ \\underbrace{(\\nu+P_{h}(t\\mid s,\\alpha)+P_{h}(t\\mid s,\\alpha))}_{\\geq\\nu_{h}^{\\beta}(t+1),\\ \\forall\\alpha}\\right)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The second lined used (SR). The third line split the optimization into the first decision and the remaining decisions and decomposed the sum in the constraint. The fourth line used the fact that $\\alpha$ is a non-decreasing function of both its arguments and the fact that the second optimization only concerns the second argument. The last line used the induction hypothesis. ", "page_idx": 20}, {"type": "text", "text": "The observation that $\\begin{array}{r}{\\operatorname*{min}_{a\\in\\mathcal{A}}c_{h}(s,a)\\!+g_{h,v}^{s,a}(1,r_{h}(s,a))=\\bar{C}_{h}^{*}(s,v)}\\end{array}$ then follows from the definition of $\\bar{\\mathcal{A}}_{h}(s,v)$ and (BU): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{a\\in\\mathcal{A}}c_{h}(s,a)+g_{h,v}^{s,a}(1,r_{h}(s,a))=\\displaystyle\\operatorname*{min}_{a\\in\\mathcal{A}}c_{h}(s,a)+\\displaystyle\\operatorname*{min}_{\\displaystyle v\\in\\mathcal{V}^{S}}{\\displaystyle\\operatorname*{min}_{\\displaystyle^{\\forall}\\mathcal{V}^{S}}}~g_{h,v}^{s,a}(1)}\\\\ {\\displaystyle=\\operatorname*{min}_{a\\in\\mathcal{A}}\\displaystyle\\operatorname*{min}_{\\displaystyle\\operatorname*{ver}^{*}(s^{\\prime}\\mid s,a)\\in\\mathcal{G}_{h,v}^{s,a}(1)}}\\\\ {\\displaystyle=\\operatorname*{min}_{\\displaystyle(a,v)\\in\\mathcal{A}_{h}(s,a)\\in\\mathcal{V}^{S}}{\\displaystyle\\operatorname*{min}_{\\displaystyle\\operatorname*{ver}^{*}(s^{\\prime}\\mid s,a)v_{s}\\geq v}}}\\\\ {\\displaystyle=\\operatorname*{min}_{\\displaystyle(a,v)\\in\\bar{\\mathcal{A}}_{h}(s,v)}c_{h}(s,a)+g_{h,\\mathbf{v}}^{s,a}(1)}\\\\ {\\displaystyle=\\bar{C}_{h}^{*}(s,v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Base Case. For the base case, we consider $t=S+1$ . By definition, $\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(S+1,u)=u$ so the constraint is satisfied iff $u\\geq v$ . Since for any $\\hat{\\textbf{v}}$ , $\\hat{g}_{h,\\hat{\\mathbf{v}}}^{s,a}(S+1)\\,=\\,0$ by definition, the minimum defining $\\hat{g}_{h,\\hat{\\mathbf{v}}}^{s,a}(S+1,u)$ is 0 when $u\\,\\geq\\,v$ and is $\\infty$ due to infeasibility otherwise. In symbols, $\\hat{g}_{h,v}^{s,a}(S+1,u)=\\chi_{\\{u\\geq v\\}}$ as was to be shown. ", "page_idx": 20}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $t\\leq S$ . We see that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{g}_{h,v}^{s,a}(t,u)=\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}~\\varepsilon_{h,v}^{s,a}(t)}\\\\ &{=\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}}\\\\ &{=\\underset{\\bar{\\alpha}_{h,v}^{s,b}(t^{1})\\leq v}{\\operatorname*{min}}\\,\\bigg(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),g_{h,v}^{s,a}(t+1)\\bigg)}\\\\ &{=\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}\\,\\bigg(\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}\\,\\bigg(\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),g_{h,v}^{s,a}(t+1)\\right)}\\\\ &{=\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}\\,\\bigg(\\varepsilon_{h,v}^{s,a}(t+1,[u+b_{h}(t\\mid s,a)v_{t}]_{Q})\\geq v}\\\\ &{=\\underset{v\\in\\bar{\\gamma}_{h,v}^{s}}{\\operatorname*{min}}\\,\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t+1,[u+b_{h}(t),a]v_{t}]\\geq v}{\\operatorname*{min}}\\,g_{h,v}^{s,a}(t+1)\\right)}\\\\ &{=\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}\\,\\bigg(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),~\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1})\\leq v}{\\operatorname*{min}}\\,\\bigg(\\varepsilon_{h,v}^{s,a}(t)-\\varepsilon_{h,v}^{s}\\bigg)}\\\\ &{=\\underset{\\bar{\\alpha}_{h,v}^{s,a}(t^{1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second lined used (SR). The third line split the optimization into the first decision and the remaining decisions and used the recursive definition of $\\hat{\\sigma}$ in the constraint. The fourth line used the fact that $\\alpha$ is a non-decreasing function of both its arguments and the fact that the second optimization only concerns the second argument. The last line used the induction hypothesis. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.3 Proof of Proposition 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. The runtime guarantee is easily seen since Algorithm 4 consists of nested loops. The fact that it computes an optimal solution for $\\vec{M}$ absent rounding or lower bounding follows immediately from Lemma 6. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "E Proofs for Section 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Helpful Technical Lemmas (Additive) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following claims all assume Definition 7. ", "page_idx": 21}, {"type": "text", "text": "Observation 2. For any $v\\in\\mathbb R$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nv-\\delta\\leq\\lfloor v\\rfloor_{\\mathcal{G}}\\leq v.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 7. For any $h\\in[H]$ , $s\\in{\\mathcal{S}}$ , $a\\in A$ , $\\mathbf{v}\\in\\mathbb{R}^{S}$ , $u\\in\\mathbb R$ , and $t\\in[S+1]$ , we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma_{h,\\mathbf{v}}^{s,a}(t,u)-(S-t+1)\\delta\\leq\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(t,u)\\leq\\sigma_{h,\\mathbf{v}}^{s,a}(t,u).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 8 (Cost). For any $h\\in[H+1]$ and $(s,v)\\in\\bar{\\mathcal{S}},\\,\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})\\leq\\bar{C}_{h}^{*}(s,v).$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 9 (Approximation). Suppose that $\\pi\\;\\in\\;\\Pi^{D}$ . For all $h\\ \\in\\ [H\\mathrm{~+~}1]$ and $(s,{\\hat{v}})\\;\\in\\;{\\hat{S}}$ , if $\\hat{C}_{h}^{\\pi}(s,\\hat{v})<\\infty,$ , then $\\hat{V}_{h}^{\\pi}(s,\\hat{v})\\geq\\hat{v}-\\delta(S+1)(H-h+1)$ . ", "page_idx": 21}, {"type": "text", "text": "E.2 Helpful Technical Lemmas (Relative) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following claims all assume Definition 8. ", "page_idx": 21}, {"type": "text", "text": "Observation 3. For any $v\\in\\mathbb R$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nv(1-\\delta)\\leq\\lfloor v\\rfloor_{\\mathcal{G}}\\leq v.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 10. For any $h\\in[H]$ , $s\\in S$ , $a\\in{\\mathcal{A}}$ , $\\mathbf{v}\\in\\mathbb{R}_{\\ge0}^{S}$ , $u\\in\\mathbb{R}_{\\geq0}$ , and $t\\in[S+1]$ , we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma_{h,\\mathbf{v}}^{s,a}(t,u)(1-\\delta)^{S-t+1}\\le\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(t,u)\\le\\sigma_{h,\\mathbf{v}}^{s,a}(t,u).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 11 (Cost). Suppose all rewards are non-negative. For any $h\\in[H+1]$ and $(s,v)\\in\\bar{S}$ , $\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})\\leq\\bar{C}_{h}^{*}(s,v)$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 12 (Approximation). Suppose all rewards are non-negative and $\\pi\\in\\Pi^{D}$ . For all $h\\in[H\\!+\\!1]$ and $(s,{\\hat{v}})\\in{\\hat{S}}$ , $i f\\hat{C}_{h}^{\\pi}(s,\\hat{v})<\\infty$ , then $\\hat{V}_{h}^{\\pi}(s,\\hat{v})\\geq\\hat{v}(1-\\delta)^{(S+1)(H-h+1)}$ . ", "page_idx": 21}, {"type": "text", "text": "E.3 Proof of Observation 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Using properties of the floor function, we can infer that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lfloor v\\rfloor_{\\mathcal{G}}=\\left\\lfloor{\\frac{v}{\\delta}}\\right\\rfloor\\delta\\leq{\\frac{v}{\\delta}}\\delta=v,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lfloor v\\rfloor_{\\mathcal{G}}=\\left\\lfloor{\\frac{v}{\\delta}}\\right\\rfloor\\delta\\geq(\\left\\lceil{\\frac{v}{\\delta}}\\right\\rceil-1)\\delta=\\left\\lceil{\\frac{v}{\\delta}}\\right\\rceil\\delta-\\delta\\geq v-\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E.4 Proof of Lemma 7 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We proceed by induction on $t$ . ", "page_idx": 22}, {"type": "text", "text": "Base Case. For the base case, we consider $t=S+1$ . By definition, we have $\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(S+1,u)=$ $u=\\sigma_{h,\\mathbf{v}}^{s,a}(S+1,u)$ . ", "page_idx": 22}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $t\\leq S$ . We first see that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(t,u)=\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}})}\\\\ &{\\quad\\quad\\quad\\le\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}})}\\\\ &{\\quad\\quad\\quad=\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}}+\\displaystyle\\sum_{s^{\\prime}=t+1}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}}\\\\ &{\\quad\\quad\\quad\\le u+\\displaystyle\\sum_{s^{\\prime}=t}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}}\\\\ &{\\quad\\quad\\quad=\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}(t,u).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first inequality used the induction hypothesis and the second inequality used the fact that $\\lfloor x\\rfloor_{\\mathcal{G}}\\leq x$ . ", "page_idx": 22}, {"type": "text", "text": "We also see that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\bar{\\sigma}_{h,\\bar{\\mathbf{v}}}^{s,a}(t,u)=\\hat{\\sigma}_{h,\\bar{\\mathbf{v}}}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}})}&{}\\\\ {\\displaystyle\\geq\\sigma_{h,\\bar{\\mathbf{v}}}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}})-\\delta(S-t)}&{}\\\\ {\\displaystyle=\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}}+\\sum_{s^{\\prime}=t+1}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}-\\delta(S-t)}&{}\\\\ {\\displaystyle\\geq\\boldsymbol{u}+\\sum_{s^{\\prime}=t}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}-\\delta(S-t+1)}&{}\\\\ {\\displaystyle=\\sigma_{h,\\bar{\\mathbf{v}}}^{s,a}(t,u)-\\delta(S-t+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first inequality used the induction hypothesis and the second inequality used the fact that $\\lfloor x\\rfloor_{\\mathcal{G}}\\geq x-\\delta$ . ", "page_idx": 22}, {"type": "text", "text": "E.5 Proof of Lemma 8 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $(s,v)\\in\\bar{S}$ be arbitrary. ", "page_idx": 22}, {"type": "text", "text": "Base Case. For the base case, we consider $h=H+1$ . Since $\\lfloor v\\rfloor_{\\mathcal{G}}\\leq v$ , we immediately see, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{C}_{H+1}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})=\\chi_{\\left\\{{\\lfloor v\\rfloor_{\\mathcal{G}}\\leq0}\\right\\}}\\leq\\chi_{\\{v\\leq0\\}}=\\bar{C}_{H+1}^{*}(s,v).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\,\\leq\\,H$ . If $\\bar{C}_{h}^{*}(s,v)\\,=\\,\\infty$ , then trivially $\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})\\leq\\bar{C}_{h}^{*}(s,v)$ . Instead, suppose that $\\bar{C}_{h}^{*}(s,v)<\\infty$ . Let $\\pi$ be a solution to the optimality equations for $\\bar{M}$ so that $\\bar{C}_{h}^{\\pi}(s,v)=\\bar{C}_{h}^{*}(s,\\underline{{{v}}})<\\infty$ . Since $\\bar{C}_{h}^{*}(s,v)<\\infty$ , we know that $(a^{*},\\mathbf{v}^{*})=$ $\\pi_{h}(s,v)\\in\\bar{\\mathcal{A}}_{h}(s,v)$ . By the definition of $\\bar{\\mathcal{A}}_{h}(s,v)$ , we know that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{h,\\mathbf{v}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h}(s,a^{*})+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a^{*})v_{s^{\\prime}}^{*}\\geq v\\geq\\lfloor v\\rfloor_{\\mathcal{G}}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each $s^{\\prime}\\in\\mathcal{S}$ , define $\\hat{v}_{s^{\\prime}}^{*}\\overset{\\mathrm{def}}{=}\\lfloor v_{s^{\\prime}}^{*}\\rfloor_{\\mathcal{G}}$ and recall that $v_{s^{\\prime}}^{*}\\in\\mathcal{V}$ . We first observe that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h}(s,a^{*})+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\left\\lfloor v_{s^{\\prime}}\\right\\rfloor_{\\mathcal{G}}}\\\\ &{\\phantom{\\sigma_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h}(s,a^{*})+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)(v_{s^{\\prime}}-\\delta)}}\\\\ &{\\phantom{\\sigma_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h}(s,a^{*})+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}-\\delta}}\\\\ &{\\phantom{\\sigma_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h,\\mathbf{v}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))-\\delta.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then by Lemma 7, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))\\geq\\sigma_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))-\\delta S}\\\\ &{\\qquad\\qquad\\qquad\\geq\\sigma_{h,\\mathbf{v}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))-\\delta(S+1)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\lfloor v\\rfloor_{\\mathcal{G}}-\\delta(S+1)}\\\\ &{\\qquad\\qquad\\qquad=\\kappa(\\lfloor v\\rfloor_{\\mathcal{G}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, $\\left(a^{\\ast},\\hat{\\mathbf{v}}^{\\ast}\\right)\\in\\hat{\\mathcal{A}}_{h}(s,\\lfloor v\\rfloor_{\\mathcal{G}})$ . ", "page_idx": 23}, {"type": "text", "text": "Since $v_{s^{\\prime}}^{*}\\in\\mathcal{V}$ , the induction hypothesis implies that $\\hat{C}_{h+1}^{*}(s^{\\prime},\\hat{v}_{s^{\\prime}}^{*})\\leq\\bar{C}_{h+1}^{*}(s^{\\prime},v_{s^{\\prime}}^{*})=\\bar{C}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}}^{*}).$ The optimality equations for M\u02c6 then imply that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})=\\underset{(a,\\hat{\\mathrm{v}})\\in\\hat{\\mathcal{A}}_{h}(s,v)}{\\operatorname*{min}}\\,c_{h}(s,a)+f\\left(\\Big(P_{h}(s^{\\prime}\\mid s,a),\\hat{C}_{h+1}^{*}\\left(s^{\\prime},\\hat{v}_{s^{\\prime}}\\right)\\Big)_{s^{\\prime}\\in P_{h}(s,a)}\\right)}\\\\ &{\\qquad\\qquad\\leq c_{h}(s,a^{*})+f\\left(\\Big(P_{h}(s^{\\prime}\\mid s,a^{*}),\\hat{C}_{h+1}^{*}\\left(s^{\\prime},\\hat{v}_{s^{\\prime}}^{*}\\right)\\Big)_{s^{\\prime}\\in P_{h}(s,a^{*})}\\right)}\\\\ &{\\qquad\\qquad\\leq c_{h}(s,a^{*})+f\\left(\\big(P_{h}(s^{\\prime}\\mid s,a),\\bar{C}_{h+1}^{\\pi}\\left(s^{\\prime},v_{s^{\\prime}}^{*}\\right)\\big)_{s^{\\prime}\\in P_{h}(s,a^{*})}\\right)}\\\\ &{\\qquad=\\bar{C}_{h}^{\\pi}(s,v)}\\\\ &{\\qquad=\\bar{C}_{h}^{*}(s,v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first inequality used the fact that $(a^{\\ast},\\mathbf{v}^{\\ast})\\in\\hat{\\mathcal{A}}_{h}(s,v)$ . The second inequality relied on $f$ being non-decreasing and the induction hypothesis. The penultimate equality used (TR). This completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E.6 Proof of Lemma 9 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $(s,{\\hat{v}})\\in{\\hat{S}}$ be arbitrary. ", "page_idx": 23}, {"type": "text", "text": "Base Case. For the base case, we consider $h=H\\!+\\!1$ . By definition and assumption, $\\hat{C}_{H+1}^{\\pi}(s,\\hat{v})=$ $\\chi_{\\{\\hat{v}\\leq0\\}}<\\infty$ . Thus, it must be the case that $\\hat{v}\\leq0$ and so by definition $\\hat{V}_{H+1}^{\\pi}(s,\\hat{v})=0\\stackrel{.}{\\geq}\\hat{v}$ . ", "page_idx": 23}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\leq H$ . As in the proof of Lemma 5, we know that $\\pi_{h}(s,v)=(a,{\\hat{\\mathbf{v}}})\\in{\\hat{\\mathcal{A}}}_{h}(s,{\\hat{v}})$ and for any $s^{\\prime}\\in\\mathcal{S}$ with $P_{h}(s^{\\prime}\\mid s,a)>0$ that $\\hat{C}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}})<\\infty$ . Thus, the induction hypothesis implies that $\\hat{V}_{h+1}^{\\pi}(s^{\\prime},\\hat{v}_{s^{\\prime}})\\,\\geq\\,\\hat{v}_{s^{\\prime}}-\\delta(S+1)(H-h)$ for any such ", "page_idx": 23}, {"type": "text", "text": "$s^{\\prime}\\in\\mathcal{S}$ . By the policy evaluation equations, we see that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{V}_{h}^{\\pi}(s,\\hat{v})=r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\hat{V}_{h+1}^{\\pi}(s^{\\prime},\\hat{v}_{s^{\\prime}})}\\\\ &{\\qquad\\qquad\\geq r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{s^{\\prime}}-\\delta(S+1)(H-h)}\\\\ &{\\qquad=\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}(1,r_{h}(s,a))-\\delta(S+1)(H-h)}\\\\ &{\\qquad\\geq\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(1,r_{h}(s,a))-\\delta(S+1)(H-h)}\\\\ &{\\qquad\\geq\\hat{v}-\\delta(S+1)-\\delta(S+1)(H-h)}\\\\ &{\\qquad=\\hat{v}-\\delta(S+1)(H-h+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first inequality used the induction hypothesis. The second inequality used Lemma 7. The third inequality used the fact that by definition of $\\hat{A}_{h}(s,\\hat{v})$ and $\\kappa$ , $\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(1,r_{h}(s,a))\\ge\\kappa(\\hat{v})=\\hat{v}-\\delta(S\\!+\\!1)$ . This completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "E.7 Proof of Theorem 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. ", "page_idx": 24}, {"type": "text", "text": "Correctness. If $\\hat{C}_{1}^{*}(s_{0},v)\\,>\\,B$ for all $\\hat{v}\\,\\in\\,\\hat{\\mathcal{V}}$ , then $C_{M}^{*}\\,>\\,B$ since otherwise we would have $\\hat{C}_{1}^{*}(s_{0},\\lfloor v\\rfloor_{\\mathcal{G}})\\le\\bar{C}_{1}^{*}(s_{0},v)\\le C_{M}^{*}\\le B$ by Lemma 8. Thus, if Algorithm 5 outputs \u201cinfeasible\u201d it is correct. ", "page_idx": 24}, {"type": "text", "text": "On the other hand, suppose that there exists some $\\hat{v}\\in\\hat{\\mathcal{V}}$ for which $\\hat{C}_{1}^{*}(s_{0},\\hat{v})\\leq B$ . By standard MDP theory, we know that since $\\pi\\in\\Pi^{D}$ is a solution to $\\hat{M}$ , it must satisfy the optimality equations. In particular, $\\hat{C}_{1}^{\\pi}(s_{0},\\hat{v})=\\hat{C}_{1}^{*}(s_{0},v)\\le B$ . As in the proof of Theorem 1, since $C_{M}^{\\pi}=\\hat{C}_{1}^{\\bar{\\pi}}(s_{0},\\hat{v})$ , we see that there exists a $\\pi\\in\\Pi^{D}$ for which $C_{M}^{\\pi}\\le B$ and so $V_{M}^{*}>-\\infty$ . ", "page_idx": 24}, {"type": "text", "text": "Since $V_{M}^{*}$ is the value of some deterministic policy, Lemma 3 implies that $V_{M}^{*}\\in\\mathcal{V}$ . Thus, Lemma 9 implies that $\\hat{V}_{1}^{\\pi}(s_{0},\\lfloor V_{M}^{*}\\rfloor_{\\mathcal{G}})\\ge\\lfloor V_{M}^{*}\\rfloor_{\\mathcal{G}}-\\delta(S+1)H\\ge V_{M}^{*}-\\delta(1+(S+1)H)=V_{M}^{*}-\\epsilon$ and $\\hat{C}_{1}^{\\pi}(s_{0},V_{M}^{*})\\le C_{1}^{*}(s_{0},V_{M}^{*})\\le B$ . Consequently, running $\\pi$ with initial state $\\bar{s}_{0}=(s_{0},\\lfloor V_{M}^{*}\\rfloor_{\\mathcal{G}})$ is an optimal solution to (CON). In either case, Algorithm 5 is correct. ", "page_idx": 24}, {"type": "text", "text": "Complexity. For the complexity claim, we observe that the running time of Algorithm 5 is $O(H S^{2}A|\\hat{\\nu}|^{2}|\\hat{\\mathcal{U}}|)$ . To bound $|\\hat{\\mathcal{V}}|$ , we observe that the number of integer multiples of $\\delta$ required to capture the range $[-H r_{m a x},H r_{m a x}]$ is at most $\\begin{array}{r}{O(\\frac{H r_{m a x}}{\\delta})=O(H^{2}S r_{m a x}/\\epsilon)}\\end{array}$ by definition of $\\delta$ . Moreover, $|\\hat{\\mathcal{U}}|=O(|\\hat{\\mathcal{V}}|+S)=O(|\\hat{\\mathcal{V}}|)$ for sufficiently large $\\frac{r_{m a x}}{\\epsilon}$ . ", "page_idx": 24}, {"type": "text", "text": "In particular, we see that the range of the rounded sums defining $\\hat{\\mathcal{U}}$ is at widest $[-2H r_{m a x}\\textrm{--}$ $\\delta S,2H r_{m a x}]$ since for any $t+1$ the rounded input is, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Big[\\big\\lfloor r_{h}(s,a)+P_{h}(1\\mid s,a)\\hat{\\mathbf{v}}_{1}\\big\\rfloor_{\\mathcal{G}}+\\ldots+P_{h}(t\\mid s,a)\\hat{\\mathbf{v}}_{t}\\Big]_{\\mathcal{G}}\\leq r_{h}(s,a)+\\sum_{s^{\\prime}=1}^{t}P_{h}(s^{\\prime}\\mid s,a)\\hat{\\mathbf{v}}_{s^{\\prime}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is at most $2H r_{m a x}$ , and, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Big|\\{r_{h}(s,a)+P_{h}(1\\mid s,a)\\hat{\\mathbf{v}}_{1}\\}_{\\boldsymbol{g}}+.\\,.\\,.\\,+P_{h}(t\\mid s,a)\\hat{\\mathbf{v}}_{t}\\Big|_{\\boldsymbol{g}}\\geq r_{h}(s,a)+\\sum_{s^{\\prime}=1}^{t}P_{h}(s^{\\prime}\\mid s,a)\\hat{\\mathbf{v}}_{s^{\\prime}}-\\delta t,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is at least $-2H r_{m a x}-\\delta S$ . Overall, we see that $O(|\\hat{\\mathcal{V}}|^{2}|\\hat{\\mathcal{U}}|)=O(|\\hat{\\mathcal{V}}|^{3})=O(H^{6}S^{3}r_{m a x}^{3}/\\epsilon^{3})$ implying that the total run time is $O(H^{7}S^{5}A r_{m a x}^{3}/\\epsilon^{3})$ as claimed. ", "page_idx": 24}, {"type": "text", "text": "E.8 Proof of Observation 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Using properties of the floor function, we can infer that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lfloor v\\rfloor_{\\mathcal{G}}=v^{m i n}\\left(\\frac{1}{1-\\delta}\\right)^{\\left\\lfloor\\log_{\\frac{1}{1-\\delta}}\\frac{v}{v^{m i n}}\\right\\rfloor}\\leq v^{m i n}\\left(\\frac{1}{1-\\delta}\\right)^{\\log_{\\frac{1}{1-\\delta}}\\frac{v}{v^{m i n}}}=\\frac{v}{v^{m i n}}v^{m i n}=v,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lfloor v\\rfloor_{\\mathcal{G}}=v^{m i n}\\left(\\frac{1}{1-\\delta}\\right)^{\\left\\lfloor\\log\\frac{1}{1-\\delta}\\frac{v}{v^{m i n}}\\right\\rfloor}\\geq v^{m i n}\\left(\\frac{1}{1-\\delta}\\right)^{\\log\\frac{1}{1-\\delta}\\frac{v}{v^{m i n}}-1}=v(1-\\delta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E.9 Proof of Lemma 10 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. We proceed by induction on $t$ . ", "page_idx": 25}, {"type": "text", "text": "Base Case. For the base case, we consider $t=S+1$ . By definition, we have $\\hat{\\sigma}_{h,\\mathbf{v}}^{s,a}(S+1,u)=$ $u=\\sigma_{h,\\mathbf{v}}^{s,a}(S+1,u)$ . ", "page_idx": 25}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $t\\leq S$ . We first see that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(t,u)=\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}})}\\\\ &{\\quad\\quad\\quad\\le\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}(t+1,\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}})}\\\\ &{\\quad\\quad\\quad=\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}}+\\displaystyle\\sum_{s^{\\prime}=t+1}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}}\\\\ &{\\quad\\quad\\quad\\le u+\\displaystyle\\sum_{s^{\\prime}=t}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}}\\\\ &{\\quad\\quad\\quad=\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}(t,u).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality used the induction hypothesis and the second inequality used the fact that $\\lfloor x\\rfloor_{\\mathcal{G}}\\leq x$ . ", "page_idx": 25}, {"type": "text", "text": "We also see that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(t,u)=\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}\\left(t+1,\\left|u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\right|_{\\mathcal{G}}\\right)}\\\\ &{\\qquad\\qquad\\geq\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}\\left(t+1,\\left|u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\right|_{\\mathcal{G}}\\right)(1-\\delta)^{S-t}}\\\\ &{\\qquad\\qquad=\\left(\\lfloor u+P_{h}(t\\mid s,a)\\hat{v}_{t}\\rfloor_{\\mathcal{G}}+\\displaystyle\\sum_{s^{\\prime}=t+1}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}\\right)(1-\\delta)^{S-t}}\\\\ &{\\qquad\\qquad\\geq\\left((1-\\delta)u+(1-\\delta)\\displaystyle\\sum_{s^{\\prime}=t}^{S}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{t}\\right)(1-\\delta)^{S-t}}\\\\ &{\\qquad\\qquad=\\sigma_{h,\\hat{\\mathbf{v}}}^{s,a}(t,u)(1-\\delta)^{S-t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality used the induction hypothesis and the second inequality used the fact that $|x|_{\\mathcal{G}}\\;\\geq\\;x\\,-\\,\\delta$ and the fact that all rewards and values are non-negative allowing us to add a $(1-\\delta)$ -factor to the other value demands. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "E.10 Proof of Lemma 11 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $(s,v)\\in\\bar{S}$ be arbitrary. ", "page_idx": 25}, {"type": "text", "text": "Base Case. For the base case, we consider $h=H+1$ . Since $\\lfloor v\\rfloor_{\\mathcal{G}}\\leq v$ , we immediately see, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{C}_{H+1}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})=\\chi_{\\left\\{{\\lfloor v\\rfloor_{\\mathcal{G}}\\leq0}\\right\\}}\\leq\\chi_{\\{v\\leq0\\}}=\\bar{C}_{H+1}^{*}(s,v).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\,\\leq\\,H$ . If $\\bar{C}_{h}^{*}(s,v)\\,=\\,\\infty$ , then trivially $\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})\\leq\\bar{C}_{h}^{*}(s,v)$ . Instead, suppose that $\\bar{C}_{h}^{*}(s,v)<\\infty$ . Let $\\pi$ be a solution to the optimality ", "page_idx": 25}, {"type": "text", "text": "equations for $\\bar{M}$ so that $\\bar{C}_{h}^{\\pi}(s,v)=\\bar{C}_{h}^{*}(s,\\underline{{{v}}})<\\infty$ . Since $\\bar{C}_{h}^{*}(s,v)<\\infty$ , we know that $(a^{*},\\mathbf{v}^{*})=$ $\\pi_{h}(s,v)\\in\\bar{\\mathcal{A}}_{h}(s,v)$ . By the definition of $\\bar{\\mathcal{A}}_{h}(s,v)$ , we know that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sigma_{h,\\mathbf{v}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h}(s,a^{*})+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a^{*})v_{s^{\\prime}}^{*}\\geq v\\geq\\lfloor v\\rfloor_{\\mathcal{G}}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For each $s^{\\prime}\\in\\mathcal{S}$ , define $\\hat{v}_{s^{\\prime}}^{*}\\stackrel{\\mathrm{def}}{=}\\lfloor v_{s^{\\prime}}^{*}\\rfloor_{\\mathcal{G}}$ and recall that $v_{s^{\\prime}}^{*}\\in\\mathcal{V}$ . We first observe that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sigma_{h,\\sqrt{s^{*}}}^{s,a^{*}}(1,r_{h}(s,a^{*}))=r_{h}(s,a^{*})+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\left|v_{s^{\\prime}}\\right|_{\\mathscr G}}&{}&\\\\ {\\geq r_{h}(s,a^{*})+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}(1-\\delta)}&{}&\\\\ {\\geq\\left(r_{h}(s,a^{*})+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\right)(1-\\delta)}&{}&\\\\ {=\\sigma_{h,\\mathrm{v}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))(1-\\delta).}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The second inequality used the fact that all rewards are non-negative. Then by Lemma 10, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))\\geq\\sigma_{h,\\hat{\\mathbf{v}}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))(1-\\delta)^{S}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\sigma_{h,\\mathbf{v}^{*}}^{s,a^{*}}(1,r_{h}(s,a^{*}))(1-\\delta)^{S+1}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\lfloor v\\rfloor_{\\mathcal{G}}\\,(1-\\delta)^{S+1}}\\\\ &{\\qquad\\qquad\\qquad=\\kappa(\\lfloor v\\rfloor_{\\mathcal{G}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, $\\left(a^{\\ast},\\hat{\\mathbf{v}}^{\\ast}\\right)\\in\\hat{\\mathcal{A}}_{h}(s,\\lfloor v\\rfloor_{\\mathcal{G}})$ . ", "page_idx": 26}, {"type": "text", "text": "Since $v_{s^{\\prime}}^{*}\\in\\mathcal{V}$ , the induction hypothesis implies that $\\hat{C}_{h+1}^{*}(s^{\\prime},\\hat{v}_{s^{\\prime}}^{*})\\leq\\bar{C}_{h+1}^{*}(s^{\\prime},v_{s^{\\prime}}^{*})=\\bar{C}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}}^{*}).$ The optimality equations for $\\hat{M}$ then imply that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{C}_{h}^{*}(s,\\lfloor v\\rfloor_{\\mathcal{G}})=\\underset{(a,\\tilde{v})\\in\\tilde{\\mathcal{A}}_{h}(s,v)}{\\operatorname*{min}}\\,c_{h}(s,a)+f\\left(\\Big(P_{h}(s^{\\prime}\\mid s,a),\\hat{C}_{h+1}^{*}\\left(s^{\\prime},\\hat{v}_{s^{\\prime}}\\right)\\Big)_{s^{\\prime}\\in P_{h}(s,a)}\\right)}\\\\ &{\\qquad\\qquad\\leq c_{h}(s,a^{*})+f\\left(\\Big(P_{h}(s^{\\prime}\\mid s,a^{*}),\\hat{C}_{h+1}^{*}\\left(s^{\\prime},\\hat{v}_{s^{\\prime}}^{*}\\right)\\Big)_{s^{\\prime}\\in P_{h}(s,a^{*})}\\right)}\\\\ &{\\qquad\\qquad\\leq c_{h}(s,a^{*})+f\\left(\\big(P_{h}(s^{\\prime}\\mid s,a),\\bar{C}_{h+1}^{\\pi}\\left(s^{\\prime},v_{s^{\\prime}}^{*}\\right)\\big)_{s^{\\prime}\\in P_{h}(s,a^{*})}\\right)}\\\\ &{\\qquad=\\bar{C}_{h}^{\\pi}(s,v)}\\\\ &{\\qquad=\\bar{C}_{h}^{*}(s,v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first inequality used the fact that $(a^{\\ast},\\mathbf{v}^{\\ast})\\in\\hat{\\mathcal{A}}_{h}(s,v)$ . The second inequality relied on $f$ being non-decreasing and the induction hypothesis. The penultimate equality used (TR). ", "page_idx": 26}, {"type": "text", "text": "This completes the proof. ", "page_idx": 26}, {"type": "text", "text": "E.11 Proof of Lemma 12 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. We proceed by induction on $h$ . Let $(s,{\\hat{v}})\\in{\\hat{S}}$ be arbitrary. ", "page_idx": 26}, {"type": "text", "text": "Base Case. For the base case, we consider $h=H\\!+\\!1$ . By definition and assumption, $\\hat{C}_{H+1}^{\\pi}(s,\\hat{v})=$ $\\chi_{\\{\\hat{v}\\leq0\\}}<\\infty$ . Thus, it must be the case that $\\hat{v}\\leq0$ and so by definition $\\hat{V}_{H+1}^{\\pi}(s,\\hat{v})=0\\geq\\hat{v}$ . ", "page_idx": 26}, {"type": "text", "text": "Inductive Step. For the inductive step, we consider $h\\leq H$ . As in the proof of Lemma 5, we know that $\\pi_{h}(s,v)=(a,{\\hat{\\mathbf{v}}})\\in{\\hat{\\mathcal{A}}}_{h}(s,{\\hat{v}})$ and for any $s^{\\prime}\\in\\mathcal{S}$ with $P_{h}(s^{\\prime}\\mid s,a)>0$ that $\\hat{C}_{h+1}^{\\pi}(s^{\\prime},v_{s^{\\prime}})<\\infty$ . Thus, the induction hypothesis implies that $\\hat{V}_{h+1}^{\\pi}(s^{\\prime},\\hat{v}_{s^{\\prime}})\\:\\geq\\:\\hat{v}_{s^{\\prime}}(1\\,-\\,\\delta)^{(S+1)(H-h)}$ for any such ", "page_idx": 26}, {"type": "text", "text": "$s^{\\prime}\\in\\mathcal{S}$ . By the policy evaluation equations, we see that, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{V}_{h}^{\\pi}(s,\\hat{v})=r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\hat{V}_{h+1}^{\\pi}(s^{\\prime},\\hat{v}_{s^{\\prime}})}\\\\ &{\\qquad\\qquad\\geq r_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)\\hat{v}_{s^{\\prime}}(1-\\delta)^{(S+1)(H-h)}}\\\\ &{\\qquad\\geq\\sigma_{h,\\hat{\\mathcal{V}}}^{s,a}(1,r_{h}(s,a))(1-\\delta)^{(S+1)(H-h)}}\\\\ &{\\qquad\\geq\\hat{\\sigma}_{h,\\hat{\\mathcal{V}}}^{s,a}(1,r_{h}(s,a))(1-\\delta)^{(S+1)(H-h)}}\\\\ &{\\qquad\\geq\\hat{\\sigma}(1-\\delta)^{S+1}(1-\\delta)^{(S+1)(H-h)}}\\\\ &{\\qquad=\\hat{v}(1-\\delta)^{(S+1)(H-h+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first inequality used the induction hypothesis. The second inequality used the fact that the rewards are non-negative. The third inequality used Lemma 10. The fourth inequality used the fact that by definition of $\\hat{A}_{h}(s,\\hat{v})$ and $\\kappa$ $\\hat{\\sigma}_{h,\\hat{\\mathbf{v}}}^{s,a}(1,r_{h}(s,a))\\geq\\kappa(\\hat{v})=\\hat{v}(1-\\delta)^{S+1}$ . ", "page_idx": 27}, {"type": "text", "text": "This completes the proof. ", "page_idx": 27}, {"type": "text", "text": "E.12 Proof of Theorem 3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "text", "text": "Correctness. If $\\hat{C}_{1}^{*}(s_{0},v)\\,>\\,B$ for all $\\hat{v}\\,\\in\\,\\hat{\\mathcal{V}}$ , then $C_{M}^{*}\\,>\\,B$ since otherwise we would have $\\hat{C}_{1}^{*}(s_{0},\\lfloor v\\rfloor_{\\mathcal{G}})\\le\\bar{C}_{1}^{*}(s_{0},v)\\le C_{M}^{*}\\le B$ by Lemma 11. Thus, if Algorithm 5 outputs \u201cinfeasible\u201d it is correct. ", "page_idx": 27}, {"type": "text", "text": "On the other hand, suppose that there exists some $\\hat{v}\\in\\hat{\\mathcal{V}}$ for which $\\hat{C}_{1}^{*}(s_{0},\\hat{v})\\leq B$ . By standard MDP theory, we know that since $\\pi\\in\\Pi^{D}$ is a solution to $\\hat{M}$ , it must satisfy the optimality equations. In particular, $\\hat{C}_{1}^{\\pi}(s_{0},\\hat{v})=\\hat{C}_{1}^{*}(s_{0},v)\\le B$ . As in the proof of Theorem 1, since $C_{M}^{\\pi}=\\hat{C}_{1}^{\\bar{\\pi}}(s_{0},\\hat{v})$ , we see that there exists a $\\pi\\in\\Pi^{D}$ for which $C_{M}^{\\pi}\\le B$ and so $V_{M}^{*}>-\\infty$ . ", "page_idx": 27}, {"type": "text", "text": "Since $V_{M}^{*}$ is the value of some deterministic policy, Lemma 3 implies that $V_{M}^{*}\\;\\in\\;\\mathcal{V}$ . Thus, Lemma 12 implies that $\\hat{V}_{1}^{\\pi}(s_{0},\\lfloor V_{M}^{*}\\rfloor_{\\mathcal{G}})\\ \\geq\\ \\lfloor V_{M}^{*}\\rfloor_{\\mathcal{G}}\\,(1\\,-\\,\\delta)^{(S+1)H}\\ \\geq\\ V_{M}^{*}(1\\,-\\,\\delta)^{(S+1)H+1}\\ =$ $\\begin{array}{r}{V_{M}^{*}(1-\\frac{\\epsilon}{(S+1)H+1})^{(S+1)H+1}\\geq V_{M}^{*}(1-\\epsilon)}\\end{array}$ and $\\hat{C}_{1}^{\\pi}(s_{0},V_{M}^{*})\\leq C_{1}^{*}(s_{0},V_{M}^{*})\\leq B$ . Consequently, running $\\pi$ with initial state $\\bar{s}_{0}\\,=\\,(s_{0},\\lfloor V_{M}^{*}\\rfloor_{\\mathcal{G}})$ is an optimal solution to (CON). In either case, Algorithm 5 is correct. ", "page_idx": 27}, {"type": "text", "text": "Complexity. For the complexity claim, we observe that the running time of Algorithm 5 is $O(H S^{2}A|\\hat{\\nu}|^{2}|\\hat{\\mathcal{U}}|)$ . To bound $|\\hat{\\mathcal{V}}|$ , we observe that the number of vmin-scaled powers of $1/(1-\\delta)$ required to capture the range $[0,H r_{m a x}]$ is at most one plus the largest power needed, which is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{O(\\log_{1/(1-\\delta)}(\\frac{H r_{m a x}}{v_{m i n}}))=O(\\log(\\frac{H r_{m a x}}{v_{m i n}})/\\log(1/(1-\\delta)))}&{}\\\\ {=O(\\log(\\frac{H r_{m a x}}{v_{m i n}})/\\delta)}&{}\\\\ {=O(\\log(H S\\frac{H r_{m a x}}{p_{m i n}^{H}r_{m i n}})/\\epsilon)}&{}\\\\ {=O(H^{2}S\\log(\\frac{r_{m a x}}{p_{m i n}r_{m i n}})/\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by definition of $\\delta$ and the fact that $\\begin{array}{r}{\\log\\bigl(\\frac{1}{1-\\delta}\\bigr)\\;=\\;-\\log(1\\,-\\,\\delta)\\;\\geq\\;-\\log(e^{-\\delta})\\;=\\;\\delta}\\end{array}$ . Moreover, $|\\hat{\\mathcal{U}}|=O(|\\hat{\\mathcal{V}}|)$ . ", "page_idx": 27}, {"type": "text", "text": "We see that the range of the rounded sums is at widest $[0,2H r_{m a x}]$ since for any $t+1$ rounding non-negative sums is at least 0 and, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Big[\\big\\lfloor r_{h}(s,a)+P_{h}(1\\mid s,a)\\hat{\\mathbf{v}}_{1}\\big\\rfloor_{\\mathcal{G}}+.\\,.\\,.\\,+P_{h}(t\\mid s,a)\\hat{\\mathbf{v}}_{t}\\Big]_{\\mathcal{G}}\\leq r_{h}(s,a)+\\sum_{s^{\\prime}=1}^{t}P_{h}(s^{\\prime}\\mid s,a)\\hat{\\mathbf{v}}_{s^{\\prime}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is at most $2H r_{m a x}$ . Then, the same analysis from before shows that the number of scaled powers of $1/(1-\\delta)$ needed to cover this interval is $O(|\\hat{\\mathcal{V}}|)$ . Thus, we see that ${\\cal O}(|\\hat{\\mathcal{V}}|^{2}|\\hat{\\mathcal{U}}|)={\\cal O}(|\\hat{\\mathcal{V}}|^{\\bar{3}})=$ $O(H^{6}S^{3}\\log(\\frac{\\cdot\\,r_{m a x}}{p_{m i n}r_{m i n}})^{3}/\\epsilon^{3})$ implying that the total run time is $\\begin{array}{r}{O(H^{7}S^{5}A\\log(\\frac{\\dot{r}_{m a x}}{p_{m i n}r_{m i n}})^{3}/\\epsilon^{3})}\\end{array}$ as claimed. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "F Extensions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Stochastic Costs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Suppose each cost $c_{h}(s,a)$ is replaced with a cost distribution $C_{h}(s,a)$ . Here, we consider finitely supported cost distributions whose supports are at most $m\\in\\mathbb{N}$ . Then, instead of the agent occurring cost $c_{h}(s,a)$ upon taking action $a$ in state $s$ at time $h$ , the agent occurs a random cost $c_{h}\\sim C_{h}(s,a)$ . Generally, this necessitates histories be cost dependent, and so the policy evaluation equations become, ", "page_idx": 28}, {"type": "equation", "text": "$$\nV_{h}^{\\pi}(\\tau_{h})=r_{h}(s,a)+\\sum_{c^{\\prime},s^{\\prime}}C_{h}(c^{\\prime}\\mid s,a)P_{h}(s^{\\prime}\\mid s,a)V_{h+1}^{\\pi}(\\tau_{h},a,c^{\\prime},s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Cover MDP. This implicitly changes the definition of $\\nu$ since the histories considered in the definition must now include cost history. Since the cost distributions are finitely supported, $\\mathcal{V}$ remains a finite set. The main difference for $M$ is that the future value demands must depend on both the immediate cost and the next state. This slightly changes the action space: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\bar{A}_{h}(s,v)\\overset{\\mathrm{def}}{=}\\left\\{(a,\\mathbf{v})\\in A\\times\\mathcal{V}^{m\\times S}\\mid r_{h}(s,a)+\\sum_{c^{\\prime},s^{\\prime}}C_{h}(c^{\\prime}\\mid s,a)P_{h}(s^{\\prime}\\mid s,a)v_{c^{\\prime},s^{\\prime}}\\geq v\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bellman Updates. In order to solve $\\bar{M}$ using Algorithm 4, we must extend the definition of TSR to also be recursive in the immediate costs. The key difference of the TSRC condition is that $g$ \u2019s recursion is now two dimensional. ", "page_idx": 28}, {"type": "text", "text": "Definition 11 (TSRC). We call a criterion $C$ time-space-cost-recursive (TSRC) if $C_{M}^{\\pi}=C_{1}^{\\pi}(s_{0})$ where $C_{H+1}^{\\pi}(\\cdot)=\\mathbf{0}$ and for any $h\\in[H]$ and $\\tau_{h}\\in\\mathcal{H}_{h}$ letting $s=s_{h}(\\tau_{h})$ and $a=\\pi_{h}(\\tau_{h})$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nC_{h}^{\\pi}(\\tau_{h})=c_{h}(s,a)+f\\left(\\left(C_{h}(c^{\\prime}\\mid s,a),P_{h}(s^{\\prime}\\mid s,a),C_{h+1}^{\\pi}\\left(\\tau_{h},a,c^{\\prime},s^{\\prime}\\right)\\right)_{c^{\\prime},s^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the above, $c^{\\prime}\\in C_{h}(s,a)$ and $s^{\\prime}\\in P_{h}(s,a)$ . We now require that $f$ be computable in $O(m S)$ time. We also require that the $f$ term above is equal to $g_{h}^{\\tau_{h},a}(1,1)$ , where, $g_{h}^{\\tau_{h},a}(m+1,1)=0$ , $g_{h}^{\\tau_{h},a}(k,S+1)=g_{h}^{\\tau_{h},a}(k+1,1)$ , and, ", "page_idx": 28}, {"type": "equation", "text": "$$\ng_{h}^{\\tau_{h},a}(k,t)=\\alpha\\left(\\beta\\left(C_{h}(c_{k}\\mid s,a),P_{h}(t\\mid s,a),C_{h+1}^{\\pi}\\left(\\tau_{h},a,t\\right)\\right),g_{h}^{\\tau_{h},a}(k,t+1)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the above, we assume $c_{k}$ is the $k$ th supported cost of $C_{h}(s,a)$ . Again, both $\\alpha,\\beta$ can be computed in $O(1)$ time, but now $\\alpha(\\beta(y,\\cdot),x)=x$ whenever $0\\in y$ . ", "page_idx": 28}, {"type": "text", "text": "Our examples from before also carry over to the stochastic cost setting. ", "page_idx": 28}, {"type": "text", "text": "Proposition 5 (TSCR examples). The following classical constraints can be modeled by a TSCR cost constraint. ", "page_idx": 28}, {"type": "text", "text": "1. (Expectation Constraints) We capture these constraints by defining $\\alpha(x,y)\\stackrel{d e f}{=}x+y$ and $\\beta(x,y,z)\\stackrel{d e f}{=}x y z.$   \n2. (Almost Sure Constraints) We capture these constraints by defining $\\alpha(x,y)\\stackrel{d e f}{=}\\operatorname*{max}(x,y)$ and $\\beta(x,y,z)\\stackrel{d e f}{=}[x>0\\land y>0]z$ .   \n3. (Anytime Constraints) We capture these constraints by defining $\\alpha(x,y)\\ \\stackrel{d e f}{=}\\ \\operatorname*{max}(0,$ $\\operatorname*{max}(x,y))$ and $\\beta(x,y,z)\\stackrel{d e f}{=}[x>0\\land y>0]z$ . ", "page_idx": 28}, {"type": "text", "text": "We can then modify our approximate recursion from before. ", "page_idx": 28}, {"type": "text", "text": "Definition 12. We define, $\\hat{g}_{h,v}^{s,a}(m+1,1,u)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\chi_{\\{u\\geq v\\}}$ , $\\hat{g}_{h,v}^{s,a}(k,S+1,u)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\hat{g}_{h,v}^{s,a}(k+1,1,u)$ and for $t\\leq S$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{g}_{h,\\hat{v}}^{s,a}(k,t,u)\\overset{\\mathrm{def}}{=}\\underset{v_{k,t}\\in\\mathcal{V}}{\\operatorname*{min}}\\alpha\\Big(\\beta\\left(C_{h}(c_{k}\\mid s,a),P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{k,t}\\right)\\right),}\\\\ {\\hat{g}_{h,v}^{s,a}\\left(k,t+1,\\lfloor u+C_{h}(c_{k}\\mid s,a)P_{h}(t\\mid s,a)v_{k,t}\\rfloor_{\\mathcal{G}}\\right)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "nAepepdr toox islmigathitolyn .moLdaifstyl yo, uor urro ruonudnidnign gf unnocwti ooncsc.u rT ehrer omr aoivne rc htiamneg,e  sipsa cwee,  aunsed $\\delta\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{\\epsilon}{H(m S\\!+\\!1)\\!+\\!1}}$ H(mS+1)+1 .m pWley also further relax our lower bounds to $\\kappa(v)\\ {\\stackrel{\\mathrm{def}}{=}}\\ v_{-}-\\delta(m S+1)$ and $\\kappa\\ {\\stackrel{\\mathrm{def}}{=}}\\ v(1-\\delta)^{m S+1}$ respectively. Our running times correspondingly will have $m^{3}$ terms now. ", "page_idx": 29}, {"type": "text", "text": "F.2 Infinite Discounting ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Approximations. Since we focus on approximation algorithms, the infinite discounted case can be immediately handled by using the idea of effective horizon. We can treat the problem as a finite horizon problem where the finite horizon $H$ defined so that $\\begin{array}{r}{\\sum_{h=H}^{\\infty}\\gamma^{h-1}r_{m a x}\\leq\\'\\epsilon^{\\prime}}\\end{array}$ . By choosing $\\epsilon^{\\prime}$ and $\\epsilon$ small enough, we can get traditional value approximations. The discounting also ensures the effective horizon $H$ is polynomially sized implying efficient computation. We just need to assume that 0-cost actions are always available so that the policy can guarantee feasibility after the effective horizon has passed. ", "page_idx": 29}, {"type": "text", "text": "Hardness. We also note that all of the standard hardness results concerning deterministic policy computation carries over to the infinite discounting case even if all quantities are stationary. ", "page_idx": 29}, {"type": "text", "text": "F.3 Faster Approximations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We can significantly improve the running time of our FPTAS. The main guarantee is given in Corollary 1. They key step is to modify Algorithm 3 to use the differences instead of the sums. It is easy to see that this is equivalent since, ", "page_idx": 29}, {"type": "equation", "text": "$$\nr_{h}(s,a)+\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\ge v\\iff v-\\sum_{s^{\\prime}}P_{h}(s^{\\prime}\\mid s,a)v_{s^{\\prime}}\\le r_{h}(s,a).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since rounding down the differences make them larger, it becomes harder to be below $r_{h}(s,a)$ . Consequently, we now interpret $\\kappa$ as an upper bound for $r_{h}(s,a)$ instead of a lower bound on $v$ The approximate dynamic programming method based on differences can be seen in Definition 13. ", "page_idx": 29}, {"type": "text", "text": "Definition 13. Fix a rounding down function $\\lfloor\\cdot\\rfloor_{\\mathcal{G}}$ and upper bound function $\\kappa$ . For any $h\\in[H]$ , $s\\in S$ , $v\\in\\mathcal{V}$ , and $u\\in\\mathbb R$ , we define, $\\hat{g}_{h,v}^{s,a}(S+1,u)=\\chi_{\\{u\\leq\\kappa(r_{h}(s,a))\\}}$ and for $t\\leq S$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{g}_{h}^{s,a}(t,u)\\overset{\\mathrm{def}}{=}\\operatorname*{min}_{v_{t}\\in\\mathcal{V}}\\alpha\\left(\\beta\\left(P_{h}(t\\mid s,a),\\bar{C}_{h+1}^{*}\\left(t,v_{t}\\right)\\right),\\hat{g}_{h}^{s,a}(t+1,\\lfloor u-P_{h}(t\\mid s,a)v_{t}\\right)\\rfloor_{\\mathcal{G}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The recursion is nearly identical to the originally, and unsurprisingly, it retains the same theoretical guarantees but in the reverse order. The guarantees can be seen in Lemma 13, which is straightforward to prove following the approach in the proof of Lemma 1. ", "page_idx": 29}, {"type": "text", "text": "Lemma 13. For any $t\\in[S+1]$ and $u\\in\\mathbb{R}$ , we have that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\hat{g}_{h}^{s,a}(t,u)=\\displaystyle\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}^{S-t+1}}\\hat{g}_{h,\\hat{\\mathbf{v}}}^{s,a}(t)}\\\\ {s.t.\\quad\\tilde{\\sigma}_{h,\\mathbf{v}}^{s,a}(t,u)\\leq\\kappa(r_{h}(s,a)),}\\\\ {\\tilde{\\sigma}_{h,\\mathbf{v}}^{s,a}(t,u)\\overset{d e f}{=}\\left\\lfloor[u-P_{h}(t\\mid s,a)v_{t}\\rfloor_{\\mathcal{G}}-\\ldots-P_{h}(S\\mid s,a)v_{S}\\right\\rfloor_{\\mathcal{G}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The difference version so far does not help us get faster algorithms. The key is in how we use it. Since the base case of the recursion is $r_{h}(s,a)$ and not $v$ , we can compute the approximate bellman update for all $v$ \u2019s simultaneously. This ends up saving us a factor of $\\vert\\nu\\vert$ that we had in the original Algorithm 4. The new algorithm is defined in Algorithm 6. The inputs to the recursion are define in Definition 14. ", "page_idx": 29}, {"type": "text", "text": "Input: $(\\bar{M},\\bar{C})$   \n1: $\\hat{C}_{H+1}^{*}(s,v)\\gets\\chi_{\\{v\\leq0\\}}$ for all $(s,v)\\in\\bar{S}$   \n2: for $h\\leftarrow H$ down to 1 do   \n3: for $s\\in S$ do   \n4: for $a\\in{\\mathcal{A}}$ do   \n5: $\\hat{g}_{h}^{s,a}(S+1,u)\\gets\\chi_{\\{u\\leq\\kappa(r_{h}(s,a))\\}}\\;\\forall u\\in\\hat{\\mathcal{U}}_{h}^{s,a}$   \n6: for $t\\gets S$ down to 1 do   \n7: for $u\\in\\hat{\\mathcal{U}}_{h}^{s,a}$ do   \n8: v\u02c6t,ag\u02c6sh, a(t, u) \u2190(DIF)   \n9: for $v\\in\\mathcal{V}$ do   \n10: $\\begin{array}{r l}&{a^{*},\\hat{C}_{h}^{*}(s,v)\\gets\\operatorname*{min}_{a\\in\\mathcal{A}}c_{h}(s,a)+\\hat{g}_{h}^{s,a}(1,v)}\\\\ &{\\pi_{h}(s,v)\\gets a^{*}}\\end{array}$   \n11:   \n12: return $\\pi$ and $\\hat{C}^{*}$ ", "page_idx": 30}, {"type": "text", "text": "Definition 14. For any $h\\in[H]$ , $s\\in S$ , and $a\\in A$ , we define $\\hat{\\mathcal{U}}_{h}^{s,a}(1)\\stackrel{\\mathrm{def}}{=}\\mathcal{V}$ and for any $t\\in[S]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{U}}_{h}^{s,a}(t+1)\\stackrel{\\mathrm{def}}{=}\\bigcup_{v_{t}\\in\\mathcal{V}}\\bigcup_{\\hat{\\sigma}\\in\\hat{\\mathcal{U}}_{h}^{s,a}(t)}\\left\\{\\lfloor\\hat{\\sigma}-P_{h}(t\\mid s,a)v_{t}\\rfloor_{\\mathcal{G}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proposition 6. The running time of Algorithm $^ \u1e0a 6 \u1e0c$ is $O(H S^{2}A|\\mathcal{V}|\\hat{\\sigma})$ . ", "page_idx": 30}, {"type": "text", "text": "Corollary 1 (Running Time Improvements). Using Algorithm $6,$ , the running time of our additive$\\begin{array}{r}{O(H^{5}S^{4}A\\log(\\frac{r_{m a x}}{r_{m i n}p_{m i n}})^{2}/\\epsilon^{2})}\\end{array}$ $O(\\bar{H}^{5}S^{4}A r_{m a x}^{2}/\\epsilon^{2})$ , and the running time of our relative-FPTAS becomes ", "page_idx": 30}, {"type": "text", "text": "Approximation Details. Although the running times our clear from removing the factor of $|\\hat{V}|$ , we need to slightly alter our approximation schemes for this to work. First, we need to use $\\kappa(r_{h}(s,a))\\stackrel{\\mathrm{def}}{=}$ $r_{h}(s,a)+\\delta$ for the additive approximation. The proof from before goes through almost identically. However, for the relative approximation, no choice of upper bound can ensure enough feasibility. Thus, we simply use $\\kappa(r_{h}(s,a))\\ \\stackrel{\\mathrm{def}}{=}\\ r_{h}(s,a)$ and apply a different analysis. We also note that technically, differences can become negative. To deal with this the relative rounding function should simply send any negative number to 0: $\\lfloor-x\\rfloor_{\\mathcal{G}}\\ \\overset{\\mathrm{def}}{=}\\ 0$ . The analysis is mostly the same, but the feasibility statement must be slightly modified. ", "page_idx": 30}, {"type": "text", "text": "Lemma 14. Suppose all rewards are non-negative. For any $h~\\in~[H~+~1]$ and $(s,v)\\ \\in\\ \\bar{S}$ , $\\hat{C}_{h}^{\\ast}(s,\\lfloor v(1-\\delta)^{H-h+1}\\rfloor_{\\mathcal{G}})\\leq\\bar{C}_{h}^{\\ast}(s,v)$ . ", "page_idx": 30}, {"type": "text", "text": "The idea is that since no fixed upper bound can capture arbitrary input values, we simply input relative values. Then, the feasibility part of Lemma 11 goes through as before. The proofs mostly remain the same, but the rounding must again change. We must now start at the smaller $v_{m i n}$ that is the original $v_{m i n}$ scaled by a factor of $(1-\\delta)^{H}$ to ensure that $\\left\\lfloor V_{M}^{*}(1-\\delta)^{H}\\right\\rfloor$ is in $\\hat{V}$ . This makes $\\hat{V}$ larger, but not by too much as we argued in previous analyses. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We believe the claims made are accurate. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We point out our assumed reward bounds and note they are unavoidable under worst-case analysis. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper provides all proofs in the Appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We beleive our paper follows the code of ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss in the introduction and conclusions potential positive impacts of our work. We do not believe there are any direct negative impacts. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any releasable materials. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not include any experiments. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not use any crowdsourcing nor human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper is purely theoretical and does not use any crowdsourcing nor human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]