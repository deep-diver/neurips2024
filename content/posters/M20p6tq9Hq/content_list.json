[{"type": "text", "text": "Identifiability Guarantees for Causal Disentanglement from Purely Observational Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ryan Welch\u2217 Jiaqi Zhang\u2217 Massachusetts Institute of Technology LIDS, Massachusetts Institute of Technology Broad Institute of MIT and Harvard Broad Institute of MIT and Harvard ", "page_idx": 0}, {"type": "text", "text": "Caroline Uhler LIDS, Massachusetts Institute of Technology Broad Institute of MIT and Harvard ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal disentanglement aims to learn about latent causal factors behind data, holding the promise to augment existing representation learning methods in terms of interpretability and extrapolation. Recent advances establish identifiability results assuming that interventions on (single) latent factors are available; however, it remains debatable whether such assumptions are reasonable due to the inherent nature of intervening on latent variables. Accordingly, we reconsider the fundamentals and ask what can be learned using just observational data. ", "page_idx": 0}, {"type": "text", "text": "We provide a precise characterization of latent factors that can be identified in nonlinear causal models with additive Gaussian noise and linear mixing, without any interventions or graphical restrictions. In particular, we show that the causal variables can be identified up to a layer-wise transformation and that further disentanglement is not possible. We transform these theoretical results into a practical algorithm consisting of solving a quadratic program over the score estimation of the observed data. We provide simulation results to support our theoretical guarantees and demonstrate that our algorithm can derive meaningful causal representations from purely observational data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Advances in representation learning play a pivotal role in the application of machine learning across various fields, including natural language processing, computer vision, and life sciences (c.f., [6, 52]). The emerging field of causal disentanglement holds the promise to augment such advances by identifying and learning some aspects about the latent causal factors behind data [39, 18]. These latent causal factors have been shown to improve the interpretability of high-level concepts behind complex high-dimensional data [23, 32, 60, 53] and enable extrapolation to predict how novel interventions will affect the data [57, 59, 36]. ", "page_idx": 0}, {"type": "text", "text": "In pursuit of causal disentanglement, two critical questions are: (1) to theoretically understand to what extent the latent causal factors are identifiable, and (2) to algorithmically design efficient methods to learn these factors with finite samples. Despite the recent surge of interest in this area, these questions remain difficult given the inherent challenges of both disentanglement and causal discovery. In the disentanglement literature, the latent factors are assumed to be independent, and it is known that identifying them is not possible without further knowledge on the data-generating process [15]. Relaxing the independence assumption, causal disentanglement considers potentially related latent factors and aims to discover not only the latent factors but also their latent causal relations. Since this extends disentanglement, the latent factors are unidentifiable without additional information. Furthermore, learning causal relations is notoriously challenging as the number of variables grows: the underlying structure is generally not unique [4], and it is computationally and sample inefficient to learn complex causal graphs [12, 51]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To overcome these difficulties, a trend in recent works has been to consider having access to interventional data, where a common assumption is to assume that interventions on all single latent factors are available [43, 2, 47, 59, 50, 48, 9]. Although a goal of causal disentanglement is to be able to control individual latent factors, it is debatable whether assuming existence of direct interventions on latent factors is reasonable [36, 7, 49, 3], since one can argue that direct interventions on (single) latent factors make these factors non-latent. Furthermore, it might be infeasible to perform interventions on (some of) the factors due to ethical or cost reasons. As a consequence, it is important to understand what can be achieved solely based on observational data. ", "page_idx": 1}, {"type": "text", "text": "In our work, we consider causal disentanglement from purely observational data. The key idea behind our approach is to utilize asymmetries in the joint distribution of the latent factors. In particular, we consider latent factors that are generated by an unknown nonlinear causal model with additive Gaussian noises, from which we obtain observations after an unknown linear mixing. Nonlinear models with additive Gaussian noises have been a popular choice in the causal discovery literature due to their flexibility, intriguing identifiability properties (in the fully observed setting), and benign statistical sample complexities [31, 38, 35, 61]. These models imply asymmetric relationships between causes and effects, which can be utilized to distinguish causal directions. Beyond their theoretical properties, these models are commonly chosen to represent real world causal systems, such as gene regulatory networks [16], given their ability to fit non-parametric relationships. ", "page_idx": 1}, {"type": "text", "text": "Contributions and Organization. We define nonlinear additive Gaussian noise models in the context of causal disentanglement in Section 2. We provide a precise characterization of latent factors that can be identified in such models, with purely observational data and no graphical restrictions. In particular, we show that the latent variables can be identified up to a layer-wise transformation that is consistent with the underlying causal ordering, and that further disentanglement is not possible. These results are provided in Section 3. We transform these theoretical results into practical algorithms in Section 4 by building upon recent successes of combining score matching and causal discovery [35, 29] to devise a method that solves a quadratic program over the score estimation of the observed data. The resulting algorithm enjoys efficiency and flexibility to be combined with any existing off-the-shelf score estimation method. We demonstrate our results empirically with simulations in Section 5, and conclude with a discussion in Section 6. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Causal disentanglement. Previous works in causal disentanglement have mostly considered varying assumptions on: the available data, the underlying causal model of the latent factors, and the mixing function between latent factors and observed data. Assuming the availability of interventional data, [24, 43, 9] established results for parametric causal models, whereas [47, 2, 59, 50, 17] studied non-parametric causal models. Most of these works assume linear mixing (or a special case of polynomial mixing that can be easily reduced to linear functions), with the exception of [9, 50, 17], where stronger assumptions on either the parametric causal model or more interventions are required to compensate for the general mixing functions. Prior to these works, [1, 8] established results assuming counterfactual data, which usually leads to stronger identifiability as one can now contrast counterfactual pairs. ", "page_idx": 1}, {"type": "text", "text": "Few recent works considered identifiability without interventions [45, 58, 56]. These works typically assume that (parts of) the latent factors can be observed after multiple different mixing functions. In the case where only one observational dataset is available, which is the setting of this paper, previous works have obtained results assuming both parametric models as well as additional structural restrictions on the mixing function [11, 19, 54, 55, 21]. Such structural restrictions refer to constraints on the set of latent variables that determine each observed variable, which is distinct from functional restrictions on the mixing function such as linearity. An example of such restrictions is the pure child assumption [40, 14], specifying that each observed variable has only one latent parent. To the best of our knowledge, our work is the first to establish identifiability guarantees of causal disentanglement in the purely observational setting without imposing any structural assumptions over the mixing function. We summarize these comparisons to prior works in Table 1. ", "page_idx": 1}, {"type": "table", "img_path": "M20p6tq9Hq/tmp/16e93cfa00b84d17b7196a20a984c0a818320faa05d077e6b13d5675c0c0caa0.jpg", "table_caption": ["Table 1: Comparison of our results to prior works on causal disentanglement. For the latent model, $L$ stands for linear mechanisms whereas NL stands for nonlinear mechanisms; $G$ stands for Gaussian noise whereas NG stands for non-Gaussian noise; Discrete refers to discrete causal variables. Here, we summarize the identifiability results in terms of latent causal graph identification. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We additionally recognize that identifiability of latent factors from purely observational data has been considered outside of causal disentanglement [35, 20]. However, these results do not extend to the setting considered in this work, given the assumed data generating processes to do encapsulate the causal graph. ", "page_idx": 2}, {"type": "text", "text": "Score matching in causal discovery. Since our algorithm builds upon discovery methods using score matching, we briefly review these approaches. Works in this direction have mainly focused on causal discovery when all causal variables are observable in identifiable paramteric causal models such as nonlinear additive Gaussian noise or additive non-Gaussian noise models [35, 29, 34, 37, 28]. These methods first learn a topological ordering of the causal variables using the second-order derivative of the log-likelihood estimated from score matching. They then apply regression based DAG pruning techniques [10, 26] to retrieve the full causal structure. We note that these works do not inherently extend to causal disentanglement, for they assume direct access to the causal variables that disentanglement intends to learn. ", "page_idx": 2}, {"type": "text", "text": "Expanding these ideas to causal disentanglement is difficult, since we do not observe the latent factors and can only estimate the log-likelihood of the observed variables. Surprisingly, our theory suggests a simple principle to obtain meaningful estimates of the latent factors from the log-likelihood of the observed variables. Moreover, we show that a simple quadratic program can be used to implement this principle, which leads to an efficient algorithm borrowing strength from both nonlinear optimization and machine learning. ", "page_idx": 2}, {"type": "text", "text": "Our principle for identifiability directly expands the main result from [35], where variance properties on the diagonal elements of the Jacobian over the score of causal variables are used to derive a topological ordering. While we utilize this result, it is not sufficient for disentanglement given the aforementioned Jacobian can only be determined up to an unknown quadratic form when the causal variables are unobserved. Therefore, our principle additionally relies on properties of the entire Jacobian matrix, which we present in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "Additionally, we recognize the inherent difficulties of both non-convex optimization and second-order score estimation essential for modern score matching methods, which we discuss further in Section 4.1 and Section 5.1 respectively. ", "page_idx": 2}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now formally define the causal disentanglement problem and introduce relevant definitions. We consider the observed variables $\\boldsymbol{X}=\\left(\\boldsymbol{X}_{1},...,\\boldsymbol{X}_{d}\\right)^{\\top}\\in\\mathbb{R}^{d\\times1}$ as generated from the latent causal factors $Z=\\left(Z_{1},...,Z_{n}\\right)^{\\top}\\in\\mathbb{R}^{n\\times1}$ via an unknown invertible linear mixing. We do not assume that the latent dimension $n$ is known a priori, but rather can be learned as given by the principle presented in Lemma 1 of [59]. These latent factors follow a joint distribution $p(\\cdot)$ , which factorizes according to an unknown directed acyclic graph (DAG) $\\mathcal{G}$ . We summarize the setup in the following assumption. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Linear mixing). Our data-generating process can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\nX=H\\cdot Z,\\qquad Z\\sim p(Z)=\\prod_{i=1}^{n}p(Z_{i}|Z_{p a(i)}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $H\\in\\mathbb{R}^{d\\times n}$ has full column rank and $p a(i)$ denotes the parents of node $i$ in $\\mathcal{G}$ . ", "page_idx": 3}, {"type": "text", "text": "We assume linear mixing as it is essential for our theoretical guarantees presented in Section 3.2. However, our results also extend to settings where the true mixing function can be reduced to a linear map, such as in the case of a special class of polynomials [2, 59]. For the distribution of $Z$ , we consider nonlinear additive Gaussian noise models as follows. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Nonlinear additive Gaussian noise model.). The factorization term in the joint distribution over $Z$ is specified by ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ_{i}=f_{i}(Z_{p a(i)})+\\mathcal{E}_{i},\\qquad\\forall i\\in[n],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f=\\{f_{i}:i\\in[n]\\}$ are twice continuously differentiable, non-linear2functions that capture the dependence of $Z_{i}$ on its parents, and $\\mathcal{E}=\\{\\mathcal{E}_{i}:i\\in[n]\\}$ denote exogenous noise variables, which are mutually independent and mean-zero Gaussians, i.e., $\\dot{\\mathcal{E}}_{i}\\sim\\mathcal{N}(0,\\sigma_{i}^{\\breve{2}})$ . ", "page_idx": 3}, {"type": "text", "text": "We use $p_{X}(\\cdot)$ to denote the induced distribution over the observed variables $X$ . We consider causal disentanglement from purely observational data, where we only have access to a dataset consisting of samples from $p_{X}(\\cdot)$ . Our goal is to learn the most about $Z$ (or equivalently, $\\mathcal{E},\\mathcal{G}$ , and $f$ ) using this dataset. We additionally note that this problem has also been called causal representation learning in literature. Figure 1 illustrates the described setup. ", "page_idx": 3}, {"type": "image", "img_path": "M20p6tq9Hq/tmp/e6cec1fbc03746473a483ff7f1b6d8658c653e477e3e7845d27013b80dd97e6b.jpg", "img_caption": ["Figure 1: The considered data-generating pro- Figure 2: Layers of the causal DAG. A latent cess. The latent variables $Z$ follow a nonlinear causal variable is contained in $\\ l a y e r(k)$ if its longest model with additive Gaussian noises. We observe path to a leaf node is $k$ . them after an unknown linear mixing. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Estimators. We denote generic estimators of $Z$ and $\\mathcal{E}$ from $X$ by $\\hat{Z}(X):\\mathbb{R}^{d}\\to\\mathbb{R}^{n}$ and $\\widehat{\\mathcal{E}}(X):$ $\\mathbb{R}^{d}\\to\\mathbb{R}^{n}$ respectively. In our setup, these estimators are constructed by learning the inverse of the unknown mixing matrix $H$ . We denote a valid estimate of this mixing matrix by $\\hat{H}\\in\\mathbb{R}^{d\\times n}$ , and its Moore-Penrose inverse by $\\hat{H}^{\\dagger}\\,=\\,\\hat{H}^{\\top}\\,\\cdot\\,(\\hat{H}\\hat{H}^{\\top})^{-1}$ . To obtain an estimate of $Z$ , we use $\\hat{Z}(X)=\\hat{H}^{\\dagger}\\cdot X$ . For simplicity, we denote the transformation from the estimated latent ${\\hat{Z}}(X)$ to the true latent factors $Z$ by the matrix $\\beta\\in\\mathbb{R}^{n\\times n}$ , where $\\beta=H^{\\dagger}\\cdot\\hat{H}$ and $Z=\\beta\\cdot{\\hat{Z}}(X)$ . ", "page_idx": 3}, {"type": "text", "text": "Graph notation. We use $c h(i),a n(i)$ and $d e(i)$ to denote the children, ancestors and descendants of node $i$ in $\\mathcal{G}$ , respectively. Node $i$ is called a root node if $a n(i)=\\emptyset$ , and a leaf node if $d e(i)=\\mathcal{O}$ . We define the $k^{t h}$ layer of $\\mathcal{G}$ , denoted by $\\ l a y e r(k)$ , to be the set of all nodes whose longest path to a leaf node is $k$ . Figure 2 illustrates this concept. With a slight abuse of notation, we will interchangeably use $Z_{i}\\in l a y e r(k)$ to denote $i\\in l a y e r(k)$ . ", "page_idx": 3}, {"type": "text", "text": "3 Identifiability Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main theoretical results. We start by providing a precise characterization of latent factors that are identifiable in Section 3.1. We then demonstrate identifiability by providing a constructive proof in Section 3.2. Counterexamples showing that further disentanglement is not possible and our results cannot be strengthened are given in Section 3.3. Detailed proofs are deferred to Appendix A. Throughout this section, we consider the infinite-data regime where enough samples are obtained to exactly determine the observational distribution $p_{X}(\\cdot)$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Layer-wise Transformations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For each latent causal factor $Z_{i}$ , we show that its identifiability is dependent on the layer of the corresponding node. Specifically, we show that $Z_{i}\\;\\in\\;l a y e r(k)$ can be identified up to a linear combination of all variables in $l a y e r(k)\\cup l a y e r(k+1)\\cup\\cdot\\cdot\\cdot\\cup l a y e r(r)$ , where $r$ denotes the top most layer. The formal definition is as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Identifiability up to upstream layers). The latent causal variables $Z$ are identifiable up to upstream layers if it is possible to learn $\\hat{Z}(X)$ from $p_{X}(\\cdot)$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\hat{Z}}(X)=P_{\\pi}\\cdot C\\cdot Z,\\qquad\\forall Z\\in\\mathbb{R}^{n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P_{\\pi}\\,\\in\\,\\mathbb{R}^{n\\times n}$ is a permutation matrix, and $C\\,\\in\\,\\mathbb{R}^{n\\times n}$ is a constant matrix with non-zero diagonal entries and $[C]_{i,j}=O$ for all $i,j$ such that $i\\in l a y e r(k)$ and $j\\!\\in\\cup\\!_{l\\leq k}l a y e r(l)$ . ", "page_idx": 4}, {"type": "text", "text": "This identifiability notion implies that each causal variable can be learned up to a linear combination that does not depend on its descendants. Intuitively, this implies that variables that are more upstream in the underlying causal DAG can more easily be identified. In particular, the root nodes (i.e., the most upstream causal factors) can be identified up to a linear transformation of themselves. ", "page_idx": 4}, {"type": "text", "text": "Beyond this $Z$ -based notion of identifiability, we can further disentangle the exogenous noise variables up to a transformation that depends only on its own layer. The formal definition is as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Identifiability up to layers). The exogenous noise variables $\\mathcal{E}$ are identifiable up to layers if it is possible to learn $\\hat{\\mathcal{E}}(X)$ from $p_{X}(\\cdot)$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\hat{\\mathcal{E}}}(X)=P_{\\pi}\\cdot C\\cdot{\\mathcal{E}},\\qquad\\forall{\\mathcal{E}}\\in\\mathbb{R}^{n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P_{\\pi}\\,\\in\\,\\mathbb{R}^{n\\times n}$ is a permutation matrix, and $C\\,\\in\\,\\mathbb{R}^{n\\times n}$ is a constant matrix with non-zero diagonal entries and $[C]_{i,j}=O$ for all $i,j$ such that $i\\in l a y e r(k)$ and $j\\not\\in l a y e r(k)$ . ", "page_idx": 4}, {"type": "text", "text": "Next, we prove these notions of identifiability in a constructive way using the score function of $p_{X}(\\cdot)$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Identification via Score Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our analysis will rely on the score function of the observational distribution of $X$ , denoted by ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{X}(x)=\\nabla_{x}\\log p_{X}(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as well as its Jacobian matrix whose $i j^{t h}$ entry is given by $[J_{X}(x)]_{i j}=\\nabla_{x_{i}}\\nabla_{x_{j}}\\log p_{X}(x)$ . Since $X$ and $Z$ are related through a linear transformation, we can easily write out the closed form for both the score and associated Jacobian of the latent variables $Z$ as follows. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Under Assumption $^{\\,I}$ , the score functions and associated Jacobian matrices over $X$ and $Z$ are related via the following transformations: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{Z}(z)=H^{\\top}s_{X}(x),\\qquad J_{Z}(z)=H^{\\top}J_{X}(x)H.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For an estimator ${\\hat{Z}}(X)={\\hat{H}}\\cdot X$ , we utilize Lemma 1 to obtain the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ_{\\hat{Z}}(\\hat{z})=\\hat{H}^{\\top}J_{X}(x)\\hat{H}=\\beta^{\\top}J_{Z}(z)\\beta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This shows that we can compute $J_{\\hat{Z}}$ once we estimate $\\hat{H}$ and $J_{X}$ from $p_{X}(\\cdot)$ , and that $J_{\\hat{Z}}$ relates to the Jacobian matrix over the true latent variables, $J_{Z}$ , via a quadratic form $J_{\\hat{Z}}=\\beta^{\\top}J_{Z}\\beta$ , where $\\beta$ is a product of the unknown $H^{\\dagger}$ and $\\hat{H}$ . ", "page_idx": 4}, {"type": "text", "text": "Under the nonlinear additive Gaussian noise model in Assumption 2, [35] demonstrated that the $i^{t h}$ diagonal element of $J_{Z}$ will have zero variance if and only if node $i$ is a leaf node in $\\mathcal{G}$ . Building on this result, we can derive a sufficient and necessary condition for when the $i^{t h}$ diagonal element of $J_{\\hat{Z}}$ will have zero variance involving the unknown matrix $\\beta$ as follows. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. The $i^{t h}$ diagonal element of $[J_{\\hat{Z}}(\\hat{z})]_{i i}$ has zero variance, i.e., Var $\\begin{array}{r}{\\left([J_{\\hat{Z}}(\\hat{z})]_{i i}\\right)=0,}\\end{array}$ , if and only if the $i^{t h}$ column of $\\beta$ has zero entries in every element corresponding to non-leaf nodes. ", "page_idx": 5}, {"type": "text", "text": "This result provides the intuition that leads to the principle for achieving identifiability. In particular, if we maximize the number of zero-variance terms in the diagonal elements of the estimated Jacobian $J_{\\hat{Z}}$ , then the unknown matrix $\\beta$ must have a maximum number of columns with zeros in all indices corresponding to non-leaf nodes. Since $Z={\\boldsymbol{\\beta}}\\cdot{\\hat{Z}}$ , we can derive the relation between $\\hat{Z}$ and $Z$ under this maximization, which we summarize in the following lemma. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3. If we learn $\\hat{H}$ by solving ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\hat{H}\\in\\mathbb{R}^{n}}{\\mathrm{min}}}&{\\left\\|\\mathrm{Var}\\Big(\\,\\mathrm{diag}(J_{\\hat{Z}}\\big(\\hat{H}^{\\dagger}x)\\big)\\Big)\\right\\|_{0},}\\\\ {\\mathrm{such~that}}&{\\mathrm{rank}(\\hat{H})=n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then it follows that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{Z}_{i}=\\left\\{\\operatorname*{linear}(Z_{n o n-l e a f})\\right.\\ \\ \\mathrm{i}f\\ \\ \\mathrm{Var}\\left(\\left[J_{\\hat{Z}}(\\hat{z})\\right]_{i i}\\right)\\neq0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the number of $i\\in[n]$ such that Var $\\big(\\big[J_{\\hat{Z}}(\\hat{z})\\big]_{i i}\\big)=0$ equals to the number of leaf nodes in $\\mathcal{G}$ . ", "page_idx": 5}, {"type": "text", "text": "It follows from this lemma that we can obtain representations of all non-leaf nodes as linear transformations of all non-leaf latent variables (i.e. layer(1) and above). In other words, we can disentangle the leaf nodes out from the non-leaf nodes. Given this identified linear transformation of the nonleaf nodes, we can iteratively apply Lemma 3 to prune representations of each variable as a linear combination of all variables in its own and upstream layers. This leads to our main theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Under Assumptions 1 and 2, the latent variables $Z$ are identifiable up to their upstream layers from purely observational data. ", "page_idx": 5}, {"type": "text", "text": "Importantly, this result holds without any structural restrictions on the mixing function or the latent causal DAG. It indicates that we can derive representations of latent factors free of all downstream variables, and that it is easier to disentangle the more upstream causal factors. ", "page_idx": 5}, {"type": "text", "text": "Building on Theorem 1, we can show a stronger notion of identifiability for the exogenous noise variables. Consider any $l a y e r(i)$ representation given by a linear combination of all variables in $l a y e r(i+1)\\cup\\cdot\\cdot\\cup l a y e r(r)$ , where $r$ denotes the top most layer. Then from the structural equations, it follows that this representation depends nonlinearly on the exogenous noise variables associated with $l a y e r(i+1)\\cup\\cdot\\cdot\\cup l a y e r(r)$ and linearly on the the exogenous noise variables associated with layer $(i)$ , which we denote by $\\mathcal{E}_{l a y e r(i)}$ . Thus, if we regress this representation on all upstream layer representations, e.g., using kernel regression, then the residual terms will equate to a linear combination of $\\mathcal{E}_{l a y e r(i)}$ . Performing this procedure over all layers $i$ , we can determine layer-wise transformations of $\\mathcal{E}$ , giving rise to the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Under Assumptions $^{\\,l}$ and 2, the exogenous noise variables $\\mathcal{E}$ are identifiable up to their layers from purely observational data. ", "page_idx": 5}, {"type": "text", "text": "3.3 Impossibility Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we show that further disentanglement is not possible. In particular, we cannot further disentangle the exogenous noise variables within any given layer. The following example illustrates this with two variables. Suppose the exogenous noise variables ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ are identified via two linear combinations denoted by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{E}}_{1}=a_{1}\\mathcal{E}_{1}+a_{2}\\mathcal{E}_{2},\\quad\\hat{\\mathcal{E}}_{2}=b_{1}\\mathcal{E}_{1}+b_{2}\\mathcal{E}_{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We only know that they are independent mean-zero Gaussian variables. However, any linear coefficients with $a_{1}b_{1}\\sigma_{1}^{2}+a_{2}b_{2}\\sigma_{2}^{2}=0$ satisfy $C o v(\\hat{\\mathcal{E}}_{1},\\hat{\\mathcal{E}}_{2})=a_{1}b_{1}\\sigma_{1}^{2}+a_{2}b_{2}\\sigma_{2}^{2}=0$ , which means $\\hat{\\mathcal{E}}_{1}$ and $\\hat{\\mathcal{E}}_{2}$ are independent. This indicates that $\\hat{\\mathcal{E}}_{1}$ and $\\hat{\\mathcal{E}}_{2}$ do not provide enough information to further disentangle ${\\mathcal{E}}_{1}$ or $\\mathcal{E}_{2}$ . In general, this impossibility result holds for arbitrary graphs. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Under Assumptions 1 and 2, the exogenous noise variables $\\mathcal{E}$ are generally unidentifiable beyond layer-wise transformation from observational data. ", "page_idx": 6}, {"type": "text", "text": "4 Algorithm for Layer Recovery ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now transition to developing practical algorithms to recover the guaranteed causal representations. Our approach consist of two steps: (1) solving for the representations of latent variables up to upstream-layer transformations, and (2) solving for representations of exogenous noise variables up to layer-wise transformations, where step 2 utilizes the output of step 1. ", "page_idx": 6}, {"type": "text", "text": "4.1 Step 1: Quadratic Programming on Estimated Scores ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The proof sketch in Section 3.2 provides a simple principle for causal disentanglement. It suggests that we can solve for the estimated mixing function, $\\hat{H}$ , at each iteration by maximizing the number of zero-variance terms in the Jacobian of the estimated latent score (i.e., Equation (1)). However, this rank-constrained optimization problem is discontinuous and non-convex, leading to an NP-hard problem [46]. Moreover, the objective function involves the $\\ell_{0}$ -norm of a vector of variance terms, which can be hard to optimize. ", "page_idx": 6}, {"type": "text", "text": "To resolve these difficulties, we reduce this optimization problem into a sequence of easier problems. Note that $V a r[J_{\\hat{Z}}(z)]_{i i}$ depends only on the $i^{t h}$ column of $\\hat{H}$ , which we denote as $[\\hat{H}]_{i}$ . It follows that we can solve for each column separately by solving for $[\\hat{H}]_{i}$ such that $V a r[J_{\\hat{Z}}(z)]_{i i}=0$ while not violating the rank constraint. Considering the finite-sample setting where we plug in the sample estimate for $V a r[J_{\\hat{Z}}(z)]_{i i}$ , this problem can be formulated as the following quadratically constrained quadratic program (QCQP): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{h\\in\\mathbb{R}^{n}}{\\operatorname*{min}}}&{0}\\\\ {\\mathrm{such\\,that}}&{h^{\\top}\\tilde{J}_{X}(x^{(m)})h=0,\\quad\\forall m\\in[N],}\\\\ &{h^{\\top}h=1,}\\\\ &{h^{\\top}[\\hat{H}]_{j}=0,\\quad\\forall j\\in[i-1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, we use estimated zero-centered Jacobians $\\tilde{J}_{X}$ of observed samples $x^{(m)}$ , given by $\\tilde{J}_{X}(x^{(m)})\\triangleq$ $\\hat{J}_{X}(x^{(m)})\\,-\\,\\bar{J}_{X}(X)$ with $\\begin{array}{r}{\\bar{J}_{X}(X)\\triangleq1/{N}\\sum_{m=1}^{N}\\hat{J}_{X}(x^{(m)})}\\end{array}$ . The constraint $h^{\\top}\\tilde{J}_{X}(x^{(m)})h=0$ is equivalent to enforcing the sample estima te of $\\bar{V}a r[J_{\\hat{Z}}(z)]_{i i}$ to be zero. The additional constraints $h^{\\top}h=1$ and $h^{\\top}[\\hat{H}]_{j}=0$ ensure that we do not violate the rank constraint. A formal derivation of equivalence is given in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Breaking the problem in Equation (1) into a series of problems in Equation (2) allows us to operate over a lower dimensional space and use any off-the-shelf solvers for QCQP. In practice, we use the cutting plane method for mixed integer programming [25]. Algorithm 1 summarizes the overall approach, where we construct the layer $(k)$ representations iteratively by solving a series of QCQPs. ", "page_idx": 6}, {"type": "text", "text": "4.2 Step 2: Layer-wise Nonlinear Regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given the learned representation $\\hat{Z}$ from Algorithm 1, we now proceed to disentangle the exogenous noise variables, which fully determine the randomness of the observations. ", "page_idx": 6}, {"type": "text", "text": "Following the proof of Theorem 2, we can recover a representation of the exogenous noise variables $\\mathcal{E}_{l a y e r(k)}$ by non-linearly regressing the $\\ l a y e r(k)$ representation of $Z$ on all upstream representations and taking the residual terms. This procedure is summarized in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We test our proposed algorithms using simulations3. Algorithm 1 requires estimating the score of the observational distribution and its performance relies on the quality of this estimation. To evaluate this, ", "page_idx": 6}, {"type": "text", "text": "1: Input: $N$ samples of $X$ in the observational distribution.   \n2: Estimate $\\tilde{J}_{X}(x^{(m)}),\\forall m\\in[N]$ using any off-the-shelf score estimation method (see Section 5).   \n3: Initialize $\\hat{Z}=0^{n\\times N}$ , $\\hat{X}=(x^{(1)},\\ldots,x^{(N)})\\in\\mathbb{R}^{d\\times N}$ , and $k=n$ .   \n4: while $k>0$ do   \n5: Initialize H\u02c6 = 0k\u00d7d.   \n6: for $i=1,\\ldots,d$ do   \n7: Set $[\\hat{H}]_{i}$ to be the solution of Equation (2). Break when no feasible solution is found.   \n8: end for   \n9: Fill in all-zero columns of $\\hat{H}$ with random vectors to remain full column rank.   \n10: Compute $\\tilde{Z}=\\hat{H}^{\\dagger}\\hat{X}$ and $J_{\\tilde{Z}}(\\tilde{Z}^{(m)})=\\hat{H}^{\\top}\\tilde{J}_{X}(\\hat{x}^{(m)})\\hat{H},\\forall m\\in[N]$ .   \n11: Set $\\hat{X}=0^{0\\times N}$ .   \n12: for $i=1,...,k$ do   \n13: if $V a r[J_{\\hat{Z}}(\\hat{z})]_{i,i}=0$ then   \n14: Set $[\\hat{Z}]_{n-k}=[\\tilde{Z}]_{i}$ and let $k\\gets k-1$ .   \n15: else   \n16: $\\begin{array}{r}{\\dot{X}\\leftarrow\\left[\\hat{X},[\\tilde{Z}]_{i}\\right].}\\end{array}$   \n17: end if   \n18: end for   \n19: end while   \n20: Return: $\\hat{Z}$ ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Recovering $\\mathcal{E}$ up to layers. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Input: $\\hat{Z}\\in\\mathbb{R}^{n\\times N}$ estimated from Algorithm 1. Denote the number of layers as $K$ .   \n2: Initialize E\u02c6 = 0n\u00d7N. Set E\u02c6layer(K) = Z\u02c6layer(K).   \n3: for $k=K-1,...,0$ do   \n4: Fit nonlinear regression on Z\u02c6layer(k) using E\u02c6layer(k+1), . . . , E\u02c6layer(K).   \n5: Set $\\hat{\\mathcal{E}}_{l a y e r(k)}$ as the residual terms.   \n6: end for   \n7: Return: $\\hat{\\mathcal{E}}$ . ", "page_idx": 7}, {"type": "text", "text": "we conduct two sets of experiments. In Section 5.1, we use perfect score oracles, which compute the Jacobian matrices exactly using the ground-truth data-generating process. This serves as a verification of our theoretical results. In Section 5.2, we estimate the score functions from the samples using two popular score-estimation methods. Details of the experiments can be found in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "5.1 Score Oracle Simulations: Validation of Theoretical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further validate our theoretical results, we run Algorithms 1 and 2 to learn the latent causal factors and the exogenous noise variables. We consider the following causal graphs with 4 nodes: (1) a line graph represented as $Z_{1}\\rightarrow Z_{2}\\rightarrow Z_{3}\\rightarrow Z_{4}$ , and (2) a $\\mathrm{\\bfY}.$ -structure represented as $Z_{1}\\rightarrow Z_{2}\\rightarrow Z_{3}$ , $Z_{2}\\to Z_{4}$ . For each case, we generate 2000 observational samples and compute the corresponding scores using the ground-truth link functions. ", "page_idx": 7}, {"type": "text", "text": "We present the results of our estimation in Figure 3. The scatter plots depict the relationships between the ground-truth $\\mathcal{Z}_{i}$ and the estimated $\\hat{\\mathcal{Z}}_{j}$ , where we color the dots with the values of $Z_{1}$ . The heatmaps show the mean absolute correlations (MAC) between the ground-truth $\\mathcal{E}_{i}$ and the estimated $\\hat{\\mathcal{E}}_{j}$ . For the estimated latent causal factors $\\hat{Z}$ , we see trends that are consistent with Theorem 1 in both cases, where the root node is perfectly identified and $Z_{2}$ is estimated with some mixing of $Z_{1}$ . For the estimated exogenous noise variables $\\hat{\\mathcal{E}}$ , the results validate Theorem 2. In the line graph, our algorithm perfectly disentangles all variables; in the Y-structure, we can perfectly disentangle ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ , while ${\\mathcal{E}}_{3}$ and ${\\mathcal{E}}_{4}$ are mixed. ", "page_idx": 7}, {"type": "image", "img_path": "M20p6tq9Hq/tmp/07268f4f8e09b274ee42a3e07a35da195b100703fb4dec10ec7409fbc2383047.jpg", "img_caption": ["Figure 3: Score oracle simulations. (A) Estimated versus true latent variables on the line graph. (B) Estimated versus true latent variables on the Y-structure. (C) Estimated versus true exogenous variables on the line graph. (D) Estimated versus true exogenous variables on the Y-structure. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Results using Score Estimation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this set of experiments, we aim to mimic real-world settings where the score functions are estimated from samples. We use two popular methods to generate point-wise estimates of the Jacobians of the scores: the second-order Stein estimator [5, 44] and the sliced score matching with variance reduction (SSM-VR) estimator [42]. We then plug these estimators into Algorithms 1 and 2. ", "page_idx": 8}, {"type": "text", "text": "We use the same sampling procedure on the four-node line graph as described in the previous section with varying sample sizes. Here we evaluate the mean absolute correlation (MAC) between the true and estimated exogenous noise variables. We adjust the tolerance of our QCQP solver to account for noisy estimates (see Appendix C). Table 2 reports the results averaged across 10 repeated runs. With noisy score estimates, we can still learn these variables although, as expected, accuracy decreases as compared to the results using oracle score estimates, where we can recover the exogenous noise almost perfectly. ", "page_idx": 8}, {"type": "text", "text": "As reliable higher-order score estimation is an active area of research [27, 41, 30], we seek to evaluate how the accuracy of our algorithm can increase under improved score estimation. Specifically, we consider how the MAC of exogenous noise estimates behaves under varying levels of noise in the plug-in Jacobians. We perturb the true Jacobian matrices with noise and plot the returned MAC with respect to the signal-to-error ratio (SER) in Figure 4. This shows that the accuracy improves with higher SER. We also mark the MAC of Stein estimation and SSM-VR, which are approximately around 2 and 6 SERs respectively. ", "page_idx": 8}, {"type": "table", "img_path": "M20p6tq9Hq/tmp/332bf789dd09de1766a4daa5810ad47fdfb77ef6228aed71b10307d4d4667fdc.jpg", "table_caption": ["Table 2: Mean absolute correlation of the exogenous noise estimates using score estimations. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "M20p6tq9Hq/tmp/e9d6d56ce06f29c659e0ca457777fbd195f51492bd9cafb1ccca712c723e3df9.jpg", "img_caption": ["Figure 4: Mean absolute correlation (MAC) of $\\mathcal{E}$ estimations v.s. Signal-to-error ratio (SER) of Jacobian matrices. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we derive partial identifabilty guarantees of causal disentanglement from purely observational data and linear mixing without any structural restrictions. In particular, we utilize asymmetries in nonlinear causal models with additive Gaussian noise. We provide a precise characterization of identifiability in this setting, where the latent causal factors can be identified up to upstream layers and the exogenous noise variables can be identified up to their layers. We show that further disentanglement is not possible without additional assumptions or alternative datasets. ", "page_idx": 9}, {"type": "text", "text": "These theoretical analyses indicate a simple but hard to optimize principle for deriving efficient algorithms. We show that this optimization problem can be solved via a series of simpler quadratically constrained quadratic programs. This leads to a flexible algorithm that allows us to use any off-theshelf QCQP solvers and score estimation methods. We demonstrate its correctness and efficiency using simulations. ", "page_idx": 9}, {"type": "text", "text": "While we view this work as having a primarily theoretical contribution, we additionally believe that the notion of layer-wise identifiability has many practical implications. In particular, our methods can be used to identify hierarchical topics at various layers of a causal system. For instance, when applied to a latent genealogical tree, each layer representation would contain all prior ancestral information used to determine the traits of a given generation. ", "page_idx": 9}, {"type": "text", "text": "In future work, it would be interesting to extend our results to latent causal models with other asymmetries. In particular, we believe our result could be extended to learn upstream layer representations of nonlinear additive models with generic noise as an extension of [28], by modifying the principle to achieve identifiability in Equation (1). It would also be interesting to understand how our identifability results in the purely observational setting could aid when additional external data, such as interventions or multi-modal data, are available. Additionally, since our work shows that causal disentanglement can be solved orthogonally to score estimation, extending and testing our proposed approaches to applications where there exist pretrained score estimators would be another interesting avenue to pursue. Furthermore, given further disentanglement beyond layer-wise identifiability is not possible with purely observational data, it remains unclear what minimum faithfulness assumptions are required to achieve stronger identifiability guarantees, which we view as an import question. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Our work advances the field of causal representation learning, where it was commonly thought that without interventional data causal variables could only be discovered up to linear combinations of all variables without structural assumptions. While our work has many potential applications, we feel that no particular societal consequence needs to be highlighted. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Chandler Squires for helpful discussions, as well as the anonymous reviewers for their valuable feedback. This work was partially supported by ONR (N00014-22-1-2116), NCCIH/NIH (1DP2AT012345), DOE (DE-SC0023187), and a Simons Investigator Award to Caroline Uhler. R.W. was supported by a fellowship by the Eric and Wendy Schmidt Center at the Broad Institute and the Advanced Undergraduate Research Opportunities Program at MIT. J.Z. was partially supported by an Apple AI/ML PhD Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kartik Ahuja, Jason S Hartford, and Yoshua Bengio. Weakly supervised representation learning with sparse perturbations. Advances in Neural Information Processing Systems, 35:15516\u2013 15528, 2022.   \n[2] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In International Conference on Machine Learning, pages 372\u2013407. PMLR, 2023.   \n[3] Kartik Ahuja, Amin Mansouri, and Yixin Wang. Multi-domain causal representation learning via weak distributional invariances. In International Conference on Artificial Intelligence and Statistics, pages 865\u2013873. PMLR, 2024.   \n[4] Steen A Andersson, David Madigan, and Michael D Perlman. A characterization of markov equivalence classes for acyclic digraphs. The Annals of Statistics, 25(2):505\u2013541, 1997.   \n[5] Pierre C Bellec and Cun-Hui Zhang. Second-order stein: Sure for sure and other applications in high-dimensional inference. The Annals of Statistics, 49(4):1864\u20131903, 2021.   \n[6] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u2013 1828, 2013.   \n[7] Simon Bing, Urmi Ninad, Jonas Wahl, and Jakob Runge. Identifying linearly-mixed causal representations from multi-node interventions. arXiv preprint arXiv:2311.02695, 2023.   \n[8] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. Advances in Neural Information Processing Systems, 35:38319\u201338331, 2022.   \n[9] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Sch\u00f6lkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Peter B\u00fchlmann, Jonas Peters, and Jan Ernest. CAM: causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 42:2526\u20132556, 2014.   \n[11] Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, and Kun Zhang. Triad constraints for learning causal structure of latent variables. Advances in Neural Information Processing Systems, 32, 2019.   \n[12] Max Chickering, David Heckerman, and Chris Meek. Large-sample learning of bayesian networks is np-hard. Journal of Machine Learning Research, 5:1287\u20131330, 2004.   \n[13] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.   \n[14] Yoni Halpern, Steven Horng, and David Sontag. Anchored discrete factor analysis. arXiv preprint arXiv:1511.03299, 2015.   \n[15] Aapo Hyv\u00e4rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429\u2013439, 1999.   \n[16] Seiya Imoto, Takao Goto, and Satoru Miyano. Estimation of genetic networks and functional structures between genes by using bayesian networks and nonparametric regression. In Biocomputing 2002, pages 175\u2013186. World Scientific, 2001.   \n[17] Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown interventions. Advances in Neural Information Processing Systems, 36, 2024.   \n[18] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal machine learning: A survey and open problems. arXiv preprint arXiv:2206.15475, 2022.   \n[19] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Learning latent causal graphs via mixture oracles. Advances in Neural Information Processing Systems, 34:18087\u201318101, 2021.   \n[20] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Identifiability of deep generative models without auxiliary information. Advances in Neural Information Processing Systems, 35:15687\u201315701, 2022.   \n[21] Lingjing Kong, Biwei Huang, Feng Xie, Eric Xing, Yuejie Chi, and Kun Zhang. Identification of nonlinear latent hierarchical models. Advances in Neural Information Processing Systems, 36, 2024.   \n[22] Adam Li, Jaron Lee, Francesco Montagna, Chris Trevino, and Robert Ness. Dodiscover: Causal discovery algorithms in Python.   \n[23] Phillip Lippe, Sara Magliacane, Sindy L\u00f6we, Yuki M Asano, Taco Cohen, and Stratis Gavves. Citris: Causal identifiability from temporal intervened sequences. In International Conference on Machine Learning, pages 13557\u201313603. PMLR, 2022.   \n[24] Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, and Javen Qinfeng Shi. Identifying weight-variant latent causal models. arXiv preprint arXiv:2208.14153, 2022.   \n[25] Hugues Marchand, Alexander Martin, Robert Weismantel, and Laurence Wolsey. Cutting planes in integer and mixed integer programming. Discrete Applied Mathematics, 123(1-3):397\u2013446, 2002.   \n[26] Giampiero Marra and Simon N Wood. Practical variable selection for generalized additive models. Computational Statistics & Data Analysis, 55(7):2372\u20132387, 2011.   \n[27] Chenlin Meng, Yang Song, Wenzhe Li, and Stefano Ermon. Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems, 34:25359\u201325369, 2021.   \n[28] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. Causal discovery with score matching on additive models with arbitrary noise. In Conference on Causal Learning and Reasoning, pages 726\u2013751. PMLR, 2023.   \n[29] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. Scalable causal discovery with score matching. arXiv preprint arXiv:2304.03382, 2023.   \n[30] Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Efficient learning of generative models via finite-difference score matching. Advances in Neural Information Processing Systems, 33:19175\u201319188, 2020.   \n[31] Jonas Peters, Joris Mooij, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Identifiability of causal graphs using functional models. arXiv preprint arXiv:1202.3757, 2012.   \n[32] Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Sch\u00f6lkopf, and Pradeep Ravikumar. Learning interpretable concepts: Unifying causal representation learning and foundation models. arXiv preprint arXiv:2402.09236, 2024.   \n[33] Alexander G Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag! varsortability in additive noise models. arXiv preprint arXiv:2102.13647, 2021.   \n[34] Patrik Reizinger, Yash Sharma, Matthias Bethge, Bernhard Sch\u00f6lkopf, Ferenc Husz\u00e1r, and Wieland Brendel. Jacobian-based causal discovery with nonlinear ica. Transactions on Machine Learning Research, 2022.   \n[35] Paul Rolland, Volkan Cevher, Matth\u00e4us Kleindessner, Chris Russell, Dominik Janzing, Bernhard Sch\u00f6lkopf, and Francesco Locatello. Score matching enables causal discovery of nonlinear additive noise models. In International Conference on Machine Learning, pages 18741\u201318753. PMLR, 2022.   \n[36] Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Ravikumar, Niklas Pfister, and Jonas Peters. Identifying representations for intervention extrapolation. arXiv preprint arXiv:2310.04295, 2023.   \n[37] Pedro Sanchez, Xiao Liu, Alison Q O\u2019Neil, and Sotirios A Tsaftaris. Diffusion models for causal discovery via topological ordering. arXiv preprint arXiv:2210.06201, 2022.   \n[38] Bernhard Sch\u00f6lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.   \n[39] Bernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612\u2013634, 2021.   \n[40] Ricardo Silva, Richard Scheines, Clark Glymour, Peter Spirtes, and David Maxwell Chickering. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7(2), 2006.   \n[41] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pages 574\u2013584. PMLR, 2020.   \n[43] Chandler Squires, Anna Seigal, Salil S Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In International Conference on Machine Learning, pages 32540\u201332560. PMLR, 2023.   \n[44] Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, volume 6, pages 583\u2013603. University of California Press, 1972.   \n[45] Nils Sturma, Chandler Squires, Mathias Drton, and Caroline Uhler. Unpaired multi-domain causal representation learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] Chuangchuang Sun and Ran Dai. Rank-constrained optimization and its applications. Automatica, 82:128\u2013136, 2017.   \n[47] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Scorebased causal representation learning with interventions. arXiv preprint arXiv:2301.08230, 2023.   \n[48] Burak Varici, Emre Acart\u00fcrk, Karthikeyan Shanmugam, and Ali Tajer. General identifiability and achievability for causal representation learning. In International Conference on Artificial Intelligence and Statistics, pages 2314\u20132322. PMLR, 2024.   \n[49] Burak Var\u0131c\u0131, Emre Acart\u00fcrk, Karthikeyan Shanmugam, and Ali Tajer. Score-based causal representation learning: Linear and general transformations. arXiv preprint arXiv:2402.00849, 2024.   \n[50] Julius von K\u00fcgelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Keki\u00b4c, Elias Bareinboim, David Blei, and Bernhard Sch\u00f6lkopf. Nonparametric identifiability of causal representations from unknown interventions. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Samir Wadhwa and Roy Dong. On the sample complexity of causal discovery and the value of domain expertise. arXiv preprint arXiv:2102.03274, 2021.   \n[52] Michael Wainberg, Daniele Merico, Andrew Delong, and Brendan J Frey. Deep learning in biomedicine. Nature Biotechnology, 36(9):829\u2013838, 2018.   \n[53] Liang Wendong, Armin Kekic\u00b4, Julius von K\u00fcgelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and Bernhard Sch\u00f6lkopf. Causal component analysis. Advances in Neural Information Processing Systems, 36, 2024.   \n[54] Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. Advances in Neural Information Processing Systems, 33:14891\u201314902, 2020.   \n[55] Feng Xie, Biwei Huang, Zhengming Chen, Yangbo He, Zhi Geng, and Kun Zhang. Identification of linear non-gaussian latent hierarchical structure. In International Conference on Machine Learning, pages 24370\u201324387. PMLR, 2022.   \n[56] Danru Xu, Dingling Yao, S\u00e9bastien Lachapelle, Perouz Taslakian, Julius von K\u00fcgelgen, Francesco Locatello, and Sara Magliacane. A sparsity principle for partially observable causal representation learning. arXiv preprint arXiv:2403.08335, 2024.   \n[57] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentangled representation learning via neural structural causal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9593\u20139602, 2021.   \n[58] Dingling Yao, Danru Xu, S\u00e9bastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von K\u00fcgelgen, and Francesco Locatello. Multi-view causal representation learning with partial observability. arXiv preprint arXiv:2311.04056, 2023.   \n[59] Jiaqi Zhang, Kristjan Greenewald, Chandler Squires, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. Advances in Neural Information Processing Systems, 36, 2024.   \n[60] Yudi Zhang, Yali Du, Biwei Huang, Ziyan Wang, Jun Wang, Meng Fang, and Mykola Pechenizkiy. Interpretable reward redistribution in reinforcement learning: A causal approach. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Zhenyu Zhu, Francesco Locatello, and Volkan Cevher. Sample complexity bounds for scorematching: Causal discovery and generative modeling. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs for Identifiability ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Given the linear relation $X=H\\cdot Z$ , we relate the probability density functions $p(\\cdot)$ of $Z$ and $p_{X}(\\cdot)$ via ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{X}(x)=p(H^{\\dagger}x)|\\operatorname*{det}(H^{\\dagger})|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, we write the gradient of the log density of $p_{X}(x)$ with respect to $X$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{X}\\log p_{X}(x)=\\frac{\\nabla_{X}p_{X}(x)}{p_{X}(x)}}\\\\ &{\\phantom{\\nabla_{X}\\log p_{X}(x)}=\\frac{\\nabla_{X}p\\left(H^{\\dagger}x\\right)|\\operatorname*{det}(H^{\\dagger})|}{p\\left(H^{\\dagger}x\\right)|\\operatorname*{det}(H^{\\dagger})|}}\\\\ &{\\phantom{\\nabla_{X}\\log p_{X}(x)}=\\frac{\\nabla_{X}p(H^{\\dagger}x)}{p\\left(H^{\\dagger}x\\right)}}\\\\ &{\\phantom{\\nabla_{X}\\log p_{X}(X)}=\\nabla_{X}\\log p(H^{\\dagger}x)}\\\\ &{\\phantom{\\nabla_{X}\\log p_{X}(X)}=(H^{\\dagger})^{\\top}\\nabla_{Z}\\log p(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, it follows that $\\nabla_{Z}\\log p(z)=H^{\\top}\\nabla_{X}\\log p_{X}(x)$ , or $s_{Z}(z)=H^{\\top}s_{X}(x)$ as desired. ", "page_idx": 14}, {"type": "text", "text": "Differentiating $\\nabla_{X}\\log{p_{X}(x)}$ with respect to $X$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{X}^{2}\\log p_{X}(x)=\\nabla_{X}(H^{\\dagger})^{\\top}\\nabla_{Z}\\log p(z)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(H^{\\dagger})^{\\top}\\nabla_{Z}^{2}\\log p(z)(H^{\\dagger}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, it additionally follows that $\\nabla_{Z}^{2}\\log p(z)=H^{\\top}(\\nabla_{X}^{2}\\log p_{X}(x))H,$ , or $J_{Z}(z)=H^{\\top}J_{X}(x)H$ ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Before proceeding to the proof of Lemma 2, we must prove the following supplementary lemma.   \nLemma 4. For any two distinct leaf nodes $k$ and $l$ , it follows that $\\begin{array}{r}{\\frac{\\partial s_{k}(z)}{\\partial z_{l}}=0}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. From [35], we denote the score of the latent variable $Z_{k}$ evaluated at $z$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\ns_{k}(z)=-\\frac{z_{k}-f_{k}(z_{p a(k)})}{\\sigma_{k}^{2}}+\\sum_{i\\in c h(k)}\\frac{\\partial f_{i}(z_{p a(i)})}{\\partial z_{k}}\\cdot\\frac{z_{i}-f_{i}(z_{p a(i)})}{\\sigma_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We further derive the following expression: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(\\partial P_{k}(\\varepsilon)}{\\partial\\varepsilon_{l}})}\\\\ &{=\\left(\\frac{1}{\\partial\\varepsilon_{l}}\\right)\\left(\\frac{\\partial f_{k}(\\varepsilon_{m(k)})}{\\partial\\varepsilon_{l}}\\right)}\\\\ &{+\\sum_{i\\in\\partial i\\setminus i}\\left[\\left(\\frac{\\partial f_{k}^{2}\\tilde{f}_{k(\\varepsilon_{m(i)})}}{\\partial\\varepsilon_{k}\\partial\\varepsilon_{l}}\\right)\\left(\\frac{\\varepsilon_{i}-f_{k}(\\varepsilon_{m(i)})}{\\sigma_{i}^{2}}\\right)+\\left(\\frac{1}{\\sigma_{i}^{2}}\\right)\\left(\\frac{\\partial f_{k}(\\varepsilon_{m(i)})}{\\partial\\varepsilon_{k}}\\right)\\left(\\frac{\\partial\\varepsilon_{i}}{\\partial\\varepsilon_{l}}-\\frac{\\partial f_{k}(\\varepsilon_{m(i)})}{\\partial\\varepsilon_{l}}\\right)\\right]}\\\\ &{=\\left(\\frac{1}{\\partial\\varepsilon_{l}^{2}}\\right)\\left(\\frac{\\partial f_{k}(\\varepsilon_{m(k)})}{\\partial\\varepsilon_{l}}\\right)}\\\\ &{+\\sum_{i\\in\\partial i\\setminus i}\\left[\\left(\\frac{\\partial f_{j}^{2}\\tilde{f}_{k(\\varepsilon_{m(i)})}}{\\partial\\varepsilon_{k}\\partial\\varepsilon_{l}}\\right)\\left(\\frac{\\varepsilon_{i}-f_{k}(\\varepsilon_{m(i)})}{\\sigma_{i}^{2}}\\right)+\\left(\\frac{1}{\\sigma_{i}^{2}}\\right)\\left(\\frac{\\partial f_{k}(\\varepsilon_{m(i)})}{\\partial\\varepsilon_{k}}\\right)\\left(1_{\\{\\varepsilon=\\ t_{l}-\\}}-\\frac{\\partial f_{k}(\\varepsilon_{m(i)})}{\\partial\\varepsilon_{l}}\\right)\\right.}\\\\ &{=\\left.\\left(\\frac{1}{\\partial\\varepsilon_{l}^{2}}\\right)\\left(\\frac{\\partial f_{k}(\\varepsilon_{m(i)})}{\\partial\\varepsilon_{l}}\\right)+1\\{\\varepsilon_{l}\\in\\partial i\\setminus i\\}\\left(\\frac{1}{\\sigma_{i}^{2}}\\right)\\left(\\frac{\\partial f_{l}(\\varepsilon_{m(i)})}{\\partial\\varepsilon_{k}}\\right)+}\\\\ &{\\qquad\\times\\frac{1}{\\varepsilon_{l}}\\rho_{k}^{2}\\left(\\frac{1}{\\partial\\varepsilon_{l}}\\nabla_{k}f_{l}(\\varepsilon_{m(i)})\\cdot\\varepsilon_{l}-\\mathrm{Fr}_{k}\\nabla_{k}f_{l}(\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $k$ and $l$ are distinct leaf nodes, $c h(k)=c h(l)=\\emptyset$ , and our expression simplifies to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{k}(z)}{\\partial z_{l}}=\\left(\\frac{1}{\\sigma_{k}^{2}}\\right)\\left(\\frac{\\partial f_{k}(z_{p a(k)})}{\\partial z_{l}}\\right)=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $l\\not\\in p a(k)$ , which completes our proof. ", "page_idx": 15}, {"type": "text", "text": "We now proceed to the proof of Lemma 2. ", "page_idx": 15}, {"type": "text", "text": "Proof. (Lemma 2) We first prove the backward direction. Given $J_{\\hat{Z}}(\\hat{z})=\\beta^{\\top}J_{Z}(z)\\beta$ , we express $J_{\\hat{Z}}(\\hat{z})_{i i}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{\\hat{Z}}(\\hat{z})_{i i}=\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\beta_{j i}\\beta_{k i}\\frac{\\partial s_{j}(z)}{\\partial z_{k}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assuming that $\\beta_{j i}=0$ for all $j\\not\\in l a y e r(0)$ , the above expression simplifies to ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{\\hat{Z}}(\\hat{z})_{i i}=\\sum_{j,k\\in l a y e r(0)}^{n}\\beta_{j i}\\beta_{k i}\\frac{\\partial s_{j}(z)}{\\partial z_{k}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Utilizing Lemma 4 and Lemma 1 from [35], which states that $\\begin{array}{r}{\\mathrm{Var}\\left[\\frac{\\partial s_{k}\\left(z\\right)}{\\partial z_{k}}\\right]=0}\\end{array}$ for all $k\\in l a y e r(0)$ , it follows that Var $\\left[J_{\\hat{Z}}(\\hat{z})_{i i}\\right]=0$ . ", "page_idx": 15}, {"type": "text", "text": "Now, we prove the forward direction. Denote $C_{i}:=\\{j:\\beta_{j i}\\neq0\\}$ as the set of all indices in the $i^{t h}$ column of $\\beta$ that are non-zero. It suffices to show that if there exists some $k\\,\\in\\,C_{i}$ such that $k\\not\\in l a y e r(0)$ , then Var $\\left[J_{\\hat{Z}}(\\hat{z})_{i i}\\right]\\neq0$ . ", "page_idx": 15}, {"type": "text", "text": "Let $i_{c}$ be the most downstream node in $c h(C_{i}):=\\{c h(j):j\\in C_{i}\\}$ such that $i_{c}\\notin p a(i)$ for any $i\\in c h(C_{i})$ . Such a node must exist under the assumption that some non-leaf node is contained in $C_{i}$ . We express $J_{\\hat{Z}}(\\hat{z})_{i i}$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{2}(\\hat{L}|u_{n})}\\\\ &{=-\\displaystyle\\sum_{k\\neq0}^{\\infty}J_{1}\\frac{\\partial u_{k}(k)}{\\partial u_{1}}\\frac{\\partial u_{k}^{(k)}}{\\partial u_{2}}-\\sum_{k\\neq1}^{\\infty}\\frac{\\partial u_{k}^{(k)}(k)}{\\partial u_{1}}+\\sum_{i=0}^{\\infty}\\beta u_{i}\\frac{\\partial u_{i}(k)}{\\partial u_{2}}}\\\\ &{-\\displaystyle\\sum_{k\\neq1}^{\\infty}\\beta u_{i}\\left[\\frac{1}{\\partial u_{k}}+\\sum_{i=1}^{\\infty}\\frac{1}{\\partial u_{i}^{(k)}(k)}|\\nabla u_{i}(k)|u_{i}^{-}\\right]+\\mathbb{V}_{u}\\mathbb{I}[J_{1}(k)u_{i}(k)]-\\mathbb{I}[J_{2}(k)u_{1}(k)]}\\\\ &{+\\displaystyle\\sum_{k\\neq1}^{\\infty}\\Delta u_{k}\\Delta u_{i}\\left[\\left(\\frac{1}{\\partial u_{k}}\\right)\\left(\\frac{\\partial u_{i}(k)}{\\partial u_{1}}\\right)+1|u_{1}(k)|\\left(\\frac{1}{\\partial u_{1}}\\right)\\left(\\frac{\\partial u_{i}(k)}{\\partial u_{2}}\\right)+\\right.}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left.\\displaystyle\\sum_{k\\neq1}^{\\infty}\\left[\\frac{1}{\\partial v_{k}}\\nabla J_{1}(k)u_{1}(k)\\right]-\\mathbb{I}_{\\mathbb{R}}\\mathbb{I}_{\\{2,k\\}}[J_{1}(k)u_{1}(k)]-\\mathbb{I}_{\\{2,k\\}}[J_{2}(k)u_{1}(k)]\\right]}\\\\ &{=\\displaystyle\\sum_{k\\neq1}^{\\infty}\\beta u_{k}\\left[\\frac{1}{\\partial v_{k}}+\\sum_{i=1}^{\\infty}\\sum_{s=0}^{\\infty}\\left[\\nabla J_{1}(k)u_{i}(k)\\right]+\\mathbb{I}_{\\{2,k\\}}[J_{2}(k)u_{1}(k)]\\right]\\Bigg]+\\mathbb{I}_{\\{4,k\\}}}\\\\ &{\\displaystyle\\sum_{k\\neq1}^{\\infty}\\beta u_{k}\\left[\\left(\\frac{1}{\\partial u_{k}}\\right)\\left(\\frac{\\partial v_{k}(k)}{\\partial u_{1}}\\right)+1|u \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, by sep ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{and~}t\\leq\\operatorname{tand}_{\\mathbf{x}\\in\\mathbf{R}^{n}}\\operatorname{const}_{i\\in\\mathbf{Z}}\\operatorname{min}_{\\mathbf{x}^{t},\\mathbf{y}^{t}\\in\\mathbf{Z}}\\left(\\mathbb{I}_{q}\\times\\mathbb{E}_{q}^{n}\\right)(\\mathbb{E}_{q})+}\\\\ &{\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\}\\operatorname}\\\\ &{\\prod_{s=1}^{n}\\left(\\mathbb{E}_{q}\\right)\\left(\\frac{\\mathrm{d}_{p}}{n\\xi}\\right)\\left(\\mathbb{E}_{q}^{n}\\right)\\left(\\mathbb{E}_{q}\\times\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\left.\\sum_{k=0}^{n}\\left(\\mathbb{E}_{q}\\right)\\left(\\frac{\\mathrm{d}_{p}}{n\\xi}\\right)\\left(\\mathbb{E}_{q},\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q},\\mathbb{E}_{q}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sum_{k=0}^{n}\\left(\\mathbb{E}_{q}\\right)\\left(\\frac{\\mathrm{d}_{q}}{n\\xi}\\right)\\left(\\mathbb{E}_{q},\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q},\\mathbb{E}_{q}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\ \\sum_{k=0}^{n}\\left(\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q}\\times\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q},\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q},\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\sum_{k=0}^{n}\\left(\\mathbb{E}_{q}\\right)^{-1}\\frac{1}{n\\xi}\\prod_{s=0}^{n}\\left(\\mathbb{E}_{q}\\right)\\left(\\mathbb{E}_{q}(\\mathbf{z})\\right)\\mathcal{E}_{q}-\\mathbb{E}_{q}\\left(\\mathbb{E}_{q}(\\mathbf{z})\\right)\\left(\\mathbb{E}_{q}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sum_{k=0}^{n}\\mathbb{E}_{q}\\left(\\frac{1}{n\\xi}\\right)\\left(\\mathbb{E} \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $g(z_{-i_{c}})=(5)+(6)+(7)+(8)+(9).$ . Given that (5), (6), (7), (8) and (9) are functions of only variables upstream of $Z_{i_{c}}$ , we have that $g(z_{\\_i_{c}})$ \u22a5\u22a5 $\\mathcal{E}_{i_{c}}$ . Furthermore, let ", "page_idx": 16}, {"type": "equation", "text": "$$\nh(z_{-i_{c}})=\\sum_{\\boldsymbol{k},\\boldsymbol{l}\\in\\boldsymbol{C}_{i}}\\left(\\beta_{k i}\\beta_{l i}\\right)\\left(\\frac{1}{\\sigma_{i_{c}}^{2}}\\right)\\left(\\nabla_{\\boldsymbol{k}}\\nabla_{\\boldsymbol{l}}f_{i_{c}}(z_{p a(i_{c})})\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which similarly contains only variables upstream of $z_{i_{c}}$ . Thus it holds that $h(z_{-i_{c}})\\perp\\mathcal{E}_{i_{c}}$ . Now we write ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ_{\\hat{Z}}(\\hat{z})_{i i}=h(z_{-i_{c}})\\cdot\\mathcal{E}_{i_{c}}+g(z_{-i_{c}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and it suffices to show that $V a r[h(z_{-i_{c}})\\cdot\\mathcal{E}_{i_{c}}+g(z_{-i_{c}})]\\neq0$ . Expanding this expression, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\prime}a r[h(z_{-i_{c}})\\cdot\\mathcal{E}_{i_{c}}+g(z_{-i_{c}})]=V a r[h(z_{-i_{c}})\\cdot\\mathcal{E}_{i_{c}}]+V a r[g(z_{-i_{c}})]+2C o v[h(z_{-i_{c}})\\cdot\\mathcal{E}_{i_{c}},g(z_{-i_{c}})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\mathbb{E}[h(z_{-i_{c}})^{2}\\mathcal{E}_{i_{c}}^{2}]-\\mathbb{E}[[h(z_{-i_{c}})\\mathcal{E}_{i_{c}}]^{2}+V a r(g)+2\\mathbb{E}[h(z_{-i_{c}})\\mathcal{E}_{i_{c}}g(z_{-i_{c}})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -2\\mathbb{E}[h(z_{-i_{c}})\\mathcal{E}_{i_{c}}]\\mathbb{E}[g(z_{-i_{c}})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\mathbb{E}[h(z_{-i_{c}})^{2}]\\mathbb{E}[\\mathcal{E}_{i_{c}}^{2}]+V a r(g)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =V a r[h(z_{-i_{c}})]\\sigma_{i_{c}}^{2}+\\mathbb{E}[h(z_{-i_{c}})]^{2}\\sigma_{i_{c}}^{2}+V a r(g).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the variance of $J_{\\hat{Z}}(\\hat{z})_{i i}$ is positive if and only if $V a r[h(z_{-i_{c}})]>0,\\mathbb{E}[h(z_{-i_{c}})]\\neq0$ , or $V a r\\big(g(z_{-i_{c}})\\big)>0$ . We will now show that $V a r[h(z_{-i_{c}})]>0$ or $\\mathbb{E}[h(z_{-i_{c}})]\\neq0$ . ", "page_idx": 16}, {"type": "text", "text": "Let us rewrite $h(z_{-i_{c}})$ in matrix form as $\\begin{array}{r l r}{h(z_{-i_{c}})}&{\\propto}&{(\\beta_{i}^{\\prime})^{\\top}\\partial_{z_{p a(i_{c})},z_{p a(i_{c})}}^{2}f_{i_{c}}(z_{p a(i_{c})})(\\beta_{i}^{\\prime})\\;=\\;}\\end{array}$ $\\partial_{\\beta_{i}^{\\prime},\\beta_{i}^{\\prime}}^{2}f_{i_{c}}(z_{p a(i_{c})})$ , where $\\beta_{i}^{\\prime}\\,\\in\\,\\mathbb{R}^{|p a(i_{c})|}$ is a vector formed by finding the entries that correspond to $p a(i_{c})$ in $\\beta$ . Note that by our assumption $p a(i_{c})\\cap C_{i}\\;\\neq\\;\\emptyset$ , we thus have $\\beta_{i}^{\\prime}\\ \\neq\\ 0$ . By the directional non-linear assumption in Assumption 2, we know that \u2202\u03b22\u2032,\u03b2\u2032fic(zpa(ic)) cannot be 0 for all realizations of $z_{p a}(i_{c})$ . This implies that $\\mathbb{P}[h(z_{-i_{c}})=0]\\neq1$ , which further implies that either $V a r[h(z_{-i_{c}})]>0$ or $\\dot{\\mathbb{E}}[\\dot{h}(z_{-i_{c}})]\\neq0$ as desired. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Lemma 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Without loss of generality, assume that layer(0) corresponds to the the last $l$ indexed latent variables. We claim there must be exactly $l$ such columns of $\\beta$ containing zero entries in every element corresponding to non-leaf nodes, or $\\hat{H}$ could not be an optimal solution of Equation (1). We write $\\beta$ in block form as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta=\\binom{A}{B}\\ \\ \\ C\\biggr)\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $A$ is $d-l\\times d-l,B$ is $d-l\\times l$ , ad $C$ is $l\\times l$ . We note that the columns of $\\beta$ need not be ordered in this way to achieve our result and that we assume this structure to simplify notation. We derive the inverse of $\\beta$ via taking the Schur complement as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta^{-1}=\\left(\\!\\!\\begin{array}{c c}{{A^{-1}}}&{{0}}\\\\ {{-C^{-1}B A^{-1}}}&{{C^{-1}}}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $A^{-1}$ and $C^{-1}$ are full column rank. Now taking, $\\hat{Z}=\\beta^{-1}Z$ , we derive the desired equalities ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathsf{Z}}_{i}=\\left\\{A_{i}^{-1}(Z_{0},...,Z_{d-l})=l i n e a r(Z_{\\mathrm{non-leaf}}),\\quad\\mathrm{~if~}V a r[J_{\\hat{Z}}(\\hat{Z}(X))]]_{i i}\\neq0}\\\\ {-C^{-1}B A_{i}^{-1}(Z_{0},...,Z_{d-l})+C_{i}^{-1}(Z_{d-l},...,Z_{n})=l i n e a r(Z)\\quad\\mathrm{if~}V a r[J_{\\hat{Z}}(\\hat{Z}(X))]]_{i i}=0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "thereby completing the proof. ", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Assume without loss of generality that the latent variables $Z$ are reverse layerly ordered such that $l a y e r(0)=\\{Z_{0},...,Z_{l_{0}}\\}$ , $l a y e r(1)=\\left\\{Z_{l_{0}+1},...,Z_{l_{1}}\\right\\}$ , and so on. Solving for $\\bar{\\hat{H}}$ according to the optimization problem framed in Equation (1), it follows from Lemma 3 that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{Z}_{i}=\\left\\{l i n e a r(Z_{0},...,Z_{d}),\\begin{array}{l}{\\mathrm{if}\\ V a r[J_{\\hat{Z}}(\\hat{Z}(X))]]_{i i}=0}\\\\ {\\mathrm{if}\\ V a r[J_{\\hat{Z}}(\\hat{Z}(X))]]_{i i}\\neq0,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\{\\hat{Z}_{i}\\ :\\ V a r[J_{\\hat{Z}}(\\hat{Z}(X))]]_{i i}\\;=\\;0\\}$ denotes the representations of $\\ l a y e r(0)$ variables up to upstream layers. Now, if we denote $X^{\\prime}$ as the set of vectors in $\\{\\hat{Z}_{i}:V a r[J_{\\hat{Z}}(\\hat{Z}(X))]]_{i i}\\neq0\\}.$ , it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\nX^{\\prime}=H^{\\prime}(Z_{l_{0}+1},...,Z_{d}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $H^{\\prime}$ is a full column rank matrix. Thus, viewing $X^{\\prime}$ as our observations of the true non-leaf latent variables, we can utilize Lemma 3 to show that we can recover the layer(1) up to its upstream layer representation. Continuing this method of pruning at each iteration, it is clear to see that we can derive the upstream layer representation for all variables. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.5 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Assume there are $n$ distinct layers of $\\mathcal{G}$ . From Definition 1, it follows that $\\hat{\\mathcal{E}}_{l a y e r(n)}\\,=$ $\\hat{Z}_{l a y e r(n)}$ , given $p a(i)=\\emptyset$ for all $i\\in l a y e r(n)$ . ", "page_idx": 17}, {"type": "text", "text": "Now, considering the next layer, namely $l a y e r(n-1)$ , we can express $\\hat{Z}_{l a y e r(n-1)}$ as follows given the principle in Assumption 2, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{Z}_{l a y e r(n-1)}=\\hat{\\mathcal{E}}_{l a y e r(n-1)}+\\mathrm{NONLINEAR}\\big(\\hat{Z}_{l a y e r(n)}\\big),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where NONLINEAR $\\big(\\hat{Z}_{l a y e r(n)}\\big)$ denotes the nonlinear relationship between $\\hat{Z}_{l a y e r(n-1)}$ and $Z_{p a(l a y e r(n-1))}\\ \\in\\ Z_{l a y e r(n)}$ specified by the linear combination of nonlinear functions $\\{f_{i}~:$ $i\\;\\;\\in\\;\\;l a y e r(n\\;-\\;1)\\}$ . However, given $\\begin{array}{r c l}{\\hat{\\mathcal{E}}_{l a y e r(n)}}&{=}&{\\hat{Z}_{l a y e r(n)}}\\end{array}$ , $\\begin{array}{r l}{\\mathrm{NONLINEAR}\\big(\\hat{Z}_{l a y e r(n)}\\big)}&{=}\\end{array}$ $\\begin{array}{r l}{\\lefteqn{\\mathrm{NONLINEAR}\\big(\\hat{\\mathcal{E}}_{l a y e r(n)}\\big)}\\quad}&{{}}\\end{array}$ can be determined. Thus, it follows we can learn $\\hat{\\mathcal{E}}_{l a y e r(n-1)}$ as the residual of the nonlinear regression of $\\hat{Z}_{l a y e r(n-1)}$ on $\\hat{\\mathcal{E}}_{l a y e r(n)}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{E}}_{l a y e r(n-1)}=\\hat{Z}_{l a y e r(n-1)}-\\mathrm{NoNLINEAR}\\big(\\hat{\\mathcal{E}}_{l a y e r(n)}\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now generalizing to layer $n-i$ , we can express $\\hat{Z}_{l a y e r(n-i)}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{Z}_{l a y e r(n-i)}=\\hat{\\mathcal{E}}_{l a y e r(n-i)}+\\mathrm{NONLINEAR}\\big(\\hat{\\mathcal{E}}_{l a y e r(n-i+1)},...,\\hat{\\mathcal{E}}_{l a y e r(n)}\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, by the same principle, we can determine $\\hat{\\mathcal{E}}_{l a y e r(n-i)}$ as the residual of the nonlinear regression of $\\hat{Z}_{l a y e r(n-i)}$ on $\\hat{\\mathcal{E}}_{l a y e r(n-i+1)},...,\\hat{\\mathcal{E}}_{l a y e r(n)}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{E}}_{l a y e r(n-i)}=\\hat{Z}_{l a y e r(n-i)}-\\mathrm{NONLINEAR}\\big(\\hat{\\mathcal{E}}_{l a y e r(n-i+1)},...,\\hat{\\mathcal{E}}_{l a y e r(n)}\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we can determine $\\hat{\\mathcal{E}}_{l a y e r(n-i)}$ for all $i=0,...,n$ . ", "page_idx": 18}, {"type": "text", "text": "B Derivation of Algorithms ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We show that the optimization problem in Equation (1) can equivalently be solved by solving the QCQP in Equation (2) for each column sequentially. Given that the $i^{t h}$ element of $d i\\bar{a}g(H^{\\top}J_{X}(X)H)$ can be expressed as ", "page_idx": 18}, {"type": "equation", "text": "$$\nd i a g(\\hat{H}^{\\top}J_{X}(X)\\hat{H})_{i}=[\\hat{H}_{i}]^{\\top}J_{X}(X)[\\hat{H}_{i}],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we can naturally break our optimization problem into sub-problems of solving for the optimal column vector $[H_{i}]$ that results in zero variance for the term above. We will now determine an equivalent expression for the variance of this term in terms of a given vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma_{G\\nu}(\\tau^{\\prime},T_{X}(X))=\\mathbb{E}\\left[(v^{T}x_{t}(X))^{T}\\right]-\\mathbb{E}\\left[\\tau^{\\prime}X_{t}(X)\\tau^{\\prime}\\right]}\\\\ &{=\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)\\dot{\\tau}\\big)^{2}}{n}-\\left(\\frac{1}{n}\\frac{\\sum_{i}}{\\big(n+1)}v^{T}x_{t}(x^{t})v\\right)^{2}}\\\\ &{=\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)\\dot{\\tau}\\big)^{2}}{n}-\\left(v^{T}x_{t}(X)v\\right)^{2}}\\\\ &{=\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)v\\big)^{2}}{n}-\\left(v^{T}x_{t}(X)v\\right)^{2}}\\\\ &{=\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)v\\big)^{2}}{n}-\\frac{2v^{T}x_{t}(i)v\\tau^{\\prime}\\big(2v^{T}x_{t}(i)v\\big)^{2}}{n}}\\\\ &{\\quad-\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)v\\big)^{2}-2v^{T}x_{t}(i)\\tau^{\\prime}\\big(2v^{T}x_{t}(i)v\\big)^{2}\\big(\\tau^{\\prime}x_{t}(i)v+(v^{T}x_{t}(x)v\\big)^{2}\\big)}{n}}\\\\ &{\\quad+\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)v^{T}x_{t}(i)v\\big)^{2}\\tau_{X}(X)\\tau+2(v^{T}x_{t}(X)\\tau^{\\prime})}{n}}\\\\ &{=\\frac{1}{n}\\frac{\\sum_{i=1}^{n}\\big(v^{T}x_{t}(i)v^{T}x_{t}(i)v\\big)^{2}}{n}-\\tau^{\\prime}x_{t}(X)\\tau^{\\prime}}\\\\ &{\\quad+\\frac{2}{n}(v^{T}x_{t}(X)v)^{2}-2(v^{T}x_{t}(X)\\tau^{\\prime})}\\\\ & \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With this reformulation, the following implication clearly follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nV a r(v^{\\top}J_{X}(X)v)=0\\iff v^{\\top}\\tilde{J}_{X}(x_{i})v=0\\ \\ \\forall i=1,...,n.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This indicates that the first constraint in the QCQP from Equation (2) solves for a column vector that results in a zero variance term in the diagonal of the estimated Jacobian matrix, as desired. The additional constraints ensure that all of the column vectors added to H\u02c6 are linearly independent, fulfliling the full column rank constraint from Equation (1). Thus, continuously solving this QCQP is equivalent to solving the rank-constrained optimization problem from Equation (1). ", "page_idx": 18}, {"type": "text", "text": "C Details of Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Synthetic Data Sampling Procedure ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We establish the causal relationships between all nodes and their parents to follow the parametric function $f_{i}(\\boldsymbol{X})=||\\boldsymbol{X}||^{2}+\\mathcal{E}_{i}$ , where each $\\mathcal{E}_{i}$ is independently sampled from a mean-zero Gaussian distribution with variance uniformly distributed over [0.1, 1]. For each experiment, we generate random samples of the exogenous noise terms and produce the latent variables via the data generating procedure from Assumption 2. We perform min-max scaling such that every variable is within the range [0, 1]. We perform this scaling, since [33] warns of the fact that valid causal orderings can often be recovered by order of the variables\u2019 variance in synthetically generated data, and we wish to show our algorithm performs as desired without this seeming advantage. We randomly sampled $n\\times n$ full rank matrices and took the linear transformation of the latent variables on this matrix to derive the corresponding observational samples. ", "page_idx": 19}, {"type": "text", "text": "C.2 Implementation of QCQP Solver ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.2.1 Perfect Score Estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "With perfect score estimation, we solve each QCQP to global optimality efficiently using Gurobi optimization solvers [13] on an Apple M2 CPU with 8 cores. We continuously solve the QCQP indicated by Equation (2) with feasibility tolerance set to 0.001, until no feasible solutions remain. We continue by iteratively appending linearly independent column vectors of unit magnitude until $\\hat{H}$ is of the desired dimension. ", "page_idx": 19}, {"type": "text", "text": "C.2.2 Score Estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "When using data-driven score estimation methods, we are not guaranteed to find any column vector that solves the specific QCQP indicated by Equation (2) perfectly. To combat this challenge, we first prune the top $25\\%$ of de-meaned Jacobian estimates, ${\\tilde{J}}_{X}(x)$ , by Frobenius norm to remove outliers. We then solve for the minimum value $t$ such that $|v^{\\top}\\tilde{J}_{X}(x^{(m)})v|\\leq t$ optimizing over all $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ . Then, we use the same procedure as in the perfect score estimation case with feasibility tolerance set to $t+0.001$ to solve for the estimated matrix $\\hat{H}$ . ", "page_idx": 19}, {"type": "text", "text": "C.3 Implementation of Score Estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Stein Estimator. We implemented the second-order Stein estimator introduced in [5] to generate the point-wise estimates of the score\u2019s Jacobian matrices. We use RBF kernels with bandwidth value selected as the median of pairwise distances between points in $X$ . Our implementation is adapted from [22] and [35]. ", "page_idx": 19}, {"type": "text", "text": "SSM-VR Estimator. We implemented the sliced score matching with variance reduction (SSM-VR) model developed by [42] to generate functional score estimators. We then estimated the Jacobian of the score estimator using automatic differentiation. Our implementation is adapted from [48]. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We specified all the assumptions made in the paper in the abstract and introduction. The main claims are backed up by theorems and empirical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The limitations are discussed in the discussion. Detailed limitations of the empirical studies is provided in the experiment section. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The full set of assumptions are specified in separate environments in section 2.   \nThe complete proof is provided in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All the experimental details are provided in section 5 and Appendix C. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We uploaded the source code to reproduce simulated data and experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: These are provided in section 5 and Appendix C. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We reported error bars over multiple runs in section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is provided in Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We adhere to the ethics guidelines. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discussed this in discussion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work uses simulated data and presents a methodology without deploying models. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the used packages are cited and described. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We uploaded the source code to implement our methods. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We did not do any crowdsourcing experiments or research with human subjects Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Our work does not have study participants. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]