[{"type": "text", "text": "PFDiff: Training-free Acceleration of Diffusion Models through the Gradient Guidance of Past and Future ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Diffusion Probabilistic Models (DPMs) have shown remarkable potential in image   \n2 generation, but their sampling efficiency is hindered by the need for numerous   \n3 denoising steps. Most existing solutions accelerate the sampling process by propos  \n4 ing fast ODE solvers. However, the inevitable discretization errors of the ODE   \n5 solvers are significantly magnified when the number of function evaluations (NFE)   \n6 is fewer. In this work, we propose PFDiff, a novel training-free and orthogonal   \n7 timestep-skipping strategy, which enables existing fast ODE solvers to operate with   \n8 fewer NFE. Based on two key observations: a significant similarity in the model\u2019s   \n9 outputs at time step size that is not excessively large during the denoising process   \n0 of existing ODE solvers, and a high resemblance between the denoising process   \n11 and SGD. PFDiff, by employing gradient replacement from past time steps and   \n12 foresight updates inspired by Nesterov momentum, rapidly updates intermediate   \n13 states, thereby reducing unnecessary NFE while correcting for discretization errors   \n14 inherent in first-order ODE solvers. Experimental results demonstrate that PFDiff   \n15 exhibits flexible applicability across various pre-trained DPMs, particularly ex  \n16 celling in conditional DPMs and surpassing previous state-of-the-art training-free   \n17 methods. For instance, using DDIM as a baseline, we achieved 16.46 FID (4 NFE)   \n18 compared to 138.81 FID with DDIM on ImageNet 64x64 with classifier guidance,   \n19 and 13.06 FID (10 NFE) on Stable Diffusion with 7.5 guidance scale. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 In recent years, Diffusion Probabilistic Models (DPMs) [1\u20134] have demonstrated exceptional mod  \n22 eling capabilities across various domains including image generation [5\u20137], video generation [8],   \n23 text-to-image generation [9, 10], speech synthesis [11], and text-to-3D generation [12, 13]. They have   \n24 become a key driving force advancing deep generative models. DPMs initiate with a forward process   \n25 that introduces noise onto images, followed by utilizing a neural network to learn a backward process   \n26 that incrementally removes noise, thereby generating images [2, 4]. Compared to other generative   \n27 methods such as Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs)   \n28 [15], DPMs not only possess a simpler optimization target but also are capable of producing higher   \n29 quality samples [5]. However, the generation of high-quality samples via DPMs requires hundreds or   \n30 thousands of denoising steps, significantly lowering their sampling efficiency and becoming a major   \n31 barrier to their widespread application.   \n32 Existing techniques for rapid sampling in DPMs primarily fall into two categories. First, training  \n33 based methods [16\u201319], which can significantly compress sampling steps, even achieving single-step   \n34 sampling [19]. However, this compression often comes with a considerable additional training cost,   \n35 and these methods are challenging to apply to large pre-trained models. Second, training-free samplers   \n36 [20\u201330], which typically employ implicit or analytical solutions to Stochastic Differential Equations   \n37 (SDE)/Ordinary Differential Equations (ODE) for lower-error sampling processes. For instance, Lu   \n38 et al. [21, 22], by analyzing the semi-linear structure of the ODE solvers for DPMs, have sought to   \n39 analytically derive optimally the solutions for DPMs\u2019 ODE solvers. These training-free sampling   \n40 strategies can often be used in a plug-and-play fashion, compatible with existing pre-trained DPMs.   \n41 However, when the NFE is below 10, the discretization error of these training-free methods will be   \n42 significantly amplified, leading to convergence issues [21, 22], which can still be time-consuming.   \n43 To further enhance the sampling speed of DPMs, we have analyzed the potential for improvement   \n44 in existing training-free accelerated methods. Initially, we observed a notably high similarity in the   \n45 model\u2019s outputs for the existing ODE solvers of DPMs when time step size $\\Delta t$ is not extremely large,   \n46 as illustrated in Fig. 2a. This observation led us to utilize the gradients that have been computed   \n47 from past time steps to approximate current gradients, thereby reducing unnecessary estimation of   \n48 noise network. Furthermore, due to the similarities between the sampling process of DPMs and   \n49 Stochastic Gradient Descent (SGD) [33] as noted in Remark 1, we incorporated a foresight update   \n50 mechanism using Nesterov momentum [34], known for accelerating SGD training. Specifically, we   \n51 ingeniously employ prior observation to predict future gradients, then utilize the future gradients as a   \n52 \u201cspringboard\u201d to facilitate larger update step size $\\Delta t$ , as shown in Fig. 2b.   \n53 Motivated by these insights, we propose PFDiff, a timestep-skipping sampling algorithm that rapidly   \n54 updates the current intermediate state through the gradient guidance of past and future. Notably,   \n55 PFDiff is training-free and orthogonal to existing DPMs sampling algorithms, providing a new or  \n56 thogonal axis for DPMs sampling. Unlike previous orthogonal sampling algorithms that compromise ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "IfpNsorodK/tmp/a4365ac00b6e9670d342b2644d7dbabf0f1d68b9e50bd1fa10c464f9c45fab06.jpg", "img_caption": ["Text Prompts: Winter night with snow -covered rooftops and soft yellow lights. (Left) A Corgi running towards me in Times Square. (Right) ", "Figure 1: Sampling by conditional pre-trained DPMs [5, 9] using DDIM [20] and our method PFDiff (dashed box) with DDIM as a baseline, varying the number of function evaluations (NFE). ", "(b) Results from Guided-Diffusion [5] on ImageNet 64x64 [32] (Classifier Guidance, $s=1.0$ ) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "IfpNsorodK/tmp/0ef85fddde35728f5ada2211d2511c44a5b0bf13187a5cc7cf2cc6d23c8c6aca.jpg", "img_caption": ["(a) Gradient Changes in SDE/ODE Solvers "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "IfpNsorodK/tmp/7c2ce5e0fcf7841c1261755b114ad3507fd26855c7e7f412c98311ea19718f16.jpg", "img_caption": ["(b) Comparison of Sampling Trajectories "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: (a) The trend of the MSE of the noise network output $\\epsilon_{\\theta}(x_{t},t)$ over time step size $\\Delta t$ , where $\\eta$ in DDPM [2] comes from $\\bar{\\sigma}_{t}$ in Eq. (6). Solid lines: ODE solvers, dashed lines: SDE solvers. (b) Comparison of partial sampling trajectories between PFDiff-1 and a first-order ODE solver, where the update directions are guided by the tangent direction of the sampling trajectories. ", "page_idx": 2}, {"type": "text", "text": "57 sampling quality for speed [28], we prove that PFDiff corrects for errors in the sampling trajectories   \n58 of first-order ODE solvers. This improves sampling quality while reducing unnecessary NFE in   \n59 existing ODE solvers, as illustrated in Fig. 2b. To validate the orthogonality and effectiveness of   \n60 PFDiff, extensive experiments were conducted on both unconditional [2, 4, 20] and conditional [5, 9]   \n61 pre-trained DPMs, with the visualization experiment of conditional DPMs depicted in Fig. 1. The   \n62 results indicate that PFDiff significantly enhances the sampling performance of existing ODE solvers.   \n63 Particularly in conditional DPMs, PFDiff, using only DDIM as the baseline, surpasses the previous   \n64 state-of-the-art training-free sampling algorithms. ", "page_idx": 2}, {"type": "text", "text": "65 2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "66 2.1 Diffusion SDEs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "67 Diffusion Probabilistic Models (DPMs) [1\u20134] aim to generate $D$ -dimensional random variables   \n68 $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{D}$ that follow a data distribution $q(x_{0})$ . Taking Denoising Diffusion Probabilistic Models   \n69 (DDPM) [2] as an example, these models introduce noise to the data distribution through a forward   \n70 process defined over discrete time steps, gradually transforming it into a standard Gaussian distribution   \n71 $\\dot{\\boldsymbol{x}}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ . The forward process\u2019s latent variables $\\{x_{t}\\}_{t\\in[0,T]}$ are defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(x_{t}\\mid x_{0})=\\mathcal{N}(x_{t}\\mid\\alpha_{t}x_{0},\\sigma_{t}^{2}\\boldsymbol{I}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "72 where $\\alpha_{t}$ is a scalar function related to the time step $t$ , with $\\alpha_{t}^{2}+\\sigma_{t}^{2}=1$ . In the model\u2019s reverse pro  \n73 cess, DDPM utilizes a neural network model $p_{\\theta}(x_{t-1}\\mid x_{t})$ to approximate the transition probability   \n74 $q(x_{t-1}\\mid x_{t},x_{0})$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}\\big(x_{t-1}\\mid x_{t}\\big)=\\mathcal{N}(x_{t-1}\\mid\\mu_{\\theta}(x_{t},t),\\sigma_{\\theta}^{2}(t)I),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "75 where $\\sigma_{\\theta}^{2}(t)$ is defined as a scalar function related to the time step $t$ . By sampling from a standard   \n76 Gaussian distribution and utilizing the trained neural network, samples following the data distribution   \n77 $\\begin{array}{r}{p_{\\theta}(x_{0})=\\prod_{t=1}^{T}p_{\\theta}(x_{t-1}\\mid x_{t})}\\end{array}$ can be generated.   \n78 Furthermore, Song et al. [4] introduced SDE to model DPMs over continuous time steps, where the   \n79 forward process is defined as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=f(t)x_{t}\\mathrm{d}t+g(t)\\mathrm{d}w_{t},\\quad x_{0}\\sim q(x_{0}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "80 where $w_{t}$ represents a standard Wiener process, and $f$ and $g$ are scalar functions of the time step $t$ .   \n81 It\u2019s noteworthy that the forward process in Eq. (1) is a discrete form of Eq. (3), where f(t) = d lodgt \u03b1t   \n82 and 83 reverse process from time step $\\begin{array}{r}{g^{2}(t)=\\frac{\\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}-2\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}\\sigma_{t}^{2}}\\end{array}$ $T$ Song et al. [4] further demonstrated that there exists an equivalent to 0 for the forward process in Eq. (3): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[f(t)x_{t}-g^{2}(t)\\nabla_{x}\\log q_{t}(x_{t})\\right]\\mathrm{d}t+g(t)\\mathrm{d}\\bar{w}_{t},\\quad x_{T}\\sim q(x_{T}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "84 where $\\bar{w}$ denotes a standard Wiener process. In this reverse process, the only unknown is the score   \n85 function $\\nabla_{x}\\log{q_{t}(x_{t})}$ , which can be approximated through neural networks. ", "page_idx": 3}, {"type": "text", "text": "86 2.2 Diffusion ODEs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "87 In DPMs based on SDE, the discretization of the sampling process often requires a significant number   \n88 of time steps to converge, such as the $T=1000$ time steps used in DDPM [2]. This requirement   \n89 primarily stems from the randomness introduced at each time step by the SDE. To achieve a more   \n90 efficient sampling process, Song et al. [4] utilized the Fokker-Planck equation [35] to derive a   \n91 probability flow $O D E$ related to the SDE, which possesses the same marginal distribution at any given   \n92 time $t$ as the SDE. Specifically, the reverse process ODE derived from Eq. (3) can be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\bigg[f(t)x_{t}-\\frac{1}{2}g^{2}(t)\\nabla_{x}\\log q_{t}(x_{t})\\bigg]\\,\\mathrm{d}t,\\quad x_{T}\\sim q(x_{T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "93 Unlike SDE, ODE avoids the introduction of randomness, thereby allowing convergence to the data   \n94 distribution in fewer time steps. Song et al. [4] employed a high-order RK45 ODE solver [36],   \n95 achieving sample quality comparable to SDE at 1000 NFE with only 60 NFE. Furthermore, research   \n96 such as DDIM [20] and DPM-Solver [21] explored discrete ODE forms capable of converging in   \n97 fewer NFE. For DDIM, it breaks the Markov chain constraint on the basis of DDPM, deriving a new   \n98 sampling formula expressed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{t-1}=\\sqrt{\\alpha_{t-1}}\\left(\\frac{x_{t}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}(x_{t},t)}{\\sqrt{\\alpha_{t}}}\\right)+\\sqrt{1-\\alpha_{t-1}-\\bar{\\sigma}_{t}^{2}}\\epsilon_{\\theta}(x_{t},t)+\\bar{\\sigma}_{t}\\epsilon_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "99 where $\\bar{\\sigma}_{t}=\\eta\\sqrt{\\left(1-\\alpha_{t-1}\\right)/\\left(1-\\alpha_{t}\\right)}\\sqrt{1-\\alpha_{t}/\\alpha_{t-1}},$ , and $\\alpha_{t}$ corresponds to $\\alpha_{t}^{2}$ in Eq. (1). When   \n100 $\\eta=1$ , Eq. (6) becomes a form of DDPM [2]; when $\\eta=0$ , it degenerates into an ODE, the form   \n101 adopted by DDIM [20], which can obtain high-quality samples in fewer time steps.   \n102 Remark 1. In this paper, we regard the gradient $\\mathrm{d}\\bar{x}_{t}$ , the noise network output $\\epsilon_{\\theta}(x_{t},t)$ , and the   \n103 score function $\\nabla_{x}\\log{q_{t}(x_{t})}$ as expressing equivalent concepts. This is because Song et al. $[4]$   \n104 demonstrated that $\\epsilon_{\\theta}(x_{t},t)=-\\sigma_{t}\\nabla_{x}\\log q_{t}(x_{t})$ . Moreover, we have discovered that any first-order   \n105 solver of DPMs can be parameterized as $x_{t-1}\\,=\\,\\bar{x}_{t}\\,-\\,\\gamma_{t}\\mathrm{d}\\bar{x}_{t}+\\xi\\epsilon_{t}$ . Taking DDIM [20] as an   \n1 06 example, where x\u00aft = \u03b1t\u03b1\u2212t1 xt, \u03b3t = t\u03b1\u2212t1 $\\begin{array}{r}{\\gamma_{t}=\\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_{t}}-\\alpha_{t-1}}-\\sqrt{1-\\alpha_{t-1}},}\\end{array}$ , $\\mathrm{d}\\bar{x}_{t}\\,=\\,\\epsilon_{\\theta}\\big(x_{t},t\\big)$ , and $\\xi=0$   \n107 This indicates the similarity between $S G D$ and the sampling process of DPMs, a discovery also   \n108 implicitly suggested in the research of Xue et al. $[30]$ and Wang et al. $I37J$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "109 3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "110 3.1 Solving for reverse process diffusion ODEs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "111 By substituting $\\epsilon_{\\theta}(x_{t},t)=-\\sigma_{t}\\nabla_{x}\\log q_{t}(x_{t})$ [4], Eq. (5) can be rewritten as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}x_{t}}{\\mathrm{d}t}=s(\\epsilon_{\\theta}(x_{t},t),x_{t},t):=f(t)x_{t}+\\frac{g^{2}(t)}{2\\sigma_{t}}\\epsilon_{\\theta}(x_{t},t),\\quad x_{T}\\sim q(x_{T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "112 Given an initial value $x_{T}$ , we define the time steps $\\{t_{i}\\}_{i=0}^{T}$ to progressively decrease from $t_{0}=T$   \n113 to $t_{T}=0$ . Let $\\tilde{x}_{t_{0}}=x_{T}$ be the initial value. Using $T$ steps of iteration, we compute the sequence   \n114 $\\{\\tilde{x}_{t_{i}}\\}_{i=0}^{T}$ to obtain the solution of this ODE. By integrating both sides of Eq. (7), we can obtain the   \n115 exact solution of this sampling ODE. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i}}=\\tilde{x}_{t_{i-1}}+\\int_{t_{i-1}}^{t_{i}}s(\\epsilon_{\\theta}(x_{t},t),x_{t},t)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "116 For any $p$ -order ODE solver, Eq. (8) can be discretely represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i-1}\\rightarrow t_{i}}\\approx\\tilde{x}_{t_{i-1}}+\\sum_{n=0}^{p-1}h(\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n}),\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\cdot\\Delta\\hat{t},\\quad i\\in[1,\\dots,T],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "117 where $\\hat{t}_{0}\\,=\\,t_{i-1}$ , $\\hat{t}_{p}\\,=\\,t_{i}$ , and $\\Delta\\hat{t}=\\hat{t}_{n+1}-\\hat{t}_{n}$ denote the time step size. The function $h$ rep  \n118 resents the different solution methodologies applied by various $p$ -order ODE solvers to the func  \n119 tion $s$ . For the Euler-Maruyama solver [38], $h$ is the identity mapping of $s$ . Further, we define   \n120 $\\begin{array}{r}{\\phi(Q,\\tilde{x}_{t_{i-1}},t_{i-1},t_{i}):=\\tilde{x}_{t_{i-1}}+\\sum_{n=0}^{p-1}h(\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n}),\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\cdot\\Delta\\hat{t}}\\end{array}$ . Here, $\\phi$ is any $p$ -order ODE   \n121 solver, and buffer $Q=\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i-1},t_{i}\\right)$ , where $\\hat{t}_{0}=t_{i-1}$ and $\\hat{t}_{p}=t_{i}$ .   \n122 When using the ODE solver defined in Eq. (9) for sampling, the choice of $T\\,=\\,1000$ leads to   \n123 significant inefficiencies in DPMs. The study on DDIM [20] first revealed that by constructing a new   \n124 forward sub-state sequence of length $M+1$ $(M\\leq T)$ , $\\{\\tilde{x}_{t_{i}}\\}_{i=0}^{M}$ , from a subsequence of time steps   \n125 $[0,\\dots,T]$ and reversing this sub-state sequence, it is possible to converge to the data distribution in   \n126 fewer time steps. However, as illustrated in Fig. 2a, for ODE solvers, as the time step $\\Delta t=t_{i}-t_{i-1}$   \n127 increases, the gradient direction changes slowly initially, but undergoes abrupt changes as $\\Delta t\\rightarrow T$ .   \n128 This phenomenon indicates that under minimal NFE (i.e., maximal time step size $\\Delta t$ ) conditions, the   \n129 discretization error in Eq. (9) is significantly amplified. Consequently, existing ODE solvers, when   \n130 sampling under minimal NFE, must sacrifice sampling quality to gain speed, making it an extremely   \n131 challenging task to reduce NFE to below 10 [21, 22]. Given this, we aim to develop an efficient   \n132 timestep-skipping sampling algorithm, which reduces NFE while correcting discretization errors,   \n133 thereby ensuring that sampling quality is not compromised, and may even be improved. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "134 3.2 Sampling guided by past gradients ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "135 For any $p$ -order timestep-skipping sampling algorithm for DPMs, the sampling process can be   \n136 reformulated according to Eq. (9) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i}}\\approx\\phi(Q,\\tilde{x}_{t_{i-1}},t_{i-1},t_{i}),\\quad i\\in[1,\\ldots,M],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "137 where buffer $Q\\,=\\,\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i-1},t_{i}\\right)$ and $[1,\\L\\cdot\\cdot,M]$ is an increasing subsequence of   \n138 $[1,\\dots,T]$ . As illustrated in Fig. 2a, when the time step size $\\Delta t$ (i.e., $t_{i}-t_{i-1})$ is not excessively   \n139 large, the MSE of the noise network, defined as $\\begin{array}{r}{\\frac{1}{T-\\Delta t}\\sum_{t=0}^{T-\\Delta t-1}\\|\\epsilon_{\\theta}(x_{t},t)-\\epsilon_{\\theta}(x_{t+\\Delta t},t+\\Delta t)\\|^{2}}\\end{array}$ , is   \n140 remarkably similar. This phenomenon is especially pronounced in ODE-based sampling algorithms,   \n141 such as DDIM [20] and DPM-Solver [21]. This observation suggests that there are many unnecessary   \n142 time steps in ODE-based sampling methods during the complete sampling process (e.g., when   \n143 $T=1000)$ ), which is one of the reasons these methods can generate samples in fewer steps. Based on   \n144 this, we propose replacing the noise network of the current timestep with the output from a previous   \n145 timestep to reduce unnecessary NFE without compromising the quality of the final generated samples.   \n146 Initially, we store the output of the previous timestep\u2019s noise network in a buffer as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ\\gets\\sum_{i=0}^{\\mathrm{buffer}}\\;\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i-1},t_{i}\\right),\\quad\\mathrm{where~}\\hat{t}_{0}=t_{i-1},\\hat{t}_{p}=t_{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "147 Then, in the current timestep, we directly use the noise network output saved in the buffer from   \n148 the previous timestep to replace the current timestep\u2019s noise network output, thereby updating the   \n149 intermediate states to the next timestep, as detailed below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i+1}}\\approx\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+1}),\\quad\\mathrm{where}\\;Q=\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i-1},t_{i}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "150 By using this approach, we can effectively accelerate the sampling process, reduce unnecessary NFE,   \n151 and ensure the quality of the samples is not affected. The convergence proof is in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "152 3.3 Sampling guided by future gradients ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "153 As stated in Remark 1, considering the similarities between the sampling process of DPMs and SGD   \n154 [33], we introduce a foresight update mechanism of Nesterov momentum, utilizing future gradient   \n155 information as a \u201cspringboard\u201d to assist the current intermediate state in achieving more efficient   \n156 leapfrog updates. Specifically, for the intermediate state $\\tilde{x}_{t_{i+1}}$ predicted using past gradients as   \n157 discussed in Sec. 3.2, we first estimate the future gradient and update the current buffer as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q\\overbrace{\\mathrm{~\\t~e~}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\Big\\}_{n=0}^{p-1},t_{i+1},t_{i+2}\\right),\\quad\\mathrm{where~}\\hat{t}_{0}=t_{i+1},\\hat{t}_{p}=t_{i+2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 Subsequently, leveraging the concept of foresight updates, we predict a further future intermedi  \n159 ate state $\\Tilde{x}_{t_{i+2}}$ using the current intermediate state $\\tilde{x}_{t_{i}}$ along with the future gradient information   \n160 corresponding to $\\tilde{x}_{t_{i+1}}$ , as shown below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i+2}}\\approx\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+2}),\\quad\\mathrm{where~}Q=\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+1},t_{i+2}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 Furthermore, Zhou et al. [39] performed a Principal Component Analysis (PCA) on the sampling   \n162 trajectories generated by ODE solvers for DPMs and discovered they almost lie in a two-dimensional   \n163 plane embedded within a high-dimensional space. This implies that the Mean Value Theorem   \n164 approximately holds during the sampling process using ODE solvers. Specifically, updating the   \n165 current intermediate state $\\tilde{x}_{t_{i}}$ at an optimal time point $s$ with the corresponding gradient information,   \n166 ground truth $\\epsilon_{\\theta}(\\tilde{x}_{t_{s}},t_{s})$ , results in the smallest update error, where $s$ is between time points $i$ and   \n167 $i+2$ . Further, we can reason that for any first-order ODE solver, under the same time step, the use   \n168 of future gradient information $\\epsilon_{\\theta}(\\tilde{x}_{t_{i+1}},t_{i+1})$ from Eq. (13) to update the current intermediate state   \n169 $\\tilde{x}_{t_{i}}$ results in a smaller sampling error compared to using the gradient information at the current   \n170 time point $\\epsilon_{\\theta}(\\tilde{x}_{t_{i}},t_{i})$ . A detailed proof is provided in Appendix B.2. However, for higher-order   \n171 ODE solvers, the solving process implicitly utilizes future gradients as mentioned in Sec. 3.5, and   \n172 the additional explicit introduction of future gradients increases sampling error. Therefore, when   \n173 using higher-order ODE solvers as a baseline, the sampling process is accelerated by only using past   \n174 gradients. It is only necessary to modify Eq. (14) to $\\tilde{x}_{t_{i+2}}\\approx\\phi(Q,\\tilde{x}_{t_{i+1}},t_{i+1},t_{i+2})$ while keeping $Q$   \n175 constant. Ablation experiments can be found in Sec. 4.3. ", "page_idx": 5}, {"type": "text", "text": "176 3.4 PFDiff: sampling guided by past and future gradients ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "177 Combining Sec. 3.2 and Sec. 3.3, the intermediate state $\\tilde{x}_{t_{i+1}}$ obtained through Eq. (12) is used to   \n178 update the buffer $Q$ in Eq. (13). In this way, we achieve our proposed efficient timestep-skipping   \n179 algorithm, which we name PFDiff, as shown in Algorithm 1. For higher-order ODE solvers $(p>1)$ ),   \n180 PFDiff only utilizes past gradient information, while for first-order ODE solvers $(p=1)$ ), it uses   \n181 both past and future gradient information to predict further future intermediate states. Notably,   \n182 during the iteration from intermediate state $\\tilde{x}_{t_{i}}$ to $\\Tilde{x}_{t_{i+2}}$ , we only perform a single batch computation   \n183 $\\left(\\mathrm{NFE}=p\\right)$ ) of the noise network in Eq. (13). Furthermore, we propose that in a single iteration   \n184 process, $\\Tilde{x}_{t_{i+2}}$ in Eq. (14) can be modified to $\\tilde{x}_{t_{i+(k+1)}}$ , achieving a $k$ -step skip to sample more distant   \n185 future intermediate states. Additionally, when $k\\neq1$ , the buffer $Q$ , which acts as an intermediate   \n186 \u201cspringboard\u201d from Eq. (13), has various computational origins. This can be accomplished by   \n187 modifying $\\tilde{x}_{t_{i+1}}$ in Eq. (12) to $\\tilde{x}_{t_{i+l}}$ . We collectively refer to this multi-step skipping and different   \n188 \u201cspringboard\u201d selection strategy as PFDiff- $k_{-}l$ $[l\\leq k)$ . Further algorithmic details can be found   \n189 in Appendix C. Finally, through the comparison of sampling trajectories between PFDiff-1 and   \n190 a first-order ODE sampler, as shown in Fig. 2b, PFDiff-1 showcases its capability to correct the   \n191 sampling trajectory of the first-order ODE sampler while reducing the NFE.   \n192 Proposition 3.1. For any given DPM first-order ODE solver $\\phi_{i}$ , the PFDiff- $k\\_l$ algorithm can   \n193 describe the sampling process within an iteration cycle through the following formula: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i+(k+1)}}\\approx\\phi\\big(\\epsilon_{\\theta}\\big(\\phi\\big(\\epsilon_{\\theta}\\big({_{i-(k-l+1)}},t_{i-(k-l+1)}\\big),\\tilde{x}_{t_{i}},t_{i},t_{i+l}\\big),t_{i+l}\\big),\\tilde{x}_{t_{i}},t_{i},t_{i+(k+1)}\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 PFDiff-1   \nRequire: initial value $x_{T}$ , NFE $N$ , model $\\epsilon_{\\theta}$ , any $p$ -order solver $\\phi$   \n1: Define time steps $\\{t_{i}\\}_{i=0}^{M}$ with $M=2N-1p$   \n2: $\\tilde{x}_{t_{0}}\\gets x_{T}$   \n3: $\\begin{array}{r}{Q\\stackrel{\\mathrm{buffer}}{\\overbrace{\\sum_{\\ell=1}^{\\infty}\\left(\\sum_{\\ell_{n}}(\\tilde{x}_{\\ell_{n}},\\hat{t}_{n})\\right)_{n=0}^{p-1},t_{0},t_{1}\\right)}}}\\end{array}$ , where $\\hat{t}_{0}=t_{0},\\hat{t}_{p}=t_{1}$ \u25b7Initialize buffer   \n4: $\\tilde{x}_{t_{1}}=\\phi(Q,\\tilde{x}_{t_{0}},t_{0},t_{1})$   \n5: for $i\\gets1$ to $\\frac{M}{p}\\,-\\,2\\,\\mathbf{do}$   \n6: if $(i-1)$ mod $2=0$ then   \n7: $\\tilde{x}_{t_{i+1}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+1})$ $\\triangleright$ Updating guided by past gradients   \n8: $\\begin{array}{r}{Q\\stackrel{\\mathrm{buffer}}{\\overleftarrow{}}\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+1},t_{i+2}\\right)}\\end{array}$ \u25b7Update buffer (overwrite)   \n9: if $p=1$ then   \n10: $\\tilde{x}_{t_{i+2}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+2})$ \u25b7Anticipatory updating guided by future gradients   \n11: else if $p>1$ then   \n12: $\\tilde{x}_{t_{i+2}}=\\phi(Q,\\tilde{x}_{t_{i+1}},t_{i+1},t_{i+2})$ $\\triangleright$ The higher-order solver uses only past gradients   \n13: end if   \n14: end if   \n15: end for   \n16: return $\\tilde{x}_{t_{M}}$   \n194 where the value of $\\epsilon_{\\theta}(\\tilde{x}_{t_{i-(k-l+1)}},t_{\\underline{{i}}-(k-l+1)})$ can be directly obtained from the buffer $Q$ , without the   \n195 need for additional computations. The iterative process defined by Eq. (15) ensures that the sampling   \n196 outcomes converge to the data distribution consistent with the solver $\\phi$ , while effectively correcting   \n197 errors in the sampling process (Proof in Appendix $B$ ).   \n198 It is noteworthy that, although the PFDiff is conceptually orthogonal to the SDE/ODE solvers of   \n199 DPMs, even when the time size $\\Delta t$ is relatively small, the MSE of the noise network in the SDE   \n200 solver exhibits significant differences, as shown in Fig. 2a. Consequently, PFDiff shows marked   \n201 improvements on the ODE solver, and our experiments are almost exclusively based on ODE solvers,   \n202 with exploratory experiments on SDE solvers referred to Sec. 4.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "203 3.5 Connection with other samplers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "204 Relationship with $p$ -order solver [21, 22, 27]. According to Eq. (10), a single iteration of the   \n205 $p$ -order solver can be represented as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i+1}}\\approx\\mathrm{Solver-p}(\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i},t_{i+1}\\right),\\tilde{x}_{t_{i}},t_{i},t_{i+1}),\\quad i\\in[0,\\ldots,M-1].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "206 A single iteration of the $p$ -order solver uses $p$ NFE to predict the next intermediate state. The   \n207 intermediate step gradients obtained during this process can be considered as an approximation of   \n208 future gradients. This approximation is implicitly contained within the sampling guided by future   \n209 gradients that we propose. Furthermore, as shown in Eq. (15), a single iteration update of PFDiff   \n210 based on a first-order solver can be seen as using a 2-order solver with only one NFE. ", "page_idx": 6}, {"type": "text", "text": "211 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "212 In this section, we validate the effectiveness of PFDiff as an orthogonal and training-free sampler   \n213 through a series of extensive experiments. This sampler can be integrated with any order of ODE   \n214 solvers, thereby significantly enhancing the sampling efficiency of various types of pre-trained DPMs.   \n215 To systematically showcase the performance of PFDiff, we categorize the pre-trained DPMs into two   \n216 main types: conditional and unconditional. Unconditional DPMs are further subdivided into discrete   \n217 and continuous, while conditional DPMs are subdivided into classifier guidance and classifier-free   \n218 guidance. In choosing ODE solvers, we utilized the widely recognized first-order DDIM [20],   \n219 Analytic-DDIM [23], and the higher-order DPM-Solver [21] as baselines. For each experiment, we   \n220 use the Fr\u00e9chet Inception Distance $(\\mathrm{FID}\\downarrow)$ [40] as the primary evaluation metric, and provide the   \n221 experimental results of the Inception Score $(\\mathrm{IS}\\uparrow)$ ) [41] in the Appendix D.7 for reference. Lastly,   \n222 apart from the ablation studies on parameters $k$ and $l$ discussed in Sec. 4.3, we showcase the optimal   \n223 results of PFDiff- $k\\_l$ (where $k\\,=\\,1,2,3$ and $l\\leq k$ ) across six configurations as a performance   \n224 demonstration of PFDiff. As described in Appendix C, this does not increase the computational   \n225 burden in practical applications. All experiments were conducted on an NVIDIA RTX 3090 GPU. ", "page_idx": 6}, {"type": "text", "text": "226 4.1 Unconditional sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "227 For unconditional DPMs, we selected discrete DDPM [2] and DDIM [20], as well as pre-trained   \n228 models from continuous ScoreSDE [4], to assess the effectiveness of PFDiff. For these pre-trained   \n229 models, all experiments sampled 50k instances to compute evaluation metrics.   \n230 For unconditional discrete DPMs, we first select first-order ODE solvers DDIM [20] and Analytic  \n231 DDIM [23] as baselines, while implementing SDE-based DDPM [2] and Analytic-DDPM [23]   \n232 methods for comparison, where $\\eta=1.0$ is from $\\bar{\\sigma}_{t}$ in Eq. (6). We conduct experiments on the   \n233 CIFAR10 [42] and CelebA $64\\mathrm{x}64$ [43] datasets using the quadratic time steps employed by DDIM. By   \n234 varying the NFE from 6 to 20, the evaluation metric $\\mathrm{FID}\\downarrow$ is shown in Figs. 3a and 3b. Additionally,   \n235 experiments with uniform time steps are conducted on the CelebA $64\\mathrm{x}64$ , LSUN-bedroom $256\\mathrm{x}256$   \n236 [44], and LSUN-church 256x256 [44] datasets, with more results available in Appendix D.2. Our   \n237 experimental results demonstrate that PFDiff, based on pre-trained models of discrete unconditional   \n238 DPMs, significantly improves the sampling efficiency of DDIM and Analytic-DDIM samplers across   \n239 multiple datasets. For instance, on the CIFAR10 dataset, PFDiff combined with DDIM achieves a   \n240 FID of 4.10 with only 15 NFE, comparable to DDIM\u2019s performance of 4.04 FID with 1000 NFE. This   \n241 is something other time-step skipping algorithms [23, 28] that sacrifice sampling quality for speed ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "IfpNsorodK/tmp/1fb9ca480103484e382f8a63251ee756d5c54d1752326d9e9985a10271dc1a2f.jpg", "img_caption": ["Figure 3: Unconditional sampling results. We report the $\\mathrm{FID}\\downarrow$ for different methods by varying the number of function evaluations (NFE), evaluated on $50\\mathrm{k}$ samples. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "IfpNsorodK/tmp/ac701ad83e2779bfebda00f715c41331a1259b86a41a4e92ec651ef5972712f3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Conditional sampling results. We report the $\\mathrm{FID\\downarrow}$ for different methods by varying the NFE. Evaluated: ImageNet 64x64 with $50\\mathrm{k}$ , others with $10\\mathbf{k}$ samples. \u2217AutoDiffusion [26] method requires additional search costs. \u2020We borrow the results reported in DPM-Solver-v3 [27] directly. ", "page_idx": 7}, {"type": "text", "text": "242 cannot achieve. Furthermore, in Appendix D.2, by varying $\\eta$ from 1.0 to 0.0 in Eq. (6) to control the   \n243 scale of noise introduced by SDE, we observe that as $\\eta$ decreases (reducing noise introduction), the   \n244 performance of PFDiff gradually improves. This once again validates our assumption proposed in   \n245 Sec. 3.2, based on Fig. 2a, that there is a significant similarity in the model\u2019s outputs at the time step   \n246 size that is not excessively large for the existing ODE solvers.   \n247 For unconditional continuous DPMs, we choose the DPM-Solver-1, -2 and -3 [21] as the baseline   \n248 to verify the effectiveness of PFDiff as an orthogonal timestep-skipping algorithm on the first and   \n249 higher-order ODE solvers. We conducted experiments on the CIFAR10 [42] using quadratic time   \n250 steps, varying the NFE. The experimental results using $\\mathrm{FID\\downarrow}$ as the evaluation metric are shown in Fig.   \n251 3c. More experimental details can be found in Appendix D.3. We observe that PFDiff consistently   \n252 improves the sampling performance over the baseline with fewer NFE settings, particularly in cases   \n253 where higher-order ODE solvers fail to converge with a small NFE (below 10) [21]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "254 4.2 Conditional sampling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "255 For conditional DPMs, we selected the pre-trained models of the widely recognized classifier guidance   \n256 paradigm, ADM-G [5], and the classifier-free guidance paradigm, Stable-Diffusion [9], to validate   \n257 the effectiveness of PFDiff. We employed uniform time steps setting and the DDIM [20] ODE solver   \n258 as a baseline across all datasets. Evaluation metrics were computed by sampling 50k samples on the   \n259 ImageNet 64x64 [32] dataset for ADM-G and 10k samples on other datasets, including ImageNet   \n260 $256\\mathrm{x}256$ [32] in ADM-G and MS-COCO2014 [31] in Stable-Diffusion.   \n261 For conditional DPMs employing the classifier guidance paradigm, we conducted experiments on the   \n262 ImageNet $64\\mathrm{x}64$ dataset [32] with a guidance scale (s) set to 1.0. For comparison, we implemented   \n263 DPM-Solver-2 and -3 [21], and DPM-Solver++(2M) [22], which exhibit the best performance on   \n264 conditional DPMs. Additionally, we introduced the AutoDiffusion method [26] using DDIM as a   \n265 baseline for comparison, noting that this method incurs additional search costs. We compared $\\mathrm{FID\\downarrow}$   \n266 scores by varying the NFE as depicted in Fig. 4a, with corresponding visual comparisons shown   \n267 in Fig. 1b. We observed that PFDiff reduced the FID from 138.81 with 4 NFE in DDIM to 16.46,   \n268 achieving an $88.14\\%$ improvement in quality. The visual results in Fig. 1b further demonstrate that, at   \n269 the same NFE setting, PFDiff achieves higher-quality sampling. Furthermore, we evaluated PFDiff\u2019s   \n270 sampling performance based on DDIM on the large-scale ImageNet 256x256 dataset [32]. Detailed   \n271 results are provided in Appendix D.4.   \n272 For conditional, classifier-free guidance paradigms of DPMs, we employed the sd-v1-4 checkpoint   \n273 and computed the $\\mathrm{FID\\downarrow}$ scores on the validation set of MS-COCO2014 [31]. We conducted experi  \n274 ments with a guidance scale (s) set to 7.5 and 1.5. For comparison, we implemented DPM-Solver-2   \n275 and -3 [21], and DPM-Solver $^{++}$ (2M) [22] methods. At $s=7.5$ , we introduced the state-of-the-art   \n276 method reported in DPM-Solver-v3 [27] for comparison, along with DPM-Solver $^{++}$ (2M) [22], UniPC   \n277 [29], and DPM-Solver-v3(2M) [27]. The $\\mathrm{FID\\downarrow}$ metrics by varying the NFE are presented in Figs. 4b   \n278 and 4c, with additional visual results illustrated in Fig. 1a. We observed that PFDiff, solely based   \n279 on DDIM, achieved state-of-the-art results during the sampling process of Stable-Diffusion, thus   \n280 demonstrating the efficacy of PFDiff. Further experimental details can be found in Appendix D.5. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "281 4.3 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "282 We conducted ablation experiments on the six different algorithm configurations of PFDiff mentioned   \n283 in Appendix C, with $k\\ =\\ 1,2,3$ $(l\\ \\leq\\ k)$ . Specifically, we evaluated the $\\mathrm{FID}\\downarrow$ scores on the   \n284 unconditional and conditional pre-trained DPMs [2, 4, 5, 9] by varying the NFE. Detailed experimental   \n285 setups and results can be found in Appendix D.6.1. The experimental results indicate that for various   \n286 pre-trained DPMs, the choice of parameters $k$ and $l$ is not critical, as most combinations of $k$ and $l$   \n287 within PFDiff can enhance the sampling efficiency over the baseline. Moreover, with $k=1$ fixed,   \n288 PFDiff-1 can significantly improve the baseline\u2019s sampling quality within the range of $8{\\sim}20\\$ NFE.   \n289 For even better sampling quality, one can sample a small subset of examples (e.g., 5k) to compute   \n290 evaluation metrics or directly conduct visual analysis, easily identifying the most effective $k$ and $l$   \n291 combinations.   \n292 To validate the PFDiff algorithm as mentioned in Sec. 3.3, which necessitates the joint guidance   \n293 of past and future gradients for first-order ODE solvers, and only past gradients for higher-order   \n294 ODE solvers, offering a more effective means of accelerating baseline sampling. This study employs   \n295 the first-order ODE solver DDIM [20] as the baseline, isolating the effects of both past and future   \n296 gradients, and uses the higher-order ODE solver DPM-Solver [21] as the baseline, removing the   \n297 influence of future gradients for ablation experiments. Specific experimental configurations and   \n298 results are shown in Appendix D.6.2. The results indicate that, as described by the PFDiff algorithm   \n299 in Sec. 3.3, it is possible to further enhance the sampling efficiency of ODE solvers of any order. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "300 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "301 In this paper, based on the recognition that the ODE solvers of DPMs exhibit significant similarity in   \n302 model outputs when the time step size is not excessively large, and with the aid of a foresight update   \n303 mechanism, we propose PFDiff, a novel method that leverages the gradient guidance from both past   \n304 and future to rapidly update the current intermediate state. This approach effectively reduces the   \n305 unnecessary number of function evaluations (NFE) in the ODE solvers and significantly corrects the   \n306 errors of first-order ODE solvers during the sampling process. Extensive experiments demonstrate   \n307 the orthogonality and efficacy of PFDiff on both unconditional and conditional pre-trained DPMs,   \n308 especially in conditional pre-trained DPMs where PFDiff outperforms previous state-of-the-art   \n309 training-free sampling methods.   \n310 Limitations and broader impact Although PFDiff can effectively accelerate the sampling speed of   \n311 existing ODE solvers, it still lags behind the sampling speed of training-based acceleration methods   \n312 and one-step generation paradigms such as GANs. Moreover, there is no universal setting for the   \n313 optimal combination of parameters $k$ and $l$ in PFDiff; adjustments are required according to different   \n314 pre-trained DPMs and NFE. It is noteworthy that PFDiff may be utilized to accelerate the generation   \n315 of malicious content, thereby having a detrimental impact on society. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "316 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "317 [1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised   \n318 learning using nonequilibrium thermodynamics. In International conference on machine learning, pages   \n319 2256\u20132265. PMLR, 2015.   \n320 [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural   \n321 information processing systems, 33:6840\u20136851, 2020.   \n322 [3] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.   \n323 Advances in neural information processing systems, 32, 2019.   \n324 [4] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben   \n325 Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint   \n326 arXiv:2011.13456, 2020.   \n327 [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in   \n328 neural information processing systems, 34:8780\u20138794, 2021.   \n329 [6] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.   \n330 Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research,   \n331 23(47):1\u201333, 2022.   \n332 [7] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the   \n333 IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n334 [8] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,   \n335 Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n\u2019pack: Navit, a   \n336 vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems,   \n337 36, 2024.   \n338 [9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution   \n339 image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer   \n340 vision and pattern recognition, pages 10684\u201310695, 2022.   \n341 [10] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang   \n342 Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science.   \n343 https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.   \n344 [11] Kaitao Song, Yichong Leng, Xu Tan, Yicheng Zou, Tao Qin, and Dongsheng Li. Transcormer: Transformer   \n345 for sentence scoring with sliding language modeling. Advances in Neural Information Processing Systems,   \n346 35:11160\u201311174, 2022.   \n347 [12] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.   \n348 arXiv preprint arXiv:2209.14988, 2022.   \n349 [13] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,   \n350 Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In   \n351 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309,   \n352 2023.   \n353 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron   \n354 Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing   \n355 systems, 27, 2014.   \n356 [15] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,   \n357 2013.   \n358 [16] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv   \n359 preprint arXiv:2202.00512, 2022.   \n360 [17] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer   \n361 data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \n362 [18] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan:   \n363 Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022.   \n364 [19] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint   \n365 arXiv:2303.01469, 2023.   \n366 [20] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint   \n367 arXiv:2010.02502, 2020.   \n368 [21] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode   \n369 solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information   \n370 Processing Systems, 35:5775\u20135787, 2022.   \n371 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solve $^{++}$ : Fast solver   \n372 for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n373 [23] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal   \n374 reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022.   \n375 [24] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo Zhang. Estimating the optimal covariance with   \n376 imperfect mean in diffusion probabilistic models. arXiv preprint arXiv:2206.07309, 2022.   \n377 [25] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on   \n378 manifolds. arXiv preprint arXiv:2202.09778, 2022.   \n379 [26] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao,   \n380 and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated   \n381 diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer   \n382 Vision, pages 7105\u20137114, 2023.   \n383 [27] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with   \n384 empirical model statistics. arXiv preprint arXiv:2310.13268, 2023.   \n385 [28] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv   \n386 preprint arXiv:2312.00858, 2023.   \n387 [29] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector   \n388 framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36,   \n389 2024.   \n390 [30] Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma.   \n391 Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural Information   \n392 Processing Systems, 36, 2024.   \n393 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,   \n394 and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:   \n395 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages   \n396 740\u2013755. Springer, 2014.   \n397 [32] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical   \n398 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.   \n399 Ieee, 2009.   \n400 [33] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical   \n401 statistics, pages 400\u2013407, 1951.   \n402 [34] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o $(1/\\mathbf{k}^{**}\\*2)$ .   \n403 Doklady Akademii Nauk SSSR, 269(3):543, 1983.   \n404 [35] Bernt \u00d8ksendal and Bernt \u00d8ksendal. Stochastic differential equations. Springer, 2003.   \n405 [36] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational   \n406 and applied mathematics, 6(1):19\u201326, 1980.   \n407 [37] Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, and Yang You. Neural   \n408 network diffusion. arXiv preprint arXiv:2402.13144, 2024.   \n409 [38] Peter E Kloeden, Eckhard Platen, Peter E Kloeden, and Eckhard Platen. Stochastic differential equations.   \n410 Springer, 1992.   \n411 [39] Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ode-based sampling for diffusion models in   \n412 around 5 steps. arXiv preprint arXiv:2312.00094, 2023.   \n413 [40] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans   \n414 trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information   \n415 processing systems, 30, 2017.   \n416 [41] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved   \n417 techniques for training gans. Advances in neural information processing systems, 29, 2016.   \n418 [42] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n419 [43] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In   \n420 Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738, 2015.   \n421 [44] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Con  \n422 struction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint   \n423 arXiv:1506.03365, 2015.   \n424 [45] Elliott Ward Cheney, EW Cheney, and W Cheney. Analysis for applied mathematics, volume 1. Springer,   \n425 2001.   \n427 While the solvers for Diffusion Probabilistic Models (DPMs) are categorized into two types, SDE and   \n428 ODE, most current accelerated sampling techniques are based on ODE solvers due to the observation   \n429 that the stochastic noise introduced by SDE solvers hampers rapid convergence. ODE-based solvers   \n430 are further divided into training-based methods [16\u201319] and training-free samplers [20\u201330]. Training  \n431 based methods can notably reduce the number of sampling steps required for DPMs. An example of   \n432 such a method is the knowledge distillation algorithm proposed by Song et al. [19], which achieves   \n433 one-step sampling for DPMs. This sampling speed is comparable to that of GANs [14] and VAEs   \n434 [15]. However, these methods often entail significant additional costs for distillation training. This   \n435 requirement poses a challenge when applying them to large pre-trained DPM models. Therefore, our   \n436 work primarily focuses on training-free, ODE-based accelerated sampling strategies.   \n437 Training-free accelerated sampling techniques based on ODE can generally be applied in a plug  \n438 and-play manner, adapting to existing pre-trained DPMs. These methods can be categorized based   \n439 on the order of the ODE solver\u2014that is, the NFE required per sampling iteration\u2014into first-order   \n440 [20, 23\u201325] and higher-order [21, 22, 27, 29, 36]. Typically, higher-order ODE solvers tend to sample   \n441 at a faster rate, but may fail to converge when the NFE is low (below 10), sometimes performing   \n442 even worse than first-order ODE solvers. In addition, there are orthogonal techniques for accelerated   \n443 sampling. For instance, Li et al. [26] build upon existing ODE solvers and use search algorithms to   \n444 find optimal sampling sub-sequences and model structures to further speed up the sampling process;   \n445 Ma et al. [28] observe that the low-level features of noise networks at adjacent time steps exhibit   \n446 similarities, and they use caching techniques to substitute some of the network\u2019s low-level features,   \n447 thereby further reducing the number of required time steps.   \n448 The algorithm we propose belongs to the class of training-free and orthogonal accelerated sampling   \n449 techniques, capable of further accelerating the sampling process on the basis of existing first-order   \n450 and higher-order ODE solvers. Compared to the aforementioned orthogonal sampling techniques,   \n451 even though the skipping strategy proposed by Ma et al. [28] effectively accelerates the sampling   \n452 process, it may do so at the cost of reduced sampling quality, making it challenging to reduce the   \n453 NFE below 50. Although Li et al. [26] can identify more optimal subsampling sequences and model   \n454 structures, this implies higher search costs. In contrast, our proposed orthogonal sampling algorithm   \n455 is more efficient in skipping time steps. First, our skipping strategy does not require extensive search   \n456 costs. Second, we can correct the sampling errors of first-order ODE solvers while reducing the   \n457 number of sampling steps required by existing ODE solvers, achieving more efficient accelerated   \n458 sampling. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "459 B Proof of convergence and error correction for PFDiff ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "460 In this section, we prove the convergence of PFDiff and elaborate on how it theoretically corrects   \n461 first-order ODE solver errors. To delve deeper into PFDiff, we propose the following assumptions:   \n462 Assumption B.1. When the time step size $\\Delta t=t_{i}-t_{i-(k-l+1)}$ is not excessively large, the output es  \n463 timates of the noise network based on the $p$ -order ODE solver at different time steps are approximately   \n464 the same, that is, $\\begin{array}{r}{\\bigg(\\big\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\big\\}_{n=0}^{p-1},t_{i},t_{i+l}\\bigg)\\approx\\bigg(\\big\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\big\\}_{n=0}^{p-1},t_{i-(k-l+1)},t_{i}\\bigg).}\\end{array}$ .   \n46 5 Assumption B.2. For the integral from time step ti to ti+(k+1), i tti+(k+1)s(\u03f5\u03b8(xt, t), xt, t)dt,   \n466 there exist intermediate time steps $t_{\\tilde{s}},t_{s}\\in(t_{i},t_{i+(k+1)})$ such that $\\begin{array}{r}{\\int_{t_{i}}^{t_{i+(k+1)}}s(\\epsilon_{\\theta}(x_{t},t),x_{t},t)\\mathrm{d}t=}\\end{array}$   \n467 $s(\\epsilon_{\\theta}(x_{t_{\\bar{s}}},t_{\\bar{s}}),x_{t_{\\bar{s}}},t_{\\bar{s}})\\cdot(t_{i+(k+1)}-t_{i})\\,=\\,h(\\epsilon_{\\theta}(x_{t_{s}},t_{s}),x_{t_{s}},t_{s})\\cdot(t_{i+(k+1)}-t_{i})$ holds, where the   \n468 definition of the function $h$ remains consistent with Sec. 3.1.   \n469 The first assumption is based on the observation in Fig. 2a that when $\\Delta t$ is not excessively large,   \n470 the MSE of the noise network remains almost unchanged across different time steps. The second   \n471 assumption is based on the Mean Value Theorem for Integrals, which states that if $f(x)$ is a continuous   \n472 real-valued function on a closed interval $[a,b]$ , then there exists at least one point $c\\in[a,b]$ such that   \n473 $\\begin{array}{r}{\\int_{a_{.}}^{b_{}}f(x)\\mathrm{d}x=f_{.}(c)(b-a)}\\end{array}$ holds. It is important to note that the Mean Value Theorem for Integrals   \n474 originally applies to real-valued functions and does not directly apply to vector-valued functions   \n475 [45]. However, the study by Zhou et al. [39] using Principal Component Analysis (PCA) on the   \n476 trajectories of the ODE solvers for DPMs demonstrates that these trajectories are primarily distributed   \n477 on a two-dimensional plane, which allows the Mean Value Theorem for Integrals to approximately   \n478 hold under Assumption B.2. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "479 B.1 Proof of convergence for sampling guided by past gradients ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "480 Starting from Eq. (8), we consider an iteration process of a $p$ -order ODE solver from $\\tilde{x}_{t_{i}}$ to $\\tilde{x}_{t_{i+l}}$ ,   \n481 where $l$ is the \u201cspringboard\u201d choice determined by PFDiff- $k_{-}l$ . This iterative process can be expressed   \n482 as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i+l}}=\\tilde{x}_{t_{i}}+\\int_{t_{i}}^{t_{i+l}}s(\\epsilon_{\\theta}(x_{t},t),x_{t},t)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "483 Discretizing Eq. (B.1) yields: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i}\\rightarrow t_{i+l}}\\approx\\tilde{x}_{t_{i}}+\\sum_{n=0}^{p-1}h(\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n}),\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\cdot(\\hat{t}_{n+1}-\\hat{t}_{n}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "484 where $\\hat{t}_{0}=t_{i}$ and $\\hat{t}_{p}=t_{i+l}$ . Consistent with Sec. 3.1, the function $h$ represents the different solution   \n485 methodologies applied by various $p$ -order ODE solvers to the function $s$ . To accelerate sampler con  \n486 vergence and reduce unnecessary evaluations of the NFE, we adopt Assumption B.1, namely guiding   \n487 the sampling of the current intermediate state by utilizing past gradient information. Specifically,   \n488 we approximate that $\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i},t_{i+l}\\right)\\approx\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{\\hat{t}_{n}})\\right\\}_{n=0}^{p-1},t_{i-(k-l+1)},t_{i}\\right)$ , where $k$   \n489 represents the number of steps skipped in one iteration by PFDiff- $k_{-}l$ . Eq. (B.2) can be rewritten as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{x}_{t_{i}\\to t_{i+l}}\\approx\\tilde{x}_{t_{i}}+\\displaystyle\\sum_{n=i}^{i+l-1}h\\big(\\epsilon_{\\theta}\\big(\\tilde{x}_{t_{n}},t_{n}\\big),\\tilde{x}_{t_{n}},t_{n}\\big)\\cdot\\big(t_{n+1}-t_{n}\\big)}\\\\ &{\\qquad\\qquad\\approx\\tilde{x}_{t_{i}}+\\displaystyle\\sum_{n=i-(k-l+1)}^{i-1}h\\big(\\epsilon_{\\theta}\\big(\\tilde{x}_{t_{n}},t_{n}\\big),\\tilde{x}_{t_{n}},t_{n}\\big)\\cdot\\big(t_{n+1}-t_{n}\\big)}\\\\ &{\\qquad\\qquad=\\phi\\big(\\Big(\\{\\epsilon_{\\theta}\\big(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n}\\big)\\big\\}_{n=0}^{p-1},t_{i-(k-l+1)},t_{i}\\Big)\\,,\\tilde{x}_{t_{i}},t_{i},t_{i+l}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "490 where $\\phi$ is any $p$ -order ODE solver. Eq. (B.3) demonstrates that under Assumption B.1, for any $p$ -   \n491 order ODE solver $\\phi$ , PFDiff- $k_{-}l$ utilizes past gradient information as a substitute for current gradient   \n492 information to update the current intermediate state. This method not only reduces the NFE but also   \n493 approximates the solution of $\\Tilde{x}_{t_{i+l}}$ , ensuring convergence to the data distribution corresponding to   \n494 the solver $\\phi$ . It is important to note that the sampling process described in Eq. (B.3) relies solely on   \n495 past gradient information and does not estimate the output of the noise network based on the current   \n496 intermediate state.   \n497 In particular, within Proposition 3.1 for any first-order $(p=1]$ ) ODE solver $\\phi$ , according to Eq.   \n498 (B.3), we can approximate $\\tilde{x}_{t_{i+l}}\\approx\\phi\\big(\\epsilon_{\\theta}\\big(\\tilde{x}_{t_{i-(k-l+1)}},t_{i-(k-l+1)}\\big),\\tilde{x}_{t_{i}},t_{i},t_{i+l}\\big)$ . Thus, Eq. (15) can   \n499 be rewritten as: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{x}_{t_{i+(k+1)}}\\approx\\phi\\big(\\epsilon_{\\theta}\\big(\\tilde{x}_{t_{i+l}},t_{i+l}\\big),\\tilde{x}_{t_{i}},t_{i},t_{i+(k+1)}\\big).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500 For any first-order ODE solver $\\phi$ , Eq. (B.3) and (B.4) utilize the gradient information from both past   \n501 and future to constitute a complete sampling iteration process for PFDiff- $k_{-}l$ . Eq. (B.4) indicates   \n502 that under the Assumption B.1 and upon the convergence of Eq. (B.4), PFDiff- $k_{-}l$ is guaranteed to   \n503 converge to the data distribution corresponding to the sampler $\\phi$ for any first-order ODE solver. ", "page_idx": 13}, {"type": "text", "text": "504 B.2 Error correction and proof of convergence of Proposition 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "505 Based on Eq. (8), we consider an iteration process of a first-order $(p=1$ ) ODE solver from $\\tilde{x}_{t_{i}}$ to   \n506 $\\tilde{x}_{t_{i+(k+1)}}$ , which can be expressed as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{x}_{t_{i+(k+1)}}=\\tilde{x}_{t_{i}}+\\int_{t_{i}}^{t_{i+(k+1)}}s\\big(\\epsilon_{\\theta}(x_{t},t),x_{t},t\\big)\\mathrm{d}t}\\\\ &{\\qquad\\quad\\approx\\tilde{x}_{t_{i}}+h\\big(\\epsilon_{\\theta}(\\tilde{x}_{t_{i}},t_{i}),\\tilde{x}_{t_{i}},t_{i}\\big)\\cdot\\big(t_{i+(k+1)}-t_{i}\\big)}\\\\ &{\\qquad\\quad=\\phi\\big(\\epsilon_{\\theta}(\\tilde{x}_{t_{i}},t_{i}),\\tilde{x}_{t_{i}},t_{i},t_{i+(k+1)}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 where the second line of Eq. (B.5) is obtained by discretizing the first line with an existing first-order   \n508 ODE solver $(p=1)$ ), and the definition of $\\phi$ and $h$ are consistent with Appendix B.1. It is well-known   \n509 that the discretization method used in Eq. (B.5) restricts the sampling step size $\\Delta t=t_{i+(k+1)}-t_{i}$   \n510 of the first-order ODE solver. A too-large step size will cause the first-order ODE solver to not   \n511 converge. This indicates that although Assumption B.2 points out that the sampling trajectory of   \n512 21 the first-order ODE solver lies on a two-dimensional plane, this trajectory is not a straight line (if   \n513 it were a straight line, a larger sampling step size could be used). Therefore, using $\\epsilon_{\\theta}(\\tilde{x}_{t_{i}},t_{i})$ for   \n514 discretized sampling in Eq. (B.5) introduces a significant sampling error, as shown by the first-order   \n515 ODE sampling trajectory in Fig. 2b. To reduce the first-order ODE solver sampling error, we have   \n516 revised Eq. (B.5) based on Assumption B.2, as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{x}_{t_{i+(k+1)}}=\\tilde{x}_{t_{i}}+\\displaystyle\\int_{t_{i}}^{t_{i+(k+1)}}s\\big(\\epsilon_{\\theta}(x_{t},t),x_{t},t\\big)\\mathrm{d}t}\\\\ &{\\qquad\\qquad=\\tilde{x}_{t_{i}}+s\\big(\\epsilon_{\\theta}(\\tilde{x}_{t_{\\delta}},t_{\\tilde{s}}),\\tilde{x}_{t_{\\tilde{s}}},t_{\\tilde{s}}\\big)\\cdot\\big(t_{i+(k+1)}-t_{i}\\big)}\\\\ &{\\qquad\\quad=\\tilde{x}_{t_{i}}+h\\big(\\epsilon_{\\theta}(\\tilde{x}_{t_{s}},t_{s}),\\tilde{x}_{t_{s}},t_{s}\\big)\\cdot\\big(t_{i+(k+1)}-t_{i}\\big)}\\\\ &{\\qquad\\quad=\\phi\\big(\\epsilon_{\\theta}(\\tilde{x}_{t_{s}},t_{s}),\\tilde{x}_{t_{i}},t_{i},t_{i+(k+1)}\\big)}\\\\ &{\\approx\\phi\\big(\\epsilon_{\\theta}(\\tilde{x}_{t_{i+l}},t_{i+l}),\\tilde{x}_{t_{i}},t_{i},t_{i+(k+1)}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "517 where $k$ and $l$ are determined by the selected PFDiff- $k_{-}l$ and the second and third lines are obtained   \n518 based on Assumption B.2. Combining Eq. (B.6) and Eq. (B.3) leads to the complete Eq. (15),   \n519 thereby completing the convergence proof of Proposition 3.1. Moreover, $t_{s}$ falls within the interval   \n520 $\\displaystyle\\bigl[t_{i},t_{i+(k+1)}\\bigr]$ , and since the sampling trajectory of the first-order ODE solver is not a straight line,   \n521 generally $t_{s}\\neq t_{i}$ and $t_{s}\\neq t_{i+(k+1)}$ . The interval $[t_{i},t_{i+(k+1)}]$ is amended to $(t_{i},t_{i+(k+1)})$ . By   \n522 adopting the foresight update mechanism of the Nesterov momentum [34], and guiding the current in  \n523 termediate state sampling with future gradient information, we replace $\\epsilon_{\\theta}(\\tilde{x}_{t_{s}},t_{s})$ with $\\epsilon_{\\theta}(\\tilde{x}_{t_{i+l}},t_{i+l})$ .   \n524 According to the definition of PFDiff- $k\\_l$ , $t_{i+l}$ also lies within the interval $(t_{i},t_{i+(k+1)})$ , and for   \n525 different combinations of $k$ and $l$ , this means searching and approximating the ground truth $t_{s}$ within   \n526 the interval $(t_{i},t_{i+(k+1)})$ . Among the six different versions of PFDiff- $k_{-}l$ defined in Appendix $\\mathbf{C}$ , we   \n527 believe that the optimal $t_{s}$ has been approximated. Compared to the direct discretization of $\\epsilon_{\\theta}(x_{t},t)$   \n528 in Eq. (B.5), we corrected the sampling error of the first-order ODE solver and proved its convergence   \n529 by guiding sampling based on the future gradient information $\\epsilon_{\\theta}(\\tilde{x}_{t_{i+l}},t_{i+l})$ under Assumption B.2,   \n530 as shown in the sampling trajectory of PFDiff-1 in Fig. 2b. Together with this section and Appendix   \n531 B.1, this completes the error correction and convergence proof of Proposition 3.1. ", "page_idx": 14}, {"type": "text", "text": "532 C Algorithms of PFDiffs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "533 As described in Sec. 3.4, during a single iteration, we can leverage the foresight update mechanism to   \n534 skip to a more distant future. Specifically, we modify Eq. (14) to $\\tilde{x}_{t_{i+(k+1)}}\\approx\\delta(\\bar{Q},\\tilde{x}_{t_{i}},t_{i},t_{i+(k+1)})$   \n535 5 to achieve a $k$ -step skip. We refer to this method as PFDiff- $k$ . Additionally, when $k\\neq1$ , the   \n536 computation of the buffer $Q$ , originating from Eq. (13), presents different selection choices. We   \n537 modify Eq. (12) to $\\tilde{x}_{t_{i+l}}\\approx\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+l}),l\\leq k$ to denote different \u201cspringboard\u201d choices with   \n538 the parameter $l$ . This strategy of multi-step skips and varying \u201cspringboard\u201d choices is collectively   \n539 termed as PFDiff- $\\cdot k\\_l\\;(l\\leq k)$ . Consequently, based on modifications to parameters $k$ and $l$ in Eq.   \n540 (12) and Eq. (14), Eq. (13) is updated to $\\begin{array}{r}{Q\\ensuremath{\\stackrel{\\mathrm{buffer}}{\\leftarrow}}\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+l},t_{i+(k+1)}\\right)}\\end{array}$ , and Eq. (11)   \n541 is updated to $\\begin{array}{r}{Q\\ensuremath{\\stackrel{\\mathrm{buffer}}{\\longleftarrow}}\\left(\\left\\{\\epsilon_{\\theta}\\big(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n}\\big)\\right\\}_{n=0}^{p-1},t_{i-(k-l+1)},t_{i}\\right)}\\end{array}$ . When $k=1$ , since $l\\leq k$ , then $l=1$ ,   \n542 and PFDiff- $k\\_l$ is the same as PFDiff-1, as shown in Algorithm 1 in Sec. 3.4. When $k\\,=\\,2$ , $l$   \n543 B can be either 1 or 2, forming Algorithms PFDiff-2_1 and PFDiff-2_2, as shown in Algorithm 2.   \n544 Furthermore, when $k=3$ , this forms three different versions of PFDiff-3, as shown in Algorithm   \n545 3. In this study, we utilize the optimal results from the six configurations of PFDiff- $k_{-}l$ ${\\_l}\\,(k=1,2,3$   \n546 $(l\\leq k)$ ) to demonstrate the performance of PFDiff. As described in Appendix B.2, this is essentially   \n547 an approximation of the ground truth $t_{s}$ . Through these six different algorithm configurations, we   \n548 approximately search for the optimal $t_{s}$ . It is important to note that despite using six different   \n549 algorithm configurations, this does not increase the computational burden in practical applications.   \n550 This is because, by visual analysis of a small number of generated images or computing specific   \n551 evaluation metrics, one can effectively select the algorithm configuration with the best performance.   \n552 Moreover, even without any selection, directly using the PFDiff-1 configuration can achieve significant   \n553 performance improvements on top of existing ODE solvers, as shown in the ablation study results in   \n554 Sec. 4.3.   \nAlgorithm 2 PFDiff-2   \nRequire: initial value $x_{T}$ , NFE $N$ , model $\\epsilon_{\\theta}$ , any $p$ -order solver $\\phi$ , skip type l   \n1: Define time steps $\\{t_{i}\\}_{i=0}^{M}$ with $M=3N-2p$   \n2: $\\tilde{x}_{t_{0}}\\gets x_{T}$   \n3: $\\begin{array}{r}{Q\\stackrel{\\mathrm{buffer}}{\\overleftarrow{}}\\left(\\left\\{\\epsilon_{\\theta}\\big(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n}\\big)\\right\\}_{n=0}^{p-1},t_{0},t_{1}\\right)}\\end{array}$ , where $\\hat{t}_{0}=t_{0},\\hat{t}_{p}=t_{1}$ \u25b7Initialize buffer   \n4: $\\tilde{x}_{t_{1}}=\\phi(Q,\\tilde{x}_{t_{0}},t_{0},t_{1})$   \n5: for $i\\gets1$ to $\\begin{array}{r}{\\frac{M}{p}-3}\\end{array}$ do   \n6: if $(i-1)$ mod $3=0$ then   \n7: 8: if $l=1$ $\\begin{array}{r l r l}&{\\mathrm{\\bf~\\Xi~\\Xi~}\\Gamma\\Gamma\\mathbf{i}\\mathbf{r}\\mathbf{e}\\cap}&&{\\>\\Gamma\\Gamma\\mathbf{i}\\mathbf{r}\\mathbf{i}\\mathbf{r}\\mathbf{1}\\Gamma\\mathbf{\\Xi}-\\mathbf{\\Xi}_{-1}^{\\mathrm{\\bf~D}}}\\\\ &{\\tilde{x}_{t_{i+1}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+1})\\quad}&&{\\>\\mathrm{Updating~guided~by~past~gradients}}\\\\ &{Q\\;\\underbrace{\\mathrm{buffer~}}_{\\propto}\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{{\\hat{t}}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+1},t_{i+3}\\right)\\quad}&&{\\;\\;\\forall\\mathrm{~Update~buffer~(overwrite)}}\\\\ &{\\;\\mathbf{e}\\;\\mathbf{if}\\;l=2\\;\\mathbf{fhen}}\\\\ &{\\tilde{x}_{t_{i+2}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+2})\\quad}&&{\\;\\;\\forall\\mathrm{~Updating~guided~by~past~gradients}}\\\\ &{Q\\;\\underbrace{\\mathrm{buffer~}}_{\\propto}\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{{\\hat{t}}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+2},t_{i+3}\\right)\\quad}&&{\\>\\mathrm{Update~buffer~(overwrite)}}\\\\ &{\\;\\underbrace{\\mathrm{\\bf~\\Xi~}\\mathbf{f}\\mathbf{i}\\mathbf{r}\\mathbf{i}\\mathbf{r}}_{\\propto}\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{{\\hat{t}}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+2},t_{i+3}\\right)\\quad}&&{\\;\\;\\forall\\mathrm{~Update~}\\mathrm{buffer~(overwrite)}}\\end{array}$ then   \n9:   \n10: els   \n11:   \n12:   \n13: end if   \n14: if $p=1$ then   \n15: $\\tilde{x}_{t_{i+3}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+3})$ \u25b7Anticipatory updating guided by future gradients   \n16: else if $p>1$ then   \n17: $\\tilde{x}_{t_{i+3}}=\\phi(Q,\\tilde{x}_{t_{i+l}},t_{i+l},t_{i+3})$ $\\triangleright$ The higher-order solver uses only past gradients   \n18: end if   \n19: end if   \n20: end for   \n21: return x\u02dctM   \nRequire: initial value $x_{T}$ , NFE $N$ , model $\\epsilon_{\\theta}$ , any $p$ -order solver $\\phi$ , skip type l   \n1: Define time steps $\\{t_{i}\\}_{i=0}^{M}$ with $M=4N-3p$   \n2: $\\tilde{x}_{t_{0}}\\gets x_{T}$   \n$\\begin{array}{r}{Q\\stackrel{\\mathrm{buffer}}{\\overbrace{\\sum_{\\ell=1}^{\\infty}\\left(\\sum_{\\ell_{n}}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right)_{n=0}^{p-1},t_{0},t_{1}\\right)}}}\\end{array}$ , where $\\hat{t}_{0}=t_{0},\\hat{t}_{p}=t_{1}$ \u25b7Initialize buffer   \n4: $\\tilde{x}_{t_{1}}=\\phi(Q,\\tilde{x}_{t_{0}},t_{0},t_{1})$   \n5: for $i\\gets1$ to $\\frac{M}{p}\\mathrm{~-~}4\\,\\mathbf{do}$   \n6: if $(i-1)$ mod $4=0$ then   \n7: $\\tilde{x}_{t_{i+4}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+l})$ $\\triangleright$ Updating guided by past gradients   \n8: $\\begin{array}{r}{Q\\stackrel{\\mathrm{buffer}}{\\overleftarrow{}}\\left(\\left\\{\\epsilon_{\\theta}(\\tilde{x}_{\\hat{t}_{n}},\\hat{t}_{n})\\right\\}_{n=0}^{p-1},t_{i+l},t_{i+4}\\right)}\\end{array}$ $\\triangleright$ Update buffer (overwrite)   \n9: if $p=1$ then   \n10: $\\tilde{x}_{t_{i+4}}=\\phi(Q,\\tilde{x}_{t_{i}},t_{i},t_{i+4})$ \u25b7Anticipatory updating guided by future gradients   \n11: else if $p>1$ then   \n12: $\\tilde{x}_{t_{i+4}}=\\phi(Q,\\tilde{x}_{t_{i+l}},t_{i+l},t_{i+4})$ $\\triangleright$ The higher-order solver uses only past gradients   \n13: end if   \n14: end if   \n15: end for   \n16: return x\u02dctM ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "555 D Additional experiment results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "556 In this section, we provide further supplements to the experiments on both unconditional and   \n557 conditional pre-trained Diffusion Probabilistic Models (DPMs) as mentioned in Sec. 4. Through   \n558 these additional supplementary experiments, we more fully validate the effectiveness of PFDiff as an   \n559 orthogonal and training-free sampler. Unless otherwise stated, the selection of pre-trained DPMs,   \n560 choice of baselines, algorithm configurations, GPU utilization, and other related aspects in this section   \n561 are consistent with those described in Sec. 4. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "562 D.1 License ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "IfpNsorodK/tmp/68a072b88d729f46e48b347179ebb4fa88f3eb8ec8b346c4ec065c181393b0f0.jpg", "table_caption": ["Table 1: The used datasets, codes, and their licenses. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "564 D.2 Additional results for unconditional discrete-time sampling ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "565 In this section, we report on experiments with unconditional, discrete DPMs on the CIFAR10 [42]   \n566 and CelebA 64x64 [43] datasets using quadratic time steps. The $\\mathrm{FID\\downarrow}$ scores for the PFDiff algorithm   \n567 are reported for changes in the number of function evaluations (NFE) from 4 to 20. Additionally,   \n568 we present FID scores on the CelebA $64\\mathrm{x}64$ [43], LSUN-bedroom $256\\mathrm{x}256$ [44], and LSUN-church   \n569 256x256 [44] datasets, utilizing uniform time steps. The experimental results are summarized   \n570 in Table 2. Results indicate that using DDIM [20] as the baseline, our method (PFDiff) nearly   \n571 achieved significant performance improvements across all datasets and NFE settings. Notably, PFDiff   \n572 facilitates rapid convergence of pre-trained DPMs to the data distribution with NFE settings below 10,   \n573 validating its effectiveness on discrete pre-trained DPMs and the first-order ODE solver DDIM. It is   \n574 important to note that on the CIFAR10 and CelebA 64x64 datasets, we have included the FID scores   \n575 of Analytic-DDIM [23], which serves as another baseline. Analytic-DDIM modifies the variance in   \n576 DDIM and introduces some random noise. With NFE lower than 10, the presence of minimal random   \n577 noise amplifies the error introduced by the gradient information approximation in PFDiff, reducing   \n578 its error correction capability compared to the Analytic-DDIM sampler. Thus, in fewer-step sampling   \n579 $(\\mathrm{NFE}{<}10)$ ), using DDIM as the baseline is more effective than using Analytic-DDIM, which requires   \n580 recalculating the optimal variance for different pre-trained DPMs, thereby introducing additional   \n581 computational overhead. In other experiments with pre-trained DPMs, we validate the efficacy of the   \n582 PFDiff algorithm by combining it with the overall superior performance of the DDIM solver.   \n583 Furthermore, to validate the motivation proposed in Sec. 3.2 based on Fig. 2a\u2014that at not excessively   \n584 large time step size $\\Delta t$ , an ODE-based solver shows considerable similarity in the noise network   \n585 outputs\u2014we compare it with the SDE-based solver DDPM [2]. Even at smaller $\\Delta t$ , the mean   \n586 squared error (MSE) of the noise outputs from DDPM remains high, suggesting that the effectiveness   \n587 of PFDiff may be limited when based on SDE solvers. Further, we adjusted the $\\eta$ parameter in   \n588 Eq. (6) (which controls the amount of noise introduced in DDPM) from 1.0 to 0.0 (at $\\eta\\:=\\:0.0$ ,   \n589 9 the SDE-based DDPM degenerates into the ODE-based DDIM [20]). As shown in Fig. 2a, as $\\eta$   \n590 decreases, the MSE of the noise network outputs gradually decreases at the same time step size $\\Delta t$ ,   \n591 indicating that reducing noise introduction can enhance the effectiveness of PFDiff. To verify this   \n592 motivation, we utilized quadratic time steps on CIFAR10 and CelebA 64x64 datasets and controlled   \n593 the amount of noise introduced by adjusting $\\eta$ , to demonstrate that PFDiff can leverage the temporal   \n594 redundancy present in ODE solvers to boost its performance. The experimental results, as shown   \n595 in Table 3, illustrate that with the reduction of $\\eta$ from 1.0 (SDE) to 0.0 (ODE), PFDiff\u2019s sampling   \n596 performance significantly improves at fewer time steps $(\\mathrm{NFE}{\\le}20)$ ). The experiment results regarding   \n597 FID variations with NFE as presented in Table 3, align with the trends of MSE of noise network   \n598 outputs with changes in time step size $\\Delta t$ as depicted in Fig. 2a. This reaffirms the motivation we   \n599 proposed in Sec. 3.2. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "IfpNsorodK/tmp/cc63f694b05122bb0d63a83adc419ce4a13ed64849274463ddedc9a3893187b6.jpg", "table_caption": ["Table 2: Sample quality measured by $\\mathrm{FID}\\downarrow$ on the CIFAR10 [42], CelebA 64x64 [43], LSUNbedroom 256x256 [44], and LSUN-church $256\\mathrm{x}256$ [44] using unconditional discrete-time DPMs, varying the number of function evaluations (NFE). Evaluated on 50k samples. PFDiff uses DDIM [20] and Analytic-DDIM [23] as baselines and introduces DDPM [2] and Analytic-DDPM [23] with $\\eta=1.0$ from Eq. (6) for comparison. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "600 D.3 Additional results for unconditional continuous-time sampling ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "601 In this section, we supplement the specific $\\mathrm{FID\\downarrow}$ scores for the unconditional, continuous pre  \n602 trained DPMs models with first-order and higher-order ODE solvers, DPM-Solver-1, -2 and   \n603 -3, [21] as baselines, as shown in Table 4. For all experiments in this section, we con  \n604 ducted tests on the CIFAR10 dataset [42], using the checkpoint checkpoint_8.pth under the   \n611 the basis of first-order and higher-order ODE solvers. Particularly, in the $_{6\\sim12}$ NFE range, PFDiff   \n612 significantly improved the convergence issues of higher-order ODE solvers under fewer NFEs. For   \n613 instance, at 9 NFE, PFDiff reduced the FID of DPM-Solver-3 from 233.56 to 5.67, improving the   \n614 sampling quality by $97.57\\%$ . These results validate the effectiveness of using PFDiff with first-order   \n615 or higher-order ODE solvers as the baseline. ", "page_idx": 17}, {"type": "table", "img_path": "IfpNsorodK/tmp/87a41c022aab7381848f61af8343fda01166c53358fad5d5e73722ff9f9f22a3.jpg", "table_caption": ["Table 3: Sample quality measured by $\\mathrm{FID\\downarrow}$ on the CIFAR10 [42] and CelebA 64x64 [43] using unconditional discrete-time DPMs with and without our method (PFDiff), varying the number of function evaluations (NFE) and $\\eta$ from Eq. (6). Evaluated on 50k samples. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "IfpNsorodK/tmp/0ce5a1397c5cbc853ed51d54c2e4dc1d4b81538aefc1beee9224602ee570ea56.jpg", "table_caption": ["Table 4: Sample quality measured by $\\mathrm{FID\\downarrow}$ of different orders of DPM-Solver [21] on the CIFAR10 [42] using unconditional continuous-time DPMs with and without our method (PFDiff), varying the number of function evaluations (NFE). Evaluated on $50\\mathrm{k}$ samples. "], "table_footnote": ["605 vp/cifar10_ddpmpp_deep_continuous configuration provided by ScoreSDE [4]. For the hyper606 parameter method of DPM-Solver [21], we adopted singlestep_fixed; to maintain consistency 607 with the discrete-time model in Appendix D.2, the parameter skip was set to time_quadratic (i.e., 608 quadratic time steps). Unless otherwise specified, we used the parameter settings recommended by DPM-Solver. The results in Table 4 show that by using the PFDiff method described in Sec. 3.4 610 and taking DPM-Solver as the baseline, we were able to further enhance sampling performance on "], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "616 D.4 Additional results for classifier guidance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 5: Sample quality measured by $\\mathrm{FID}\\downarrow$ on the ImageNet $64\\mathrm{x64}$ [32] and ImageNet $256\\mathrm{x}256$ [32], using ADM-G [5] model with guidance scales of 1.0 and 2.0, varying the number of function evaluations (NFE). Evaluated: ImageNet $64\\mathrm{x64}$ with 50k, ImageNet 256x256 with 10k samples. $\\ast\\mathrm{We}$ directly borrowed the results reported by AutoDiffusion [26], and AutoDiffusion requires additional search costs. $\\mathrm{^{*}W e}$ directly borrowed the results reported by AutoDiffusion [26], and AutoDiffusion requires additional search costs. ${\\bf\\omega}^{\\bullet}\\setminus$ represents missing data in the original paper. ", "page_idx": 19}, {"type": "table", "img_path": "IfpNsorodK/tmp/33420e471ad93d3797974b9b5fe1f8ae361145884b9595412f8b69eeaf1ece39.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "617 In this section, we provide the specific FID scores for pre-trained DPMs in the conditional, classifier   \n618 guidance paradigm on the ImageNet $64\\mathrm{x}64$ [32] and ImageNet $256\\mathrm{x}256$ datasets [32], as shown in   \n619 Table 5. We now describe the experimental setup in detail. For the pre-trained models, we used the   \n620 ADM-G [5] provided $64\\tt x64$ _diffusion.pt and 64x64_classifier.pt for the ImageNet 64x64   \n621 dataset, and $256\\mathtt{x}256,$ _diffusion.pt and $256\\mathtt{x}256,$ _classifier.pt for the ImageNet 256x256   \n22 dataset. All experiments were conducted with uniform time steps and used DDIM as the baseline [20].   \n23 We implemented the second-order and third-order methods from DPM-Solver [21] for comparison and   \n624 explored the method hyperparameter provided by DPM-Solver for both singlestep (corresponding   \n625 to \u201cSingle\u201d in Table 5) and multistep (corresponding to \u201cMulti\u201d in Table 5). Additionally, we   \n626 implemented the best-performing method from DPM-Solver $^{++}$ [22], multi-step DPM-Solver++(2M),   \n627 as a comparative measure. Furthermore, we also introduced the superior-performing AutoDiffusion   \n628 [26] method as a comparison. \u2217We directly borrowed the results reported in the original paper,   \n629 emphasizing that although AutoDiffusion does not require additional training, it incurs additional   \n630 search costs. \u201c\\\u201d represents missing data in the original paper. The specific experimental results of the   \n631 configurations mentioned are shown in Table 5. The results demonstrate that PFDiff, using DDIM   \n32 as the baseline on the ImageNet 64x64 dataset, significantly enhances the sampling efficiency of   \n633 DDIM and surpasses previous optimal training-free sampling methods. Particularly, in cases where   \n634 $\\mathrm{NFE}{\\le}10$ , PFDiff improved the sampling quality of DDIM by $41.88\\%{\\sim}88.14\\%$ . Moreover, on the   \n635 large ImageNet $256\\mathrm{x}256$ dataset, PFDiff demonstrates a consistent performance improvement over   \n636 the DDIM baseline, similar to the improvements observed on the ImageNet 64x64 dataset. ", "page_idx": 19}, {"type": "text", "text": "637 D.5 Additional results for classifier-free guidance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "638 In this section, we supplemented the specific $\\mathrm{FID\\downarrow}$ scores for the Stable-Diffusion [9] (conditional,   \n639 classifier-free guidance paradigm) setting with a guidance scale (s) of 7.5 and 1.5. Specifically, for   \n640 the pre-trained model, we conducted experiments using the sd-v1-4.ckpt checkpoint provided by   \n641 Stable-Diffusion. All experiments used the MS-COCO2014 [31] validation set to calculate $\\mathrm{FID\\downarrow}$   \n642 scores, with uniform time steps. PFDiff employs the DDIM [20] method as the baseline. Initially,   \n643 under the recommended $s=7.5$ configuration by Stable-Diffusion, we implemented DPM-Solver-2   \n644 and -3 as comparative methods, and conducted searches for the method hyperparameters provided by   \n645 DPM-Solver as singlestep (corresponding to \u201cSingle\u201d in Table 6) and multistep (corresponding   \n646 to \u201cMulti\u201d in Table 6). Additionally, we introduced previous state-of-the-art training-free methods,   \n647 including DPM-Solver $\\vdash+$ (2M) [22], UniPC [29], and DPM-Solver-v3(2M) [27] for comparison.   \n648 The experimental results are shown in Table 6. \u2020We borrow the results reported in DPM-Solver-v3   \n649 [27] directly. The results indicate that on Stable-Diffusion, PFDiff, using only DDIM as a baseline,   \n650 surpasses the previous state-of-the-art training-free sampling methods in terms of sampling quality in   \n651 fewer steps $(\\mathrm{NFE}{<}20)$ ). Particularly, at $_\\mathrm{NFE=10}$ , PFDiff achieved a 13.06 FID, nearly converging   \n652 to the data distribution, which is a $14.25\\%$ improvement over the previous state-of-the-art method   \n653 DPM-Solver-v3 at 20 NFE, which had a 15.23 FID. Furthermore, to further validate the effectiveness   \n654 of PFDiff on Stable-Diffusion, we conducted experiments using the $s=1.5$ setting with the same   \n655 experimental configuration as $s=7.5$ . For the comparative methods, we only experimented with the   \n656 multi-step versions of DPM-Solver-2 and -3 and DPM-Solver++(2M), which had faster convergence   \n657 at fewer NFE under the $s=7.5$ setting. As for UniPC and DPM-Solver-v3(2M), since DPM-Solver  \n658 v3 did not provide corresponding experimental results at $s=1.5$ , we did not list their comparative   \n659 results. The experimental results show that PFDiff, using DDIM as the baseline under the $s=1.5$   \n660 setting, demonstrated consistent performance improvements as seen in the $s=7.5$ setting, as shown   \n661 in Table 6. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "IfpNsorodK/tmp/3a2471f7c1cdb08cde3e8ce1bd5f7380471cd0e3497d7a96c875571b4b467f8e.jpg", "table_caption": ["Table 6: Sample quality measured by $\\mathrm{FID\\downarrow}$ on the validation set of MS-COCO2014 [31] using Stable-Diffusion model [9] with guidance scales of 7.5 and 1.5, varying the number of function evaluations (NFE). Evaluated on $10\\mathbf{k}$ samples. \u2020We borrow the results reported in DPM-Solver-v3 [27] directly. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "662 D.6 Additional ablation study results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "663 D.6.1 Additional results for PFDiff hyperparameters study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "664 In this section, we extensively investigate the impact of the hyperparameters $k$ and $l$ on the perfor  \n665 mance of the PFDiff algorithm, supplemented by a series of ablation experiments regarding their   \n666 configurations and outcomes. Specifically, we first conducted experiments on the CIFAR10 dataset   \n667 [42] using quadratic time steps, based on pre-trained unconditional discrete DDPM [2] and continuous   \n668 ScoreSDE [4] DPMs. For the conditional DPMs, we used uniform time steps in classifier guidance   \n669 ADM-G [5] pre-trained DPMs, setting the guidance scale (s) to 1.0 for experiments on the ImageNet   \n670 $64\\mathrm{x}64$ dataset [32]; for the classifier-free guidance Stable-Diffusion [9] pre-trained DPMs, we set   \n671 the guidance scale (s) to 7.5. All experiments were conducted using the DDIM [20] algorithm as a   \n672 baseline, and PFDiff- $k_{-}l$ configurations $(k=1,2,3$ $(l\\leq k)$ ) were tested in six different algorithm   \n673 configurations. The change in NFE and the corresponding $\\mathrm{FID\\downarrow}$ scores are shown in Table 7. The   \n674 experimental results show that under various combinations of $k$ and $l$ , PFDiff is able to enhance the   \n675 sampling performance of the DDIM baseline in most cases across different types of pre-trained DPMs.   \n676 Particularly when $k=1$ is fixed, PFDiff-1 significantly improves the sampling performance of the   \n677 DDIM baseline within the range of $8{\\sim}20$ NFE. For practical applications requiring higher sampling   \n678 quality at fewer NFE, optimal combinations of $k$ and $l$ can be identified by fixing NFE and sampling   \n679 a small number of samples for visual analysis or computing specific metrics, without significantly   \n680 increasing the computational burden. However, as discussed in Sec. 5, although the experimental   \n681 results presented in Table 7 demonstrate the excellent performance of the combinations of $k$ and $l$   \n682 under various pre-trained DPMs and NFE settings, no universally optimal configuration exists. This   \n683 finding somewhat limits the generality of the proposed PFDiff algorithm and sets objectives for our   \n684 future research. ", "page_idx": 20}, {"type": "table", "img_path": "IfpNsorodK/tmp/f056d2d340120d78c77b935ecb4cde0068dfef4b340ebe3440711e44c8bf5cb2.jpg", "table_caption": ["Table 7: Ablation of the impact of $k$ and $l$ on PFDiff in CIFAR10 [42], ImageNet 64x64 and MSCOCO2014 using DDPM [2], ScoreSDE [4], ADM-G [5] and Stable-Diffusion [9] models. We report the $\\mathrm{FID\\downarrow}$ , varying the number of function evaluations (NFE). Evaluated: MS-COCO2014 with $10\\mathbf{k}$ , others with $50\\mathrm{k}$ samples. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "685 D.6.2 Ablation study of gradient guidance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "686 To further investigate the impact of gradient guidance from the past or future on the rapid updating   \n687 of current intermediate states, this section supplements experimental results and analysis using   \n688 first-order and higher-order ODE solvers as baselines. Specifically, as described in Sec. 3.3, PFDiff   \n689 uses a first-order ODE solver as a baseline, where future gradient guidance corrects sampling errors,   \n690 with detailed proofs provided in Appendix B.2. Hence, using the first-order ODE solver DDIM   \n691 [20] as a baseline, we removed past and future gradients separately and employed quadratic time   \n692 steps. This was based on the pre-trained model from DDPM [2] on the CIFAR10 [42] dataset,   \n693 evaluating the $\\mathrm{FID\\downarrow}$ metric by changing the number of function evaluations (NFE). For higher-order   \n694 ODE solvers, the solving process implicitly utilizes future gradients as mentioned in Sec. 3.5, and   \n695 the additional explicit introduction of future gradients increases sampling error. Therefore, when   \n696 using higher-order ODE solvers as a baseline, PFDiff accelerates the sampling process using only   \n697 past gradients. Specifically, for higher-order ODE solvers, we selected DPM-Solver-2 and -3 [21]   \n698 as the baseline, also employing quadratic time steps, and based on the ScoreSDE [4] pre-trained   \n699 model on CIFAR10 [42]. Only the future gradients were removed to validate the effectiveness of the   \n700 PFDiff algorithm by changing the NFE and evaluating the $\\mathrm{FID}\\downarrow$ metric. As shown in Table 8, the   \n701 experimental results indicate that using the first-order ODE solver DDIM as a baseline, employing   \n702 only past gradients (similar to DeepCache [28]), or only future gradients, only slightly improves the   \n703 baseline\u2019s sampling performance; however, combining both significantly enhances the baseline\u2019s   \n704 sampling performance. Meanwhile, using higher-order ODE solvers DPM-Solver-2 and -3 as the   \n705 baseline, because the algorithm inherently contains future gradients, continuing to explicitly introduce   \n706 future gradients increases the overall error. Therefore, using only past gradients (PFDiff) significantly   \n707 improves the baseline\u2019s sampling efficiency, especially under fewer steps (NFE<10), where PFDiff   \n708 markedly ameliorates the non-convergence issues of the higher-order ODE solvers.   \n710 To evaluate the effectiveness of the PFDiff algorithm and the widely used Fr\u00e9chet Inception Distance   \n711 $(\\mathrm{FID}\\downarrow)$ metric [40] in the sampling process of Diffusion Probabilistic Models (DPMs), we have also   \n712 incorporated the Inception Score $(\\mathrm{IS}\\uparrow)$ metric [41] for both unconditional and conditional pre-trained   \n713 DPMs. Specifically, for the unconditional discrete-time pre-trained DPMs DDPM [2], we maintained   \n714 the experimental configurations described in Table 2 of Appendix D.2, and added IS scores for   \n715 the CIFAR10 dataset [42]. For the unconditional continuous-time pre-trained DPMs ScoreSDE[4],   \n716 the experimental configurations are consistent with Table 4 in Appendix D.3, and IS scores for the   \n717 CIFAR10 dataset were also added. For the conditional classifier guidance paradigm of pre-trained   \n718 DPMs ADM-G [5], the experimental setup aligned with Table 5 in Appendix D.4, including IS scores   \n719 for the ImageNet 64x64 and ImageNet 256x256 datasets [32]. Considering that the computation   \n720 of IS scores relies on features extracted using InceptionV3 pre-trained on the ImageNet dataset,   \n721 calculating IS scores for non-ImageNet datasets was not feasible, hence no IS scores were provided for   \n722 the classifier-free guidance paradigm of Stable-Diffusion [9]. The experimental results are presented   \n723 in Table 9. A comparison between the $\\mathrm{FID}\\downarrow$ metrics in Tables 2, 4, and 5 and the $\\mathrm{IS}\\uparrow$ metrics in Table   \n724 9 shows that both IS and FID metrics exhibit similar trends under the same experimental settings,   \n725 i.e., as the number of function evaluations (NFE) changes, lower FID scores correspond to higher   \n726 IS scores. Further, Figs. 1a and 1b, along with the visualization experiments in Appendix D.8,   \n727 demonstrate that lower FID scores and higher IS scores correlate with higher image quality and richer   \n728 details generated by the PFDiff sampling algorithm. These results further confirm the effectiveness   \n729 of the PFDiff algorithm and the FID metric in evaluating the performance of sampling algorithms. ", "page_idx": 22}, {"type": "table", "img_path": "IfpNsorodK/tmp/79b187288ee68b353420c7bf322c1ec404fd83309c38164aa9d7d371d851b9bd.jpg", "table_caption": ["Table 8: Ablation of the impact of the past and future gradients on PFDiff, using different orders of ODE Solver as the baseline, in CIFAR10 [42] using DDPM [2] and ScoreSDE [4] models. We report the $\\mathrm{FID}\\downarrow$ , varying the number of function evaluations (NFE). Evaluated on 50k samples. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "IfpNsorodK/tmp/c83c3d3551c1663a762a48291118ba7dadd6c295f24f99604f9d889b2897487b.jpg", "table_caption": ["Table 9: Sample quality measured by $\\mathrm{IS}\\uparrow$ on the CIFAR10 [42], ImageNet 64x64 [32] and ImageNet $256\\mathrm{x}256$ [32] using DDPM [2], ScoreSDE [4] and ADM-G [5] models, varying the number of function evaluations (NFE). Evaluated: ImageNet $256\\mathrm{x}256$ with $10\\mathbf{k}$ , others with $50\\mathrm{k}$ samples. $\\mathrm{^{*}W e}$ directly borrowed the results reported by AutoDiffusion [26], and AutoDiffusion requires additional search costs. \u201c\\\u201d represents missing data in the original paper and DPM-Solver-2 [21] implementation. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "730 D.8 Additional visualize study results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "731 To demonstrate the effectiveness of PFDiff, we present the visual sampling results on the CIFAR10   \n732 [42], CelebA 64x64 [43], LSUN-bedroom 256x256 [44], LSUN-church 256x256 [44], ImageNet   \n733 64x64 [32], ImageNet 256x256 [32], and MS-COCO2014 [31] datasets in Figs. 5-10. These results   \n734 illustrate that PFDiff, using different orders of ODE solvers as a baseline, is capable of generating   \n735 samples of higher quality and richer detail on both unconditional and conditional pre-trained Diffusion   \n736 Probabilistic Models (DPMs). ", "page_idx": 24}, {"type": "image", "img_path": "IfpNsorodK/tmp/3b1a97d0b5f5dde96710d03ce1dd558478ab5f07e4228acaea95d2fcb75d7e6c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Figure 5: Random samples by DDIM [20], Analytic-DDIM [23], and PFDiff (baseline: DDIM) with 4, 8, and 10 number of function evaluations (NFE), using the same random seed, quadratic time steps, and pre-trained discrete-time DPMs [2, 20] on CIFAR10 [42] (left) and CelebA 64x64 [43] (right). ", "page_idx": 25}, {"type": "image", "img_path": "IfpNsorodK/tmp/21e7f82ae868b89572a2c512d94ad52052e6b8d878c38a517aa7a2ab12668f63.jpg", "img_caption": ["(b) DDIM+PFDiff (Ours) "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 6: Random samples by DDIM [20] and PFDiff (baseline: DDIM) with 5 and 10 number of function evaluations (NFE), using the same random seed, uniform time steps, and pre-trained discrete-time DPMs [2] on LSUN-bedroom $256\\mathrm{x}256$ [44] (left) and LSUN-church $256\\mathrm{x}256$ [44] (right). ", "page_idx": 25}, {"type": "image", "img_path": "IfpNsorodK/tmp/e63cfd3db84e8232bb4b7b8dc38e8e1c6c99386fe388e042fa1c66853755efc2.jpg", "img_caption": ["(f) DPM-Solver-3+PFDiff (Ours) "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 7: Random samples by DPM-Solver-1, -2, and -3 [21] with and without our method (PFDiff) with 6 and 12 number of function evaluations (NFE), using the same random seed, quadratic time steps, and pre-trained continuous-time DPMs [4] on CIFAR10 [42]. ", "page_idx": 26}, {"type": "image", "img_path": "IfpNsorodK/tmp/639a5a6f7cf57facc56bcbee4db006742d4456ed7878f582a597cd2fe10e5675.jpg", "img_caption": ["(d) DDIM+PFDiff (Ours) "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: Random samples by DDIM [20], DPM-Solver-2 [21], DPM-Solver $^{++}$ (2M) [22], and PFDiff (baseline: DDIM) with 4 and 8 number of function evaluations (NFE), using the same random seed, uniform time steps, and pre-trained Guided-Diffusion [5] on ImageNet 64x64 [32] with a guidance scale of 1.0. ", "page_idx": 27}, {"type": "image", "img_path": "IfpNsorodK/tmp/6614f89aa6479fd400566165dff2555a72dc3f50ece3031135502ef98416dcf7.jpg", "img_caption": ["(b) DDIM+PFDiff (Ours) "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 9: Random samples by DDIM [20] and PFDiff (baseline: DDIM) with 4 and 8 number of function evaluations (NFE), using the same random seed, uniform time steps, and pre-trained Guided-Diffusion [5] on ImageNet $256\\mathrm{x}256$ [32] with a guidance scale of 2.0. ", "page_idx": 27}, {"type": "text", "text": "Text Prompts (listed from left to right):   \nA large bird is standing in the water by some rocks.   \nA candy covered cup cake sitting on top of a white plate.   \nPeople at a wine tasting with a table of wine bottles and glasses of red wine.   \nA bathtub sits on a tiled floor near a sink that has ornate mirrors over it while greenery grows on the other side of the tub.   \nA kitchen and dining area in a house with an open floor plan that looks out over the landscape from a large set of windows. ", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "image", "img_path": "IfpNsorodK/tmp/caa1e1c7638418c75a5a3e87043659e33966c931ba238a00c252c5ff787b6b73.jpg", "img_caption": ["Figure 10: Random samples by DDIM [20], DPM-Solver++(2M) [22], and PFDiff (baseline: DDIM) with 5 and 10 number of function evaluations (NFE), using the same random seed, uniform time steps, and pre-trained Stable-Diffusion [9] with a guidance scale of 7.5. Text prompts are a random sample from the MS-COCO2014 [31] validation set. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "737 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "738 1. Claims   \n739 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n740 paper\u2019s contributions and scope?   \n741 Answer: [Yes]   \n742 Justification: See abstract and section 1.   \n743 Guidelines:   \n744 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n745 made in the paper.   \n746 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n747 contributions made in the paper and important assumptions and limitations. A No or   \n748 NA answer to this question will not be perceived well by the reviewers.   \n749 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n750 much the results can be expected to generalize to other settings.   \n751 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n752 are not attained by the paper.   \n753 2. Limitations   \n754 Question: Does the paper discuss the limitations of the work performed by the authors?   \n755 Answer: [Yes]   \n756 Justification: See section 5.   \n757 Guidelines:   \n758 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n759 the paper has limitations, but those are not discussed in the paper.   \n760 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n761 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n762 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n763 model well-specification, asymptotic approximations only holding locally). The authors   \n764 should reflect on how these assumptions might be violated in practice and what the   \n765 implications would be.   \n766 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n767 only tested on a few datasets or with a few runs. In general, empirical results often   \n768 depend on implicit assumptions, which should be articulated.   \n769 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n770 For example, a facial recognition algorithm may perform poorly when image resolution   \n771 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n772 used reliably to provide closed captions for online lectures because it fails to handle   \n773 technical jargon.   \n774 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n775 and how they scale with dataset size.   \n776 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n777 address problems of privacy and fairness.   \n778 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n779 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n780 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n781 judgment and recognize that individual actions in favor of transparency play an impor  \n782 tant role in developing norms that preserve the integrity of the community. Reviewers   \n783 will be specifically instructed to not penalize honesty concerning limitations.   \n784 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n785   \n786 a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "0 \u2022 The answer NA means that the paper does not include theoretical results.   \n91 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n92 referenced.   \n93 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n94 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n95 they appear in the supplemental material, the authors are encouraged to provide a short   \n96 proof sketch to provide intuition.   \n97 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n98 by formal proofs provided in appendix or supplemental material.   \n99 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "800 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "801 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n802 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n803 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "805 Justification: See section 4 and Appendix D.   \n806 Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "839 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n840 tions to faithfully reproduce the main experimental results, as described in supplemental   \n841 material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provide code in the supplemental materials, which will be made available on the GitHub platform. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Our method is training-free, but we also report the hyperparameters used when evaluating our proposed method. Details can be found in section 4 and Appendix D. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Justification: We generate $50\\mathrm{k}$ images for $32\\mathtt{x32}$ , $64\\mathrm{x64}$ datasets and $10\\mathbf{k}$ for $256\\mathrm{x}256$ to evaluate the FID metric. According to previous works [20, 21, 26, 27], when evaluating with the generated samples mentioned above, the standard deviation of the FID evaluations is rather small (mainly less than 0.01). These small standard deviations do not change the conclusions. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ", "page_idx": 31}, {"type": "text", "text": "895 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n896 call to a library function, bootstrap, etc.)   \n897 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n898 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n899 of the mean.   \n900 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n901 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n902 of Normality of errors is not verified.   \n903 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n904 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n905 error rates).   \n906 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n907 they were calculated and reference the corresponding figures or tables in the text.   \n908 8. Experiments Compute Resources   \n909 Question: For each experiment, does the paper provide sufficient information on the com  \n910 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n911 the experiments?   \n912 Answer: [Yes]   \n913 Justification: In section 4, we mentioned that all experiments were conducted on an NVIDIA   \n914 RTX 3090 GPU.   \n915 Guidelines:   \n916 \u2022 The answer NA means that the paper does not include experiments.   \n917 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n918 or cloud provider, including relevant memory and storage.   \n919 \u2022 The paper should provide the amount of compute required for each of the individual   \n920 experimental runs as well as estimate the total compute.   \n921 \u2022 The paper should disclose whether the full research project required more compute   \n922 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n923 didn\u2019t make it into the paper).   \n924 9. Code Of Ethics   \n925 Question: Does the research conducted in the paper conform, in every respect, with the   \n926 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n927 Answer: [Yes]   \n928 Justification: Our work is conducted with the NeurIPS code of ethics.   \n929 Guidelines:   \n930 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n931 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n932 deviation from the Code of Ethics.   \n933 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n934 eration due to laws or regulations in their jurisdiction).   \n935 10. Broader Impacts   \n936 Question: Does the paper discuss both potential positive societal impacts and negative   \n937 societal impacts of the work performed?   \n938 Answer: [Yes]   \n939 Justification: See section 5.   \n940 Guidelines:   \n941 \u2022 The answer NA means that there is no societal impact of the work performed.   \n942 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n943 impact or why the paper does not address societal impact.   \n944 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n945 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n946 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n947 groups), privacy considerations, and security considerations.   \n948 \u2022 The conference expects that many papers will be foundational research and not tied   \n949 to particular applications, let alone deployments. However, if there is a direct path to   \n950 any negative applications, the authors should point it out. For example, it is legitimate   \n951 to point out that an improvement in the quality of generative models could be used to   \n952 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n953 that a generic algorithm for optimizing neural networks could enable people to train   \n954 models that generate Deepfakes faster.   \n955 \u2022 The authors should consider possible harms that could arise when the technology is   \n956 being used as intended and functioning correctly, harms that could arise when the   \n957 technology is being used as intended but gives incorrect results, and harms following   \n958 from (intentional or unintentional) misuse of the technology.   \n959 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n960 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n961 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n962 feedback over time, improving the efficiency and accessibility of ML).   \n963 11. Safeguards   \n964 Question: Does the paper describe safeguards that have been put in place for responsible   \n965 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n966 image generators, or scraped datasets)?   \n967 Answer: [NA]   \n968 Justification: Our method is a training-free accelerated sampling approach. It relies on   \n969 existing pre-trained DPMs and does not involve the release of data or models, thus there is   \n970 no need for safeguards.   \n971 Guidelines:   \n972 \u2022 The answer NA means that the paper poses no such risks.   \n973 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n974 necessary safeguards to allow for controlled use of the model, for example by requiring   \n975 that users adhere to usage guidelines or restrictions to access the model or implementing   \n976 safety filters.   \n977 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n978 should describe how they avoided releasing unsafe images.   \n979 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n980 not require this, but we encourage authors to take this into account and make a best   \n981 faith effort.   \n982 12. Licenses for existing assets   \n983 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n984 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n985 properly respected?   \n986 Answer: [Yes]   \n987 Justification: See Appendix D.1.   \n988 Guidelines:   \n989 \u2022 The answer NA means that the paper does not use existing assets.   \n990 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n991 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n992 URL.   \n993 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n994 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n995 service of that source should be provided.   \n96 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n97 package should be provided. For popular datasets, paperswithcode.com/datasets   \n98 has curated licenses for some datasets. Their licensing guide can help determine the   \n99 license of a dataset.   \n00 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n01 the derived asset (if it has changed) should be provided.   \n02 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n03 the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not involve crowdsourcing nor research with human subjects. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "35 Question: Does the paper describe potential risks incurred by study participants, whether   \n36 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n37 approvals (or an equivalent approval/review based on the requirements of your country or   \n38 institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]