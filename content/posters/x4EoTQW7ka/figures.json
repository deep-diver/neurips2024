[{"figure_path": "x4EoTQW7ka/figures/figures_1_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure demonstrates the effectiveness of DropBP in accelerating the fine-tuning of large language models.  Subfigure (a) shows a significant reduction in training time per sample for LLaMA2-7B when using DropBP compared to the baseline, with training time reduced by 57% for full fine-tuning and 48% for LoRA. Subfigure (b) illustrates the increase in the maximum sequence length achievable during fine-tuning of LLaMA2-70B using DropBP.  The maximum sequence length increases by up to 6.2x compared to the baseline when using DropBP with a drop rate of 0.875.", "section": "1 Introduction"}, {"figure_path": "x4EoTQW7ka/figures/figures_2_1.jpg", "caption": "Figure 3: The overview of DropBP.", "description": "This figure shows the concept of DropBP (Dropping Backward Propagation) in two parts: (a) illustrates how DropBP randomly drops layers during backward propagation, and (b) demonstrates how DropBP can be interpreted as training shallow submodules created by the undropped layers and residual connections. It highlights the mechanism where layers are randomly turned off with probability 'p' during backward propagation, significantly reducing computational costs and activation memory.", "section": "3 Methodology"}, {"figure_path": "x4EoTQW7ka/figures/figures_3_1.jpg", "caption": "Figure 3: The overview of DropBP.", "description": "The figure shows the concept of DropBP (Dropping Backward Propagation) and how it works as a combination of shallow submodules.  Panel (a) illustrates the process, highlighting the random dropping of layers during backward propagation with probabilities (p) assigned based on layer sensitivity.  Panel (b) visually represents the effect of DropBP, showing that by dropping certain layers, the overall model effectively becomes an ensemble of various shallower submodules, reducing computational cost and activation memory. ", "section": "3 Methodology"}, {"figure_path": "x4EoTQW7ka/figures/figures_4_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure demonstrates the effectiveness of DropBP in accelerating the fine-tuning of large language models.  (a) shows a significant reduction in training time per sample for LLaMA2-7B when using DropBP compared to a baseline without DropBP, across various fine-tuning methods (Full-FT and LoRA). (b) showcases the substantial increase in the maximum sequence length achievable for LLaMA2-70B when using DropBP, highlighting its capacity to handle longer sequences.", "section": "1 Introduction"}, {"figure_path": "x4EoTQW7ka/figures/figures_6_1.jpg", "caption": "Figure 5: Validation perplexity (PPL) for fine-tuning LLaMA2-70B through QLoRA (baseline) with DropBP on the Alpaca dataset. The p represents the target average drop rate for backward propagation.", "description": "This figure shows the validation perplexity (PPL) over training time (in hours) for fine-tuning the LLaMA2-70B language model using the QLoRA method with and without DropBP.  The baseline (no DropBP) is shown for comparison.  Multiple lines represent different DropBP configurations with varying target average drop rates (p) for backward propagation.  The figure highlights that DropBP accelerates convergence, reaching a similar perplexity in less time than the baseline. The speedup is quantified as 1.5x.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/figures/figures_7_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure demonstrates the effectiveness of DropBP in accelerating the fine-tuning of large language models.  Subfigure (a) shows a significant reduction in training time per sample for LLaMA2-7B when using DropBP compared to a baseline. Subfigure (b) showcases a substantial increase in the maximum sequence length achievable for LLaMA2-70B using DropBP, highlighting its capability to handle longer sequences during training.", "section": "4 Evaluation"}, {"figure_path": "x4EoTQW7ka/figures/figures_7_2.jpg", "caption": "Figure 7: Validation perplexity (PPL) for fine-tuning LLaMA2-7B through LoRA (baseline) with LayerDrop (LD), Progress Layer Dropping (PLD), and DropBP on the Alpaca dataset. The p represents the target average drop rate for backward propagation in DropBP.", "description": "This figure compares the performance of DropBP against baseline LoRA and other layer dropping methods (LayerDrop and Progressive Layer Drop) on the Alpaca dataset.  The x-axis represents the FLOPs used, and the y-axis shows the validation perplexity (PPL), a measure of the model's performance. DropBP demonstrates faster convergence to lower PPL than LD and PLD, while maintaining comparable accuracy.", "section": "E Comparisons between Layer Dropping and DropBP on fine-tuning LLMs"}, {"figure_path": "x4EoTQW7ka/figures/figures_8_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure showcases the performance improvements achieved by using DropBP in fine-tuning large language models.  Subfigure (a) compares the training time per sample for LLaMA2-7B with and without DropBP, demonstrating a significant reduction in training time with DropBP. Subfigure (b) illustrates the increase in the maximum available sequence length for LLaMA2-70B when using DropBP, highlighting its ability to handle longer sequences more efficiently.  The variable 'p' represents the target average drop rate used in the backward propagation process.", "section": "4 Evaluation"}, {"figure_path": "x4EoTQW7ka/figures/figures_15_1.jpg", "caption": "Figure 9: The impact of path length for fine-tuning LLaMA2-7B.", "description": "This figure shows the impact of path length on gradient magnitude during the fine-tuning of LLaMA2-7B.  It consists of three sub-figures. (a) shows the distribution of path lengths, indicating the probability of encountering paths of various lengths within the network. (b) demonstrates the gradient magnitude at the input for each path length. It reveals that shorter paths generally have larger gradient magnitudes compared to longer ones.  (c) presents the total gradient magnitude for each path length, showcasing that shorter paths contribute more significantly to the overall gradient. The combined observations indicate the importance of short paths (shallow submodules) for effective training in residual networks, which forms the basis for the DropBP method's design.", "section": "A The importance of short paths in residual networks"}, {"figure_path": "x4EoTQW7ka/figures/figures_17_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure showcases the performance improvements achieved by incorporating DropBP into the fine-tuning process of large language models.  Specifically, it demonstrates (a) a significant reduction in training time per sample for LLaMA2-7B when using DropBP (with different drop rates), and (b) a considerable increase in the maximum sequence length achievable for LLaMA2-70B when DropBP is applied.  These improvements highlight the efficiency gains offered by the DropBP method, allowing for faster training and the handling of longer sequences.", "section": "1 Introduction"}, {"figure_path": "x4EoTQW7ka/figures/figures_18_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure showcases the performance improvements achieved by using DropBP in fine-tuning large language models.  Subfigure (a) compares the training time per sample for LLaMA2-7B with and without DropBP, demonstrating a significant reduction in training time with DropBP. Subfigure (b) illustrates the impact of DropBP on the maximum sequence length achievable during fine-tuning of LLaMA2-70B, showing a considerable increase in sequence length when DropBP is employed.", "section": "1 Introduction"}, {"figure_path": "x4EoTQW7ka/figures/figures_18_2.jpg", "caption": "Figure 8: Distribution of drop rates and the validation PPL when fine-tuning LLaMA2-7B through LORA with DropBP at uniform or sensitivity-based drop rate on Alpaca datasets.", "description": "This figure shows two subfigures. The left subfigure (a) is a scatter plot showing the sensitivity of each layer (x-axis) against its assigned drop rate (y-axis).  A histogram is overlaid showing the distribution of drop rates determined by sensitivity.  The right subfigure (b) shows a line graph comparing the validation perplexity (PPL) against FLOPS (floating point operations) for both uniform and sensitivity-based drop rates. The sensitivity-based allocation leads to better validation perplexity.", "section": "4.4 Ablation Study"}, {"figure_path": "x4EoTQW7ka/figures/figures_18_3.jpg", "caption": "Figure 8: Distribution of drop rates and the validation PPL when fine-tuning LLaMA2-7B through LORA with DropBP at uniform or sensitivity-based drop rate on Alpaca datasets.", "description": "This figure shows two subfigures. The left subfigure is a histogram showing the distribution of drop rates determined by the sensitivity of each layer when the average drop rate is set to 0.875. The right subfigure is a line graph showing the validation perplexity (PPL) for fine-tuning LLaMA2-7B through LORA with DropBP, comparing uniform and sensitivity-based drop rates. The x-axis represents the FLOPs, and the y-axis represents the PPL. The results show that sensitivity-based drop rates achieve a 1.6% higher accuracy compared to uniform drop rates with a relatively high learning rate of 3e-4.", "section": "4.4 Ablation Study"}, {"figure_path": "x4EoTQW7ka/figures/figures_18_4.jpg", "caption": "Figure 8: Distribution of drop rates and the validation PPL when fine-tuning LLaMA2-7B through LORA with DropBP at uniform or sensitivity-based drop rate on Alpaca datasets.", "description": "This figure visualizes the distribution of drop rates assigned to different layers of a LLaMA2-7B model during fine-tuning using the DropBP algorithm. The left panel shows the sensitivity of each layer, calculated based on how much changes in the gradient that layer produces, impacting the overall loss. The right panel displays the resulting drop rates based on two allocation methods: a uniform approach assigning same drop rate to all layers, and a sensitivity-based approach where rates are tailored to each layer's sensitivity.  The graph also shows how the validation perplexity (PPL), a metric to evaluate the model's performance, changes depending on the chosen drop rate allocation method.", "section": "4.4 Ablation Study"}, {"figure_path": "x4EoTQW7ka/figures/figures_18_5.jpg", "caption": "Figure 8: Distribution of drop rates and the validation PPL when fine-tuning LLaMA2-7B through LORA with DropBP at uniform or sensitivity-based drop rate on Alpaca datasets.", "description": "This figure shows two sub-figures. The left sub-figure shows the distribution of drop rates determined by sensitivity when the average drop rate is set to 0.875 for fine-tuning LLaMA2-7B through LORA with DropBP on Alpaca datasets. The right sub-figure shows the validation perplexity (PPL) with uniform and sensitivity-based allocated drop rates for fine-tuning LLaMA2-7B through LORA with DropBP on Alpaca datasets.  The x-axis represents the FLOPs and y-axis represents the validation PPL. The sensitivity-based drop rate allocation achieves a lower validation perplexity than uniform drop rate allocation.", "section": "4.4 Ablation Study"}, {"figure_path": "x4EoTQW7ka/figures/figures_19_1.jpg", "caption": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the p represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.", "description": "This figure demonstrates the performance improvements achieved by using DropBP in fine-tuning large language models.  Subfigure (a) shows a comparison of training time per sample for LLaMA2-7B with and without DropBP, highlighting the significant reduction in training time. Subfigure (b) showcases the increase in the maximum sequence length achievable when fine-tuning LLaMA2-70B using DropBP, indicating its effectiveness in handling longer sequences on a single GPU.  Different values of *p* (target average drop rate for backward propagation) are used in the experiments.", "section": "1 Introduction"}, {"figure_path": "x4EoTQW7ka/figures/figures_19_2.jpg", "caption": "Figure 8: Validation perplexity (PPL) for fine-tuning LLaMA2-7B through LoRA (baseline) with LayerDrop (LD), Progress Layer Dropping (PLD), or DropBP on the Alpaca dataset. The p represents the target average drop rate for backward propagation in DropBP.", "description": "This figure compares the performance of DropBP against baseline LoRA, Layerdrop (LD), and Progressive Layer Dropping (PLD) in terms of validation perplexity (PPL) achieved on the Alpaca dataset while varying the FLOPs (floating-point operations).  It shows that DropBP achieves comparable PPL to the baseline with significantly fewer FLOPs, outperforming LD and PLD which suffer from accuracy drops when reducing FLOPs.", "section": "4.4 Ablation Study"}]