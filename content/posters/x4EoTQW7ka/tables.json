[{"figure_path": "x4EoTQW7ka/tables/tables_5_1.jpg", "caption": "Table 1: Test accuracy on MMLU and commonsense reasoning tasks. In DropBP, drop rate is the target average drop rate across all layers in backward propagation. Note that Full-FT stands for full fine-tuning.", "description": "This table presents the results of the experiments conducted to evaluate the accuracy of different fine-tuning methods on two benchmark datasets: the Massive Multitask Language Understanding (MMLU) and a set of commonsense reasoning tasks.  The table compares the performance of full fine-tuning (Full-FT), LoRA, and LoRA combined with DropBP (Dropping Backward Propagation) at different drop rates (0.5, 0.75, and 0.875).  The results are shown separately for Alpaca and Dolly datasets.  The table aims to demonstrate that DropBP can achieve comparable accuracy while significantly reducing the training time and memory consumption.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_5_2.jpg", "caption": "Table 2: Training time, memory usage, and test score on MT-Bench task when fine-tuning LLaMA3-8B with DropBP on Oasst1 datasets.", "description": "This table presents the results of fine-tuning the LLaMA3-8B language model on the Oasst1 dataset using different methods: no fine-tuning (No-tunes), LoRA, and LoRA combined with DropBP at various drop rates (0.5, 0.75, and 0.875).  For each method, it shows the memory usage (Mem), training time (Time), and test scores on the MT-Bench task, broken down by different sub-tasks (Human, STEM, Role, Extract, Writing, Reason, Coding, Math). The average score across all sub-tasks is also provided (Avg.). The table demonstrates DropBP's impact on reducing both memory usage and training time while maintaining comparable performance to the baseline (LoRA).", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_6_1.jpg", "caption": "Table 3: Time required for fine-tuning LLaMA2 models with DropBP on the Alpaca datasets when p denotes the target average drop rate. The number of fine-tuning samples is 50K.", "description": "This table shows the time taken to fine-tune three different sizes of LLaMA2 models (7B, 13B, and 70B parameters) using different parameter-efficient fine-tuning (PEFT) methods (LoRA and Full-FT) with various DropBP drop rates (p=0, 0.5, 0.75, 0.875).  The baseline (p=0) represents the training time without DropBP. The table highlights the significant reduction in training time achieved by DropBP, particularly at higher drop rates.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_6_2.jpg", "caption": "Table 4: Available maximum sequence length for fine-tuning LLaMA2-70B using QLoRA with DropBP on a NVIDIA A100 GPU, at a micro batch size of 1.", "description": "This table shows the maximum sequence length achievable when fine-tuning the LLaMA2-70B model using QLoRA with DropBP on a single NVIDIA A100 GPU.  The experiment was conducted with a micro-batch size of 1.  The results demonstrate a significant increase in maximum sequence length as the DropBP rate increases, highlighting the memory efficiency gains from employing DropBP.", "section": "4.3 Usability of DropBP"}, {"figure_path": "x4EoTQW7ka/tables/tables_8_1.jpg", "caption": "Table 5: The number of submodules being trained and test accuracy on the 5-shot MMLU tasks with layer freezing or DropBP on the Alpaca datasets.", "description": "This table compares the number of submodules trained and the accuracy achieved on the 5-shot Massive Multitask Language Understanding (MMLU) benchmark when using Layer Freezing and DropBP for fine-tuning LLAMA2-7B and LLAMA2-70B models on the Alpaca dataset. It shows that DropBP, despite dropping layers during backward propagation, maintains comparable accuracy to the baseline while training a significantly larger number of submodules compared to the layer freezing method. This is particularly evident in the larger LLAMA2-70B model.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_8_2.jpg", "caption": "Table 6: Test accuracy on the 0-shot commonsense reasoning tasks when fine-tuning LLaMA2-7B and 13B through LoRA with DropBP at uniform or sensitivity-based drop rate on the Alpaca datasets. The target average drop rate is 0.875.", "description": "This table presents the results of 0-shot commonsense reasoning tasks using LLaMA2-7B and 13B models fine-tuned with LoRA and DropBP.  It compares the accuracy achieved with uniform versus sensitivity-based drop rate allocation in DropBP at a target average drop rate of 0.875 on the Alpaca dataset.  Different learning rates (LR) are also tested to assess performance variation.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_16_1.jpg", "caption": "Table 7: Training time (ms) per iteration for a sequence length of 512 through Full-FT, LORA or QLORA using DropBP. Mixed refers to mixed precision training [56] using BFloat16 (BF16) and 32-bit. MBS is denoted as the micro batch size. FW, BW, and Total respectively denote the time consumed for forward propagation, backward propagation, and the entire training process.", "description": "This table presents the training time per iteration in milliseconds (ms) for different model sizes (LLaMA2-7B, LLaMA2-13B, LLaMA2-70B) and fine-tuning methods (Full-FT, LORA, QLORA).  It shows the impact of DropBP on training time by comparing the time taken with different drop rates (0, 0.5, 0.75, 0.875).  The columns specify whether mixed precision training was used, micro batch size, forward propagation time, backward propagation time, and total time.  The percentage reduction in backward propagation (BW) and total training time is indicated in parenthesis for each drop rate.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_19_1.jpg", "caption": "Table 8: Test accuracy on the 0-shot commonsense reasoning tasks when fine-tuning LLaMA2-7B through LoRA with layerdrop (LD), progressive layer dropping (PLD), and DropBP.", "description": "This table compares the performance of three different layer dropping methods (LayerDrop, Progressive Layer Drop, and DropBP) against a baseline LoRA model on a 0-shot commonsense reasoning task.  It shows the relative FLOPs (floating point operations) and accuracy for each method, highlighting DropBP's ability to maintain high accuracy even with significantly reduced FLOPs compared to the other layer dropping methods.", "section": "4.2 Main Results: Accuracy and Efficiency"}, {"figure_path": "x4EoTQW7ka/tables/tables_20_1.jpg", "caption": "Table 9: Detailed Setup for Table 1-2. BS and MBS are denoted as the batch size and micro batch size, respectively. Mixed refers to mixed precision training [56] using BFloat16 (BF16) and 32-bit.", "description": "This table shows the detailed hyperparameter settings used for the experiments reported in Table 1 and 2 of the paper.  It lists the fine-tuning method (LoRA, Full-FT, QLoRA), the dataset used (Alpaca, Dolly, Oasst1), the number of training iterations, batch size (BS), micro-batch size (MBS), precision (Mixed or BF16), and the learning rate range used for each experiment.", "section": "4.1 Implementation and Settings"}]