{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This paper introduced the Transformer architecture, which is fundamental to the development of large language models (LLMs) and is the basis for many of the models discussed in this paper."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-20", "reason": "This paper established scaling laws that govern the performance of LLMs, providing crucial insights into the relationship between model size, dataset size, and computational resources, which directly impacts the fine-tuning strategies explored in this work."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper demonstrated the few-shot learning capabilities of LLMs, highlighting their potential for efficient fine-tuning on smaller datasets, a key concept influencing the parameter-efficient fine-tuning methods compared in this research."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduced the LoRA (Low-Rank Adaptation) technique, a parameter-efficient fine-tuning method directly relevant to and compared within this paper\u2019s DropBP method."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "QLoRA: Efficient finetuning of quantized LLMs", "publication_date": "2023-05-14", "reason": "This paper presented QLoRA, another parameter-efficient fine-tuning method that combines quantization with LoRA, offering a computationally efficient approach compared in this paper's DropBP method."}]}