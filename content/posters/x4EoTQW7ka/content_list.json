[{"type": "text", "text": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sunghyeon Woo1\u2217\u2020 Baesung Park2\u2217 Byeongwook Kim2 Minjung Jo2 Se Jung Kwon2 Dongsuk Jeon1 Dongsoo Lee2 Seoul National University1 NAVER Cloud2 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by $44\\%$ with comparable accuracy to the baseline, accelerate convergence to the same perplexity by $1.5\\times$ , and enable training with a sequence length $6.2\\times$ larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of $79\\%$ on a NVIDIA A100 GPU and $117\\%$ on an Intel Gaudi2 HPU. The code is available at https://github.com/WooSunghyeon/dropbp. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since the advent of the Transformer architecture [1], the field of language modelling has experienced dramatic advancements. Especially, following the scaling laws [2, 3], the development of Large Language Models (LLMs) [4, 5, 6, 7, 8, 9] continues with the aim of achieving or outperforming human capabilities. However, tremendously increasing the size of the model results in significant costs for training from scratch. An alternative approach for developing high-capability LLMs without the costly pretraining on extensive datasets is instruction tuning [10, 11, 12, 13]. This method fine-tunes well-trained foundation models on relatively small instruction-following datasets, enabling the models to better understand and follow prompts. ", "page_idx": 0}, {"type": "text", "text": "While fine-tuning Large Language Models (LLMs) on instruction-following datasets is more costeffective than training from scratch, it still requires substantial memory for parameters and activations, along with significant floating-point operations (FLOPs). In this context, Parameter-Efficient FineTuning (PEFT) techniques [14, 15, 16] effectively reduce the memory required for parameter gradients and optimizer states by freezing pretrained weights and selectively training newly added modules. Moreover, when combined with quantization methods [17, 18, 19, 20], these techniques can further significantly decrease the memory requirements for parameters. ", "page_idx": 0}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/2efcd5384b0d219bdd4ae035f22f58d33daae4bc21b5aaba355b2789338eb452.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the $p$ represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU. ", "page_idx": 1}, {"type": "text", "text": "While Parameter-Efficient Fine-Tuning (PEFT) has successfully reduced memory associated with parameters, two significant challenges remain for efficient fine-tuning: computational cost and activation memory, both of which are linked to the backpropagation [21]. First, fine-tuning Large Language Models (LLMs) using a backpropagation requires substantial floating-point operations (FLOPs). Specifically, the backpropagation algorithm necessitates forward propagation to calculate outputs and backward propagation to compute gradients for inputs and parameters. Notably, backward propagation demands twice the computational operations compared to forward propagation, thus becoming the primary bottleneck. Second, all intermediate outputs (i.e., activations) generated during forward propagation must be stored for compute in backward propagation. This activations consume considerable memory, which becomes especially critical when training LLMs on long sequence contexts [22, 23]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce Dropping Backward Propagation (DropBP), an efficient fine-tuning algorithm for LLMs that significantly reduces computational costs and activation memory. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. As a result, these undropped layers no longer require FLOPs and activation memory during backward propagation. Additionally, DropBP calculates the sensitivity of each layer, an indicator of its impact on the total training process, to adjust drop rate. This careful calibration of drop rate according to layer sensitivity ensures more stable training. This DropBP algorithm can be seamlessly integrated with any PEFT algorithm, operating orthogonally by simply dropping layers during backward propagation. ", "page_idx": 1}, {"type": "text", "text": "We implemented DropBP as an easy-to-integrate PyTorch library [24], requiring only minimal changes to the existing training codes. In experiments, DropBP successfully reduces training time as shown in Fig. 1a, maintaining comparable accuracy on the MMLU [25] and commonsense reasoning tasks [26, 27, 28, 29, 30]. The DropBP also accelerated the convergence of the same perplexity by $1.5\\times$ in LLaMA2-70B [8]. Moreover, DropBP substantially decreases activation memory, increasing an available maximum sequence length by up to $6.2~\\times$ in LLaMA2-70B on a single NVIDIA A100 GPU [31], as shown in Fig. 1b. Finally, our DropBP increases training throughput by up to $79\\%$ and $117\\%$ on a single NVIDIA A100 GPU and Intel Gaudi2 HPU [32], respectively, when fully fine-tuning LLaMA3-8B [9]. In summary, the main contributions of our paper are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose DropBP, an efficient fine-tuning algorithm that randomly drops backward propagation based on layer sensitivity. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We implemented DropBP as a user-friendly PyTorch extension with a straightforward API for ease of use, making it easily applicable to existing training codes. \u2022 DropBP reduces training time by $44\\%$ with comparable accuracy, increases convergence speed by $1.5\\times$ , increases the available maximum sequence length up to $6.2\\times$ , and enhances training throughput up to $117\\%$ . ", "page_idx": 2}, {"type": "text", "text": "2 Background & Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Backpropagation Backpropagation [33], a core algorithm for training deep neural networks, involves both forward and backward propagation. Specifically, the training process in the linear layer is represented as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Forward~Prop:}}&{\\mathbf{H}_{o u t}=\\mathbf{W}\\times\\mathbf{H}_{i n}}\\\\ {\\mathrm{Backward~Prop:}}&{\\nabla\\mathbf{H}_{i n}=\\mathbf{W}^{T}\\times\\nabla\\mathbf{H}_{o u t}}\\\\ &{\\nabla\\mathbf{W}=\\nabla\\mathbf{H}_{o u t}\\times\\mathbf{H}_{i n}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where H and W represent the activations and parameters, respectively, with $\\mathbf{\\Psi}^{,}\\times\\mathbf{\\Psi}^{,}$ indicating matrix multiplication operation. The gradients of $\\mathbf{H}$ and $\\mathbf{W}$ are denoted by $\\nabla\\mathbf{H}$ and $\\nabla\\mathbf{W}$ . The computational costs during forward propagation primarily arises from matrix multiplication for computing output activations by Eq. 1. In backward propagation, the computational burden is primarily due to matrix multiplication for calculating input gradients by Eq. 2 and parameter gradients by Eq. 3. Note that the computational costs of these equations are almost equal. Consequently, the FLOPs required for backward propagation including Eqs. 2 and 3 are approximately $2\\times$ as large as the FLOPs needed for forward propagation by Eq. 1. Furthermore, the activations of all layers $(\\mathbf{H}_{i n}^{T})$ must be stored in memory for use in backward propagation computations in Eq. 3. Therefore, focusing on reducing the computations during backward propagation is crucial for decreasing both the overall computational costs and the activation memory. ", "page_idx": 2}, {"type": "text", "text": "Interpretation the model with residual connections Residual connections are one of the widely used methods to address the issue of vanishing gradients [34]. Transformers [1] also incorporate residual connections that bypass multi-head attention and feedforward networks. Networks utilizing these residual connections can be interpreted as ensembles of several submodules [35]. For example, if we expand the residual connection that bypasses $F_{2}$ in the model shown in Fig. 2a, it can be represented as a combination of four submodules, as depicted in Fig. 2b From this perspective, a network with $n$ layers can be interpreted as an ensemble of $2^{n}$ submodules [35]. ", "page_idx": 2}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/94c1bc187589e04886fd1bf66833d303807e0a468021aa5f563e710d5a792484.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Interpreting the model with residual connections as a combination of multiple submodules. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Dropping Backward propagation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 2, we observed that the backpropagation algorithm consumes a significant amount of FLOPs and activation memory, particularly for the backward propagation. To reduce this overhead, ", "page_idx": 2}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/5c0470dc335626c75ea5829e71477a25274cd6beb8ecccc7c1dea892a07ea3c8.jpg", "img_caption": ["Figure 3: The overveiw of DropBP. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "we propose a straightforward approach: Dropping Backward Propagation (DropBP). DropBP is designed to drop layers exclusively during backward propagation, effectively reducing both FLOPs and activation memory for the dropped layers, as demonstrated in following equations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X}_{i m m}=\\mathbf{X}_{i n}+\\mathcal{D}_{p_{i}}(f_{A T T N}(f_{L N}(\\mathbf{X}_{i n})))\\qquad}\\\\ {\\mathbf{X}_{o u t}=\\mathbf{X}_{i m m}+\\mathcal{D}_{p_{i+1}}(f_{F F N}(f_{L N}(\\mathbf{X}_{i m m})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{X}_{i n}$ , $\\mathbf{X}_{o u t}$ , and $\\mathbf{X}_{i m m}$ represent the input, output, and immediate activation between the attention layer and feedforward network in a transformer block, respectively. $f_{A T T N},\\,f_{F F N}$ , and $f_{L N}$ denote the attention layer, feedforward network, and layer normalization of the transformer block. The DropBP layer, defined as $\\mathcal{D}_{p}(\\mathbf{X})$ , skips backward propagation in the input $\\mathbf{X}$ with a probability of $p$ , while not dropping forward propagation. Following to Eqs. 4 and 5, backward propagation in the attention layer and feedforward network is dropped with probabilities $p_{i}$ and $p_{i+1}$ , respectively, as shown in Fig. 3a. ", "page_idx": 3}, {"type": "text", "text": "We can view the Transformer as a collection of a lot of submodules composed of various modules (i.e., $f_{A T T N}\\circ f_{L N}$ and $f_{F F N}\\circ f_{L N})$ with residual connections, as described in Section 2. When the Transformer block contains $n$ units, each including multi-head attention and a feedforward network, the model can be interpreted as an ensemble of $2^{2\\bar{n}}$ submodules. From this perspective, DropBP can be interpreted as training only certain shallow submodules. For example, as shown in Fig. 3b, if the $F_{2}$ layer is dropped, only the shallow submodule composed of the remaining layers is trained during backward propagation. Therefore, if the overall drop rate is $p$ , DropBP can be interpreted as training shallow submodules with the depth of $2n(1-p)$ or less, since $2n p$ layers are dropped. We anticipate that training these smaller modules alone can effectively fine-tune well-pretrained LLMs, based on the analysis that shallow submodules have a significant impact on the overall training process as detailed in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.2 Sensitivity-based Drop Rate Allocation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Section 3.1, we introduce DropBP, which selectively drops layers during backward propagation and trains only certain shallow submodules. In addition, we hypothesized that the significance of individual layers and the submodules encompassing these layers varies in their impact on the overall training process. Therefore, we assign different drop rate to each layer based on sensitivity, which is defined by defined by how much each layer and its encompassing submodules affect the overall training process in terms of parameter gradient. Specifically, we calculate the sensitivity of a layer by the variance in L2-norm of parameter gradients between when the backward propagation of that layer is skipped or not, inspired by GradNormVar [36], a memory efficient gradient variance approximation, as below: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{l}=\\sum_{i}(||\\nabla\\mathbf{W}_{i}||_{2}-||\\nabla\\mathbf{W}_{i}^{(l)}||_{2})^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $S_{l}$ denotes of the $l_{\\cdot}$ -th layer. Here, $\\nabla\\mathbf{W}_{i}$ represnets the parameter gradient of the $i$ -th layer when no layers are dropped, while $\\nabla{\\mathbf{W}}_{i}^{(l)}$ denotes the parameter gradient of the $i$ -th layer when the $l$ -th layer is dropped during backward propagation. After calculating the sensitivity for each layer, we aim to minimize the expected sensitivities across all layers-essentially the expected gradient variance caused by DropBP-under a given FLOPs as follow: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\sum_{i}(1-p_{i})S_{i},\\;\\;\\mathrm{s.t.}\\sum_{i}(1-p_{i})F_{i}\\leq F_{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{i}$ denotes the drop rate, and $F_{i}$ denotes the FLOPs of the $i$ -th layer during backward propagation. $F_{t}$ represents the target FLOPs, derived from the target average drop rate $p_{a v g}$ (i.e. $F_{t}=\\left(1-p_{a v g}\\right)\\sum_{i}F_{i})$ . In other words, we determine the drop rates across all layers that minimize the expected sensitivities of the model, while satisfying a given FLOPs budget, by solving Problem 7. In practice, DropBP addresses the Prob. 7 using a simple greedy algorithm, as detailed in Section 4.1. ", "page_idx": 4}, {"type": "text", "text": "4 Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Implementation and Settings ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/7f828fe6d9b5ef933a4badb30e8a9b13fa9ac59844da7ea18d5350dd6e540c83.jpg", "img_caption": ["Figure 4: Code implementation for integrating DropBP. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "DropBP aims to decrease the training costs in fine-tuning based on backpropagation, consequently enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation. To facilitate practical implementation, we developed a user-friendly DropBP library in PyTorch [24], as demonstrated in Fig. 4. In detail, we implemented a DropBP layer that internally drops backward propagation in the input direction according to a given drop rate as shown in Fig. 4a. The DropBP layer designed to initially receive the FLOPs of the layers that would be skipped $(F_{i})$ as initial value to solve Prob. 7. ", "page_idx": 4}, {"type": "text", "text": "Additionally, we developed a DropBPHandler that automatically solve Prob. 7 by assigning varying drop rate to each layer using a simple greedy algorithm, as demonstrated in Fig. 4b. Specifically, the process begins by setting the drop rate $(p_{i})$ of all layers to 0 and then gradually increases them to align with the target average drop rate $(p_{a v g})$ set by the user. In each step, the drop rate for each layer is incremented by 0.1, ensuring that the increase in total expected sensitivity is kept to a minimum. We train with uniform drop rate for the initial $10\\%$ of total iterations and then adjust the drop rate for each layer based on sensitivity when training is stable. Since sensitivity is calculated only once at the $10\\%$ of the entire iteration, the overhead from this calculation is negligible. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We integrated our DropBP code into LitGPT [37] and HuggingFace [38], repositories for training and evaluating LLMs. We first fine-tuned LLaMA2-7B, 13B, and 70B [8] on Alpaca [11] and Dolly [13] datasets. The fine-tuned models were evaluated on 5-shot Massive Multitask Language Understanding (MMLU) tasks [25] and 0-shot commonsense reasoning tasks [26, 27, 28, 29, 30] using lm-evaluation-harness [39] library. We also fine-tunes LLaMA3-8B [9] on the Oasst1 dataset [40] and evaluating the model on MT-Bench task [41] using GPT4o-mini [5] as a judge. These experiments were conducted on a single NVIDIA A100 GPU, and we measured the training time, memory usage, and convergence speed. More details about setup can be found in Appendix F. ", "page_idx": 5}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/de62afc7c698ce6ad02edf8cddde2e64825668eb178d899f72fa5408e75dfdcf.jpg", "table_caption": ["Table 1: Test accuracy on MMLU and commonsense reasoning tasks. In DropBP, drop rate is the target average drop rate across all layers in backward propagation. Note that Full-FT stands for full fine-tuning. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/71c12202f819469446fdd798ef058e66c12b679977ee21f2cce1fd7d6b9645fc.jpg", "table_caption": ["Table 2: Training time, memory usage, and test score on MT-Bench task when fine-tuning LLaMA3- 8B with DropBP on Oasst1 datasets. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Main Results: Accuracy and Efficiency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Accuracy on MMLU and Commonsense Reasoning We employ DropBP to accelerate baseline fine-tuning processes, including full fine-tuning (Full-FT), LoRA [14], and QLoRA [18], on the ", "page_idx": 5}, {"type": "text", "text": "Alpaca and Dolly datasets. As demonstrated in Table 1, DropBP achieves accuracy comparable to the baseline, with deviations less than $1\\%$ in all scenarios, and it even outperforms the baseline in several instances. Specifically, when DropBP is applied to fine-tune LLaMA2-7B, there is a $1\\%$ or less decrease in 5-shot MMLU accuracy compared to the baseline, while maintaining comparable 0-shot commonsense reasoning accuracy. In contrast, for LLaMA2-13B and LLaMA2-70B, fine-tuning with DropBP results in almost no decrease in accuracy on the MMLU and commonsense reasoning tasks, even at the high drop rate of 0.875. ", "page_idx": 6}, {"type": "text", "text": "Accuracy on MT-Bench Similar trends are observed in the MT-Bench tasks, with negligible decreases in accuracy as shown in Table 2. Specifically, when fine-tuning LLaMA3-8B on the Oasst1 dataset, DropBP generates responses of comparable quality to the baseline across various generation tasks. Although scores slightly decrease as the DropBP rate increases, the model fine-tuned with a high DropBP rate of 0.875 still achieves significantly higher scores compared to non-tuned models. ", "page_idx": 6}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/4716226ae4687994f6b06ce7b60601eaa5f3996f8463760a80c59382548149a6.jpg", "table_caption": ["Table 3: Time required for fine-tuning LLaMA2 models with DropBP on the Alpaca datasets when $p$ denotes the target average drop rate. The number of fine-tuning samples is 50K. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Training Speed and Memory Usage We measured the fine-tuning time required to obtain the results in Table 1, as presented in Table 3. When using DropBP to LoRA or QLoRA, training time is reduced by $25\\%$ , $38\\%$ , and $44\\%$ at drop rates of 0.5, 0.75, and 0.875, respectively. In contrast, using DropBP to Full-FT resulted in even higher training time reductions of $33\\%$ , $50\\%$ , and $57\\%$ at the same drop rates. These findings align with the theoretical reduction in FLOPs due to DropBP, as detailed in Appendix B. We also confirmed that DropBP can significantly reduce memory usage during fine-tuning, as shown in Table 2. Specifically, while not using DropBP results in a memory consumption of 57GB, applying DropBP with a drop rate of 0.875 reduces memory usage to 32GB by eliminating the storage of activation memory for dropped layers. Additionally, we evaluated the convergence speed to reach the same validation perplexity (PPL) on downstream tasks, as illustrated in Fig. 5 and Fig. 10-11 in Appendix C. The results indicate that our DropBP increases training speed by up to $1.5\\times$ compared to the baseline in LLaMA2-70B. ", "page_idx": 6}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/cbb91221d72cd4cd5eaf023606c44c05467e20b8b0e8226d61af8923157766fd.jpg", "img_caption": ["Figure 5: Validation perplexity (PPL) for finetuning LLaMA2-70B through QLoRA (baseline) with DropBP on the Alpaca dataset. The $p$ represents the target average drop rate for backward propagation. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Usability of DropBP ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the usability of DropBP, including its ability to train on long sequence data and its training throughput in constrained environments, such as a single NVIDIA A100 GPU or Intel Gaudi2 HPU. ", "page_idx": 6}, {"type": "text", "text": "Table 4: Available maximum sequence length for fine-tuning LLaMA2-70B using QLoRA with DropBP on a NVIDIA A100 GPU, at a micro batch size of 1. ", "page_idx": 6}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/98ab093f785cbe86c68ddad459eefb5ff09c576bbf5a8e5225c6f1b39987d68c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We first measured the maximum sequence length that could be trained without an Out Of Memory (OOM) on a single NVIDIA A100 GPU. The results in Table 4 indicate that our DropBP considerably increases the maximum sequence length, by up to $6.2\\times$ the baseline when the drop rate was 0.875. This is because DropBP allows skipping certain layers during backward propagation, eliminating the need to store activations required for calculating parameter gradients of those skipped layers. We believe that this property of DropBP will be particularly useful for fine-tuning LLMs with long-context data [22, 23]. ", "page_idx": 7}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/c445187c2fd993fa6454d0ff6e228bbb107cc48321a90d72a614fb4161808b84.jpg", "img_caption": ["Figure 6: Throguhput (sentences/s) on a single NVIDIA A100 GPU on a single NVIDIA A100 GPU and Intel Gaudi2 HPU when fine-tuning LLaMA3-8B with a sequence length of 512. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We also evaluated training throughput when full fine-tuning LLaMA3-8B using BF16 precision on a single NVIDIA A100 GPU and an Intel Gaudi2 HPU, increasing the batch size up to the point of OOM errors. As shown in Fig. 6, applying DropBP allows for an increase in batch size per iteration by up to $3.3\\times$ on the NVIDIA A100 GPU and $5.2\\times$ on the Intel Gaudi2 HPU, ensuring high hardware utilization and scalability. Furthermore, DropBP demonstrates a sustained increase in throughput over the baseline at an identical batch size. Ultimately, with a drop rate of 0.875, DropBP achieves a throughput of 16.4 sentences/s on the NVIDIA A100 GPU and 28.4 sentences/s on the Intel Gaudi2 HPU, increasing by $79\\%$ and $117\\%$ over the baseline, respectively. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Impact of the Number of Submodules We conducted an ablation study to investigate the impact of the number of trainable submodules on the finetuning of LLMs. This study compared DropBP, which trains varying submodules randomly at each iteration, with layer freezing, which trains submodules composed of only upper layers. Here, the skip rate $p$ denotes the drop rate in DropBP and the proportion of layers that are frozen in the layer freezing. ", "page_idx": 7}, {"type": "text", "text": "First, we analyzed the number of submodules trained by layer freezing and DropBP. In the case of layer freezing, the lower 2np layers are frozen and only the remaining $2n(1-p)$ layers are trained. In this case, the number of trainable upper submodules is 22n(1\u2212p). In contrast, DropBP randomly drops 2np layers at each iteration, allowing it to train all submodules with a depth of $2n(1-p)$ or less without the restriction of training only the submodules composed of the upper layers. In this scenario, since the number of different submodules at depth $i$ in the entire network is $2n C_{i}$ , DropBP can train $\\textstyle\\sum_{i=0}^{2n(1-p)}$ $2n C_{i}$ submodules. ", "page_idx": 7}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/46f6032d2b9cb71630bdea7ad7c9704746ce446446f08a36a0af6cd267f817c0.jpg", "img_caption": ["Figure 7: Validation perplexity (PPL) for finetuning LLaMA2-7B through LoRA with layer freezing or DropBP on the Alpaca dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "As shown in Table 5, when fine-tuning LLaMA2-7B using layer freezing or DropBP with a high skip rate of 0.875, we observed a significant $1.8\\%$ decrease in accuracy with layer freezing compared to the baseline, while DropBP exhibited a relatively smaller accuracy decrease of $1.0\\%$ . Furthermore, as illustrated in Fig. 7, the convergence speed to the same validation PPL on the downstream task is much slower for layer freezing compared to DropBP, especially at high skip rates, where it converges even more slowly than the baseline. We believe this is due to the ability of DropBP to train a relatively larger number of submodules $(\\textstyle\\sum_{i=0}^{8}\\ \\mathrm{64}C_{i})$ , compared to the fewer submodules trained by layer freezing $(2^{8})$ . Moreover, when fine-tuning LLaMA2-70B, DropBP resulted in a $0.5\\%$ increase in MMLU 5-shot accuracy compared to the baseline, despite a high skip rate of 0.875. This improvement is due to the large number of layers in LLaMA2-70B, enabling DropBP to train deeper and more numerous submodules $(\\sum_{i=0}^{20}{_{160}C_{i}})$ even with a high skip rate of 0.875. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/dc9bbe0ce307fb31d69d565cd5c0d28f6c236824c9e1648b27ef2c3981c095c6.jpg", "table_caption": ["Table 5: The number of submodules being trained and test accuracy on the 5-shot MMLU tasks with layer freezing or DropBP on the Alpaca datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Impact of Sensitivity-based Drop Rate We ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "also conducted an ablation study to analyze the effectiveness of sensitivity-based drop rate allocations. First, we identified the sensitivity of different layers by calculating Eq. 6 during the training of LLMs in various scenarios, as illustrated in Fig. 8a and Fig. 7 in Appendix D. While the distribution varies slightly depending on the number of parameters, fine-tuning approach, and target average drop rate, there is a consistent tendency to assign importance to both the initial and final layers. Consequently, drop rates for these layers are allocated to be lower ", "page_idx": 8}, {"type": "text", "text": "Table 6: Test accuracy on the 0-shot commonsense reasoning tasks when fine-tuning LLaMA2-7B and 13B through LoRA with DropBP at uniform or sensitivity-based drop rate on the Alpaca datasets. The target average drop rate is 0.875. ", "page_idx": 8}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/4cd59c66f7d24c110929924d188da11ed749c5914a0727f62a9da6bb1ab3bae9.jpg", "table_caption": [], "table_footnote": ["by a simple greedy algorithm, as explained in Section 4.1 "], "page_idx": 8}, {"type": "text", "text": "Additionally, we fine-tuned the LLaMA2-7B and 13B using DropBP on Alpaca datasets, comparing sensitivity-based allocated drop rates with uniform drop rate. In detail, we compared the average accuracy of commonsense reasoning tasks when fine-tuning the models with a learning rate of 1e-4 and 3e-4, as shown in Table 6. Note that the PPL for fine-tuning LLaMA2-7B in Fig. 8b corresponds to a learning rate of 3e-4. The results indicate that sensitivity-based drop rates achieved a $1.6\\%$ higher accuracy compared to uniform drop rates with a relatively high learning rate of 3e-4, while there was no significant difference in accuracy when the learning rate was set to 1e-4 in LLaMA2-7B. Fig. 8b also shows that sensitivity-based drop rates consistently stabilized the convergence of validation loss, whereas uniform drop rates occasionally diverged when the learning rate was set to 3e-4 in LLaMA2-7B. This phenomenon is even more pronounced with LLaMA2-13B, resulting in a $1.1\\%$ increase in accuracy through sensitivity-based drop rate allocation, even with a low learning rate of 1e-4. In other words, sensitivity-based drop rate allocation helps stabilize the training process, especially in the case of large learning rates or larger models. ", "page_idx": 8}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/ee02937eb9aaacf5c132c9061690cfe63d7ee23a86bd149fc8cf881babf8d1f5.jpg", "img_caption": ["the average drop rate is set to 0.875. ", "Figure 8: Distribution of drop rates and the validation PPL when fine-tuning LLaMA2-7B through LoRA with DropBP at uniform or sensitivity-based drop rate on Alpaca datasets. ", ""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Parameter-efficient fine-tuning When fine-tuning LLM, substantial amount of memory is required to store parameters, gradients, and optimizer states. LoRA [14] successfully reduces the memory allocated to gradients and optimizer states by inserting trainable rank decomposition matrices into the linear layers of the model while keeping the original LLM parameters frozen. LLaMA-Adapter [15] and LLaMA-Adapter V2 [16] significantly reduce training memory using trainable adoption prompts and zero-initialized attention mechanisms. Some studies attempt to reduce not only the memory footprint of gradients and optimizer states but also that of parameters by considering quantization. PEQA [20], for instance, quantizes the original LLM parameters into a low-bit format and fine-tunes only the scale factor, thus saving memory for parameters during training. QLoRA [18] and QALoRA [19], built upon LoRA, also employ quantization on the original LM parameters, significantly reducing parameter memory during training. Our DropBP is orthogonal and easily combinable with these PEFT techniques, enabling memory and computationally efficient fine-tuning. ", "page_idx": 9}, {"type": "text", "text": "Parallelism Parallelism techniques are widely used to accelerate training LLM using multiple GPU efficiently. Data parallelism [42] is a technique that involves dividing data along the batch dimension for training across multiple GPUs, which still requires sufficient memory to load the entire model on each GPU. Conversely, tensor parallelism [43, 44, 45] partitions the model across GPUs, dividing matrix multiplication operations column-wise and row-wise. Pipeline parallelism [46, 47, 48] involves partitioning the model depth-wise across GPUs, which enables efficient pipeline scheduling. The Zero Redundancy Optimizer (ZeRO) [49] and Fully Sharded Data Parallelism (FSDP) [50] shard parameters, gradients, and optimizer states across multiple GPUs, retrieving parameters when needed to restore their non-partitioned form, enabling the overlapping of computation and communication during training. While these parallelism techniques are designed to efficiently manage the massive computational costs across multiple GPUs, our DropBP specifically aims to reduce the inherent computational costs required for training process. ", "page_idx": 9}, {"type": "text", "text": "Layer dropping Stochastic Depth [51], the first approach to randomly drop layers during neural network training, reduces overftiting and costs in image recognition. Layerdrop [52] randomly drops layers during training and selectively uses some layers during inference, accelerating both processes for transformers. Progressive Layer Dropping (PLD) [53] progressively increases the drop rate across depth and iterations, improving training speed without accuracy degradation in transformers. These techniques speed up pretraining of small transformer models like BERT [54] by dropping layers during the entire training process, whereas DropBP, specific to fine-tuning LLMs, drops layers only during backward propagation. Consequently, as detailed in Appendix E, our DropBP achieves higher performance compared to these layer dropping when fine-tuning LLMs. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose DropBP, an effective algorithm that accelerates the fine-tuning of LLMs by randomly dropping layers during backward propagation, which can be orthogonally integrated into both fullfine tuning and parameter-efficient fine-tuning. We developed the DropBP library as a user-friendly PyTorch extension to facilitate easy integration into existing training codes. Experimental results demonstrate that DropBP significantly accelerates training speed during the fine-tuning of LLMs, achieving comparable accuracy to baseline fine-tuning. Furthermore, DropBP reduces activation memory, enabling long-context training and increasing batch size on limited resources. Consequently, applying DropBP enables a $79\\%$ higher throughput on an NVIDIA A100 GPU and a $117\\%$ higher throughput on an Intel Gaudi2 HPU. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported in part by the NAVER-Intel Co-Lab. The work was conducted by Seoul National University and reviewed by both NAVER and Intel. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Neurips, Long Beach, CA, USA, December 4-9, 2017,, pages 5998\u20136008, 2017. ", "page_idx": 10}, {"type": "text", "text": "[2] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. ", "page_idx": 10}, {"type": "text", "text": "[3] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, NeurIPS, New Orleans, LA, USA November 28 - December 9, 2022, 2022. ", "page_idx": 10}, {"type": "text", "text": "[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, NeurIPS, virtual, December 6-12, 2020, 2020. ", "page_idx": 10}, {"type": "text", "text": "[5] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. ", "page_idx": 10}, {"type": "text", "text": "[6] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. ", "page_idx": 10}, {"type": "text", "text": "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. ", "page_idx": 10}, {"type": "text", "text": "[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, ", "page_idx": 10}, {"type": "text", "text": "Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. ", "page_idx": 11}, {"type": "text", "text": "[9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. ", "page_idx": 11}, {"type": "text", "text": "[10] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR, virtual, April 25-29, 2022. OpenReview.net, 2022. ", "page_idx": 11}, {"type": "text", "text": "[11] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.   \n[12] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment. CoRR, abs/2305.11206, 2023.   \n[13] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023.   \n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, April 25-29, 2022, virtual. OpenReview.net, 2022.   \n[15] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. CoRR, abs/2303.16199, 2023.   \n[16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter V2: parameter-efficient visual instruction model. CoRR, abs/2304.15010, 2023.   \n[17] Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee. Alphatuning: Quantizationaware parameter-efficient adaptation of large-scale pre-trained language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, EMNLP, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 3288\u20133305. Association for Computational Linguistics, 2022.   \n[18] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. CoRR, abs/2305.14314, 2023.   \n[19] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. CoRR, abs/2309.14717, 2023.   \n[20] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. CoRR, abs/2305.14152, 2023.   \n[21] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.   \n[22] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. CoRR, abs/2309.12307, 2023.   \n[23] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023.   \n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, NeurIPS 2019, Vancouver, BC, Canada, December 8-14, 2019, pages 8024\u20138035, 2019.   \n[25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, virtual, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[26] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In AAI, New York, NY, USA, February 7-12, 2020, pages 7432\u20137439. AAAI Press, 2020.   \n[27] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez, editors, ACL, Florence, Italy, July 28- August 2, 2019, pages 4791\u20134800. Association for Computational Linguistics, 2019.   \n[28] Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge. CoRR, abs/2102.03315, 2021.   \n[29] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, EMNLP, Brussels, Belgium, October 31 - November 4, 2018, pages 2381\u20132391. Association for Computational Linguistics, 2018.   \n[30] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In AAAI, New York, NY, USA, February 7-12, 2020, pages 8732\u20138740. AAAI Press, 2020.   \n[31] Martin Svedin, Steven W. D. Chien, Gibson Chikafa, Niclas Jansson, and Artur Podobas. Benchmarking the nvidia gpu lineage: From early k80 to modern a100 with asynchronous memory transfers. arXiv preprint arXiv:2106.04979, 2021.   \n[32] Intel Corporation. Intel gaudi2 ai accelerators white paper. Technical report, Intel Corporation, 2023. Accessed: 2024-09-28.   \n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016.   \n[35] Andreas Veit, Michael J. Wilber, and Serge J. Belongie. Residual networks behave like ensembles of relatively shallow networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 550\u2013558, 2016.   \n[36] Sunghyeon Woo, Sunwoo Lee, and Dongsuk Jeon. ALAM: Averaged low-precision activation for memory-efficient training of transformer models. In The Twelfth International Conference on Learning Representations, 2024.   \n[37] Lightning-AI. Lit-gpt. https://github.com/Lightning-AI/lit-gpt, 2023.   \n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface\u2019s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019.   \n[39] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.   \n[40] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich\u00e1rd Nagyf,i Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations - democratizing large language model alignment. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[42] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. Pytorch distributed: Experiences on accelerating data parallel training. Proc. VLDB Endow., 13(12):3005\u20133018, 2020.   \n[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR, abs/1909.08053, 2019.   \n[44] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model. CoRR, abs/2201.11990, 2022.   \n[45] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. CoRR, abs/2205.05198, 2022.   \n[46] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, NeurIPS 2019, Vancouver, BC, Canada, VDecember 8-14, 2019, pages 103\u2013112, 2019.   \n[47] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, and Phillip B. Gibbons. Pipedream: Fast and efficient pipeline parallel DNN training. CoRR, abs/1806.03377, 2018.   \n[48] Taebum Kim, Hyoungjoo Kim, Gyeong-In Yu, and Byung-Gon Chun. Bpipe: Memory-balanced pipeline parallelism for training large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, ICML, Honolulu, Hawaii, USA, 23-29 July 2023, volume 202 of Proceedings of Machine Learning Research, pages 16639\u201316653. PMLR, 2023.   \n[49] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer, editors, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM, 2020.   \n[50] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):3848\u20133860, 2023.   \n[51] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, ECCV, Amsterdam, The Netherlands, October 11-14, 2016, volume 9908 of Lecture Notes in Computer Science, pages 646\u2013661. Springer, 2016.   \n[52] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[53] Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, NeurIPS, virtual, December 6-12, 2020, 2020.   \n[54] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics, 2019.   \n[55] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016.   \n[56] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David Garc\u00eda, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. CoRR, abs/1710.03740, 2017.   \n[57] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[58] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In ICL, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A The importance of short paths in residual networks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 3.1, we interpret transformer models as a collection of numerous blocks, each composed of various modules with residual connections. Our hypothesis is that we can fine-tune LLMs well by training only certain shallow submodules. To theoretically analyze this hypothesis, we measured the impact of submodules based on their path lengths in LLaMA2-7B, as shown in Fig. 9. Specifically, we followed these steps: ", "page_idx": 15}, {"type": "text", "text": "\u2022 We first perform a forward pass through the entire network.   \n\u2022 During the backward pass, we randomly sample $k$ residual blocks, which are backpropagated without passing through skip connections, while the remaining $n-k$ blocks are bypassed through the skip connections.   \n\u2022 We then measure the norm of the gradient at the input. ", "page_idx": 15}, {"type": "text", "text": "We take 100 measurements for each path length $k$ . Subsequently, we multiply by the distribution of all possible path lengths, which follows a Binomial distribution, to quantify the gradient contribution from paths of a specific length. ", "page_idx": 15}, {"type": "text", "text": "In Fig. 9b, we observed that the gradient per path length decreases as the path length increases. Consequently, Fig. 9c demonstrates that shorter path lengths have a greater impact on the gradient in LLaMA2-7B. These observations are consistent with the existing findings [35] in ResNet [55], which attributed this phenomenon to vanishing gradients. Therefore, our DropBP enables effective training LLMs by focusing on training important short submodules. ", "page_idx": 15}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/1c4e3ab4978ec0b66013bdfb3a5fde8bc4be9be4439fd8dd8673455a6666dc74.jpg", "img_caption": ["Figure 9: The impact of path length for fine-tuning LLaMA2-7B. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Theoretical FLOPs and Actual Training Time Using DropBP ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we calculate the theoretical FLOPs reduction afforded by DropBP and compare this reduction to the actual training time reduction as shown in Table 7. As outlined in Section 2, the computational costs arise from output activation calculations by Eq. 1 during forward propagation, and input and parameter gradient calculations by Eqs. 2 and 3 during backward propagation. We denote the FLOPs for these operations as $F_{o u t}$ , $F_{g r a d}$ , and $F_{p a r a m}$ , respectively. Therefore, the total FLOPs for the backpropagation algorithm can be calculated by the following equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{T}=F_{f w}+F_{b w}\\ \\ \\ }\\\\ {=F_{o u t}+F_{g r a d}+F_{p a r a m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $F_{T}$ represents the FLOPs during the entire training process, $F_{f w}$ for forward propagation (i.e. $F_{f w}=F_{o u t})$ , and $F_{b w}$ for backward propagation (i.e. $F_{b w}=F_{g r a d}+F_{p a r a m})$ . DropBP reduces FLOPs for backward propagation by a target average drop rate $(p_{a v g})$ ). Therefore, total FLOPs in DropBP can be formulated as below: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{T}=F_{f w}+(1-p_{a v g})F_{b w}\\qquad\\qquad}\\\\ {=F_{o u t}+(1-p_{a v g})(F_{g r a d}+F_{p a r a m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consequently, the theoretical FLOPs reduction ratio by DropBP can be represented as follow: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\mathrm{Reduction~Ratio~by~DropBP:}}}&{{\\frac{p_{a v g}\\left(F_{g r a d}+F_{p a r a m}\\right)}{F_{o u t}+F_{g r a d}+F_{p a r a m}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that in full fine-tuning (Full-FT), the computational costs for output calculations, input gradient calculations, and parameter gradient calculations are nearly identical (i.e., $F_{o u t}=F_{g r a d}=F_{p a r a m})$ Conversely, in parameter-efficient fine-tuning techniques (PEFT) such as LoRA and QLoRA, the costs of calculating parameter gradients are negligible $'P_{o u t}=F_{g r a d}$ , $F_{p a r a m}=0)$ ) due to a very small number of trainable parameters and the freezing of original LLM parameters. By substituting this into Eq. 10, the theoretical FLOPs reduction ratio by DropBP can be expressed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\centering\\mathrm{Reduction\\;ratio\\;in\\;Full\u2013FT}\\cdot}&{\\frac{2}{3}p_{a v g}}\\\\ &{\\mathrm{Reduction\\;Ratio\\;in\\;PEFT}\\cdot}&{\\frac{1}{2}p_{a v g}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, with target average drop rates of 0.5, 0.75, and 0.875, DropBP achieves theoretical FLOPs reductions in Full-FT of $33\\%$ , $50\\%$ , and $58\\%$ , respectively, according to Eq. 11. This aligns with the actual training time reduction when utilizing DropBP in Full-FT as shown in Table 3 and Table 7. This trend is also evident when utilizing DropBP in LoRA and QLoRA. According to Eq. 12, the reductions in FLOPs for various target average drop rates of 0.5, 0.75, 0.875 are derived as $25\\%$ , $38\\%$ , and $44\\%$ , respectively. This closely aligns with the actual training time reductions observed when DropBP is applied to LoRA and QLoRA as demonstrated in Table 7. ", "page_idx": 16}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/ad9e27c361875282290a7a1c8e3e6050d3b27e5f9b0cfd6939081c7ab15f1aa6.jpg", "table_caption": ["Table 7: Training time (ms) per iteration for a sequence length of 512 through Full-FT, LoRA or QLoRA using DropBP. Mixed refers to mixed precision training [56] using BFloat16 (BF16) and 32-bit. MBS is denoted as the micro batch size. FW, BW, and Total respectively denote the time consumed for forward propagation, backward propagation, and the entire training process. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Convergence Speed Up Using DropBP ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/1099085a860039798b21c41ba610ca216e0e846237749ffbc251d41ac39ac809.jpg", "img_caption": ["Figure 10: Validation perplexity (PPL) when fine-tuning LLaMA2 models through Full-FT, LoRA, or QLoRA using DropBP on the Alpaca and Dolly datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/385826ba56c3ef9b084a76ca197f6ae4938e3ec40e4d7e1b624cfe6bf1d0db36.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 11: Training curves across training steps and time for fine-tuning LLaMA2-70B through QLoRA with DropBP on the Alpaca datasets. ", "page_idx": 18}, {"type": "text", "text": "When analyzing training curves across training steps in Fig. 11a, the convergence of loss per step at a drop rate of 0.5 is almost identical to the baseline. However, with drop rates of 0.75 and 0.875, the convergence speed per step is slower compared to baseline. Nonetheless, DropBP significantly reduces the time consumed per training step, because it skips the backward propagation computations for the dropped layers. Consequently, the convergence speed per training time is actually faster for DropBP compared to the baseline as shown in Fig. 11b. ", "page_idx": 18}, {"type": "text", "text": "D Distribution of Drop Rates Determined by Sensitivity ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/bda73251fed0a8982b9f613caea4f7fe3957d2dfe9aab29a2fe38324acdedd85.jpg", "img_caption": ["(a) LLaMA2-7B w/ LoRA $^+$ DropBP $({\\mathrm{p}}{=}0.5)$ "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/ad2044cfccc441dfd90d69f9cfece94b1319df5408bac37e379cc1fa0b39f2a8.jpg", "img_caption": ["(c) LLaMA2-7B w/ Full-FT $^+$ DropBP $({\\mathrm{p}}{=}0.5)$ ) "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/6a11d325fbe8aaeb2a199e1dafb4ea658397620c30a7de4a27a65487c0f3e099.jpg", "img_caption": ["(b) LLaMA2-7B w/ LoRA $^+$ DropBP $\\scriptstyle({\\mathrm{p}}=0.875)$ "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/c905fd040c5571eb1f63c21294d2f275c83b270998c190edffc2c5984b6087d0.jpg", "img_caption": ["(d) LLaMA2-7B w/ Full-FT $^+$ DropBP $({\\mathsf{p}}{=}0.875)$ "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/77162874a5de8a1d9572121881b88e2add0f6797cb031b98142ac7821c7e0620.jpg", "img_caption": ["Figure 7: The distribution of drop rates determined by sensitivity when fine-tuning LLaMA2 through Full-FT, LoRA, or QLoRA using DropBP on Alpaca datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Comparisons between Layer Dropping and DropBP on fine-tuning LLMs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we compare Layerdrop (LD) [52] and Progressive Layer Dropping (PLD) [53] with DropBP under the same LLMs fine-tuning scenario. We set the relative FLOPs of LD and PLD to 0.75 of the baseline (LoRA), which corresponds to the same relative FLOPs when the drop rate of DropBP is set to 0.5. ", "page_idx": 19}, {"type": "text", "text": "As shown in Fig. 8, our DropBP converges faster to the same validation PPL compared to LD and PLD. Moreover, as seen in Table 8, DropBP achieves comparable accuracy to the baseline even with a relative FLOPs of 0.56, whereas LD and PLD experience a significant accuracy drop of over $5\\%$ with a relative FLOPs of 0.75. We believe this difference arises from the high sensitivity of forward propagation throughout the fine-tuning process. Specifically, layer dropping techniques omit certain layers of well-pretrained LLMs during forward propagation, resulting in significant output deviations that adversely impact the loss and the overall training process. Conversely, DropBP maintains all layers during forward propagation, thereby ensuring precise outputs and loss calculations, which facilitate stable training. Please note that, as explained in Section 5, LD and PLD are designed to accelerate the pretraining of small transformer models (SLMs) like BERT by dropping layers throughout the entire training process while DropBP only focuses on fine-tuning LLMs. In future studies, we will explore whether DropBP can similarly accelerate the pretraining of transformer models and investigate ways to improve its effectiveness. ", "page_idx": 19}, {"type": "image", "img_path": "x4EoTQW7ka/tmp/353011f011c59deb1a562704ec75c75e1ac11ec32a2cb3328e0dc6c6a1e90a8e.jpg", "img_caption": ["Figure 8: Validation perplexity (PPL) for finetuning LLaMA2-7B through LoRA (baseline) with LayerDrop (LD), Progress Layer Dropping (PLD), or DropBP on the Alpaca dataset. The $p$ represents the target average drop rate for backward propagation in DropBP. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/2ed266c03a34d5cfa8b610fdb6b6f52791a7742c51f7c3391f50592b0506ca2d.jpg", "table_caption": ["Table 8: Test accuracy on the 0-shot commonsense reasoning tasks when fine-tuning LLaMA2-7B through LoRA with layerdrop (LD), progressive layer dropping (PLD), and DropBP. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Experimental Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our experimental setup, the AdamW [57] optimizer and a cosine annealing learning rate scheduler [58] were utilized as common settings. LoRA [14] and QLoRA [18] were integrated to every linear ", "page_idx": 19}, {"type": "text", "text": "layer of our model, with the LoRA parameters $r$ and $\\alpha$ set to 8 and 16, respectively. We experimented with all the learning rates presented in Table 9 and reported the best accuracy achieved in Table 1-2. ", "page_idx": 20}, {"type": "table", "img_path": "x4EoTQW7ka/tmp/760ea298ecf6d087956a70fe726dd682ef57e22b6c9e782cbb09dac5a091514e.jpg", "table_caption": ["Table 9: Detailed Setup for Table 1-2. BS and MBS are denoted as the batch size and micro batch size, respectively. Mixed refers to mixed precision training [56] using BFloat16 (BF16) and 32-bit. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We appropriately present the contributions of the paper in the Abstract and Section 1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: In Section 5 and Appendix E, we clearly state that DropBP is developed specifically for fine-tuning, unlike other layer dropping techniques, and we indicate plans for further improvements in future research. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Appendix B, we mathematically calculate the theoretical reduction in FLOPs achieved by DropBP. Through experiments presented in Section 4.2 and Appendix B, we confirm that this theoretical reduction closely matches the actual training time reduction. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 4.1 and Appendix F, we provide detailed information to ensure reproducibility, and in the abstract, we present the anonymous code in Abstract to implement this. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We present the anonymous code for reproducibility in the Abstract. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the experimental details in Section 4.1, Appendix F, and the anonymous code presented in the Abstract. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: Although we conducted experiments with multiple seeds and report the average values, we did not include error bars or statistical information because the deviations were minimal, and including them would detract from the clarity of the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We mention the types of devices used for the experiments in Section 4.1, and provide the time required to reproduce the experimental results in Section 4.2. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper adheres to the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Since we discuss an efficient fine-tuning algorithm for LLMs, we believe that our work does not have any direct negative societal impact. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work is related to the efficient fine-tuning of LLMs, and therefore, there are no specific safeguards described for the responsible release of data or models, as the nature of our research does not involve high-risk misuse scenarios. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We appropriately cite the original paper and existing codes in Section 4.1 and our code in Abstract. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the new code with a proper license as an anonymized URL in the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]