[{"figure_path": "ml01XyP698/figures/figures_0_1.jpg", "caption": "Figure 1: Comparison between FreeSplat and previous methods. pixelSplat [1] and MVSplat [2] fail to reconstruct geometrically consistent global 3D Gaussians, while our FreeSplat is proposed to accurately localize 3D Gaussians from long sequence input and support free view synthesis.", "description": "This figure compares the performance of FreeSplat against two existing methods, pixelSplat and MVSplat, in reconstructing 3D scenes from multiple views.  It highlights that pixelSplat and MVSplat struggle to create consistent 3D Gaussian representations, especially when dealing with longer sequences of images. In contrast, FreeSplat accurately localizes the 3D Gaussians and enables free-view synthesis. The figure shows input reference views, the depth predictions, the resulting 3D Gaussian splatting, and the rendered novel views for each method.  The red boxes highlight specific regions to visually compare the results.", "section": "Introduction"}, {"figure_path": "ml01XyP698/figures/figures_3_1.jpg", "caption": "Figure 2: Framework of FreeSplat. Given input sparse sequence of images, we construct cost volumes between nearby views and predict depth maps and corresponding feature maps, followed by unprojection to Gaussian triplets with 3D positions. We then propose Pixel-aligned Triplet Fusion (PTF) module, where we progressively aggregate and update local/global Gaussian triplets based on pixel-wise alignment. The global Gaussian triplets can be later decoded into Gaussian parameters.", "description": "This figure illustrates the framework of FreeSplat.  It begins with an input sparse sequence of RGB images.  These images are processed to construct cost volumes between nearby views, generating depth maps and feature maps. These are then unprojected to create 3D Gaussian triplets with 3D position information.  The Pixel-aligned Triplet Fusion (PTF) module progressively aggregates and updates these local and global triplets using pixel-wise alignment.  Finally, these global Gaussian triplets are decoded into Gaussian parameters, which can then be used for rendering.", "section": "4 Our Methodology"}, {"figure_path": "ml01XyP698/figures/figures_4_1.jpg", "caption": "Figure 3: Visual illustration of PTF. The PTF incrementally projects current global Gaussians to input views and computes their pixel-wise distance with local Gaussians. Nearby local Gaussians are then fused using a lightweight Gate Recurrent Unit (GRU) network [49].", "description": "This figure illustrates the Pixel-wise Triplet Fusion (PTF) module.  The left side shows the pixel-wise alignment step: global Gaussians are projected onto the current view and compared to local Gaussians. Only nearby Gaussians (within a threshold) are selected for fusion. The right side depicts the Gaussian latent fusion:  geometrically weighted sums and a GRU network fuse these selected local and global Gaussian triplets.", "section": "4.3 Pixel-wise Triplet Fusion"}, {"figure_path": "ml01XyP698/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative Results of Long Sequence Explicit Reconstruction. For each sequence, the first two rows are view interpolation results, and the last two rows are view extrapolation results.", "description": "This figure displays qualitative results of long sequence explicit reconstruction using the FreeSplat method.  Each sequence shows two view interpolation results (top two rows) and two view extrapolation results (bottom two rows), demonstrating the model's ability to reconstruct consistent 3D scenes from long sequences of input images and generate novel views both within and beyond the input range.  The comparison visually highlights FreeSplat's superior performance in accurately localizing 3D Gaussians and maintaining fine-grained details compared to existing methods. The results emphasize the method's robustness and accuracy in handling longer image sequences and more challenging camera trajectories.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/figures/figures_8_1.jpg", "caption": "Figure 2: Framework of FreeSplat. Given input sparse sequence of images, we construct cost volumes between nearby views and predict depth maps and corresponding feature maps, followed by unprojection to Gaussian triplets with 3D positions. We then propose Pixel-aligned Triplet Fusion (PTF) module, where we progressively aggregate and update local/global Gaussian triplets based on pixel-wise alignment. The global Gaussian triplets can be later decoded into Gaussian parameters.", "description": "This figure illustrates the FreeSplat framework. It begins by taking a sparse sequence of RGB images as input.  Cost volumes are created between nearby views, and depth and feature maps are predicted. These maps are then unprojected into 3D Gaussian triplets. A Pixel-aligned Triplet Fusion (PTF) module is then used to progressively aggregate and update these triplets, resulting in a set of global Gaussian triplets. Finally, these triplets are decoded into Gaussian parameters for rendering.", "section": "4 Our Methodology"}, {"figure_path": "ml01XyP698/figures/figures_14_1.jpg", "caption": "Figure 4: Qualitative Results of Long Sequence Explicit Reconstruction. For each sequence, the first two rows are view interpolation results, and the last two rows are view extrapolation results.", "description": "This figure shows qualitative results of long sequence explicit reconstruction.  It demonstrates FreeSplat's ability to generate novel views from long sequences of input images. The top two rows for each scene represent view interpolation (generating views within the range of the input views). The bottom two rows show view extrapolation (generating views outside the range of the input views). The results are compared to those of MVSplat and pixelSplat, highlighting FreeSplat's superior performance in terms of image quality and accurate depth estimation.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/figures/figures_15_1.jpg", "caption": "Figure 7: Qualitative Comparison with SurfelNeRF.", "description": "This figure compares the results of FreeSplat-fv and SurfelNeRF on the same test sequences. It visually demonstrates the superior rendering quality and efficiency of FreeSplat-fv compared to SurfelNeRF. FreeSplat-fv produces sharper, more detailed images with better reconstruction of fine-grained details, while SurfelNeRF shows blurry, less detailed reconstructions.", "section": "A.4 Additional Qualitative Results"}, {"figure_path": "ml01XyP698/figures/figures_16_1.jpg", "caption": "Figure 8: Qualitative Results given 2 and 3 reference views. We show the rendered color images (first row) and depth maps (second row) for each batch of reference views.", "description": "This figure shows a qualitative comparison of the proposed FreeSplat method against the baselines, pixelSplat and MVSplat.  The top row displays rendered color images, and the bottom row shows corresponding depth maps. The results are shown for both 2-view and 3-view input scenarios, demonstrating the ability of FreeSplat to generate more accurate and detailed results compared to the baselines, particularly in terms of depth estimation.", "section": "A.4 Additional Qualitative Results"}, {"figure_path": "ml01XyP698/figures/figures_17_1.jpg", "caption": "Figure 4: Qualitative Results of Long Sequence Explicit Reconstruction. For each sequence, the first two rows are view interpolation results, and the last two rows are view extrapolation results.", "description": "This figure displays qualitative results for long-sequence explicit reconstruction using FreeSplat.  Each sequence is shown across four rows. The first two rows demonstrate view interpolation results\u2014reconstructing views between existing input views in the sequence. The final two rows illustrate view extrapolation results, showcasing the reconstruction of views beyond the provided input sequence. Visual comparisons are made between the reference image, FreeSplat's results, and results from MVSplat and pixelSplat methods. The image pairs show both rendered color and depth maps to give a comprehensive qualitative comparison of the methods' performance for long sequences.", "section": "5.2 Results on ScanNet"}, {"figure_path": "ml01XyP698/figures/figures_17_2.jpg", "caption": "Figure 10: Qualitative Results of whole scene reconstruction.", "description": "This figure shows a qualitative comparison of the results of whole scene reconstruction.  The leftmost column displays the 3D Gaussian distribution learned by the FreeSplat model. The central column displays the rendered images (color and depth) produced by the FreeSplat model. The rightmost column shows the corresponding target views for comparison. The results demonstrate the model's ability to perform whole-scene reconstruction and render high-quality images from novel viewpoints.", "section": "5.2 Results on ScanNet"}]