[{"type": "text", "text": "Debiasing Synthetic Data Generated by Deep Generative Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Decruyenaere \u2217 Heidelinde Dehaene \u2217 Ghent University Hospital \u2013 SYNDARA Ghent University Hospital \u2013 SYNDARA ", "page_idx": 0}, {"type": "text", "text": "Paloma Rabaey Ghent University \u2013 imec ", "page_idx": 0}, {"type": "text", "text": "Christiaan Polet Ghent University Hospital \u2013 SYNDARA ", "page_idx": 0}, {"type": "text", "text": "Johan Decruyenaere Ghent University Hospital \u2013 SYNDARA ", "page_idx": 0}, {"type": "text", "text": "Thomas Demeester Ghent University \u2013 imec ", "page_idx": 0}, {"type": "text", "text": "Stijn Vansteelandt Ghent University \u2013 Department of Applied Mathematics, Computer Science and Statistics ", "page_idx": 0}, {"type": "text", "text": "\u2217Joint first authors and corresponding authors {firstname.lastname}@ugent.be ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While synthetic data hold great promise for privacy protection, their statistical analysis poses significant challenges that necessitate innovative solutions. The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses. This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation. The standard errors of such estimators then exhibit slower shrinkage with sample size than the typical 1 over root- $\\cdot n$ rate. This complicates fundamental calculations like p-values and confidence intervals, with no straightforward remedy currently available. In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses. Drawing insights from debiased and targeted machine learning, our approach accounts for biases, enhances convergence rates, and facilitates the calculation of estimators with easily approximated large sample variances. We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis. This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The concept of generating synthetic data as a means of privacy protection was initially introduced by Rubin (1993) within the framework of multiple imputation, a widely used technique for managing the statistical analysis of incomplete data sets. Since its inception, a substantial body of literature on synthetic data has emerged (Raghunathan et al., 2003; Raghunathan, 2021; Drechsler, 2011; Raab et al., 2016; Reiter, 2005), with a recent surge in interest propelled by advancements in deep generative modelling technology (Raghunathan, 2021; van Breugel et al., 2023; Wan et al., 2017; Yan et al., 2022; Nowok et al., 2016; Endres et al., 2022; Hernandez et al., 2022). In this work, we focus on tabular synthetic data and its inferential utility, which captures whether synthetic data can be used to obtain valid estimates and inference for a population parameter (Decruyenaere et al., 2024). ", "page_idx": 0}, {"type": "text", "text": "The substantial privacy protection potential offered by synthetic data is marred by significant challenges that undermine their inferential utility (Raab et al., 2016). Previous work by Decruyenaere et al. (2024) has shown that these challenges are much more severe when using Deep Generative Models (DGMs), rather than parametric statistical models, which is why we focus on the former. First, standard confidence intervals and p-values obtained on synthetic data may drastically underestimate the uncertainty in synthetic data as these ignore the size of the original data. Indeed, synthetic data obtained via generators trained on small datasets will unsurprisingly deliver much worse quality than synthetic data obtained via generators trained on large datasets. This uncertainty in the generator must therefore be translated into analysis results, such as confidence intervals and p-values. Standard confidence intervals and p-values ignore this, as they do not distinguish whether the data is synthetic or real. Second, it is well known from the literature on plug-in estimation that data-adaptive methods (such as DGMs) cannot succeed to estimate all features of the data-generating distribution well (Bickel et al., 1993; van der Laan and Rose, 2011; Chernozhukov et al., 2018; Hines et al., 2022). These methods are designed to optimally balance bias and variance w.r.t. a chosen criterion, such as prediction error. However, they cannot guarantee that such an optimal trade-off is simultaneously made w.r.t. all possible discrepancy measures that exist, such as mean squared error in specific functionals (mean, variance, least squares projections...) of the observed data distribution. Such data-adaptive methods therefore leave non-negligible bias in estimators of such functionals, leading to excess variability, slow convergence, and confidence intervals that do not cover the truth at nominal level (and may even never contain the truth, even in large samples) (Decruyenaere et al., 2024). ", "page_idx": 1}, {"type": "text", "text": "Related work. While several approaches have been proposed to account for the uncertainty arising from synthetic data generation, we are not aware of strategies for generating and analysing DGMbased synthetic data that guarantee valid inference. Raghunathan et al. (2003) developed a framework inspired by the work on multiple imputation for missing data, by combining the results of multiple synthetic datasets, but this is not readily applicable to DGM-based synthetic data. R\u00e4is\u00e4 et al. (2023) extended this work for differentially private (DP) synthetic data, acknowledging the additional DP noise, but continue to consider parametric (Bayesian) data generation strategies. ", "page_idx": 1}, {"type": "text", "text": "Our work instead focuses on obtaining valid inference from a single synthetic dataset, which is arguably more attractive for use by practitioners. Raab et al. (2016) derived alternative combining rules that reduce to the correction factor $\\sqrt{1+m/n}$ for the standard error (SE) of an estimator in the case of inference from a single (non-DP) synthetic dataset of size $m$ generated from an original dataset of size $n$ . The method suggested by Awan and Cai (2020) to preserve efficient estimators in a single (DP or non-DP) synthetic dataset relies on generating data conditional on the estimate in the original data. Both procedures are only applicable to parametric generative models and therefore suffer from the same limitation as the aforementioned approaches. To enable Bayesian inference from a single DP synthetic dataset, Wilde et al. (2021) proposed a corrected analysis that relies on the availability of additional public data, while Ghalebikesabi et al. (2022) investigated importance weighting methods to remove the noise-related bias, but they do not study the impact on inference. ", "page_idx": 1}, {"type": "text", "text": "Contributions. Our work is the\u221a first to propose a generator-agnostic solution that mitigates the impact of the typical slower-than- $\\sqrt{n}$ -convergence of estimators in synthetic data created by DGMs. As far as we are aware, our approach is thus the only one that provides some formal guarantees for (more) honest inference in this setting. In this paper, we show how the statistical bias in estimators can be removed, by adapting results on debiased or targeted machine learning (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarily used in the analysis, but rather in the generation of synthetic data. Although we build upon ideas from existing work, our extension is non-trivial, since (1) those works did not consider synthetic data; and (2) we demonstrate, with significant generality, how to mitigate the estimation errors in the DGM that would otherwise propagate into the estimator calculated on synthetic data. ", "page_idx": 1}, {"type": "text", "text": "In Section 2 and 3, we propose a generator-agnostic debiasing strategy, directed towards the downstream statistical analysis of the synthetic data. As such, we obtain estimators that are less sensitive to the typical slow (statistical) convergence affecting the generators. We illustrate this with a simulation study in Section 4, showing that the coverage of both the mean and linear regression coefficient estimators indeed improves. In Section 5, we further cement the utility of our debiasing strategy in a practical setting through two case studies. While the proposed strategy is generator-agnostic, we focus our analyses on two DGMs for tabular data: CTGAN and TVAE (Xu et al., 2019). Finally, Section 6 concludes with a discussion on our method and its limitations. ", "page_idx": 1}, {"type": "text", "text": "2 Notation and Set-Up ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The aim of this paper is to use synthetic data in order to learn a specific functional $\\theta(.)$ of the observed data distribution $P$ . We formalise the problem of learning $\\theta(P)$ based on synthetic data as follows. Suppose that, based on $n$ independent (possibly high-dimensional) samples $O_{1},...,O_{n}$ from $P$ , we estimate the observed data distribution as ${\\widehat{P}}_{n}$ . Here, ${\\widehat{P}}_{n}$ may be obtained by ftiting parametric models to the distribution of the observed data, o r alternativ ely by training DGMs. Based on $m$ independent (synthetic) random samples $S_{1},...,S_{m}$ from ${\\widehat{P}}_{n}$ , we then estimate $\\theta(P)$ . For this, the data analyst (to whom ${\\widehat{P}}_{n}$ is unknown) uses the $m$ synthetic samples $S_{1},...,S_{m}$ to approximate ${\\widehat{P}}_{n}$ as $\\widetilde{P}_{m}$ , and obtains an estimator of $\\theta(P)$ given by $\\theta(\\widetilde{P}_{m})$ . We use $\\mathbb{P}_{n}$ to denote the empirical distribution of the observed data and ${\\widetilde{\\mathbb{P}}}_{m}$ to denote the empirical distribution of the synthetic data. We index expectations by the distribution under which they are taken; e.g., $E_{P}(Y)$ denotes the population expectation of $Y$ . ", "page_idx": 2}, {"type": "text", "text": "Throughout, we assume that the parameter of interest is sufficiently smooth in the data-generating distribution so that root- ${\\mathbf{\\nabla}}n$ consistent estimators (i.e., with SEs shrinking at 1 over root- $^{\\cdot n}$ rate) can be obtained. Formally, we assume the $\\theta(P)$ is pathwise differentiable with efficient influence curve (EIC) $\\phi(.,P)$ under the nonparametric model, as is satisfied for (most) standard statistical analyses. The EIC is a functional derivative of $\\theta(P)$ w.r.t. the data-generating distribution $P$ (in the sense of a Gateaux derivative), which has mean zero under $P$ (Fisher and Kennedy, 2021; Hines et al., 2022). ", "page_idx": 2}, {"type": "text", "text": "In what follows, we illustrate our debiasing strategy via two examples. Here, we briefly outline some of their theoretical foundations we build upon. Appendix A.1 clarifies how these debiased estimators originate from their EIC. We further refer to them as debiased or EIC-based estimators. ", "page_idx": 2}, {"type": "text", "text": "Population mean. Adapting the traditional formulation of the population mean to the context of synthetic data, we choose $\\widetilde{P}_{m}=\\widetilde{\\mathbb{P}}_{m}$ . As a result, we will study the large sample behaviour of the synthetic data sample average: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta(\\widetilde{P}_{m})=\\frac{1}{m}\\sum_{i=1}^{m}S_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The EIC of this sample average is $\\phi(S,\\widehat{P}_{n})=S-\\theta(\\widehat{P}_{n})$ . ", "page_idx": 2}, {"type": "text", "text": "Linear regression coefficient. Second, suppose we are interested in a specific coefficient $\\theta\\equiv\\theta(P)$ of some exposure $A$ for the outcome $Y$ in the linear model $E_{P}(Y|A,X)=\\theta A+\\omega(X)$ , where $X$ is a possibly high-dimensional vector of covariates and $\\omega(.)$ is an unknown function. Our proposal allows $\\omega(X)$ to correspond to a standard linear model in all covariates, but is more generally valid. Building upon the nonparametric definition of $\\theta$ as given in Appendix A.1, we adjust it to obtain an estimator in the setting of synthetic data. Let $Y$ , $X$ and $A$ be jointly or sequentially modelled by some generative model, from which a single synthetic dataset $S_{i}=(\\widetilde{Y}_{i},\\widetilde{A}_{i},\\widetilde{X}_{i})$ is sampled $(i=1,\\ldots,m)$ . We then consider the estimated regression coefficient of exposure $A$ given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta(\\widetilde{P}_{m})=\\frac{\\frac{1}{m}\\sum_{i=1}^{m}\\left\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\right\\}\\left\\{\\widetilde{Y}_{i}-E_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})\\right\\}}{\\frac{1}{m}\\sum_{i=1}^{m}\\left\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\right\\}^{2}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the nuisance parameters $E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})$ and $E_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})$ are estimated based on synthetic data. This EIC-based estimator coincides with the Maximum Likelihood Estimator (MLE) when least squares predictions for these nuisance parameters are used. Its EIC is (Vansteelandt and Dukes, 2022) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\phi(S,\\widehat{P}_{n})=\\frac{\\left\\{\\widetilde{A}-E_{\\widehat{P}_{n}}(A|\\widetilde{X})\\right\\}\\left[Y-E_{\\widehat{P}_{n}}(Y|\\widetilde{X})-\\theta(\\widehat{P}_{n})\\left\\{\\widetilde{A}-E_{\\widehat{P}_{n}}(A|\\widetilde{X})\\right\\}\\right]}{E_{\\widehat{P}_{n}}\\left[\\left\\{\\widetilde{A}-E_{\\widehat{P}_{n}}(A|\\widetilde{X})\\right\\}^{2}\\right]}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In a first step, we establish an understanding of how much the estimate based on synthetic data $\\theta(\\widetilde{P}_{m})$ differs from the population parameter of interest $\\theta(P)$ . For this, we study the difference $\\theta(\\widetilde{P}_{m})-\\theta(P)$ , for which we consider 2 von Mises expansions (i.e., functional Taylor expansions). ", "page_idx": 2}, {"type": "text", "text": "In Appendix A.2 we derive that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\theta(\\widetilde{P}_{m})-\\theta(P)}&{=}&{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widehat{P}_{n})-\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widetilde{P}_{m})+R(\\widetilde{P}_{m},\\widehat{P}_{n})}\\\\ &&{\\displaystyle+\\int\\left\\{\\phi(S,\\widetilde{P}_{m})-\\phi(S,\\widehat{P}_{n})\\right\\}d\\(\\widetilde{\\mathbb{P}}_{m}-\\widehat{P}_{n})}\\\\ &&{\\displaystyle+\\frac{1}{n}\\sum_{i=1}^{n}\\phi(O_{i},P)-\\frac{1}{n}\\sum_{i=1}^{n}\\phi(O_{i},\\widehat{P}_{n})+R(\\widehat{P}_{n},P)}\\\\ &&{\\displaystyle+\\int\\left\\{\\phi(O,\\widehat{P}_{n})-\\phi(O,P)\\right\\}d(\\mathbb{P}_{n}-P).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We now examine the eight terms of this equation and discuss why some may be negligible while other may introduce bias. First, $R(\\cdot)$ are remainder terms, which can generally be shown to be small, but must be studied on a case-by-case basis; see Hines et al. (2022) for worked out examples. In order for these remainder terms to be $o_{p}(m^{-1/2})$ and $o_{p}(n^{-1/2})$ , we generally need that faster than $n^{-1/4}$ convergence rates are obtained for the unknown functionals of $\\widetilde{P}_{m}$ and ${\\widehat{P}}_{n}$ that appear in the EICs. It is unknown whether these convergence rates are attainable for  DGMs;  whether they are, will partly depend on the number of parameters in the DGM itself, the dimension of the data and the complexity of the observed data distribution. The simulation study in Section 4 will give further insight into this. ", "page_idx": 3}, {"type": "text", "text": "Second, the two empirical process terms (1) and (2) in the von Mises expansion can be shown to be $o_{p}(m^{-1/2})$ and $o_{p}(\\bar{n}^{-1/2})$ by Markov\u2019s inequality, under weak conditions. For (1) to converge to zero, we will need the difference between $\\widetilde{P}_{m}$ and ${\\widehat{P}}_{n}$ to converge to zero (in $L_{2}(\\widehat{P}_{n})$ at any rate), and the estimator to be calculated on a different part of the data than the one on which $\\widetilde{P}_{m}$ was estimated (Chernozhukov et al., 2018). For (2) to converge to zero, we will need ${\\widehat{P}}_{n}$ (e.g., the DGM) to consistently estimate $P$ (in $L_{2}(P)$ at any rate). In addition, it can be argued that the DGM needs to be trained on a different part of the data than that on which the debiasing step (see later) will be applied. In Appendix A.3, we elaborate on the necessity of sample splitting. ", "page_idx": 3}, {"type": "text", "text": "We thus foresee that the two remainder and two empirical process terms are negligible under certain conditions. Third, as elaborated in Appendix A.2, we show that the large sample behaviour of both ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\phi(O_{i},P)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widehat{P}_{n})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is standard, and well understood; in particular, these terms vary around zero with variance that can be estimated well. By contrast, the terms ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widetilde{P}_{m})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\frac{1}{n}\\sum_{i=1}^{n}\\phi(O_{i},\\widehat{P}_{n})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "need some further discussion. Term (5) generally fails to have mean zero because the synthetic data $S_{i}$ do not originate from the distribution ${\\widetilde{P}}_{m}$ . This term therefore induces a bias in $\\theta(\\widetilde{P}_{m})$ that results from using data-adaptive estimates $\\widetilde{P}_{m}$ on the synthetic data; a similar term would appear if we instead analysed the real data. Term  (6) likewise fails to have mean zero because the observed data $O_{i}$ do not originate from the distribution ${\\widehat{P}}_{n}$ . Also this term thus induces a bias, now resulting from the use of a generative model to obtain ${\\widehat{P}}_{n}$ . It may be large relative to (3) when DGMs are used, because of slow convergence of ${\\widehat{P}}_{n}$ . It is precisely this term that causes estimators based on synthetic data to converge slowly with increasing sample size, as observed in Decruyenaere et al. (2024). ", "page_idx": 3}, {"type": "text", "text": "After identifying the two problematic terms, we now propose in a second step a targeting or debiasing strategy to remove these bias terms (5) and (6). As in van der Laan and Rose (2011) and Chernozhukov et al. (2018), we will remove bias term (5) by analysing the data with debiased estimators based on the EIC that ensure that this bias term then becomes zero. Novel to our proposal is that we will additionally shift the generated data to ensure that also bias term (6) becomes zero. This bias term depends on the EIC, which itself depends on the target parameter of interest. In the next two paragraphs, we discuss how this can be done for the two considered estimators. Note that the proposed strategy does not require any actual finetuning or retraining of the DGM. For a given parameter of interest, a mere post-processing of the synthetic samples, based on access to the DGM as well as the original data it was trained on, allows eliminating the bias for a given parameter of interest. A graphical summary of the problem setting and our debiasing strategy can be found in Appendix A.4. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.1 Population mean ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the population mean, the debiasing step with respect to term (5) is implicit since the traditional estimator as given in Section 2 is a debiased estimator. Therefore, the proposal to debias a given DGM with respect to the population mean $\\begin{array}{r}{\\theta(P)=\\int o d P(o)}\\end{array}$ amounts to first training the DGM and then augmenting the output for the variable of interest to ensure that bias term (6) is zero, or hence ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\phi(O_{i},\\widehat{P}_{n})=\\frac{1}{n}\\sum_{i=1}^{n}O_{i}-\\theta(\\widehat{P}_{n})=\\overline{{{O}}}-\\theta(\\widehat{P}_{n})=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, the population mean of the synthetic data under the DGM should match the sample average of the real data. Here, $\\theta(\\widehat{P}_{n})$ can be approximated by generating a very large sample $k$ (e.g., one million observations) based on the DGM and calculating their sample mean $\\overline{{Y}}$ . The generative model must then be corrected, to ensure that this sample mean equals $\\overline{O}$ . To obtain a debiased synthetic sample, we shift all samples generated by the given DGM by adding ${\\overline{{O}}}-{\\overline{{Y}}}$ to the considered variable. Note that the sample average of such a set of $m$ corrected synthetic samples will generally differ from $\\overline{O}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Linear regression coefficient ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For linear regression coefficients, this section shows how the samples generated by a given DGM need to be adapted to eliminate bias term (6), written as follows (see Appendix A.5): ", "page_idx": 4}, {"type": "equation", "text": "$$\nb=\\frac{\\sum_{i=1}^{n}\\Big\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\Big\\}\\,\\Big\\{Y_{i}-E_{\\widehat{P}_{n}}(Y|X_{i})\\Big\\}}{\\sum_{i=1}^{n}\\Big\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\Big\\}^{2}}-\\theta(\\widehat{P}_{n}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To compute this bias, we must generate, for each observed value $X_{i}$ , a very large sample of measurements of $A$ and $Y$ with the given level $X_{i}$ , based on the DGM. Then $E_{\\widehat{P}_{n}}(Y|X_{i})$ and $E_{\\widehat{P}_{n}}(A|X_{i})$ can be estimated as the sample average of those values for respectively $Y$ and $A$ . Further based on a very large DGM-generated sample $k$ (e.g. one million observations), we calculate $\\theta(\\widehat{P}_{n})$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\sum_{j=1}^{k}\\left\\{A_{j}-E_{\\widehat{P}_{n}}(A|X_{j})\\right\\}\\left\\{Y_{j}-E_{\\widehat{P}_{n}}(Y|X_{j})\\right\\}}{\\sum_{j=1}^{k}\\left\\{A_{j}-E_{\\widehat{P}_{n}}(A|X_{j})\\right\\}^{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Debiasing of the DGM can now be done by adding the product $b\\{\\widetilde{A}_{i}-E_{\\widehat{P}_{n}}(A|\\widetilde{X}_{i})\\}$ to the synthetic outcome observations ${\\widetilde{Y}}_{i}$ generated by the DGM. An in-depth elaboration is provided in Appendix A.5. With the propose d shifting, we ensure that the debiasing with respect to term (6) is completed. We then proceed our analysis with these shifted synthetic observations and employ the EIC-based estimator of Section 2, which ensures that bias term (5) equals zero as well. ", "page_idx": 4}, {"type": "text", "text": "3.3 Properties ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To summarise, when the remainder terms $R(.)$ and the empirical process terms (1) and (2) are all $o_{p}(m^{-1/2})$ and $o_{p}(n^{-1/2})$ and, furthermore, the suggested debiasing approach is used so as to remove terms (5) and (6), then we expect that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\theta(\\widetilde{P}_{m})-\\theta(P)}&{=}&{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widehat{P}_{n})+\\frac{1}{n}\\sum_{i=1}^{n}\\phi(O_{i},P)+o_{p}(n^{-1/2})+o_{p}(m^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In particular, the resulting estimator $\\theta(\\widetilde{P}_{m})$ may even converge at root- ${\\cdot n}$ rates (under standard conditions of pathwise differentiability ( Bickel et al., 1993; Hines et al., 2022)) and has an easy-tocalculate variance that acknowledges the uncertainty in the generation of synthetic data (provided that the statistical convergence of the generator is not too slow). In Appendix A.6, we show that the variance of $\\theta(\\widetilde{P}_{m})$ may be approximated by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left({\\frac{1}{m}}+{\\frac{1}{n}}\\right)E\\left\\{\\phi(O,P)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "thereby generalising results known for parametric synthetic data generators (Raab et al., 2016; Decruyenaere et al., 2024), where the correction factor $\\sqrt{1+m/n}$ was proposed. Thus, when $m\\rightarrow\\infty$ , the debiased estimator $\\theta(\\widetilde{P}_{m})$ based on debiased synthetic data has the same distribution as when the real data were analysed. Therefore, the proposed debiasing strategy delivers analysis results that are asymptotically equivalent to those obtained from the same analysis on the real data, provided that $n/m\\stackrel{.}{=}\\dot{o}(1)$ . This means that results of the same quality and confidence intervals of the same expected length are then obtained, as will be illustrated in the case study in Section 5.2. ", "page_idx": 5}, {"type": "text", "text": "Since $E\\left\\{\\phi(O,P)^{2}\\right\\}$ is unknown, it merely remains to estimate it as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widetilde{P}_{m})^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This is a consistent estimator when $\\widetilde{P}_{m}$ converges to ${\\widehat{P}}_{n}$ as $m$ goes to infinity, and moreover, ${\\widehat{P}}_{n}$ converges to $P$ as $n$ goes to infinity. We note that this sample variance will be subject to bias that results from \u2018poor\u2019 tuning of the DGM. Removing this bias is not required, because this variance will be scaled by $1/m+1/n$ so that any bias becomes negligible in large samples. While the use of debiased estimators based on the EIC of ${\\cal E}\\left\\{\\phi({\\cal O},P)^{2}\\right\\}$ may potentially improve performance, this goes beyond the scope of this work. ", "page_idx": 5}, {"type": "text", "text": "Practical implications. Sections 3.1 and 3.2 show how bias term (6) is eliminated by shifting the synthetic variable of interest. We describe how bias term (5) is also removed by using debiased estimators, which we referred to as EIC-based estimators (see Section 2). As mentioned earlier, the EIC-based estimator for the population mean always coincides with the sample average, while for the linear regression coefficient it only reduces to the ordinary least squares estimator when data-adaptive estimation of the nuisance parameters is not used. This implies that it may suffice for the applied researcher to 1) shift the synthetic data and apply the traditional estimators, and 2) to multiply the SE of the estimator with $\\sqrt{1+m/n}$ to obtain valid inference from a single synthetic dataset. However, the EIC-based estimators are recommended since they are robust against model misspecification by allowing for more flexibility in the estimation of the nuisance parameters. ", "page_idx": 5}, {"type": "text", "text": "4 Simulation study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our proposed debiasing strategy is empirically validated by a simulation study that covers both estimators 3.1 (sample mean) and 3.2 (linear regression coefficient). Having full control over the data generating process allows us to calculate the bias, SE and convergence rate of both estimators in synthetic data, with and without our debiasing strategy. The data generating process consists of the following four variables: age (normally distributed), atherosclerosis stage (ordinal with four categories), therapy (binary), and blood pressure (normally distributed). The Directed Acyclic Graph (DAG) in Figure 1 represents the dependency structure and we refer to Appendix A.7.1 for more details. This setting allows us to simultaneously target the population mean of age and the population effect of therapy $(A)$ on blood pressure $(Y)$ adjusted for stage $(X)$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Set-up ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct a Monte Carlo simulation study, where $n$ independent records are sampled from the data generating process, forming the observed original dataset $O_{1},...,O_{n}$ . This process is repeated 250 times, with the sample size $n$ varying log-uniformly between 50 and 5000 (i.e., $\\bar{n}\\in\\{50,160,500,1600,5000\\}$ ). Per original dataset, following DGMs are trained: CTGAN and TVAE ( $\\mathrm{\\DeltaXu}$ et al., 2019), of which a detailed explanation can be found in Appendix A.7.2. From these DGMs $m$ synthetic data records are sampled that constitute the default synthetic dataset $S_{1},...,S_{m}$ . We set $m=n$ to retain the dimensionality of the original data. Subsequently, each default synthetic dataset is debiased with respect to both estimands using the steps provided in Section 3, leading to a debiased synthetic dataset. Finally, two estimators are calculated in each of three datasets: the sample mean of age and the linear regression coefficient of therapy on blood pressure adjusted for stage. We always report the maximum likelihood estimation (MLE)-based estimators, as used in traditional statistical analysis, of which the standard errors (SEs) are inflated with the correction factor $\\sqrt{1+m/n}$ to acknowledge the sampling variability of synthetic data. These estimators will deliver similar estimates as the EIC-based estimators since no data-adaptive estimation is used (see Section 3.3 and Appendix A.7.5). ", "page_idx": 5}, {"type": "image", "img_path": "aetbfmCcwg/tmp/75a23a5b33fff6cebf475b1a0acd5d1cde17b9bd0da0853380560efc73602922.jpg", "img_caption": ["Figure 2: Empirical coverage of the $95\\%$ confidence interval for the population mean of age and the population effect of therapy on blood pressure adjusted for stage. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present the results of our simulation study. The DGMs were trained using the default hyperparameters as suggested by the package Synthcity (Qian et al., 2023). We also show results obtained for other hyperparameters (the default in the package SDV (Patki et al., 2016)) in Appendix A.7.4. In Section 4.2.1 we evaluate the empirical coverage of the $95\\%$ confidence interval (CI) for the population parameters. Our debiasing strategy should enhance the coverage, preferably to the nominal level, allowing for (more) honest inference, which is the main contribution of our strategy. Then, we analyse step by step the various components that may influence the coverage by investigating the bias and SE of the estimators in Section 4.2.2, and their convergence rates in Section 4.2.3. Additional results are presented in Appendix A.7.4, including the convergence rates of the nuisance parameters estimated by the DGMs. Our code is available on Github: https: //github.com/syndara-lab/debiased-generation. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Coverage ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "By definition, $95\\%$ (empirical) of the $95\\%$ CIs (nominal) should cover the population parameter. Figure 2 depicts the empirical coverage of the $95\\%$ CIs obtained from both original and synthetic samples for the population mean of age and the population effect of therapy on blood pressure adjusted for stage. The results indicate that our debiasing strategy delivers empirical coverage levels for the population mean that approximate the nominal level for all sample sizes and DGMs considered. By contrast, the coverage based on the default synthetic datasets decreases with increasing $n$ due to slower shrinkage of the SE than the typical 1 over root- ${\\mathbf{\\nabla}}n$ rate, as calculated in Section 4.2.3 and previously elaborated in Decruyenaere et al. (2024). For the population regression coefficient, debiasing delivers coverage at the nominal level for TVAE across all sample sizes, but not for CTGAN, although it clearly provides more honest inferences than based on the default synthetic datasets. The residual loss of coverage likely results from not using (efficient) sample splitting (see Appendix A.3). ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Bias and Standard Error ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Figures 3a and 3b depict the estimates and their SE, respectively, for the sample mean of age obtained in the default and debiased synthetic datasets. Figures A5 and A6 in the appendix show these for the linear regression coefficient of therapy on blood pressure adjusted for stage. In Figure 3a, each dot is an estimate per Monte Carlo run and the true population parameters are represented by the horizontal dashed line. This figure allows a qualitative assessment of two key properties of estimators: empirical bias (i.e., the average difference between the estimates and the population parameter, as represented by the solid line) and empirical SE (i.e., the standard deviation of the estimates, as indicated by the vertical spread of the estimates). Ideally, both converge to zero as the sample size grows larger. The convergence rate conveys the rate at which this happens. The funnel represents the default behaviour of an unbiased estimator based on original data of which the SE diminishes at a rate of 1 over root- ${\\cdot n}$ . ", "page_idx": 6}, {"type": "text", "text": "As shown in Figure 3a, the sample mean of age is unbiased in the default synthetic datasets, but exhibits a large empirical SE that shrinks slowly with sample size due to the data-adaptive nature of ", "page_idx": 6}, {"type": "image", "img_path": "aetbfmCcwg/tmp/aea878a5b9be64e2fc70dace4f48b53ab65404d5c3a873a7b008d0f16a075b28.jpg", "img_caption": ["Figure 3: Each dot in Figure (a) is an estimate for the po\u221apulation mean of age per Monte Carlo run. The funnel indicates the behaviour of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data. Figure (b) depicts the empirical and average MLE-based SE for the sample mean of age. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "DGMs. It is exactly this variability that is, on average, underestimated by the MLE-based SE in Figure 3b. Debiasing reduces the empirical variability and accelerates its shrinkage, such that the average MLE-based SE approximates the empirical SE. For the linear regression coefficient of therapy on blood pressure adjusted for stage, debiasing reduces finite-sample bias and also improves shrinkage of the empirical SE. Albeit less pronounced, the average MLE-based SE still underestimates the empirical SE with CTGAN despite debiasing (see Figures A5 and A6 in the appendix). ", "page_idx": 7}, {"type": "text", "text": "4.2.3 Convergence Rate of Standard Error ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Assuming a power law $n^{-a}$ in convergence rate for the empirical SE, we estimated the exponent $a$ from five logarithmically spaced sample sizes $n$ between 50 and 5000, shown in Table 1 and Figure A7 in the appendix. Standard statistical a\u221analysis assumes that the bias converges faster than the SE with the latter diminishing at a rate of $1/\\sqrt{n}$ . However, the SEs produced by DGMs converge much slower (i.e., $a_{S E}<0.5)$ , leading to a p\u221arogressively increasing underestimation of the empirical SE by the MLE-based SE (which assumes $\\bar{\\sqrt{n}}$ -convergence) as the sample size grows larger. In turn, this results in too narrow CIs and poor coverage, as observed in Section 4.2.1. By contrast, our debiasing strategy renders estimators of which the SE converges at approximately root- $n$ rates (i.e., $a_{S E}=0.5)$ ), explaining the improvement in coverage of the CIs as a result of debiasing. ", "page_idx": 7}, {"type": "table", "img_path": "aetbfmCcwg/tmp/20d9f4f3c0effde894e969015e9d7b50f7dcb7925c918613804f63c9c1993058.jpg", "table_caption": ["Table 1: Estimated exponent $a$ $95\\%$ CI] for the convergence rate $n^{-a}$ for empirical SE. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Case studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To illustrate our findings and highlight their implications for the applied researcher, we conduct two case studies. First, Section 5.1 transfers the framework from our simulation study to the International Stroke Trial (IST) dataset (Sandercock et al., 2011). Second, Section 5.2 describes whether analysis results from the Adult Census Income dataset (Becker and Kohavi, 1996) are similar to those obtained from the real data, when the sample size $m$ of the generated synthetic data is very large. In both case studies, estimated SEs in the default synthetic and debiased synthetic datasets are corrected by multiplying with factor $\\sqrt{1+m/n}$ to acknowledge the sampling variability of synthetic data. ", "page_idx": 7}, {"type": "text", "text": "5.1 International Stroke Trial ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We adapt the framework discussed in Section 4.1 to the IST dataset, one of the biggest randomised trials in acute stroke research (Sandercock et al., 2011). The dataset with 19285 complete cases now constitutes our population. We mimic different hypothetical settings where an institution only has access to a limited sample of observations, with the sample size $n$ varying between 50 and 5000. In order to easily share the data with other researchers, the institution generates a synthetic dataset with sample size $m$ , where $m=n$ . We repeated this process 100 times per sample size $n$ to be able to calculate the empirical coverage levels. For illustration purposes, we focus on the effect of aspirin on the outcome at 6 months and report the proportion of deaths for the two treatment arms (aspirin and no aspirin), and its corresponding risk difference. For each value of $n$ , two default synthetic datasets were generated using both CTGAN and TVAE. Next, for each of these, we first split the default synthetic dataset by treatment, debias the data with relation to the population mean within each treatment arm, and then combine them back into one debiased synthetic dataset. We noticed that using the same hyperparameters as in the simulation study resulted in biased estimates, as can be seen in Figure A12 in the appendix. For this reason, we highlight the results obtained by training with the default hyperparameters suggested by the package SDV (Patki et al., 2016) instead. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "One of the original research questions in Sandercock et al. (2011) was whether or not there is a difference in risk of death between the treatment arms. Suppose a researcher can repeatedly collect information on 500 subjects and uses original, default synthetic and debiased synthetic data to make an inferential statement about this risk difference. Figure 4 depicts the confidence intervals for the first 15 repetitions, with the vertical dashed lines representing the true risk difference of $-0.009$ as calculated based on our population (the full dataset). Should the researcher use the default synthetic data, they would falsely conclude (in 7 out of these 15 repetitions) that the risk is significantly different from $-0.009$ , while using the debiased synthetic dataset basically eliminates this high number of false-positives (with all these 15 intervals containing the population parameter), as is the case in the original data as well. More results can be found in Appendix A.8.1. ", "page_idx": 8}, {"type": "image", "img_path": "aetbfmCcwg/tmp/1d97ddf6f61a343ae357d9c28d7c4b9f283689a424679e251975ec26a52e7557.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Empirical coverage of $95\\%$ CIs for Figure 5: $95\\%$ CIs for the population mean and the risk difference for death in each of the three regression coefficient for $\\bar{m}=10^{6}$ and different datasets $m=n=500)$ ) for the IST case study. sample sizes $n$ for the Adult Income case study. ", "page_idx": 8}, {"type": "text", "text": "5.2 Adult Census Income Dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We also perform a case study on the Adult Census Income dataset, which comprises 45222 complete cases and 14 unique variables (Becker and Kohavi, 1996). We assume the researcher\u2019s interest lies in inferring the population mean of hours worked per week (estimated via the sample mean) and the average sex-adjusted difference in age between persons with an income of $>{\\mathfrak{G}}{\\mathfrak{H}}0\\mathbf{K}$ a year vs. not (estimated via a linear regression model age $(Y)\\sim$ income $(A)+s e x\\left(X\\right))$ . Our goal in this case study is to confirm whether inferential results obtained using the debiased synthetic dataset are asymptotically equivalent (i.e. with $m>>n$ in our debiasing strategy) to those obtained using the original data. To test this across different sample sizes, the original data constitute five different samples of the Adult Census Income dataset with sizes $n$ varying log-uniformly between 50 and 45222. For each original dataset, a default synthetic dataset of size $\\bar{m}=10^{6}$ was generated by TVAE. Subsequently, this dataset was debiased following the steps described in Section 3.1 (sample mean) and Section 3.2 (linear regression coefficient). ", "page_idx": 8}, {"type": "text", "text": "Figure 5 depicts the $95\\%$ CIs for both estimators, the five original sample sizes and the three versions of datasets. This indeed confirms that analysis on the debiased synthetic dataset leads to results of similar quality and CIs of similar length compared to the original dataset. By contrast, the analysis on the default synthetic dataset may yield results of inferior quality and even incorrect conclusions. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a new debiasing strategy that targets synthetic data created by DGMs towards the downstream task of statistical inference from the resulting synthetic data. We establish our theory for two estimators by applying insights from debiased or targeted machine learning literature (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarily used in the analysis, but rather in the generation of synthetic data. We obtain estimators that are less sensitive to the typical slow (statistical) convergence affecting DGMs and thereby improve the inferential utility of the synthetic data. ", "page_idx": 9}, {"type": "text", "text": "We illustrated the impact of our proposal through a simulation study on toy data and two case studies on real-world data. Our debiasing strategy results in root- ${\\mathbf{\\nabla}}n$ consistent estimators based on the synthetic data and thereby better coverage of the confidence intervals, allowing for more honest inference. While coverage was clearly improved, it was not guaranteed to be at the nominal level. Indeed, it may remain anti-conservative for some estimators and DGMs, due to slow convergence inherent to these models and/or due to residual overfitting bias that could not be removed since sample splitting was not performed. Future work should focus on efficient sample splitting, where the resulting bias reduction outweighs the increase in finite-sample bias that arises from training on smaller sample sizes. Alternatively, findings from Ghalebikesabi et al. (2022) on importance weighting could be incorporated, with the weights being targeted to eliminate the impact of the data-adaptive estimation of the weights. This may potentially relax the fast baseline convergence assumption, and enable the same debiased synthetic data to be used for multiple downstream analyses. ", "page_idx": 9}, {"type": "text", "text": "A key advantage of our debiasing strategy is that it may deliver synthetic data created by DGMs of which the analysis is equivalent to the original data analysis, provided that the synthetic sample size is chosen to be much larger than the original sample size. Although the risk of disclosure may increase with the size of non-DP synthetic data (Reiter and Drechsler, 2010), this trade-off is beyond the scope of our paper. More interestingly, in the case of DP synthetic data, our debiasing strategy exploits their post-processing immunity that allows for data transformations without compromising any privacy guarantees (Dwork and Roth, 2014). However, our strategy needs to be extended to incorporate the DP-constraints when studying the difference between $\\theta(\\bar{\\tilde{P}_{m}})$ and $\\theta(P)$ in DP synthetic data. ", "page_idx": 9}, {"type": "text", "text": "Limitations of our proposal include the low-dimensional setting of our simulation and case studies, for which DGMs might be less well suited. The positive results found for two widely used estimators in this simple setting highlight the utility of a debiasing approach and are encouraging in terms of future larger-scale applications. However, before addressing these, it is important to first understand low-dimensional settings, where valid inference is already challenging to attain. While our debiasing strategy boils downs to a post-processing step, one could argue that the lack of change to the DGM\u2019s training strategy itself is actually a strength, since it renders our strategy generator-agnostic. ", "page_idx": 9}, {"type": "text", "text": "Another limitation concerns the fact that our debiasing strategy for a regression coefficient requires sampling of synthetic data conditional on a covariate, which is not available in all DGMs. However, this issue is partially mitigated in the case of conditioning on categorical variables, since one can always generate a synthetic dataset unconditionally and then only select samples that fti the condition \u2013 though this approach also has its limits, especially when conditioning on multiple covariates at once. Zhou et al. (2023) propose a deep generative approach to sample from a conditional distribution, even when working with high-dimensional data. Future work could explore this strategy. ", "page_idx": 9}, {"type": "text", "text": "Finally, our proposal requires that the person generating synthetic data is aware of the analyses that will be run on those data, and has access to the corresponding EICs needed for debiasing (which in particular rules out the possibility for debiasing w.r.t. non-pathwise differentiable parameters, such as conditional means or predictions). For each parameter of interest, the data generated by the DGM will then need to be debiased (simultaneously) w.r.t. that parameter\u2019s EIC, which is left for future research. In the case of original data, several debiased estimation strategies that do not require exact knowledge about the EIC already exist. These methods include a) approximating the EIC through finite-differencing (Carone et al., 2019; Jordan et al., 2022) or stochastic approximations via Monte Carlo (Agrawal et al., 2024), and b) automatically estimating the EIC from the data through auto-DML (Chernozhukov et al., 2022). Alternatively, kernel debiased plug-in estimation methods (Cho et al., 2023) enable simultaneous debiasing of all pathwise differentiable target parameters that meet certain regularity conditions, without requiring any knowledge about the EIC. Integrating their insights could further strengthen the foundations of our current work on the interplay between synthetic data, deep generative modelling, and debiased machine learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Paloma Rabaey\u2019s research is funded by the Research Foundation Flanders (FWO-Vlaanderen) with grant number 1170124N. This research also received funding from the Flemish government under the \u201cOnderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen\u201d programme. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Agrawal, R., Witty, S., Zane, A., and Bingham, E. (2024). Automated efficient estimation using monte carlo efficient influence functions. arXiv preprint arXiv:2403.00158. ", "page_idx": 10}, {"type": "text", "text": "Awan, J. and Cai, Z. (2020). One step to efficient synthetic data. arXiv preprint arXiv:2006.02397.   \nBecker, B. and Kohavi, R. (1996). Adult. UCI Machine Learning Repository.   \nBickel, P., Klaassen, C., Ritov, Y., and Wellner, J. (1993). Efficient and Adaptive Estimation for Semiparametric Models, volume 4. Baltimore Johns Hopkins University Press.   \nCarone, M., Luedtke, A. R., and van der Laan, M. J. (2019). Toward computerized efficient estimation in infinite-dimensional models. Journal of the American Statistical Association.   \nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1\u2013C68.   \nChernozhukov, V., Newey, W. K., and Singh, R. (2022). Automatic debiased machine learning of causal and structural effects. Econometrica, 90(3):967\u20131027.   \nCho, B., Gan, K., Malenica, I., and Mukhin, Y. (2023). Kernel Debiased Plug-in Estimation. arXiv preprint arXiv:2306.08598.   \nDecruyenaere, A., Dehaene, H., Rabaey, P., Polet, C., Decruyenaere, J., Vansteelandt, S., and Demeester, T. (2024). The real deal behind the artificial appeal: Inferential utility of tabular synthetic data. In The 40th Conference on Uncertainty in Artificial Intelligence.   \nDrechsler, J. (2011). Synthetic datasets for statistical disclosure control: theory and implementation, volume 201. Springer Science & Business Media.   \nDwork, C. and Roth, A. (2014). The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science, 9(3\u20134):211\u2013407.   \nEndres, M., Mannarapotta Venugopal, A., and Tran, T. S. (2022). Synthetic data generation: a comparative study. In Proceedings of the 26th International Database Engineered Applications Symposium, pages 94\u2013102.   \nFisher, A. and Kennedy, E. H. (2021). Visually communicating and teaching intuition for influence functions. The American Statistician, 75(2):162\u2013172.   \nGhalebikesabi, S., Wilde, H., Jewson, J., Doucet, A., Vollmer, S., and Holmes, C. (2022). Mitigating statistical bias within differentially private synthetic data. In Cussens, J. and Zhang, K., editors, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180 of Proceedings of Machine Learning Research, pages 696\u2013705. PMLR.   \nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.   \nHernandez, M., Epelde, G., Alberdi, A., Cilla, R., and Rankin, D. (2022). Synthetic data generation for tabular health records: A systematic review. Neurocomputing, 493:28\u201345.   \nHines, O., Dukes, O., Diaz-Ordaz, K., and Vansteelandt, S. (2022). Demystifying statistical learning based on efficient influence functions. American Statistician, 76(3):292\u2013304. ", "page_idx": 10}, {"type": "text", "text": "Jordan, M., Wang, Y., and Zhou, A. (2022). Empirical gateaux derivatives for causal inference. Advances in Neural Information Processing Systems, 35:8512\u20138525. ", "page_idx": 11}, {"type": "text", "text": "Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint.   \nNowok, B., Raab, G. M., and Dibben, C. (2016). Synthpop: Bespoke creation of synthetic data in R. Journal of Statistical Software, 74(11):1\u201326.   \nPatki, N., Wedge, R., and Veeramachaneni, K. (2016). The synthetic data vault. In IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 399\u2013410.   \nQian, Z., Cebere, B.-C., and van der Schaar, M. (2023). Synthcity: facilitating innovative use cases of synthetic data in different data modalities. arXiv preprint.   \nRaab, G. M., Nowok, B., and Dibben, C. (2016). Practical data synthesis for large samples. Journal of Privacy and Confidentiality, 7(3):67\u201397.   \nRaghunathan, T. E. (2021). Synthetic data. Annual review of statistics and its application, 8:129\u2013140.   \nRaghunathan, T. E., Reiter, J. P., and Rubin, D. B. (2003). Multiple imputation for statistical disclosure limitation. Journal of official statistics, 19(1):1.   \nR\u00e4is\u00e4, O., J\u00e4lk\u00f6, J., Kaski, S., and Honkela, A. (2023). Noise-aware statistical inference with differentially private synthetic data. In International Conference on Artificial Intelligence and Statistics, pages 3620\u20133643. PMLR.   \nReiter, J. P. (2005). Releasing multiply imputed, synthetic public use microdata: an illustration and empirical study. Journal of the Royal Statistical Society Series A: Statistics in Society, 168(1):185\u2013205.   \nReiter, J. P. and Drechsler, J. (2010). Releasing multiply-imputed synthetic data generated in two stages to protect confidentiality. Statistica Sinica, pages 405\u2013421.   \nRubin, D. B. (1993). Statistical disclosure limitation. Journal of official Statistics, 9(2):461\u2013468.   \nSandercock, P. A., Niewada, M., Cz\u0142onkowska, A., and Group, I. S. T. C. (2011). The international stroke trial database. Trials, 12(1):101.   \nvan Breugel, B., Qian, Z., and van der Schaar, M. (2023). Synthetic data, real errors: How (not) to publish and use synthetic data. arXiv preprint.   \nvan der Laan, M. J. and Rose, S. (2011). Targeted Learning. Springer Series in Statistics. Springer New York, New York, NY.   \nVansteelandt, S. and Dukes, O. (2022). Assumption-lean inference for generalised linear model parameters. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(3):657\u2013 685.   \nWan, Z., Zhang, Y., and He, H. (2017). Variational autoencoder based synthetic data generation for imbalanced learning. In 2017 IEEE symposium series on computational intelligence (SSCI), pages 1\u20137. IEEE.   \nWilde, H., Jewson, J., Vollmer, S., and Holmes, C. (2021). Foundations of bayesian learning from synthetic data. In International Conference on Artificial Intelligence and Statistics, pages 541\u2013549. PMLR.   \nXu, L., Skoularidou, M., Cuesta-Infante, A., and Veeramachaneni, K. (2019). Modeling tabular data using conditional GAN. Advances in neural information processing systems, 32.   \nYan, C., Yan, Y., Wan, Z., Zhang, Z., Omberg, L., Guinney, J., Mooney, S. D., and Malin, B. A. (2022). A multifaceted benchmarking of synthetic electronic health record generation models. Nature communications, 13(1):7609.   \nZhou, X., Jiao, Y., Liu, J., and Huang, J. (2023). A deep generative approach to conditional sampling. Journal of the American Statistical Association, 118(543):1837\u20131848. ", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Elaboration estimators for synthetic data ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Population mean. The population mean of the observed data $o$ is defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta(P)=\\int o\\frac{d P(o)}{d o}d o=\\int o d P(o),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which we will denote short as $\\textstyle\\int O d P$ and its efficient influence curve (EIC) is $\\phi(O,P)=O-\\theta(P)$ . Choosing $\\widetilde{P}_{m}=\\widetilde{\\mathbb{P}}_{m}$ , we will then study the large sample behaviour of the synthetic data estimator: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta(\\widetilde P_{m})=\\int s d\\widetilde{\\mathbb P}_{m}(s)=\\frac{1}{m}\\sum_{i=1}^{m}S_{i}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Linear regression. Suppose we are interested in a specific coefficient $\\theta\\equiv\\theta(P)$ of some exposure $A$ in the linear model ", "page_idx": 12}, {"type": "equation", "text": "$$\nE_{P}(Y|A,X)=\\theta A+\\omega(X),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $X$ is a possibly high-dimensional covariate and $\\omega(.)$ is an unknown function. Our proposal allows $\\omega(X)$ to correspond to a standard linear model, but is less restrictive. The parameter $\\theta$ in this linear model can be nonparametrically defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta(P)=\\frac{E_{P}\\left[\\left\\{A-E_{P}(A|X)\\right\\}\\left\\{Y-E_{P}(Y|X)\\right\\}\\right]}{E_{P}\\left[\\left\\{A-E_{P}(A|X)\\right\\}^{2}\\right]}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "in the sense that it reduces to $\\theta$ when the above model holds. Its EIC is (Vansteelandt and Dukes, 2022) ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\phi(O,P)=\\frac{\\left\\{A-E_{P}(A|X)\\right\\}\\left[Y-E_{P}(Y|X)-\\theta(P)\\left\\{A-E_{P}(A|X)\\right\\}\\right]}{E_{P}\\left[\\left\\{A-E_{P}(A|X)\\right\\}^{2}\\right]}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Denote the obtained synthetic data samples as $S=(\\widetilde{Y},\\widetilde{A},\\widetilde{X})$ . An estimator $\\theta(\\widetilde{P}_{m})$ is then obtained by substituting in the above expression for $\\theta(P)$ , the fir st e xpectation in the nu merator and denominator by a sample average, $E_{P}(A|X)$ and $E_{P}(Y|X)$ by data-adaptive predictions $E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})$ and $E_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})$ obtained based on the synthetic data. This delivers estimator ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta(\\widetilde{P}_{m})=\\frac{\\frac{1}{m}\\sum_{i=1}^{m}\\left\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\right\\}\\left\\{\\widetilde{Y}_{i}-E_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})\\right\\}}{\\frac{1}{m}\\sum_{i=1}^{m}\\left\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\right\\}^{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 Derivation of the impact of uncertainty affecting deep generative models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We are interested in knowing how much $\\theta(\\widetilde{P}_{m})$ differs from $\\theta(P)$ . For this, we study the difference $\\theta(\\widetilde{P}_{m})-\\theta(P)$ , for which we consider 2 von Mises expansions (i.e., functional Taylor expansions). Th roughout the calculation below, we use that influence curves $\\phi(O,P)$ have the property of being mean zero when averaging w.r.t. the distribution $P$ . We moreover let $R(.)$ be a remainder term, which can generally be shown to be small, but must be studied on a case-by-case basis. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sqrt{\\sigma_{\\mathrm{p}}}-\\mu(P_{1})}&{=\\delta\\left(P_{2}-\\rho_{0}^{\\ast}-\\delta\\left(P_{2}\\right)\\right)+\\delta\\left(P_{3}\\right)}\\\\ &{=\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(S_{p}\\right)\\wedge\\sigma_{\\mathrm{p}}}-\\sum_{1}\\delta\\left(P_{3},P_{2}\\right)}\\\\ &{\\quad+\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(P_{3}\\right)\\wedge\\sigma_{\\mathrm{p}}}+\\gamma_{1}\\delta\\left(P_{3},P_{2}\\right)}\\\\ &{=-\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(S_{p}\\right)\\wedge\\sigma_{\\mathrm{p}}}+\\delta\\left(P_{3},P_{3}\\right)-\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(Q_{3},P_{3}\\right)\\cdot\\delta\\eta^{\\ast}}+\\delta\\left(P_{3},P_{2}\\right)}\\\\ &{=-\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(S_{p}\\right)\\wedge\\sigma_{\\mathrm{p}}}-\\sum_{1}\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(S_{p}\\right)\\wedge\\sigma_{\\mathrm{p}}}+\\delta\\left(P_{3},P_{3}\\right)}\\\\ &{\\quad+\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(P_{3}\\right)\\wedge\\sigma_{\\mathrm{p}}}-\\gamma_{2}\\left(\\delta\\left(P_{3},P_{2}\\right)\\right)\\delta\\eta_{1}+\\delta\\left(P_{3},P_{3}\\right)}\\\\ &{=\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(S_{p}\\right)\\wedge\\sigma_{\\mathrm{p}}}+\\sum_{1}\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(P_{3}\\right)\\wedge\\sigma_{\\mathrm{p}}}+\\delta\\left(P_{3},P_{3}\\right)}\\\\ &{\\quad+\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(S_{p}\\right)\\wedge\\sigma_{\\mathrm{p}}}-\\delta\\left(P_{3},P_{3}\\right)\\mathrm{d}\\bar{\\sigma}_{\\mathrm{p}}-\\delta\\left(P_{3},P_{3}\\right)}\\\\ &{\\quad+\\int\\mathrm{e}^{\\phi_{\\mathrm{G}}\\left(P_{3}\\right)\\wedge\\sigma_{\\mathrm{p}}}+\\delta\\left(P_{3},P_{3}\\right)\\mathrm{d}\\bar{\\sigma}_{\\mathrm{\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\phi(O_{i},P)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "equals 1 over root- $n$ times a term that is asymptotically normal (as $n$ goes to infinity) with mean zero and variance ${\\cal E}\\left\\{\\phi({\\cal O},P)^{2}\\right\\}$ . Further, conditional on ${\\widehat{P}}_{n}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widehat{P}_{n})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "${}^{m}$ $E\\left\\{\\phi(S,\\widehat{P}_{n})^{2}|\\widehat{P}_{n}\\right\\}^{1/2}$ distribution (as $m$ goes to infinity). This follows from the mean zero property of influence curves and the fact that the synthetic data are drawn from the distribution $\\hat{P}_{n}$ . This mean zero property is essential as it implies that the variation of ${\\widehat{P}}_{n}$ across repeated (observed data) samples does not induce excess variability. Next letting also the sample size $n$ go to infinity, we obtain convergence of $\\begin{array}{r}{m^{-1/2}\\sum_{i=1}^{m}\\phi(S_{i},\\widehat{P}_{n})}\\end{array}$ to $E\\left\\{\\phi(O,P)^{2}\\right\\}N(0,1)$ under the assumption that $E\\left\\{\\phi(S,\\widehat{P}_{n})^{2}|\\widehat{P}_{n}\\right\\}$ converges in probability to ${\\cal E}\\left\\{\\phi({\\cal O},P)^{2}\\right\\}$ . This is a weak assumption because ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nE\\left\\{\\phi(S,\\widehat{P}_{n})^{2}|\\widehat{P}_{n}\\right\\}=\\int\\phi(o,\\widehat{P}_{n})^{2}d\\widehat{P}_{n}(o)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is generally smooth (continuous) in the distribution of the data (as in the considered two examples). Moreover, the flexibility offered by deep generative models (DGMs) makes it reasonable to assume that ${\\widehat{P}}_{n}$ converges to $P$ (e.g., in $L_{2}(P))$ ; while this convergence may be slow, no requirements on the rate of convergence are needed for the above assumption to be satisfied. ", "page_idx": 14}, {"type": "text", "text": "A.3 Note on sample splitting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In order to let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int\\left\\{\\phi(O,\\widehat{P}_{n})-\\phi(O,P)\\right\\}d(\\mathbb{P}_{n}-P)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "converge to zero, we will need the DGM to be trained on a different part of the data than the one on which the debiasing step will be applied. Such sample splitting is needed to prevent overfitting bias that may otherwise result from the highly data-adaptive nature of DGMs. In addition, we need ${\\widehat{P}}_{n}$ (e.g., the DGM) to consistently estimate $P$ in the sense that the squared mean (under $P$ ) of $\\phi({\\cal O},\\bar{\\hat{\\cal P}_{n}})-\\phi({\\cal O},{\\cal P})$ (at fixed ${\\widehat{P}}_{n}$ ) converges to zero in probability. ", "page_idx": 15}, {"type": "text", "text": "To prevent efficiency loss with sample splitting, one may consider the use of $k$ -fold cross-ftiting. For this, we randomly split the data in $k$ folds, each time train the DGM on $k-1$ folds to obtain an estimator of the observed data distribution $\\widehat{P}_{(k-1)n/k}$ and calculate the bias ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\frac{1}{n/k}\\sum_{i}\\phi(O_{i},\\widehat{P}_{(k-1)n/k}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "based on the $n/k$ data points $O_{i}$ in the remaining fold. The average of the $k$ obtained bias estimates can next be used for debiasing (see the next section for specific examples). ", "page_idx": 15}, {"type": "text", "text": "However, it remains to be seen from future work if the resulting bias reduction outweighs the increase in finite-sample bias that may result from training the DGM on smaller sample sizes. Preliminary simulations with a simple implementation suggested that this was not the case, which is why our results in the main text are reported without the use of sample splitting. Furthermore, the remainder of the theory discards this nuance for now as well. ", "page_idx": 15}, {"type": "image", "img_path": "aetbfmCcwg/tmp/e22537442a4a577547741ff0cffe9e56bf752814c694dd64f310bce9c6a2843b.jpg", "img_caption": ["A.4 Graphical summary "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure A1: Suppose that interest lies in inferring the mean $\\theta(.)$ (orange solid vertical line) of the ground truth distribution $P$ (orange solid curve) from synthetic data. (1) First, a random sample of original data of size $_n$ with empirical distribution $\\mathbb{P}_{n}$ (orange histogram) is collected from this ground truth. Theory on asymptotic linearity prescribes that the sample mean (orange dashed vertical line) will deviate from the population mean by order of 1 over root- $_n$ (grey arrow). (2) Subsequently, a deep generative model is trained on these original data, yielding an estimated distribution ${\\widehat{P}}_{n}$ (blue solid curve). Its mean (blue solid vertical line) may in turn differ from the original sample mean by an order larger than 1 over root- $_n$ (red arrow), which is referred to as regularisation bias in Decruyenaere et al. (2024). (3) Finally, synthetic data of size $m$ (here $m=n)$ ) are sampled from the estimated distribution ${\\widehat{P}}_{n}$ , forming the empirical distribution $\\widetilde{\\mathbb{P}}_{m}=\\widetilde{P}_{m}$ (blue histogram with sample mean indicated by the blue dashed vertical line). The mean of both distributions ${\\widehat{P}}_{n}$ and ${\\widetilde{P}}_{m}$ will again differ by order of 1 over root- $^m$ (green arrow). Ideally, the data analyst, who uses the synthetic data to estimate $\\theta(P)$ by $\\theta(\\widetilde{P}_{m})$ , needs to take into account these three sources of random variability. (4) The large sample behaviour of the synthetic data estimator $\\theta(\\widetilde{P}_{m})$ is depicted by repeating the above procedure multiple times across increasing sample sizes of $n=m$ and storing each estimate of the synthetic sample mean. Although the estimator remains unbiased for $\\theta(P)$ (dashed line), its empirical standard error becomes larger than in the original data due to the additional sources of variability. However, the correction factor $\\textstyle{\\sqrt{1+m/n}}$ to the model-based standard error previously proposed by Raab et al. (2016) only captures the original data sampling variability (grey funnel) and a lower bound of the synthetic data sampling variability (green funnel), while the uncertainty associated with the regularisation bias (red funnel) remains unaccounted for. Since it cannot readily be expressed analytically, our debiasing strategy will eliminate the latter by shifting the mean of the distribution $\\theta(\\widehat{P}_{n})$ estimated by the generative model towards the original sample mean (thereby removing the red arrow and funnel). Additionally, choosing synthetic sample sizes of $m\\rightarrow\\infty$ will shrink the synthetic data sampling variability (ultimately removing the green arrow and funnel), such that the synthetic data estimator exhibits similar large sample behaviour as in original data. ", "page_idx": 16}, {"type": "text", "text": "A.5 Elaboration on debiased strategy for linear regression coefficient ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the linear regression coefficient case, this section shows how the samples generated by a given DGM need to adapted to eliminate the bias $b$ , given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{b}&{=}&{\\displaystyle\\frac{\\sum_{i=1}^{n}\\left\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\right\\}\\left[Y_{i}-E_{\\widehat{P}_{n}}(Y|X_{i})-\\theta(\\widehat{P}_{n})\\left\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\right\\}\\right]}{\\sum_{i=1}^{n}\\left\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\right\\}^{2}}}\\\\ &{=}&{\\displaystyle\\frac{\\sum_{i=1}^{n}\\left\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\right\\}\\left\\{Y_{i}-E_{\\widehat{P}_{n}}(Y|X_{i})\\right\\}}{\\sum_{i=1}^{n}\\left\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\right\\}^{2}}-\\theta(\\widehat{P}_{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To compute this bias, we must generate, for each observed value $X_{i}$ , a very large sample of measurements of $A$ and $Y$ with the given level $X_{i}$ , based on the DGM. Then $E_{\\widehat{P}_{n}}(Y|X_{i})$ can be calculated as the sample average of those values for $Y$ , and likewise $E_{\\widehat{P}_{n}}(A|X_{i})$ can be calculated as the sample average of those values for $A$ . Further based on a large sample generated based on the DGM, we calculate $\\theta(\\widehat{P}_{n})$ as the sample average of $\\left\\{A-E_{\\widehat{P}_{n}}(A|\\dot{X})\\right\\}\\left\\{\\dot{Y}-E_{\\widehat{P}_{n}}(Y|X)\\right\\}$ divided by the sample average of $\\Big\\{A-E_{\\widehat{P}_{n}}(A|X)\\Big\\}^{2}.$ ", "page_idx": 17}, {"type": "text", "text": "Debiasing of the DGM can now be done by adding $b\\{\\widetilde{A}_{i}-E_{\\widehat{P}_{n}}(A|\\widetilde{X}_{i})\\}$ to the synthetic outcome observations generated by the DGM. This change does not affect the predictions $E_{\\widehat{P}_{n}}(Y|X_{i})$ from the DGM (because $\\widetilde{A}_{i}-E_{\\widehat{P}_{n}}(A|X_{i})$ averages to zero for each choice of $X_{i}$ ). Further, with this change, $\\theta(\\widehat{P}_{n})$ also increases with $b$ units as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i=1}^{N}\\left\\{\\widetilde{A}_{i}-E_{\\widehat{P}_{n}}(A|\\widetilde{X}_{i})\\right\\}\\left\\{\\widetilde{Y}_{i}+b\\left\\{\\widetilde{A}_{i}-E_{\\widehat{P}_{n}}(A|\\widetilde{X}_{i})\\right\\}-E_{\\widehat{P}_{n}}(Y|\\widetilde{X}_{i})\\right\\}}{\\sum_{i=1}^{N}\\left\\{\\widetilde{A}_{i}-E_{\\widehat{P}_{n}}(A|\\widetilde{X}_{i})\\right\\}^{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the sum runs over a large sample of synthetic observations. With this change in $\\theta(\\widehat{P}_{n})$ , the previously calculated bias becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\small\\frac{\\sum_{i=1}^{n}\\Big\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\Big\\}\\Big\\{Y_{i}-E_{\\widehat{P}_{n}}(Y|X_{i})\\Big\\}}{\\sum_{i=1}^{n}\\Big\\{A_{i}-E_{\\widehat{P}_{n}}(A|X_{i})\\Big\\}^{2}}-\\Big\\{\\theta(\\widehat{P}_{n})+b\\Big\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is zero, and as such the debiasing with respect to term (6) is complete. Based on a new sample of synthetic observations (independent of those generated to calculate the bias), the synthetic data estimator is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\theta(\\widetilde{P}_{m})}&{=}&{\\frac{\\frac{1}{m}\\sum_{i=1}^{m}\\Big\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\Big\\}\\,\\Big[\\widetilde{Y}_{i}+b\\,\\Big\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\Big\\}-E_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})\\Big]}{\\frac{1}{m}\\sum_{i=1}^{m}\\Big\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\Big\\}^{2}}}\\\\ &{=}&{\\frac{\\frac{1}{m}\\sum_{i=1}^{m}\\Big\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\Big\\}\\,\\Big\\{\\widetilde{Y}_{i}-E_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})\\Big\\}}{\\frac{1}{m}\\sum_{i=1}^{m}\\Big\\{\\widetilde{A}_{i}-E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})\\Big\\}^{2}}+b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This ensures that bias term (5) equals zero. Please note that this estimator coincides with the standard linear regression estimator on the debiased synthetic data when least squares predictions for $E_{\\widetilde{P}_{m}}(A|\\widetilde{X}_{i})$ and $\\overline{{E}}_{\\widetilde{P}_{m}}(Y|\\widetilde{X}_{i})$ are used. ", "page_idx": 17}, {"type": "text", "text": "A.6 Derivation of variance of debiased estimator $\\theta(\\widetilde{P}_{m})$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "With the suggested debiasing, we thus expect that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\theta(\\widetilde{P}_{m})-\\theta(P)}&{=}&{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\phi(S_{i},\\widehat{P}_{n})+\\frac{1}{n}\\sum_{i=1}^{n}\\phi(O_{i},P)+o_{p}(n^{-1/2})+o_{p}(m^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the synthetic data are independently drawn from the observed data, any covariance between the 2 leading terms must originate from the fact that ${\\widehat{P}}_{n}$ depends on the observed data. This covariance equals zero since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{E\\left\\{\\phi(S_{i},\\widehat{P}_{n})\\phi(O_{j},P)\\right\\}}&{=}&{E\\left[E\\left\\{\\phi(S_{i},\\widehat{P}_{n})|O_{1},...,O_{n}\\right\\}\\phi(O_{j},P)\\right]}\\\\ &{=}&{E\\left[E\\left\\{\\phi(S_{i},\\widehat{P}_{n})|\\widehat{P}_{n},O_{1},...,O_{n}\\right\\}\\phi(O_{j},P)\\right]}\\\\ &{=}&{E\\left[E\\left\\{\\phi(S_{i},\\widehat{P}_{n})|\\widehat{P}_{n}\\right\\}\\phi(O_{j},P)\\right]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the second equality we use that ${\\widehat{P}}_{n}$ is determined by $O_{1},...,O_{n}$ , in the third equality we use that $S_{i}$ only depends on $O_{1},...,O_{n}$ via ${\\widehat{P}}_{n}$ , and in the final equality we use that $\\phi(S_{i},\\widehat{P}_{n})$ has mean zero when the synthetic data are sampled from ${\\widehat{P}}_{n}$ . This renders these terms asymptotically independent and their sum, hence, asymptotically normal. Moreover, since the variance of $\\bar{\\phi}(S_{i},\\widehat{P}_{n})$ converges to $E\\left\\{\\phi(O,P)^{2}\\right\\}$ (see the previous section; Appendix A.2), the variance of $\\theta(\\widetilde{P}_{m})$ may thus be approximated by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left({\\frac{1}{m}}+{\\frac{1}{n}}\\right)E\\left\\{\\phi(O,P)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "thereby generalising results known for parametric synthetic data generators (Raab et al., 2016;   \nDecruyenaere et al., 2024). ", "page_idx": 18}, {"type": "text", "text": "Algorithm 1: Data generating process for hypothetical disease. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "input :Requested number of data records $_n$ .   \noutput :Dataframe $D$ with $n$ records, each made up of 4 attributes: age, stage, therapy, bp.   \n$D\\leftarrow E$ mpty dataframe   \nfor $i\\gets1$ to $n$ do $a g e\\gets N o r m a l(m e a n=50,s t d=10)$ $\\nu_{a g e}\\leftarrow0.05$ $\\nu_{I},\\nu_{I I},\\nu_{I I I}\\leftarrow2,3,4$ $c p_{I},c p_{I I},c p_{I I I}\\leftarrow$ $S i g m o i d(\\nu_{I}-\\nu_{a g e}\\times a g e),$ Sigmoid(\u03bdII \u2212\u03bdage \u00d7 age), Sigmoid(\u03bdIII \u2212\u03bdage \u00d7 age) stage \u2190Categorical $(c a t=[I,I I,I I I,I V]$ , probs = [cpI, cpII \u2212cpI, cpIII \u2212cpII, 1 \u2212cpIII]) $t h e r a p y\\gets C a t e g o r i c a l(c a t=[F a l s e,T r u e],p=[0.5,0.5])$ $\\beta_{t h e r a p y}\\leftarrow-20$ $\\beta_{I},\\beta_{I I},\\beta_{I I I},\\beta_{I V}\\leftarrow0,10,20,30$ $\\mu_{b p}\\leftarrow120+\\beta_{s t a g e}+\\beta_{t h e r a p y}\\times t h e r a p y$ $b p\\gets N o r m a l(m e a n=\\mu_{b p},s t d=10)$ $D_{i}\\gets\\{a g e,s t a g e,t h e r a p y,b p\\}$   \nend ", "page_idx": 19}, {"type": "text", "text": "Inspired by an applied medical setting, we create a hypothetical disease, defined by a low-dimensional tabular data generation process. The dependency structure depicted by the directed acyclic graph (DAG) in Figure 1 in the main text displays the presence of four variables, each of them chosen to obtain a mix of data types. In our hypothetical disease, it is assumed that a patient is observed at a given point in time. At this time, patient data about age, atherosclerosis stage, and the random assignment of therapy is gathered. The continuous outcome variable blood pressure is evaluated at a later time point, making this design a simplification, since we do not consider the data as longitudinal. ", "page_idx": 19}, {"type": "text", "text": "The exact routine to reconstruct this data generating process is presented in the pseudo-code in Algorithm 1. $A g e$ (continuous) follows a normal distribution with mean 50 and standard deviation 10. Atherosclerosis stage (ordinal) was generated according to a proportional odds cumulative logit model where an increase in age causes an increase in the odds of having a stage higher than a given stage $k$ $[\\nu_{a g e}\\,=\\,-0.05$ and intercepts $\\nu_{s t a g e}\\,=\\,\\{2,3,4\\}$ for stage I-III). Therapy (binary) is considered to be 1:1 randomly assigned and is therefore sampled from a Bernouilli distribution with a probability of 0.50. The last variable, blood pressure (continuous), is sampled from a normal distribution with standard deviation 10 and where the baseline average of 120 increases with higher atherosclerosis stage $(\\beta_{s t a g e}=\\{0,10,20,30\\}$ for stage I-IV, respectively) and absence of therapy $(\\beta_{t h e r a p y}=-20)$ . ", "page_idx": 19}, {"type": "text", "text": "A.7.2 Deep Generative Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We elaborate on the DGMs used to create synthetic data in our simulation study and cases studies. All experiments were run on our institutional high performance computing cluster using a single GPU (NVIDIA Ampere A100; 80GB GPU memory) and single CPU (AMD EPYC 7413), taking less than 24 hours to complete (simulation study: less than 15 minutes per individual run across 5 sample sizes; International Stroke Trial case study: less than 75 minutes per individual run across 5 sample sizes; Adult Census Income Dataset case study: less than 4 hours). We focus on two commonly used DGMs, namely Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational Autoencoders (VAEs) (Kingma and Welling, 2013). ", "page_idx": 19}, {"type": "text", "text": "A GAN consists of two competing neural networks, a generator and discriminator, and aims to achieve an equilibrium between both (Hernandez et al., 2022). This translates to a mini-max game, since the generator aims to minimise the difference between the real and generated data, while the discriminator aims to maximise the possibility to distinguish the real and generated data (Goodfellow et al., 2014). We use the CTGAN implementation that was designed specifically for tabular data, proposed by Xu et al. (2019). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "A VAE is a deep latent variable model, consisting of an encoder and a decoder (Kingma and Welling, 2013). The encoder models the approximate posterior distribution of the latent variables given an input instance, whereby typically a standard normal prior is assumed for the latent variables. The decoder allows reconstructing an input instance, based on a sample from the predicted latent space distribution. Encoder and decoder can be jointly trained by maximising the evidence lower bound (ELBO), i.e. the marginal likelihood of the training instances. Maximising the ELBO corresponds to minimising the Kullback-Leibler (KL) divergence between the predicted latent variable distribution for a given input instance and the standard normal priors, and minimising the reconstruction error of the input instance at the decoder output. Once again, we use the tabular implementation of a VAE (TVAE) proposed by Xu et al. (2019). ", "page_idx": 20}, {"type": "text", "text": "The results presented in the rest of the Appendix are obtained by training both types of DGM with default hyperparameters as suggested by the packages Synthcity and SDV (for both CTGAN and TVAE). A comparison of the default hyperparameters in both packages is provided in Tables A1 and A2. Note that both packages implement the CTGAN and TVAE modules as originally proposed by $\\mathrm{Xu}$ et al. (2019), where a cluster-based normaliser is used to preprocess numerical features. ", "page_idx": 20}, {"type": "table", "img_path": "aetbfmCcwg/tmp/82dc31236f2801ff6a4b7a72fc70f8b52f0d47d6845e62afe3ca860211b8381c.jpg", "table_caption": ["Table A1: Comparison of CTGAN default hyperparameters between SDV and Synthcity. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "aetbfmCcwg/tmp/ba1305848b4c8647f4c3927cec32738f7716407834825897c40bd62098934115.jpg", "table_caption": ["Table A2: Comparison of TVAE default hyperparameters between SDV and Synthcity. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.7.3 Quality of synthetic data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We performed some additional analyses to assess the quality of the synthetic data obtained in our simulation study. ", "page_idx": 20}, {"type": "text", "text": "Average IKLD. The inverse of the KL divergence (IKLD) between original and synthetic data, averaged over 250 Monte Carlo runs and standardised between 0 and 1, is presented in Table A3, where the debiased synthetic datasets have slightly higher IKLD than their default versions for all DGMs considered. ", "page_idx": 20}, {"type": "table", "img_path": "aetbfmCcwg/tmp/36c3c3ec8abc68abf865e120cded19e2a66c59391a69e5bb4db5c142e57cbcfd.jpg", "table_caption": ["Table A3: The IKLD between original and synthetic data, averaged over 250 Monte Carlo runs and standardised between 0 and 1. Higher values indicate similar datasets in terms of underlying distribution. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Failed generators. CTGAN (Synthcity) could not be trained in 14 runs (runs 38, 69, 102, 225 for $n=500$ ; runs 19, 23, 98, 107, 117, 128, 129, 140, 148, 169 for $n=5000)$ ) due to an internal error in the package. As such, it was not possible to generate default and debiased synthetic data with CTGAN (Synthcity) in these runs. This comprises $1.12\\%$ (14/1250) of all CTGAN (Synthcity) trained and $0.28\\%$ (14/5000) of all generators trained. The other generative models did not produce errors during training, so that default synthetic data could be generated in every run. ", "page_idx": 21}, {"type": "text", "text": "Failed debiasing. TVAE (SDV) could not be debiased in 82 runs (runs $\\mathrm{1,3,4,5,6,8}$ , 15, 23, 24, 28, 30, 42, 46, 50, 51, 58, 64, 72, 78, 92, 96, 104, 106, 111, 112, 117, 118, 119, 122, 126, 127, 130, 133, 135, 140, 143, 146, 149, 154, 157, 159, 160, 166, 167, 168, 170, 178, 180, 187, 191, 193, 196, 197, 200, 205, 210, 214, 216, 218, 220, 225, 230, 231, 235, 236, 245, 247, 248 for $n\\,=\\,50$ ; runs 13, 22, 97, 117, 139, 145, 146, 148, 161, 186, 193, 197, 235, 246 for $n~=~160)$ ) due to sparse data, especially for small sample sizes. In particular, some $X_{i}$ in the original dataset may not be present in the default synthetic dataset generated by TVAE (SDV), leading to an error when estimating the nuisance parameters needed for the debiasing step. This comprises $6.56\\%$ (82/1250) of all TVAE (SDV) trained and $1.64\\%$ (82/5000) of all generators trained. The other generative models did not produce errors during debiasing, so that debiased synthetic data could be generated in every run. ", "page_idx": 21}, {"type": "text", "text": "Exact memorisation. A sanity check was conducted to ensure that no records of the original data were memorised by the generative model. CTGAN (Synthcity) made the following number of exact copies of the original data in the synthetic data: one record $(2.00\\%)$ for $n=50$ in three runs (runs 63, 158, 192). The other generative models did not make exact copies. ", "page_idx": 21}, {"type": "text", "text": "Non-estimable estimators. Due to sparse data, especially for small sample sizes, the linear regression coefficient of therapy on blood pressure adjusted for stage could not be estimated in a small subset of the 11140 (original and synthetic) datasets, producing extremely small $(<1\\mathrm{e}{-}10)$ or large $(>1\\mathrm{e2})$ standard errors. Overall, $0.09\\%$ (10/11140) regression coefficient estimates could not be obtained: in 4 runs for default synthetic dataset of size $m=50$ generated by CTGAN (SDV) and in 1 run for default synthetic dataset of size $m=160$ generated by CTGAN (Synthcity), and for their corresponding debiased synthetic datasets. The sample mean of age was always estimable. ", "page_idx": 21}, {"type": "text", "text": "A.7.4 Additional results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "While the main text only reports results using the Synthcity default hyperparameters, we additionally wanted to report a wider variety of trained models, without manually tuning anything, which is why results for SDV are presented here as well. ", "page_idx": 21}, {"type": "text", "text": "Coverage for all estimators and models. The results shown in Figure A2 indicate that our debiasing strategy delivers empirical coverage levels for the population mean that approximate the nominal level for all sample sizes and DGMs considered. By contrast, the coverage in the default synthetic datasets decreases with increasing $n$ due to slower shrinkage of the standard error (SE) than the typical 1 over root- ${\\cdot n}$ rate, as calculated in Section 4.2.3 and previously addressed in Decruyenaere et al. (2024). While our debiasing strategy clearly improves the empirical coverage for the population regression coefficient, it does, however, not guarantee this to be at the nominal level for all sample sizes and DGMs considered. In particular, debiasing only seems to achieve coverage at the nominal level for TVAE (Synthcity) across all sample sizes and for TVAE (SDV) at sufficiently large sample sizes. Our approach still falls short for CTGAN (Synthcity) and CTGAN (SDV), although providing more honest inference than the default synthetic datasets. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "aetbfmCcwg/tmp/db8f514f80de1275671efd663ac6b3737626b9c54d70c485829552ac950838e9.jpg", "img_caption": ["Figure A2: Empirical coverage of the $95\\%$ confidence interval for the MLE-based estimators. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Bias and SE for mean age. As shown in Figure A3, the sample mean of age is unbiased in the default synthetic datasets, but exhibits large variability that shrinks slowly with sample size due to the data-adaptive nature of DGMs. Debiasing reduces this variability and accelerates shrinkage. Figure A4 indicates that the empirical SE for the sample mean of age is indeed, on average, underestimated by the MLE-based SE in default synthetic datasets. After debiasing, the average MLE-based SE approximates the empirical SE, albeit with minor deviations at smaller sample sizes. ", "page_idx": 22}, {"type": "image", "img_path": "aetbfmCcwg/tmp/aeff7703ee3557272c788f4fcd90916c1da599f0eef090686f9c47cc647942bd.jpg", "img_caption": ["Figure A3: The horizontal dashed line represents the population mean of age and each dot is a MLE-based estimate per Monte Carlo run \u221a(250 dots in total per value of $n$ ). The dashed funnel indicates the behaviour of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Bias and SE for effect therapy on blood pressure adjusted for stage. It can be seen from Figure A5 that the linear regression coefficient of therapy on blood pressure adjusted for stage in the default synthetic datasets has finite-sample bias towards the null that converges to zero as the sample size grows larger. Additionally, its variability seems to diminish slower than expected. Debiasing reduces the finite-sample bias and improves the shrinkage of the SE. However, Figure A6 shows that the average MLE-based SE in the debiased synthetic datasets still underestimates the empirical SE with CTGAN (SDV) and CTGAN (Synthcity) despite debiasing, albeit less pronounced. ", "page_idx": 22}, {"type": "image", "img_path": "aetbfmCcwg/tmp/59c9f4122fb3da795748ea8eb4cc1623ce72a1550a3fc3abc9a4770ff7d50519.jpg", "img_caption": ["Figure A4: The empirical and MLE-based standard error for the sample mean of age. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "aetbfmCcwg/tmp/d186ba33eec1c780ec0176b35c8bdf418b786be1c04fcdc5a923304a5bec9c87.jpg", "img_caption": ["Figure A5: The horizontal dashed line represents the population effect of therapy on blood pressure adjusted for stage and each dot is a MLE-based estimate per Monte Carlo ru\u221an (250 dots in total per value of $n$ ). The dashed funnel indicates the behaviour of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "aetbfmCcwg/tmp/4bb551024395ea80c252d78c027fd54311c7e810355cf2aa3eeeae20504270e1.jpg", "img_caption": ["Figure A6: The empirical and MLE-based standard error for the (MLE-based) effect of therapy on blood pressure adjusted for stage. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Convergence of SE for all estimators and models. The convergence rate of the empirical SE for the MLE-based estimators is shown in Table A4 and represented b\u221ay the slope in Figure A7. The dashed line indicates the behaviour of the SE of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data, whereas the dotted line indicates the assumed behaviour of the SE of the same estimator based on synthetic data, following the correction proposed by Raab et al. (2016). Lines for the empirical SE that are parallel to these lines indicate that the estimator converges at 1 over root- $n$ rate, whereas more horizontal or vertical lines imply slower or\u221a faster shrinkage, respectively. After debiasing, all lines are parallel for both estimators, suggesting $\\sqrt{n}$ -consistency. Note that the vertical offset of some lines, which represents the log asymptotic variance, is still too large for some DGMs despite debiasing. This is the case for the linear regression coefficient for therapy on blood pressure adjusted for stage obtained in the debiased synthetic dataset generated by CTGAN (SDV) and CTGAN (SDV). ", "page_idx": 24}, {"type": "table", "img_path": "aetbfmCcwg/tmp/58b02fbee9b0111ce1857819416e22735c156fa9ee00a5640fcf9c089300ddff.jpg", "table_caption": ["Table A4: Estimated exponent $a$ $[95\\%$ CI] for the power law convergence rate $n^{-a}$ for empirical SE. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "aetbfmCcwg/tmp/42cd5b8da7b56c93215c66197d3599733370bcdd95da072e0e0e0480bb251e93.jpg", "img_caption": ["Figure A7: Convergence rate of the empirical standard error (SE) for the MLE-based estimators. If the SE is of the form $\\mathrm{SE}=c n^{-a}$ , where $c$ is a constant, then $\\log\\left(S E\\right)=\\log(c)+(-a)\\log\\left(n\\right)$ . Therefore slope $a$ represents the convergence rate and the vertical offset $\\log(c)$ indicates the \u221alog asymptotic variance. The dashed line indicates the behaviour of the SE of an unbiased and $\\sqrt{n}$ - consistent estimator based on observed data, whereas the dotted line indicates the assumed behaviour of the SE of the same estimator based on synthetic data, following the correction proposed by Raab et al. (2016). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Nuisance parameters. Our debiasing strategy assumes that the two remainder terms in the von Mises expansion presented in Section 3 are $o_{p}(m^{-1/2})$ and $o_{p}(n^{-1/2})$ . This requires the difference between $\\widetilde{P}_{m}$ (i.e., estimate of ${\\widehat{P}}_{n}$ based on the sampled synthetic dataset of $m$ records) and ${\\widehat{P}}_{n}$ (i.e., estimate of $P$ by the DGM trained on $n$ original records) and the difference between ${\\widehat{P}}_{n}$ and $P$ (i.e., the data generating process), to converge to zero in $L_{2}(\\widehat{P}_{n})$ and $L_{2}(P)$ , respectively, at faster than $n$ -to-the-quarter convergence rates. While this holds true for the convergence of ${\\widetilde{P}}_{m}$ to ${\\widehat{P}}_{n}$ (see Table A5), this seems not always strictly attained for the functionals of ${\\widehat{P}}_{n}$ with respect to $P$ that appear in the efficient influence curves for the regression coefficient (see Table A6), in particular for CTGAN (Synthcity) and, to a lesser extent, for CTGAN (SDV) and TVAE (Synthcity). Nevertheless, a root- ${\\mathbf{\\nabla}}n$ consistent estimator was still obtained after debiasing these DGMs (see Table A4). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "table", "img_path": "aetbfmCcwg/tmp/5d2f0855a42ec36bfd67a56a5a0af3a734f3977293a3e9a92689a14e1c154373.jpg", "table_caption": ["Table A5: Estimated exponent $a$ $[95\\%$ CI] for the power law convergence rate $n^{-a}$ for the difference between functionals of $\\mathbf{\\widetilde{\\boldsymbol{P}}}_{m}$ and ${\\widehat{P}}_{n}$ in $L_{2}(\\widehat{P}_{n})$ . "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "aetbfmCcwg/tmp/8179a186cfaaa7f4334c4fad204e4c3b2bad67d3c8f6ae19bcbbdaa7693a25d1.jpg", "table_caption": ["Table A6: Estimated exponent $a$ $[95\\%$ CI] for the power law convergence rate $n^{-a}$ for the difference between functionals of $\\widehat{\\widehat{P}}_{n}$ and $P$ in $L_{2}(P)$ . "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Summary. Table A7 summarises the effect of our debiasing strategy on bias, SE and coverage in the simulation study. Our strategy results in uniformly valid coverage for the population mean, allowing for honest inference. For the regression coefficient, coverage was clearly improved but may remain anti-conservative for some DGMs. This may originate from residual overftiting bias inherent to these DGMs that could not be removed since (efficient) sample splitting was not performed (see Appendix A.3). ", "page_idx": 25}, {"type": "table", "img_path": "aetbfmCcwg/tmp/d4865134385a3b5c56b58af4976259e4b63362ff1a537d759458cf80bebbe395.jpg", "table_caption": ["Table A7: Behaviour of estimators in debiased synthetic data in the simulation study. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "A.7.5 Influence curve based estimation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, two estimators are estimated in the original and synthetic datasets: a maximum likelihood estimation (MLE)-based one, as used in traditional statistical analysis, and an efficient influence curve (EIC)-based one, as proposed in this paper and obtained after 5-fold cross-fitting during estimation of the nuisance parameters. Note that sample splitting was not used during the debiasing step. For the former, SEs are calculated via the regular expressions which discard the uncertainty associated with data-adaptive prediction during estimation, while for the latter, SEs are based on the EIC which acknowledges this uncertainty. Throughout, all estimated (model- or EIC-based) SEs are corrected with $\\sqrt{1+m/n}$ to acknowledge the sampling variability of synthetic data. This correction factor was initially proposed by Raab et al. (2016) for parametric synthetic data generators, but was found to be insufficient for synthetic data created by DGMs (Decruyenaere et al., 2024). In Section 3.3, we give the formula for the variance of our EIC-based estimator on the debiased synthetic data, which generalises this correction factor to the setting where synthetic data were generated by DGMs. ", "page_idx": 26}, {"type": "text", "text": "The results of our simulation study using the EIC-based estimators, as shown below in Figures A8-A11 and Table A8, remain unchanged as compared to using the MLE-based estimators, since no data-adaptive predictions (e.g., machine learning) were used during estimation of the nuisance parameters. If data-adaptive estimation were to be used, we expect the MLE-based estimators to be overly optimistic, while the EIC-based estimators could handle the additional uncertainty introduced by data-adaptive estimation. ", "page_idx": 26}, {"type": "image", "img_path": "aetbfmCcwg/tmp/931eb09ccc298ad6601f2c0953770a69757680d4a20096914eb95dd59de292c9.jpg", "img_caption": ["Figure A8: Empirical coverage of the $95\\%$ confidence interval for the maximum likelihood estimation (MLE)-based and efficient influence curve (EIC)-based estimators. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "aetbfmCcwg/tmp/300bd4a14f114f7b92df3ae67fc2a51492bf5117dec1809b511505d86ecfc6af.jpg", "img_caption": ["Figure A9: The horizontal dashed line represents the population parameter and each dot is a maximum likelihood estimation (MLE)-based or efficient influence curve (EIC)-based estimate per Monte Carlo run (250 dots in total per value of $n$ ). The dashed funnel indicates the behaviour of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "aetbfmCcwg/tmp/379a90e4cc067a42d8756f1fd3599cc636c4004f3882f89fb36f61bc097e072e.jpg", "img_caption": ["Figure A10: The empirical standard error of the efficient influence curve (EIC)-based estimators is shown. Standard errors are estimated via the maximum likelihood estimation (MLE)-based or EIC-based expressions. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "aetbfmCcwg/tmp/fcf8ed5511031f07e34d0787880d84f73d5ffc536869087a321068e8c88dc607.jpg", "table_caption": ["Table A8: Estimated exponent $a$ $[95\\%$ CI] for the power law convergence rate $n^{-a}$ for empirical SE of the maximum likelihood estimation (MLE)-based and efficient influence curve (EIC)-based estimators. "], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "aetbfmCcwg/tmp/876fc0f0a78728a8eb6d103e656d85670deb6e82f3ec70d4051fe3fa3a85d3ef.jpg", "img_caption": ["Figure A11: Convergence rate of the empirical standard error (SE) for the maximum likelihood estimation (MLE)-based and efficient influence curve (EIC)-based estimators. If the SE is of the form $\\mathrm{SE}=c n^{-a}$ , where $c$ is a constant, then $\\log\\left(S E\\right)=\\log(c)+(-a)\\log\\left(n\\right)$ . Therefore slope $a$ represents the convergence rate and the vertical offset $\\log(c)$ indicate\u221as the log asymptotic variance. The dashed line indicates the behaviour of the SE of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data, whereas the dotted line indicates the assumed behaviour of the SE of the same estimator based on synthetic data, following the correction proposed by Raab et al. (2016). "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "A.8 Case studies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "A.8.1 International Stroke Trial ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We adapt the framework discussed in Section 4.1 to the International Stroke Trial (IST), one of the biggest randomised trials in acute stroke research (Sandercock et al., 2011). The dataset with 19285 complete cases now constitutes our population. We mimic different hypothetical settings where an institution only has access to a limited sample of observations, with the sample size $n$ varying between 50 and 5000. ", "page_idx": 29}, {"type": "text", "text": "In order to easily share the data with other researchers, the institution generates a synthetic dataset with sample size $m$ , where $m\\,=\\,n$ . Similarly to the simulation study, we repeated this process 100 times per sample size $n$ , to be able to calculate the empirical coverage levels. For illustration purposes, we focus on the effect of aspirin on the outcome at 6 months and report the proportion of deaths for the two treatment arms (aspirin and no aspirin), and its corresponding risk difference. ", "page_idx": 29}, {"type": "text", "text": "For each value of $n$ , two default synthetic datasets were generated using both CTGAN and TVAE. Given the interest in the proportion of death in the group with and without aspirin, we use the debiasing strategy with respect to the population mean. This implies that the default synthetic dataset was first split by treatment and then debiased with relation to the population mean within each treatment arm. The two debiased subdatasets were then afterwards combined into one debiased synthetic dataset for each generative model. For both the default and debiased synthetic dataset, the sampling variability of synthetic data is acknowledged by inflating the standard errors (SEs) by the correction factor $\\sqrt{1+m/n}$ . ", "page_idx": 29}, {"type": "text", "text": "The funnel plots for the proportion of deaths in both treatment arms and the risk difference are shown in Figure A12. We noticed that using the same hyperparameters as in the simulation study resulted in biased estimates, as can be seen in Figure A12. For this reason, we highlight the results obtained by training with the default hyperparameters suggested by the package SDV (Patki et al., 2016) instead. Analogously to the simulation study, our debiasing strategy\u221a decreases the variance of the mean estimator in both treatment arms, remedying the slower-than- $\\sqrt{n}$ -convergence observed in the default synthetic datasets. The impact for the applied researcher can be better understood by looking at the empirical coverage levels of the $95\\%$ CI for the true proportion of deaths in the aspirin arm, for all sample sizes and DGMs considered. Figure A13a illustrates that in contrast to the default synthetic datasets, the coverage levels based on the debiased synthetic datasets are all positioned around the nominal level. ", "page_idx": 29}, {"type": "text", "text": "One of the original research questions in Sandercock et al. (2011) was whether or not there is a difference in risk of death between the treatment arms. Figure A13b depicts the empirical type 1 error rate for the risk difference in death between aspirin and no aspirin group based on original data, default and debiased synthetic data. For the aforementioned reason, we focus on the results obtained by training with the default hyperparameters suggested by the package SDV (Patki et al., 2016). Should the researcher use the default synthetic data, they would very often falsely conclude that the risk is significantly different from the true difference of $-0.009$ , as calculated based on our population (the full dataset), while using the debiased synthetic dataset basically eliminates this high number of false-positives, as is the case in the original data as well. ", "page_idx": 29}, {"type": "table", "img_path": "aetbfmCcwg/tmp/515784c0be6b0f6e18b2135c8dbad441c1073eb1738810c514456b723076c0c6.jpg", "table_caption": ["Table A9: Estimated exponent $a$ $[95\\%$ CI] for the power law convergence rate $n^{-a}$ for empirical SE. Note that a convergence rate could not be estimated for the default synthetic data when hyperparameters suggested by the package Synthcity. This occurred because there was no variance in the estimates for sample sizes of 1600 and 5000. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "aetbfmCcwg/tmp/0e7cced87b238701271130c013fadeb881b52730923905e024a61414ad0094ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure A12: Estimates for the proportion of death in both treatment arms and their corresponding risk difference estimates. We show results for original data, and both default synthetic data (left) and debiased synthetic data (right) for all four generators. The horizontal dashed line represents the population proportion of death in each group and the corresponding risk difference, and each dot is an estimate per Monte Carlo run (100 dots in total per value of n). The dashed funnel indicates the behaviour of an unbiased and $\\sqrt{n}$ -consistent estimator based on observed data. ", "page_idx": 30}, {"type": "image", "img_path": "aetbfmCcwg/tmp/6f382007d6cefad389de0c7f42e81e6f47daeeccb5d29a851c560db2b1a520a2.jpg", "img_caption": ["Figure A13: Figure (a) shows the empirical coverage of the $95\\%$ CI for the true proportion of death in the aspirin treatment arm. In Figure (b), one can find the empirical type 1 error rate for the risk difference in death between aspirin and no aspirin group based on original data, default and targeted synthetic data. The null hypothesis states that the risk difference is equal to $-0.009$ , the risk difference as observed in the population (i.e. the original IST data). Tests were conducted at the $5\\%$ significance level, where the black horizontal line on the figure depicts this nominal level. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We state why our research is relevant (bias in synthetic data generated by deep generative models, leading to imprecision in statistical analysis and wrong conclusions), and propose a new strategy to debias this synthetic data. While this methodology is general, we clearly state that we apply it on two Deep Generative Models (CTGAN and TVAE), in a simulation study and two case studies to convince the reader of its merits. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We list limitations that are related to several facets of this paper. We refer to the assumption that is needed for the formal derivation of our strategy but frame this in the context of well-known analyses. We also discuss the low-dimensional setting of our simulation and case studies, for which DGMs might be less suited. Nevertheless, the positive results for two widely used estimators in this simple setting highlights the utility of a debiased approach and is simultaneously encouraging in terms of future larger-scale applications. In contrast to other work, we are aware that we do not suggest a tuning strategy of the DGM but instead rely on the debiasing of the generated synthetic data in a post-processing step. Nonetheless, we perceive this as a strength, since it renders our strategy generator-agnostic. However, the debiasing method of the regression coefficient still requires sampling of synthetic data conditional on a covariate, which is not a given in all types of DGM. Finally, when multiple parameters are of interest, the data generated by the DGM will need to be debiased to ensure that several such restrictions hold (simultaneously) w.r.t. several EICs $\\phi(.)$ , which is left for future research. In the Discussion section, we offer various suggestions and ideas for future research aimed at addressing the limitations. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Section 2 we clearly present our set up and its assumptions. Section 3 contains a detailed theoretical proof, including reasoning steps along the way, of our debiasing strategy, complemented by additional notes in the supplementary material (referred to as \u2018Appendix\u2019 in the manuscript). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Besides fully describing our debiasing methodology, we also provide ample details on the simulation study and case studies. Moreover, we have made our code for the simulation and case studies available online through our GitHub page, with the link included in the main text. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed ", "page_idx": 32}, {"type": "text", "text": "instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 33}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The simulation study is built on a fictitious yet realistic data generating process, which is provided in full in Appendix A.7.1. As DGMs, we use the default implementations provided in the code libraries Synthcity and SDV, which are open-source. The case studies are based on two public datasets: the International Stroke Trial dataset (Sandercock et al., 2011) and the Adult Census Income dataset (Becker and Kohavi, 1996). While we aimed to provide all details necessary to reproduce our experiments in the paper itself, we additionally make all code available in a public Github repository (the link is included in the main text). These code enables reproduction of all results for the simulation study and case studies. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Since the focus is not on building novel generative models, but rather on proposing a general methodology for debiasing the data generated by a generic generator, we simply use default hyperparameters for the DGMs as suggested by two code libraries (SynthCity and SDV). In our simulation study, our training data is obtained by sampling from the data generating process. We do not use any test splits, since the evaluation is done in terms of utility of statistical analyses, where we compare the obtained estimands with their ground truth values. Similar considerations are valid for the case studies. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: While we do include experimental results, we do not compare multiple generative models or methods. Rather, we present a novel methodology for debiasing synthetic data, and use various experiments to verify that it works as expected, by comparing the statistical utility of synthetic data with and without our debiasing strategy. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: All experiments were run on our institutional high performance computing cluster using a single GPU (NVIDIA Ampere A100; 80GB GPU memory) and single CPU (AMD EPYC 7413), taking less than 24 hours to complete (simulation study: less than 15 minutes per individual run across 5 sample sizes; International Stroke Trial case study: less than 75 minutes per individual run across 5 sample sizes; Adult census Income Dataset case study: less than 4 hours). This is stated as such in the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This research did not involve human subjects or participants. Furthermore, there are no data-related concerns: the simple data generating process that is proposed in the simulation study is fictitious, and apart from that we use public datasets. There are potential harmful consequences related to synthetic data, such as privacy disclosure risk and incorrect statistical inference from tabular synthetic datasets. In fact, our work addresses the second risk by debiasing synthetic datasets towards improving their inferential utility. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Positive societal impacts stem from the increased reliability of statistical analyses on synthetic data, which is made possible by our debiasing method. We do not foresee major negative societal impacts of our method. General concerns regarding tabular synthetic data do apply, though we believe our method remedies part of them (see our answer to Question 9). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not believe our paper poses such risks, since we propose a generic debiasing strategy for synthetic data, without releasing any new models or datasets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In the paper, we credit the creators of the deep generative models used in our work (i.e. CTGAN and TVAE), as well as the open source code libraries whose implementations we used (Synthcity and SDV). Furthermore, we cite the sources of the public datasets we used. The International Stroke Trial exists under an Open Database License, and the Adult Census Income dataset exists under a Creative Commons Attribution 4.0 International License. Both are free to use, but require attribution, which we provided. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]