[{"figure_path": "5qPmQtfvhy/figures/figures_4_1.jpg", "caption": "Figure 1: Estimates of algorithmic progress of models selected by cross validation. Figure 3a shows aggregated estimates over doubling times, and Figure 3b illustrates via swarm plots sorted from left to right in order of decreasing cross validation performance (increasing MSE test loss). Note that model 14 is omitted from Figure 3b-we elaborate on our reasoning in appendix J.2.", "description": "This figure presents the findings of cross-validation exercises to determine the best-fitting model for estimating algorithmic progress.  Figure 1a shows a density plot of the doubling times (in months) estimated by the preferred model and an aggregate of many models, revealing a median doubling time around 7-8 months. Figure 1b displays swarm plots illustrating model estimates of the rate of algorithmic progress across various model structures; models performing better in cross-validation are on the left, with doubling times decreasing as cross-validation performance improves. The omission of model 14 is discussed in the appendix.", "section": "Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_4_2.jpg", "caption": "Figure 2: Left: Comparison of estimated doubling times for effective compute from algorithmic progress, before and after set cutoff years from 2016\u20132020. Shorter doubling times in the \u201cpost\u201d period relative to \u201cpre\u201d indicate an acceleration in the rate of algorithmic progress after that cutoff year. Longer doubling times indicate a deceleration. Right: A stylized illustration of the relative contribution of compute scaling and algorithmic progress to effective compute. The physical compute contribution is estimated from the doubling times in Sevilla et al. [2022], and the algorithmic progress contribution is based on the aggregated doubling time estimate across model specifications (see section 3.1). We further plot the physical training compute values for several notable models (e.g., GPT-2) in their publication years.", "description": "The left panel shows the estimated doubling times for effective compute from algorithmic progress before and after different cutoff years (2016-2020).  Shorter doubling times after the cutoff year suggest an acceleration in algorithmic progress, while longer doubling times imply deceleration. The right panel visualizes the relative contributions of compute scaling and algorithmic progress to the overall growth of effective compute, using data from Sevilla et al. (2022) and the aggregated doubling time estimate from the paper.  The plot also includes the physical training compute values for some notable language models.", "section": "3 Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_7_1.jpg", "caption": "Figure 3: Estimates of algorithmic progress of models selected by cross validation. Figure 3a shows aggregated estimates over doubling times, and Figure 3b illustrates via swarm plots sorted from left to right in order of decreasing cross validation performance (increasing MSE test loss). Note that model 14 is omitted from Figure 3b-we elaborate on our reasoning in appendix J.2.", "description": "Figure 3 presents the analysis of doubling times in algorithmic progress for language models, based on model selection using cross-validation.  Subfigure (a) shows the aggregated estimates of doubling times, while subfigure (b) provides swarm plots showing the distribution of doubling time estimates across different model structures, sorted by their cross-validation performance (in increasing mean squared error). Note that Model 14 is excluded from subfigure (b), with explanations provided in Appendix J.2.", "section": "Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_12_1.jpg", "caption": "Figure 4: Relative compute (relative to baseline model) used to train models that achieve the same evaluated perplexity as Megatron-LM and ALiBi respectively. Doubling times of effective compute are 11.7 and 16.6 months using least squares regression for Megatron-LM (cross-entropy range 2.87-3.06) and ALiBi (cross-entropy range 1.18-1.34), respectively. Circles are proportional to the compute used during training.", "description": "This figure shows the relative compute needed to achieve the same perplexity as Megatron-LM and ALiBi models over time.  The left panel shows that to match Megatron-LM's performance, the compute needed has halved approximately every 11.7 months. The right panel shows a similar trend for ALiBi, with compute halving roughly every 16.6 months.  The size of the circles reflects the compute used during training.", "section": "3 Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_13_1.jpg", "caption": "Figure 5: Compute equivalent multiplier from optimal scaling from switching from Kaplan et al. [2020] to Chinchilla (Hoffmann et al. [2022]) scaling laws as a function of training compute for dense autoregressive transformer models. Note that GPT-3 and PaLM (540B) use around 1.7 and 1.44 tokens/parameter respectively, close to what the Kaplan scaling laws recommend, suggesting that Kaplan-scaling was close to what was practiced at the time.", "description": "This figure shows the compute-equivalent gain obtained from switching from Kaplan et al.'s scaling law to Chinchilla's scaling law. The x-axis represents the training compute, and the y-axis shows the compute-equivalent multiplier. It shows that the compute-equivalent gain increases as the training compute increases, and that GPT-3 and PaLM models already used parameter-to-token ratios that were close to the optimal values suggested by Kaplan et al.", "section": "C Core model parameter estimates"}, {"figure_path": "5qPmQtfvhy/figures/figures_14_1.jpg", "caption": "Figure 1: Estimates of algorithmic progress of models selected by cross validation. Figure 3a shows aggregated estimates over doubling times, and Figure 3b illustrates via swarm plots sorted from left to right in order of decreasing cross validation performance (increasing MSE test loss). Note that model 14 is omitted from Figure 3b-we elaborate on our reasoning in appendix J.2.", "description": "This figure presents the results of cross-validation analysis for different model structures used to estimate the rate of algorithmic progress in language models.  Panel (a) shows the distribution of doubling time estimates from the preferred model and aggregated estimates across all models. Panel (b) uses swarm plots to illustrate the model estimates for different model structures, ordered by cross-validation performance (from best to worst).  Note that model 14 is excluded from (b), with the reason explained in Appendix J.2.", "section": "Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_15_1.jpg", "caption": "Figure 1: Estimates of algorithmic progress of models selected by cross validation. Figure 3a shows aggregated estimates over doubling times, and Figure 3b illustrates via swarm plots sorted from left to right in order of decreasing cross validation performance (increasing MSE test loss). Note that model 14 is omitted from Figure 3b-we elaborate on our reasoning in appendix J.2.", "description": "This figure shows the results of cross-validation for different models used to estimate algorithmic progress.  Figure 1a presents a density plot of doubling times for effective compute, comparing the core estimates from the preferred model and the aggregation of all considered models. Figure 1b uses swarm plots to show the robustness of the doubling time estimates across various model specifications, ordered by cross-validation performance.  Figure 1c summarizes the different model structures and degrees of freedom. The core analysis focuses on the doubling time of compute and data efficiency improvements due to algorithmic progress in pre-training.", "section": "3 Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_16_1.jpg", "caption": "Figure 8: Log of perplexity of models used in our work, of over 231 language models analyzed in our work spanning over 8 orders of magnitude of compute, with each shape representing a model. The size of the shape is proportional to the compute used during training. Comparable perplexity evaluations are curated from the existing literature and from our own evaluations.", "description": "This figure shows the log perplexity of over 231 language models plotted against their publication date.  The size of each point corresponds to the amount of compute used in training that model. The models are categorized by whether they were evaluated on WikiText or Penn Treebank. The plot visually demonstrates the rapid decrease in perplexity (improvement in performance) and the massive increase in compute used over time.", "section": "E.1 Performance measure and dataset"}, {"figure_path": "5qPmQtfvhy/figures/figures_18_1.jpg", "caption": "Figure 9: Histogram showing the most common vocabulary sizes for models in our dataset, separated by benchmark.", "description": "This figure shows the distribution of vocabulary sizes across the three benchmarks used in the study: PTB, WT2, and WT103. The x-axis represents the vocabulary size, and the y-axis represents the number of models. The figure helps to visualize the concentration of models around certain vocabulary sizes within each benchmark.  It highlights the prevalence of certain vocabulary size ranges within each dataset used for the language modeling experiments.", "section": "E.2.3 Inconsistencies in perplexity evaluations"}, {"figure_path": "5qPmQtfvhy/figures/figures_30_1.jpg", "caption": "Figure 2: Left: Comparison of estimated doubling times for effective compute from algorithmic progress, before and after set cutoff years from 2016\u20132020. Shorter doubling times in the \"post\" period relative to \"pre\" indicate an acceleration in the rate of algorithmic progress after that cutoff year. Longer doubling times indicate a deceleration. Right: A stylized illustration of the relative contribution of compute scaling and algorithmic progress to effective compute. The physical compute contribution is estimated from the doubling times in Sevilla et al. [2022], and the algorithmic progress contribution is based on the aggregated doubling time estimate across model specifications (see section 3.1). We further plot the physical training compute values for several notable models (e.g., GPT-2) in their publication years.", "description": "The left panel shows the estimated doubling times for effective compute before and after different cutoff years. The shorter doubling times in the period after the cutoff year indicate an acceleration in the rate of algorithmic progress. Conversely, the longer doubling times suggest a deceleration. The right panel shows a stylized illustration of how compute scaling and algorithmic progress contribute to the effective compute.  The physical compute contribution is estimated from the doubling time found by Sevilla et al (2022). The contribution from algorithmic progress is based on the aggregated doubling times obtained across various model specifications, which are explained in section 3.1 of the paper. The figure also includes the physical training compute values for some notable models at the time of their publication.", "section": "3 Empirical results"}, {"figure_path": "5qPmQtfvhy/figures/figures_31_1.jpg", "caption": "Figure 1: Estimates of algorithmic progress of models selected by cross validation. Figure 3a shows aggregated estimates over doubling times, and Figure 3b illustrates via swarm plots sorted from left to right in order of decreasing cross validation performance (increasing MSE test loss). Note that model 14 is omitted from Figure 3b-we elaborate on our reasoning in appendix J.2.", "description": "This figure presents the results of the cross-validation analysis used to select the best statistical model for estimating the rate of algorithmic progress.  Panel (a) shows the distribution of doubling time estimates from the preferred model and from an aggregation across all models considered. Panel (b) displays a swarm plot showing the doubling time estimates for various model structures sorted from best to worst cross-validation performance.  Model 14 is excluded from this plot for reasons detailed in Appendix J.2. ", "section": "Empirical results"}]