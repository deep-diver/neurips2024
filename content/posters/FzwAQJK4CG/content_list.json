[{"type": "text", "text": "Learning Plaintext-Ciphertext Cryptographic Problems via ANF-based SAT Instance Representation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinhao Zheng, Yang Li, Cunxin Fan, Huaijin Wu, Xinhao Song, Junchi Yan\u2021   \nDept. of CSE & School of AI & Moe Key Lab of AI, Shanghai Jiao Tong University {void_zxh, yanglily, sjtu18017295729, whj1201, sxh001, yanjunchi}@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cryptographic problems, operating within binary variable spaces, can be routinely transformed into Boolean Satisfiability (SAT) problems regarding specific cryptographic conditions like plaintext-ciphertext matching. With the fast development of learning for discrete data, this SAT representation also facilitates the utilization of machine-learning approaches with the hope of automatically capturing patterns and strategies inherent in cryptographic structures in a data-driven manner. Existing neural SAT solvers consistently adopt conjunctive normal form (CNF) for instance representation, which in the cryptographic context can lead to scale explosion and a loss of high-level semantics. In particular, extensively used XOR operations in cryptographic problems can incur an exponential number of clauses. In this paper, we propose a graph structure based on Arithmetic Normal Form (ANF) to efficiently handle the XOR operation bottleneck. Additionally, we design an encoding method for AND operations in these ANF-based graphs, demonstrating improved efficiency over alternative general graph forms for SAT. We then propose CryptoANFNet, a graph learning approach that trains a classifier based on a message-passing scheme to predict plaintext-ciphertext satisfiability. Using ANF-based SAT instances, CryptoANFNet demonstrates superior scalability and can naturally capture higher-order operational information. Empirically, CryptoANFNet achieves a $50\\mathrm{x}$ speedup over heuristic solvers and outperforms SOTA learning-based SAT solver NeuroSAT, with $96\\%$ vs. $91\\%$ accuracy on small-scale and $72\\%$ vs. $55\\%$ on large-scale datasets from real encryption algorithms. We also introduce a key-solving algorithm that simplifies ANF-based SAT instances from plaintext and ciphertext, enhancing key decryption accuracy from $76.5\\%$ to $82\\%$ and from $72\\%$ to $75\\%$ for datasets generated from two real encryption algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine Learning (ML) has shown promise in solving discrete problems such as Mixed Integer Programming (MIP) [1] and Boolean Satisfiability (SAT) [2]. Meanwhile, ML has also made significant strides in the field of SAT-based cryptanalysis [3, 4]. Neural solvers for SAT [5, 6, 7] enjoy flexibility and automation in handling large datasets through data-driven learning, demonstrating advantages in prediction speed and adaptive learning. Plaintext-ciphertext cryptographic problems running in binary variable spaces can seamlessly be transformed into Boolean Satisfiability (SAT) problems, involving specific cryptographic conditions like plaintext-ciphertext matching. This SAT representation facilitates the utilization of machine learning methods to automatically learn inherent patterns and strategies in cryptographic structures in a data-driven manner. ", "page_idx": 0}, {"type": "text", "text": "However, the large scale and complex structure of cryptographic instances pose significant challenges for previous ML algorithms, which typically rely on conjunctive normal form (CNF) to encode cryptographic problems and are primarily effective with instances containing relatively small numbers of variables, usually in the range of tens. Cryptographic algorithms frequently entail constraints like XOR, modular addition, AND, and OR operations, thus making the logical XOR operator prevalent in SAT-based cryptanalysis and resulting in the proliferation of XOR clauses. However, XOR operations are generally challenging to represent in CNF. Converting a clause connected by XOR with $k$ literals into CNF is a well-known process that results in nearly $\\bar{2}^{k-1}$ OR clauses, each with $k$ literals. This could lead to the problem size ballooning considerably, thereby increasing computational complexity, and such encoding may compromise high-order operational information. ", "page_idx": 1}, {"type": "text", "text": "CNF is a conjunction (and-ing) of clauses, with each clause consisting of a disjunction (or-ing) of positive and negative variables called literals. The innate versatility of \"and\" and \"or\" constraint set of CNF allows for efficient application in SAT solving, yet it often requires introducing additional variables to represent more complex logical operations like XOR. Therefore, we propose to adopt an alternative constraint set, Algebraic Normal Form (ANF), to directly represent native XOR operations in the formulas. In ANF, formulas are expressed as collections of polynomials using addition (XOR) and multiplication (AND) operations on $\\mathbb{G F}(2)$ . Compared to CNFs, ANFs are more adept at capturing information about higher-order operations and are more concise in handling complex operations. This renders ANF a fitting choice for representing the structure and logic of cryptographic algorithms. However, representing ANF as graphs for efficient learning is challenging, and its application to existing SAT solvers is not as straightforward as CNF, resulting in obstacles to applying ANF in subsequent applications in complex scenarios like cryptographic problems. ", "page_idx": 1}, {"type": "text", "text": "To organize effective learning-based SAT practices in ANF, we propose an ANF-based graph structure to represent SAT instances for cryptographic applications. Building upon the graph representations, we propose CryptoANFNet, which employs a message-passing scheme and is trained as a classifier to predict satisfiability through dedicatedly constructed data encoded from cryptographic problems. The introduction of higher-order operations via ANF simplifies the resulting graph problems that would otherwise require hundreds or thousands of nodes using CNF to merely dozens of nodes. This simplification not only enhances the effectiveness of learning-based methods but also enables models to naturally capture higher-order operational information. Compared to learning-based SAT solvers, CryptoANFNet can handle larger instances with greater accuracy and achieves a 50x speedup over traditional heuristic solvers. Moreover, for the key-solving problem, we propose a key decryption algorithm that creates and simplifies ANF-based SAT instances from the plaintext and ciphertext, and uses the outputs from two derived SAT instances to deduce the correct key bit values. Given an input plaintext-ciphertext pair and an encryption algorithm, we proceed in three steps: 1) Use the encryption algorithm and the given plaintext-ciphertext to obtain the original ANF-based SAT instance; 2) For a specific bit in the key, create two extended SAT instances by assigning 0 or 1 to the chosen bit; 3) Pass these instances to CryptoANFNet in corresponding pairs, output paired scores, and determine the bit value based on which instance has a higher score. In this way, we can further improve the accuracy on datasets generated from two real encryption algorithms. The highlights of this paper include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Based on the Arithmetic Normal Form (ANF), we propose a graph structure to succinctly represent the excessive XOR operations in cryptographic problems. We then design two ways to encode the AND operations in the ANF-based graph to represent SAT instances derived from cryptographic problems. In Tab. 1, our ANF-graph is more efficient than the general graph form for SAT in [5]. \u2022 We propose (supervised) learning to solve (for the first time to our knowledge) the challenging cryptographic problem: plaintext-ciphertext satisfiability prediction, which could otherwise be intractable by traditional SAT methods. Our proposed GNN-based classifier CryptoANFNet with ANF-based SAT instances as input, achieves a $50\\mathrm{x}$ speedup over heuristic solvers, and outperforms the SOTA learning-based SAT solver NeuroSAT [5] by $96\\%$ vs. $91\\%$ and $72\\%$ vs. $55\\%$ accuracy on small and large scale datasets, generated from real encryption algorithms, respectively. \u2022 We extend to the key decryption problem. We propose a key-solving algorithm that derives ANF-based SAT instances as further simplified by our devised techniques, from the plaintext and ciphertext, and use the output of two derived SAT instances to infer the key values. It boosts accuracy by $76.5\\%.>82\\%$ and $72\\%{\\it-}>75\\%$ on datasets generated from two real encryption algorithms. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Learning for SAT. Learning has shown promise in automatically uncovering heuristics for solving combinatorial problems in a data-driven manner [8, 9, 10, 11, 12, 13]. There are primarily two routes dominating these efforts in SAT solving. Learning-aided solvers replace certain components in traditional solvers with learning-aided counterparts to enhance performance. The learning utilization encompasses branching heuristics [14, 15], variable initialization [16, 17, 18], glue clause prediction [19], etc. End-to-end neural solvers conceive SAT solving as a graph-based prediction task based on its CNF representation. They utilize neural networks to directly predict satisfiability or minimal unsatisfied cores via graph massage-passing networks [5, 6] or transformers [20]. These methods do not rely on handcrafted strategies and demonstrate the potential for magnitude speedup by circumventing exhaustive search procedures. However, neural networks may struggle with large-scale problems, posing a significant challenge, particularly in cryptographic applications where the XOR operation can lead to scale explosion in CNF-based literal-clause graph representations. Besides solving, there are also learning-based efforts that provide support for problem solving, like generating pseudo-industrial instances to resolve data bottlenecks [21, 22, 23, 11]. ", "page_idx": 2}, {"type": "text", "text": "Learning for cryptanalysis. The rise of deep learning has dramatically transformed the ability to analyze encrypted data in its raw state [24, 25], particularly for lightweight block ciphers, as noted in recent studies [26, 27, 28, 29]. Gohr [30] pioneered the application of learning-based cryptanalysis on round-reduced SPECK, demonstrating that deep differential-neural distinguishers could identify features that elude traditional strong distinguishers. This breakthrough highlighted the potential of neural networks in cryptanalysis. Subsequently, further insights into differentialneural distinguishers were provided by [31], showcasing that these networks could learn not only the differential distribution on the output pairs but also those in the penultimate and ante-penultimate rounds. Moreover, [32] developed a machine learning-based key-recovery attack that successfully carried out the first practical 13-round attacks on SPECK, with extensions to 14-rounds subsequently reported in [33]. However, despite the introduction of neural networks, these methods still fall within the framework of differential analysis and do not directly utilize machine learning to handle plaintext-ciphertext satisfiability prediction or key-solving problems. ", "page_idx": 2}, {"type": "text", "text": "SAT for cryptanalysis The cryptography community has significantly increased its use of automated tools for cryptanalysis, specifically in searching for linear and differential trails, leveraging methods such as the Boolean Satisfiability Problem (SAT)[34, 35, 4, 36] and Mixed Integer Linear Programming (MILP)[37]. In SAT-based cryptanalysis, significant strides have been made: [38] explored differential trails in ARX ciphers using SAT methods, while [39] developed an automated search technique for ciphers with Sboxes to achieve more accurate and probable differential trails. Additionally, [40] created an SAT-based automated search toolkit applied to SHA-3. These efforts essentially involve converting cryptographic algorithms into SAT instances and then conducting cryptanalysis by searching for differential paths, but they all need handcrafted strategies. ", "page_idx": 2}, {"type": "text", "text": "To date, no methods have focused on establishing effective graph representations and developing SAT solvers based on the XOR-friendly ANF representation. This paper pioneers the utilization of graph forms to represent the excessive XOR operations in cryptographic problems succinctly. As far as we know, we are the first to integrate graph neural networks and SAT for encryption algorithms. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "ANF formula. Algebraic Normal Form (ANF) is a polynomial representation of Boolean functions, instrumental in cryptographic applications due to its efficient handling of XOR and AND operations. Formally, a logical formula can be rewritten as a boolean function: $\\mathbb{G F}_{2}$ : multiplication in $\\mathbb{G F}_{2}(\\cdot)$ becomes the AND operation and addition in $\\mathbb{G F}_{2}(+)$ becomes the XOR operation. Then, the SAT instance constituted by the conjunction of logical formulas can be rewritten as several Boolean functions equal to 0, i.e., several Boolean equations. Therefore, a standard SAT instance in ANF form can be described by multiple Boolean equations, as illustrated by the example of an ANF-form SAT instance shown at the top of Fig. 1 (a). ", "page_idx": 2}, {"type": "text", "text": "Here, each variable is referred to as a vanilla literal. In a Boolean equation, the right side is always 0 and the Boolean function on the left side, formed by the XOR connection of monomials, is called a clause. Each monomial is either a constant term 1, a vanilla literal, or a product of variables. The ", "page_idx": 2}, {"type": "image", "img_path": "FzwAQJK4CG/tmp/11e6efa908582f9a02a146a0fbcba7f1088ea9f635bd23647a464428373a22a8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: (a) Example ANF formula graph from MQ problem; (b) The transformations to express the circular left shifts $(\\ll)$ , circular right shifts $(\\gg)$ , modular addition $(\\boxplus)$ , bitwise XOR $\\left(\\oplus\\right)$ , and bitwise AND (\u00b7) operations in ANF. ", "page_idx": 3}, {"type": "text", "text": "product of variables is called a literal of different orders based on the number of variables multiplied; for example, $x_{1}x_{2}$ is called a second-order literal. A literal appears in a clause if and only if that literal exists in the Boolean function represented by the clause. For simplicity, we will omit the multiplication operator (\u00b7) for the rest of the paper - as long as its use in monomials is implicit. ", "page_idx": 3}, {"type": "text", "text": "MQ problem. Encryption algorithms, like SPECK, typically use a set of simple and efficient operations, including circular left shifts $(\\lll$ , circular right shifts $(\\gg)$ , modular addition $(\\boxplus)$ , bitwise XOR $\\left(\\oplus\\right)$ , and bitwise AND (\u00b7) operations. The algebraic properties of these operations allow them to be converted into systems of boolean polynomial equations. In the examples shown in Fig. 1(b), circular shifts, XOR, and bitwise AND operations can be directly expressed in polynomial form, while modular addition can be represented through quadratic equations. These equations consist solely of variables, second-order products of variables, and the Boolean constant term, which means the encryption process can be transformed into a Multivariate Quadratic (MQ) problem. ", "page_idx": 3}, {"type": "text", "text": "In practice, for example, consider the toy formula for the 8-bit plaintext in one round shown below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X=\\overline{{x_{3}x_{2}x_{1}x_{0}}},\\qquad Y=\\overline{{y_{3}y_{2}y_{1}y_{0}}},\\qquad K=\\overline{{k_{3}k_{2}k_{1}k_{0}}}}\\\\ &{\\qquad\\qquad Z=\\overline{{z_{3}z_{2}z_{1}z_{0}}},\\qquad W=\\overline{{w_{3}w_{2}w_{1}w_{0}}}}\\\\ &{\\quad Z=\\left(\\left(X\\gg2\\right)\\boxplus Y\\right)\\oplus K,\\qquad W=\\left(Y\\ll1\\right)\\cdot X}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $K$ represents the key for the encryption algorithm, $X$ and $Y$ represent the high and low bits of the round function\u2019s input, respectively, and $Z$ and $W$ represent the high and low bits of the round function\u2019s output. It can be transformed into several standard multivariate quadratic equations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x_{2}+y_{0}+s_{0}=0,\\quad x_{2}y_{0}+c_{0}=0,\\quad x_{3}+y_{1}+c_{0}+s_{1}=0,\\quad x_{3}c_{0}+y_{1}c_{0}+x_{3}y_{1}+c_{1}=0}\\\\ {x_{0}+y_{2}+c_{1}+s_{2}=0,\\quad x_{0}c_{1}+y_{2}c_{1}+x_{0}y_{2}+c_{2}=0,\\quad x_{1}+y_{3}+c_{2}+s_{3}=0}\\\\ {z_{3}+s_{3}+k_{3}=0,\\quad z_{2}+s_{2}+k_{2}=0,\\quad z_{1}+s_{1}+k_{1}=0,\\quad z_{0}+s_{0}+k_{0}=0}\\\\ {w_{3}+x_{3}y_{2}=0,\\quad w_{2}+x_{2}y_{1}=0,\\quad w_{1}+x_{1}y_{0}=0,\\quad w_{0}+x_{0}y_{3}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{i},y_{i},z_{i},w_{i},s_{i}.$ , and $c_{i}$ are all Boolean variables and each equation has its right side equal to 0. Thus, by using these transformations, we can convert the encryption process into an MQ problem in ANF, and in the following section, we will only address solving the MQ problem. ", "page_idx": 3}, {"type": "text", "text": "4 Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Approach Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "ANF-based Graph Structures Given the ANF formula of an MQ problem, we propose a graphbased representation for ANF. First, we categorize clause nodes into positive and negative based on whether they contain a constant term. We then directly encode ANF formula into an undirected graph for representing variables and second-order literals: We encode the ANF as an undirected graph with one node for every independent literal and second-order literal, two complementary nodes for every clause with the same set of literals, an edge between every literal and every second-order literal it corresponds to, an edge between every second-order literal and every clause it appears in, and a different type of edge between each pair of complementary clauses. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "General Architecture Similar to NeuroSAT [5], we employ a message-passing neural network to derive clause and literal embeddings by executing a specified number of iterations. Then, we use these embeddings to predict a satisfiability vote through classification multilayer perceptrons(MLPs). Note that we have two key differences: 1) We only retain embeddings for vanilla literals and clauses. The second-order literals\u2019 embeddings are derived by concatenating first-order literals\u2019s embeddings. 2) We only use clause embeddings and apply two separate classification MLPs for positive and negative clause embeddings to predict the satisfiability vote, respectively. ", "page_idx": 4}, {"type": "text", "text": "Key-solving Algorithm With the model CryptoANFNet, we can evaluate the satisfiability of an individual ANF instance individual. Due to the uniqueness of solutions in key-solving problems, we propose an algorithm to determine the value of a specific bit in a key. Given a plaintext-ciphertext pair and an encryption algorithm, our approach derives an ANF-based SAT instance from the input data, generates two derived SAT instances for a specific bit by assigning 0 or 1, and then uses a scoring model to evaluate and determine the most likely value for the bit. ", "page_idx": 4}, {"type": "text", "text": "4.2 ANF formula Graph ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Similar to CNF, the literal-clause structure in an ANF formula has inherent permutation invariance, making it compatible with graph-based representations. However, unlike in CNF, there are no negated literals in ANF. Instead, due to the constant term, there are complementary relationships between clauses that contain the same literals but have different constant terms. Additionally, compared to CNF, the clauses in ANF formulas exhibit internal asymmetry, where high-order literals, created by combining vanilla literals with AND operations, form a special structure within the clause and require further decomposition. Note that in the MQ problem, the ANF formula has at most second-order literals. To represent second-order literals, we directly encode an ANF into an undirected graph $G$ , which can directly be derived from an ANF formula as follows: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Each vanilla literal $x_{i}$ and second-order literal $x_{i}x_{j}$ (including $x_{i}x_{i}$ ) in the ANF becomes a vertex in graph $G$ .   \n\u2022 Each clause becomes two vertices $\\overline{{c}}_{i}$ and $c_{i}$ ) in graph $G$ according to the set of literals it contains, where $c_{i}$ denotes clauses with constant term 0, called the positive clause, while $\\overline{{c}}_{i}$ denotes clauses with constant term 1, called the negative clause.   \n\u2022 An edge between second-order literal $x_{i}x_{j}$ and clause $c_{k}$ is in $g$ if and only if $x_{i}x_{j}$ appears in clause $c_{k}$ . Especially, an edge between second-order literal $x_{i}x_{i}$ and clause $c_{k}$ is in $G$ if and only if $x_{i}$ appears in clause $c_{k}$ .   \n\u2022 A different edge between each clause vertex $c_{i}$ and its complementary clause vertex $\\bar{c}_{i}$ .   \n\u2022 A special edge between vanilla literal $x_{i}$ (and $x_{j}$ ) and second-order literal $x_{i}x_{j}$ is in $G$ . ", "page_idx": 4}, {"type": "text", "text": "Fig. 1 (a) shows an example graph derived from an ANF formula. The graph is a tripartite graph with three columns of nodes from top to bottom: vanilla literals, second-order literals, and clauses. Nodes at each level are connected to related nodes at the next level, and clauses are additionally connected to their complementary clauses. Besides, we introduce another graph structure that transforms ANF into a bipartite graph by replacing second-order literals with independent variables. This facilitates the use of networks designed for the CNF formula but introduces more nodes. Please refer to Appendix B for more details. ", "page_idx": 4}, {"type": "text", "text": "4.3 Model Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the ANF formula graph, we propose the neural network model, CryptoANFNet, which first extracts the embedding of vanilla literals and clauses from the ANF graph and then predicts the satisfiability vote based on the embedding of clauses. CryptoANFNet uses a message-passing neural network to iteratively refine a vector space embedding for nodes in the graph. Here, different from NeuroSAT, at each time step $t$ , we only save the embedding of vanilla literals and pairs of complementary clauses. ", "page_idx": 4}, {"type": "text", "text": "In each iteration, the model aggregates information from neighboring nodes to update the embeddings. First, each clause receives information from associated literals and its complementary clause and updates its embedding accordingly. We obtain the second-order literal embedding by concatenating the vanilla literals. Each clause then receives messages from neighboring second-order literal nodes and its complementary clause based on the graph\u2019s connectivity. Next, each literal updates its embedding by receiving messages from associated clauses. Specifically, this information transfer occurs in two rounds: first from the clause to the intermediate second-order literal and then from the second-order literal to the vanilla literal. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Formally, at every time step $t$ , we have a matrix $L^{(t)}\\in\\mathbb{R}^{n\\times d}$ whose $i$ -th row contains the embedding for the vanilla literal $l_{i}$ and two matrices Cp(ots), Cn(etg) \u2208Rm\u00d7d whose j-th row contains the embedding for the positive clause $c_{j}$ and the relevant negative clause $\\overline{{c}}_{j}$ . We initialize these matrices by tiling $L^{(0)},C_{\\mathrm{pos}}^{(0)},C_{\\mathrm{neg}}^{(0)}\\in\\mathbb{R}^{d}$ , respectively. Then, a single iteration consists of the following two updates: ", "page_idx": 5}, {"type": "text", "text": "First, each clause receives messages from associated literals and its complementary clause and updates its embedding accordingly. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{L_{l2c}^{(t)}=\\!L_{l2l}([L^{(t)}[I],L^{(t)}[J]])\\!\\!\\!\\!\\!\\!}\\\\ &{}&{[C_{m,\\mathrm{pos}}^{(t)},C_{m,\\mathrm{neg}}^{(t)}]=\\!\\!\\!M_{l2c}^{T}L_{\\mathrm{msg}}(L_{l2c}^{(t)})\\!\\!\\!\\!\\!\\!}\\\\ &{}&{(C_{\\mathrm{pos}}^{(t+1)},C_{h,\\mathrm{pos}}^{(t+1)})\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $I=[0,1,\\cdots\\,,n-1,0,1,\\cdots\\,,n-2,n-1]\\in\\mathbb{R}^{n^{2}}$ and $J=[0,0,\\cdot\\cdot\\cdot\\,,0,1,1,\\cdot\\cdot\\cdot\\,,n-1,n-$ $1]\\in\\mathbb{R}^{n^{2}}$ represents the row and column index respectively. $M_{l2c}$ is the adjacency matrix defined by $\\bar{M}_{l2c}(i,j)={\\bf1}$ (the $i$ -th second-order literal appears in the $j$ -th clause). $L_{l2l},L_{\\mathrm{msg}}$ are two MLPs and $C_{u,\\mathrm{pos}},C_{u,\\mathrm{neg}}$ are two layer-norm LSTMs [41] with hidden states $C_{h,\\mathrm{pos}}^{(t)},C_{h,\\mathrm{neg}}^{(t)}\\in\\mathbb{R}^{m\\times d}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "Next, each literal updates its embedding by receiving messages from associated clauses. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{L}_{c2l}^{(t)}=M_{l2c}\\boldsymbol{C}_{\\mathrm{msg}}([\\boldsymbol{C}_{\\mathrm{pos}}^{(t)},\\boldsymbol{C}_{\\mathrm{neg}}^{(t)}])}\\\\ &{\\quad\\quad\\boldsymbol{L}_{m}^{(t)}=M_{l2l}\\boldsymbol{L}_{l2m}(\\boldsymbol{L}_{c2l}^{(t)})}\\\\ &{(\\boldsymbol{L}^{(t+1)},\\boldsymbol{L}_{h}^{(t+1)})\\gets\\boldsymbol{L}_{u}([\\boldsymbol{L}_{h}^{(t)},\\boldsymbol{L}_{m}^{(t)}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M_{l2l}$ is the the adjacency matrix defined by $M_{l2l}(i,j)=\\mathbf{1}$ (the $i$ -th vanilla literal appears in the $j$ -th second-order literal). $L_{l2m},C_{\\mathrm{msg}}$ are two MLPs and $L_{u}$ is a layer-norm LSTMs with hidden states $L_{h}^{(t)}\\in\\mathbb{R}^{n\\times d}$ . ", "page_idx": 5}, {"type": "text", "text": "After $T$ iterations for updating clause and literal embeddings, we use these embeddings to predict the satisfiability of the ANF formula. This prediction process involves a feedforward neural network that combines the embeddings of clauses to output a satisfiability vote representing whether the formula is satisfiable or not. The feedforward neural network consists of two MLPs, which are designed for the positive and negative clauses respectively, and they compute each clause\u2019s satisfiability score for the SAT instance based on its embedding. Then we sum up the votes of all the clauses and output the final satisfiability vote. This architecture is designed to be flexible and scalable, accommodating various ANF graph sizes and complexities. ", "page_idx": 5}, {"type": "text", "text": "Formally, the feedforward neural network outputs the satisfiability vote based on the clauses\u2019s embedding $(C_{\\mathrm{pos}},C_{\\mathrm{neg}}\\in\\mathbb{R}^{m\\times d})$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{*}\\gets C_{\\mathrm{vote}}^{1}(C_{\\mathrm{pos}}),\\ \\ \\overline{{V}}_{*}\\gets C_{\\mathrm{vote}}^{2}(C_{\\mathrm{neg}})}\\\\ {s\\gets\\sigma(\\mathrm{sum}(V_{*}+\\overline{{V}}_{*}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $V_{*},\\overline{{V}}_{*}\\in\\mathbb{R}^{m\\times1}$ represent the satisfiability score of positive and negative clauses, respectively. $C_{\\mathrm{vote}}^{1},C_{\\mathrm{vote}}^{2}$ denote two MLPs. $\\operatorname{sum}(V_{*}+\\overline{{V}}_{*}))\\in\\mathbb{R}$ represent the sum of the clause votes, and $\\sigma(\\cdot)$ represent the sigmoid funtion. $s\\in[0,1]$ represents the output prediction. We train the network to minimize the binary cross-entropy loss between the output prediction $s$ and the true label $y$ . ", "page_idx": 5}, {"type": "text", "text": "4.4 Key-solving Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While we train CryptoANFNet as a classifier to predict satisfiability, we can also use CryptoANFNet for key solving. Given a plaintext-ciphertext pair and an encryption algorithm, we can transform ", "page_idx": 5}, {"type": "image", "img_path": "FzwAQJK4CG/tmp/c4f105f516653bf19062030d6da351c3784534e77b9f395d1e93a4df28b8bf48.jpg", "img_caption": ["plaintext-ciphertext pair "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: The pipeline of the key-solving algorithm. Given a plaintext-ciphertext pair and an encryption algorithm, we first transform them into an ANF-based instance of an MQ problem. Then, for a specific key bit $K_{i}$ (the $i$ -th bit of key), we guess its value as either 0 or 1 and generate two derived SAT instances. We then employ CryptoANFNet to predict the satisfiability of each instance. The final determination of $K_{i}$ is based on which instance receives a higher satisfiability score. ", "page_idx": 6}, {"type": "text", "text": "them into an ANF-based instance of an MQ problem. Since key solutions are typically unique, a natural idea is to assign values to each bit of the key one by one, derive extended instances from the original ANF instance, and then use CryptoANFNet\u2019s prediction results to determine each bit\u2019s value. For example, consider a 4-bit key $K=\\overline{{k_{3}k_{2}k_{1}k_{0}}}$ . We could guess $K=\\overline{{1k_{2}k_{1}k_{0}}}$ to get a derived ANF instance and then let CryptoANFNet predict whether the derived ANF instance is satisfiable. If it is, we conclude $k_{3}=1$ otherwise, $k_{3}=0$ . ", "page_idx": 6}, {"type": "text", "text": "In essence, it is not necessary to obtain exact classification probabilities for individual samples. Instead, we only need to compare the satisfiability votes for samples generated by making two different guesses for the same bit. Based on this idea, we propose the key-solving algorithm to determine the value of a specific bit in the key. The algorithm consists of three steps. First, we obtain the original ANF-based SAT instance from the encryption algorithm and the plaintext-ciphertext pair. Second, for a specific key bit, we create two derived SAT instances by assigning either 0 or 1 to the selected bit. Third, we use CryptoANFNet to predict the satisfiability votes of these two instances and determine the bit\u2019s value based on which instance scores higher. Fig. 2 provides a high-level illustration of the algorithm. ", "page_idx": 6}, {"type": "text", "text": "Besides, in this algorithm, we use the derived SAT instances in pairs to train the network, CryptoANFNet, by minimizing the loss function, as follows. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\cal L}_{\\mathrm{cls}}=\\displaystyle\\sum_{i}^{2N}{\\bf B C E}(\\sigma(s_{i}),y_{i}),\\ \\ {\\cal L}_{\\mathrm{comp}}=\\displaystyle\\sum_{i}^{N}{\\bf C E}(\\mathrm{softmax}([s_{2i},s_{2i+1}]),[y_{2i},y_{2i+1}])}\\\\ {{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {\\cal L}_{\\mathrm{final}}={\\cal L}_{\\mathrm{cls}}+\\lambda\\times{\\cal L}_{\\mathrm{comp}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $s_{i}\\in\\mathbb{R}$ denotes the satisfiability vote of the $i$ -th instance, $y_{i}\\in\\{0,1\\}$ denotes the corresponding label. $B C E(\\cdot),C E(\\cdot),\\sigma(\\cdot)$ and softmax $(\\cdot)$ represent the binary cross-entropy loss, the cross-entropy loss, the sigmoid function, and the softmax function. Especially, $s_{2i}$ and $s_{2i+1}$ are two corresponding votes for derived SAT instances, generated by assigning either 0 or 1 to the selected bit of the key in the same original ANF instance. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For both training and test datasets, we evaluate our approach on two types of synthetic datasets. ", "page_idx": 6}, {"type": "text", "text": "For the first synthetic dataset, we use a similar approach to NeuroSAT to generate instances of the MQ problem. These instances consist of the dataset called SR(n), where $n$ is the number of variables in the instance. The distribution SR(n) contains pairs of $n$ variables of stochastic SAT instances in the ANF formula. The pair of instances have the following properties: one of them is satisfiable and the other is unsatisfiable, and the difference between the two instances is only in the constant term in one of the clauses. To generate a clause $c_{i}$ of an SAT instance in ANF with $n$ variables, we randomly sample a number $k$ $3\\leq k\\leq2n)$ as the clause\u2019s length. Then, from all $n^{2}$ second-order polynomials, we sample $k$ to include as literals in the clause. Polynomials of the form $x_{i}^{2}$ are regarded as producing vanilla literal $x_{i}$ , while those of the form $x_{i}x_{j}(i\\neq j)$ are considered as producing second-order literal $x_{i}x_{j}$ . Additionally, there\u2019s a $50\\%$ probability of adding a constant term of 1 to the clause. Based on this, in the approach, we continue generating clauses adding them to the SAT instance, and then using a traditional ANF-accessible solver, WDSat [42], to check the satisfiability until adding clause $c_{m}$ finally made the instance unsatisfiable. ", "page_idx": 6}, {"type": "table", "img_path": "FzwAQJK4CG/tmp/e0844391341395534f025aa17d3a51265550edbe6de17c71ed37d791de2c2837.jpg", "table_caption": ["Table 1: Parameters of SAT problems in CNF and ANF "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "FzwAQJK4CG/tmp/23279b7a09b8c2d4407c429062a4f24acabb78251df9e6d1aed4c402db3f6522.jpg", "table_caption": ["Table 2: Performance of different learning-based solvers on synthetic datasets "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For the second synthetic dataset, we utilize instances generated from the real encryption algorithms to construct the dataset. Specifically, we use a lightweight and popular block cipher (for certain reasons, we do not disclose the specific name of the encryption algorithm, referred to as Scipher; for its detailed encryption process, please refer to Appendix C) and the SPECK algorithm. Given the length $k$ of the seed key, the length $n$ of the plaintext, and the number of encryption rounds $r$ , we randomly generate the seed key and plaintext to generate an SAT instance in ANF corresponding to the encryption algorithm. Then, in each round, we use the seed key to encrypt the plaintext and obtain the final ciphertext. Based on the generated plaintext-ciphertext pairs and the encryption process, we obtain the original ANF instance. Then, we select a random bit of the seed key and modify it with 0 and 1 to obtain the corresponding satisfiable and unsatisfiable instances, thus forming the final dataset. Depending on the differences in the encryption algorithms used, the datasets generated by this process are called Scipher-r-k-n and Speck-r-k-n. ", "page_idx": 7}, {"type": "text", "text": "5.2 Satisfiability Prediction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Complexity of SAT instances. To compare the solving efficiency of CNF solvers and ANF solvers, we first conduct complexity comparison experiments on SAT instances. By representing the same data set using CNF and ANF, we compare the number of variables, the number of clauses, and the number of parameterized nodes in the constructed graphs in the learning-based solvers. Table 1 lists the parameters of SAT instances for each dataset under CNF and ANF. Our ANF graph is more efficient than the CNF graph, which is a general graph previously used to represent SAT problems. In datasets containing XOR operations, CNF typically requires transforming logical expressions into conjunctive normal form, which may increase the number of literals and clauses; whereas ANF directly represents logical functions using polynomials, thus potentially having fewer literals. ", "page_idx": 7}, {"type": "text", "text": "Evaluation on different datasets. To evaluate the performance of our proposed CryptoANFNet, we conduct a series of experiments comparing it against the state-of-the-art model, NeuroSAT [5]. The primary focus of these experiments is solving SAT instances derived from the real encryption algorithms. For each dataset, we use Bosphorus [43] to convert (without simplification) the ANFbased SAT instances in our dataset into CNF-based SAT instances, resulting in a CNF form of the same dataset. We then train CryptoANFNet on the CNF-form dataset and compared its results with those of NeuroSAT, which was trained on the ANF-form dataset. Table 2 lists the results of ", "page_idx": 7}, {"type": "table", "img_path": "FzwAQJK4CG/tmp/943646762a6a36431e6e16edb4179a23ddbec997e1a92791ce539fe9a1451d1c.jpg", "table_caption": ["Table 3: Performance for key-solving algorithm in solving MQ problems on synthetic datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "FzwAQJK4CG/tmp/a552e7aa6553b73e1efd3edf2f68fc6c325ef43caa7a6d49dc4cfbb916e1ff57.jpg", "table_caption": ["Table 4: Comparing the efficiency of different solvers for solving the MQ problem on synthetic datasets. (Average runtime: (SAT, UNSAT) ms/instance) "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "NeuroSAT and CryptoANFNet on synthetic datasets of different scales. On small-scale datasets, both models achieve high accuracy, but CryptoANFNet still outperforms NeuroSAT, such as $96\\%$ vs. $91\\%$ accuracy on the SR(5) dataset. On large-scale datasets, like SR(25), NeuroSAT can hardly predict the satisfiability of instances, with its prediction accuracy hovering around $50\\%$ and CryptoANFNet significantly outperforms NeuroSAT, such as $72\\%$ vs. $57\\%$ accuracy on the SR(25) dataset and $72\\%$ vs. $55\\%$ accuracy on the Speck-3-8-16 dataset. ", "page_idx": 8}, {"type": "text", "text": "5.3 Key Bit Prediction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition to evaluating the satisfiability of individual ANF instances with the model CryptoANFNet, we conduct a comparative experiment to validate the key-solving algorithm. We assess the effectiveness of the key-solving algorithm in determining specific bit values in cryptographic keys by testing on datasets generated from real encryption algorithms, such as Scipher-r-k-n and Speck-r-k-n. For each dataset, we use the associated SAT-UNSAT instance pairs generated by the key-solving algorithm to train CryptoANFNet. In this experiment, we set $\\lambda=0.1$ . Table 3 lists the comparison of results before and after using the key-solving algorithm. As shown, the key-solving algorithm somewhat improves performance on the tested datasets. Notably, it boosts the accuracy from $76.5\\%$ to $82\\%$ and from $72\\%$ to $75\\%$ on the Scipher-3-8-16 and Speck-3-8-16 datasets, respectively. ", "page_idx": 8}, {"type": "text", "text": "5.4 Comparsion with Heuristic SAT solvers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate the efficiency of our proposed model, CryptoANFNet, in comparison with traditional heuristic SAT solvers. These experiments focused on assessing the efficiency of solving SAT instances derived from real encryption algorithms. We compare CryptoANFNet to the following solvers: the best currently available implementation of CryptoMiniSat [44] (a CNF-based solver specifically designed for handling complex problems in cryptanalysis), Kissat [45] (a highly efficient CNF-based solver that has demonstrated its ability to solve challenging SAT instances in the SAT competition), and WDSat [42] (an ANF-based solver for instances in algebraic normal form). ", "page_idx": 8}, {"type": "text", "text": "To fairly compare the efficiency of solvers, we test the unit solving time (ms/instance) for UNSAT and SAT instances for each dataset. For the ANF-based solver, we directly test the instances in ANF form. For CNF-based solvers, we use Bosphorus [43] to convert the ANF instances (without simplification) to CNF form for testing. We test all solvers on an AMD Ryzen Threadripper 3970X 32-Core Processor and an NVIDIA GeForce RTX 3090 GPU. Table 4 lists the results of different solvers and more results are shown in Appendix A. We find that the learning-based model CryptoANFNet and NeuroSAT solve SAT instances derived from cryptographic problems much more quickly and CryptoANFNet achieves a $50\\mathrm{x}$ speedup on average over traditional heuristic solvers. These results make it possible to apply learning-based solvers in practice. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Outlook ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a graph structure based on Arithmetic Normal Form (ANF) to efficiently handle XOR operations in cryptographic problems. We also design an encoding method for AND operations within these ANF-based graphs, demonstrating improved efficiency over traditional graph forms. Building on this, we introduce CryptoANFNet, a graph learning framework for SATbased cryptanalysis utilizing ANF. CryptoANFNet addresses the challenging problem of plaintextciphertext satisfiability prediction and achieves a remarkable $50\\mathrm{x}$ speedup over heuristic solvers. It also outperforms the state-of-the-art learning-based SAT solver, NeuroSAT. Furthermore, we introduce a key-solving algorithm that simplifies ANF-based SAT instances derived from plaintext and ciphertext, enhancing key decryption accuracy. In this way, future research could explore further optimization of the ANF-based graph structures and the integration of more advanced neural network models to push the boundaries of cryptographic problem-solving. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Zhang, C. Liu, X. Li, H.-L. Zhen, M. Yuan, Y. Li, and J. Yan, \u201cA survey for solving mixed integer programming via machine learning,\u201d Neurocomputing, vol. 519, pp. 205\u2013217, 2023.   \n[2] W. Guo, H.-L. Zhen, X. Li, W. Luo, M. Yuan, Y. Jin, and J. Yan, \u201cMachine learning methods in solving the boolean satisfiability problem,\u201d Machine Intelligence Research, 2023.   \n[3] L. Sun, D. Gerault, A. Benamira, and T. Peyrin, \u201cNeurogift: Using a machine learning based sat solver for cryptanalysis,\u201d in Cyber Security Cryptography and Machine Learning, S. Dolev, V. Kolesnikov, S. Lodha, and G. Weiss, Eds. Cham: Springer International Publishing, 2020, pp. 62\u201384.   \n[4] S. Nejati and V. Ganesh, \u201cCdcl (crypto) sat solvers for cryptanalysis,\u201d arXiv preprint arXiv:2005.13415, 2020.   \n[5] D. Selsam, M. Lamm, B. B\u00fcnz, P. Liang, L. de Moura, and D. L. Dill, \u201cLearning a sat solver from single-bit supervision,\u201d in International Conference on Learning Representations, 2019.   \n[6] Z. Li, J. Guo, and X. Si, \u201cG4satbench: Benchmarking and advancing sat solving with graph neural networks,\u201d arXiv preprint arXiv:2309.16941, 2023.   \n[7] C. Cameron, R. Chen, J. Hartford, and K. Leyton-Brown, \u201cPredicting propositional satisfiability via end-to-end learning,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, 2020, pp. 3324\u20133331.   \n[8] Y. Bengio, A. Lodi, and A. Prouvost, \u201cMachine learning for combinatorial optimization: a methodological tour d\u2019horizon,\u201d European Journal of Operational Research, vol. 290, no. 2, pp. 405\u2013421, 2021.   \n[9] Y. Li, J. Guo, R. Wang, and J. Yan, \u201cT2t: From distribution learning in training to gradient search in testing for combinatorial optimization,\u201d in Advances in Neural Information Processing Systems, 2023.   \n[10] X. Li, F. Zhu, H.-L. Zhen, W. Luo, M. Lu, Y. Huang, Z. Fan, Z. Zhou, Y. Kuang, Z. Wang et al., \u201cMachine learning insides optverse ai solver: Design principles and applications,\u201d arXiv preprint arXiv:2401.05960, 2024.   \n[11] Z. Guo, Y. Li, C. Liu, W. Ouyang, and J. Yan, \u201cAcm-milp: Adaptive constraint modification via grouping and selection for hardness-preserving milp instance generation,\u201d in The Forty-first International Conference on Machine Learning, 2024.   \n[12] H. Geng, H. Ruan, R. Wang, Y. Li, Y. Wang, L. Chen, and J. Yan, \u201cThere is no silver bullet: Benchmarking methods in predictive combinatorial optimization,\u201d Advances in Neural Information Processing Systems, 2024.   \n[13] Y. Li, J. Guo, R. Wang, H. Zha, and J. Yan, \u201cFast t2t: Optimization consistency speeds up diffusion-based training-to-testing solving for combinatorial optimization,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2024.   \n[14] D. Selsam and N. Bj\u00f8rner, \u201cGuiding high-performance sat solvers with unsat-core predictions,\u201d in International Conference on Theory and Applications of Satisfiability Testing, 2019.   \n[15] V. Kurin, S. Godil, S. Whiteson, and B. Catanzaro, \u201cCan q-learning with graph networks learn a generalizable branching heuristic for a sat solver?\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 9608\u20139621, 2020.   \n[16] H. Wu, \u201cImproving sat-solving with machine learning,\u201d in Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education, 2017, pp. 787\u2013788.   \n[17] H. Duan, S. Nejati, G. Trimponias, P. Poupart, and V. Ganesh, \u201cOnline bayesian moment matching based sat solver heuristics,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 2710\u20132719.   \n[18] Z. Li and X. Si, \u201cNsnet: A general neural probabilistic framework for satisfiability problems,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 25 573\u201325 585, 2022.   \n[19] J. M. Han, \u201cEnhancing sat solvers with glue variable predictions,\u201d arXiv preprint arXiv:2007.02559, 2020.   \n[20] Z. Shi, M. Li, Y. Liu, S. Khan, J. Huang, H.-L. Zhen, M. Yuan, and Q. Xu, \u201cSatformer: Transformer-based unsat core learning,\u201d in 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1\u20134.   \n[21] J. You, H. Wu, C. Barrett, R. Ramanujan, and J. Leskovec, \u201cG2sat: Learning to generate sat formulas,\u201d in Advances in neural information processing systems, vol. 32, 2019.   \n[22] Y. Li, X. Chen, W. Guo, X. Li, W. Luo, J. Huang, H.-L. Zhen, M. Yuan, and J. Yan, \u201cHardsatgen: Understanding the difficulty of hard sat formula generation and a strong structure-hardnessaware baseline,\u201d in ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2023.   \n[23] X. Chen, Y. Li, R. Wang, and J. Yan, \u201cMixsatgen: Learning graph mixing for sat instance generation,\u201d in The Twelfth International Conference on Learning Representations, 2024.   \n[24] E. Wenger, M. Chen, F. Charton, and K. E. Lauter, \u201cSalsa: Attacking lattice cryptography with transformers,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 34 981\u201334 994, 2022.   \n[25] C. Li, E. Wenger, Z. Allen-Zhu, F. Charton, and K. E. Lauter, \u201cSalsa verde: a machine learning attack on lwe with sparse small secrets,\u201d Advances in Neural Information Processing Systems, vol. 36, pp. 53 343\u201353 361, 2023.   \n[26] A. Singh, K. B. Sivangi, and A. N. Tentu, \u201cMachine learning and cryptanalysis: An in-depth exploration of current practices and future potential,\u201d Journal of Computing Theories and Applications, vol. 2, no. 1, pp. 27\u201342, 2024.   \n[27] J. So, \u201cDeep learning-based cryptanalysis of lightweight block ciphers,\u201d Security and Communication Networks, vol. 2020, pp. 1\u201311, 2020.   \n[28] A. Baksi and A. Baksi, \u201cMachine learning-assisted differential distinguishers for lightweight ciphers,\u201d Classical and Physical Security of Symmetric Key Cryptographic Algorithms, pp. 141\u2013162, 2022.   \n[29] Z. Hou, J. Ren, and S. Chen, \u201cCryptanalysis of round-reduced simon32 based on deep learning,\u201d Cryptology ePrint Archive, 2021.   \n[30] A. Gohr, \u201cImproving attacks on round-reduced speck32/64 using deep learning,\u201d in Advances in Cryptology\u2013CRYPTO 2019: 39th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18\u201322, 2019, Proceedings, Part II 39. Springer, 2019, pp. 150\u2013179.   \n[31] A. Benamira, D. Gerault, T. Peyrin, and Q. Q. Tan, \u201cA deeper look at machine learning-based cryptanalysis,\u201d in Advances in Cryptology\u2013EUROCRYPT 2021: 40th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, October 17\u201321, 2021, Proceedings, Part I 40. Springer, 2021, pp. 805\u2013835.   \n[32] Z. Bao, J. Guo, M. Liu, L. Ma, and Y. Tu, \u201cEnhancing differential-neural cryptanalysis,\u201d in International Conference on the Theory and Application of Cryptology and Information Security. Springer, 2022, pp. 318\u2013347.   \n[33] Z. Bao, J. Lu, Y. Yao, and L. Zhang, \u201cMore insight on deep learning-aided cryptanalysis,\u201d in International Conference on the Theory and Application of Cryptology and Information Security. Springer, 2023, pp. 436\u2013467.   \n[34] F. Lafitte, \u201cCryptosat: a tool for sat-based cryptanalysis,\u201d IET Information Security, vol. 12, no. 6, pp. 463\u2013474, 2018.   \n[35] M. Soos, \u201cThe cryptominisat 5 set of solvers at sat competition 2016,\u201d Proceedings of SAT Competition, p. 28, 2016.   \n[36] J. Lu, Y. Liu, T. Ashur, B. Sun, and C. Li, \u201cImproved rotational-xor cryptanalysis of simon-like block ciphers,\u201d IET Information Security, vol. 16, no. 4, pp. 282\u2013300, 2022.   \n[37] N. Mouha, Q. Wang, D. Gu, and B. Preneel, \u201cDifferential and linear cryptanalysis using mixed-integer linear programming,\u201d in Information Security and Cryptology: 7th International Conference, Inscrypt 2011, Beijing, China, November 30\u2013December 3, 2011. Revised Selected Papers 7. Springer, 2012, pp. 57\u201376.   \n[38] N. Mouha and B. Preneel, \u201cTowards finding optimal differential characteristics for arx: Application to salsa20,\u201d Cryptology ePrint Archive, 2013.   \n[39] L. Sun, W. Wang, and M. Wang, \u201cMore accurate differential properties of led64 and midori64,\u201d IACR Transactions on Symmetric Cryptology, pp. 93\u2013123, 2018.   \n[40] J. Guo, G. Liu, L. Song, and Y. Tu, \u201cExploring sat for cryptanalysis:(quantum) collision attacks against 6-round sha-3,\u201d in International Conference on the Theory and Application of Cryptology and Information Security. Springer, 2022, pp. 645\u2013674.   \n[41] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio, \u201cNeural combinatorial optimization with reinforcement learning,\u201d 2017.   \n[42] M. Trimoska, S. Ionica, and G. Dequen, Parity (XOR) Reasoning for the Index Calculus Attack. Springer International Publishing, 2020, p. 774\u2013790. [Online]. Available: http://dx.doi.org/10.1007/978-3-030-58475-7_45   \n[43] D. Choo, M. Soos, K. M. A. Chai, and K. S. Meel, \u201cBosphorus: Bridging anf and cnf solvers,\u201d in Proceedings of Design, Automation, and Test in Europe(DATE), 3 2019.   \n[44] M. Soos, K. Nohl, and C. Castelluccia, \u201cExtending sat solvers to cryptographic problems,\u201d in International Conference on Theory and Applications of Satisfiability Testing. Springer, 2009, pp. 244\u2013257.   \n[45] A. Biere and M. Fleury, \u201cGimsatul, IsaSAT and Kissat entering the SAT Competition 2022,\u201d in Proc. of SAT Competition 2022 \u2013 Solver and Benchmark Descriptions, ser. Department of Computer Science Series of Publications B, T. Balyo, M. Heule, M. Iser, M. J\u00e4rvisalo, and M. Suda, Eds., vol. B-2022-1. University of Helsinki, 2022, pp. 10\u201311.   \n[46] B. Selman, H. A. Kautz, B. Cohen et al., \u201cNoise strategies for improving local search,\u201d in AAAI, vol. 94, 1994, pp. 337\u2013343.   \n[47] J. Elffers and J. Nordstr\u00f6m, \u201cDivide and conquer: Towards faster pseudo-boolean solving.\u201d in IJCAI, vol. 18, 2018, pp. 1291\u20131299.   \n[48] A. Kyrillidis, A. Shrivastava, M. Vardi, and Z. Zhang, \u201cFouriersat: A fourier expansion-based algebraic framework for solving hybrid boolean constraints,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 02, 2020, pp. 1552\u20131560. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "FzwAQJK4CG/tmp/2d6f608176449afcd657172c8e5418e2876de36ed0c616b616583876274e7092.jpg", "img_caption": ["Figure 3: (a) A example ANF formula for changing the original ANF formula to a formula without AND operation; (b) Example ANF formula graph focusing on second-order literals "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Efficiency of incomplete solvers ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 5: Comparing the efficiency of incomplete solvers for solving the MQ problem on synthetic datasets. (Average runtime: (SAT, UNSAT) ms/instance) ", "page_idx": 12}, {"type": "table", "img_path": "FzwAQJK4CG/tmp/b5dfef05d582995801bbe906e10e68359fc56b302dde33f9fc10ec4ef1b3270f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B ANF Graph Structure focusing on second-order literals ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This ANF graph structure retains only second-order literals and regards them as independent vanilla literals, representing original literals through these new literals and adding additional constraint equations. In this case, the graph has a node for every literal and every clause, an edge between every literal and each clause in which it appears, a different edge for each pair of complementary clauses, and a special edge between corresponding sets of literals. ", "page_idx": 12}, {"type": "text", "text": "For this type of graph, we focus on the unified representation of the vanilla literal and second-order literal. Note that we can represent all the vanilla literals with the second-order literals and their negations. For example, the vanilla literal $x_{1}$ can be represent as $x_{1}=x_{1}x_{2}+x_{1}{\\overline{{x_{2}}}}$ . Based on this idea, we introduce the negations of a second-order literal (e.g. $\\overline{{x_{1}}}x_{2},x_{1}\\overline{{x_{2}}}$ of $x_{1}x_{2}$ ) as the new vanilla literals and we update the original ANF formula, where every three corresponding literals consist of a literal subset. Then, the ANF instance of an MQ problem with $n$ variables is represented as the ANF instance of a linear problem with $3n(n-1)$ variables. ", "page_idx": 12}, {"type": "text", "text": "Since the newly introduced variables are independent of each other except for the atomic clauses, to ensure that the original problem doesn\u2019t become too relaxed due to the increase in variables, we add the following two types of constraints: 1) The new vanilla literal combinations must represent the same literal in the original ANF instance; 2) At most one of the three new vanilla literals derived from the same second-order literal in the original ANF instance can be true. Notably, the second constraint is not presented in the form of an ANF formula but as an inequality, represented as a special type of edge in the graph construction. As the toy example shown in Fig. 3(a), the redefined vanilla literals $u_{1}=x_{1}x_{2}$ , $u_{2}=\\overline{{x_{1}}}x_{2}$ , $u_{3}=x_{1}\\overline{{x_{2}}}$ and $u_{4}=x_{1}x_{3},~u_{5}=\\overline{{{x_{1}}}}x_{3},~u_{3}=x_{1}\\overline{{{x_{3}}}}$ have the constraint $u_{1}\\!+\\!u_{3}\\!+\\!u_{4}\\!+\\!u_{6}=0$ due to $x_{1}=x_{1}x_{2}+x_{1}{\\overline{{x_{2}}}}=x_{1}x_{3}+x_{1}{\\overline{{x_{3}}}}=u_{1}+u_{3}=u_{4}+u_{6}$ . Besides, since $u_{1},u_{2},u_{3}$ and $u_{4},u_{5},u_{6}$ belong to the same literal subset respectively, we have $u_{1}+u_{2}+u_{3}\\leq1$ and $u_{4}+u_{5}+u_{6}\\leq1$ . ", "page_idx": 12}, {"type": "text", "text": "Then, an undirected graph is derived from an updated ANF formula as follows and Fig. 3(b) shows the graph derived from the ANF formula in Fig. 3(a). ", "page_idx": 12}, {"type": "text", "text": "\u2022 Each vanilla literal $u_{i}$ in the ANF becomes a vertex in graph $G$ . ", "page_idx": 12}, {"type": "text", "text": "\u2022 Each clause becomes two vertices ( $\\overline{{c}}_{i}$ and $c_{i}$ ) in graph $G$ according to the set of literals it contains, where $c_{i}$ denotes clauses with constant term 0, called the positive clause, while $\\overline{{c}}_{i}$ denotes clauses with constant term 1, called the negative clause. \u2022 An edge between literal $u_{i}$ and clause $c_{k}$ is in $g$ if and only if $x_{i}x_{j}$ appears in clause $c_{k}$ . \u2022 A different edge between each clause vertex $c_{i}$ and its complementary clause vertex $\\bar{c}_{i}$ . \u2022 Two special edges between each literal $u_{i}$ and other literals in their common literal subset. ", "page_idx": 13}, {"type": "text", "text": "C Encryption Process of Scipher", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Scipher has the following encryption process. Given the length $k$ of the seed key, the length $n$ of the plaintext, and the number of encryption rounds $r$ , Scipher consists of multiple rounds of transformations. Each round uses a round key that is derived from the original key through specific linear transformations, with a length equal to half of the input and output lengths of that round. The input for the first round is the plaintext, and the output of the $r$ -th round is the ciphertext. For the $i$ -th round, we have the following round function: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T_{1}=(L_{i}\\ll a)\\&(L_{i}\\ll b),\\ T_{2}=L_{i}\\ll c}}\\\\ {{T_{3}=T_{1}\\oplus T_{2}\\oplus K_{i}}}\\\\ {{L_{i+1}=T_{3}\\oplus R_{i}}}\\\\ {{R_{i+1}=L_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $K_{i}$ represents the round key for the $i$ -th round; $a,\\,b,\\,c$ are constants representing the number of shifted bits; $L_{i}$ and $R_{i}$ represent the high and low parts of the $i$ -th round function\u2019s input, while $L_{i+1}$ and $R_{i+1}$ represent the high and low parts of the output. ", "page_idx": 13}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The primary limitation of this paper lies in our focus on plaintext-ciphertext cryptographic Problems, while not fully exploring other cryptographic issues. On the one hand, our objective is to introduce a novel graph construction approach by leveraging the commonly encountered XOR operation in cryptographic problems, thereby proposing a graph structure based on the Arithmetic Normal Form (ANF). This allows plaintext-ciphertext cryptographic Problems to be conveniently transformed into ANF-based SAT instances. On the other hand, plaintext-ciphertext encryption algorithms are built on the hardness of the MQ problem, and thus the SAT instances derived from plaintext-ciphertext problems are also instances of the MQ problem. This provides a basis for exploring various aspects of the MQ problem. Consequently, we have chosen the widely studied plaintext-ciphertext cryptographic Problems for comprehensive decomposition and investigation. We believe that our exploration of ANF-based SAT instances will offer valuable insights and reference points for the field of SAT-based cryptanalysis. Furthermore, the proposed model can be adapted to other cryptographic problems that require the transformation of the original problem into an instance of the MQ problem. ", "page_idx": 13}, {"type": "text", "text": "E Impact Statements ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The proposed CryptoANFNet, a novel graph learning framework for SAT-based cryptanalysis utilizing Arithmetic Normal Form (ANF), presents a fresh approach to tackling the challenging task of predicting plaintext-ciphertext satisfiability. Its speed improvement, surpassing heuristic solvers by a remarkable $50\\mathrm{x}$ and outperforming advanced learning-based SAT solvers like NeuroSAT, marks a significant advancement in cryptographic problem-solving. These strides not only enhance the efficiency and accuracy of cryptographic solutions but also open up new avenues for optimizing learning-based SAT solvers, advancing decryption algorithms, and ultimately fostering the refinement of encryption techniques. ", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We summarize the contributions of this paper in both the abstract and the concluding paragraph of the introduction. Each claimed contribution is supported by explicit results presented within the paper. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We have discussed the limitations in Appendix D ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have provided the dataset settings in Section 5.1, and detailed experimental settings in Section 5.2 and Section 5.4. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: The source code will be made publicly available upon acceptance. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have provided the dataset settings in Section 5.1, and detailed experimental settings in Section 5.2 and Section 5.4. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: We keep the same setting as the previous works. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have provided the dataset settings in Section 5.1, and detailed experimental settings in Section 5.2 and Section 5.4. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have discussed the potential impacts of the paper in Appendix E Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have cited the assets used in the paper in Section 5.1, Section 5.2 and Section 5.4. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Currently, the paper does not release new assets. New assets, including code and toolkit, will be released upon the acceptance of the paper with comprehensive documents. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]