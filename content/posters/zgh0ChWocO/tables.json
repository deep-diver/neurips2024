[{"figure_path": "zgh0ChWocO/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), AW and Variance (S-VAR and L-VAR) as evaluation metrics. The best result is bolded.", "description": "This table compares the performance of the proposed Decomposition-based Policy Learning (DPPL) method and the Linear Weighting method on two datasets (IHDP and JOBS).  It shows the short-term rewards, long-term rewards, the change in welfare (AW), and the variance of short-term and long-term rewards for each method across ten different preference vectors. The best performing method for each metric is highlighted in bold. This helps to demonstrate the effectiveness of the DPPL method in balancing short-term and long-term rewards, especially when compared to the simpler linear weighting approach.", "section": "4 Experiments"}, {"figure_path": "zgh0ChWocO/tables/tables_8_1.jpg", "caption": "Table 2: The \u025b values correspond to each preference vector in IHDP and JOBS datasets, where T = 4 and r = 0.2, obtained according to Theorem 1.", "description": "This table presents the minimum acceptable short-term reward (-\u025b) for different preference vectors, calculated using Theorem 1.  The values are obtained from the IHDP and JOBS datasets, with a time step of 4 and a missing rate of 0.2.  The table shows how the minimum acceptable short-term reward changes based on the preference given to long-term versus short-term rewards in the preference vector.", "section": "3.3 Deep Analysis of the Preference Vector"}, {"figure_path": "zgh0ChWocO/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), AW and Variance (S-VAR and L-VAR) as evaluation metrics. The best result is bolded.", "description": "This table compares the performance of the proposed decomposition-based policy learning (DPPL) method and the linear weighting method on two benchmark datasets (IHDP and JOBS).  For each method and dataset, it shows the short-term reward, long-term reward, the overall balanced reward (AW), and the variance of the short-term and long-term rewards across multiple runs. The best performance for each metric is highlighted in bold, demonstrating the superiority of the DPPL approach.", "section": "4 Experiments"}, {"figure_path": "zgh0ChWocO/tables/tables_18_1.jpg", "caption": "Table D2: The \u03b5 values corresponding to each preference vector in the two datasets IHDP and JOBS, where T = 4 and r = 0.3, which are derived according to Theorem 1.", "description": "This table shows the minimum acceptable short-term reward (\u03b5) for different preference vectors, calculated using Theorem 1.  The table displays the results for both the IHDP and JOBS datasets under the condition of T=4 and r=0.3. Each row represents a specific preference vector, and the corresponding \u03b5 values for IHDP and JOBS are listed in separate columns.", "section": "D.1 Sensitivity Analysis on Missing Ratio"}, {"figure_path": "zgh0ChWocO/tables/tables_18_2.jpg", "caption": "Table D2: The \u025b values corresponding to each preference vector in the two datasets IHDP and JOBS, where T = 4 and r = 0.3, which are derived according to Theorem 1.", "description": "This table shows the minimum acceptable value (\u03b5) of the short-term reward for each preference vector, while maximizing the long-term reward.  These values are calculated based on Theorem 1 in the paper, with a time step (T) of 4 and a missing ratio (r) of 0.3.  The table helps decision-makers select preference vectors based on their acceptable short-term reward thresholds.", "section": "D.1 Sensitivity Analysis on Missing Ratio"}, {"figure_path": "zgh0ChWocO/tables/tables_18_3.jpg", "caption": "Table D2: The \u025b values corresponding to each preference vector in the two datasets IHDP and JOBS, where T = 4 and r = 0.3, which are derived according to Theorem 1.", "description": "This table shows the minimum acceptable level of short-term reward (\u03b5) for different preference vectors, considering the trade-off between short-term and long-term rewards.  The values are calculated based on Theorem 1, with a missing ratio (r) of 0.3 and a time step (T) of 4.  It helps to understand how the preference for long-term versus short-term rewards affects the acceptable minimum short-term reward.", "section": "D Additional Experimental Results"}, {"figure_path": "zgh0ChWocO/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), AW and Variance (S-VAR and L-VAR) as evaluation metrics. The best result is bolded.", "description": "This table presents a comparison of the proposed Decomposition-based Policy Learning (DPPL) method and the Linear Weighting method for balancing short-term and long-term rewards.  The comparison is made using two datasets (IHDP and JOBS) across multiple evaluation metrics. The metrics include short-term rewards, long-term rewards, a welfare change metric (AW), and the variance of both short and long-term rewards. The best performing method for each metric in each dataset is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "zgh0ChWocO/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), AW and Variance (S-VAR and L-VAR) as evaluation metrics. The best result is bolded.", "description": "This table compares the performance of the proposed Decomposition-based Policy Learning (DPPL) method and the traditional linear weighting method on two benchmark datasets (IHDP and JOBS) using several metrics including short-term rewards, long-term rewards, the combined welfare change (AW), and the variance of short-term and long-term rewards.  The best performance for each metric is highlighted in bold.  The table demonstrates the DPPL method's superior performance and stability across various preference vectors.", "section": "4 Experiments"}, {"figure_path": "zgh0ChWocO/tables/tables_19_3.jpg", "caption": "Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), AW and Variance (S-VAR and L-VAR) as evaluation metrics. The best result is bolded.", "description": "This table compares the performance of the proposed Decomposition-based Policy Learning (DPPL) method and the traditional linear weighting method on two benchmark datasets, IHDP and JOBS.  The comparison is based on several metrics: short-term rewards, long-term rewards, a combined welfare change (AW), and the variance of both short-term and long-term rewards.  The best result for each metric is highlighted in bold, illustrating DPPL's superiority in most cases.", "section": "4 Experiments"}]