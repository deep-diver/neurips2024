[{"figure_path": "Wy9UgrMwD0/tables/tables_15_1.jpg", "caption": "Table 2: Hyperparameters for ALE.", "description": "This table lists all the hyperparameters used for the experiments conducted on the Arcade Learning Environment (ALE).  It details settings for the environment (e.g., sticky actions, frame skip, observation transformations), the data collection process (e.g., total steps, parallel environments), model architecture (e.g., convolutional and linear layers, activation function), the optimization process (e.g., optimizer, learning rate, loss functions), and logging frequency.", "section": "B Experiment details"}, {"figure_path": "Wy9UgrMwD0/tables/tables_16_1.jpg", "caption": "Table 1: Hyperparameters for the toy setting in Figure 5.", "description": "This table lists the hyperparameters used in the toy setting to simulate the effects of rank collapse on the trust region, as shown in Figure 5 of the paper.  It includes details about the environment (sampling of states and actions), the policy network architecture, and the optimization process (clipping epsilon, optimizer, learning rate, minibatch size, number of epochs, and number of steps). This setup is designed to create a simplified scenario to demonstrate how the PPO trust region constraint can be bypassed when representations collapse.", "section": "3.2.1 A toy setting to understand the effects of rank collapse on trust region"}, {"figure_path": "Wy9UgrMwD0/tables/tables_18_1.jpg", "caption": "Table 2: Hyperparameters for ALE.", "description": "This table lists the hyperparameters used for the Arcade Learning Environment (ALE) experiments in the paper.  It includes details about the environment setup (sticky actions, frameskip, image resizing), the data collection process (number of environments, total steps), model architecture (convolutional and linear layers), the optimization process (optimizer, learning rate, clipping epsilon, entropy bonus), and logging frequency.", "section": "B Experiment details"}, {"figure_path": "Wy9UgrMwD0/tables/tables_19_1.jpg", "caption": "Table 3: Hyperparameters for MuJoCo.", "description": "This table lists the hyperparameters used in the MuJoCo experiments.  It covers environment settings (frameskip, maximum steps per episode, etc.), observation transformations (normalization), data collection details (total steps, number of parallel environments), model architecture (activation functions, layer sizes), optimization parameters (advantage estimator, value loss, policy loss, learning rate, betas, etc.), and logging frequency for various metrics (training, capacity). The table also indicates whether or not minibatch normalization and linearly annealing were applied.  In short, it provides a complete specification of the experimental setup for the MuJoCo reinforcement learning tasks.", "section": "Experiment details"}]