[{"type": "text", "text": "Unified Insights: Harnessing Multi-modal Data for Phenotype Imputation via View Decoupling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qiannan Zhang, Weishen Pan, Zilong Bai, Chang Su, Fei Wang Weill Cornell Medicine, Cornell University {qiz4005,wep4001,zib4001,chs4001,few2001}@med.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Phenotype imputation plays a crucial role in improving comprehensive and accurate medical evaluation, which in turn can optimize patient treatment and bolster the reliability of clinical research. Despite the adoption of various techniques, multi-modal biological data, which can provide crucial insights into a patient\u2019s overall health, is often overlooked. With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view. However, the heterogeneity and imprecise nature of the multimodal data still pose challenges in developing an effective method to model from two views. In this paper, we propose a novel framework to incorporate multi-modal biological data via view decoupling. Specifically, we segregate the modeling of biological data from phenotype data in a graph-based learning framework. From the biological view, the latent factors in biological data are discovered to model patient correlation. From the phenotype view, phenotype co-occurrence can be modeled to reveal patterns across patients. Hence, patients are encoded from these two distinct views. To mitigate the influence of noise and irrelevant information in biological data, we devise the cross-view contrastive knowledge distillation that distills insights from the biological view to enhance phenotype imputation. Phenotype imputation with the proposed model demonstrates superior performance over state-of-the-art models on the real-world biomedical database. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clinical records, serving as a critical resource for understanding disease patterns and patient outcomes, are valuable for observational studies. However, its collection can be biased or incomplete due to the limits on infrastructures and expertise, the inconsistency in data types across healthcare systems, and the variability in patient cohorts, etc [3, 15]. For instance, it is recognized that patients with dementia and its related conditions can have under-documented phenotypes [36], probably resulting from a lack of clear symptoms early on or ignorance of related diseases. The issue of missing or incomplete phenotypic data is pervasive and can lead to biased results in medical research and suboptimal patient care [21]. In light of this, phenotype imputation is essential to ensure a more holistic and precise medical evaluation, thereby optimizing patient care and enhancing the validity of clinical studies. ", "page_idx": 0}, {"type": "text", "text": "Traditional imputation methods [11, 2] rely on informative statistical characteristics of the clinical data to infer the missing phenotypes, yet often neglect the broad, interconnected nature of clinical data with multi-modal biological information such as proteomics and metabolomics, while the latter might provide deeper insights into the patient\u2019s health status. The growing development of extensive biobanks [4, 33], collecting various biological and lifestyle data alongside traditional clinical records, unlocks a potential to address incomplete phenotypic data in clinical records. By leveraging multi-modal biological data as external information, as shown in Figure 1, the associations between biological observations and clinical phenotypes might improve the inference of incomplete phenotypes. ", "page_idx": 0}, {"type": "image", "img_path": "8B3sAX889P/tmp/91c075a89a9583db538f08becc5aa6a8c3cbeabc12e68973c402adee1ac78ae2.jpg", "img_caption": ["Figure 1: Phenotype imputation with multi-modal biological data. \"M1\" denotes Modality 1, and \"M2\" represents Modality 2. \"\u2014\" refers to the missing modality and the red question mark refers to the phenotype that needs to be imputed. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, leveraging multi-modal information for phenotype imputation remains a complex challenge in two folds: 1) The heterogeneity of multi-modal data typically results in significant variances from clinical data, as it includes different data types and characteristics. For instance, continuous variables in proteomics may exhibit different patterns and correlations with a patient\u2019s health status compared to discrete phenotype data. Multi-modal biological data often contain measurement noise and irrelevant information unrelated to phenotypic observations, which hinders accurate phenotype imputation. Furthermore, biological data are frequently missing for many individuals due to the labor-intensive and costly nature of data collection. ", "page_idx": 1}, {"type": "text", "text": "Despite the compelling need to leverage multi-modal data, the challenges outlined above have posed significant obstacles to developing an effective approach for phenotype imputation. In recent years, graphs have gained traction as a powerful tool for modeling complex data and capturing relationships between real-world entities. Representing patients and phenotypes within a graph structure and imputing missing phenotypes using Graph Neural Networks (GNNs) offers a promising path forward. Biological data could, in principle, be incorporated as patient attributes and propagated through the graph. However, the joint modeling conflicts with the heterogeneity between biological and phenotypic data, as each encapsulates distinct rationales for unveiling patient-specific health conditions. First, from a statistical and collaborative view, the patient-phenotype graph connecting patients and their phenotypes reflects phenotype co-occurrence patterns across all patients\u2019 interactions. These co-occurrence patterns indicate an underlying principle in imputation: if phenotype $x$ and $y$ are frequently co-diagnosed, it is sensible to impute $y$ for a patient once $x$ is observed. Second, from $a$ biological view, a patient\u2019s biological data reveals their fine-grained health status. This highlights another rationale for imputation: understanding the detailed health conditions from biological data can guide the imputation of phenotypes that correspond to similar biological health status. Therefore, in this paper, we propose a view decoupling approach to segregate the modeling of biological data from phenotypic data, thereby fully utilizing the information from both sources. ", "page_idx": 1}, {"type": "text", "text": "To model the correlation between patients and phenotypes, one can construct and encode a bipartite graph. Nevertheless, the use of biological data is not a straightforward task. Biological data is characteristically composed of a wide range of variables, including protein concentrations, metabolic profiles, gene expression levels, etc. These variables exhibit high-dimensional and continuous characteristics, making it challenging to model the data effectively. More importantly, the biological conditions of patients uncover major underlying factors that indicate health status. In other words, patients sharing similar underlying biological factors could have similar phenotypes. Identifying these latent factors would facilitate the effective characterization of patients and their phenotypes. ", "page_idx": 1}, {"type": "text", "text": "To tackle these challenges, in this paper, we propose a novel framework MPI, aiming to harness the Multimodal data for Phenotype Imputation. First, to identify the latent biological factors, we propose quantizing the biological data and uncovering the corresponding factors using Residual Quantization. Then, the obtained factors in conjunction with the patients themselves, are utilized to create a graph that models the correlation between patients from a biological view. To decouple views and segregate the modeling of biological data from phenotypic data, the patients and phenotypes are additionally incorporated into another separate graph that depicts the patterns of co-occurrence from the collaborative view. GNNs are then employed to encode both graphs. Second, with the two separate graphs, we aim to leverage the biological information to facilitate the phenotype imputation. However, due to the presence of noise and irrelevant information in biological data, relying solely on biological factors may lead to inaccurate imputation. Thus, we employ a cross-view contrastive knowledge distillation strategy to distill biological knowledge for enhancing phenotype imputation. Within a teacher-student framework, we consider the biological-view GNN as the teacher model and the collaborative-view GNN as the student model. Rather than replicating the teacher model entirely, the aim is for the student model to glean useful knowledge by receiving partial guidance from the teacher model. The main contributions of this work are summarized as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1) We propose leveraging multi-modal data to enhance phenotype imputation through view decoupling, thereby segregating the modeling of multi-modal biological data from phenotype data. 2) To enhance the depiction of patient profiling and facilitate the imputation, we propose to uncover the latent biological factors of patients and accordingly model the correlation among the patients based on these factors. 3) To avoid the impact of noise and irrelevant information in biological data, we adopt a novel cross-view contrastive knowledge distillation to subtly leverage information from biological data. 4) Extensive experiments over a real-world biomedical database demonstrate the superiority of our proposed method over state-of-the-art methods. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Phenotype Imputation. Phenotype imputation involves predicting missing phenotypic information in clinical electronic health records (EHRs), e.g., diseases and symptoms, generally leveraging various methods ranging from traditional statistical approaches to advanced machine learning techniques. Early research relies on statistical modeling and matrix analysis [41, 40, 10, 1], while deep learning demonstrates effectiveness in modeling more complex dependencies with deep networks [14, 50, 27, 2]. Despite existing efforts to explore the correlations between phenotypes and genotypes [2], multi-modal biological data is largely overlooked in EHR analysis. Our approach differentiates itself by utilizing multi-modal biological data to enhance phenotype imputation in EHRs. ", "page_idx": 2}, {"type": "text", "text": "Graph Neural Networks in Biomedicine. Graph Neural Networks (GNNs) [13, 54] have been employed to model the interconnectivity of either clinical data or biological information. A line of research devises GNN models for EHRs to enhance healthcare representation learning and patientspecific outcomes [9, 35, 20, 28]. By leveraging the entities and connections in EHRs, e.g., diseases, symptoms, and drug interactions, GNNs show effectiveness in producing patient proflies and clinical predictions [23, 26]. Meanwhile, biological studies leverage GNNs to explore biological networks, promote disease mechanism discovery, analyze drug response, etc. For instance, single-cell biology adopts GNNs to analyze cellular heterogeneity, aiming for an improved understanding of cellular functions and interactions [18, 31]. Besides, some work integrates clinical and molecular data to predict adverse drug reaction signals [22], exemplifying the integration of EHRs and biological data for combined healthcare analysis. Our approach leverages biological data to aid phenotype imputation in EHRs by bridging the gap between clinical data and underlying biological mechanisms. ", "page_idx": 2}, {"type": "text", "text": "Multi-modal Representation Learning on EHRs. Multi-modal learning on EHRs aims to integrate varied modalities in EHRs, e.g., medication records, lab test results, imaging data, and clinical notes, to obtain optimized patient representations [23, 17]. Given the potential unavailability of modalities, research efforts are made to improve model robustness in the face of partially or completely missing modalities. Strategies include imputing the missing modalities, exploring the data generation process, and preserving the structure of observed data [48, 29, 52, 6, 47, 53]. However, existing works primarily explore modalities within EHRs as clinical insights, often overlooking biological knowledge in EHR analysis. Different from existing work, we explore multi-modal biological data with random missingness to enhance phenotype imputation in EHRs, via addressing the heterogeneity and inaccuracy in multi-modal biological data. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Electronic Health Records (EHRs). Clinical records, integral for encoding patient health information, are commonly digitized into electronic health records (EHRs) and formatted as high-dimensional medical codes. Typically, a clinical record includes a series of clinical entities, such as diagnoses, medications, procedures, laboratory tests, and clinical notes. In this paper, our primary focus is on the phenotypic information within EHRs, which is generally encoded as one-hot vectors, thus indicating the presence or absence of specific medical symptoms or diseases. ", "page_idx": 2}, {"type": "text", "text": "Phenotype. Define the phenotype data in EHRs for a patient cohort as $\\mathbf{X}=\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{N}\\}$ , where $N$ represents the total number of patients. Each $\\mathbf{x}_{i}$ encapsulates the phenotypic attributes for patient $i$ , represented by medical codes for symptoms and diseases, denoted as $\\mathbf{x}_{i}\\overset{\\cdot}{=}\\{p_{1},p_{2},\\ldots,p_{|\\mathbf{x}_{i}|}\\}$ . ", "page_idx": 2}, {"type": "image", "img_path": "8B3sAX889P/tmp/79bec583674bb06161b9dd61e4cd2e074c69e29d0fefcfa3e3a75c6979e29be8.jpg", "img_caption": ["Figure 2: An overview of the MPI framework: (1) Residual Quantization quantizes the biological data and uncovers the underlying factors. (2) Biological-view GNN and Phenotype-view GNN are employed to encode the correlation between patients, biological factors, and phenotypes in separate graphs. (3) Cross-view knowledge distillation makes use of learned representations from different views and enhances the imputation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Patient Multi-modal Data with Irregular Missingness. In biological multi-modal datasets, we represent each patient by a collection of data points from various biological modalities, such as gmeunlteit-icms,o dparlo tdeaotamsiect sf,o ro rp amtieetnatbs oclaonm bices .e xLpreet $Z$ rd eapsr $\\mathbf{X}^{M}=\\{\\mathbf{x}_{1}^{M},\\mathbf{x}_{2}^{M},\\ldots,\\mathbf{x}_{N}^{M}\\}$ ,o dwahleitriee $N$ tdheenn otthees the number of patients. Given the potential for absent modalities, we define the observed multi-modal data for patient $i$ as $\\mathbf{x}_{i}^{M}=\\{\\mathbf{x}_{i}^{1},\\bar{\\mathbf{x}_{i}^{2}},\\dots,\\mathbf{x}_{i}^{m}\\}$ , adhering to the condition $0\\leq m\\leq Z$ . We focus on the most relaxed setting where the modality missingness is irregular across patients, i.e., random missingness. This randomness persists through the phases of training, validation, and testing, allowing for the possibility that a patient might lack data for any, or in extreme cases, all modalities. ", "page_idx": 3}, {"type": "text", "text": "Phenotype Imputation. Phenotype imputation aims to address critical gaps in clinical records, where certain medical symptoms, disease attributes, or outcomes are not documented or are incompletely recorded. Given a patient cohort and the incomplete phenotypic data in a clinical dataset, the problem we focus on aims to impute the other possible phenotypes by leveraging available biological multi-modal data. Let $\\mathbf{X}$ be the incomplete phenotype data, and $\\mathbf{X}^{\\tilde{M}}$ be the biological multi-modal data with irregular missingness, the objective is to design a model that infers the existence of other possible phenotypes. Thereby, a model $\\Phi$ is expected to perform ${\\bf Y}=\\Phi({\\bf X},{\\bf X}^{M};\\cdot)$ and minimize the discrepancy between the actual phenotype $\\tilde{\\bf Y}$ and the imputed phenotype Y. Here $\\mathbf{Y}$ and $\\hat{\\textbf{Y}}$ denote one-hot vectors. Given the extensive set of phenotypes, measuring discrepancy through classification is impractical. Therefore, we frame the imputation task as a ranking problem, aiming to position the correct phenotype higher than the incorrect ones. ", "page_idx": 3}, {"type": "text", "text": "4 The Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the proposed method MPI. As shown in Figure 2, our proposed model includes three components, i.e., biological data quantization, dual-view graph representation learning, and cross-view contrastive knowledge distillation. Next, we describe each component in detail. ", "page_idx": 3}, {"type": "text", "text": "4.1 Biological Data Quantization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The biological state reveals analogous latent factors among patients. Existing approaches primarily use biological data as features and apply traditional machine learning techniques to encode them, yet they often struggle to disentangle the complex, heterogeneous factors inherent in biological data [44, 12]. The learned representation of patients could be non-robust (e.g., prone to overreact to an irrelevant factor) and hardly explainable. To identify the latent biological factors among patients, we propose quantizing the biological data and uncovering the corresponding factors using residual quantization [24], which employs a multi-level vector quantizer to convert residuals into a series of codes. Specifically, the input $\\mathbf{x}^{m}$ is initially encoded into a latent representation $\\mathbf{z}^{m}:=\\mathbf{E}(\\mathbf{x}^{m})$ by an encoder $\\mathbf{E}$ . At the first level $[d=0]$ ), the residual is set to $\\mathbf{r}_{0}:=\\mathbf{z}^{m}$ . For each level $d$ , we define a codebook $C_{d}:=\\{\\mathbf{e}_{k}\\}_{k=1}^{K}$ with size $K$ . The residual $\\mathbf{r}_{\\mathrm{0}}$ is quantized by mapping it to the nearest embedding from the codebook. The index of the closest embedding $\\mathbf{e}_{c_{0}}$ at $d=0$ , which is $c_{0}=\\arg\\operatorname*{min}_{k}\\left\\|\\mathbf{r}_{0}-\\mathbf{e}_{k}\\right\\|$ , represents the zero-th code. For the next level $[d=1]$ ), the residual is updated to $\\mathbf{r}_{1}:=\\mathbf{r}_{0}-\\mathbf{e}_{c_{0}}$ . The code for this level is determined by finding the embedding in the first level\u2019s codebook that is nearest to $\\mathbf{r}_{1}$ . This quantization process is recursively repeated $l$ times, producing a tuple of $l$ codes that constitute the disentangled biological factors. This hierarchical approach approximates the input biological data from coarse to fine granularity. Notably, separate codebooks are used for each of the $l$ levels rather than a single, large codebook. This strategy is preferred as the norm of residuals tends to decrease with increasing levels, facilitating the capture of different granularity levels from the input data. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Upon obtaining the disentangled biological factors $(c_{0},\\dotsc,c_{l-1})$ , the quantized representation of ${\\mathbf z}^{m}$ is determined as $\\begin{array}{r}{\\hat{\\mathbf{z}}^{m}:=\\sum_{d=0}^{l-1}\\mathbf{e}_{c_{d}}}\\end{array}$ . This quma ntized vectomr $\\hat{\\mathbf{z}}^{m}$ is sumbsequently fed into a decoder $\\mathbf{D}$ , which attempts to recon struct the input based on $\\hat{\\bf x}^{m}={\\bf D}(\\hat{\\bf z}^{m})$ . The loss function for the residual quantization is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{bio}}:=\\mathcal{L}_{\\mathrm{recon}}+\\mathcal{L}_{\\mathrm{rq}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{recon}}:=\\;\\|\\mathbf{x}^{m}-\\hat{\\mathbf{x}}^{m}\\|^{2}$ and $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{rq}}:=\\sum_{i=0}^{l-1}\\|\\mathbf{s}\\mathbf{g}[\\mathbf{r}_{i}]-\\mathbf{e}_{c_{i}}\\|^{2}+\\beta\\|\\mathbf{r}_{i}-\\mathbf{s}\\mathbf{g}[\\mathbf{e}_{c_{i}}]\\|^{2}}\\end{array}$ . Here, $\\hat{\\mathbf{x}}^{m}$ represents the decoder\u2019s output, and sg denotes the stop-gradient operation [42]. The training of this autoencoder involves simultaneous updating of the quantization codebooks and the parameters of the encoder-decoder. Note that the exclusive autoencoder and quantization codebooks are learned to capture the disentangled biological factors for each modality. For example, a patient\u2019s biological data includes two types of modalities, the disentangled biological factors can be represented as $(c_{0}^{1},\\dots,c_{l-1}^{1})$ and $(\\bar{c}_{0}^{2},\\ldots,c_{l-1}^{2})$ . We use $\\mathcal{C}$ to denote the set of learned biological factors in all codebooks in subsequent sections. ", "page_idx": 4}, {"type": "text", "text": "4.2 Dual-view Graph Representation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With disentangled biological factors and phenotypes, a patient can be described from two perspectives: a phenotype view and a biological view. To effectively capture the relationship between patients and biological factors and phenotypes, and fully utilize the information from both views, we construct two separate graphs instead of a single patient-centric graph. ", "page_idx": 4}, {"type": "text", "text": "Patient-Phenotype Graph Construction. From the phenotype view, we construct a patientphenotype graph, denoted as $\\mathcal{G}_{p}$ , to depict the collaborative relationships between phenotypes, specifically focusing on phenotype-phenotype co-occurrences. The construction of $\\mathcal{G}_{p}$ begins with defining a set of phenotypes $\\mathcal{P}$ and a set of patients X. Each patient $\\mathbf{x}\\in\\mathbf{X}$ is associated with one or more phenotypes $p\\in\\mathcal{P}$ . An edge is created between a patient node and a phenotype node if the patient exhibits that phenotype. By linking patients to their respective phenotypes, $\\mathcal{G}_{p}$ captures the complex interactions and shared occurrences of different phenotypes across the patient cohort, and provides a comprehensive view of how different phenotypes interact within the patient population. ", "page_idx": 4}, {"type": "text", "text": "Patient-Factor Graph Construction. From the biological view, we first construct a patient-factor graph, denoted as $\\mathcal{G}_{f}$ , to explore the biology-level correlation between patients. Specifically, the graph $\\mathcal{G}_{f}$ is constructed using the same set of patients $\\mathbf{X}$ and disentangled biological factors $\\mathcal{C}$ from learned codebooks as the set of nodes. To connect patients and factors, we build edges between each patient $\\mathbf{x}$ and their corresponding factors $(c_{0},\\dotsc,c_{l-1})$ . This patient-factor graph $\\mathcal{G}_{f}$ reveals patient correlations through shared factors, offering a distinct approach to characterizing patients. ", "page_idx": 4}, {"type": "text", "text": "With the constructed graphs $\\mathcal{G}_{f}$ and $\\mathcal{G}_{p}$ , we denote the adjacency matrices of $\\mathcal{G}_{f}$ and $\\mathcal{G}_{p}$ as $\\mathbf{A}_{f}$ and $\\mathbf{A}_{p}$ , respectively. To capture the structural information of the graphs $\\mathcal{G}_{f}$ and $\\mathcal{G}_{p}$ and learn the representation of patients, phenotypes, and biological factors, we utilize basic Graph Convolutional Networks (GCNs) as the graph encoder. Taking $\\mathcal{G}_{p}$ as an example, the phenotype-view graph encoder for $\\mathcal{G}_{p}$ works by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{H}_{p}^{(l+1)}=\\sigma\\left(\\hat{\\mathbf{A}}_{p}\\mathbf{H}_{p}^{(l)}\\mathbf{W}_{p}^{(l)}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{H}_{p}^{(0)}=\\mathbf{F}_{p}$ represents the initial input features, to be more specific, for patients and phenotypes, the input features are randomly initialized. In contrast, for biological factors, the input features are initialized using the corresponding code embedding of factors. And $\\mathbf{H}_{p}^{(l)}$ denotes the node representations at the $l$ -th layer. The matrix $\\hat{\\mathbf{A}}_{p}=\\hat{\\mathbf{D}}_{p}^{-1/2}\\tilde{\\mathbf{A}}_{p}\\hat{\\mathbf{D}}_{p}^{-1/2}$ is the symmetrically normalized adjacency matrix, with $\\hat{\\mathbf{D}}_{p}\\,\\in\\,\\mathbb{R}^{N\\times N}$ being the degree matrix of $\\tilde{\\mathbf{A}}_{p}\\,=\\,\\mathbf{A}_{p}\\,+\\,\\mathbf{I}_{N}$ , where ${\\mathbf{I}}_{N}$ is the identity matrix. Similarly, the representation $\\mathbf{H}_{f}^{(l)}$ can be learned from the graph $\\mathcal{G}_{f}$ using the biological-view graph encoder. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To optimize both graph encoders and to effectively differentiate between the positive and negative edges in graphs, we define a margin-based ranking loss for graph $\\mathcal{G}_{p}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p}=\\sum_{(i,j)\\in\\mathcal{E}_{p}}\\sum_{(i,k)\\in\\mathcal{N}_{p}}\\operatorname*{max}(0,\\gamma-f(i,j)+f(i,k)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma$ is the margin hyperparameter, $(i,j)\\in\\mathcal{E}_{p}$ denotes the set of positive edges in graph $\\mathcal{G}_{p}$ , and $(i,k)\\in\\ensuremath{\\mathcal{N}}_{p}$ denotes the set of negative edges and $(i,k)$ does not present in $\\mathcal{G}_{p}$ . $\\bar{f}(,)$ is a multi-layer perceptron (MLP) that takes node embeddings as inputs and outputs the similarity score between two node embeddings. We use the same loss function to update the biological-view graph encoder of graph $\\mathcal{G}_{f}$ and denote the loss as $\\mathcal{L}_{f}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Cross-view Contrastive Knowledge Distillation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Due to the noisy and irrelevant information in the biological data that could mislead the phenotype imputation, the learning from the biological view and the learning from the phenotype view are separative and we propose a cross-view contrastive knowledge distillation strategy to subtly leverage the biological knowledge to facilitate the phenotype imputation. Following the teacher-student framework [19, 8, 39], we regard the biological-view graph encoder as the teacher model and the phenotype-view graph encoder as the student model. Since the teacher model cannot provide the completely precise knowledge to represent patients [34], instead of fully imitating the behavior of the teacher model, the student model is expected to extract the beneficial knowledge only incorporating partial supervision from the teacher model. Specifically, with the patient representation $\\mathbf{H}_{f}$ learned from biological-view graph $\\mathcal{G}_{f}$ and patient representation $\\mathbf{H}_{p}$ learned from the collaborative-view graph $\\mathcal{G}_{p}$ , we propose cross-view contrastive knowledge distillation to distill useful knowledge from the biological-view graph encoder. This approach leverages view-specific embeddings, represented as $\\mathbf{h}_{f}^{i}$ from the biological view and $\\mathbf{h}_{p}^{i}$ from the phenotype view for patient $i$ . Our objective is to align these embeddings into a shared space, facilitating discriminative representation learning through contrastive loss. Initially, embeddings are processed through a transformation with hidden layers to project them into the desired space as $\\mathbf{h}_{f}^{i}=\\sigma\\left(\\mathbf{W}^{(2)}\\sigma\\left(\\bar{\\mathbf{W}_{}^{(1)}}\\mathbf{h}_{f}^{i}+\\mathbf{b}_{}^{(1)}\\right)+\\mathbf{b}_{}^{(2)}\\right)$ where $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$ are the trainable weight matrices, $\\mathbf{b}^{(1)}$ and $\\mathbf{b}^{(2)}$ are the bias terms, and $\\sigma$ represents the ELU activation function. $\\mathbf{h}_{p}^{i}$ can also be processed using the same transformation. ", "page_idx": 5}, {"type": "text", "text": "We then define positive and negative samples to compute the contrastive loss. Embeddings of the same patient form positive samples from two different views, while negative samples consist of embeddings from different patients. Specifically, for a given patient $i$ , the positive sample pair is $(\\mathbf{h}_{f}^{i},\\mathbf{h}_{p}^{i})$ , and negative samples include both intra-view and inter-view pairs. The contrastive knowledge distillation loss is formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CKD}}=-\\log\\frac{e^{s(\\mathbf{h}_{f}^{i},\\mathbf{h}_{p}^{i})/\\tau}}{e^{s(\\mathbf{h}_{f}^{i},\\mathbf{h}_{p}^{i})/\\tau}+\\sum_{k\\neq i}\\left(e^{s(\\mathbf{h}_{f}^{i},\\mathbf{h}_{f}^{k})/\\tau}\\right)+\\sum_{k\\neq i}\\left(e^{s(\\mathbf{h}_{f}^{i},\\mathbf{h}_{p}^{k})/\\tau}\\right)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $s(\\cdot,\\cdot)$ denotes the cosine similarity, and $\\tau$ is a temperature parameter. This loss function incorporates negative samples from both intra-view and inter-view sources, ensuring a comprehensive learning process. By applying this cross-view contrastive optimization, our model effectively captures the intricate relationships within both the biological and collaborative views, leading to robust representations of the patients. Since the biological knowledge is distilled from the biological-view graph encoder to enhance the phenotype-view graph encoder, the loss function for $\\mathcal{G}_{p}$ to optimize the phenotype-view graph encoder is updated to $\\hat{\\mathcal{L}}_{p}=\\mathcal{L}_{p}+\\alpha\\mathcal{L}_{\\mathrm{CKD}}$ where $\\alpha$ is a tradeoff parameter. ", "page_idx": 5}, {"type": "text", "text": "4.4 Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In optimization, residual quantization involves the pretraining of autoencoders for biological data, and the quantization codebooks using loss function $\\mathcal{L}_{\\mathrm{bio}}$ to learn the disentangled biological factors and their corresponding factor embeddings. Subsequently, we utilize an iterative optimization strategy to optimize the biological-view graph encoder using $\\mathcal{L}_{f}$ and phenotype-view graph encoder using $\\hat{\\mathcal{L}}_{p}$ . ", "page_idx": 5}, {"type": "table", "img_path": "8B3sAX889P/tmp/672dd5850a21856e32bc69186df5c59d8a27a982a58c58c6f1c722f9c8647443.jpg", "table_caption": ["Table 1: Dataset Statistics "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Specifically, we leverage the patient representation learned from the biological view as the teacher signal and optimize the phenotype-view graph encoder through contrastive knowledge distillation following loss function $\\hat{\\mathcal{L}}_{p}$ . The process is iterated until both graph encoders converge. During the evaluation phase, we employ the patient representation learned from the phenotype-view graph encoder and evaluate a positive testing phenotype along with a set of candidate negative phenotypes to assess performance. The pseudocode of MPI training procedure is described in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "5 Real-World Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. We evaluate MPI and baseline approaches using the UK Biobank [4], a comprehensive biomedical database and research resource collecting extensive biological samples and clinical EHRs. We focus on phenotype imputation for populations suffering from chronic diseases and thus extract a cohort of patients diagnosed with Alzheimer\u2019s disease and related dementia. Specifically, we leverage the EHRs from inpatient and primary care to obtain phenotypic data before disease onset after preprocessing and transformation. Besides, we utilize biological data across two modalities: proteomics, measuring levels of roughly 3,000 proteins; and metabolomics, testing around 250 metabolic biomarkers. The biological data is preprocessed following common practice [7, 55]. We observe significant modality missingness at random: approximately $90\\%$ in proteomics and $50\\%$ in metabolomics. Table 1 shows the statistics of the dataset, with dataset details and preprocessing methods described in the Appendix A.1. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare the proposed model to baselines across three categories: (1) modality imputation methods, including CMAE [32] and SMIL [30]; (2) graph neural networks comprising GraphSage [16] and GIN [49], which utilize multi-modal biological information as patient features; (3) multi-modal models on EHRs that handle missingness, consisting of M3Care [53], GRAPE [51] and MUSE [47]. Note that all these methods primarily focus on patient classification tasks and rely on supervision signals from patient labels. We adapt their training objectives to suit our problem setting and evaluate the baselines on the same testing data for a fair comparison. Additional details on the baselines are provided in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Experimental Settings. We implement MPI with PyTorch and run it on an NVIDIA RTX A6000 GPU. To implement MPI, a two-layer GCN is utilized for each decomposed view with 128 and 64 hidden units respectively. It\u2019s worth noting that our focus is not on the complexity of the GNN itself; we use GCN as the foundational backbone model, which can be substituted with any advanced GNNs as needed. Besides, the quantization of proteomics and metabolomics is conducted with respective autoencoders including a two-layer encoder and one-layer decoder, with a hidden size of 32 units. To determine the trade-off weight for knowledge distillation, we choose 0.1 after a grid search in {0.01, 0.1, 1, 5, 10}. The margin hyperparameter $\\gamma$ is determined as 3 through a search in {1, 3, 5,10}. The model is trained with Adam optimizer and evaluated at every epoch with an early-stopping strategy at patience of 40 per the validation set performance. Baselines including Graphsage and GIN utilize the same hidden sizes as MPI. CMAE and SMIL first conduct feature imputation for the missing modalities, afterwards an MLP model is conducted with the imputed features for our ranking objectives. As M3Care, Grape, and MUSE build graphs for patients and EHR modalities, we use their published implementations and conduct adaptations to suit our problem setting. Thus, we build the connections between patients and multi-modal modalities and meanwhile incorporate patient phenotype connections for a fair comparison. Baseline hyperparameters are determined by parameter search. Besides, the model learning rate is selected from {0.01, 0.001, 0.0005} for MPI and all baseline models. ", "page_idx": 6}, {"type": "table", "img_path": "8B3sAX889P/tmp/ed2c59a47f5038fea03e5f0df106a4ca8b4b6edfd22441fe995b43adcfbcee3e.jpg", "table_caption": ["Table 2: Performance comparison for different models on varying dataset proportions. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8B3sAX889P/tmp/bbacb1e101b444818d4c6010d0843f5fcf38915da041c6f0490af78cebdf377d.jpg", "table_caption": ["Table 3: Ablation study of variants comparison on $30\\%$ and $100\\%$ of the dataset. ", "Evaluation Protocol. The discussion on the evaluation protocol can be found in the Appendix A.3. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Performance Comparison. Table 2 presents the performance of the MPI and baseline models trained with different proportions of the dataset. The best results are highlighted in bold, while the top baseline scores are underlined. The baselines based on imputation, including CMAE and SMIL, exhibit inferior performance. We attribute this to their reliance on modeling transformations from the hidden space to reconstruct the input features. The imputed data can be inaccurate due to the high dimensionality of the multi-modal data and the severity of missingness. GraphSage and GIN achieve competitive performance compared to both imputation-based models and the multi-modal learning approaches that explicitly handle missing data. The graph-based multi-modal models outperform GNNs in some cases; however, they are sometimes inferior to applying naive integration of clinical and biological data in naive GNNs. This may be due to the complexity and confilct between clinical and biological views. For example, GRAPE, which uses each feature dimension as a node, is not suitable for high-dimensional feature imputation. Additionally, M3Care computes patient similarity for each modality separately, thereby failing to explore cross-modality correlations. MUSE connects patients with modalities while representing each modality type as a node, possibly introducing dense and noisy edges. In contrast, MPI demonstrates improvements across all settings, verifying its capability to handle heterogeneity and noise through a decoupled view. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study. To validate the effectiveness of MPI and gain deeper insight into the contributions of each component in the proposed approach, we conduct ablation studies by comparing the following variants with the original MPI: (1) V1, which does not utilize the biological data and only model the correlation of patients and phenotypes. (2) V2, which only uses proteomics data and contrastive knowledge distillation. (3) V3, which solely leverages metabolomics data and contrastive knowledge distillation. (4) V4, which organizes biological factors, patients, and phenotypes in a single graph and does not require contrastive knowledge distillation. The results on $30\\%$ and $100\\%$ of the UK biobank dataset are summarized in Table 3. First, we observe that variant V1 is outperformed by both V2 and V3. This performance disparity arises since V2 and V3 effectively model the biological data and distill beneficial knowledge, thus enhancing phenotype imputation through knowledge distillation. Second, V4 is inferior to the proposed model MPI. This demonstrates that modeling biological data and phenotype data in separate graphs yields better performance compared to a single graph model. The likely reason for this is that multi-modal biological data often contain measurement inaccuracies and irrelevant information, which can impede accurate phenotype imputation Third, we observe that V3 exhibits superior performance compared to V2. We attribute this to the higher sparsity ratio of proteomics data relative to metabolomics data. The severe missing data issue in proteomics likely affects the performance of imputation. Lastly, compared to all variants, MPI demonstrates the best performance, highlighting the effectiveness of the proposed method. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The Impact of Codebook Settings. To analyze the impact of codebook settings on imputation performance, we varied the number and sizes of the codebooks and the results for the entire dataset are presented in Figure 3. First, as shown in Figure 3(left), MPI achieves optimal performance with three codebooks. A smaller number of codebooks, such as one or two, may fail to capture sufficient fine-grained information from the biological data. ", "page_idx": 8}, {"type": "text", "text": "Conversely, larger codebooks might introduce additional underlying factors due to finer granularity, which could reduce their discriminative power for patient profiling. Second, Figure 3(right) illustrates that the performance of MPI varies with changes in codebook sizes. The optimal codebook sizes for proteomics and metabolomics are 64 and 96, respectively. Smaller codebook sizes may fail to capture underlying biological factors, resulting in insufficient information for patient profiling. Conversely, larger codebook sizes might lead to certain codes being underutilized, which can hinder the overall optimization of the codebook. ", "page_idx": 8}, {"type": "image", "img_path": "8B3sAX889P/tmp/1523437421774712c6f8c271542b1ad0f25d026d5be62b523043e793ef26eed5.jpg", "img_caption": ["Figure 3: (Left) Results for varying the number of codebooks while keeping the codebook size fixed. (Right) Performance variation with changes in the codebook sizes while keeping a fixed number of codebooks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Sensitivity to Tradeoff Parameter. Figure 4 illustrates the impact of varying tradeoff parameters on the performance of MPI, evaluated on $30\\%$ and $100\\%$ of the dataset. The tradeoff parameter mediates between the contrastive knowledge distillation loss and the graph representation loss. The results indicate that MPI achieves optimal performance with a tradeoff parameter of 0.01. ", "page_idx": 8}, {"type": "text", "text": "Notably, when the tradeoff parameter is set to 0, the imputation performance largely declines. This is due to the disabling of knowledge distillation, which prevents the model from leveraging biological knowledge. Conversely, as the tradeoff parameter increases to a high value, the model\u2019s performance diminishes. The model might overly depends on biological knowledge and neglects the information from the collaborative view, leading to suboptimal outcomes. ", "page_idx": 8}, {"type": "image", "img_path": "8B3sAX889P/tmp/a27324a47a3ee20b5faf016251b769a62f7b3b6b3e522ab4143dadb4e5b3ba65.jpg", "img_caption": ["Figure 4: Effect of tradeoff parameter for MPI on $30\\%$ (left) and $100\\%$ (right) of the dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In conclusion, this work introduces a novel framework that leverages multi-modal data to enhance phenotype imputation, aiming for a more comprehensive medical evaluation. The proposed approach involves uncovering latent biological factors to enhance patient profiling and modeling correlations based on these factors. To mitigate the impact of noise and irrelevant information in biological data, we employ a cross-view contrastive knowledge distillation technique. Extensive experiments on a large-scale biomedical database demonstrate that our proposed method outperforms existing state-of-the-art approaches, showcasing its effectiveness and potential for improving biomedical data analysis and patient care. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The data utilized in this study were obtained through the UK Biobank Application Number 98304. The authors express their gratitude to all UK Biobank participants for their generous contribution of time to the study. This research is supported by NSF 2212175, NIH RF1AG084178, R01AG076448, R01AG080624, R01AG076234, R01AG080991 and RF1AG072449. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Evrim Acar and B\u00fclent Yener. Unsupervised multiway data analysis: A literature survey. IEEE Transactions on Knowledge and Data Engineering, 21(1):6\u201320, 2009.   \n[2] Ulzee An, Ali Pazokitoroudi, Marcus Alvarez, Lianyun Huang, Silviu Bacanu, Andrew J Schork, Kenneth Kendler, P\u00e4ivi Pajukanta, Jonathan Flint, Noah Zaitlen, et al. Deep learning-based phenotype imputation on population-scale biobank data increases genetic discoveries. Nature Genetics, 55(12):2269\u20132276, 2023.   \n[3] Thomas Beaney, Jonathan Clarke, David Salman, Thomas Woodcock, Azeem Majeed, Mauricio Barahona, and Paul Aylin. Identifying potential biases in code sequences in primary care electronic healthcare records: a retrospective cohort study of the determinants of code frequency. BMJ open, 13(9):e072884, 2023.   \n[4] Clare Bycroft, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T Elliott, Kevin Sharp, Allan Motyer, Damjan Vukcevic, Olivier Delaneau, Jared O\u2019Connell, et al. The uk biobank resource with deep phenotyping and genomic data. Nature, 562(7726):203\u2013209, 2018. [5] Donna J Cartwright. Icd-9-cm to icd-10-cm codes: what? why? how?, 2013. [6] Jiayi Chen and Aidong Zhang. Hgmf: heterogeneous graph-based fusion for multimodal data with incompleteness. In KDD, pages 1295\u20131305, 2020. [7] Lingyan Chen, James E Peters, Bram Prins, Elodie Persyn, Matthew Traylor, Praveen Surendran, Savita Karthikeyan, Ekaterina Yonova-Doing, Emanuele Di Angelantonio, David J Roberts, et al. Systematic mendelian randomization using the human plasma proteome to discover potential therapeutic targets for stroke. Nature communications, 2022.   \n[8] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015.   \n[9] Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng Sun. Gram: graph-based attention model for healthcare representation learning. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 787\u2013795. ACM, 2017.   \n[10] Anna Cichonska, Juho Rousu, Mikko Saarela, Kaisa Rantanen, Antti Honkela, Heikki Mannila, and Samuel Kaski. Computational methods for metabolomics: From the statistical analysis of datasets to integrative analysis. Briefings in Bioinformatics, 17(6):896\u2013908, 2016.   \n[11] Andrew Dahl, Valentina Iotchkova, Amelie Baud, \u00c5sa Johansson, Ulf Gyllensten, Nicole Soranzo, Richard Mott, Andreas Kranis, and Jonathan Marchini. A multiple-phenotype imputation method for genetic studies. Nature genetics, 48(4):466\u2013472, 2016.   \n[12] Timothy MD Ebbels, Justin JJ van der Hooft, Haley Chatelaine, Corey Broeckling, Nicola Zamboni, Soha Hassoun, and Ewy A Math\u00e9. Recent advances in mass spectrometry-based computational metabolomics. Current opinion in chemical biology, 74, 2023.   \n[13] Sara Nouri Golmaei and Xiao Luo. Deepnote-gnn: predicting hospital readmission using clinical notes and patient network. In Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, pages 1\u20139, 2021.   \n[14] Lovedeep Gondara and Ke Wang. Mida: Multiple imputation using denoising autoencoders. In AAAI, pages 116\u2013122, 2018.   \n[15] Varadraj P Gurupur, Paniz Abedin, Sahar Hooshmand, and Muhammed Shelleh. Analyzing the data completeness of patients\u2019 records using a random variable approach to predict the incompleteness of electronic health records. Applied Sciences, 12(21):10746, 2022.   \n[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. NeurIPS, 30, 2017.   \n[17] Nasir Hayat, Krzysztof J Geras, and Farah E Shamout. Medfuse: Multi-modal fusion with clinical time-series data and chest x-ray images. In Machine Learning for Healthcare Conference, pages 479\u2013503. PMLR, 2022.   \n[18] Laura Hetzel, David S Fischer, Stephan G\u00fcnnemann, and Fabian J Theis. Graph representation learning for single cell biology. Current Opinion in Systems Biology, 2021.   \n[19] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[20] Chenguang Hong, Elizabeth Rush, Mengling Liu, Dingsheng Zhou, Jimeng Sun, Adam Sonabend, Victor M Castro, Peter Schubert, Vijay A Panickan, and Tianxi Cai. Clinical knowledge extraction via sparse embedding regression (keser) with multi-center large scale electronic health record data. medRxiv, 2021.   \n[21] Hyun Kang. The prevention and handling of the missing data. Korean journal of anesthesiology, 64(5):402, 2013.   \n[22] Haewon Kwak, Myungsook Lee, Seon Yoon, Jinhyuck Chang, Sun Park, and Kyomin Jung. Drug-disease graph: Predicting adverse drug reaction signals via graph neural network with clinical data. In PAKDD, 2020.   \n[23] Doheon Lee, Xiaoqian Jiang, and Hongfang Yu. Harmonized representation learning on dynamic ehr graphs. Journal of Biomedical Informatics, 2020.   \n[24] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, pages 11523\u201311532, 2022.   \n[25] Qian Li, Xi Yang, Jie Xu, Yi Guo, Xing He, Hui Hu, Tianchen Lyu, David Marra, Amber Miller, Glenn Smith, et al. Early prediction of alzheimer\u2019s disease and related dementias using real-world electronic health records. Alzheimer\u2019s & Dementia, 19(8):3506\u20133518, 2023.   \n[26] Yi Li, Jiaheng Wang, Xiaoyi Zhang, and Ming Liu. Learning the graphical structure of electronic health records with graph convolutional transformer. AAAI, 34(04):4368\u20134375, 2020.   \n[27] Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzel. Learning to diagnose with lstm recurrent neural networks. In ICLR, 2016.   \n[28] Tong Liu, Yijun Wang, Yulong Wang, Enze Zhao, Yawen Yuan, and Zhaohui Yang. Representation learning of ehr data via graph-based medical entity embedding. NeurIPS Graph Representation Learning Workshop, 2019.   \n[29] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil: Multimodal learning with severely missing modality. In arXiv preprint arXiv:2103.05677, 2021.   \n[30] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil: Multimodal learning with severely missing modality. In AAAI, volume 35, pages 2302\u20132310, 2021.   \n[31] Sajad Mohammadi, Julian Davila-Velderrain, and Manolis Kellis. Reconstruction of cell-typespecific interactomes at single-cell resolution. Cell Systems, 2019.   \n[32] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In ICML, pages 689\u2013696, 2011.   \n[33] All of Us Research Program Investigators. The \u201call of us\u201d research program. New England Journal of Medicine, 381(7):668\u2013676, 2019.   \n[34] Shichao Pei, Ziyi Kou, Qiannan Zhang, and Xiangliang Zhang. Few-shot low-resource knowledge graph completion with multi-view task representation generation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1862\u20131871, 2023.   \n[35] Elie Rocheteau, Chang Tong, Petar Veli\u02c7ckovi\u00b4c, Nicholas Lane, and Pietro Li\u00f2. Predicting patient outcomes with graph representation learning. arXiv preprint arXiv:2103.13344, 2021.   \n[36] Joanne Ryan, Peter Fransquet, Jo Wrigglesworth, and Paul Lacaze. Phenotypic heterogeneity in dementia: a challenge for epidemiology and biomarker studies. Frontiers in public health, 6:181, 2018.   \n[37] Meghan I Short, Alison E Fohner, H\u00e5vard K Skjellegrind, Alexa Beiser, Mitzi M Gonzales, Claudia L Satizabal, Thomas R Austin, WT Longstreth Jr, Joshua C Bis, Oscar Lopez, et al. Proteome network analysis identifies potential biomarkers for brain aging. Journal of Alzheimer\u2019s Disease, 2023.   \n[38] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu. Learning to hash with graph neural networks for recommender systems. In the Web, pages 1988\u20131998, 2020.   \n[39] Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, and Nitesh V Chawla. Knowledge distillation on graphs: A survey. arXiv preprint arXiv:2302.00219, 2023.   \n[40] Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Patrick Brown, Trevor Hastie, Robert Tibshirani, David Botstein, and Russ B Altman. Missing value estimation methods for dna microarrays. Bioinformatics, 17(6):520\u2013525, 2001.   \n[41] Stef van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations in r. Journal of Statistical Software, 45(3):1\u201367, 2011.   \n[42] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30, 2017.   \n[43] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative filtering. In SIGIR, pages 165\u2013174, 2019.   \n[44] Henry Webel, Lili Niu, Annelaura Bach Nielsen, Marie Locard-Paulet, Matthias Mann, Lars Juhl Jensen, and Simon Rasmussen. Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning. Nature Communications, 2024.   \n[45] Lotta Wik, Niklas Nordberg, John Broberg, Johan Bj\u00f6rkesten, Erika Assarsson, Sara Henriksson, Ida Grundberg, Erik Pettersson, Christina Westerberg, Elin Liljeroth, et al. Proximity extension assay in combination with next-generation sequencing for high-throughput proteome-wide analysis. Molecular & Cellular Proteomics, 20, 2021.   \n[46] Patrick Wu, Aliya Gifford, Xiangrui Meng, Xue Li, Harry Campbell, Tim Varley, Juan Zhao, Robert Carroll, Lisa Bastarache, Joshua C Denny, et al. Mapping icd-10 and icd-10-cm codes to phecodes: workflow development and initial evaluation. JMIR medical informatics, 7(4):e14325, 2019.   \n[47] Zhenbang Wu, Anant Dadu, Nicholas Tustison, Brian Avants, Mike Nalls, Jimeng Sun, and Faraz Faghri. Multimodal patient representation learning with missing modalities and labels. In ICLR, 2023.   \n[48] Zhenbang Wu, Anant Dadu, Nicholas Tustison, Brian Avants, Mike Nalls, Jimeng Sun, and Faraz Faghri. Multimodal patient representation learning with missing modalities and labels. In ICLR, 2024.   \n[49] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR, 2018.   \n[50] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Gain: Missing data imputation using generative adversarial nets. In ICML, pages 1189\u20131198, 2018.   \n[51] Jiaxuan You, Xiaobai Ma, Yi Ding, Mykel J Kochenderfer, and Jure Leskovec. Handling missing data with graph representation learning. NeurIPS, 33:19075\u201319087, 2020.   \n[52] Zhu You et al. Handling missing modalities in multimodal representations using bipartite graphs. In ICLR, 2020.   \n[53] Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang, and Junfeng Zhao. M3care: Learning with missing modalities in multimodal healthcare data. In KDD, pages 2418\u20132428, 2022.   \n[54] Qiannan Zhang, Xiaodong Wu, Qiang Yang, Chuxu Zhang, and Xiangliang Zhang. Few-shot heterogeneous graph learning via cross-domain knowledge transfer. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2450\u20132460, 2022.   \n[55] Xinyu Zhang, Wenyi Hu, Yueye Wang, Wei Wang, Huan Liao, Xiayin Zhang, Katerina V Kiburg, Xianwen Shang, Gabriella Bulloch, Yu Huang, et al. Plasma metabolomic profiles of dementia: a prospective study of 110,655 participants in the uk biobank. BMC medicine, 2022.   \n[56] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. NeurIPS, 34:29476\u2013 29490, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Database ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We conduct the experimental evaluation for the proposed model and the baselines on the UK Biobank database [4], which is a lasting endeavor for biomedical research. The UK Biobank recruited half a million participants between 2006 and 2010, maintaining a collection of long-term EHRs from distributed health assessment centers with de-identified lifestyle and health information while retaining biological samples for detailed biological analyses. Chronic diseases frequently exhibit missing phenotypes due to mild or nonspecific initial symptoms. Routine data collection processes might overlook these subtle signs until more pronounced symptoms emerge. This can be particularly challenging in the context of neurodegenerative diseases like Alzheimer\u2019s Disease and Related Dementias (ADRD), where early detection is crucial for timely intervention and management. Research on ADRD particularly emphasizes early detection and intervention, which aligns well with the research goals of identifying historical phenotypes. Meanwhile, as one of the most common neurodegenerative diseases, ADRD cohorts might include a wide range of phenotypic expressions and stages of disease. This diversity is crucial for studying the full spectrum of phenotype presentation and identifying underlying missing signs. Therefore, we focus on phenotype imputation for populations suffering from chronic diseases and extracting a specific cohort of patients diagnosed with Alzheimer\u2019s disease and related dementia. ", "page_idx": 13}, {"type": "text", "text": "To build this cohort, we leverage the HESIN inpatient EHRs and the primary care EHRs from the UK Biobank. As the EHRs are collected from distributed places and organizations, the medical codings vary across different systems. Thus we standardize their variously formatted diagnosis records into uniform ICD codes [5], and fliter for patients with ADRD-related ICD codes, following methodologies employed in related research [25]. We further refine the cohort by removing patients whose ADRD onset occurred before or within one year of their biological sample collection, ensuring that the biological information and EHR data used in our analysis reflect the preclinical states of the disease and minimizing confounding factors post-diagnosis. For the extracted cohort, we eliminate any EHRs recorded after the ADRD onset dates, and preprocess the EHRs by converting recorded diagnoses and symptoms into distinct phenotypes [46]. We fliter out phenotypes with an occurrence of less than 20 while our cohort population reaches around 15000. The small occurrence $(0.06\\%)$ reflects the less practical value in this work of imputing these phenotypes, and meanwhile, their rarity often introduces noise rather than providing valuable insights. Besides, there are a few phenotypes with quite high frequency (e.g., hypertension). Since ADRD generally focuses on the elderly population, the widespread prevalence typically indicates a low specificity and can be regarded as possible confounders due to aging. These phenotypes may dominate the dataset, potentially obscuring other important associations, whereas focusing on moderately prevalent phenotypes could uncover more subtle associations. ", "page_idx": 13}, {"type": "text", "text": "Beyond EHR data, proteomic analysis has been conducted on blood plasma samples from over 56,000 UK Biobank participants. Enabled by Olink\u2019s Proximity Extension Assay (PEA) [45], this analysis measured the abundance of nearly 3,000 circulating proteins. Additionally, the UK Biobank measures around 250 metabolic biomarkers in EDTA plasma samples from approximately 280,000 participants. These biomarkers span multiple metabolic pathways, including lipoprotein lipids, fatty acids, and low-molecular-weight metabolites. Since biological processes could begin years before the onset of clinical symptoms, proteomics and metabolomics which comprise the end-product of genes, transcripts, and protein regulations, offer insights into identifying alterations in multiple biochemical processes and the risk of ADRD among cognitively healthy adults [55, 37]. We leverage biological data across the two modalities of proteomics and metabolomics. Specifically, proteomics data are provided as Normalized Protein eXpression (NPX) values, obtained after UK Biobank preprocessing, which includes median centering normalization between plates and log transformation. We used these NPX values directly as the encoder input without further processing [7]. For metabolomics, we applied a natural logarithmic transformation $(\\ln(x+1))$ to all metabolite values, followed by Z-transformation [55]. Owing to the resource-intensive nature of these tests and the random unavailability for certain patients, we observe significant modality missingness at random: approximately $90\\%$ in proteomics and $50\\%$ in metabolomics. ", "page_idx": 13}, {"type": "text", "text": "A.2 Addiontal Details on Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 CMAE [32] employs a cross-modality auto-encoder to address missing modalities. Initially, a subset of patients who have complete modalities is sampled where CMAE is trained to reconstruct a purposely masked-out modality. After training, the CMAE model is used to fill in missing modalities for all patients. We use the imputed modality information to perform downstream phenotype imputation via ranking objective.   \n\u2022 SMIL [30] integrates Bayesian meta-learning techniques to modify the latent feature space, enabling embeddings with missing modalities to closely resemble those with complete modalities. SMIL estimates the missing modality using a weighted sum of modality priors based on the complete modalities. We adopt the same strategy as CMAE to use SMIL to perform downstream phenotype imputation.   \n\u2022 GraphSage [16] is evaluated by learning on the bipartite graph built from EHRs to form the patient-phenotype graph. The built graph will directly leverage the multi-modal biological information as the node features for the patient nodes, where missing biological information is represented as zeros vectors. This baseline serves as a naive combination of clinical data and biological data via joint modeling.   \n\u2022 GIN [49] follows the same setup with Graphsage when evaluated. We use the ranking loss to train the Graphsage and GIN baselines.   \n\u2022 GRAPE [51] infers missing features by building a bipartite graph to include patients and individual feature dimensions as the graph nodes. The value of the feature is regarded as an edge attribute, where the target is to predict the value assigned to each edge. In this work, around 3000 proteomic features and 250 metabolomics features are included in the graph alongside the patient nodes. We meanwhile include the phenotype nodes and their connections with patients for a fair comparison.   \n\u2022 M3Care [53] aims for patient representation learning. It calculates patient similarity within each modality and constructs a similarity graph for each modality respectively. Afterward, overall patient similarities by averaging the similarities from each modality are utilized to model crosspatient interactions by GNNs. The embeddings for each patient across different modalities are then aggregated using a Transformer head. We leverage the learned patient representations in the same way with CMAE and SMIL.   \n\u2022 MUSE [47] models the patient-modality relationship in a bipartite graph, where patients and modalities constitute the graph nodes, and modality features serve as edges between them. MUSE applies a Siamese GNN on the bipartite graph and its augmented graph that is obtained via random edge dropout. We also incorporate phenotype nodes in MUSE in a similar manner as in GRAPE to address our specific problem and ensure a fair evaluation. ", "page_idx": 14}, {"type": "text", "text": "A.3 Evaluation Protocol ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We randomly hold out $10\\%$ of the patient-phenotype interactions as the testing set and train a model on the remaining interactions following previous works [43, 38, 56]. From the training set, we randomly select $10\\%$ of the interactions as the validation set to monitor the training process and help early stop. For each observed patient-phenotype interaction, we treat it as one positive pair, while negative instances are sampled from negative phenotypes with which the patient has no interactions. Upon training our model, we generate personalized ranking lists for each patient in the test set, where these lists rank the phenotypes not observed for each patient during training. To evaluate our model\u2019s effectiveness, we assess performance using Hit Ratio at specific thresholds $(\\operatorname{Hit}@10\\$ , Hit $@20$ , Hi $\\mathbb{\\left(o50\\right)}$ ) and Mean Reciprocal Rank (MRR). Hit Ratio, as a recall-based metric, measures whether the test phenotype appears within the top- $\\mathcal{K}$ list. The MRR is position-sensitive, assigning higher weight to hits that occur at higher ranks. Higher values for both metrics indicate better performance. We report the average scores and their standard derivations on the testing set over three random runs. To assess the effectiveness of the proposed model with varying dataset sizes, we evaluate its performance on different proportions of the dataset: $30\\%$ , $50\\%$ , $70\\%$ , $90\\%$ , and $100\\%$ The extraction of different dataset proportions is based on sampling patient-phenotype edges in the graph $\\mathcal{G}_{p}$ . Specifically, for each patient, we sample the required proportion of edges connected to phenotypes. ", "page_idx": 14}, {"type": "text", "text": "1: Input: Patient multi-modal data $\\mathbf{X}^{M}$ , Codebook $\\mathcal{C}$ , Encoder E, Decoder D, GCNs;   \n2: Output: Patient representation $\\mathbf{H}$ ;   \n3: for each iteration do   \n4: for each patient $\\mathbf{x}^{m}$ in $\\mathbf{X}^{M}$ do   \n4: $\\mathbf{z}^{m}:=\\mathbf{\\bar{E}}(\\mathbf{x}^{m})$ ;   \n4: Retrieve disentangled biological factors $(c_{0},\\ldots,c_{l-1})$ from Codebook $\\mathcal{C}$ ;   \n4: Obtain the quantized vector $\\begin{array}{r}{\\hat{\\mathbf{z}}^{m}:=\\sum_{d=0}^{l-1}\\mathbf{e}_{c_{d}}}\\end{array}$ ;   \n4: Reconstruct the input $\\mathbf{x}^{m}$ based on $\\hat{\\bf x}^{m}={\\bf D}(\\hat{\\bf z}^{m})$ ;   \n5: end for   \n5: Optimize loss in Eq.(1);   \n6: end for   \n6: Obtain the disentangled biological factors $\\mathcal{C}$ ;   \n6: Patient-Phenotype Graph $\\mathcal{G}_{p}$ Construction;   \n6: Patient-Factor Graph $\\mathcal{G}_{f}$ Construction;   \n7: for each iteration do   \n7: Learn node representation $\\mathbf{H}_{p}$ and $\\mathbf{H}_{f}$ for graph $\\mathcal{G}_{p}$ and $\\mathcal{G}_{f}$ using GCNs in Eq.(2), respectively;   \n7: Optimize loss in Eq.(3) and Eq.(4);   \n8: end for ", "page_idx": 15}, {"type": "text", "text": "A.4 Time Complexity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The time complexity of data quantization for each patient is composed of three primary components. The first component is the encoder, which has a time complexity of $O(\\bar{D}\\cdot F)$ , where $D$ represents the input dimensionality and $F$ is a factor that depends on the number of layers and the operations performed within the encoder. The second component is vector quantization, with a time complexity of $O(K\\cdot d)$ , where $K$ denotes the number of entries in the codebooks and $d$ represents the dimensionality of latent embeddings. The third component is the decoder, which has a time complexity of $O(d\\cdot G)$ , where $G$ is a factor related to the number of layers and operations in the decoder. Consequently, the overall time complexity of data quantization can be expressed as $O(D\\cdot F+K\\cdot d+d\\cdot\\bar{G})$ . Given that both the encoder and the decoder in this study are implemented as multi-layer perceptrons (MLPs), we simplify the expression to $O(D\\cdot d+K^{\\cdot}\\cdot d+d^{\\dot{2}})$ for ease of calculation. Then the updating of GNNs in each iteration mainly involves the updating of node vectors and weight matrices, whose time complexity is $O(n_{t}\\cdot d^{2}+z\\cdot d)$ , where $n_{t}=n+m$ and $z$ are the total number of nodes and the total number of edges in graph $\\mathcal{G}_{f}$ and $\\mathcal{G}_{p}$ , respectively. $d$ is the embedding dimensionality. Lastly, the time complexity of cross-view contrastive knowledge distillation for each patient is $O(d\\cdot N)$ where $N$ denotes the number of negative patients. Therefore the time complexity of MPI is $O(D\\cdot d+K\\cdot d+d^{2}+n_{t}\\cdot d^{2}+z\\cdot d+{\\bar{d}}\\cdot{\\bar{d}})$ . Since $K\\ll D$ , $d\\ll D$ , $N\\ll D$ , and $D\\ll z$ , the time complexity simplifies to $O(n_{t}\\cdot d^{2}+z\\cdot{d})$ which is linear with $(n_{t}\\cdot d^{2}+z\\cdot d)$ , depending on the number of nodes and edges in the constructed graphs. It is well-known that canonical GCNs are not characterized by high time complexity, indicating the efficiency and scalability of our model. ", "page_idx": 15}, {"type": "text", "text": "A.5 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The current research primarily focuses on two modalities. Future work will explore the incorporation of additional modalities. Another limitation is the selected patient cohort, as this study concentrates on Alzheimer\u2019s disease and related dementias. To enhance the generalizability of our findings, we aim to apply the proposed model to a broader range of patient cohorts and various downstream tasks. ", "page_idx": 15}, {"type": "text", "text": "A.6 Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Phenotype imputation using biological data can advance healthcare by enabling a deeper understanding of diseases and patients\u2019 health states. It helps aids early diagnosis and personalized treatments, leading to better health outcomes. However, there are potential risks, including the possibility of exacerbating health disparities if data is not diverse, and the risk of inaccurate imputations leading to erroneous conclusions. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: In the abstract and Section 1, we show our contributions and scope. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We declare in A.5. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: NA. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We present our experiment results in Section 5, the details and parameter settings in Section 5.1. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: https://github.com/aslandery/MPI. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The training and the test details can be found in Section 5.1 and Appendix. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In Section 5.1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In Appendix A.6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our model does not have a high risk for misuse. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have obtained authorization from the UK Biobank (UKBB) to use their data. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No new asset is proposed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]