{"references": [{"fullname_first_author": "Su", "paper_title": "Rectified rotary position embeddings", "publication_date": "2023-01-01", "reason": "This paper introduces ReRoPE, a weave PE method that improves extrapolation performance without requiring additional training, which is a key concept in the current paper."}, {"fullname_first_author": "Kazemnejad", "paper_title": "The impact of positional encoding on length generalization in transformers", "publication_date": "2023-05-01", "reason": "This paper challenges the prevailing assumption that positional encoding is essential for long-context understanding, which is a central argument addressed and expanded upon in the current paper."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This foundational paper introduces the concept of in-context learning in large language models (LLMs), which is the basis for the extrapolation problem discussed in the current paper."}, {"fullname_first_author": "Press", "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation", "publication_date": "2021-08-01", "reason": "This paper introduces ALiBi, a relative positional encoding method, that is compared to the proposed method in the current paper for extrapolation performance."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This seminal paper introduces the Transformer architecture, which forms the foundation for most modern LLMs and is the basis for the positional encoding methods discussed in the current paper."}]}