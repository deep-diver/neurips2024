[{"type": "text", "text": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xin Ma1, Yang Liu2,3,\u2217 Jingjing Liu2, Xiaoxu Ma1 ", "page_idx": 0}, {"type": "text", "text": "1Digital Research Institute, Enn Group, Beijing, China 2Institute for AI Industry Research, Tsinghua University, Beijing, China 3Shanghai Artificial Intelligence Laboratory, China {xin.ma0206, xiaoxuma}@gmail.com, {liuy03, jjliu}@air.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why No Position Encoding (NoPE) fails outside its effective range, as well as examining the power of Position Encoding (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with weave $P E$ can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, MesaExtrapolation, which utilizes a chunk-based triangular attention matrix and applies Stair $P E$ to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs\u2019 applicative reach. Our code is available at https: //github.com/soacker/Mesa-Extrapolation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) with their powerful in-context learning capabilities Brown et al. (2020) offer versatile solutions to a wide-range of intelligent applications. However, one pressing challenge, the extrapolation problem Del\u00e9tang et al. (2022) Zhang et al. (2022), dictates that the inference ability of LLMs sharply declines beyond their max training lengths, imposing a serious limitation on applications with long inputs. An naive solution is to extend the length of training samples. However, the inherent quadratic complexity of calculations presents practical challenges, demanding more resources, longer training time and higher cost. ", "page_idx": 0}, {"type": "text", "text": "Positional encoding (PE) has become a pivotal component of Transformer architecture, to compensate for the overlooking of position information by the attention mechanism. In the realm of extrapolation capability, PE is considered as a key factor influencing the extrapolating ability of LLMs. A few PE approaches, such as the popular RoPE Su et al. (2023) and ALiBi Press et al. (2021), claim to offer improved extrapolation capabilities and have gained widespread usage in industrial applications. Meanwhile, a counter-narrative has emerged. Some works demonstrate that Transformer can achieve better extrapolation capabilities by removing position encoding (NoPE) Kazemnejad et al. (2023), and contend that the mask already plays a significant role in capturing position information. ", "page_idx": 0}, {"type": "text", "text": "Inspired by Kazemnejad et al. (2023), we conduct a thorough investigation of the extrapolation problem by designing a specific Transformer model for this purpose and presenting detailed theoretical analysis. To the best of our knowledge, this is the first theoretical endeavor to understand the inner workings of extrapolation. We first elucidate the cause of NoPE\u2019s failure when input exceeds the effective window length in Theorem 3.1, and analyse PE\u2019s failure in 3.2. Building upon Theorem 3.2, we prove in Theorem 3.3 that through meticulous weave position, (i.e., weave $P E$ ), it is feasible to achieve extrapolation beyond the effective window length. ", "page_idx": 1}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/9e869d91153ef5b773df605338be5aeb54b08ec5a1194b331164b42ea7225933.jpg", "img_caption": ["Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We further introduce a novel weave-PE-based method, Mesa-Extrapolation, which demonstrates substantial extrapolation prowess without the need for additional training. Specifically, we use chunk-based triangular attention matrix, aiming at memory-friendly resource consumption (Figure 1 left), and we employ Stair PE to handle the last chunk (Figure 1 right below), effectively making Mesa-Extrapolation a completely free plug-in for LLMs. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. Theoretical Analysis We provide a comprehensive theoretical analysis on the challenges encountered by NoPE beyond its effective window length, and investigate the effect of PE in this context. Our theorems prove that through meticulous weave position, PE can be effectively extrapolated beyond the effective window length. This theoretical foundation establishes a clear understanding of the extrapolation problem and potential solutions for LLMs utilizing PE.   \n2. Introduction of Mesa-Extrapolation We introduce a novel extrapolation approach called \"Mesa-Extrapolation.\" Based on triangular attention matrix, this method strategically organizes input tokens into chunks, with the final chunk employing a weave PE method (e.g., Stair PE) to integrate all token states. Mesa-Extrapolation provides a practical and effective solution to the extrapolation problem.   \n3. Empirical Validation Comprehensive experiments demonstrate that our approach achieves competitive performance, offering the benefits of extremely low memory usage and the fastest inference speed compared to existing methods. Importantly, it is an easy plugin and can enhance extrapolation performance of LLMs without any additional resource consumption. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Since the self-attention mechanism itself does not contain position information, PE components have become an integral part of the Transformer architecture. Cmmon choices for PE are either absolute, where each absolute position (e.g., 1, 2, 3, ...) is directly represented, or relative, where the distance between tokens is used as positional information. Absolute Position Embedding (APE) embeds each absolute position $i$ into position vector $\\mathbf{\\omega}_{p_{i}}$ and adds word embeddings to their corresponding $\\textstyle p_{i}$ , before feeding them to the LLMs Vaswani et al. (2017). However, the extrapolation ability is limited because it cannot generalize to unseen positions. RoPE Su et al. (2023) rotates the query and key vectors with an angle proportional to their absolute positions, so the attention dot production only depends on relative distance between tokens, providing a relative positional encoding. T5\u2019s Relative Bias first maps the relative distance $(i-j)$ between tokens at positions $i$ and $j$ to a scalar bias value $b\\,=\\,f(i\\,-\\,j)$ , where $f$ is a lookup table. ALiBi is similar to T5\u2019s Relative Bias but instead subtracts a scalar bias from the attention score Press et al. (2021)(refer to Appendix D.2 for more details). Recent works such as ReRoPE and Leaky-ReRoPE Su (2023b) achieve effective extrapolation without fine-tuning by meticulously weaving relative positions of RoPE. We refer to this class of methods that achieve extrapolation by weaving the relative positions of PE without fine-tuning as Weave $\\pmb{P E}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Kazemnejad et al. (2023) indicates that the decoder-only transformer with position encoding removed (NoPE) demonstrates stronger extrapolation capabilities. Furthermore, it theoretically shows that a specific transformer model can get relative and absolute positional information, even in the absence of PE. Haviv et al. (2022) also demonstrates NoPE achieves comparative performance to standard Transformer models. These new studies pose a key challenge regarding the choice of whether using PE or not in Transformer architecture (refer to Appendix A for more related works). ", "page_idx": 2}, {"type": "text", "text": "3 Model Extrapolation: NoPE vs. Weave PE", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We mainly consider relative PE methods and formally define their self-attention dot product as a function $f_{\\mathrm{PE}}$ , which takes the query $\\pmb q_{t}$ located on position $t$ , the key $\\pmb{k}_{i}$ located on position $i$ , and their relative positions $t-i$ as input parameters, as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{PE}}(q_{t},k_{i},t-i),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f_{\\mathrm{PE}}$ denotes a relative PE method such as RoPE or ALiBi. ", "page_idx": 2}, {"type": "text", "text": "For ALiBi,", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle\\pmb{q}_{t},\\pmb{k}_{i}\\rangle:=f_{\\mathrm{ALiBi}}(\\pmb{q}_{t},\\pmb{k}_{i},t-i)=\\pmb{q}_{t}^{T}\\pmb{k}_{i}-(t-i)\\cdot C^{m+1},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where m is head index and C = 2\u22122\u2212log2(#head ", "page_idx": 2}, {"type": "text", "text": "For NoPE,", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle\\pmb q_{t},\\pmb k_{i}\\rangle:=\\pmb q_{t}^{T}\\pmb k_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Based on Equ.1 we formally define weave PE as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{weavePE}}(q_{t},k_{i},t-i)=f_{\\mathrm{PE}}(q_{t},k_{i},\\mathcal{W}(t-i)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{W}$ is a weave function which takes the relative position $t-i$ as input parameter. ", "page_idx": 2}, {"type": "text", "text": "For example, ReRoPE Su (2023b) can be considered as an example of weave PE, with its $\\mathcal{W}$ function defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(t-i):=\\left\\{\\begin{array}{r l r}{t-i}&{{},}&{t-i\\leq N}\\\\ {N}&{{},}&{t-i>N}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $N$ is a constant. ReRoPE\u2019s dot-product attention is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{ReRoPE}}(q_{t},k_{i},t-i)=f_{\\mathrm{RoPE}}(q_{t},k_{i},\\mathcal{W}(t-i))=q_{t}^{T}R^{\\mathcal{W}(t-i)\\theta}k_{i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $R$ is a rotation matrix that rotates $\\mathcal{W}(t-i)\\theta$ radians. This is based on RoPE\u2019s dot-product attention: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{RoPE}}(q_{t},k_{i},t-i))=q_{t}^{T}R^{(t-i)\\theta}k_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3.2 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Chen et al. (2023) explores the evolution of hidden state values and reveals a noticable phenomenon: as the position increases, the hidden state values will explode. This finding appears consistent with observed failures in extrapolation. Through probe experiments (refer to Appendix F), we investigate the alterations in hidden state values across various positions and layers. Results show a significant shift in the hidden state\u2019s value range upon surpassing the effective window. Interestingly, when employing extrapolation techniques, hidden state values exhibit noticeable suppression. This indicates that the effective range of hidden state values likely lies within a specific threshold. When the position exceeds the effective window length, the hidden state values surpass this threshold, resulting in extrapolation failures. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Previous work Kazemnejad et al. (2023) utilizes a constructive approach. By constructing the Transformer\u2019s weights, it enables the first and second layers to independently generate position information. Drawing inspiration from this, we endeavor to construct a Transformer model capable of mirroring this observation. ", "page_idx": 3}, {"type": "text", "text": "3.3 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In a multi-layer neural network, each layer\u2019s outputs, a.k.a hidden state values $o$ , become the inputs for the subsequent layer. To maintain stable network behavior, these values must remain within a reasonable range. We define this observable boundary as the threshold $\\mathcal{H}$ . This threshold can be either an upper bound or a lower bound. For our analysis, we focus on the lower bound of this threshold. A successful extrapolation occurs when a large model consistently generates accurate next tokens for a long input sequence. Conversely, a failed extrapolation happens when the model produces incorrect or nonsensical next tokens. Based on these definitions, we make the following assumptions: ", "page_idx": 3}, {"type": "text", "text": "Assumptions. In LLM, there is a lower bound as threshold $\\mathcal{H}$ for the hidden state value $o$ in specific dimension and specific layer. Let $M$ be the max window length for LLM. Predefine query $W_{Q}$ , key $W_{K}$ , value $W_{V}$ and output $W_{O}$ matrices, and feed-forward sub-layer $W_{1}$ , ", "page_idx": 3}, {"type": "text", "text": "$W_{2}$ matrices. When $o>\\mathcal{H}$ , LLM extrapolates successfully. Once $o<\\mathcal{H}$ , LLM extrapolation fails. ", "page_idx": 3}, {"type": "text", "text": "These assumptions indicate that by observing whether the hidden state value $o$ in this dimension exceed the threshold $\\mathcal{H}$ , we can predict whether the large model\u2019s extrapolation has failed. Building upon these assumptions, theoretical results for NoPE exceeding the effective window length are as follows: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (NoPE Extrapolation). Let $x=[<b o s>,x_{1},...\\,,x_{T}]$ be an input sequence of length $T+1$ to the model. Then, there exists $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices, such that when $T<M$ , $o_{T}>\\mathcal{H}$ ; and when $T>M$ , $o_{T}<\\mathcal{H}$ . ", "page_idx": 3}, {"type": "text", "text": "Full proof is given in Appendix E.1. This theorem reveals the internal mechanism of NoPE extrapolation as the input length changes. The theoretical results for PE are as follows: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2 (PE Extrapolation). Let $x=[<b o s>,x_{1},...\\,,x_{T}]$ be an input sequence of length $T{+}I$ to the model. Consider a simple relative $P E$ schema where dot product between query $\\pmb q_{t}$ and key $k_{i}$ at positions $t$ and $i$ $(t\\geq i)$ can be expressed as: $\\langle\\pmb q_{t},\\pmb k_{i}\\rangle:=\\pmb q_{t}^{T}\\pmb k_{i}-(t-i)$ . Then, there exists $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices, such that when $T<M$ , $o_{T}>\\mathcal{H}$ ; and when $T>M$ , $o_{T}<\\mathcal{H}$ . ", "page_idx": 3}, {"type": "text", "text": "Full proof is given in Appendix E.2. Theorems 3.1 and 3.2 state the failure of length extrapolation in NoPE and PE, respectively. ", "page_idx": 3}, {"type": "text", "text": "Building on Theorem 3.2, we further investigate the case for carefully orchestrated weave PE. The theoretical result is as follows: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3 (Weave PE Extrapolation). Let $N$ be a positive constant. Consider a simple weave $P E$ extrapolation schema: when $t-i<N$ , $\\dot{\\mathcal{W}}(t-i)=t-i,$ ; and when $t-i\\geq N$ , $\\mathcal{W}(t-i)=N$ . Then, the attention dot product is fixed as below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=\\left\\{\\begin{array}{r l r}{q_{t}^{T}k_{i}-(t-i)}&{{},}&{t-i<N}\\\\ {q_{t}^{T}k_{i}-N}&{{},}&{t-i\\geq N}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": ", where $N\\ll M$ . Then, applying $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices from Theorem 3.2, we have when $T>M$ , $o_{T}>\\mathcal{H}$ . ", "page_idx": 3}, {"type": "text", "text": "Full proof is given in Appendix E.3. This theorem suggests that for existing LLMs relying on PE, simply weaving its relative positional encoding can effectively extend the input window. ", "page_idx": 4}, {"type": "text", "text": "Through Theorem 3.1 and Theorem 3.2, we formulate pertinent theoretical models for NoPE and PE, respectively, shedding light on the intricate relationship between extrapolation and the effective window length. Building upon these findings, Theorem 3.3 delves deeper into the realm of explicit PE, revealing that a well-designed weave PE scheme can effectively broaden the original effective window length. The theorems show the existence of an effective length $M$ , which is typically related to the maximum training window length of the LLM. Furthermore, within the proof of Theorem 3.3, our results indicate that $N\\ll M$ , consistent to experimental findings in Su (2023b), where although the extrapolation position can theoretically start from 2048, the best extrapolation starting position is 512. More experimental parameters also validate this setting (refer to Appendix B.2). ", "page_idx": 4}, {"type": "text", "text": "3.4 Validating Extrapolation Using Observed Thresholds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Further, we design probe experiments (refer to Appendix F.3 for more results) to validate the observed phenomena and our theorems, as shown in Figure 2. From Figure 2, two observations are noted: Firstly, for hidden state values of the same dimension, the first layer undergoes minimal change, while the second layer exhibits a more pronounced transition. Exceeding the threshold implies extrapolation failure. This observation aligns with our theoretical model construction, where the first layer primarily refines positional information, with significant signal changes occurring in the second layer, as demonstrated in Theorem 3.2. Secondly, based on observational thresholds, when the length of the input sequence is around 12k, the values of hidden state corresponding to Dynamic-NTK Liu et al. (2023) surpass the threshold, implying extrapolation failure. Conversely, for ReRoPE Su (2023b), extrapolation succeeds. These two predictive outcomes corroborate with subsequent experimental results. ", "page_idx": 4}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/149988f9284767c2e045d0f2ea9ca0bf9d65a4543e70eaba3866d468ca81ecf6.jpg", "img_caption": ["Figure 2: Thresholds for hidden states observed at specific dimensions on LLaMA2-7B-Chat, allowing for extrapolative judgments based on these thresholds. The vertical black dashed line indicate the position of maximum training length of the model. In this case, it is 4k for LLaMA2-7B-Chat model. The hidden state value at this position is designated as the observed threshold and marked with a horizontal red dashed line. When the hidden state value exceeds the red dashed line as the position changes, it signifies that the hidden state value has surpassed the threshold, suggesting a failure in extrapolation after that position. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Mesa-Extrapolation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we begin by introducing a novel weave position encoding method, termed Stair Position Encoding (Stair PE). Following this, we propose a chunk-based triangular attention matrix. Building on these concepts, we introduce the implementation of Mesa-Extrapolation. Lastly, we discuss the theoretical properties of these innovations. ", "page_idx": 4}, {"type": "text", "text": "4.1 Stair PE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the concept of weave PE in Equ.2, we define a novel weave PE method, namely Stair PE as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{S t a i r P E}(q_{t},k_{i},t-i)=f_{P E}(q_{t},k_{i},W(t-i)),\\;\\mathrm{~and~}\\mathcal{W}(t-i):=\\left\\{\\begin{array}{r l r}{t-i}&{{},}&{t-i\\leq N}\\\\ {I}&{{},}&{t-i>N}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{I\\mathrm{~=~}N+\\left\\lceil\\frac{t-i-N}{E}\\right\\rceil}\\end{array}$ . $N$ denotes the extrapolated position, and $E$ denotes the extrapolated width. Both $N$ and $E$ are positive constants. Stair PE can be applied to existing relative PEs such as RoPE and ALiBi. For example, for RoPE: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{StairRoPE}}(q_{t},k_{i},t-i)=f_{\\mathrm{RoPE}}(q_{t},k_{i},\\mathcal{W}(t-i))=q_{t}^{T}R^{\\mathcal{W}(t-i)\\theta}k_{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Taken a sequence of length 10 as an example, the right subplot of Figure 1 shows that the relative positions generated by PE (Both RoPE and ALiBi) are linear, while those generated by Stair PE (here $N=4$ and $E=2$ ) are non-linear. Since weave PE changes the linear relative position, it has to calculate the attention matrix more than once Su (2023b). Compared with ReRoPE (detailed on Appendix B.3), Stair PE provides a finer-grained extrapolated positions. Meanwhile, compared with Leaky-ReRoPE, Stair PE reuses known positions, reducing possible generalization errors. We also provide an ablation experiment to compare these weave PE methods (refer to Appendix C.6). ", "page_idx": 5}, {"type": "text", "text": "While our work was conducted independently, we note that Jin et al. (2024) have recently explored a similar idea through flooring the original positions and obtaining the relative position matrix with grouped self-attention. Although the two parallel thought processes are different, under certain conditions their formulations are equivalent (refer to Appendix G). Consequently, Self-Extend can be categorized as a Weave PE method. Our proposed Chunk-based Triangular Attention Matrix (detailed in Section 4.2) and its corresponding theoretical properties (Section 4.4) are also applicable to this parallel approach. ", "page_idx": 5}, {"type": "text", "text": "4.2 Chunk-based Triangular Attention Matrix ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We design a chunk-based triangular attention matrix as shown in the left subplot of Figure 1. To achieve approximate linear memory consumption and computational speed, we further split the triangular attention matrix into several chunks and concatenate these chunks. We segment the input sequence into several sub-sequences according to DynamicSplit function (defined in Appendix 2), which divides a sequence into sub-sequences of equal length, with the exception of the first and last sequences. The length of each sub-sequence is determined by both the input token length and the max training length. Each of the generated sub-sequences then undergoes a self-attention operation to generate a corresponding chunk. That is, a sub-sequence of length $l$ will generate a corresponding chunk with the size $l\\times l$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Mesa-Extrapolation mainly utilizes the chunkbased triangular attention matrix and Stair PE. Notice that regular PE (such as RoPE or ALiBi) is applied to all chunks except for the last chunk, for which Stair PE is applied. For the last chunk, all previous chunks are concatenated, and Stair PE is used to rearrange relative positional encoding to achieve extrapolation beyond the effective window length. ", "page_idx": 5}, {"type": "text", "text": "In summary, the process of Mesa-Extrapolation mainly contains four steps (Algorithm 1): The first three steps correspond to the prefill stage, which is used to calculate all input tokens. The last step corresponds to the decoding stage, which is used to generate next-token one by one. ", "page_idx": 5}, {"type": "text", "text": "Firstly, DynamicSplit function segments the input sequence, and the first segmented subsequence is fed into LLM to generate the first attention matrix chunk (line 3 in Algo.1). Secondly, subsequential sequences are iteratively processed while simultaneously feeding the key ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Mesa-Extrapolation Algorithm   \nRequire: DynamicSplit, LLM, StairPE   \nInput: $s[0:T-1]$ (input tokens with length T)   \nOutput: $s[T,T+1,\\ldots]$   \n# Prefill Stage   \n1: first_length, chunk_width $\\leftarrow$ DynamicSplit(s)   \n2: $K.$ _cache, $V\\_c a c h e\\gets[],[]$   \n3: first_ $K$ , $f i r s t\\_V\\leftarrow\\mathrm{LLM}(s[0:f i r s t\\_l e n g t h])$   \n4: Append first_ $K$ to $K.$ _cache, first_V to   \n$V.$ _cache   \n5: $i\\leftarrow f$ irst_length   \n6: while i < T \u22121 \u2212chunk_width do   \n7: $K,V\\gets\\mathrm{LLM}(s[i:i+c h u n k\\_w i d t h],f i r s t\\_K$ ,   \n$f i r s t\\_V)$   \n8: $K_{\\perp}$ _cache append $K$ , $V$ _cache append $V$   \n9: i \u2190i + chunk_width   \n10: end while   \n11: apply StairPE to fix positions   \n12: $K$ , $V\\leftarrow\\mathrm{LLM}(s[i:T-1]$ , K_cache, V _cache)   \n# Decoding Stage   \n13: apply Stair $P E$ to fix positions   \n14: generate next-token one by one ", "page_idx": 5}, {"type": "text", "text": "and value pairs of the first chunk into LLM to generate subsequent chunks (line 6-10 in Algo.1). Thirdly, the last sub-sequence is processed by concatenating the key and value pairs of all previous chunks together and using Stair PE to modify the relative positional encoding. Then, it is fed into the LLM to produce the last chunk (line 11-12 in Algo.1). Finally, Stair PE is applied to process the current token and cached Key and Value pairs to generate next-token one by one (line 13-14 in Algo.1). We establish the effectiveness of our proposed Mesa-Extrapolation in the next section. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.4 Theoretical Properties ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 establishes a theoretical measurement for evaluating the effectiveness of extrapolation. We consistently apply this indicator, along with adjustments in relative positioning using Stair PE, to validate the feasibility of extrapolation. The result is as below: ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.1 (Mesa Extrapolation). Let $N$ be a positive constant. Consider a simple Stair $P E$ extrapolation schema, and the attention dot product is fixed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{stairPE}}(q_{t},k_{i},t-i)=\\left\\{\\begin{array}{r l r}{q_{t}^{T}k_{i}-(t-i)}&{{},}&{t-i<N}\\\\ {q_{t}^{T}k_{i}-I}&{{},}&{t-i\\geq N}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $N\\ll M$ , $\\begin{array}{r}{I=N+\\left\\lceil\\frac{t-i-N}{E}\\right\\rceil}\\end{array}$ , and the extrapolated width $E$ is a constant. Then, Apply $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices from Theorem 3.2. Although $T>M$ , it still $o_{T}>\\mathcal{H}$ . ", "page_idx": 6}, {"type": "text", "text": "Full proof is provided in Appendix E.4. We prove that Mesa-Extrapolation can effectively extrapolate outside the max window length. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we validate the performance of Mesa-Extrapolation through experiments measured over multiple metrics. We choose GovReport Huang et al. (2021), Pile Gao et al. (2020), LongBench Bai et al. (2023), and LongEval Krishna et al. (2023) datasets, and also generate a passkey dataset, which has been integrated in the code warehouse. More experimental details and results are referred to Appendix B and C. ", "page_idx": 6}, {"type": "text", "text": "Since our method is completely free plug-in and does not require fine-tuning, we choose methods of this type for comparison, including: model self (Origin), ReRoPE Su (2023b), Leaky-ReRoPE Su (2023b), Dynamic-NTK Liu et al. (2023), LM-Infinite Han et al. (2023), and Streaming-LLM Xiao et al. (2023). ", "page_idx": 6}, {"type": "text", "text": "We evaluate Mesa-Extrapolation using three prominent LLM families: LLaMA Touvron et al. (2023a) Touvron et al. (2023b) (including LLaMA-3B (Open-LLaMA-3B), LLaMA2-7B-Chat, and Vicuna13B-V1.3), MPT Team (2023) (including MPT-7B), and PyThia Biderman et al. (2023) (including PyThia-6.9B and PyThia-12B). Notably, LLaMA and PyThia incorporate RoPE Su et al. (2023), whereas MPT employs ALiBi Press et al. (2021) \u2013 two of the most influential PE techniques in recent research. Furthermore, we validated our approach using the Phi-3-mini-128k-instruct model Microsoft (2024) (refer to Appendix C.9). ", "page_idx": 6}, {"type": "text", "text": "We also conduct ablation experiments (refer to Appendix C.6). We use a 2xA800 80GB NVIDIA GPU server as the experimental environment and adopt the PyTorch framework. ", "page_idx": 6}, {"type": "text", "text": "5.1 Evaluation on Passkey Retrieval Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We assess the accuracy of Mesa-Extrapolation using the generated passkey dataset. This dataset comprises samples of varying lengths, each storing a random password at a random position. The sample length initiates at 1024 and increments by 1024. Simultaneously, 100 samples are randomly generated for each length. The proportion of correct answers found by LLMs is calculated for each input length. ", "page_idx": 6}, {"type": "text", "text": "Fig.3 shows the results of 6 LLMs on passkey retrieval task. The LLaMA families can employ various methods, including Origin, ReRoPE, Leaky-ReRoPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and our Mesa-Extrapolation. Note that these methods are model-specific and may not be universally applicable across all model series. For the MPT model, Origin, Streaming-LLM, ReRoPE, and Mesa-Extrapolation can be utilized. Similarly, for the PyThia model, Origin, Streaming-LLM, and Mesa-Extrapolation can be applied. ", "page_idx": 6}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/ff7ea4da972025fd8c72559f3f18cdf81e94c2dc5bc80b4279fc46ebf3d7806b.jpg", "img_caption": ["Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReRoPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Analyzing the LLaMA model series reveals that weave PE-based methods, including ReRoPE, LeakyReRoPE and Mesa-Extrapolation, achieve superior extrapolation capabilities. Additionally, under our existing hardware constraints, Mesa-Extrapolation demonstrates longer extrapolation capabilities. ", "page_idx": 7}, {"type": "text", "text": "In the case of MPT model, after surpassing the maximum training length, the extrapolation capabilities of Mesa-Extrapolation and ReRoPE show a decline. We speculate that this may be attributed to an approximate ALiBi PE applied on MPT model. This approximate ALiBi would lead to significant disruptions when weaving its positions. (refer to Appendix C.2 for more details about MPT). ", "page_idx": 7}, {"type": "text", "text": "In the PyThia models, an additional observation is that $100\\%$ accuracy is still not achieved within the training length. We attribute this to the PyThia model being a pre-trained model without instruction alignment, resulting in a weakened intrinsic understanding of the task. Results with enhanced prompts are referred to Appendix C.5. ", "page_idx": 7}, {"type": "text", "text": "Dynamic-NTK exhibits partial extrapolation capabilities beyond the maximum training length. Streaming-LLM and LM-Infinite also exhibit poor performance. This is because these methods discard portions of the input token, leading to potential information loss and incorrect answers. ", "page_idx": 7}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/2652f277eeec6d099259dadf8f43b55f14c671478e19bd447eebf60d65f034fa.jpg", "img_caption": ["Figure 4: Perplexity (PPL) metrics on LLaMA models using the Pile dataset. Some observations: (1) The PPL value of Origin consistently increases when the maximum training length is exceeded. (2) Other methods maintain low PPL values, with Dynamic-NTK exhibiting a slight increase as the input length grows. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Evaluation on Language Modeling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further assess the fluency of Mesa-Extrapolation utilizing the perplexity metric. Results evaluated on the Pile dataset are presented in Fig.4. X-axis represents the length of the input token, while the ", "page_idx": 7}, {"type": "text", "text": "Y-axis corresponds to NLL (Negative Log-Likelihood) values. It can be observed that the NLL value of Origin consistently increases when the maximum training length is exceeded. Other methods maintain low NLL values. LM-Infinite performs marginally better on Vicuna-13B. Dynamic-NTK method exhibits slightly weaker performance after 11k, and the performance continues to drop as the input length increases. In summary, our Mesa-Extrapolation demonstrates comparable performance to other methods on PPL metric. ", "page_idx": 8}, {"type": "text", "text": "5.3 Evaluation on Summary of Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct a summary task using the GovReport dataset and employ ROUGE ROUGE (2004) (ROUGE-1/2/L) as evaluation metrics. ROUGE assess overlapping N-grams by comparing the generated text with reference answers. ", "page_idx": 8}, {"type": "text", "text": "For the GovReport dataset, we segment the range from $3^{*}1024$ to $11{}^{\\ast}1024$ based on sample length, with each interval of 1024 units. A test set is created by randomly selecting 8 samples from each interval. We choose LLaMA2-7B-Chat as the evaluated LLM. The experimental results for ROUGE is presented in Tables 1 below: ", "page_idx": 8}, {"type": "text", "text": "Table 1: ROUGE metric on LLaMA2-7B-Chat, averaged on 8 samples within each interval using the GovReport dataset. Each cell contains ROUGE-1/ROUGE-2/ROUGE-L. The best values are marked in bold. Some observations: (1) Dynamic-NTK shows slightly better performance within 11k. (2) Other methods showcase the ability to achieve scores of varying degrees. ", "page_idx": 8}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/de1b061df35a09846b64717c2c5fc1b5ce9f34451c2a64ab71704061f34db2a7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In Table 1, we record the average scores of Rouge-1, Rouge-2, and Rouge-L within each interval. It is evident that once the effective input window is exceeded, the performance of Origin and StreamingLLM declines rapidly, rendering it useless. For LM-Infinite, the scores exhibit a slight decrease as the length increases. Dynamic-NTK shows slightly better performance within 11k. However, combined with the Fluency experiment on Fig.4, it seems that the effective extrapolation range of Dynamic-NTK cannot exceed $12\\mathbf{k}$ , which is consistent with the threshold observed in Fig.2. Weave PE-based methods, including ReRoPE, Leaky-ReRoPE and Mesa-Extrapolation maintain similar generation quality as the length increases. Note our Mesa-Extrapolation shows slight variability in performance within mid-length (8k-11k) in the summary task. We speculate that with a fixed extrapolation width param ( $E=50$ ), the 7k-11k range may spread the model\u2019s attention more thinly compared to the $4\\mathrm{k-}6\\mathrm{k}$ range. We hypothesize that optimizing the extrapolation width could alleviate or improve performance in the 7k-11k range. ", "page_idx": 8}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/f53d171353f7313c1a28c0eff3554fd8b42fb7a3605d387b384344890d15c185.jpg", "img_caption": ["(a) Memory Usage & Latency for Open-LLaMA-3B ", "(b) Memory Usage & Latency for LLaMA2-7B "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Memory Usage and Decoding Speed Comparison for LLaMA Models: 3B and 7B. The X-axis represents the input token length, the left Y-axis denotes memory usage, and the right Y-axis indicates speed about decoding time during inference. Some observations: (1) ReRoPE and Leaky-ReRoPE exhibit the largest memory footprint for the same input length, and their inference speed follows a quadratic function trend. (2) Mesa-Extrapolation shows an approximately linear inference speed, boasting the fastest inference speed and the smallest memory usage under the same input conditions. ", "page_idx": 8}, {"type": "text", "text": "5.4 Latency & Memory Usage ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To compare actual memory consumption and inference speed, we conduct experiments using both the 3B and 7B versions of the LLaMA model. The results are presented in Fig.5. In Fig.5, the $\\Chi$ -axis represents the input token length, the left Y-axis denotes memory usage, and the right Y-axis indicates decoding time. It is noteworthy that decoding time is closely related to memory usage, primarily from the computation of the attention matrix. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Observing Fig.5, both memory usage and decoding time for Origin and Dynamic-NTK exhibit a quadratic trend. Similarly, ReRoPE and Leaky-ReRoPE exhibit the highest memory usage and decoding time, showcasing a quadratic trend, which aligns with our analysis (refer to Appendix C.7). Although LM-Infinite demonstrates a linear trend, its increase is substantial. In contrast, Mesa-Extrapolation method also exhibits a linear trend but significantly outperforms other methods in terms of memory usage and decoding time. Furthermore, as the input length increases, this trend becomes more pronounced. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our study addresses the critical challenge faced by Large Language Models (LLMs) when confronted with longer input lengths, commonly referred to as the extrapolation problem. Through theoretical exploration, we uncover the underlying mechanisms of this challenge, shedding light on the reason of extrapolation failure for both NoPE and PE. Furthermore, we present theoretical evidence demonstrating the potential for effective extrapolation using Weave PE. Based on Weave PE, we introduce a practical solution called Mesa-Extrapolation, which strategically organizes input tokens into chunks to achieve competitive performance with minimal resource usage. Empirical validation demonstrates its effectiveness. Our work not only advances the understanding of the extrapolation problem but also offers a practical solution, with a complete free plug-in for LLMs. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Mesa-Extrapolation is a plug-and-play method that does not require additional finetuning. However, previous work, \"NTK-aware\" ntk (2023) also shows that applying further fine-tuning to plug-in extrapolation is possible. Therefore exploring fine-tuning based on Mesa-Extrapolation can be an interesting next step. Due to limitations of resources, we have not yet validated our method at longer lengths. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts We contend that the significance of completely free plug-in extrapolation method lies in two aspects. Firstly, it enables the expansion of the effective window length of already trained LLMs with no additional cost. Secondly, it allows for training LLMs from scratch with short texts and subsequently expanding their effective window length with a free plug-in extrapolation method, which can greatly help the industry improve the extrapolation capabilities of diverse LLMs. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Key R&D Program of China under Grant No.2022ZD0160504 and Tsinghua University(AIR)-Asiainfo Technologies (China) Inc. Joint Research Center. We would also like to acknowledge anonymous reviewers and Zengxiang Lu for their valuable feedback and discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ntk-aware scaled rope allows llama models to have extended $(8\\mathrm{k}+)$ context size without any fine-tuning and minimal perplexity degradation, 2023. URL https://www.reddit.com/r/ LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_ have/.   \nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \nBai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.   \nBertsch, A., Alon, U., Neubig, G., and Gormley, M. R. Unlimiformer: Long-range transformers with unlimited length input. arXiv preprint arXiv:2305.01625, 2023.   \nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., Skowron, A., Sutawika, L., and van der Wal, O. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:xxxx.xxxxx, 2023.   \nBlackSamorez. TensorParallel: A Library for Efficient Model Parallelization in PyTorch. https: //github.com/BlackSamorez/tensor_parallel, 2023.   \nbloc97. Add ntk-aware interpolation \"by parts\" correction, 2023. URL https://github.com/ jquesnelle/scaled-rope/pull/1.   \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nChen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.   \nDel\u00e9tang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L. K., Catt, E., Cundy, C., Hutter, M., Legg, S., Veness, J., et al. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098, 2022.   \nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., et al. A mathematical framework for transformer circuits, 2021. URL https://transformer-circuits.pub/2021/framework/index.html. Transformer Circuits Thread.   \nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \nGeng, Xinyang and Liu, Hao. Open LLAMA 3B - Large Language Model. https://huggingface. co/openlm-research/open_llama_3b, 2023.   \nHan, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.   \nHaviv, A., Ram, O., Press, O., Izsak, P., and Levy, O. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022.   \nHendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \nHsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., and Ginsburg, B. Ruler: What\u2019s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024.   \nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021.   \nJin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y., Chen, H., and Hu, X. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325, 2024.   \nKazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466, 2023.   \nKrishna, K., Bransom, E., Kuehl, B., Iyyer, M., Dasigi, P., Cohan, A., and Lo, K. Longeval: Guidelines for human evaluation of faithfulness in long-form summarization. arXiv preprint arXiv:2301.13298, 2023.   \nLangley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207\u20131216, Stanford, CA, 2000. Morgan Kaufmann.   \nLindner, D., Kram\u00e1r, J., Rahtz, M., McGrath, T., and Mikulik, V. Tracr: Compiled transformers as a laboratory for interpretability. arXiv preprint arXiv:2301.05062, 2023.   \nLiu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023.   \nMicrosoft. Phi-3-mini-128k-instruct. https://huggingface.co/microsoft/ Phi-3-mini-128k-instruct, 2024.   \nMohtashami, A. and Jaggi, M. Random-access infinite context length for transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311\u2013318, 2002.   \nPeng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.   \nPress, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.   \nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \nROUGE, L. C. A package for automatic evaluation of summaries. In Proceedings of Workshop on Text Summarization of ACL, Spain, volume 5, 2004.   \nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \nSu, J. Understanding the scale operation of attention from the perspective of entropy invariance, Dec 2021. URL https://kexue.fm/archives/8823.   \nSu, J. The road to transformer upgrade: 7. length extrapolation and local attention, Jan 2023a. URL https://kexue.fm/archives/9431.   \nSu, J. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023b.   \nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, pp. 127063, 2023.   \nTeam, M. N. Introducing mpt-7b: A new standard for open-source, commercially usable llms. www.mosaicml.com/blog/mpt-7b, 2023. Accessed: 2023-05-05.   \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.   \nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.   \nTworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u00b4s, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023.   \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nVon Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151\u201335174. PMLR, 2023.   \nWeiss, G., Goldberg, Y., and Yahav, E. Thinking like transformers. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11080\u201311090. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/weiss21a.html.   \nXiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with trainingfree memory. arXiv, 2024.   \nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.   \nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \nZhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Soaring from 4k to 400k: Extending llm\u2019s context with activation beacon. arXiv preprint arXiv:2401.03462, 2024.   \nZhang, Y., Backurs, A., Bubeck, S., Eldan, R., Gunasekar, S., and Wagner, T. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Extrapolation for PE A distinct line of research is dedicated to refining PE for enhanced extrapolation capabilities. Notable contributions include the introduction of Rotary Position Encoding (RoPE) Su et al. (2023), implementing relative PE through absolute position information. Similarly, Press et al. (2021) proposes a novel PE method ALiBi, fusing position information by directly introducing the relative position distance term in dot multiplication. In addition, there are other PE methods such as absolute position embedding (APE) Vaswani et al. (2017) and T5\u2019s Relative PE Raffel et al. (2020). ", "page_idx": 13}, {"type": "text", "text": "Another school of research focuses on enhancing the extrapolation performance of existing LLMs, categorized by whether to include further training. The first subcategory involves further fine-tuning, enlarging the effective window length by training LLMs on longer input texts. Chen et al. (2023) demonstrates that Position Interpolation (PI) method has a superior fine-tune effect, resulting in extended extrapolation capabilities with fewer fine-tuning steps. Mohtashami & Jaggi (2023) utilizes a new landmark token to represent individual input blocks, and enables model to select relevant blocks by further training, allowing for longer extrapolation. Zhang et al. (2024) follows similar idea by training a special token. Bertsch et al. (2023) proposes Unlimiformer, a $\\mathbf{k}$ -nearest-neighbor (kNN) indexed encoder-decoder Transformer, enhancing efficiency by retrieving top-k keys for each decoder layer. Despite claiming support for decoder-only Transformer, differences in retrieval results across decoder layers may lead to potential failure. Tworkowski et al. (2023) introduces Focused Transformer (FOT), utilizing a contrastive learning-inspired training process to enhance the (key, value) space structure and enable effective context extension. The second subcategory explores methods that require no fine-tuning yet offer improved extrapolation through plug-ins. Su (2023b) introduces Rectified Rotary Position Embeddings $(R e R o P E)$ , a method that rectifies extrapolated relative positions based on RoPE within a specified interval to extend the effective window length. In addition, Leaky-ReRoPE offers an alternative by allowing a controlled leakage of position extrapolation within an interval. Both approaches are well-suited for LLaMA model families Touvron et al. (2023b) Touvron et al. (2023a), eliminating the need for fine-tuning. These methods do suffer from some drawbacks, such as twofold increase in memory consumption and longer inference time. We refer to this class of methods that achieve extrapolation by weaving the relative positions of PE without fine-tuning as Weave PE. We also notice that, recent work, InfLLM Xiao et al. (2024), proposes an additional memory units, which lookup token-relevant units for attention computation. In addition, it uses a modified encoding scheme similar to ReRoPE to achieve longer extrapolation. It is worth noting that the methods of the weave PE class can be applied seamlessly to these new designs. However, there is still no theory to explain why the methods of weave PE class can work. In a different line of inquiry, ntk (2023) proposes the \"NTK-aware\" method by drawing from the Neural Tangents (NTK) idea, which explores the high-frequency extrapolation and low-frequency interpolation concept for RoPE. Based on \"NTK-aware\", recent works bloc97 (2023), Peng et al. (2023), Roziere et al. (2023), Liu et al. (2023) perform further fine-tuning for optimal results. ", "page_idx": 13}, {"type": "text", "text": "Han et al. (2023) proposes LM-Infinite, which employs a $\\Lambda$ -shaped mask to prevent surpassing the effective window length by discarding central tokens. However, this design choice inevitably results in a loss of information. Likewise, Streaming-LLM Xiao et al. (2023) adopts a similar strategy to avoid surpassing the effective window length by discarding a portion of input tokens. ", "page_idx": 13}, {"type": "text", "text": "Extrapolation for NoPE There also exists a counter perspective asserting that the position information of an input sequence can be perceived without utilizing PE. In a comprehensive experimental comparison presented in Kazemnejad et al. (2023), the decoder-only Transformer is shown to exhibit superior extrapolation properties with no position encoding (NoPE). Haviv et al. (2022) conjectures that causal attention enables the Transformer to infer position information without the help of PE, demonstrating its comparable performance with standard Transformer models through probing experiments. These new studies pose a key challenge regarding the choice of whether using PE or not in Transformer architecture. ", "page_idx": 13}, {"type": "text", "text": "Theorems for Extrapolation We mainly focus on Transformer architecture. Although the Transformer architecture is considered as a \"black box\", there are ongoing efforts trying to explain its inner workings. Von Oswald et al. (2023) begins by providing a specific weight construction that elucidates the mechanistic understanding of in-context learning within optimized Transformers. Lindner et al. (2023) introduces interpretability for Transformers through programming, allowing the derivation of a program Transformer architecture tailored for specific tasks. In Han et al. (2023), the authors conclude that long-distance attention logits will explode. However, this does not clarify the relationship between the effective window length and the existing supremum. It merely indicates an increase in the supremum, without directly implying an increase in the obtained actual attention logits. In Kazemnejad et al. (2023), inspired by programming Transformers Lindner et al. (2023), the authors construct a specific Transformer with predefined matrix parameters, theoretically demonstrating that NoPE can recover both absolute and relative position information even without the use of PE. However, these results fall short in explaining the underlying reasons for the failure of NoPE extrapolation. Built upon the entropy increase theory, Han et al. (2023) proves that with the expansion of input length, the entropy of attention experiences a corresponding increase. Additionally, Su (2021) introduces a scaling factor, denoted as $\\bar{l o}g_{\\mathrm{train-len}}(n)$ , which serves to mitigate entropy increase. Nevertheless, their theorems do not show that extrapolation fails beyond the effective window length. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "We observe that extrapolation failure is intricately linked to the max window length. Nevertheless, existing theories on Transformers do not reveal the internal mechanism about extrapolation and the max window length. In light of this, our endeavor is to establish a correlation between these two aspects, aiming to offer theoretical insights that can guide us towards designing applicable extrapolation methodologies. ", "page_idx": 14}, {"type": "text", "text": "B Experiments Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Passkey Retrieval Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The data used to perform the passkey task is constructed as follows: ", "page_idx": 14}, {"type": "text", "text": "TASK-DESCRIPT $=$ \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize it. I will quiz you about the important information there.\" ", "page_idx": 14}, {"type": "text", "text": "DEFAULT-CONTENT $=$ \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\" ", "page_idx": 14}, {"type": "text", "text": "KEY-CONTENT $=$ \"The pass key is {KEY}. Remember it. {KEY} is the pass key.\" ", "page_idx": 14}, {"type": "text", "text": "The TASK-DESCRIPT is placed at the beginning of the sample. Subsequently, the DEFAULTCONTENT is repeated multiple times to serve as filler text. A randomly generated password is then created and inserted into the designated KEY-CONTENT section. Finally, a random position is selected, and the KEY-CONTENT, containing the generated password, is seamlessly incorporated into the sample. ", "page_idx": 14}, {"type": "text", "text": "The LLM is required to find the correct password from the sample. ", "page_idx": 14}, {"type": "text", "text": "B.2 Params Setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Mesa-Extrapolation We design the DynamicSplit function used in Algorithm 1 to dynamically divide the input sequence into several subsequences according to the input length. The length of each subsequence corresponds to the size of the chunk. Let $\\mathcal{C}$ denote the length of the each chunk (except the first chunk length $\\mathcal{F}$ and the last chunk length $\\mathcal{L}$ ). Define $\\mathcal{T}$ as input token length and $\\tau$ as max training length. ", "page_idx": 14}, {"type": "text", "text": "The DynamicSplit function is detailed as follows: ", "page_idx": 14}, {"type": "text", "text": "In general, we set ${\\mathcal{F}}=100$ , $\\mathcal{M}_{m a x}=200$ and $\\mathcal{L}=512$ . Based on these criteria, we have devised a method for dynamically segmenting sequence. Leveraging the total length of the input token and the aforementioned requirements, we dynamically determine the length of each individual chunk. ", "page_idx": 14}, {"type": "text", "text": "Additionally, Stair PE is primarily employed in the manipulation of the last chunk. We need to concatenate all chunks together. After splicing chunks, the required positons will far exceed the max training length. At this time, we need to rearrange the positions according to Equ.3. In Equ.3, we generally set the extrapolated position $N=512$ and set the extrapolated width $E=50$ . ", "page_idx": 14}, {"type": "text", "text": "Implement Stair PE on RoPE and ALiBi The handling of RoPE Su et al. (2023) involves assigning positions to the query vector and key vector. The length of the query corresponds to the length of the last chunk, while the lengths of the key and value align with the combined lengths of all chunks. We assign position encoding to the query vector according the same sequence positions. Regarding the key\u2019s position encoding, we utilize the extrapolated width $E$ and extrapolated point $N$ . By designing ", "page_idx": 14}, {"type": "text", "text": "Require: LLM parameters   \nInput: $s$ (input token sequence)   \nOutput: ${\\mathcal{F}},{\\mathcal{C}}$   \n1: set the values of $\\mathcal{F}$ and $\\mathcal{L}$ respectively as constants   \n2: set $\\tau$ according to the LLM parameters   \n3: get $\\mathcal{T}$ according to input sequence $s$   \n4: $\\begin{array}{r}{\\bar{N}=\\left\\lfloor\\frac{\\mathbb{Z}-\\mathbb{\\mathcal{L}}-\\bar{\\mathcal{F}}}{T-\\bar{\\mathcal{F}}}\\right\\rfloor}\\end{array}$   \n5: $\\mathcal{M}=(\\mathcal{T}-\\mathcal{L}-\\mathcal{F})$ mod $(\\mathcal T-\\mathcal F)$   \n6: if $\\mathcal{M}<\\mathcal{M}_{m a x}$ then   \n7: $\\mathcal{C}=\\mathcal{T}-\\mathcal{F}$   \n8: else   \n9: $\\begin{array}{r}{\\tilde{\\mathcal{C}}=\\left\\lfloor\\frac{\\mathbb{Z}-\\mathcal{L}-\\mathcal{F}}{\\mathcal{N}+1}\\right\\rfloor}\\end{array}$   \n10: end if ", "page_idx": 15}, {"type": "text", "text": "the position encoding of query and key respectively, the relative position scheme defined by Stair PE can be implemented. And, for computational efficiency, we ensure that the relative PE of the last token in the last chunk completely follows Stair PE. The other tokens of the last chunk are similar to Stair PE. ", "page_idx": 15}, {"type": "text", "text": "For ALiBi\u2019s Press et al. (2021) position processing, we directly transform the relative position of the last chunk into the aforementioned structure within the mask matrix, refer to RoPE. ", "page_idx": 15}, {"type": "text", "text": "It is emphasized that the considerations outlined above primarily address the generation mechanism of LLM. The position encoding of the last chunk must align as closely as possible with the original position setting. This consideration is rooted in the belief that the latent concept Xie et al. (2021) formation heavily relies on there, with subsequent chunks providing essential information support. Experiments affirm that the last chunk significantly influences the accurate output of LLMs. This serves as the experimental foundation for our Mesa-Extrapolation method design. ", "page_idx": 15}, {"type": "text", "text": "ReRoPE & Leaky-ReRoPE We follow Su (2023b) for configuring the parameters of ReRoPE and Leaky-ReRoPE. The extrapolated position for ReRoPE is established at 512, mirroring the setting for Leaky-ReRoPE. Simultaneously, the extrapolated increment for Leaky-ReRoPE is calculated in relation to the input length, following the formula below: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{k}=\\frac{\\tau-w}{\\mathcal{I}-w}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $w$ is the extrpolated position as 512, $\\tau$ is the maxium training length, and $\\mathcal{T}$ is the input token length. ", "page_idx": 15}, {"type": "text", "text": "The ReRoPE and Leaky-ReRoPE methods are currently only applied to the LLaMA model series.   \nWe implemented ReRoPE based on its ideas on the MPT models. ", "page_idx": 15}, {"type": "text", "text": "LM-Infinite & Streaming-LLM LM-Infinite Han et al. (2023) introduces two branches for attention masking: a global branch on the left and a local branch on the right. The global branch enables each token to attend to the preceding $n_{g l o b a l}$ tokens if they appear before the current token. Conversely, the local branch allows each token to attend to preceding tokens within a distance of $n_{l o c a l}$ . Tokens outside these two branches are disregarded during the attention operation. Subsequently, following its setting, set $n_{l o c a l}=\\mathcal{T}$ , where $\\mathcal{T}$ still represents the training length. The choice of $n_{g l o b a l}$ has a minor impact on model performance within the range [10, 100], and thus, we fix $n_{g l o b a l}=100$ as its code setting. The distance limit entails the \"effective distance\" $d$ within $\\mathcal{T}$ , and we set $d=\\mathcal{T}$ . ", "page_idx": 15}, {"type": "text", "text": "Streaming-LLM Xiao et al. (2023) adopts a similar design to LM-Infinite. It introduces an initial token known as Attention Sinks, whose length is denoted by $x$ . It also defines a Rolling KV Cache for retaining the most recent tokens, whose length is denoted by $y$ . It set $x=4$ and $y=\\tau-x$ where $\\tau$ denotes the maximum training length. The central tokens are referred to Evicted Tokens. ", "page_idx": 15}, {"type": "text", "text": "The distinguishing factor between the two approaches lies in the selection of the initial token length, with LM-Infinite opting to splice longer tokens. We speculate that this is also the reason why LM-Infinite performs better than Streaming-LLM in the experiments. ", "page_idx": 15}, {"type": "text", "text": "Origin & Dynamic-NTK For Origin, it refers to loading the model directly, without using any methods. Dynamic-NTK differs from Origin by only adjusting the angle value of its RoPE component, according to the input length Liu et al. (2023). ", "page_idx": 16}, {"type": "text", "text": "The Dynamic-NTK method is currently applied to the LLaMA model series. ", "page_idx": 16}, {"type": "text", "text": "B.3 Weave PE-based Schemes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We list the details of the weave PE-based methods, including ReRoPE and Leaky-ReRoPE, as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{ReRoPE}=\\left[\\begin{array}{l l l l l l l}{0}&&&&&\\\\ {1}&{0}&&&&\\\\ {2}&{1}&{0}&&&&\\\\ {\\cdots}&{2}&{1}&{0}&&&\\\\ {N}&{\\cdots}&{2}&{1}&{0}&&\\\\ {\\cdots}&{N}&{\\cdots}&{2}&{1}&{0}\\\\ {N}&{\\cdots}&{N}&{\\cdots}&{2}&{1}&{0}\\\\ {\\cdots}&{N}&{\\cdots}&{N}&{\\cdots}&{2}&{1}&{0}\\\\ {N}&{\\cdots}&{N}&{\\cdots}&{N}&{\\cdots}&{2}&{1}&{0}\\end{array}\\right],\\ L e a k y\\cdot R e\\mathbf{RoPE}=\\left[\\begin{array}{l l l l l l l}{0}&&&&&\\\\ {1}&&{0}&&&&\\\\ {2}&&{1}&&{0}&&&\\\\ {\\vdots}&{2}&&{1}&&{0}&&\\\\ {N}&{\\cdots}&{2}&{1}&{0}&&\\\\ {N+\\frac{1}{k}}&{N}&{\\cdots}&{2}&{1}&{0}\\\\ {\\vdots}&{N+\\frac{1}{k}}&{N}&{\\cdots}&{2}&{1}&{0}\\\\ {N+\\frac{L}{k}}&{\\cdots}&{N+\\frac{1}{k}}&{N}&{\\cdots}&{2}&{1}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where left ReRoPE shows relative position on attention matrix, including (1) Numbers marked on each cell denote its relative position $(t-i)$ between $\\mathbf{\\Psi}_{q_{t}}$ and $k_{i}$ . (2) It start to extrapolate position from relative position $N$ . And right Leaky-ReRoPE shows relative position on attention matrix, including (1) Numbers marked on each cell denote its relative position $(t-i)$ between $\\scriptstyle q_{t}$ and $\\pmb{k}_{i}$ . (2) It start to extrapolate position from relative position $N$ , with an incremental factor $\\textstyle{\\frac{1}{k}}$ . ", "page_idx": 16}, {"type": "text", "text": "With Stair PE defined on Equ.3, the extrapolated position after $N$ exhibits a fixed extrapolated width $E$ , resembling a stair-like increment. Our Mesa-Extrapolation is designed with a finer position granularity compared to ReRoPE. Simultaneously, unlike Leaky-ReRoPE, ours utilizes previously trained positions, ensuring greater stability. ", "page_idx": 16}, {"type": "text", "text": "While these weave PE schemes, including ReRoPE, Leaky-ReRoPE, and Stair PE, demonstrate theoretical feasibility, their practical implementation may encounter computational complexities. For instance, both ReRoPE and Leaky-ReRoPE are primarily employed in the LLaMA models, which is based on RoPE Su et al. (2023). Due to the distinctive nature of RoPE, these methods necessitate the calculation of the attention matrix more than once, resulting in double the normal memory consumption. Moreover, given the quadratic complexity of the calculations, as the input length expands, both inference time and memory consumption grow proportionally. ", "page_idx": 16}, {"type": "text", "text": "B.4 ALiBi on MPT-7B ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is worth noting that the ALiBi implementation of MPT is only an approximation, as follow: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[{\\begin{array}{c c c c c c c c c c}{-9}&&&&&&&&\\\\ {-9}&{-8}&&&&&&&\\\\ {-9}&{-8}&{-7}&&&&&&\\\\ {-9}&{-8}&{-7}&{-6}&{-5}&&&&\\\\ {-9}&{-8}&{-7}&{-6}&{-5}&{-4}&&&\\\\ {-9}&{-8}&{-7}&{-6}&{-5}&{-4}&{-3}&&\\\\ {-9}&{-8}&{-7}&{-6}&{-5}&{-4}&{-3}&{-2}&\\\\ {-9}&{-8}&{-7}&{-6}&{-5}&{-4}&{-3}&{-2}&{-1}&\\\\ {-9}&{-8}&{-7}&{-6}&{-5}&{-4}&{-3}&{-2}&{-1}&{0}\\end{array}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where showcase an input token length 10 for ALiBi PE mask. Based on this approximate implementation of ALiBi, we still use splitting chunk and Stair PE to implement Mesa-Extrapolation. We speculate that the approximation of ALiBi is the reasons for the instability of the extrapolation on Accuracy. ", "page_idx": 16}, {"type": "text", "text": "Table 2: BLEU metric for mean and standard variance on LLaMA2-7B-Chat, averaged on 8 samples within each interval using the GovReport dataset. The best values are marked in bold. Some observations: (1) Dynamic-NTK shows slightly better performance within 11k. (2) Weave PE-based methods showcase the ability to achieve scores of varying degrees. ", "page_idx": 17}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/26e5da0f8c2268341553125d654936b3ccb77723eab8e7ca00de761a84290d5a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C More Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 BLEU Results on GovReport ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use the GovReport dataset and employ BLEU Papineni et al. (2002) as evaluation metric, with a same setting applied on Table 1. The experimental result for BLEU is presented in Table 2. In Table 2, we present the mean and standard variance for each method across various interval lengths. It is evident that, as the input token length increases, the performance consistently deteriorates for LM-Infinite. Streaming-LLM also experiences extrapolation failure beyond the model\u2019s inherent effective window length of 4k. ", "page_idx": 17}, {"type": "text", "text": "Dynamic-NTK shows slightly better performance within 11k. Here, the BLEU scores exhibit almost consistent results with those of ROUGE on Table 1. It is also noteworthy that additional experiments show performance degradation of Dynamic-NTK beyond 11k, likely suggesting a limited effective extrapolation range for Dynamic-NTK. ", "page_idx": 17}, {"type": "text", "text": "Weave PE-based methods, including ReRoPE, Leaky-ReRoPE, and Mesa-Extrapolation, consistently demonstrates competitive results, maintaining similar generation quality as the length increases. ", "page_idx": 17}, {"type": "text", "text": "Overall, despite the chunking applied to our Mesa-Extrapolation method, it maintains a competitive edge compared to ReRoPE and Leaky-ReRoPE. This is achieved even with the trade-off between losing a certain amount of information and reducing memory consumption and inference time. ", "page_idx": 17}, {"type": "text", "text": "The results of Mesa-Extrapolation in the summary task, focusing on the generation of summary text within 1000 tokens across different input lengths, are detailed in Table 5. ", "page_idx": 17}, {"type": "text", "text": "C.2 Perplexity (PPL) metrics on MPT-7b ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Following the same metrics setting about LLaMA models on Fig.4, we plot the NLL result about MPT-7B on Fig.6 as below: ", "page_idx": 17}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/05a6a09b0e40f60a216c115d4b4cf305860b880936844e0409b1508bbfe5f9a0.jpg", "img_caption": ["Figure 6: PPL Metric Comparison for MPT-7B using Origin, Streaming-LLM, and Mesa-Extrapolation. The white dashed line represents MPT-7B\u2019s maximum training length at $^{2\\mathbf{k}}$ . Some observations: (1) For Origin, after surpassing the maximum training length, extrapolation extends to approximately $3.5\\mathbf{k}$ , leading to a rapid escalation in PPL. (2) Both Mesa-Extrapolation and Streaming-LLM models exhibit effective negative loglikelihood (NLL) values. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "In Fig.6, both Mesa-Extrapolation and Streaming-LLM demonstrate effective negative log-likelihood (NLL) values. For the original MPT model, after the extrapolation is extended $3.5\\mathbf{k}$ , it will cause the perplexity (PPL) level to rise rapidly. ", "page_idx": 18}, {"type": "text", "text": "ALiBi Press et al. (2021) can still effectively extrapolate to around $3.5\\mathrm{k}$ position after surpassing the maximum training length of 2k. This observation affirms the extrapolation capability of the ALiBi method, especially when compared with RoPE\u2019s extrapolation results. ", "page_idx": 18}, {"type": "text", "text": "However, it is essential to highlight that while ALiBi achieves extrapolation to $3.5\\mathbf{k}$ in above NLL task, its performance in the Passkey and LongEval tasks (refer to C.3) reveals limitations. This discrepancy is attributed to an inherent characteristic of ALiBi. ALiBi\u2019s expression (refer to D.2) indicates a constant decrease in the value of its position encoding as the relative distance increases. Considering the SoftMax operation within the attention mechanism, under identical conditions, the attention score obtained becomes smaller. This implies that tokens situated farther away receive less attention, possibly leading to the neglect of these distant tokens. Therefore, despite ALiBi\u2019s apparent success in effectively extrapolating the NLL task to $3.5\\mathrm{k}$ , it falls short in attending to tokens beyond the maximum training length in practice. A similar analysis process is as Su (2023a). ", "page_idx": 18}, {"type": "text", "text": "It\u2019s noted that, NoPE Kazemnejad et al. (2023) claims to possess extrapolation capabilities akin to ALiBi. We hypothesize that the underlying reasons for NoPE\u2019s extrapolation performance align with the analysis presented here. ", "page_idx": 18}, {"type": "text", "text": "C.3 Evaluation on LongEval ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct additional testing on LongEval Krishna et al. (2023) lines task, a recently prominent evaluation task for long texts. ", "page_idx": 18}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/168921682922fc8284a0f31c32f8eebefe6a9f5999934b4b72ab62e75feb6f3f.jpg", "img_caption": ["Figure 7: LongEval Lines Task on LLMs using various methods. Some observations: (1) Origin and StreamingLLM consistently exhibit an inability to extrapolate beyond the effective window length. (2) LM-Infinite shows weak extrapolation ability on the LLaMA2-7B-Chat model and fails to extrapolate on the MPT-7B model. (3) Weave PE-based methods, including ReRoPE, Leaky-ReRoPE, and Mesa-Extrapolation, all exhibit commendable extrapolation capabilities on the LLaMA2-7B-Chat model. (4) For the MPT-7B model, all methods display weak extrapolation performance, with Mesa-Extrapolation slightly outperforming other methods. (5) Mesa-Extrapolation also shows a certain extrapolation ability on the PyThia-6.9B model, with a decrease in extrapolation performance as the input length increases. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Fig.7 show the accuracy of LongEval Lines Task. Origin and Streaming-LLM consistently exhibit an inability to extrapolate beyond the effective window length. LM-Infinite shows weak extrapolation ability on the LLaMA2-7B-Chat model and fails to extrapolate on the MPT-7B model. Dynamic-NTK method, as shown earlier, almost fails after the input token length exceeds 11k. Weave PE-based methods, including ReRoPE, Leaky-ReRoPE, and Mesa-Extrapolation, all exhibit commendable extrapolation capabilities on the LLaMA2-7B-Chat model. For the MPT-7B model, all methods display weak extrapolation performance, with Mesa-Extrapolation slightly outperforming other methods. We analyze that the reason for the failure of extrapolation in the MPT-7B model is attributed to the approximated implementation of ALiBi PE. This approximation makes it susceptible to interference when use weave PE (refer to C.2). ", "page_idx": 18}, {"type": "text", "text": "Mesa-Extrapolation also shows a certain extrapolation ability on the PyThia-6.9B model, with a decrease in extrapolation performance as the input length increases. ", "page_idx": 18}, {"type": "text", "text": "C.4 Evaluation on LongBench ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We select LongBench Bai et al. (2023) dataset and use 5 major categories of tasks, including SingleDocument QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks and Code Completion. Among them, each task selects a dataset, namely qasper, hotpotqa, samsum, passage-retrieval-en, and repobench-p. Taking into account the varying memory consumption of different methods, we opt to filter out samples with an input length exceeding 10k to prevent discrepancies caused by out-of-memory (OOM) issues. We use the abbreviations in Table 3: S-Document (Single Document) QA, M-Document (Multi Document) QA, F-Learning (Few-shot Learning), S-Tasks (Synthesis Tasks), C-Completion (Code Completion). ", "page_idx": 19}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/5650d54a171d84c921956346f5077e9046d5c9d5bae390d9e97bd38027fae380.jpg", "table_caption": ["Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In Table 3, Origin and Streaming-LLM barely work after exceeding the maximum training length of 4k. LM-Infinite shows slightly weaker performance. Dynamic-NTK shows good performance, especially for Code Completion. Considering the effective scope of Dynamic-NTK, we speculate that this is related to the setting within 10k length. Mesa-Extrapolation shows better performance on most tasks. ", "page_idx": 19}, {"type": "text", "text": "C.5 Enhanced Prompts for Mesa-Extrapolation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Considering that Mesa-Extrapolation involves positional approximation, as the input length increases, attention becomes more dispersed, resulting in increased entropy. One possible approach to mitigate these entropy increases is to utilize instruction-aligned prompts. These prompts may represent the strongest latent concepts that the model can follow, while clearly articulating the goals to be achieved. Similarly, it may be the reason why PyThia cannot achieve $100\\%$ accuracy within its training length on Fig.3, due to a lack of instruction alignment. ", "page_idx": 19}, {"type": "text", "text": "Based on these considerations, we employ the model\u2019s corresponding prompt Geng, Xinyang and Liu, Hao (2023) and augment it with explicit goal statements, referred to as an enhanced prompt, like below: ", "page_idx": 19}, {"type": "text", "text": "or ", "page_idx": 19}, {"type": "text", "text": "Additionally, we leverage model parallel inference technology BlackSamorez (2023) and perform verification concurrently on 2 A800 GPUs. ", "page_idx": 19}, {"type": "text", "text": "Fig.8 show the accuracy on 3 LLaMA models with enhanced prompts using Mesa-Extrapolation. We declare that the longest input length that these 3 models can currently reach is the limit of our existing hardware resources. For LLaMA-3B, the enhanced prompts improve the Accuracy and is extrapolated to $60\\mathbf{k}$ . (2) Both of LLaMA2-7B-Chat and Vicuna-13B models, show good improvements by the enhanced prompts. ", "page_idx": 19}, {"type": "text", "text": "Through this experiment, we hypothesize that by writing prompts more carefully and accurately, we can maximize the extrapolation performance of free plug-in. ", "page_idx": 19}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/f3d9455b1ab15fb5e8e7fa621ac5a55e727d5b9c7486ad16542ac222ff3cd752.jpg", "img_caption": ["Figure 8: Accuracy about Passkey Retrieval Tasks using enhanced prompts for Mesa-Extrapolation. The gray line represents the max training length at 2k, 4k, and 2k, respectively. Some observations: (1) On the LLaMA-3B model, the enhanced prompts significantly improves the Accuracy and is extrapolated to 60k. (2) Both of LLaMA2-7B-Chat and Vicuna-13B models, show good improvements by the enhanced prompts. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.6 Ablation Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We design ablation experiment about weave PE based methods. We apply the encoding schemes of ReRoPE and Leaky-ReRoPE to chunk-based triangular attention matrices, aligning them with our implemented Mesa-Extrapolation. The entire design of the ablation experiment involved controlling only one variable, the weaving PE scheme, applying different weaving PE methods to the processing of the last chunk, while using the same dataset and testing environment. The experimental results 9 are as follows: ", "page_idx": 20}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/d9b0ef6cd75ca96eab448a991aeb28a8504b45ad068710ed81787ea044b0a15f.jpg", "img_caption": ["Figure 9: Accuracy about Passkey Retrieval Tasks using chunk-based triangular attention matrix. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 9 demonstrate that both ReRoPE and Leaky-ReRoPE experience a certain loss in accuracy as the input sequence grows. We speculate that this may be due to ReRoPE repeatedly using the same extrapolation positions, while Leaky-ReRoPE, employing some fractions, may not be as precise as Stair PE in comparison to normal relative positions, resulting in a slight decrease in effectiveness. ", "page_idx": 20}, {"type": "text", "text": "C.7 Theoritical Speed & Memory ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We assess the computational memory usage and inference time about decoding speed on various methods. ", "page_idx": 20}, {"type": "text", "text": "Table 4: Theoretical Memory Usage Based on Attention Matrix for Different Methods. Observations: (1) Origin and Dynamic-NTK exhibit identical quadratic memory consumption. (2) ReRoPE and Leaky-ReRoPE demonstrate $2\\times$ the memory consumption of Origin. (3) LM-Infinite, Streaming-LLM, and Mesa-Extrapolation showcase linear memory consumption. ", "page_idx": 20}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/596c5d71a055f5ccc13d810ad89e98fc4b5a52c83553ab262404a684cb96e86e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 4 shows the theoretical result about the memory usage. For Origin, the attention matrix calculation requires the current token to compute the attention score with each previous token, resulting in a memory footprint of $O(n^{2})$ . Dynamic-NTK only alters the angle base, making it different from the Origin but still quadratic. For ReRoPE and Leaky-ReRoPE, as showed in Su (2023b), since the attention matrix needs to be calculated twice, their memory footprint is $2\\,\\times\\,{\\mathcal{O}}(n^{2})$ . For Mesa-Extrapolation, its strategy involves splitting chunks while avoiding the quadratic ter\u221am. Accounting for splicing in the last chunk, its total memory usage scales proportional to $O((2+{\\sqrt{2}})n)$ . LM-\u221aInfinite and Streaming-LLM adopt a similar $\\Lambda$ -shaped mask, that scales proportional to $\\mathcal{O}((1+{\\sqrt{2}})n)$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "C.8 Generated Summary using Mesa-Extrapolation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present here the results of the summary generation task using GovReport Huang et al. (2021). In this experiment, task is to generate a summary for texts of varying lengths, limited to 1000 tokens. The outcomes yielded by Mesa-Extrapolation are outlined below on Table 5: ", "page_idx": 21}, {"type": "text", "text": "Table 5: Mesa-Extrapolation undertakes summarization tasks and generates text based on varying input lengths from the GovReport dataset. Some observations: (1) Mesa-Extrapolation adeptly and systematically summarizes key points one by one across various input lengths. It showcases logical coherence and demonstrates commendable summarization capabilities. (2) Additionally, Mesa-Extrapolation accurately incorporates punctuation and exhibits an automatic stopping mechanism for proper stopping. ", "page_idx": 21}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/180cedce896038056199dd4a22b7b9bd59b7539d535c489334c801cbd018f5ca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/8941247b8559c234af11ba90199bfe39bbcdb478a29c9a6225a77736cc199d6a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/204ce752389b95eb190420ea7dc2ad511fefc1880dfe567bb451c588dcbd8a90.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/c2df11f58da147e243f8acb977170a19efb6f6c9510941fdda3d802f953ffecf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/3a5dd4e9a89c566ee146e285789adad63bc1dc0662cb9d76c26723a05fea8c4a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/608c4932737b1e8a3af1f7eeacfdf082ac11a220200e600c9c596310c49b95e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "The report discusses the challenges faced by the US Department of Veterans Affairs (VA) in providing disability compensation to veterans. The VA pays monthly disability benefits to veterans who have service-connected conditions, which are conditions that were caused or aggravated by military service. The VA has been facing challenges in determining when to reevaluate these conditions, and the report highlights several areas where improvements can be made. Firstly, the report notes that the VA does not have a clear understanding of the health outcomes of veterans with service-connected conditions. Despite the fact that the VA spends billions of dollars each year on disability compensation and health care for veterans, it does not have adequate data to assess the effectiveness of these efforts. The report recommends that the VA should develop a plan to collect and analyze data on the health outcomes of veterans with serviceconnected conditions.   \nSecondly, the report highlights the issue of unwarranted reevaluations. Despite the fact that the VA has a rating schedule that provides guidelines for evaluating the severity of service-connected conditions, claims processors are not always following these guidelines. As a result, many veterans are undergoing unnecessary reevaluations, which can be time-consuming and costly. The report recommends that the VA should clarify the knowledge, skills, and abilities required for claims processors to determine when to reevaluate veterans\u2019 service-connected conditions.   \nThirdly, the report notes that the VA\u2019s procedures for reevaluating serviceconnected conditions are not always consistent. While the VA has a rating schedule that provides guidelines for evaluating the severity of service-connected conditions, claims processors are not always following these guidelines. As a result, many veterans are undergoing unnecessary reevaluations, which can be time-consuming and costly. The report recommends that the VA should develop and implement a plan to ensure that claims processors have the necessary knowledge, skills, and abilities to determine when to reevaluate veterans\u2019 service-connected conditions.   \nFourthly, the report highlights the issue of inconsistent application of rating criteria. Despite the fact that the VA has a rating schedule that provides guidelines for evaluating the severity of service-connected conditions, claims processors are not always applying these criteria consistently. As a result, many veterans are receiving inadequate compensation for their service-connected conditions. The report recommends that the VA should clarify the knowledge, skills, and abilities required for claims processors to apply the rating criteria consistently. Fifthly, the report notes that the VA\u2019s training program for claims processors is not adequate. While the VA provides training on disability compensation and health care for veterans, it does not provide enough training on the reevaluation process. As a result, many claims processors are not adequately equipped to determine when to reevaluate veterans\u2019 service-connected conditions. The report recommends that the VA should develop and implement a plan to provide additional training to claims processors on the reevaluation process.   \nFinally, the report highlights the issue of limited data on the health outcomes of veterans with service-connected conditions. Despite the fact that the VA spends billions of dollars each year on disability compensation and health care for veterans, it does not have adequate data to assess the effectiveness of these efforts. The report recommends that the VA should develop a plan to collect and analyze data on the health outcomes of veterans with service-connected conditions.   \nIn summary, the report highlights several areas where the VA can improve its disability compensation program for veterans, including collecting and analyzing data on the health outcomes of veterans with service-connected conditions, clarifying the knowledge, skills, and abilities required for claims processors to determine when to reevaluate veterans\u2019 service-connected conditions, developing a plan to provide additional training to claims processors on the reevaluation process, and ensuring that claims processors are applying the rating criteria consistently. ", "page_idx": 26}, {"type": "table", "img_path": "zaXuMqOAF4/tmp/b34ef2b7e1653f6943938156c5307652e2dce7ec4ade2878fb1c6b7f129e97e0.jpg", "table_caption": ["Table 5 \u2013 Continued from previous page "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 5 shows that Mesa-Extrapolation adeptly and systematically summarizes key points one by one across various input lengths. It showcases logical coherence and demonstrates commendable summarization capabilities. It\u2019s worth noting that the initial symbols used for sub-items beneath each summary vary, including symbols such as \"\\*\", \"first, second,...\", or the number \"1, 2, ...\", etc. We attribute this variability to the inherent content of the input text. LLM follows the expression format of the text itself and generates corresponding output. Additionally, Mesa-Extrapolation accurately incorporates punctuation and exhibits an automatic stopping mechanism for proper stopping. ", "page_idx": 27}, {"type": "text", "text": "C.9 Evaluation of Phi-3-mini-128k-instruct Model on Ruler Datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We further conducted experimental validation on the Ruler datasets Hsieh et al. (2024), focusing on the single-keys NIAH task. The needle-in-a-haystack (NIAH) test assesses the ability to retrieve a specific piece of information (the \u201cneedle\u201d) from long distractor texts (the \u201chaystack\u201d). ", "page_idx": 27}, {"type": "text", "text": "In this experiment, we employed the microsoft/Phi-3-mini-128k-instruct model, which has practical value in real-world applications and is officially claimed to support extrapolation lengths of up to $128\\mathbf{k}$ tokens. To evaluate performance, we gradually increased the input length by 32k increments, comparing the original model\u2019s results with those achieved using our Mesa-Extrapolation method. ", "page_idx": 27}, {"type": "text", "text": "As shown in Figure 10, the original Phi-3-mini-128k-instruct model can only extrapolate up to its official limit of $128\\mathbf{k}$ tokens, beyond which it fails. In contrast, when utilizing the Mesa-extrapolation method, the model successfully extrapolates to at least 192k tokens. It\u2019s important to note that this $192\\mathrm{k}$ limit is due to our current hardware resource constraints\u2014attempting to extend beyond this length results in an out-of-memory error. ", "page_idx": 27}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/69b3ce0949741e45faf669cac990635fa39e7370f2fb612c587d101309e1ecf8.jpg", "img_caption": ["Figure 10: NIAH Task on Phi-3-mini-128k-instruct model using Origin and Mesa-Extrapolation. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D Background ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "D.1 Preliminary ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we lay the groundwork and introduce the notation for below theorem analysis. Bold letters typically denote vectors or matrices, while normal letters represent scalars. We mainly follow Kazemnejad et al. (2023) to define these notations. ", "page_idx": 28}, {"type": "text", "text": "Let $f_{\\theta}$ be a decoder-only Transformer model, where $\\theta$ denotes the model parameters. $f_{\\theta}$ processes the input sequence $x=[<b o s>,x_{1},\\ldots,x_{T}]$ by applying a series of layers, where $<b o s>$ represents the first token Tokenizer transformation. ", "page_idx": 28}, {"type": "text", "text": "Each layer $l$ , consisting of self-attention heads and a feed-forward sub-layer, produces the hidden state $H^{l}$ at layer $l$ , after reading the previous hidden state $H^{(l-1)}$ . ", "page_idx": 28}, {"type": "text", "text": "Each head is parameterized by a query $W_{Q}^{m}$ , key ${\\pmb{W}}_{K}^{m}$ , value ${W}_{V}^{m}$ , and output ${\\cal W}_{{\\cal O}}^{m}$ matrices, where $m$ represents the attention head index, and $W_{Q}^{m}$ , ${\\pmb{W}}_{K}^{m}$ , ${W_{V}^{m}}\\in\\mathbb{R}^{h\\times d}$ and $W_{O}^{m}\\in\\mathbb{R}^{d\\times h}$ . $d$ is the model\u2019s hidden state size and $h$ is the attention dimension $\\begin{array}{r}{(h=\\frac{d}{\\#h e a d s})}\\end{array}$ . Note that we drop the attention head index $m$ where it is clear from the context. ", "page_idx": 28}, {"type": "text", "text": "The Transformer layer $\\mathrm{TLayer}^{(l)}(H^{(l-1)};\\theta_{l})$ consist of self-attention heads and a feed-forward sub-layer, and input the previous hidden state $H^{(l-1)}$ , and generate the hidden state $H^{(l)}$ at layer $l.\\;l$ is the layer index, and $\\theta_{l}$ is the set of parameters of the $l$ -th layer. Each hidden state $\\pmb{H}^{(l)}\\in\\mathbb{R}^{d\\times(T+1)}$ is a matrix, and $h_{t}^{(l)}$ denotes its hidden state at column $t$ , i.e. at position $t$ . ", "page_idx": 28}, {"type": "text", "text": "Each feed-forward sub-layer FF is parameterized by $W_{1}$ , $W_{2}\\in\\mathbb{R}^{d\\times k.d}$ matrices, where $k$ denotes a multiplier of the hidden state size in this sub-layer, and is usually set to 4 in common implementations of the Transformer. ", "page_idx": 28}, {"type": "text", "text": "The Transformer layer TLayer(l) processes each column of $\\pmb{H}^{(l-1)}$ independently and in parallel to generate the output. The computation of the $t$ -th column of $\\pmb{H}^{(l)}$ is as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{t}^{(l)}=\\mathrm{FF}(\\lambda(\\pmb{a}_{t}+\\pmb{h}_{t}^{(l-1)}))+\\pmb{a}_{t}+\\pmb{h}_{t}^{(l-1)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\lambda$ is layer normalization, and $\\mathbf{a}_{t}\\in\\mathbb{R}^{d}$ is the output of the multi-head self-attention sub-layer at position $t$ . ", "page_idx": 28}, {"type": "text", "text": "$\\mathbf{\\deltaa}_{t}$ is computed as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{a}_{t}=\\sum_{m}\\mathrm{Attn}^{(m)}(\\pmb{h}_{t}^{(l-1)},\\pmb{H}^{(l-1)})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathrm{Attn}^{(m)}$ denotes the $m$ -th attention head. Let $\\pmb{o}_{t}\\in\\mathbb{R}^{d}$ denote the output of an attention head at position $t$ . Then, $\\pmb{o}_{t}$ is computed as: ", "page_idx": 28}, {"type": "equation", "text": "$$\no_{t}=W_{O}\\left(\\sum_{i\\leq t}\\hat{\\alpha}_{i}\\boldsymbol{v}_{i}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\hat{\\pmb{\\alpha}}=\\mathrm{softmax}(\\pmb{\\alpha})\\in\\mathbb{R}^{(t+1)}$ , and $_{\\alpha}$ is the attention weight vector such that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\alpha=\\left[\\langle\\pmb{q}_{t},\\pmb{k}_{0}\\rangle,\\;\\langle\\pmb{q}_{t},\\pmb{k}_{1}\\rangle,\\;...\\,,\\;\\langle\\pmb{q}_{t},\\pmb{k}_{t}\\rangle\\right]^{T}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\pmb{q}_{t}=\\pmb{W}_{Q}\\pmb{h}_{t}^{(l-1)}\\in\\mathbb{R}^{h}$ , $\\pmb{k}_{i}=\\pmb{W}_{K}\\pmb{h}_{i}^{(l-1)}\\in\\mathbb{R}^{h}$ , and $\\langle\\cdot,\\cdot\\rangle$ denotes the dot product operation. ", "page_idx": 29}, {"type": "text", "text": "The feed-forward sub-layer $\\mathrm{FF}(\\cdot)\\in\\mathbb{R}^{d}$ is a two-layer MLP as below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{FF}(x)=W_{2}\\sigma(W_{1}^{\\mathrm{T}}x)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\sigma$ is a non-linear activation function(usually ReLU or GeLU Hendrycks & Gimpel (2016)), and $\\sigma(\\cdot)\\in\\mathbb{R}^{d}$ is layer normalization Ba et al. (2016). It\u2019s worthy noted that the additive view of attention heads Elhage et al. (2021) in Eq.(5) is mathematically equivalent with the view of concatenate and multiple Vaswani et al. (2017). The additive view is easier to understand and analyze. ", "page_idx": 29}, {"type": "text", "text": "The hidden state $H^{(0)}$ is initialized with a learned embedding of the input sequence $\\mathbf{\\deltaX}$ , i.e. $H^{(0)}=$ $W_{E}X$ , where $W_{E}\\in\\mathbb{R}^{d\\times V}$ is the embedding matrix and $\\pmb{X}\\in\\mathbb{R}^{V\\times(T+1)}$ is the one-hot encoded input sequence. $V$ is the vocabulary size. ", "page_idx": 29}, {"type": "text", "text": "D.2 NoPE & PE", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "NoPE We emphasize here that NoPE generally refers to these architectures that do not use position encoding components or remove the position encoding schemas, which is reflected in the attention dot product operation and is formally expressed as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle\\pmb q_{t},\\pmb k_{i}\\rangle=\\pmb q_{t}^{T}\\pmb k_{i}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "PE For PE, this generally refers to position encoding components, including Alibi, Rope, APE, etc.   \nTheir main difference from NoPE can be reflected in the attention dot product operation. ", "page_idx": 29}, {"type": "text", "text": "For ALiBi Press et al. (2021), it takes the form: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle\\pmb q_{t},\\pmb k_{i}\\rangle=\\pmb q_{t}^{T}\\pmb k_{i}-(t-i)\\cdot\\pmb{C}^{(m+1)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $m$ is head index and $C$ is a constant defined as: ", "page_idx": 29}, {"type": "equation", "text": "$$\nC=2^{-2^{-\\log_{2}(\\#{\\mathrm{heads}}+3)}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For example, if the number of heads is 8, we have $\\textstyle{\\frac{1}{2}},\\,{\\frac{1}{2^{2}}},\\dotsc,\\,{\\frac{1}{2^{8}}}$ ", "page_idx": 29}, {"type": "text", "text": "For RoPE Su et al. (2023), it\u2019s a relative PE that applies a rotation to the query and key representations based on their absolute positions before dot product attention. We formulate RoPE for model dimension $d=2$ , and its dot product as below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle\\pmb q_{t},\\pmb k_{i}\\rangle=\\pmb q_{t}^{T}R^{(i-t)\\theta}\\pmb k_{i}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $R$ is a rotation matrix that rotates $(i-t)\\theta$ radians: ", "page_idx": 29}, {"type": "equation", "text": "$$\nR=\\left[\\!\\!\\left[\\sin((i-t)\\theta)\\right.\\right.\\left.-\\sin((i-t)\\theta)\\right]\\!\\!\\left[\\sin((i-t)\\theta)\\right.\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $d>2$ , RoPE applies the same approach on every two consecutive dimensions of $\\pmb q_{t}$ and $k_{i}$ , but with different $\\theta$ angles. ", "page_idx": 29}, {"type": "text", "text": "For other PE methods, we recommend readers to consult the corresponding papers. ", "page_idx": 29}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We provide proof about Theorems 3.1, 3.2, 3.3, and Corollary 4.1. Our proofs are inspired by Kazemnejad et al. (2023) Weiss et al. (2021) Lindner et al. (2023) and relies on the causal attention mask in the decoder-only Transformer and the SoftMax function. ", "page_idx": 29}, {"type": "text", "text": "Please refer to Appendix $\\mathrm{D}$ for some necessary background knowledge. ", "page_idx": 29}, {"type": "text", "text": "Theorem E.1 (NoPE Extrapolation). Let $x=[<b o s>,x_{1},\\ldots,x_{T}]$ be an input sequence of length $T+1$ to the model. Then, there exists $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices, such that when $T<M$ , $o_{T}>\\mathcal{H}$ ; and when $T>M$ , $o_{T}<\\mathcal{H}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Our proof only specifies the weights of a single attention head in the first layer. In this parameterization, we only require the first three dimensions of the hidden states and regard the third dimension as the specific dimension. As for the remaining heads, they can be arbitrary as long as they don\u2019t override the first three dimensions. This doesn\u2019t pose any challenges, considering that Transformers used in practice usually have a very large model dimension $d$ . ", "page_idx": 30}, {"type": "text", "text": "First, we construct the word embedding matrix $W_{E}\\in\\mathbb{R}^{d\\times V}$ , where each column is the embedding of a token in the vocabulary. We construct $W_{E}$ such that it always sets the first dimension of every embedding vector to be 1, and sets the second dimension to 0 except the token $<b o s>$ . We always assume $<b o s>$ is the first token in the vocabulary, i.e. the first column. Then, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{E}=\\left[\\begin{array}{c c c c c}{1}&{1}&{1}&{...}&{1}\\\\ {1}&{0}&{0}&{...}&{0}\\\\ {0}&{0}&{0}&{...}&{0}\\\\ {e_{4,1}}&{e_{4,2}}&{e_{4,3}}&{...}&{e_{4,V}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{...}&{\\vdots}\\\\ {e_{d,1}}&{e_{d,2}}&{e_{d,3}}&{...}&{e_{d,V}}\\end{array}\\right]_{d\\times V}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $e_{d,i}\\in\\mathbb{R}$ . ", "page_idx": 30}, {"type": "text", "text": "Then, we use the word embedding matrix $W_{E}$ to compute the embedding $H^{(0)}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pmb{H}^{(0)}=\\pmb{W}_{E}\\pmb{X}=\\left[\\begin{array}{c c c c c}{1}&{1}&{1}&{...}&{1}\\\\ {1}&{0}&{0}&{...}&{0}\\\\ {0}&{0}&{0}&{...}&{0}\\\\ {e_{4,1}}&{e_{4,2}}&{e_{4,3}}&{...}&{e_{4,T+1}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{...}&{\\vdots}\\\\ {e_{d,1}}&{e_{d,2}}&{e_{d,3}}&{...}&{e_{d,T+1}}\\end{array}\\right]_{d\\times(T+1)}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Second, for head dimensions $h\\geq1$ , we construct the weights ${\\pmb W}_{K},{\\pmb W}_{V},{\\pmb W}_{O}$ of the first attention head in the first layer. $W_{Q}$ can be any arbitrary matrix. Specifically, ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{K}={\\left[\\begin{array}{l l l l}{1}&{0}&{\\dots}&{0}\\\\ {1}&{0}&{\\dots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {1}&{0}&{\\dots}&{0}\\end{array}\\right]}_{h\\times d}W_{V}={\\left[\\begin{array}{l l l l}{0}&{M}&{\\dots}&{0}\\\\ {1-\\mathcal{H}}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{\\dots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\dots}&{0}\\end{array}\\right]}_{h\\times d}W_{O}={\\left[\\begin{array}{l l l l l}{0}&{0}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{0}&{\\dots}&{0}\\\\ {1}&{-1}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{0}&{\\dots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{0}&{\\dots}&{0}\\end{array}\\right]}_{d\\times h}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "$W_{K}$ reads from the first dimension of the hidden state. Since all word embeddings have 1 in their first dimension, this parameterization will result all key vectors to be the same. Considering $k_{i}=W_{K}h_{i}^{(0)}$ , then: ", "page_idx": 30}, {"type": "equation", "text": "$$\nk_{1}={\\left[\\begin{array}{l}{1}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]}_{h\\times1},k_{2}={\\left[\\begin{array}{l}{1}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]}_{h\\times1},\\ldots,k_{T+1}={\\left[\\begin{array}{l}{1}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]}_{h\\times1}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We use $W_{Q}$ to compute the query vector $\\pmb q_{t}$ by applying $\\pmb{q}_{t}=\\pmb{W}_{Q}\\pmb{h}_{t}^{(0)}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pmb q_{t}=[q_{1},q_{2},...,q_{h}]^{T}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $q_{j}\\in\\mathbb{R}$ can take any arbitrary value. ", "page_idx": 30}, {"type": "text", "text": "Next, we compute the attention weight vectors $\\alpha$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha=[\\langle\\pmb{q}_{t},\\pmb{k}_{1}\\rangle,\\langle\\pmb{q}_{t},\\pmb{k}_{2}\\rangle,\\cdots,\\langle\\pmb{q}_{t},\\pmb{k}_{t}\\rangle]^{T}}\\\\ &{\\quad=[\\alpha^{*},\\alpha^{*},...,\\alpha^{*}]^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\alpha^{*}=q_{1}+q_{2}+\\cdot\\cdot\\cdot+q_{h}$ . ", "page_idx": 31}, {"type": "text", "text": "Then, we apply SoftMax to compute the attention weight score. Since all $\\alpha^{*}$ are the same, we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=\\mathrm{SoftMax}(\\alpha)=[\\frac{1}{t},\\frac{1}{t},\\dots,\\frac{1}{t}]^{T}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, we compute the value vectors by applying $\\pmb{v}_{i}=\\pmb{W}_{V}\\pmb{h}_{i}^{(0)}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{1}=\\left[\\!\\!\\begin{array}{c}{M}\\\\ {1-\\mathcal{H}}\\\\ {\\vdots}\\\\ {0}\\end{array}\\!\\!\\right]_{h\\times1},v_{2}=\\left[\\!\\!\\begin{array}{c}{0}\\\\ {1-\\mathcal{H}}\\\\ {\\vdots}\\\\ {0}\\end{array}\\!\\!\\right]_{h\\times1},\\ldots,v_{t}=\\left[\\!\\!\\begin{array}{c}{0}\\\\ {1-\\mathcal{H}}\\\\ {\\vdots}\\\\ {0}\\end{array}\\!\\!\\right]_{h\\times1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, we compute the attention value: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{i\\leq t}\\hat{\\alpha}_{i}\\pmb{v}_{i}=\\frac{1}{t}\\sum_{i\\leq t}\\pmb{v}_{i}=[\\frac{M}{t},1-\\mathcal{H},\\mathscr{\\ldots},0]^{T}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, we compute the output of the attention head by applying $W_{O}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\pmb{o}_{t}=W_{O}\\left(\\sum_{i\\leq t}\\hat{\\alpha}_{i}\\pmb{v}_{i}\\right)=[0,0,\\frac{M}{t}-1+\\mathcal{H},\\dots,0]^{T}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From Equ.24, it can be observed that when $t<M$ , the hidden state value on the third dimension is greater than $\\mathcal{H}$ , indicating a successful extrapolation. Conversely, when $t>M$ , the hidden state value on the third dimension is less than $\\mathcal{H}$ , indicating a failed extrapolation. ", "page_idx": 31}, {"type": "text", "text": "Next, we provide the proof for Theorem 3.2 as below: ", "page_idx": 31}, {"type": "text", "text": "Theorem E.2 (PE Extrapolation). Let $x=[<b o s>,x_{1},...\\,,x_{T}]$ be an input sequence of length $T{+}I$ to the model. Consider an simple relative $P E$ schema where dot product between query $\\pmb q_{t}$ and key $k_{i}$ at positions $t$ and $i$ $\\mathit{\\check{\\Psi}}(\\mathit{\\Sigma})\\geq i)$ can be expressed as: $\\langle\\pmb q_{t},\\pmb k_{i}\\rangle:=\\pmb q_{t}^{T}\\pmb k_{i}-(t-i)$ . Then, there exists $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices, such that when $T<M$ , $o_{T}>\\mathcal{H}$ ; and when $T>M$ , $o_{T}<\\mathcal{H}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. We regard the third dimension as a specific dimension. And there is a threshold in this dimension in the second layer. Our proof construct the matrices on the first two layers. The first layer is used to extract position information. The second layer will generate a significant hidden state value in its third dimension. If the input length exceeds the max training length, the hidden state value will exceed the threshold. ", "page_idx": 31}, {"type": "text", "text": "(1)Consider the first layer operation. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "First, following Theorem E.1, we construct the same word embedding matrix $W_{E}$ ", "page_idx": 31}, {"type": "text", "text": "For head dimensions $h\\geq3$ , we construct the weights $W_{K}$ and $W_{V}$ for the first attention head in the first layer. $W_{Q}$ still can be any arbitrary matrix. Specifically, ", "page_idx": 31}, {"type": "equation", "text": "$$\nW_{K}=\\left[\\!\\!\\begin{array}{c c c}{\\mathbf{0}}&{\\dots}&{\\mathbf{0}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}}&{\\dots}&{\\mathbf{0}}\\end{array}\\!\\!\\right]_{h\\times d},W_{V}=\\left[\\!\\!\\begin{array}{c c c c}{\\mathbf{0}}&{\\mathbf{1}}&{\\dots}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\dots}&{\\mathbf{0}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\dots}&{\\mathbf{0}}\\end{array}\\!\\!\\right]_{h\\times d}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since the elements in $W_{K}$ is all 0, by applying $k_{i}=W_{K}h_{i}^{(0)}$ , we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\nk_{1}=\\left[\\!\\!\\begin{array}{c}{\\!\\!0}\\\\ {\\!\\!0}\\\\ {\\!\\!\\vdots\\!\\!}\\\\ {\\!\\!0\\!\\!}\\end{array}\\!\\!\\right]_{h\\times1},k_{2}=\\left[\\!\\!\\begin{array}{c}{\\!\\!0}\\\\ {\\!\\!0}\\\\ {\\!\\!\\vdots\\!\\!}\\\\ {\\!\\!0\\!\\!}\\end{array}\\!\\!\\right]_{h\\times1},\\ldots,k_{T+1}=\\left[\\!\\!\\begin{array}{c}{\\!\\!0}\\\\ {\\!\\!0}\\\\ {\\!\\!\\vdots\\!\\!}\\\\ {\\!\\!0\\!\\!}\\end{array}\\!\\!\\right]_{h\\times1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We use $W_{Q}$ to compute the query vector $\\pmb q_{t}$ by applying $\\pmb{q}_{t}=\\pmb{W}_{Q}\\pmb{h}_{t}^{(0)}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb q_{t}=[q_{1},q_{2},...,q_{h}]^{T}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $q_{j}\\in\\mathbb{R}$ can take any arbitrary value. ", "page_idx": 32}, {"type": "text", "text": "Next, we compute the attention weight vectors $\\alpha$ . Consider $\\pmb{q}_{t}^{T}\\pmb{k}_{i}=0$ and position information $t$ and $i$ , we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb{\\alpha}=[-(t-1),-(t-2),\\ldots,0]^{T}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, apply SoftMax to $_{\\alpha}$ , we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=\\mathrm{SoftMax}(\\alpha)=[\\frac{e^{-(t-1)}}{S},\\frac{e^{-(t-2)}}{S},\\dots,\\frac{e^{0}}{S}]^{T}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where define $\\begin{array}{r}{S(t)=\\sum_{j=0}^{t-1}e^{-j}}\\end{array}$ . We adopt its abbreviation $S$ . ", "page_idx": 32}, {"type": "text", "text": "Now, we compute the value vectors by applying $\\pmb{v}_{i}=\\pmb{W}_{V}\\pmb{h}_{i}^{(0)}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb{v}_{1}=\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right],\\pmb{v}_{2}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right],\\pmb{\\ldots},\\pmb{v}_{t}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, we compute the attention value: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i\\leq t}\\hat{\\alpha}_{i}\\pmb{v}_{i}=\\frac{e^{-(t-1)}}{S}\\pmb{v}_{1}=[\\frac{e^{-(t-1)}}{S},0,\\dots,0]^{T}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "After that, we compute the output of the attention head by applying $W_{O}$ (using the same construction as Equ.16 in Theorem E.1): ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\boldsymbol{o}_{t}=W_{O}\\left(\\sum_{i\\leq t}\\hat{\\alpha_{i}}\\boldsymbol{v}_{i}\\right)=[0,0,\\frac{e^{-(t-1)}}{S},\\dots,0]^{T}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, e\u2212(t\u22121) can be regarded as a monotonically decreasing function of $t$ . Then through the MLP feedforward layer, we can always let the MLP layer recover the value of $t$ . ", "page_idx": 32}, {"type": "text", "text": "Therefore, we can get that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb{H}^{(l=1)}=\\left[\\begin{array}{c c c c c c}{1}&{1}&{1}&{...}&{1}\\\\ {1}&{0}&{0}&{...}&{0}\\\\ {1}&{2}&{3}&{...}&{T+1}\\\\ {e_{4,1}}&{e_{4,2}}&{e_{4,3}}&{...}&{e_{4,T+1}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{...}&{\\vdots}\\\\ {e_{d,1}}&{e_{d,2}}&{e_{d,3}}&{...}&{e_{d,T+1}}\\end{array}\\right]_{d\\times(T+1)}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where position information is embedded in the third dimension of hidden state. ", "page_idx": 32}, {"type": "text", "text": "(2)Consider the second layer operation. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For head dimension $h\\geq3$ , we construct the weights $W_{Q}$ and $W_{K}$ for the first attention head in the second layer. $W_{V}$ follows the Theorem 3.1 setting. Specifically, ", "page_idx": 32}, {"type": "equation", "text": "$$\nW_{Q}=\\left[\\begin{array}{c c c c c c}{0}&{0}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{1}&{\\dots}&{0}\\\\ {q_{4,1}}&{q_{4,2}}&{q_{4,3}}&{\\dots}&{q_{4,d}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {q_{h,1}}&{q_{h,2}}&{q_{h,3}}&{\\dots}&{q_{h,d}}\\end{array}\\right]_{h\\times d},W_{K}=\\left[\\begin{array}{c c c c c c}{0}&{0}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{0}&{\\dots}&{0}\\\\ {0}&{0}&{-1}&{\\dots}&{0}\\\\ {1}&{0}&{0}&{\\dots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {1}&{0}&{0}&{\\dots}&{0}\\end{array}\\right]_{h\\times d},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, by applying $\\pmb q_{i}=\\pmb W_{Q}\\pmb h_{i}^{(0)}$ , we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb q}_{1}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {1}\\\\ {q_{4}}\\\\ {\\vdots}\\\\ {q_{h}}\\end{array}\\right]_{h\\times1},{\\pmb q}_{2}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {2}\\\\ {q_{4}}\\\\ {\\vdots}\\\\ {q_{h}}\\end{array}\\right]_{h\\times1},}\\end{array}_{,}\\dots,{\\pmb q}_{T+1}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {T+1}\\\\ {q_{4}}\\\\ {\\vdots}\\\\ {q_{h}}\\end{array}\\right]_{h\\times1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $q_{j}$ can be arbitrary value for $j\\geq4$ . ", "page_idx": 33}, {"type": "text", "text": "By applying $k_{i}=W_{K}h_{i}^{(0)}$ , we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\nk_{1}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {-1}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]_{h\\times1},k_{2}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {-2}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]_{h\\times1},k_{T+1}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {-(T+1)}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]_{h\\times1}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n{\\pmb q}_{t}^{T}{\\pmb k}_{i}=\\sum_{4<=j<=h}q_{j}+(t-i)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Next, according to the PE definition, apply the position information $t$ and $i$ to the dot production, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle=\\sum_{4<=j<=h}q_{j}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": ", which means $\\langle\\pmb q_{t},\\pmb k_{i}\\rangle=\\langle\\pmb q_{t},\\pmb k_{l}\\rangle$ for any $l\\neq i$ . ", "page_idx": 33}, {"type": "text", "text": "Then, we know that, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pmb{\\alpha}=[\\alpha^{*},\\alpha^{*},\\ldots,\\alpha^{*}]^{T}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where \u03b1\u2217= 4\u2264j\u2264t qj. ", "page_idx": 33}, {"type": "text", "text": "Then, apply SoftMax to $_{\\alpha}$ , we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=\\mathrm{SoftMax}(\\alpha)=[\\frac{1}{t},\\frac{1}{t},\\cdot\\cdot\\cdot,\\frac{1}{t}]^{T}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Next, follow same process as Equ.(21) (22) (23) (24) in Theorem E.1, we can get same result in the second layer. It shows the limitation of extraordinary for position encoding. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Next, we provide the proof for Theorem 3.3 as below: ", "page_idx": 33}, {"type": "text", "text": "Theorem E.3 (Weave PE Extrapolation). Let $N$ be a positive constant. Consider a simple weave $P E$ extrapolation schema: when $t-i<N$ , $\\mathcal{W}(t-i)=t-i,$ ; and when $t-i\\geq N$ , $\\mathcal{W}(t-i)=N$ . Then, the attention dot product is fixed as below: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=\\left\\{\\begin{array}{r l r}{q_{t}^{T}k_{i}-(t-i)}&{{},}&{t-i<N}\\\\ {q_{t}^{T}k_{i}-N}&{{},}&{t-i\\geq N}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": ", where $N\\ll M$ . Then, applying $W_{Q},W_{K},W_{V},W_{O},W_{1},$ , and $W_{2}$ matrices from Theorem 3.2, we have when $T>M$ , $o_{T}>\\mathcal{H}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Our proof is closely related to Theorem 3.2. We completely adopt the LLM setting by Theorem 3.2, and only modify the representation of the relative position. The purpose is to illustrate that effective extrapolation beyond the maximum window length can be achieved only by rearranging the relative position. ", "page_idx": 33}, {"type": "text", "text": "Following Theorem E.2, we can get $H^{(0)}$ ", "page_idx": 33}, {"type": "text", "text": "Then, compute the attention weight vectors $_{\\alpha}$ . Consider $\\pmb{q}_{t}^{T}\\pmb{k}_{i}=0$ and position information $t$ and $i$ . By applying this Extrapolation schema, we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\alpha=[-N,-N,\\ldots,-1,0]^{T}}\\\\ {\\ \\ \\ =[-(t-(t-N)),-(t-(t-N)),\\ldots,-1,0]^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Follow Eqs.28 and 29, and apply SoftMax to $_{\\alpha}$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=\\mathrm{SoftMax}(\\alpha)=[\\frac{e^{-(t-(t-N))}}{S^{\\#}},\\frac{e^{-(t-(t-N))}}{S^{\\#}},\\dots,\\frac{e^{0}}{S^{\\#}}]^{T}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $t>N$ and define $\\begin{array}{r}{S^{\\#}(t)=\\sum_{j=0}^{N-1}e^{-j}+(t-N)e^{-N}}\\end{array}$ . We adopt the abbreviation $S^{\\#}$ where there is no confusion. ", "page_idx": 34}, {"type": "text", "text": "Then, compute the value vector by applying $\\pmb{v}_{i}=\\pmb{W}_{V}\\pmb{h}_{i}^{(0)}$ , still have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\pmb{v}_{1}=\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right],\\pmb{v}_{2}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right],\\pmb{\\ldots},\\pmb{v}_{t}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, we compute the attention value: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i\\leq t}\\hat{\\alpha}_{i}v_{i}=\\frac{e^{-(t-(t-N))}}{S^{\\#}}v_{1}=[\\frac{e^{-(t-(t-N))}}{S^{\\#}},0,\\dots,0]^{T}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "After that, we compute the output of the attention head by applying $W_{O}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\pmb{o}_{t}=W_{O}\\left(\\sum_{i\\leq t}\\hat{\\alpha}_{i}\\pmb{v}_{i}\\right)=[0,0,\\frac{e^{-(t-(t-N))}}{S^{\\#}},\\dots,0]^{T}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Notice that we set the MLP feed-forward process simulate a a piecewise function about $\\frac{e^{-(t-1)}}{S(t)}$ which produce a Integer $t$ to recover the position. ", "page_idx": 34}, {"type": "text", "text": "Considering t > N, we know e\u2212S((tt\u2212)1) $\\begin{array}{r}{\\frac{e^{-(t-1)}}{S(t)}\\,<\\,\\frac{e^{-(t-(t-N))}}{S^{\\#}(t)}\\,<\\,\\frac{e^{-(t-(t-N))}}{S(N+1)}\\ }\\end{array}$ . And the e\u2212(t\u2212(t\u2212N)) function decreases extremely slowly as $t$ increases, almost equal to $\\frac{e^{-(t-(t-N))}}{S(N+1)}$ ", "page_idx": 34}, {"type": "text", "text": "At the same time, the e\u2212S((tt\u2212)1) function decreases very quickly when $t$ is relatively small; As $t$ increases, it also decreases extremely slowly. ", "page_idx": 34}, {"type": "text", "text": "Combining the characteristics of these two functions, we can always choose a relatively small $N$ to keep it away from M (that is N \u226aM), so that the value recovered after e\u2212(tS\u2212#((tt\u2212)N)) passing through MLP process is around $N+1$ , because $\\frac{e^{-(t-(t-N))}}{S(N+1)}$ can recover N + 1 and e\u2212(tS\u2212#((tt\u2212)N)) i s almost equal to e\u2212(t\u2212(t\u2212N)). For the sake of simplicity, considering the rounding characteristics of piecewise functions, we might as well set e\u2212(tS\u2212#((tt\u2212)N)) to recover $N+1$ . ", "page_idx": 34}, {"type": "text", "text": "Therefore, we can get the hidden state like that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}^{(l=1)}=\\left[\\begin{array}{c c c c c c c}{1}&{1}&{\\hdots\\cdot}&{1}&{\\hdots\\cdot}&{1}\\\\ {1}&{0}&{\\hdots\\cdot}&{0}&{\\hdots\\cdot}&{0}\\\\ {1}&{2}&{\\hdots\\cdot}&{N+1}&{\\hdots}&{N+1}\\\\ {e_{4,1}}&{e_{4,2}}&{\\hdots\\cdot}&{e_{4,3}}&{\\hdots\\cdot}&{e_{4,T+1}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {e_{d,1}}&{e_{d,2}}&{\\cdot\\cdot}&{e_{d,3}}&{\\cdot\\cdot\\cdot}&{e_{d,T+1}}\\end{array}\\right]_{d\\times(T+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Continue the second layer operations. ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "By applying $\\pmb q_{i}=\\pmb W_{Q}\\pmb h_{i}^{(0)}$ , we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\nq_{1}=\\left[\\!\\!\\begin{array}{c}{{0}}\\\\ {{0}}\\\\ {{1}}\\\\ {{q_{4}}}\\\\ {{\\vdots}}\\\\ {{q_{h}}}\\end{array}\\!\\right],q_{2}=\\left[\\!\\!\\begin{array}{c}{{0}}\\\\ {{0}}\\\\ {{2}}\\\\ {{q_{4}}}\\\\ {{\\vdots}}\\\\ {{q_{h}}}\\end{array}\\!\\!\\right],\\ldots,\\pmb{q}_{(t>N)}=\\left[\\!\\!\\begin{array}{c}{{0}}\\\\ {{0}}\\\\ {{N+1}}\\\\ {{q_{4}}}\\\\ {{\\vdots}}\\\\ {{q_{h}}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $q_{j}$ can be arbitrary value for $j\\geq4$ . ", "page_idx": 35}, {"type": "text", "text": "By applying $k_{i}=W_{K}h_{i}^{(0)}$ , we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\nk_{1}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {-1}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right],k_{2}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {-2}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right],\\ldots,k_{(i>N)}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {-(N+1)}\\\\ {1}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, consider $t>N$ , and apply Extrapolation. ", "page_idx": 35}, {"type": "text", "text": "when $t-i<N$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle-f_{r e l}(t-i)=\\sum_{4\\leq j\\leq h}q_{j}-(t-i)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "when $t-i\\geq N$ and $i>N$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle-f_{r e l}(t-i)=\\sum_{4\\leq j\\leq h}q_{j}-N\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "when $t-i\\geq N$ and $i<=N$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle-f_{r e l}(t-i)=\\sum_{4\\leq j\\leq h}q_{j}+(N+1-i)-N\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let $\\begin{array}{r}{\\tau=\\sum_{4\\leq j\\leq h}q_{j}}\\end{array}$ . We can get the attention score: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\alpha=\\underbrace{[\\tau-0,\\tau-1,\\ldots,\\tau-(N-1)}_{\\mathrm{first-part}},\\underbrace{\\tau-N,\\ldots,\\tau-N}_{\\mathrm{second-part}},\\underbrace{\\tau-(N-1),\\ldots,\\tau-1,\\tau-0}_{\\mathrm{third-part}}]^{T}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where third\u2212part represents Equ.49, second\u2212part represents Equ.50, and first\u2212part represents Equ.51. ", "page_idx": 35}, {"type": "text", "text": "Apply SoftMax to $_{\\alpha}$ , we can get that: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\boldsymbol{\\hat{\\alpha}}=\\mathrm{SoftMax}(\\alpha)=[\\hat{\\alpha_{1}},\\hat{\\alpha_{2}},\\dots,\\hat{\\alpha_{t}}]^{T}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since the first element of $_{\\alpha}$ is the maximum value, $\\hat{\\alpha_{1}}$ must be greater than $\\frac{1}{t}$ . ", "page_idx": 35}, {"type": "text", "text": "Next, similar process, we compute the value vectors by applying $\\pmb{v}_{i}=\\pmb{W}_{V}\\pmb{h}_{i}^{(0)}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\nv_{1}=\\left[\\!\\!\\begin{array}{c}{{M}}\\\\ {{1-\\mathcal{H}}}\\\\ {{\\vdots}}\\\\ {{0}}\\end{array}\\!\\!\\right],v_{2}=\\left[\\!\\!\\begin{array}{c}{{0}}\\\\ {{1-\\mathcal{H}}}\\\\ {{\\vdots}}\\\\ {{0}}\\end{array}\\!\\!\\right],\\ldots,v_{t}=\\left[\\!\\!\\begin{array}{c}{{0}}\\\\ {{1-\\mathcal{H}}}\\\\ {{\\vdots}}\\\\ {{0}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, we compute the attention value: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{i\\leq t}\\hat{\\alpha_{i}}v_{i}=[\\hat{\\alpha_{1}}\\cdot M,1-\\mathcal{H},\\ldots,0]^{T}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, we compute the output of the attention head by applying $W_{O}$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{o}_{t}={W}_{O}\\left(\\sum_{i\\leq t}\\hat{\\alpha}_{i}\\pmb{v}_{i}\\right)=[0,0,\\mathcal{H}+\\hat{\\alpha_{1}}\\cdot M-1,\\ldots,0]^{T}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When $t>M$ , due to $\\hat{\\alpha_{1}}>\\frac{1}{t}$ , it\u2019s possible to satisfy $\\boldsymbol{\\hat{\\alpha}}_{1}\\cdot\\boldsymbol{M}-1>0$ condition, such that the hidden state value $o_{t}$ in the third dimension is greater than $\\mathcal{H}$ . \u53e3 ", "page_idx": 36}, {"type": "text", "text": "We conjecture that while the attention mechanism in RoPE Su et al. (2023) involves multiplying by the rotation matrix of the relative position, it\u2019s essential to acknowledge that neural networks possess the capability to model any function. This implies the potential to decompose the relative position information into an additive form, as exemplified in our extrapolation scheme detailed in Theorem 3.3. ", "page_idx": 36}, {"type": "text", "text": "Next, we provide the proof for Corollary 4.1 as below: ", "page_idx": 36}, {"type": "text", "text": "Corollary E.4 (Mesa Extrapolation). Let $N$ be a positive constant. Consider a simple Stair $P E$ extrapolation schema, and the attention dot product is fixed as: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\langle q_{t},k_{i}\\rangle:=f_{\\mathrm{stairPE}}(q_{t},k_{i},t-i)=\\left\\{\\begin{array}{r l r}{q_{t}^{T}k_{i}-(t-i)}&{{},}&{t-i<N}\\\\ {q_{t}^{T}k_{i}-I}&{{},}&{t-i\\geq N}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $N\\ll M$ , $\\begin{array}{r}{I=N+\\left\\lceil\\frac{t-i-N}{E}\\right\\rceil}\\end{array}$ , and the extrapolated width $E$ is a constant. Then, Apply $W_{Q}$ , $W_{K}$ , $W_{V}$ , $W_{O}$ , $W_{1}$ , and $W_{2}$ matrices from Theorem 3.2. Although $T>M$ , it still $o_{T}>\\mathcal{H}$ . ", "page_idx": 36}, {"type": "text", "text": "Proof. Due to the striking similarity between Stair PE and the positional arrangement scheme proposed in Theorem 3.3, the proof process largely mirrors Theorem 3.3. Following the proof structure of Theorem 3.3, in the first layer, according to Equ.45 and .46, we obtain the hidden state matrix as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{H}^{(l=1)}=\\left[\\begin{array}{c c c c c c c}{1}&{1}&{\\hdots\\cdot}&{1}&{\\hdots\\cdot}&{1}\\\\ {1}&{0}&{\\hdots}&{0}&{\\hdots\\cdot}&{0}\\\\ {1}&{2}&{\\hdots}&{I_{N}}&{\\hdots\\cdot}&{I_{T+1}}\\\\ {e_{4,1}}&{e_{4,2}}&{\\hdots}&{e_{4,N}}&{\\hdots}&{e_{4,T+1}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {e_{d,1}}&{e_{d,2}}&{\\hdots}&{e_{d,N}}&{\\hdots\\cdot}&{e_{d,T+1}}\\end{array}\\right]_{d\\times(T+1)}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $I$ is defined on Stair PE extrapolation schema. ", "page_idx": 36}, {"type": "text", "text": "In the proof of the second layer, we still get the similar result like Equ.52, as below: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha=\\underbrace{{{\\left[{\\tau-0,\\tau-1,\\dots,\\tau-\\bar{\\tau}}\\right.}\\right.}}_{\\mathrm{first-part}},}\\\\ &{\\qquad\\underbrace{\\tau-\\bar{c},\\tau-\\bar{c}-1,\\dots,\\tau-N-1}_{\\mathrm{second-part}},\\underbrace{\\dots,\\dots,\\dots,\\tau}_{\\mathrm{repeat-part}}}\\\\ &{\\qquad\\underbrace{\\tau-N-1,\\tau-\\bar{c}-1,\\dots,\\tau-\\bar{\\mathcal{C}}}_{\\mathrm{third-part}},}\\\\ &{\\qquad\\underbrace{\\tau-\\bar{c},\\dots,\\tau-1,\\tau-0}_{\\mathrm{fourth-part}}]^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\mathcal{C}$ is a constant, and each element within repeat-part is smaller than $\\tau-0$ . ", "page_idx": 36}, {"type": "text", "text": "Apply SoftMax to $_{\\alpha}$ , we can get that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\boldsymbol{\\hat{\\alpha}}=\\mathrm{SoftMax}(\\alpha)=[\\hat{\\alpha_{1}},\\hat{\\alpha_{2}},\\dots,\\hat{\\alpha_{t}}]^{T}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which also shows that the first element of $_{\\alpha}$ is the maximum value and $\\hat{\\alpha_{1}}$ must be greater than $\\textstyle{\\frac{1}{t}}$ .   \nConsequently, following Equ.56, we reach the same conclusion. ", "page_idx": 37}, {"type": "text", "text": "F Probe Experiment Visualization ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We hypothesize that when the input length surpasses the effective window length of the model, some dimensions\u2019 values in the exceeded positions will experience a jump as the position changes. ", "page_idx": 37}, {"type": "text", "text": "To investigate this jump phenomenon\u2019s correlation with extrapolation failure, we design the following experiment: we adopt a standardized input by repeating the word \"hello\" 8000 times, resulting in 8001 tokens (automatically fill in initial token) after tokenization. For a more accurate explanation, we take the LLaMA2-7B-Chat model and list the sequences converted by the tokenizer, as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\nt o k e n s=[1,22172,\\dots,22172]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where 1 denotes the initial token $<b o s>$ , and 22172 denotes the word \"hello\". It\u2019s noted that the initial token $<b o s>$ is filled in automatically. ", "page_idx": 37}, {"type": "text", "text": "F.1 Normal Case ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We input this token sequence into the model and observe the hidden state. We focus on the hidden states produced by the first 11 layers, specifically selecting the position intervals from 4000 to 5000 and the last 1000 positions. We then concatenated the hidden states from these two intervals. This selection was deliberate, considering that the LLaMA2-7B-Chat model\u2019s training length is 4096, implying the effective input window is in proximity to this location. ", "page_idx": 37}, {"type": "text", "text": "Following this, we created a matrix graph, yielding the following results Fig.11: ", "page_idx": 37}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/dae7848cb322192553d3cf3f8b601bd82a7c28c83cf494b2f42e62ece1b80bf1.jpg", "img_caption": ["Figure 11: Origin with normal input token visualization on LLaMA2-7B-Chat model for position regions between (4000-5000, 7000-8000) for first 11 layers. X-axis represent positions, which correspond position regions between 4000-5000 and 7000-8000. Y-axis represent Dimentions from first 0-th to first 640-th. Some observation: noticeable color jumps in most dimensions as position changes. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "In Fig.11, the X-axis denotes the position of the input token, ranging from 0 to 1000, representing tokens at positions 4000 to 5000. The scale of 1000-2000 represents tokens at positions 7001 to 8001. The Y-axis signifies the token dimension, ranging from small to large. The LLaMA2-7B-Chat model has a total of 4096 dimensions, and we display the first 640 dimensions in Y-axis. Red color means greater than 0, and blue color means less than 0. Fig.11 shows some noticeable jump from 0-th to 640-th dimensions, especially at the 1000 scale for the original model. ", "page_idx": 37}, {"type": "text", "text": "F.2 Extrapolation Case ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Given the effectiveness of the extrapolation method ReRoPE in extending to lengths of up to $8\\mathbf{k}$ , we apply it to the LLaMA2-7B-Chat model. Employing the same conditions as those in the Normal Case settings, the results are as follows: ", "page_idx": 37}, {"type": "text", "text": "In Fig.12, the obvious jumping phenomenon is successfully suppressed. It can be seen that each dimension at different positions still maintains consistent values. This illustrates that by suppressing sudden changes in the values of these dimensions, extrapolation can be made successful even outside the effective window length. ", "page_idx": 37}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/87370e082c5cb280eb8b0b2460d8efb777a827dc889dd9b5fe837ab566df3b57.jpg", "img_caption": ["Figure 12: ReRoPE with normal input tokens visualization on LLaMA2-7B-Chat model for regions between (4000-5000, 7000-8000) for first 11 layers. X-axis represent positions, which correspond position regions between 4000-5000 and 7000-8000. Y-axis represent Dimentions from first 0-th to first 640-th. Observation: Each dimension stays consistently at each position "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Overall, comparing Normal Case and Extrapolation Case again, it shows that using the extrapolation method can suppress mutations in dimensions, thereby making the extrapolation successful. ", "page_idx": 38}, {"type": "text", "text": "Additionally, for layers 0 and 1, the hidden state values of different positions in each dimension are nearly identical. According to our Theorem 3.2, layer 1 may extract position information, while subsequent layers determine distances beyond the effective window, generating a positive or negative jump signal. We speculate that this signal change may not be strictly positive or negative, but there is a threshold. When the signal exceeds the threshold, the extrapolation will fail; when the signal does not exceed the threshold, the extrapolation will be successful. ", "page_idx": 38}, {"type": "text", "text": "We also use the LLaMA-3B model and the Vicuna-13B model to compare and demonstrate the effects of using Origin and ReRoPE, as follows on below Fig.15 and .16. ", "page_idx": 38}, {"type": "text", "text": "F.3 Validating extrapolation using observed thresholds ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We continue our probe experiments using the \"hello\", and input sequences is up to 16000 tokens. These were validated on the LLaMA2-7B-Chat model. Given that the hidden state in LLaMA2 has a total of 4096 dimensions, we selected the first 20 dimensions to search observable thresholds. In the main text, we present the prediction results for the first and 6-th dimensions. The results for the 7-th and 9-th dimensions are shown in Figure 13. ", "page_idx": 38}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/6adfdeee624c44ee48645949edee0c0a740929fedac8aa90c6ddcae9aaf8223a.jpg", "img_caption": ["Figure 13: Thresholds for hidden states observed at specific dimensions on LLaMA2-7B-Chat, allowing for extrapolative judgments based on these thresholds. The red dashed line denotes the observed threshold, while the green dashed line indicates the position of extrapolation failure for the Dynamic-NTK based on the observed threshold. The black dashed line indicates the maximum training length of the model. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 13 also shows the same results as Figure 2 in the main text. When the length of the input sequence is around $12\\mathbf{k}$ , the hidden state values of Dynamic-NTK in the 7-th and 9-th dimensions surpass the thresholds, implying extrapolation failure. Due to not outside the thresholds, ReRoPE shows successful extrapolation. ", "page_idx": 38}, {"type": "text", "text": "Our experiments also reveal that not every dimension exhibits a clear threshold. Considering that the Transformer model is a type of neural network, we hypothesize that these dimensions with threshold undergo significant signal changes due to the characteristics of activation functions. These changes are amplified through successive layers, eventually leading to model output failures. We believe this internal mechanism explains why observable thresholds can predict the success or failure of extrapolation. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "We also validate our findings using the Vicuna-13B model, and the results are shown in Figure 14: ", "page_idx": 39}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/589f6e0d675c56d9a9da970a6db9b9567ebd22a9d9de348972900df241a5b9c6.jpg", "img_caption": ["Figure 14: Thresholds for hidden states observed at specific dimensions on Vicuna-13B, allowing for extrapolative judgments based on these thresholds. The red dashed line denotes the observed threshold, while the green dashed line indicates the position of extrapolation failure for the Dynamic-NTK based on the observed threshold. The black dashed line indicates the maximum training length of the model. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 14 shows the hidden state value on the specific 4-th and 10-th dimensions. Similar to the observations with LLaMA2-7B-Chat model, we find no significant differences for the hidden state values at the first layer. However, substantial differences are observed at the fourth layer. Based on these observed thresholds, we can predict that Dynamic-NTK will fail to extrapolate when the input length approaches around 9k. Conversely, ReRoPE can extrapolate further. These predictions align well with our subsequent experiments, further validating the feasibility of using observed thresholds to determine extrapolation success. ", "page_idx": 39}, {"type": "text", "text": "In contrast to LLaMA2-7B-Chat model, we observed threshold phenomena at the fourth layer in the Vicuna-13B model. We hypothesize that although our theoretical model predicts the threshold to occur at the second layer, considering the integration of positional information from preceding layers, this still aligns with our theoretical framework. ", "page_idx": 39}, {"type": "text", "text": "G Stair PE and Self-Extend ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The formulation of Self-Extend is as follows: ", "page_idx": 39}, {"type": "text", "text": "Let $t$ denote the position of query, $i$ denote the position of key, $i$ denote the position of key, $W$ denote the neighbor size and $G$ denote group size. ", "page_idx": 39}, {"type": "text", "text": "According to its equation 3 in Self-Extend (Jin et al. (2024)), $P=P\\,//\\,G$ , and the shifted relative position $\\bar{(W-W\\,^{\\prime}//\\,G)}$ , we can get the position of query as: ", "page_idx": 39}, {"type": "equation", "text": "$$\nt\\left/\\right/G+W-W\\left/\\right/G\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and the position of key as: ", "page_idx": 39}, {"type": "equation", "text": "$$\ni\\,//\\,G\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By RoPE, their relative position between $t$ and $i$ (consider $t>i$ ), is remapped to: ", "page_idx": 39}, {"type": "equation", "text": "$$\nt\\,//\\,G+W-W\\,//\\,G-i\\,//\\,G=W+t\\,//\\,G-i\\,//\\,G-W\\,//\\,G\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "It\u2019s worthy noting that only under the necessary and sufficient condition ( $t\\;\\mathrm{mod}\\;G>=i\\;\\mathrm{mod}\\;G\\;)$ ), we get that ", "page_idx": 39}, {"type": "equation", "text": "$$\nt\\left//\\left(G-i\\left//\\right.G=\\left(t-i\\right)//\\left.G\\right.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Next, when $W$ mod $G==0$ , the equation of Self-Extend becomes: ", "page_idx": 39}, {"type": "equation", "text": "$$\nW+(t-i)\\left//\\left(G-W//\\,G=W+(t-i-W)\\,//\\,G\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If we adjust this flooring operation to ceiling operation, and replace $_\\mathrm{N}$ with $\\mathrm{W}$ and $\\boldsymbol{\\mathrm E}$ with $\\mathrm{G}$ , then Stair-PE is equivalent to Self-Extend. ", "page_idx": 40}, {"type": "text", "text": "In summary, under conditions $\\pmb{t}$ mod $G\\ >=\\ i$ mod $\\boldsymbol{G}$ and $W$ mod $G\\:=\\:=\\:{\\bf0}$ , as well as changing the flooring operation to ceiling operartion in Self-Extend, these two formulas are equivalent. Except for this condition, they yield slightly different results. ", "page_idx": 40}, {"type": "text", "text": "Note because $W$ and $\\boldsymbol{G}$ are constants, the condition $W$ mod $G==0$ can be met easily. But $\\pmb{i}$ and $\\pmb{t}$ change with positions, therefore there will always be positions where the condition $\\pmb{t}$ mod $G>=i$ mod $\\pmb{G}$ is not satisfied. ", "page_idx": 40}, {"type": "equation", "text": "$$\nt=10,i=5,G=2,\\mathrm{but}\\,10\\,//\\,2-5\\,//\\,2\\neq(10-5)\\,//\\,2.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/2433031fdb03674749665c717ea3f6999ab6e48a25ea30be269449132a1b389b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "(a) Visualization of Hidden states with Origin Model for the first 11 layers and the first 640 dimensions.   \nObservation: noticeable color jumps in most dimensions as position changes, especially at X-axis 1000. ", "page_idx": 41}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/0fbeb7923fc0b3c840fad1831b3d0fa3c3353455daf772ad0d652de137d6bcef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "(b) Visualization of Hidden states with ReRoPE for the first 11 layers and the first 640 dimensions. Observation: Each dimension stays consistently at each position ", "page_idx": 41}, {"type": "text", "text": "Figure 15: Visualization of Origin and ReRoPE probe on LLaMA-3B model for position regions between (2000-3000, 4000-5000) ", "page_idx": 41}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/a78d941f08f7f4404e52f3e2210db43a5848db32122c00b2f942828ec78d7afd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "(a) Visualization of Hidden states with Origin Model for the first 11 layers and the first 640 dimensions.   \nObservation: noticeable color jumps in most dimensions as position changes, especially at X-aixs 1000. ", "page_idx": 41}, {"type": "image", "img_path": "zaXuMqOAF4/tmp/bd57e1ec9c56f6577492bf67a890a466b4ca1b961d894abf2293f1a5d31d75ba.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "(b) Visualization of Hidden states with ReRoPE for the first 11 layers and the first 640 dimensions. Observation: Each dimension stays nearly same without color jump as position changes. ", "page_idx": 41}, {"type": "text", "text": "Figure 16: Visualization of Origin and ReRoPE probe on the Vicuna-13B model for position regions between (2000-3000, 4000-5000) ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The abstract and introduction of our paper accurately reflect its contribution and scope. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We discuss the limitations on Section Discussion. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: We give a full proof on Appendix. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We disclose all the needed information. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide code and data. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We specify all the training and test details. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide detailed explanations about report. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g., negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide the hardware and software information about experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We follow the NIPS Code of Ethics. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We discuss the impacts on section Discussion. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We follow the license and terms. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 46}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We don\u2019t release new assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}]