[{"type": "text", "text": "Hamiltonian Score Matching and Generative Flows ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peter Holderrieth Yilun Xu MIT CSAIL NVIDIA   \nphold@mit.edu yilunx@nvidia.com Tommi Jaakkola MIT CSAIL tommi@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Classical Hamiltonian mechanics has been widely used in machine learning in the form of Hamiltonian Monte Carlo for applications with predetermined force fields. In this work, we explore the potential of deliberately designing force fields for Hamiltonian ODEs, introducing Hamiltonian velocity predictors (HVPs) as a tool for score matching and generative models. We present two innovations constructed with HVPs: Hamiltonian Score Matching (HSM), which estimates score functions by augmenting data via Hamiltonian trajectories, and Hamiltonian Generative Flows $(H G F s)$ , a novel generative model that encompasses diffusion models and flow matching as HGFs with zero force fields. We showcase the extended design space of force fields by introducing Oscillation HGFs, a generative model inspired by harmonic oscillators. Our experiments validate our theoretical insights about HSM as a novel score matching metric and demonstrate that HGFs rival leading generative modeling techniques. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hamiltonian mechanics is a cornerstone of classical physics, providing a powerful framework for analyzing the dynamics of physical systems [33, 4]. The Hamiltonian formalism has been widely applied in machine learning and Bayesian statistics via Hamiltonian Monte Carlo (HMC) [14, 36, 8]. In this setting, the goal is to sample from a probability distribution $\\pi$ whose density $\\pi(x)$ is known up to a normalization factor. In HMC, one interprets $\\dot{\\nabla}\\log\\pi(x)$ as a force function and plugs it into a Hamiltonian ODE to construct a fast-mixing Markov chain exploring the data space quickly. This makes HMC one of the most powerful sampling algorithms to date [36, 23]. ", "page_idx": 0}, {"type": "text", "text": "In generative modeling, the density $\\pi(x)$ is unknown, only data samples $x_{1},\\ldots,x_{n}\\sim\\pi$ are given, and the goal is to learn to generate novel samples from $\\pi$ . Current state-of-the-art models are based on diffusion [41, 42, 45, 21] and enjoy widespread success in image generation [39], molecular generation [11], and robotics [10]. Diffusion models learn the score function $\\nabla\\log\\pi_{\\sigma}(x)$ for a range of noise scales $\\sigma$ via denoising score matching (DSM) [48]. This enables one to subsequently generate high-quality samples by following a stochastic differential equation [45]. ", "page_idx": 0}, {"type": "text", "text": "In light of the success of HMC, it is natural to ask whether the Hamiltonian formalism can also improve generative models or provide novel insights into their construction. Previous works have exploited the connection between Hamiltonian physics and generative modeling for specific force fields [13]. However, these works usually consider particular (fixed) force fields and stay within the diffusion framework. ", "page_idx": 0}, {"type": "text", "text": "More recently, flow-based generative models such as flow matching have enabled scalable training of continuous normalizing flows (CNFs) [31, 32]. These ODE-based models allow to craft first-order ODEs transforming arbitrary distributions from one to another. Inspired by these successes, we consider the Hamiltonian ODE as a Neural ODE [7]. We show that we can marginalize out velocity distributions and then follow backward ODEs that faithfully recover data distributions. Importantly, we provide an associated theorem that holds for any force field. With the growing success of generative models in physical sciences [11, 49, 1], it is striking that most approaches do not use existing known force fields - sometimes leading to physically implausible results [1]. A framework that allows to reason natively about force fields in the context of generative models would be very promising for such applications [2, 12]. This work aims to build towards such a deeper integration. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We explore the intricate relationship between Hamiltonian dynamics, force fields, and generative models. Specifically, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. Hamiltonian velocity predictor: We introduce the concept of a Hamiltonian Velocity Predictor (HVP) and show the utility of HVPs for score matching and generative modeling (Section 3).   \n2. Hamiltonian score discrepancy (HSD): We introduce and validate Hamiltonian score discrepancy (HSD), a novel score matching metric based on HVPs and a corresponding score matching method (Section 4).   \n3. Hamiltonian generative flows (HGFs): We show that the location marginal of a Hamiltonian ODE is generated via the Hamiltonian Velocity Predictor (Section 5). This leads to a novel generative model generalizing diffusion models and flow matching (Section 6).   \n4. Oscillation HGFs: As special HGFs, we study Oscillation HGFs, a simple generative model rivaling the performance of diffusion models due to in-built scale-invariance (Section 7). ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Hamiltonian Dynamics ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The inspiration of using Hamiltonian dynamics in machine learning comes from considering a data point $\\bar{x}\\in\\mathbb R^{d}$ as coordinates of an object in $\\mathbb{R}^{d}$ . Such an object also has a velocity $v\\in\\mathbb{R}^{d}$ . Let $\\pi$ be a probability distribution with density function $\\pi:\\mathbb{R}^{d}\\to\\mathbb{R}_{\\geq0}$ . Assuming unit mass, the energy of such an object is given by its Hamiltonian [33, 4], defined by: ", "page_idx": 1}, {"type": "equation", "text": "$$\nH(x,v)=U(x)+\\frac{1}{2}\\|v\\|^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $U:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is a potential function. The key idea behind using Hamiltonian dynamics in the context of probabilistic modeling and sampling is to set the potential function as negative log-likelihood, i.e., $U(x)=-\\log\\pi(x)$ . One defines the Boltzmann-Gibbs distribution $\\pi_{B G}$ then as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi_{B G}=\\pi\\otimes\\mathcal{N}(0,\\mathbf{I}_{d}),\\quad\\pi_{B G}(x,v)=\\exp(-H(x,v))/Z=\\pi(x)\\mathcal{N}(v;0,\\mathbf{I}_{d}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "i.e. the product distribution of the data distribution and normal distribution. In particular, one can easily draw a sample $z$ from $z\\sim\\pi_{B G}$ by sampling $x\\sim\\pi,v\\sim\\mathcal{N}(0,\\mathbf{I}_{d})$ and setting ${z}=(x,v)$ . ", "page_idx": 1}, {"type": "text", "text": "Hamiltonian dynamics describe how an object described by ${z}=(x_{0},v_{0})$ evolves over time. It is defined by the ODE [4] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\displaystyle\\frac{d}{d t}x(t),\\displaystyle\\frac{d}{d t}v(t))=\\left(v(t),-\\nabla U(x(t))\\right)=(v(t),\\nabla\\log\\pi(x(t))),}\\\\ &{\\qquad\\left(x(0),v(0)\\right)=z}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "i.e. the change of location is the velocity and the change of velocity is the force (here, equals acceleration as we assume unit mass). Let $\\dot{\\varphi}:\\mathbb{R}^{2d}\\times\\mathbb{R}\\rightarrow\\bar{\\mathbb{R}}^{2d},(z,t)\\mapsto\\varphi_{t}(z)$ be the corresponding flow, i.e. the function $t\\mapsto\\varphi_{t}(z)$ is a solution to the above ODE with starting point $z$ . ", "page_idx": 1}, {"type": "text", "text": "As one would expect from physics, Hamiltonian dynamics $\\varphi_{t}$ preserve the energy of a system, i.e. $H(\\varphi_{t}(x,v))={\\dot{H}}(x,v)$ for all $t,x,v$ (see proof in Appendix B.1). This physical intuition translates into the fact that Hamiltonian dynamics preserve the Boltzmann-Gibbs distribution, i.e. ", "page_idx": 1}, {"type": "equation", "text": "$$\nZ\\sim\\pi_{B G}\\Rightarrow\\varphi_{t}(Z)\\sim\\pi_{B G}\\;\\mathrm{for}\\;\\mathrm{all}\\;t\\geq0\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We include a derivation of this well-known statement in Appendix C.1 as it is so central to this work. ", "page_idx": 1}, {"type": "text", "text": "2.2 Score Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of score matching is to learn the score function $\\nabla\\log\\pi$ from data samples $x_{1},\\ldots,x_{n}\\sim\\pi$ As the score function naturally appears in the Hamiltonian ODE (see Equation (2)), we interpret it as a force function and denote a parameterized score model by $F_{\\theta}:\\mathbb{R}^{d}\\stackrel{=}{\\rightarrow}\\mathbb{R}^{d}$ . A natural approach to fitting $F_{\\theta}$ is to minimize the mean squared error between $F_{\\theta}$ and the true score weighted by their likelihood under $\\pi$ . This leads to the explicit score matching loss [25] given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\mathrm{esm}}(\\theta;\\pi)=\\mathbb{E}_{x\\sim\\pi}\\left[{\\frac{1}{2}}\\|\\nabla\\log\\pi(x)-F_{\\theta}(x)\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This loss cannot be minimized directly as one does not have access to $\\nabla\\log\\pi$ and various scorematching methods differ in how they circumvent not having access to $\\nabla\\log\\pi$ (see Appendix A for a detailed overview). ", "page_idx": 2}, {"type": "text", "text": "A different approach to score matching is to slightly modify the objective by adding Gaussian noise to the data distribution $\\pi$ to get the noisy distribution $\\begin{array}{r}{\\pi_{\\sigma}(x)^{'}\\!=\\int\\!\\tilde{\\mathcal{N}(x;x_{0};\\sigma^{2}\\mathbf{I}_{d})}\\pi(\\breve{x_{0}})d x_{0}}\\end{array}$ [48]. The objective can then be expressed as denoising score matching: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\mathrm{dsm}}(\\theta;\\pi_{\\sigma})=\\mathbb{E}_{x\\sim\\pi,\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}\\left[\\lVert F_{\\theta}(x+\\sigma\\epsilon)+\\frac{\\epsilon}{\\sigma}\\rVert^{2}\\right]=L_{\\mathrm{esm}}(\\theta;\\pi_{\\sigma})+C_{\\sigma}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a constant $C_{\\sigma}$ . Noised data distributions naturally appear in diffusion models, and the denoising score-matching objective, therefore, became the state-of-the-art method to train diffusion models. However, denoising score matching suffers from high variance leading to long training times as well as computationally expensive sampling [44, 52]. In addition, it would be an unreasonable choice for an application where it is important to learn the original data distribution. ", "page_idx": 2}, {"type": "text", "text": "3 Hamiltonian Velocity Predictors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The Hamiltonian ODE has been widely used in Bayesian statistics in the form Hamiltonian Monte Carlo [14, 36]. However, in such settings, it is assumed that one has access to the score $\\nabla\\log\\pi$ (=force), and the goal is to sample from $\\pi$ (by sampling from $\\pi_{B G}$ ). In machine learning tasks, the inverse is true. For such tasks, one has access to data samples $x_{1},\\ldots,x_{n}\\sim\\pi$ and the goal is to (1) learn $\\nabla\\log\\pi$ (score matching) or (2) create new samples $x\\sim\\pi$ (generative modeling) - or both. ", "page_idx": 2}, {"type": "text", "text": "Parameterized Hamiltonian ODEs (PH-ODEs). This inspires the definition of a parameterized Hamiltonian ODE (PH-ODE). A PH-ODEs consists of two components: ", "page_idx": 2}, {"type": "text", "text": "Initial distribution \u03a0: The starting condition $z=(x,v)\\sim\\Pi$ is distributed according to a joint location-velocity distribution $\\Pi$ such that its marginal over $x$ is $\\pi$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\int\\Pi(x,v)d v=\\pi(x)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the default cause, $\\Pi$ equals the Boltzmann-Gibbs distribution $\\pi_{B G}$ , i.e. $\\Pi=\\pi\\otimes\\mathcal{N}(0,\\mathbf{I}_{d})$ . ", "page_idx": 2}, {"type": "text", "text": "2. Force field: The evolution is governed by a parameterized force field $F_{\\theta}:\\mathbb{R}^{d}\\times\\mathbb{R}\\rightarrow\\mathbb{R}^{d}$ via: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{(\\displaystyle\\frac{d}{d t}x(t),\\displaystyle\\frac{d}{d t}v(t))=(v(t),F_{\\theta}(x(t),t))}\\\\ {(x(0),v(0))=(x_{0},v_{0})=z}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As we consider $\\Pi$ as part of the definition of a PH-ODE, we write with a slight abuse of notation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varphi_{t}^{\\theta}(z)=(x_{t}^{\\theta}(z),v_{t}^{\\theta}(z))=(x_{t}^{\\theta},v_{t}^{\\theta})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for the solution of the ODE assuming $z=(x_{0},v_{0})\\sim\\Pi$ . We note that while $F_{\\theta}$ might refer to a $\\theta$ -parameterized neural network, we can also set it to a fixed known vector field. Both cases will be important. ", "page_idx": 2}, {"type": "text", "text": "Velocity prediction is all you need. The crucial idea of this work is that one can use PH-ODEs for both score matching and generative modeling by predicting velocities. For this, we use an auxiliary family of functions $V_{\\phi}:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}^{d}$ , here usually a neural network. Let us consider the following velocity prediction loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\mathrm{V}}(\\phi|\\theta,t)=\\!\\mathbb{E}_{(x,v)\\sim\\Pi}[||V_{\\phi}(x_{t}^{\\theta},t)-v_{t}^{\\theta}||^{2}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By minimizing the above loss over $\\phi$ , we train $V_{\\phi}$ to predict the velocity given the location after running the Hamiltonian ODE with starting conditions defined by $\\Pi$ . For a sufficiently rich class of functions $V_{\\phi}$ , the minimizer $V_{\\phi^{*}}$ of the above loss is the expected velocity conditioned on the location (see Appendix D): ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{\\phi^{*}}(x,t)=\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}=x]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As we will see, this quantity is \"all you need\" for generative modeling and score matching. We call $V_{\\phi^{*}}$ the Hamiltonian velocity predictor (HVP). We note that many of flow-based works also implicitly learn to predict velocities [31], although they do not consider them as separate states. ", "page_idx": 3}, {"type": "image", "img_path": "JJGfCvjpTV/tmp/c94a5cb0011545a4aa61be0284f9cb1b492e8c7d34019251bc8230dad45a2517.jpg", "img_caption": ["Figure 1: Results of training HSM on a Gaussian mixture. The score vector field faithfully recovers gradients of the density. The optimal velocity predictor is zero everywhere. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Hamiltonian Score Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide a novel score-matching method using PH-ODEs and velocity predictors. We will first connect the score function to a preservation property of Hamiltonian systems (Section 4.1), and then introduce a new score-matching objective derived from this property (Sections 4.2 and Section 4.3). ", "page_idx": 3}, {"type": "text", "text": "4.1 Characterizing Scores with Hamiltonian Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The conservation of the Boltzmann-Gibbs distribution $\\pi_{B G}$ (see Equation (4)) is the crucial property that enables Hamiltonian Monte Carlo as it allows for proposals of distant states enabling fast mixing of a Markov chain. The inspiration of this work was to take the inverse perspective: rather than considering the preservation of $\\pi_{B G}$ a useful property of the Hamiltonian ODE, we ask whether it is the defining property of the score function. In other words, is any vector field that preserves the Boltzmann-Gibbs distribution under a PH-ODE automatically the score? And if yes, could we train for this property to learn a score? As we will show, we can characterize the score with an even easier-to-train preservation property solely depending on the velocity predictor. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $T>0$ and $F_{\\theta}(x)$ a force field. Let $\\Pi=\\pi_{B G}\\,=\\,\\pi\\otimes\\mathcal{N}(0,{\\bf I}_{d})$ . The following statements are equivalent: ", "page_idx": 3}, {"type": "text", "text": "1. Score vector field: The force field $F_{\\theta}$ equals the score, i.e. $F_{\\theta}(x)\\,=\\,\\nabla_{x}\\log\\pi(x)$ for $\\pi$ -almost every $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ .   \n2. Preservation of Boltzmann-Gibbs: The PH-ODE with $F_{\\theta}$ preserves the Boltzmann-Gibbs distribution $\\pi_{B G}$ .   \n3. Conditional velocity is zero: The velocity given the location after running the PH-ODE with $F_{\\theta}$ is zero if starting conditions $z=\\bar{(x_{0},v_{0})}$ are sampled from $\\pi_{B G}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nz\\sim\\pi_{B G}\\quad\\Rightarrow\\quad\\mathbb{E}[v_{t}^{\\theta}(z)|x_{t}^{\\theta}(z)]=0\\quad f o r\\,a l l\\,0\\leq t<T\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A proof can be found in Appendix C and is based on the fact that test functions linear in $v$ have vanishing expectation if Equation (13) holds. We note that the equivalence of conditions (1) and (2) is well-known in statistical physics. ", "page_idx": 4}, {"type": "text", "text": "4.2 Hamiltonian Score Discrepancy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Condition (3) in Theorem 1 naturally motivates a new way of training $F_{\\theta}$ to approximate a score. Specifically, our goal is train the force field $F_{\\theta}$ such that its optimal velocity predictor is zero. By Theorem 1, it necessarily holds $F_{\\theta}=\\nabla\\log\\pi$ in this case. Unfortunately, such a bilevel optimization is not tractable with stochastic gradient descent in general, as it contains two different objectives. ", "page_idx": 4}, {"type": "text", "text": "However, a simple trick allows us to convert the above into a single objective. For this, we define the Hamiltonian Score Matching loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{hsm}}(\\phi|\\theta,t)=\\!\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[\\lVert V_{\\phi}(x_{t}^{\\theta},t)\\rVert^{2}-2V_{\\phi}(x_{t}^{\\theta},t)^{T}v_{t}^{\\theta}\\right]=L_{\\mathrm{V}}(\\phi|\\theta,t)-C(\\theta,t)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C(\\theta,t)=\\mathbb{E}[\\|v_{t}^{\\theta}\\|^{2}]$ . As the value of $C(\\theta,t)$ is a constant in $\\phi$ , it holds that the optimal velocity predictor is also the unique minimizer of the Hamiltonian Score Matching loss $L_{\\mathrm{hsm}}(\\phi|\\theta,t)$ . However, while the argmin is the same, the actual obtained minimum value is drastically different as the next proposition shows. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. For a sufficiently rich class of functions $\\displaystyle(V_{\\phi})_{\\phi\\in I}$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{D}_{h s m}(\\theta|t,\\pi):=-\\operatorname*{min}_{\\phi\\in I}L_{h s m}(\\phi|\\theta,t)=\\mathbb{E}_{z\\sim\\pi_{B G}}[\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof relies on plugging the identity of $V_{\\phi^{*}}$ (see Equation (12)) into Equation (14)) and can be found in Appendix E. By condition (3) in Theorem 1 (see Equation (13)), we want to minimize $D_{\\mathrm{hsm}}(\\theta|t,\\pi)$ in order to learn scores. For this, let\u2019s define a distribution $\\lambda$ with full support over $[0,T)$ for $T\\in\\mathbb{R}_{>0}\\cup\\{\\infty\\})$ ). With this, we define the Hamiltonian score discrepancy (HSD) as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\mathrm{hsm}}(\\theta|\\pi)=\\mathbb{E}_{t\\sim\\lambda}\\left[\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the discrepancy is defined for an arbitrary (regular) vector field $F_{\\theta}$ - not restricted to scores of probability distributions. By Theorem 1, the discrepancy fulfills all properties that we would expect from a discrepancy to hold: $D(\\theta|\\pi)\\geq0$ for all $\\theta$ and $\\dot{D}(\\theta|\\pi)=0$ if and only if $F_{\\theta}=\\nabla\\log\\pi$ We summarize the findings in the below theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Minimization of the Hamiltonian score discrepancy results in learning the score $\\nabla\\log\\pi$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta^{*}=\\operatorname*{arg\\,min}_{\\theta}\\mathbb{D}_{h s m}(\\theta|\\pi)\\Rightarrow s_{\\theta^{*}}=\\nabla\\log\\pi\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The full proof is stated in Appendix F. The Hamiltonian score discrepancy gives a natural measure of how far a vector field is from the desired score vector field. However, at first, it seems rather abstract. The following proposition shows that minimizing this measure has a very intuitive interpretation. In fact, it is closely connected to the explicit score matching loss $L_{\\mathrm{esm}}$ (see Equation (5)). ", "page_idx": 4}, {"type": "text", "text": "Proposition 2 (Taylor approximation of HSM loss). There exists an error term $\\epsilon(t)$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{D}_{h s m}(\\theta|t,\\pi)=2t^{2}L_{e s m}(\\theta;\\pi)+\\epsilon(t)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to0}{\\frac{1}{t^{2}}}\\|\\epsilon(t)\\|\\to0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A proof can be found in Appendix G. Intuitively, minimizing the Hamiltonian score discrepancy, therefore, consists of pushing the parabola in Equation (18) down onto the $\\mathbf{X}$ -axis. The above theorem also indicates the optimal choice of $T$ : one should choose $T$ high enough to have a loss value high enough to give signal but low enough to avoid errors due to numerical integration of the ODE. ", "page_idx": 4}, {"type": "text", "text": "4.3 Hamiltonian Score Matching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Beyond its theoretical value, we can explicitly minimize the HSD, a method we coin Hamiltonian Score Matching (HSM). To minimize the HSD, two networks $V_{\\phi}$ and $F_{\\theta}$ can jointly optimize Equation (14). There are two difficulties coming along with this: (1) One has to simulate trajectories. This can be done via Neural ODEs [7] with constant memory. (2) One has to run a min-max optimization. Here, a big toolbox developed for GANs for training stabilization can be used [35, 19]. On the other hand, we hypothesize that HSM has two advantages: (1) every trajectory of HSM gives several points of supervision effectively augmenting our data and (2) we can learn the original (\"unnoised\") data distribution $\\pi$ . However, please note that we do not propose Hamiltonian Score Matching as a replacement for denoising score matching in diffusion models. Rather, it is a scalable alternative to score matching methods that learn the original (\"unnoised\") data distribution $\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "5 Hamiltonian Generative Flows ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we show that training a general velocity predictor of a Hamiltonian ODE is useful even if $F_{\\theta}\\,\\ne\\,\\nabla\\log\\pi$ . This leads to a generative model that we coin Hamiltonian Generative Flows (HGFs). As $F_{\\theta}=F$ is fixed and not trained here, we write $(\\boldsymbol{x}_{t},\\boldsymbol{v}_{t})=(\\boldsymbol{x}_{t}^{\\theta},\\boldsymbol{v}_{t}^{\\theta})$ for the solution of the PH-ODE. Let us denote $\\Pi(x,v,t)$ as the distribution of $\\left({{x}_{t}},{{v}_{t}}\\right)$ at time $t$ and the location marginal ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int\\Pi(x,v,t)d v=\\pi(x,t)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The location marginal describes a probability flow starting from our data distribution $\\pi=\\pi(\\cdot,0)$ . It turns out that the optimal velocity predictor is exactly the vector field that generates $\\pi(\\boldsymbol{x},t)$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. Let $\\pi_{T}$ be the distribution such that $x_{T}^{\\theta}\\sim\\pi_{T}$ . Let $V_{\\phi}^{*}$ be the Hamiltonian Velocity Predictor (see Equation (12)). Then by sampling $x_{T}\\sim\\pi_{T}$ and running the velocity predictor ODE ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x(t)=V_{\\phi^{*}}(x,t)\\Rightarrow x(0)\\sim\\pi\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "backwards in time, we will have $x(0)\\sim\\pi$ , i.e. we can sample from the data distribution $\\pi$ . More specifically, the optimal velocity predictor $V_{\\phi^{*}}$ generates the probability path $\\pi(\\cdot,t)$ . ", "page_idx": 5}, {"type": "text", "text": "The proof uses the fact that the vector field $G(\\boldsymbol{x},\\boldsymbol{v})=(v,F_{\\theta}(\\boldsymbol{x},t))$ is divergence-free to show that $V_{\\phi^{*}}$ fulflis the deterministic Fokker-Planck equation (see Appendix H). The above proposition allows us to build a generative model by training an HVP. We coin this model Hamiltonian Generative Flows $(H G F s)$ . To make this framework tractable, we need two criteria to be fulfilled: ", "page_idx": 5}, {"type": "text", "text": "1. (C1) Forward ODE efficiently computed: For efficient training, we need to be able to compute $x_{t}^{\\theta},v_{t}^{\\theta}$ efficiently - either via an analytical expression or ODE solvers. 2. (C2) Initial distribution should be approximately known: In order to be able to sample the initial point of the ODE faithfully, we need to (approximately) know $\\pi(x,T)$ . ", "page_idx": 5}, {"type": "image", "img_path": "JJGfCvjpTV/tmp/c912262d393ccaa9921c0573ced1a1a002b56d8b623f9c0ba5b9a616f0400cd4.jpg", "img_caption": ["Figure 2: Evolution of various HGFs in joint coordinate-velocity space from $t=0$ (blue) to $t=T$ (red) with trajectories (black). Data distribution $\\pi(x)=0.4*\\bar{\\mathcal{N}}(-\\bar{2},1)+0.6*\\mathcal{N}(2,1)$ . Diffusion models and flow matching have zero force fields, i.e. the velocity does not change. Diffusion models do not converge in finite time (here, $T=3$ ). The coupled distribution in FM allow for a convergence for $T=1$ . Both distort the joint distribution. Oscillation HGFs only rotate the distribution. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "6 Diffusion Models and Flow Matching as HGFs with zero force field ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "6.1 Diffusion Models as HGFs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We can recover diffusion models with a variance-preserving (VP-SDE) noising process [45] as a special case of HGFs. If we simply set $\\Pi=\\pi_{B G}$ and $F_{\\theta}(x)\\,{\\bar{=}}\\,0$ - no force applied. In this case, we get that $x_{t}=x+t v$ and $v_{t}=v$ leading to the training objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\pi,v\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}[\\|V_{\\phi}(x_{t},t)-v_{t}\\|^{2}]=\\!\\!\\mathbb{E}_{x\\sim\\pi,\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}[\\|V_{\\phi}(x+t\\epsilon,t)-\\epsilon\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which equals denoising score matching (Equation (6)) with score model $-t V_{\\phi}(x,t)=\\nabla\\log\\pi_{\\sigma(t)}$ with $\\sigma(t)=t$ . In this case, Hamiltonian Generative Flows correspond to training a diffusion model ", "page_idx": 5}, {"type": "text", "text": "and the velocity predictor corresponds to a denoising network (often denoted as $\\epsilon_{\\theta}$ in DDPMs [21]). It then holds $x_{T}\\stackrel{<}{\\sim}\\pi_{T}\\approx\\mathcal{N}(0,\\sigma^{2}(t)\\mathbf{I}_{d})$ and the velocity predictor ODE then reduces to the well-known probability flow ODE formulation of diffusion models with noise schedule $\\sigma(t)=t$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{T}\\sim\\pi_{T}\\approx{\\cal N}(0,\\sigma^{2}(t){\\bf I}_{d}),\\quad\\frac{d}{d t}x(t)=-\\,\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x}\\log\\pi_{\\sigma(t)}(x)={\\cal V}(x(t),t)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In fact, the above is a universal way of modeling diffusion models [26]. In other words, diffusion models are a special case of HGFs for the zero-force field. In this perspective, different diffusion models correspond to different time rescaling and preconditioning of the network. The location marginals $\\pi(x,t)$ only fully converge to a Gaussian in the limit of $t\\to\\infty$ (see Figure 2). ", "page_idx": 6}, {"type": "text", "text": "6.2 Flow Matching as HGFs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Flow matching with the CondOT probability path [31], a current state-of-the-art generative model, can be easily considered an HGF model. As in diffusion, let us consider the zero force field $F_{\\theta}$ and let\u2019s consider a coupled initial distribution $\\Pi$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nx\\sim\\pi,\\quad v=\\epsilon-x,\\quad\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{d})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similarly, the velocity prediction loss corresponds to the OT-flow matching loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\pi(x),\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}[\\|V_{\\phi}((1-t)x+t\\epsilon,t)-(\\epsilon-x)\\|^{2}]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and flow model corresponds to the velocity predictor ODE. Therefore, diffusion models and OT-flow matching are both HGFs with the zero force field - the difference lies in a coupled construction of the initial distribution (see Figure 2). The coupled construction allows OT-flow matching to have straighter probability paths, leading to improved generation quality for the same number of steps [31]. ", "page_idx": 6}, {"type": "text", "text": "7 Oscillation HGFs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "So far, we studied optimal velocity predictors $V_{\\phi}$ for two extreme cases: either $F_{\\theta}\\,=\\,0$ or $F_{\\theta}=$ $\\nabla\\log\\pi$ . Finally, we want to investigate a different choice of $F_{\\theta}$ to construct HGFs. Here, we study Oscillation $H G F s$ that correspond to a natural extension. In Appendix I, we give another example that we coin $R$ eflection HGFs. ", "page_idx": 6}, {"type": "text", "text": "A simple design of a force field is to use the physical model of a harmonic oscillator, i.e. to set $F_{\\theta}(x)^{'}{=}-\\alpha^{2}{\\overset{\\underset{\\mathbf{0}}{}}{x}}$ with $\\Pi=\\pi_{B G}$ and $\\alpha>0$ . The flow of the ODE then becomes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(x_{t},v_{t})=\\left(\\cos(\\alpha t)x+\\frac{1}{\\alpha}\\sin(\\alpha t)v,-\\alpha\\sin(\\alpha t)x+\\cos(\\alpha t)v\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "I.e. condition (C1) is fulfliled as we can simply compute the ODE analytically. Setting $T=\\pi/(2\\alpha)$ , it holds that $(x_{t},v_{t})=(v,-\\alpha x)$ . In particular, $\\pi_{T}=\\mathcal{N}(0,\\mathbf{I}_{d}/\\alpha^{2})$ - condition (C1) is easily fulfliled. Therefore, the above choice gives us a natural generative model based on harmonic oscillators that we coin Oscillation $H G F s$ . To summarize, they have the following simple training objective: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\pi,v\\sim_{N}(0,\\mathbf{I}_{d})}[\\|V_{\\phi}(\\cos(\\alpha t)x+\\frac{\\sin(\\alpha t)}{\\alpha}v,t)-[-\\alpha\\sin(\\alpha t)x+\\cos(\\alpha t)v]\\|^{2}]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A natural choice for $\\alpha$ is to set $\\alpha=\\sqrt{d/\\mathbb{E}_{x\\sim\\pi}[\\|x\\|^{2}]}$ . With this, the scale of the $n$ -th derivative (including $n=0$ ) of the inputs and outputs in the training objective is constant in time (see Figure 2), i.e. for all $t=0$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\pi,v\\sim{N(0,\\mathbf{I}_{d})}}\\left[\\|\\frac{d^{n}}{d^{n}t}x_{t}\\|^{2}\\right]=\\alpha^{n-2}d,\\quad\\mathbb{E}_{x\\sim\\pi,v\\sim{N(0,\\mathbf{I}_{d})}}\\left[\\|\\frac{d^{n}}{d^{n}t}v_{t}\\|^{2}\\right]=\\alpha^{n}d\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the context of critically-damped Langevin diffusion [13], it was already observed that a constant scale in velocity space leads to improved training and better performance. Here, we extend this idea of a constant scale from the velocity to the $n$ -th derivative. ", "page_idx": 6}, {"type": "text", "text": "8 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Assessing and training energy-based models. Stein\u2019s discrepancy [16] is a well-known measure to assess the quality of energy-based models based on Stein\u2019s identity [46]. Based on this metric, [17] developed a method that is similar to ours where a critic is optimized to assess the quality of an energy-based model via Stein\u2019s discrepancy and jointly trained with the energy model via min-max optimization. [25] introduced score matching as a method by showing that the explicit score matching loss (see Equation (5)) can be implicitly trained if one computes the trace of Hessian of the energy function - an expensive step. To expedite this, [34] introduced curvature propagation for an unbiased Hessian estimate, while [43] used Hutchinson\u2019s Trick to estimate the trace. In practice, both methods suffer from high variance due to their underlying Monte Carlo estimators. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Flow matching and Stochastic interpolants. As already seen for a special case in Section 6, HGFs are strongly connected to Flow Matching [31] and Stochastic Interpolants [3]. They construct probability paths that fulflil the continuity equation by predicting derivatives of flows (i.e. velocities) in the same way how in this work, we predict velocities as marginals of an extended state space. The differences of these 3 works lie in the design perspective: HGFs consider 2nd-order ODEs in an extended state space $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ with a simple initial velocity distribution (here, $\\mathcal{N}(0,\\mathbf{I}_{d}))$ , while FM considers 1st-order ODE paths in $\\mathbb{R}^{d}$ converging to a simple final location distribution. Flow matching conditions on final states (usually at $t=\\bar{1}$ ), while our framework conditions on the velocity of the current state (arbitrary $t$ ) and is centered around forces. This work arrives at the ideas of conditional velocity predictors via the search of properties that are conserved under Hamiltonian dynamics (see Theorem 1). We elucidate the mathematical connection in more detail in Appendix J. ", "page_idx": 7}, {"type": "text", "text": "Generative models and Hamiltonian physics. V-Flows also consider augmenting the state space with velocities deriving an ELBO objective for CNFs [6]. [13] extend diffusion models to joint state-velocity samples that converge to a joint normal distribution. One difference is that we only need to run the backward equation in state space $\\mathbb{R}^{d}$ as opposed to extended state-velocity space $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ . Though rather unmotivated, Oscillation HGFs could, in principle, also be derived as an EDM model with preconditioning [26]. Finally, several works have, like this, explored generative models based on specific physical processes, e.g. Poisson flow generative models [50, 51]. A few works also combined Hamiltonian physics with deep learning. For example, [18] use conservation of energy as an implicit bias to learn networks for physical data. Conversely, deep learning was also used to accelerate HMC sampling [15], e.g. by training custom MCMC kernels [30] or correct for complex geometries via flows [22]. Very recently, score matching approaches were also designed to leverage existing force fields as part of a diffusion model that samples from an energy landscape [2, 12]. ", "page_idx": 7}, {"type": "text", "text": "Acceleration Generative Model (AGM). The AGM model [9] also uses constructions in phase space (joint position and velocity space) and 2nd order ODEs. While AGM focuses on learning the force field, our approach primarily focuses on learning the optimal velocity predictor. While we also consider optimizing the force field by minimizing the norm of the optimal velocity predictor, this happens in the \u201couter loop\u201d of the maximization - the inner loop optimizes the optimal velocity predictor. Further, ATM focuses on bridging two desired distributions by posing a stochastic bridge problem in phase space. We do not consider the problem of bridging distributions. In contrast, our framework centers around energy preservation and divergence from that preservation (for optimal velocity predictors that are not zero). Specifically, we establish a connection to Hamiltonian physics and a property of the preservation of energy. This allows us to introduce a further bi-level optimization and the possibility of joint training for score matching. ", "page_idx": 7}, {"type": "text", "text": "9 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "9.1 Hamiltonian Score Discrepancy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As we introduced Hamiltonian score discrepancy as a novel score-matching metric, we first empirically investigate our theoretical insights on Gaussian mixtures (see Figure 1). As one can see in Figure 3a, the Hamiltonian score discrepancy is highly correlated with the explicit score matching loss. Further, we can validate empirically that the Taylor approximation derived in Proposition 2 is pretty accurate for large $t$ (see Appendix I). Overall, these results indicate that the Hamiltonian score discrepancy is a natural metric to assess score approximations. ", "page_idx": 7}, {"type": "text", "text": "Further, we investigate whether explicitly minimizing the Hamiltonian score discrepancy leads to accurate score approximations. We jointly train velocity predictors and score networks as described in Section 4. As one can see visually in Figure 1, this approach can faithfully learn score vector fields. In addition, we investigate the signal-to-noise ratio for gradient estimation. As shown in Figure 3c, the gradient estimates of HSM have significantly lower variance compared to denoising score matching at lower noise levels $\\sigma$ . The reason for that is that we allow for supervision across a full trajectory at locations for the same data points - effectively acting as data augmentation. Of course, this comes at the expense of simulating the Hamiltonian trajectories for $\\sim\\,5$ steps. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "JJGfCvjpTV/tmp/90ed92ae2a4bac459c6a8ba4d7fe58d2544ea28bb4ef29480bfd9f8ba4683069.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Empirical investigation of Hamiltonian score discrepancy (HSD). (a) The Taylor approximation is a good approximation. (b) Hamiltonian score discrepancy is strongly correlated with explicit score matching loss. (c) Signal-to-noise ratio is significantly better for HSM vs DSM for low $\\sigma$ . ", "page_idx": 8}, {"type": "image", "img_path": "JJGfCvjpTV/tmp/cbbb5534348aa2c43011479ef59ce185e234c128e85b7b83e3b5b914c51f54bb.jpg", "img_caption": ["Figure 4: Image generation examples based on Oscillation HGFs for FFHQ. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "9.2 HGF experiments $^{\\ast}$ Image Generation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the form of diffusion models and flow matching, HGFs have already been extensively optimized and achieved state-of-the-art results. Instead, we investigate whether also other non-zero force fields, specifically Oscillation HGFs, can lead to generative models of high quality. We focus on image generation benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Specifically, we train a Oscillation HGF on CIFAR-10 unconditional and conditional. As two central benchmarks, we use the original SDE formulation of diffusion models [45] as well as the EDM framework [26], a highly-tuned optimization of diffusion models. Our hypothesis is that Oscillation HGFs should work well out-of-the-box, as the scale of their inputs and outputs stay around constant ( $\\mathit{\\check{c}}.\\mathit{f}.$ Eq. 25). Therefore, we remove any preconditioning optimized for diffusion models (scaling of inputs and outputs and skip connections) [26] and train on the default DDPM architecture [21] (see details in Appendix L). Our results are encouraging: without hyperparameter tuning, Oscillations HGFs can sample high-quality images and surpass most previous methods (see Table 1) measured by Frechet Inception Distance (FID) [20]. While they still lack behind the EDM model, this difference might well be explained by the fact that ", "page_idx": 8}, {"type": "table", "img_path": "JJGfCvjpTV/tmp/0606e5f1e72c3feff244ef805f57c4769f2260884f8151128a8d47c9c2d47341.jpg", "table_caption": ["Table 1: Sample quality (FID) and number of function evaluation (NFE). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "architectures and hyperparameters have been optimized for diffusion models over several works that are hard to replicate. ", "page_idx": 8}, {"type": "text", "text": "To investigate whether similar results can be achieved similar performance at higher resolutions, we perform another benchmark on the FFHQ dataset at $64\\mathrm{x}64$ resolution. Here, our results are similar: Oscillation HGFs improve upon original diffusion models with a small performance margin to the EDM model. They can generate high-quality faces that appear realistic (see Figure 4). ", "page_idx": 9}, {"type": "text", "text": "10 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work systematically elucidates the synergy between Hamiltonian dynamics, force fields, and generative models - extending and giving a new perspective on many known generative models. We believe that this opens up new avenues for applications of machine learning in physical sciences and dynamical systems. However, several limitations remain. Minimizing the Hamiltonian Score Discrepancy (HSD) via a default min-max algorithm is scalable but requires adversarial optimization. Future work can focus on adapting the HSD framework, e.g. to develop denoising Hamiltonian score matching that could allow for guaranteed convergence. For HGFs, the extended design space presented has only been explored for data without known force fields (image generation, here). Future work can focus on specific applications that require domain-specific force fields, e.g. for molecular data. Further adaptions might be required in such settings as such data often lie on manifolds. A further challenge is that HGFs not necessarily converge to a known distribution for more complex force fields. Therefore, we anticipate that future work will focus on adapting HGFs and related models to this challenge to design domain-specific models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was funded in part by GIST-MIT Research Collaboration grant (funded by GIST), the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program, and the NSF Expeditions grant (award 1918839) Understanding the World Through Code. ", "page_idx": 9}, {"type": "text", "text": "P.H. would like to thank Gabriele Corso, Ziming Liu, and Timur Garipov for helpful discussions during early stages of the work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1\u20133, 2024.   \n[2] T. Akhound-Sadegh, J. Rector-Brooks, A. J. Bose, S. Mittal, P. Lemos, C.-H. Liu, M. Sendera, S. Ravanbakhsh, G. Gidel, Y. Bengio, et al. Iterated denoising energy matching for sampling from boltzmann densities. arXiv preprint arXiv:2402.06121, 2024.   \n[3] M. S. Albergo, N. M. Boff,i and E. Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   \n[4] V. I. Arnol\u2019d. Mathematical methods of classical mechanics, volume 60. Springer Science & Business Media, 2013.   \n[5] N. Bou-Rabee and J. M. Sanz-Serna. Geometric integrators and the hamiltonian monte carlo method. Acta Numerica, 27:113\u2013206, 2018.   \n[6] J. Chen, C. Lu, B. Chenli, J. Zhu, and T. Tian. Vflow: More expressive generative flows with variational data augmentation. In International Conference on Machine Learning, pages 1660\u20131669. PMLR, 2020.   \n[7] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[8] T. Chen, E. Fox, and C. Guestrin. Stochastic gradient hamiltonian monte carlo. In International conference on machine learning, pages 1683\u20131691. PMLR, 2014.   \n[9] T. Chen, J. Gu, L. Dinh, E. A. Theodorou, J. Susskind, and S. Zhai. Generative modeling with phase stochastic bridges. arXiv preprint arXiv:2310.07805, 2023.   \n[10] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.   \n[11] G. Corso, H. St\u00e4rk, B. Jing, R. Barzilay, and T. Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776, 2022.   \n[12] V. De Bortoli, M. Hutchinson, P. Wirnsberger, and A. Doucet. Target score matching. arXiv preprint arXiv:2402.08667, 2024.   \n[13] T. Dockhorn, A. Vahdat, and K. Kreis. Score-based generative modeling with critically-damped langevin diffusion. arXiv preprint arXiv:2112.07068, 2021.   \n[14] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid monte carlo. Physics letters B, 195(2):216\u2013222, 1987.   \n[15] S. Foreman, X.-Y. Jin, and J. C. Osborn. Deep learning hamiltonian monte carlo. arXiv preprint arXiv:2105.03418, 2021.   \n[16] J. Gorham and L. Mackey. Measuring sample quality with kernels. In International Conference on Machine Learning, pages 1292\u20131301. PMLR, 2017.   \n[17] W. Grathwohl, K.-C. Wang, J.-H. Jacobsen, D. Duvenaud, and R. Zemel. Learning the stein discrepancy for training and evaluating energy-based models without sampling. In International Conference on Machine Learning, pages 3732\u20133747. PMLR, 2020.   \n[18] S. Greydanus, M. Dzamba, and J. Yosinski. Hamiltonian neural networks. Advances in neural information processing systems, 32, 2019.   \n[19] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017.   \n[20] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[21] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[22] M. Hoffman, P. Sountsov, J. V. Dillon, I. Langmore, D. Tran, and S. Vasudevan. Neutralizing bad geometry in hamiltonian monte carlo using neural transport. arXiv preprint arXiv:1903.03704, 2019.   \n[23] M. D. Hoffman, A. Gelman, et al. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593\u20131623, 2014.   \n[24] M. F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989.   \n[25] A. Hyv\u00e4rinen and P. Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[26] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[27] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila. Training generative adversarial networks with limited data. ArXiv, abs/2006.06676, 2020.   \n[28] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.   \n[29] U. K\u00f6ster, J. T. Lindgren, and A. Hyv\u00e4rinen. Estimating markov random field potentials for natural images. In International Conference on Independent Component Analysis and Signal Separation, pages 515\u2013522. Springer, 2009.   \n[30] D. Levy, M. D. Hoffman, and J. Sohl-Dickstein. Generalizing hamiltonian monte carlo with neural networks. arXiv preprint arXiv:1711.09268, 2017.   \n[31] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n[32] X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \n[33] J. E. Marsden and T. S. Ratiu. Introduction to mechanics and symmetry: a basic exposition of classical mechanical systems, volume 17. Springer Science & Business Media, 2013.   \n[34] J. Martens, I. Sutskever, and K. Swersky. Estimating the hessian by back-propagating curvature. In International Conference on Machine Learning, 2012.   \n[35] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.   \n[36] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2, 2011.   \n[37] B. \u00d8ksendal and B. \u00d8ksendal. Stochastic differential equations. Springer, 2003.   \n[38] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[39] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[40] N. Shaul, J. Perez, R. T. Chen, A. Thabet, A. Pumarola, and Y. Lipman. Bespoke solvers for generative flow models. arXiv preprint arXiv:2310.19075, 2023.   \n[41] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[42] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[43] Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pages 574\u2013584. PMLR, 2020.   \n[44] Y. Song and D. P. Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288, 2021.   \n[45] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[46] C. Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, volume 6, pages 583\u2013603. University of California Press, 1972.   \n[47] A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. In Neural Information Processing Systems, 2021.   \n[48] P. Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[49] J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[50] Y. Xu, Z. Liu, M. Tegmark, and T. Jaakkola. Poisson flow generative models. $A r X i\\nu$ , abs/2209.11178, 2022.   \n[51] Y. Xu, Z. Liu, Y. Tian, S. Tong, M. Tegmark, and T. Jaakkola. Pfgm $^{++}$ : Unlocking the potential of physics-inspired generative models. In International Conference on Machine Learning, 2023.   \n[52] Y. Xu, S. Tong, and T. Jaakkola. Stable target field for reduced variance score estimation in diffusion models. arXiv preprint arXiv:2302.00670, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Score matching methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To make score matching tractable, one can express the explicit score matching loss via [25] ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{\\mathrm{ism}}(\\theta;\\pi)=\\mathbb{E}_{x\\sim\\pi}\\left[\\nabla\\cdot F_{\\theta}(x)+\\frac{1}{2}\\|F_{\\theta}(x)\\|^{2}\\right]=L_{\\mathrm{esm}}(\\theta;\\pi)+C,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\nabla\\cdot F_{\\theta}$ is the divergence of the vector field and the constant $C$ is independent of $\\theta$ . While this loss has been widely used [29], this objective is hard to optimize with neural networks as the divergence requires to backpropagate $d$ times. Still, the divergence can be approximated via Hutchinson\u2019s trick [24] leading to sliced score-matching [43]. ", "page_idx": 13}, {"type": "text", "text": "B Hamiltonian ODE: Conservation of energy and volume ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we prove the fundamental properties of the flow $\\varphi_{t}$ of the Hamiltonian ODE (see Equation (2)). As these properties are used throughout this work and usually presented in the context of the physics literature, we include the proofs here for completeness, solely expressing it in the language of probability theory. Throughout this section, let $\\bar{J}\\in\\mathbb{R}^{2d\\times2d}$ be the matrix defined by: ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ=\\left(\\begin{array}{c c}{0}&{{\\mathbf{1}_{d}}}\\\\ {-\\mathbf{1}_{d}}&{0}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B.1 Preservation of energy. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 4. For all $t\\in\\mathbb R$ it holds that $H\\circ\\varphi_{t}=H$ ", "page_idx": 13}, {"type": "text", "text": "Proof. We follow [5, theorem 2.2]. For all $z\\in\\mathbb{R}^{2d}$ one has $\\langle z,J z\\rangle=0$ . Hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d t}H(\\varphi_{t}(z))=\\langle\\nabla H(\\varphi_{t}(z)),\\frac{d}{d t}\\varphi_{t}(z)\\rangle=\\langle\\nabla H(\\varphi_{t}(z)),J\\nabla H(\\varphi_{t}(z))\\rangle=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that $H(\\varphi_{t}(z))=H(\\varphi_{0}(z))=H(z).$ . ", "page_idx": 13}, {"type": "text", "text": "B.2 Preservation of volume. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 5 (Symplectic property). For all $z=(x,v)\\in\\mathbb{R}^{2d}$ and $t\\in\\mathbb R$ the Jacobian $D\\varphi_{t}(z)$ satisfies the following equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\nD\\varphi_{t}(z)^{T}J^{T}D\\varphi_{t}(z)=J^{T}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In particular, $|d e t D\\varphi_{t}|\\equiv1$ , i.e. the $\\varphi_{t}$ is volume-preserving. ", "page_idx": 13}, {"type": "text", "text": "Proof. Here, we follow [33, proposition 2.6.2]. First of all, it can be easily seen that Equation (27) is equivalent to the statement that the bilinear form $\\beta(u,v)=\\langle u,J^{T}v\\rangle$ on $\\bar{\\mathbb{R}}^{2d}$ fulfils ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\beta(u,v)=\\beta(D\\varphi_{t}(z)u,D\\varphi_{t}(z)v)\\quad\\forall z,u,v\\in\\mathbb{R}^{2d}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By taking derivatives of the right-hand side and afterwards using the identities $\\beta(J x,y)=\\langle J^{2}x,y\\rangle=$ $-\\langle x,y\\rangle$ and $\\beta(x,J y)=\\langle x,J^{T}J y\\rangle=\\langle x,y\\rangle$ , one gets: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d}{H}\\beta(D\\varphi_{t}(z)u,D\\varphi_{t}(z)v)=\\beta(\\frac{d}{d t}D\\varphi_{t}(z)u,D\\varphi_{t}(z)v)+\\beta(D\\varphi_{t}(z)u,\\frac{d}{d t}D\\varphi_{t}(z)v)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\beta(D\\frac{d}{d t}\\varphi_{t}(z)u,D\\varphi_{t}(z)v)+\\beta(D\\varphi_{t}(z)u,D\\frac{d}{d t}\\varphi_{t}(z)v)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\beta(J\\nabla^{2}H(\\varphi_{t}(z))D\\varphi_{t}(z)u,D\\varphi_{t}(z)v)+\\beta(D\\varphi_{t}(z)u,J\\nabla^{2}H(\\varphi_{t}(z))D\\varphi_{t}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\left\\langle\\nabla^{2}H(\\varphi_{t}(z))D\\varphi_{t}(z)u,D\\varphi_{t}(z)v\\right\\rangle+\\langle D\\varphi_{t}(z)u,\\nabla^{2}H(\\varphi_{t}(z))D\\varphi_{t}(z)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in the last step I have used that the Hessian is symmetric. One can conclude ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta(D\\varphi_{t}(z)u,D\\varphi_{t}(z)v)=\\beta(D\\varphi_{0}(z)u,D\\varphi_{0}(z)v)=\\beta(u,v)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since |det $J|=1$ , one immediately gets $|\\mathbf{det}D\\varphi_{t}|\\equiv1$ . ", "page_idx": 13}, {"type": "text", "text": "C Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 $(1){\\Rightarrow}\\left(2\\right)$ : Score preserves Boltzmann-Gibbs distribution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section gives proof that the Hamiltonian ODE defined with the score preserves the BoltzmannGibbs distribution. This implication is well-known, and the proof is included for completeness following [5]. ", "page_idx": 14}, {"type": "text", "text": "For all $t\\in\\mathbb R$ and Borel sets $D$ , it holds by Propositions 4 and 5 and a change of variables: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{B G}(\\varphi_{t}(D))=\\!Z^{-1}\\int\\mathbf{1}_{\\varphi_{t}(D)}\\exp{(-H)}d z}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!Z^{-1}\\int\\mathbf{1}_{\\varphi_{t}(D)}\\circ\\varphi_{t}\\exp{(-H\\circ\\varphi_{t})}|\\mathrm{det}D\\varphi_{t}|d z}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!Z^{-1}\\int\\mathbf{1}_{D}\\exp(-H)|d z}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\pi_{B G}(D)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that the symplectic property is crucial, e.g. consider the simple pendulum $(U(x)={\\textstyle{\\frac{1}{2}}}x^{2})$ and $g(z)=(\\|z\\|,0)$ . This function fulfills $\\pi_{B G}(g(z))=\\pi_{B G}(z)$ but it does not leave the distribution invariant (it is not symplectic, either). ", "page_idx": 14}, {"type": "text", "text": "C.2 $(2){\\Rightarrow}\\left(1\\right)$ : Invariance under Boltzmann-Gibbs uniquely defines score ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We write $\\varphi_{t}(z)\\;=\\;(x_{t}^{\\nabla U},v_{t}^{\\nabla U})$ for the solution with force field $-\\nabla U$ . Let\u2019s pick an arbitrary sufficiently regular test function $f:\\ensuremath{\\mathbb{R}^{2d}}\\to\\ensuremath{\\mathbb{R}}$ and let\u2019s define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{\\theta}(t)=\\mathbb{E}_{z\\sim\\pi_{B G}}[f(\\varphi_{t}^{\\theta}(z))]}\\\\ {F_{\\nabla U}(t)=\\mathbb{E}_{z\\sim\\pi_{B G}}[f(\\varphi_{t}(z))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As the Hamiltonian dynamics with force network $-\\nabla U$ preserve the Boltzmann-Gibbs distribution, $F_{\\nabla U}$ must be a constant function, i.e. it derivative is zero. Therefore, we can compute: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0=\\cfrac{d}{d t}F_{\\nabla U}(t)}\\\\ {=\\cfrac{d}{d t}\\cfrac{\\d H_{z\\sim\\pi_{B G}}[f(\\varphi_{t}(z))]}{d t}}\\\\ {=\\cfrac{d}{d t}\\cfrac{\\d H_{z\\sim\\pi_{B G}}}{d t}\\,\\left[\\cfrac{d}{d t}f(\\varphi_{t}(z))\\right]}\\\\ {=\\cfrac{d}{d t}_{z\\sim\\pi_{B G}}\\,\\left[\\nabla_{z}f(\\varphi_{t}(z)^{T}\\,\\cfrac{d}{d t}\\varphi_{t}(z)\\right]}\\\\ {=\\cfrac{d}{d t}_{z\\sim\\pi_{B G}}\\,\\left[\\nabla_{z}f(\\varphi_{t}(z))^{T}\\left(-\\,\\nabla_{t}^{\\nabla U}(z)\\,\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "I.e. at time $t=0$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\frac{d}{d t}F_{\\nabla U}(t)_{|t=0}=\\mathbb{E}_{z=(x,v)\\sim\\pi_{B G}}\\left[\\nabla_{z}f(z)^{T}\\left({-\\nabla U(x)}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As $\\varphi_{t}^{\\theta}$ also preserves the Boltzmann-Gibbs distribution, we can follow the same computation to also get that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\frac{d}{d t}F_{\\theta}(t)_{|t=0}=\\mathbb{E}_{z=(x,v)\\sim\\pi_{B G}}\\left[\\nabla_{z}f(z)^{T}\\left({v\\atop F_{\\theta}(x)}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Substracting Equation (33) from Equation (34), we get that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\mathbb{E}_{z=(x,v)\\sim\\pi_{B G}}\\left[\\nabla_{z}f(z)^{T}\\left(\\overset{0}{F_{\\theta}(x)+\\nabla U(x)}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let\u2019s set $f(z)=f(x,v)=v^{T}(F_{\\theta}(x)+\\nabla U(x))$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{z}f(x,v)=\\left(v^{T}(D F_{\\theta}(x)+\\nabla^{2}U(x))\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $D F_{\\theta}$ denotes the Jacobian and $\\nabla^{2}U$ the Hessian. Then inserting Equation (36) into Equation (35) we get that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\!\\!\\mathbb{E}_{z=(x,v)\\sim\\pi_{B G}}\\left[\\left(\\!\\!\\begin{array}{c}{v^{T}(D F_{\\theta}(x)+\\nabla^{2}U(x))}\\\\ {F_{\\theta}(x)+\\nabla U(x)}\\end{array}\\!\\!\\right)^{T}\\left(\\!\\!\\begin{array}{c}{0}\\\\ {F_{\\theta}(x)+\\nabla U(x)}\\end{array}\\!\\!\\right)\\right]}\\\\ &{~~=\\!\\!\\mathbb{E}_{z=(x,v)\\sim\\pi_{B G}}\\left[\\|F_{\\theta}(x)+\\nabla U(x)\\|^{2}\\right]}\\\\ &{~~=\\!\\!\\mathbb{E}_{x\\sim\\pi}\\left[\\|F_{\\theta}(x)-[-\\nabla U(x)]\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This implies that $F_{\\theta}(x)=-\\nabla U(x)=\\nabla\\log\\pi$ for $\\pi$ -almost every $x$ . ", "page_idx": 15}, {"type": "text", "text": "C.3 $(3){\\leftrightarrow}\\left(1\\right)$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Finally, we show that condition (3) is equivalent to condition (1). Note that if $(x_{t}^{\\theta},v_{t}^{\\theta})\\sim\\pi_{B G}=$ $\\pi\\otimes{\\mathcal{N}}(0,\\mathbf{I}_{d})$ , then $v_{t}^{\\theta}$ is independent of $\\ensuremath{\\boldsymbol{x}}_{t}^{\\theta}$ and it holds that $\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\,=\\,0$ . Therefore, condition (2) trivially implies condition (3). As we have already have shown that condition (2) is equivalent to condition (1), it is sufficient to prove that (3) implies (1). The proof is similar to the proof for $(2){\\Rightarrow}(1)$ . ", "page_idx": 15}, {"type": "text", "text": "Let\u2019s assume that the following condition holds for $z\\sim\\pi_{B G}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n0=\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}=x]\\quad\\mathrm{for~all~}x\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where for brevity we write $(x_{t}^{\\theta},v_{t}^{\\theta})=\\varphi_{t}^{\\theta}(z)$ . Then for $f(z)=f(x,v)=v^{T}(F_{\\theta}(x)+\\nabla U(x))$ as selected above, we can equally derive: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\theta}(t)=\\mathbb{E}_{z\\sim\\pi_{B G}}[f(\\varphi_{t}^{\\theta}(z))]}\\\\ &{\\quad\\quad=\\mathbb{E}[f(x_{t}^{\\theta},v_{t}^{\\theta})]}\\\\ &{\\quad\\quad=\\mathbb{E}[(v_{t}^{\\theta})^{T}(F_{\\theta}(x_{t}^{\\theta})+\\nabla U(x_{t}^{\\theta}))]}\\\\ &{\\quad\\quad=\\mathbb{E}[\\mathbb{E}[(v_{t}^{\\theta})^{T}(F_{\\theta}(x)+\\nabla U(x))]x=x_{t}^{\\theta}]}\\\\ &{\\quad\\quad=\\mathbb{E}[\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}(F_{\\theta}(x_{t}^{\\theta})+\\nabla U(x_{t}^{\\theta}))]}\\\\ &{\\quad\\quad=\\mathbb{E}\\left[0^{T}(F_{\\theta}(x_{t}^{\\theta})-\\nabla U(x_{t}^{\\theta}))\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}[0]}\\\\ &{\\quad\\quad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the same way as above, we can now deduce that $\\begin{array}{r}{\\frac{d}{d t}F_{\\theta}(t)_{|t=0}=0}\\end{array}$ . Similarly to above, we can complete the proof by using Equation (36): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle0\\!=\\!\\frac{d}{d t}\\mathbb{E}_{z\\sim\\pi_{B G}}[f(\\varphi_{t}^{\\theta}(z))]-\\frac{d}{d t}\\mathbb{E}_{z\\sim\\pi_{B G}}[f(\\varphi_{t}(z))]}\\\\ &{\\quad\\displaystyle=\\!\\mathbb{E}_{z=(x,v)\\sim\\pi_{B G}}\\left[\\nabla_{z}f(z)^{T}\\left({\\cal F}_{\\theta}(x)+\\nabla U(x)\\right)\\right]}\\\\ &{\\quad\\displaystyle=\\mathbb{E}_{x\\sim\\pi}\\left[\\|{\\cal F}_{\\theta}(x)-[-\\nabla U(x)]\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Again, we can deduce that $F_{\\theta}(x)=-\\nabla U(x)=\\nabla\\log\\pi(x)$ for $\\pi$ -almost every $x$ . ", "page_idx": 15}, {"type": "text", "text": "D Proof of Equation (12) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We formally proof the identity in Equation (12). We will use the following well-known characterization of the expectation: ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Let $X\\in\\mathbb{R}^{d}$ be a random variable. Then: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[X]=\\underset{m\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}[\\|X-m\\|^{2}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $c=\\mathbb{E}[X]\\in\\mathbb{R}^{d}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|X-m\\|^{2}]=\\mathbb{E}[\\|X-c+c-m\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\|X-c\\|^{2}+2(X-c)^{T}(X-m)+\\|c-m\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\|X-c\\|^{2}]+2(\\mathbb{E}[X]-c)^{T}(X-m)]+\\|c-m\\|^{2}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[\\|X-c\\|^{2}]+\\|c-m\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\mathbb{E}[\\|X-c\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This implies the statement. ", "page_idx": 16}, {"type": "text", "text": "Therefore, we can apply Lemma 1 conditionally on $\\ensuremath{\\boldsymbol{x}}_{t}^{\\theta}$ to see that ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{\\phi^{*}}(x,t)=\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}=x]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We can derive: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathrm{hsm}}(\\phi|\\theta,t)=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[-2V_{\\phi}(x_{t}^{\\theta},t)^{T}v_{t}^{\\theta}+\\|V_{\\phi}(x_{t}^{\\theta},t)\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[\\|v_{t}^{\\theta}\\|^{2}-2V_{\\phi}(x_{t}^{\\theta},t)^{T}v_{t}^{\\theta}+\\|V_{\\phi}(x_{t}^{\\theta},t)\\|^{2}\\right]-\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[\\|v_{t}^{\\theta}\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[\\|V_{\\phi}(x_{t}^{\\theta},t)-v_{t}^{\\theta}\\|^{2}\\right]-\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[\\|v_{t}^{\\theta}\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $\\mathbb{E}_{z\\sim\\pi_{B G}}\\left[\\|v_{t}^{\\theta}\\|^{2}\\right]$ is a constant in $\\phi$ , we can see that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{\\phi}{\\arg\\operatorname*{min}}\\,L_{\\mathrm{hsm}}(\\phi|\\theta,t)=\\underset{\\phi}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{z\\sim\\pi_{B G}}[\\|V_{\\phi}(x_{t}^{\\theta},t)-v_{t}^{\\theta}\\|^{2}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, the Hamiltonian velocity predictor is the minimizer of the above objective. Inserting Equation (12) into Equation (39), we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi}{\\operatorname*{max}}\\,L_{\\mathrm{hsm}}(\\phi|\\theta,t)=\\!L_{\\mathrm{hsm}}(\\phi^{*}|\\theta,t)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\,\\left[2\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}v_{t}^{\\theta}-\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\,\\left[2\\mathbb{E}[\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}v_{t}^{\\theta}|x_{t}^{\\theta}]-\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\,\\left[2\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]-\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\,\\left[2\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}-\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\!\\mathbb{E}_{z\\sim\\pi_{B G}}\\,\\left[\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where have used that the conditional expectation $\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]$ is a constant conditioned on $\\ensuremath{\\boldsymbol{x}}_{t}^{\\theta}$ . This finishes the proof. ", "page_idx": 16}, {"type": "text", "text": "F Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As $\\lambda$ is a distribution with full support over $[0,T)$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\mathrm{hsm}}(\\theta|\\pi)=\\mathbb{E}_{t\\sim\\lambda,z\\sim\\pi_{B G}}\\left[\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}\\right]=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if and only if for very $0\\leq t<T$ (up to measure zero) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Theorem 1, this is equivalent to $F_{\\theta}=\\nabla\\log\\pi$ . Hence, $\\mathbb{D}_{\\mathrm{hsm}}(\\theta|\\pi)=0$ if and only if $F_{\\theta}=\\nabla\\log\\pi$ . ", "page_idx": 16}, {"type": "text", "text": "Remark. Technically speaking, we maximized $V_{\\phi}$ for a fixed $t$ in Proposition 1. However, we remark that maximizing it across $t$ leads to the same result for all $0\\,\\leq\\,t\\,<\\,T$ under reasonable regularity conditions. More specifically, assuming that $\\pi$ is a smooth density with full support, the map $(x,\\dot{t})\\,\\mapsto\\,\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}\\,=\\,x]$ is continuous in $t$ and $x$ . Therefore, as long as $\\left(V_{\\phi}\\right)_{\\phi\\in I}$ covers all continuous function in $t$ and $x$ , the result above is the same. ", "page_idx": 16}, {"type": "text", "text": "G Proof of Proposition 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proof relies on a Taylor approximation of $L_{\\mathrm{hsm}}(\\theta,t)$ around $t=0$ . To finish the proof, we have to show the following three equations: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)_{|t=0}=0}\\\\ &{\\quad\\displaystyle\\frac{d}{d t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)_{|t=0}=0}\\\\ &{\\displaystyle\\frac{d^{2}}{d^{2}t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)_{|t=0}=4L_{\\mathrm{esm}}(\\theta;\\pi)=2\\mathbb{E}_{x\\sim\\pi}[\\|F_{\\theta}(x)-\\nabla\\log\\pi(x)\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Equation (43) Note that at time $t=0$ , it holds that $v_{t}^{\\theta}=v\\sim\\mathcal{N}(0,\\mathbf{I}_{d})$ and $x_{t}^{\\theta}=x\\sim\\pi$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]_{|t=0}=\\mathbb{E}_{x\\sim\\pi,v\\sim\\mathcal{N}(0,\\mathbf{I}_{d}}[v|x]=\\mathbb{E}_{v\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}[v]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by Proposition 1 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\mathrm{hsm}}(\\theta|0,\\theta)=\\mathbb{E}[\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]_{|t=0}\\|^{2}]=\\mathbb{E}[\\|0\\|^{2}]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Equation (44) We can compute: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)=\\!\\!\\mathbb{E}[\\frac{d}{d t}\\|\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}]=2\\mathbb{E}[\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So at time $t=0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)_{|t=0}=2\\mathbb{E}[\\mathbb{E}[v|x]^{T}\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]_{|t=0}]=2\\mathbb{E}[0^{T}\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]_{|t=0}]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Equation (45) Let\u2019s take the second derivative: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d^{2}}{d^{2}t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)=2\\frac{d}{d t}\\mathbb{E}[\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=2\\mathbb{E}[\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]^{T}\\frac{d^{2}}{d^{2}t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]]+2\\mathbb{E}[\\|\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by the product rule. And at time $t=0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d^{2}}{d^{2}t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\pi)_{\\mid t=0}=2\\mathbb{E}\\big[\\mathbb{E}[0^{T}\\frac{d^{2}}{d^{2}t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]]+2\\mathbb{E}[\\|\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]\\|^{2}]=2\\mathbb{E}[\\|\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]_{\\mid t=0}\\|^{2}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\pi_{t}^{\\theta}:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be the density of $(x_{t}^{\\theta},v_{t}^{\\theta})$ and compute: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}=x]=\\int v\\pi_{t}^{\\theta}(v|x)d v=\\int v\\frac{\\pi_{t}^{\\theta}(x,v)}{\\pi_{t}^{\\theta}(x)}d v}\\\\ &{\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}=x]=\\int v\\frac{d}{d t}\\frac{\\pi_{t}^{\\theta}(x,v)}{\\pi_{t}^{\\theta}(x)}d v}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\int v\\frac{\\pi_{t}^{\\theta}(x)\\frac{d}{d t}\\pi_{t}^{\\theta}(x,v)-\\pi_{t}^{\\theta}(x,v)\\frac{d}{d t}\\pi_{t}^{\\theta}(x)}{(\\pi_{t}^{\\theta}(x))^{2}}d v}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $G(x,v)\\,=\\,(v,F_{\\theta}(x))^{T}$ be the Hamiltonian vector field. By the deterministic Fokker-Planck equation [37], we can derive that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\pi_{t}^{\\theta}=-\\textstyle\\nabla\\cdot[\\pi_{t}^{\\theta}G]=-G^{T}\\nabla\\pi_{t}^{\\theta}-\\pi_{t}^{\\theta}\\nabla G\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the Jacobian of $F$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nD G(x,v)=\\left(\\!\\!\\begin{array}{c c}{{0}}&{{{\\bf I}_{d}}}\\\\ {{D F_{\\theta}(x)}}&{{0}}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In particular, $G$ is divergence-free, i.e. $\\nabla\\cdot G=\\operatorname{tr}(D G)=0$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\pi_{t}^{\\theta}=-\\;G^{T}\\nabla\\pi_{t}^{\\theta}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and for particular $\\boldsymbol{x},\\boldsymbol{v}\\in\\mathbb{R}^{d}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\pi_{t}^{\\theta}(x,v)_{|t=0}=-\\left({v}_{\\theta}(x)\\right)^{T}\\nabla\\pi_{0}^{\\theta}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $\\pi_{0}^{\\theta}=\\pi_{B G}$ by construction, we can derive that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla\\pi_{0}^{\\theta}(x,v)=\\nabla\\pi_{B G}(x,v)}}\\\\ {{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{Z}\\nabla[\\exp(-U(x)-\\frac{1}{2}\\|v\\|^{2})]}}\\\\ {{\\quad\\quad\\quad\\quad=\\displaystyle-\\,\\frac{1}{Z}\\left(\\nabla U(x)\\right)\\exp(-U(x)-\\frac{1}{2}\\|v\\|^{2})}}\\\\ {{\\quad\\quad\\quad\\quad=\\displaystyle-\\,\\binom{\\nabla U(x)}{v}\\,\\pi_{B G}(x,v)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Inserting this into Equation (50), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{d}{d t}\\pi_{t}^{\\theta}(x,v)=\\left(\\begin{array}{c}{v}\\\\ {F_{\\theta}(x)}\\end{array}\\right)^{T}\\left(\\nabla U(x)\\right)\\pi_{B G}(x,v)}\\\\ {\\displaystyle=v^{T}(F_{\\theta}(x)+\\nabla U(x))\\pi_{B G}(x,v)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d}{d t}\\pi_{t}^{\\theta}(x)=\\int\\frac{d}{d t}\\pi_{t}^{\\theta}(x,v)d v\\quad}\\\\ {\\displaystyle\\qquad=\\int v^{T}(F_{\\theta}(x)+\\nabla U(x))\\pi_{B G}(x,v)d v\\quad}\\\\ {\\displaystyle\\qquad=\\left[\\int(F_{\\theta}(x)+\\nabla U(x))\\pi(x)d v\\right]^{T}\\left[\\int v\\mathcal{N}(v;0,\\mathbf{I}_{d})d v\\right]}\\\\ {\\displaystyle\\qquad=0\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can insert these identities into Equation (49) to get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}=x]_{t=0}=\\int v\\frac{\\pi_{t}^{\\theta}(x)\\frac{d}{d t}\\pi_{t}^{\\theta}(x,v)-\\pi_{t}^{\\theta}(x,v)\\frac{d}{d t}\\pi_{t}^{\\theta}(x)}{(\\pi_{t}^{\\theta}(x))^{2}}d v_{t=0}}&{}\\\\ {=\\int v\\frac{\\pi(x)v^{T}(F_{\\theta}(x)+\\nabla U(x))\\pi_{B G}(x,v)-0}{\\pi(x)^{2}}d v}&{}\\\\ {=\\int v\\frac{v^{T}(F_{\\theta}(x)+\\nabla U(x))\\pi(x)^{2}}{\\pi(x)^{2}}\\mathcal{N}(v;0\\mathbf{I}_{d})d v}&{}\\\\ {=\\left[\\int v v^{T}\\mathcal{N}(v;0\\mathbf{I}_{d})d v\\right](F_{\\theta}(x)+\\nabla U(x))}&{}\\\\ {=\\mathbf{I}_{d}(F_{\\theta}(x)+\\nabla U(x))}&{}\\\\ {=F_{\\theta}(x)+\\nabla U(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining this with Equation (46), we get that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d^{2}}{d^{2}t}L(\\theta,t)_{\\mid t=0}=\\!2\\mathbb{E}[\\|\\frac{d}{d t}\\mathbb{E}[v_{t}^{\\theta}|x_{t}^{\\theta}]_{\\mid t=0}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad=\\!2\\mathbb{E}[\\|F_{\\theta}(x)+\\nabla U(x))\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad=\\!2\\mathbb{E}[\\|F_{\\theta}(x)-\\nabla\\log\\pi(x)\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad=\\!4L_{\\mathrm{esm}}(\\theta;\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taylor approximation Finally, we can combine the above derivations to get a Taylor approximation of Lhsm(\u03b8, t) around t = 0, i.e. for \u03f5 : R \u2192R with tli\u2192m0t12 |\u03f5(t)| = 0 we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\theta)=\\mathbb{D}_{\\mathrm{hsm}}(\\theta|0,\\theta)+t\\frac{d}{d t}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\theta)_{|t=0}+\\frac{1}{2}t^{2}\\mathbb{D}_{\\mathrm{hsm}}(\\theta|t,\\theta)_{|t=0}+\\epsilon(t)}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{2}t^{2}4L_{\\mathrm{esm}}(\\theta;\\pi)+\\epsilon(t)}\\\\ &{\\quad\\quad\\quad\\quad=2t^{2}L_{\\mathrm{esm}}(\\theta;\\pi)+\\epsilon(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 19}, {"type": "text", "text": "H Proof of Proposition 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Again, let\u2019s consider a probability distribution $\\pi:\\mathbb{R}^{d}\\to\\mathbb{R}$ and the ODE ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\frac{d}{d t}x(t),\\frac{d}{d t}v(t))^{T}=(v(t),F_{\\theta}(x(t),t))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we know allow $F_{\\theta}$ to be time-dependent. Let $(\\boldsymbol{x}_{t},\\boldsymbol{v}_{t})$ be a solution to the above with ODE with $(x_{0},v_{0})=(x,v)\\sim\\pi\\otimes\\mathcal{N}(0,\\mathbf{I}_{d})$ . In addition, write $\\Pi(x,v,t)$ for the distribution at time $t$ (i.e. $(x_{t},v_{t})\\sim\\Pi(\\cdot,\\cdot,\\dot{t}))$ and the location marginal ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int\\Pi(x,v,t)d v=\\pi(x,t)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, we write ", "page_idx": 19}, {"type": "equation", "text": "$$\nV(x,t)=\\mathbb{E}[v_{t}|x_{t}]=\\int v\\pi(v|x,t)d v=V_{\\phi^{*}}(x,t)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for the optimal velocity predictor. ", "page_idx": 19}, {"type": "text", "text": "Deriving marginal ODE. We now show that the evolution of the first marginal can be replicated by an ODE that only depends on $V$ . By the Fokker-Planck equation, we can derive for $G(\\boldsymbol{x},\\boldsymbol{v})=$ $(v,F_{\\theta}(x))^{T}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cfrac{\\partial}{\\partial t}\\Pi(x,v,t)=-\\;\\nabla_{x,v}\\cdot[\\Pi G](x,v,t)}\\\\ &{\\qquad\\qquad\\qquad=-\\;G(x,v,t)^{T}\\nabla_{x,v}\\Pi(x,v,t)-[\\nabla_{x,v}\\cdot G(x,v,t)]\\Pi(x,v,t)}\\\\ &{\\qquad\\qquad\\quad=-\\;\\left(\\underset{F_{\\theta}(x)}{v}\\right)^{T}\\nabla_{x,v}\\Pi(x,v,t)}\\\\ &{\\qquad\\qquad\\qquad=-\\;[v^{T}\\nabla_{x}\\Pi(x,v,t)+F_{\\theta}(x)^{T}\\nabla_{v}\\Pi(x,v,t)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used in the third equation that $\\nabla_{x,v}\\cdot G=0$ (i.e. that $G$ is divergence-free). Therefore, we can derive that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial}{\\partial t}\\pi(x,t)=\\int\\frac{\\partial}{\\partial t}\\Pi(x,v,t)d v}\\\\ &{=-\\int[v^{T}\\nabla_{x}\\Pi(x,v,t)+F_{0}(x)^{T}\\nabla_{x}\\Pi(x,v,t)]d v}\\\\ &{=-\\int v^{T}\\nabla_{x}\\Pi(x,v,t)d v-F_{0}(x)^{T}\\int\\nabla_{x}\\Pi(x,v,t)d v}\\\\ &{=-\\int v^{T}\\nabla_{x}\\Pi(x,v,t)d v-F_{0}(x)^{T}0}\\\\ &{=-\\int v^{T}\\nabla_{x}\\Pi(x,v,t)d v}\\\\ &{=-\\int v^{T}\\nabla_{x}\\Pi(x,v,t)d v}\\\\ &{=-\\nabla_{x}\\cdot\\int\\Pi(x,v,t)d v}\\\\ &{=-\\nabla_{x}\\cdot\\Pi(x,t)\\int v\\frac{\\Pi(x,v,t)}{\\Pi(x,v,t)}d v}\\\\ &{=-\\nabla_{x}\\cdot\\Pi(x,t)\\int v\\Pi(v,t)d v]}\\\\ &{=-\\nabla_{x}\\cdot\\Pi(x,t)V(x,t)d v}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In other words, the vector field $V(x,t)$ satisfies the continuity equation. Therefore, if we initialize $x_{T}\\sim\\pi(\\cdot,T)$ and evolve the ODE ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}x(t)=V(x(t),t)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "backwards until $t=0$ , we know that $x(0)\\sim\\pi(\\cdot,0)=\\pi$ - we sample from our data distribution. ", "page_idx": 20}, {"type": "text", "text": "I Reflection HGFs - A New Example of HGFs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We briefly show that HGFs can give us models other than Oscillation HGFs. For this, we introduce Reflection HGFs here and plot the result of training them in Figure 5 and Figure 6. The idea of the model is that particles can move freely in a box without collision with walls (\u201cvery strong forces\u201d) at the boundaries of the data domain making the particles bounce back (this can be made rigorous with von Neumann boundary conditions). With normally distributed velocities, the distribution of particles will converge towards a uniform distribution. Further, this model can be trained in a simulation-free manner. We trained this HGF model on a simple toy distribution (see figure 3 and figure 4 in the attached PDF). Such a HGF model is distinct from previous models and illustrates that HGFs are not restricted to only Oscillation HGFs, diffusion models, or flow matching. ", "page_idx": 20}, {"type": "image", "img_path": "JJGfCvjpTV/tmp/b88261fd1c08168144aa1547f67e00c6883c12faf188c1865066a1a82e8cdab7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 5: Data distribution (left) and velocity distribution (right) used for Reflection HGFs as initial distribution. With the above starting conditions, a reflection $\\risingdotseq$ \u201dinfinite force\u201d) at the boundaries of the domain is used to simulate trajectories forward (this can be computed in closed form in a simulation-free manner). ", "page_idx": 20}, {"type": "image", "img_path": "JJGfCvjpTV/tmp/5f1bc631e64b435999f1d65ad20f2d7eb6a5ffbaf49e70d617b942e67854e13b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Illustration of sampling with trained Reflection HGFs. At time $t=3.0$ , the distribution is a uniform distribution (sampled by construction). By running the parameterized Hamiltonian ODE backwards in time, we recover the data distribution (see Figure 5). ", "page_idx": 21}, {"type": "text", "text": "J Connection between Flow Matching and HGFs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let $A:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}^{d}$ be a time-dependent vector field and $\\psi$ the diffeomorphic flow defined by the ODE: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{d}{d t}x(t)=A(x(t),t)}\\\\ {x(0)=x}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e. $t\\mapsto\\psi_{t}(x)$ is a solution to the above ODE. With this, we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{d^{2}}{d^{2}t}}x(t)={\\cfrac{d}{d t}}\\left[{\\cfrac{d}{d t}}x(t)\\right]}\\\\ &{\\qquad\\qquad={\\cfrac{d}{d t}}A(x(t),t)}\\\\ &{\\qquad=D_{x}A(x(t),t){\\dot{x}}(t)+{\\cfrac{\\partial}{\\partial t}}A(x(t),t)}\\\\ &{\\qquad=D_{x}A(x(t),t)A(x(t),t)+{\\cfrac{\\partial}{\\partial t}}A(x(t),t)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, if we define the force field, ", "page_idx": 21}, {"type": "equation", "text": "$$\nF(x,t)=D_{x}A(x,t)A(x,t)+\\frac{\\partial}{\\partial t}A(x,t)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we can extend the state space to $(x,v)$ and consider the ODE: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(x(0),v(0))=(x,A(x,0))}}\\\\ {{(\\displaystyle{\\frac{d}{d t}}x(t),\\displaystyle{\\frac{d}{d t}}v(t))=(v(t),F(x(t),t))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then every solution $(\\boldsymbol{x}_{t},\\boldsymbol{v}_{t})$ to the above ODE is also a solution to the flow matching ODE and vice versa. The conditional velocity predictor loss looks as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|V_{\\phi}(\\psi_{t}(x_{t}),t)-\\frac{d}{d t}\\psi_{t}(x_{t})\\|^{2}]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This is exactly the conditional flow matching loss (see equation (14) in [31]). ", "page_idx": 21}, {"type": "text", "text": "K Connection between EDM and Oscillation HGFs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we discuss the relation between HGF and EDM [26]. The EDM paper assumes the perturbation kernel is isotropic Gaussian with standard deviation $\\sigma(t)$ . Thus, the intermediate distribution $p_{t}(\\cdot;\\sigma(t))=p_{d a t a}\\,\\mathbf{\\dot{*}}\\,\\mathcal{N}(\\mathbf{0},\\sigma(t)\\mathbf{I})$ . If we further scale the original variable $x$ with $s(t)$ and consider $\\tilde{y}=s(t)x$ , [26] shows that the corresponding backward ODE of $\\tilde{y}$ is as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{y}=[\\dot{s}(t)\\tilde{y}/s(t)-s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\nabla_{\\tilde{y}}\\log p(\\tilde{y}/s(t);\\sigma(t))]\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will show that, the minimizer of the objective of the Oscillation HFG, i.e., $\\mathbb{E}_{y\\sim\\pi,v\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}[\\|V_{\\phi}(\\cos(t)y+\\sin(t)v,t)-[-\\sin(t)\\bar{y}+\\cos(t)v]\\|^{2}]$ ], equals to the drift term in Eq. 55, when setting $s(t)=\\cos(t)$ and $\\sigma(t)=\\tan(t)$ . ", "page_idx": 21}, {"type": "text", "text": "Denote $\\begin{array}{r l r}{\\tilde{y}}&{{}=}&{\\cos(t)y\\ +\\ \\sin(t)v}\\end{array}$ , then the training objective can be rewritten as $\\begin{array}{r l}&{\\mathbb{E}_{y\\sim\\pi,v\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}[\\|V_{\\phi}(\\tilde{y},t)-[-\\frac{y}{\\sin(t)}+\\frac{\\tilde{y}}{\\tan(y)}]\\|^{2}].}\\end{array}$ The minimizer of the training objective is ", "page_idx": 22}, {"type": "equation", "text": "$$\nV_{\\phi}^{*}(\\tilde{y},t)=\\mathbb{E}_{y\\mid\\tilde{y}}\\left[-\\frac{y}{\\sin(t)}\\right]+\\frac{\\tilde{y}}{\\tan(y)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, we can re-express the score function in Eq. 55 as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\tilde{y}}\\log p(\\tilde{y}/\\cos(t);\\tan(t))=\\nabla_{\\frac{\\tilde{y}}{\\cos(t)}}\\log p(\\tilde{y}/\\cos(t);\\tan(t))\\frac{1}{\\cos(t)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\mathbb{E}_{y\\mid\\tilde{y}}[y]-\\tilde{y}/\\cos(t)}{\\tan^{2}(t)}\\frac{1}{\\cos(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plug Eq. 57 into the backward ODE (Eq. 55), we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\tilde{y}=[\\dot{s}(t)\\tilde{y}/s(t)-s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\nabla_{\\tilde{y}}\\log p(\\tilde{y}/s(t);\\sigma(t))]\\mathrm{d}t}\\\\ &{\\quad=\\bigg[{-\\tan(t)\\tilde{y}-\\tan(t)(\\frac{\\mathbb{E}_{y\\mid\\tilde{y}}[y]-\\tilde{y}/\\cos(t)}{\\tan^{2}(t)}\\frac{1}{\\cos(t)})}\\bigg]\\,\\mathrm{d}t}\\\\ &{\\quad=\\bigg[\\mathbb{E}_{y\\mid\\tilde{y}}\\left[-\\frac{y}{\\sin(t)}+\\frac{\\tilde{y}}{\\tan(y)}\\right]\\bigg]\\,\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "in which the drift term matches the optimal velocity predictor in Eq. 57. Hence, when picking the proper scaling factors, the backward ODE (Eq. 55) is equivalent to $\\dot{\\mathrm{d}}\\tilde{y}=V_{\\phi}^{*}(\\tilde{y},t)\\mathrm{d}t$ . ", "page_idx": 22}, {"type": "text", "text": "Recall that the EDM paper employs the simple scaling $s(t)=1$ and $\\sigma(t)=t$ in Eq. 55. Hence, to align with the time discretization $\\{t_{1},\\dots,t_{n}\\}$ used in EDM during sampling, it suffices to set the time discretization in Oscillation HFG to $\\{\\arctan(t_{1}),\\dots,\\arctan(\\bar{t}_{n})\\}$ , to ensure that the score functions are evaluated on the same $\\sigma{\\mathbf s}$ . ", "page_idx": 22}, {"type": "text", "text": "Remark. Rescaling of the EDM ODE will necessarily lead to the same endpoint if $s(0)\\,=\\,1$ - this is the case by construction. Similarly, changing the noise schedule will lead to the same ODE. However, mapping discretizations will not result in the same ODE. The reason for that is that in general ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s^{\\prime}(t_{n+1})(t_{n+1}-t_{n})\\neq s(t_{t+1})-s(t_{n})}\\\\ &{\\sigma^{\\prime}(t_{n+1})(t_{n+1}-t_{n})\\neq\\sigma(t_{t+1})-\\sigma(t_{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "L Details for Image Generation Benchmarks ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we include more details about the training and sampling of Oscillation HGFs. All the experiments are run on 8 NVIDIA A100 GPUs. We used PyTorch as a library for automatic differentiation [38]. Our image processing pipeline follows [26]. We use the $\\scriptstyle\\mathrm{DDPM++}$ backbone [45, 21]. The preconditioning was removed. We set the reference batch size to 516 on CIFAR-10 and 256 on FFHQ. We train for 200 million images in total, corresponding to approximately 3000 epochs and $\\sim48$ hours of training time for CIFAR-10 and $\\sim96$ hours for FFHQ. As outlined in the experiments section, the hyperparameters and training procedure are the same as [26]: namely, we used the Adam optimizer with learning rate 0.001, exponential moving average (EMA) with momentum 0.5, data augmentation pipeline adapted from [28], dropout probability of 0.13, and FP32 precision. For sampling, we use the 2nd order Heun\u2019s sampler [26]. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 23}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 23}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 23}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 23}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 23}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The abstract is a summary of the contributions and the scope of the method. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: While we did not include the a separate \"Limitations\" section, we discussed limitations in the \"Conclusion\" section as well as in the main text (e.g. Section 4.3). Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The assumptions are fully stated in the supplementary material and referenced. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide experimental details in the supplementary material (Appendix L). Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Code can be provided upon request. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We list them in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Generating error bars for image generation benchmarks at this scale would be computationally infeasible. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide them in the supplementary material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We follow all guidelines and the code of conduct. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: The core contribution of our paper is a method. The extent of the societal impact via deepfakes is not changed due to this work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We do not release any data. We will release models upon publication Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We list the resources we used in the supplementary material. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No assets are released. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No research on human subjects was performed. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No applicable in this case. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]