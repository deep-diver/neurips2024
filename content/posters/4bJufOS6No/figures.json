[{"figure_path": "4bJufOS6No/figures/figures_0_1.jpg", "caption": "Figure 1: Multi-Modal Detection (MM-Det) leverages features from spatiotemporal (ST) information (\u25cf), a CLIP encoder (\u25a0), and an LMM (\u25b2). The Fused feature (\u25a0) achieves state-of-the-art performance in our Diffusion Video Forensics (DVF) dataset.", "description": "This figure shows a radar chart comparing the performance of different feature extraction methods used in the Multi-Modal Detection (MM-Det) model for diffusion video forgery detection.  The methods compared are spatiotemporal (ST) features, CLIP encoder features, and Large Multimodal Model (LMM) features.  The chart shows that combining these features ('Fused') results in the best performance on the Diffusion Video Forensics (DVF) dataset. Each axis represents a different diffusion video generation method, and the distance from the center to the line represents the performance of the method on that dataset. The figure demonstrates the effectiveness of MM-Det in detecting diffusion-generated videos.", "section": "1 Introduction"}, {"figure_path": "4bJufOS6No/figures/figures_1_1.jpg", "caption": "Figure 2: LMMs detect visual artifacts and anomalies, offering detailed textual reasoning that explains whether the image is generated using Artificial Intelligence (AI) techniques. The powerful representation in the visual domain enables LMMs to understand complex contexts within frames. Furthermore, their advanced language reasoning capability implicitly reveals image authenticity and provenance. For instance, the term like \"consistent shape\" refers to common features in authentic content, while \u201cunrealistic color\u201d signifies typical artifacts in forged content. This linguistic proficiency stems from the superior perception and comprehension abilities of LMMs, contributing to a generalizable multimodal feature space. By leveraging the visual understanding and textual reasoning abilities of LMMs, we construct a Multi-Modal Forgery Representation (MMFR).", "description": "This figure illustrates how Large Multi-modal Models (LMMs) are used to detect forgeries by analyzing both visual and textual information.  The LMMs perceive visual artifacts and anomalies in an image and provide a textual explanation for why it is considered real or fake. This textual explanation highlights features like \"consistent shape\" (indicative of authenticity) and \"unrealistic color\" (indicative of forgery). The process culminates in the generation of a Multi-Modal Forgery Representation (MMFR), used for forgery detection.", "section": "1 Introduction"}, {"figure_path": "4bJufOS6No/figures/figures_2_1.jpg", "caption": "Figure 3: The residual difference between VQ-VAE [51] reconstructed images and real ones. Given an encoder E and a decoder D of a VQ-VAE and taking the input video v, the reconstructed video v' is obtained as v' = D(E(v)). The VQ-VAE reconstruction of real images exhibits obvious edges and visible traces, whereas diffusion-generated ones are reconstructed more effectively, offering residual difference images with fewer visible traces.", "description": "This figure shows the residual difference between the original images and the images reconstructed using a VQ-VAE.  The reconstruction of real images shows clear edges and visible traces, while the reconstruction of diffusion-generated images is much more effective, resulting in residual images with far fewer visible traces. This highlights the ability of the VQ-VAE to amplify the artifacts present in diffusion-generated videos, making them easier to detect.", "section": "3.2 Capturing Spatial-Temporal Forgery Traces"}, {"figure_path": "4bJufOS6No/figures/figures_3_1.jpg", "caption": "Figure 4: Multi-Modal Detection network (MM-Det) architecture. Given an input video X, the Large Multi-modal Model (LMM) branch takes the frame and instructions to generate Multi-Modal Forgery Representation (MMFR). Hidden states from the visual encoder and large language model are extracted to form the MMFR, denoted as FM, which helps capture the forgery traces among different diffusion-generated videos. In the Spatio-Temporal (ST) branch, videos are first reconstructed via a VQ-VAE, and then fed into a CNN encoder, followed by In-and-Across Frame Attention (IAFA) modules detailed in Sec. 3.2. IAFA is introduced to capture features based on spatial artifacts and temporal inconsistencies, termed as FST. At last, a dynamic fusion strategy combines FM and FST for the final forgery prediction.", "description": "This figure illustrates the architecture of the proposed Multi-Modal Detection (MM-Det) network for diffusion video forgery detection.  It shows two main branches: the LMM branch and the ST branch. The LMM branch uses a Large Multi-modal Model to generate a Multi-Modal Forgery Representation (MMFR) that captures forgery traces from various videos. The ST branch uses a VQ-VAE, CNN encoder, and IAFA to extract spatiotemporal features based on spatial artifacts and temporal inconsistencies.  Finally, a dynamic fusion strategy combines the features from both branches for the final forgery prediction.", "section": "3 Methods"}, {"figure_path": "4bJufOS6No/figures/figures_4_1.jpg", "caption": "Figure 5: (a) In-and-Across Frame Attention (IAFA): Each input frame (or its feature map) is divided into patches that are transformed into tokens, termed P-tokens (). We introduce additional frame-centric tokens FC-tokens () encapsulating the global forgery information of the video frame. In each transformer layer, self-attention is applied alternately among all P-tokens from video frames as well as among the same frame's P-tokens and its FC-tokens. (b) The dynamic fusion strategy captures that takes FST and FM as inputs and output channel-wise dependencies, which help refine forgery representations for the fusion.", "description": "This figure details the architecture of the In-and-Across Frame Attention (IAFA) mechanism and the dynamic fusion strategy in the MM-Det model.  IAFA processes both local (patch-level) and global (frame-level) features within each frame and across frames using a transformer network. The dynamic fusion strategy combines features from the IAFA module and a multi-modal forgery representation to generate a final forgery prediction.", "section": "3. Methods"}, {"figure_path": "4bJufOS6No/figures/figures_5_1.jpg", "caption": "Figure 6: Sampled videos from DVF dataset. DVF contains 8 video generation methods, including 7 text-to-video methods and 1 image-to-video method. Real videos are selected from Internvid-10M [54] and Youtube-8M [1]. [Key: OSora: OpenSora; VC1: Videocrafter1 [5]; Zscope: Zeroscope; St. V. D.: Stable Video Diffusion [4]; St.Diff.: Stable Diffusion [42]; St. V.:Stable Video]", "description": "This figure shows sample videos from the Diffusion Video Forensics (DVF) dataset, which is a new dataset created for this research.  It showcases the variety of videos included, highlighting both real videos (from Internvid-10M and Youtube-8M) and fake videos generated using eight different diffusion models (OSora, VC1, Zscope, Sora, Pika, St. V. D., St. Diff., and St.V.). The figure visually demonstrates the diversity of content and quality in the DVF dataset.", "section": "4 Diffusion Video Forensics (DVF) Dataset"}, {"figure_path": "4bJufOS6No/figures/figures_6_1.jpg", "caption": "Figure 7: The overview of DVF dataset: (a) The procedure of forged video generation and collection. Real frames and captions are sampled from Internvid-10M [54] and Youtube-8M [1] for text-to-video and image-to-video generation, respectively. (b) DVF contains videos at various resolutions and durations. (c) The scale of each video dataset in DVF is measured by the frame and video numbers. [Key: VC1: Videocrafter1 [5]; Zscope: Zeroscope; OSora: OpenSora; St.Diff.: Stable Diffusion [42]; St. V.:Stable Video; St. V. D.: Stable Video Diffusion [4]]", "description": "This figure provides an overview of the Diffusion Video Forensics (DVF) dataset.  Panel (a) illustrates the process of generating fake videos using both text-to-video and image-to-video methods, drawing on real data from Internvid-10M and YouTube-8M.  Panel (b) shows the distribution of videos across different resolutions and durations. Panel (c) presents a bar chart visualizing the number of videos and frames within the dataset for each of the eight included video generation methods.", "section": "4 Diffusion Video Forensics (DVF) Dataset"}, {"figure_path": "4bJufOS6No/figures/figures_8_1.jpg", "caption": "Figure 8: Visualization of artifacts captured from our IAFA and ViViT [3]. We use activation maps to highlight spatial weights within each frame. All content is generated by VideoCrafter1. Features from the last layer of transformers are extracted for visualization.", "description": "This figure compares the attention maps from the last layer of the transformer in IAFA and ViViT to highlight spatial weights within each frame.  The activation maps show where the models focus their attention when trying to detect forgery.  All the video frames shown are generated by VideoCrafter1.  The comparison illustrates differences in how IAFA and ViViT identify spatial artifacts in generated videos.", "section": "6.4 Spatio Temporal Information Anaylsis"}, {"figure_path": "4bJufOS6No/figures/figures_9_1.jpg", "caption": "Figure 9: (a): Clustering accuracy using features from different layers in LMM branch showcases MMFR\u2019s ability to discern forgeries. (b)(c): t-SNE [52] visualization of features from ST and LMM branches. For each dataset, 100 videos are sampled and clustered for good visibility. Both features demonstrate boundaries between real and forgery videos. (b) Features from the ST branch. (c): Features from the LMM branch.", "description": "This figure presents an analysis of the Multimodal Forgery Representation (MMFR) and its effectiveness in distinguishing real and fake videos.  (a) shows clustering accuracy using features from different layers of the LMM branch, demonstrating that certain layers are better at distinguishing forgeries. (b) and (c) use t-SNE to visualize features from the ST (spatio-temporal) and LMM branches, respectively, showing clear separation between real and fake videos, highlighting the effectiveness of MMFR in capturing discriminatory characteristics.", "section": "Multimodal Forgery Representation Analysis"}]