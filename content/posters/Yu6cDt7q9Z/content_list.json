[{"type": "text", "text": "Schedule Your Edit: A Simple yet Effective Diffusion Noise Schedule for Image Editing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haonan Lin1 Yan Chen1 Jiahao Wang1 Wenbin An3 Mengmeng Wang2,5\u2217 ", "page_idx": 0}, {"type": "text", "text": "Feng Tian1 Yong Liu4,5 Guang Dai5 Jingdong Wang6 Qianying Wang7 ", "page_idx": 0}, {"type": "text", "text": "1 School of Comp. Science & Technology, MOEKLINNS Lab, Xi\u2019an Jiaotong University 2 College of Comp. Science & Technology, Zhejiang University of Technology   \n3 School of Auto. Science & Engineering, MOEKLINNS Lab, Xi\u2019an Jiaotong University 4 Institute of Cyber-Systems and Control, Zhejiang University 5 SGIT AI Lab, State Grid Corporation of China 6 Baidu Inc 7 Lenovo Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Text-guided diffusion models have significantly advanced image editing, enabling high-quality and diverse modifications driven by text prompts. However, effective editing requires inverting the source image into a latent space, a process often hindered by prediction errors inherent in DDIM inversion. These errors accumulate during the diffusion process, resulting in inferior content preservation and edit fidelity, especially with conditional inputs. We address these challenges by investigating the primary contributors to error accumulation in DDIM inversion and identify the singularity problem in traditional noise schedules as a key issue. To resolve this, we introduce the Logistic Schedule, a novel noise schedule designed to eliminate singularities, improve inversion stability, and provide a better noise space for image editing. This schedule reduces noise prediction errors, enabling more faithful editing that preserves the original content of the source image. Our approach requires no additional retraining and is compatible with various existing editing methods. Experiments across eight editing tasks demonstrate the Logistic Schedule\u2019s superior performance in content preservation and edit fidelity compared to traditional noise schedules, highlighting its adaptability and effectiveness. (Project page: https://lonelvino.github.io/SYE/) ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Text-guided diffusion models have emerged as a leading technique in image generation, offering remarkable visual quality and diversity [2, 42, 50, 69] . The noise latent space of these models can be leveraged to retain and modify images [32, 66, 68], enabling text-guided editing where a source image is adjusted based on a target prompt. This requires first inverting the source image into a latent variable (e.g., via DDIM inversion), due to the absence of its predefined latent space [28, 39]. ", "page_idx": 0}, {"type": "text", "text": "While DDIM inversion proves effective for unconditional diffusion models [43, 55], it results in inferior content preservation and suboptimal edit fidelity when applied to conditional inputs [12, 17]. This phenomenon is particularly evident in image editing, which requires incorporating new conditionals into the generation process [16, 59, 33, 61]. DDIM converts the DDPM into a deterministic process by approximating the Markov process as a non-Markov process based on a local linearization assumption [55]. This approximation introduces noise prediction errors that accumulate throughout the diffusion process, leading to deviations in the inverted latent representation from its original distribution, as illustrated in Fig. 2 left. Recently, inversion-based editing methods have emerged as a promising paradigm to address these issues by aligning the reconstruction path more closely with the DDIM inversion trajectory, thereby ensuring the preservation of the original content in the edited images [41, 15, 44, 10, 25]. However, these methods still heavily rely on the accuracy of the DDIM inversion. This leads us to a fundamental question: What if we correct the DDIM inversion errors to naturally reduce the loss of original content in the edited images? ", "page_idx": 0}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/9f645df3fc5f52b0c39da067e223e0f9cf068d084bf3ba1c27c79e85e1d452b7.jpg", "img_caption": ["Figure 1: Compared to linear noise schedule, Logistic Schedule $\\pmb{\\mathrm{\\Sigma}}$ demonstrates high fidelity in attributes content editing (a, b) with EF-DDPM [21], $\\pmb{\\varphi}$ preserves the high-level semantics of the source image while performing object translation (c) with pix2pix-zero [45] and style/scene transferring (d, e) with StyleDiffusion [63], and $\\pmb{\\otimes}$ successfully conducts non-rigid alteration (f) via MasaCtrl [6]. Text prompts corresponding to each input image are presented beneath each sample, with words introduced for image editing distinctly highlighted in red. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Unlike previous inversion-based editing methods that focus on minimizing the distance between $\\mathbf{X}_{t}^{\\prime\\prime}$ and $\\mathbf{x}_{t}^{*}$ (Fig. 2 left), we investigate the primary reason for error accumulation in DDIM inversion. Based on the fact that DDIM samplers can be derived by deterministic ODE processes [3, 38, 71], our analysis reveals that these traditional noise schedule designs result in a singularity problem (Fig. 2 right) when treating the DDIM inversion process as solving a differentiable ODE. This results in unreliable noise predictions from the start, and as errors accumulate, the editing results degrade (Fig. 1). This insight motivates us to address the problem at its source: the noise schedule itself. To our knowledge, this is the first work focusing on designing noise schedules specifically for image editing, providing an optimized solution without requiring complete model retraining [14, 20, 23, 29, 26, 34]. ", "page_idx": 1}, {"type": "text", "text": "We present a simple yet effective noise schedule, Logistic Schedule, designed to resolve the singularity problem of previous noise schedules and enhance inverted latents for image editing. The key ideas behind Logistic Schedule are twofold: (1) creating a well-defined noise schedule to improve inversion stability, and (2) providing a better noise space that enables editing faithful to the source image. Specifically, Logistic Schedule eliminates singularities at the beginning of the inversion process, thereby reducing noise prediction errors in the inverted latents. It enables more stable data perturbation to preserve the original content of the source image in the edited image. Importantly, this design is effective and compatible with other editing methods without requiring additional retraining. ", "page_idx": 1}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/a2f8d30947de488a84b4958b6d69ab2da25f11be3ccf91574807fb3f8979161e.jpg", "img_caption": ["Figure 2: Illustration of the DDIM inversion in image editing and its challenges. Left: starting from the source image $\\mathbf{X}_{0}$ , the ideal latent $\\mathbf{X}_{t}$ is approximated by the inverted latent $\\mathbf{x}_{t}^{*}$ using DDIM inversion. The perturbed noisy latent $\\mathbf{x}_{T}^{*}$ is then sampled in two branches\u2014one for the source condition and one for the target condition\u2014yielding the reconstructed and edited images respectively. Right: the numerical computations of $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ for scaled linear and cosine noise schedules, highlighting the singularity at $t=0$ that leads to potential inaccuracies in noise prediction during inversion. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "We conducted experiments across eight distinct editing tasks using approximately 1600 images from diverse scenes. Fig. 1 illustrates that our Logistic Schedule effectively enhances editing results in terms of essential content preservation and edit fidelity compared to commonly used noise schedules like the linear schedule. Moreover, our schedule can be seamlessly integrated with various existing diffusionbased editing techniques, demonstrating its versatility and effectiveness. Our main contributions are summarized as follows: (1) Theoretical Analysis: We analyze the failure of DDIM inversion in realimage editing step by step, identifying the singularity in the noise schedule as the key issue to address. (2) Methodology: We introduce Logistic Schedule, a novel diffusion noise schedule specifically tailored for real-image editing, which effectively reduces prediction errors during inversion. (3) Superiority: We showcase Logistic Schedule\u2019s adaptability by integrating it with various editing methods and demonstrate its consistent superior performance across different editing tasks. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section will introduce diffusion models and their noise schedules, along with DDIM inversion, which are crucial for text-guided editing of real images. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Models. Denoising Diffusion Probabilistic Models (DDPM) [18] are designed to transform a random noise vector $\\mathbf{X}T$ into a series of intermediate samples $\\mathbf{X}_{t}$ , and eventually a final image $\\mathbf{X}_{0}$ by progressively adding Gaussian noise $\\epsilon\\sim\\cal N(0,\\bf{I})$ according to a noise schedule $\\beta_{1},\\dots,\\beta_{T}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1}+\\sqrt{\\beta_{t}}\\epsilon_{t-1},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t\\ \\sim\\ [1,T]$ and $T$ denotes the number of timesteps. The noise schedule determines the distribution of noise scales and is designed to ensure that the noise scale at each step is proportional to the remaining signal, which is usually fixed without additional learning. According to the properties of conditional Gaussian distributions, $\\mathbf{X}_{t}$ can be derived from a real image $\\mathbf{X}_{0}$ in the following closed form by reparameterizing $\\begin{array}{r}{\\alpha_{t}=1-\\beta_{t},\\bar{\\alpha_{t}}=\\prod_{i=1}^{t}\\alpha_{i}}\\end{array}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Another commonly used sampling method is Denoising Diffusion Implicit Models (DDIM) [55], which formulate a denoising process to generate $\\mathbf{X}_{t-1}$ from a sample $\\mathbf{X}_{t}$ via: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t-1}=\\sqrt{\\alpha_{t-1}}\\left(\\frac{\\mathbf{x}_{t}-\\sqrt{1-\\alpha_{t}}\\boldsymbol{\\epsilon}_{\\theta}^{(t)}\\left(\\mathbf{x}_{t}\\right)}{\\sqrt{\\alpha_{t}}}\\right)+\\underbrace{\\sqrt{1-\\alpha_{t-1}-\\sigma_{t}^{2}}\\cdot\\boldsymbol{\\epsilon}_{\\theta}^{(t)}\\left(\\mathbf{x}_{t}\\right)}_{\\mathrm{~,~}}+\\underbrace{\\vphantom{\\sum_{t}\\epsilon_{t}}\\boldsymbol{\\epsilon}_{t}}_{\\mathrm{~,~}}\\quad\\sigma_{t}\\in\\mathrm{~\\Omega~}_{\\times}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\epsilon_{t}\\sim{\\cal N}(0,{\\bf I}),$ $\\sigma_{t}$ is the variance schedule, and $\\epsilon_{\\theta}$ is a network trained to predict the noise added. $\\begin{array}{r}{\\sigma_{t}=\\sqrt{\\frac{1-\\alpha_{t-1}}{1-\\alpha_{t}}}\\sqrt{1-\\frac{\\alpha_{t}}{\\alpha_{t-1}}}}\\end{array}$ for all $t$ , the forward process becomes Markovian, and the generation process becomes a DDPM. And in a special case when $\\sigma_{t}=0$ for all $t$ , the forward process become deterministic given $\\mathbf{X}_{t-1}$ and $\\mathbf{X}_{0}$ , except for $t=1$ , and the generate process becomes a DDIM. ", "page_idx": 3}, {"type": "text", "text": "Inversion in Image Editing. Although text-to-image diffusion models [50, 52, 19] have advanced feature spaces that support various downstream tasks [67, 37, 36], applying them to real images (non-generated images) is challenging because these images lack a natural diffusion feature space. Editing a real image first requires obtaining the latent variables $\\mathbf{X}T$ from the original image $\\mathbf{X}_{0}$ and then performing the generation process under new conditions. To bridge this gap, DDIM inversion [55] is predominantly used due to its deterministic process, which can be represented by reversing the generation process in Eq. 2 with $\\sigma_{t}=0$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}^{*}=\\frac{\\sqrt{\\alpha_{t}}}{\\sqrt{\\alpha_{t-1}}}\\mathbf{x}_{t-1}^{*}+\\sqrt{\\alpha_{t}}\\left(\\sqrt{\\frac{1}{\\alpha_{t}}-1}-\\sqrt{\\frac{1}{\\alpha_{t-1}}-1}\\right)\\epsilon_{\\theta}\\left(\\mathbf{x}_{t-1}^{*},t-1\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, existing editing methods that rely on vanilla DDIM inversion struggle to achieve both content preservation and edit fidelity when applied to real images [1, 4, 16]. Recently, inversionbased editing methods have improved the edited results by maintaining two simultaneous procedures: reconstruction and editing, as shown in Fig. 2 left. These methods align the reconstruction path $(\\mathbf{x^{'}})$ more closely with the DDIM inversion trajectory $(\\mathbf{x}^{*})$ , ensuring better preservation of the original content in the edited image [41, 15, 44, 25, 10]. Despite their effectiveness, these methods still heavily rely on the accuracy of the inverted latents obtained from DDIM inversion. In contrast, we start from a different perspective, focusing on improving the DDIM inversion accuracy to naturally enhance the edited results. In the following section, we begin with the transition from DDPM to DDIM, emphasizing the need for a better noise schedule for the inversion process. ", "page_idx": 3}, {"type": "text", "text": "3 On the Failure of DDIM Inversion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Warmup: Error Accumulation of DDIM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "DDIM inversion for real images is unstable due to its reliance on a local linearization assumption at each step, leading to error accumulation and content loss from the original image. Specifically, DDIM assumes that the denoising process in Eq. 2 is roughly invertible, meaning $\\mathbf{x}_{t}^{*}$ can be approximately recovered from $\\mathbf{x}_{t-1}^{*}$ via: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}^{*}=\\frac{\\mathbf{x}_{t-1}^{*}-b_{t}\\epsilon(\\mathbf{x}_{t}^{*},t)}{a_{t}}\\approx\\frac{\\mathbf{x}_{t-1}^{*}-b_{t}\\epsilon(\\mathbf{x}_{t-1}^{*},t)}{a_{t}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $a_{t}\\,=\\,\\sqrt{\\alpha_{t-1}/\\alpha_{t}}$ and $b_{t}\\,=\\,-\\sqrt{\\alpha_{t-1}(1-\\alpha_{t})/\\alpha_{t}}+\\sqrt{1-\\alpha_{t-1}}$ . This approximation assumes $\\epsilon(\\mathbf{x}_{t}^{*},t)\\approx\\epsilon(\\mathbf{x}_{t-1}^{*},t)$ , and the inversion\u2019s accuracy depends on this assumption. However, ensuring accurate inversion under this assumption requires a sufficient number of discretization steps, which increases time costs and is impractical for many applications. With fewer timesteps or higher noise levels, error accumulation becomes more pronounced, resulting in distorted reconstructions, as shown in Fig. 2 left. This occurs because once we deviate from the linearization assumption, the interpolation operation in Eq. 3 fails. The primary issue arises when estimating the \u201cpredicted $\\mathbf{X}_{0}\"$ in Eq. 2 at the initial step ${\\mathit{\\omega}}^{t}=1$ , indicated by the red arrow in Fig. 2 left), where a simple expression for the posterior mean conditioned on $\\mathbf{X}_{t}$ no longer exists [55]. Moreover, this problem is exacerbated in image editing, where the denoising process must incorporate new conditions into the image content. This increases the difficulty of noise predictions, leading to more severe artifacts and distortions. ", "page_idx": 3}, {"type": "text", "text": "3.2 The Devil Is in the Singularities ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To get around this issue, our first insight is to reduce the prediction error at the beginning of the forward (inversion) process. But before we can figure out how to fix the error, we need to pinpoint the problem. We first provide the continuous generalization of DDPM, since sampling from diffusion models can be viewed alternatively as solving the corresponding ODE process [57, 38]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}=\\left[\\mathbf{f}(\\mathbf{x},t)-\\frac{1}{2}g(t)^{2}\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/7e904668922160fda9bd024905423af45a85ef3e801199bdd9c70900e7f3f819.jpg", "img_caption": ["Figure 3: Left: trends of $\\sqrt{1-\\alpha_{t}}$ (noise scales) for scaled linear, cosine, and logistic noise schedules. Right: $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ for the logistic schedule, highlighting its smooth transition, which prevents singularities and maintains the integrity of the initial latent vector $\\mathbf{X}_{0}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{f}(\\cdot,t)$ is a vector-valued function called the drift coefficient of ${\\bf x}(t)$ , and $g(\\cdot)$ is a scalar function known as the diffusion coefficient of ${\\bf x}(t)$ . And the ODE form of DDIM is equivalent to a special case of Eq. 4, as long as $\\alpha_{t}$ and $\\alpha_{t-\\Delta t}$ are close enough (refer to details in Appendix A). ", "page_idx": 4}, {"type": "text", "text": "By treating the DDIM inversion process as solving a differentiable ODE, we emphasize that precise and stable computation of $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ at each timestep $t$ is crucial for accurate noise prediction, especially at the start of the inversion process. Fig. 2 right highlights the pitfalls of widely-used scaled linear [18] and cosine [43] noise schedules through numerical computations of $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 (Singularity in Inversion Process). During the inversion process, there exists $a$ singularity at $t=0$ for both the scaled linear and cosine schedule (Fig. 2 right): ", "page_idx": 4}, {"type": "equation", "text": "$$\nW h e n\\left.t=0,\\,\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}\\right|_{t\\rightarrow0}=\\frac{0}{0}\\cdot s i g n(\\epsilon)=\\infty\\cdot s i g n(\\epsilon).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This singularity significantly affects the starting point of the inversion process during image editing tasks. Properly modeling $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ ensures that the inversion closely aligns with the true continuous dynamics of the diffusion process, thereby reducing errors and enhancing the fidelity of the inverted latents, which is critical for high-quality image editing. The proof can be found in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "We argue that singularities in modeling $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ cause significant issues in inversion-based text-guided image editing. Specifically, (1) the instability in the inversion process arises from the singularity of the derivatives at $t\\,=\\,0$ , leading to inaccurate noise component estimates, making the starting point inconsistent with the data\u2019s true characteristics. The fast sampling in DDIM exacerbates error accumulation, where minor initial errors lead to substantial deviations in the final inverted latents. As a result, reconstructed or edited images may display visual inconsistencies, distorted details, or unnatural artifacts, reducing the overall quality and fidelity. Furthermore, the singularity can also lead to (2) poor handling of complex data distributions in the real world. Discontinuities in derivatives result in the model receiving inconsistent and unreliable signals during the diffusion probabilistic modeling. This hinders the model\u2019s ability to capture intricate patterns and details, disrupting the consistency and integrity within an image [30]. ", "page_idx": 4}, {"type": "text", "text": "4 Better Noise Schedule Helps Inversion and Editing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Well-Defined Schedule Improve Inversion Stability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To address the issues highlighted in Proposition 3.1, we propose a new noise schedule in terms of $\\bar{\\alpha}_{t}$ , since $\\bar{\\alpha}_{t}$ represents the remaining signals in the latents during the diffusion process (Eq. 1). Following the recommendations from iDDPM [43], the noise schedule should ensure that noise is added more slowly at the beginning to preserve image information in the middle of the diffusion process. We introduce our logistic noise schedule as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}=\\frac{1}{1+e^{-k(t-t_{0})}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k$ and $t$ are hyperparameters that control the steepness and midpoint of the logistic function, respectively. In our experiments, we set $k=0.015$ and $t_{0}=\\mathrm{int}(0.6T)$ , as discussed in Section 5.3.1. ", "page_idx": 4}, {"type": "text", "text": "Our logistic schedule is designed to have a linear drop-off of $\\alpha_{t}$ in the middle of the diffusion process, with minimal changes near the extremes of $t=0$ and $t=T$ , thus preventing abrupt changes in the noise level. Fig. 3 left demonstrates the progression of $\\sqrt{1-\\alpha_{t}}$ for different schedules, in which linear and cosine schedules tend to add noise too quickly during the early stage of the inversion process. Crucially, our logistic noise schedule avoid the singularity of $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ at $t=0$ . For simplicity in expression, we set $k=0.015$ and $t_{0}=30$ , resulting in the following: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{\\Psi}\\mathbf{W}\\mathbf{h}\\mathbf{e}\\mathbf{n}\\;t=0,\\left.{\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}}\\right|_{t\\rightarrow0}=1.486e^{-3}\\epsilon-1.318e^{-3}\\mathbf{x}_{0}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is provided in Appendix B, and the trend of the derivatives of our logistic schedule is illustrated in Fig. 5 right. By ensuring a smooth and continuous transition in noise levels, the logistic schedule maintains the integrity of the initial latent vector $\\mathbf{X}_{0}$ . This alignment with the diffusion process\u2019s continuous dynamics prevents undesired deviations, reduces errors, and leads to more accurate and stable latent predictions, improving the inversion process\u2019s fidelity. ", "page_idx": 5}, {"type": "text", "text": "4.2 Exploring Noise Space of Logistic Schedule ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now explore the properties of our logistic noise schedule and its influence on the noise space, specifically comparing the logSNR trends and inversion processes of different noise schedules. ", "page_idx": 5}, {"type": "text", "text": "Steady Information Perturbation. As depicted in Fig. 4 (left), the linear and cosine schedules tend to drastically degrade image information at the initial stage of inversion, as evidenced by the rapid drop in logSNR. In contrast, our logistic schedule exhibits a more linear decrease in logSNR before the final stage, ensuring steady data perturbation. This steadiness allows the logistic schedule to capture a richer set of features and nuances from the original image, facilitating more detailed reproduction and higher fidelity in the edited images. ", "page_idx": 5}, {"type": "text", "text": "Comprehensive Pattern Capture. As shown in Fig. 4 (right), we visualize the latents during the inversion (forward) process, using 50 timesteps with the final step at 981 instead of 999. In the early stage, our Logistic Schedule preserves more original image information, reflecting the logSNR trend. Considering the later stage, linear and cosine schedules retain more low-frequency components due to higher endpoint SNRs, explaining why their noise maps don\u2019t fully cover the image. In contrast, our Logistic Schedule ensures that the inverted latent closely resembles pure Gaussian noise, minimizing the retention of low-frequency components. This thorough process ensures that the inversion encodes a broader array of the original image\u2019s information, thereby enhancing the quality and fidelity of the edited images. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the section below, we evaluate our method both quantitatively and qualitatively on text-guided editing of real images. To validate the versatility and effectiveness of our proposed Logistic Schedule, we compare it with linear and concise schedules by employing different editing approaches across various editing tasks. Refer to Appendix E for detailed experimental results. ", "page_idx": 5}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/c524c15b17154f8c5f78507d1891b05f84938a706bbf345efdbb1104f20fdd75.jpg", "img_caption": ["Figure 5: Qualitative comparison of the Logistic Schedule with linear and cosine schedules across various image editing tasks. To preserve background content during $\\textcircled{1}$ attribute editing tasks (e.g., colors, and materials), we employ Edit Friendly DDPM [21]; for tasks requiring background preservation such as $\\circledcirc$ object translation, we use Zero-shot Pix2Pix [45]; for tasks involving $\\circled{3}$ scene or style transfer, while maintaining object semantics, we utilize StyleDiffusion [63]; to validate spatial context preservation in $\\circledast$ non-rigid editing tasks (e.g., motion, pose), we consider MasaCtrl [6]. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Implementation Details. We perform the inference of different editing and inversion methods under consistent conditions. We use Stable Diffusion v1.5 as the base model, with 100 timesteps, an inversion guidance scale of 3.5, and a reverse guidance scale of 7.5. All experiments are conducted on a single Nvidia A100 GPU. Quantitative results are averaged over 10 random runs. Additional implementation details are provided in Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "Datasets. Experiments are conducted on the PIEBench dataset [25]. Recognizing the dataset\u2019s limited size and scenarios, we extend it by incorporating face images from FFHQ [27] and AFHQ [11], as well as indoor/outdoor common objects from MS-COCO [35]. This results in approximately 1600 images in total, across eight editing types (see Appendix D.1). ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. As the editing process involves altering both the foreground and background of the images, we follow Ju et al. in adopting three types of metrics: structure (DINO-I [7, 58, 51]), background preservation (PSNR, LPIPS [24, 72], MSE, SSIM [64]), and image-image, text-image consistency (CLIP score [48]). Detailed descriptions of each metric can be found in Appendix D.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/fb804d7725e5600ee1cc030487f9b51d08a5c6b414b83e020d02dbbf81dee01d.jpg", "table_caption": ["Table 1: Comparative table of diffusion noise schedules and their performance metrics. Bold values indicate the best results, while underlined values denote the second-best results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Qualitative and Quantitative Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Qualitative Comparison. As shown in Fig. 5, our Logistic Schedule demonstrates superior content preservation in each task. In tasks requiring fine-grained editing, such as attributes editing, the Logistic Schedule better preserves other attributes while making the desired changes. For tasks involving highlevel semantics, such as object translation and style/scene transfer, the Logistic Schedule maintains the overall structure and pose more effectively. In tasks that involve low-level semantics like color and texture, such as pose and attributes editing, the Logistic Schedule shows better fidelity and consistency. For tasks that require background preservation, such as object translation and pose editing, the Logistic Schedule excels in maintaining the background integrity. Overall, the Logistic Schedule ensures higher edit fidelity across various tasks, whereas the linear and cosine schedules sometimes fail to maintain the desired quality and consistency. ", "page_idx": 7}, {"type": "text", "text": "Quantitative Comparison. Table 1 shows that when employing the Logistic Schedule, all editing tasks exhibit improved retention of background and overall structure. While in some situations, the Logistic Schedule achieves slightly lower text alignment than the linear schedule, its preservation of background and structure is significantly superior. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate the effects of different configurations of the Logistic Schedule and the adaptability of the Logistic Schedule with various inversion techniques and diffusion models. More experiments on hyperparameters (e.g., guidance scale, input scale) can be found in Appendix E. The comparison with more design of the noise scheduler is provided in Appendix E.4. ", "page_idx": 7}, {"type": "text", "text": "5.3.1 Effects of Configuration of Logistic Schedule ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments with different configurations of the Logistic Schedule in Eq. 5, providing further evidence for the noise space analysis (Section 4.2). The parameters of the Logistic Schedule (Eq. 5)\u2014specifically the steepness $(k)$ and the midpoint $\\left(t_{0}\\right)$ \u2014play a crucial role in balancing content preservation and edit fidelity. Table 2 provides the quantitative results of varying $k$ and $t_{0}$ . ", "page_idx": 7}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/e458ef55d39ab84f85881ea965ff13884a6e4d5f186dff8515c5920dcdbfd197.jpg", "img_caption": ["Figure 6: Impact of $k$ on the logistic schedule. Left: change in $\\bar{\\alpha}_{t}$ and logSNR with different $k$ values. Right: the effect of $k$ on edited images. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/48fa6be77fabf92e9dd23ffd80dfa2c9bde3d7062e6a4fa22eb52835164096d7.jpg", "img_caption": ["Figure 7: Impact of $t_{0}$ on the logistic schedule. Left: change in $\\bar{\\alpha}_{t}$ and logSNR with different $t_{0}$ values. Right: each column represents edited results within three random seeds, under a specific $t_{0}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/a8b8ca035c2d3652a0475d38f97f6e89e668a99aa03a500ffbcf6cf48b79a779.jpg", "table_caption": ["Table 2: Quantitative results of the Logistic Schedule across various hyperparameter settings. The best method is indicated in bold, and the worst method is shown in purple. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Different $k$ : Changing the steepness of logSNR. When $k$ is larger, the logSNR values span a larger range (Fig.6, left). However, if the range is too large, excessive steepness of logSNR results in excessive loss of original image information in edited images (Fig.6, right). Interestingly, when $k$ is small, the logSNR resembles that of linear and cosine schedules, but the logistic schedule better preserves the original image content without altering the overall structure. This further supports Proposition 3.1 that the singularity in linear and cosine schedules tends to destroy original image information, causing undesired changes. ", "page_idx": 8}, {"type": "text", "text": "Different $t_{0}$ : Introducing shifts in logSNR. When $t_{0}$ is close to 0, the lower bound of logSNR is higher, affecting editability by reducing diversity and fidelity, as shown in Fig. 7. Conversely, when $t_{0}$ is close to $T$ , the original information is lost too quickly, degrading content preservation. ", "page_idx": 8}, {"type": "text", "text": "Balancing these parameters, we find that $k=0.015$ and $t_{0}=\\mathrm{int}(0.6T)$ strike the optimal trade-off between content preservation and edit fidelity, providing robust performance across various tasks. ", "page_idx": 8}, {"type": "text", "text": "5.3.2 Adapting Inversion Techniques and Diffusion Models ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To validate the adaptability and robustness of the Logistic Schedule, we first apply it with Plug-andPlay [59] using Stable Diffusion v1.5 [50] as the baseline. We then design experiments with two other diffusion models, Stable Diffusion v2.1 and Stable Diffusion XL [47], and incorporate three advanced inversion approaches: Null-Text Inversion [41], Negative Prompt Inversion (NPI) [40], and Direct Inversion [25]. As shown in Table 3, while more advanced stable diffusion models increase textual similarity, they degrade content preservation. Conversely, incorporating advanced inversion approaches improves both content preservation and edit fidelity. Furthermore, Table 4 presents the detailed comparison between the Logistic Schedule and the scaled linear schedule across different inversion techniques. ", "page_idx": 9}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/9ea5641c605f998e585836f2a0edc5085c2aec1fcb0dad442f78c2c698eadff8.jpg", "table_caption": ["Table 3: Comparative performance metrics with different base models and inversion techniques. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/6a593e07f906436e360dc9f1c0ed119b22ec96e8b557d0d35a703d04950fa8ab.jpg", "table_caption": ["Table 4: Comparison of inversion techniques with the scaled linear schedule and our proposed Logistic Schedule. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents the Logistic Schedule, a novel noise schedule that eliminates singularities and improves inversion stability for image editing. Our method enhances content preservation and edit fidelity without requiring additional retraining, making it a plug-and-play solution for existing workflows. Through in-depth analysis of the diffusion inversion process, we identify that current schedulers suffer from singularity issues at the start of inversion. The proposed Logistic Schedule provides a straightforward solution to this problem, offering superior performance and adaptability across various image editing tasks. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China (62403429, 62293551, 62377038, 62177038, 62277042). Project of China Knowledge Centre for Engineering Science and Technology, Project of Chinese academy of engineering \u201cThe Online and Offilne Mixed Educational Service System for \u2018The Belt and Road\u2019 Training in MOOC China\". \u201cLENOVO-XJTU\" Intelligent Industry Joint Laboratory Project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Transactions on Graphics (TOG), 42(4):1\u201311, 2023. [2] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18370\u201318380, 2023.   \n[3] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. Advances in Neural Information Processing Systems, 36, 2024.   \n[4] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402, 2023.   \n[6] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22560\u2013 22570, 2023.   \n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[8] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. [9] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and David J Fleet. A generalist framework for panoptic segmentation of images and videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 909\u2013919, 2023.   \n[10] Hansam Cho, Jonghyun Lee, Seoung Bum Kim, Tae-Hyun Oh, and Yonghyun Jeong. Noise map guidance: Inversion with spatial context for real image editing. arXiv preprint arXiv:2402.04625, 2024.   \n[11] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188\u20138197, 2020.   \n[12] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=AAWuCvzaVt.   \n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\cdot$ NAQvF08TcyG.   \n[14] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel \u00c1ngel Bautista, and Joshua M. Susskind. f-DM: A multi-stage diffusion model via progressive signal transformation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=iBdwKIsg4m.   \n[15] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Ding Liu, Qilong Zhangli, Anastasis Stathopoulos, Xiaoxiao He, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, and Dimitris N. Metaxas. Proxedit: Improving tuning-free real image editing with proximal guidance. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 4279\u20134289, 2023. URL https://api.semanticscholar. org/CorpusID:259287564.   \n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=_CDixzkzeyb.   \n[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview. net/forum?id $\\cdot$ qw8AKxfYbI.   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[19] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1\u201333, 2022.   \n[20] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 13213\u2013 13232. PMLR, 2023.   \n[21] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. arXiv preprint arXiv:2304.06140, 2023.   \n[22] Aapo Hyv\u00e4rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[23] Allan Jabri, David J. Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923, page 21. JMLR.org, 2023.   \n[24] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694\u2013711. Springer, 2016.   \n[25] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ FoMZ4ljhVw.   \n[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 26565\u201326577, 2022.   \n[27] Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1867\u20131874, 2014.   \n[28] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022.   \n[29] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[30] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[31] Peter Eris Kloeden, Eckhard Platen, and Henri Schurz. Numerical solution of SDE through computer experiments. Springer Science & Business Media, 2012.   \n[32] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=pd1P2eUBVfq.   \n[33] Haonan Lin. Dreamsalon: A staged diffusion framework for preserving identity-context in editable face generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8589\u20138598, 2024.   \n[34] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5404\u20135411, 2024.   \n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[36] Zhenzhen Liu, Jin Peng Zhou, Yufan Wang, and Kilian Q Weinberger. Unsupervised outof-distribution detection with diffusion inpainting. In International Conference on Machine Learning, pages 22528\u201322538. PMLR, 2023.   \n[37] Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Q Weinberger. Latent diffusion for language generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[38] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[39] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id $\\cdot$ aBsCjcPu_tE.   \n[40] Daiki Miyake, Akihiro Iohara, Yuriko Saito, and Toshiyuki TANAKA. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. ArXiv, abs/2305.16807, 2023. URL https://api.semanticscholar.org/CorpusID:258947366.   \n[41] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n[42] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:245335086.   \n[43] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[44] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15912\u201315921, 2023.   \n[45] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[46] Hamza Pehlivan, Yusuf Dalva, and Aysegul Dundar. Styleres: Transforming the residuals for real image editing with stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1828\u20131837, 2023.   \n[47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ di52zR8xgf.   \n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022.   \n[51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.   \n[52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n[53] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International conference on machine learning, pages 30105\u201330118. PMLR, 2023.   \n[54] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.1556.   \n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=St1giarCHLP.   \n[56] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[57] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ArXiv, abs/2011.13456, 2020. URL https://api.semanticscholar.org/CorpusID: 227209335.   \n[58] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10748\u201310757, 2022.   \n[59] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921\u20131930, 2023.   \n[60] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22532\u201322541, 2023.   \n[61] Jiahao Wang, Caixia Yan, Haonan Lin, and Weizhan Zhang. Oneactor: Consistent character generation via cluster-conditioned guidance. arXiv preprint arXiv:2404.10267, 2024.   \n[62] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18359\u201318369, 2023.   \n[63] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7677\u20137689, 2023.   \n[64] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4): 600\u2013612, 2004.   \n[65] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, and Nenghai Yu. E2style: Improve the efficiency and effectiveness of stylegan inversion. IEEE Transactions on Image Processing, 31:3267\u20133280, 2022.   \n[66] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1900\u20131910, 2023.   \n[67] Ruihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[68] Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. Disdiff: Unsupervised disentanglement of diffusion probabilistic models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\equiv$ 3ofe0lpwQP.   \n[69] Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, and Fernando De la Torre. Iti-gen: Inclusive text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3969\u20133980, 2023.   \n[70] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024.   \n[71] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gDDIM: Generalized denoising diffusion implicit models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $=$ 1hKE9qjvz-.   \n[72] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[73] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-world image variation by aligning diffusion inversion chain. Advances in Neural Information Processing Systems, 36, 2024.   \n[74] Peiye Zhuang, Oluwasanmi O Koyejo, and Alex Schwing. Enjoy your editing: Controllable $\\{{\\mathrm{gan}}\\}{\\mathrm{s}}$ for image editing via latent space navigation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=HOFxeCutxZR. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Neural ODEs of DDIM 17 ", "page_idx": 15}, {"type": "text", "text": "A.1 Preliminaries: Score-Based Generative Modeling with SDEs . . 17   \nA.2 Rewrite the DDIM Process as ODEs . . 18 ", "page_idx": 15}, {"type": "text", "text": "B Proofs 20 ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof Preliminaries . 20   \nB.2 Derivation of Singularities w.r.t. Linear and Cosine Schedules 21   \nB.3 Derivatives of the Logistic Schedule . 24 ", "page_idx": 15}, {"type": "text", "text": "C Related Works 24 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D Experimental Settings 25 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Introduction of Editing Types . . 25   \nD.2 Implementation Details . . 26   \nD.3 Evaluation Metrics 26 ", "page_idx": 15}, {"type": "text", "text": "E Experimental Results 27 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Quantitative Comparison Across Editing Types 27   \nE.2 Qualitative Comparison Across Editing Types . . . . 28   \nE.3 Broader Application: Training-Based Methods . . 29   \nE.4 Comparison With Other Noise Schedulers . . . 29   \nE.5 Reconstruction Ability of Different Noise Schedule . . 30   \nE.6 Effects of Guidance Scale . . . . . . . . . . 32   \nE.7 Effects of Input Scale . . . . 32 ", "page_idx": 15}, {"type": "text", "text": "F Limitations and Future Works 33 ", "page_idx": 15}, {"type": "text", "text": "G Broader Impacts 35 ", "page_idx": 15}, {"type": "text", "text": "H Ethics Statement 35 ", "page_idx": 15}, {"type": "text", "text": "A Neural ODEs of DDIM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To support the analysis in Section 3 on the failure of DDIM in inversion, we present the following connections to neural ODEs and DDIM. ", "page_idx": 16}, {"type": "text", "text": "A.1 Preliminaries: Score-Based Generative Modeling with SDEs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We beginning with the process of constructing a diffusion process using SDEs, extending DDPM to infinite noise scales for evolving data distributions from initial to prior distributions. ", "page_idx": 16}, {"type": "text", "text": "Perturbing Process with SDEs. DDPM [18] sets noise scales so that $\\mathbf{X}T$ approximates ${\\cal N}(0,{\\bf I})$ , leveraging multiple noise scales for success. Song et al. extended this to infinite noise scales, evolving the data distribution via an SDE. The goal is to construct a diffusion process $\\{\\mathbf{x}(t)\\}_{t=0}^{T}$ , where (data distribution) and $\\mathbf{x}(T)\\sim p_{T}$ (prior distribution). The process is modeled by the SDE: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}=\\mathbf{f}(\\mathbf{x},t)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where w is the standard Wiener process with time flowing backwards from $T$ to $0,\\mathbf{f}(\\cdot,t)$ is the drift coefficient, and $g(\\cdot)$ is the diffusion coefficient. ", "page_idx": 16}, {"type": "text", "text": "Generating Samples by Reversing the SDE. Starting from $\\mathbf{x}(T)\\sim p_{T}$ and reversing the process, we can obtain $\\mathbf{x}(0)\\sim p_{0}$ , given by the reverse-time SDE: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}=\\left[\\mathbf{f}(\\mathbf{x},t)-g(t)^{2}\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})\\right]\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The score $\\nabla_{\\mathbf{x}}\\log{p_{t}(\\mathbf{x})}$ can be estimated by training a score-based model on samples using score matching [22, 56]. ", "page_idx": 16}, {"type": "text", "text": "Solving Reverse-Time SDE: Probability Flow ODE. Numerical solvers approximate trajectories from SDEs. General-purpose methods like Euler-Maruyama and stochastic Runge-Kutta [31] discretize the stochastic dynamics. In addition to these, score-based models enable solving the reverse-time SDE via a deterministic process, known as the probability flow $O D E$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{d}\\mathbf{x}=\\left[\\mathbf{f}(\\mathbf{x},t)-\\frac{1}{2}g(t)^{2}\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})\\right]\\mathbf{d}t\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This ODE is determined from the SDE once scores are known. When the score function is approximated by a neural network, it exemplifies a neural ODE [8]. ", "page_idx": 16}, {"type": "text", "text": "From Score-Based Models to DDPM: VE, VP SDEs The noise perturbations in SMLD [56] and DDPM [18] are discretizations of two SDEs: Variance Exploding $(V E)$ SDE and Variance Preserving $(V P)$ SDE. ", "page_idx": 16}, {"type": "text", "text": "For SMLD with $N$ noise scales, each perturbation kernel $p_{\\sigma_{i}}(\\mathbf{x}\\mid\\mathbf{x}_{0})$ corresponds to the distribution of $\\mathbf{X}_{i}$ in this Markov chain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i}=\\mathbf{x}_{i-1}+\\sqrt{\\sigma_{i}^{2}-\\sigma_{i-1}^{2}}\\mathbf{z}_{i-1},\\quad i=1,\\cdots,N,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ${\\bf z}_{i-1}\\sim{\\cal N}(0,{\\bf I})$ and $\\sigma_{0}=0$ . As ${\\cal{N}}\\rightarrow\\infty,\\,\\left\\{\\sigma_{i}\\right\\}_{i=1}^{N}$ becomes $\\boldsymbol{\\sigma}(t)$ , $\\mathbf{z}_{i}$ becomes ${\\bf z}(t)$ , and the Markov chain $\\left\\{{\\bf x}_{i}\\right\\}_{i=1}^{N}$ becomes a continuous stochastic process $\\{\\mathbf{x}(t)\\}_{t=0}^{1}$ , given by the SDE: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}=\\sqrt{\\frac{\\mathrm{d}[\\sigma^{2}(t)]}{\\mathrm{d}t}}\\mathrm{d}\\mathbf{w}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For DDPM, the perturbation kernels $\\{p_{\\alpha_{i}}(\\mathbf{x}\\mid\\mathbf{x}_{0})\\}_{i=1}^{N}$ follow this Markov chain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i}=\\sqrt{1-\\beta_{i}}\\mathbf{x}_{i-1}+\\sqrt{\\beta_{i}}\\mathbf{z}_{i-1},\\quad i=1,\\cdots,N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $N\\to\\infty$ , this converges to the SDE: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}=-\\frac{1}{2}\\beta(t)\\mathbf{x}\\mathrm{d}t+\\sqrt{\\beta(t)}\\mathrm{d}\\mathbf{w}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, noise perturbations in SMLD and DDPM correspond to the SDEs 10 and 12, respectively. Notably, the SDE 10 results in an exploding variance as $t\\to\\infty$ , while the SDE 12 maintains a fixed variance of one, demonstrating the superiority of VP SDE for stable variance preservation. ", "page_idx": 16}, {"type": "text", "text": "A.2 Rewrite the DDIM Process as ODEs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "DDIM\u2019s Local Linearization Assumption: DDIM inversion for real images is unstable due to its reliance on a local linearization assumption at each step, leading to error accumulation and content loss. DDIM assumes that the denoising process in Eq. 2 is roughly invertible: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}^{*}=\\frac{\\mathbf{x}_{t-\\Delta t}^{*}-b_{t}\\epsilon(\\mathbf{x}_{t}^{*},t)}{a_{t}}\\approx\\frac{\\mathbf{x}_{t-\\Delta t}^{*}-b_{t}\\epsilon(\\mathbf{x}_{t-\\Delta t}^{*},t)}{a_{t}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $a_{t}\\ =\\ \\sqrt{\\alpha_{t-\\Delta t}/\\alpha_{t}}$ and $b_{t}\\ =\\ -\\sqrt{\\alpha_{t-\\Delta t}(1-\\alpha_{t})/\\alpha_{t}}\\,+\\sqrt{1-\\alpha_{t-\\Delta t}}$ . This assumes $\\epsilon(\\mathbf{x}_{t}^{*},t)\\;\\approx\\;$ $\\epsilon(\\mathbf{x}_{t-\\Delta t}^{*},t)$ , and inversion accuracy depends on this assumption. Moreover, estimating the \u201cpredicted $\\mathbf{X}_{0}^{\\\"}$ at the beginning $(t=1)$ ) lacks a simple expression for the posterior mean conditioned on $\\mathbf{X}_{t}$ . This deviating from the linearization assumption causes the interpolation to break down from the start, resulting in server error accumulation problem. ", "page_idx": 17}, {"type": "text", "text": "Relevance to Neural ODEs: Under this assumption, the DDIM iteration process (Eq. 2) can be rewritten in a format similar to Euler integration for solving ODEs: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\pmb{x}_{t-\\Delta t}}{\\sqrt{\\alpha_{t-\\Delta t}}}=\\frac{\\pmb{x}_{t}}{\\sqrt{\\alpha_{t}}}+\\left(\\sqrt{\\frac{1-\\alpha_{t-\\Delta t}}{\\alpha_{t-\\Delta t}}}-\\sqrt{\\frac{1-\\alpha_{t}}{\\alpha_{t}}}\\right)\\epsilon_{\\theta}^{(t)}\\left(\\pmb{x}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Reparameterizing $({\\sqrt{1-\\alpha/{\\sqrt{\\alpha}}}})$ with $\\sigma$ and $(\\mathbf{x}/{\\sqrt{\\alpha}})$ with $\\bar{\\bf x}$ , in the continuous case, $\\sigma$ and $\\mathbf{X}$ are functions of $t$ , with $\\sigma:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ continuous and increasing, $\\sigma(0)=0$ . Eq. 13 can be seen as an Euler method over the ODE: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\bar{\\mathbf{x}}(t)=\\epsilon_{\\theta}^{(t)}\\left(\\frac{\\bar{\\mathbf{x}}(t)}{\\sqrt{\\sigma^{2}+1}}\\right)\\mathrm{d}\\sigma(t),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which corresponds to the Eq. 10 of probability flow ODE. This suggests that with enough discretization steps and the optimal model $\\epsilon_{\\theta}^{(t)}$ , the generation process Eq. 2 can be reversed, encoding $\\mathbf{X}_{0}$ to $\\mathbf{X}T$ and simulating the reverse of the ODE in Eq. 14. ", "page_idx": 17}, {"type": "text", "text": "Theorem A.1 (DDIM ODEs). While the ODEs are equivalent, the sampling procedures differ significantly. The Euler method for the probability flow ODE updates: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{X}_{t-\\Delta t}}{\\sqrt{\\alpha_{t-\\Delta t}}}=\\frac{\\mathbf{x}_{t}}{\\sqrt{\\alpha_{t}}}+\\frac{1}{2}\\left(\\frac{1-\\alpha_{t-\\Delta t}}{\\alpha_{t-\\Delta t}}-\\frac{1-\\alpha_{t}}{\\alpha_{t}}\\right)\\cdot\\sqrt{\\frac{\\alpha_{t}}{1-\\alpha_{t}}}\\cdot\\epsilon_{\\theta}^{(t)}(\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is equivalent to Eq. 13 if $\\alpha_{t}$ and $\\alpha_{t-\\Delta t}$ are close enough. However, achieving this closeness is challenging with fewer time steps, and an inferior model can exacerbate the errors from this assumption. Moreover, the Variance Exploding SDE (VE SDE) has inherent flaws compared to the Variance Preserving SDE (VP SDE). VE SDEs tend to increase variance exponentially, leading to instability and less accurate representations, whereas VP SDEs maintain a stable variance, ensuring a more consistent and reliable modeling process. ", "page_idx": 17}, {"type": "text", "text": "Modeling with d\ud835\udc61in Euler steps, as done in the probability flow ODE, ensures that the step size directly correlates with the temporal evolution, maintaining the integrity of the stochastic process and providing a more faithful representation of the underlying data distribution over time. [55] state that the ODE of DDIM is a special case of the probability flow ODE (continuous-time analog of DDPM). ", "page_idx": 17}, {"type": "text", "text": "Proof. We consider $t$ as a continuous, independent \u201ctime\" variable and $\\mathbf{X}$ and $\\alpha$ as functions of $t$ Let\u2019s reparameterize DDIM and VE-SDE using $\\bar{\\mathbf{X}}$ and $\\sigma$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}(t)=\\bar{\\mathbf{x}}(0)+\\sigma(t)\\epsilon,\\quad\\epsilon\\sim N(0,I),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $t\\in[0,\\infty)$ and a continuous function $\\sigma:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ where $\\sigma(0)=0$ . ", "page_idx": 17}, {"type": "text", "text": "Define $\\alpha(t)$ and ${\\bf x}(t)$ for DDIM as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}(t)=\\frac{\\mathbf{x}(t)}{\\sqrt{\\alpha(t)}},\\quad\\sigma(t)=\\sqrt{\\frac{1-\\alpha(t)}{\\alpha(t)}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{x}(t)=\\frac{\\bar{\\mathbf{x}}(t)}{\\sqrt{\\sigma^{2}(t)+1}},\\quad\\alpha(t)=\\frac{1}{1+\\sigma^{2}(t)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Equation 1, noting $\\alpha(0)=1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{{\\bf x}(t)}{\\sqrt{\\alpha(t)}}=\\frac{{\\bf x}(0)}{\\sqrt{\\alpha(0)}}+\\sqrt{\\frac{1-\\alpha(t)}{\\alpha(t)}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which reparameterizes to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}(t)=\\bar{\\mathbf{x}}(0)+\\sigma(t)\\epsilon.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "ODE form for DDIM: Simplify Equation 13 to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}(t-\\Delta t)=\\bar{\\mathbf{x}}(t)+(\\sigma(t-\\Delta t)-\\sigma(t))\\cdot\\boldsymbol{\\epsilon}_{\\theta}^{(t)}(x(t)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Dividing by $-\\Delta t$ and taking $\\Delta t\\to0$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d\\bar{\\bf x}(t)}{d t}=\\frac{d\\sigma(t)}{d t}\\epsilon_{\\theta}^{(t)}\\left(\\frac{\\bar{\\bf x}(t)}{\\sqrt{\\sigma^{2}(t)+1}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "matching Equation 14. ", "page_idx": 18}, {"type": "text", "text": "ODE form for VE-SDE: Define $p_{t}(\\bar{\\bf x})$ as the data distribution perturbed with $\\sigma^{2}(t)$ Gaussian noise. The probability flow for VE-SDE is given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\nd\\bar{\\bf x}=-\\frac{1}{2}g(t)^{2}\\nabla_{\\bar{\\bf x}}\\log p_{t}(\\bar{\\bf x})d t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{g(t)=\\sqrt{\\frac{d\\sigma^{2}(t)}{d t}}}\\end{array}$ \ufe03\ud835\udc51\ud835\udf0e\ud835\udc512\ud835\udc61(\ud835\udc61). The perturbed score function \u2207\u00afx log \ud835\udc5d\ud835\udc61(x\u00af) minimizes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\bar{\\mathbf{x}}}\\log p_{t}=\\arg\\operatorname*{min}_{g_{t}}\\mathbb{E}_{x(0)\\sim q(x),\\epsilon\\sim N(0,I)}\\big[||g_{t}(\\bar{\\mathbf{x}})+\\epsilon/\\sigma(t)||_{2}^{2}\\big],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\bar{\\mathbf{x}}=\\bar{\\mathbf{x}}(t)+\\sigma(t)\\epsilon$ . ", "page_idx": 18}, {"type": "text", "text": "The equivalence between ${\\bf x}(t)$ and $\\bar{\\bf x}(t)$ gives: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\bar{\\mathbf{x}}}\\log{p_{t}(\\bar{\\mathbf{x}})}=-\\frac{\\epsilon_{\\theta}^{(t)}\\left(\\frac{\\bar{\\mathbf{x}}(t)}{\\sqrt{\\sigma^{2}(t)+1}}\\right)}{\\sigma(t)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using Equation A.2, and the definition of $g\\left(t\\right)$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d\\bar{\\mathbf{x}}(t)}{d t}=\\frac{1}{2}\\frac{d\\sigma^{2}(t)}{d t}\\frac{\\epsilon_{\\theta}^{(t)}\\left(\\frac{\\bar{\\mathbf{x}}(t)}{\\sqrt{\\sigma^{2}(t)+1}}\\right)}{\\sigma(t)}d t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "rearranging terms: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d\\bar{\\bf x}(t)}{d t}=\\frac{d\\sigma(t)}{d t}\\epsilon_{\\theta}^{(t)}\\left(\\frac{\\bar{\\bf x}(t)}{\\sqrt{\\sigma^{2}(t)+1}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which matches Equation 16. Both initial conditions are $\\bar{\\bf x}(T)\\,\\sim\\,N(0,\\sigma^{2}(T)I)$ , showing that the ODEs are identical. \u25a1 ", "page_idx": 18}, {"type": "text", "text": "However, the above proof is based on several assumptions as follows: ", "page_idx": 18}, {"type": "text", "text": "1. Equivalence Between ${\\bf x}(t)$ and $\\bar{\\bf x}(t)$ : The bijective mapping between the variables ${\\bf x}(t)$ and $\\bar{\\bf x}(t)$ is crucial for transforming the DDIM formulation into the VE-SDE framework. If this equivalence does not hold perfectly, the transformation could introduce errors. Small discrepancies can accumulate over time, leading to significant deviations in the modeling process, resulting in unreliable outcomes. ", "page_idx": 18}, {"type": "text", "text": "2. Gaussian and Constant Noise $\\epsilon$ : The noise $\\epsilon$ is assumed to be Gaussian $N(0,I)$ and constant throughout the process, which simplifies the mathematical formulation and integration. However, in real-world scenarios, the noise might not be perfectly Gaussian or constant. Variations in the noise can affect the accuracy of the model\u2019s predictions, leading to inconsistencies and unreliable results. ", "page_idx": 18}, {"type": "text", "text": "3. Continuity and Differentiability of $\\alpha(t)$ and $\\sigma(t)$ : The functions $\\alpha(t)$ and $\\sigma(t)$ are assumed to be continuous and differentiable. This ensures smooth transitions and allows for the derivation of the differential equations. If $\\alpha(t)$ or $\\boldsymbol{\\sigma}(t)$ are not continuous or differentiable, the resulting differential equations may not accurately represent the underlying processes. This can lead to instability and errors in the model\u2019s behavior. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "4. Optimal Model $\\epsilon_{\\theta}^{(t)}$ : The model $\\epsilon_{\\theta}^{(t)}$ is assumed to be optimal, meaning it perfectly minimizes the given loss function. In practice, achieving an optimal model is challenging. Suboptimal models can lead to inaccuracies in the predictions, and the error can propagate, reducing the reliability of the entire process. ", "page_idx": 19}, {"type": "text", "text": "5. Closeness of $\\alpha_{t}$ and $\\alpha_{t-\\Delta t}$ : It is assumed that $\\alpha_{t}$ and $\\alpha_{t-\\Delta t}$ are close enough, which is necessary for the equivalence between the DDIM and VE-SDE formulations to hold. With fewer time steps, this assumption may not hold, leading to significant errors. Additionally, if the model is inferior, the errors arising from this assumption can be magnified, resulting in an unreliable process. ", "page_idx": 19}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we first provide the detailed expressions of $\\mathbf{X}_{t}$ with respect to different noise schedules.   \nThen we provide the proof of the singularities problem in Proposition 3.1. ", "page_idx": 19}, {"type": "text", "text": "By reparameterizing: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{t}=1-\\beta_{t},\\qquad\\bar{\\alpha_{t}}=\\prod_{i=1}^{t}\\alpha_{i},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the forward process of DDPM can be expressed as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Apply the chain rule to $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ , we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=\\frac{1}{2}\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\mathbf{x}_{0}\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}+\\frac{1}{2}\\frac{-1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.1 Proof Preliminaries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1.1 Scaled Linear Schedule ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The linear beta schedule is defined by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta_{t}=\\beta_{\\mathrm{start}}+t\\cdot\\frac{\\beta_{\\mathrm{end}}-\\beta_{\\mathrm{start}}}{T-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{start}}={\\frac{0.0001\\cdot1000}{T}}={\\frac{0.1}{T}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{end}}={\\frac{0.02\\cdot1000}{T}}={\\frac{20}{T}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta_{t}={\\frac{0.1}{T}}+t\\cdot{\\frac{{\\frac{20}{T}}-{\\frac{0.1}{T}}}{T-1}}={\\frac{0.1}{T}}+t\\cdot{\\frac{19.9}{T(T-1)}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In general form, the expression for $\\beta_{t}$ is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta_{t}=\\frac{0.1}{T}+\\frac{19.9\\cdot t}{T(T-1)},\\quad t=0,1,2,\\ldots,T-1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Incorporating Eq. 17, the $\\bar{\\alpha}_{t}$ of scaled linear schedule is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\bar{\\alpha}}_{t}=\\prod_{i=1}^{t}\\left(1-{\\frac{0.1}{T}}-{\\frac{19.9\\cdot i}{T(T-1)}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.1.2 Cosine Schedule ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The cosine schedule is proposed in the iDDPM [43], where the definition of the schedule is given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}=\\frac{f(t)}{f(0)},\\quad f(t)=\\cos\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s$ is a small offset to prevent $\\beta_{t}$ from being too small near $t=0$ . Nichol and Dhariwal chose this setting since they found that having tiny amounts of noise at the beginning of the process\u221a made it hard for the network to predict accurately enough. Specifically, $s$ is set as 0.008 such that $\\sqrt{\\beta}_{0}$ was slightly smaller than the pixel bin size 1/127.5. ", "page_idx": 20}, {"type": "text", "text": "Plugging $f(t)$ into the expression, we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}=\\frac{\\cos^{2}(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2})}{\\cos^{2}(\\frac{s}{1+s}\\cdot\\frac{\\pi}{2})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.1.3 Sigmoid Schedule ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The sigmoid schedule is introduced in Jabri et al., which is designed for scalable data generation, especially for high-dimensional data, without addressing the challenges of DDIM inversion. The formulation of the sigmoid schedule can be presented as below: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}_{t}=\\frac{-\\left(\\frac{t(e-s)+s}{r}\\right)\\cdot\\mathrm{sigmoid}()+\\nu_{e}}{\\nu_{e}-\\nu_{s}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s$ and $e$ are the start and end of the sigmoid function\u2019s range, and $\\nu_{s}=(s/r)\\cdot\\mathrm{sigmoid}(),\\;\\nu_{\\cdot}$ $\\nu_{e}=$ $(e/r)$ \u00b7 sigmoid(). ", "page_idx": 20}, {"type": "text", "text": "B.1.4 Logistic Schedule ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall the expression of the logistic schedule in Eq. 5: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}=\\mathrm{Normalized}\\left(\\frac{1}{1+e^{-k(t-t_{0})}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $k$ and $t$ are hyperparameters that control the steepness and midpoint of the logistic function, respectively. ", "page_idx": 20}, {"type": "text", "text": "B.2 Derivation of Singularities w.r.t. Linear and Cosine Schedules ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall the Proposition 3.1: ", "page_idx": 20}, {"type": "text", "text": "Proposition B.1 (Singularity in Inversion Process). During the inversion process, there exists a singularity at $t=0$ for both the scaled linear and cosine schedule: ", "page_idx": 20}, {"type": "equation", "text": "$$\nW h e n\\left.t=0,\\,\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}\\right|_{t\\rightarrow0}=\\frac{0}{0}\\cdot s i g n(\\epsilon)=\\infty\\cdot s i g n(\\epsilon).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we provide the derivatives for scaled linear and cosine in order, to support Proposition 3.1. ", "page_idx": 20}, {"type": "text", "text": "B.2.1 Scaled Linear Schedule ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For dx\ud835\udc61/d\ud835\udc61, where x\ud835\udc61 =  \ud835\udefc\u00af\ud835\udc61x0 +  1 \u2212\ud835\udefc\u00af\ud835\udc61\ud835\udf16, cannot use \ud835\udefc\u00af\ud835\udc61 =  \ud835\udc56\ud835\udc61=1 1 \u22120\ud835\udc47.1\u2212\ud835\udc471(9\ud835\udc47.9\u2212\u00b71\ud835\udc56) to find the feasible derivatives. Since the expression $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\left(1-\\frac{0.1}{T}-\\frac{19.9\\cdot i}{T\\left(T-1\\right)}\\right)}\\end{array}$ represents a product of terms, which makes it difficult to differentiate directly. Taking the derivative of a product involves applying the product rule multiple times, which becomes impractical as the number of terms increases. Instead, we can use logarithms to simplify the expression into a sum, which is easier to handle analytically. This approach allows us to find an analytic approximation for $\\bar{\\alpha}_{t}$ and subsequently for $\\mathrm{d}\\ensuremath{\\mathbf{x}}_{t}/\\mathrm{d}t$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The logarithm of the product in Eq. 19 reads: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log(\\bar{\\alpha}_{t})=\\sum_{i=1}^{t}\\log\\left(1-\\frac{0.1}{T}-\\frac{19.9\\cdot i}{T(T-1)}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given the small terms $\\frac{0.1}{T}$ and $\\frac{19.9\\cdot i}{T(T{-}1)}$ , we can consider using a first-order Taylor expansion for the logarithm around 1. The Taylor expansion of $\\log(1-x)$ around $x=0$ is $\\log(1-x)\\approx-x$ for small $x$ . Substituting, we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log(\\bar{\\alpha}_{t})\\approx-\\sum_{i=1}^{t}\\left(\\frac{0.1}{T}+\\frac{19.9\\cdot i}{T(T-1)}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plugging $\\begin{array}{r}{\\sum_{i=1}^{t}i=\\frac{t\\left(t+1\\right)}{2}}\\end{array}$ into the expression, we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log({\\bar{\\alpha}}_{t})\\approx-\\sum_{i=1}^{t}{\\frac{0.1}{T}}-\\sum_{i=1}^{t}{\\frac{19.9\\cdot i}{T(T-1)}}=-{\\frac{0.1t}{T}}-{\\frac{19.9}{T(T-1)}}\\cdot{\\frac{t(t+1)}{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(t)=-{\\frac{0.1t}{T}}-{\\frac{19.9t(t+1)}{2T(T-1)}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf^{\\prime}(t)=-\\frac{0.1}{T}-\\frac{19.9}{2T(T-1)}\\left(2t+1\\right)=-\\frac{0.1}{T}-\\frac{19.9(2t+1)}{2T(T-1)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plug $\\bar{\\alpha}_{t}=e^{f(t)}$ into the chain rule of $\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}$ and substituting $f(t)$ and $f^{\\prime}(t)$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(e^{f(t)}\\right)=e^{f(t)}\\cdot f^{\\prime}(t)}\\\\ {\\displaystyle\\qquad=\\exp\\left(-\\frac{0.1t}{T}-\\frac{19.9t(t+1)}{2T(T-1)}\\right)\\cdot\\left(-\\frac{0.1}{T}-\\frac{19.9(2t+1)}{2T(T-1)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Substituting $\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}$ back into the expression for dx\ud835\udc61 in Eq. 18: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=\\frac{\\left(\\epsilon e^{-\\frac{t\\left(0.1T+9.95t+9.85\\right)}{T\\left(T-1\\right)}}-x_{0}\\sqrt{1-e^{-\\frac{t\\left(0.1T+9.95t+0.85\\right)}{T\\left(T-1\\right)}}}\\sqrt{e^{-\\frac{t\\left(0.1T+9.95t+9.85\\right)}{T\\left(T\\left(T\\right)}\\right)}}\\right)\\left(0.1T+19.9t+9.85\\right)}{2T\\sqrt{1-e^{-\\frac{t\\left(0.1T+9.95t+9.85\\right)}{T\\left(T-1\\right)}}}\\left(T-1\\right)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{When}\\;t=0,\\;\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}\\Bigg|_{t\\rightarrow0}=\\frac{\\tilde{\\infty}\\epsilon(0.1T+9.85)}{T(T-1)}=\\tilde{\\infty}\\cdot\\mathrm{sign}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\tilde{\\infty}$ denotes an unspecified directed infinity in the complex plane. ", "page_idx": 21}, {"type": "text", "text": "B.2.2 Cosine Schedule ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. Given the expression: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}=\\frac{\\cos^{2}\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)}{\\cos^{2}\\left(\\frac{s}{1+s}\\cdot\\frac{\\pi}{2}\\right)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By differentiating $\\bar{\\alpha}_{t}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}=\\frac{\\left(2\\cos\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\\left(-\\sin\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\\right)\\cdot\\frac{\\pi}{2T(1+s)}\\right)\\cos^{2}\\left(\\frac{s}{1+s}\\cdot\\frac{\\pi}{2}\\right)}{\\cos^{4}\\left(\\frac{s}{1+s}\\cdot\\frac{\\pi}{2}\\right)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Simplifying the expression: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}=\\frac{2\\cos\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\\left(-\\sin\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\\right)\\cdot\\frac{\\pi}{2T(1+s)}}{\\cos^{2}\\left(\\frac{s}{1+s}\\cdot\\frac{\\pi}{2}\\right)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting $\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}$ back into the expression for dx\ud835\udc61 in Eq. 18: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{i}\\mathbf{x}_{t}}{\\mathrm{d}t}=\\frac{1}{2}\\left(\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\mathbf{x}_{0}-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon\\right)\\cdot2\\cos\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\\left(-\\sin\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\\right)\\cdot\\frac{\\pi}{2T(1+s)}\\cdot\\frac{1}{\\cos^{2}\\left(\\frac{s}{1+s}\\cdot\\frac{\\pi}{2}\\right)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Considering the special cases: ", "page_idx": 22}, {"type": "text", "text": "\u2022 When $t=0$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{0}}{\\mathrm{d}t}=1.0(\\infty\\cdot\\epsilon-0.5\\pi\\mathbf{x}_{0})\\cdot\\frac{\\tan\\left(\\pi\\frac{s}{2(1+s)}\\right)}{T(1+s)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 When $t=T$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}\\mathbf{x}_{T}}{\\mathrm{d}t}}=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B.2.3 Sigmoid Schedule ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Given definition of $x_{t}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{t}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To express the coefficient of $\\epsilon$ in the derivative of $\\mathbf{X}_{t}$ with respect to $t$ , we start with the expression: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=a(t)\\cdot\\epsilon+b(t)\\cdot\\mathbf{x}_{0},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\epsilon$ represents the noise, and $\\mathbf{X}_{0}$ is the original image. Given that $\\epsilon$ and $\\mathbf{X}_{0}$ are constants with respect to $t$ , the differentiation yields: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbf{x}_{t}=\\epsilon\\cdot\\frac{\\mathrm{d}}{\\mathrm{d}t}a(t)+\\mathbf{x}_{0}\\cdot\\frac{\\mathrm{d}}{\\mathrm{d}t}b(t).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recall the definition of $\\bar{\\alpha}_{t}$ in sigmoid schedule in Eq. 21, we put it in the expression of $a(t)$ , then the coefficient of $\\epsilon$ in Eq. 24 can be expressed as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}a(t)=\\frac{(e-s)e^{-s-t(e-s)}}{2\\tau}\\left(\\sqrt{1-\\left(\\frac{\\frac{1}{1+e^{\\frac{s+t(e-s)}{\\tau}}}-\\frac{1}{1+e^{\\frac{t(e-s)}{\\tau}}}}{\\frac{1}{1+e^{\\frac{s}{\\tau}}}-\\frac{1}{1+e^{\\frac{t(e-s)}{\\tau}}}}\\right)^{2}\\left(e^{-s-t(e-s)}+1\\right)^{2}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $t\\rightarrow0$ , we have the derivative diverges to infinity: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to0}{\\frac{\\mathrm{d}}{\\mathrm{d}t}}a(t)\\to\\infty,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B.3 Derivatives of the Logistic Schedule ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. The $\\bar{\\alpha}_{t}$ given by the Logistic Schedule is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{t}=\\frac{1}{1+e^{-k(t-t_{0})}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By differentiating $\\bar{\\alpha}_{t}$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\bar{\\alpha}_{t}}{\\mathrm{d}t}=\\frac{d}{d t}\\left(\\frac{1}{1+e^{-k(t-t_{0})}}\\right)=\\frac{k e^{-k(t-t_{0})}}{\\left(1+e^{-k(t-t_{0})}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting d\ud835\udc61 d \ud835\udefc\u00af\ud835\udc61 back into the expression for dx\ud835\udc61 in Eq. 18: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=\\frac{1}{2}\\left(\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\mathbf{x}_{0}-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon\\right)\\cdot\\frac{k e^{-k\\left(t-t_{0}\\right)}}{\\left(1+e^{-k\\left(t-t_{0}\\right)}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substitute $\\bar{\\alpha}_{t}$ back into the expression: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=\\frac{k e^{-k\\left(t-t_{0}\\right)}}{2\\left(1+e^{-k\\left(t-t_{0}\\right)}\\right)^{2}}\\left(\\frac{\\mathbf{x}_{0}}{\\sqrt{\\frac{1}{1+e^{-k\\left(t-t_{0}\\right)}}}}-\\frac{\\epsilon}{\\sqrt{1-\\frac{1}{1+e^{-k\\left(t-t_{0}\\right)}}}}\\right)}\\\\ {\\displaystyle=\\frac{k e^{-k\\left(t-t_{0}\\right)}}{2\\left(1+e^{-k\\left(t-t_{0}\\right)}\\right)^{2}}\\left(\\mathbf{x}_{0}\\sqrt{1+e^{-k\\left(t-t_{0}\\right)}}-\\epsilon\\sqrt{\\frac{e^{-k\\left(t-t_{0}\\right)}}{1+e^{-k\\left(t-t_{0}\\right)}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $t\\rightarrow0$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}\\Bigg\\vert_{t\\rightarrow0}=\\frac{0.5\\epsilon k\\left(\\frac{1}{e^{k t_{0}}+1.0}\\right)^{0.5}e^{k t_{0}}}{e^{k t_{0}}+1.0}-\\frac{0.5k x_{0}e^{k t_{0}}}{\\left(1.0-\\frac{1}{e^{k t_{0}}+1.0}\\right)^{0.5}\\left(e^{k t_{0}}+1.0\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substitute the setting $k=0.015,t_{0}=\\mathrm{int}(0.3T)$ and $T=100$ into the expression, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}\\Bigg|_{t\\rightarrow0}=1.486e^{-3}\\epsilon-1.318e^{-3}\\mathbf{x}_{0}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C Related Works ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Text-guided Image Editing. Text-guided image editing significantly enhances the controllability and accessibility of visual manipulation by following human commands. With the advancement of large-scale training, diffusion models [49, 52, 50] have shown remarkable capabilities in transforming images based on human-given instructions [13, 62, 70]. Some approaches train end-to-end models for image editing [5, 28], while others propose training-free methods that merge information from source and target images using masks for controllability [39, 1]. A breakthrough by Hertz et al. leveraged the attention maps within the UNet to eliminate the need for manual masks, achieving promising results. This insight has been adopted and improved upon across multiple tasks by several works [59, 5, 6, 15, 73]. However, most current image editing approaches still rely on predefined noise schedules without evaluating their effectiveness. In this paper, we propose a newly designed noise schedule for image editing that provides high content preservation and enhanced editability. ", "page_idx": 23}, {"type": "text", "text": "Inversion-based Image Editing. Editing real images requires first inverting the image back to the latent space of the diffusion model due to the lack of a native latent space for these real images [46, 53, 65, 74], a process called image inversion. To address this, DDIM [55] introduced a deterministic sampling process for diffusion, allowing the inversion of the sampling process to recover the latent noise. However, the invertible properties of DDIM rely on its linearization assumption, which introduces deviations that drive the inverted latent away from its true distribution. As the Markov properties of the diffusion process come into play, these deviations gradually enlarge, resulting in suboptimal inverted latents that degrade reconstruction and editing quality. Recently, several inversion-based methods have been proposed to mitigate this issue [41, 60, 40, 25]. These methods attempt to correct errors on the reconstruction path to the desired DDIM trajectory, ensuring that the original content in the source image is highly preserved and can be injected into the editing process for better content preservation. However, these methods still rely on the accuracy of DDIM inversion. This brings us to the root of the issue: correcting the DDIM errors themselves. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Noise Schedule Adjustments. Previous work on noise scheduling focuses on training diffusion models from scratch to improve image quality or optimize the variational lower bound [29, 23, 26, 14, 20, 34]. Hoogeboom et al. propose noise schedule adjustments and other strategies to effectively train standard denoising diffusion models on high-resolution images without additional sampling modifiers. Lin et al. reveal that common diffusion noise schedules fail to enforce zero terminal SNR, causing discrepancies between training and inference. However, none focus on designing an off-the-shelf noise schedule for image editing\u2014a downstream task that does not require training from scratch and leverages existing models for sampling. This highlights the need for a simple but effective noise schedule tailored for downstream tasks like image editing. ", "page_idx": 24}, {"type": "text", "text": "D Experimental Settings ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/1f4cc4f5a0337a5799de4775f13f66e05ac83ef15d9ffddfb201182dcce3362d.jpg", "table_caption": ["D.1 Introduction of Editing Types "], "table_footnote": ["Table 5: Editing tasks with example source and target prompts. The change parts are noted in red. "], "page_idx": 24}, {"type": "text", "text": "We conducts eight editing tasks based on the real images to verify the effectiveness and versatility, along with the corresponding challenges for each task (Table 5): ", "page_idx": 24}, {"type": "text", "text": "1. Attributes Content Editing (Fig. 13): Modifies specific attributes, like changing a cat\u2019s expression. The challenge is ensuring high fidelity and preserving the original content without artifacts.   \n2. Attributes Color Editing (Fig. 14): Alters color attributes, like changing a bird\u2019s color or eye color. The challenge is maintaining natural and coherent lighting and shading.   \n3. Attributes Material Editing (Fig. 15): Changes material properties, like transforming a tiger into a silver sculpture. The challenge is accurately rendering new materials while preserving shape and avoiding unrealistic artifacts.   \n4. Object Switch (Fig. 16): Switches objects, like replacing bread with meat or transforming a fox into a horse. The challenge is maintaining scene composition and seamlessly integrating new objects.   \n5. Object Addition (Fig. 17): Adds new objects, like sunglasses to a dog or more strawberries in a bowl. The challenge is naturally integrating new objects, ensuring consistent lighting, shadows, and perspective.   \n6. Non-Rigid Editing (Fig. 18): Makes non-rigid modifications, like changing the pose of a tiger. The challenge is preserving anatomical correctness and natural appearance while making pose changes.   \n7. Scene Transferring (Fig. 19): Transfers scene context, like changing the background from mountains to the sea. The challenge is blending new backgrounds seamlessly with the foreground, maintaining consistent lighting, shadows, and color tones.   \n8. Style Transferring (Fig. 20): Transfers artistic style, like converting a photo into a watercolor painting. The challenge is preserving essential details and content while accurately applying the new artistic style. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "D.2 Implementation Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "All primary experiments are conducted using Stable Diffusion $\\mathrm{v}1.5^{2}$ , with an image size of $512\\mathrm{x}512\\mathrm{x}3$ and a latent space of $64\\mathrm{x}64\\mathrm{x}4$ . For ablation studies (Section 5.3.2), SD $\\mathrm{v}2.1^{3}$ and $\\mathrm{SDXL^{4}}$ are employed. Experiments run on a single Nvidia A100 GPU with 100 timesteps. The inversion (forward) guidance scale is set to 3.5, and the generation (reverse) guidance scale is set to 7.5. For the logistic schedule, $k$ is set to 0.015, and $t_{0}$ is set to $\\operatorname{int}(0.3T)$ , where $T$ is the number of timesteps. Default hyperparameter settings are used unless otherwise specified. For each incorporated method, default hyperparameters are as follows: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Edit Friendly DDPM [21]: $T_{\\mathrm{skip}}=36$ , starting generation from timestep $T-T_{\\mathrm{skip}}$ .   \n\u2022 StyleDiffusion [63]: Uses SD v1.5 with 1000 inference timesteps and $T_{\\mathrm{trans}}=301$ for style transfer.   \n\u2022 MasaCtrl [6]: Starts mutual self-attention control at step $S=4$ and layer $L=10$ .   \n\u2022 Pix2Pix Zero [45]: Applies noise regularization for 5 iterations at each timestep with a weight $\\lambda$ of 20.   \n\u2022 Null-Text Inversion [41]: 500 iterations for null-text optimization, with early stopping at $\\epsilon=1e^{-5}$ .   \n\u2022 Negative Prompt Inversion [40]: Uses early stopping with a threshold increasing linearly from $1e^{-5}$ by a factor of $2e^{-5}$ through the sampling steps. ", "page_idx": 25}, {"type": "text", "text": "D.3 Evaluation Metrics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this work, seven metrics are employed to evaluate the effectiveness of the Logistic Schedule, including three aspects introduced below. ", "page_idx": 25}, {"type": "text", "text": "Structure Distance: Structure Distance: To evaluate the structure distance between the source and edited images, we leverage DINO-I 5, which was proposed by DreamBooth [51] to emphasize unique properties of identities. For DINO-I, we calculate the cosine similarity between the ViT-B/16 DINO [7] embeddings of source and generated images. Additionally, we consider fine-tuning and editing time as metrics to evaluate the efficiency of the editing process. Since DINO is trained in a self-supervised manner, it highlights general features rather than category-based distinctions, making it suitable for capturing the structural integrity of images. ", "page_idx": 25}, {"type": "text", "text": "Background Preservation: To measure how the background is preserved during editing, we apply PSNR, LPIPS [24, 72], MSE, and SSIM [64] in the area outside of the annotated masks. These metrics serve different roles in evaluating image quality and preservation: ", "page_idx": 25}, {"type": "text", "text": "\u2022 PSNR (Peak Signal-to-Noise Ratio) measures the ratio between the maximum possible power of a signal and the power of corrupting noise, reflecting the overall quality of the image: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{PSNR}=10\\cdot\\log_{10}\\left({\\frac{\\mathrm{MAX}^{2}}{\\mathrm{MSE}}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where MAX is the maximum possible pixel value of the image (e.g., 255 for an 8-bit image), and MSE is the Mean Squared Error. \u2022 LPIPS (Learned Perceptual Image Patch Similarity) evaluates perceptual similarity by comparing the differences in deep feature space using a pretrained deep network (such as VGG [54]), capturing human visual perception better than pixel-wise metrics, by calculating the Euclidean distance between the feature representations of two images: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{LPIPS}=\\sum_{l}w_{l}\\left\\|\\phi_{l}(x)-\\phi_{l}(y)\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\phi_{l}$ denotes the feature map at layer $l$ of the pretrained network, and $w_{l}$ are the weights for each layer. \u2022 MSE (Mean Squared Error) calculates the average squared difference between original and edited image pixels, indicating the overall fidelity and error magnitude: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{MSE}=\\frac{1}{N}\\sum_{i=1}^{N}(I_{1}[i]-I_{2}[i])^{2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $N$ is the number of pixels in the image, $I_{1}$ and $I_{2}$ are the original and edited images respectively, and $i$ indexes the pixels.   \n\u2022 SSIM (Structural Similarity Index Measure) assesses image similarity by comparing luminance, contrast, and structure, providing a holistic view of image quality. The SSIM index between two images $x$ and $y$ is calculated as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{SSIM}(x,y)=\\frac{(2\\mu_{x}\\mu_{y}+C_{1})(2\\sigma_{x y}+C_{2})}{(\\mu_{x}^{2}+\\mu_{y}^{2}+C_{1})(\\sigma_{x}^{2}+\\sigma_{y}^{2}+C_{2})},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mu_{x}$ and $\\mu_{y}$ are the average pixel values of $x$ and $y$ , $\\sigma_{x}^{2}$ and $\\sigma_{\\mathrm{y}}^{2}$ are the variances of $x$ and $y$ , $\\sigma_{x y}$ is the covariance of $x$ and $y$ , and $C_{1}$ and $C_{2}$ are constants to stabilize the division when the denominator is close to zero. ", "page_idx": 26}, {"type": "text", "text": "Incorporating these metrics together can demonstrate background preservation more robustly since multiple metrics offer a comprehensive evaluation from different perspectives. ", "page_idx": 26}, {"type": "text", "text": "Text-Image Consistency: CLIP Similarity [48] evaluates the text-image consistency between the edited images and the corresponding target editing text prompts. CLIP-I and CLIP- $\\mathbf{\\nabla}\\cdot\\mathbf{\\nabla}T$ assess visual similarity and text-image alignment, respectively. For CLIP-I, we calculate the CLIP visual similarity between the source and generated images. For CLIP-T, we calculate the CLIP text-image similarity between the generated images and the given text prompts. These metrics help ensure that the edited images accurately reflect the intended modifications described in the text prompts. ", "page_idx": 26}, {"type": "text", "text": "E Experimental Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.1 Quantitative Comparison Across Editing Types ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide the performance of Logistic Schedule following the editing methods configuration in Section 5 in Table 6. The results vary across different editing types. Attributes content editing shows high PSNR and CLIP visual similarity. We attribute this to the relatively straightforward nature of modifying content attributes, which allows for high fidelity and coherence in the edited images compared to more complex edits. In the more challenging editing types, such as object addition (5th row) and non-rigid editing (e.g., pose, motion, 6th row), the model shows minimal changes, resulting in relatively better evaluation results in essential content preservation metrics. The limited alterations required in these tasks help maintain the original structure and details, leading to higher PSNR and SSIM values. Object switch shows high CLIP visual similarity. This can be attributed to the clear and distinct nature of object switching, which allows for more precise visual matching with the target object compared to other editing tasks. Style transferring (8th row) shows the highest CLIP text similarity. This is likely because the task involves applying well-defined artistic styles that closely align with the textual descriptions, resulting in edits that match the intended style effectively. Scene and style transferring (7th and 8th row) show high LPIPS and SSIM. We attribute this to the nature of the task, which involves changing backgrounds or styles while keeping the main subjects intact. This process maintains the overall scene coherence and structure consistency, resulting in enhanced perceptual quality and structural similarity. ", "page_idx": 26}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/e7cdf96b00618a83d4f2b555a9255ef0ed861fb7831f6563b498c385726e6375.jpg", "img_caption": ["Figure 8: Impact of different combinations of inversion and reverse guidance scales on various performance metrics. Results are averaged on Attributes Editing tasks using Prompt-to-Prompt as the editing method [16], highlighting optimal scale settings for balanced performance across tasks. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/b7a16d12d602442112d331193ffcf95e8e85c83b83cfbe2cd47d3e1a8563bbe1.jpg", "table_caption": ["Table 6: Performance of Logistic Schedule on different editing tasks in have ten independent runs with random seeds. Bold values indicate the best results, while underlined values denote the second-best results. \u2018Attr.\u2019, \u2018Obj.\u2019 and \u2018Trans.\u2019 denote \u2018Attributes\u2019, \u2018Objects\u2019, and \u2018Transferring\u2019, respectively. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "E.2 Qualitative Comparison Across Editing Types ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The logistic noise schedule consistently outperforms linear and cosine schedules across various editing tasks. It excels in preserving the original image content while making specified changes without artifacts (Fig. 13), maintaining natural and coherent lighting in color edits (Fig. 14), and accurately rendering new material properties (Fig. 15). For object switching and addition, it ensures seamless integration with consistent lighting and spatial relationships (Figs. 16, 17). In non-rigid editing, it preserves anatomical correctness and smooth transitions (Fig. 18). It also blends new backgrounds naturally in scene transfers (Fig. 19) and maintains essential details while applying new artistic styles (Fig. 20). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "E.3 Broader Application: Training-Based Methods ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "While the Logistic Schedule has shown broad applicability in various image editing tasks, we further explore its use in training-based methods, specifically in Text-to-Image synthesis. For this, we conducted experiments by fine-tuning the UNet using DreamBooth [51], leveraging approximately 100 images for each configuration. ", "page_idx": 28}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/016b266adcb7a0068c07bee8f85bc188b6c23b4f04692845799b6a7b7ebb0954.jpg", "table_caption": ["Table 7: Performance comparison of DreamBooth fine-tuning using different noise schedules on the SD-1.5 model. The best results are in bold. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "The related qualitative comparisons are shown in Fig. 9. These results demonstrate that training DreamBooth with the Logistic Schedule improves performance across key metrics such as DINO and CLIP-I similarity, as well as PSNR and PRES, outperforming both linear and cosine schedules. ", "page_idx": 28}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/7994d066806f0e15a5e84b90693c80075396ffd839bb510692575ef1a8a81a04.jpg", "img_caption": ["Figure 9: Qualitative comparisons of fine-tuning DreamBooth using different noise schedules (Linear, Cosine, and Logistic). The top column presents the fine-tune samples, and the instruction prompts, and the below column displays the corresponding fine-tuned outputs. The Logistic Schedule produces superior outputs with improved fidelity and alignment to the prompts. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "E.4 Comparison With Other Noise Schedulers ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We conducted experiments comparing our Logistic Schedule with other schedules under the DDIM paradigm, such as exponential, sigmoid, hyperbolic, and geometric schedules. Table 8 displays the quantitative results. The best-performing method is indicated in bold, the worst method is marked in purple, and the second-best method is underlined. ", "page_idx": 28}, {"type": "text", "text": "The results demonstrate that our Logistic Schedule achieves competitive performance across various metrics. Notably, it offers significant improvements in content preservation and edit fidelity compared to other schedules. As shown in Fig. 10, the Logistic Schedule preserves the visual characteristics of the source image more faithfully during reconstruction and enables more precise control during editing. ", "page_idx": 28}, {"type": "text", "text": "Table 8: Comparison of different noise schedules. Metrics include Structure Distance $(\\times10^{-3})$ , PSNR (higher is better), LPIPS $(\\times10^{-3}$ , lower is better), MSE $(\\times10^{-4}$ , lower is better), SSIM $(\\times10^{-2}$ , higher is better), and CLIP Similarity for both visual and textual content. ", "page_idx": 29}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/ce77967afcb5bc18d1cf0da133660911329c031991154893fe864150bcada2d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/1ab0a90ad2c98b787c2c9a4fd29cb6f970ab1d05f7818bb2fb989967a7ca5887.jpg", "img_caption": ["Figure 10: Qualitative comparison between different noise schedules for both reconstruction and editing tasks. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "E.5 Reconstruction Ability of Different Noise Schedule ", "text_level": 1, "page_idx": 29}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/ba922a4c154d9f8031543ac14fffc4ae63ff607b10b828a17b3bc2724b830b06.jpg", "table_caption": ["Table 9: Comparison of reconstruction quality using different noise schedules for DDIM inversion and Direct Inversion, showing the superior performance of the Logistic Schedule. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "To demonstrate the ability of the Logistic Schedule to better align inversion by eliminating the singularity at the start point, we evaluate the reconstruction results of DDIM Inversion [55] and Direct Inversion [25] using scaled linear, cosine, and our logistic schedule. During reconstruction, the condition is the source prompt, which is also applied as a condition during inversion. The comparison of reconstruction quality is shown in Table 9. ", "page_idx": 29}, {"type": "text", "text": "Source Image ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Source Prompt: a train traveling down the tracks ", "page_idx": 30}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/0b888f95e31f79a9f5fa284f2696ae9d3fc8df3f5034026a2c1a3d3f941a0fd7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Target Prompt: a skyblue train traveling down the tracks ", "page_idx": 30}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/3e3c6d5e7fa8dc0947a7960271118a0f247077116c16a54c34ec516174ec8b51.jpg", "img_caption": ["", "Inversion (Forward) Guidance Scale "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 11: Qualitative comparison of varying guidance scales during the inversion (forward) and denoising (reversing) processes of DDIM. The guidance scales for inversion are varied across the columns (1 to 10), and the guidance scales for denoising are varied across the rows (3 to 25). ", "page_idx": 30}, {"type": "text", "text": "E.6 Effects of Guidance Scale ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We investigate the impact of the guidance scale on the inversion (forward) and generation (reserve) processes of DDIM with the Logistic Schedule, consequently affecting the editing results. We illustrate the impact of varying guidance scales during the inversion and denoising processes of DDIM on performance metrics in Fig.8, with an example of how the edited images are affected shown in Fig.11. When keeping the inverse guidance scale constant, we observed that as the reverse guidance scales increased gradually, background preservation initially decreased. The inflection point occurred when the inverse guidance scale equaled the forward guidance scale. In contrast, CLIP similarity showed a consistently increasing trend until the reverse guidance scale exceeded 15. The quantitative heatmaps and qualitative results highlight a noticeable trade-off between essential content preservation and edit fidelity. Optimal incorporation of both scales ensures a balance between structural preservation, perceptual quality, and text-image consistency, with a combination of 5.0 for inversion and 7.5 for reverse generally providing the best performance across most metrics. This trade-off arises because current editing methods struggle to differentiate between regions needing modification and those that do not, leading to substantial alterations of the source image and confilcting with content preservation objectives. ", "page_idx": 31}, {"type": "text", "text": "E.7 Effects of Input Scale ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Chen et al. proposed to modify noise scheduling by scaling the input $\\mathbf{X}_{0}$ by a constant factor $b$ via: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}b\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/2b8afc114c712ceb7ba97b2cecb1dac626c550f20ea45c9a71f96e15dfcb9d7c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "As the scaling factor $^b$ decreases, the original image\u2019s strength lessens and noise levels grow [9]. As previous image editing works have not extensively investigated the effects of input scale, we investigate the effects of input scale in image editing in this work. We change the input scale from 0.5 to 1.4 with a step size of 0.05, and illustrate the effects of the input scale on both content preservation and edit fidelity in Fig. 12. As observed, the input scale significantly impacts the balance between content preservation and edit fidelity. Higher input scales (closer to 1.4) better preserve the original image structure, as shown by lower structure distances and higher SSIM values but reduce edit fidelity (lower CLIP-T and CLIP-I scores). Conversely, lower input scales (closer to 0.5) enhance edit fidelity but degrade content preservation. The optimal input scale, found to be 0.8-0.95, achieves a balance between these objectives. This is because slightly higher noise levels improve editability while maintaining acceptable content preservation, providing a satisfactory trade-off in image editing. ", "page_idx": 31}, {"type": "table", "img_path": "Yu6cDt7q9Z/tmp/680c7d97cdb84b7e1ed70ebd087824df40a83f806e847fde0e24f56935014b4c.jpg", "table_caption": ["Table 10: Performance comparison of input scale normalization in object switch task using Zero-shot Pix2Pix, showing no improvement in content preservation or edit fidelity. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "A strategy to improve training a diffusion model from scratch is to normalize $\\mathbf{X}_{t}$ by its variance to ensure it has unit variance before feeding it to the denoising network. This prevents performance issues caused by variance changes in $\\mathbf{X}_{t}$ when $\\mathbf{X}_{0}$ has the same mean and variance as $\\epsilon$ [26]. However, in our image editing work using off-the-shelf diffusion models, we find that normalization does not improve content preservation or edit fidelity, as shown in Table 10. ", "page_idx": 31}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/df506a0b2a0961c4efcc3026d9917a505c9cad35a949912c1ede707a35c3d6c6.jpg", "img_caption": ["Figure 13: Comparison of linear, cosine, and logistic schedules in editing attributes content. The logistic schedule shows superior fidelity in preserving the original image content while accurately making the specified changes, without introducing artifacts or inconsistencies. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "F Limitations and Future Works ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This work endeavors to enhance inversion-based editing methods, focusing on improving noise schedule design during the inversion process. Even though this work reveals that modifying the noise schedule using off-the-shelf diffusion models can lead to editing improvements, there is still a lack of research on how other designs of noise schedules can lead to different effects. For example, is it possible to design dynamic adjustments of the noise schedule at each time step to achieve better results? Furthermore, the editing capabilities of the Logistic Schedule are inherently constrained by the limitations of inversion-based methods. For example, MasaCtrl [6] editing requires manual determination of timesteps and layers for attention control, limiting its ability to automatically adapt to diverse real-world objects with varying attributes. ", "page_idx": 32}, {"type": "text", "text": "Even though extensive experiments prove the effectiveness of the Logistic Schedule, it is worth diving deeper into the schedule\u2019s performance in the generation task. Due to computational resource constraints, we have not conducted training on the diffusion model from scratch to validate the full potential of the Logistic Schedule. Future work will include training diffusion models using the Logistic Schedule to validate its generation ability. ", "page_idx": 32}, {"type": "text", "text": "Another potential future research direction lies in exploring whether a steadier decrease in logSNR during perturbation, as described in Section 4, enhances editing quality. Additional experiments on both generation and editing are required to confirm if this trend extends to the generation process as well. ", "page_idx": 32}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/76d78fa7d254cc999c1a89de4b4d66372beada57a2f9b869f9ad1e36df637c5b.jpg", "img_caption": ["Figure 14: Comparison of linear, cosine, and logistic schedules in editing color attributes. The logistic schedule excels in maintaining natural and coherent lighting and shading, resulting in more realistic and seamless color changes without introducing visual inconsistencies. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/3affd962d18c15a03a66d876152e67d4583190f573ef1ff5e42145bcdfcbb513.jpg", "img_caption": ["Figure 15: Comparison of linear, cosine, and logistic schedules in changing material properties. The logistic schedule excels in accurately rendering new material properties, such as reflections and textures while preserving the shape and form of the original objects, avoiding unrealistic artifacts, and ensuring a natural appearance. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/a0419e6b202dbdd8f24efb87a70da5c813e7fd4bb284297d9e4dc99c63c40b24.jpg", "img_caption": ["Figure 16: Comparison of linear, cosine, and logistic schedules in switching objects. The logistic schedule excels in maintaining the overall composition and context of the scene while seamlessly integrating the new objects with consistent lighting, shadows, and spatial relationships, ensuring a natural and coherent appearance. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Our work introduces a novel editing technique for manipulating real images using state-of-the-art text-to-image diffusion models. While this technology could potentially be exploited by malicious parties to create fake content and spread disinformation, this is a common issue across all image editing techniques. Significant progress is already being made in identifying and preventing such malicious editing. Our research contributes to this effort by providing a detailed analysis of the inversion and editing processes in text-to-image diffusion models, thereby aiding in the development of more robust detection and prevention methods. ", "page_idx": 34}, {"type": "text", "text": "H Ethics Statement ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Generative models for synthesizing images carry several ethical concerns, particularly when used by bad actors to generate disinformation or potentially displace creative workers through automation. These models, trained on large amounts of user data from the internet without explicit consent, may generate augmentations that resemble or copy such data. This issue is not unique to our work but inherent to large-scale models like Stable Diffusion, which we employ in our data augmentation strategy. To mitigate this, we allow for the deletion of harmful or copyrighted concepts from the model\u2019s weights before augmentation, ensuring such material cannot be copied during the process. ", "page_idx": 34}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/741e3d663586ff465161180203ccde69764c211aa721c339dd0ad095eba1fa4b.jpg", "img_caption": ["Figure 17: Comparison of linear, cosine, and logistic schedules in adding objects. The logistic schedule excels in naturally integrating the new objects into the scene, ensuring consistent lighting, shadows, and perspective with the existing elements, resulting in a realistic and seamless appearance. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Linear Cosine Logistic Linear Cosine Logistic Real Image Schedule Schedule Schedule Real Image Schedule Schedule  Schedule ", "page_idx": 35}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/3bd1724a591440eb03f031b4bf03a538bba8424b4725c296c9057fb45524c81b.jpg", "img_caption": ["Figure 18: Comparison of linear, cosine, and logistic schedules in making non-rigid modifications. The logistic schedule excels in preserving anatomical correctness and natural appearance while making significant pose changes, ensuring smooth transitions and avoiding unnatural distortions. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/9cfe2bb081ac761f22057cc0b154db6813a2f762473adb26a3a3f340162a021c.jpg", "img_caption": ["Figure 19: Comparison of linear, cosine, and logistic schedules in transferring scenes. The logistic schedule excels in seamlessly blending the new background with the foreground objects, maintaining consistent lighting, shadows, and color tones, avoiding visible seams, and ensuring a natural and coherent appearance. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "Yu6cDt7q9Z/tmp/8ca0d2cd02b88debe9b190ab33b6b0ee6101363654b23446cb1ba8decf094e46.jpg", "img_caption": ["Figure 20: Comparison of linear, cosine, and logistic schedules in transferring styles. The logistic schedule excels in preserving essential details and content of the original image while accurately applying the new artistic style, ensuring consistency across the entire image and avoiding artifacts, resulting in a more natural and coherent style transfer. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Despite these concerns, these tools may also foster growth and improve accessibility in the creative industry. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction (Section 1) accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Appendix F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Concerning Proposition 3.1, Appendix B provides its corresponding proofs, with a short proof sketch to provide intuition. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Refer to Appendix D.1 and D.2 for implementation details, the provided code in the supplementary materials ensures the reproducibility as well. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Refer to the provided code in the supplementary materials. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Table 5 specifies all the tasks included in this work. Appendix D.2 provides the employed hyperparameters. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Appendix E.1 provides the statistical significance of the experiments. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Appendix D.2 provide sufficient information on the computer resources. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The authors have reviewed and understand the NeurIPS Code of Ethics, and confirm that their research conforms to it in every respect. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Potential societal impacts of the work are discussed in Appendix G. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 41}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The original papers of assets, and the corresponding versions are provided in Section 5, References and Appendix D.2. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We will document the assets well when we officially release our code. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not describe potential risks incurred by study participants. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}]