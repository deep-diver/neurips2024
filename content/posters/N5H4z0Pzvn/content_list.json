[{"type": "text", "text": "On Divergence Measures for Training GFlowNets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tiago da Silva Eliezer de Souza da Silva Diego Mesquita {tiago.henrique, eliezer.silva, diego.mesquita}@fgv.br School of Applied Mathematics Getulio Vargas Foundation Rio de Janeiro, Brazil ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative Flow Networks (GFlowNets) are amortized samplers of unnormalized distributions over compositional objects with applications to causal discovery, NLP, and drug design. Recently, it was shown that GFlowNets can be framed as a hierarchical variational inference (HVI) method for discrete distributions. Despite this equivalence, attempts to train GFlowNets using traditional divergence measures as learning objectives were unsuccessful. Instead, current approaches for training these models rely on minimizing the log-squared difference between a proposal (forward policy) and a target (backward policy) distribution. In this work, we first formally extend the relationship between GFlowNets and HVI to distributions on arbitrary measurable topological spaces. Then, we empirically show that the ineffectiveness of divergence-based learning of GFlowNets is due to the large gradient variance of the corresponding stochastic objectives. To address this issue, we devise a collection of provably variance-reducing control variates for gradient estimation based on the REINFORCE leave-one-out estimator. Our experimental results suggest that the resulting algorithms often accelerate training convergence when compared against previous approaches. All in all, our work contributes by narrowing the gap between GFlowNet training and HVI, paving the way for algorithmic advancements inspired by the divergence minimization viewpoint. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The approximation of intractable distributions is one of the central issues in machine learning and modern statistics [7, 35]. In reinforcement learning (RL), a recurring goal is to find a diverse set of high-valued state\u2013action trajectories according to a reward function. This problem may be cast as sampling trajectories proportionally to the reward, which is generally an intractable distribution over the environment [3, 9, 38, 50]. Similarly, practical Bayesian inference and probabilistic models computations involve assessing intractable posterior distributions [36, 78, 102]. In the variational inference (VI) approach, circumventing this intractability involves searching for a tractable approximation to the target distribution within a family of parametric models. Conventionally, the problem reduces to minimizing a divergence measure, such as Kullback-Leibler (KL) divergence [7, 36, 92] or Renyi- $\\alpha$ divergence [51, 70], between the variational approximation and the target. ", "page_idx": 0}, {"type": "text", "text": "In particular, Generative Flow Networks (GFlowNets) [3, 4, 48] are a recently proposed family of variational approximations well-suited for distribution over compositional objects (e.g., graphs and texts). GFlowNets have found empirical success within various applications from causal discovery [15, 16], NLP [30], and chemical and biological modeling [3, 32]. In a nutshell, a GFlowNet learns an iterative generative process (IGP) [26] over an extension of the target\u2019s support, which, for sufficiently expressive parameterizations of transition kernels, yields independent and correctly distributed samples [3, 48]. Remarkably, training GFlowNets typically consists of minimizing the log-squared difference between a proposal and target distributions over the extended space via SGD [4, 55], contrasting with divergence-minimizing algorithms commonly used in VI [7, 72]. ", "page_idx": 0}, {"type": "text", "text": "Indeed, Malkin et al. [56] suggests that trajectory balance (TB) loss training for GFlowNets leads to better approximations of the target distribution than directly minimizing the reverse and forward KL divergence, particularly in setups with sparser rewards. Nevertheless, as we highlight in Section 3, these results are a potential consequence of biases and high variance in gradient estimates for the divergence\u2019s estimates, which can be overlooked in the evaluation protocol reliant upon sparse target distributions. Therefore, in Section 5, we present a comprehensive empirical investigation of the minimization of well-known $f$ -divergence measures (including reverse and forward KL), showing it is an effective procedure that often accelerates the training convergence of GFlowNets relative to alternatives. To achieve these results, we develop in Section 4 a collection of control variates (CVs) [63, 71] to reduce the variance without introducing bias on the estimated gradients, improving the efficiency of the optimization algorithms [77, 85]. In summary, our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "1. We evaluate the performance of forward and reverse KL- [47], Renyi- $\\alpha$ [74] and Tsallis- $\\alpha$ [91] divergences as learning objectives for GFlowNets through an extensive empirical campaign and highlight that they frequently outperform traditionally employed loss functions.   \n2. We design control variates for the gradients of GFlowNets\u2019 divergence-based objectives. Therefore, it is possible to perform efficient evaluations of the optimization objectives using automatic differentiation frameworks [69], and the resulting experiments showcase the significant reduction in the variance of the corresponding estimators.   \n3. We developed a theoretical connection between GFlowNets and VI beyond the setup of finitely supported measures [56, 112], establishing results for arbitrary topological spaces. ", "page_idx": 1}, {"type": "text", "text": "2 Revisiting the relationship between GFlowNets and VI ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Initially, we review Lahlou et al. [48]\u2019s work on GFlowNets for distributions on topological spaces, a perspective applied consequentially to obtain the equivalence between GFlowNets training and VI divergence minimization in a more generic setting. Finally, we describe standard variance reduction techniques for solving stochastic optimization problems. ", "page_idx": 1}, {"type": "text", "text": "Notations. Let $(S,\\tau)$ be a topological space with topology $\\tau$ and $\\Sigma$ be the corresponding Borel $\\sigma$ -algebra. Also, let $\\nu\\colon\\Sigma\\,\\rightarrow\\,\\mathbb{R}_{+}$ be a measure over $\\Sigma$ and $\\kappa_{f},\\kappa_{b}\\colon S\\times\\Sigma\\to\\mathbb{R}_{+}$ be transition kernels over $\\boldsymbol{S}$ . For each $(B_{1},B_{2})\\in\\Sigma\\times\\Sigma$ , we denote by $\\begin{array}{r}{\\nu\\,\\mathring{\\otimes}\\,\\kappa(B_{1},B_{2}):=\\int_{B_{1}}\\nu(\\mathrm{d}s)k(s,B_{2})}\\end{array}$ . Likewise, we recursively define the product kernel as $\\kappa^{\\otimes0}(s,\\cdot)=\\kappa(s,\\cdot)$ and, for $n\\geq1$ , $\\kappa^{\\otimes n}(s,\\cdot)=$ $\\kappa^{\\otimes n-1}(s,\\cdot)\\otimes\\kappa$ for a transition kernel $\\kappa$ and $s\\in S$ . Note, in particular, that $\\kappa^{\\otimes n}$ is a function from $\\textstyle S\\times\\sum^{\\otimes n+1}$ to $\\mathbb{R}_{+}$ , with $\\Sigma^{\\otimes n+1}$ representing the product $\\sigma$ -algebra of $\\Sigma$ [1, 96]. Moreover, if $\\mu$ is an absolutely continuous measure relatively to $\\nu$ , denoted $\\mu\\ll\\nu$ , we write $\\mathrm{d}\\mu_{\\int\\mathrm{d}\\nu}$ for the corresponding density (Radom-Nikodym derivative) [1]. Furthermore, we denote by ${\\mathcal{P}}(A)=\\{S\\colon S\\subseteq A\\}$ the power-set of a set $A\\subset S$ and by $[d]=\\{1,\\ldots,d\\}$ the first $d$ positive integers. ", "page_idx": 1}, {"type": "text", "text": "GFlowNets. A GFlowNet is, in its most general form, built upon the concept of a measurable pointed directed acyclic graph (DAG) [48], which we define next. Intuitively, it extends the notion of a flow network to arbitrary measurable topological spaces, replacing the directed graph with a transition kernel specifying how the underlying states are connected. ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (Measurable pointed DAG [48]). Let $(\\bar{S},\\tau,\\Sigma)$ be a measurable topological space endowed with a reference measure $\\nu$ and forward $\\kappa_{f}$ and backward $\\kappa_{b}$ kernels. Also, let $\\bar{s}_{o}\\in\\bar{S}$ and $s_{f}\\in\\bar{S}$ be distinguished elements in $\\bar{S}$ , respectively called initial and final states, and $S=\\bar{S}\\setminus\\{s_{f}\\}$ . We assume $\\left\\{s_{f}\\right\\}$ is open. A measurable pointed $D A G$ is then a tuple $(S,\\mathcal{T},\\Sigma,\\kappa_{f},\\kappa_{b},\\nu)$ satisfying: ", "page_idx": 1}, {"type": "text", "text": "1. (Terminality) If $\\kappa_{f}(s,\\{s_{f}\\})>0$ , then $\\kappa_{f}\\big(s,\\big\\{s_{f}\\big\\}\\big)=1\\,\\forall s\\in\\bar{S}$ . Also, $\\kappa_{f}\\big(s_{f},\\cdot\\big)=\\delta_{s_{f}}$ .   \n2. (Reachability) For all $B\\in\\Sigma$ , $\\exists\\,n\\in\\mathbb{N}$ s.t. $\\kappa_{f}^{\\otimes n}(s_{o},B)>0$ , i.e., $B$ is reachable from $s_{o}$ .   \n3. (Consistency) For every $(B_{1},B_{2})\\;\\in\\;\\Sigma\\;\\times\\;\\Sigma$ such that $(B_{1},B_{2})\\;\\;\\notin\\;\\;\\{(s_{o},s_{o}),(s_{f},s_{f})\\}$ , $\\nu\\otimes\\kappa_{f}(B_{1},B_{2})=\\nu\\otimes\\kappa_{b}(B_{2},B_{1})$ . Moreover, $\\kappa_{b}(s_{o},B)=0$ for every $B\\in\\Sigma$ .   \n4. (Continuity) $s\\mapsto\\kappa_{f}(s,B)$ is continuous for $B\\in\\Sigma$ .   \n5. (Finite absorption) There is a $N\\in\\mathbb{N}$ such that $\\kappa_{f}^{\\otimes N}(s,\\cdot)=\\delta_{s_{f}}$ for every $s\\in S$ . We designate the corresponding DAG as finitely absorbing. ", "page_idx": 1}, {"type": "text", "text": "In this setting, the elements in the set $\\mathcal{X}=\\{s\\in\\mathcal{S}\\setminus\\{s_{f}\\}:\\kappa_{f}(s,\\{s_{f}\\})>0)\\}$ are called terminal states. Illustratively, when $\\boldsymbol{S}$ is finite and $\\nu$ is the counting measure, the preceding definition corresponds to a connected DAG with an edge from $s\\ \\in{\\cal S}$ to $s^{\\prime}\\,\\in\\,S$ iff $\\kappa_{f}(s,\\{s^{\\prime}\\})>0$ , with condition 5) ensuring acyclicity and condition 2) implying connectivity. A GFlowNet, then, is characterized by a measurable pointed DAG, a potentially unnormalized distribution over terminal states $\\mathcal{X}$ and learnable transition kernels on $\\boldsymbol{S}$ (Definition 2). Realistically, its goal is to find an IGP over $\\boldsymbol{S}$ which, starting at $s_{o}$ , samples from $\\mathcal{X}$ proportionally to a given positive function. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 2 (GFlowNets [48]). A GFlowNet is a tuple $(\\mathcal{G},P_{F},P_{B},\\mu)$ composed of a measurable pointed DAG $\\mathcal{G}$ , a $\\sigma$ -finite measure $\\mu\\ll\\nu$ , and $\\sigma$ -finite Markov kernels $P_{F}\\ll\\kappa_{f}$ and $P_{B}\\ll\\kappa_{b}$ , respectively called forward and backward policies. ", "page_idx": 2}, {"type": "text", "text": "Training GFlowNets. In practice, we denote by $p_{F_{\\theta}}\\colon S\\times S\\to\\mathbb{R_{+}}$ the density of $P_{F}$ relative to $\\kappa_{f}$ , which we parameterize using a neural network with weigths $\\theta$ . Similarly, we denote by $p_{B}$ the density of $P_{B}$ wrt $k_{b}$ . Our objective is, for a given target measure $R\\ll\\mu$ on $\\mathcal{X}$ with $r=\\mathrm{d}R_{\\int\\mathrm{d}\\mu}$ , estimate the $\\theta$ for which the distribution over $\\mathcal{X}$ induced by $P_{F}(s_{o},\\cdot)$ matches $R$ , i.e., for every $B\\in\\Sigma$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sum_{n\\geq0}\\int_{S^{n}}p_{F_{\\theta}}^{\\otimes n}(s_{o},s_{1:n},s_{f})\\mathbb{1}_{s_{n}\\in B}\\kappa_{f}^{\\otimes n}(s_{o},\\mathrm{d}s_{1:n})=\\frac{R(B)}{R(\\chi)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Importantly, the above sum contains only finitely many non-zero terms due to the finite absorption property of $\\kappa_{f}$ . To ensure that $p_{F_{\\theta}}$ abides by this equation, Lahlou et al. [48] showed it suffices that one of the next balance conditions are concomitantly satisfied by $P_{F}$ and $P_{B}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Trajectory balance condition). For all $n\\,\\geq\\,0$ and $\\mu^{\\otimes n}$ -almost surely $\\forall s_{1:n}\\,\\in\\,S^{n}$ , $\\begin{array}{r}{p_{F_{\\theta}}^{\\otimes n}(s_{o},s_{1:n},s_{f})\\!=\\!\\frac{r(s_{n})}{Z_{\\theta}}p_{B}^{\\otimes n}(s_{n},s_{n:1},s_{o})}\\end{array}$ r(Zs\u03b8n )pB\u2297n (sn, sn:1, so), w/ Z\u03b8 denoting the target distribution\u2019s partition function. ", "page_idx": 2}, {"type": "text", "text": "Definition 4 (Detailed balance condition). For an auxiliary parametric function $u\\colon S\\to\\mathbb{R}_{+}$ and $\\mu^{\\otimes2}$ -almost surely on $(s,s^{\\prime})\\in S^{2}$ $,u(s)p_{F_{\\theta}}(s,s^{\\prime})=u(s^{\\prime})\\bar{p_{B}}\\bar{(s^{\\prime},s)}$ and $u(x)=r(x)$ for $x\\in\\mathscr{X}$ . ", "page_idx": 2}, {"type": "text", "text": "To enforce the trajectory balance (TB) or detailed balance (DB) conditions, we conventionally define a stochastic optimization problem to minimize the expected log-squared difference between the leftand right-hand sides of the corresponding condition under a probability measure $\\xi$ supported on an appropriate space [4, 15, 48, 49, 52, 55, 67]. For TB, e.g., we let $\\xi$ be defined on $\\check{\\Sigma}^{\\otimes N}$ with support $\\operatorname{supp}({\\dot{\\mu}}^{\\otimes N})$ . Then, we estimate the GFlowNet\u2019s parameters $\\theta$ by minimizing ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau\\sim\\xi}\\left[\\left(\\sum_{0\\leq i\\leq N-1}\\left(\\log\\frac{p_{F_{\\theta}}(s_{i},s_{i+1})}{p_{B}(s_{i+1},s_{i})}\\right)+\\log\\frac{Z_{\\theta}}{r(s_{h})}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\tau=s_{o;N}$ and $h=\\operatorname*{max}\\{i\\colon s_{i}\\neq s_{f}\\}$ being the last non-final state\u2019s index in $\\tau$ . As denoted by the subscript on $Z_{\\theta}$ , using the TB loss incurs learning the target\u2019s normalizing constant. While tuning $p_{B}$ during training is also possible, the common practice is to keep it fixed. ", "page_idx": 2}, {"type": "text", "text": "Henceforth, we will consider the measurable space of trajectories $(\\mathcal{P}_{S},\\Sigma_{P})$ , with $\\mathcal{P}_{\\mathcal{S}}~~=$ $\\{(s,s_{1},\\ldots,s_{n},s_{f})\\,\\in\\,S^{n+1}\\times\\{s_{f}\\}\\colon0\\,\\le\\,n\\,\\le\\,\\hat{N}-1\\}$ and $\\Sigma_{P}$ as the $\\sigma$ -algebra generated by $\\textstyle{\\bigcup_{n=1}^{N+1}\\Sigma^{\\otimes n}}$ . For notational convenience, we use the same letters for representing the measures and kernels of $(S,\\Sigma)$ and their natural product counterparts in $(\\mathcal{P}_{S},\\Sigma_{P})$ , which exist by Carath\u00e9odory extension\u2019s theorem [96]; for example, $\\nu(B)\\;=\\;\\overline{{{\\nu}}}^{\\otimes n}(B)$ for $B\\ {\\stackrel{\\cdot}{=}}\\ (B_{1},\\ldots,{\\bar{B_{n}}})\\ \\in\\ \\Sigma^{\\otimes n}$ and $p_{F_{\\theta}}(\\tau|s_{o};\\theta)$ is the density of $P_{F}^{\\otimes n+1}(s_{o},\\cdot)$ for $\\boldsymbol{\\tau}=(s_{o},s_{1},\\dots,s_{n},s_{f})$ relatively to $\\mu^{\\otimes n}$ . In this case, we will write $\\tau$ for a generic element of $\\mathcal{P}_{\\cal S}$ and $x$ for its terminal state (which is unique by Definition 1). For a comprehensive overview of GFlowNets, please refer to [48, 55]. ", "page_idx": 2}, {"type": "text", "text": "GFlowNets and VI. GFlowNets can be interpreted as hierarchical variational models by framing the forward policy $p_{F_{\\theta}}(\\tau|s_{o};\\theta)$ in $(\\mathcal{P}_{S},\\Sigma_{P})$ as a proposal to r(Zx )pB(\u03c4|x). Malkin et al. [56] demonstrated that, for discrete target distributions, the TB loss in (1) aligns with the KL divergence in terms of expected gradients. Extending this, our Proposition 1 establishes that this relationship also holds for distributions over arbitrary topological spaces. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 (TB loss- and $\\mathrm{KL}$ divergence gradients for topological spaces). Let $\\mathcal{L}_{T B}(\\tau;\\theta)=$ $(\\log Z p_{F_{\\theta}}(\\tau|s_{o};\\theta)/r(x)p_{B}(\\tau|x))^{2}$ and $\\begin{array}{r}{p_{B}(\\tau)=\\frac{r(x)}{Z}p_{B}(s_{n-1:o}|x)f o r\\,\\tau=(s_{o},\\dots,s_{n-1},x,s_{f})}\\end{array}$ . Then, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\mathcal{L}_{T B}(\\tau;\\theta)\\right]=2\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{D}_{K L}[p_{F_{\\theta}}||p_{B}]=\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\log p_{F_{\\theta}}(\\tau|s_{o};\\theta)/p_{B}(\\tau)\\right]$ is the $K L$ divergence between $P_{F}$ and $P_{B}$ . ", "page_idx": 2}, {"type": "text", "text": "This proposition shows that minimizing the on-policy TB loss is theoretically comparable to minimizing the KL divergence between $P_{F}$ and $P_{B}$ in terms of convergence speed. Since the TB loss requires estimating the intractable $R(\\mathcal{X})$ , the KL divergence, which avoids this estimation, can be a more suitable objective. Our experiments in Section 5 support this, with proofs provided in Appendix C. Extending this result to general topological spaces broadens the scope of divergence minimization strategies, extending guarantees for discrete spaces to continuous and mixed spaces. This generalization aligns with advances in generalized Bayesian inference [45] and generalized VI in function spaces [95], via optimization of generic divergences. We make the method theoretically firm and potentially widely applicable by proving the equivalence in these broader contexts. ", "page_idx": 3}, {"type": "text", "text": "Variance reduction. A naive Monte Carlo estimator for the gradient in Equation 2 has high variance [19], impacting the efficiency of stochastic gradient descent [97]. To mitigate this, we use control variates\u2014random variables with zero expectation added to reduce the estimator\u2019s variance without bias [63, 71]. This method, detailed in Section 4, significantly reduces noise in gradient estimates and pragmatically improves training convergence, as shown in the experiments in Section 5. ", "page_idx": 3}, {"type": "text", "text": "3 Divergence measures for learning GFlowNets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This Section presents four different divergence measures for training GFlowNets and the accompanying gradient estimators for stochastic optimization. Regardless of the learning objective, recall that our goal is to estimate $\\theta$ by minimizing a discrepancy measure $D$ between $P_{F}$ and $P_{B}$ that is globally minimized if and only if $P_{F}=P_{B}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta^{*}=\\operatorname*{arg\\,min}_{\\theta}D(P_{F},P_{B}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in which $P_{B}$ is typically fixed and $P_{F}$ \u2019s density $p_{F}^{\\theta}$ is parameterized by $\\theta$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Renyi- $\\alpha$ and Tsallis- $\\alpha$ divergences ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Renyi- $\\alpha$ [74] and Tsallis- $\\cdot\\alpha$ [91] are families of statistical divergences including, as limiting cases, the widespread KL divergence (Section 3.2) [58]; see Definition 5. These divergences have been successfully applied to both variational inference [51] and policy search for model-based reinforcement learning [18]. Moreover, as we highlight in Section 5, their performance is competitive with, and sometimes better than, traditional learning objectives for GFlowNets based on minimizing log-squared differences between proposal and target distributions. ", "page_idx": 3}, {"type": "text", "text": "Definition 5 (Renyi- $\\cdot\\alpha$ and Tsallis- $\\alpha$ divergences). Let $\\alpha\\in\\mathbb R$ . Also, let $p_{F_{\\theta}}$ and $p_{B}$ be GFlowNet\u2019s forward and backward policies, respectively. Then, the Renyi- $\\alpha$ divergence between $P_{F}$ and $P_{B}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\alpha}(P_{F}||P_{B})=\\frac{1}{\\alpha-1}\\log\\int_{\\mathcal{P}_{S}}p_{F_{\\theta}}(\\tau|s_{o})^{\\alpha}p_{B}(\\tau)^{1-\\alpha}\\kappa_{f}(s_{o},\\mathrm{d}\\tau).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similarly, the Tsallis- $\\alpha$ divergence between $P_{F}$ and $P_{B}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{\\alpha}(P_{F}||P_{B})=\\frac{1}{\\alpha-1}\\left(\\int_{\\mathcal{P}_{S}}p_{F_{\\theta}}(\\tau|s_{o})^{\\alpha}p_{B}(\\tau)^{1-\\alpha}\\kappa_{f}(s_{o},\\mathrm{d}\\tau)-1\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From Definition 5, we see that both Renyi- $\\alpha$ and Tsallis- $\\alpha$ divergences transition from a masscovering to a mode-seeking behavior as $\\alpha$ ranges from $-\\infty$ to $\\infty$ . Regarding GFlowNet-training, this flexibility suggests that $\\mathcal{R}_{\\alpha}$ and $\\mathcal{T}_{\\alpha}$ are appropriate choices both, e.g., for carrying out Bayesian inference [16] \u2014 where interest lies in obtaining an accurate approximation to a posterior distribution \u2014, and for combinatorial optimization [109] \u2014 where the goal is to find a few high-valued samples. Additionally, the choice of $\\alpha$ provides a mechanism for controlling which trajectories are preferentially sampled during training, with larger values favoring the selection of trajectories leading to highprobability terminal states, resembling the effect of $\\varepsilon$ -greedy [55], thompson-sampling [73], localsearch [40], and forward-looking [65, 66] techniques for carrying out off-policy training of GFlowNets [56]. ", "page_idx": 3}, {"type": "text", "text": "To illustrate the effect of $\\alpha$ on the learning dynamics of GFlowNets, we show in Figure 1 an early stage of training to sample from a homogeneous mixture of Gaussian distributions by minimizing Renyi- $\\alpha$ divergence for different values of $\\alpha$ ; see Section 5.1 for details on this experiment. At this stage, we note that the GFlowNet covers the target distribution\u2019s modes but fails to separate them when $\\alpha$ is large and negative. In contrast, a large positive $\\alpha$ causes the model to focus on a single high-probability region. Therefore, the use of an intermediate value for $\\alpha=0.5$ culminates in a model that accurately approximates the target distribution. Also, our early experiments suggested the persistence of such dependence on $\\alpha$ for diverse learning tasks, with $\\alpha=0.5$ leading to the best results. Thus, we fix $\\alpha=0.5$ throughout our experimental campaign. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Importantly, we need only the gradients of $\\mathcal{R}_{\\alpha}$ and $\\mathcal{T}_{\\alpha}$ for solving the optimization problem in Equation 3 and, in particular, learning the target distribution\u2019s normalizing constant is unnecessary, as we underline in the lemma below. This property distinguishes such divergence measures from both TB and DB losses in Equation 1 and, in principle, simplifies the training of GFlowNets. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Gradients for $\\mathcal{R}_{\\alpha}$ and $\\mathcal{T}_{\\alpha}$ ). Let $\\theta$ be the parameters of $p_{F_{\\theta}}$ in Definition $5$ and, for $\\ensuremath{\\tau}\\mathrm{~\\boldmath~\\in~}\\,\\mathcal{P}_{\\ensuremath{S}}$ , $\\begin{array}{r l}{g(\\tau,\\theta)}&{{}=}\\end{array}$ $\\left(p_{B}(\\tau|x)r(x)\\middle/p_{F_{\\theta}}(\\tau|s_{o};\\theta)\\right)^{1-\\alpha}$ . The gradient of $\\mathcal{R}_{\\alpha}$ wrt $\\theta$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{R}_{\\alpha}(p_{F_{\\theta}}||p_{B})=\\frac{\\mathbb{E}[\\nabla_{\\theta}g(\\tau,\\theta)+g(\\tau,\\theta)\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau|s_{o};\\theta)]}{(\\alpha-1)\\mathbb{E}[g(\\tau,\\theta)]};\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/7cdc967c55b6b41900bf851b70651bb0ae9bf2aa6eb3fa31c42ca4f9d02946f6.jpg", "img_caption": ["Figure 1: Mode-seeking $(\\alpha=2)$ ) versus mass-covering $(\\alpha=-2)$ ) behaviour in $\\alpha$ -divergences. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "the expectations are computed under $P_{F}$ . Analogously, the gradient of $\\mathcal{T}_{\\alpha}$ wrt $\\theta$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{T}_{\\alpha}(p_{F_{\\theta}}||p_{B})\\overset{C}{=}\\frac{\\mathbb{E}[\\nabla_{\\theta}g(\\tau,\\theta)+g(\\tau,\\theta)\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau|s_{o};\\theta)]}{(\\alpha-1)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in which $\\underline{{\\underline{{C}}}}$ denotes equality up to a multiplicative constant. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 uses the REINFORCE method [97] to compute the gradients of both $\\mathcal{R}_{\\alpha}$ and $\\mathcal{T}_{\\alpha}$ , and we implement Monte Carlo estimators to approximate the ensuing expectations based on a batch of trajectories $\\{\\tau_{1},\\dotsc,\\tau_{N}\\}$ sampled during training [56]. Also, note that the function $g$ is computed outside the log domain and, therefore, particular care is required to avoid issues such as numerical underflow of the unnormalized distribution [3, 87]. In our implementation, we sample an initial batch of trajectories $\\{\\tau_{i}\\}_{i=1}^{N}$ and compute the maximum of $r$ among the sampled terminal states in log space, i.e., $\\log\\bar{r}=\\operatorname*{min}_{i}\\log r(x_{i})$ . Then, we consider $\\log\\tilde{r}(\\bar{x})=\\log\\bar{r}(x)-\\log\\tilde{r}$ as the target\u2019s unnormalized log density. In Section 4, we will consider the design of variance reduction techniques to decrease the noise level of gradient estimates and possibly speed up the learning process. ", "page_idx": 4}, {"type": "text", "text": "3.2 Kullback-Leibler divergence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The KL divergence [47] is a limiting member of the Renyi- $\\alpha$ and Tsallis- $\\alpha$ families of divergences, derived when $\\alpha\\rightarrow1$ [70], and is the most widely deployed divergence measure in statistics and machine learning. To conduct variational inference, one regularly considers both the forward and reverse KL divergences, which we review in the definition below. ", "page_idx": 4}, {"type": "text", "text": "Definition 6 (Forward and reverse KL). The forward KL divergence between a target $P_{B}$ and a proposal $P_{F}$ is $\\mathcal{D}_{K L}[P_{B}||P_{F}]=\\mathbb{E}_{\\tau\\sim P_{B}(s_{f},\\cdot)}\\left[\\log p_{B}(\\tau)/p_{F_{\\theta}}(\\tau|s_{o})\\right]$ . Also, the reverse KL divergence is defined by $D_{K L}[P_{F}||P_{B}]=\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\log p_{F_{\\theta}}(\\tau|s_{o})\\middle/p_{B}(\\tau)\\right]\\!.$ ", "page_idx": 4}, {"type": "text", "text": "Remarkably, we cannot use a simple Monte Carlo estimator to approximate the forward $\\mathrm{KL}$ due to the presumed intractability of $P_{B}$ (which depends directly on $R$ ). As a first approximation, we could estimate $\\mathcal{D}_{K L}[P_{B}||P_{F}]$ via importance sampling w/ $P_{F}$ as a proposal distribution as in [56]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}_{K L}[P_{B}||P_{F}]=\\mathbb{E}_{\\tau\\sim P_{F}}\\left[\\frac{p_{B}(\\tau)}{p_{F_{\\theta}}(\\tau|s_{o})}\\log\\frac{p_{B}(\\tau)}{p_{F_{\\theta}}(\\tau|s_{o})}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and subsequently implement a REINFORCE estimator to compute $\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{B}||P_{F}]$ . Nevertheless, as we only need the divergence\u2019s derivatives to perform SGD, we apply the importance weights directly to the gradient estimator. We summarize this approach in the lemma below. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 (Gradients for the $\\mathrm{KL}$ divergence). Let $\\theta$ be the parameters of $P_{F}$ and $s(\\tau;\\theta)\\;=\\;$ $\\log p_{F_{\\theta}}(\\tau|s_{o};\\theta)$ . Then, the gradient of $\\mathcal{D}_{K L}[P_{F}||P_{B}]$ relatively to $\\theta$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{D}_{K L}\\left[P_{F}||P_{B}\\right]=\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\nabla_{\\theta}s(\\tau;\\theta)+\\log\\frac{p_{F_{\\theta}}(\\tau|s_{o})}{p_{B}(\\tau|x)r(x)}\\nabla_{\\theta}s(\\tau;\\theta)\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/80073e41102c492cd944f3bd788b02362712fb41c1b89f8bff3f2fa4c0a8c73c.jpg", "img_caption": ["Figure 2: Variance of the estimated gradients as a function of the trajectories\u2019 batch size. Our control variates greatly reduce the estimator\u2019s variance, even for relatively small batch sizes. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Correspondingly, the gradient of $\\mathcal{D}_{K L}[P_{B}||P_{F}]$ wrt $\\theta$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{B}||P_{F}]\\stackrel{C}{=}-\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\frac{p_{F_{\\theta}}(\\tau|s_{o})}{p_{B}(\\tau|x)r(x)}\\nabla_{\\theta}s(\\tau;\\theta)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Crucially, choosing an appropriate learning objective is an empirical question that one should consider on a problem-by-problem basis \u2014 similar to the problem of selecting among Markov chain simulation techniques [27]. In particular, a one-size-fits-all solution does not exist; see Section 5 for a thorough experimental investigation. Independently of the chosen method, however, the Monte Carlo estimators for the quantities outlined in Lemma 2 are of potentially high variance and may require a relatively large number of trajectories to yield a reliable estimate of the gradients [97]. The following sections demonstrate that variance reduction techniques alleviate this issue. ", "page_idx": 5}, {"type": "text", "text": "4 Control variates for low-variance gradient estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Control variates. We first review the concept of a control variate. Let $f\\colon\\mathcal{P}_{S}\\rightarrow\\mathbb{R}$ be a real-valued measurable function and assume that our goal is to estimate $\\mathbb{E}_{\\tau\\sim\\pi}\\left[f(\\tau)\\right]$ according to a probability measure $\\pi$ on $\\Sigma_{P}$ (see Section 2 to recall the definitions). The variance of a naive Monte Carlo estimator for this quantity is $\\operatorname{Var}_{\\pi}(f(\\tau))/n$ . On the other hand, consider a random variable (RV) $g\\colon\\mathcal{P}_{S}\\rightarrow\\mathbb{R}$ positively correlated with $f$ and with known expectation $\\mathbb{E}_{\\pi}[g(\\tau)]$ . Then, the variance of a naive Monte Carlo for $\\mathbb{E}_{\\pi}\\left[f(\\tau)-\\dot{a}(g(\\tau)-\\mathbb{E}_{\\pi}[g(\\tau)])\\right]$ for a baseline $a\\in\\mathbb{R}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left[\\mathrm{Var}_{\\pi}(f(\\tau))\\!-\\!2a\\mathrm{Cov}_{\\pi}(f(\\tau),g(\\tau))+a^{2}\\mathrm{Var}_{\\pi}(g(\\tau))\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is potentially smaller than $\\begin{array}{r}{\\frac{1}{n}\\mathrm{Var}_{\\pi}\\big(f(\\tau)\\big)}\\end{array}$ if the covariance between $f$ and $g$ is sufficiently large. Under these conditions, we choose the value of $a$ that minimizes Equation 5 [94], namely, $a=$ $\\mathrm{Cov}_{\\pi}\\big(f(\\tau),g(\\tau)\\big)\\big/\\mathrm{var}_{\\pi}(g(\\tau))$ . We then call the function $g$ a control variate [63]. Also, although the quantities defining the best baseline $a$ are generally unavailable in closed form, one commonly uses a batchbased estimate of $\\operatorname{Cov}_{\\pi}(f(\\tau),g(\\tau)\\bar{)}$ and $\\dot{\\mathrm{Var}}_{\\pi}(g(\\tau))$ ; the incurred bias is generally negligible relatively to the reduced variance [71, 79, 85]. For vector-valued RVs, we let $a$ be a diagonal matrix and exhibit, in the next proposition, the optimal baseline minimizing the covariance matrix\u2019s trace. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2 (Control variate for gradients). Let $f,g\\colon\\mathcal{P}_{S}\\to\\mathbb{R}^{d}$ be vector-valued functions and $\\pi$ be a probability measure on $\\mathcal{P}_{\\cal S}$ . Consider $a$ baseline $a\\in\\mathbb{R}$ and assume $\\mathbb{E}_{\\pi}[g(\\tau)]=0$ . Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{a\\in\\mathbb{R}}\\mathrm{Tr}\\,\\mathsf{C o v}_{\\pi}[f(\\tau)-a\\cdot g(\\tau)]\\!\\!=\\!\\frac{\\mathbb{E}_{\\pi}[g(\\tau)^{T}(f(\\tau)-\\mathbb{E}_{\\pi}[f(\\tau^{\\prime})])]}{\\mathbb{E}_{\\pi}[g(\\tau)^{T}g(\\tau)]}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that, when implementing the REINFORCE gradient estimator, the expectation we wish to estimate may be generally written as $\\mathbb{E}_{P_{F}(s_{o},\\cdot)}\\left[\\nabla_{\\theta}f(\\tau)+f(\\tau)\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau)\\right]$ . For the second term, we use a leave-one-out estimator [85]; see below. For the first term, we use $\\nabla_{\\boldsymbol{\\theta}}\\log{p_{F_{\\boldsymbol{\\theta}}}}$ as a control variate, which satisfies $\\mathbb{E}_{P_{F}(s_{o},\\cdot)}\\left[\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau|s_{o};\\theta)\\right]\\,=\\,0$ . Importantly, estimating the optimal baseline $a^{\\star}$ in Proposition 2 cannot be done efficiently due to the non-linear dependence of the corresponding Monte Carlo estimator on the sample-level gradients [2]; i.e., it cannot be represented as a vector-Jacobian product, which is efficient to compute in reverse-mode automatic differentiation (autodiff) frameworks [8, 69]. Consequently, we consider a linear approximation of both numerator and denominator defining $a^{\\star}$ in Proposition 2, which may be interpreted as an instantiation of the delta method [83, Sec. 7.1.3]. Then, given a batch $\\{\\tau_{1},\\dots,\\tau_{N}\\}$ of trajectories, we instead use ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{a}=\\frac{\\left\\langle\\sum_{n=1}^{N}\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau_{n}),\\sum_{n=1}^{N}\\nabla_{\\theta}f(\\tau_{n})\\right\\rangle}{\\epsilon+\\left\\|\\sum_{n=1}^{N}\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau_{n})\\right\\|^{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as the REINFORCE batch-based estimated baseline; $\\langle\\cdot,\\cdot\\rangle$ represents the inner product between vectors. Intuitively, the numerator is roughly a linear approximation to the covariance between $\\nabla_{\\boldsymbol{\\theta}}\\log{p_{F_{\\boldsymbol{\\theta}}}}$ and $\\nabla_{\\theta}f$ under $P_{F}$ . In contrast, the denominator approximately measures the variance of $\\nabla_{\\boldsymbol{\\theta}}\\log{p_{F_{\\boldsymbol{\\theta}}}}$ , and $\\epsilon>0$ is included to enhance numerical stability. As a consequence, for the reverse KL divergence, $\\nabla_{\\boldsymbol{\\theta}}f(\\tau)\\,=\\,\\nabla_{\\boldsymbol{\\theta}}\\log p_{F_{\\boldsymbol{\\theta}}}(\\tau)$ , $\\hat{a}\\approx1$ and the term corresponding to the expectation of $\\nabla_{\\boldsymbol{\\theta}}f(\\tau)$ vanishes. We empirically find that this approach frequently reduces the variance of the estimated gradients by a large margin (see Figure 2 above and Section 5 below). ", "page_idx": 6}, {"type": "text", "text": "Leave-one-out estimator. We now focus on obtaining a low-variance estimate of $\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}[f(\\tau)\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau)]$ . As an alternative to the estimator of Proposition 2, Shi et al. [85] and Salimans and Knowles [82] proposed a sample-dependent baseline of the form $a(\\tau_{i})=$ $\\begin{array}{r}{\\frac{1}{N-1}\\sum_{1\\le n\\le N,n\\neq i}f(\\tau_{n})}\\end{array}$ for $i\\in\\{1,\\ldots,N\\}$ . The resulting estimator, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\delta=\\frac{1}{N}\\sum_{n=1}^{N}\\left(f(\\tau_{n})-\\frac{1}{N-1}\\sum_{j=1,j\\neq i}^{N}f(\\tau_{j})\\right)\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau_{n}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is unbiased for $\\mathbb{E}\\left[f(\\tau)\\nabla_{\\theta}\\log p_{F_{\\theta}}(\\tau)\\right]$ due to the independence between $\\tau_{i}$ and $\\tau_{j}$ for $i\\neq j$ . Strikingly, $\\delta$ can be swiftly computed with autodiff: if $\\mathbf{f}=(f(\\tau_{n}))_{n=1}^{N}$ and $\\mathbf{p}=(\\log p_{F_{\\theta}}(\\tau_{n}))_{n=1}^{N}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\delta=\\nabla_{\\theta}\\frac{1}{N}\\left\\langle\\mathrm{sg}\\left(\\mathbf{f}-\\frac{1}{N-1}(\\mathbf{1}-\\mathbf{I})\\mathbf{f}\\right),\\mathbf{p}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with sg as the stop-gradient operation (e.g., lax.stop_gradient in JAX [8] and torch.detach in PyTorch [69]). Importantly, these techniques incur a minimal computational overhead to the stochastic optimization algorithms relative to the considerable reduction in variance they enact. ", "page_idx": 6}, {"type": "text", "text": "Relationship with previous works. Importantly, Malkin et al. [56] used $\\begin{array}{r}{\\hat{a}\\;=\\;\\frac{1}{N}\\sum_{n}f(\\tau_{n})}\\end{array}$ as baseline and an importance-weighted aggregation to adjust for the off-policy sampling of trajectories, introducing bias in the gradient estimates and relinquishing guarantees of the optimization procedure. A learnable baseline independently trained to match $\\hat{b}$ was also considered. This potentially entailed the inaccurate conclusion that the TB and DB are superior to standard divergence-based objectives. Indeed, the following section underlines that such divergence measures are sound and practical learning objectives for GFlowNets for a range of tasks. ", "page_idx": 6}, {"type": "text", "text": "Illustration of the control variates\u2019 effectiveness. We train the GFlowNets using increasingly larger batches of $\\{2^{i}\\colon i\\in[[5,10]]\\}$ trajectories with and without CVs. In this setting, Figure 2 showcases the drastic reduction in the variance, represented by the covariance matrix\u2019s trace, of the estimated learning objectives\u2019 gradients w.r.t. the model\u2019s parameters promoted by the CVs. Impressively, as we show in Figure 6, this approach significantly increases the efficiency of the underlying stochastic optimization algorithm. See Section 5 and Appendix D for further details. ", "page_idx": 6}, {"type": "text", "text": "5 Training GFlowNets with divergence measures ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our experiments serve two purposes. Firstly, we show in Section 5.2 that minimizing divergencebased learning objectives leads to competitive and often better approximations than the alternatives based on log-squared violations of the flow network\u2019s balance. This underlines the effectiveness of well-established divergence measures for training GFlowNets [56, 112]. Secondly, we highlight in Section 5.3 that the reduction of variance enacted by our control variates critically accelerates the convergence of GFlowNets. We consider widely adopted benchmark tasks from GFlowNet literature, described in Section 5.1, contemplating both discrete and continuous target distributions. Please refer to Appendix B and Appendix E for additional information on the experimental setup. ", "page_idx": 6}, {"type": "text", "text": "5.1 Generative tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Below, we provide a high-level characterization of the generative tasks used for synthetic data generation and training. For a more rigorous description in the light of Section 2, see Appendix B. ", "page_idx": 6}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/10ef136f00b9cf81d475d74cdb299d3aa141ffb1ae950ec98fdddaae9158b3f2.jpg", "img_caption": ["Figure 3: Divergence-based learning objectives often lead to faster training than TB loss. Notably, contrasting with the experiments of [56], there is no single best loss function always conducting to the fastest convergence rate, and minimizing well-known divergence measures is often on par with or better than minimizing the TB loss in terms of convergence speed. Results were averaged across three different seeds. Also, we fix $\\alpha=0.5$ for both Tsallis- $\\alpha$ and Renyi- $\\alpha$ divergences. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Set generation [3, 34, 65, 66]. A state $s$ corresponds to a set of size up to a given $S$ and the terminal states $\\mathcal{X}$ are sets of size $S$ ; a transition corresponds to adding an element from a deposit $\\mathcal{D}$ to $s$ . The IGP starts at an empty set, and the log-reward of a $x\\in\\mathscr{X}$ is $\\breve{\\sum}_{d\\in x}\\,f(d)$ for a fixed $f\\colon{\\mathcal{D}}\\to\\mathbb{R}$ . ", "page_idx": 7}, {"type": "text", "text": "Autoregressive sequence generation [32, 55]. Similarly, a state is a seq. $s$ of max size $S$ and a terminal state is a seq. ended by an end-of-sequence token; a transition appends $d\\in\\mathcal D$ to $s$ . The IGP starts with an empty sequence and, for $x\\in\\mathscr{X}$ , $\\begin{array}{r}{\\log r(x)=\\sum_{i=1\\ldots|x|}g(\\bar{i})\\^{\\!}f(x_{i})}\\end{array}$ for functions $f,g$ . ", "page_idx": 7}, {"type": "text", "text": "Bayesian phylogenetic inference (BPI) [111]. A state $s$ is a forest composed of binary trees with labeled leaves and unlabelled internal nodes, and a transition amounts to joining the roots of two trees to a newly added node. Then, $s$ is terminal when it is a single connected tree \u2014 called a phylogenetic tree. Finally, given a dataset of nucleotide sequences, the reward function is the unnormalized posterior over trees induced by J&C69\u2019s mutation model [37] and a uniform prior. ", "page_idx": 7}, {"type": "text", "text": "Hypergrid navigation [3, 55, 56, 66]. A state $s\\in\\{0,\\ldots,H-1\\}^{d}$ is a component of a $H$ -sized and $d$ -dimensional Euclidean grid. The IGP starts at 0 and, if we let $\\delta_{i}$ be the $i$ -th line of the identity matrix and $\\Delta(s)=\\{\\delta_{i}:i\\in\\{1,\\dots,d\\}\\land\\operatorname*{max}_{j}(s+\\delta_{i})_{j}<H\\}$ , a transition either adds a $\\delta\\in\\Delta(\\bar{s})$ to $s$ or stops at $s$ . We use Malkin et al. [55, Section 5.1]\u2019s reward function with $R_{o}=10^{-3}$ . ", "page_idx": 7}, {"type": "text", "text": "Bayesian structure learning [15, 16]. A state $s$ is a DAG representing a Bayesian network; a transition either adds an edge to $s$ or stops the IGP. Similarly to Deleu et al. [15], we ensure the added edges preserve the state\u2019s acyclicity. The reward function is defined as the maximum likelihood of the linear Gaussian structural model induced by the current state based on a fixed i.i.d. data set. ", "page_idx": 7}, {"type": "text", "text": "Mixture of Gaussians (GMs) [48, 110]. The IGP starts at $\\mathbf{0}\\in\\mathbb{R}^{d}$ and proceeds by sequentially substituting each coordinate with a sample from a real-valued distribution. For a $K$ -component GM, the reward of $\\mathbf{x}\\in\\mathbb{R}^{d}$ is defined as $\\begin{array}{r}{\\sum_{k=}\\alpha_{k}\\mathcal{N}(\\mathbf{x}|\\mu_{k},\\Sigma_{k})}\\end{array}$ with $\\alpha_{k}\\geq0$ and $\\textstyle\\sum_{k}\\alpha_{k}=1$ . ", "page_idx": 7}, {"type": "text", "text": "Banana-shaped distribution [57, 76]. We use the same IGP implemented for a bi-dimensional GM. For $\\mathbf{z}\\in\\mathbb{R}^{2}$ , we set $r(\\mathbf{x})$ to a normal likelihood defined on a quadratic function of $\\mathbf{x}$ , see Equation 8 in the supplement. We use HMC samples as ground truth to gauge performance on this task. ", "page_idx": 7}, {"type": "text", "text": "5.2 Assessing convergence speed ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we provide evidence that minimizing divergence-based objectives frequently leads to faster convergence than minimizing the standard TB loss [55]. ", "page_idx": 7}, {"type": "text", "text": "Experimental setup. We compare the convergence speed in terms of the rate of decrease of a measure of distributional error when using different learning objectives for a GFlowNet trained to sample from each of the distributions described in Section 5.1. For discrete distributions, we adopt the evaluation protocols of previous works [3, 54, 55, 66] and compute the $L_{1}$ distance between the learned $p_{T}(x;\\bar{\\theta})$ and target $r(x)$ , namely, $\\begin{array}{r}{\\sum_{x\\in\\mathcal{X}}|p_{T}(x;\\theta)-r(x)/Z|}\\end{array}$ . To approximate $p_{T}$ , we use a Monte Carlo estimate of $p_{T}(x;\\theta)=\\mathbb{E}_{\\tau\\sim P_{B}(x,\\cdot)}\\left[p_{F_{\\theta}}(\\tau|s_{o};\\theta)\\middle/p_{B}(\\tau|x)\\right]$ . For continuous distributions, we echo [48, 110] and compute Jensen-Shannon\u2019s divergence between $P_{T}(x;\\theta)$ and $R(x)$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Im_{J S}[P_{T}||R]=1/2\\left(\\mathcal{D}_{K L}[P_{T}||M]+\\mathcal{D}_{K L}[R||M]\\right)=\\underset{x\\sim P_{T}}{\\mathbb{E}}\\left[\\log p_{T}(x)/m(x)\\right]+\\underset{x\\sim R}{\\mathbb{E}}\\left[\\log r(x)/Z m(x)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/4a1ce803bf174bb310da3d6f5f01b5254ef6b73de9b04d4899e903638870e4f3.jpg", "img_caption": ["Figure 4: Average reward for the $K$ highest scoring samples (top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}$ ) and Number of Modes found during training for the tasks of sequence design, set generation, hypergrid and DAG environments. With the only exception of the hypergrid task, the minimization of divergence-based measures leads to similar and often faster discovery of high-valued states relatively to their balance-based counterparts. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/e11e605faa3bdfcf3535f5f2bd9cbdb7c94d671655b7ecb2eb28657083a7c72d.jpg", "img_caption": ["Figure 5: Learned distributions for the banana-shaped target. Tsallis- $\\alpha$ , Renyi- $\\alpha$ and for. KL leads to a better model than TB and Rev. KL, which behave similarly \u2014 as predicted by Proposition 1. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "with $M(B)\\;=\\;1/2\\left(P_{T}(B)+R(B)\\!\\Big/_{R(X)}\\right)$ being the averaged measure of $P_{T}$ and $R$ and $m$ its corresponding density relatively to the reference measure $\\mu$ . Remarkably, for the GMs distribution, we can directly sample from the target to estimate $\\mathcal{D}_{K L}[R||M]$ , and the autoregressive nature of the generative process ensures that $p_{T}(x)~=~p_{F_{\\theta}}(\\bar{\\tau}|s_{o}^{\\cdot})$ for the unique trajectory $\\tau$ starting at $s_{o}$ and finishing at $x$ . Hence, we get an unbiased estimate of $\\mathcal{D}_{K L}[P_{T}||M]$ . Finally, let $\\mathrm{X}_{t}$ be the first $t$ terminal states encountered during training and $\\{x_{(1)},\\ldots,x_{(k)}\\}$ be an descending ordering of $\\mathrm{X}_{t}$ according to $r$ . Then, we select a threshold $\\tau\\,\\in\\,\\mathbb{R}$ and an integer $K$ to report $\\operatorname{NoM}(\\operatorname{X}_{t})\\ =\\ |\\{r(x)\\colon r(x)\\geq\\tau\\land x\\in\\mathrm{X}_{t}\\}|$ , called number of modes, and $\\operatorname{TopK}(\\operatorname{X}_{t})=\\operatorname{AVG}\\left(\\{r(x_{(i)})\\colon1\\leq i\\leq K\\}\\right)$ , referred to as top- $K$ average reward. Both NoM and TopK are metrics of substantial interest in the GFlowNet literature [3, 55, 56, 64, 65]. ", "page_idx": 8}, {"type": "text", "text": "Results. Figure 3 shows that the procedure minimizing divergence-based measures accelerates the training convergence of GFlowNets, whereas Figure 5 (for the banana-shaped distribution) and Table 1 highlight that we obtain a more accurate model with a fix compute budget. The difference between learning objectives is not statistically significant for the BPI task. Also, we may attribute the superior performance of reverse KL compared to the forward in the sequence generation task to the high variance of the importance", "page_idx": 8}, {"type": "table", "img_path": "N5H4z0Pzvn/tmp/21a02a2a88414c7e21d39211696759c2e935c1f84dacb56af89418b9385b79e8.jpg", "table_caption": ["Table 1: Divergence minimization achieves better than or similar accuracy compared to enforcing TB. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "sampling-based gradient estimates. Indeed, the observed differences disappear when we increase the batch of trajectories to reduce the estimator\u2019s variance (see Figure 8 in Appendix D). In conclusion, our empirical results based on experiments testing diverse generative settings and expanding prior art [48, 56, 112], shows that training methods based on minimizing $f$ -divergence VI objectives with adequate CVs implemented are practical and effective in many tasks. Correlatively, Figure 4 supports the fact that minimizing divergence-based objectives frequently implies better coverage of the target\u2019s high-probability regions; the only exception is the (extremely sparse) hypergrid task [55]. ", "page_idx": 8}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/0728fba02f0d3e1f2257905aa261fa61bc3f7127f7d9d7b3358cf961a550a470.jpg", "img_caption": ["Figure 6: Learning curves for different objective functions in the task of set generation. The reduced variance of the gradient estimates notably increases training stability and speed. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.3 Reducing the variance of the estimated gradients ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Figure 2 demonstrates that implementing CVs for the REINFORCE estimator reduces the noise level of gradient estimates significantly. This reduction in variance also accelerates training convergence. To illustrate this, we use the same experimental setup from Section 5.2 and analyze the learning curves for each divergence measure with and without control variates. ", "page_idx": 9}, {"type": "text", "text": "Results. Figure 6 shows that the implemented gradient reduction techniques significantly enhance the learning stability of GFlowNets and drastically accelerate training convergence when minimizing the reverse KL divergence. Our results indicate that well-designed CVs for gradient estimation can greatly benefit GFlowNets training. Notably, similar improvements have been observed in the context of Langevin dynamics simulations [22, 31, 44] and policy gradient methods for RL [68, 100]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions, limitations and broader impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Discussion. We showed in a comprehensive range of experiments that divergence measures common in VI \u2014 forward KL, reverse KL, Renyi- $\\alpha$ , and Tsallis- $\\alpha$ \u2014 are effective learning objectives for training GFlowNets, performing competitively with or better than their balance-based counterparts. To achieve this, the introduction of efficacious control variates for low-variance gradient estimation of the divergence-based objectives was crucial, which is a key distinction between our work an prior art [55, 112]. Additionally, we developed the theoretical connection between GFlowNets and VI beyond the setting of finitely supported measures, establishing results for arbitrary topological spaces. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Albeit comprehensive and on par with the wider literature, our empirical evaluation was performed on problems of relatively small size due to the intractability of probing a GFlowNet\u2019s distributional accuracy on very large state spaces. That said, we acknowledge that an assessment on the domains of natural language processing [30] and drug discovery [3] based on context-specific metrics would strengthen our conclusions; we leave these tasks to future endeavors. Similarly, while we observed promising results for $\\alpha=0.5$ , there might be different choices of $\\alpha$ that, depending on the application, might strike a better explore-exploit tradeoff and incur faster convergence. Thus, thoroughly exploring different $\\alpha$ might be especially useful to practitioners. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Overall, our work highlights the potential of the once-dismissed VI-inspired schemes for training GFNs, paving the way for further research towards improving the GFlowNets by drawing inspiration from the VI literature. For instance, one could develop $\\chi$ -divergence-based losses for GFNs [19], combine GFNs with MCMC using Ruiz and Titsias [81]\u2019s divergence, or employ an objective similar to that of importance-weighted autoencoders [10]. Finally, although an $\\epsilon$ -greedy off-policy sampling scheme can be easily incorporated into a divergence-minimizing algorithm through an importance-sampling correction, it remains elusive whether this would be possible for more sophisticated sampling techniques such as replay buffer [15] and local search [40]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Funda\u00e7\u00e3o Carlos Chagas Filho de Amparo \u00e0 Pesquisa do Estado do Rio de Janeiro FAPERJ (SEI-260003/000709/2023), the S\u00e3o Paulo Research Foundation FAPESP (2023/00815-6), the Conselho Nacional de Desenvolvimento Cient\u00edfico e Tecnol\u00f3gico CNPq (404336/2023-0), and the Silicon Valley Community Foundation through the University Blockchain Research Initiative (Grant #2022-199610). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. Axler. Measure, Integration &; Real Analysis. Springer International Publishing, 2020. ISBN 9783030331436. doi: 10.1007/978-3-030-33143-6.   \n[2] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. Journal of Marchine Learning Research, 2018.   \n[3] E. Bengio, M. Jain, M. Korablyov, D. Precup, and Y. Bengio. Flow network based generative models for non-iterative diverse candidate generation. In NeurIPS (NeurIPS), 2021.   \n[4] Y. Bengio, S. Lahlou, T. Deleu, E. J. Hu, M. Tiwari, and E. Bengio. Gflownet foundations. Journal of Machine Learning Research (JMLR), 2023.   \n[5] M. Betancourt. A conceptual introduction to hamiltonian monte carlo. arXiv preprint arXiv:1701.02434, 2017.   \n[6] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2007.   \n[7] D. M. Blei and et al. Variational inference: A review for statisticians. Journal of the American Statistical Association, 2017.   \n[8] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018.   \n[9] L. Buesing, N. Heess, and T. Weber. Approximate inference in discrete distributions with monte carlo tree search and value functions. In AISTATS, pages 624\u2013634. PMLR, 2020.   \n[10] Y. Burda, R. B. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. In ICLR (Poster), 2016.   \n[11] P. Carbonetto, M. King, and F. Hamze. A stochastic approximation method for inference in probabilistic graphical models. NeurIPS, 2009.   \n[12] B. Carpenter, A. Gelman, M. D. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker, J. Guo, P. Li, and A. Riddell. Stan: A probabilistic programming language. Journal of statistical software, 2017.   \n[13] T. da Silva, E. Silva, A. Ribeiro, A. G\u00f3is, D. Heider, S. Kaski, and D. Mesquita. Humanin-the-loop causal discovery under latent confounding using ancestral gflownets. arXiv preprint:2309.12032, 2023.   \n[14] M. P. Deisenroth and C. E. Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In ICML, Proceedings of Machine Learning Research, pages 465\u2013472. PMLR, 2011.   \n[15] T. Deleu, A. G\u00f3is, C. C. Emezue, M. Rankawat, S. Lacoste-Julien, S. Bauer, and Y. Bengio. Bayesian structure learning with generative flow networks. In UAI, 2022.   \n[16] T. Deleu, M. Nishikawa-Toomey, J. Subramanian, N. Malkin, L. Charlin, and Y. Bengio. Joint Bayesian inference of graphical structure and parameters with a single generative flow network. In Advances in Neural Processing Systems (NeurIPS), 2023.   \n[17] T. Deleu, P. Nouri, N. Malkin, D. Precup, and Y. Bengio. Discrete probabilistic inference as control in multi-path environments. CoRR, abs/2402.10309, 2024.   \n[18] S. Depeweg, J. M. Hern\u00e1ndez-Lobato, F. Doshi-Velez, and S. Udluft. Learning and policy search in stochastic dynamical systems with bayesian neural networks. arXiv preprint arXiv:1605.07127, 2016.   \n[19] A. B. Dieng, D. Tran, R. Ranganath, J. Paisley, and D. Blei. Variational inference via $\\chi$ upper bound minimization. NeurIPS, 2017.   \n[20] J. Domke. Provable gradient variance guarantees for black-box variational inference. In NeurIPS, pages 328\u2013337, 2019.   \n[21] J. Domke. Provable smoothness guarantees for black-box variational inference. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 2587\u20132596. PMLR, 2020.   \n[22] K. A. Dubey, S. J. Reddi, S. A. Williamson, B. Poczos, A. J. Smola, and E. P. Xing. Variance reduction in stochastic gradient langevin dynamics. In NeurIPS, 2016.   \n[23] S. Dutta, M. Das, and U. Maulik. Towards causality-based explanation of aerial scene classifiers. IEEE Geoscience and Remote Sensing Letters, 2023.   \n[24] J.-P. Falet, H. B. Lee, N. Malkin, C. Sun, D. Secrieru, D. Zhang, G. Lajoie, and Y. Bengio. Delta-ai: Local objectives for amortized inference in sparse graphical models, 2023.   \n[25] J. Felsenstein. Evolutionary trees from DNA sequences: A maximum likelihood approach. Journal of Molecular Evolution, 1981.   \n[26] T. Garipov, S. D. Peuter, G. Yang, V. Garg, S. Kaski, and T. S. Jaakkola. Compositional sculpting of iterative generative processes. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[27] C. J. Geyer. Markov chain monte carlo maximum likelihood. 1991.   \n[28] G. Hinton, N. Srivastava, and K. Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. 2012.   \n[29] E. J. Hu, N. Malkin, M. Jain, K. E. Everett, A. Graikos, and Y. Bengio. Gflownet-em for learning compositional latent variable models. In International Conference on Machine Learning (ICLR), 2023.   \n[30] E. J. Hu, M. Jain, E. Elmoznino, Y. Kaddar, G. Lajoie, Y. Bengio, and N. Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[31] Z. Huang and S. Becker. Stochastic gradient langevin dynamics with variance reduction. CoRR, 2021.   \n[32] M. Jain, E. Bengio, A. Hernandez-Garcia, J. Rector-Brooks, B. F. P. Dossou, C. A. Ekbote, J. Fu, T. Zhang, M. Kilgour, D. Zhang, L. Simine, P. Das, and Y. Bengio. Biological sequence design with GFlowNets. In International Conference on Machine Learning (ICML), 2022.   \n[33] M. Jain, T. Deleu, J. Hartford, C.-H. Liu, A. Hernandez-Garcia, and Y. Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2023.   \n[34] H. Jang, M. Kim, and S. Ahn. Learning energy decompositions for partial inference in GFlownets. In The Twelfth International Conference on Learning Representations, 2024.   \n[35] M. J\u00e4rvenp\u00e4\u00e4 and J. Corander. On predictive inference for intractable models via approximate bayesian computation. Statistics and Computing, 33(2), Feb. 2023. ISSN 1573-1375.   \n[36] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Mach. Learn., 37(2):183\u2013233, 1999.   \n[37] T. H. Jukes and C. R. Cantor. Evolution of protein molecules. In Mammalian Protein Metabolism. Elsevier, 1969.   \n[38] H. J. Kappen, V. G\u00f3mez, and M. Opper. Optimal control as a graphical model inference problem. Mach. Learn., 87(2):159\u2013182, 2012.   \n[39] K. Kim, Y. Ma, and J. Gardner. Linear convergence of black-box variational inference: Should we stick the landing? In AISTATS, volume 238 of Proceedings of Machine Learning Research, pages 235\u2013243. PMLR, 2024.   \n[40] M. Kim, T. Yun, E. Bengio, D. Zhang, Y. Bengio, S. Ahn, and J. Park. Local search gflownets. arXiv preprint arXiv:2310.02710, 2023.   \n[41] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[42] D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling. Semi-supervised learning with deep generative models. NeurIPS, 2014.   \n[43] D. P. Kingma, T. Salimans, B. Poole, and J. Ho. Variational diffusion models, 2023.   \n[44] Y. Kinoshita and T. Suzuki. Improved convergence rate of stochastic gradient langevin dynamics with variance reduction and its application to optimization. In NeurIPS, 2022.   \n[45] J. Knoblauch, J. Jewson, and T. Damoulas. An optimization-centric view on bayes\u2019 rule: Reviewing and generalizing variational inference. J. Mach. Learn. Res., 23:132:1\u2013132:109, 2022.   \n[46] A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei. Automatic differentiation variational inference. J. Mach. Learn. Res., 18:14:1\u201314:45, 2017.   \n[47] S. Kullback and R. A. Leibler. On Information and Sufficiency. The Annals of Mathematical Statistics, 1951.   \n[48] S. Lahlou, T. Deleu, P. Lemos, D. Zhang, A. Volokhova, A. Hern\u00e1ndez-Garc\u00eda, L. N. Ezzine, Y. Bengio, and N. Malkin. A theory of continuous generative flow networks. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 18269\u201318300. PMLR, 2023.   \n[49] E. Lau, N. M. Vemgal, D. Precup, and E. Bengio. DGFN: Double generative flow networks. In NeurIPS 2023 Generative AI and Biology (GenBio) Workshop, 2023.   \n[50] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR, abs/1805.00909, 2018.   \n[51] Y. Li and R. E. Turner. R\u00e9nyi divergence variational inference. NeurIPS, 29, 2016.   \n[52] D. Liu and et al. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[53] R. Liu, J. Regier, N. Tripuraneni, M. I. Jordan, and J. D. McAuliffe. Rao-blackwellized stochastic gradients for discrete distributions. In ICML, volume 97 of Proceedings of Machine Learning Research, pages 4023\u20134031. PMLR, 2019.   \n[54] K. Madan, J. Rector-Brooks, M. Korablyov, E. Bengio, M. Jain, A. C. Nica, T. Bosc, Y. Bengio, and N. Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning, 2022.   \n[55] N. Malkin, M. Jain, E. Bengio, C. Sun, and Y. Bengio. Trajectory balance: Improved credit assignment in GFlownets. In NeurIPS (NeurIPS), 2022.   \n[56] N. Malkin, S. Lahlou, T. Deleu, X. Ji, E. Hu, K. Everett, D. Zhang, and Y. Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR), 2023.   \n[57] D. Mesquita, P. Blomstedt, and S. Kaski. Embarrassingly parallel MCMC using deep invertible transformations. In UAI, 2019.   \n[58] T. Minka. Divergence measures and message passing. Technical report, Research, Microsoft, 2005. URL https://www.seas.harvard.edu/courses/cs281/papers/ minka-divergence.pdf.   \n[59] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21:132:1\u2013132:62, 2020.   \n[60] S. Mohammadpour, E. Bengio, E. Frejinger, and P.-L. Bacon. Maximum entropy gflownets with soft q-learning, 2023.   \n[61] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2011.   \n[62] A. C. Nica, M. Jain, E. Bengio, C.-H. Liu, M. Korablyov, M. M. Bronstein, and Y. Bengio. Evaluating generalization in gflownets for molecule design. In ICLR2022 Machine Learning for Drug Discovery, 2022.   \n[63] A. B. Owen. Monte Carlo theory, methods and examples. 2013.   \n[64] L. Pan, N. Malkin, D. Zhang, and Y. Bengio. Better training of GFlowNets with local credit and incomplete trajectories. In International Conference on Machine Learning (ICML), 2023.   \n[65] L. Pan, N. Malkin, D. Zhang, and Y. Bengio. Better training of gflownets with local credit and incomplete trajectories. arXiv preprint arXiv:2302.01687, 2023.   \n[66] L. Pan, D. Zhang, A. Courville, L. Huang, and Y. Bengio. Generative augmented flow networks. In International Conference on Learning Representations (ICLR), 2023.   \n[67] L. Pan, D. Zhang, M. Jain, L. Huang, and Y. Bengio. Stochastic generative flow networks. In UAI, volume 216 of Proceedings of Machine Learning Research, pages 1628\u20131638. PMLR, 2023.   \n[68] M. Papini, D. Binaghi, G. Canonaco, M. Pirotta, and M. Restelli. Stochastic variance-reduced policy gradient. In International Conference on Machine Learning, 2018.   \n[69] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.   \n[70] B. Poczos and J. Schneider. On the estimation of $\\alpha$ -divergences. In AISTATS. PMLR, 2011.   \n[71] R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In AISTATS, volume 33 of JMLR Workshop and Conference Proceedings, pages 814\u2013822. JMLR.org, 2014.   \n[72] R. Ranganath, D. Tran, J. Altosaar, and D. M. Blei. Operator variational inference. In NeurIPS, pages 496\u2013504, 2016.   \n[73] J. Rector-Brooks, K. Madan, M. Jain, M. Korablyov, C.-H. Liu, S. Chandar, N. Malkin, and Y. Bengio. Thompson sampling for improved exploration in gflownets, 2023.   \n[74] A. R\u00e9nyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. University of California Press, 1961.   \n[75] D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International conference on machine learning. PMLR, 2015.   \n[76] B. Rhodes and M. U. Gutmann. Variational noise-contrastive estimation. In AISTATS, 2019.   \n[77] L. Richter, A. Boustati, N. N\u00fcsken, F. J. R. Ruiz, and \u00d6. D. Akyildiz. Vargrad: A low-variance gradient estimator for variational inference. 2020.   \n[78] C. P. Robert et al. The Bayesian choice: from decision-theoretic foundations to computational implementation, volume 2. Springer, 2007.   \n[79] G. Roeder, Y. Wu, and D. Duvenaud. Sticking the landing: simple, lower-variance gradient estimators for variational inference. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 6928\u20136937, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.   \n[80] T. G. J. Rudner, V. Pong, R. McAllister, Y. Gal, and S. Levine. Outcome-driven reinforcement learning via variational inference. In NeurIPS, pages 13045\u201313058, 2021.   \n[81] F. Ruiz and M. Titsias. A contrastive divergence for combining variational inference and mcmc. In International Conference on Machine Learning, 2019.   \n[82] T. Salimans and D. A. Knowles. On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression, 2014.   \n[83] M. J. Schervish. Theory of statistics. Springer Science & Business Media, 2012.   \n[84] M. W. Shen, E. Bengio, E. Hajiramezanali, A. Loukas, K. Cho, and T. Biancalani. Towards understanding and improving gflownet training. In International Conference on Machine Learning, 2023.   \n[85] J. Shi, Y. Zhou, J. Hwang, M. Titsias, and L. Mackey. Gradient estimation with discrete stein operators. NeurIPS, 35, 2022.   \n[86] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models, 2023.   \n[87] D. Tiapkin, N. Morozov, A. Naumov, and D. Vetrov. Generative flow networks as entropyregularized rl, 2024.   \n[88] E. Todorov. General duality between optimal control and estimation. In CDC, pages 4286\u2013 4292. IEEE, 2008.   \n[89] M. Toussaint and A. J. Storkey. Probabilistic inference for solving discrete and continuous state markov decision processes. In ICML, volume 148 of ACM International Conference Proceeding Series, pages 945\u2013952. ACM, 2006.   \n[90] M.-N. Tran, M. K. Pitt, and R. Kohn. Adaptive metropolis\u2013hastings sampling using reversible dependent mixture proposals. Statistics and Computing, 2016.   \n[91] C. Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of statistical physics, 52:479\u2013487, 1988.   \n[92] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1\u2013305, 2008.   \n[93] X. Wang, T. Geffner, and J. Domke. Joint control variate for faster black-box variational inference. In AISTATS, volume 238 of Proceedings of Machine Learning Research, pages 1639\u20131647. PMLR, 2024.   \n[94] L. Weaver and N. Tao. The optimal reward baseline for gradient-based reinforcement learning. arXiv preprint arXiv:1301.2315, 2013.   \n[95] V. D. Wild, R. Hu, and D. Sejdinovic. Generalized variational inference in function spaces: Gaussian measures meet bayesian deep learning. In NeurIPS, 2022.   \n[96] D. Williams. Probability with Martingales. Cambridge University Press, 1991.   \n[97] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 1992.   \n[98] B. Xu, N. Wang, T. Chen, and M. Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.   \n[99] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? International Conference on Learning Representations (ICLR), 2019.   \n[100] P. Xu, F. Gao, and Q. Gu. An improved convergence analysis of stochastic variance-reduced policy gradient. In UAI, 2020.   \n[101] Z. Yang. Molecular Evolution: A Statistical Approach. Oxford University PressOxford, May 2014. ISBN 9780191782251. doi: 10.1093/acprof:oso/9780199602605.001.0001.   \n[102] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Bethe free energy, kikuchi approximations, and belief propagation algorithms. Advances in neural information processing systems, 13(24), 2001.   \n[103] M. Yin and M. Zhou. Semi-implicit variational inference. In International conference on machine learning. PMLR, 2018.   \n[104] L. Yu and C. Zhang. Semi-implicit variational inference via score matching. arXiv preprint arXiv:2308.10014, 2023.   \n[105] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In NeurIPS, 2017.   \n[106] C. Zhang, B. Shahbaba, and H. Zhao. Hamiltonian monte carlo acceleration using surrogate functions with random bases. Statistics and computing, 2017.   \n[107] D. Zhang, R. T. Chen, N. Malkin, and Y. Bengio. Unifying generative models with gflownets and beyond. ICML Beyond Bayes workshop, 2022.   \n[108] D. Zhang, N. Malkin, Z. Liu, A. Volokhova, A. Courville, and Y. Bengio. Generative flow networks for discrete probabilistic modeling. In International Conference on Machine Learning (ICML), 2022.   \n[109] D. Zhang, H. Dai, N. Malkin, A. Courville, Y. Bengio, and L. Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. In NeurIPS (NeurIPS), 2023.   \n[110] D. Zhang, R. T. Q. Chen, C.-H. Liu, A. Courville, and Y. Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. In The Twelfth International Conference on Learning Representations, 2024.   \n[111] M. Y. Zhou, Z. Yan, E. Layne, N. Malkin, D. Zhang, M. Jain, M. Blanchette, and Y. Bengio. PhyloGFN: Phylogenetic inference with generative flow networks. In The Twelfth International Conference on Learning Representations, 2024.   \n[112] H. Zimmermann, F. Lindsten, J. van de Meent, and C. A. Naesseth. A variational perspective on generative flow networks. Trans. Mach. Learn. Res., 2023, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Generative Flow Networks. GFlowNets [3, 4] were initially proposed as a reinforcement learning algorithm for finding diverse high-valued states in a discrete environment by sampling from a distribution induced by a reward function. Shortly after, they were extended to sample from complex distributions in arbitrary topological spaces [48]. Remarkably, this family of models was successfully applied to problems as diverse as structure learning and causal discovery [13, 15, 16], discrete probabilistic modeling and graphical models [24, 29, 107, 108], combinatorial optimization and stochastic control [109, 110], drug discovery [3, 33, 62], design of biological sequences [32], natural language processing [30], and aerial scene classification [23]. Concomitantly to these advances, there is a growing literature concerned with the development of more efficient training algorithms for GFlowNets [4, 40, 55, 84] \u2014 primarily drawing inspiration from existing techniques in the reinforcement learning literature [60, 65, 66, 87]. In the same spirit, Tiapkin et al. [87] showed it is possible to frame GFlowNets as an entropy-regularized reinforcement learning. In a study closely related to ours, Malkin et al. [56] proved the equivalence between GFlowNets and hierarchical variational inference (HVI) for discrete distributions; however, when training GFlowNets using divergence-based methods from the VI literature, the authors found no improvement relatively to the traditional flow-matching objectives. Thus, extending beyond discrete distributions, this work provides a definitive analysis of training GFlowNets by directly optimizing a set of divergences typically employed in variational inference training, given a clear context and conditions for effective use of divergence objectives for efficient learning procedures applied on GFlowNets models. ", "page_idx": 16}, {"type": "text", "text": "Reinforcement Learning as Inference. Reinforcement Learning (RL) has been studied as a form of probabilistic inference extensively, generating relevant insights in the literature, and alternatively referred to as control as inference. Todorov [88] demonstrates a duality between estimation and optimal control, establishing conditions where estimation algorithms could applied for control problems. Kappen et al. [38] demonstrated that optimal control problems could be framed as inference problems in graphical models, providing a unified perspective for solving control tasks. Levine [50] presents a complete and modern RL formulation, linking with VI in particular. Rudner et al. [80] integrates even further RL with VI methods, demonstrating the conceptual and algorithmic gains of leveraging outcome-driven RL with variational inference to optimize policy distributions. Developing further, Toussaint and Storkey [89] applies approximate probabilistic inference methods to solve Markov Decision Processes (MDPs) with discrete and continuous states. The approach also aligns with model-based RL techniques, such as PILCO, which utilizes probabilistic models to enhance data efficiency in policy search [14]. Recent work by Deleu et al. [17] positions discrete probabilistic inference as a control problem in multi-path environments, highlighting the synergy between control theory and probabilistic modeling in the context of GFlowNets. This body of works relates to the approach presented in this paper, comparing optimization of trajectory balance and flow-matching losses related to sequential decisions modeled by the GFlowNet with $f$ -divergence measures minimization procedures \u2013 related to approximated variational inference and generalized posterior inference [36, 45, 51, 92, 95]. ", "page_idx": 16}, {"type": "text", "text": "Divergence measures and gradient reduction for VI. Approximate inference via variational inference (VI) methods [6, 7, 36, 92] initially relied on message passing and coordinate ascent methods to minimize the KL divergence of an unnormalized distribution and a proposal in a parameterized tractable family of distributions. Despite the initial generality of the optimization perspective, the concrete implementation of algorithms often requires specialized update equations and learning objectives for specific classes of models. On the other hand, the development of algorithms and software for automatic differentiation [2] and stochastic gradient estimators [59] unlocked the potential application of generic gradient-based optimization algorithms in inference and learning tasks for a comprehensive class of models. Seminal works such as Black-Box VI (BBVI) [71], using the REINFORCE/score function estimator, and Automatic Differentiation VI (ADVI) [46], using reparameterization and change-of-variables, demonstrated practical algorithms for Bayesian inference in generic models, including models combining classical statistical modeling with neural networks. Overall, Mohamed et al. [59] explain the development of the main gradient estimators: the score function [11, 71, 97, 103], and the pathwise gradient estimator, also known as the parametrization trick [42, 43, 75]. The vanilla REINFORCE/score function estimator has notoriously high variance [11, 71, 77, 97], which prompted a body of work exploring variance reduction techniques. In the original BBVI proposal, Ranganath et al. [71] explored Rao-Blackwellization, combining iterated conditional expectations and control variates, using the score function estimator (given its zero expectation) as a control variate. Subsequent works have continued to refine these techniques; Liu et al. [53] uses Rao-Blackwellized stochastic gradients for discrete distributions, while Kim et al. [39] and Wang et al. [93] explored joint control variates and provable linear convergence in BBVI. Additionally, Domke [21] and Domke [20] provided smoothness and gradient variance guarantees, further enhancing the robustness of score function estimator for VI methods. Our work demonstrates that effective variance reduction techniques applied to a $f$ -divergence minimization training can significantly enhance the convergence speed and stability of the procedure. In theory and practice, we observed high compatibility between our results of variance-reduced $f$ -divergence GFlowNets training and the body of work of variance-reduced score-function estimators for VI. Furthermore, by showing that these techniques apply to a broad class of models and optimization objectives, including continuous and mixed structured supports, we move GFlowNets\u2019 $f$ -divergence minimization training closer to recent notions of generalized Bayesian inference and generalized VI[45] and variational inference in function spaces [95] \u2013 with the common thread of casting posterior inference as an optimization problem guided by some divergence measure. This generalization can enable applications of GFlowNets to a diverse range of machine learning tasks, enhancing their versatility and practical relevance. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B Detailed description of the generative tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Set generation [3, 34, 65, 66]. $\\boldsymbol{S}$ contains the sets of size up to $N$ with elements extracted from a fixed deposit $\\mathcal{D}$ of size $D\\geq N$ and $s_{o}=\\emptyset$ . For $s\\in S$ with $|s|<N$ , $\\kappa_{f}(s,\\cdot)$ is a counting measure supported at (the $\\sigma$ -algebra induced by) $\\{s\\cup\\{d\\}\\colon d\\in{\\mathcal{D}}\\setminus s\\}$ ; for $|s|=N$ , $\\kappa_{f}(s,\\cdot)=\\delta_{s_{f}}$ . Then, $\\mathcal{X}=\\{s\\in\\mathcal{S}\\colon|s|=N\\}$ . Similarly, $\\kappa_{b}(s,\\cdot)$ \u2019s support is $\\{s\\backslash\\{d\\}:d\\in{\\mathcal{D}}\\}$ for $s\\neq s_{o}$ . We define the unnormalized target\u2019s density by $\\textstyle\\log r(x)=\\sum_{d\\in x}{\\dot{f}}(d)$ for a fixed function $f\\colon{\\mathcal{D}}\\rightarrow\\mathbb{R}$ . We parameterize $p_{F}(s,\\cdot)$ as a deep set [105] and fix $p_{B}(s,\\cdot)$ as a uniform density for $s\\in S$ . ", "page_idx": 18}, {"type": "text", "text": "Autoregressive sequence generation [32, 55]. A sequence $s$ in $\\textstyle{\\mathcal{D}}^{n}$ , for any $K>n$ , is bijectively associated to an element of $\\mathcal{D}\\times[K]$ by $s\\mapsto\\{(s_{m},m)\\colon1\\leq m\\leq n\\}\\cup\\{(\\bot,m)\\colon K\\geq m>n\\};$ ; $\\perp$ is a token denoting the sequence\u2019s end. In this context, we let $\\mathscr{S}\\,\\subset\\,\\mathscr{P}(\\mathscr{D}\\times[N+1])$ be the set of sequences of size up to $N$ , i.e., if $s\\ \\in{\\cal S}$ and $(\\perp,n+1)\\,\\in\\,s$ , then $(d,m)\\,\\in\\,s$ iff $d\\,=\\,\\bot$ for $n<m\\leq N+1$ and there is $v\\in(D\\cup\\{\\bot\\})^{n}$ such that $(v_{m},m)\\in\\,s$ for $m\\leq n$ ; the initial state is $s_{o}\\;=\\;\\emptyset$ . For conciseness, we write $d\\notin\\textbf{\\em s}$ , meaning that $(d,i)\\notin\\textit{s}$ for every $i$ . Next, $\\kappa_{f}(s,\\cdot)$ is the counting measure supported at $\\{s\\cup\\{(d,|s|+1)\\}\\colon d\\in\\mathcal{D}\\cup\\{\\bot\\}\\}$ if $|s|<N$ and $\\perp\\not\\in{\\mathfrak{s}}$ ; at $\\{s\\cup\\{(\\bot,\\bar{N}+1)\\}\\}$ if $|s|=N$ ; and at $\\left\\{s_{f}\\right\\}$ otherwise. Thus, $\\mathcal{X}=\\{s\\in\\mathcal{S}\\colon\\bot\\in s\\}$ . Also, $k_{b}(s,\\cdot)$ is supported at $\\{s\\ \\backslash\\ \\{(d,\\vert s\\vert)\\}:d\\in{\\mathcal{D}}\\}$ , which has a single element corresponding to the removal of the element of $s$ of the largest index. On the other hand, the target\u2019s density is $\\begin{array}{r}{\\log r(x)=\\sum_{(d,i)\\in x:\\ d\\neq\\bot}f(d)g(i)}\\end{array}$ for functions $f,g\\colon D\\to\\mathbb{R}$ . We employ an MLP to parameterize $p_{F}(s,\\cdot)$ . ", "page_idx": 18}, {"type": "text", "text": "Bayesian phylogenetic inference (BPI) [111]. A (rooted) phylogeny is a complete binary tree with labeled leaves and weighted edges; each leaf corresponds to a biological species, and the edges\u2019 weights are a measurement of evolutionary time. Formally, we let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ be the set of observed biological species and $\\nu_{B}$ be the set of $|\\boldsymbol{\\dot{B}}|+1$ unobserved species. Next, we represent a phylogeny over $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ as a weighted directed tree $G_{B}\\;=\\;(\\mathcal{B}\\cup\\mathcal{V}_{B},\\mathcal{E}_{B},\\omega_{B})$ with edges $E_{B}$ featured with a weight assignment $\\omega_{B}$ ; we denote its root by $r(G_{B})$ . Under these conditions, we define ${\\cal{S}}\\,=\\,\\left\\{\\bigcup_{k=1}^{K}G_{{\\mathcal{F}}_{k}}\\colon\\bigcup_{k}{\\mathcal{F}}_{k}={\\mathcal{B}}\\wedge{\\mathcal{G}}_{{\\mathcal{F}}_{k}}\\right.$ is a tree as the set of forests built upon phylogenetic trees over disjoint subsets of $B;\\bigcup$ represents a disjoint union of non-empty sets and $\\textstyle s_{o}^{-}=\\bigcup_{b\\in B}G_{\\{b\\}}$ is the forest composed  of singleton trees $G_{\\{b\\}}$ . Also, we define the amalgamation of phylogenies $G_{\\mathcal{F}_{k}}$ and $G_{\\mathcal{F}_{j}}$ , $A(G_{\\mathcal{F}_{k}},G_{\\mathcal{F}_{j}})$ , as the tree obtained by joining their roots $r(G_{\\mathcal{F}_{k}})$ and $r(G_{\\mathcal{F}_{j}})$ to a new node $r(G_{\\mathcal{F}_{k}}\\cup G_{\\mathcal{F}_{j}})$ , with a corresponding extension of the edges\u2019 weights. In contrast, the dissolution of a tree $G_{\\mathcal{F}}$ , $\\mathcal{R}(G_{\\mathcal{F}})$ , returns the union of the two subtrees obtained by removing $r(G_{\\mathcal{F}})$ from $G_{\\mathcal{F}}$ . Then, $\\kappa_{f}(s,\\cdot)$ is the counting measure supported at $\\left\\{\\bigcup_{k=1,k\\neq i,j}^{K}G_{\\mathcal{F}_{k}}\\cup\\mathcal{A}(G_{\\mathcal{F}_{i}},G_{\\mathcal{F}_{j}})\\colon(i,j)\\in[K]^{2},\\,i\\neq j\\right\\}$ with $\\textstyle s\\,=\\,\\bigcup_{k=1}^{K}G_{{\\mathcal{F}}_{k}}$ and $K\\,\\geq\\,2$ ; if $s=\\mathcal{G}_{B}$ , $\\kappa_{f}(s,\\cdot)=\\delta_{s_{f}}$ . Hence, $\\mathcal{X}$ is the set of phylogenies over $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . Likewise, $\\kappa_{b}(s,\\cdot)$ \u2019s support is $\\left\\{\\bigcup_{k=1,k\\neq j}^{K}G_{\\mathcal{F}_{k}}\\cup\\mathcal{R}(G_{\\mathcal{F}_{i}})\\colon i\\in[K]\\wedge r(G_{\\mathcal{F}_{i}})\\not\\in B\\right\\}$ df obr $\\begin{array}{r}{s=\\bigcup_{k=1}^{K}G_{\\mathcal{F}_{k}}}\\end{array}$ o na nmd $K\\leq|\\beta|$ . gFivineanl lay ,d tahtae set of genome sequences of the species in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . More specifically, we let the prior be a uniform distribution and compute the model-induced likelihood function by the efficient Felsenstein\u2019s algorithm [25]. We assume the edges\u2019 weights are constant. See [101] for further details. We parameterize $p_{F}(s,\\cdot)$ with GIN [99] and fix $p_{B}(s,\\cdot)$ as an uniform distribution. ", "page_idx": 18}, {"type": "text", "text": "Mixture of Gaussians [48, 110]. The training of GFlowNets in continuous spaces is challenging, and the problem of designing highly expressive models in this setting is still unaddressed [16, 48]. However, as we show in Section 5.2, divergence-based measures seem to be very effective learning objectives for autoregressive sampling of a sparse mixture of Gaussians. For a $d$ -dimensional Gaussian distribution, $S=\\{\\{(0,0),(x_{i},i)\\colon1\\leq i\\leq n\\},\\colon n\\leq d,\\,x\\in\\mathbb{R}^{n}\\}\\subset\\{(0,0)\\}\\cup\\mathcal{P}(\\mathbb{R}\\times[d])$ and $s_{o}\\,=\\,(0,0)$ ; note $\\boldsymbol{S}$ is isomorphic to $\\mathbb{R}^{d}$ . Also, for $s\\,=\\,\\{(x_{i},i)\\}_{i=1}^{n}$ , $\\kappa_{f}(s,\\cdot)$ is Lebesgue\u2019s measure at $\\{s\\cup(x,n+1)\\colon x\\in\\mathbb{R}\\}$ if $n\\,<\\,d$ and $\\kappa_{f}(s,\\cdot)\\,=\\,\\delta_{s_{f}}$ otherwise. In particular, $\\chi\\,=$ $\\{s\\,\\in\\,{\\mathcal{S}}\\colon\\,\\operatorname*{max}_{(x,i)\\in s}i\\,=\\,d\\}$ . Moreover, $\\kappa_{b}(s,\\cdot)$ is a measure on $\\{s\\setminus(x,|s|)\\colon x\\in\\mathbb{R}\\}$ , which is isomorphic to $\\mathbb{R}$ , which is a singleton. We define the target\u2019s density with a homogeneous mixture of Gaussian distributions, $\\textstyle{\\frac{1}{K}}\\sum_{i=1}^{K}{\\mathcal{N}}(\\mu_{i},\\sigma^{2}I)$ with $\\mu_{i}\\in\\mathbb{R}^{d}$ . We similarly define $P_{F}(s,\\cdot)$ as a mixture of one-dimensional Gaussians with mean and variance learned via an MLP [48]. ", "page_idx": 18}, {"type": "text", "text": "Banana distribution. [57, 76] We consider sampling from the banana distribution, defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\nN\\left(\\left[\\!\\!{\\begin{array}{c}{x_{1}}\\\\ {x_{2}+x_{1}^{2}+1}\\end{array}}\\!\\!\\right]\\left|\\,\\left[\\!\\!{\\begin{array}{c}{0}\\\\ {0}\\end{array}}\\!\\!\\right],\\left[\\!\\!{\\begin{array}{c c}{1}&{0.9}\\\\ {0.9}&{1}\\end{array}}\\!\\!\\right]\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given its geometry and shape, this distribution is a common baseline in the approximate Bayesian inference literature [90, 104, 106]. This task is identical to sampling from a mixture of Gaussian distributions, except for the different target density specified by the model in Equation (8). Also, we rely on the implementation Hamiltonian Monte Carlo (HMC) [5, 61] provided by Stan [12] to obtain accurate samples from (8). ", "page_idx": 19}, {"type": "text", "text": "A similar description may be utilized for the hypergrid and structure learning domains. ", "page_idx": 19}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will consider the measurable space of trajectories $(\\mathcal{P}_{S},\\Sigma_{P})$ , with $\\mathcal{P}_{S}=\\{(s,s_{1},\\ldots,s_{n},s_{f})\\in$ $S^{n+1}\\times\\{s_{f}\\}:0\\leq n\\leq N-1\\}$ and $\\Sigma_{P}$ as the $\\sigma$ -algebra generated by $\\textstyle{\\bigcup_{n=1}^{N+1}\\Sigma^{\\otimes n}}$ . For notational convenience, we use the same letters for representing the measures and  kernels of $(S,\\Sigma)$ and their natural product counterparts in $(\\mathcal{P}_{S},\\Sigma_{P})$ , which exist by Carath\u00e9odory extension\u2019s theorem [96]; for example, $\\nu(B)\\,=\\,\\bar{\\nu}^{\\otimes n}(B)$ for $B\\,=\\,(B_{1},\\ldots,B_{n})\\,\\in\\,\\Sigma^{\\otimes n}$ and $p_{F_{\\theta}}(\\tau|s_{o};\\theta)$ is the density of $P_{F}^{\\otimes n+1}(s_{o},\\cdot)$ for $\\tau=(s_{o},s_{1},\\ldots,s_{n},s_{f})$ relatively to $\\mu^{\\otimes n}$ . In this case, we will write $\\tau$ for a generic element of $\\mathcal{P}_{S}$ and $x$ for its terminal state (which is unique by Definition 1). ", "page_idx": 20}, {"type": "text", "text": "C.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will show that the gradient of the expected on-policy TB loss matches the gradient of the KL divergence between the forward and backward policies. Firstly, note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}]=\\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim P_{F}(s_{\\sigma},\\cdot)}\\left[\\log\\frac{p_{F}(T|s_{\\sigma},\\cdot)}{p_{B}(\\tau)}\\right]}\\\\ &{\\phantom{\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}]}=\\nabla_{\\theta}\\int_{\\tau}\\log\\frac{p_{F}(\\tau|s_{\\sigma};\\theta)}{p_{B}(\\tau)}\\mathrm{d}P_{F}(s_{\\sigma},\\mathrm{d}\\tau)}\\\\ &{\\phantom{\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||s_{\\sigma},\\cdot\\theta]}=\\nabla_{\\theta}\\int_{\\tau}\\log\\frac{p_{F}(\\tau|s_{\\sigma};\\theta)}{p_{B}(\\tau)}p_{F}(\\tau|s_{\\sigma};\\theta)\\mathrm{d}\\kappa_{f}(s_{\\sigma},\\mathrm{d}\\tau)}\\\\ &{\\phantom{\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||s_{\\sigma},\\cdot\\theta]}=\\int_{\\tau}\\nabla_{\\theta}\\log\\frac{p_{F}(\\tau|s_{\\sigma};\\theta)}{p_{B}(\\tau)}P_{F}(s_{\\sigma},\\mathrm{d}\\tau)}\\\\ &{\\phantom{\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||s_{\\sigma},\\cdot\\theta]}+\\int_{\\tau}\\log\\frac{p_{F}(\\tau|s_{\\sigma};\\theta)}{p_{B}(\\tau)}\\nabla_{\\theta}p_{F}(\\tau|s_{\\sigma};\\theta)\\mathrm{d}\\kappa_{f}(s_{\\sigma},\\mathrm{d}\\tau)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by Leibniz\u2019s rule for integrals and the product rule for derivatives. Then, since $\\nabla_{\\theta}f(\\theta)\\;\\;=$ $f(\\theta)\\nabla\\log f(\\theta)$ for any differentiable function $f\\colon\\theta\\mapsto f(\\theta)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\theta}\\mathcal{D}_{K L}\\left[P_{F}||P_{B}\\right]}\\\\ {=\\underset{\\tau\\sim P_{F}(s_{o},\\cdot)}{\\mathbb{E}}\\left[\\nabla_{\\theta}\\log p_{F}(\\tau|s_{o})+\\log\\frac{p_{F}(\\tau|s_{o})}{p_{B}(\\tau)}\\nabla_{\\theta}\\log p_{F}(\\tau|s_{o})\\right]}\\\\ {=\\underset{\\tau\\sim P_{F}(s_{o},\\cdot)}{\\mathbb{E}}\\left[\\log\\frac{p_{F}(\\tau|s_{o})}{p_{B}(\\tau)}\\nabla_{\\theta}\\log p_{F}(\\tau|s_{o})\\right];}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we omitted the dependency of $P_{F}$ (and of $p_{F}$ thereof) on the parameters $\\theta$ for conciseness. On the other hand, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{T B}(\\tau;\\boldsymbol{\\theta})=2\\left(\\log\\frac{p_{F}(\\tau|s_{o};\\boldsymbol{\\theta})}{p_{B}(\\tau)}\\right)\\nabla_{\\boldsymbol{\\theta}}\\log p_{F}(\\tau)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by the chain rule for derivatives. Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\nabla_{\\theta}\\mathcal{L}_{T B}(\\tau;\\theta)=2\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "ensuring that the equivalence between $\\mathcal{L}_{T B}$ and $\\mathcal{D}_{K L}$ in terms of expected gradients holds in a context broader than that of finitely supported distributions [56]. ", "page_idx": 20}, {"type": "text", "text": "C.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Henceforth, we will recurrently refer to the score estimator for gradients of expectations [97], namely, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\underset{\\tau\\sim P_{F}(s_{o},\\cdot)}{\\mathbb{E}}\\left[f_{\\theta}(\\tau)\\right]=\\underset{\\tau\\sim P_{F}(s_{o},\\cdot)}{\\mathbb{E}}\\left[\\nabla_{\\theta}f_{\\theta}(\\tau)+f_{\\theta}(\\tau)\\nabla_{\\theta}\\log p_{F}(\\tau|s_{o};\\theta)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which can be derived using the arguments of the preceding section. In this context, the Renyi- $\\alpha$ \u2019s divergence satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}R_{\\alpha}(P_{F}||P_{B})=\\frac{\\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}[g(\\tau,\\theta)]}{(\\alpha-1)\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}g(\\tau,\\theta)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $g(\\tau;\\theta)={\\left(p_{B}(\\tau|x)r(x)\\middle/\\vphantom{\\left(p_{B}(\\tau|s_{o};\\theta)\\right)^{1-\\alpha}}\\right.\\right.}$ and $\\alpha\\neq1$ ; similarly, the Tsallis- $\\alpha$ \u2019s divergence abides by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}T_{\\alpha}(P_{F}||P_{B})=\\frac{1}{(\\alpha-1)}\\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}[g(\\tau,\\theta)].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The statement then follows by substituting $\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[g(\\tau,\\boldsymbol{\\theta})\\right]$ with the corresponding score estimator given by Equation (12). ", "page_idx": 21}, {"type": "text", "text": "C.3 Proof of Lemma 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Forward KL divergence. The gradient of $\\mathcal{D}_{K L}[P_{B}||P_{F}]$ is straightforwardly obtained through the application of Leibniz\u2019s rule for integrals, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{B}||P_{F}]=-\\mathbb{E}_{\\tau\\sim P_{B}(s_{f},\\cdot)}\\left[\\nabla_{\\theta}\\log p_{F}(\\tau|s_{o};\\theta)\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since the averaging distribution $P_{B}$ do not depend on the varying parameters $\\theta$ . However, as we compute Monte Carlo averages over samples of $P_{F}$ , we apply an importance reweighting scheme [63, Chapter 9] to the previous expectation to infer that, up to a positive multiplicative constant, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{B}||P_{F}]\\stackrel{C}{=}-\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\frac{p_{B}(\\tau|x)r(x)}{p_{F}(\\tau|s_{o};\\theta)}\\nabla_{\\theta}\\log p_{F}(\\tau|s_{o};\\theta)\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with= denoting equality up to a positive multiplicative constant. We emphasize that most modern stochastic gradient methods for optimization, such as Adam [41] and RMSProp [28], remain unchanged when we multiply the estimated gradients by a fixed quantity; thus, we may harmlessly compute gradients up to multiplicative constants. ", "page_idx": 21}, {"type": "text", "text": "Reverse KL divergence. We verified in Equation (9) that ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}]=\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\log\\frac{p_{F}(\\tau|s_{o})}{p_{B}(\\tau)}\\nabla_{\\theta}s_{\\theta}(\\tau)\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $p_{B}(\\tau)=p_{B}(\\tau|x)r(x)/Z$ and $\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\nabla_{\\theta}s_{\\theta}(\\tau)=0$ , the quantity $\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}]$ may be rewritten as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathcal{D}_{K L}[P_{F}||P_{B}]=\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\log\\frac{p_{F}\\left(\\tau|s_{o}\\right)}{p_{B}(\\tau)}\\nabla_{\\theta}s_{\\theta}(\\tau)\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[(\\log Z)\\nabla_{\\theta}s_{\\theta}(\\tau)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\tau\\sim P_{F}(s_{o},\\cdot)}\\left[\\log\\frac{p_{F}\\left(\\tau|s_{o}\\right)}{p_{B}(\\tau|x)r(x)}\\nabla_{\\theta}s_{\\theta}(\\tau)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in which $x$ is the terminal state corresponding to the trajectory $\\tau$ . Thus proving the statement in Lemma 2. ", "page_idx": 21}, {"type": "text", "text": "C.4 Proof of Proposition 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We will derive an expression for the optimal baseline of a vector-valued control variate. For this, let $f$ be the averaged function and $g\\colon\\tau\\mapsto g(\\tau)$ be the control variate. Assume, without loss of generality, that $\\mathbb{E}_{\\pi}[g]=0$ for the averaging distribution $\\pi$ over the space of trajectories. In this case, the optimal baseline for the control variate $a^{\\star}$ is found by ", "page_idx": 21}, {"type": "equation", "text": "$$\na^{\\star}=\\underset{a\\in\\mathbb{R}}{\\arg\\operatorname*{min}}\\operatorname{Tr}\\left(\\operatorname{Cov}_{\\tau\\sim\\pi}[f(\\tau)-a\\cdot g(\\tau)]\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\na^{\\star}=\\underset{a\\in\\mathbb{R}}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\mathrm{Tr}\\bigg(-2a\\cdot\\mathsf{C o v}_{\\pi}[f(\\tau),g(\\tau)]+a^{2}\\mathsf{C o v}_{\\pi}(g(\\tau))\\bigg),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is a convex optimization problem solved by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{a^{\\star}=\\frac{\\mathrm{Tr}\\big(\\mathrm{Cov}_{\\pi}[f(\\tau),g(\\tau)]\\big)}{\\mathrm{Tr}\\big(\\mathrm{Cov}_{\\pi}[g(\\tau)]\\big)}}\\\\ &{\\quad=\\frac{\\mathrm{Tr}\\,\\mathbb{E}_{\\pi}\\big[\\!(f-\\mathbb{E}_{\\pi}[f(\\tau)])g(\\tau)^{T}\\big]}{\\mathrm{Tr}\\,\\mathbb{E}_{\\pi}\\big[g(\\tau)g(\\tau)^{T}\\big]}}\\\\ &{\\quad=\\frac{\\mathbb{E}_{\\pi}\\big[\\mathrm{Tr}\\big(f-\\mathbb{E}_{\\pi}[f(\\tau)]\\big)^{T}g(\\tau)\\big]}{\\mathbb{E}_{\\pi}\\big[\\mathrm{Tr}\\,g(\\tau)^{T}g(\\tau)\\big]}}\\\\ &{\\quad=\\frac{\\mathbb{E}_{\\pi}\\big[\\big(f-\\mathbb{E}_{\\pi}[f(\\tau)]\\big)^{T}g(\\tau)\\big]}{\\mathbb{E}_{\\pi}\\big[g(\\tau)^{T}g(\\tau)\\big]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in which we used the circular property of the trace. This equation exactly matches the result in Proposition 2. In practice, we use $g(\\tau)=\\nabla_{\\theta}\\log p_{F}(\\tau)$ for both the reverse KL- and $\\alpha$ -divergences, rendering a baseline $a^{\\star}$ that depends non-linearly on the sample gradients and is hence difficult to compute in a GPU-powered autodiff framework efficiently. We thus use Equation (6) to estimate $a^{\\star}$ . ", "page_idx": 21}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/6a68f96f49baf9d89ed9c0a0abd782ec71edee04edc027f48888d8364549ca1a.jpg", "img_caption": ["Figure 7: Learning curves for a GFlowNet trained by minimizing the TB loss. The curves\u2019 smoothness highlights the low variance of the optimization steps incurred by the stochastic gradients of $\\mathcal{L}_{T B}$ , which do not use a score function estimator. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/d0c9d8306f2083e503d3654bdc762fb74e44f7e38c34887e8c27277f46e8a8ab.jpg", "img_caption": ["Figure 9: Additional illustration of the effect of $\\alpha$ when learning GFlowNets by minimizing the Renyi- $\\alpha$ divergence in the hypergrid environment. For such a sparse target distribution, a large and negative value of $\\alpha$ (left) is beneficial to ensure that all modes are appropriately covered by the learned distribution. In contrast, the mode-seeking behavior induced by a large value of $\\alpha$ entails the collapse of the model in a single mode (right). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Gradient variance for flow-network-based objectives. Figure 7 shows the learning curve for the TB loss in each of the generative tasks. Notoriously, it underlines the low variance of the optimization steps \u2014 which, contrarily to their divergence-based counterparts, do not rely on a score function estimator \u2014 and suggests that the design of control variates for estimating the gradients of these objectives would not be significantly helpful. Also, the gradient of $\\mathcal{L}_{T B}$ depends non-linearly on the score function $\\log{p_{F}}$ and, consequently, it is unclear how to implement computationally efficient variance reduction techniques in this case. ", "page_idx": 22}, {"type": "text", "text": "Forward KL for sequence generation. Figure 3 shows that alternative approaches in terms of convergence speed outperformed a GFlowNet trained to minimize the forward KL. One possible cause of this underperformance is the high variance induced by the underlying importance sampling estimator. To verify this, we re-run the corresponding experiments, increasing the size of the batch of trajectories for the forward KL estimator to 1024. Figure 8 presents the experiment\u2019s results, with an increased batch size corresponding to an estimator of smaller variance that accelerates the GFlowNet\u2019s training convergence. More broadly, this suggests that the design of GFlowNet-specific variance reduction techniques, which we leave to future endeavors, may further improve this family of models. ", "page_idx": 22}, {"type": "image", "img_path": "N5H4z0Pzvn/tmp/5a50bf7e70d5b02594f347ab9d44d08373b4fc004262af9fef07de57bbf447fe.jpg", "img_caption": ["Figure 8: Results for sequence generation with larger batches. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Influence of $\\alpha$ on the learning of GFlowNets. As mentioned earlier, divergence-based measures fall short compared to their balance-based counterparts for the hypergrid navigation task. For this extremely sparse problem, the benefits from off-policy exploration during training seem to supersede the statistical efficiency enacted by the minimization of divergences, which fail to properly cover the high-probability regions of the target distribution. In this scenario, Figure 9 suggests that this issue can be mildly circumvented by choosing a sufficiently negative $\\alpha$ for the Renyi- $\\alpha$ divergences, thereby imposing a mass-covering behavior to the learned model. However, these results should be substantiated by further experimental analysis to be conclusive. Currently, we would suggest a practitioner to stick to the balance-based objectives when dealing with very sparse target distributions. ", "page_idx": 22}, {"type": "text", "text": "E Experimental details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The following paragraph provides further implementation details. Regarding open access to the code, we will make the code public upon acceptance. ", "page_idx": 23}, {"type": "text", "text": "Shared configurations. For every generative task, we used the Adam optimizer [41] to carry out the stochastic optimization, employing a learning rate of $10^{-1}$ for $\\log{Z_{\\theta}}$ when minimizing $\\mathcal{L}_{T B}$ and $10^{-3}$ for the remaining parameters, following previous works [48, 55, 56, 66]. We polynomially annealed the learning rate towards 0 along training, similarly to [86]. Also, we use LeakyReLU [98] as the non-linear activation function of all implemented neural networks. ", "page_idx": 23}, {"type": "text", "text": "Set generation. We implement an MLP of 2 64-dimensional layers to parameterize the policy\u2019s logits $\\log p_{F}(s,\\cdot)$ . We train the model for 512 epochs with a batch of 128 trajectories for estimating the gradients. Also, we let $D=32$ and $N=16$ be the source\u2019s and set\u2019s sizes, respectively. ", "page_idx": 23}, {"type": "text", "text": "Autoregressive sequence generation. We parameterize the logits of the forward policy with a MLP of 2 64-dimensional layers; we pad the sequences to account for their variable sizes. We respectively consider $D=8$ and $N=6$ for the source\u2019s and sequence\u2019s sizes. To approximate the gradients, we rely on a batch of 128 sequences. ", "page_idx": 23}, {"type": "text", "text": "Bayesian phylogenetic inference. We parameterize the logits of the forward policy with a 2-layer GIN [99] with a 64-dimensional latent embedding, which is linearly projected to $\\log p_{F}$ . Moreover, we simulated the JC69 model [37] to obtain 25-sized sequences of nucleotides for each of the 7 observed species, setting $\\lambda=0.3$ for the instantaneous mutation rate; see [101] for an introduction to computational phylogenetics and molecular evolution. To estimate the gradients, we relied on batches of 64 trajectories. ", "page_idx": 23}, {"type": "text", "text": "Hypergrid navigation. We consider a $H=12$ for Figure 3 and Figure 4 and $H=9$ for Figure 9. In both cases, $d=2$ and $R_{o}=10^{-3}$ ; see [55, Section 5.1]. To parameterize the policy, we used a 2-layer 256-dimensional MLP with ReLU activations. We trained the models for 10000 epochs. ", "page_idx": 23}, {"type": "text", "text": "Bayesian structure learning. We simulated a 100-sized 5-variable data set $\\mathbf{X}$ from a randomly parameterized linear Gaussian structural model based on a fixed Bayesian network. We implemented a 2-layer 256-dimensional MLP with ReLU activations for the policy network, which received the flattened adjacency matrix of the current state as input. Training lasted for 4000 epochs. ", "page_idx": 23}, {"type": "text", "text": "Mixture of Gaussian distributions. We consider a mixture of 9 2-dimensional Gaussian distributions centered at $\\mu_{i j}\\mathrm{~}=(i,j)$ for $0\\leq i,j\\leq2$ , each of which having an isotropic variance of $10^{-1}$ ; see Figure 1. We use an MLP of 2 64-dimensional layers to parameterize the forward policy. ", "page_idx": 23}, {"type": "text", "text": "Banana-shaped distribution. The model is specified by Equation (8). We also consider an MLP of 2 64-dimensional layers to parameterize the forward policy. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 24}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: the proofs for the main claims in Proposition 1, Lemma 1, Proposition 2, Lemma 2 are presented in Appendix C, and practical training of GFN empirical validation in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The middle paragraph in Section 6 discusses limitations, including the potential impact of different $\\alpha$ values and the need for further exploration in this area. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The statements of the propositions and lemma include the assumptions, as well as in the notations and definitions presented in Section 4, and proofs provided in Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Details are provided in Section 5, Appendix B and Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submis  \nsions to provide some reasonable avenue for reproducibility, which may depend on the   \nnature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Provided in a zip file during the review period, with the plan for public release once the paper is made public. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \"No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 5 in the main text presents information about the experiments with full details provided in Appendix B and Appendix E, and supplemented zip-file with the code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Plots count on error bars and tables count on standard deviation. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: First paragraph of Appendix E. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our submission follow the NeurIPS ethical guidelines. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide a perspective on broader impacts in Section 6, but do not foresee any direct negative societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not foresee any direct risk stemming from our work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: All code was made by the authors ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 29}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No experiments with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No experiments with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]