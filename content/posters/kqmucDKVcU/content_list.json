[{"type": "text", "text": "Optimal Flow Matching: Learning Straight Trajectories in Just One Step ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nikita Kornilov ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Petr Mokrov ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Skolkovo Institute of Science and Technology R.Center for AI, Innopolis University ", "page_idx": 0}, {"type": "text", "text": "Skolkovo Institute of Science and Technology petr.mokrov@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "kornilov.nm@phystech.edu ", "page_idx": 0}, {"type": "text", "text": "Alexander Gasnikov ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Korotin ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Innopolis University Moscow Institute of Physics and Technology Steklov Mathematical Institute of $\\mathrm{\\mathbf{RAS^{*}}}$ gasnikov@yandex.ru ", "page_idx": 0}, {"type": "text", "text": "Skolkovo Institute of Science and Technology Artificial Intelligence Research Institute a.korotin@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the several recent years, there has been a boom in development of Flow Matching (FM) methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the Optimal Transport (OT) displacements. Straightness is crucial for the fast integration (inference) of the learned flow\u2019s paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative FM procedures which accumulate the error during training or exploit heuristics based on minibatch OT. To address these issues, we develop and theoretically justify the novel Optimal Flow Matching (OFM) approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step. The main idea of our approach is the employment of vector field for FM which are parameterized by convex functions. The code of our OFM implementation and the conducted experiments is available at https://github.com/Jhomanik/Optimal-Flow-Matching. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent success in generative modeling [41, 17, 9] is mostly driven by Flow Matching (FM) [38] models. These models move a known distribution to a target one via ordinary differential equations (ODE) describing the mass movement. However, such processes usually have curved trajectories, resulting in time-consuming ODE integration for sampling. To overcome this issue, researches developed several improvements of the FM [39, 40, 48], which aim to recover more straight paths. ", "page_idx": 0}, {"type": "text", "text": "Rectified Flow (RF) method [39, 40] iteratively solves FM and gradually rectifies trajectories. Unfortunately, in each FM iteration, it accumulates the error, see [40, $\\S2.2]$ and [39, $\\S6]$ . This may spoil the performance of the method. The other popular branch of approaches to straighten trajectories is based on the connection between straight paths and Optimal Transport (OT) [60]. The main goal of OT is to find the way to move one probability distribution to another with the minimal effort. Such OT maps are usually described by ODEs with straight trajectories. In OT Conditional Flow Matching (OT-CFM) [48, 55], the authors propose to apply FM on top of OT solution between batches from considered distributions. Unfortunately, such a heuristic does not guarantee straight paths because of minibatch OT biases, see, e.g., [55, Figure 1, right] for the practical illustration. ", "page_idx": 0}, {"type": "text", "text": "Contributions. In this paper, we fix the above-mentioned problems of the straightening methods. We propose a novel Optimal Flow Matching (OFM) approach (\u00a73) that after a single FM iteration obtains straight trajectories which can be simulated without ODE solving. It recovers OT flow for the quadratic transport cost function, i.e., it solves the Benamou\u2013Brenier problem (Figure 1). We demonstrate the potential of OFM in the series of experiments and benchmarks $(\\S4)$ . ", "page_idx": 1}, {"type": "text", "text": "The main idea of our OFM is to consider during FM only specific vector fields which yield straight paths by design. These vector fields are the gradients of convex functions, which in practice are parametrized by Input Convex Neural Networks [3]. In OFM, one can optionally use minibatch OT or any other transport plan as the input, and this is completely theoretically justified. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we provide all necessary backgrounds for the theory. First, we recall static $(\\S2.1)$ and dynamic $(\\S2.2)$ formulations of Optimal Transport and solvers (\u00a72.3) for them. Then, we recall Flow Matching (\u00a72.4.1) and flow straightening approaches: OT-CFM (\u00a72.4.2) and Rectified Flow (\u00a72.4.3). ", "page_idx": 1}, {"type": "text", "text": "Notations. For vectors $x,y\\,\\in\\,\\mathbb{R}^{D}$ , we denote the inner product by $\\langle x,y\\rangle$ and the corresponding $\\ell_{2}$ norm by $\\|x\\|:={\\sqrt{\\langle x,x\\rangle}}$ . We use $\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ to refer to the set of absolute continuous probability distributions with the finite second moment. For vector $x\\in\\mathbb{R}^{D}$ and distribution $p\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ , notation $x\\sim p$ means that $x$ is sampled from $p$ . For the push-forward operator, we use symbol #. ", "page_idx": 1}, {"type": "image", "img_path": "kqmucDKVcU/tmp/6aff31af0d82d5f6edc56e2535aba78e2ffb0295a4795fc9aa3ef33ed683a4a0.jpg", "img_caption": ["Figure 1: Our Optimal Flow Matching (OFM). For any initial transport plan $\\pi$ between $p_{0}$ and $p_{1}$ , OFM obtains exactly straight trajectories (in just a single FM loss minimization) which carry out the OT displacement for the quadratic cost function. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2.1 Static Optimal Transport ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Monge\u2019s and Kantorovich\u2019s formulations. Consider two distributions $p_{0},p_{1}\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ and a cost function $c:\\mathbb{R}^{D}\\times\\mathbb{R}^{D}\\to\\mathbb{R}$ . Monge\u2019s Optimal Transport formulation is given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\#p_{0}=p_{1}}\\int_{\\mathbb{R}^{D}}c(x_{0},T(x_{0}))p_{0}(x_{0})d x_{0},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the infimum is taken over measurable functions $T:\\mathbb{R}^{D}\\,\\rightarrow\\,\\mathbb{R}^{D}$ which satisfy the masspreserving constraint $T\\#p_{0}\\,=\\,p_{1}$ . Such functions are called transport maps. If there exists a transport map $T^{*}$ that achieves the infimum, then it is called the optimal transport map. ", "page_idx": 1}, {"type": "text", "text": "Since the optimal transport map $T^{*}$ in Monge\u2019s formulation may not exist, there is Kantorovich\u2019s relaxation for problem (1) which addresses this issue. Consider the set of transport plans $\\Pi(p_{0},p_{1})$ , i.e., the set of joint distributions on $\\mathbb{R}^{D}\\times\\mathbb{R}^{D}$ which marginals are equal to $p_{0}$ and $p_{1}$ , respectively. Kantorovich\u2019s Optimal Transport formulation is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\pi\\in\\Pi(p_{0},p_{1})}\\int_{\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}c(x_{0},x_{1})\\pi(x_{0},x_{1})d x_{0}d x_{1}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "With mild assumptions on $p_{0},p_{1}$ , the infimum is always achieved (possibly not uniquely). An optimal plan $\\pi^{*}\\in\\Pi(p_{0},p_{1})$ is called an optimal transport plan. If optimal $\\pi^{*}$ has the form $[\\mathrm{id},T^{*}]\\#p_{0}$ , then $T^{*}$ is the solution of Monge\u2019s formulation (1). ", "page_idx": 1}, {"type": "text", "text": "Quadratic cost function. In our paper, we mostly consider the quadratic cost function $c(x_{0},x_{1})=$ \u2225x0\u22122x1\u22252. In this case, infimums in both Monge\u2019s and Kantorovich\u2019s OT are always uniquely attained [60, Brenier\u2019s Theorem 2.12]. They are related by $\\pi^{*}=[\\mathrm{id},T^{*}]\\#p_{0}$ . Moreover, the optimal values of (1) and (2) are equal to each other. The square root of the optimal value is called the Wasserstein-2 distance $\\mathbb{W}_{2}(p_{0},p_{1})$ between distributions $p_{0}$ and $p_{1}$ , i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{W}_{2}^{2}(p_{0},p_{1}):=\\underset{\\pi\\in\\Pi(p_{0},p_{1})}{\\operatorname*{min}}\\,\\int\\,\\frac{\\|x_{1}-x_{0}\\|^{2}}{2}\\pi(x_{0},x_{1})d x_{0}d x_{1}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}\\\\ &{}&{\\qquad=\\underset{T\\neq p_{0}=p_{1}\\int\\mathbb{R}^{D}}{\\operatorname*{min}}\\underset{\\mathbb{R}^{D}}{\\int}\\frac{\\|x_{0}-T(x_{0})\\|^{2}}{2}p_{0}(x_{0})d x_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Dual formulation. For the quadratic cost, problem (3) has the equivalent dual form [60]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{W}_{2}^{2}(p_{0},p_{1})}&{=}&{\\displaystyle\\mathrm{CoNST}(p_{0},p_{1})-\\operatorname*{min}_{\\mathrm{convex}}\\psi\\underbrace{\\left[\\int_{\\mathbb{R}^{D}}\\Psi(x_{0})p_{0}(x_{0})d x_{0}+\\int_{\\mathbb{R}^{D}}\\overline{{\\Psi}}(x_{1})p_{1}(x_{1})d x_{1}\\right]}_{=:\\mathcal{L}_{O T}(\\Psi)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the minimum is taken over convex functions $\\Psi(x)\\ :\\ \\mathbb{R}^{D}\\ \\ \\to\\ \\ \\mathbb{R}.$ . Here $\\overline{{\\Psi}}(x_{1})\\;\\;:=\\;\\;$ $\\operatorname*{sup}_{x_{0}\\in\\mathbb{R}^{D}}\\left[\\left\\langle x_{0},x_{1}\\right\\rangle-\\Psi(x_{0})\\right]$ is the convex (Fenchel) conjugate function of $\\Psi$ . It is also convex. The term $\\mathrm{CONST}(p_{0},p_{1})$ does not depend on $\\Psi$ . Therefore, the minimization (3) over transport plans $\\pi$ is equivalent to the minimization of ${\\mathcal{L}}_{O T}(\\Psi)$ from (4) over convex functions $\\Psi$ . Moreover, the optimal transport map $T^{*}$ can be expressed via an optimal $\\Psi^{*}$ (the Brenier potential [60]), namely, ", "page_idx": 2}, {"type": "equation", "text": "$$\nT^{*}=\\nabla\\Psi^{*}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.2 Dynamic Optimal Transport ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In [5], the authors show that the calculation of Optimal Transport map in (3) for the quadratic cost can be equivalently reformulated in a dynamic form. This form operates with a vector fields defining time-dependent mass transport instead of just static transport maps. ", "page_idx": 2}, {"type": "text", "text": "Preliminaries. We consider the fixed time interval [0, 1]. Let $u(t,\\cdot)\\equiv u_{t}(\\cdot):[0,1]\\times\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D}$ be a vector field and $\\{\\{z_{t}\\}_{t\\in[0,1]}\\}$ be the set of random trajectories such that for each trajectory $\\{z_{t}\\}_{t\\in[0,1]}$ the starting point $z_{0}$ is sampled from $p_{0}$ and $z_{t}$ satisfies the differential equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d z_{t}}&{=}&{u_{t}(z_{t})d t,\\quad z_{0}\\sim p_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In other words, the trajectory $\\{z_{t}\\}_{t\\in[0,1]}$ is defined by its initial point $z_{0}\\sim p_{0}$ and goes along the speed vector $u_{t}(z_{t})$ . Under mild assumptions on $u$ , for each initial $z_{0}$ , the trajectory is unique. ", "page_idx": 2}, {"type": "text", "text": "Let $\\phi^{u}(t,\\cdot)\\equiv\\phi_{t}^{u}(\\cdot):[0,1]\\times\\mathbb{R}^{D}\\to\\mathbb{R}^{D}$ denote the flow map, i.e., it is the function that maps the initial $z_{0}$ to its position at moment of time $t$ according to the ODE (6), i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d\\phi_{t}^{u}(z_{0})}&{=}&{u_{t}(\\phi_{t}^{u}(z_{0})),\\quad\\phi_{0}^{u}(z_{0})=z_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If initial points $z_{0}$ of trajectories are distributed according to $p_{0}$ , then (6) defines a distribution $p_{t}$ of $z_{t}$ at time $t$ , which can be expressed via with the push-forward operator, i.e., $p_{t}^{u}:=\\phi_{t}^{u}\\#p_{0}$ . ", "page_idx": 2}, {"type": "text", "text": "Benamou\u2013Brenier problem. Dynamic OT is the following minimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{W}_{2}^{2}(p_{0},p_{1})=\\underset{u}{\\operatorname*{inf}}\\quad}&{\\int_{0}^{1}\\int_{\\mathbb{R}^{D}}\\frac{\\|u_{t}(x_{t})\\|_{2}^{2}}{2}\\underset{:=p_{t}^{u}(x_{t})}{\\underbrace{\\phi_{t}^{u}\\#p_{0}(x_{t})}}d x_{t}d t,}\\\\ {s.t.\\quad}&{\\phi_{1}^{u}\\#p_{0}=p_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In (8), we look for the vector fields $u$ that define the flows which start at $p_{0}$ and end at $p_{1}$ . Among such flows, we seek for the one which has the minimal kinetic energy over the entire time interval. ", "page_idx": 2}, {"type": "text", "text": "There is a connection between the static OT map $T^{*}\\,=\\,\\nabla\\Psi^{*}$ and the dynamic OT solution $\\boldsymbol{u}^{*}$ Namely, for every initial point $z_{0}$ , the vector field $\\boldsymbol{u}^{*}$ defines a linear trajectory $\\{z_{t}\\}_{t\\in[0,1]}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{t}=t\\nabla\\Psi^{*}(z_{0})+(1-t)z_{0},\\quad\\forall t\\in[0,1].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.3 Continuous Optimal Transport Solvers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There exist a variety of continuous OT solvers [21, 52, 54, 43, 19, 14, 59, 15, 34, 51, 40, 36, 35, 13, 18, 57, 2, 55, 23, 44, 4, 20]. For a survey of solvers designed for OT with quadratic cost, see [33]. In this paper, we focus only on the most relevant ones, called the ICNN-based solvers [54, 33, 43, 2]. These solvers directly minimize objective $\\mathcal{L}_{O T}$ from (4) parametrizing a class of convex functions with convex in input neural networks called ICNNs [3] (for more details, see \u201cParametrization of $\\Psi\"$ in $\\S3.2)$ . Solvers details may differ, but the main idea remains the same. To calculate the conjugate function $\\overline{{\\Psi}}(x_{1})$ at the point $x_{1}$ , they solve the convex optimization problem from conjugate definition. Envelope Theorem [1] allows obtaining closed-form formula for the gradient of the loss. ", "page_idx": 2}, {"type": "text", "text": "2.4 Flow Matching Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we recall popular approaches [40, 39, 48] to find fields $u$ which transport a given probability distribution $p_{0}$ to a target $p_{1}$ and their relation to OT. ", "page_idx": 2}, {"type": "text", "text": "2.4.1 Flow Matching (FM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To find such a field, one samples points $x_{0},x_{1}$ from a transport plan $\\pi\\;\\in\\;\\Pi(p_{0},p_{1})$ , e.g., the independent plan $p_{0}\\times p_{1}$ . The vector field $u$ is encouraged to follow the direction $x_{1}-x_{0}$ of the linear interpolation $x_{t}=(1-t)x_{0}+t x_{1}$ at any moment $\\bar{t}\\in[0,1]$ , i.e., one solves: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{u}\\mathcal L_{F M}^{\\pi}(u);=\\!\\int_{0}^{1}\\left\\{\\sum_{\\mathbb R^{D}\\times\\mathbb R^{D}}\\|u_{t}(x_{t})-(x_{1}-x_{0})\\|^{2}\\pi(x_{0},x_{1})d x_{0}d x_{1}\\right\\}\\!\\!d t,x_{t}=(1-t)x_{0}+t x_{1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We denote the solution of (10) and the flow map (7) by $u^{\\pi}$ and $\\phi^{\\pi}$ , respectively. The concept of FM is depicted in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "The intuition of this procedure is as follows: linear interpolation $x_{t}=(1-t)x_{0}+t x_{1}$ is an intuitive way to move $p_{0}$ to $p_{1}$ , but it requires knowing $x_{1}$ . By fitting $u$ with the direction $x_{1}-x_{0}$ , one yields the vector field that can construct this interpolation without any information about $x_{1}$ . ", "page_idx": 3}, {"type": "text", "text": "The set of trajectories $\\left\\{\\left\\{z_{t}\\right\\}_{t\\in[0,1]}\\right\\}$ generated by $u_{t}^{\\pi}$ (with $z_{0}\\sim p_{0}]$ ) has a useful property: the flow map $\\phi_{1}^{\\pi}$ transforms distribution $p_{0}$ to distribution $p_{1}$ for any initial transport plan $\\pi$ . Moreover, marginal distribution $p_{t}\\,=\\,\\phi_{t}^{\\pi}\\#p_{0}$ is equal to the distribution of linear interpolation $x_{t}=(1-t)x_{0}+t x_{1}$ for any $t$ and $x_{0},x_{1}\\sim\\pi$ . This feature is called the marginal preserving property. ", "page_idx": 3}, {"type": "text", "text": "To push point $x_{0}$ according to learned $u$ , one needs to integrate ODE (6) via numerical solvers. The vector fields with straight (or nearly straight) paths incur much smaller time-discretization error and increase effectiveness of computations, which is in high demand for applications. ", "page_idx": 3}, {"type": "image", "img_path": "kqmucDKVcU/tmp/0fb42e9553b3a86498cef61ef6d81057a8bbcd685f5156f455dca51f93516a22.jpg", "img_caption": ["Figure 2: Flow Matching (FM) obtains a vector field $u$ moving $p_{0}$ to $p_{1}$ . FM typically operates with the independent transport plan $\\pi=p_{0}\\times p_{1}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Researchers noticed that some initial plans $\\pi$ can result in more straight paths after FM rather than the standard independent plan $p_{0}\\times p_{1}$ . The two most popular approaches to choose better plans are Optimal Transport Conditional Flow Matching [48, 55] and Rectified Flow [40]. ", "page_idx": 3}, {"type": "text", "text": "2.4.2 Optimal Transport Conditional Flow Matching (OT-CFM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "If one uses the OT plan $\\pi^{*}$ as the initial plan for FM, then it returns the Brenier\u2019s vector field $\\boldsymbol{u}^{*}$ , which generates exactly straight trajectories (9). However, typically, the true OT plan $\\pi^{*}$ is not available. In such a case, in order to achieve some level of straightness in the learned trajectories, a natural idea is to take the initial plan $\\pi$ to be close to the optimal $\\pi^{*}$ . Inspired by this, the authors of OT-CFM [48, 55] take the advantage of minibatch OT plan approximation. Firstly, they independently sample batches of points from $p_{0}$ and $p_{1}$ . Secondly, they join the batches together according to the discrete OT plan between them. The resulting joined batch is then used in FM. The concept of OT-CFM is depicted in Figure 3. ", "page_idx": 3}, {"type": "text", "text": "The main drawback of OT-CFM is that it recovers only biased dynamic OT solution. In order to converge to the true transport plan the batch size should be large [6], while with a growth of batch size computational time increases drastically [56]. In practice, batch sizes that ensure approximation good enough for applications are nearly infeasible to work with. ", "page_idx": 3}, {"type": "image", "img_path": "kqmucDKVcU/tmp/a9714f3906a5167c50dd602319bb523df202657650e465f91148cf5b500625fb.jpg", "img_caption": ["Figure 3: OT-CFM uses minibatch OT plan to obtain more straight trajectories. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.4.3 Rectified Flow (RF) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In [40], the authors propose an iterative approach to refine the plan $\\pi$ , straightening the trajectories more and more with each iteration. Formally, Flow Matching procedure denoted by FM takes the transport plan $\\pi$ as input and returns an optimal flow map via solving (10): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi^{\\pi}:=\\operatorname{FM}(\\pi).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "kqmucDKVcU/tmp/7f317de1ac1e7d3ebb8047a6e19a9adc5ca13a8cbd53f0f732d94eb9fd5c5008.jpg", "img_caption": ["Figure 4: Rectified Flow iteratively applies FM to straighten the trajectories after each step. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "One can iteratively apply FM to the initial transport plan (e.g., the independent plan), gradually rectifying it. Namely, Rectified Flow Algorithm on $K$ -th iteration has the following update rule ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\phi^{K+1}}&{=}&{\\mathbf{F}\\mathbf{M}(\\pi^{K}),\\quad\\pi^{K+1}=[\\mathrm{id},\\phi^{K+1}]\\#p_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi^{K},\\pi^{K}$ denote flow map and transport plan on $K$ -th iteration, respectively. ", "page_idx": 4}, {"type": "text", "text": "With each new FM iteration, the generated trajectories $\\{\\{z_{t}\\}_{t\\in[0,1]}\\}^{K}$ provably become more and more straight, i.e., error in approximation $z_{t}^{K}\\approx(1-t)z_{0}^{K}+t z_{1}^{K},\\forall t\\in[0,1]$ decreases as the number of iterations $K$ grows. The concept of RF is depicted on Figure 4. ", "page_idx": 4}, {"type": "text", "text": "The authors also notice that for any convex cost function $c$ the flow map $\\phi_{1}^{\\pi}$ from Flow Matching yields lower or equal transport cost than initial transport plan $\\pi$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{D}}c(x_{0},\\phi_{1}^{\\pi}(x_{0}))p_{0}(x_{0})d x_{0}\\leq\\int_{\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}c(x_{0},x_{1})\\pi(x_{0},x_{1})d x_{0}d x_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, the transport costs are guaranteed to decrease because the trajectories of FM as solutions of well-defined ODE do not intersect each other, even if the initial lines connecting $x_{0}$ and $x_{1}$ can. With each iteration of RF (12), transport costs for all convex cost functions do not increase, but, for a given cost function, convergence to its own OT plan is not guaranteed. In [39], the authors address this issue and, for any particular convex cost function $c$ , modify Rectified Flow to converge to OT map for $c$ . In this modification, called $c$ -Rectified Flow ( $c$ -RF), the authors slightly change the FM training objective and restrict the optimization domain only to potential vector fields $\\begin{array}{r}{u_{t}\\bar{(}\\cdot)=\\nabla\\overline{{c}}(\\nabla f_{t}(\\cdot)\\bar{)}}\\end{array}$ , where $f_{t}(\\cdot):\\mathbb{R}^{D}\\to\\mathbb{R}$ is an arbitrary time-dependent scalar valued function and $\\overline{c}$ is the convex conjugate of the cost function $c$ . In case of the quadratic cost function, the training objective remains the same, and the vector field $u_{t}$ is set as the simple gradient $\\nabla f_{t}(\\cdot)$ of the scalar valued function $f_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Unfortunately, in practice, with each iteration $(c-)R{\\mathrm{F}}$ accumulates error caused by inexactness from previous iterations, the issue mentioned in [39, $\\S6$ , point 3]. Due to neural approximations, we can not get exact solution of FM (e.g., $\\phi_{1}^{K}\\#p_{0}\\bar{\\neq}\\,p_{1})$ ), and this inexactness only grows with iterations. In addition, training of $(c-)\\mathsf{R F}$ becomes non-simulation free after the first iteration, since to calculate the plan $\\pi^{K+1}=[\\dot{\\mathrm{id}},\\phi^{\\dot{K}+1}]\\#p_{0}$ it has to integrate ODE. ", "page_idx": 4}, {"type": "text", "text": "3 Optimal Flow Matching (OFM) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide the design of our novel Optimal Flow Matching algorithm (1) that fixes main problems of Rectified Flow and OT-CFM approaches described above. In theory, it obtains exactly straight trajectories and recovers the unbiased optimal transport map for the quadratic cost just in one FM iteration with any initial transport plan. Moreover, during inference, our OFM does not require solving ODE to transport points. ", "page_idx": 4}, {"type": "text", "text": "We discuss the theory behind our approach (\u00a73.1), its practical implementation aspects (\u00a73.2) and the relation to prior works (\u00a73.3). All our proofs are located in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3.1 Theory: Deriving the Optimization Loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We want to design a method of moving distribution $p_{0}$ to $p_{1}$ via exactly straight trajectories. Namely, we aim to obtain straight paths from the solution of the dynamic OT (8). Moreover, we want to limit ourselves to just one minimization iteration. Hence, we propose our novel Optimal Flow Matching (OFM) procedure satisfying the above-mentioned conditions. The main idea of our OFM is to minimize the Flow Matching loss (10) not over all possible vector fields $u$ , but only over specific optimal ones, which yield straight paths by construction and include the desired dynamic OT field $\\boldsymbol{u}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Optimal vector fields. We say that a vector field $u^{\\Psi}$ is optimal if it generates linear trajectories $\\{\\bar{\\{z_{t}\\}}_{t\\in[0,1]}\\}$ such that there exist a convex function $\\Psi:\\vec{\\mathbb{R}^{D}}\\rightarrow\\mathbb{R}$ , which for any path $\\{\\bar{z}_{t}\\}_{t\\in[0,1]}$ pushes the initial point $z_{0}$ to the final one as $z_{1}=\\nabla\\Psi(z_{0})$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{z_{t}}&{=}&{(1-t)z_{0}+t\\nabla\\Psi(z_{0}),\\quad t\\in[0,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The function $\\Psi$ defines the ODE ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d z_{t}}&{=}&{(\\nabla\\Psi(z_{0})-z_{0})d t,\\quad z_{t}|_{t=0}=z_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Equation (14) does not provide a closed formula for $u^{\\Psi}$ as it depends on $z_{0}$ . The explicit formula is constructed as follows: for a time $t\\in[0,1]$ and point $x_{t}$ , we can find a trajectory $\\{z_{t}\\}_{t\\in[0,1]}$ s.t. ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{t}=z_{t}=(1-t)z_{0}+t\\nabla\\Psi(z_{0})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and recover the initial point $z_{0}$ . We postpone the solution of this problem to $\\S3.2$ . For now, we define the inverse of flow map (7) as $(\\phi_{t}^{\\Psi})^{-1}(x_{t})\\;:=\\;z_{0}$ and the vector field $u_{t}^{\\Psi}(x_{t})\\;:=\\;\\bar{\\nabla}\\Psi(z_{0})\\;-\\;z_{0}\\;\\,=$ $\\nabla\\Psi((\\phi_{t}^{\\Psi})^{-1}(x_{t}))\\ -\\ (\\mathring\\phi_{t_{-}}^{\\Psi})^{-1}(x_{t})$ , which generates ODE (14), i.e., $d z_{t}\\,=\\,u_{t}^{\\Psi}(z_{t})d t$ . The concept of optimal vector fields is depicted on Figure 5. ", "page_idx": 5}, {"type": "image", "img_path": "kqmucDKVcU/tmp/651c460dafec10f95fc8949b5effbc050f03afae90c03e38e17546fe15697a12.jpg", "img_caption": ["Figure 5: An Optimal Vector Field: a vector field $u^{\\Psi}$ with straight paths is parametrized by a gradient of a convex function $\\Psi$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We highlight that the solution of dynamic OT lies in the class of optimal vector fields, since it generates linear trajectories (9) with the Brenier potential $\\Psi^{*}$ (5). ", "page_idx": 5}, {"type": "text", "text": "Training objective. Our Optimal Flow Matching (OFM) approach is as follows: we restrict the optimization domain of FM (10) with fixed plan $\\pi$ only to the optimal vector fields. We put the formula for the vector field $u_{\\Psi}$ into FM loss from (10) and define our Optimal Flow Matching loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{L}_{O F M}^{\\pi}(\\Psi)}&{:=}&{\\mathcal{L}_{F M}^{\\pi}(u^{\\Psi})\\!=\\!\\displaystyle\\int_{0}^{1}\\!\\left\\{\\!\\sum_{\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}\\!||u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})||^{2}\\pi(x_{0},x_{1})d x_{0}d x_{1}\\!\\right\\}\\!\\!d t,}\\\\ {x_{t}}&{=}&{(1-t)x_{0}+t x_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our Theorem 1 states that OFM solves the dynamic OT via single FM minimization for any initial $\\pi$ . Theorem 1 (OFM and OT connection). Consider two distributions $p_{0},p_{1}\\in\\mathcal{P}_{a c,2}(\\mathbb{R}^{D})$ and any transport plan $\\pi\\in\\Pi(p_{0},p_{1})$ between them. Then, the dual Optimal Transport loss $\\mathcal{L}_{O T}$ (4) and Optimal Flow Matching loss ${\\mathcal{L}}_{O F M}^{\\pi}$ (16) have the same minimizers, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{c o n v e x\\;\\Psi}\\mathcal{L}_{O F M}^{\\pi}(\\Psi)=\\operatorname*{arg\\,min}_{c o n v e x\\;\\Psi}\\mathcal{L}_{O T}(\\Psi).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.2 Practical implementation aspects ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we explain the details of optimization of our Optimal Flow Matching loss (16). ", "page_idx": 5}, {"type": "text", "text": "Parametrization of $\\Psi$ . In practice, we parametrize the class of convex functions with Input Convex Neural Networks (ICNNs) [3] $\\Psi_{\\theta}$ and parameters $\\theta$ . These are scalar-valued neural networks built in such a way that the network is convex in its input. They consist of fully-connected or convolution blocks, some weights of which are set to be non-negative in order to keep the convexity. In addition, activation functions are considered to be only non-decreasing and convex in each input coordinate. These networks are able to support most of the popular training techniques (e.g., gradient descent optimization, dropout, skip connection, etc.). In Appendix B, we discuss the used architectures. ", "page_idx": 5}, {"type": "text", "text": "OFM loss calculation. We provide an explicit formula for gradient of OFM loss (16). ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (Explicit Loss Gradient Formula). The gradient of ${\\mathcal{L}}_{O F M}^{\\pi}$ can be calculated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle z_{0}}&{=\\ N0\\!-\\!\\mathrm{GRAD}\\left\\{(\\phi_{t}^{\\Psi_{\\theta}})^{-1}(x_{t})\\right\\},}\\\\ {\\displaystyle\\frac{i\\mathcal{L}_{O F M}^{\\pi}}{d\\theta}:=\\frac{d}{d\\theta}\\mathbb{E}_{t;x_{0},x_{1}\\sim\\pi}\\!\\left\\langle\\!\\ N0\\!-\\!\\mathrm{GRAD}\\left\\{2\\left(t\\nabla^{2}\\Psi_{\\theta}(z_{0})+(1-t)I\\right)^{-1}\\frac{(x_{0}-z_{0})}{t}\\right\\},\\nabla\\Psi_{\\theta}(z_{0})\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where variables under NO-GRAD remain constants during differentiation. ", "page_idx": 5}, {"type": "text", "text": "Flow map inversion. In order to find the initial point $z_{0}=(\\phi_{t}^{\\Psi})^{-1}(x_{t})$ , we note that (15) ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{t}=(1-t)z_{0}+t\\nabla\\Psi(z_{0})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is equivalent to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla\\left({\\frac{(1-t)}{2}}\\|\\cdot\\|^{2}+t\\Psi(\\cdot)-\\langle x_{t},\\cdot\\rangle\\right)(z_{0})=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The function under gradient operator $\\nabla$ has minimum at the required point $z_{0}$ , since at $z_{0}$ the gradient of it equals 0. If $t<1$ the function is at least $(1-t)$ -strongly convex, and the minimum is unique. The case $t=1$ is negligible in practice, since it has zero probability to appear during training. ", "page_idx": 6}, {"type": "text", "text": "We can reduce the problem of inversion to the following minimization subproblem ", "page_idx": 6}, {"type": "equation", "text": "$$\n(\\phi_{t}^{\\Psi})^{-1}(x_{t})=\\arg\\operatorname*{min}_{z_{0}\\in\\mathbb{R}^{D}}\\left[\\frac{(1-t)}{2}\\|z_{0}\\|^{2}+t\\Psi(z_{0})-\\langle x_{t},z_{0}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Optimization subproblem (17) is at least $(1-t)$ -strongly convex and can be effectively solved for any given point $x_{t}$ (in comparison with typical non-convex optimization tasks). ", "page_idx": 6}, {"type": "text", "text": "Algorithm. The Optimal Flow Matching pseudocode is presented in listing 1. We estimate math expectation over plan $\\pi$ and time $t$ with uniform distribution on $[0,1]$ via unbiased Monte Carlo. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Optimal Flow Matching ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: Initial transport plan $\\pi\\in\\Pi(p_{0},p_{1})$ , number of iterations $K$ , batch size $B$ , optimizer $O p t$ , sub-problem optimizer SubOpt, ICNN $\\Psi_{\\theta}$   \n1: for $k=0,\\ldots,K-1$ do   \n2: Sample batch $\\{(x_{0}^{i},x_{1}^{i})\\}_{i=1}^{B}$ of size $B$ from plan $\\pi$ ;   \n3: Sample times batch $\\{t^{i}\\}_{i=1}^{B}$ of size $B$ from $U[0,1]$ ;   \n4: Calculate linear interpolation $x_{t^{i}}^{i}=(1-t^{i})x_{0}^{i}+t^{i}x_{1}^{i}$ for all $i\\in{\\overline{{1,B}}}$ ;   \n5: Find the initial points $z_{0}^{i}$ via solving the convex problem with $S u b O p t$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{0}^{i}=\\mathrm{NO-GRAD}\\left\\{\\arg\\operatorname*{min}_{z_{0}^{i}}\\left[\\frac{(1-t^{i})}{2}\\|z_{0}^{i}\\|^{2}+t^{i}\\Psi_{\\theta}(z_{0}^{i})-\\langle x_{t^{i}}^{i},z_{0}^{i}\\rangle\\right]\\right\\};\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "6: Calculate loss $\\hat{\\mathcal{L}}_{O F M}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}_{O F M}=\\frac{1}{B}\\sum_{i=1}^{B}\\left\\langle\\mathrm{NO-GRAD}\\left\\{2\\left(t^{i}\\nabla^{2}\\Psi_{\\theta}(z_{0}^{i})+(1-t^{i})I\\right)^{-1}\\frac{(x_{0}^{i}-z_{0}^{i})}{t^{i}}\\right\\},\\nabla\\Psi_{\\theta}(z_{0}^{i})\\right\\rangle;\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "7: Update parameters $\\theta$ via optimizer Opt step with $\\frac{d\\hat{\\mathcal{L}}_{O F M}}{d\\theta}$ ;", "page_idx": 6}, {"type": "text", "text": "8: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.3 Relation to Prior Works ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we compare our Optimal Flow Matching and previous straightening approaches. One unique feature of OFM is that it works only with flows which have straight paths by design and does not require ODE integration to transport points. Other methods may result in non-straight paths during training, and they still have to solve ODE even with near-straight paths. ", "page_idx": 6}, {"type": "text", "text": "OT Solvers [54, 43, 2]. According to Theorem 1, our OFM and dual OT solvers basically minimize the same OT loss (4). However, our OFM actively utilizes the temporal component of the dynamic process. It allows us to pave a novel theoretical bridge between OT and FM. Such a direct connection can lead to the adoption of the strengths of both methods and a deeper understanding of them. ", "page_idx": 6}, {"type": "text", "text": "OT-CFM [48, 55]. Unlike our OFM approach, OT-CFM method retrieves biased OT solution, and the recovery of straight paths is not guaranteed. In OT-CFM, minibatch OT plan appears as a heuristic that helps to get better trajectories in practice. In contrast, usage of any initial transport plan $\\pi$ in our OFM is completely justified in Theorem 1. ", "page_idx": 6}, {"type": "text", "text": "Rectified Flow [40, 39]. In Rectified Flows [40], the authors iteratively apply Flow Matching to refine the obtained trajectories. However, in each iteration, RF accumulates error since one may not learn the exact flow due to neural approximations. In addition, RF does not guarantee convergence to the OT plan for the quadratic cost. The $c$ -Rectified Flow [39] modification can converge to the OT plan for any cost function $c$ , but still remains iterative. In addition, RF and $c$ -RF both requires ODE simulation after the first iteration to continue training. In OFM, we work only with the quadratic cost function, but retrieve its OT solution in just one FM iteration without simulation of the trajectories. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Light and Optimal Schr\u00f6dinger Bridge. In [22], the authors observe the relation between Entropic Optimal Transport (EOT) [42, 12] and Bridge Matching (BM) [53] problems. These are stochastic analogs of OT and FM, respectively. In EOT and BM, instead of deterministic ODE and flows, one considers stochastic processes with non-zero stochasticity. The authors prove that, during BM, one can restrict considered processes only to the specific ones and retrieve the solution of EOT. Hypothetically, our OT/FM case is a limit of their EOT/BM case when the stochasticity tends to zero. Proofs in [22] for EOT are based on sophisticated KL divergence properties. We do not know whether our results for OFM can be derived by taking the limit of their stochastic case. To derive the properties of our OFM, we use completely different proof techniques based on computing integrals over curves rather than KL-based techniques. Besides, in practice, the authors of [22] mostly focus on Gaussian mixture parametrization while our method allows using neural networks (ICNNs). ", "page_idx": 7}, {"type": "text", "text": "3.4 Theory: properties of OFM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we provide the OFM\u2019s theoretical properties, which give an intuition for understanding of its main working principles and behavior. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2 (Simplified OFM Loss). We can simplify (16) to a more suitable form: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{O F M}^{\\pi}(\\Psi)\\!=\\!\\!\\int_{0}^{1}\\left\\{\\sum_{\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}\\left|\\left|\\frac{(\\phi_{t}^{\\Psi})^{-1}(x_{t})-x_{0}}{t}\\right|\\right|^{2}\\pi(x_{0},x_{1})d x_{0}d x_{1}\\right\\}d t,x_{t}=(1-t)x_{0}\\!+\\!t x_{1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The simplified form (18) shows that OFM loss actually measures how well $\\Psi$ restores initial points $x_{0}$ of linear interpolations depending on future point $x_{t}$ and time $t$ . ", "page_idx": 7}, {"type": "text", "text": "Generative properties of OFM. In this paragraph, we provide another view on our OFM approach. In our OFM, we aim to construct a vector field $u$ which is as close to the dynamic OT field $\\boldsymbol{u}^{*}$ as possible. We can use the least square regression to measure the distance between them: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{DIST}(u,u^{*}):=\\int_{0}^{1}\\int_{\\mathbb{R}^{D}}\\|u_{t}(x_{t})-u_{t}^{*}(x_{t})\\|^{2}\\underbrace{\\phi_{t}^{*}\\#p_{0}(x_{t})}_{:=p_{t}^{*}(x_{t})}d x_{t}d t.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proposition 3 (Intractable Distance). The distance DIST $(u,u^{*})$ between an arbitrary vector field $u$ and OT field $\\boldsymbol{u}^{*}$ equals to the FM loss from (10) with the optimal plan $\\pi^{*}$ , i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{DIST}(u,u^{*})=\\mathcal{L}_{F M}^{\\pi^{*}}(u)-\\underbrace{\\mathcal{L}_{F M}^{\\pi^{*}}(u^{*})}_{=0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We can not minimize intractable DIST $(u,u^{*})$ since the optimal plan $\\pi^{*}$ is unknown. In OT-CFM [55], authors heuristically approximate $\\pi^{*}$ in $\\mathcal{L}_{F M}^{\\pi^{*}}(u)$ , but obtain biased solution. Surprisingly, for the optimal vector fields, the distance can be calculated explicitly via any known plan $\\pi$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 4 (Tractable Distance For OFM). The distance DIST $(u^{\\Psi},u^{\\Psi^{*}})$ between an optimal vector field $u^{\\Psi}$ generated by a convex function $\\Psi$ and the vector field $u^{\\Psi^{*}}$ with the Brenier potential $\\Psi^{*}$ can be evaluated directly via OFM loss (16) and any plan $\\pi$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{D I S T}\\big(u^{\\Psi},u^{\\Psi^{\\bullet}}\\big)=\\mathcal{L}_{F M}^{\\pi}\\big(u^{\\Psi}\\big)-\\mathcal{L}_{F M}^{\\pi}\\big(u^{\\Psi^{\\bullet}}\\big)=\\mathcal{L}_{O F M}^{\\pi}\\big(\\Psi\\big)-\\mathcal{L}_{O F M}^{\\pi}\\big(\\Psi^{\\ast}\\big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In (20), the first term is our tractable OFM loss, and the second term does not depend on $\\Psi$ . Hence, during the whole minimization process in our OFM, we gradually lower the distance (19) between the current vector field and the dynamic OT field up to the complete match. ", "page_idx": 7}, {"type": "text", "text": "4 Experimental Illustrations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we showcase the performance of our Optimal Flow Matching method on illustrative 2D scenario $(\\S4.1)$ and Wasserstein-2 benchmark [33] (\u00a74.2). Finally, we apply our approach for solving high-dimensional unpaired image-to-image translation in the latent space of pretrained ALAE autoencoder (\u00a74.3). The PyTorch implementation of our method is publicly available at ", "page_idx": 7}, {"type": "text", "text": "https://github.com/Jhomanik/Optimal-Flow-Matching ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The technical details of our experiments (architectures, hyperparameters) are in the Appendix B. ", "page_idx": 8}, {"type": "text", "text": "4.1 Illustrative 2D Example ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we illustrate the proof-of-concept of our OFM on 2D setup and demonstrate that OFM\u2019s solutions do not depend on the initial transport plan $\\pi$ . We run our Algorithm 1 between a standard Gaussian $p_{0}\\,=\\,\\bar{\\mathcal{N}}(0,I)$ and a Mixture of eight Gaussians $p_{1}$ depicted in the Figure 6a. We consider different stochastic plans $\\pi$ : independent plan $p_{0}\\,\\times\\,p_{1}$ (Figure 6b), minibatch and antiminibatch (Figures 6c, 6d) discrete OT (quadratic cost) with batch size $B_{\\mathrm{mb}}=64$ . In the antiminibatch case, we compose the pairs of source and target points by solving discrete OT with minus quadratic cost $-\\|\\boldsymbol{x}-\\boldsymbol{y}\\|_{2}^{2}$ . The fitted OFM maps and trajectories are presented in Figure 6. We empirically see that our OFM finds the same solution for all initial plans $\\pi$ . ", "page_idx": 8}, {"type": "text", "text": "For completeness, in Appendix B.2, we apply these plans to the original FM (10), and show that, in comparison with our OFM, the resulting paths obtained by FM considerably depend on the plan. ", "page_idx": 8}, {"type": "image", "img_path": "kqmucDKVcU/tmp/5f33d63e109a27ecd4b0fff88100313a810a1161510cddbea2cd24a1e349607d.jpg", "img_caption": ["Figure 6: Performance of our Optimal Flow Matching on Gaussian $\\rightarrow$ Eight Gaussians 2D setup. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 High-dimensional OT Benchmarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we quantitatively compare our OFM and other methods testing their ability to solve OT. We run our OFM, FM based methods and OT solvers on OT Benchmark [33]. The authors provide high-dimensional continuous distributions $p_{0},p_{1}$ for which the ground truth OT map $T^{*}$ for the quadratic cost is known by the construction. To assess the quality of retrieved transport maps, we use standard unexplained variance percentage $\\mathcal{L}^{2}\\mathrm{-UVP}(T):=10\\dot{0}\\cdot\\|T-T^{*}\\|_{\\mathcal{L}^{2}(p_{0})}^{2}\\hat{/}\\mathrm{Var}(p_{1})\\hat{\\mathcal{V}}_{0}$ [33]. It directly computes the normalized squared error between OT map $T^{*}$ and learned map $T$ . ", "page_idx": 8}, {"type": "text", "text": "Competitors. We evaluate Conditional Flow Matching (OT-CFM), Rectified Flow (RF), $c$ -Rectified Flow ( $\\scriptstyle{\\mathrm{\\ddot{c}}}$ -RF), the most relevant OT solver MMv-1 [54] and its amortized version from [2]. In [54] and [2], the authors directly minimize the dual formulation loss $\\mathcal{L}_{O T}$ (4) by parametrizing $\\Psi$ with ICNNs and calculating $\\overline{{\\Psi}}(x_{1})$ via solving a convex optimization subproblem. The latter is similar to our inversion (17). Additionally, in [2], the authors use MLPs to parametrize $\\Psi$ , and we include these results as well. Following [33], we also provide results for a linear OT map (baseline) which translates means and variances of distributions to each other. For our OFM, we consider two initial plans: independent plan (Ind) and minibatch OT (MB), the batch size for the latter is $B_{\\mathrm{mb}}=64$ . ", "page_idx": 8}, {"type": "table", "img_path": "kqmucDKVcU/tmp/afc4329faef9c22a4ba3d13b259543cbac0445066f72771fe6febd676369ba48.jpg", "table_caption": ["The overall results are presented in Table 1. More details are given in Appendix B.3. "], "table_footnote": ["Table 1: ${\\mathcal{L}}^{2}$ \u2212UVP values of solvers fitted on high-dimensional benchmarks in dimensions D = 2, 4, 8, 16, 32, 64, 128, 256. The best metric over Flow Matching based methods is bolded. \\* Metrics are taken from [33]. $^{**}$ Metrics are taken from [2]. "], "page_idx": 8}, {"type": "text", "text": "Results. Among FM-based methods, our OFM with any plan demonstrates the best results. For all plans, OFM convergences to close final solutions and metrics. Minibatch plan provides a little bit better results, especially in high dimensions. In theory, the OFM results for any plan $\\pi$ must be similar. However, in stochastic optimization, plans with large variance yield convergence to slightly worse solutions. ", "page_idx": 9}, {"type": "text", "text": "MLP-based OT solver usually beats our OFM, since MLPs do not have ICNNs\u2019 limitations in practice. However, usage of MLP is an empirical trick and is not completely justified. We also run OFM with MLP instead ICNN, and, unfortunately, the method fails to converge. ", "page_idx": 9}, {"type": "text", "text": "RF demonstrates worse performance than even linear baseline, but it is ok since it is not designed to solve $\\mathbb{W}_{2}$ OT. In turn, $c$ -RF works better, but rapidly deteriorates with increasing ", "page_idx": 9}, {"type": "image", "img_path": "kqmucDKVcU/tmp/0ad2861ab2af9cc427333263d7b9f229d5d0acf6bd21c1dff839dd191c1b8901.jpg", "img_caption": ["ALAE FFHQ latent space. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "dimensions. OT-CFM demonstrates the best results among baseline FM-based methods, but still underperforms compared to our OFM solver in high dimensions. ", "page_idx": 9}, {"type": "text", "text": "4.3 Unpaired Image-to-image Transfer ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Another task that involves learning a translation between two distributions is unpaired image-to-image translation [63]. We follow the setup of [32] where translation is computed in the 512 dimensional latent space of the pre-trained ALAE autoencoder [46] on $1024\\times1024$ FFHQ dataset [29]. In particular, we split the train FFHQ sample (60K faces) into children and adults subsets and consider the corresponding ALAE latent codes as the source and target distributions $p_{0}$ and $p_{1}$ . At the inference stage, we take a new (unseen) adult face from a test FFHQ sample, extract its latent code, process with our learned model and then decode back to the image space. The qualitative translation results and FID metric [25] are presented in Figure 7 and Table 2, respectively. ", "page_idx": 9}, {"type": "table", "img_path": "kqmucDKVcU/tmp/051f5b05c894373daaf16ae974c8b6a9ea7171ebd8d348797545f905acf54968.jpg", "table_caption": ["Table 2: FID metric on Adult $\\rightarrow$ Child translation task for the Flow Matching based methods. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The batch size for minibatch OT methods (\u230aOFM, MB\u2309, \u230aOT-CFM\u2309) is $B_{\\mathrm{mb}}=128$ . Our OFM converges to nearly the same solution for both independent and MB plans, and demonstrates qualitatively plausible translations. The most similar results to our method are demonstrated by $\\lfloor c{-}\\mathrm{RF}\\rceil$ . Similar to OFM, this method (in the limit of RF steps) also recovers the quadratic OT mapping. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Potential impact. We believe that our novel theoretical results have a huge potential for improving modern flow matching-based methods and inspiring the community for further studies. We think this is of high importance especially taking into account that modern generative models start to extensively use flow matching methods [61, 41, 17]. ", "page_idx": 9}, {"type": "text", "text": "Limitations and broader impact are discussed in Appendix C. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of N. Kornilov has been financially supported by The Analytical Center for the Government of the Russian Federation (Agreement No. 70-2021-00143 01.11.2021, IGK 000000D730324P540002). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] SN Afriat. Theory of maxima and the method of lagrange. SIAM Journal on Applied Mathematics, 20(3):343\u2013357, 1971. [2] Brandon Amos. On amortizing convex conjugates for optimal transport. In The Eleventh International Conference on Learning Representations, 2023.   \n[3] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017.   \n[4] Arip Asadulaev, Alexander Korotin, Vage Egiazarian, Petr Mokrov, and Evgeny Burnaev. Neural optimal transport with general cost functionals. In The Twelfth International Conference on Learning Representations, 2024. [5] Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the monge-kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.   \n[6] Espen Bernton, Pierre E Jacob, Mathieu Gerber, and Christian P Robert. On parameter estimation with the wasserstein distance. Information and Inference: A Journal of the IMA, 8(4):657\u2013676, 2019.   \n[7] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[8] Charlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised training of conditional monge maps. Advances in Neural Information Processing Systems, 35:6859\u20136872, 2022.   \n[9] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. A survey on generative diffusion models. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[10] Shreyas Chaudhari, Srinivasa Pranav, and Jos\u00e9 MF Moura. Gradient networks. arXiv preprint arXiv:2404.07361, 2024.   \n[11] Yize Chen, Yuanyuan Shi, and Baosen Zhang. Optimal control via neural networks: A convex approach. arXiv preprint arXiv:1805.11835, 2018.   \n[12] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. On the relation between optimal transport and schr\u00f6dinger bridges: A stochastic control viewpoint. Journal of Optimization Theory and Applications, 169:671\u2013691, 2016.   \n[13] Jaemoo Choi, Jaewoong Choi, and Myungjoo Kang. Generative modeling through the semidual formulation of unbalanced optimal transport. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[14] Max Daniels, Tyler Maunu, and Paul Hand. Score-based generative neural networks for largescale optimal transport. Advances in neural information processing systems, 34:12955\u201312965, 2021.   \n[15] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[16] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational and applied mathematics, 6(1):19\u201326, 1980.   \n[17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024.   \n[18] Jiaojiao Fan, Shu Liu, Shaojun Ma, Hao-Min Zhou, and Yongxin Chen. Neural monge map estimation and its applications. Transactions on Machine Learning Research, 2023. Featured Certification.   \n[19] Jiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Scalable computations of wasserstein barycenter via input convex neural networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1571\u20131581. PMLR, 18\u201324 Jul 2021.   \n[20] Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Extremal domain translation with neural optimal transport. Advances in Neural Information Processing Systems, 36:40381\u201340413, 2023.   \n[21] Aude Genevay, Marco Cuturi, Gabriel Peyr\u00e9, and Francis Bach. Stochastic optimization for large-scale optimal transport. Advances in neural information processing systems, 29, 2016.   \n[22] Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, and Alexander Korotin. Light and optimal schr\u00f6dinger bridge matching. In Forty-first International Conference on Machine Learning, 2024.   \n[23] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry P Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[26] J Hiriart-Urruty and Yves Lucet. Parametric computation of the legendre-fenchel conjugate with application to the computation of the moreau envelope. Journal of Convex Analysis, 14(3):657, 2007.   \n[27] Pieter-Jan Hoedt and G\u00fcnter Klambauer. Principled weight initialisation for input-convex neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Chin-Wei Huang, Ricky T. Q. Chen, Christos Tsirigotis, and Aaron Courville. Convex potential flows: Universal probability distributions with optimal transport and convex optimization. In International Conference on Learning Representations, 2021.   \n[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[30] Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev. Wasserstein-2 generative networks. In International Conference on Learning Representations, 2021.   \n[31] Alexander Korotin, Vage Egiazarian, Lingxiao Li, and Evgeny Burnaev. Wasserstein iterative networks for barycenter estimation. Advances in Neural Information Processing Systems, 35:15672\u201315686, 2022.   \n[32] Alexander Korotin, Nikita Gushchin, and Evgeny Burnaev. Light schr\u00f6dinger bridge. In The Twelfth International Conference on Learning Representations, 2023.   \n[33] Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. Advances in neural information processing systems, 34:14593\u201314605, 2021.   \n[34] Alexander Korotin, Lingxiao Li, Justin Solomon, and Evgeny Burnaev. Continuous wasserstein2 barycenter estimation without minimax optimization. In International Conference on Learning Representations, 2021.   \n[35] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport. In The Eleventh International Conference on Learning Representations, 2023.   \n[36] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. In The Eleventh International Conference on Learning Representations, 2023.   \n[37] Maciej \u0141awry\u00b4nczuk. Input convex neural networks in nonlinear predictive control: A multimodel approach. Neurocomputing, 513:273\u2013293, 2022.   \n[38] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023.   \n[39] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022.   \n[40] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023.   \n[41] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and qiang liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2024.   \n[42] Christian L\u00e9onard. A survey of the schr\u00f6dinger problem and some of its connections with optimal transport. Discrete and Continuous Dynamical Systems, 34(4):1533\u20131574, 2014.   \n[43] Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport mapping via input convex neural networks. In International Conference on Machine Learning, pages 6672\u20136681. PMLR, 2020.   \n[44] Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. In The Twelfth International Conference on Learning Representations, 2024.   \n[45] Daniel Morales-Brotons, Thijs Vogels, and Hadrien Hendrikx. Exponential moving average of weights in deep learning: Dynamics and beneftis. Transactions on Machine Learning Research, 2024.   \n[46] Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14104\u201314113, 2020.   \n[47] ES Polovinkin and MV Balashov. Elements of convex and strongly convex analysis, 2007.   \n[48] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. In International Conference on Machine Learning, pages 28100\u201328127. PMLR, 2023.   \n[49] Jack Richter-Powell, Jonathan Lorraine, and Brandon Amos. Input convex gradient networks. arXiv preprint arXiv:2111.12187, 2021.   \n[50] Ralph Tyrell Rockafellar. Convex analysis. 2015.   \n[51] Litu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In International Conference on Learning Representations, 2022.   \n[52] Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large scale optimal transport and mapping estimation. In International Conference on Learning Representations, 2018.   \n[53] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[54] Amirhossein Taghvaei and Amin Jalali. 2-wasserstein approximation via restricted convex potentials with application to improved training for gans. arXiv preprint arXiv:1902.07197, 2019.   \n[55] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. Expert Certification.   \n[56] Nazarii Tupitsa, Pavel Dvurechensky, Darina Dvinskikh, and Alexander Gasnikov. Numerical methods for large-scale optimal transport. arXiv preprint arXiv:2210.11368, 2022.   \n[57] Th\u00e9o Uscidda and Marco Cuturi. The monge gap: A regularizer to learn all transport maps. In International Conference on Machine Learning, pages 34709\u201334733. PMLR, 2023.   \n[58] Bryan Van Scoy, Randy A Freeman, and Kevin M Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. IEEE Control Systems Letters, 2(1):49\u201354, 2017.   \n[59] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021.   \n[60] C\u00e9dric Villani. Topics in optimal transportation, volume 58. American Mathematical Soc., 2021.   \n[61] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator, 2024.   \n[62] Shu Yang and B Wayne Bequette. Optimization-based control using input convex neural networks. Computers & Chemical Engineering, 144:107143, 2021.   \n[63] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223\u20132232, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs and auxiliary statements ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we place the proofs of all our results from the main manuscript and some auxiliary results. We reorder the proofs of Prop. 1 and Prop. 2 since the former is based on the latter. ", "page_idx": 14}, {"type": "text", "text": "Note that in all of our theoretical derivations, if not stated explicitly, we assume the differentiability of convex potential $\\Psi$ at given points $z_{0},x_{t}$ etc. This assumption is done for simplicity and does not spoil our theory. The convex functions are known to be differentiable almost surely w.r.t Lebesgue measure [50]. Therefore, since we consider absolutely continuous reference distributions $p_{0},p_{1}$ (see $\\S2.1)$ , the differentiability of $\\Psi$ at the considered points also holds almost surely. ", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 2 (Simplified OFM Loss) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. By definition ${\\mathcal{L}}_{O F M}^{\\pi}(\\Psi)$ equals to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{O F M}^{\\pi}(\\Psi):=\\!\\!\\int\\!\\!\\left\\{\\!\\!\\left\\{\\!\\!\\begin{array}{c}{{\\!}}\\\\ {{\\!\\!\\!\\int\\!\\!\\!\\!\\mathrm{d}t\\!\\!\\!^{D}\\!\\!\\!\\!\\times\\!\\!\\!\\mathbb{R}^{D}}\\!\\!\\!\\!\\phantom{+}}\\end{array}\\!\\!\\!\\!\\right\\}|u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})||^{2}\\pi(x_{0},x_{1})d x_{0}d x_{1}\\!\\!\\right\\}\\!\\!d\\!t,x_{t}=(1-t)x_{0}+t x_{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For fixed points $x_{0},x_{1}$ and time $t$ in integrand, we find a point $z_{0}=(\\phi_{t}^{\\Psi})^{-1}(x_{t})$ such that in moment $t\\in[0,1]$ it is transported to point $x_{t}=(1-t)x_{0}+t x_{1}$ . This point $z_{0}$ satisfies equality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{t}=t\\nabla\\Psi(\\boldsymbol{z}_{0})+(1-t)\\boldsymbol{z}_{0}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We define the vector field $u_{t}^{\\Psi}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{t}^{\\Psi}(x_{t})\\;\\;=\\;\\;\\nabla\\Psi(z_{0})-z_{0}=\\frac{x_{t}-z_{0}}{t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting $u_{t}^{\\Psi}(x_{t})$ in the integrand of (21), we obtain simplified integrand ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|x_{1}-x_{0}-u_{t}^{\\Psi}(x_{t})\\|^{2}}&{=}&{\\displaystyle\\bigg|\\bigg|x_{1}-x_{0}-\\bigg(\\frac{x_{t}-z_{0}}{t}\\bigg)\\bigg|\\bigg|^{2}}\\\\ &{=}&{\\displaystyle\\frac{1}{t^{2}}\\|t x_{1}-t x_{0}-((1-t)x_{0}+t x_{1})+z_{0}\\|^{2}}\\\\ &{=}&{\\displaystyle\\frac{1}{t^{2}}\\|z_{0}-x_{0}\\|^{2}=\\bigg|\\bigg|\\frac{\\big(\\phi_{t}^{\\Psi}\\big)^{-1}\\big(x_{t}\\big)-x_{0}}{t}\\bigg|\\bigg|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition $\\mathbf{1}$ (Explicit Loss Gradient Formula) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Point $z_{0}=(\\phi_{t}^{\\Psi_{\\theta}})^{-1}(x_{t})$ now depends on parameters $\\theta$ . We differentiate the integrand from the simplified OFM loss (18) for fixed points $x_{0},x_{1}$ and time $t$ , i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\nd\\left(\\frac{1}{t^{2}}\\|z_{0}-x_{0}\\|^{2}\\right)\\;\\;=\\;\\;2\\left\\langle\\frac{z_{0}-x_{0}}{t^{2}},\\frac{d z_{0}}{d\\theta}d\\theta\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For point $z_{0}$ , the equation (27) holds true: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t}=(1-t)z_{0}+t\\nabla\\Psi_{\\theta}(z_{0}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We differentiate (23) w.r.t. $\\theta$ and obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{0}&{=}&{\\displaystyle(1-t)\\frac{d z_{0}}{d\\theta}+t\\nabla^{2}\\Psi_{\\theta}(z_{0})\\frac{d z_{0}}{d\\theta}+t\\frac{\\partial\\nabla\\Psi_{\\theta}}{\\partial\\theta}(z_{0})\\Rightarrow}\\\\ {\\frac{d z_{0}}{d\\theta}}&{=}&{\\displaystyle-\\left(t\\nabla^{2}\\Psi_{\\theta}(z_{0})+(1-t)I\\right)^{-1}\\cdot t\\frac{\\partial\\nabla\\Psi_{\\theta}}{\\partial\\theta}(z_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{(22)}&{=}&{\\displaystyle\\left\\langle2\\frac{x_{0}-z_{0}}{t},\\left(t\\nabla^{2}\\Psi_{\\theta}(z_{0})+(1-t)I\\right)^{-1}\\frac{\\partial\\nabla\\Psi_{\\theta}}{\\partial\\theta}(z_{0})d\\theta\\right\\rangle}\\\\ &{=}&{\\displaystyle\\left\\langle2\\left(t\\nabla^{2}\\Psi_{\\theta}(z_{0})+(1-t)I\\right)^{-1}\\frac{\\left(x_{0}-z_{0}\\right)}{t},\\frac{\\partial\\nabla\\Psi_{\\theta}}{\\partial\\theta}(z_{0})d\\theta\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now the differentiation over $\\theta$ is located only in the right part of (24) in the term \u2202\u2207\u2202\u03b8\u03a8 \u03b8. Hence, point $z_{0}$ and the left part of (24) can be considered as constants during differentiation. To get the gradient of OFM loss we also need to take math expectation over plan $\\pi$ and time $t$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "The following two Lemmas are used to prove our main theoretical result, Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1 (Properties of convex functions and their conjugates). Let $\\Psi:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$ be a convex function; $x_{0},x_{1}\\,\\in\\,\\mathbb{R}^{D}$ . Let $\\Psi$ and $\\overline{{\\Psi}}$ be differentiable at $x_{0}$ and $x_{1}$ correspondingly. Then the following statements are equivalent: ", "page_idx": 15}, {"type": "text", "text": "(i) $x_{1}=\\nabla\\Psi(x_{0})$ ;   \n(ii) $x_{0}=\\underset{z\\in\\mathbb{R}^{D}}{\\arg\\operatorname*{max}}\\left\\{\\langle x_{1},z\\rangle-\\Psi(z)\\right\\};$ (iii) Fenchel-Young\u2019s equality: $\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})=\\langle x_{1},x_{0}\\rangle$ ;   \n(iv) $x_{0}=\\nabla\\overline{{\\Psi}}\\big(x_{1}\\big)$ ;   \n(v) $x_{1}=\\underset{z\\in\\mathbb{R}^{D}}{\\arg\\operatorname*{max}}\\left\\{\\left\\langle z,x_{0}\\right\\rangle-\\overline{{\\Psi}}(z)\\right\\},$ ; ", "page_idx": 15}, {"type": "text", "text": "Proof. The lemma is a simplified version of [47, Theorem 1.16.4]. Also, the proof can be constructed by combining facts from $[7,\\,\\S3.3]$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 2 (Main Integration Lemma). For any two points $\\boldsymbol{x}_{0},\\boldsymbol{x}_{1}\\in\\mathbb{R}^{D}$ and a convex function $\\Psi$ , the following equality holds true: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\|u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})\\|^{2}d t=2\\cdot[\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})-\\langle x_{0},x_{1}\\rangle],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x_{t}=t x_{0}+(1-t)x_{1}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Following Proposition 2, we use the simplified loss form, i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})\\|^{2}=\\frac{1}{t^{2}}\\|z_{0}-x_{0}\\|^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $z_{0}=z_{0}(t)=(\\phi_{t}^{\\Psi})^{-1}(x_{t})$ satisfies the equality: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{t}=t\\nabla\\Psi(\\boldsymbol{z}_{0})+(1-t)\\boldsymbol{z}_{0}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we substitute (26) into rhs of (25) integrate w.r.t. time $t$ from 0 excluding to 1 excluding (This exclusion does not change the integral): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\|u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})\\|^{2}d t=\\int_{0}^{1}\\frac{1}{t^{2}}\\|z_{0}-x_{0}\\|^{2}d t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To further simplify (28) we need some preliminary work. Following (27) we note: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x_{t}=t\\nabla\\Psi(z_{0})+(1-t)z_{0}}&{=}&{(1-t)x_{0}+t x_{1}\\Rightarrow}\\\\ {t(\\nabla\\Psi(z_{0})-x_{1})}&{=}&{(1-t)(x_{0}-z_{0})\\Rightarrow}\\\\ {(\\nabla\\Psi(z_{0})-x_{1})}&{=}&{\\displaystyle\\left(\\frac{1-t}{t}\\right)(x_{0}-z_{0})\\Rightarrow}\\\\ {||\\nabla\\Psi(z_{0})-x_{1}||^{2}}&{=}&{\\displaystyle\\frac{(1-t)^{2}}{t^{2}}||z_{0}-x_{0}||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Changing in (28) time variable $t$ to $\\textstyle s={\\frac{t}{1-t}}$ , $\\begin{array}{r}{d s=\\frac{d t}{(1-t)^{2}}}\\end{array}$ (1d\u2212tt)2 and substitution of (30) yield: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{0}^{1}{\\frac{1}{t^{2}}}\\|z_{0}(t)-x_{0}\\|^{2}d t=\\int_{0}^{1}{\\frac{(1-t)^{2}}{t^{2}}}\\|z_{0}(t)-x_{0}\\|^{2}{\\frac{d t}{(1-t)^{2}}}=\\int_{0}^{\\infty}\\|\\nabla\\Psi(z_{0}(s))-x_{1}\\|^{2}d s.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We notice that set of points $z_{0}(s(t))=(\\phi_{t}^{\\Psi})^{-1}(x_{t}),t\\in(0,1)$ forms a curve in $\\mathbb{R}^{D}$ with parameter $t$ (or $s(t)\\rangle$ ). Now we are to process formula (31) by switching from the integration w.r.t. parameter $s$ to the integration along this curve. To do it properly we need two things: ", "page_idx": 16}, {"type": "text", "text": "1. Limits of integration. The limits of integration along the curve $z_{0}(t)$ are: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{0}(t)|_{t=0}=x_{0},}\\\\ &{z_{0}(t)|_{t=1}=(\\nabla\\Psi)^{-1}(x_{1})\\overset{\\mathrm{Lemma}\\,1;\\,(\\mathrm{i}),\\,(\\mathrm{i}\\mathrm{v})}{=}\\nabla\\overline{{\\Psi}}(x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. Expression under integral sign w.r.t. differential $d z_{0}$ . Starting with (29), we derive: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{(29)}&{\\Rightarrow\\quad s(\\nabla\\Psi(z_{0})-x_{1})}&{=}&{(x_{0}-z_{0})\\Rightarrow}\\\\ {d[s(\\nabla\\Psi(z_{0})-x_{1})]}&{=}&{d[x_{0}-z_{0}]\\;\\Rightarrow}\\\\ {s\\nabla^{2}\\Psi(z_{0})d z_{0}+(\\nabla\\Psi(z_{0})-x_{1})d s}&{=}&{-d z_{0}\\;\\Rightarrow}\\\\ {(\\nabla\\Psi(z_{0})-x_{1})d s}&{=}&{-(s\\nabla^{2}\\Psi(z_{0})+I)d z_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we proceed with (31): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\begin{array}{r l}{\\infty}&{\\displaystyle\\int\\langle\\nabla\\Psi(z_{0})-x_{1},\\nabla\\Psi(z_{0})-x_{1}\\rangle\\,d s}\\\\ &{0}\\end{array}}\\quad}\\\\ {\\stackrel{(33)}{=}}&{\\displaystyle\\int_{z_{0}}\\langle x_{1}-\\nabla\\Psi(z_{0}),(s\\nabla^{2}\\Psi(z_{0})+I)d z_{0}\\rangle}\\\\ {=}&{\\displaystyle\\int_{z_{0}}\\langle x_{1}-\\nabla\\Psi(z_{0}),d z_{0}\\rangle+\\int_{z_{0}}\\langle s(x_{1}-\\nabla\\Psi(z_{0})),\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle}\\\\ &{\\stackrel{(29)}{=}}&{\\displaystyle\\int_{z_{0}}\\langle x_{1}-\\nabla\\Psi(z_{0}),d z_{0}\\rangle+\\int_{z_{0}}\\langle z_{0}-x_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d\\langle z_{0},\\nabla\\Psi(z_{0})\\rangle}&{=}&{\\langle z_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle+\\langle d z_{0},\\nabla\\Psi(z_{0})\\rangle\\;\\Rightarrow}\\\\ {\\langle z_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle}&{=}&{d\\langle z_{0},\\nabla\\Psi(z_{0})\\rangle-\\langle\\nabla\\Psi(z_{0}),d z_{0}\\rangle.\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a consequence, we further proceed with (34): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{=}&{\\displaystyle\\int_{z_{0}}\\langle x_{1}-\\nabla\\Psi(z_{0}),d z_{0}\\rangle+\\int_{z_{0}}\\langle z_{0}-x_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle}\\\\ {=}&{\\displaystyle\\int_{z_{0}}\\langle x_{1},d z_{0}\\rangle-\\int_{z_{0}}\\langle\\nabla\\Psi(z_{0}),d z_{0}\\rangle}\\\\ &{+\\displaystyle\\int_{z_{0}}d\\langle z_{0},\\nabla\\Psi(z_{0})\\rangle-\\int_{z_{0}}\\langle\\nabla\\Psi(z_{0}),d z_{0}\\rangle-\\int_{z_{0}}\\langle x_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle}\\\\ {=}&{\\displaystyle\\int_{z_{0}}\\langle x_{1},d z_{0}\\rangle-2\\int_{z_{0}}\\langle\\nabla\\Psi(z_{0}),d z_{0}\\rangle+\\int_{z_{0}}d\\langle z_{0},\\nabla\\Psi(z_{0})\\rangle-\\int_{z_{0}}\\langle x_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Under all integrals we have closed form differentials ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\langle x_{1},d z_{0}\\rangle}&{=}&{d\\langle x_{1},z_{0}\\rangle,}\\\\ {\\langle\\nabla\\Psi(z_{0}),d z_{0}\\rangle}&{=}&{d\\Psi(z_{0}),}\\\\ {\\langle x_{0},\\nabla^{2}\\Psi(z_{0})d z_{0}\\rangle}&{=}&{d\\langle x_{0},\\nabla\\Psi(z_{0})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We integrate them from initial point $x_{0}$ to the final $\\nabla\\overline{{\\Psi}}(x_{1})$ according to limits (32) and get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\displaystyle=}&{\\displaystyle\\int d\\langle x_{1},z_{0}\\rangle-2\\int d\\Psi(z_{0})+\\int d\\langle z_{0},\\nabla\\Psi(z_{0})\\rangle-\\int d\\langle x_{0},\\nabla\\Psi(z_{0})\\rangle}\\\\ &{z_{0}}&\\\\ {\\displaystyle=}&{\\langle x_{1},\\nabla\\overline{{\\Psi}}(x_{1})\\rangle-\\langle x_{1},x_{0}\\rangle+2(\\Psi(x_{0})-\\Psi(\\nabla\\overline{{\\Psi}}(x_{1})))+\\langle(\\nabla\\overline{{\\Psi}}(x_{1}),\\nabla\\Psi(\\nabla\\overline{{\\Psi}}(x_{1}))\\rangle}\\\\ &{-\\langle x_{0},\\nabla\\Psi(x_{0})\\rangle+\\langle x_{0},\\nabla\\Psi(x_{0})\\rangle-\\langle x_{0},\\nabla\\Psi(\\nabla\\overline{{\\Psi}}(x_{1}))\\rangle.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we use properties of conjugate functions (Lemma (1)): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Psi\\big(\\nabla\\overline{{\\Psi}}(x_{1})\\big)~}&{{}\\stackrel{\\mathrm{(iv)}\\,+\\,(\\mathrm{iii})}{=}~~\\langle\\nabla\\overline{{\\Psi}}(x_{1}),x_{1}\\rangle-\\overline{{\\Psi}}(x_{1}),}\\\\ {\\nabla\\Psi\\big(\\nabla\\overline{{\\Psi}}(x_{1})\\big)~}&{{}\\stackrel{\\mathrm{(iv)}\\,+\\,(\\mathrm{i})}{=}~~x_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This allows us to simplify (36): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{=}&{{}\\langle x_{1},\\nabla\\overline{{\\Psi}}(x_{1})\\rangle-\\langle x_{1},x_{0}\\rangle+2(\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})-\\langle\\nabla\\overline{{\\Psi}}(x_{1}),x_{1}\\rangle)+\\langle(\\nabla\\overline{{\\Psi}}(x_{1}),x_{1})\\rangle}\\\\ {}&{{}-\\langle x_{0},\\nabla\\Psi(x_{0})\\rangle+\\langle x_{0},\\nabla\\Psi(x_{0})\\rangle-\\langle x_{0},x_{1}\\rangle}\\\\ {=}&{{}2[\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})-\\langle x_{0},x_{1}\\rangle].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Integrating equality (25) over the given transport plan $\\pi$ and considering the formulas for the losses (4) and (16), we derive our Theorem 1. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 1 (OFM and OT connection) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Main Integration Lemma 2 states that for any fixed points $x_{0},x_{1}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\displaystyle\\int_{0}^{1}\\|x_{1}-x_{0}-u_{t}^{\\Psi}(x_{t})\\|^{2}d t=2[\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})-\\langle x_{0},x_{1}\\rangle].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking math expectation over any plan $\\pi$ (integration w.r.t. points $x_{0},x_{1}\\sim\\pi$ ) gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathfrak{T}_{x_{0},x_{1}\\sim\\pi}\\int_{0}^{1}\\|u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})\\|^{2}d t=2\\cdot\\underbrace{\\mathbb{E}_{x_{0},x_{1}\\sim\\pi}[\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})]}_{=\\mathcal{L}_{O r}(\\Psi)}-\\underbrace{2\\cdot\\mathbb{E}_{x_{0},x_{1}\\sim\\pi}[\\langle x_{0},x_{1}\\rangle]}_{=:\\mathrm{Cosv}\\cdot\\prime(\\pi)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\mathrm{CoNST}}^{\\prime}(\\pi)$ does not depend on $\\Psi$ . Hence, both minimums of OFM loss ${\\mathcal{L}}_{O F M}^{\\pi}(\\Psi)$ and of OT dual form loss ${\\mathcal{L}}_{O T}(\\Psi)$ are achieved at the same functions. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 3 (Intractable Distance) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We recall the definitions of DIST $(u,u^{*})$ (19) and FM loss $\\mathcal{L}_{F M}^{\\pi^{*}}(u)$ (10): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{DIST}(u,u^{*})\\:=\\:\\int_{0}^{1}\\int_{\\mathbb{R}^{D}}\\|u_{t}(x_{t})-u_{t}^{*}(x_{t})\\|^{2}\\underbrace{\\phi_{t}^{*}\\#p_{0}(x_{t})}_{=p_{t}^{*}(x_{t})}d x_{t}d t,}}\\\\ {{\\displaystyle\\mathcal{L}_{F M}^{\\pi^{*}}(u)\\:=\\:\\int_{0}^{1}\\left\\{\\sum_{\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}\\|u_{t}(x_{t})-(x_{1}-x_{0})\\|^{2}\\pi^{*}(x_{0},x_{1})d x_{0}d x_{1}\\right\\},x_{t}=(1-t)x_{0}+t x_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the optimal plan $\\pi^{*}$ , each point $x_{0}$ almost surely goes to the single point $\\nabla\\Psi^{*}(x_{0})$ . Hence, in FM loss, we can leave only integration over initial points $x_{0}$ substituting $x_{1}=\\nabla\\Psi^{*}(x_{0})$ for fixed time $t$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\mathbb{R}^{D}\\times\\mathbb{R}^{D}}\\|u_{t}(x_{t})-(x_{1}-x_{0})\\|^{2}\\pi^{*}(x_{0},x_{1})d x_{0}d x_{1}}}\\\\ &{}&{\\;=\\int_{\\mathbb{R}^{D}}\\|u_{t}(x_{t})-(\\nabla\\Psi^{*}(x_{0})-x_{0})\\|^{2}p_{0}(x_{0})d x_{0}\\;,\\;x_{t}=(1-t)x_{0}+t\\nabla\\Psi^{*}(x_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We notice that dynamic OT vector field $u^{*}=u^{\\Psi^{*}}$ is the optimal one with potential $\\Psi^{*}$ . Moreover, for any point $x_{t}=(1-t)x_{0}+t\\nabla\\Psi^{*}(x_{0})$ generated by $\\boldsymbol{u}^{*}$ , we can calculate $u_{t}^{*}(x_{t})=u_{t}^{\\Psi^{*}}(x_{t})=$ $\\nabla\\Psi^{*}(x_{0})-x_{0}$ . It is the same expression as from (37), i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{(37)}&{=}&{\\displaystyle\\int\\|u_{t}(x_{t})-(\\nabla\\Psi^{*}(x_{0})-x_{0})\\|^{2}p_{0}(x_{0})d x_{0}}\\\\ &&{=}&{\\displaystyle\\int\\|u_{t}(x_{t})-u_{t}^{*}(x_{t})\\|^{2}p_{0}(x_{0})d x_{0}\\;,\\ x_{t}=(1-t)x_{0}+t\\nabla\\Psi^{*}(x_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we change the variable $x_{0}$ to $x_{t}\\;=\\;\\phi_{t}^{*}(x_{0})$ , and probability changes as $p_{0}(x_{0})d x_{0}\\;=\\;$ $\\bar{\\phi_{t}^{*}}\\#\\bar{p_{0}}(x_{t})d x_{t}=\\bar{p_{t}^{*}}(x_{t})d x_{t}$ . In new variables, we obtain the result ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{D}}\\|u_{t}(x_{t})-(x_{1}-x_{0})\\|^{2}\\pi^{*}(x_{0},x_{1})d x_{0}d x_{1}=\\int_{\\mathbb{R}^{D}}\\|u_{t}(x_{t})-u_{t}^{*}(x_{t})\\|^{2}p_{t}^{*}(x_{t})d x_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, the integration over time $t$ gives the desired equality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{DIST}\\big(u,u^{*}\\big)=\\mathcal{L}_{F M}^{\\pi^{*}}(u),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition 4 (Tractable Distance For OFM) ", "page_idx": 18}, {"type": "text", "text": "Proof. For the vector field $u^{\\Psi}$ , we apply the formula for intractable distance from Proposition 3, i.e, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\index{\\operatorname{DIST}}\\big(\\boldsymbol{u}^{\\Psi},\\boldsymbol{u}^{\\Psi^{*}}\\big)=\\mathcal{L}_{F M}^{\\pi^{*}}(\\boldsymbol{u}^{\\Psi})-\\mathcal{L}_{F M}^{\\pi^{*}}(\\boldsymbol{u}^{\\Psi^{*}})\\overset{(16)}{=}\\mathcal{L}_{O F M}^{\\pi^{*}}(\\Psi)-\\mathcal{L}_{O F M}^{\\pi^{*}}(\\Psi^{*}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "According to Main Integration Lemma 2, for any plan $\\pi$ and convex function $\\Psi$ , we have equality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathfrak{T}_{x_{0},x_{1}\\sim\\pi}\\int_{0}^{1}\\Vert u_{t}^{\\Psi}(x_{t})-(x_{1}-x_{0})\\Vert^{2}d t=2\\cdot\\underbrace{\\mathbb{E}_{x_{0},x_{1}\\sim\\pi}[\\Psi(x_{0})+\\overline{{\\Psi}}(x_{1})]}_{=\\mathcal{L}_{O r}(\\Psi)}-\\underbrace{2\\cdot\\mathbb{E}_{x_{0},x_{1}\\sim\\pi}[\\langle x_{0},x_{1}\\rangle]}_{=:\\mathrm{Cosv}^{\\prime}(\\pi)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since ${\\mathrm{CoNST}}^{\\prime}(\\pi)$ does not depend on $\\Psi$ , we have the same constant with $\\Psi=\\Psi^{*}$ and can eliminate it, i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big\\{\\mathcal{L}_{O F M}^{\\pi}(\\Psi)=2\\cdot\\mathcal{L}_{O T}(\\Psi)-\\mathrm{ConsT}^{\\prime}(\\pi),\\phantom{\\cdot\\mathcal{L}_{O F M}^{\\pi}(\\Psi)}}\\\\ {\\mathcal{L}_{O F M}^{\\pi}(\\Psi^{*})=2\\cdot\\mathcal{L}_{O T}(\\Psi^{*})-\\mathrm{CONST}^{\\prime}(\\pi)}\\\\ {\\~~~~~~~~~~~~~~~~~~~~\\Downarrow~}\\\\ {\\mathcal{L}_{O F M}^{\\pi}(\\Psi)-\\mathcal{L}_{O F M}^{\\pi}(\\Psi^{*})=2\\cdot\\mathcal{L}_{O T}(\\Psi)-2\\cdot\\mathcal{L}_{O T}(\\Psi^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The right part of (38) does not depend on a plan $\\pi$ , thus, the left part is invariant for any plan including optimal plan $\\pi^{*}$ , i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{O F M}^{\\pi}(\\Psi)-\\mathcal{L}_{O F M}^{\\pi}(\\Psi^{*})=\\mathcal{L}_{O F M}^{\\pi^{*}}(\\Psi)-\\mathcal{L}_{O F M}^{\\pi^{*}}(\\Psi^{*})=\\mathrm{DIST}(u^{\\Psi},u^{\\Psi^{*}}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B Experiments details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 OFM implementation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To implement our proposed approach in practice, we adopt fully-connected ICNN architectures proposed in [30, Appendix B2] (W2GN_ICNN) and [28, Appendix E1] (CPF_ICNN). To ensure the convexity, both architectures place some restrictions on the NN\u2019s weights and utilized activation functions, see the particular details in the corresponding papers. We take the advantage of their official repositories: ", "page_idx": 18}, {"type": "text", "text": "https://github.com/iamalexkorotin/Wasserstein2Benchmark; https://github.com/CW-Huang/CP-Flow. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We aggregate the hyper-parameters of our Algorithm 1 and utilized ICNNs for different experiments in Table 3. In all our experiments as the $S u b O p t$ optimizer we use LBFGS (torch.optim.LBFGS) with $K_{\\mathrm{sub}}$ optimization steps and early stopping criteria based on gradient norm. To find the initial point $z_{0}^{i}$ (Step 5 of our Algorithm 1), we initalize SubOpt with $x_{t^{i}}^{i}$ . As the Opt optimizer we adopt Adam with learning rate $l r$ and other hyperparameters set to be default. ", "page_idx": 18}, {"type": "text", "text": "Minibatch. Similarly to OT-CFM, in some of our experiments we use non-independent initial plans $\\pi$ to improve convergence. We construct $\\pi$ as follows: for independently sampled minibatches $X_{0},X_{1}$ of the same size $B$ , we build the optimal discrete map and apply it to reorder the pairs of samples. We stress that considering minibatch OT for our method is done exclusively to speed up the training process. Theoretically, our method is agnostic to initial plan $\\pi$ and is guaranteed to have an optimum in dynamic OT solution. ", "page_idx": 18}, {"type": "table", "img_path": "kqmucDKVcU/tmp/561256f90a29ef55211e36314b5288e38b9045ab8832f659e1d0cf7ec7aec7b1.jpg", "table_caption": ["Table 3: Hyper-parameters of our OFM solvers in different experiments "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "kqmucDKVcU/tmp/9e1327c7d47bfa2a3194ad44e89172faa4cdba9edce36261236eded4ac651cb3.jpg", "img_caption": ["Figure 8: Performance of Flow Matching on Gaussian $\\rightarrow$ Eight Gaussians 2D setup. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.2 2D Example: Comparison with Flow Matching ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this subsection, we illustrate that the restriction of the optimization domain only to optimal vector fields in FM loss is crucial for the plan independency and straightness of the obtained trajectories. ", "page_idx": 19}, {"type": "text", "text": "For that, we run the same setup from Section 4.1 but with vanila Flow Matching instead of OFM.   \nThe obtained trajectories and learned distributions for different initial plans are depicted in Figure 8. ", "page_idx": 19}, {"type": "text", "text": "In comparison with our OFM (Figure 6), basic FM yields more curved trajectories, especially with the misleading anti-minibatch plan. The learned distributions for all plans are similar to the target. ", "page_idx": 19}, {"type": "text", "text": "B.3 Benchmark details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Metrics. Following the authors of the benchmark [33], to assess the quality of retrieved transport map $T$ between $p_{0}$ and $p_{1}$ , we use unexplained variance percentage (UVP): $\\bar{\\mathcal{L}}^{2}\\mathrm{-UVP}(T):=10\\bar{0}\\cdot\\|T\\stackrel{*}{-}$ $T^{*}\\Vert_{\\mathcal{L}^{2}(p_{0})}^{2}/\\mathrm{Var}(p_{1})\\%$ . For values $\\dot{\\mathcal{L}}^{2}\\mathrm{-UVP}(T)\\approx0\\dot{\\mathcal{V}}_{\\mathrm{0}}$ , $T$ approximates $T^{*}$ , while for values $\\ge100\\%$ $T$ is far from optimal. We also calculate the cosine similarity between ground truth directions $T^{*}-\\mathrm{id}$ and obtained directions $T-\\mathrm{id}$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\cos(T-\\mathrm{id},T^{*}-\\mathrm{id})=\\frac{\\langle T-\\mathrm{id},T^{*}-\\mathrm{id}\\rangle_{\\mathcal{L}^{2}(p_{0})}}{\\|T-\\mathrm{id}\\|_{\\mathcal{L}^{2}(p_{0})}\\cdot\\|T^{*}-\\mathrm{id}\\|_{\\mathcal{L}^{2}(p_{0})}}\\in[-1,1].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For good approximations the cosine metric is approaching 1. We estimate $\\mathcal{L}^{2}\\mathrm{-UVP}$ and cos metrics with $2^{14}$ samples from $p_{0}$ . ", "page_idx": 19}, {"type": "text", "text": "In the experiments, we use the exponential moving average (EMA) [45, 24] of the trained model weights. EMA creates a smoothed copy of the model whose weights are updated at each new training iteration $t+1$ as $\\theta_{t+1}^{\\mathrm{ema}}=\\alpha\\theta_{t}^{\\mathrm{ema}}+(1-\\alpha)\\theta_{t+1}$ , where $\\theta_{t+1}$ are the newly updated original trained weights. We calculate final metrics with $\\alpha=0.999$ . ", "page_idx": 19}, {"type": "table", "img_path": "kqmucDKVcU/tmp/bc4d36dbe521b7bef6a8793848ce3217ab5ecd97154495f090864d3adf771fef.jpg", "table_caption": ["Solvers\u2019 results for cos metric are presented in Table 4. "], "table_footnote": ["Table 4: cos values of solvers fitted on high-dimensional benchmarks in dimensions $D=2$ , 4, 8, 16, 32, 64, 128, 256. The best metric over Flow Matching based solvers is bolded. \\* Metrics for MMv1 and linear baseline are taken from [33]. "], "page_idx": 19}, {"type": "text", "text": "Details of Solvers. Neural networks\u2019 architectures of competing Flow Matching methods and their parameters used in benchmark experiments are presented in Table 5. In this Table, \u201cFC\u201d stands for \u201cfully-connected\u201d. ", "page_idx": 20}, {"type": "table", "img_path": "kqmucDKVcU/tmp/17f8a211874c2e324a25226e2c0a76f8b3828386fe773cb8b926763024bfd865.jpg", "table_caption": ["Table 5: Parameters of models fitted on benchmark in dimensions $D=2,4,8,16,32,64,128,256$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Time variable $t$ in $(c-)\\mathbf{RF}$ and OT-CFM\u2019s architectures is added as one more dimensionality in input without special preprocessing. In RF and $c$ -RF, ODE are solved via Explicit Runge-Kutta method of order 5(4) [16] with absolute tolerance $10^{-4}-10^{-6}$ . In OFM and $c$ -RF, gradients over input are calculated via autograd of PyTorch. ", "page_idx": 20}, {"type": "text", "text": "Following the authors of RF [40], we run only $2-3$ rounds in RF. In further rounds, straightness and metrics change insignificantly, while the error of target distribution learning still accumulates. ", "page_idx": 20}, {"type": "text", "text": "Our implementations of OT-CFM [55] and RF [40] are based on the official repositories: ", "page_idx": 20}, {"type": "text", "text": "Implementation of $c$ -RF follows the RF framework with the modification of optimized NN\u2019s architecture. Instead of $\\mathbb{R}^{D}\\times[0,1]\\rightarrow\\mathbb{R}^{D}$ net, we parametrize time-dependent scalar valued model $\\mathbb{R}^{D}\\times[0,1]\\rightarrow\\mathbb{R}$ which gradients are set to be the vector field. ", "page_idx": 20}, {"type": "text", "text": "B.4 Unpaired Image-to-image transfer details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To conduct the experiments with high-dimensional I2I translation empowered with pretrained ALAE autoencoder, we adopt the publicly available code: ", "page_idx": 20}, {"type": "text", "text": "Additional qualitative results for our method are provided in Figure 9. ", "page_idx": 20}, {"type": "image", "img_path": "kqmucDKVcU/tmp/7e027047a8fed931c5d25355b952e0da775fddce2c370f5d30c7da8ffa46c2f1.jpg", "img_caption": ["Figure 9: Unpaired I2I Adult $\\rightarrow$ Child by our OFM solver, ALAE $1024\\!\\times\\!1024$ FFHQ latent space. The samples are uncurated. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.5 Computation time ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In what follows, we provide approximate running times for training our OFM and other FM-based method in different experiments with hyper-parameters provided in Table 3. ", "page_idx": 21}, {"type": "text", "text": "In the Illustrative 2D experiment, the training takes $\\approx1.5$ hours on a single 1080 ti GPU. In the Wasserstein-2 benchmark, the computation time depends on the dimensionality $D=$ $2,4,\\dots,256$ . Totally, all the benchmark experiments (both with Ind and MB plan $\\pi$ ) take $\\approx3$ days on three A100 GPUs. In the ALAE experiment, the training stage lasts for $\\approx5$ hours on a single 1080 ti GPU. ", "page_idx": 21}, {"type": "text", "text": "For better understanding of methods\u2019 behaviour ", "page_idx": 21}, {"type": "image", "img_path": "kqmucDKVcU/tmp/cf814c4d607201d7eb0a759f4684e9def8b07e88fecc69c17b96907e5d58e615.jpg", "img_caption": ["Figure 10: $\\mathcal{L}^{2}\\mathrm{-UVP}$ metric depending on the elapsed training time in dimension $D=32$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "over time, we depict achieved $\\mathcal{L}^{2}\\mathrm{-UVP}$ metric on the benchmark $\\langle D=32\\rangle$ ) depending on elapsed training time in Figure 10. We note that the training iteration of OFM is computationally expensive, but it requires less steps to achieve the best results. ", "page_idx": 21}, {"type": "text", "text": "B.6 Amortization technique ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In order to train our OFM, we need to efficiently solve subproblem (17). As an example of more advanced technique rather than LBFGS solver, we discuss amortization trick proposed in [2]. ", "page_idx": 21}, {"type": "text", "text": "Namely, we find an approximate solution of (17) at point $x_{t}$ and time $t$ with an extra MLP $A_{\\phi}(\\cdot,\\cdot)$ : $\\mathbb{R}^{D}\\times[0,1]\\rightarrow\\mathbb{R}^{D}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{\\phi}(x_{t},t)\\approx\\arg\\operatorname*{min}_{z_{0}\\in\\mathbb{R}^{D}}\\left[\\frac{(1-t)}{2}\\|z_{0}\\|^{2}+t\\Psi(z_{0})-\\langle x_{t},z_{0}\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and then run sub-problem solver (LBFGS) initialized with $A_{\\phi}(x_{t},t)$ until convergence. We modify the training pipeline and include learning of parameters $\\phi$ of $A_{\\phi}$ in Algorithm 2. ", "page_idx": 21}, {"type": "text", "text": "Algorithm 2 Optimal Flow Matching with Amortization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input: Initial transport plan $\\pi\\in\\Pi(p_{0},p_{1})$ , number of iterations $K$ , batch size $B$ , optimizer $O p t$ , amortization optimizer AmorOpt, sub-problem optimizer SubOpt, ICNN $\\Psi_{\\theta}$ , MLP $A_{\\phi}$ ", "page_idx": 21}, {"type": "text", "text": "1: for $k=0,\\ldots,K-1$ do   \n2: Sample batch $\\{(x_{0}^{i},x_{1}^{i})\\}_{i=1}^{B}$ of size $B$ from plan $\\pi$ ;   \n3: Sample times batch $\\{t^{i}\\}_{i=1}^{B}$ of size $B$ from $U[0,1]$ ;   \n4: Calculate linear interpolation $x_{t^{i}}^{i}=(1-t^{i})x_{0}^{i}+t^{i}x_{1}^{i}$ for all $i\\in{\\overline{{1,B}}}$ ;   \n5: Compute initialization $z_{i n i t}^{i}=A_{\\phi}(x_{t^{i}}^{i},t^{i})$ for all $i\\in{\\overline{{1,B}}}$ ;   \n6: Find detached solution $z_{0}^{i}$ of (17) via SubOpt initialized with $z_{i n i t}^{i}$ for all $i\\in{\\overline{{1,B}}}$ ;   \n7: Calculate OFM loss L\u02c6OF M ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}_{O F M}=\\frac{1}{B}\\sum_{i=1}^{B}\\left\\langle\\mathrm{NO-GRAD}\\left\\{2\\left(t^{i}\\nabla^{2}\\Psi_{\\theta}(z_{0}^{i})+(1-t^{i})I\\right)^{-1}\\frac{\\left(x_{0}^{i}-z_{0}^{i}\\right)}{t^{i}}\\right\\},\\nabla\\Psi_{\\theta}(z_{0}^{i})\\right\\rangle;\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "8: Update parameters $\\theta$ via optimizer Opt step with $\\frac{d\\hat{\\mathcal{L}}_{O F M}}{d\\theta}$ ; ", "page_idx": 21}, {"type": "text", "text": "9: Calculate Amortization loss ${\\mathcal{L}}_{A m o r}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A m o r}=\\frac{1}{B}\\sum_{i=1}^{B}\\|z_{i n i t}^{i}-z_{0}^{i}\\|^{2};\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "10: Update parameters $\\phi$ via optimizer AmorOpt step with $\\frac{d\\mathcal{L}_{A m o r}}{d\\phi}$ ; ", "page_idx": 21}, {"type": "text", "text": "11: end for ", "page_idx": 21}, {"type": "text", "text": "During the experiments, we did not find any improvements of the final metrics, in comparison with the original OFM with the same hyperparameters. However, this augmentation potentially can cause a shrinking of the overall training time. During training, $A_{\\phi}(\\cdot,\\cdot)$ learns to predict more and more accurate initial solution $z_{i n i t}^{i}$ and, thus, reduces the required number of the expensive SubOpt steps. ", "page_idx": 22}, {"type": "text", "text": "C Limitations and Broader Impact ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We find 3 limitations of our OFM which are to be addressed in the future research. ", "page_idx": 22}, {"type": "text", "text": "(a) Flow map inversion. During training, we need to compute $(\\phi_{t}^{\\Psi_{\\theta}})^{-1}(\\cdot)$ via solving strongly convex subproblem (17). In practice, we approach it by the standard gradient descent (with LBFGS optimizer), but actually there exist many improved methods to solve such conjugation problems more effectively in both the optimization [58, 26] and OT [2, 43]. This provides a dozen of opportunities for improvement, but leave such advanced methods for future research. ", "page_idx": 22}, {"type": "text", "text": "(b) ICNNs. It is known that ICNNs may underperform compared to regular neural networks [33, 31]. Thus, ICNN parametrization may limit the performance of our OFM. Fortunately, deep learning community actively study ways to improve ICNNs [10, 8, 49, 27] due to their growing popularity in various tasks [62, 37, 11]. We believe that the really expressive ICNN architectures are yet to come. ", "page_idx": 22}, {"type": "text", "text": "(c) Hessian inversion. We get the gradient of our OFM loss via formula from Proposition 1. There we have to invert the hessian $\\nabla^{2}\\Psi(\\cdot)$ , which is expensive. We point to addressing this limitation as a promising avenue for future studies. ", "page_idx": 22}, {"type": "text", "text": "Broader impact. This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 22}, {"type": "text", "text": "D Check List ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Abstract and Introduction, we completely describe our contributions. For every contribution, we provide a link to the section about it. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss limitations in Appendix C. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Proofs are provided in Appendix A. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Experimental details are discussed in Appendix B. Code for the experiments is provided in supplementary materials. All the datasets are available in public. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [YES] ", "page_idx": 23}, {"type": "text", "text": "Justification:Code will be made public after the paper acceptance (now we provide it in the supplementary). Experimental details are provided in Appendix B. All the datasets are publicly available. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Experimental details are discussed in Appendix B. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: Unfortunetely, running each experiment several times for statistics is too computentionaly expensive. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Appendix B, we mention time and resources. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \nAnswer: [Yes]   \nJustification: Research conforms with NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative   \nsocietal impacts of the work performed?   \nAnswer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss broader impact in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: The research does not need safeguards. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cite each used assets. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: New code is attached in the supplementary materials. The license will be provided after acceptance. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research does not engage with Crowdsourcing or Human subjects. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: The research does not engage with Crowdsourcing or Human subjects. ", "page_idx": 24}]