[{"figure_path": "bf0MdFlz1i/tables/tables_4_1.jpg", "caption": "Table 1: Two examples of floating point accumulation error when rounding arithmetic performed higher precision (e.g. FP32) down to lower precision (e.g. FP16).", "description": "This table shows two examples to demonstrate how floating point accumulation errors can occur when rounding from higher precision (FP32) to lower precision (FP16). In the first example, the same result is obtained regardless of the order of summation because the error introduced by rounding is less than the least significant bit of the FP16 representation. However, in the second example, the accumulation order affects the final rounded FP16 result, illustrating non-determinism due to floating point arithmetic.", "section": "4 The Nondeterminism Challenge"}, {"figure_path": "bf0MdFlz1i/tables/tables_7_1.jpg", "caption": "Table 2: Efficient encoding reduces storage requirements by 77%, and rounding to b = 26 improves the compression further between 5-20% (values reported for 1 step of training). The original proof-of-learning protocol from Jia et al. [2021] requires storing 2.78 GB of model weights for GPT-2, or more than 140x our storage cost, while still incurring statistical error.", "description": "This table compares the storage costs of the proposed verifiable training method with different encoding schemes and rounding amounts (b). It highlights the significant reduction in storage costs achieved by efficient encoding and aggressive rounding compared to a naive approach and a previous method (Jia et al., 2021), demonstrating the scalability and efficiency of the proposed approach.", "section": "6 Empirical Results"}, {"figure_path": "bf0MdFlz1i/tables/tables_8_1.jpg", "caption": "Table 3: Average # of rev corrections performed by auditor per training step. Even at b = 32, auditing only requires 20-25 corrections (2e-6 to 9e-6% of samples) per training step.", "description": "This table shows the average number of rounding corrections the auditor needs to perform per training step for different rounding amounts (b) in both the forward and backward passes of ResNet-50 and GPT-2 models.  The results demonstrate that even with a high-precision rounding (b=32), the number of corrections required is extremely low (less than 0.01% of samples), highlighting the efficiency of the proposed verifiable training method.", "section": "6 Empirical Results"}, {"figure_path": "bf0MdFlz1i/tables/tables_8_2.jpg", "caption": "Table 4: Adaptive thresholds identified for different operations using Algorithm 3 with b = 32.", "description": "This table shows the adaptive thresholds (\u03c4) determined by Algorithm 3 for different layer types in neural networks (2D Convolution, Batch Norm, Linear, Layer Norm).  The threshold is crucial for efficiently controlling nondeterminism during training by selectively logging rounding decisions. The dimensions shown represent the input/output shapes for each layer.", "section": "5.4 Reducing storage cost"}, {"figure_path": "bf0MdFlz1i/tables/tables_14_1.jpg", "caption": "Table 5: Training time requirements, including Merkle tree operations (at k = 5), for 1 step of training broken down by stage of our verifiable training process. Note that reported times are specific to the particular dataset, batch size, and task, and using a non-optimized prototype codebase \u2013 therefore the relative increase is time is more important.", "description": "This table shows the training time requirements for both the trainer and auditor for one step of training for ResNet-50 and GPT-2. The time is broken down into the original time without any rounding or disk I/O, the trainer time and the auditor time. The relative increase in time for both trainer and auditor is more important than the absolute time.", "section": "6.1 Implementation and Findings"}, {"figure_path": "bf0MdFlz1i/tables/tables_14_2.jpg", "caption": "Table 6: Comparison of model divergence due to data ordering versus GPU non-determinism. Reported numbers are averaged between 10 pairs of models, error bars are standard deviation.", "description": "This table compares the model divergence caused by data ordering and GPU non-determinism.  The metrics used are the L2 weight difference (the Euclidean distance between the model weights) and the L2 output distance (the difference in model outputs). The results show that data ordering has a much greater impact on model divergence than GPU non-determinism.", "section": "6 Empirical Results"}]