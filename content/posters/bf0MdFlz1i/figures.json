[{"figure_path": "bf0MdFlz1i/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our scheme. After an auditor challenges a trainer, they train the model, storing weights in a Merkle tree, and enter a binary search procedure to identify the exact steps of the dispute. We show how to control GPU nondeterminism between auditor and trainer, expanding the set of potential auditors.", "description": "This figure illustrates the optimistic verifiable training scheme proposed in the paper.  A malicious trainer trains a model for a client, potentially introducing a training-time attack (e.g., data poisoning). An auditor can then challenge the trainer by replicating the training process using their own resources.  To ensure correctness, model weights are stored in a Merkle tree, allowing for efficient verification.  A binary search game is used to pinpoint any discrepancies. The scheme addresses the challenge of GPU nondeterminism, making verification robust across different GPU architectures.", "section": "1 Introduction"}, {"figure_path": "bf0MdFlz1i/figures/figures_3_1.jpg", "caption": "Figure 2: Even after ensuring the same software version, random seed, and use of deterministic algorithms via library flags, training nondeterminism persists between three GPU types.", "description": "This figure shows the results of training the same model (ResNet-50 for image classification and GPT-2 for text generation) on three different NVIDIA GPUs (A40, Titan XP, and RTX 2080 Ti) using the same software version, random seed, and deterministic algorithms. Despite these measures, significant differences in the model's outputs are observed, as indicated by the varying accuracy and perplexity scores for the same inputs. This demonstrates that nondeterminism persists even under controlled conditions. ", "section": "GPU Nondeterminism"}, {"figure_path": "bf0MdFlz1i/figures/figures_5_1.jpg", "caption": "Figure 3: Divergence between outputs on two different GPUs (in FP64) for a given function and input can result in different rounding choices when rounding to the nearest FP32. We only wish to log rounding decisions for Case A, where the auditor should copy the trainer\u2019s rounding choice in order to reach the same value. This requires defining a logging region, determined by a threshold \u03c4, the rounding boundary. Consider Case A in Figure 3, which shows a divergence in the output of a computation using FP64 on two different GPUs. Because the outputs of GPU 1 and 2 are on different sides of the boundary, rounding to the nearest FP32 results in different values, introducing error.", "description": "This figure illustrates how nondeterminism in floating-point arithmetic across different GPUs can lead to different rounding results, even when starting with the same high-precision input.  It introduces the concept of a \"logging region\" and a threshold (\u03c4) to control when rounding decisions are recorded and shared between the trainer and auditor to maintain consistency. The three cases (A, B, C) show different scenarios and how the threshold affects the logging strategy.", "section": "5 Method Overview"}, {"figure_path": "bf0MdFlz1i/figures/figures_7_1.jpg", "caption": "Figure 4: We successfully control for nondeterminism between GPU types for both ResNet-50 (a.) and GPT-2 (b.) tasks, while standard training and simple rounding without performing rev corrections result in model divergence over the course of training. Stronger rounding has minimal affect to model performance (c.), but at the cost of increasing time for trainer (d.).", "description": "This figure shows the results of applying different training methods to ResNet-50 and GPT-2 models on different GPUs.  (a) and (b) demonstrate that the proposed method successfully prevents model divergence caused by hardware nondeterminism, unlike standard training and simple rounding. (c) and (d) analyze the trade-off between rounding strength and performance, showing that stronger rounding improves determinism but increases training time.", "section": "6 Empirical Results"}, {"figure_path": "bf0MdFlz1i/figures/figures_12_1.jpg", "caption": "Figure 5: We define rounding to b bits as rounding to the nearest 32-bit FP number that has 0s in the last 32 - b bits of the mantissa, after accounting for the exponent.", "description": "This figure illustrates how rounding to a certain number of bits (b) in floating-point representation works.  It shows that rounding to b bits means selecting the nearest 32-bit floating-point number with zeros in the least significant 32-b bits of the mantissa.  The exponent part of the number remains unchanged by the rounding operation.  This process is crucial for controlling non-determinism in the training process, as it ensures that even with different GPUs, rounding will produce the same value when the intermediate computations are performed with higher precision.", "section": "5 Method Overview"}]