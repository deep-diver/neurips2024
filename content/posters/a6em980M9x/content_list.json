[{"type": "text", "text": "Amortized Fourier Neural Operators ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zipeng Xiao1, Siqi $\\mathbf{Kou^{1}}$ , Zhongkai Hao2, Bokai Lin1, Zhijie Deng1 ", "page_idx": 0}, {"type": "text", "text": "1 Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University 2 Dept. of Comp. Sci. & Tech., Tsinghua University {xiaozp_25, happy-karry}@sjtu.edu.cn, hzj21@mails.tsinghua.edu.cn, {19821172068,zhijied}@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fourier Neural Operators (FNOs) have shown promise for solving partial differential equations (PDEs). Typically, FNOs employ separate parameters for different frequency modes to specify tunable kernel integrals in Fourier space, which, yet, results in an undesirably large number of parameters when solving high-dimensional PDEs. A workaround is to abandon the frequency modes exceeding a predefined threshold, but this limits the FNOs\u2019 ability to represent high-frequency details and poses non-trivial challenges for hyper-parameter specification. To address these, we propose AMortized Fourier Neural Operator (AM-FNO), where an amortized neural parameterization of the kernel function is deployed to accommodate arbitrarily many frequency modes using a fixed number of parameters. We introduce two implementations of AM-FNO, based on the recently developed, appealing Kolmogorov\u2013Arnold Network (KAN) and Multi-Layer Perceptrons (MLPs) equipped with orthogonal embedding functions respectively. We extensively evaluate our method on diverse datasets from various domains and observe up to $31\\%$ average improvement compared to competing neural operator baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural operators (NOs) have been extensively studied for their potential in accelerating the solving of partial differential equations (PDEs) in science and engineering fields [19, 20, 18, 24, 25, 2]. In contrast to approaches limited to specific discretizations or PDE instances [8, 36, 28, 29], NOs characterize the solving operator of a family of PDEs across different discretizations and hence enjoy higher efficiency and usability, e.g., for weather forecasting [26] and material analysis [5]. ", "page_idx": 0}, {"type": "text", "text": "Fourier neural operator (FNO) [18] and its variants [17, 33, 31, 35] stand out as a significant subclass of NOs, which explore the convolution theorem and Fast Fourier Transform (FFT) to efficiently performs kernel integral, a central module for the learning operator of PDEs. Typically, FNO separately parameterizes the values of the Fourier-transformed kernel function for different frequency modes, and hinges on frequency truncation\u2014abandons the parameters corresponding to frequencies exceeding some threshold\u2014to reduce modeling costs, particularly for high-dimensional PDEs. ", "page_idx": 0}, {"type": "text", "text": "Frequency truncation can be problematic when solving PDE systems with intense high-frequency components. To address this, IFNO dynamically adjusts the threshold for frequency truncation during training, though it still experiences exponential parameter growth with increasing dimensionality [35]. This complexity can result in substantial memory consumption and hinder the development of largescale pretrained models [9]. Conversely, AFNO [7] employs a shared MLP to transform the outcomes of FFT, but the uniform treatment of frequency modes may constrain expressiveness and lead to suboptimal performance. ", "page_idx": 0}, {"type": "image", "img_path": "a6em980M9x/tmp/2b10e1f18b1d9e05f8ec3cb65139d779d889aa2d4a244b5bf8bc9719616ddd16.jpg", "img_caption": ["Figure 1: Comparison between FNO and AM-FNO: FNO assigns each value at the discretized frequencies of the Fourier-transformed kernel function as a learnable parameter, while AM-FNO utilizes neural network parameterization (MLP or KAN) to approximate the mapping between frequencies and function values. The frequencies are embedded using a set of orthogonal basis functions before being processed by the MLP. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We address this by developing AMortized Fourier Neural Operator (AM-FNO), where we introduce extra neural networks (NNs) to specify the kernel integral operator to amortize the modeling cost. As illustrated in Figure 2, AM-FNO is simple and intuitive\u2014learnable NN transformations are leveraged to directly define the Fourier-space kernel function to accommodate arbitrarily many frequency modes at the cost of a fixed number of parameters. An amortized parameterization also provides an inherent regularization mechanism for resisting overfitting to potential high-frequency noise. ", "page_idx": 1}, {"type": "text", "text": "The NN transformation in AM-FNO can be flexibly defined. One natural choice is the Kolmogorov\u2013Arnold Network (KAN) due to its superior accuracy in function fitting [21]. Doing so, we, for the first time, reveal the potential of KAN for the operator learning of PDEs. Considering the widely criticized inefficiency issues of KAN in both time and memory consumption, we also investigate the regular Multi-Layer Perceptrons (MLPs) for amortized parameterization. We empirically identify the necessity of embedding the frequency modes with orthogonal basis functions before MLP transformation. ", "page_idx": 1}, {"type": "text", "text": "We experiment on challenging benchmarks governed by diverse typical PDEs, covering six standard PDE benchmarks [18, 17, 30]. AM-FNO (KAN) and AM-FNO (MLP) achieve an average $22\\%$ and $31\\%$ reduction in relative error on these benchmarks, respectively. We also analyze the error of different frequency modes, and the results show that our models outperform in all frequency ranges. Additionally, we perform zero-shot super-resolution on three benchmarks to assess the generalization ability of AM-FNO across discretizations. We observe that AM-FNOs outperform the baselines and achieve lower error even compared to FNO trained on the test resolution. The results reflect that AM-FNOs have the promising potential to substantially reduce both the data collection and training costs for solving high-resolution PDE problems. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural Operators. Neural operators have attracted considerable interest for their capacity to map infinite-dimensional function spaces [24, 19, 1], thereby facilitating solutions across diverse discretizations without retraining. The pioneering work DeepONet [24] introduces a trunk and branch network architecture grounded in the universal approximation theorem for operators. Transformerbased neural operators represent a notable line of work in the domain. Galerkin Transformer [2] proposes self-attention operators that theoretically correspond to a learnable kernel integral operator and projection. OFormer [16] introduces an architecture with input and query encoders for querying arbitrary output locations. GNOT [10] proposes a heterogeneous normalized attention layer to encode different input information. ", "page_idx": 1}, {"type": "text", "text": "FNO and its Variants. Fourier neural operator (FNO) [18] represents a novel approach with exceptional efficiency and accuracy due to its ability to learn kernel integral operators in the Fourier domain. The theoretical proof establishes its capability to approximate arbitrary continuous operator [15]. Tailored for addressing multiphase flow problems, U-FNO [33] augments its representation in higher frequency information through U-Net paths. Geo-FNO [17] extends the applicability of FNO beyond uniform grids by employing a learnable mapping from irregular domains to uniform latent meshes. ", "page_idx": 1}, {"type": "text", "text": "F-FNO [31] reduces the model parameters and addresses performance degradation with increasing layers by factorizing the integral operator and enhancing the architecture. AFNO [7] transforms the function values after FFT with a shared MLP for each frequency, effectively reducing the parameter count. FNOs have made notable contributions across various challenging tasks [6, 25, 33, 26]. However, minimizing model complexity while effectively managing high-frequency information and ensuring generalization across different discretizations poses a persistent challenge in FNOs. Additionally, to the best of our knowledge, there has been no attempt to incorporate KANs with neural operators. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents the foundations of operator learning and the integral operator used in FNO. ", "page_idx": 2}, {"type": "text", "text": "3.1 Operator Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider the input function space $\\pmb{\\mathcal{A}}=\\pmb{\\mathcal{A}}(\\pmb{D};\\mathbb{R}^{d_{a}})$ and the target function space $\\mathcal{U}=\\mathcal{U}(D;\\mathbb{R}^{d_{u}})$ defined on a bounded and open set $D\\subset\\mathbb{R}^{d}$ . Operator learning seeks to learn a $\\theta$ -parameterized operator $g_{\\theta}$ to approximate the ground-truth mapping $\\mathcal{G}:\\mathcal{A}\\rightarrow\\mathcal{U}$ specified by a PDE. This learning process is based on a finite set of function observations $\\{a_{i},u_{i}\\}_{i=1}^{N}$ , where functions $a_{i}$ and $u_{i}$ are discretized on meshes $\\{x_{j}\\in D\\}_{j=1}^{M}$ . The optimization problem is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\|\\mathcal{G}_{\\theta}(a_{i})-u_{i}\\|_{2}}{\\|u_{i}\\|_{2}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the regular mean-squared error (MSE) is extended with a normalizer $\\lVert u_{i}\\rVert_{2}$ to handle scale variations across benchmarks, denoted as $l_{2}$ relative error. The relative error can also be substituted with other loss functions. ", "page_idx": 2}, {"type": "text", "text": "3.2 Fourier Integral Operator ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A learnable kernel integral is a central module for defining mappings among functions. Specifically, we denote the hidden state of the input function in the $l$ -th transformation stage as $h^{(l)}(y):D\\rightarrow\\mathbb{R}^{d_{h}}$ , where $d_{h}$ represents the dimensionality, assumed to be consistent across all stages. The kernel integral operator makes the following transformation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\mathcal{K}(h^{(l)}))(x)=\\int_{D}\\kappa(x,y)h^{(l)}(y)d y,\\quad\\forall x\\in D\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, the integral is not computation-friendly within deep learning frameworks. To address this, FNO [18] assumes the kernel is shift-invariant, i.e., $\\kappa(x,y)=\\kappa(x-y)$ , and leverages the convolution theorem to efficiently compute the integral in the Fourier domain: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\mathcal{K}(h^{(l)}))(x)=\\mathcal{F}^{-1}(R\\cdot\\mathcal{F}(h))(x),\\quad\\forall x\\in D\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{F}$ and ${\\mathcal{F}}^{-1}$ denote the FFT and its inverse (IFFT), and $R(k):{\\mathcal{E}}\\to\\mathbb{C}^{(d_{h}\\times d_{h})}$ represents the Fourier-transformed complex-valued kernel function with $k\\,\\in\\,\\mathcal{E}$ denoting a frequency mode. Typically, FNO individually parameterizes the values of $R(k)$ for a fixed range of frequency modes (denoting the number as $k_{T}$ ) to avoid high modeling costs. This, yet, limits the exploration of highfrequency details in the function and poses non-trivial challenges for hyper-parameter specification. ", "page_idx": 2}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section elaborates on amortized FNO (AM-FNO), which amortizes an arbitrary number of frequency modes by sharing a fixed number of parameters. ", "page_idx": 2}, {"type": "text", "text": "4.1 Amortized Parameterization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Instead of parameterizing the kernel function point-by-point, we propose to build the mapping between frequency and the values of the Fourier-transformed kernel function using an NN. On one ", "page_idx": 2}, {"type": "text", "text": "hand, this mitigates the issue that the number of parameters increases significantly with that of frequency modes and dimensionality of PDEs. On the other hand, this approach avoids AFNO\u2019s uniform transformation, ensuring richer expressiveness in the Fourier domain. ", "page_idx": 3}, {"type": "text", "text": "Concretely, by the rule of Fourier transformation, there is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(k)=\\int_{D}\\kappa(x)e^{-2i\\pi x\\cdot k}d x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the output of $R$ is a matrix, so we can use $R_{p,q},p,q\\in\\{1,2,\\ldots,d_{h}\\}$ to denote the complex scalar-valued function yielding one element of the matrix output. Our AM-FNO directly uses NNs to define the real and imaginary parts of the function $R_{p,q}$ to maximize the modeling flexibility. Formally, there is ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{p,q}(k):=\\mathrm{NN}_{r e}(k)+\\sqrt{-1}\\,\\mathrm{NN}_{i m}(k).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The detailed implementations of the two NNs have no essential difference, so we only discuss one of them in the following. ", "page_idx": 3}, {"type": "text", "text": "4.2 Kolmogorov-Arnold Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Kolmogorov-Arnold Networks (KANs) have been empirically shown to show promise in function approximation [21]. According to theoretical analysis, KANs are usually defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{KAN}(\\pmb{x})=\\sum_{j=1}^{2H+1}\\eta_{j}^{\\prime}(\\sum_{t=1}^{H}\\eta_{j,t}(x_{t}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta_{j}^{\\prime},\\eta_{j,l}:\\mathbb{R}\\to\\mathbb{R}$ denote the learnable basis functions. In practice, we can generalize the above definition and set a KAN layer as (with $\\textbf{\\em x}$ as a $H$ -dim variable): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}^{\\prime}=\\left(\\begin{array}{c c c c}{\\eta_{1,1}(\\cdot)}&{\\eta_{1,2}(\\cdot)}&{\\dots\\cdot}&{\\eta_{1,H}(\\cdot)}\\\\ {\\eta_{2,1}(\\cdot)}&{\\eta_{2,2}(\\cdot)}&{\\dots\\cdot}&{\\eta_{2,H}(\\cdot)}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\eta_{H^{\\prime},1}(\\cdot)}&{\\eta_{H^{\\prime},2}(\\cdot)}&{\\dots}&{\\eta_{H^{\\prime},H}(\\cdot)}\\end{array}\\right)\\pmb{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can specify the function by learnable coefficients and multiple local B-spline basis functions [21]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta(x)=w(r(x)+\\operatorname{spline}(x)),\\quad\\operatorname{spline}(x)=\\sum_{g}c_{g}B_{g}(x)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r(x)$ represents a basis function (typically sigmoid linear unit or SiLU [3]) and $w$ is a factor controlling the magnitude. ", "page_idx": 3}, {"type": "text", "text": "We utilize two two-layer KANs in each layer of AM-FNO to define the real and imaginary parts of $R_{p,q}$ . The inputs to the network are all frequencies of $h^{(l)}$ after FFT. In fact, we can share weights among the KANs associated with $R_{p,q}$ with different $p$ and $q$ . Empirical results (Table 4) indicate superior performance compared to the corresponding MLP implementation. ", "page_idx": 3}, {"type": "text", "text": "4.3 Multi-Layer Perceptrons ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, KANs require extensive training time. To address this, we propose an alternative parameterization aimed at improving the performance of Multi-Layer Perceptrons (MLPs). Despite its universal approximation ability, MLPs empirically suffer from compromising performance. The spectral bias of vanilla MLPs, i.e., their tendency to favor low-frequency functions, may limit their capacity to represent more complex functions. Motivated by the success of leveraging orthogonal basis functions for function approximation [12, 14, 27], we propose to augment the MLP with orthogonal embedding functions to construct AM-FNO. ", "page_idx": 3}, {"type": "text", "text": "Specifically, we can embed the frequency mode input using a set of orthogonal functions before the MLP transformation. The Fourier basis is a natural choice due to its capacity to capture high-frequency components, thereby enhancing the high-frequency representation of vanilla MLPs. However, our empirical results show that Chebyshev basis functions can perform better (see Table 4). Of note, we use only the first $n_{\\mathrm{{max}}}$ orthogonal basis functions in the family for the parameterization as we cannot employ infinite parameters. ", "page_idx": 3}, {"type": "image", "img_path": "a6em980M9x/tmp/56672fa9d1eb2eb2c2cb3b88bf38ee835a59b103b8455f8065251ae5e9c23c21.jpg", "img_caption": ["Figure 2: AM-FNO structure for 2D PDEs: The input function $a$ is mapped to a higher-dimensional space. Stacked operators and activation functions are applied for function propagation. Within the operator layers, a linear transformation $R$ is applied to $\\bar{h}^{(l)}$ after FFT, followed by a feed-forward network (FFN) after the Inverse Fast Fourier Transform (IFFT). The values of $R$ result from KAN or multiplying the MLP transformations of selected one-dimensional orthogonal basis functions (w denotes linear weights.). Finally, the function is projected to the solution dimension space. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Factorization trick for high-dimensional PDEs. For the Fourier-transformed kernel, the dimensionality of the frequency modes matches that of the PDE being solved. To approximate functions with $d$ -dimensional inputs, a common approach is to construct $d_{\\cdot}$ -dimensional orthogonal functions based on one-dimensional basis functions. Specifically, for $n_{\\mathrm{max}}$ one-dimensional basis functions in each of the d dimensions, the complete space contains ndmax high-dimensional basis functions, resulting in an exponential complexity for modeling. ", "page_idx": 4}, {"type": "text", "text": "Motivated by the dimension factorization in the integral operator [31], we separate the input dimensions and construct the final kernel by the products of the kernel approximated in each dimension. Such a process is illustrated in Figure 2. In this way, the total parameter count scales linearly w.r.t. the dimension of the PDEs. ", "page_idx": 4}, {"type": "text", "text": "4.4 AM-FNO Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In FNO, a stacked structure is employed to approximate the entire mapping $\\mathcal{G}$ , as illustrated below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\mathcal{P}\\circ\\mathcal{Q}^{(L)}\\circ\\cdots\\circ\\mathcal{Q}^{(1)}\\circ\\mathcal{L}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}$ maps the input function $a$ to the hidden state $h^{(0)}$ , and $\\mathcal{P}$ maps the hidden state $h^{(L)}$ to the output function $u$ , both in a point-wise manner. $\\mathcal{Q}:h^{(l)}\\to h^{(l+1)}$ is the operator layer responsible for the iterative update: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{Q}(h^{(l)})=\\sigma(W h^{(l)}+K(h^{(l)})+b)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\kappa$ represents the kernel integral operator in Equation (3), and $b$ represents the bias. FNO employs a pointwise linear mapping with $W$ to enable the propagation of high-frequency information, but this is empirically limited, validated by the results in Table 8. ", "page_idx": 4}, {"type": "text", "text": "Given the non-truncated Fourier transform of the kernel function $R$ in AM-FNO, we employ the operator layer architecture in [31], which replaces the linear map $W$ with residual connection [11]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{N}(h^{l})=h^{(l)}+(W_{2}\\sigma(W_{1}\\mathcal{K}(h^{(l)})+b_{1})+b_{2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 2, we also incorporate activation functions between the operator layers for enhanced flexibility. The resultant model structure is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\mathcal{P}\\circ\\mathcal{N}^{(L)}\\circ\\sigma\\circ\\mathcal{N}^{(L-1)}\\circ\\sigma\\circ\\cdots\\circ\\mathcal{N}^{(1)}\\circ\\mathcal{L}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "a6em980M9x/tmp/52bfcb3d6e9e336f06b0106a3cb91f74b5eba8b7b6b946d5544deb6758729a0b.jpg", "table_caption": ["Table 1: Overview of benchmarks including their spatial dimensions $d$ , spatial resolution $M$ , temporal resolution $N_{t}$ , and training data $N_{\\mathrm{train}}$ and test data $N_{\\mathrm{test}}$ . "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we validate the effectiveness of our proposed method by conducting extensive experiments on challenging benchmarks governed by typical solid and fluid PDEs. ", "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Benchmarks. We evaluate the performance of AM-FNO on six well-established benchmarks. These benchmarks include Burger, Darcy, and NS-2D, which are presented in regular grids with varying dimensions [18]. We extend our experiments to assess the method\u2019s performance in different geometries, including Pipe, Airfoil, and Elasticity benchmarks [17] Additionally, we incorporate the compressible fluid dynamics (CFD) 1D and 2D benchmarks [30], which involves more high-frequency information. The benchmarks are summarized in Table 1. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We conduct a comparative evaluation of our neural operator against seven baseline methods. These baselines include well-recognized approaches such as FNO [18] and its variants Geo-FNO [17], U-FNO [33], F-FNO [31] and AFNO [7]. Additionally, we consider other models, including OFormer [16] and LSM [34]. Notably, LSM represents the latest state-of-the-art (SOTA) neural operator among the baselines. ", "page_idx": 5}, {"type": "text", "text": "Implementation details. We train all models for 500 epochs using the AdamW optimizer [23] with a cosine annealing scheduler [22]. The initial learning rate is $10^{-3}$ , and the weight decay is set to $10^{-4}$ . Our models consist of 4 layers with a width of 32 and process all the frequency modes of training data. The Gaussian Error Linear Unit (GELU) is used as the activation function [13]. For AM-FNO (KAN), the number of spline grids is selected from $\\{24,32,48\\}$ , while for AM-FNO (MLP), the number of basis functions is set to 32 or 48. AM-FNO (MLP) utilizes Chebyshev basis functions as the orthogonal basis functions, as elaborated in the appendix. The batch size is selected from $\\{4,8,16,32\\}$ , and the experiments are conducted on a single 4090 GPU. The evaluation metric and training loss are based on the $l_{2}$ relative error in Equation (1), unless otherwise specified. We employ the transformation method from geo-FNO [17] to map between irregular input domains and uniform meshes for the Elasticity benchmark on point clouds. More details about the baselines can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The main results are shown in Table 2. Our models consistently achieve state-of-the-art (SOTA) performance on all six benchmarks with various PDEs, geometries, and dimensions. AM-FNOs exhibit a significant average performance improvement of $22\\%$ from KAN implementation and $31\\%$ from MLP implementation compared to the top-performing baseline. We provide a comparison of GPU memory and training time in Appendix D, showing that although AM-FNO retains all frequency modes, it achieves comparable memory usage and training time to other FNOs. ", "page_idx": 5}, {"type": "text", "text": "AM-FNOs significantly reduce prediction error on Darcy and NS-2D benchmarks, standard benchmarks with strong low-frequency components. Specifically, AM-FNO (KAN) reduces the error by $39\\%$ (2.73e-3) and $11\\%$ (1.40e-2), while AM-FNO (MLP) reduces the error by $40\\%$ (2.80e-3) and $30\\%$ (3.69e-2). This improvement can be attributed to AM-FNO\u2019s kernel parameterization, which effectively captures the low-frequency information. Additionally, AM-FNO demonstrates robustness across irregular geometries, with reductions of $5\\%$ (3.30e-4), $32\\%$ (1.66e-3) and $7\\%(1.5\\mathrm{e}{-3})$ from AM-FNO (KAN), and $12\\%$ (7.50e-4), $34\\%$ (1.76e-3) and $10\\%$ (2.20e-3) from AM-FNO (MLP) in prediction error on Airfoil, Pipe and Elasticity benchmarks. For CFD-1D and CFD-2D benchmarks characterized by stronger high-frequency components, AM-FNO (KAN) achieves improvements of $25\\%$ (6.10e-3) and $40\\%$ (1.83e-3), while AM-FNO (MLP) achieves improvements of $40\\%$ (9.70e-3) and $52\\%$ (2.36e-3). The promotion highlights the effectiveness of our method of handling highfrequency components. We also find that AM-FNO (MLP) outperforms AM-FNO (KAN) across all benchmarks, likely due to the enhanced expressiveness of the orthogonal embedding. ", "page_idx": 5}, {"type": "table", "img_path": "a6em980M9x/tmp/3daf78ce923b1af45f867120dc81ef4a12828b74c0c339b3c65547e1465afd21.jpg", "table_caption": ["Table 2: Comparison of the primary findings across six benchmark tests with six baseline methods. Lower scores signify superior performance, with the best outcome highlighted in bold and the secondbest outcome underlined. The presence of a \u201c- \" indicates that the corresponding baseline is incapable of addressing the benchmark. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.3 Frequency-Based Error Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "CFD-1D. To assess the performance across various frequency modes, we calculate the error of different frequency modes after FFT on CFD-1D benchmark and incorporate FNO without truncation $(\\mathrm{FNO^{+}})$ for comparison. The results are visualized in Figure 3. ", "page_idx": 6}, {"type": "image", "img_path": "a6em980M9x/tmp/a2064cce5f78edceaad6113a89d1ef8f30b4da593ffda88be1fbdfdea64ffd86.jpg", "img_caption": ["Figure 3: Comparison of L2 norm error on different frequency modes on CFD-1D benchmark. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "As shown, the errors primarily stem from the first few frequency modes and decrease as the frequency increases. The baselines exhibit similar errors in the truncated frequency range, whereas our models demonstrate significantly lower errors. Our models maintain an advantage over the baselines in the initial modes within the truncated frequency range. As the frequency increases, the strength of the high-frequency components diminishes, and all the errors become negligible. ", "page_idx": 6}, {"type": "text", "text": "CFD-2D. We further evaluate the performance across various frequency modes with the metrics outlined in [30] (detailed in Appendix B) and present the results on CFD-2D benchmark in Table 3. ", "page_idx": 6}, {"type": "text", "text": "The results showcase that $\\mathrm{FNO^{+}}$ demonstrates lower train error but higher test error than FNO and U-FNO. Notably, $\\mathrm{FNO^{+}}$ exhibits similar prediction errors to FNO in the high-frequency range, suggesting potential overfitting to the training data. We propose that the substantial complexity parameterization of $\\mathrm{FNO^{+}}$ may render it sensitive to high-frequency details, limiting the effectiveness of the additional parameters in handling such components. The fL2 error of U-FNO across all frequency ranges is lower than FNO, which can be attributed to the enhanced expressiveness enabled by the additional U-Net architecture. AM-FNO (MLP) achieves the lowest training error and fL2 error across all frequency ranges, while AM-FNO (KAN) achieves the second lowest fL2 error. Specifically, AM-FNO (MLP) and AM-FNO (KAN) achieve $55\\%(7.04\\mathrm{e}{-2})$ and $50\\%(5.15\\mathrm{e}{-2})$ reduction in the high-frequency range. This outcome can be attributed to our amortized parameterization, which significantly reduces model complexity while maintaining adequate expressiveness to approximate the Fourier-transformed kernel function. ", "page_idx": 6}, {"type": "table", "img_path": "a6em980M9x/tmp/36eef0c3a3604d30da7a6121f8bc2d94a0523d56e7fd0cab54161e92831cfecb.jpg", "table_caption": ["Table 3: Comparison of the error in different frequency regions on CFD-2D benchmarks. Each complex-valued parameter is considered as 2 in the parameter count (Param). Train error (Train Err.) and test error (Test Err.) are evaluated using the $l_{2}$ relative error at each time step. fL2 signifies the $l_{2}$ relative error in Fourier space (fRMSE) pertaining to the low, middle, and high-frequency regions. $\\mathrm{FNO^{+}}$ refers to FNO without frequency truncation. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "a6em980M9x/tmp/97bcb2ddecb3be7e52b4916cdb06d51585141b120305a095f3c3413fc2c80c24.jpg", "table_caption": ["Table 4: Comparison of the $l_{2}$ relative error for different components of AM-FNO (MLP) on Darcy, Airfoil, and Pipe benchmarks. Chebyshev basis functions are substituted with triangular basis functions (TBF) and non-orthogonal polynomial basis functions (PBF). A version of the model without orthogonal embedding (Non) is included for comparison. The training time and memory requirements are derived from the Airfoil benchmark. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.4 Ablation Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct a detailed ablation study to assess the effectiveness of different components and hyperparameters of our models. ", "page_idx": 7}, {"type": "text", "text": "Necessity of Orthogonal Embedding. We study the impact of orthogonal embedding on Darcy, Airfoil, and Pipe benchmarks. Table 4 presents the findings. Although AM-FNO (KAN) outperforms the MLP version without embedding (Non), its efficiency is significantly lower than that of versions utilizing MLPs. Removal of the orthogonal embedding leads to a notable performance decline across all three benchmarks. Meanwhile, replacing the orthogonal functions with non-orthogonal polynomial basis functions results in the highest prediction error among the baseline models. These outcomes showcase the efficacy and indispensability of the orthogonal embedding. To validate the robustness of the embedding, we use triangular basis functions and observe comparable errors on the Darcy and Pipe benchmarks, but higher errors on the Airfoil benchmark. We attribute this difference to the accuracy of function approximation achieved by Chebyshev basis functions. ", "page_idx": 7}, {"type": "text", "text": "Influence of Some Hyperparameters We conduct experiments to assess how prediction error varies with different numbers of basis functions, hidden sizes of KANs, and grid sizes of splines (linearly scaled with local spline count) in KANs. Figure 4 shows the results. The left figure illustrates that the error decreases with an increasing number of basis functions in orthogonal embedding on both benchmarks. This reduction is particularly evident initially on the Airfoil benchmark, followed by diminishing returns. In this case, employing 24 basis functions achieves a favorable balance between efficiency and accuracy. This performance enhancement can be attributed to the increased expressiveness from the additional orthogonal basis functions. In the middle figure, while the error decreases with increasing hidden size, we observe a decline in performance compared to KANs with larger grid sizes but smaller hidden sizes. This may suggest that the expressive power of KANs is primarily derived from the number of local spline functions (grid size). The right figure displays a trend similar to the left one: a noticeable decrease initially, followed by a less pronounced reduction later. In this study, we suggest increasing the grid size to enhance performance rather than focusing on adjusting the hidden layer size. We also present the performance of AM-FNOs, retaining the same frequency modes as other FNOs, in Table 9. AM-FNOs consistently outperform baseline models, underscoring the advantages of our amortized parameterization over the standard FNO approach. ", "page_idx": 7}, {"type": "image", "img_path": "a6em980M9x/tmp/d18653099bf7bfc884df5684ee3bbcfd6ce0d9618c5b75c6505285fa291eb365.jpg", "img_caption": ["Figure 4: $l_{2}$ relative error varies w.r.t. the number of basis functions (Left), hidden layer size of KANs (middle), and grid size of KANs (right) on Darcy and Airfoil benchmarks. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "a6em980M9x/tmp/976d746373b282e11fc60e82601d2657a11c6208211519ef56db9d272df1a318.jpg", "img_caption": ["Figure 5: Comparison of zero-shot super-resolution absolute errors on NS-2D benchmark. The top row displays the ground truth, FNO, U-FNO, AM-FNO (KAN), and AM-FNO (MLP) predictions from left to right. The bottom row illustrates their errors. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.5 Zero-Shot Super-Resolution ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A notable characteristic of neural operators is their ability to generalize across various discretizations. We conduct experiments training on lower resolution data and evaluate on higher resolution data on NS-2D benchmark. We visualize the results in Figure 5 and provide the numerical results in Table 5. ", "page_idx": 8}, {"type": "text", "text": "U-FNO exhibits a significant performance degradation when evaluated on higher resolution. This decline can be attributed to the convolutional operation in the U-Net architecture, which possesses a fixed receptive field and cannot effectively generalize across different discretizations. In contrast, FNO and AM-FNOs demonstrate the ", "page_idx": 8}, {"type": "table", "img_path": "a6em980M9x/tmp/9995b0ba6c4a08574cfc35f11fe8152a586674dc8fa22bb30626eef777b846e5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Comparison of $l_{2}$ relative error across different resolutions on NS-2D benchmark, with all models trained with $32\\times32$ resolution. ", "page_idx": 8}, {"type": "text", "text": "ability to generalize across different discretizations. Notably, AM-FNOs achieve superior performance at $64\\times64$ resolution compared to baselines trained with the same resolution (see Table 2), which underscores the data efficiency of AM-FNOs. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes AM-FNOs to improve Fourier neural operator (FNO)\u2019s efficiency in addressing PDEs without frequency truncation. Our approach utilizes Kolmogorov\u2013Arnold Networks (KANs) and Multi-Layer Perceptrons (MLPs) with orthogonal embedding functions to mitigate exponential complexity and overfitting to high-frequency noise. Comprehensive experiments across various datasets demonstrate the effectiveness of AM-FNOs compared to baseline approaches. ", "page_idx": 9}, {"type": "text", "text": "Limitations. This work attempts to enhance FNO\u2019s handling of high-frequency information but has the following limitations. The benchmarks used are idealized physical systems, excluding real-world complex problems. Meanwhile, although AM-FNOs reduce the parameter count, the extremely high-dimensional PDEs still pose challenges for FNOs due to the complexity of FFT. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSF of China (No. 62306176), Natural Science Foundation of Shanghai (No. 23ZR1428700), CCF-Zhipu.AI Large Model Innovation Fund, and CCF-Baichuan-Ebtech Foundation Model Fund. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural networks for parametric pdes. The SMAI journal of computational mathematics, 7:121\u2013157, 2021.   \n[2] Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing systems, 34:24924\u201324940, 2021.   \n[3] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3\u201311, 2018.   \n[4] David Elliott, DF Paget, GM Phillips, and PJ Taylor. Error of truncated chebyshev series and other near minimax polynomial approximations. Journal of approximation theory, 50(1):49\u201357, 1987.   \n[5] Somdatta Goswami, Minglang Yin, Yue Yu, and George Em Karniadakis. A physics-informed variational deeponet for predicting crack path in quasi-brittle materials. Computer Methods in Applied Mechanics and Engineering, 391:114587, 2022.   \n[6] Steven Guan, Ko-Tsung Hsu, and Parag V Chitnis. Fourier neural operator networks: A fast and general solver for the photoacoustic wave equation. arXiv preprint arXiv:2108.09374, 2021. [7] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. arXiv preprint arXiv:2111.13587, 2021.   \n[8] Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow approximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 481\u2013490, 2016.   \n[9] Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training. arXiv preprint arXiv:2403.03542, 2024.   \n[10] Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, and Jun Zhu. Gnot: A general neural operator transformer for operator learning. In International Conference on Machine Learning, pages 12556\u201312569. PMLR, 2023.   \n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[12] Mingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional neural networks on graphs with chebyshev approximation, revisited. Advances in neural information processing systems, 35:7264\u20137276, 2022.   \n[13] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[14] Lina J Karam and James H McClellan. Complex chebyshev approximation for fir fliter design. IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, 42(3):207\u2013 216, 1995.   \n[15] Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for fourier neural operators. The Journal of Machine Learning Research, 22(1):13237\u201313312, 2021.   \n[16] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations\u2019 operator learning. arXiv preprint arXiv:2205.13671, 2022.   \n[17] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209, 2022.   \n[18] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations, 2020.   \n[19] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485, 2020.   \n[20] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations. Advances in Neural Information Processing Systems, 33:6755\u20136766, 2020.   \n[21] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Solja\u02c7ci\u00b4c, Thomas Y Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024.   \n[22] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.   \n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[24] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature machine intelligence, 3(3):218\u2013229, 2021.   \n[25] Nicholas H Nelsen and Andrew M Stuart. The random feature model for input-output maps between banach spaces. SIAM Journal on Scientific Computing, 43(5):A3212\u2013A3243, 2021.   \n[26] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.   \n[27] S Qian, YC Lee, RD Jones, CW Barnes, and K Lee. Function approximation with an orthogonal basis net. In 1990 IJCNN International Joint Conference on Neural Networks, pages 605\u2013619. IEEE, 1990.   \n[28] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686\u2013707, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[29] Jonathan D Smith, Kamyar Azizzadenesheli, and Zachary E Ross. Eikonet: Solving the eikonal equation with deep neural networks. IEEE Transactions on Geoscience and Remote Sensing, 59(12):10685\u201310696, 2020. ", "page_idx": 11}, {"type": "text", "text": "[30] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pfl\u00fcger, and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. Advances in Neural Information Processing Systems, 35:1596\u20131611, 2022.   \n[31] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators. In The Eleventh International Conference on Learning Representations, 2023.   \n[32] Lloyd N Trefethen. Approximation Theory and Approximation Practice, Extended Edition. SIAM, 2019.   \n[33] Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fno\u2014an enhanced fourier neural operator-based deep-learning model for multiphase flow. Advances in Water Resources, 163:104180, 2022.   \n[34] Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, and Mingsheng Long. Solving highdimensional pdes with latent spectral models. In International Conference on Machine Learning, 2023.   \n[35] Jiawei Zhao, Robert Joseph George, Yifei Zhang, Zongyi Li, and Anima Anandkumar. Incremental fourier neural operator. arXiv preprint arXiv:2211.15188, 2022.   \n[36] Yinhao Zhu and Nicholas Zabaras. Bayesian deep convolutional encoder\u2013decoder networks for surrogate modeling and uncertainty quantification. Journal of Computational Physics, 366:415\u2013447, 2018. ", "page_idx": 11}, {"type": "text", "text": "A Hyperparameters and Details for Models ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "AM-FNOs. For AM-FNO (MLP), Chebyshev polynomials are chosen due to their favorable theoretical accuracy [32, 4]. Formally, they are defined as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathrm{For}\\;k\\in[-1,1],\\quad T_{0}(k)=1,\\quad T_{1}(k)=k,}\\\\ {T_{n+1}(k)=2k T_{n}(k)-T_{n-1}(k)\\quad n\\geq1.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The polynomial demonstrates an increase in the degree of its highest power term concerning $k$ as $n$ increases. It can be alternatively formulated as: ", "page_idx": 11}, {"type": "equation", "text": "$$\nT_{n}(k)=\\cos(n\\operatorname{arccos}(k)),\\quad\\mathrm{for}\\:k\\in[-1,1].\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "For AM-FNO (KAN), we fix the grid size during training to ensure a consistent parameter count. The order of the spline is fixed as 3. ", "page_idx": 11}, {"type": "text", "text": "FNO and its Variants. We employ 4 layers with modes set to 12 and widths set to 32 for FNO and its variations (Geo-FNO, U-FNO, F-FNO). U-FNO incorporates the U-Net path in the last two layers. For AFNO, we set the width to 512 and use 4 layers to maintain a comparable parameter count. Notably, Geo-FNO reverts to the vanilla FNO when applied to benchmarks with regular grids, resulting in equivalent performance for Darcy and NS-2D benchmarks. ", "page_idx": 11}, {"type": "text", "text": "LSM. The model is employed with 8 basis operators and 4 latent tokens. The width of the first scale is set to 32, with a downsampling ratio of 0.5. ", "page_idx": 11}, {"type": "text", "text": "OFormer The depth of the encoder is fixed at 6, while the hidden dimension is set to 96. ", "page_idx": 11}, {"type": "text", "text": "B Metrics ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Following [30], we use fL2 error to quantify errors in different frequency ranges. It is computed as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{||\\mathcal{F}(u_{p r e d})-\\mathcal{F}(u_{t r u e})||_{2}}{||\\mathcal{F}(u_{t r u e})||_{2}}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $\\mathcal{F}$ denotes the FFT and the frequency range is restricted to $k_{\\mathrm{min}}\\leq k\\leq k_{\\mathrm{max}}$ . For fL2 low, $k_{\\operatorname*{min}}=0$ and $k_{\\operatorname*{max}}=4$ ; for fL2 mid, $k_{\\mathrm{min}}=5$ and $k_{\\mathrm{max}}=12$ ; for fL2 high, $k_{\\mathrm{min}}=12$ and $k_{\\operatorname*{max}}=\\infty$ . ", "page_idx": 11}, {"type": "text", "text": "C Comparison of GPU Memory, Training Time, and Parameter Counts. ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "a6em980M9x/tmp/a69fb6743840ed7cbea7ecf039ed68c85a70d7cee5fbf5dd083dab889e3ea93e.jpg", "table_caption": ["Table 6: Comparison of GPU memory, training time per epoch, and parameter counts on Darcy benchmark. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "D Repeated results on NS-2D and CFD-2D benchmarks. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 7: Comparison of the $l_{2}$ relative error on NS-2D and CFD-2D benchmark. ", "page_idx": 12}, {"type": "table", "img_path": "a6em980M9x/tmp/8c83d4dce6929f4be0cd4432ac4ff5a007f4b1479b36ab76825d042d74e2b8ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "E Abltation Study ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We investigate the impact of the dimensional factorization trick and the model architecture. The results are shown in Table 8. ", "page_idx": 12}, {"type": "text", "text": "Table 8: Comparison of the $l_{2}$ relative error for different components of AM-FNO (MLP) on Darcy, Airfoil, and Pipe benchmarks. The version with the vanilla FNO architecture (Vanilla) and the version without dimensional factorization (No-DF) are included. The training time and memory requirements are derived from the Airfoil benchmark. ", "page_idx": 12}, {"type": "table", "img_path": "a6em980M9x/tmp/f5e74660bfe050ef1391b6ada5fa1681447f44c0afa8fb0b28311ced20e313e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "The results indicate that the version without dimensional factorization performs comparably. Dimensional factorization can be unnecessary for handling low-dimensional PDEs. Meanwhile, the employed architecture achieves an average error reduction of $20\\%$ . ", "page_idx": 12}, {"type": "text", "text": "We also report the performance of AM-FNOs retaining the same number of frequency modes as other FNOs on the Darcy benchmark. The results, shown in Table 9, indicate that AM-FNOs consistently outperform the baselines, highlighting the advantages of our amortized parameterization. ", "page_idx": 12}, {"type": "text", "text": "F Discussion about KAN and MLP. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our findings in Section 5 demonstrate that implementing MLPs with orthogonal embeddings results in superior accuracy and efficiency. We attribute this success to the efficacy of the embedding technique employed. Meanwhile, the compatibility of the KAN architecture with widely used optimizers, such as AdamW, remains questionable. However, AM-FNO (KAN) offers several beneftis over AM-FNO (MLP). First, as illustrated in Figure 4, the expressiveness of KANs primarily stems from the grid size. To uphold a constant parameter count, the grid size remains unchanged. However, the architecture is inherently extensible during training, which could potentially enhance accuracy. Second, KANs provide a level of interpretability, as discussed in [21], which holds significant value within this domain. Third, there is no need to select orthogonal basis functions for AM-FNO (KAN). ", "page_idx": 12}, {"type": "table", "img_path": "a6em980M9x/tmp/47a9b53002a3be33b2abfe0a3bec8377ec7db5a9acef2660b8c5b80bbb748ce5.jpg", "table_caption": ["Table 9: Comparison of the $l_{2}$ relative error on Darcy benchmark. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "G Impact Statements ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This study introduces a neural operator specifically designed for the effective solution of partial differential equations (PDEs), potentially contributing to advancements in scientific and engineering domains. Positioned as foundational research in machine learning, the immediate identification of negative consequences is not evident, and the current risk of misuse remains low. ", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: See Section 5 and Appendix A. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: Due to the high computational cost of training on all benchmarks, we conducted each experiment only once. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See Appendix G. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 18}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]