[{"type": "text", "text": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kai Xiong\u2660 Xiao Ding\u2660\u2217 Ting Liu\u2660 Bing Qin\u2660 Dongliang $\\mathbf{X}\\mathbf{u}^{\\bar{\\bullet}}$ Qing Yang\u2666 Hongtao Liu\u2666 Yixin Cao\u2663\u2217 ", "page_idx": 0}, {"type": "text", "text": "\u2660Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, Harbin, China \u2663Fudan University, Shanghai, China   \n\u2666Du Xiaoman (Beijing) Science Technology Co., Ltd., Beijing, China {kxiong, xding, tliu, qinb}@ir.hit.edu.cn   \n{xudongliang, yangqing, liuhongtao01}@duxiaoman.com yxcao@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with several simple questions supported by a generic fact, LLMs often struggle to abstract and apply the generic fact to provide consistent and precise answers, revealing a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts. The code is available at https://github.com/Waste-Wood/MeanLearn. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans proceed with meaningful learning to induce common patterns or high-level abstractions to acquire abstract reasoning abilities [29, 37]. Such capabilities allow us to apply broad principles across diverse situations, demonstrating a deep understanding and versatile application of knowledge. As shown in Figure 1 (a), humans can deduce \u201crock dissolved\u201d and \u201cthe skin suffers pain\u201d when given \u201cadding rock into hydrochloric acid\u201d and \u201cacid touches human skin\u201d, respectively. This stems from the established expertise in the human mind (the generic fact \u201cacid is corrosive\u201d), and extends to applications in different scenarios. Of late, remarkable headways of LLMs have pushed AI much further towards human-like intelligence [2, 47]. Interestingly, can LLMs act like humans to instinctively consider the generic fact for flexible applications in different scenarios? ", "page_idx": 0}, {"type": "text", "text": "In our pilot investigation, we observed that LLMs appear to lack satisfying capabilities in abstract reasoning. As illustrated in Table 1, there is a notable discrepancy\u2014exceeding $17\\%$ \u2014between vanilla accuracy and abstract reasoning accuracy (AbsAcc) for all LLMs, while humans show only a small disparity. Despite the extensive pre-training of LLMs, which equips them with a vast repository of generic facts, they seem unable to leverage this information as flexibly as humans do (Figure 1 (b)). ", "page_idx": 0}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/34c88b4a4e20637bc9b52658732bbe65ba679faa042284622b49e49bd7efa1c3.jpg", "img_caption": ["(a) Humans conduct reasoning    (b) LLMs conduct reasoning ", "Figure 1: The responses of (a) Humans and (b) LLMs when facing two questions which are supported by the same generic fact. "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/013e52091b1ec67dca4d4dae313bd69cddd4f96ae6865103d6ba67572d4fc7b1.jpg", "table_caption": ["Table 1: The vanilla accuracy and abstract reasoning accuracy (AbsAcc) on e-CARE [14]. We will formally define AbsAcc in Sec. 2.1. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to systematically and quantitatively investigate the abstract reasoning of LLMs. We first formally define the abstract reasoning metric. Then we carry out abstract reasoning and knowledge probing tasks for a preliminary study. Our investigation comprises two main components: the reasoning tasks can quantify the abstract reasoning capabilities of LLMs. The generic fact probing task can give a further analysis from the perspective of generic fact mastery. Hereafter, we improve abstract reasoning in LLMs based on the analysis of the preliminary study. Unlike existing chain-of-thought (CoT) methods [52, 27], which conduct reasoning with uncontrolled step-by-step explanation, we create a dataset AbsR tailored for abstract reasoning. AbsR not only includes generic facts but also their guided explanations, offering a coached approach to understand the process of reasoning with common patterns. Finally, to make LLMs subconsciously exploit the generic fact like humans, we design a simple but effective learning paradigm called meaningful learning (MeanLearn) to simulate the implicit knowledge learning process [13], enabling LLMs to implicitly learn and utilize generic facts for reasoning without requiring generic facts as input. ", "page_idx": 1}, {"type": "text", "text": "We evaluate MeanLearn on six out-of-domain (OOD) reasoning and language understanding benchmarks. Experimental results demonstrate that MeanLearn not only improves general reasoning of LLMs but also excels in abstract reasoning. This distinction in performance is particularly pronounced in the realm of abstract reasoning, underscoring the unique efficacy of our approach in nurturing the higher-order thinking skills that are essential for sophisticated cognitive processing in artificial intelligence systems. Further analysis and ablation studies yield additional evidence supporting the effectiveness of our approach. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide a systematic and quantitative analysis of abstract reasoning in current LLMs, and we develop an abstract reasoning dataset with generic-fact-guided explanations. \u2022 We propose meaningful learning to improve abstract reasoning in LLMs with generic fact. \u2022 We achieve significant improvement in general reasoning and abstract reasoning on various OOD reasoning and language understanding benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Abstract reasoning study ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we initially define the evaluation metric of abstract reasoning to establish quantifiable standards. Then we introduce the LLMs used in this study. Finally, we conduct two experiments: one to directly estimate the abstract reasoning abilities of LLMs, and the other to further analyze it from the perspective of general fact mastery. ", "page_idx": 1}, {"type": "text", "text": "2.1 Abstract reasoning metric ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Abstract reasoning requires models to apply general patterns of high-level abstractions to different scenarios or questions. Different from general reasoning, which focuses on each single example, abstract reasoning takes the examples supported by a generic fact as a whole. We define abstract reasoning accuracy due to the lack of a proper metric. ", "page_idx": 1}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/eda6ca72d1fbc2823fed37f6ad63546468d7c108f5a4d238bfd3a1f3e091d366.jpg", "table_caption": ["Table 2: Accuracy of generic fact probing. "], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/059b0497ef5ff52a0a1e57e7849f418b74e03d59b1b1e56d0dec9554462fbed3.jpg", "table_caption": ["Table 3: The categorized abstract reasoning accuracy based on whether the generic facts are known. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Given a dataset $D$ with $n$ generic facts $R\\;=\\;\\{r_{1},r_{2}\\cdot\\cdot\\cdot\\;,r_{n}\\}$ , for the $i$ -th generic fact $r_{i}$ , it supports $m_{i}$ examples $\\begin{array}{r l}{S_{i}}&{{}=}\\end{array}$ $\\{s_{1}^{i},s_{2}^{i},\\cdot\\cdot\\cdot\\,,s_{m_{i}}^{\\bar{i}}\\}\\in D$ . ", "page_idx": 2}, {"type": "text", "text": "As shown in Figure 2, akin to Qiu et al. [37], we suppose an LLM $\\mathcal{L M}$ has grasped the generic fact $r_{i}$ if and only if $\\mathcal{L M}$ can correctly answer all examples in $S_{i}$ . Hence, we define abstract reasoning accuracy (denoted as AbsAcc) as the proportion of generic facts that $\\mathcal{L M}$ has grasped: ", "page_idx": 2}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/8adcba38b6089b40c18ae404a835646bc790f67830257a8c6381a95d084f0da9.jpg", "img_caption": ["Figure 2: Computation of abstract reasoning metric. "], "img_footnote": [], "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{A b s A c c}=\\frac{\\sum_{i}\\mathbb{I}(\\Phi(\\mathcal{L}\\mathcal{M},S_{i})=|S_{i}|)}{n},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Phi$ quantifies the number of examples in $S_{i}$ that $\\mathcal{L M}$ answers correctly. I serves as the indicator function, taking 1 when the specified condition is met and 0 otherwise. ", "page_idx": 2}, {"type": "text", "text": "2.2 Large language models for evaluation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We employ several LLMs in our preliminary experiments: (1) LLaMA-2 [47] is an open-access LLM. We use 7B and 13B LLaMA-2 for experiments; (2) Orca-2 [35] is an open-access LLM finetuned on LLaMA-2 with over 80K reasoning-specific examples. We use 7B and 13B Orca-2 for experiments. (3) GPT-3.5 [7] is a limited access LLM, we use gpt-3.5-turbo-0125 for experiments. ", "page_idx": 2}, {"type": "text", "text": "2.3 Experiment I: abstract reasoning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We formulate the first preliminary experiment to gain an initial insight into the abstract reasoning abilities of current LLMs. ", "page_idx": 2}, {"type": "text", "text": "To achieve this goal, we choose a multiple-choice question-answering dataset e-CARE [14] for implementation. e-CARE is a large-scale explainable causal reasoning dataset with a generic fact in each example, and a generic fact can support more than one example. The full set (train, test, and dev) of e-CARE is selected for experiments. We filter out the examples that the corresponding generic facts support only one example. Finally, we obtain 13,045 examples supported by 5,608 generic facts. ", "page_idx": 2}, {"type": "text", "text": "We follow zero-shot setting to prompt the LLMs for answer selection from multiple choices, and the generic facts are not provided in the inputs (refer to Appendix B.1). Table 1 shows the overall results: ", "page_idx": 2}, {"type": "text", "text": "(1) The significant disparity (over $17\\%$ ) between the vanilla accuracy and AbsAcc for all LLMs proves two aspects: on the one hand, vanilla accuracy is not ideal for estimating abstract reasoning. On the other hand, although LLMs could answer questions accurately, they still cannot capture the common patterns or generic facts behind the questions. ", "page_idx": 2}, {"type": "text", "text": "(2) Larger LLMs outperform smaller ones due to their richer knowledge and stronger reasoning ability. Orca-2, additional trained on LLaMA-2, notably outperforms LLaMA-2, indicating that supervised fine-tuning enhances both reasoning and abstract reasoning abilities. However, even with large-scale post-training, the significant gap between vanilla accuracy and AbsAcc remains unbridged. ", "page_idx": 2}, {"type": "text", "text": "(3) The minimal gap between vanilla accuracy and AbsAcc for humans suggests the large gap for LLMs is not due to varying difficulties of the example supported by the same generic fact. ", "page_idx": 2}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/80250f5b205ccf3af250d2b547c27c759a700d562d75a255d3a583db9ad4e083.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.4 Experiment II: generic fact probing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To delve into the abstract reasoning from the perspective of generic fact mastery, we design a generic fact probing task to ask LLMs whether they possess the given generic fact (refer to Appendix B.2 for more details). Drawing inspiration from knowledge uncertainty estimation [3], we formulate this task as a yes or no problem. The overall results are shown in Table 2. We could conclude as follows: ", "page_idx": 3}, {"type": "text", "text": "(1) LLaMA-2 and GPT-3.5 have substantial knowledge reserves. This indicates the poor AbsAcc of LLaMA-2 might come from a lack of ability to apply the generic fact to different reasoning scenarios. This problem also exists in GPT-3.5, but it is not as serious as LLaMA-2. ", "page_idx": 3}, {"type": "text", "text": "(2) In contrast, Orca-2 knows limited generic facts, and the larger the model, the less it knows. This contrasts sharply with its strong reasoning abilities (vanilla accuracy). We suppose training Orca-2 is more like rote learning rather than meaningful learning, which implies that Orca-2 tends to lose knowledge while acquiring reasoning. ", "page_idx": 3}, {"type": "text", "text": "To deeply investigate the relation between generic facts and abstract reasoning, we categorize the examples in e-CARE according to whether LLMs know the generic facts. The results are represented in Table 3, from which we can draw the following conclusions: ", "page_idx": 3}, {"type": "text", "text": "(1) The performance of \u201cKnown\u201d category has superiority over \u201cUnknown\u201d category across all LLMs. This demonstrates knowing the generic facts is helpful for abstract reasoning, which provides a potential avenue for abstract reasoning enhancement through generic fact retrieval. ", "page_idx": 3}, {"type": "text", "text": "(2) Nonetheless, most of the results in the \u201cUnknown\u201d category still have undeniable performance, suggesting that LLMs may engage in reasoning based on suspicious correlations or rely on memorization. ", "page_idx": 3}, {"type": "text", "text": "In conclusion, we can enhance the abstract reasoning abilities of LLMs in two ways: (1) Knowledge, injecting generic facts through additional training or prompting; (2) Reasoning, teaching LLMs how to utilize the generic facts to provide more precise responses to questions. ", "page_idx": 3}, {"type": "text", "text": "3 AbsR: abstract reasoning dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To enhance the abstract reasoning abilities of LLMs through Knowledge and Reasoning ways, we need a dataset that can not only provide generic facts but also teach LLMs how to exploit the generic fact to derive the correct answer in different scenarios (meaningful learning). Since there is no suitable dataset to assist us in achieving the goals, we create an Abstract Reasoning dataset (AbsR). ", "page_idx": 3}, {"type": "text", "text": "3.1 Generic fact collection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To obtain the generic facts, We chose GenericsKB [5] as the foundational generic fact base for AbsR. GenericsKB is a large-scale knowledge base with over 3.4 million sentences (e.g. Dogs bark) ", "page_idx": 3}, {"type": "text", "text": "expressing general truths, each of which also includes the corresponding concept (e.g. Dog) and the confidence score (between 0 and 1). Hence, we filter and then sample a collection of 4,613 high-quality generic facts of different concepts from GenericsKB (details of generic fact flitering and sampling can refer to Appendix C). ", "page_idx": 4}, {"type": "text", "text": "3.2 Dataset construction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To construct the whole dataset, and inspired by previous works [49, 56], we choose GPT4 [2] as our data annotator. The API we used is gpt-4-1106-preview. ", "page_idx": 4}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/eb64c0fb85758c83b4170fd1ab66496458603508277d9ca54c902cdfc1017543.jpg", "table_caption": ["Table 4: The statistics of AbsR. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Specifically, for each sampled generic fact $r_{i}$ , we would ask GPT-4 to create samples $S_{i}~=$ $\\{\\hat{s}_{1}^{i},\\cdot\\cdot\\cdot,s_{m_{i}}^{\\bar{i}}|1\\,\\leq\\,m_{i}\\,\\leq\\,\\hat{3\\}$ in various scenarios based on $r_{i}$ . Each sample $s_{j}^{i}$ contains the following terms: (1) a question $X_{j}^{i}$ with a few options, (2) a response $Y_{j}^{i}$ with an answer and an explanation guided by $r_{i}$ . All terms form a triple $s_{j}^{i}=<X_{j}^{i},r_{i},Y_{j}^{i}>$ for each sample. The prompt for dataset creation can refer to Appendix D. Figure 3 shows an illustration of the created samples. ", "page_idx": 4}, {"type": "text", "text": "Finally, without loss of generality, given the $j$ -th sample $s_{j}^{i}$ of generic fact $r_{i}$ , we can create two kinds of examples for meaningful learning. One is predicting $Y_{j}^{i}$ given $<X_{j}^{i},r_{i}>(\\mathrm{K}\\mathrm{-example})$ , while the other is predicting $Y_{j}^{i}$ given only $X_{j}^{i}$ (R-example). The examples can implicitly enhance abstract reasoning in LLMs through Knowledge and Reasoning ways. Table 4 shows the statistics of AbsR. ", "page_idx": 4}, {"type": "text", "text": "3.3 The quality of AbsR ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We conduct human evaluations to measure the quality of AbsR from the following dimensions (details and more statistics such as evaluation criteria, agreements, and pay can refer to Appendix E and F): ", "page_idx": 4}, {"type": "text", "text": "\u2022 Human Performance: humans can achieve vanilla accuracy of $95\\%$ and AbsAcc of $93.27\\%$ ; \u2022 Support Rate: $89\\%$ (the rate of examples which can be supported by the generic fact); \u2022 Diversity: $88.5\\%$ (the diversity of the examples supported by the same generic fact. Greater diversity indicates greater differences among the examples of the same generic fact). ", "page_idx": 4}, {"type": "text", "text": "For comparison, e-CARE [14] is also human-annotated, with examples generated from generic facts. On e-CARE, humans reached $92\\%$ vanilla accuracy, $89.9\\%$ AbsAcc, and an $87\\%$ support rate [14], showing that AbsR matches the quality of the human-annotated dataset. ", "page_idx": 4}, {"type": "text", "text": "4 Method: meaningful learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To imitate humans\u2019 instinctive use of general facts in reasoning, we develop a simple but effective learning paradigm called meaningful learning (MeanLearn). It can enhance the abstract reasoning abilities of LLMs in Knowledge and Reasoning ways. This is inspired by the implicit knowledge learning process, which uses hidden variables to learn event background knowledge [13]. MeanLearn can make LLMs implicitly learn generic facts and solve problems under the guidance of generic facts. ", "page_idx": 4}, {"type": "text", "text": "Specifically, an LLM $\\mathcal{L M}$ with parameters $\\theta$ and the input $x$ can construct a conditional probability for the output $y$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L M}(x,y,\\theta)=-\\sum_{t}\\log p_{\\theta}(y_{t}|x,y_{<t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In MeanLearn, for a $\\mathbf{K}$ -example $<X,r,Y>$ and $\\mathbf{R}$ -example $<X,Y>$ pair guided by generic fact $r$ , we send the example pair into $\\mathcal{L M}$ to model two conditional probabilities: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{L M}(X,Y,\\theta)=-\\sum_{t}\\log p_{\\theta}(Y_{t}|X,Y_{<t}),}}\\\\ {{\\displaystyle\\mathcal{L M}(X,r,Y,\\theta)=-\\sum_{t}\\log q_{\\theta}(Y_{t}|X,r,Y_{<t}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hereafter, on the one hand, we train $\\mathcal{L M}$ to learn policies $p_{\\theta}(Y_{t}|X,Y_{<t})$ and $q_{\\theta}(Y_{t}|X,r,Y_{<t})$ together to enable $\\mathcal{L M}$ implicitly learn the generac fact $r$ . On the other hand, $\\mathcal{L M}$ can learn how ", "page_idx": 4}, {"type": "text", "text": "to apply $r$ into different scenarios by learning the explanations in $Y$ of different $X$ supported by $r$ . Finally, we can reason with the implicit guidance of $r$ when only given $X$ , just like humans instinctively answer questions without explicitly giving the general facts. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training setup. We use LoRA [20] for parameter effcient finetuning. 7B and 13B MeanLearn are trained on 7B and 13B Orca-2, respectively, 8B MeanLearn is trained on 8B LLaMA-3, with batch sizes of 256 for 7B and 8B, and 240 for 13B. MeanLearn of various sizes are trained for one epoch at a 5e-5 learning rate with AdamW [26]. 7B and 8B MeanLearn are trained on 2 NVIDIA A100 80GB PCIe GPUs for 3 hours. 13B MearnLearn is trained on 3 such GPUs for 4 hours. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We adopt a wide range of open and limited access LLMs across different sizes as baselines. Open Access LLMs: (1) LLaMA-2 [47] 7B and 13B; (2) LLaMA-3 [34] 8B; (3) Vicuna [8] 7B and 13B (finetuned on ShareGPT [1] data); (4) WizardLM [56] 7B and 13B (fintuned with evolved instruction data); (5) Orca-2 [35] 7B and 13B (prograssively finetuned on massive reasoning data). Limited Access LLMs: (1) GPT-3.5 (gpt-3.5-turbo-0125); (2) PaLM-2 (text-bison-001). ", "page_idx": 5}, {"type": "text", "text": "Evaluation benchmarks. In addition to AbsR, we also include a wide range of benchmarks of reasoning and natural language understanding for evaluation: (1) AGIEval [60] consists of tests ranging from college admission tests, to national civil service examinations; (2) RACE [28] consists of tests with reading comprehension questions; (3) BBH [43] is a subset of Big-Bench [42], which contains 23 hardest tasks focusing on challenging scenarios; (4) Com. [55] is a collection of 7 commonsense reasoning datasets (\u03b1NLI [4], CSQA [44], COPA [39], e-CARE [14], SocialIQa [40], PIQA [6], and StrategyQA [16]); (5) MMLU [18] is a massive multitask language understanding benchmark; (6) ARC [10] is a benchmark of easy (ARC-e) and challenge (ARC-c) science questions. ", "page_idx": 5}, {"type": "text", "text": "Evaluation setup. Since generation-based evaluation is time-consuming due to limited computing resources, we follow OpenCompass [11] to have hybrid evaluation criteria with high reproducibility: (1) For PaLM-2 and GPT-3.5 on all tasks, and open access LLMs on 4 generation tasks (4 tasks in BBH), greedy decoding and exact match are utilized for evaluation; (2) For open access LLMs on classification and multiple-choice tasks, perplexity (PPL) is adopted for prediction. The option or category with the lowest PPL is chosen as the answer. For all baselines, evaluation is conducted once since there is no randomness on such evaluation criteria. For MeanLearn, results are averaged of MeanLearn(s) trained with 3 different random seeds. The evaluation prompts can refer to Appendix H. ", "page_idx": 5}, {"type": "text", "text": "5.2 Results: vanilla accuracy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 5 shows the $\\overline{{\\mathtt{V a n i l l a\\ \\mathtt{A c c u r a c y}}}}$ of MeanLearn and baselines: ", "page_idx": 5}, {"type": "text", "text": "(1) Additional training usually brings improvements (Vicuna, WizardLM, Orca-2, and MeanLearn), but it still can result in performance losses on some benchmarks (e.g. AGIEval). This signifies the effectiveness of massive reasoning-specific data and the necessity of data coverage. ", "page_idx": 5}, {"type": "text", "text": "(2) MeanLearn has superiority over all open access baselines on nearly all benchmarks. This demonstrates that MeanLearn can use generic facts to effectively guide LLMs to reason more properly and logically, making them more flexible and task-adaptive. ", "page_idx": 5}, {"type": "text", "text": "(3) The more challenging the benchmark (e.g. BBH), the less pronounced the improvement in the performance of MeanLearn. This is mainly because the improvement is mostly constrained by the complex scenarios in harder benchmarks, which require mightier base LLMs or more complex training data to perform and learn multi-step and compositional reasoning. ", "page_idx": 5}, {"type": "text", "text": "(4) MeanLearn does not have many advantages over LLaMA-3. LLaMA-3 stores dense knowledge in its parameters, making it harder to learn new knowledge without forgetting existing knowledge. ", "page_idx": 5}, {"type": "text", "text": "(5) Larger open-sourced LLMs do not always yield better results. We suppose the main factors behind this are in two ways: on the one hand, LLMs with different parameter sizes might be good at different tasks, and excessive contemplation may lead to decreases on some tasks [53]. On the other hand, the perplexity-based evaluation setting might have moderate cross-scale generalization ability. ", "page_idx": 5}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/c0c12fb18955291ad510be968e9fa4b5ea139a7014104388b1f1bb0e01ef4580.jpg", "table_caption": ["Table 5: Overall vanilla accuracy and AbsAcc $(\\%)$ of baselines and MeanLearn. Due to the space limit, we only report the standard deviation of Average performance of each method. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "(6) LLMs possess more than 20B parameters still struggle with abstract reasoning. MeanLearn can achieve comparable performance to PaLM-2 despite their huge gap in model sizes. ", "page_idx": 6}, {"type": "text", "text": "5.3 Results: abstract reasoning accuracy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since the generic facts are not given in the OOD benchmarks, we train a RoBERTa-Large [32] with constrastive learning to cluster examples supported by the same generic fact. In particular, we utilize e-CARE as the training set, examples guided by the same generic fact will be pushed closer, or they will be pushed away. Cosine similarity is used to measure the distance between examples (training details can refer to Appendix I). For clustering, examples sharing a similarity above 0.6 are considered to be supported by the same generic fact. Each cluster contains no more than 3 examples. ", "page_idx": 6}, {"type": "text", "text": "Hereafter, we calculate the abstract reasoning metrics based on the results of clustering. The overall results are shown in Table 5 $(\\overline{{\\underline{{\\mathbf{A}\\mathsf{b}\\mathsf{s}}\\mathbf{A}\\mathsf{c}\\mathsf{c}}}})$ , we can have the following observations: ", "page_idx": 6}, {"type": "text", "text": "(1) AbsAcc is much lower than vanilla accuracy, indicating the large gap between general reasoning and abstract reasoning. More efforts should be made to fill this gap in the future. ", "page_idx": 6}, {"type": "text", "text": "(2) MeanLearn can surpass all baselines, usually exhibiting greater advantages in AbsAcc than in vanilla accuracy. This suggests that the improvement in vanilla accuracy is more likely due to the enhancement of abstract reasoning abilities. MeanLearn can tell LLMs the generic facts and teach them how to implicitly utilize them in different scenarios, resulting in better abstract reasoning. ", "page_idx": 6}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/1201103dc8b53c217b53f080f1a75bdce2a3f45e149b9562c295357039638ff8.jpg", "table_caption": ["Table 6: The overall reasults of ablation studies. \u201cw/o\u201d denotes without. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Further analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further investigate the pros, cons, and effectiveness of MeanLearn, we conduct several analyses and ablation studies. LLaMA-2, Orca2, and MeanLearn are adopted for analysis. ", "page_idx": 7}, {"type": "text", "text": "6.1 Performance on different domains ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We categorize tasks of MMLU based on their respective domains to demonstrate the pros and cons of MeanLearn for future improvements. The results are shown in Figure 4 with a total of 16 categories: ", "page_idx": 7}, {"type": "text", "text": "(1) All LLMs possess poor vanilla accuracy and AbsAcc in math and chemistry. We conjecture the primary factors are the complexity of math problems and the shortage of chemistry knowledge. ", "page_idx": 7}, {"type": "text", "text": "(2) Compared to Orca-2, MeanLearn possesses considerable advantages in engineering, geography, and physics & astronomy domains. We suppose this is related to the domain coverage of AbsR. ", "page_idx": 7}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/003493e13c85f681213c21b948b1bf8ccd58bbbf9884b718ea5e2064e6beab06.jpg", "img_caption": ["Figure 4: Visualization of reasoning capabilities on MMLU, which is categorized by task domians. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "(3) Since the overall AbsAcc is not relatively high, for potential improvement in the future, apart from focusing more on the domains with poor vanilla accuracy and AbsAcc, we should also increase the overall volume of finetuning data. ", "page_idx": 7}, {"type": "text", "text": "6.2 How much do knowledge and reasoning ways contribute to MeanLearn? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We utilize Knowledge and Reasoning ways to enhance abstract reasoning in LLMs. To investigate the effect of them, we conduct an ablation study of AbsR to respectively remove the Knowledge (generic facts) and Reasoning (explanations) in each example. Results are shown in Table 6 (w/o Knowledge and w/o Reasoning), we can observe: ", "page_idx": 7}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/2bdca6d3d483917b1921416b5a69c3d275175b2e499a185cdd94bdb7629b1984.jpg", "table_caption": ["Table 7: MeanLearn trained based on Mistral. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/7a9f99ca13ab2462f7151134029a910e7ae66be93b23b5e21902ae3856dbcbae.jpg", "table_caption": ["Table 8: Performance on mathematical reasonin datasets from MMLU [18]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "(1) Removing Knowledge (generic facts) or Reasoning (explanation) leads to a decrease in performance. This is because the generic facts and their guided reasoning process in AbsR can teach LLMs to learn and utilize the generic fact behind the questions to reason more logically and properly. ", "page_idx": 8}, {"type": "text", "text": "(2) In MeanLearn, Knowledge contributes less in boosting the vanilla and abstract reasoning than Reasoning. It indicates the importance of teaching LLMs how to reason under the guidance of generic facts. Furthermore, it reveals the bottleneck of small-scale LLMs reasoning is caused more by reasoning itself rather than knowledge. ", "page_idx": 8}, {"type": "text", "text": "(3) MeanLearn without Knowledge can defeat Orca-2 on vanilla accuracy and AbsAcc, while MeanLearn without Reasoning cannot. This implies the synergy effects between Knowledge and Reasoning (MeanLearn) can push LLMs further. ", "page_idx": 8}, {"type": "text", "text": "(4) In AGIEval, Knowledge and Reasoning are less critical due to their focus on complex reading comprehension without needing vast knowledge. Moreover, Knowledge seems useless in Commonsense. When providing LLMs with knowledge, we should also teach them how to use it. ", "page_idx": 8}, {"type": "text", "text": "6.3 Is the improvement in performance solely attributed to additional data? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Supervised fine-tuning can also improve AbsAcc (Table 1), so we design another ablation study to investigate whether the improvement of MeanLearn only comes from additional data. ", "page_idx": 8}, {"type": "text", "text": "To be precise, for each sample in AbsR, we use GPT-4 to generate a new explanation based on the question and label without the guidance of the generic fact. Hereafter, we obtain $\\mathbf{A}\\mathbf{b}\\mathbf{S}\\mathbf{R}^{*}$ . The only difference between AbsR and $\\mathbf{A}\\mathbf{b}\\mathbf{S}\\mathbf{R}^{*}$ is the explanations in AbsR are guided by the generic fact. Table 6 (w/ AbsR\u2217) shows the results of MeanLearn trained with $\\mathbf{A}\\mathbf{b}\\mathbf{S}\\mathbf{R}^{*}$ , we can infer: ", "page_idx": 8}, {"type": "text", "text": "(1) Training 7B and 13B Orca-2 with AbsR\u2217 can still outperform Orca-2 on vanilla accuracy. However, it cannot bring stable improvements on AbsAcc. This underscores that reasoning-specific post-training can boost performance, while improving abstract reasoning performance might depend on more careful design such as MeanLearn. ", "page_idx": 8}, {"type": "text", "text": "(2) MeanLearn without Knowledge outperforms MeanLearn trained with $\\mathbf{A}\\mathbf{b}\\mathbf{R}^{*}$ , showing the superiority of training with generic-fact-guided explanation. This stems from the generic-fact-guided explanation that can teach LLMs to use the regular pattern behind the questions for better reasoning. ", "page_idx": 8}, {"type": "text", "text": "6.4 Expanding MeanLearn to LLMs besides the LLaMA series ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To investigate the universality of MeanLearn, we train MeanLearn with Mistral-7B-Instruct-v0.2 [21] (denoted as Mistral). All settings are consistent with Sec 5. We choose AbsR, Com., MMLU, and RACE for evaluation. As shown in Table 7, we can find MeanLearn has good applicability on Mistral. ", "page_idx": 8}, {"type": "text", "text": "6.5 Expanding reasoning domains to mathematical reasoning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although we do not incorporate math examples in AbsR, we want to investigate whether MeanLearn can improve the performance of mathematical reasoning by enhancing abstract reasoning in LLMs. ", "page_idx": 8}, {"type": "text", "text": "Following [33], we select five mathematical reasoning datasets from MMLU [18] to demonstrate this: abstract algebra (AA), college mathematics (CM), elementary mathematics (EM), high school statistics (HSS), and high school mathematics (HSM). We choose 7B version of Orca-2 and Mistral for comparison. Results are shown in Table 8, it is interesting to find that MeanLearn can outperform baselines even if there is no math data in AbsR. This demonstrates the effectiveness of MeanLearn in enhancing abstract reasoning in LLMs. ", "page_idx": 9}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "7.1 Reasoning with LLMs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LLMs have overturned the research paradigm of NLP, which makes reasoning more accessible. Using LLMs to perform reasoning can be categorized into tunning-based and prompt-based methods. ", "page_idx": 9}, {"type": "text", "text": "Tunning-based methods, originating from T0 [48] and FLAN [52], finetuned T5 [38] and LaMDAPT [46] on massive NLP tasks, achieving notable zero-shot performance on unseen tasks. FlanPaLM [9] and $\\mathrm{T}k$ -INSTRUCT [50] further scaled up the model and task size for improvement. Orca [36] and Orca-2 [35] leveraged vast instruction data to teach small-scale LLMs reasoning skills. ", "page_idx": 9}, {"type": "text", "text": "As for prompt-based methods, Wei et al. [53] introduced CoT prompt to elicit reasoning in LLMs by encouraging step-by-step thinking. Additionally, Kojima et al. [27] presented zero-shot CoT to bypass manual annotation issues. Subsequently, Zhang et al. [59] proposed AutoCoT to automatically generate few-shot examples with CoT for reasoning. Various CoT-based methods [22, 51] have since emerged, such as complex CoT [15], and the tree of thought [57], etc. ", "page_idx": 9}, {"type": "text", "text": "These methods are designed for improving the general reasoning abilities of LLMs, while we focus on the abstract reasoning capabilities of LLMs. ", "page_idx": 9}, {"type": "text", "text": "7.2 LLMs-as-annotators ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LLMs are increasingly used as annotators to create data for training and evaluation due to their advanced instruction-following skills. ", "page_idx": 9}, {"type": "text", "text": "Some studies distilled various CoT data from LLMs to train student models ([30, 19, 58], inter alia), achieving impressive performance. For instance, Li et al. [30] distilled symbolic CoT from GPT-3 and observed enhanced performance in commonsense reasoning. Dai et al. [12] used ChatGPT to generate augmentation data to enhance BERT [24]. With the advent of open-source LLMs like LLaMA [47], other studies distilled instruction data from large-scale LLMs to train small-scale models ([56, 36, 25, 31, 23, 17], inter alia). For example, Xu et al. [56] and Luo et al. [33] employed an evolving strategy to obtain complex data, trained LLaMA, and achieved strong performance. Tang et al. [45] constructed a concept graph and used GPT-3.5 to synthesize math data with high diversity. We employ LLMs to generate training data and develop AbsR tailored for abstract reasoning. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the abstract reasoning of existing LLMs. To achieve this, we first define the evaluation metric of abstract reasoning and design a series of preliminary experiments. Then we discover a significant performance gap between general reasoning and abstract reasoning. Hence, we tailor an abstract reasoning dataset AbsR with the help of GPT-4 to enhance LLMs. Finally, we devise a simple but effective paradigm (MeanLearn) to teach LLMs abstract reasoning in Knowledge and Reasoning ways. Extensive experiments demonstrate the superiority and effectiveness of MeanLearn in vanilla and abstract reasoning accuracies. The limitations of MeanLearn are discussed in Appendix A. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to acknowledge the editors and reviewers for their efforts and advice, and gratefully acknowledge the support of the National Natural Science Foundation of China under Grants U22B2059 and 62176079, Natural Science Foundation of Heilongjiang Province under Grant YQ2022F005. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[1] Sharegpt: Share your wildest chatgpt conversations with one click, 2023. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[3] Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. arXiv preprint arXiv:2305.13712, 2023. [4] Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In International Conference on Learning Representations, 2019. [5] Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660, 2020.   \n[6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality, March 2023. [9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.   \n[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[11] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models, 2023.   \n[12] Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, et al. Auggpt: Leveraging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007, 2023.   \n[13] Li Du, Xiao Ding, Ting Liu, and Zhongyang Li. Modeling event background for if-then commonsense reasoning using context-aware variational autoencoder. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2682\u20132691, 2019.   \n[14] Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. e-care: a new dataset for exploring explainable causal reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 432\u2013446, 2022.   \n[15] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2023.   \n[16] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013361, 2021.   \n[17] Sonam Gupta, Yatin Nandwani, Asaf Yehudai, Mayank Mishra, Gaurav Pandey, Dinesh Raghu, and Sachindra Joshi. Selective self-rehearsal: A fine-tuning approach to improve generalization in large language models. arXiv preprint arXiv:2409.04787, 2024.   \n[18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021.   \n[19] Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14852\u201314882, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[20] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[21] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[22] Ziqi Jin and Wei Lu. Self-harmonized chain of thought. arXiv preprint arXiv:2409.04057, 2024.   \n[23] Hoang Anh Just, Mahavir Dabas, Lifu Huang, Ming Jin, and Ruoxi Jia. Dipt: Enhancing llm reasoning through diversified perspective-taking. arXiv preprint arXiv:2409.06241, 2024.   \n[24] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2. Minneapolis, Minnesota, 2019.   \n[25] Bumjun Kim, Kunha Lee, Juyeon Kim, and Sangam Lee. Small language models are equation reasoners. arXiv preprint arXiv:2409.12393, 2024.   \n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[27] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.   \n[28] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785\u2013794, 2017.   \n[29] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017.   \n[30] Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic chain-of-thought distillation: Small models can also \u201cthink\u201d step-by-step. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2665\u20132679, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[31] Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, and Jun Zhao. Neural-symbolic collaborative distillation: Advancing small language models for complex reasoning tasks. arXiv preprint arXiv:2409.13203, 2024.   \n[32] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[33] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. ", "page_idx": 12}, {"type": "text", "text": "[34] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2024.   \n[35] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045, 2023.   \n[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023.   \n[37] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559, 2023.   \n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \n[39] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.   \n[40] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463\u20134473, 2019.   \n[41] Jihao Shi, Xiao Ding, Li Du, Ting Liu, and Bing Qin. Neural natural logic inference for interpretable question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3673\u20133684, 2021.   \n[42] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.   \n[43] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 13003\u201313051, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[44] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, 2019.   \n[45] Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, 2024.   \n[46] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.   \n[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[48] Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022.   \n[49] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.   \n[50] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on $1600+$ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022.   \n[51] Yu Wang, Shiwan Zhao, Zhihu Wang, Heyuan Huang, Ming Fan, Yubo Zhang, Zhixing Wang, Haijun Wang, and Ting Liu. Strategic chain-of-thought: Guiding accurate reasoning in llms through strategy elicitation. arXiv preprint arXiv:2409.03271, 2024.   \n[52] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.   \n[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[54] Kai Xiong, Xiao Ding, Zhongyang Li, Li Du, Ting Liu, Bing Qin, Yi Zheng, and Baoxing Huai. Reco: Reliable causal chain reasoning via structural causal recurrent neural networks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6426\u20136438, 2022.   \n[55] Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7572\u20137590, 2023.   \n[56] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.   \n[57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.   \n[58] Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, and Shuicheng Yan. Llms-as-instructors: Learning from errors toward automating model improvement. Findings of EMNLP, 2024.   \n[59] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[60] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "There are several limitations of our work: First, the size of the training data should be larger to obtain better and more stable results. Second, there should be some examples (whether MeanLearn can generate meaningful explanations) to demonstrate the superiority of MeanLearn. Third, more efforts should be made to focus more on abstract reasoning itself. Finally, complex scenarios (with complex generic facts) such as causal chain reasoning [54] and logical reasoning [41] is a potential direction of abstract reasoning. ", "page_idx": 14}, {"type": "text", "text": "B Asbstract reasoning study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Prompt for experiment I ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1.1 Orca-2 ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/df7ab8dc4234d34f38686d43976c2c9dbb74696341d2d835474d8541e726e3d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.1.2 LLaMA-2 and GPT-3.5 ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/8e9ff962730e0adf2d0a59bbcd049b4c6f8b445fd150280c452ad33f807fefa1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The placeholders {premise}, {hypothesis1}, and {hypothesis2} will be fliled with the corresponding terms in each example of e-CARE dataset. ", "page_idx": 14}, {"type": "text", "text": "B.2 Prompt for experiment II: Orca-2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.2.1 Orca-2 ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/e7c53237f62acd09cbd4512b4f02aa862952e18957d2cd9d6e412a80ff6c798f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2.2 LLaMA-2 and GPT-3.5 ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/04b64a331fb1d4e0456e9108bc03d3b88e8379c0371949df8867788fb54a9489.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The placeholders $\\overline{{\\{\\mathrm{premise}\\}}}$ , {hypothesis1}, {hypothesis1}, and $\\boxed{\\{\\mathtt{f a c t}\\}}$ will be fliled with the corresponding terms in each example of e-CARE dataset. ", "page_idx": 15}, {"type": "text", "text": "C Details for generic facts filtering ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We apply the following steps to filter generics facts from GenericsKB: ", "page_idx": 15}, {"type": "text", "text": "(1) Exclude sentences with confidence $<0.7$ .   \n(2) Categorize the sentences by the concepts. ", "page_idx": 15}, {"type": "text", "text": "(3) Randomly sample 4,613 concepts, and then randomly sample one sentence as the generic fact from the category of each sampled concept. ", "page_idx": 15}, {"type": "text", "text": "Hereafter, we can obtain a collection of 4,613 generic facts. ", "page_idx": 15}, {"type": "text", "text": "D Prompt for AbsR construction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the following prompt to generate our AbsR examples. ", "page_idx": 15}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/4fb95aacd5751cb523d3eaddb289718b8b7f103028890ea8d4380ef8447f75b5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The placeholder $\\boxed{\\{\\mathrm{number}\\}}$ will be randomly filled with \u201cone\u201d, \u201ctwo\u201d or \u201cthree\u201d. ", "page_idx": 15}, {"type": "text", "text": "E Details of human evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We choose three annotators with good backgrounds in textual inference and event reasoning. The whole test set of AbsR is involved in the human evaluation (200 instances). For each annotator, we pay $\\mathbb{S}10$ per hour, while in our country, the minimum wage is less than $\\mathbb{S5}$ per hour. All annotators agreed to let us use their annotations. The evaluation agreements are $95\\%$ , $91\\%$ , and $90\\%$ for the question answering, generic fact supportance, and diversity tasks, respectively. ", "page_idx": 16}, {"type": "text", "text": "F Instructions of human evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "F.1 Performance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "You are given some questions each with some options, all the questions are about commonsense and are generated by GPT-4. Please choose the most plausible option for each question. Just type your choice (such as (A)) in the Answer column. ", "page_idx": 16}, {"type": "text", "text": "F.2 Support rate ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "You are given some questions each with some options and a generic fact, all the questions are about commonsense and are generated by GPT-4. Please judge whether the generic can support to answer the question. Just type your choice (Yes or No) in the Answer column. ", "page_idx": 16}, {"type": "text", "text": "F.3 Diversity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "You are given some generic facts, each generic fact possesses several samples (each sample consists of a question and a few options). The samples can be supported by the corresponding generic fact. Please judge whether the samples within a generic are similar and are just different in nouns or expressions. If sample 1 and sample 2 are similar and are just different in nouns or expressions, just type a tuple (1, 2) in the Answer column. If there are multiple similar sample pairs, just separate them with a ${\\bf\\omega}^{\\left(\\ast\\right)}\\backslash{\\bf n}^{\\left(\\ast\\right)}$ . For example, $(1,2)\\backslash\\mathfrak{n}(3,4)$ . ", "page_idx": 16}, {"type": "text", "text": "G K-example and R-example ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "G.1 K-example ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "TIhiFqGOYC/tmp/a0e7458a9dacea51f2c38469467d85f1a7084e1df5a84f2b5f3c44bdaa42971d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/620bb7dd608aedb55d371688568416a9a2cf681cb0a7b32969a08a328e625037.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "H Templates for evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "H.1 LLaMA-2, LLaMA-3, and MeanLearn (8B) ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/6085f03052a71820b0b1b90c9c78f4b97bb779b7b289d388af23243ef5635203.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "H.2 Orca-2 and MeanLearn (7B and 13B) ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "TIhiFqGOYC/tmp/c9bfb58b71ef43d8b4f3a86f196d276136264c736033d4b5bd7f638e116ccdf5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "I Training details of RoBERTa-Large-based clusterer ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use the RoBERTa-Large [32] released by Meta. The batch size is 256. We use AdamW [26] to optimize the model with a learning rate of 1e-5 for 5 epochs. The computing device is one NVIDIA A100-SXM-64GB. The running time is about 12 minutes. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main claims can be supported by the experiments and ablation studies. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have provided limitations of the work in Sec. 6.1 and Appendix. A ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have provided complete proof in all the tables and figures in this paper. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have provided the training and evaluation details for reproducibility in Sec. 5. We also submit the code and data in the supplementary material. We also run experiments for multiple times to report the mean and standard deviation of MeanLearn. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the link for the data and code at the end of the Abstract. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the training and test details of all baselines and MeanLearn in Sec. 5. Some other details such as the prompts and human evaluation are provided in the Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have reported the mean and standard deviation results across 3 runs of MeanLearn in Table 5. There is no randomness with all the baselines since the evaluation criterias we used is greedy decoding and PPL-based prediction. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have provided the compute resources and time of execution in Sec. 5. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have provided the detials of human evaluation (such as the pay) in Appendix E. We also obtain the agreements of the human annotators to use their annotation. We also follow the other terms in the code of ethics which would be covered in our paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We focus on the reasoning abilities of LLMs, which is a foundational research and does not concentrate on specific application scenarios. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We use publicly accessible data and LLMs for experiments Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We cite every resource (such as LLMs and benchmarks) used in this work. Please refer to Sec. 5. We also follow the the license and terms of use of corresponding LLMs and data. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided the structure of the AbsR dataset in Sec. 3.2. Figure 3 also shows two samples of AbsR. We also provide the create AbsR dataset with a readme flie in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We have provided the instructions of human evaluations in Appendix F. We also offer the information such as the pay in Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our human evaluations are answering some questions about commonsense. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]