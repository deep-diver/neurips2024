[{"figure_path": "TIhiFqGOYC/tables/tables_1_1.jpg", "caption": "Table 1: The vanilla accuracy and abstract reasoning accuracy (AbsAcc) on e-CARE [14]. We will formally define AbsAcc in Sec. 2.1.", "description": "This table presents the vanilla accuracy and abstract reasoning accuracy (AbsAcc) for several large language models (LLMs) and humans on the e-CARE dataset.  Vanilla accuracy represents the overall accuracy on the dataset, while AbsAcc measures the ability of the models to abstract and apply generic facts to answer questions consistently.  The significant difference between the vanilla accuracy and AbsAcc for LLMs highlights their limitations in abstract reasoning compared to humans.", "section": "2.3 Experiment I: abstract reasoning"}, {"figure_path": "TIhiFqGOYC/tables/tables_2_1.jpg", "caption": "Table 2: Accuracy of generic fact probing.", "description": "This table presents the accuracy of generic fact probing for different LLMs (LLaMA-2, Orca-2, and GPT-3.5) with varying model sizes.  The accuracy reflects the percentage of generic facts that the LLMs correctly identified as known to them.  It provides insights into the extent of generic fact mastery by different LLMs.", "section": "2.4 Experiment II: generic fact probing"}, {"figure_path": "TIhiFqGOYC/tables/tables_2_2.jpg", "caption": "Table 3: The categorized abstract reasoning accuracy based on whether the generic facts are known.", "description": "This table presents the abstract reasoning accuracy (AbsAcc) categorized by whether the LLMs (LLaMA-2, Orca-2, GPT-3.5) already know the generic facts.  The \"Known\" category shows AbsAcc when the LLM has prior knowledge of the generic fact, while \"Unknown\" represents cases where the generic fact is new to the LLM.  The results are further broken down by the model size (7B, 13B, >20B). This allows for a nuanced understanding of how prior knowledge of generic facts influences the ability of LLMs to perform abstract reasoning.", "section": "2.3 Experiment I: abstract reasoning"}, {"figure_path": "TIhiFqGOYC/tables/tables_4_1.jpg", "caption": "Table 4: The statistics of AbsR.", "description": "This table shows the number of examples, questions, and generic facts in the AbsR dataset.  The dataset is split into training and testing sets. The training set contains 18,020 examples, 9,010 questions, and 4,613 generic facts.  The test set contains 200 examples, 200 questions, and 104 generic facts.", "section": "3 AbsR: abstract reasoning dataset"}, {"figure_path": "TIhiFqGOYC/tables/tables_6_1.jpg", "caption": "Table 5: Overall vanilla accuracy and AbsAcc (%) of baselines and MeanLearn. Due to the space limit, we only report the standard deviation of Average performance of each method.", "description": "This table presents the vanilla accuracy and abstract reasoning accuracy (AbsAcc) for various large language models (LLMs) across multiple benchmarks.  The LLMs include different sizes and architectures and encompass both open-access and limited-access models.  Benchmarks cover diverse reasoning and language understanding tasks. The results show the performance of each model in terms of vanilla accuracy (overall accuracy) and AbsAcc (abstract reasoning accuracy) across each benchmark, highlighting the relative strengths and weaknesses of different approaches.", "section": "5.2 Results: vanilla accuracy"}, {"figure_path": "TIhiFqGOYC/tables/tables_7_1.jpg", "caption": "Table 5: Overall vanilla accuracy and AbsAcc (%) of baselines and MeanLearn. Due to the space limit, we only report the standard deviation of Average performance of each method.", "description": "This table presents the vanilla accuracy and abstract reasoning accuracy (AbsAcc) achieved by various Large Language Models (LLMs) across multiple benchmarks.  The LLMs are categorized by size (7B, 8B, 13B, >20B parameters), and the results showcase the performance of both standard LLMs and the MeanLearn approach.  The table highlights the improvements offered by the MeanLearn method, particularly regarding the substantial gap often observed between general reasoning and abstract reasoning capabilities in large language models.", "section": "5.2 Results: vanilla accuracy"}, {"figure_path": "TIhiFqGOYC/tables/tables_8_1.jpg", "caption": "Table 7: MeanLearn trained based on Mistral.", "description": "This table presents the performance comparison between Mistral and MeanLearn (trained on Mistral) on various benchmarks including AbsR, Com, MMLU, and RACE.  Both vanilla accuracy and abstract reasoning accuracy (AbsAcc) are reported, showcasing the improvement achieved by MeanLearn across different metrics.", "section": "5.3 Results: abstract reasoning accuracy"}, {"figure_path": "TIhiFqGOYC/tables/tables_8_2.jpg", "caption": "Table 8: Performance on mathematical reasoning datasets from MMLU [18].", "description": "This table presents the results of evaluating various LLMs on mathematical reasoning tasks from the MMLU benchmark.  It compares the vanilla accuracy (correct answers without considering abstract reasoning) and abstract reasoning accuracy (AbsAcc,  a metric focusing on the ability to apply general principles to different scenarios) of Orca-2, Mistral, and MeanLearn models of different sizes (7B parameter count).  The table shows performance across five specific mathematical reasoning sub-tasks (AA, CM, EM, HSS, HSM) within MMLU and the average performance across all five sub-tasks. The results highlight the improvements achieved using the MeanLearn approach in both vanilla accuracy and especially abstract reasoning accuracy.", "section": "7.1 Reasoning with LLMs"}, {"figure_path": "TIhiFqGOYC/tables/tables_14_1.jpg", "caption": "Table 1: The vanilla accuracy and abstract reasoning accuracy (AbsAcc) on e-CARE [14]. We will formally define AbsAcc in Sec. 2.1.", "description": "This table presents the baseline performance of various LLMs on the e-CARE dataset.  It compares the standard accuracy (Vanilla Accuracy) with a newly defined metric, AbsAcc (Abstract Reasoning Accuracy), which assesses the model's ability to abstract and apply generic facts to answer questions consistently.  The size of each LLM is also shown.  Human performance is included for comparison.", "section": "Experiment I: abstract reasoning"}, {"figure_path": "TIhiFqGOYC/tables/tables_15_1.jpg", "caption": "Table 2: Accuracy of generic fact probing.", "description": "This table presents the accuracy of generic fact probing for different LLMs (LLaMA-2, Orca-2, and GPT-3.5) with varying model sizes (7B, 13B, and >20B parameters).  The accuracy reflects the models' ability to correctly identify whether they possess knowledge of specific general facts.  Higher accuracy indicates a stronger grasp of general knowledge.", "section": "2.4 Experiment II: generic fact probing"}, {"figure_path": "TIhiFqGOYC/tables/tables_16_1.jpg", "caption": "Table 1: The vanilla accuracy and abstract reasoning accuracy (AbsAcc) on e-CARE [14]. We will formally define AbsAcc in Sec. 2.1.", "description": "This table presents the vanilla accuracy and abstract reasoning accuracy (AbsAcc) achieved by various LLMs (LLaMA-2, Orca-2, GPT-3.5) and humans on the e-CARE dataset.  Vanilla accuracy represents the overall accuracy of the models on the questions, while AbsAcc measures the ability of the models to abstract and apply a generic fact to answer multiple related questions consistently.  The table highlights a significant difference in performance between LLMs and humans, demonstrating the challenge LLMs face with abstract reasoning.", "section": "2.3 Experiment I: abstract reasoning"}]