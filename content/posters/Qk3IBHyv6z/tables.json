[{"figure_path": "Qk3IBHyv6z/tables/tables_2_1.jpg", "caption": "Table 1: A summary of our results: upper and lower bounds on the regret gap (i.e. R\u03c6(\u03c3) \u2013 R\u03c6(\u03c3\u03b5)) of various approaches to multi-agent IL. Here, \u03b2 is the coverage constant in Assumption 5.2, u is the recoverability constant in Assumption 5.1, H is the horizon.", "description": "This table summarizes the upper and lower bounds on the regret gap for different multi-agent imitation learning (MAIL) algorithms.  The regret gap measures how much better an expert's policy performs compared to a learner's policy, even when strategic agents may deviate from recommendations.  The table shows bounds for both existing single-agent IL algorithms adapted to the multi-agent setting (J-BC, J-IRL) and the novel algorithms proposed in the paper (MALICE, BLADES).  The bounds depend on factors like the horizon (H), recoverability (u), and coverage (\u03b2).", "section": "Efficient Algorithms for Minimizing the Regret Gap"}, {"figure_path": "Qk3IBHyv6z/tables/tables_12_1.jpg", "caption": "Table 1: A summary of our results: upper and lower bounds on the regret gap (i.e. R\u03c6(\u03c3) \u2013 R\u03c6(\u03c3\u03b5)) of various approaches to multi-agent IL. Here, \u03b2 is the coverage constant in Assumption 5.2, u is the recoverability constant in Assumption 5.1, H is the horizon.", "description": "This table summarizes the upper and lower bounds on the regret gap for different multi-agent imitation learning algorithms.  The regret gap measures how much better an expert's policy performs than a learned policy, even when agents are allowed to strategically deviate from recommendations. The algorithms are compared under two different assumptions: (1) a coverage assumption on the expert's demonstrations (\u03b2-coverage) and (2) access to a queryable expert.  The bounds are expressed in terms of the horizon (H) of the Markov game, the coverage constant (\u03b2), and the recoverability constant (u).", "section": "Efficient Algorithms for Minimizing the Regret Gap"}, {"figure_path": "Qk3IBHyv6z/tables/tables_20_1.jpg", "caption": "Table 1: A summary of our results: upper and lower bounds on the regret gap (i.e. R\u03c6(\u03c3) \u2013 R\u03c6(\u03c3\u03b5)) of various approaches to multi-agent IL. Here, \u03b2 is the coverage constant in Assumption 5.2, u is the recoverability constant in Assumption 5.1, H is the horizon.", "description": "This table summarizes the upper and lower bounds on the regret gap for different multi-agent imitation learning (MAIL) approaches.  The regret gap measures the difference in the maximum incentive for an agent to deviate from the learned policy versus the expert policy.  The table shows results for four approaches (J-BC, J-IRL, MALICE, BLADES), each under different assumptions (coverage, queryable expert).  The bounds are expressed in terms of the horizon (H) of the Markov game, the recoverability constant (u), and the coverage constant (\u03b2).", "section": "Efficient Algorithms for Minimizing the Regret Gap"}]