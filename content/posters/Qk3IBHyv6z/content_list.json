[{"type": "text", "text": "Multi-Agent Imitation Learning: Value is Easy, Regret is Hard ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jingwu Tang Carnegie Mellon University jingwutang@cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Gokul Swamy Carnegie Mellon University gswamy@cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Fei Fang Carnegie Mellon University feifang@cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Zhiwei Steven Wu Carnegie Mellon University zstevenwu@cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to coordinate a group of agents based on demonstrations of an expert doing so. Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert within the support of the demonstrations. While doing so is sufficient to drive the value gap between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents. Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator\u2019s recommendations outside of the state distribution their recommendations induce. In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the regret gap that explicitly accounts for potential deviations by agents in the group. We first perform an in-depth exploration of the relationship between the value and regret gaps. First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even value equivalence can lead to an arbitrarily large regret gap. This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL. We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap (a) under a coverage assumption on the expert (MALICE) or $(b)$ with access to a queryable expert (BLADES). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of a mediator learning to coordinate a group of strategic agents via recommendations of actions to take without knowledge of their underlying utility functions (e.g. routing a group of drivers through a road network). Given the difficulty of manually specifying the quality of a recommendation in such situations, it is natural to provide the mediator with data of desired coordination behavior, turning our problem into one of multi-agent imitation learning (MAIL, [27, 6, 19, 26, 11]). In our work, we explore the nuances of a fundamental MAIL question: ", "page_idx": 0}, {"type": "text", "text": "What is the right objective for the learner in a multi-agent imitation learning problem? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We can begin to answer this question by exploring the following scenario: consider developing a routing application to provide personalized route recommendations $(\\sigma)$ to a group of users with joint policy $\\pi$ (e.g. the routing policy that underlies the recommendations provided in Google Maps [3]). As usual in imitation learning (IL), we assume we are given access to demonstrations from an expert $\\sigma_{E}$ (e.g. a past iteration of the application). We can imagine two kinds of users of our application (i.e. ", "page_idx": 0}, {"type": "text", "text": "agents): non-strategic users who blindly follow the recommendations of our routing application and strategic users who will deviate from our recommendations if they have the incentive to do so under their (unknown) personal utility function (e.g. we recommend a long route to a busy driver). We use $J_{i}(\\pi_{\\sigma})$ below to denote the value of the mediator\u2019s learned policy $\\sigma$ under the ith agent\u2019s utility. ", "page_idx": 1}, {"type": "text", "text": "Case 1: No Strategic Agents. In the idealized situation where all agents in the population are perfectly obedient, we can essentially treat a MAIL problem as a single-agent IL (SAIL) problem over joint policies. It is therefore natural to use a direct extension of the well-studied value gap criterion from the SAIL literature [1, 28, 21, 24, 22, 23, 25, 16] to the multi-agent setting: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[m]}J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Intuitively, driving the value gap to 0 (i.e. achieving value equivalence in the terminology of [10]) implies that along as long as all agents blindly follow our recommendations, we have learned a policy that performs at least as well as that of the expert from the perspective of any agent in the population. In our running routing application example, this means that if no driver deviates from the previous behavior, all drivers will be at least as happy as they were with the prior iteration of the application. ", "page_idx": 1}, {"type": "text", "text": "Case 2: Strategic Agents. Of course for any MAIL problem where agents actually have agency, we need to account for the fact that agents may deviate from our recommendations if it appears beneficial to do so from their subjective perspective. Let us denote the class of deviations (i.e. policy modifications) for agent $i$ as $\\Phi_{i}$ . Then, we can define the regret induced by the mediator\u2019s policy as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma):=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}\\in\\Phi_{i}}\\big(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})\\big),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\phi_{i}$ is a strategic deviation of agent $i$ and $\\pi_{\\sigma,\\phi_{i}}$ is the joint agent policy induced by all agents other than i following $\\sigma$ \u2019s recommendations. Intuitively, regret captures the maximum incentive any agent in the population has to deviate from the mediator\u2019s recommendations. We can then compare this metric between the expert and learner policies to arrive at the notion of a regret gap [27]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Driving the regret gap to zero (i.e. achieving regret equivalence) implies that even if agents are free to deviate, our learned policy is at least as good as the expert\u2019s from the perspective of an arbitrary agent in the population. In our preceding example, this means that despite the fact that they are not forced to follow our application\u2019s recommendations, all agents would have no more incentive to take an alternate route than they did under the previous iteration of the application. ", "page_idx": 1}, {"type": "text", "text": "A simple decomposition allows us to show that a small value gap does not in general imply a small regret gap. Consider the performance difference between the learner\u2019s policy under all obedient $(J_{i}(\\pi_{\\sigma}))$ and a deviating ith agent $(J_{i}(\\pi_{\\sigma,\\phi_{i}}))$ . We can decompose this quantity into the following: ", "page_idx": 1}, {"type": "equation", "text": "$$\nI_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})=\\underbrace{(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}}))}_{\\mathrm{(L:~value~gap~under~}\\phi_{i})}+\\underbrace{(J_{i}(\\pi_{\\sigma_{E},\\phi_{i}}))-J_{i}(\\pi_{\\sigma_{E}})}_{\\mathrm{(II:~expert~regret~under~}\\phi_{i})}+\\underbrace{(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))}_{\\mathrm{(II:~SAIL~value~gap)}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where we use $\\pi_{\\sigma_{E},\\phi_{i}}$ to denote agent joint behavior under expert recommendations and deviation $\\phi_{i}$ . Term III is the standard single-agent value gap (i.e. the performance difference under the assumption that no agents deviate). Term $\\mathrm{II}$ is the expert\u2019s regret under deviation $\\phi_{i}$ (i.e. a quantity we cannot control). Thus, the difference between the regret gap and value gap objectives can be boiled down to Term I: $J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})$ . Observe that because of the state distribution shift induced by deviation $\\phi_{i}$ , minimizing Term III doesn\u2019t give us any guarantees with respect to Term 1. This underlies our key insight: regret is hard in MAIL as it requires knowing what the expert would have done in response to an arbitrary agent deviation. More explicitly, our contributions are three-fold: ", "page_idx": 1}, {"type": "text", "text": "1. We initiate the study of the regret gap for MAIL in Markov Games. Unlike the value gap \u2013 the standard objective in single-agent IL \u2013 the regret gap captures the fact that agents in the population may choose to deviate from the mediator\u2019s recommendations. The shift from value to regret gap captures what is fundamentally different about the SAIL and the MAIL problems. ", "page_idx": 1}, {"type": "text", "text": "2. We investigate the relationship between regret gap and the value gap. We show that under the assumption of complete reward and deviation function classes, regret equivalence implies value equivalence. However, we also prove that value equivalence provides essentially no guarantees on the regret gap, establishing a fundamental limitation of applying SAIL algorithms to MAIL problems. ", "page_idx": 1}, {"type": "text", "text": "3. We provide a pair of efficient algorithms to minimize the regret gap under certain assumptions. While regret equivalence is hard to achieve in general as it depends on counter-factual expert recommendations, we derive a pair of efficient reductions for minimizing the regret gap that operate under different assumptions: MALICE (which operates under a coverage assumption) and BLADES (which requires access to a queryable expert). We prove that both algorithms can provide $O(H)$ bounds on the regret gap, where $H$ is the horizon, matching the strongest known results for the value gap in single-agent IL. See Table 1 for a summary of our regret gap bounds. ", "page_idx": 2}, {"type": "table", "img_path": "Qk3IBHyv6z/tmp/ddd85dbf7f65604d71332b58be036242930b4124b0244067cc8a6b1250905611.jpg", "table_caption": [], "table_footnote": ["Table 1: A summary of our results: upper and lower bounds on the regret gap (i.e. $\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E}))$ of various approaches to multi-agent IL. Here, $\\beta$ is the coverage constant in Assumption 5.2, $u$ is the recoverability constant in Assumption 5.1, $H$ is the horizon. "], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Single-Agent Imitation Learning. Much of the theory of imitation learning focuses on the singleagent setting [14]. Offline approaches like behavioral cloning (BC, [15]) reduce the problem of imitation to mere supervised learning. Ignoring the covariate shift in state distributions between the expert and learner policies can cause compounding errors [17, 21] and associated poor performance. In response, interactive IL approaches like inverse reinforcement learning (IRL, [1, 28]) allow the learner to observe the consequences of their actions during the training procedure, preventing compounding errors [21]. However, such approaches can be rather sample-inefficient due to the need to repeatedly solve a hard RL problem [25, 16]. Alternative approaches include interactively querying the expert to get action labels on the learner\u2019s induced state distribution (DAgger, [17]) or, assuming full coverage of the demonstrations, using importance weighting to correct for the covariate shift (ALICE, [20]). Our BLADES and MALICE algorithms can be seen as the regret gap analog of the value gap-centric DAgger and ALICE algorithms, operating under the same assumptions. ", "page_idx": 2}, {"type": "text", "text": "Multi-Agent Imitation Learning. The concept of the regret gap was first introduced in the exceptional work of Waugh et al. [27], though their exploration was limited to Normal Form Games (NFGs), in contrast to the more general Markov Games (MGs) we focus on. Fu et al. [7] briefly consider the regret gap in Markov Games (MGs) but do not explore its properties nor provide algorithms for efficient minimization. Most empirical MAIL work [19, 12, 4, 26, 11] is value gap-based, while we take a step back and ask what the right objective is for MAIL in the first place. ", "page_idx": 2}, {"type": "text", "text": "Inverse Game Theory. Another line of work focuses on inverse game theory in Markov Games [13, 8], where the goal is to recover a set of utility functions that rationalize the observed agent behavior, rather than learning to coordinate from demonstrations. A detailed comparison between the goals of our work at that of inverse game theory provided in Appendix F. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin with the notation we will use in our paper. Throughout, we use $\\Delta(X)$ denote the space of probability distribution over a set $\\mathcal{X}$ . We will use $\\ell$ to denote the loss function each algorithm optimizes, which should be thought of as a convex upper bound on the total variation distance TV. We use $\\ell_{\\mathsf{T V}}$ when the loss function is exactly the TV distance. ", "page_idx": 2}, {"type": "text", "text": "Markov Games. We use $M G(H,S,\\mathcal{A},\\mathcal{T},\\{r_{i}\\}_{i=1}^{m},\\rho_{0})$ to denote a Markov Game (MG) between $m$ agents. Here, $H$ is the horizon, $\\boldsymbol{S}$ is the state space, and $\\mathcal{A}=\\mathcal{A}_{1}\\times\\ldots\\times\\mathcal{A}_{m}$ is the joint action space for all agents. We use $\\mathcal{T}:S\\times A\\rightarrow\\Delta(S)$ to denote the transition function. Furthermore, the reward (utility) function for agent $i\\in[m]$ is denoted by $r_{i}:S\\times A\\to[-1,1]$ . Lastly, we use $\\rho_{0}$ to denote the initial state distribution from which the initial state $s_{0}\\sim\\rho_{0}$ is sampled. ", "page_idx": 2}, {"type": "text", "text": "Learning to Coordinate. Rather than considering the problem of learning individual agent policies in the MG, we take the perspective of a mediator who is giving recommendations to each agent to help them coordinate their behavior (e.g. a smartphone mapping application providing directions to a set of users). At each time step, the mediator gives each agent $i$ a private action recommendation $a_{i}$ to take at the current state $s$ . Critically, no agent observes the recommendations the mediator provides to another agent. We can represent the mediator as a Markovian joint policy $\\sigma\\in\\Sigma$ , where $\\bar{\\sigma}:S\\rightarrow\\Delta(A)$ . We use $\\sigma(\\vec{a}|s)$ to denote the probability of recommending joint action $\\vec{a}$ in state $s$ . We use $\\pi:S\\rightarrow\\Delta(A)$ to denote the joint policy that agents play in response to the mediator\u2019s policy. When agents exactly follow the mediator\u2019s recommendations, we denote their joint policy as $\\pi_{\\sigma}$ . ", "page_idx": 3}, {"type": "text", "text": "A trajectory $\\xi\\,\\sim\\,\\pi\\,=\\,\\{s_{h},\\vec{a}_{h}\\}_{h=1,\\dots,H}$ refers to a sequence of state-action pairs generated by starting from $s_{0}\\sim\\rho_{0}$ and repeatedly sampling joint action $\\vec{a}_{h}$ and next states $s_{h+1}$ from $\\pi$ and $\\tau$ for $H-1$ time steps. Let $d_{h}^{\\pi}$ denote the state visitation distribution at timestep $h$ following $\\pi$ and let $\\begin{array}{r}{d^{\\pi}=\\frac{1}{H}\\sum_{h=1}^{H}d_{h}^{\\pi}}\\end{array}$ be the average state distribution. Let $\\rho_{h}^{\\pi}(s_{h},\\vec{a}_{h})$ denote the occupancy measure \u2013 i.e., probability of reaching state $s$ and then taking action $\\vec{a}$ at time step $h$ . By definition, we know that $\\forall_{h},\\sum_{s,\\vec{a}}\\rho_{h}^{\\pi}(s,\\vec{a})=1$ . Let $\\begin{array}{r}{\\rho^{\\pi}(s,\\vec{a})=\\frac{1}{H}\\sum_{h=1}^{\\bar{H}}\\rho_{h}^{\\pi}(s,\\vec{a})}\\end{array}$ be the average occupancy measure. ", "page_idx": 3}, {"type": "text", "text": "We use $V_{i,h}^{\\pi}$ to denote the expected cumulative reward of agent $i$ under this policy from time step $h$ , i.e. $\\begin{array}{r}{V_{i,h}^{\\pi}(s)=\\mathbb{E}_{\\xi\\sim\\pi}[\\sum_{t=h}^{H}r_{i}(s_{t},\\vec{a}_{t})|s_{h}=s]}\\end{array}$ . We define Q-value function of agent $i$ as $Q_{i,h}^{\\pi}(s,\\vec{a})=$ $\\begin{array}{r}{\\mathbb{E}_{\\xi\\sim\\pi}[\\sum_{t=h}^{H}r_{i}(s_{t},\\vec{a}_{t})|s_{h}\\,=\\,s,\\vec{a}_{h}\\,=\\,\\vec{a}]}\\end{array}$ . We define advantage of an agent $i$ to be the difference between its Q-value on a selected action and the -value on the state, i.e. $A_{i,h}^{\\pi}(s,\\vec{a})=Q_{i,h}^{\\pi}(s,\\vec{a})-$ $V_{i,h}^{\\pi}(s)$ . We also define the performance of a policy $\\pi$ from the perspective of agent $i$ as $J_{i}(\\pi)=$ $\\begin{array}{r}{\\mathbb{E}_{s_{0}\\sim\\rho_{0}}[\\mathbb{E}_{\\xi\\sim\\pi}[\\sum_{t=1}^{H}r_{i}(s_{t},\\vec{a}_{t})|s=s_{0}]]}\\end{array}$ . Observe that performance is the inner product between the occupancy measure and the agent\u2019s reward function, i.e. $\\begin{array}{r}{J_{i}(\\pi)=H\\sum_{s,{\\vec{a}}}\\rho^{\\pi}({\\bar{s}},{\\vec{a}})r_{i}(s,{\\vec{a}})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Correlated Equilibria. We now introduce the notion of a correlated equilibrium (CE, Aumann [2]). First, we define a strategy deviation $\\phi_{i}$ for the $i$ -th agent as a map $\\phi_{i}\\,:\\,S\\,\\times\\,A_{i}\\,\\,\\to\\,A_{i}$ . Intuitively, a strategy deviation captures how the agent responds to the current state of the world and the recommendation of the mediator \u2013 they can either obey (in which case $\\phi_{i}(s,a)=a)$ ) or defect (in which case $\\phi_{i}(s,a)\\neq a)$ ). Let $\\Phi_{i}$ be the set of deviations for agent $i$ , which is a subset of all possible deviations. We use $\\Phi:=\\{\\Phi_{i}\\}_{i=1}^{m}$ to denote deviations for all agents. We assume that for all $i$ , the identity mapping $\\phi_{i}(s,a)\\equiv a$ is in $\\Phi_{i}$ . We use $\\pi_{\\sigma,\\phi_{i}}$ to denote $(\\phi_{i}\\circ\\pi_{\\sigma,i})\\odot\\pi_{\\sigma,-i}$ : the joint agent policy induced by mediator policy $\\sigma$ being over-ridden by deviation $\\phi_{i}$ . We can now define a CE. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Regret and CE in General-Sum MGs). Let $\\sigma\\,\\in\\,\\Sigma$ be the mediator\u2019s policy in $a$ Markov Game, and $\\Phi_{i}$ , $i\\in[m]$ be the deviation classes for each agent. Then, ", "page_idx": 3}, {"type": "text", "text": "1. We define the regret of a mediator policy $\\sigma$ to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma):=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}\\in\\Phi_{i}}\\big(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2. We say a mediator with policy $\\sigma$ induces an $\\epsilon$ -approximate Correlated Equilibrium (CE) if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, regret captures the maximum utility any agent can gain by defecting from the mediator\u2019s recommendation. A CE is an induced joint policy where no agent has a large incentive to deviate. ", "page_idx": 3}, {"type": "text", "text": "4 On the Relationship between the Value Gap and the Regret Gap ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As sketched above, we consider two potential objectives for the learner in MAIL: ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Value Gap). We define the value gap between the expert\u2019s policy $\\sigma_{E}$ and the learner\u2019s policy $\\sigma\\in\\Sigma$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 4.2 (Regret Gap). We define the regret gap between the expert\u2019s policy $\\sigma_{E}$ and the learner\u2019s policy $\\sigma\\in\\Sigma$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}\\in\\Phi_{i}}\\big(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})\\big)-\\operatorname*{max}_{k\\in[m]}\\operatorname*{max}_{\\phi_{k}\\in\\Phi_{k}}\\big(J_{k}(\\pi_{\\sigma_{E},\\phi_{k}})-J_{k}(\\pi_{\\sigma_{E}})\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "Qk3IBHyv6z/tmp/9a424bd0ee2f4825908ae8bb458d2be5192573260f2bdcf88325ed621b8de287.jpg", "img_caption": ["Figure 1: Under expressive enough reward function and deviation classes, regret equivalence implies value equivalence but not vice versa, making the regret gap a \u201cstronger\u201d objective than the value gap. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We say that the learner\u2019s policy satisfies value / regret equivalence when the value / regret gap is 0. We now explore the relationship between the value and regret gap in MAIL, 1summarized in Figure 1. We use $\\bar{J}_{i}(\\pi_{\\sigma},f)$ and $\\mathcal{R}_{\\Phi}(\\sigma,f)$ to denote the value/regret of policy $\\sigma$ under the reward function $f$ ", "page_idx": 4}, {"type": "text", "text": "4.1 Regret Equivalence $^+$ Complete Reward / Deviation Class $\\Longrightarrow$ Value Equivalence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we show that if the reward function class and deviation class are both complete, then regret equivalence implies value equivalence. We say that the reward function class is complete when $\\bar{\\mathcal{F}}=\\{\\mathcal{S}\\times\\mathcal{A}\\stackrel{!}{\\rightarrow}[-1,1]\\}$ (i.e. all convex combinations of state-action indicators), and that the deviation class is complete if for every agent $i$ , $\\Phi_{i}=\\{S\\times A_{i}\\rightarrow A_{i}\\}$ (i.e. all possible deviations). ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Complete Classes). If the reward function class $\\mathcal{F}$ and deviation class $\\Phi$ are complete and regret equivalence is satisfied (i.e. $\\begin{array}{r}{\\operatorname*{sup}_{f\\in\\mathcal{F}}(\\mathcal{R}_{\\Phi}(\\sigma,f)\\!-\\!\\mathcal{R}_{\\Phi}(\\sigma_{E},f))=0;}\\end{array}$ , then value equivalence is also satisfied: $\\begin{array}{r}{\\operatorname*{sup}_{f\\in\\mathcal{F}}\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}},f)-J_{i}(\\pi_{\\sigma},f))=0.}\\end{array}$ . [Proof] ", "page_idx": 4}, {"type": "text", "text": "Next, we prove that large classes are needed for this implication to hold true. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2 (Incomplete Classes). There exists an $M G$ , an expert policy $\\sigma_{E}$ , and a trained policy $\\sigma$ such that even though the regret equivalence is satisfied under the true reward function $r$ , i.e. $\\mathcal{R}_{\\Phi}(\\sigma,r)-\\mathcal{R}_{\\Phi}(\\sigma_{E},r)=0$ , the value $g a p\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}},r)-J_{i}(\\pi_{\\sigma},r))\\neq0.$ . [Proof] ", "page_idx": 4}, {"type": "text", "text": "Together, these results tell us that with an expressive enough class of reward functions / deviations, regret equivalence is stronger than value equivalence. We now turn our attention to the converse. ", "page_idx": 4}, {"type": "text", "text": "4.2 Value Equivalence $\\nRightarrow$ Regret Equivalence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now show a surprising result: value equivalence does not directly imply a low regret gap! In the worst case, value equivalence fails to provide any meaningful guarantees on the regret gap. This reveals a critical distinction between SAIL and MAIL not fully addressed in the prior work. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.3. There exists a Markov Game, an expert policy $\\sigma_{E}$ , and a learner policy $\\sigma$ , such that even occupancy measure of $\\pi_{\\sigma}$ exactly matches $\\pi_{\\sigma_{E}}$ , i.e. $\\forall(s,\\vec{a}),\\rho^{\\pi_{\\sigma}}(s,\\vec{a})=\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})$ (i.e. we have value equivalence under all rewards), the regret gap $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\geq\\Omega(H).}\\end{array}$ . [Proof] ", "page_idx": 4}, {"type": "text", "text": "We leave the details of the proof for this theorem in Appendix E.3. As visualized in Figure 2, both the expert and learner policies only visit the states in the lower path $s_{2},s_{4},...,s_{2H-2}$ . The trained policy perfectly matches the occupancy measure of the expert by taking identical actions in visited states $s_{2},s_{4},...,s_{2H-2}$ . However, expert demonstrations lack coverage of state $s_{1}$ as it is unreachable by executing $\\pi_{E}$ . This omission becomes critical when agent 1 deviates from the original policy, making $s_{1}$ unreachable with high probability. Consequently, the trained policy may perform poorly in $s_{1}$ , in stark contrast to the expert playing a CE under the true reward function. This example highlights the key difference between value equivalence and regret equivalence: the former only depends on states actually visited by the policy, while the latter depends on the counterfactual recommendations the learner would make at unvisited states in response to an agent deviations. ", "page_idx": 4}, {"type": "text", "text": "Remark 4.1. As shown in Theorem 4.3, even if the learner has access to infinite samples on the equilibrium path from expert demonstrations, it is possible that the learner remains unaware of the expert\u2019s behavior in states unvisited by the expert (but reachable by the deviated agents joint policy). Thus, from an information theoretic perspective, it is impossible for the learner to minimize the regret gap without knowing how the expert would behave on those states. This demonstrates the fundamental difficulty of minimizing the regret gap, and thus, regret is \u2018hard\u2019 in MAIL. We therefore need a fundamentally new paradigm of MAIL algorithm to minimize the regret gap. ", "page_idx": 4}, {"type": "image", "img_path": "Qk3IBHyv6z/tmp/f9ab9b36c787a3228656be0a4955e7d9733a410adebb7df4083bdbee00a6c527.jpg", "img_caption": ["Figure 2: Illustration of an Markov Game that captures why \u201cregret is hard\u201d. Here, $\\sigma_{E}(a_{1}a_{1}|s_{0})=1$ . Observe that $s_{1}$ is un-visited when all agents obediently follow $\\sigma_{E}$ but is with probability 1 under deviation $\\phi_{1}$ $(\\phi_{1}(s_{0},a_{1})=\\phi_{1}(s_{1},a_{1})=a_{2})$ ). This means that unless we know what the expert $\\sigma_{E}$ would have recommended counter-factually in $s_{1}$ , we cannot minimize the regret gap. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.3 Low Regret $\\mathbf{Gap}\\implies\\mathbf{CE}$ , Low Value Gap \u0338=\u21d2 CE", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the deep connections between regret and correlated equilibrium discussed above, it is perhaps intuitive that if the expert $\\sigma_{E}$ is playing a CE, a low regret gap means the learner is as well. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4 (Regret Gap Implies CE). If the expert policy $\\sigma_{E}$ induces a $\\delta_{1}$ -approximate $C E,$ , and the learner policy $\\sigma$ satisfies $\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\leq\\delta_{2}$ , then $\\sigma$ induces a $\\delta_{1}+\\delta_{2}$ -approximate $C E$ . [Proof] ", "page_idx": 5}, {"type": "text", "text": "Then, by combining our preceding result with Theorem 4.3, it follows that a low value gap does not imply that the learner is playing a CE. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.5. There exists a Markov Game, an expert policy $\\sigma_{E}$ , and a learner policy $\\sigma$ , such that $\\sigma_{E}$ induces a $\\delta_{1}$ -approximate $C E$ , and $\\sigma$ satisfies $\\mathrm{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))=\\delta_{2}$ , $\\sigma$ induces $a$ $\\Omega(H)$ -approximate $C E$ . ", "page_idx": 5}, {"type": "text", "text": "Together, these results imply that if we are interested in inducing a CE amongst the agents in the population, the regret gap is a more suitable objective. ", "page_idx": 5}, {"type": "text", "text": "4.4 Efficient Algorithms for Minimizing the Value Gap ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although we have shown that the value gap is a \u2018weaker\u2019 objective in some sense, in many real-world scenarios, the agents may be non-strategic. In these scenarios, minimizing value gap can be a reasonable learning objective. As we will demonstrate here, the natural multi-agent generalization of single-agent IL algorithms can efficiently minimize the value gap\u2014hence, value is \u2018easy\u2019 in MAIL. ", "page_idx": 5}, {"type": "text", "text": "Behavior Cloning (BC) and Inverse Reinforcement Learning (IRL) are two single-agent IL algorithms aimed at minimizing the value gap. By running these algorithms over joint policies, we can apply BC and IRL to the multi-agent setting, which we call Joint Behavior Cloning (J-BC) and Joint Inverse Reinforcement Learning (J-IRL). Doing so results in the same value gap bounds as in the single-agent setting. More details on of J-BC and J-IRL can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.6 (J-BC Value Gap Upper Bound). If J-BC returns a policy $\\sigma$ that satisfies $\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}[\\ell(\\sigma_{E}(s),\\sigma(s))]\\le\\epsilon,$ , then the value gap $\\begin{array}{r}{\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))\\le O(\\epsilon H^{2})}\\end{array}$ . [Proof] ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.7 (J-IRL Value Gap Upper Bound). If $J$ -IRL outputs a policy $\\sigma$ with moment-matching error ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}_{\\pi_{\\sigma_{E}}}\\left[\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})\\right]-\\mathbb{E}_{\\pi_{\\sigma}}\\left[\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})\\right]\\leq\\epsilon H,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then the value gap $\\begin{array}{r}{\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))\\leq O(\\epsilon H).}\\end{array}$ . [Proof] ", "page_idx": 5}, {"type": "text", "text": "As argued by Swamy et al. [21], satisfying the conditions for either of the above theorems can be achieved oracle-efficiently via a reduction to no-regret online learning. We now turn our attention to sufficient conditions for there to exist efficient algorithms for minimizing the regret gap. ", "page_idx": 5}, {"type": "text", "text": "5 Efficient Algorithms for Minimizing the Regret Gap ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our following analysis, we will make a recoverability assumption: that a single-step agents deviation could at most cost the expert a fixed constant. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.1 ( $u$ -recoverability). We say that an MG is $u$ -recoverable if the expert advantage function is bounded for all deviations, i.e. \u2200s,\u20d7a, h, i, \u03d5i,  Ai\u03c0,\u03c3hE , $\\forall s,\\vec{a},h,i,\\phi_{i},\\left|A_{i,h}^{\\pi_{\\sigma_{E},\\phi_{i}}}(s,\\vec{a})\\right|\\leq\\bar{u}.$ . ", "page_idx": 6}, {"type": "text", "text": "Intuitively, a small value of $u$ means that we\u2019re not in a problem where a single agent can deviate and a joint mistake happens that even the expert couldn\u2019t recover from for the rest of the episode. In the worst case, $u$ is $\\bar{O}(H)$ . However, we believe that in many cases $u$ is small. For instance, in the route planning example, at some point many cars may miss their turns/intersections/exits, but this can be recovered within a constant time, even when a single driver chooses not to follow its recommendation, rather than $u$ increasing as $H$ increases. ", "page_idx": 6}, {"type": "text", "text": "This assumption can be thought of natural multi-agent generalization of the standard recoverability assumption in SAIL [18, 21, 20] which is necessary and sufficient to avoid compounding errors while maintaining computational efficiency. While we define recoverability with respect to the actual reward function for implicitly, one can instead easily define it with respect to the worst-case reward function in a class $(\\operatorname{sup}_{f\\in{\\mathcal{F}}})$ \u2013 moment recoverability \u2013 as in [21] to avoid the need to know the ground truth set of agent reward functions $r$ to bound $u$ . ", "page_idx": 6}, {"type": "text", "text": "In Section 4.2, we proved that for general MGs, J-BC and J-IRL don\u2019t give any guarantees on the regret gap. Fundamentally, without the ability to observe how the expert would have responded in the counter-factual state induced by a deviation, the learner cannot ensure that they match the expert\u2019s regret. We now explore two different sets of assumptions that give us this ability. ", "page_idx": 6}, {"type": "text", "text": "5.1 Assumption 1: Full Coverage of Expert Demonstrations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we introduce a coverage assumption on the expert\u2019s state distribution $d^{\\pi_{\\sigma_{E}}}(s)$ which states that the expert visits every state with a positive probability. We will show that this assumption is sufficient to give a regret gap guarantee. The state coverage assumption is a common theoretical assumption in the analysis of learning in MDPs/MGs [5] and has been explored in SAIL [20]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.2 $\\beta.$ -coverage). There exists a constant $\\beta>0$ such that for the expert\u2019s policy $\\sigma_{E}$ , it holds that $d^{\\pi_{\\sigma_{E}}}(s)\\geq\\beta$ for all $s$ . ", "page_idx": 6}, {"type": "text", "text": "Intuitively, this assumption implies that in the infinite sample limit, there are no states where we are unsure what the expert would recommend. As discussed in Remark 4.1, without the ability to interactively query the expert, a coverage assumption is necessary because we cannot minimize the regret gap without knowing the expert mediator\u2019s actions in counter-factual states. ", "page_idx": 6}, {"type": "text", "text": "We first show that under Assumption 5.2, J-BC and J-IRL get a (relatively weak) regret gap guarantee. ", "page_idx": 6}, {"type": "text", "text": "5.1.1 Regret Gaps of J-BC and J-IRL under Full Demonstration Coverage ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin by analyzing joint behavioral cloning (J-BC). ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1 (J-BC Regret Gap Upper Bound). Under Assumption 5.1 and Assumption 5.2, if the J-BC algorithm returns a policy $\\sigma$ that satisfies $\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}[\\ell(\\sigma_{E}(s),\\sigma(s))]\\le\\epsilon$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O\\left(\\frac{1}{\\beta}\\epsilon u H\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "[Proof] ", "page_idx": 6}, {"type": "text", "text": "We leave the proof in Appendix E.7. It is worth to note that although the dependency of $H$ is linear under our recoverability assumption, we still need to pay for the term $\\frac1\\beta$ in our regret gap bound. In general, this term can grow exponentially with the horizon, making this guarantee relatively weak. We can show its tightness by slightly modifying the example in Theorem 4.3 to satisfy the assumptions. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 (J-BC Regret Gap Lower Bound). There exists a Markov Game, an expert policy $\\sigma_{E}$ , and learner policy $\\sigma$ such that $\\sigma_{E}$ satisfies Assumption $5.1$ and Assumption 5.2, $\\sigma$ achieves $B C$ error ", "page_idx": 6}, {"type": "text", "text": "$\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}[\\ell_{\\mathsf{T V}}(\\sigma_{E}(s),\\sigma(s))]\\leq\\epsilon$ , and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\Omega\\left(\\frac{1}{\\beta}\\epsilon u H\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "[Proof] ", "page_idx": 7}, {"type": "text", "text": "We now prove analogous results for joint inverse reinforcement learning (J-IRL). ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3 (J-IRL Regret Gap Upper Bound). Under Assumption 5.2 and Assumption 5.1 and with a complete reward function class $\\mathcal{F}$ , if J-IRL returns a policy $\\sigma$ with moment-matching error ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}_{\\pi_{\\sigma_{E}}}\\left[\\frac{\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})}{H}\\right]-\\mathbb{E}_{\\pi_{\\sigma}}\\left[\\frac{\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})}{H}\\right]\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O\\left(\\frac{1}{\\beta}\\epsilon u H\\right)}\\end{array}$ . [Proof] ", "page_idx": 7}, {"type": "text", "text": "There are two interesting features of this theorem. The first is that we needed to assume that the reward function class is complete \u2013 otherwise, a small value gap can still translate to a large regret gap. The second is that the upper-bound for J-IRL matches that for J-BC, which is in stark contrast to the single-agent setting, where IRL enjoys linear-in- $\\mathcal{H}$ guarantees with respect to the value gap [21]. We now show this is not an artifact of our analysis by providing a matching lower bound. ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.4 (J-IRL Regret Gap Lower Bound). There exists a Markov Game, an expert policy $\\sigma_{E}$ , and a policy $\\sigma$ such that $\\sigma_{E}$ satisfies Assumption 5.1 and Assumption 5.2, the trained policy $\\sigma$ gets moment-matching error ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}_{\\pi_{\\sigma_{E}}}\\left[\\frac{\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})}{H}\\right]-\\mathbb{E}_{\\pi_{\\sigma}}\\left[\\frac{\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})}{H}\\right]\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\Omega\\left(\\frac{1}{\\beta}\\epsilon u H\\right)}\\end{array}$ . [Proof] ", "page_idx": 7}, {"type": "text", "text": "This result implies another fundamental distinction between SAIL and MAIL: in contrast to the value gap, interactive training alone is not sufficient to effectively minimize the regret gap. ", "page_idx": 7}, {"type": "text", "text": "5.1.2 MALICE: Multi-agent Aggregation of Losses to Imitate Cached Experts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Observe that the upper bounds for both J-BC and J-IRL include a dependence on the inverse of the coverage coefficient $\\textstyle{\\frac{1}{\\beta}}$ , which can be rather large for problems with long horizons or large action spaces. We now present an efficient algorithm that is able to avoid this dependence by extending the ALICE algorithm [20] to the multi-agent setting. ALICE is an interactive algorithm that, at each round, uses importance sampling to re-weight the behavior cloning (BC) loss based on the density ratio between the current learner policy and that of the expert. Accordingly, ALICE requires a full demonstration coverage assumption to ensure that these importance weights are finite. ALICE uses a no-regret algorithm to learn a policy that minimizes reweighed on-policy error, which guarantees a linear-in- $H$ bound on the value gap under a recoverability assumption [20]. ", "page_idx": 7}, {"type": "text", "text": "In Algorithm 1, we describe Multi-agent ALICE (MALICE), where adapt ALICE to the multi-agent setting (i.e. minimizing the regret gap). Specifically, we modify the ALICE loss function to include a maximum over all deviations. This gives us ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{MALICE}}(\\sigma,D_{E},\\hat{\\sigma})=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}\\in\\Phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}\\left[\\frac{d^{\\pi_{\\hat{\\sigma},\\phi_{i}}}(s)}{d^{\\pi_{\\sigma_{E}}}(s)}\\ell(\\sigma_{E}(s),\\sigma(s))\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since dd\u03c0\u03c0\u03c3\u02c6\u03c3,E\u03d5 i ((ss)) \u2113(\u03c3E(s), \u03c3(s)) is a convex loss function, and the maximum of convex functions is still a convex function, we know that $\\ell_{\\mathrm{MALICE}}(\\sigma,D_{E},\\hat{\\sigma})$ is a valid convex loss function with scales in $[0,1]$ . As a result, we can run an (arbitrary) no-regret online convex optimization (OCO) algorithm to efficiently optimize it, giving us an efficient reduction from regret gap minimization to no-regret online convex optimization under demonstration coverage. ", "page_idx": 7}, {"type": "text", "text": "We now provide regret gap guarantees on the policy returned by MALICE. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 1 MALICE (Multi-agent Aggregation of Losses to Imitate Cached Experts) ", "page_idx": 8}, {"type": "text", "text": "1: Input: Expert demonstrations $D_{E}$ .   \n2: Initialize $\\bar{\\sigma}^{(1)}\\in\\Sigma$ .   \n3: for $n=1$ to $N$ do   \n4: for $i=1$ to $m$ do   \n5: for $\\phi_{i}\\in\\Phi_{i}$ do   \n6: Sample states from $s_{t}\\sim d^{\\pi_{\\sigma,\\phi_{i}}^{(n)}}$ .   \n7: end for   \n8: end for   \n9: Construct loss function $\\ell^{(n)}(\\sigma)=\\ell_{\\mathrm{MALICE}}(\\sigma,D_{E},\\sigma^{(n)}).$ .   \n10: // Run arbitrary no-regret OCO algorithm on sequence of losses, e.g. FT(R)L:   \n11: $\\begin{array}{r}{\\sigma^{(n+1)}\\gets\\arg\\operatorname*{min}_{\\sigma\\in\\Sigma}\\breve{\\sum_{j=1}^{n}\\ell^{(n)}(\\sigma)}}\\end{array}$   \n12: end for   \n13: Return Best of $\\sigma^{(1:N)}$ on validation data. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5 (MALICE Regret Gap Upper Bound). Let \u03c3 be a policy such that $\\ell_{\\mathrm{MALICE}}(\\sigma,D_{E},\\sigma)\\le\\epsilon$ . Under Assumption 5.1 and Assumption 5.2, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O(\\epsilon u H).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "[Proof] ", "page_idx": 8}, {"type": "text", "text": "As promised, observe that adapting the importance sampling technique of Spencer et al. [20] to the multi-agent setting allows us to efficiently minimize the regret gap while avoiding an upper bound that depends on the coverage coefficient of the expert demonstrations. ", "page_idx": 8}, {"type": "text", "text": "We now show that the bound in Theorem 5.5 is tight by constructing a matching lower bound. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.6 (MALICE Regret Gap Lower Bound). There exists a Markov Game, an expert policy $\\sigma_{E}$ that satisfies Assumption 5.1, and a trained policy $\\sigma$ that gets error $\\ell_{\\mathsf{T V,M A L I C E}}(\\sigma,D_{E},\\sigma)\\le\\epsilon_{i}$ , and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\Omega\\left(\\epsilon u H\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "[Proof] ", "page_idx": 8}, {"type": "text", "text": "We now turn our attention to an alternate assumption and the corresponding regret gap algorithm. ", "page_idx": 8}, {"type": "text", "text": "5.2 Assumption 2: Access to a Queryable Expert ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For many problems, full coverage of expert demonstrations is not a reasonable assumption. Thus, we explore another natural assumption that allows us to observe expert recommendations at counterfactual states: access to a queryable expert. In their classic DAgger algorithm, Ross et al. [18] showed that access to a queryable expert allows one to eliminate the covariate shift that results from the difference between expert and learner induced state distributions. When we transition to the multi-agent setting, we can again use access to a queryable expert to handle yet another source of covariate shift: potential strategic deviations by agents in the population that push the learner outside of the support of the expert. We refer to our multi-agent extension of DAgger as BLADES. ", "page_idx": 8}, {"type": "text", "text": "In each iteration of BLADES, we request the expert to provide recommendations under all possible agent deviations, before training on the aggregated data. More formally, we minimize the following sequence of loss functions: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{BLADES}}(\\sigma,\\hat{\\sigma})=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}\\in\\Phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\hat{\\sigma}},\\phi_{i}}}\\big[\\ell(\\sigma_{E}(s),\\sigma(s))\\big].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Similar to MALICE, we know that the loss $\\ell_{\\mathrm{BLADES}}$ is also a valid convex loss function, and thus we can use a no-regret algorithm to efficiently minimize it. This gives us an efficient reduction from regret gap minimization to no-regret online convex optimization with access to a queryable expert. We now derive and upper and lower bounds on the regret gap of a policy returned by BLADES. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.7 (BLADES Regret Gap Upper Bound). Under Assumption 5.1, if a policy $\\sigma$ satisfies $\\ell_{\\mathrm{BLADES}}(\\sigma,\\sigma)\\leq\\epsilon$ , then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O(\\epsilon u H).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "[Proof] ", "page_idx": 8}, {"type": "text", "text": "Algorithm 2 BLADES (Bend Learner, Aggregate Datasets of Expert Suggestions) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "1: Input: Expert demonstrations $D_{E}$ .   \n2: Initialize learner $\\sigma^{(1)}=\\arg\\operatorname*{min}_{\\sigma}\\mathbb{E}_{s\\sim D_{E}}\\ell(\\sigma_{E}(s),\\sigma(s))$ .   \n3: for $n=1$ to $N$ do   \n4: for $i=1$ to $m$ do   \n5: for $\\phi_{i}\\in\\Phi_{i}$ do   \n6: Sample trajectories from \u03c0(\u03c3,n\u03d5)i.   \n7: Query expert for action recommendations to construct dataset $D_{\\phi_{i}}^{(n)}=\\{(s,\\sigma_{E}(s))\\}$ .   \n8: end for   \n9: end for   \n10: Construct loss function $\\ell^{(n)}(\\sigma)=\\ell_{\\mathrm{BLADES}}(\\sigma,\\sigma^{(n)})$ .   \n1112:: $\\begin{array}{r}{\\sigma^{(n+1)}\\gets\\arg\\operatorname*{min}_{\\sigma\\in\\Sigma}\\breve{\\sum_{j=1}^{n}\\ell^{(n)}(\\sigma)}}\\end{array}$ .lgorithm on sequence of losses, e.g. FT(R)L:   \n13: end for   \n14: Return Best of $\\sigma^{(1:N)}$ on validation data. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.8 (BLADES Regret Gap Lower Bound). There exists a Markov Game, an expert policy $\\sigma_{E}$ , and a trained policy $\\sigma$ such that $\\sigma_{E}$ satisfies Assumption 5.1, $\\sigma$ achieves error $\\ell_{\\mathsf{T V,B L A D E S}}(\\sigma,\\sigma)\\le\\epsilon,$ , and ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\Omega\\left(\\epsilon u H\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "[Proof] ", "page_idx": 9}, {"type": "text", "text": "In short, under either a demonstration coverage assumption or with access to a queryable expert, we are able to efficiently minimize the regret gap on a recoverable MAIL problem. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work focuses on the core question of what fundamentally distinguishes multi-agent IL problems from single-agent ones. In short, our answer is that on problems with strategic agents that are not mere puppets, we need to deal with another source of distribution shift: deviations by agents in the population. This new source of distribution shift cannot be efficiently controlled with environment interaction (i.e. inverse RL). Instead, we need to be able to estimate how the expert would act in counter-factual states. Based on this core insight, we derive two reductions that are able to minimize the regret gap under a coverage or queryable expert assumption. We leave the development and implementation of practical approximations of our idealized algorithms to future work. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Drew Bagnell and Brian Ziebart for their incredible patience and detailed answers to a somewhat absurdly large number of questions about their prior work. We thank Simon Shaolei Du for a discussion on our coverage assumptions and Noah Golowich for references to relevant lower bounds in the MARL literature. We also thank Sanjiban Choudhury and Wen Sun for comments on our draft. ZSW, GS, and Jingwu Tang are supported in part by an Air Force STTR grant and the NSF Award #1763786. ZSW is supported in part by the NSF FAI Award #1939606, a Google Faculty Research Award, a J.P. Morgan Faculty Award, a Facebook Research Award, an Okawa Foundation Research Grant, and a Mozilla Research Grant. FF is supported in part by NSF grant IIS-2046640 (CAREER) and the Sloan Research Fellowship. GS is supported by his family and friends. ", "page_idx": 9}, {"type": "text", "text": "8 Contribution Statements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "1. JT uncovered the distinction between the regret and value gaps, proved all of the core results in the paper, and drafted the initial version of the paper.   \n2. GS initially proposed the project, came up with the sufficient conditions and associated algorithms for minimizing the regret gap, and wrote most of the final version of the paper.   \n3. FF and ZSW advised the project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004.   \n[2] R. J. Aumann. Correlated equilibrium as an expression of bayesian rationality. Econometrica: Journal of the Econometric Society, pages 1\u201318, 1987.   \n[3] M. Barnes, M. Abueg, O. F. Lange, M. Deeds, J. Trader, D. Molitor, M. Wulfmeier, and S. O\u2019Banion. Massively scalable inverse reinforcement learning in google maps. arXiv preprint arXiv:2305.11290, 2023. [4] R. P. Bhattacharyya, D. J. Phillips, B. Wulfe, J. Morton, A. Kuefler, and M. J. Kochenderfer. Multi-agent imitation learning for driving simulation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1534\u20131539. IEEE, 2018.   \n[5] L. Erez, T. Lancewicki, U. Sherman, T. Koren, and Y. Mansour. Regret minimization and convergence to equilibria in general-sum markov games. In International Conference on Machine Learning, pages 9343\u20139373. PMLR, 2023. [6] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.   \n[7] J. Fu, A. Tacchetti, J. Perolat, and Y. Bachrach. Evaluating strategic structures in multi-agent inverse reinforcement learning. Journal of Artificial Intelligence Research, 71:925\u2013951, 2021.   \n[8] D. Goktas, A. Greenwald, S. Zhao, A. Koppel, and S. Ganesh. Generative adversarial inverse multiagent learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[10] C. Grimm, A. Barreto, S. Singh, and D. Silver. The value equivalence principle for model-based reinforcement learning. Advances in Neural Information Processing Systems, 33:5541\u20135552, 2020.   \n[11] C. Gulino, J. Fu, W. Luo, G. Tucker, E. Bronstein, Y. Lu, J. Harb, X. Pan, Y. Wang, X. Chen, et al. Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] H. M. Le, Y. Yue, P. Carr, and P. Lucey. Coordinated multi-agent imitation learning. In International Conference on Machine Learning, pages 1995\u20132003. PMLR, 2017.   \n[13] X. Lin, S. C. Adams, and P. A. Beling. Multi-agent inverse reinforcement learning for certain general-sum stochastic games. Journal of Artificial Intelligence Research, 66:473\u2013502, 2019.   \n[14] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics, 7(1-2):1\u2013179, 2018.   \n[15] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988.   \n[16] J. Ren, G. Swamy, Z. S. Wu, J. A. Bagnell, and S. Choudhury. Hybrid inverse reinforcement learning. arXiv preprint arXiv:2402.08848, 2024.   \n[17] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661\u2013668. JMLR Workshop and Conference Proceedings, 2010.   \n[18] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.   \n[19] J. Song, H. Ren, D. Sadigh, and S. Ermon. Multi-agent generative adversarial imitation learning. Advances in neural information processing systems, 31, 2018.   \n[20] J. Spencer, S. Choudhury, A. Venkatraman, B. Ziebart, and J. A. Bagnell. Feedback in imitation learning: The three regimes of covariate shift. arXiv preprint arXiv:2102.02872, 2021.   \n[21] G. Swamy, S. Choudhury, J. A. Bagnell, and S. Wu. Of moments and matching: A gametheoretic framework for closing the imitation gap. In International Conference on Machine Learning, pages 10022\u201310032. PMLR, 2021.   \n[22] G. Swamy, S. Choudhury, D. Bagnell, and S. Wu. Causal imitation learning under temporally correlated noise. In International Conference on Machine Learning, pages 20877\u201320890. PMLR, 2022.   \n[23] G. Swamy, S. Choudhury, J. Bagnell, and S. Z. Wu. Sequence model imitation learning with unobserved contexts. Advances in Neural Information Processing Systems, 35:17665\u201317676, 2022.   \n[24] G. Swamy, N. Rajaraman, M. Peng, S. Choudhury, J. Bagnell, S. Z. Wu, J. Jiao, and K. Ramchandran. Minimax optimal online imitation learning via replay estimation. Advances in Neural Information Processing Systems, 35:7077\u20137088, 2022.   \n[25] G. Swamy, D. Wu, S. Choudhury, D. Bagnell, and S. Wu. Inverse reinforcement learning without reinforcement learning. In International Conference on Machine Learning, pages 33299\u201333318. PMLR, 2023.   \n[26] E. Vinitsky, N. Lichtl\u00e9, X. Yang, B. Amos, and J. Foerster. Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world. Advances in Neural Information Processing Systems, 35:3962\u20133974, 2022.   \n[27] K. Waugh, B. D. Ziebart, and J. A. Bagnell. Computational rationalization: The inverse equilibrium problem. arXiv preprint arXiv:1308.3506, 2013.   \n[28] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "As the algorithms we proposed are theoretical, we do not foresee any direct societal concerns resulting from this work. However, these theoretical algorithms can serve as a foundation for developing practical algorithms or provide guidance for designing practical algorithms in MAIL, which could be applied to real world problems in the future. ", "page_idx": 12}, {"type": "text", "text": "B Extending Single-Agent IL Algorithms to Minimize the Value Gap ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Multi-Agent Joint Behavior Cloning ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Behavioral Cloning (BC, Pomerleau [15]) treats the problem of imitation learning as supervised learning and performs maximum likelihood estimation with expert states as inputs and expert actions as labels. Unfortunately, as first analyzed by Ross and Bagnell [17], the covariate shift between the training (expert states) and test (learner states) distributions can lead to compounding errors \u2013 i.e. a value gap that increases quadratically as a function of the horizon $H$ . We note that this is not an artifact of the particular objective used in BC \u2013 as argued by Swamy et al. [21], the same can be said for any offline imitation learning algorithm. J-BC extends BC to a multi-agent setting by learning a map from the state space $\\boldsymbol{S}$ to the joint action space $\\boldsymbol{\\mathcal{A}}$ . By adapting the analysis of Ross and Bagnell [17] and Swamy et al. [21] to the multi-agent setting, we establish a similar compounding error result for multi-agent behavior cloning in Theorem 4.6. There exists an example of MDP/MG that matches this bound, which shows that the bound is tight [21]. ", "page_idx": 12}, {"type": "text", "text": "B.2 Multi-Agent Inverse Reinforcement Learning ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A popular family of online techniques for imitation learning is inverse reinforcement learning (IRL). Intuitively, IRL can be thought of as being similar to a GAN [9] but in the space of trajectories: the generator is the learner\u2019s policy coupled with a world model to actually give us trajectories, while the discriminator is trained between expert and learner trajectories and is used as a reward function for policy updates. More formally, IRL can be viewed as a two-player zero-sum game between a reward player and a policy player [21]. In each round, the reward player picks a reward function from $\\mathcal{F}$ that maximizes the value gap between $\\sigma_{E}$ and $\\sigma$ , while the policy player uses a reinforcement learning algorithm to learn a new policy in $\\Sigma$ that maximizes the performance under this reward function. ", "page_idx": 12}, {"type": "text", "text": "Intuitively, as the learner can see policy rollouts during training procedure, they cannot be \u201csurprised\u201d by where their policy ends up at test time, removing the covariate shift issue that lies at the heart of compounding errors. More formally, Swamy et al. [21] proved that value gap for single-agent IRL algorithm is $\\bar{O}(\\epsilon H)$ . We now generalize this result to the multi-agent setting. Accordingly, our policy class $\\Sigma$ becomes one of joint policies. We use a reward function class $\\mathcal{F}$ that is identical for all agents (i.e. we assume the the game is common payoff). Then, by following the proof in Swamy et al. [21], we prove a $O(\\epsilon H)$ value gap bound for multi-agent IRL algorithm in Theorem 4.7. ", "page_idx": 12}, {"type": "table", "img_path": "Qk3IBHyv6z/tmp/fd08286d17e61356a3aa7254862bc70363cc46b6a566649bb6ae667a09ee3604.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Useful Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We introduce a lemma which will be very useful in the analysis under the recoverability assumption. It is used in the analysis in the single-agent DAgger [18] and ALICE [20], and we will also use ", "page_idx": 12}, {"type": "text", "text": "it in the analysis for MAIL. It shows that if the policy achieves small on-policy error, then, with recoverability assumption, the value gap is linear over $H$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma C.1. [Ross et al. [18]] For agent joint policy $\\pi_{1}$ and $\\pi_{2}$ , if the advantage of $\\pi_{1}$ is bounded under the true reward function $\\forall i,h,s,\\vec{a},|A_{i,h}^{\\pi_{1}}(s,\\vec{a})|\\,\\le\\,u,$ , and $\\pi_{2}$ get on-policy error $\\mathbb{E}_{s\\sim d^{\\pi_{2}}}[\\ell(\\pi_{1}(s),\\pi_{2}(s))]\\le\\epsilon,$ , then $|J_{i}(\\pi_{1})-J_{i}(\\pi_{2})|\\le\\epsilon u H,\\forall i\\in[m]$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Via the performance difference lemma, $\\forall i\\in[m]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|J_{i}(\\pi_{1})-J_{i}(\\pi_{2})|=\\displaystyle\\left\\lvert\\sum_{h=1}^{H}\\mathbb{E}_{s\\sim d_{h}^{\\pi_{2}}}[A_{i,h}^{\\pi_{1}}(s,\\pi(s))]\\right\\rvert}\\\\ &{}&{\\leq u H\\mathbb{E}_{s\\sim d^{\\pi_{2}}}[\\ell(\\pi_{1}(s),\\pi_{2}(s))]}\\\\ &{}&{\\leq\\epsilon u H}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For our analysis of MALICE and BLADES, we will let $\\pi_{1}$ be any deviated expert policy $\\pi_{\\sigma_{E},\\phi_{i}}$ and $\\pi_{2}$ be the deviated trained policy $\\pi_{\\sigma,\\phi_{i}}$ under the same deviation. ", "page_idx": 13}, {"type": "text", "text": "D Equivalence of Regret Gap and Value Gap in Single-Agent IL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For single-agent $\\mathrm{IL}$ we prove that the regret gap and the value gap are equivalent. ", "page_idx": 13}, {"type": "text", "text": "Theorem D.1 (Equivalence in Single-Agent IL). For single-agent MDP, regret gap and value gap are equivalent to each other ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\pi_{\\sigma_{E}})-J(\\pi_{\\sigma})=\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. For single-agent MDP, we ignore the index $i$ in the following proof. A strategy deviation in single-agent MDP is equivalent to taking another policy, because there are no other agents affecting the dynamics of the agent. We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)=\\operatorname*{max}_{\\phi\\in\\Phi}(J(\\pi_{\\sigma,\\phi})-J(\\pi_{\\sigma}))=J(\\pi^{*})-J(\\pi_{\\sigma})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\pi^{*}$ is the optimal policy under the true reward function. Similarly, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma_{E})=J(\\pi^{*})-J(\\pi_{\\sigma_{E}})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}({\\boldsymbol\\sigma})-\\mathcal{R}_{\\Phi}({\\boldsymbol\\sigma}_{E})=(J({\\boldsymbol\\pi}^{*})-J({\\boldsymbol\\pi}_{\\sigma}))-(J({\\boldsymbol\\pi}^{*})-J({\\boldsymbol\\pi}_{\\sigma_{E}}))=J({\\boldsymbol\\pi}_{\\sigma_{E}})-J({\\boldsymbol\\pi}_{\\sigma})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In single-agent MDPs, the dynamics are fixed because no other agents affect the agent\u2019s dynamics, and therefore, the regret gap is equivalent to the value gap. ", "page_idx": 13}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Proof of Theorem 4.1 15   \nE.2 Proof of Theorem 4.2 15   \nE.3 Proof of Theorem 4.3 15   \nE.4 Proof of Theorem 4.4 16   \nE.5 Proof of Theorem 4.6 16   \nE.6 Proof of Theorem 4.7 16   \nE.7 Proof of Theorem 5.1 16   \nE.8 Proof of Theorem 5.2 17   \nE.9 Proof of Theorem 5.3 17   \nE.10 Proof of Corollary 5.4 . 18   \nE.11 Proof of Theorem 5.5 19   \nE.12 Proof of Theorem 5.6 19   \nE.13 Proof of Theorem 5.7 20   \nE.14 Proof of Theorem 5.8 20 ", "page_idx": 14}, {"type": "text", "text": "E.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We prove the lemma by showing that the occupancy measures of $\\pi_{\\sigma}$ and $\\pi_{\\sigma_{E}}$ exactly match, i.e. $\\rho^{\\pi_{\\sigma}}(s,\\vec{a})=\\rho^{\\pi_{\\sigma}}\\mathnormal{\\varepsilon}\\left(s,\\vec{a}\\right)$ for every $(s,\\vec{a})$ . Consider a cooperative reward function $\\bar{f}_{s^{\\prime},\\vec{a}^{\\prime}}=-{\\bf1}(s=$ $s^{\\prime},\\vec{a}=\\dot{\\vec{a}}^{\\prime}$ ). ", "page_idx": 14}, {"type": "text", "text": "Under $f_{s,\\vec{a}}$ , we have $J(\\pi_{\\sigma})\\;=\\;-H\\rho^{\\pi_{\\sigma}}(s,\\vec{a}),J(\\pi_{\\sigma_{E}})\\;=\\;-H\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})$ . The maximum value performance the expert/learner can get after deviation is $0$ because the reward function is non-positive. (0 can be achieved by simply not taking $\\vec{a}$ on $s$ ). ", "page_idx": 14}, {"type": "text", "text": "Therefore $\\mathcal{R}_{\\Phi}(\\sigma)\\;=\\;0\\,-\\,\\left(-H\\rho^{\\pi_{\\sigma}}(s,\\vec{a})\\right)\\;=\\;H\\rho^{\\pi_{\\sigma}}(s,\\vec{a}),\\;\\mathcal{R}_{\\Phi}(\\sigma_{E})\\;=\\;0\\,-\\,\\left(-H\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})\\right)\\;=\\;0.$ $H\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})$ . ", "page_idx": 14}, {"type": "text", "text": "Since $\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=0$ , we know that $\\rho^{\\pi_{\\sigma}}(s,\\vec{a})=\\rho^{\\pi_{\\sigma}}\\varepsilon\\left(s,\\vec{a}\\right)$ . This implies that the occupancy measures of two policies exactly match. As a result, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}},f)-J_{i}(\\pi_{\\sigma},f))=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "E.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We can construct an example in normal form games, in which there are mulitple CEs with different pay-offs. We can let the $\\sigma_{E}$ plays CE 1 and $\\sigma$ plays CE 2. Therefore, although the regret gap $\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=0$ , the value gap $\\mathrm{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))\\neq0$ . The NFG in Figure 5 is an example, where $(a_{1},a_{1})$ and $(a_{2},a_{2})$ are two CEs with different values. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "E.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We prove the theorem by constructing such a Markov Game and policies that can get $\\Omega(H)$ regret gap. For simplicity, we construct a two-player cooperative game where the reward is identical for all agents. Agents can not visit the same state at different time steps. These allow us to omit the index $i$ in the reward function in the proof. The notation $a_{i}a_{j}$ is used to represent the action pair $(a_{i},a_{j})$ . ", "page_idx": 14}, {"type": "text", "text": "The transition dynamics are illustrated in Figure 2, and the rewards are action free. The reward function $r(s_{3})\\stackrel{.}{=}r(s_{5})=\\ldots=r(s_{2H-3})=\\stackrel{.}{1}$ , with all other states yielding a reward of 0. Each agent has an action space $\\mathcal{A}_{i}=\\{a_{1},a_{2},a_{3}\\}$ . ", "page_idx": 15}, {"type": "text", "text": "The expert policy $\\sigma_{E}$ satisfies $\\sigma_{E}(a_{1}a_{1}|s_{0})=1.\\sigma_{E}(a_{3}a_{3}|s_{1})=1$ . Action on all other states don\u2019t matter because the transition and the reward would be the same. The trained policy $\\sigma$ satisfies $\\sigma(a_{1}a_{1}|s_{0})=1,\\sigma(a_{1}a_{1}|s_{1})=1$ , and plays the same as the expert in all other states. ", "page_idx": 15}, {"type": "text", "text": "It is not hard to verify that $\\sigma_{E}$ plays a CE under this reward function, which means ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma_{E})=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The worst deviation for $\\sigma$ is to deviate action of agent 1 from playing $a_{1}$ to $a_{2}$ on both $s_{0}$ and $s_{1}$ . We get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)=H-2\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the regret gap $\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=H-2=\\Omega(H)$ ", "page_idx": 15}, {"type": "text", "text": "E.4 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. From the definition of CE, we know $\\mathcal{R}_{\\Phi}(\\sigma_{E})\\leq\\delta_{1}$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)=\\mathcal{R}_{\\Phi}(\\sigma_{E})+\\big(\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\big)\\leq\\delta_{1}+\\delta_{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we know that $\\sigma$ induces a $\\delta_{1}+\\delta_{2}$ -approximate CE. ", "page_idx": 15}, {"type": "text", "text": "E.5 Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. For any $i$ , we can view multi-agent problem as a single agent MDP over the joint action space under reward function $r_{i}$ . Following the proof in Ross and Bagnell [17], Swamy et al. [21], we can prove $J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\le{\\cal O}(\\epsilon H^{2})$ . Therefore, $\\begin{array}{r}{\\operatorname*{max}_{i\\in[m]}(J_{i}(\\bar{\\pi}_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))\\stackrel{!}{\\leq}O(\\epsilon H^{2})}\\end{array}$ . \u518f\u53e3 ", "page_idx": 15}, {"type": "text", "text": "E.6 Proof of Theorem 4.7 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. For any $i$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\leq\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}_{\\xi\\sim\\pi_{\\sigma_{E}}}\\left[\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})\\right]-\\mathbb{E}_{\\xi\\sim\\pi_{\\sigma}}\\left[\\sum_{h=1}^{H}f(s_{h},\\vec{a}_{h})\\right]\\leq\\epsilon H\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\operatorname*{max}_{i\\in[m]}(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma}))\\leq O(\\epsilon H).}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "E.7 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. With Assumption 5.2, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim d^{\\pi\\sigma}}[\\ell(\\sigma_{E}(s),\\sigma(s))]\\le\\frac{1}{\\beta}\\mathbb{E}_{s\\sim d^{\\pi\\sigma_{E}}}[\\ell(\\sigma_{E}(s),\\sigma(s))]\\le\\frac{\\epsilon}{\\beta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Lemma C.1, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\le{\\cal O}\\left(\\frac{1}{\\beta}\\epsilon u H\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For any deviation $\\phi_{i}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{\\tilde{C}}_{s\\sim d^{\\pi},\\phi_{i}}\\left[\\mathsf{T V}(\\pi_{\\sigma_{E},\\phi_{i}}(s),\\pi_{\\sigma,\\phi_{i}}(s))\\right]\\le\\mathbb{E}_{s\\sim d^{\\pi},\\phi_{i}}\\left[\\mathsf{T V}(\\pi_{\\sigma_{E}}(s),\\pi_{\\sigma}(s))\\right]\\le\\frac{1}{\\beta}\\mathbb{E}_{s\\sim d^{\\pi_{E}}}\\left[\\mathsf{T V}(\\sigma_{E}(s),\\sigma(s))\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Lemma C.1, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\leq O\\left(\\frac{1}{\\beta}\\epsilon u H\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})=\\left(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\right)+\\left(J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\right)-J_{i}(\\pi_{\\sigma_{E}}))+\\left(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\right)}\\\\ &{\\phantom{I_{i}(\\pi_{\\sigma,\\phi_{i}})=}\\leq J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E}})+O\\left(\\frac{1}{\\beta}\\epsilon u H\\right)}\\\\ &{\\phantom{I_{i}(\\pi_{\\sigma,\\phi_{i}})=}\\leq J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E}})+O\\left(\\frac{1}{\\beta}\\epsilon u H\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "Qk3IBHyv6z/tmp/86b0c1abeda5b6b0855307e05796d56adb136d0e3b2d3a578a6e32099d0acd02.jpg", "img_caption": ["Figure 3: Example of $\\Omega\\bigl({\\textstyle{\\frac{1}{\\beta}}}\\epsilon u H\\bigr)$ regret gap for J-BC and J-IRL "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Taking the maximum over $i$ , $\\phi_{i}$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le{O}\\left(\\frac{1}{\\beta}\\epsilon u H\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E.8 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We prove the theorem by constructing such a Markov Game policies that can get $\\Omega\\bigl({\\textstyle{\\frac{1}{\\beta}}}\\epsilon u H\\bigr)$ regret gap. We consider the two-player cooperative game similar to the example in Theorem 4.3. What we need to do is to slightly modify the MG and the policy to satisfy Assumption 5.1 and Assumption 5.2. The rewards are action free. Let $u^{\\prime}=\\lfloor u\\rfloor$ , the reward function $r(s_{3})=r(s_{5})=$ $\\dots=r{\\bar{(}}s_{2u^{\\prime}-3})=1$ , with all other states yielding a reward of 0. The transition of the MG is shown in Figure 3. We know that the value is between $[0,u^{\\prime}]$ for any policy, which means Assumption 5.1 is satisfied. ", "page_idx": 16}, {"type": "text", "text": "Let $\\sigma_{E}$ be the policy that $\\begin{array}{r c l c r}{{\\sigma_{E}(a_{1}a_{1}|s_{0})}}&{{=}}&{{1\\,\\,-\\,\\,2\\beta,\\sigma_{E}(a_{2}a_{1}|s_{0})}}&{{=}}&{{2\\beta,\\sigma_{E}(a_{2}a_{1}|s1)}}&{{=}}\\end{array}$ $\\begin{array}{r}{\\frac{1}{2},\\sigma_{E}(a_{3}a_{3}|s_{1})\\stackrel{!}{=}\\frac{1}{2}}\\end{array}$ . Action at all other states doesn\u2019t matter because the transition and the reward would be the same. $\\sigma_{E}$ satisfies Assumption 5.2. ", "page_idx": 16}, {"type": "text", "text": "Let trained policy $\\sigma$ be the policy that $\\sigma(a_{1}a_{1}|s_{0})\\,=\\,1\\,-\\,2\\beta,\\sigma(a_{2}a_{1}|s_{0})\\,=\\,2\\beta,\\sigma(a_{2}a_{1}|s1)\\,=$ $\\begin{array}{r}{\\frac{1}{2},\\sigma(a_{1}a_{1}|s1)=\\frac{\\epsilon H}{2\\beta},\\sigma(a_{3}a_{3}|s_{1})=\\frac{1}{2}-\\frac{\\epsilon H}{2\\beta}}\\end{array}$ . $\\sigma$ and $\\sigma_{E}$ only differs at $s_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "Behavior cloning error of $\\sigma$ satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}\\left[\\ell_{\\mathsf{T V}}(\\sigma_{E}(s),\\sigma(s))\\right]\\leq2\\beta\\cdot\\frac{\\epsilon H}{2\\beta}\\cdot\\frac{1}{H}=\\epsilon\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is not hard to verify, the worst deviation for $\\pi_{\\sigma_{E}}$ is to deviate action of agent 1 at $s_{0}$ from playing $a_{1}$ to $a_{2}$ , and thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\frac{1}{2}(1-2\\beta)(u^{\\prime}-2)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The worst deviation of $\\pi_{\\sigma}$ is to deviate action of agent 1 from playing $a_{1}$ to $a_{2}$ at $s_{0}$ and $s_{1}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)=\\frac{1}{2}(1-2\\beta)(u^{\\prime}-2)+\\frac{\\epsilon H}{2\\beta}(u^{\\prime}-2)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the regret gap $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\frac{\\epsilon H}{2\\beta}(u^{\\prime}-2)=\\Omega(\\frac{1}{\\beta}\\epsilon u H).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "E.9 Proof of Theorem 5.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We prove it by showing that under complete reward function class $\\mathcal{F}$ , low IRL error will imply low BC error, and then apply Theorem 5.1. ", "page_idx": 16}, {"type": "text", "text": "When $\\mathcal{F}=[-1,1]^{|\\mathcal{S}||\\mathcal{A}|}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\mathbb{E}_{\\pi_{\\sigma_{E}}}\\left[\\frac{\\sum_{h=1}^{H}f\\left(s_{h},\\vec{a}_{h}\\right)}{H}\\right]-\\mathbb{E}_{\\pi_{\\sigma}}\\left[\\frac{\\sum_{h=1}^{H}f\\left(s_{h},\\vec{a}_{h}\\right)}{H}\\right]}\\\\ &{=\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{s,\\vec{a}}[\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})-\\rho^{\\pi_{\\sigma}}(s,\\vec{a})]f(s,\\vec{a})}\\\\ &{=\\displaystyle\\sum_{s,\\vec{a}}|\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})-\\rho^{\\pi_{\\sigma}}(s,\\vec{a})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have $\\begin{array}{r}{\\sum_{s,\\vec{a}}|\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})-\\rho^{\\pi_{\\sigma}}(s,\\vec{a})|\\leq\\epsilon.}\\end{array}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{=}\\sum_{k,l}|\\rho^{\\nu_{R}}\\varepsilon(s,\\vec{u})-\\rho^{\\nu_{R}}(s,\\vec{u})|}\\\\ &{=\\sum_{k,l}|\\vec{u}^{\\prime\\prime}\\varepsilon(s)\\gamma(\\vec{u})\\varepsilon(\\vec{u})-d^{\\nu_{P}}(s)|}\\\\ &{=\\sum_{k,l}|\\vec{u}^{\\prime\\prime}\\varepsilon(s)\\gamma(\\vec{u})\\varepsilon(\\vec{u})|-d^{\\nu_{P}}(s)|\\sigma(\\vec{u})|}\\\\ &{=\\sum_{k,l}|\\vec{u}^{\\prime\\prime}\\varepsilon(s)\\gamma(\\vec{u})\\varepsilon(\\vec{u})|-d^{\\nu_{P}}\\varepsilon(s)|\\sigma(\\vec{u})+d^{\\nu_{P}}\\varepsilon(s)|\\sigma(\\vec{u})-d^{\\nu_{P}}(s)|\\sigma(\\vec{u})|}\\\\ &{\\ge\\sum_{k,l}|(|\\vec{u}^{\\prime\\prime}\\varepsilon(s)\\gamma(\\vec{u})\\varepsilon(\\vec{u})|-d^{\\nu_{P}}\\varepsilon(s)|\\sigma(\\vec{u})|\\varepsilon||)-|d^{\\nu_{P}}\\varepsilon(s)|\\sigma(\\vec{u})|-d^{\\nu_{P}}(s)|\\sigma(\\vec{u})|\\varepsilon||}\\\\ &{\\ge\\sum_{k,l}|(|\\vec{u}^{\\prime\\prime}\\varepsilon(s)\\gamma(\\vec{u})\\varepsilon(\\vec{u})|)-\\sum_{l}|d^{\\nu_{P}}\\varepsilon(s)-d^{\\nu_{P}}(s)|}\\\\ &{=\\sum_{\\vec{u}^{\\prime}\\to\\vec{u}^{\\prime}\\varepsilon}|\\nabla(\\sigma(\\vec{u},\\vec{u}),\\sigma(s))|-\\sum_{l}\\left|\\sum_{k}|\\rho^{\\nu_{P}\\varepsilon}\\varepsilon(s,\\vec{u})-\\rho^{\\nu_{P}}(s,\\vec{u})|\\right|}\\\\ &{\\ge\\sum_{k,l}a^{\\nu_{P}}\\varepsilon\\left|\\nabla(\\sigma(\\vec{u},\\vec{u}),\\sigma(s))-\\sum_{l}|\\rho^{\\nu_{P}\\varepsilon}(s,\\vec{u})-\\rho^{\\nu_{P}}(s,\\vec{u})|\\right|}\\\\ &{\\ge\\sum_{k,l}a^{\\nu_{P}}\\varepsilon\\left|\\nabla(\\sigma(\\vec{u},\\vec{u}),\\sigma(s))\\right|-\\sum_{l}|\\rho^{\\nu_{P}}\\varepsilon(s,\\vec{u})-\\rho^{\\nu_{P}}(s\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim d^{\\pi\\sigma_{E}}}[\\mathsf{T V}(\\sigma_{E}(s),\\sigma(s))]\\le2\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Directly applying Theorem 5.1, we get $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O\\left(\\frac{1}{\\beta}\\epsilon u H\\right).}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "E.10 Proof of Corollary 5.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Consider the same example in proof of Theorem 5.2 with parameter $\\epsilon^{\\prime}$ . In the example, the only difference between the occupancy measures of two policies are $\\rho(s,{\\vec{a}})$ at state $s_{1}$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}_{\\pi_{\\sigma_{E}}}\\left[\\frac{\\sum_{h=1}^{H}f\\left(s_{h},\\vec{a}_{h}\\right)}{H}\\right]-\\mathbb{E}_{\\pi_{\\sigma}}\\left[\\frac{\\sum_{h=1}^{H}f\\left(s_{h},\\vec{a}_{h}\\right)}{H}\\right]}\\\\ &{\\displaystyle=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{s,\\vec{a}}[\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})-\\rho^{\\pi_{\\sigma}}(s,\\vec{a})]f(s,\\vec{a})}\\\\ &{\\displaystyle\\le\\sum_{s,\\vec{a}}|\\rho^{\\pi_{\\sigma_{E}}}(s,\\vec{a})-\\rho^{\\pi_{\\sigma}}(s,\\vec{a})|}\\\\ &{\\displaystyle\\le|\\rho^{\\pi_{\\sigma_{E}}}(s_{1},a_{3}a_{3})-\\rho^{\\pi_{\\sigma}}(s_{1},a_{3}a_{3})|+|\\rho^{\\pi_{\\sigma_{E}}}(s_{1},a_{1}a_{1})-\\rho^{\\pi_{\\sigma}}(s_{1},a_{1}a_{1})|}\\\\ &{\\displaystyle=\\frac{1}{H}\\left(2\\beta\\cdot\\frac{\\epsilon^{\\prime}H}{2\\beta}\\cdot2\\right)=2\\epsilon^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\begin{array}{r}{\\epsilon^{\\prime}=\\frac{1}{2}\\epsilon}\\end{array}$ . Then the regret gap $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\frac{\\epsilon H}{4\\beta}(u^{\\prime}-2)=\\Omega(\\frac{1}{\\beta}\\epsilon u H).}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "E.11 Proof of Theorem 5.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. From the definition of $\\ell_{\\mathrm{MALICE}}$ , we know ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\mathrm{MALICE}}(\\sigma,D_{E},\\sigma)=\\underset{i\\in[m]}{\\operatorname*{max}}\\operatorname*{max}_{\\phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}\\left[\\frac{d^{\\pi_{\\sigma,\\phi_{i}}}}{d^{\\pi_{\\sigma_{E}}}}\\ell(\\pi_{E}(s),\\pi(s))\\right]}\\\\ &{\\phantom{\\sum_{\\substack{\\ell_{\\mathrm{MALICE}}\\left(\\sigma,D_{E},\\sigma\\right)=\\pi_{\\sigma_{i}}}}}\\ge\\underset{i\\in[m]}{\\operatorname*{max}}\\operatorname*{max}_{\\phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma_{E}}}}\\left[\\frac{d^{\\pi_{\\sigma,\\phi_{i}}}}{d^{\\pi_{\\sigma_{E}}}}\\ell(\\pi_{E\\phi_{i}}(s),\\pi_{\\phi_{i}}(s))\\right]}\\\\ &{\\phantom{\\sum_{\\substack{\\ell_{\\mathrm{MALICE}}\\left(\\sigma,D_{E},\\sigma\\right)=\\pi_{\\sigma_{i}}}}}\\ge\\underset{i\\in[m]}{\\operatorname*{max}}\\operatorname*{max}_{\\phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma,\\phi_{i}}}}\\left[\\ell(\\pi_{E\\phi_{i}}(s),\\pi_{\\phi_{i}}(s))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From Lemma C.1, we know that for all $i,\\phi_{i}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\leq O(\\epsilon u H)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\le{\\cal O}(\\epsilon u H)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{I_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})=\\left(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\right)+\\left(J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\right)-J_{i}(\\pi_{\\sigma_{E}}))+\\left(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\right)}}\\\\ &{}&{\\leq J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E}})+O\\left(\\epsilon u H\\right)\\quad}&{,...}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking the maximum over $i,\\phi_{i}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O\\left(u\\epsilon H\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "E.12 Proof of Theorem 5.6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We prove the theorem by constructing such a Markov Game policies that MALICE can get $\\Omega(\\epsilon u H)$ regret gap. We consider a single-agent MDP shown in Figure 4. The rewards are action free. Let $u^{\\prime}=\\lfloor u\\rfloor$ , the reward function $r(s_{1})=r(s_{3})=...=r(s_{2u^{\\prime}-3})=1.$ , with all other states yielding a reward of 0. The transition of the MDP is shown in Figure 4. We know that the value is between $[0,u^{\\prime}]$ for any policy, and thus Assumption 5.1 is satisfied. ", "page_idx": 18}, {"type": "text", "text": "Let $\\sigma_{E}$ be the policy that $\\sigma_{E}(a_{1}|s_{0})=1-\\beta,\\sigma_{E}(a_{2}|s_{0})=\\beta$ . Action at all other states doesn\u2019t matter because the transition and the reward would be the same. It is easy to verify that $\\sigma_{E}$ satisfies Assumption 5.2. ", "page_idx": 18}, {"type": "text", "text": "Let trained policy $\\sigma$ be the policy that $\\sigma(a_{1}|s_{0})=1-\\beta-H\\epsilon,\\sigma(a_{2}|s_{0})=\\beta+H\\epsilon$ . $\\sigma$ and $\\sigma_{E}$ only differ at s0. ", "page_idx": 18}, {"type": "text", "text": "Now we verify that $\\ell_{\\mathsf{T V,M A L I C E}}(\\sigma,D_{E},\\sigma)\\le\\epsilon$ . ", "page_idx": 18}, {"type": "text", "text": "Since $\\sigma$ and $\\sigma_{E}$ only differ at state $s_{0}$ , and $d^{\\pi_{\\sigma,\\phi_{i}}}(s_{0})=1$ for any $i,\\phi_{i}$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim d^{\\pi_{\\mathcal{E}}}}\\left[\\frac{d^{\\pi_{\\mathcal{O},\\phi_{i}}}}{d^{\\pi_{\\mathcal{O}_{E}}}}\\mathsf{T V}(\\sigma_{E}(s),\\sigma(s))\\right]=\\mathbb{E}_{s\\sim d^{\\pi_{\\mathcal{O},\\phi_{i}}}}\\left[\\mathsf{T V}(\\sigma_{E}(s),\\sigma(s))\\right]\\le\\frac{1}{H}\\cdot H\\epsilon=\\epsilon\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ell_{\\mathsf{T V,M A L I C E}}(\\sigma,D_{E},\\sigma)=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma}}E}\\left[\\frac{d^{\\pi_{\\sigma,\\phi_{i}}}}{d^{\\pi_{\\sigma_{E}}}}\\mathsf{T V}(\\sigma_{E}(s),\\sigma(s))\\right]\\le\\epsilon\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is not hard to verify, the worst deviation for $\\pi_{E}$ is to deviate action on $s_{0}$ from playing $a_{2}$ to $a_{1}$ , and thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\pi_{E},r)=\\beta(u^{\\prime}-1)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the worst deviation for $\\pi_{E}$ is also to deviate action on $s_{0}$ from playing $a_{2}$ to $a_{1}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\pi,r)=(\\beta+\\epsilon H)(u^{\\prime}-1)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the regret gap $\\mathcal{R}_{\\Phi}(\\pi)-\\mathcal{R}_{\\Phi}(\\pi_{E})=\\epsilon(u^{\\prime}-1)H=\\Omega(\\epsilon u H).$ ", "page_idx": 18}, {"type": "image", "img_path": "Qk3IBHyv6z/tmp/b3a1b4e62cf0677a4bcd2399f2626cccffb47b14c1af52811db3804f6b95003c.jpg", "img_caption": ["Figure 4: Example of $\\Omega(\\epsilon u H)$ regret gap for MALICE and BLADES "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.13 Proof of Theorem 5.7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. From the definition of $\\ell_{\\mathrm{BLADES}}$ , we know ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\mathrm{BLADES}}(\\sigma,\\sigma)=\\underset{i\\in[m]}{\\operatorname*{max}}\\,\\underset{\\phi_{i}}{\\operatorname*{max}}\\,\\mathbb{E}_{s\\sim d^{\\pi}\\sigma,\\phi_{i}}\\,\\left[\\ell(\\sigma_{E}(s),\\sigma_{(}s))\\right]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\underset{i\\in[m]}{\\operatorname*{max}}\\,\\underset{\\phi_{i}}{\\operatorname*{max}}\\,\\mathbb{E}_{s\\sim d^{\\pi}\\sigma,\\phi_{i}}\\,\\left[\\ell(\\pi_{\\sigma_{E},\\phi_{i}}(s),\\pi_{\\sigma,\\phi_{i}}(s))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Lemma C.1, we know that for all $i,\\phi_{i}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\leq O(\\epsilon u H)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And ", "page_idx": 19}, {"type": "equation", "text": "$$\nJ_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\le{\\cal O}(\\epsilon u H)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma})=\\left(J_{i}(\\pi_{\\sigma,\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\right)+\\left(J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})\\right)-J_{i}(\\pi_{\\sigma_{E}}))+\\left(J_{i}(\\pi_{\\sigma_{E}})-J_{i}(\\pi_{\\sigma})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq J_{i}(\\pi_{\\sigma_{E},\\phi_{i}})-J_{i}(\\pi_{\\sigma_{E}})+O\\left(\\epsilon u H\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking the maximum over $i,\\phi_{i}$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})\\le O\\left(\\epsilon u H\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "E.14 Proof of Theorem 5.8 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let MDP, expert policy $\\sigma_{E}$ and the trained policy $\\sigma$ be the same example in the proof of Theorem 5.6. ", "page_idx": 19}, {"type": "text", "text": "Since $\\sigma$ and $\\sigma_{E}$ only differ at state $s_{0}$ , and $d^{\\pi_{\\sigma,\\phi_{i}}}(s_{0})=1$ for any $i,\\phi_{i}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim d^{\\pi}\\sigma,\\phi_{i}}\\left[{\\sf T V}(\\sigma_{E}(s),\\sigma(s))\\right]\\le\\frac{1}{H}\\cdot H\\epsilon=\\epsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the trained policy $\\pi$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell_{\\mathsf{T V,B L A D E S}}(\\sigma,\\sigma)=\\operatorname*{max}_{i\\in[m]}\\operatorname*{max}_{\\phi_{i}}\\mathbb{E}_{s\\sim d^{\\pi_{\\sigma,\\phi_{i}}}}\\left[{\\mathsf{T V}}(\\sigma_{E}(s),\\sigma(s))\\right]\\le\\epsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The regret gap $\\begin{array}{r}{\\mathcal{R}_{\\Phi}(\\sigma)-\\mathcal{R}_{\\Phi}(\\sigma_{E})=\\epsilon(u^{\\prime}-1)H=\\Omega(\\epsilon u H).}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "F Comparison with Goktas et al. [8] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recent work Goktas et al. [8] worked on similar problem as ours. We will highlight some of the difference between two works. ", "page_idx": 19}, {"type": "text", "text": "First, the learning goals are different. They focus on a problem of inverse game theory, where the goal is to recover a reward function to rationalize the expert\u2019s behavior, i.e. the expert policy plays an equilibrium under such a reward function. However, in our setting, instead of recovering a singe reward function, our goal is to learn a robust policy that get similar regret performance under a class of reward functions. We will show later that if the ultimate goal is to learn this robust policy, simply recovering a single reward function is not enough. ", "page_idx": 19}, {"type": "table", "img_path": "Qk3IBHyv6z/tmp/22b10c7a214587d4a2525db6c6e16220ec7cc8a6febee3977c3961c36dd98a65.jpg", "table_caption": ["Figure 5: Multiple reward functions rationalize $\\sigma_{E}$ "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Second, the solution concepts are different. they work on Nash equilibrium, while in our setting, we focus on correlated equilibrium. We note that our algorithms also work for learning independent policies, by restricting the policy class to be a class of independent policies. ", "page_idx": 20}, {"type": "text", "text": "Third, in finite demonstration setting, their objective is to find a reward function which the learned policy plays a local NE, under the constraints that $\\ell_{2}$ difference of the observations for behaviors of two learned policy is small. We note that in general simply matching this difference is not enough to guarantee that the learned policy play an equilibrium. From Theorem 4.3, we know that even if the occupancy measures of two policies exactly match, the regrets can still be significantly different under the same reward function. ", "page_idx": 20}, {"type": "text", "text": "In conclusion, they work on a inverse game theory style problem where the goal is to recover a single reward function to rationalize the agents behavior. We work on imitation learning problem, where the goal is not recovering a single reward function but learning a policy that matches the regret performance of the expert under a class of reward functions. ", "page_idx": 20}, {"type": "text", "text": "We will give examples in normal form games (NFG) to show that recovering a single reward function is not enough to learn a policy that minimizing the regret gap for a large class of reward functions. NFG can be viewed as an MG in which $H=1$ and $|{\\cal S}|=1$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma F.1. For an expert policy $\\sigma_{E}$ , there may exist multiple reward functions that rationalize it. ", "page_idx": 20}, {"type": "text", "text": "Proof. We show this by an example of normal form games in Figure 5. Consider the policy to be $\\sigma_{E}(\\dot{a}_{1}a_{1})=1$ , then the expert plays CE/NE under both reward functions $r$ and $r^{\\prime}$ , which means both reward functions rationalize $\\sigma_{E}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma F.2. For a fixed reward function, There may exist multiple CE/NEs. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. For reward function $r$ in Figure 5, we can construct such two policies $\\sigma_{1},\\sigma_{2}$ . For $\\sigma_{1}$ , let $\\sigma_{1}(\\dot{a_{1}},a_{1})=1$ . Let $\\begin{array}{r}{\\sigma_{2}(a_{1},a_{1})=\\frac{\\check{4}}{9},\\sigma_{2}(a_{1},a_{2})=\\sigma_{2}(a_{2},a_{1})=\\frac{2}{9},\\sigma_{2}(\\dot{a}_{2},a_{2})=\\frac{\\check{1}}{9}}\\end{array}$ . Tt is not hard to verify that both $\\sigma_{1}$ and $\\sigma_{2}$ play CE/NE under the reward function $r$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Therefore, since there is no one-to-one mapping between the equilibria and the pay-off structures, simply recovering a single reward function might not help recover a policy that gets small regret gap. ", "page_idx": 20}, {"type": "text", "text": "For example, the true reward function is $r$ in Figure 5, and expert policy $\\sigma_{E}$ satisfies $\\sigma_{E}(a_{1},a_{1})=1$ The algorithm may recover $r^{\\prime}$ in Figure 5, and a trained policy $\\sigma$ that plays NE/CE under recovered reward function $r^{\\prime}$ would be $\\begin{array}{r}{\\sigma(a_{1},\\dot{a}_{1})=\\sigma(a_{1},a_{2})=\\dot{\\sigma}(a_{2},a_{1})=\\dot{\\sigma}(\\dot{a_{2}},a_{2})=\\frac{1}{4}}\\end{array}$ . However, this trained policy $\\sigma$ does not play NE/CE under the true reward function $r$ . ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the relationship between the two objectives, and list our contributions in the introduction. We include the assumptions when introducing the algorithms. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We include limitations in ??, where we discuss the computational and sample complexity of the algorithms, the assumptions made, and the fact that the algorithms are designed for tabular MGs and have not yet been implemented as practical algorithms. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The assumptions are provided in Section 5 and full proofs are in Appendix E.6. We do not include proof sketches in the main paper. However, we provide links between the theorem statements and their corresponding proofs for the readers\u2019 convenience. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This is a theory paper. We do not include experiments for the algorithms. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a theory paper. We do not include experiments for the algorithms. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a theory paper. We do not include experiments for the algorithms. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a theory paper. We do not include experiments for the algorithms. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This is a theory paper. We do not include experiments for the algorithms. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conform to the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss broader impacts in Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper is a theory paper and does not pose such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a theory paper and it does not use existing assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]