[{"figure_path": "Xa3dVaolKo/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Isomorphic nodes result in identical MPNN node representation, making it impossible to distinguish links such as (v\u2081, v\u2083) and (v\u2081, v\u2085) based on these representations. (b) MPNN counts Common Neighbor through the inner product of neighboring nodes' one-hot representation.", "description": "This figure illustrates the limitations of Message Passing Neural Networks (MPNNs) in link prediction.  Panel (a) shows that isomorphic nodes (nodes with the same local structure) get identical representations in MPNNs, making it difficult to distinguish between links that connect to similar nodes (e.g., (v\u2081, v\u2083) and (v\u2081, v\u2085)).  Panel (b) demonstrates how MPNNs attempt to count common neighbors (CN) by using the inner product of one-hot encoded node vectors; however, this method is limited compared to the efficiency of dedicated heuristics.", "section": "1 Introduction"}, {"figure_path": "Xa3dVaolKo/figures/figures_2_1.jpg", "caption": "Figure 2: GNNs estimate CN, AA and RA via MSE regression, using the mean value as a Baseline. Lower values are better.", "description": "This figure shows the results of an experiment where Graph Convolutional Networks (GCNs) were trained to estimate three different link prediction heuristics: Common Neighbors (CN), Adamic-Adar (AA), and Resource Allocation (RA). The mean squared error (MSE) between the GCN's predictions and the true heuristic values is shown for eight different datasets. A baseline is also shown, which represents the MSE obtained by simply using the mean heuristic value across all links in the training set. The lower the MSE, the better the GCN is able to estimate the corresponding heuristic.", "section": "3 Can Message Passing count Common Neighbor?"}, {"figure_path": "Xa3dVaolKo/figures/figures_5_1.jpg", "caption": "Figure 3: Representation of the target link (u, v) within our model (MPLP), with nodes color-coded based on their distance from the target link.", "description": "This figure illustrates the input to the Message Passing Link Predictor (MPLP) model.  The nodes are color-coded according to their shortest path distance from the target link (u,v).  The model uses this information to learn joint structural features of the link.", "section": "4 Method"}, {"figure_path": "Xa3dVaolKo/figures/figures_8_1.jpg", "caption": "Figure 4: Evaluation of inference time on large-scale OGB datasets. The inference time encompasses the entire cycle within a full-batch inference.", "description": "This figure evaluates the inference time of various link prediction models on three large OGB datasets (Collab, PPA, and Citation2).  The x-axis represents the inference time (in seconds), while the y-axis shows the performance metric (Hits@50 for Collab, Hits@100 for PPA, and MRR for Citation2).  Each point represents a different model, illustrating the trade-off between inference speed and predictive accuracy.  The figure demonstrates that MPLP and MPLP+ achieve state-of-the-art performance while maintaining relatively fast inference times, especially when compared to other methods such as SEAL and Neo-GNN.", "section": "5 Experiments"}, {"figure_path": "Xa3dVaolKo/figures/figures_9_1.jpg", "caption": "Figure 5: MSE of estimation for #(1,1), #(1,2) and #(1,0) on Collab. Lower values are better.", "description": "This figure displays the mean squared error (MSE) of the estimations for three different structural features: #(1,1), #(1,2), and #(1,0) on the Collab dataset.  The x-axis represents the signature dimension used in the experiment, while the y-axis shows the MSE.  Lower MSE values indicate better estimation accuracy. The figure compares the performance of ELPH, MPLP without One-hot Hubs, and MPLP with One-hot Hubs, demonstrating how the inclusion of One-hot Hubs improves accuracy and reduces variance.", "section": "Estimation accuracy"}, {"figure_path": "Xa3dVaolKo/figures/figures_20_1.jpg", "caption": "Figure 7: Heatmap illustrating the inner product of node attributes across CS, Photo, and Collab datasets.", "description": "The figure displays heatmaps showing the inner product of node attributes for three datasets: CS, Photo, and Collab.  Each heatmap visualizes the pairwise inner product of node attribute vectors.  The color intensity represents the magnitude of the inner product, with darker shades indicating smaller values and lighter shades indicating larger values. This visualization helps to illustrate the degree of orthogonality (or lack thereof) between the node attribute vectors in each dataset, which is relevant to the paper's exploration of quasi-orthogonal vectors for efficient link prediction.", "section": "E Exploring Bag-Of-Words Node Attributes"}, {"figure_path": "Xa3dVaolKo/figures/figures_20_2.jpg", "caption": "Figure 8: Heatmap illustrating the inner product of node attributes, arranged by node labels, across CS and Photo. The rightmost showcases the inner product of QO vectors.", "description": "This figure presents three heatmaps visualizing the inner product of node attributes for the datasets CS and Photo, along with a heatmap showing the inner product of quasi-orthogonal (QO) vectors.  The heatmaps for CS and Photo are arranged with nodes grouped by their labels, revealing the relationships between nodes with similar characteristics.  The third heatmap illustrates the inner product of randomly generated QO vectors, highlighting their near-orthogonality, which is a key aspect of the proposed method in the paper.", "section": "Exploring Bag-Of-Words Node Attributes"}, {"figure_path": "Xa3dVaolKo/figures/figures_22_1.jpg", "caption": "Figure 9: MSE of estimation for #(2, 2), #(2,0) and estimation time on Collab. Lower values are better.", "description": "This figure shows the mean squared error (MSE) of estimating the number of nodes with shortest path distances of (2,2) and (2,0) from the target node pair in the Collab dataset, as well as the estimation time.  The results are shown for different signature dimensions (the dimensionality of the random vectors used in the model).  Lower MSE values indicate better accuracy in estimating the node counts. The estimation time also increases with the signature dimension. The plot compares the performance of ELPH, MPLP without one-hot hubs, and MPLP.", "section": "Extended ablation studies"}, {"figure_path": "Xa3dVaolKo/figures/figures_23_1.jpg", "caption": "Figure 3: Representation of the target link (u, v) within our model (MPLP), with nodes color-coded based on their distance from the target link.", "description": "This figure illustrates how the Message Passing Link Predictor (MPLP) model represents the target link (u, v).  Nodes are color-coded to show their distance from u and v, allowing the model to capture the joint structural features of the link. The different colors represent the different shortest path distances from u and v to each node. This encoding allows the model to learn sophisticated representations that go beyond simple heuristics like common neighbors.", "section": "4 Method"}]