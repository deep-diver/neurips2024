[{"figure_path": "E3HDagVPNG/figures/figures_2_1.jpg", "caption": "Figure 1: The overall architecture of our proposed method. It includes a multi-scale backbone to extract hierarchical features from the input image, a Siamese Transformer structure named LATR to integrate global semantics information and finer-scale features, and a Curve-IoU loss to supervise the fit of lane lines for model training. Swin level 1-3 denotes the multi-scale backbone using the Swin Transformer. FFN represents a feed-forward network.", "description": "The figure illustrates the overall architecture of the proposed lane detection method.  It shows a multi-scale backbone (using Swin Transformer) extracting hierarchical features from an input image. These features are then fed into a Siamese Transformer structure called LATR, which integrates both global semantic and finer-scale features. LATR processes a lane query to refine keypoints of lane lines. The output of LATR passes through two feed-forward networks (FFNs): one for curve properties (start point, angle, length) and one for offset calculations. Finally, a Curve-IoU loss function supervises the fitting of predicted lane lines to ground truth, ensuring accuracy.", "section": "3 Method"}, {"figure_path": "E3HDagVPNG/figures/figures_3_1.jpg", "caption": "Figure 2: The detailed structure of our proposed LAne TRansformer (LATR). We employ a high-to-low refinement structure in which the input lane query is refined by higher-scale features.", "description": "The figure shows the detailed architecture of the proposed LAne TRansformer (LATR).  It uses a high-to-low refinement structure. The input lane query is processed by a feed-forward network (FFN), and then fed into a deformable attention module.  This module incorporates hierarchical features which refine the lane query.  The refined lane query is further processed by a self-attention mechanism before being outputted.", "section": "3.2 Lane Transformer"}, {"figure_path": "E3HDagVPNG/figures/figures_4_1.jpg", "caption": "Figure 3: (a) a typical failure scenario for LIoU, which cannot measure the distances between Ground Truth and Predictions A and B; (b) the values of LIoU and Curve-IoU.", "description": "Figure 3(a) shows a common limitation of the LIoU loss function where it fails to accurately assess the distance between a ground truth lane line and its predictions when the lane lines have significant curvature. The inability to distinguish between predictions A and B is evident.  Figure 3(b) illustrates the comparative performance of LIoU and Curve-IoU (CIoU) loss functions when considering the L1 distance between the ground truth and predictions. This visualization emphasizes that the CIoU loss function provides more accurate distance representations, particularly for significantly curved lane lines. The CIoU loss function enhances the precision of lane line detection in scenarios with notable curvature.", "section": "3.3 Curve-IoU Loss"}, {"figure_path": "E3HDagVPNG/figures/figures_7_1.jpg", "caption": "Figure 5: Visualization results of Ground Truth (GT), CondLaneNet [17] (CondLane), CLRNet [41], and our method on CULane [26]. The results of CondLaneNet and CLRNet are generated with ResNet18 and ours are generated with Swin Transformer tiny. Different lane lines are represented by different colors. The F1 score for each predicted image is labeled in the top left corner of the image.", "description": "This figure displays a comparison of lane detection results from three different methods: CondLaneNet, CLRNet, and the authors' proposed method.  The results are shown side-by-side with the ground truth for easy comparison. Different colors represent different lane lines. The F1 score, a common metric for evaluating lane detection, is displayed in the upper left corner of each image, allowing for a quantitative assessment of performance across various methods.  The images depict diverse road conditions and lighting scenarios, illustrating how each approach handles challenges.", "section": "4.4.2 Results on CULane"}, {"figure_path": "E3HDagVPNG/figures/figures_7_2.jpg", "caption": "Figure 4: High-to-low attention maps of our proposed LATR.", "description": "This figure shows the attention maps generated by the proposed LAne TRansformer (LATR) at different levels.  The high-to-low hierarchical refinement structure of LATR is visualized.  Higher levels focus on broader contextual information along the lane, while lower levels increasingly concentrate on the precise location of key points along the lane lines. This demonstrates how LATR integrates global semantics with fine-scale features for accurate lane detection.", "section": "3.2 Lane Transformer"}, {"figure_path": "E3HDagVPNG/figures/figures_12_1.jpg", "caption": "Figure 5: Visualization results of Ground Truth (GT), CondLaneNet [17] (CondLane), CLRNet [41], and our method on CULane [26]. The results of CondLaneNet and CLRNet are generated with ResNet18 and ours are generated with Swin Transformer tiny. Different lane lines are represented by different colors. The F1 score for each predicted image is labeled in the top left corner of the image.", "description": "This figure shows a qualitative comparison of lane detection results on the CULane dataset.  It compares the ground truth lane markings (GT) with the predictions from CondLaneNet, CLRNet, and the authors' proposed method. Different colored lines represent different lanes. The F1 score for each image is shown in the top-left corner, providing a quantitative assessment to supplement the visual comparison.", "section": "4.4.2 Results on CULane"}]