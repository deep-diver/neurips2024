[{"figure_path": "GnF9tavqgc/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the idea of physical consistency. To support multiple tasks (\"Task X\" represents a general task), the model (blue solid lines) builds multiple decoders on a shared encoder, which are trained by multi-task learning with data of respective tasks (green dotted double arrows). Physical consistency losses enforce physical laws between tasks (orange dashed double arrows), hence bridge data heterogeneity and directly improve one task from others.", "description": "This figure illustrates the core idea of the paper: using physical consistency to bridge heterogeneous data in molecular multi-task learning.  A shared encoder processes molecular input, feeding into multiple decoders (for different tasks like structure, energy, and a general task X).  Multi-task learning connects these decoders at the input.  Crucially, physical consistency losses are introduced, connecting the decoders at the output by explicitly enforcing physical relationships between predictions of different tasks. This allows information from one task (e.g., high-accuracy energy data) to directly improve the accuracy of another (e.g., structure prediction). The orange dashed arrows represent these consistency losses that enforce physical laws and bridge the data heterogeneity.", "section": "1 Introduction"}, {"figure_path": "GnF9tavqgc/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of energy (eV) on the model-generated structure Rpred using the denoising method and the equilibrium structure Req in the PCQ dataset. Each point represents the model-predicted energy values on the two structures for one test molecule. Models are trained on (left) the PM6 dataset, (middle) the PM6 dataset and SPICE force dataset, and (right) the PM6 dataset with a subset of force labels. The closer a point lies to the diagonal line, the closer the energy of the predicted structure is to the minimum energy, indicating a closer prediction of equilibrium structure.", "description": "This figure compares the energy of the predicted equilibrium structure (Rpred) against the energy of the actual equilibrium structure (Req) for molecules in the PCQ dataset. Each point represents a molecule, with its x-coordinate showing the predicted energy of Req and its y-coordinate showing the predicted energy of Rpred. Three subplots show results from models trained under different conditions: (left) Only using PM6 dataset, (middle) using PM6 dataset and SPICE force dataset, and (right) using PM6 dataset and a subset of force labels. The closer the points are to the diagonal line, the better the model predicts the equilibrium structure.", "section": "4.2 Structure Prediction Results"}, {"figure_path": "GnF9tavqgc/figures/figures_19_1.jpg", "caption": "Figure 2: Comparison of energy (eV) on the model-generated structure Rpred using the denoising method and the equilibrium structure Req in the PCQ dataset. Each point represents the model-predicted energy values on the two structures for one test molecule. Models are trained on (left) the PM6 dataset, (middle) the PM6 dataset and SPICE force dataset, and (right) the PM6 dataset with a subset of force labels. The closer a point lies to the diagonal line, the closer the energy of the predicted structure is to the minimum energy, indicating a closer prediction of equilibrium structure.", "description": "This figure compares the energy of model-generated equilibrium structures (Rpred) with the true equilibrium structures (Req) from the PCQ dataset.  Each point represents a molecule, plotting its predicted energy against its true energy for both Rpred and Req. The plots illustrate the impact of different training data (PM6 only, PM6 + SPICE forces, PM6 + subset of force labels) on the accuracy of predicting equilibrium structures.  Points closer to the diagonal indicate more accurate structure predictions, demonstrating how different training data affect the ability of the model to find the minimum energy structure.", "section": "4.2 Structure Prediction Results"}, {"figure_path": "GnF9tavqgc/figures/figures_21_1.jpg", "caption": "Figure 1: Illustration of the idea of physical consistency. To support multiple tasks (\"Task X\" represents a general task), the model (blue solid lines) builds multiple decoders on a shared encoder, which are trained by multi-task learning with data of respective tasks (green dotted double arrows). Physical consistency losses enforce physical laws between tasks (orange dashed double arrows), hence bridge data heterogeneity and directly improve one task from others.", "description": "The figure illustrates the concept of physical consistency in multi-task learning for molecular properties.  A shared encoder processes molecular input, feeding into multiple decoders for different properties (e.g., energy, structure, Task X).  Multi-task learning connects the decoders at the input, while physical consistency losses connect them at the output, enforcing relationships between predicted properties based on physical laws. This allows information exchange between tasks, improving prediction accuracy, particularly leveraging higher-accuracy data to improve less-accurate predictions.", "section": "1 Introduction"}]