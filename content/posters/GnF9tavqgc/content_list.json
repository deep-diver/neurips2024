[{"type": "text", "text": "Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuxuan Ren\u2217,\u2020 Dihan Zheng\u2217\u2020, Chang Liu,\u2021 Peiran Jin, Yu Shi, Lin Huang, Jiyan He\u2020, Shengjie Luo\u2020, Tao Qin, Tie-Yan Liu ", "page_idx": 0}, {"type": "text", "text": "Microsoft Research AI for Science ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks. To support various molecular properties at scale, machine learning models are trained in the multi-task learning paradigm. Nevertheless, data of different molecular properties are often not aligned: some quantities, e.g. equilibrium structure, demand more cost to compute than others, e.g. energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning. Moreover, it is not straightforward to leverage abundant data of other tasks to benefti a particular task. To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another. Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction. We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The field of machine learning has witnessed a blossom of progress in solving molecular science tasks in recent years, including molecular property prediction [1, 2, 3], machine-learning force field (energy/force prediction) [4, 5, 6, 7, 8], electronic structure [9, 10, 11, 12, 13], molecular structure generation [14, 15, 16, 17] and design [18, 19, 20]. In molecular research, these tasks are often required jointly: for example, energy prediction is required for molecular stability and dynamics, and equilibrium structure (conformation) prediction offers the most probable and characteristic structure for understanding molecule interaction and functions. Multi-task learning is hence adopted, where a model is trained to predict multiple properties using the same number of decoders (output heads) built on a shared encoder (backbone model) (Fig. 1) [21]. This paradigm is also used for pre-training a model by leveraging as much data as possible that are scattered over various tasks and domains [22, 23, 24]. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, science tasks have some unique challenges beyond conventional machine learning tasks, which cannot be adequately addressed by multi-task learning alone. It is more costly to curate a dataset for molecular science tasks since it calls for running physics-theoretic computation algorithms, which come with a stringent accuracy-efficiency trade-off. Molecular-science datasets are hence generated each with a specific algorithmic portfolio that is economic for the particular purpose. ", "page_idx": 0}, {"type": "text", "text": "This incurs two challenges regarding data heterogeneity. (1) Different properties in a dataset may come from algorithms in different levels of theory, meaning different levels of accuracy. This limits the prediction accuracy on some tasks. For example, labeling the energy of a molecular structure is yet affordable for common molecules using algorithms in the density functional theory (DFT) level, but producing the equilibrium structure of a molecule requires repeated energy evaluations hence is tens to hundreds of times more costly. As a result, DFT-level equilibrium structure data are available only in a limited scale [25, 26, 27], while larger-scale datasets [28, 29] have to resort to lower levels of theory to generate equilibrium structures at scale, which come with a lower level of accuracy. Using such data, multi-task learning alone cannot predict structures in an accuracy higher than the data-generation method. (2) Different datasets focus on different tasks, so combining these datasets to enhance the performance on a particular task is not straightforward. For example, there are datasets [30, 31, 32] that are concerned with force prediction and off-equilibrium structures, which do not provide direct supervision to equilibrium structure prediction. Although including these additional tasks in multi-task learning could help learn a better encoder (or, representation), this is only based on an empirical observation from a general machine learning perspective and does not directly exploit relevant physical information in the additional tasks. ", "page_idx": 1}, {"type": "text", "text": "In this work, we highlight that science tasks also provide fortunate specialties that come to the rescue. In contrast to conventional machine learning tasks which are primarily defined by data, science tasks originate in fundamental physical laws, and data are rather the demonstration of such laws. These laws impose explicit constraints between tasks, hence define the \u201cphysical consistency\u201d between model predictions on these tasks. By enforcing such consistency, model predictions for different tasks are connected and can explicitly share the information in the data of one task to the prediction for other tasks, hence bridging data heterogeneity. From another perspective, while sharing a common encoder connects various decoders at the input end, physical consistency closes the loop by connecting the decoders at the output end (Fig. 1). With the additional information from the physical consistency, capabilities beyond conventional multi-task learning are enabled: (1) data from a higher-level of theory of one task can improve the accuracy of a physically related task, and (2) the abundant data of a physically related task can directly improve the performance of the concerned task. ", "page_idx": 1}, {"type": "image", "img_path": "GnF9tavqgc/tmp/1a335b8f066ce723b60171846a403005047330580290cf7e0bd4d796c2bc441e.jpg", "img_caption": ["Figure 1: Illustration of the idea of physical consistency. To support multiple tasks (\u201cTask $X^{\\bullet}$ represents a general task), the model (blue solid lines) builds multiple decoders on a shared encoder, which are trained by multi-task learning with data of respective tasks (green dotted double arrows). Physical consistency losses enforce physical laws between tasks (orange dashed double arrows), hence bridge data heterogeneity and directly improve one task from others. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Concretely, we demonstrate the practical value of the physical consistency between energy prediction and equilibrium structure prediction, two central tasks in molecular science. The consistency can be constructed from two perspectives: the equilibrium structure of a molecule is the structure that attains the minimal energy of the molecule (Sec. 3.2), and the equilibrium structure is a sample of the thermodynamic equilibrium distribution at a low temperature (Sec. 3.3). When adopting the denoising diffusion formulation [33, 34, 35] for structure prediction, we show that the two physical laws can be translated into consistency loss functions which connect the energy-prediction model with the structure-prediction model at large and small diffusion steps, respectively. We apply the consistency losses in the multi-task learning on the PubChemQC B3LYP/6-31G\\*//PM6 dataset [29], which is perhaps the largest public dataset with DFT-level (B3LYP/6-31G\\*) energy labels thus highly relevant to model pre-training. But its equilibrium structures are generated in the semi-empirical level (PM6), which is in a lower level of accuracy. We use the consistency losses to transmit the information of the higher-level theory in the energy model to the structure model by only optimizing parameters of the structure decoder, which achieves the followings. (1) Consistency losses improve structure prediction accuracy beyond multi-task learning alone, when evaluated using DFT-level structures from the PCQM4Mv2 [27] and QM9 [25] datasets. The advantage persists even after both have undergone finetuning. (2) Datasets providing DFT-level force labels on off-equilibrium structures are better leveraged to improve structure prediction accuracy beyond only including force prediction in multi-task learning. Since force is the gradient of the energy, the additional data allow the energy model to learn a better energy landscape, which in turn leads to more accurate structure prediction through the consistency losses. This is a more direct pathway for data on an additional task to benefti structure prediction beyond learning a better representation in multi-task learning. We remark that the improvement is not from any more accurate structure data, but from bridging data heterogeneity using the \u201cfree lunch\u201d redeemed from physical laws. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Structure prediction. In recent years, deep generative models have been used as a powerful tool to generate molecular structures. Due to the subtlety that a structure being rotated as a whole is essentially the same structure, early methods opt to generate intermediate geometric variables, such as inter-atomic distances [36, 37] or torsional angles [38, 39]. Directly generating Cartesian coordinates of atoms has also been explored where the rotational equivalence is handled by alignment [40] or leveraging gradient of distances [41, 42]. More recently, diffusion models have been used to generate torsional angles [43], or atom coordinates [16, 44] using equivariant models. Given the generality and superior performance, we adopt an equivariant diffusion model for structure prediction. While these works may generate multiple meta-stable structures, we aim at the single equilibrium structure for each molecule, and investigate the benefit from an energy model. ", "page_idx": 2}, {"type": "text", "text": "Leveraging physical laws between tasks. A noticeable example is leveraging the connection between energy and thermodynamic equilibrium distribution to compensate for potentially biased data to better learn the distribution. The equilibrium distribution in a canonical ensemble is the Boltzmann distribution, which is directly determined by the energy function. From the machine learning perspective, the energy function provides an unnormalized density function of the target distribution. Using a flow-based model [45], Boltzmann generators [14] inject the energy supervision via the evidence lower bound objective. Bootstrapped $\\alpha$ -divergence objective is thereafter introduced to mitigate mode collapse [46]. Zheng et al. [17] use a diffusion model where the energy supervises the score model at the start diffusion step and is propagated to intermediate steps by enforcing a PDE. Similar techniques are also explored for conventional machine learning tasks [47, 48]. More recently, Bose et al. [49] directly connect the energy to intermediate-step scores. In the opposite direction, Arts et al. [50] leverage Boltzmann-distribution samples to learn a coarse-grained energy model. In parallel with these works, the current work is devoted to the investigation of leveraging the connection between energy and structure prediction, and is not using an oracle energy function considering the cost of DFT calculation. ", "page_idx": 2}, {"type": "text", "text": "2 Technical Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Before delving into details of the consistency between energy and structure prediction, we first introduce the problem formulation, and diffusion-based generative formulation for structure prediction. ", "page_idx": 2}, {"type": "text", "text": "Chemically, a molecule is specified by the types (i.e., chemical elements) of atoms and bonds, jointly forming a molecular graph $\\mathcal{G}$ . A molecule in physical reality can take different structures (conformations) $\\mathbf{R}\\in\\mathbb{R}^{A\\times3}$ , i.e., the collection of 3-dimensional coordinates of its $A$ atoms. Many properties of molecule $\\mathcal{G}$ depend on its specific structure $\\mathbf{R}$ , e.g., the (inter-atomic potential) energy, so energy prediction is in the form $E_{\\mathcal{G}}^{(\\uptheta)}\\bar{(}\\mathbf{R})$ with model parameters \u03b8. ", "page_idx": 2}, {"type": "text", "text": "Among possible structures, the equilibrium structure is the one that attains the minimal energy, and is the most representative structure for the molecule. As mentioned, the diffusion formulation has been a preferred choice for equilibrium structure prediction, which samples from a distribution $p_{\\mathcal{G}}(\\mathbf{R})$ concentrated at the equilibrium structure. For this, a primitive structure is sampled from a simple distribution, e.g., the standard Gaussian, which undergoes a diffusion process that transforms the simple distribution to the desired distribution. This is done by reversing the process in the opposite direction, which is easier to construct. For example, the process can be taken as th\u221ae Langevin diffusion that converges to standard Gaussian [35]: $\\mathrm{d}\\mathbf{R}_{t}=\\mathbf{\\dot{\\beta}}_{t}\\nabla\\log{\\mathcal{N}}(\\mathbf{R}_{t};\\mathbf{0},\\mathbf{I})\\,\\mathrm{d}t+\\sqrt{\\beta_{t}}\\,\\mathrm{d}\\bar{\\mathbf{W}}_{t}=$ $\\begin{array}{r}{-\\frac{\\beta_{t}}{2}\\mathbf{R}_{t}\\,\\mathrm{d}t+\\sqrt{\\beta_{t}}\\,\\mathrm{d}\\mathbf{W}_{t}}\\end{array}$ , where $\\beta_{t}$ is a time dilation factor [51], and $\\mathbf{W}_{t}$ denotes the Wiener process. The process starts from the desired distribution $p g_{,0}\\,=\\,p g$ at $t\\,=\\,0$ and ends after a sufficiently long period $T$ when the distribution converges, $p_{T}=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . The reverse process is known to ", "page_idx": 2}, {"type": "text", "text": "follow [52]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}_{\\bar{t}}=\\frac{\\beta_{T-\\bar{t}}}{2}\\mathbf{R}_{\\bar{t}}\\,\\mathrm{d}\\bar{t}+\\beta_{T-\\bar{t}}\\nabla\\log p_{\\mathcal{G},T-\\bar{t}}(\\mathbf{R}_{\\bar{t}})\\,\\mathrm{d}\\bar{t}+\\sqrt{\\beta_{T-\\bar{t}}}\\,\\mathrm{d}\\mathbf{W}_{\\bar{t}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\bar{t}:=T-t$ , which transforms $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ at $\\bar{t}=0$ to the desired distribution at $\\bar{t}=T$ , hence the generation process is constructed. To simulate the process, the only unknown is the (Fisher\u2019s) score function $\\nabla\\log p_{\\mathcal{G},t}(\\mathbf{R})$ at each diffusion instant, for which a machine-learning model $\\mathbf{s}_{\\mathcal{G},t}^{(\\theta)}(\\mathbf{R})$ is introduced. To learn to fit $\\nabla\\log p_{\\mathcal{G},t}(\\mathbf{R})$ , a practical approach is by optimizing the denoising score matching loss [53]: $\\mathbb{E}_{p_{\\mathcal{G},0}(\\mathbf{R}_{0})}\\mathbb{E}_{p(\\mathbf{R}_{t}|\\mathbf{R}_{0})}\\|\\mathbf{s}_{\\mathcal{G},t}^{(\\mathbf{6})}(\\mathbf{R}_{t})\\!-\\!\\nabla_{\\mathbf{R}_{t}}\\log p(\\mathbf{R}_{t}|\\mathbf{R}_{0})\\|^{2}$ , which is convenient since from the Langevin diffusion, we can derive $p(\\mathbf{R}_{t}|\\mathbf{R}_{0})=\\mathcal{N}(\\mathbf{R}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{R}_{t},(1-\\bar{\\alpha}_{t})\\mathbf{I})$ , where $\\begin{array}{r}{\\bar{\\alpha}_{t}:=\\exp(-\\int_{0}^{t}\\beta_{s}\\,\\mathrm{d}s)}\\end{array}$ , which is a known distribution. Leveraging the reparameterization trick, the loss is reformed as [35]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathrm{Unif}(t;0,T)}\\big(1-\\bar{\\alpha}_{t}\\big)\\mathbb{E}_{p_{Q,0}(\\mathbf{R}_{0})}\\mathbb{E}_{N(\\epsilon_{t};\\mathbf{0},\\mathbf{I})}\\bigg\\|\\mathbf{s}_{\\mathcal{G},t}^{(\\mathbf{0})}\\big(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{R}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{t}\\big)+\\frac{\\epsilon_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\bigg\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation w.r.t $\\mathbb{E}_{p\\mathfrak{g},\\mathfrak{o}(\\mathbf{R}_{0})}$ can be estimated by averaging over data. Once the score model is trained, it can generate structures by replacing $\\nabla\\log p_{\\mathcal{G},t}(\\mathbf{R})$ and simulating Eq. (1). If only $p_{\\mathcal{G},0}(\\mathbf{R}_{0})$ is desired (instead of $p_{\\mathcal{G}}(\\mathbf{R}_{0:T})\\rangle$ , then an equivalent simulation approach can be adopted known as probability-flow ODE [35]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}_{\\bar{t}}=\\frac{\\beta_{T-\\bar{t}}}{2}\\big(\\mathbf{R}_{\\bar{t}}+\\nabla\\log p_{\\mathcal{G},T-\\bar{t}}(\\mathbf{R}_{\\bar{t}})\\big)\\,\\mathrm{d}\\bar{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is equivalent to the deterministic process in denoising diffusion implicit model (DDIM) [54]. ", "page_idx": 3}, {"type": "text", "text": "An alternative to the score-prediction formulation is the denoising formulation [55, 56]. By defining a \u201cdenoising model\u201d S(G\u03b8,t) (Rt) which formulates the score model following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{s}_{\\mathcal{G},t}^{(6)}(\\mathbf{R}_{t})=\\frac{\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{S}_{\\mathcal{G},t}^{(6)}(\\mathbf{R}_{t})-\\mathbf{R}_{t}}{1-\\bar{\\alpha}_{t}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "the training loss function Eq. (2) in terms of $\\mathbf{S}_{\\mathcal{G},t}^{(\\theta)}(\\mathbf{R}_{t})$ becomes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\frac{\\bar{\\alpha}_{t}}{1-\\bar{\\alpha}_{t}}\\mathbb{E}_{p_{\\mathcal{G},0}(\\mathbf{R}_{0})}\\mathbb{E}_{\\epsilon_{t}}\\Big\\lVert\\mathbf{S}_{\\mathcal{G},t}^{(\\mathbf{0})}\\big(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{R}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{t}\\big)-\\mathbf{R}_{0}\\Big\\rVert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which follows the intuition to denoise a perturbed structure by predicting the original structure. This formulation better aligns with the notion of \u201cstructure prediction\u201d, hence can be benefited from successful model architectures [18, 19], and matches structure pre-training strategies [57, 24, 22, 58]. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin with the basic formulation of multi-task learning. We then present the two consistency training approaches between energy and structure prediction, based on two physical laws between the two tasks. The approach to directly leveraging physically-related datasets is described at last. ", "page_idx": 3}, {"type": "text", "text": "3.1 Multi-Task Learning for Energy and Structure Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Both energy and structure prediction tasks require a comprehensive understanding of the input molecular graph $\\mathcal{G}$ and structure $\\mathbf{R}$ , so a shared encoder $\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)}(\\mathbf{R})$ with parameters $\\Phi$ is employed. The time step $t$ is required by the diffusion formulation, which is taken as 0 for energy prediction indicating the input structure is unperturbed and real. For energy and structure prediction, the corresponding decoders $\\mathcal{D}_{\\mathrm{E}}^{(\\uptheta_{\\mathrm{E}})}$ and $\\bar{D}_{\\mathrm{S}}^{(\\uptheta_{\\mathrm{S}})}$ are introduced. For structure prediction, the denoising formulation is adopted (end of Sec. 2). Under this formulation, the two tasks are handled by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R})=\\mathcal{D}_{\\mathrm{E}}^{(\\Theta_{\\mathrm{E}})}\\big(\\mathcal{E}_{\\mathcal{G},t=0}^{(\\Phi)}(\\mathbf{R})\\big),\\quad\\mathbf{S}_{\\mathcal{G},t}^{(\\Phi,\\Theta_{\\mathrm{S}})}(\\mathbf{R})=\\mathcal{D}_{\\mathrm{S}}^{(\\Theta_{\\mathrm{S}})}\\big(\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)}(\\mathbf{R}),\\mathbf{R}\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "On one datapoint $(\\mathcal{G},\\mathbf{R},E)$ , the multi-task loss is $(c.f.$ Eq. (5)): $L_{\\mathrm{multi-task}}(\\Phi,\\Theta_{\\mathrm{E}},\\Theta_{\\mathrm{S}}|\\mathcal{G},\\mathbf{R},E)$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n=\\lambda_{\\mathrm{E}}\\Big\\vert E_{\\mathcal{G}}^{(\\Phi,\\mathbf{0}_{\\mathrm{E}})}(\\mathbf{R})-E\\Big\\vert+\\mathbb{E}_{t}\\frac{\\bar{\\alpha}_{t}}{1-\\bar{\\alpha}_{t}}\\mathbb{E}_{\\epsilon_{t}}\\Big\\vert\\Big\\vert\\mathbf{S}_{\\mathcal{G},t}^{(\\Theta)}\\big(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{R}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{t}\\big)-\\mathbf{R}\\Big\\vert\\Big\\vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A subtlety with the models is geometric invariance and equivariance. Indeed, if a structure $\\mathbf{R}$ is translated and rotated as a whole, the resulting atom coordinates represent essentially the same structure. The energy and the probability density should keep invariant after the transformation. For energy prediction, this can be guaranteed by using an invariant encoder $\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)}(\\mathbf{R})$ (no requirement on D(E\u03b8E)). For the probability density, it is known [59] that rotational invariance can be achieved by the invariance of $p_{\\mathcal{G},T}$ , which is satisfied by $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , and the equivariance of the denoising model ${\\bf S}_{\\mathcal{G},t}^{(\\Phi,\\theta_{\\mathrm{S}})}({\\bf R})$ . For this reason, the input structure $\\mathbf{R}$ re-enters the structure decoder $\\mathcal{D}_{\\mathrm{S}}^{(6_{\\mathrm{S}})}$ , which is implemented with an equivariant architecture. Translational invariance of density can be achieved by centering the structures; see ref. [60] for reasoning. ", "page_idx": 4}, {"type": "text", "text": "Nevertheless, multi-task learning is restricted by the level of accuracy of training data. As mentioned, structure data are often generated in a lower level of accuracy than energy data due to the more demanding nature. To alleviate this limitation, we exploit physical laws between molecular energy and structure, and propose consistency training losses accordingly to bridge the energy and structure models, thereby enhancing the accuracy of structure prediction from the more accurate energy model. ", "page_idx": 4}, {"type": "text", "text": "3.2 Optimality Consistency ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One direct relationship between energy and equilibrium structure is that the equilibrium structure $\\mathbf{R}_{\\mathrm{eq}}$ minimizes the energy, i.e., $\\mathbf{R}_{\\mathrm{eq}}=\\mathop{\\mathrm{argmin}}_{\\mathbf{R}}E_{\\mathcal{G}}(\\mathbf{R})$ . To enforce this optimality condition, we propose an optimality consistency loss $L_{\\mathrm{optim-cons}}$ , in the form of \u201cincrease after perturbation\u201d loss. It is based on the idea that the energy of the equilibrium structure should be lower than that of its perturbed version. Denoting $\\mathbf{R}_{\\mathrm{pred}}^{(\\Phi,\\theta_{\\mathrm{S}})}$ as the model-predicted equilibrium structure, the loss on a molecule $\\mathcal{G}$ can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{optim-cons}}(\\Theta_{\\mathrm{S}}\\mid\\Phi,\\Theta_{\\mathrm{E}},\\mathcal{G})=\\mathbb{E}_{\\eta}\\operatorname*{max}\\Big\\{0,\\;E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R}_{\\mathrm{pred}}^{(\\Phi,\\Theta_{\\mathrm{S}})})-E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R}_{\\mathrm{pred}}^{(\\Phi,\\Theta_{\\mathrm{S}})}+\\mathfrak{n})\\Big\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta\\sim\\mathcal{N}(\\mathbf{0},\\mathrm{Diag}(\\pmb{\\sigma}^{2}))$ is a small perturbation, and each element of $\\mathbf{\\sigma}^{2}$ is independently sampled from $\\mathrm{Unif}(0,\\sigma_{\\mathrm{max}}^{2}]$ . Since the purpose is to improve structure prediction accuracy by leveraging the energy model which has seen more accurate labels, so the consistency loss only optimizes the parameters exclusively for the structure prediction utility, i.e., structure decoder parameters $\\uptheta_{\\mathrm{S}}$ . Other parameters $\\Phi$ and $\\uptheta_{\\mathrm{E}}$ do not optimize this loss. In this way, the consistency loss would not contaminate the energy prediction model with the less accurate structure prediction model. ", "page_idx": 4}, {"type": "text", "text": "The standard way to produce R(pr\u03d5ed,\u03b8S) requires simulating the diffusion process (Eq. (1)) or the equivalent ODE (DDIM) (Eq. (3)), which calls the denoising model recursively. So optimizing $L_{\\mathrm{optim-cons}}$ would involve backpropagation through the simulation process, which can be impractically costly and numerically unstable. Fortunately, we can exploit the intuition in the denoising formulation and find a much cheaper way to generate structure. The intuition of the denoising model $\\mathbf{S}_{\\mathcal{G},t}(\\mathbf{R}_{t})$ is to recover the original structure $\\mathbf{R}_{0}$ from the perturbed structure ${\\bf R}_{t}$ . Although this is informationally impossible at the instance level, the denoising model still has a definite learning target at the distributional level, which is $\\mathbb{E}[{\\bf R}_{0}|{\\bf R}_{t}]$ [56]. Particularly, when $t=T$ , the correlation between $\\mathbf{R}_{0}$ and ${\\bf R}_{T}$ diminishes, so $\\mathbf{S}_{\\mathcal{G},T}(\\mathbf{R}_{T})$ learns to output $\\mathbb{E}[\\mathbf{R}_{0}]$ , the expectation of the target distribution, which is the equilibrium structure since the distribution concentrates at that structure. Under this .e  Tmhiosd eoln-lpyr erediqcutierde s stornuec teuvrael ucaatino nb eo fg tehnee rdaetnedoi sbiyn $\\mathbf{R}_{\\mathrm{pred}}^{(\\Phi,\\theta_{\\mathrm{S}})}\\,=\\,\\mathbf{S}_{\\mathcal{G},T}^{(\\Phi,\\theta_{\\mathrm{S}})}(\\mathbf{R}_{T})$ , where $\\mathbf{R}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ ", "page_idx": 4}, {"type": "text", "text": "Nevertheless, the rotational invariance of the structure distribution introduces more subtleties. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Let $\\mathbf{S}^{(6)}\\,:\\,\\mathbb{R}^{A\\times3}\\;\\rightarrow\\;\\mathbb{R}^{A\\times3}$ be a rotationally equivariant function; that is, for any rotation matrix $\\mathbf{Q}\\,\\in\\,\\mathrm{SO(3)}$ and structure $\\textbf{R}\\in\\~\\mathbb{R}^{A\\times3}$ , we have ${\\bf S}^{(\\boldsymbol{\\theta})}\\mathbf{\\widetilde{(RQ)}}\\,=\\,{\\bf S}^{(\\boldsymbol{\\theta})}\\mathbf{(R)}{\\bf Q}$ . Then, for any target structure $\\mathbf{R}^{\\star}\\in\\mathbb{R}^{A\\times3}$ , the minimizer of the denoising loss function $L(\\theta)=$ $\\mathbb{E}_{\\mathcal{N}(\\epsilon;\\mathbf{0},\\mathbf{I})}\\|\\mathbf{S}^{(\\theta)}(\\epsilon)-\\mathbf{R}^{\\star}\\|_{2}^{2}$ is the zero map; that is, $\\mathbf{S}^{(\\theta)}(\\mathbf{R})=\\mathbf{0}$ , for any $\\mathbf{R}$ . ", "page_idx": 4}, {"type": "text", "text": "See Appendix A for proof. This conclusion reveals that the learning target of the denoising model at $T$ is trivially all-zero, which cannot serve to generate the equilibrium structure. To circumvent this, a simple choice is to denoise from a time step $\\tau$ that is close but smaller than $T$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{R}_{\\mathrm{pred}}^{(\\Phi,\\Theta_{5})}=\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\Theta_{5})}(\\epsilon),\\quad\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}).\\quad\\mathrm{(for~large~}\\tau)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The target of the denoising model at $\\tau$ is $\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\theta_{\\mathrm{S}})}(\\epsilon)=\\mathbb{E}[\\mathbf{R}_{0}|\\mathbf{R}_{\\tau}=\\epsilon]$ , which is equivariant w.r.t $\\mathbf{R}_{\\tau}$ So the input $\\mathbf{R}_{\\tau}$ provides an orientation information hence breaking the rotational symmetry of the corresponding distribution $p(\\mathbf{R}_{0}|\\mathbf{R}_{\\tau})$ . The resulting expectation then would not average a structure over orientations evenly, hence not zero. More explicitly, since $p(\\mathbf{R}_{0}|\\mathbf{R}_{\\tau})\\propto p(\\mathbf{R}_{0})\\bar{p}(\\mathbf{R}_{\\tau}|\\mathbf{R}_{0})\\propto$ $\\begin{array}{r}{p(\\mathbf{R}_{0})\\exp\\left\\{-\\frac{\\|\\mathbf{R}_{0}-\\mathbf{R}_{\\tau}/\\sqrt{\\bar{\\alpha}_{\\tau}}\\|_{2}^{2}}{2(1/\\bar{\\alpha}_{\\tau}-1)}\\right\\}}\\end{array}$ \u03b1\u00af)\u03c4 \u222522 , we have p(R0|R\u03c4) \u221d p(R0)N(R0; R\u03c4/ \u03b1\u00af\u03c4, (1/\u03b1\u00af\u03c4 \u22121)I), where the Gaussian factor assigns larger probability along the direction of $\\mathbf{R}_{\\tau}$ , hence breaks the rotational symmetry from $p(\\mathbf{R}_{0})$ . Under this choice, the optimality consistency loss in Eq. (8) is specified as: $L_{\\mathrm{optim\u2013cons}}(\\Theta_{\\mathrm{S}}\\mid\\Phi,\\Theta_{\\mathrm{E}},\\mathcal{G})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{\\mathfrak{n}}\\mathbb{E}_{\\epsilon}\\operatorname*{max}\\left\\{0,\\,\\,E_{\\mathcal{G}}^{(\\Phi,\\mathfrak{G}_{\\mathtt{E}})}\\big(\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\mathfrak{G}_{\\mathtt{S}})}(\\epsilon)\\big)-E_{\\mathcal{G}}^{(\\Phi,\\mathfrak{G}_{\\mathtt{E}})}\\big(\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\mathfrak{G}_{\\mathtt{S}})}(\\epsilon)+\\mathfrak{n}\\big)\\right\\}.\\quad\\mathrm{(for~large~}\\tau)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Score Consistency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The optimality consistency loss only supervises the denoising model at large time steps. For small time steps, an alternative perspective on the physical law between equilibrium structure and energy can help. Physically, a molecule $\\mathcal{G}$ in a real system can take different structures with different probabilities. When the system is in thermodynamic equilibrium, the probability distribution of the structures can be determined from the energy function of the molecule. Particularly, in a system with fixed volume and temperature $\\tau$ , the structure distribution is the well-known Boltzmann distribution, $p_{\\mathrm{B};\\mathcal{G},\\mathcal{T}}(\\mathbf{R})\\,\\propto\\,\\exp\\left(-E_{\\mathcal{G}}(\\mathbf{R})/(k_{\\mathrm{B}}\\mathcal{T})\\right)$ , where $k_{\\mathrm{B}}$ is the Boltzmann constant. When temperature approaches zero, the distribution becomes concentrated on the equilibrium structure. This aligns with the learning target of the diffusion model for equilibrium structure prediction. Through the expression of the Boltzmann distribution, the structure model can thus be connected to the energy model. ", "page_idx": 5}, {"type": "text", "text": "To enforce this connection, ideally, the density function p(G\u03d5,\u03b8S)(R) modeled by the denoising model should match that defined by the energy model. Since the latter only provides an unnormalized density, we enforce their scores to match: $\\mathbb{E}_{q(\\mathbf{R})}\\big\\|\\nabla\\log p_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{s}})}(\\mathbf{R})+\\nabla E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R})/(k_{\\mathrm{B}}\\mathcal{T})\\big\\|_{2}^{2},$ where $q(\\mathbf{R})$ is a reference distribution. Nevertheless, it is computationally costly to evaluate the density function from the diffusion model: $\\log p_{\\mathcal G}^{(\\Phi,\\Theta_{\\mathrm{s}})}(\\mathbf{R})\\;\\;=\\;\\;\\log p_{T}(\\mathbf{R}_{T}^{(\\Phi,\\Theta_{\\mathrm{s}})})\\;+$ $\\begin{array}{r}{\\int_{0}^{T}\\frac{\\beta_{t}}{2}\\frac{\\sqrt{\\bar{\\alpha}_{t}}}{1-\\bar{\\alpha}_{t}}\\Big(3A\\sqrt{\\bar{\\alpha}_{t}}-\\nabla\\cdot{\\bf S}_{\\mathcal{G},t}^{(\\Phi,\\Theta_{s})}\\big({\\bf R}_{t}^{(\\Phi,\\Theta_{s})}\\big)\\Big)\\,\\mathrm{d}t}\\end{array}$ , where ${\\bf R}_{t\\in[0,T]}^{(\\Phi,\\theta_{\\mathrm{S}})}$ is the solution to the ODE $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathbf{R}_{t}}{\\mathrm{d}t}=}\\end{array}$ $\\begin{array}{r}{\\frac{\\beta_{t}}{2}\\frac{\\sqrt{\\bar{\\alpha}_{t}}}{1-\\bar{\\alpha}_{t}}\\left(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{R}_{t}-\\mathbf{S}_{\\mathcal{G},t}^{(\\Phi,\\mathbf{\\theta}_{0}\\mathrm{s})}\\big(\\mathbf{R}_{t}\\big)\\right)}\\end{array}$ with initial condition ${\\bf R}_{0}={\\bf R}$ [35]. Significant computational cost would be incurred from invoking and backpropagating through an ODE solver to evaluate and optimize the density. ", "page_idx": 5}, {"type": "text", "text": "We hence turn to another way to leverage this connection. Note from Eq. (4), the denoising model can be used to recover the score model, which targets the score function of the marginal distribution at the corresponding diffusion time instant. Particularly, the score model at $t=0$ should approximate the score of the desired distribution, which is $p_{\\mathbb{B};\\mathscr{G},\\mathscr{T}}$ for small $\\tau$ . The energy model can hence provide supervision to the score model by enforcing this connection: $L_{\\mathrm{score-cons}}(\\bar{\\Theta}_{\\mathrm{S}}\\mid\\Phi,\\Theta_{\\mathrm{E}},\\mathcal{G})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{p_{\\tau}(\\mathbf{R}_{\\tau})}\\Big\\|\\frac{\\sqrt{\\bar{\\alpha}_{\\tau}}\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\Theta_{s})}(\\mathbf{R}_{\\tau})-\\mathbf{R}_{\\tau}}{1-\\bar{\\alpha}_{\\tau}}+\\frac{\\nabla_{\\mathbf{R}_{\\tau}}\\mathbf{E}_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R}_{\\tau})}{k_{\\mathrm{B}}T}\\Big\\|_{2}^{2}.\\quad\\mathrm{(for~small~}\\tau\\mathrm{)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this score consistency loss, we have avoided taking the time step $\\tau$ exactly zero for numerical stability consideration. Indeed, when $\\tau\\rightarrow0$ , the denoising model $\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\theta_{\\mathrm{s}})}$ approaches the identity map, and $\\bar{\\alpha}_{\\tau}$ approaches 1, making the first term in Eq. (11) an indeterminate form of type $0/0$ , which may render numerical stability issues. For the reference distribution to generate data to evaluate the loss, one can choose either the data distribution $p_{\\mathcal{G},0}$ for which the energy model gives more confident results, or the perturbed distribution $p_{\\mathcal{G},\\tau}$ (can be sampled by adding noise to a data sample $\\mathbf{R}_{0}$ following $p(\\mathbf{R}_{t}|\\mathbf{R}_{0}))$ for relevance to how the denoising model $\\mathbf{S}_{\\mathcal{G},\\tau}^{(\\Phi,\\theta_{\\mathrm{s}})}$ is invoked. Through some trials, we found the latter gives slightly better results. ", "page_idx": 5}, {"type": "text", "text": "Finally, as is the case for the optimality consistency loss, the score consistency loss also only optimizes the parameters $\\uptheta_{\\mathrm{S}}$ of the structure decoder, to ensure the energy model would not be misled by the less accurate structure data. To implement this unconventional optimization requirement, we list detailed algorithms for the two consistency losses in Appendix B.1 in terms of the actual model components $\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)},\\mathcal{D}_{\\mathrm{E}}^{(\\theta_{\\mathrm{E}})}$ E  and D , $\\mathcal{D}_{\\mathrm{S}}^{(\\uptheta_{\\mathrm{S}})}$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 Leveraging Physically-Related Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Besides the difference in the level of theory to generate data, the heterogeneity of molecular-science datasets also lies in the difference of concerned quantities. Compared to the enormous chemical space, available datasets for equilibrium structure are still not abundant, while there is a vast amount of data generated for physically related but different tasks, for example, labels of atomic forces, and data on off-equilibrium structures. We highlight that the consistency losses can leverage such datasets in an explicit way to further improve equilibrium structure prediction. Note that the consistency losses Eqs. (10, 11) works by offering the information at a higher level of theory in the energy model, in the form of energy landscape on the structure space; i.e., ranking different structures in optimality consistency, and providing energy gradient in score consistency. For better learning the landscape, the force labels, which are negative gradients of the energy, provides first-order information of the landscape, and energy and force labels on multiple off-equilibrium structures enable better exploration on the structure space. This approach provides a more direct and concrete information path to equilibrium structure prediction than helping learn a better representation in multi-task learning. For learning a better energy landscape, the force labels are used to directly supervise the gradient of the energy model. The loss term for a datapoint $(\\mathcal{G},\\mathbf{R},\\mathbf{F})$ is: ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{\\mathrm{force}}(\\Phi,\\mathbf{\\boldsymbol{\\theta}}_{\\mathrm{E}}\\mid\\mathbf{\\boldsymbol{R}},\\mathbf{\\boldsymbol{F}})=\\left\\|\\nabla_{\\mathbf{R}}E_{\\mathcal{G}}^{(\\Phi,\\mathbf{\\boldsymbol{\\theta}}_{\\mathrm{E}})}(\\mathbf{\\boldsymbol{R}})+\\mathbf{\\boldsymbol{F}}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{F}$ is the force label. Note that there may be multiple $(\\mathbf{R},\\mathbf{F})$ data pairs for one molecule $\\mathcal{G}$ , which provide even richer information on the energy landscape. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate the advantages of incorporating the proposed consistency losses into multi-task learning. Implementation details are provided in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We consider multi-task learning of energy and structure prediction on the PubChemQC B3LYP/6-31G\\*//PM6 dataset [29] (abbreviated as PM6), which is seemingly the largest $_{\\sim86\\mathrm{M}}$ molecules) public available dataset with DFT-level property labels, hence a preferred setting for pre-training a molecular model. The energy labels are in the DFT (B3LYP/6-31G\\*) level, while the equilibrium structures are produced at the semi-empirical PM6 [61] level, which is less accurate than DFT. Consistency training is hence considered to improve structure prediction accuracy using the more accurate energy data. To evaluate the effect of improved structure prediction accuracy beyond the PM6 level, the accuracy is evaluated against structures generated at the DFT level, which are available in the PCQM4Mv2 dataset [27] (abbreviated as PCQ) and the QM9 dataset [25]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation. Each of the evaluation datasets of PCQ and QM9 is spilt into three disjoint sets for training, validation, and test. The training and validation sets are for optional fine-tuning (see Sec. 4.4). Following existing convention [41, 44, 40], each test set is prepared by uniformly randomly selecting 200 distinct molecules from PCQ or QM9 that do not appear in the training dataset (PM6), which already makes the test molecules sufficiently dissimilar from training molecules (Appendix C.6). ", "page_idx": 6}, {"type": "text", "text": "On each test molecule, we sample 200 structures using the model, calculate their rooted mean square deviations (RMSDs) against the equilibrium structure in the test set, and evaluate the mean and the minimum over these RMSDs. Due to the geometric invariance of the structure distribution, the RMSD is evaluated after translational and rotational alignment of two structures using the Kabsch algorithm [62]. We consider both the denoising (Eq. (9)) and the DDIM (Eq. (3)) approaches for structure sampling. We also provide coverage evaluation results in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "In each setting, we independently repeat the evaluation process for five times using different random seeds, and report the mean of the repeats in the following tables. The standard deviations and t-test p-values are collectively provided in Appendix C.5. In settings using consistency training, both the optimality (Eq. (10)) and score consistency losses (Eq. (11)) are added to the multi-task training loss (Eq. (7)). Validation results for training in terms of both energy prediction and structure generation are provided in Appendix C.4. ", "page_idx": 6}, {"type": "image", "img_path": "GnF9tavqgc/tmp/09b0e05f64cb34a37b496d00100322eca5e2c77eab0103835561e42fb45e1aba.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Comparison of energy (eV) on the model-generated structure ${\\bf R}_{\\mathrm{pred}}$ using the denoising method and the equilibrium structure $\\mathbf{R}_{\\mathrm{eq}}$ in the PCQ dataset. Each point represents the modelpredicted energy values on the two structures for one test molecule. Models are trained on (left) the PM6 dataset, (middle) the PM6 dataset and SPICE force dataset, and (right) the PM6 dataset with a subset of force labels. The closer a point lies to the diagonal line, the closer the energy of the predicted structure is to the minimum energy, indicating a closer prediction of equilibrium structure. ", "page_idx": 7}, {"type": "text", "text": "4.2 Structure Prediction Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first evaluate the effect of consistency training over multi-task training following the above settings. The results are shown in Table 1. We see that consistency training enhances the accuracy of structure prediction beyond multi-task learning consistently, without using any more accurate structure data. The improvement is significant when compared to the standard deviations provided in Appendix Table C.8, which are as low as around $0.00\\bar{3}\\ \\mathring{\\mathrm{A}}$ (Appendix Table C.10 verifies t-test significance). We note that this improvement is not at the cost of a lower energy prediction accuracy, as indicated by the energy prediction results in Appendix Table C.6(left). ", "page_idx": 7}, {"type": "text", "text": "To further examine that the improvement is from the effect of consistency training, we evaluate the energy of the predicted structure and the true DFT-level equilibrium structure in the PCQ dataset for each test molecule using the model, which is shown in the scatter plot of Fig. 2(left). We observe that when using consistency training, the overall energy is reduced, indicating that the predicted structures indeed have lower energy hence closer to the true DFT-level equilibrium structure. To quantify this improvement, in Appendix C.3 we define an \u201cenergy gap\u201d metric, and the results shown in Table C.5 consolidate the observation. ", "page_idx": 7}, {"type": "text", "text": "4.3 Results using Physically-Related Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We next investigate the effect of consistency training for leveraging physically related data, as explained in Sec. 3.4. For this, we consider the force data in the SPICE dataset (PubChem subset) [32]. The force data are also available on multiple off-equilibrium structures for each molecule. We note the subtlety that the setting of DFT in SPICE, \u03c9B97M-D3(BJ)/def2-TZVPPD, is different from that for generating energy labels in the PM6 dataset. (Up to our knowledge, there does not seem to exist a public force dataset that matches the DFT setting as the PM6 dataset.) On one hand, calculation on near-equilibrium structures of small molecules is not very sensitive to DFT settings, especially for force calculation (even less affected than energy), while the force labels on multiple structures could be more valuable to learning the energy landscape despite the mismatch. So we still consider it as a relevant investigation setting. Note energy labels in SPICE are not used, and additional structures are not used for training the structure decoder. On the other hand, to reduce the gap, we also generated in-house force labels on a subset of PM6 structures using the same DFT setting (B3LYP/6-31G\\*) as for the PM6 dataset energy labels. The systematic error is controlled, although for each molecule there is only one labeled structure. ", "page_idx": 7}, {"type": "table", "img_path": "GnF9tavqgc/tmp/6aa5f77a6bda8c24f3eda1314e5060a2b8756f66204f896f27cd089f67ab6cb2.jpg", "table_caption": ["Table 1: Test RMSD (\u00c5; lower is better) of structure prediction by multi-task learning and consistency learning on PM6 dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "GnF9tavqgc/tmp/e8c93438b2fe99e407626311080ad5d75b057bb007b370878c926990d9ffe60b.jpg", "table_caption": ["Table 2: Test RMSD ( $\\mathring\\mathrm{A}$ ; lower is better) of structure prediction by multi-task learning and consistency learning on the PM6 dataset with additional SPICE force dataset or PM6 subset force data. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The results after incorporating SPICE force data and PM6 subset force data in training are shown in Table 2. We first observe that consistency training still outperforms multi-task training in structure prediction in all cases (see Appendix Tables C.8 and C.10 for standard derivations and t-test pvalues). We note that this improvement is not at the cost of a lower energy prediction accuracy, as indicated by Appendix Table C.6(middle) indicates energy prediction is also not compromised in consistency training in this case. When compared to Table 1, we see that the inclusion of force data does not uniformly enhance multi-task learning performance, since the mechanism to learn a better representation is still implicit and indirect and may require extensive tuning. In contrast, using consistency losses improves structure prediction more consistently when physically related data are available in training. These observations indicate that consistency loss training can potentially assist the model in more effectively utilizing data from different sources or modalities. ", "page_idx": 8}, {"type": "text", "text": "Energy analysis is presented in Fig. 2(middle) utilizing SPICE force data, and in Fig. 2(right) with force labels on a subset of PM6 molecules. The data indicate that training with consistency loss results in lower predicted energies than multi-task training. Furthermore, we observe that the structures predicted by models trained with force datasets (Fig. 2, middle and right) have lower predicted energies compared to those trained exclusively on the PM6 dataset (Fig. 2, left), illustrating the advantage of incorporating force labels. ", "page_idx": 8}, {"type": "text", "text": "4.4 Fine-Tuning Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition to the zero-shot prediction evaluations, we further fine-tune the pre-trained models investigated in Sec. 4.2 using DFT-level structures in PCQ or QM9 training datasets. The results presented in Table 3 indicate that the inclusion of consistency loss in pre-training still enhances the accuracy of structure prediction even after fine-tuning. See Appendix Tables C.9 and C.10 for statistical significance. This could be attributed to that using consistency losses in pre-training can already inform the model of more accurate structure information, leading to a state that is more relevant to the underlying physics, which is a favored starting point for further improvements through fine-tuning. Results of fine-tuning models that are pre-trained with SPICE force and PM6 subset force are shown in Appendix C.1, which further demonstrates the advantages of the consistency loss. ", "page_idx": 8}, {"type": "table", "img_path": "GnF9tavqgc/tmp/7faae59d33212043a1c5b5898124089c1dc135a8c7c137e744cded757eb0dfab.jpg", "table_caption": ["Table 3: Test RMSD ( $\\mathring\\mathrm{A}$ ; lower is better) after finetuning for structure prediction pre-trained by multi-task learning and consistency learning on the PM6 dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work leverages physical laws between molecular tasks to bridge data heterogeneity in multi-task learning. Consistency losses are designed to enforce physical laws between inter-atomic potential energy prediction and equilibrium structure prediction. They have shown to improve structure prediction beyond the typical accuracy level of structure data by leveraging abundant energy data in a higher level of accuracy, and can directly leverage force and off-equilibrium structure data to further improve the accuracy. The advantage still holds after finetuning. We would like to highlight that the improvement comes \u201cfor free\u201d as no additional data (e.g., more accurate structure data) are required, demonstrating the value of physical laws in learning molecular tasks. The idea bears broader generality as data heterogeneity is ubiquitous in the science domain, and data for a specific task are often limited in either abundancy or accuracy. ", "page_idx": 9}, {"type": "text", "text": "The current work is limited to the consistency between energy and structure prediction, while more consistency laws can be considered in molecular science. Apart from mentioned works in connecting energy and thermodynamic distribution, more possibilities include electronic structure and molecular properties, and fine-grained and coarse-grained structures and macroscopic statistics. The significance of improvement in this work is still limited by the abundance of the data involved. Further improvement can be expected with more abundant/diverse but possibly less accurate structure data from, e.g., RDKit [63] or experimental measurements, or energy/force datasets in matching level of theory that more extensively explore the structural space. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Oliver T Unke and Markus Meuwly. PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges. Journal of chemical theory and computation, 15(6): 3678\u20133693, 2019.   \n[2] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do Transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34:28877\u201328888, 2021.   \n[3] Kristof Sch\u00fctt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377\u20139388. PMLR, 2021.   \n[4] Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and E Weinan. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. Physical Review Letters, 120(14):143001, 2018.   \n[5] Johannes Gasteiger, Florian Becker, and Stephan G\u00fcnnemann. GemNet: Universal directional graph neural networks for molecules. Advances in Neural Information Processing Systems, 34: 6790\u20136802, 2021.   \n[6] Chi Chen and Shyue Ping Ong. A universal graph deep learning interatomic potential for the periodic table. Nature Computational Science, 2(11):718\u2013728, 2022.   \n[7] Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and G\u00e1bor Cs\u00e1nyi. MACE: Higher order equivariant message passing neural networks for fast and accurate force fields. Advances in Neural Information Processing Systems, 35:11423\u201311436, 2022.   \n[8] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications, 14(1):579, 2023.   \n[9] He Li, Zun Wang, Nianlong Zou, Meng Ye, Runzhang Xu, Xiaoxun Gong, Wenhui Duan, and Yong Xu. Deep-learning density functional theory Hamiltonian for efficient ab initio electronic-structure calculation. Nature Computational Science, 2(6):367\u2013377, 2022.   \n[10] He Zhang, Chang Liu, Zun Wang, Xinran Wei, Siyuan Liu, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Self-consistency training for density-functional-theory Hamiltonian prediction. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview. net/forum?id $\\cdot$ Vw4Yar2fmW.   \n[11] James Kirkpatrick, Brendan McMorrow, David HP Turban, Alexander L Gaunt, James S Spencer, Alexander GDG Matthews, Annette Obika, Louis Thiry, Meire Fortunato, David Pfau, et al. Pushing the frontiers of density functionals by solving the fractional electron problem. Science, 374(6573):1385\u20131389, 2021.   \n[12] R. Remme, T. Kaczun, M. Scheurer, A. Dreuw, and F. A. Hamprecht. KineticNet: Deep learning a transferable kinetic energy functional for orbital-free density functional theory. The Journal of Chemical Physics, 159(14):144113, 10 2023. ISSN 0021-9606. doi: 10.1063/5.0158275.   \n[13] He Zhang, Siyuan Liu, Jiacheng You, Chang Liu, Shuxin Zheng, Ziheng Lu, Tong Wang, Nanning Zheng, and Bin Shao. Overcoming the barrier of orbital-free density functional theory for molecular systems using deep learning. Nature Computational Science, Mar 2024. ISSN 2662-8457. doi: 10.1038/s43588-024-00605-8.   \n[14] Frank No\u00e9, Simon Olsson, Jonas K\u00f6hler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457):eaaw1147, 2019.   \n[15] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589, 2021.   \n[16] Emiel Hoogeboom, V\u00edctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8867\u20138887. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/ hoogeboom22a.html.   \n[17] Shuxin Zheng, Jiyan He, Chang Liu, Yu Shi, Ziheng Lu, Weitao Feng, Fusong Ju, Jiaxi Wang, Jianwei Zhu, Yaosen Min, He Zhang, Shidi Tang, Hongxia Hao, Peiran Jin, Chi Chen, Frank No\u00e9, Haiguang Liu, and Tie-Yan Liu. Predicting equilibrium distributions for molecular systems with deep learning. Nature Machine Intelligence, May 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00837-3.   \n[18] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with RFdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[19] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, pages 1\u20139, 2023.   \n[20] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[21] Rich Caruanaf. Multitask learning. Machine learning, 28:41\u201375, 1997.   \n[22] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-Mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=6K2RM6wVqKu.   \n[23] Duo Zhang, Xinzijian Liu, Xiangyu Zhang, Chengqian Zhang, Chun Cai, Hangrui Bi, Yiming Du, Xuejian Qin, Jiameng Huang, Bowen Li, Yifan Shan, Jinzhe Zeng, Yuzhi Zhang, Siyuan Liu, Yifan Li, Junhan Chang, Xinyan Wang, Shuo Zhou, Jianchuan Liu, Xiaoshan Luo, Zhenyu Wang, Wanrun Jiang, Jing Wu, Yudi Yang, Jiyuan Yang, Manyi Yang, Fu-Qiang Gong, Linshuang Zhang, Mengchao Shi, Fu-Zhi Dai, Darrin M. York, Shi Liu, Tong Zhu, Zhicheng Zhong, Jian Lv, Jun Cheng, Weile Jia, Mohan Chen, Guolin Ke, Weinan E, Linfeng Zhang, and Han Wang. DPA-2: Towards a universal large atomic model for molecular and material simulation. arXiv preprint arXiv:2312.15492, 2023.   \n[24] Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. One transformer can understand both 2d & 3d molecular data. In The Eleventh International Conference on Learning Representations, 2023.   \n[25] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1\u20137, 2014.   \n[26] Maho Nakata and Tomomi Shimazaki. PubChemQC project: a large-scale first-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling, 57(6):1300\u20131308, 2017.   \n[27] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. OGBLSC: A large-scale challenge for machine learning on graphs. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[28] Maho Nakata, Tomomi Shimazaki, Masatomo Hashimoto, and Toshiyuki Maeda. PubChemQC PM6: Data sets of 221 million molecules with optimized molecular geometries and electronic properties. Journal of Chemical Information and Modeling, 60(12):5891\u20135899, 2020.   \n[29] Maho Nakata and Toshiyuki Maeda. PubChemQC B3LYP/6-31G\\*//PM6 dataset: the electronic structures of 86 million molecules using B3LYP/6-31G\\* calculations. arXiv preprint arXiv:2305.18454, 2023.   \n[30] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Sch\u00fctt, and Klaus-Robert M\u00fcller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017.   \n[31] Stefan Chmiela, Valentin Vassilev-Galindo, Oliver T Unke, Adil Kabylda, Huziel E Sauceda, Alexandre Tkatchenko, and Klaus-Robert M\u00fcller. Accurate global machine learning force fields for molecules with hundreds of atoms. Science Advances, 9(2):eadf0873, 2023.   \n[32] Peter Eastman, Pavan Kumar Behara, David L Dotson, Raimondas Galvelis, John E Herr, Josh T Horton, Yuezhi Mao, John D Chodera, Benjamin P Pritchard, Yuanqing Wang, et al. SPICE, a dataset of drug-like molecules and peptides for training machine learning potentials. Scientific Data, 10(1):11, 2023.   \n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \n[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851, 2020.   \n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[36] Gregor Simm and Jose Miguel Hernandez-Lobato. A generative model for molecular distance geometry. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8949\u20138958. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/ simm20a.html.   \n[37] Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative dynamics for molecular conformation generation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=pAbm1qfheGk.   \n[38] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.   \n[39] Octavian Ganea, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green, and Tommi Jaakkola. Geomol: Torsional geometric generation of molecular 3D conformer ensembles. Advances in Neural Information Processing Systems, 34:13757\u201313769, 2021.   \n[40] Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Yusong Wang, Tong Wang, Tao Qin, Wengang Zhou, Houqiang Li, Haiguang Liu, and Tie-Yan Liu. Direct molecular conformation generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856.   \n[41] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9558\u20139568. PMLR, 18\u201324 Jul 2021. URL https://proceedings. mlr.press/v139/shi21b.html.   \n[42] Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 19784\u201319795. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ a45a1d12ee0fb7f1f872ab91da18f899-Paper.pdf.   \n[43] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. In Advances in Neural Information Processing Systems, volume 35, pages 24240\u201324253. Curran Associates, Inc., 2022.   \n[44] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. GeoDiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022.   \n[45] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015.   \n[46] Laurence Illing Midgley, Vincent Stimper, Gregor N. C. Simm, Bernhard Sch\u00f6lkopf, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Flow annealed importance sampling bootstrap. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id $\\cdot$ XCTVFJwS9LJ.   \n[47] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusionbased generative modeling. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.   \n[48] Francisco Vargas, Will Sussman Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $\\cdot$ 8pvnfTAbu1f.   \n[49] Avishek Joey Bose, Tara Akhound-Sadegh, Jarrid Rector-Brooks, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, and Alexander Tong. Iterated denoising energy matching for sampling from Boltzmann densities. In International Conference on Machine Learning, 2024.   \n[50] Marloes Arts, Victor Garcia Satorras, Chin-Wei Huang, Daniel Z\u00fcgner, Marco Federici, Cecilia Clementi, Frank No\u00e9, Robert Pinsler, and Rianne van den Berg. Two for one: Diffusion models and force fields for coarse-grained molecular dynamics. Journal of Chemical Theory and Computation, 19(18):6151\u20136159, 2023.   \n[51] Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. proceedings of the National Academy of Sciences, 113(47):E7351\u2013 E7358, 2016.   \n[52] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[53] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=St1giarCHLP.   \n[55] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances in Neural Information Processing Systems, volume 34, pages 21696\u201321707, 2021.   \n[56] Giannis Daras, Yuval Dagan, Alex Dimakis, and Constantinos Daskalakis. Consistent diffusion models: Mitigating sampling drift by learning to be consistent. In Advances in Neural Information Processing Systems, volume 36, 2024.   \n[57] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. arXiv preprint arXiv:2206.00133, 2022.   \n[58] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16284\u201316294, 2023.   \n[59] Jonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: exact likelihood generative learning for symmetric densities. In International conference on machine learning, pages 5361\u20135370. PMLR, 2020.   \n[60] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. SE(3) diffusion model with application to protein backbone generation. In Proceedings of the 40th International Conference on Machine Learning, pages 40001\u201340039, 2023.   \n[61] James JP Stewart. Optimization of parameters for semiempirical methods V: Modification of NDDO approximations and application to 70 elements. Journal of Molecular modeling, 13: 1173\u20131213, 2007.   \n[62] Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32 (5):922\u2013923, 1976.   \n[63] Greg Landrum et al. RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8(31.10):5281, 2013.   \n[64] Yu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie Luo, Chang Liu, Di He, and Tie-Yan Liu. Benchmarking graphormer on large-scale molecular modeling datasets. arXiv preprint arXiv:2203.04810, 2022.   \n[65] Tianlang Chen, Shengjie Luo, Di He, Shuxin Zheng, Tie-Yan Liu, and Liwei Wang. GeoMFormer: A general architecture for geometric molecular representation learning. In International Conference on Machine Learning, 2024.   \n[66] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, San Diega, CA, USA, 2015.   \n[67] D\u00e1vid Bajusz, Anita R\u00e1cz, and K\u00e1roly H\u00e9berger. Why is Tanimoto index an appropriate choice for fingerprint-based similarity calculations? Journal of cheminformatics, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first prove the following two preliminary Lemmas. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Let $\\pmb{\\epsilon}\\,=\\,\\left[\\pmb{\\epsilon}_{1}^{\\top},\\dots,\\pmb{\\epsilon}_{A}^{\\top}\\right]^{\\top}$ , and $\\epsilon_{i}$ are independent, identically distributed random vectors with $\\mathbf{\\epsilon}_{{i}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{3})$ . Q is a random variable uniformly distributed over SO(3). Then the random matrix $\\gamma=\\epsilon\\mathbf{Q}$ has the same distribution as $\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Consider the probability density function of $\\gamma$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\gamma}(\\gamma)=\\int p_{\\gamma,\\mathbf{Q}}(\\gamma,\\mathbf{Q})\\mathrm{d}\\mathbf{Q}=\\int p_{\\mathbf{Q}}(\\mathbf{Q})p_{\\gamma|\\mathbf{Q}}(\\gamma\\mid\\mathbf{Q})\\mathrm{d}\\mathbf{Q}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\gamma=\\epsilon\\mathbf{Q}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\gamma|\\mathbf{Q}}(\\gamma\\mid\\mathbf{Q})=\\prod_{i=1}^{A}\\frac{1}{(2\\pi)^{3/2}}\\exp\\left\\{-\\frac{\\|\\mathbf{Q}^{\\top}\\gamma_{i}\\|_{2}^{2}}{2}\\right\\}=\\frac{1}{(2\\pi)^{3/2}}\\prod_{i=1}^{A}\\exp\\left\\{-\\frac{\\|\\gamma_{i}\\|_{2}^{2}}{2}\\right\\}=p_{\\epsilon}(\\gamma).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, it follows that: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\gamma}(\\gamma)=\\int p_{\\mathbf{Q}}(\\mathbf{Q})p_{\\gamma|\\mathbf{Q}}(\\gamma\\mid\\mathbf{Q})\\mathrm{d}\\mathbf{Q}=\\int p_{\\mathbf{Q}}(\\mathbf{Q})p_{\\epsilon}(\\gamma)\\mathrm{d}\\mathbf{q}=p_{\\epsilon}(\\gamma),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which demonstrates that $\\gamma$ is identically distributed as $\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. Assume $\\mathbf{Q}$ is a random variable uniformly distributed over SO(3). Then $\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}=\\mathbf{0}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For any fixed $\\mathbf{Q}_{0}\\,\\in\\,\\mathrm{SO(3)}$ , the distribution of $\\mathbf{QQ}_{0}$ is identical to that of $\\mathbf{Q}$ due to the uniformity of the distribution over the group SO(3). Consequently, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}=\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}\\mathbf{Q}_{0}=\\left[\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}\\right]\\mathbf{Q}_{0}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Suppose $\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}=[\\mathbf{q}_{1},\\mathbf{q}_{2},\\mathbf{q}_{3}]$ , where ${\\bf q}_{1},{\\bf q}_{2},{\\bf q}_{3}$ are the columns of $\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}$ . Then, it follows that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n[{\\bf q}_{1},{\\bf q}_{2},{\\bf q}_{3}]=[{\\bf q}_{1},{\\bf q}_{2},{\\bf q}_{3}]{\\bf Q}_{0}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Selecting $\\mathbf{Q}_{0}=\\mathrm{Diag}\\{-1,-1,1\\}$ yields $\\mathbf{q}_{1}=-\\mathbf{q}_{1}$ and ${\\bf q}_{2}=-{\\bf q}_{2}$ , , which implies that ${\\bf q}_{1}={\\bf q}_{2}=$ 0. Similarly, choosing $\\mathbf{Q}_{0}=\\mathrm{Diag}\\{1,-1,-1\\}$ results in ${\\bf q}_{3}=-{\\bf q}_{3}$ , hence ${\\bf q}_{3}={\\bf0}$ . Therefore, we conclude that $\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}=\\mathbf{0}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "We now proceed to prove Proposition 1. Let $\\mathbf{Q}$ be a random variable uniformly distributed over SO(3). Then according to Lemma A.1, the random matrix $\\gamma=\\epsilon\\mathbf{Q}$ is identically distributed as $\\epsilon$ . Then we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\Theta)=\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\|\\mathbf{S}^{(\\Theta)}({\\boldsymbol{\\epsilon}})-\\mathbf{R}^{\\star}\\|_{2}^{2}=\\mathbb{E}_{\\boldsymbol{\\gamma}}\\|\\mathbf{S}^{(\\Theta)}({\\boldsymbol{\\gamma}})-\\mathbf{R}^{\\star}\\|_{2}^{2}=\\mathbb{E}_{\\boldsymbol{\\epsilon},\\mathbf{Q}}\\|\\mathbf{S}^{(\\Theta)}({\\boldsymbol{\\epsilon}}{\\mathbf{Q}})-\\mathbf{R}^{\\star}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{\\boldsymbol{\\epsilon},\\mathbf{Q}}\\|\\mathbf{S}^{(\\Theta)}({\\boldsymbol{\\epsilon}})\\mathbf{Q}-\\mathbf{R}^{\\star}\\|_{2}^{2}=\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\mathbb{E}_{\\mathbf{Q}}\\|\\mathbf{S}^{(\\Theta)}({\\boldsymbol{\\epsilon}})-\\mathbf{R}^{\\star}\\mathbf{Q}^{\\top}\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For any $\\epsilon\\in\\mathbb{R}^{A\\times3}$ , the objective function $\\mathbb{E}_{\\mathbf{Q}}\\big\\|\\mathbf{S}^{(\\theta)}(\\mathbf{\\epsilon})-\\mathbf{R}^{\\star}\\mathbf{Q}^{\\top}\\big\\|_{2}^{2}$ achieves its minimum when $\\mathbf{S}^{(\\boldsymbol{\\theta})}(\\pmb{\\epsilon})\\,=\\,\\mathbb{E}_{\\mathbf{Q}}[\\mathbf{R}^{\\star}\\mathbf{Q}^{\\top}]\\,=\\,\\mathbf{R}^{\\star}[\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}^{\\top}]$ . Since $\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}^{\\top}=\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}$ and using Lemma A.2, we have $\\mathbb{E}_{\\mathbf{Q}}\\mathbf{Q}^{\\top}=\\mathbf{0}$ , which implies $\\mathbf{S}^{(6)}(\\mathbf{\\epsilon}\\mathbf{)}=\\mathbf{0}$ . Therefore, $\\mathbf{S}^{(\\theta)}$ is a zero map. ", "page_idx": 14}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Implementation Details for Consistency Losses ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proposed consistency loss $L_{\\mathrm{optim-cons}}$ in Eq. (10) and $L_{\\mathrm{score-cons}}$ in Eq. (11) are designed to update the parameters $\\uptheta_{\\mathrm{S}}$ within the structure prediction model S(G,\u03d5\u03c4,\u03b8S). To achieve this, we employ the stop gradient operation $\\mathrm{SG}(\\cdot)$ to prevent unnecessary gradient computation for the parameters $\\Phi$ in the encoder model EG(,\u03d5t) and $\\uptheta_{\\mathrm{E}}$ in the energy decoder $\\mathcal{D}_{\\mathrm{E}}^{((\\uptheta_{\\mathrm{E}})}$ . The detailed implementations of optimality consistency and score consistency are presented in Alg. B.1 Alg. B.2, respectively. ", "page_idx": 14}, {"type": "text", "text": "Require: Encoder: $\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)}(\\mathbf{R})$ , structure decoder $\\mathcal{D}_{\\mathrm{E}}^{(\\uptheta_{\\mathrm{E}})}$ , energy decoder $\\mathcal{D}_{\\mathrm{S}}^{(\\uptheta_{\\mathrm{S}})}$ , Diffusion time step $\\tau$ Ensure: \u2207\u03b8SLoptim-cons   \n1: Sample $\\epsilon$ and $\\boldsymbol{\\mathfrak{\\eta}}$ .   \n2: Extract the molecular feature $\\mathbf{h}\\gets\\mathcal{E}_{\\mathcal{G},t=\\tau}^{(\\Phi)}(\\mathbf{\\epsilon})$ using the encoder.   \n3: Apply the stop gradient operation to the molecular feature and obtain $\\bar{\\mathbf{h}}\\gets\\mathrm{SG}(\\mathbf{h})$ through Pytorch\u2019s .detach() method.   \n4: Compute the denoised structure $\\hat{\\mathbf{R}}_{0}\\leftarrow D_{\\mathrm{S}}^{(\\uptheta_{\\mathrm{S}})}(\\bar{\\mathbf{h}},\\mathbf{\\epsilon})$ using the structure decoder.   \n5: Set requires_grad $=$ False for the parameters in the energy model ${\\cal E}_{\\mathcal{G}}^{(\\Phi,\\theta_{\\mathrm{E}})}({\\bf R})~=$ $\\mathcal{D}_{\\mathrm{E}}^{(\\theta_{\\mathrm{E}})}(\\mathcal{E}_{\\mathcal{G},t=0}^{(\\Phi)}(\\mathbf{R}))$   \n6: Evaluate the loss $L_{\\mathrm{optim-cons}}=\\operatorname*{max}\\left\\{0,\\ E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\hat{\\bf R}_{0})-E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\hat{\\bf R}_{0}+\\eta)\\right\\}.$   \n7: Determine the gradient $\\nabla_{\\boldsymbol{\\theta}_{\\mathrm{S}}}L_{\\mathrm{optim-cons}}$ through automatic differentiation. ", "page_idx": 15}, {"type": "text", "text": "Algorithm B.2 Implementation of Score Consistency Loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Require: Encoder: $\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)}(\\mathbf{R})$ , structure decoder , structure decoder DE?E', energy decoder D\\$', Diffusion time step , energy decoder , Diffusion time step $\\tau$ Ensure: \u2207\u03b8SLscore-cons ", "page_idx": 15}, {"type": "text", "text": "1: Sample $\\mathbf{R}_{\\tau}$ .   \n2: Extract the molecular feature $\\mathbf{h}_{0}\\gets\\mathcal{E}_{\\mathcal{G},t=0}^{(\\Phi)}(\\mathbf{R}_{\\tau})$ using the encoder.   \n3: Compute the free energy D(E\u03b8E) h0 with the energy decoder.   \n4: Compute the energy gradient $\\mathbf{F}\\gets\\nabla_{\\mathbf{R}_{\\tau}}\\dot{\\mathbf{E}}$ using PyTorch\u2019s torch.autograd.   \n5: Apply the stop gradient operation (.detach() method in Pytorch) to the energy gradient:   \n$\\bar{\\mathbf{F}}\\bar{\\leftarrow}\\,\\mathrm{SG}\\left(\\mathbf{F}\\right)$ .   \n6: Extract the molecular feature $\\mathbf{h}_{\\tau}\\leftarrow\\mathcal{E}_{\\mathcal{G},t=\\tau}^{(\\Phi)}(\\mathbf{R}_{\\tau})$ using the encoder.   \n7: Obtain $\\bar{\\mathbf{h}}_{\\tau}\\gets\\mathrm{SG}(\\mathbf{h}_{\\tau})$ using .detach().   \n8: Compute the denoised structure $\\hat{\\mathbf{R}}_{0}\\gets\\mathcal{D}_{\\mathbf{s}}^{(\\mathbf{\\uptheta}_{\\mathrm{s}})}\\big(\\mathrm{SG}(\\bar{\\mathbf{h}}_{\\tau}),\\mathbf{R}_{\\tau}\\big)$ with structure decoder.   \n9: Evaluate the loss $\\begin{array}{r}{L_{\\mathrm{score-cons}}=\\left\\|\\frac{\\sqrt{\\bar{\\alpha}_{\\tau}}\\hat{\\bf R}_{0}-{\\bf R}_{\\tau}}{1-\\bar{\\alpha}_{\\tau}}+\\frac{\\bar{\\bf F}}{k_{\\mathrm{B}}\\mathcal{T}}\\right\\|_{2}^{2}}\\end{array}$   \n10: Determine the gradient $\\nabla_{\\boldsymbol{\\theta}_{\\mathrm{S}}}L_{\\mathrm{score-cons}}$ through automatic differentiation. ", "page_idx": 15}, {"type": "text", "text": "B.2 Model Architecture ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The model is composed of an encoder, and two decoders for structure and energy prediction. ", "page_idx": 15}, {"type": "text", "text": "The encoder $\\mathcal{E}_{\\mathcal{G},t}^{(\\Phi)}(\\mathbf{R})$ is a simple modification to the Graphormer model [2, 64], which additionally adopts diffusion time $t$ embedding into both node features and pairwise-distance attention bias. The encoder consists of 24 layers of Graphormer, with the dimension of both hidden and feed-forward layers set to 768. It utilizes a multi-head attention mechanism with 32 heads and employs 128 Gaussian Basis kernels for enhancing the positional encoding. ", "page_idx": 15}, {"type": "text", "text": "For the time embedding, we implement a SinusoidalPositionEmbeddings module and a TimeStepEncoder module. The former generates time-dependent sinusoidal embeddings, and the latter refines these embeddings using a feed-forward network with a GELU activation function. The resulting time embeddings are then integrated into the node features to inform the model of temporal information. ", "page_idx": 15}, {"type": "text", "text": "Additionally, we incorporate time embeddings into the attention mechanism by computing a structurebased attention bias. This is achieved by calculating the outer product of the time embeddings and using the result as an additive bias in the self-attention layers. This integration allows the model to adapt its attention based on the temporal relationships between nodes in the graph. ", "page_idx": 15}, {"type": "text", "text": "On top of the encoder, the energy decoder $\\mathcal{D}_{\\mathrm{E}}^{(\\theta_{\\mathrm{E}})}(\\mathbf{h})$ is a simple MLP layer concatenated to the invariant node features h. ", "page_idx": 15}, {"type": "table", "img_path": "GnF9tavqgc/tmp/c2c6a95e179b2639354232fc943b09fb254a454a6afa9200b013b5a28eca84a8.jpg", "table_caption": ["Table C.1: Index of main result tables in the paper. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "The structure decoder $\\mathcal{D}_{\\mathrm{S}}^{(\\theta_{\\mathrm{S}})}(\\mathbf{h},\\mathbf{R})$ adopts the GeoMFormer architecture [65], which takes the invariant node features h from the output of the encoder, and the atom coordinates R. The output is denoised atom coordinates which are equivariant w.r.t the input coordinates R. These modules are combined to form the energy and structure prediction models following Eq. (6). ", "page_idx": 16}, {"type": "text", "text": "B.3 Training Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our pre-training procedure is executed in two discrete stages. Initially, the model is subjected to training exclusively utilizing the multi-task loss function, $L_{\\mathrm{multi-task}}$ , for a total of 300,000 iterations. Subsequently, in the second stage, we integrate the proposed consistency loss and the force loss, $L_{\\mathrm{force}}$ , into the training regimen, which then proceeds for an additional 200,000 iterations. All the models are trained with the Adam optimizer [66] with batch size 256. The learning rate is set to $2\\times10^{-4}$ with a linear warm-up phase in the initial 10,000 steps, which followed by a linear decay schedule thereafter. ", "page_idx": 16}, {"type": "text", "text": "The weights of the energy loss and the diffusion denoising loss, i.e., the first and the second terms in Eq. (7), are set to 1.0 and 0.01, respectively. The weights of the optimality consistency loss Eq. (10) and the score consistency loss Eq. (11) are set to 0.1 and 1.0, respectively. ", "page_idx": 16}, {"type": "text", "text": "We employ a sigmoid schedule across 1,000 diffusion time steps for $\\beta_{t}$ , with $\\beta_{0}=1\\times10^{-4}$ and $\\beta_{T}=\\dot{2}\\times\\dot{1}0^{-2}$ . For the optimality consistency loss, the diffusion time step $\\tau$ is sampled uniformly from [400, 700]. For the score consistency loss, the diffusion time step $\\tau$ is sampled uniformly from [5, 300], and $k_{\\mathrm{B}}\\tau$ is set to $0.1\\,\\mathrm{eV}$ . ", "page_idx": 16}, {"type": "text", "text": "The multi-task model is trained on an $8\\times$ Nvidia V100 GPU server for approximately one week. The model with the consistency loss is trained on one $16\\times$ Nvidia V100 GPU server. ", "page_idx": 16}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide additional results under various combinations of settings, and additional metrics and supporting evidence to complement the main results. For clarity, we summarize main result tables in Table C.1 for easier indexing. ", "page_idx": 16}, {"type": "text", "text": "C.1 Additional Fine-Tuning Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section demonstrate more fine-tuning results listed in the main text. Besides the fine-tuned PM6 dataset pre-trained mode, we performed fine-tuning experiments on the model pre-trained with the SPICE force and PM6 subset force datasets, as well. The results are presented in Table C.2. Similar to the case in Table 3, we can again observe that while finetuning improves structure prediction accuracy in all settings (compared to Table 2), pre-training the model with consistency loss still enhances the accuracy universally. ", "page_idx": 16}, {"type": "text", "text": "C.2 Coverage Evaluation for Structure Generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Following the common practice in structure-generation literature, we also test the coverage of the ground-truth structure over model-generated structures. This metric is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{COV}(\\mathbb{S}_{\\mathrm{gen}},\\mathbf{R}_{\\mathrm{eq}})=\\frac{1}{\\lvert\\mathbb{S}_{\\mathrm{gen}}\\rvert}\\Big\\lvert\\{\\hat{\\mathbf{R}}\\in\\mathbb{S}_{\\mathrm{gen}}\\ \\vert\\ \\mathrm{RMSD}(\\mathbf{R}_{\\mathrm{eq}},\\hat{\\mathbf{R}})<\\delta\\}\\Big\\rvert,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "table", "img_path": "GnF9tavqgc/tmp/866a2c04d97b4ba7a811c7a37104cf504960803aaf6da2b9460a8be477d94c11.jpg", "table_caption": ["Table C.2: Test RMSD $\\mathring\\mathrm{A}$ ; lower is better) after finetuning for structure prediction pre-trained by multi-task learning and consistency learning on the PM6 dataset with additional SPICE force data or PM6 subset force data. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "where $\\mathbb{S}_{\\mathrm{gen}}$ denotes the set of generated structures, $\\mathbf{R}_{\\mathrm{eq}}$ denotes the ground-truth equilibrium structure provided from the evaluation dataset, $\\delta$ is a threshold parameter, and $|\\cdot|$ takes the cardinality of a set. Note that since for the task of equilibrium structure prediction, there is only one ground-truth structure, we only evaluate the so-called precision coverage, since the recall coverage (by switching the roles of $\\mathbb{S}_{\\mathrm{gen}}$ and $\\mathbf{R}_{\\mathrm{eq}}$ in the definition Eq. (C.1)) evaluates to 1 in all cases. Here, the RMSD is evaluated after alignment of the two structures by Kabsch algorithm [62]. We choose $\\delta$ as 0.9 and 1.25 for QM9 and PCQ dataset. Same as the RMSD test, we use the same test molecules and sample 200 structure for each molecule. The coverage (COV) of the PM6 dataset model prediction and including force dataset model prediction are shown in Table C.3. The results present consistent results as the RMSD. After adding the consistency loss, all have improvement over the multi-task setting. Besides, incorporating the force data can also improve the accuracy. ", "page_idx": 17}, {"type": "table", "img_path": "GnF9tavqgc/tmp/d5b727b1443a4a0d055745f047fe44c4029f531f6bf5051715b83b7e2ffebec7.jpg", "table_caption": ["Table C.3: Test coverage (higher is better) of structure prediction by multi-task learning and consistency learning on the PM6 dataset, and together with additional SPICE force data or PM6 subset force data. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We also evaluated the coverage on the fine-tuned models. The results are shown in Table C.4, where we again observe that pre-training with consistency loss improves structure prediction accuracy even after fine-tuning. ", "page_idx": 17}, {"type": "text", "text": "C.3 Energy Gap Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our goal is to predict the equilibrium structure $\\mathbf{R}_{\\mathrm{eq}}\\,=\\,\\mathrm{argmin}_{\\mathbf{R}}\\,E_{\\mathcal{G}}(\\mathbf{R})$ . It is desirable for the predicted structure to approximate this state of minimal energy. To quantify the proximity of the predicted structure to the equilibrium state, we introduce the energy gap metric, which is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{EGap}=\\frac{E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R}_{\\mathrm{pred}})-E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R}_{\\mathrm{eq}})}{|E_{\\mathcal{G}}^{(\\Phi,\\Theta_{\\mathrm{E}})}(\\mathbf{R}_{\\mathrm{eq}})|}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The EGap metric serves to evaluate the energy difference between the predicted and equilibrium structures, with a smaller energy gap signifying a more accurate prediction. ", "page_idx": 17}, {"type": "table", "img_path": "GnF9tavqgc/tmp/475f00f9f223bdfaa1f336f69d20d12eb66aeaf4a11afde701ce66984f35a616.jpg", "table_caption": ["Table C.4: Test coverage (higher is better) after finetuning for structure prediction pre-trained by multi-task learning and consistency learning on the PM6 dataset, and together with additional SPICE force data or PM6 subset force data. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "To compute this metric, we randomly select 200 molecules from the intersection of PM6 and PCQ dataset, using the structure from the PCQ dataset as $\\mathbf{R}_{\\mathrm{eq}}$ . The predicted structure ${\\bf R}_{\\mathrm{pred}}$ is generated using the denoising method. The results are presented in Table C.5. We observe that the incorporation of the consistency loss reduces the EGap metric, particularly in cases with additional force labels. These results demonstrate that the consistency loss effectively transfers information from the energy model to the structure model. ", "page_idx": 18}, {"type": "table", "img_path": "GnF9tavqgc/tmp/6ef12f80c0ef027686c07a0652f862080a5b11c80e9d69b5704c0594bd9384e4.jpg", "table_caption": ["Table C.5: Comparison of averaged EGap between structure prediction by multi-task learning and consistency learning. Lower EGap values suggest that the energy of the predicted structure is closer to the theoretical minimum energy. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.4 Validation Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To make sure that the conclusions drawn from the above results are solid, we further provide validation results for energy and structure. ", "page_idx": 18}, {"type": "text", "text": "For the energy, we randomly selected 200 molecules from the intersection of the PM6 dataset (training set) and the PCQ dataset (test set). This choice allows evaluating energy on both PM6 structure and PCQ structure for each molecule, where the former reflects training quality, and the latter reflects the utility for consistency learning of structure prediction. Although the original PCQ dataset [27] does not provide energy labels, we noticed that it is curated based on the PubChemQC Project dataset [26], which provides energy labels on the same PCQ structures under the same DFT settings (B3LYP/6-31G\\*) as the PM6 energy labels. ", "page_idx": 18}, {"type": "text", "text": "The results are listed in Table C.6 and boxplots of the distributions of energy prediction MAE (eV) evaluated on the PM6 structures are shown in Fig. C.1. We can observe that energy prediction on PM6 structures are reasonably accurate, and it gets even better when consistency training is activated. This indicates that consistency training does not sacrifice energy prediction. The energy prediction error is larger on PCQ structures, which is not surprising since the model does not see data with PCQ structures in training. But as we mentioned in the main paper, better generalization can be expected if force data are available, which enriches the information on the energy landscape hence improving the generalization on unseen structures. From the table, we can observe that the energy prediction accuracy on PCQ structures is indeed improved. We also notice that including force data in training even improves energy prediction accuracy on PM6 structures, for which a possible explanation is that introducing additional relevant prediction tasks could help the model learn a more comprehensive representation of the input molecular structure, which in turn helps other tasks. ", "page_idx": 18}, {"type": "text", "text": "Table C.6: Validation MAE (eV) of energy prediction trained by multi-task learning and consistency learning. Validation molecules are randomly selected from the intersection of PM6 and PCQ, and results on both PM6 structures and PCQ structures of the molecules are shown. ", "page_idx": 19}, {"type": "table", "img_path": "GnF9tavqgc/tmp/67fca4ad65da8becd444744cfdacf6f5b8244908188b74cf448526e50c8a0745.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "GnF9tavqgc/tmp/987efebc5815245800a6fdbc7932404c33ece8bddebcc2b0061763f1ba1438a2.jpg", "img_caption": ["Figure C.1: Box plots for the distributions of energy prediction MAE (eV) evaluated on the PM6 structures of randomly selected 200 molecules from the intersection of PM6 and PCQ datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "For structure prediction, we validate the trained model by evaluating the RMSD of generated structures on 200 randomly selected PM6 molecules. From the results shown in Table C.7, we can see that the models trained by both multi-task learning and consistency learning achieve reasonable predictions. ", "page_idx": 19}, {"type": "text", "text": "Table C.7: Validation RMSD $(\\mathring{\\mathbf{A}})$ evaluated on the PM6 validation set of structure prediction trained by multi-task learning and consistency learning on the PM6 dataset, and together with additional SPICE force data or PM6 subset force data. Predicted structures are generated by the DDIM method. ", "page_idx": 19}, {"type": "table", "img_path": "GnF9tavqgc/tmp/849b8a28b70779237bf07174e52cf63b34cd08ebb318b59c218a636626e1d294.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.5 Standard Deviations and Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To evaluate the significance of our comparison results, for each experimental setup, we repeated five independent runs using different random seeds (666666, 666667, 666668, 666669, 666670). The reported results in the main paper are the means of the five repeats, while the standard deviations are listed in Table C.8 and Table C.9 here, which cover the results for the pre-trained models and for fine-tuned models, respectively. We can see that all the standard deviations are around $0.003\\,\\mathrm{\\AA}$ , which is clearly lower than the improvements by consistency training, hence indicating the significance of the effectiveness of the proposed methods. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "To more seriously assess the statistical significance, we also conducted a paired t-test for each comparison. The p-values are listed in Table C.10, where each number represents the probability of the observed results under the null hypothesis that the means by multi-task learning and consistency learning are the same, hence the lower value the more significant that consistency learning outperforms multi-task learning. Nearly all p-values are well below the 0.05 significance threshold, indicating that the improvement by consistency learning is significant. Exceptional cases almost all correspond to the setting where the result in each repeat is the \u201cMin\u201dimum RMSD over 200 generated samples, in which case multi-task learning may also has a chance to hit the target structure. ", "page_idx": 20}, {"type": "table", "img_path": "GnF9tavqgc/tmp/61310a27471a6f7e415eff3328bdb1181d5ef99c7e3401c593f9d50536f2cfeb.jpg", "table_caption": ["Table C.8: Standard deviations for the test RMSD $(\\mathring{\\mathbf{A}})$ of structure prediction by multi-task learning and consistency learning on the PM6 dataset (corresponding to Table 1), and together with additional SPICE force data or PM6 subset force data (corresponding to Table 2). "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "GnF9tavqgc/tmp/6ec89864f7a07320472031f3d8c90ff178b0f54c0bf77ea31cfeacbbe0bb39f4.jpg", "table_caption": ["Table C.9: Standard deviations for the test RMSD $(\\mathring{\\mathbf{A}})$ after finetuning for structure prediction pretrained by multi-task learning and consistency learning on the PM6 dataset (corresponding to Table 3), and together with additional SPICE force data or PM6 subset force data (corresponding to Table C.2). "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "GnF9tavqgc/tmp/29e9e3e7ced1f162d18a0d92379704f06a3aeb459144524f1a93c644382e5ad3.jpg", "table_caption": ["Table C.10: Paired t-test p-values on structure prediction RMSD means over 5 repeats (standard deviations are shown in Tables C.8 and C.9) corresponding to the results in Table 1 (row 1, pretraining on PM6), Table 2 (rows 2 and 3, pre-training on PM6 together with force labels), and Table 3 (row 4, pre-training on PM6 then finetuning). Values lower than the 0.05 significance threshold are shown in bold. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.6 Evaluation on Dissimilar Molecules ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further consolidate the effectiveness of our method, we give a closer look into the influence of the similarity of the test dataset to the training dataset. Recall that we have excluded identical molecules appearing in the training dataset from the test dataset, but there remains the possibility that the test dataset may still contain similar molecules as those in the training dataset. For this, we first investigate the Tanimoto similarity [67] between the training and test datasets. This similarity can be computed from the molecular graphs of two molecules, while considering structural similarity between the two molecules. We plot in Fig. C.2 where each column visualizes the count of PCQ test molecules that have a certain portion of similar (Tanimoto similarity $>0.7$ ) molecules in the PM6 training dataset. We can observe that most (almost all) of the test molecules only has less than $1\\times10^{-7}$ $(2.5^{-}\\times10^{-7})$ similar molecules in PM6. This indicates that excluding identical molecules appearing in the training dataset from the test dataset already makes the test dataset sufficiently dissimilar to the training dataset, hence the presented results are valid prediction evaluations that are not close to \u201cmemorizing the training molecules\u201d. ", "page_idx": 21}, {"type": "image", "img_path": "GnF9tavqgc/tmp/fdf8e48351b32ff97797b61274f4c2afcfc04abb460036926d79e6950625874f.jpg", "img_caption": ["Figure C.2: Histogram showing the distribution of the portion of similar (Tanimoto similarity $>0.7$ ) molecules in PM6 (the training dataset) over the $200\\;\\mathrm{PCQ}$ test molecules. Note that the $\\mathbf{X}_{\\mathrm{~}}$ -axis is scaled by $1\\times10^{-6}$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "For a completely sanitized evaluation on dissimilar molecules, we provide the results evaluated on PCQ test molecules that do not have any similar (Tanimoto similarity $>0.7$ ) molecules in the PM6 (training) dataset. Such molecules make up $49\\%$ of the total $200\\,\\mathrm{PCQ}$ test molecules. Table C.11 shows the results in both RMSD and Coverage using the denoising generation method, corresponding to the settings in Tables 1 and C.3 (row block 1) (pre-training on PM6), and Tables 2 and C.3 (row blocks 2 and 3) (pre-training on PM6 together with force labels). The results confirmed the advantage of consistency learning on dissimilar molecules that cannot rely on memorizing the training dataset, which further consolidates the conclusion. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "table", "img_path": "GnF9tavqgc/tmp/07e6906ac304cd94f2e8afb1fa26c9724df5259a6d34992c0e2aeae17eed87f3.jpg", "table_caption": ["Table C.11: Test RMSD ( $\\mathring\\mathrm{A}$ ; lower is better) and coverage (higher is better) on dissimilar molecules for structure prediction pre-trained by multi-task learning and consistency learning on the PM6 dataset, and together with additional SPICE force data or PM6 subset force data. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have accurately reflect the paper\u2019s contributions and scope in the abstract and introduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We have discussed the limitations of the work in the Sec. 5 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided complete proof for the assumptions in Sec. 3 and Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have disclosed all the needed information for reproducibility in Appendix B. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Releasing data and code requires an internal asset releasing review process in our organization. We have started the process, but cannot guarantee availability during the review period. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have included all details of the model architecture, data processing, and hyperparameter settings in Appendix B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have report the standard deviations and statistical significance of the main results in Appendix C.5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The required computational resources are provided in Appendix B.3. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This work develops techniques for better solving molecular science tasks using machine learning. The techniques could potentially foster the development of related industries, e.g., drug discovery and material design. Possible caveats include inaccurate generated structures or predicted properties which may lead to failure in downstream research. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper has properly credited and cited the code and data used in this paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The new asset in this paper is the in-house generated force labels on a subset of PM6 molecules. Releasing the new asset requires internal releasing review process in our organization. We cannot guarantee releasing the asset during the review period. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing experiments nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not involve research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]