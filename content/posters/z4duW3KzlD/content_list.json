[{"type": "text", "text": "Gated Inference Network: Inference and Learning State-Space Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hamidreza Hashempoor Seoul National University Department of Electrical and Computer Engineering hamidreza.hashemp@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Wan Choi Seoul National University Department of Electrical and Computer Engineering wanchoi@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper advances temporal reasoning within dynamically changing highdimensional noisy observations, focusing on a latent space that characterizes the nonlinear dynamics of objects in their environment. We introduce the Gated Inference Network (GIN), an efficient approximate Bayesian inference algorithm for state space models (SSMs) with nonlinear state transitions and emissions. GIN disentangles two latent representations: one representing the object derived from a nonlinear mapping model, and another representing the latent state describing its dynamics. This disentanglement enables direct state estimation and missing data imputation as the world evolves. To infer the latent state, we utilize a deep extended Kalman filter (EKF) approach that integrates a novel compact RNN structure to compute both the Kalman Gain (KG) and smoothing gain (SG), completing the data flow. This design results in a computational cost per step that is linearly faster than EKF but introduces issues such as the exploding gradient problem. To mitigate the exploding gradients caused by the compact RNN structure in our model, we propose a specialized learning method that ensures stable training and inference. The model is then trained end-to-end on videos depicting a diverse range of simulated and real-world physical systems, and outperforms its counterparts \u2014RNNs, autoregressive models, and variational approaches\u2014 in state estimation and missing data imputation tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A state-space model is a type of graphical model that effectively represents noise-afflicted data [1]. Typically, the objective is to conduct inference, which involves obtaining accurate estimates of the posterior distribution of latent states or noise-free measurements. In videos, inferring the state space presents a complex challenge owing to the high dimensionality of noisy observations, which only offer partial information about the states. Previous methods have proposed various inference approaches such as sampling [2], variational inference [3], or belief propagation [4]. Within the framework of a hidden Markov process, classical methods like the celebrated Kalman fliter (KF) and smoother [5, 6], along with their extensions like EKF and UKF, [7, 8] have been employed to address the posterior inference problem. However, the reliance of these methods on accurate knowledge of the underlying dynamics, coupled with computationally intensive matrix inversions, poses challenges in scalability to high-dimensional latent spaces. ", "page_idx": 0}, {"type": "text", "text": "To address these issues, this paper distinguishes between the sensory observation and its dynamics by disentangling two representations: a transformed observation ${\\bf w}_{t}$ obtained by mapping the original sensory observation $\\mathbf{o}_{t}$ through a nonlinear function modeled by neural networks, and the latent state space $\\mathbf{x}_{t}$ describing the dynamics of ${\\bf w}_{t}$ at time step $t$ . To infer high dimensional state space (dynamics) $\\mathbf{x}_{t}$ , we introduce filtering and smoothing techniques that build upon classical EKF updates but incorporate flexible function estimators without relying on a constrained graphical model. We model temporal dynamics with a linearized Gaussian SSM, which is adapted to accommodate complex dynamics. To address non-linearity and the multiplicity of dynamics, we learn multiple transition and emission matrices, each modeling a unique dynamical scenario. Then, the weight of each matrix during inference is determined by the past state $\\mathbf x_{t-1}$ through a neural network. To prevent mode collapse and ensure the system\u2019s ability to capture diverse dynamics, we introduce a loss term proportional to the KL divergence of the transition matrices during training. This design enables our model to handle various state scenarios, similar to approaches found in the switching linear dynamics systems (SLDS) literature [9]. Unlike EKF, which requires precise characterization and modeling of the underlying dynamics, we operate under the assumption that noise statistics and the underlying physical dynamics of SSM are entirely unknown. ", "page_idx": 1}, {"type": "text", "text": "The main drawback of the EKF is that it takes $\\mathcal{O}(n^{3})$ time per step, because we need to invert the posterior covariance matrix with size of $n$ . This makes the method slow to use. Moreover, we identify the computation of the KG and SG in the (extended) KF as critical components that rely on noise statistics and domain knowledge. In this paper, we efficiently model KG and SG using GRU cells, bypassing the need for matrix inversion in the KF flow. Consequently, the GIN algorithm achieves a time complexity of $O(\\beta^{2}n^{2})$ per step, making it linearly faster than EKF. To capitalize on the sparsity of the covariance matrix, we employ a convolutional approach that further reduces the size of $n$ relative to $\\beta$ , where $0<\\beta<1$ . However, the nonlinear structure of GRU cells can lead to gradient explosion [10], a phenomenon that occurs when the dynamics undergo significant changes as GRU parameters traverse bifurcation points [11] during the learning process. To address this issue, we propose a learning method based on an analysis of GRU cell dynamics. This method aims to prevent parameter values from reaching bifurcation points by stabilizing the dynamics of GRU cells, thereby addressing the issue of gradient explosion. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our primary contribution is a novel algorithm designed for the recursive inference of SSMs with arbitrary nonlinear (and possibly non-Gaussian) emission and transition models, with high-dimensional noisy stimuli observation. This algorithm utilizes a linearized Gaussian dynamics approach, with parameter estimation conducted through neural networks. This means that we can apply the recursive Bayesian updates, akin to the Kalman fliter and Kalman smoother. We introduce a likelihood for inferring the states (dynamics) and another likelihood for inferring the high-dimensional images (of video). The objective is maximized in a supervised manner, depending on the task. We propose learning schemes for GRU cells to address issues related to gradient explosion and instability. Finally, we introduce a loss term proportional to the KL divergence of the learned transition matrices to prevent the system from becoming stuck in mode collapse. ", "page_idx": 1}, {"type": "text", "text": "To substantiate our claims, we conduct five experiments. First, we simulate a nonlinear dynamic system using the pendulum sequence video, a common benchmark in this literature, to demonstrate our model\u2019s ability to infer both dynamics and images. Second, we introduce a more challenging experiment of a simulated nonlinear double pendulum, where the video sequence is heavily distorted with noise, showcasing our model\u2019s resilience of dynamics estimation and image imputation to noisy observations. Third, we present a switching dynamics irregular bouncing ball experiment to illustrate our model\u2019s ability to handle multiple dynamic scenarios. Fourth, we perform visual odometry using the KITTI dataset, demonstrating the practical applicability of our methods in real-world scenarios. Finally, we assess the effectiveness of our proposed gradient explosion handling scheme by evaluating convergence across various simulation seeds. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "EKF and UKF represent early extensions of the original Kalman filter, allowing for nonlinear transitions and emissions. These methods, categorized as model-based (MB) algorithms, rely on precise knowledge and modeling of the underlying dynamics within a fully characterized SSM. Consequently, the performance of these MB methods is heavily contingent on the accuracy of domain knowledge and model assumptions. Recent approaches, such as BackpropKF [12] and SIN [13], perform EKF in the latent state using deep encoders for feature extraction. However, they face similar limitations as EKF, including computational complexity and scalability. To address these issues, a common approach involves considering a diagonal covariance matrix in the KF flow as in [14] and [15]. However, we aim to find a richer approximation to the posterior. ", "page_idx": 1}, {"type": "image", "img_path": "z4duW3KzlD/tmp/75525ed3af61e7ead281a77151711c8b8abd0e1c5a8dc119ecbef30f9881f75c.jpg", "img_caption": ["Figure 1: Generated sequences from GIN in the irregular polygon environments. The videos are shown as single images, with color intensity representing the incremental sequence index. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "A significant body of work, including Variational Auto-Encoders (VAE) [3], Embed to Control (E2C) [16], and Importance Weighted VAE (IWVAE) [17], integrates deep learning with variational inference (VI), but lacks memory cells and recurrent structures for handling imputation tasks. To address this, EM-based VI approaches like Structure VAE (SVAE) [18], Kalman VAE (KVAE) [19], Disentangled VAE (DVAE) [20], Extended KVAE (EKVAE) [21], Robust VAE [22], and Markovian VAE (MVAE) [23] use original KF equations for filtering and smoothing. However, they cannot directly optimize states (dynamics), as noted in RKN [24] and CRU [25]. Classical memory networks like LSTMs [26], GRUs [27], and simple RNNs [28] infer latent states but fail to provide insights into uncertainties and dynamics. ", "page_idx": 2}, {"type": "text", "text": "LatentNet [29], KalmanNet [30] and SSI-SSM [31] utilize GRU in their structures for state updates, similar to the parameterization used in the GIN. However, these models typically necessitate access to complete or partial dynamics information, i.e. they are MB, and their reliance on vanilla GRU cells can cause instability of the SSM. Liu et al. [32] proposes a disentanglement model for audio data, which shares similarities with the GIN. However, their parameterization and objectives differ from those of the GIN, and they do not utilize compact RNNs in their inference structure. ", "page_idx": 2}, {"type": "text", "text": "We cover the most relevant works in this section. A more detailed discussion, including a comprehensive comparison of recent related works like System Identification (SI) with details of EM algorithm, auto-regressive (AR) and SLDS models, is provided in Appendix A.8.3 and Table 7. This builds upon the findings of the RKN with additional complements. We also conduct an empirical complexity analysis in appendix A.8.2, to evaluate the computational efficiency of our method compared to the discussed approaches. This analysis involved measuring the execution time per iteration using a clock on the wall as the benchmark. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Linear Gaussian state space models. Linear Gaussian state space models (LGSSMs) are commonly employed to model vectors of observed time series, denoted as $\\mathbf{w}~=~[\\mathbf{w}_{1},...,\\mathbf{w}_{T}]$ . LGSSMs excel in filtering and smoothing tasks. They model the first-order Markov process on the state space $\\mathbf{x}=[\\mathbf{x}_{1},...,\\mathbf{x}_{T}]$ , which may also incorporate external control inputs $\\mathbf{u}=[\\mathbf{u}_{1},...,\\mathbf{u}_{T}]$ under the multivariate normality assumption of the state: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\gamma_{t}}(\\mathbf{x}_{t}|\\mathbf{x}_{t-1},\\mathbf{u}_{t})=\\mathcal{N}(\\mathbf{x}_{t};\\mathbf{F}_{t}\\mathbf{x}_{t-1}+\\mathbf{B}_{t}\\mathbf{u}_{t},\\mathbf{Q}_{t}),\\quad p_{\\gamma_{t}}(\\mathbf{w}_{t}|\\mathbf{x}_{t})=\\mathcal{N}(\\mathbf{w}_{t};\\mathbf{H}_{t}\\mathbf{x}_{t},\\mathbf{R}_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\gamma_{t}$ represents the parameters of the system at time $t$ , encapsulating information from matrices $\\mathbf{F}_{t},\\mathbf{B}_{t},\\mathbf{H}_{t},\\mathbf{Q}_{t}$ and ${\\bf R}_{t}$ , which correspond to the state transition, control, emission, process noise covariance, and observation noise covariance, respectively. At each time step, the transition and emission procedures are subject to distortion caused by process noise and observation noise, following distributions of $\\mathcal{N}(\\mathbf{0},\\mathbf{Q}_{t})$ and $\\mathcal{N}(\\mathbf{0},\\mathbf{R}_{t})$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Filtering and smoothing algorithms. The goal of filtering is to infer the posterior distribution $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t},\\mathbf{u}_{1:t})$ , while, the smoothing aims to infer the smoothing distribution $p_{\\gamma_{1:T}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:T},\\mathbf{u}_{1:T})$ given the whole observations. Without loss of generality, we drop the input variable $\\mathbf{u}$ in the rest of paper, while one can condition all of the distributions on the input variable in the case of its existence. Considering the prior state parameterization $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t-1})=$ $\\mathcal{N}(\\mu_{t|t-1},\\Sigma_{t|t-1})$ and relying on [33], the filtering and smoothing parameterizations are as: ", "page_idx": 2}, {"type": "image", "img_path": "z4duW3KzlD/tmp/f9baaeeed0e6f6607838b12e6d308a66c667d8f7d2d0a6510542e786e3d71e70.jpg", "img_caption": ["Figure 2: Graphical model. Dashed nodes are task dependent output. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "z4duW3KzlD/tmp/4fedce4e8ef3b40380554f3962bc51e9207f9684d979a769da8cfeaf1c991bfd.jpg", "img_caption": ["Figure 3: The GIN as a HW model for system identification. By appropriate structure selection for $e(.)$ and $d(.)$ , the GIN can handle high dimensional observations. The relation between the internal variables, ${\\bf w}_{t}$ and $\\mathbf{x}_{t}$ , is simulated by the transition block. "], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t})=\\mathcal{N}\\bigg(\\mu_{t|t-1}+\\mathbf{K}_{t}\\big[\\mathbf{w}_{t}-\\mathbf{H}_{t}\\mu_{t|t-1}\\big],\\ \\ \\mathbf{\\Sigma}_{t|t-1}-\\mathbf{K}_{t}\\mathbf{S}_{f}\\mathbf{K}_{t}^{T}\\bigg)=\\mathcal{N}\\big(\\mu_{t|t},\\boldsymbol{\\Sigma}_{t|t}\\big)}\\\\ &{\\qquad p_{\\gamma_{1:T}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:T})=\\mathcal{N}\\bigg(\\mu_{t|t}+\\mathbf{J}_{t}\\big[\\mu_{t+1|T}-\\mathbf{F}_{t+1}\\mu_{t|t}\\big],\\ \\ \\mathbf{\\Sigma}_{\\mathbf{Z}_{t|t}}+\\mathbf{J}_{t}\\mathbf{S}_{s}\\mathbf{J}_{t}^{T}\\bigg)=\\mathcal{N}\\big(\\mu_{t|T},\\boldsymbol{\\Sigma}_{t|T}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\mathbf{K}_{t}=\\boldsymbol{\\Sigma}_{t|t-1}\\mathbf{H}_{t}^{T}.\\big[\\mathbf{H}_{t}\\boldsymbol{\\Sigma}_{t|t-1}\\mathbf{H}_{t}^{T}+\\mathbf{R}_{t}\\big]^{-1}\\,,\\ \\ \\ \\ \\mathbf{J}_{t}=\\boldsymbol{\\Sigma}_{t|t}\\mathbf{F}_{t+1}^{T}\\boldsymbol{\\Sigma}_{t+1|t}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\mathbf{S}_{f}=\\left[\\mathbf{H}_{t}\\pmb{\\Sigma}_{t|t-1}\\mathbf{H}_{t}^{T}+\\mathbf{R}_{t}\\right]$ and $\\mathbf{S}_{s}=\\left[\\pmb{\\Sigma}_{t+1|T}-\\pmb{\\Sigma}_{t+1|t}\\right]$ . In (4), $\\mathbf{K}_{t}$ and $\\mathbf{J}_{t}$ are KG and SG, respectively. See appendix A.1 for a full derivation of filtering-smoothing distributions. ", "page_idx": 3}, {"type": "text", "text": "4 Gated Inference Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The background section defines key elements for the GIN: original observations $\\mathbf{o}_{1:T}$ , task-dependent output which can be either the original denoised-imputed observations $\\mathbf{o}_{1:T}$ or physical system\u2019s states $\\mathbf{s}_{1:T}$ , transferred observations $\\mathbf{w}_{1:T}\\in\\mathbb{R}^{m\\times T}$ , latent states (dynamics) $\\mathbf{x}_{1:T}\\overset{\\cdot}{\\in}\\mathbb{R}^{n\\times\\check{T}}$ , noise covariance of transferred observations $\\mathbf{R}_{1:T}$ with diagonals $\\mathbf{r}_{1:T}\\in\\mathbb{R}^{\\dot{m}\\times T}$ , noise covariance of states process $\\mathbf{Q}_{1:T}$ with diagonals $\\mathbf{q}_{1:T}\\,\\in\\,\\mathbb{R}^{n\\times T}$ , and the parameters of the $\\mathrm{SSM}\\ \\gamma_{1:T}$ . We consider the dimensions of the transferred observation and state at each time step to be $m$ and $n$ , respectively. The states evolve through transition distribution $\\begin{array}{r}{p_{\\gamma_{1:T}}(\\mathbf{x}_{1:T})\\,=\\,p(\\mathbf{x}_{1})\\prod_{t=2}^{T}p_{\\gamma_{t}}(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\end{array}$ . Each ${\\bf w}_{t}$ is assumed to be drawn from noisy emission probability $p_{\\gamma_{t}}(\\mathbf{w}_{t}|\\mathbf{x}_{t})$ , then the generative process is assumed to factorize as $\\begin{array}{r}{p_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T})\\ =\\ p_{\\gamma_{1:T}}(\\mathbf{x}_{1:T})\\prod_{t=1}^{T}p_{\\gamma_{t}}(\\mathbf{w}_{t}|\\mathbf{x}_{t})}\\end{array}$ . This factorisation imposes emission conditional independence like $\\mathbf{w}_{t}\\perp\\mathbf{\\Phi}(\\mathbf{w}_{-t},\\mathbf{x}_{-t})|\\mathbf{x}_{t}$ , where $\\begin{array}{r}{\\left(\\mathbf{w}_{-t},\\mathbf{x}_{-t}\\right)=\\left(\\mathbf{w}_{(1:t-1,t+1:T)},\\mathbf{x}_{(1:t-1,t+1:T)}\\right)}\\end{array}$ . Common models that assume conditional independence include linear dynamical systems, hidden Markov models, but also nonlinear SSMs with higher-order Markov chains in latent space. Graphical model of the GIN is in figure 2, where output (one of colored nodes) is generated, contingent upon the task. In all of our scenarios, the parameters of the SSM, $\\gamma_{1:T}$ , are completely unknown. ", "page_idx": 3}, {"type": "text", "text": "From SI perspective, the GIN operates similarly to a Hammerstein-Wiener (HW) model [34], employing non-linear transfer functions $e(.)$ and $d(.)$ (Figure 3). Leveraging encoder-decoder structures for $\\bar{e(.)}$ and $d(.)$ , the GIN conducts the transfer between original sensory observations $\\mathbf{o}_{1:T}$ and lowerdimensional representations $\\mathbf{w}_{1:T}$ and task dependent output. The transition block in Figure 3 evolves states using the proposed deep EKF approach, efficiently approximating flitering-smoothing distributions while ensuring system stability with the imposed constraint. Further details and formulations are provided in the inference section. The transition block depicted in Figure 4. ", "page_idx": 3}, {"type": "text", "text": "5 Parameterization and Inference ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given original observations $\\mathbf{o}_{1:T}$ and transferred observations $\\mathbf{w}_{1:T}$ , our aim is to infer the latent states $\\mathbf{x}_{1:T}$ . To accomplish this, we seek to infer the marginal distributions $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t})$ for the online inference approach (filtering) and $p_{\\gamma_{1:T}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:T})$ for the full inference approach (smoothing). We introduce an advantageous prediction parameterization as $p_{\\gamma_{t}}(\\mathbf{x}_{t}|\\mathbf{x}_{t-1},\\mathbf{w}_{1:t-1})\\overset{}{=}\\mathcal{N}(\\bar{\\mathbf{F}}_{t}\\mathbf{x}_{t-1},\\mathbf{Q}_{t})$ , where $\\mathbf{x}_{t-1}$ is sampled from the last filtering distribution, $p_{\\gamma_{1:t-1}}(\\mathbf{x}_{t-1}|\\mathbf{w}_{1:t-1})$ . Then, we obtain the prior distribution at time $t$ , $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\bar{\\mathbf{w}}_{1:t-1})\\,=$ $\\mathcal{N}(\\mathbf{F}_{t}\\mu_{t-1\\mid t-1},\\mathbf{F}_{t}\\boldsymbol{\\Sigma}_{t-1\\mid t-1}\\mathbf{F}_{t}^{T}+\\mathbf{Q}_{t})\\ =\\ \\mathcal{N}(\\mu_{t\\mid t-1},\\boldsymbol{\\Sigma}_{t\\mid t-1})$ , by marginalizing out $\\mathbf{x}_{t-1}$ from $\\begin{array}{r}{\\int p_{\\gamma_{t}}(\\mathbf{x}_{t}|\\mathbf{x}_{t-1},\\mathbf{w}_{1:t-1})p_{\\gamma_{1:t-1}}(\\mathbf{x}_{t-1}|\\mathbf{w}_{1:t-1})d\\mathbf{x}_{t-1}}\\end{array}$ 1 integration. The Gaussianity of $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t-1})$ results from the Gaussianity of prediction parameterization. After obtaining $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t-1})$ and observing ${\\bf w}_{t}$ , it is feasible to derive the filtering parameterization using equation (2). Once all transferred observations $\\mathbf{w}_{1:T}$ are available, backward induction can be employed to propagate to previous states using the chain rule. This procedure, known as smoothing, is parameterized with (3). These parameterizations provide valuable insights into two key aspects: 1) appropriately modeling $\\gamma$ using neural networks, and 2) illustrating a tractable method to parameterize KG and SG and construct distributions approximations. These approximations serve the basis for constructing output. ", "page_idx": 3}, {"type": "image", "img_path": "z4duW3KzlD/tmp/31e021bd0d8b41921056934e304a29629113ef0e5ee7b7f77c327ca006b5f6d5.jpg", "img_caption": ["Figure 4: Transition block of figure 3 in details. In each time step, the last posterior $\\mathbf{x}_{t-1}|\\mathbf{w}_{1:t-1}$ is fed to the Dynamic Net to compute $\\gamma_{t}$ . In the filtering steps, by using the last posterior $\\mathbf{x}_{t-1}|\\mathbf{w}_{1:t-1}$ and the observation ${\\bf w}_{t}$ , the next posterior $\\mathbf{x}_{t}|\\mathbf{w}_{1:t}$ is obtained. Having posterior $\\mathbf{x}_{t}|\\mathbf{w}_{1:t}$ and the next smoothing state $\\mathbf{x}_{t+1}|\\mathbf{w}_{1:T}$ , applying smoothing for the current state is feasible so that the smoothing state $\\mathbf{x}_{t}|\\mathbf{w}_{1:T}$ is obtained. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Learning $\\gamma.$ . To handle multiple dynamic scenarios, we learn $K$ sets of state transition and emission matrices $\\check{\\mathbf{F}}^{k}$ and $\\check{\\mathbf{H}}^{k}$ , each representing a distinct dynamic status. These matrices are combined with state-dependent coefficients $\\alpha^{k}(\\mathbf{x}_{t-1})$ as : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{F}}_{t}=\\sum_{k=1}^{K}\\alpha^{k}(\\mathbf{x}_{t-1})\\check{\\mathbf{F}}^{k},\\quad\\hat{\\mathbf{H}}_{t}=\\sum_{k=1}^{K}\\alpha^{k}(\\mathbf{x}_{t-1})\\check{\\mathbf{H}}^{k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We model $\\alpha^{k}({\\bf x}_{t-1})$ with a $K$ dimension softmax output neural network called the Dynamic Net. It takes the last flitering state $\\mathbf{x}_{t-1}$ as input, containing the system\u2019s history with lower noise distortion than the transferred observations $\\mathbf{w}_{t-1}$ . This choice enhances noise robustness, demonstrated in our experiments with time-correlated noise (See Appendix A.6). ", "page_idx": 4}, {"type": "text", "text": "In the graphical model shown in Figure 2, we observe two paths for belief propagation from $\\mathbf{x}_{t-1}$ to $\\mathbf{x}_{t}$ . The first path, $\\mathbf x_{t-1}\\to\\mathbf x_{t}$ , linked with $\\hat{\\mathbf{F}}_{t}$ . The second path involves an intermediate variable $\\mathbf{q}_{t}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{Q}_{t})$ : $\\mathbf x_{t-1}\\to\\mathbf q_{t}\\to\\mathbf x_{t}$ . Since learned $\\hat{\\mathbf{F}}_{t}$ transfers information from $\\mathbf X_{t-1}$ to $\\mathbf{x}_{t}$ , we argue that it can capture effects similar to those of $\\mathbf{Q}_{t}$ , as both are intended to convey message between $\\mathbf x_{t-1}$ and $\\mathbf{x}_{t}$ [1]. By incorporating $\\hat{\\mathbf{F}}_{t}$ from equation (5) into the prior state distribution described earlier as $p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t-1})=\\mathcal{N}(\\mathbf{F}_{t}\\mu_{t-1|t-1},\\mathbf{F}_{t}\\boldsymbol{\\Sigma}_{t-1|t-1}\\mathbf{F}_{t}^{T}+\\mathbf{Q}_{t})$ , we can neglect the $\\mathbf{Q}_{t}$ term in the covariance, as its influence is accounted for by the learned $\\hat{\\mathbf{F}}_{t}$ . There are two other meaningful parameterizations for the process noise matrix. First involves direct parameterization by $\\mathbf{x}_{t-1}$ using a neural network. Second approach is to parameterize it recursively using $\\mathbf{x}_{t-1}$ and $\\mathbf q_{t-1}$ , resulting in a new graphical model with an edge from ${\\bf q}_{t-1}\\rightarrow{\\bf q}_{t}$ (See Appendix A.2). Both parameterizations are included in our results, presented in the appendix, to show their effectiveness. ", "page_idx": 4}, {"type": "text", "text": "Finally, the diagonal elements of the transferred observation noise vector $\\mathbf{r}_{t}$ are directly mapped from the original observation space using the encoder function $e(.)$ shown in Figure 3. The mapping employs an activation function, $\\mathrm{elu}\\!+\\!1$ , to handle the positivity of the diagonal elements. ", "page_idx": 4}, {"type": "text", "text": "Filtering and Smoothing Approximation. We consider $G R U^{K G}$ network to approximate KG as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{K}}_{t}=\\hat{\\Sigma}_{t|t-1}\\hat{\\mathbf{H}}_{t}^{T}\\mathbf{M}_{t}\\mathbf{M}_{t}^{T},\\quad\\mathbf{M}_{t}=G R U^{K G}\\big([\\mathrm{Conv}(\\hat{\\Sigma}_{t|t-1}),\\mathbf{r}_{t}]\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Equation (6), $\\mathrm{Conv}(.)$ represents a zero bias convolutional layer with pooling, employed to deal with sparsity of covariance matrix and extract its relevant information while reducing its size. The $[,]$ symbol denotes the concatenation operator. Furthermore, the presence of a positive $\\mathbf{r}_{t}$ vector and the consideration of the Cholesky factor, $\\mathbf{M}_{t}\\mathbf{M}_{t}^{T}$ in (6), ensure the resulting covariance matrices maintain positive definiteness. This parameterization construct a new filtering distribution $q_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t})=$ $\\mathcal{N}(\\hat{\\mu}_{t|t},\\hat{\\Sigma}_{t|t})$ that is an approximation of (2). Consequently, we consider the approximated prior distribution as $q_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:t-1})\\,={\\mathcal{N}}(\\hat{\\mathbf{F}}_{t}\\hat{\\mu}_{t-1|t-1},\\hat{\\mathbf{F}}_{t}\\hat{\\Sigma}_{t-1|t-1}\\hat{\\mathbf{F}}_{t}^{T})\\,={\\mathcal{N}}(\\hat{\\mu}_{t|t-1},\\hat{\\Sigma}_{t|t-1})$ , where $(\\hat{\\mu}_{t|t-1},\\hat{\\Sigma}_{t|t-1})$ are used in (7). ", "page_idx": 5}, {"type": "text", "text": "After obtaining flitering states from $q_{\\gamma_{1:t}}\\bigl(\\mathbf{x}_{t}\\bigl|\\mathbf{w}_{1:t}\\bigr)$ , we use $G R U^{S G}$ network to approximate SG in a similar way we used $G R U^{K G}$ in (6) as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{J}}_{t}=\\hat{\\pmb{\\Sigma}}_{t|t}\\hat{\\mathbf{F}}_{t+1}^{T}\\mathbf{N}_{t}\\mathbf{N}_{t}^{T},\\,\\,\\,\\,\\,\\,\\,\\mathbf{N}_{t}=G R U^{S G}\\big(\\mathrm{Conv}(\\hat{\\pmb{\\Sigma}}_{t+1|t})\\big)}\\\\ &{\\hat{\\mu}_{t|T}=\\hat{\\mu}_{t|t}+\\hat{\\mathbf{J}}_{t}\\big[\\hat{\\mu}_{t+1|T}-\\hat{\\mathbf{F}}_{t+1}\\hat{\\mu}_{t|t}\\big],\\,\\,\\,\\,\\,\\,\\,\\hat{\\pmb{\\Sigma}}_{t|T}=\\hat{\\pmb{\\Sigma}}_{t|t}+\\hat{\\mathbf{J}}_{t}\\big(\\hat{\\pmb{\\Sigma}}_{t+1|T}-\\hat{\\mathbf{F}}_{t+1}\\hat{\\Sigma}_{t|t}\\hat{\\mathbf{F}}_{t+1}^{T}\\big)\\hat{\\mathbf{J}}_{t}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first smoothing state is set to the last filtering state. The new smoothing distribution $q_{\\gamma_{1:T}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:T})=\\mathcal{N}(\\hat{\\mu}_{t|T},\\hat{\\Sigma}_{t|T})$ is an approximation of the exact smoothing distribution in (3). For a GRU cell with an input size of $i$ and a hidden state size of $h$ , the computational complexity in the forward pass is ${\\mathcal{O}}({\\bar{3}}h(h+i+3))$ , which scales linearly with the input size [35]. Considering $\\mathrm{Conv}(\\Sigma)\\in\\mathbb{R}^{\\beta^{2}n^{2}}$ as the input of our GRU cells in (6) and (8) with $\\beta\\in[0,1]$ as the pooling ratio, the forward pass of GIN for one time step has a time complexity of $O(3h\\bar{\\beta}^{2}n^{2})$ , where $n\\gg h$ . Compared with LGSSM matrix inversion time complexity of $\\mathcal{O}(n^{3})$ , GIN is faster by a factor of $\\frac{n}{3h\\beta^{2}}$ , which is crucial in high-dimensional regimes. ", "page_idx": 5}, {"type": "text", "text": "6 Fitting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the state estimation task, the output from $d(.)$ in figure 3 equals s, the physical system\u2019s states. However, in the imputation task, output of $d(.)$ is same as o since the original observation is reconstructed (see decoder structure for each task in A.8.1). The conditional distributions $p(\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ and $q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T}|\\mathbf{w}_{1:T})$ are modeled using an encoder $e(.)$ and smoothing parameterization, respectively. Meanwhile, the conditional distributions $p(\\mathbf{s}_{1:T}|\\mathbf{x}_{1:T},\\mathbf{w}_{1:T},\\mathbf{o}_{1:T})$ and $p\\big(\\mathbf{o}_{1:T}\\big|\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}\\big)$ are represented by the decoder $d(.)$ for the tasks of state estimation and imputation. These distributions are modeled using multivariate Gaussian and Bernoulli distributions, respectively. Depending on the characteristics of the observations and states, alternative likelihood distributions can also be employed. For instance, the beta likelihood for data in the unit interval, mixtures for multiple marginal distributions, and the negative-binomial likelihood for positive count data (See appendix A.3). ", "page_idx": 5}, {"type": "text", "text": "Likelihood for Inferring States. The following theorem defines objective for inferring physical system\u2019s states. The proof is provided in appendix A.4.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. The lower bounded conditional log likelihood of the physical system\u2019s states given original images is determined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal L}(\\mathbf{s}_{1:T}|\\mathbf{o}_{1:T})=\\frac{1}{N}\\sum_{i=1}^{N}l o g\\,p(\\mathbf{s}_{1:T}|\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)},\\mathbf{o}_{1:T})=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}l o g\\,N\\biggl(\\mathbf{s}_{t}\\Bigl|d_{m}({\\hat{\\mu}_{t}^{(i)}}),d_{c}(\\hat{\\mathbf{\\Sigma}}_{t|T}^{(i)})\\biggr)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the $d_{m}(.)$ and $d_{c}(.)$ determines those parts of the decoder obtaining the state mean and variance. $N$ sequences of $(\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)})\\sim q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ are sampled for Monte Carlo integration estimation. ", "page_idx": 5}, {"type": "text", "text": "Likelihood for Inferring Images. For the imputation task, consider the ground truth as the sequence of images $\\mathbf{o}_{1:T}$ with the dimension of $D_{o}$ . The following theorem defines objective for inferring images. See appendix A.4.2 for the proof. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. The lower bound of log likelihood of the original images is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{\\Delta}}(\\mathbf{o}_{1:T})=\\frac{1}{N}\\sum_{i=1}^{N}l o g\\,p(\\mathbf{o}_{1:T}|\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)})=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\sum_{k=1}^{D_{o}}\\mathbf{o}_{t}^{(k)}l o g\\big(d_{k}(\\hat{\\mu}_{t|T}^{(i)})\\big)+\\big(1-\\mathbf{o}_{t}^{(k)}\\big)l o g\\big(1-d_{k}(\\hat{\\mu}_{t|T}^{(i)})\\big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $N$ sequences of $(\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)})\\sim q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ for Monte Carlo integration estimation.   \n$d_{k}(.)$ defines the corresponding part of the decoder that maps the $k$ -th pixel of $\\mathbf{o}_{t}$ image. ", "page_idx": 6}, {"type": "text", "text": "We use Wishart distribution as a prior for our covariance matrix of states in (10) and (11), which pushes the covariance toward a scale of identity matrix. Such prior prevents getting high log-likelihood due to the high uncertainty. We shrink this scale toward zero as time passes, as we expect the model to finally perform with very little uncertainty, approaching deterministically. ", "page_idx": 6}, {"type": "text", "text": "Mode collapse handling. To address the issue of mode collapse, where the model becomes stuck in the same state, we propose a loss term on $(i,j)$ pair of transition matrices as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nl_{\\mathrm{mc}}(i,j)=||\\check{\\mathbf{F}}^{i}-\\mathrm{Diag}(\\mathbf{m}_{i})||_{F}^{2}-||\\check{\\mathbf{F}}^{i}-\\check{\\mathbf{F}}^{j}||_{F}^{2}-||\\check{\\mathbf{F}}^{i}-\\mathrm{Diag}(\\mathbf{m}_{j})||_{F}^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with ${\\bf m}_{i}$ and ${\\mathbf{m}}_{j}$ being distinct hyper parameter vectors, and $||.||_{F}$ is the Frobenius norm. The term $\\begin{array}{r}{-\\frac{1}{2}\\sum_{i=1}^{K}\\sum_{j=1,j\\neq i}^{K}l_{\\mathrm{mc}}(i,j)}\\end{array}$ can be added into (10) and (11). This addition ensures that each ${\\check{\\mathbf{F}}}^{i}$ captures a unique transition, distinct from others. From a statistical perspective, (12) represents a term proportional to $-K L\\big(p_{i}(\\check{\\mathbf{F}})||p_{j}(\\check{\\mathbf{F}}).p r i_{j}(\\check{\\mathbf{F}})\\big)+K L\\big(p_{i}(\\check{\\mathbf{F}})||p r i_{i}(\\mathring{\\mathbf{F}})\\big)$ (See appendix A.5). $p_{i}(\\check{\\mathbf{F}})\\sim\\mathcal{M N}_{n\\times n}(\\check{\\mathbf{F}}^{i},\\mathbf{I},\\mathbf{I})$ and $p_{j}(\\check{\\mathbf{F}})\\sim\\mathcal{M N}_{n\\times n}(\\check{\\mathbf{F}}^{j},\\mathbf{I},\\mathbf{I})$ , are matrix normal distributions with priors $p r i_{i}(\\check{\\mathbf{F}})\\sim\\mathcal{M N}_{n\\times n}(\\mathrm{Diag}(\\mathbf{m}_{i}),\\mathbf{I},\\mathbf{I})$ and $p r i_{j}(\\check{\\mathbf{F}})\\sim\\mathcal{M N}_{n\\times n}(\\mathrm{Diag}(\\mathbf{m}_{j}),\\mathbf{I},\\mathbf{I})$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Gradient explosion handling. Training (10) and (11) with SGD can be disrupted by a gradient explosion. Specifically, we limit the optimization in (10) and (11) subject to $\\sigma_{1}(\\mathbf{U}_{h})<2$ , where $\\sigma_{i}(.)$ represents the $i$ -th largest singular value and $\\mathbf{U}_{h}\\,\\in\\,\\mathbb{R}^{h\\times h}$ denotes the weight matrix of the hidden state $\\mathbf{h}\\in\\mathbb{R}^{h}$ with $h$ dimension in the GRU cell (see eq.(5) in [35] or (53) in appendix A.4.3). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. When $\\sigma_{1}(\\mathbf{U}_{h})<2$ , a GRU cell is locally stable at a fix point $\\mathbf{h}^{*}=\\mathbf{0}$ . ", "page_idx": 6}, {"type": "text", "text": "We use Lyapunov stability and two lemmas for the proof of Theorem 3 in appendix A.4.3. Solving (10) and (11) with SGD gives an updated $\\hat{\\mathbf{U}}_{h}$ in each iteration that may not satisfy $\\sigma_{1}(\\hat{\\mathbf{U}}_{h})<2$ . We offer two solutions to satisfy the constraint outlined in theorem 3. The first solution relies on the spectral theorem, offering higher accuracy but with increased computational cost. Alternatively, the second solution employs the Gershgorin circle theorem, providing lower accuracy but at a reduced cost. Due to space limitations, we provide the details of the second solution in appendix A.4.5. For the first solution, we modify $\\hat{\\mathbf{U}}_{h}$ in three steps: (i) Decompose $\\hat{\\mathbf{U}}_{h}$ by singular value decomposition (SVD) such that $\\hat{\\mathbf{U}}_{h}=\\mathbf{W}\\mathbf{A}\\mathbf{V}$ . (ii) Replace the singular values of diagonal $\\Lambda$ that are greater than 2 with the threshold $2-\\delta$ and obtain $\\overline{{\\Lambda}}=\\mathrm{Diag}(m i n(\\sigma_{1},2-\\delta),...,m i n(\\sigma_{h},2-\\delta))$ . (iii) Reconstruct $\\mathbf{U}_{h}\\gets\\mathbf{W}\\overline{{\\mathbf{A}}}\\mathbf{V}$ . Employing SVD approach, the cost of this solution is proportional to ${\\mathcal{O}}(h^{3})$ . In the next theorem we show that constructed ${\\mathbf U}_{h}$ is the optimum solution. The proof is in appendix A.4.4. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. The modified weight matrix ${\\mathbf U}_{h}$ obtained from (iii) step above, is a solution of the following optimization problem: $m i n_{\\mathbf{U}_{h}}||\\hat{\\mathbf{U}}_{h}-\\mathbf{U}_{h}||_{F}^{2}$ , s.t. $\\sigma_{1}(\\mathbf{U}_{h})<2-\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "7 Evaluation and Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The first two experiments are single pendulum and double pendulum, where the dynamics of the latter one is more complicated. The third experiment is ball bouncing in irregular polygon to demonstrate the ability of the GIN when it faces irregular environments with multiple underlying dynamics. The fourth experiment covers visual odometry task for real world data. The last experiment shows the effectiveness of theorems 3 and 4 for gradient explosion handling. Intuitive python code and training algorithm are in appendix A.11. Detailed explanations regarding hyperparameter optimization, network structure, and empirical solutions aimed at avoiding poor local minima are in appendix A.7. ", "page_idx": 6}, {"type": "text", "text": "Single Pendulum and Double Pendulum. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the pendulum experiment, observations are the image sequences comprising 100 time steps, each has the size of $24\\ \\times\\ 24$ distorted with time correlated noise. We perform the filtering-smoothing by the GIN. In state estimation task, the log-likelihood and squared error (SE) of the estimated states are given in table 1. With $\\begin{array}{r l r}{n}&{{}=}&{3m}\\end{array}$ ( $\\ln$ and $m$ representing state and transferred observation dimensions) as shown in Table 1, intuitively states contain information ", "page_idx": 6}, {"type": "table", "img_path": "z4duW3KzlD/tmp/f75ce645ce622cf87e64fface41c4dea589328abda89d59298c669df7b9a861d.jpg", "table_caption": ["Table 1: Double (left) and single pendulum (middle) and polygon (left) state estimation results. $\\left(s_{1},s_{2},s_{3},s_{4}\\right)$ are estimated position of the joints of double pendulum. For single pendulum and polygon, $\\left(s_{1},s_{2}\\right)$ denotes the estimated single joint and ball position. s is sampled from eq. (10). "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "z4duW3KzlD/tmp/3d38771ac26ac82b60e6e7cecffb2a19da9e709daf2380dcda324d31a039bd32.jpg", "img_caption": ["Figure 5: Pendulum imputation. Rows from up to down are the noise distorted ground truth, observation with uninformed missing frames and the imputation results of the GIN(smoothed). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "z4duW3KzlD/tmp/afaab7be795df2828c366ca603747400136c0480e32587c126871190668bb116.jpg", "img_caption": ["Figure 6: D-pendulum imputation.Rows from up to down are the noise distorted ground truth, observation with uninformed missing frames and the imputation results of the GIN(smoothed). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "about position, velocity, and acceleration, increasing the likelihood compared to $n~=~2m$ , where only position and velocity are allocated to states. For the imputation task, we randomly remove half of the images from the generated sequences and perform image imputation by predicting the missing parts. The numerical results are in table 2 and imputed images are in figures 5 and 6. In table 2, the models with informed boolean masks are aware of available and missing images, while uninformed masks use a black image as input for missing images. This encourages the model to accurately infer the dynamics for image generation. ", "page_idx": 7}, {"type": "text", "text": "For ablation study, first we compare GIN with simple encoder-decoder without latent parameterization, and ", "page_idx": 7}, {"type": "text", "text": "Table 2: Image imputation task log likelihood for single pendulum (left), double pendulum (middle) and polygon (right). The approaches without good results are not included. ", "page_idx": 7}, {"type": "table", "img_path": "z4duW3KzlD/tmp/e3eae934e176fa42492be860c7de0157312ef3916cf2262ebd1dbe56044610be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "then with LGSSM, where the GRU cells are omitted from the GIN structure and classic filteringsmoothing equations are used instead. We also check the usefulness of flitering-smoothing parameterization by replacing the GIN\u2019s transition block with GRU and LSTM cells and directly parameterize output distribution (like AE-RNN [36] and [37]). We compare various VI models, including VAE, IWVAE, VAE-RNN [38] which utilizes VAEs with recurrent cells, and EM-based VI approaches such as SVAE, KVAE, EKVAE and MVAE. Our comparison also encompasses DeepAR [39], an AR model. Our thorough comparison, covering the CRU, RKN, a group of VAEs, and AR models, enhances the depth of our analysis. The GIN consistently outperforms all other models. Additionally, we provide results using the MSE metric to highlight the competitive prediction accuracy of our approach (See appendix A.10). ", "page_idx": 8}, {"type": "text", "text": "Bouncing Ball In Irregular Polygon. To demonstrate the adaptability of the GIN, we subjected it to a more complex environment. We generated 1000 sequences, each comprising 70 time steps, depicting a ball moving within an irregular polygon featuring random shapes. ", "page_idx": 8}, {"type": "text", "text": "The initial position and velocity of the ball were randomized. No external forces acted upon the ball, and collisions with the walls were elastic. In addition to the aforementioned models, we include latent SLDS structure from the SVAE approach for the comparison. SLDS is particularly relevant in scenarios such as this one, where separate dynamics for each state are considered, as seen when the ball bounces off the wall. We exclude simple baselines, such as encoder-decoder, VAE, and IW-VAE, from the comparison due to their lack of competitive performance. The numerical results are in tables 1 and 2. In Figure 7, we illustrate samples obtained from the trained smoothing distributions of GIN, LGSSM, and KVAE, showcasing their proximity to the ground truth. For this demonstration, we specifically select the 35th time step for sampling, denoted as $\\mathcal{L}(\\mathbf{s}_{35}|\\mathbf{o}_{1:70})$ . Here, $s1$ and $s2$ represent the estimated xy position of the ball within the polygon. Generating 20 frames with GIN in four different irregular environments is shown in figure ", "page_idx": 8}, {"type": "image", "img_path": "z4duW3KzlD/tmp/b3dd0cf87c694a0f0e98e05ff2d0cfe55dc2bff1d66be75c385d6d8e50e7583a.jpg", "img_caption": ["Figure 7: State (position) estimation in irregular polygon (6 edges) at 35-th and 45-th time steps. The upper row shows the ground truth ball position in 35, 39, 42 and 45-th time steps, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "1. We have created animated files that demonstrate both sequence generation and imputation tasks within irregular polygons. Please visit: https://sites.google.com/view/ginneurips2024. ", "page_idx": 8}, {"type": "text", "text": "Visual Odometry of KITTI Dataset. We also evaluate the GIN with the higher dimensional observations for the visual odometry task on the KITTI dataset [40]. This dataset consists of 11 separated image sequences with their corresponding labels. We use a feature extractor network proposed by Zhou et al. in [41] to obtain the transferred observations of the GIN, i.e. (w, r). Additionally, we compare the results with AE-RNN(LSTM), AE-RNN(GRU), DeepVO [42] and KVAE. The numerical results are in table 3, where the common evaluation scheme for the KITTI dataset is exploited. The results of the KVAE degrades substantially as we have to reduce the size of the transferred observation to prevent the complexity of matrix inversion. Sampling visualization from smoothing state distribution are in appendix A.9. ", "page_idx": 8}, {"type": "table", "img_path": "z4duW3KzlD/tmp/f7afebf0e1c86fb142f861b033ffb522696a44aad409c737ca49aefdbcf0be53.jpg", "table_caption": ["Table 3: Comparison of model performance on KITTI dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Gradient Explosion Experiment. Table 4 lists the log likelihood and its standard deviation for three experiments: single pendulum, double pendulum, and irregular polygon. These experiments were trained under different settings for gradient explosion handling: the conventional Gradient Clipping (GC), our first solution using Singular Value Decomposition (SVD), and our second solution using the Gershgorin Circle Theorem (GCT). The table empirically demonstrates that our method outperforms conventional gradient clipping. In this table, $\\theta$ represents the threshold for gradient clipping as introduced in [10], and $\\delta$ is the threshold in our method that keeps the spectral radius less than 2, i.e., $\\sigma_{1}(\\mathbf{U}_{h})+\\delta=2$ . As shown in table 4, gradient clipping failed to train for high $\\theta$ , while our approach achieves lower perplexity and higher log likelihood, ensuring stability compared to gradient clipping by constraining the GRU to be stable. We provide a comprehensive discussion on the $\\delta$ variable, including how to tune it in different environments and its impact on model performance, in the appendix A.8.1. ", "page_idx": 9}, {"type": "table", "img_path": "z4duW3KzlD/tmp/448a0b1c3e940f4e4f129170f389f82a284ac44aa8a1524e4c35752c80fa46b3.jpg", "table_caption": ["Table 4: Gradient explosion handling: comparison between GC, SVD and GCT. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces the GIN for representation learning in high-dimensional SSMs. The data flow is managed by Bayesian filtering-smoothing, and the use of GRU-based KG and SG networks addresses computational issues, resulting in an efficient and numerically stable model. The GIN learns system dynamics end-to-end, making it a high-performance model with strong system identification capabilities. It also provides insightful uncertainty representations for predictions and outperforms various counterparts, including generative models with variational inference, autoregressive models, and other baselines. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Yasin Abbasi-Yadkori for useful discussions and suggestions that contributed to this work. ", "page_idx": 9}, {"type": "text", "text": "Bibliography ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning. Springer, 2006, vol. 4, no. 4.   \n[2] R. M. Neal et al., \u201cMcmc using hamiltonian dynamics,\u201d Handbook of markov chain monte carlo, vol. 2, no. 11, p. 2, 2011.   \n[3] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.   \n[4] D. Koller and N. Friedman, Probabilistic graphical models: principles and techniques. MIT press, 2009.   \n[5] R. E. Kalman, \u201cA new approach to linear filtering and prediction problems,\u201d 1960.   \n[6] H. E. Rauch, F. Tung, and C. T. Striebel, \u201cMaximum likelihood estimates of linear dynamic systems,\u201d AIAA journal, vol. 3, no. 8, pp. 1445\u20131450, 1965.   \n[7] E. A. Wan and R. Van Der Merwe, \u201cThe unscented kalman filter for nonlinear estimation,\u201d in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No. 00EX373). Ieee, 2000, pp. 153\u2013158.   \n[8] L. Ljung, \u201cAsymptotic behavior of the extended kalman fliter as a parameter estimator for linear systems,\u201d IEEE Transactions on Automatic Control, vol. 24, no. 1, pp. 36\u201350, 1979.   \n[9] S. Linderman, M. Johnson, A. Miller, R. Adams, D. Blei, and L. Paninski, \u201cBayesian learning and inference in recurrent switching linear dynamical systems,\u201d in Artificial Intelligence and Statistics. PMLR, 2017, pp. 914\u2013922.   \n[10] R. Pascanu, T. Mikolov, and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d in International conference on machine learning. Pmlr, 2013, pp. 1310\u20131318.   \n[11] K. Doya et al., \u201cBifurcations in the learning of recurrent neural networks 3,\u201d learning (RTRL), vol. 3, p. 17, 1992.   \n[12] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, \u201cBackprop kf: Learning discriminative deterministic state estimators,\u201d in Advances in neural information processing systems, 2016, pp. 4376\u20134384.   \n[13] R. Krishnan, U. Shalit, and D. Sontag, \u201cStructured inference networks for nonlinear state space models,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 1, 2017.   \n[14] P. G. Chang, K. P. Murphy, and M. Jones, \u201cOn diagonal approximations to the extended kalman fliter for online training of bayesian neural networks,\u201d in Continual Lifelong Learning Workshop at ACML 2022, 2022.   \n[15] M. Jones, T. R. Scott, M. Ren, G. F. Elsayed, K. Hermann, D. Mayo, and M. C. Mozer, \u201cLearning in temporally structured environments,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[16] M. Watter, J. T. Springenberg, J. Boedecker, and M. Riedmiller, \u201cEmbed to control: A locally linear latent dynamics model for control from raw images,\u201d arXiv preprint arXiv:1506.07365, 2015.   \n[17] Y. Burda, R. Grosse, and R. Salakhutdinov, \u201cImportance weighted autoencoders,\u201d arXiv preprint arXiv:1509.00519, 2015.   \n[18] M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R. Datta, \u201cComposing graphical models with neural networks for structured representations and fast inference,\u201d Advances in neural information processing systems, vol. 29, 2016.   \n[19] M. Fraccaro, S. Kamronn, U. Paquet, and O. Winther, \u201cA disentangled recognition and nonlinear dynamics model for unsupervised learning,\u201d arXiv preprint arXiv:1710.05741, 2017.   \n[20] Y. Li and S. Mandt, \u201cDisentangled sequential autoencoder,\u201d arXiv preprint arXiv:1803.02991, 2018.   \n[21] A. Klushyn, R. Kurle, M. Soelch, B. Cseke, and P. van der Smagt, \u201cLatent matters: Learning deep state-space models,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021.   \n[22] F. Tonolini, N. Aletras, Y. Jiao, and G. Kazai, \u201cRobust weak supervision with variational auto-encoders,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 34 394\u2013 34 408.   \n[23] H. Zhu, C. Balsells-Rodas, and Y. Li, \u201cMarkovian gaussian process variational autoencoders,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 42 938\u201342 961.   \n[24] P. Becker, H. Pandya, G. Gebhardt, C. Zhao, C. J. Taylor, and G. Neumann, \u201cRecurrent kalman networks: Factorized inference in high-dimensional deep feature spaces,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 544\u2013552.   \n[25] M. Schirmer, M. Eltayeb, S. Lessmann, and M. Rudolph, \u201cModeling irregular time series with continuous recurrent units,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 19 388\u201319 405.   \n[26] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.   \n[27] K. Cho, B. Van Merri\u00ebnboer, D. Bahdanau, and Y. Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014.   \n[28] R. Wilson and L. Finkel, \u201cA neural implementation of the kalman filter,\u201d Advances in neural information processing systems, vol. 22, pp. 2062\u20132070, 2009.   \n[29] I. Buchnik, G. Revach, D. Steger, R. J. Van Sloun, T. Routtenberg, and N. Shlezinger, \u201cLatentkalmannet: Learned kalman flitering for tracking from high-dimensional signals,\u201d IEEE Transactions on Signal Processing, 2023.   \n[30] G. Revach, N. Shlezinger, X. Ni, A. L. Escoriza, R. J. van Sloun, and Y. C. Eldar, \u201cKalmannet: Neural network aided kalman filtering for partially known dynamics,\u201d arXiv preprint arXiv:2107.10043, 2021.   \n[31] D. Ruhe and P. Forr\u00e9, \u201cSelf-supervised inference in state-space models,\u201d arXiv preprint arXiv:2107.13349, 2021.   \n[32] T. Liu, K. A. Lee, Q. Wang, and H. Li, \u201cDisentangling voice and content with self-supervision for speaker recognition,\u201d Advances in Neural Information Processing Systems, vol. 36, pp. 50 221\u201350 236, 2023.   \n[33] M. P. Deisenroth and H. Ohlsson, \u201cA general perspective on gaussian filtering and smoothing: Explaining current and deriving new algorithms,\u201d in Proceedings of the 2011 American Control Conference. IEEE, 2011, pp. 1807\u20131812.   \n[34] P. Gilabert, G. Montoro, and E. Bertran, \u201cOn the wiener and hammerstein models for power amplifier predistortion,\u201d in 2005 Asia-Pacific Microwave Conference Proceedings, vol. 2. IEEE, 2005, pp. 4\u2013pp.   \n[35] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.   \n[36] N. Srivastava, E. Mansimov, and R. Salakhudinov, \u201cUnsupervised learning of video representations using lstms,\u201d in International conference on machine learning. PMLR, 2015, pp. 843\u2013852.   \n[37] H. Li, S. Yu, and J. Principe, \u201cCausal recurrent variational autoencoder for medical time series generation,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 7, 2023, pp. 8562\u20138570.   \n[38] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio, \u201cA recurrent latent variable model for sequential data,\u201d Advances in neural information processing systems, vol. 28, 2015.   \n[39] D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski, \u201cDeepar: Probabilistic forecasting with autoregressive recurrent networks,\u201d International Journal of Forecasting, vol. 36, no. 3, pp. 1181\u20131191, 2020.   \n[40] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? the kitti vision benchmark suite,\u201d in 2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012, pp. 3354\u20133361.   \n[41] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, \u201cUnsupervised learning of depth and egomotion from video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1851\u20131858.   \n[42] S. Wang, R. Clark, H. Wen, and N. Trigoni, \u201cDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks,\u201d in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 2043\u20132050.   \n[43] K. Sohn, H. Lee, and X. Yan, \u201cLearning structured output representation using deep conditional generative models,\u201d Advances in neural information processing systems, vol. 28, 2015.   \n[44] D. Angeli, E. D. Sontag, and Y. Wang, \u201cA characterization of integral input-to-state stability,\u201d IEEE Transactions on Automatic Control, vol. 45, no. 6, pp. 1082\u20131097, 2000.   \n[45] R. A. Horn and C. R. Johnson, Matrix analysis. Cambridge university press, 2012.   \n[46] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.   \n[47] P. J. Werbos, \u201cBackpropagation through time: what it does and how to do it,\u201d Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.   \n[48] J. L. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d arXiv preprint arXiv:1607.06450, 2016.   \n[49] J. M. Wang, D. J. Fleet, and A. Hertzmann, \u201cGaussian process dynamical models for human motion,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 30, no. 2, pp. 283\u2013298, 2007.   \n[50] J. Ko and D. Fox, \u201cLearning gp-bayesfilters via gaussian process latent variable models,\u201d Autonomous Robots, vol. 30, no. 1, pp. 3\u201323, 2011.   \n[51] R. Frigola, F. Lindsten, T. B. Sch\u00f6n, and C. E. Rasmussen, \u201cBayesian inference and learning in gaussian process state-space models with particle mcmc,\u201d Advances in neural information processing systems, vol. 26, 2013.   \n[52] M. Schoukens and K. Tiels, \u201cIdentification of block-oriented nonlinear systems starting from linear approximations: A survey,\u201d Automatica, vol. 85, pp. 272\u2013292, 2017.   \n[53] R. Li, S. John, and A. Solin, \u201cImproving hyperparameter learning under approximate inference in gaussian process models,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 19 595\u201319 615.   \n[54] N. Wahlstr\u00f6m, T. B. Sch\u00f6n, and M. P. Deisenroth, \u201cFrom pixels to torques: Policy learning with deep dynamical models,\u201d arXiv preprint arXiv:1502.02251, 2015.   \n[55] E. Archer, I. M. Park, L. Buesing, J. Cunningham, and L. Paninski, \u201cBlack box variational inference for state space models,\u201d arXiv preprint arXiv:1511.07367, 2015.   \n[56] M. Karl, M. Soelch, J. Bayer, and P. Van der Smagt, \u201cDeep variational bayes filters: Unsupervised learning of state space models from raw data,\u201d arXiv preprint arXiv:1605.06432, 2016.   \n[57] C. Naesseth, S. Linderman, R. Ranganath, and D. Blei, \u201cVariational sequential monte carlo,\u201d in International conference on artificial intelligence and statistics. PMLR, 2018, pp. 968\u2013977.   \n[58] S. S. Rangapuram, M. W. Seeger, J. Gasthaus, L. Stella, Y. Wang, and T. Januschowski, \u201cDeep state space models for time series forecasting,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[59] V. Garcia Satorras, Z. Akata, and M. Welling, \u201cCombining generative and discriminative models for hybrid inference,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Filtering and Smoothing Parameterization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Kalman fliter operates through two iterative steps: prediction and update. In the prediction step, the filter uses prior state information to make an estimate, while in the update step, it adjusts this estimate based on new observations. Assuming normally distributed additive process and observation noise, the filter can effectively perform these steps. During the prediction step, the filter employs the transition matrix $\\mathbf{F}_{t+1}$ to estimate the next priors $p_{\\gamma_{1:t+1}}(\\mathbf x_{t+1}|\\mathbf w_{1:t})=(\\mu_{t+1|t},\\Sigma_{t+1|t})$ , which represent the next state estimates without incorporating any new observations. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{t+1|t}=\\mathbf{F}_{t+1}\\mu_{t|t},\\,\\mathrm{and}\\quad\\,\\Sigma_{t+1|t}=\\mathbf{F}_{t+1}\\Sigma_{t|t}\\mathbf{F}_{t+1}^{T}+\\mathbf{Q}_{t+1},\\,\\mathrm{and}\\quad\\,\\mathbf{Q}_{t+1}=\\sigma_{t+1}^{2}\\mathbf{I}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When a new observation is available, the Kalman fliter proceeds to the second step, where it updates the predicted prior using the new observation and the emission matrix $\\mathbf{H}_{t+1}$ . This results in the next posterior, $(\\mu_{t+1|t+1},\\Sigma_{t+1|t+1})$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{K}_{t+1}=\\boldsymbol{\\Sigma}_{t+1|t}\\mathbf{H}_{t+1}^{T}\\big(\\mathbf{H}_{t+1}\\boldsymbol{\\Sigma}_{t+1|t}\\mathbf{H}_{t+1}^{T}+\\mathbf{R}_{t+1}\\big)^{-1},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota_{t+1|t+1}=\\mu_{t+1|t}+\\mathbf{K}_{t+1}(\\mathbf{w}_{t}-\\mathbf{H}_{t+1}\\mu_{t+1|t}),\\ \\ \\Sigma_{t+1|t+1}=\\Sigma_{t+1|t}-\\mathbf{K}_{t+1}\\big(\\mathbf{H}_{t+1}\\Sigma_{t+1|t}\\mathbf{H}_{t+1}^{T}+\\mathbf{H}_{t+1}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The entire observation update procedure can be viewed as a weighted mean between the next prior, derived from the state update, and the new observation. This weighting is influenced by $\\mathbf{Q}$ and $\\mathbf{R}$ , reflecting their inherent uncertainties. We derive the smoothing parameterization by leveraging the Markov property, which states that $\\mathbf{x}_{t}$ is independent of future observations $\\mathbf{w}_{t+1:T}$ given $\\mathbf{x}_{t+1}$ . Although $\\mathbf{x}_{t+1}$ is unknown, there is a distribution over it. By conditioning on $\\mathbf{x}_{t+1}$ and then marginalizing out, we can obtain $\\mathbf{x}_{t}$ conditioned on $\\mathbf{w}_{1:T}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{\\gamma_{1:T}}(\\mathbf{x}_{t}|\\mathbf{w}_{1:T})=\\displaystyle\\int p_{\\gamma_{1:T}}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:T})p_{\\gamma_{1:T}}(\\mathbf{x}_{t+1}|\\mathbf{w}_{1:T})d\\mathbf{x}_{t+1}\\ }}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=\\int p_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:t},\\underline{{\\mathbf{w}_{t+1:T}}})p_{\\gamma_{1:T}}(\\mathbf{x}_{t+1}|\\mathbf{w}_{1:T})d\\mathbf{x}_{t+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By using induction and and smoothed distribution for $t+1$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{\\gamma_{1:T}}(\\mathbf{x}_{t+1}|\\mathbf{w}_{1:T})=\\mathcal{N}(\\mu_{t+1|T},\\pmb{\\Sigma}_{t+1|T})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we calculate the filtered two-slice distribution as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n.p_{\\gamma_{1:t}}(\\mathbf{x}_{t},\\mathbf{x}_{t+1}|\\mathbf{w}_{1:t})=\\mathcal{N}\\bigg(\\left(\\begin{array}{c c}{\\mu_{t|t}}\\\\ {\\mu_{t+1|t}}\\end{array}\\right),\\left(\\begin{array}{c c}{\\Sigma_{t|t}}&{\\Sigma_{t|t}\\mathbf{F}_{t+1}^{T}}\\\\ {\\mathbf{F}_{t+1}\\Sigma_{t|t}}&{\\Sigma_{t+1|t}}\\end{array}\\right)\\bigg)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "by using Gaussian conditioning we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{\\gamma_{1:t}}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:t})=\\mathcal{N}(\\mu_{t}|t+\\mathbf{J}_{t}\\big(\\mathbf{x}_{t+1}-\\mathbf{F}_{t+1}\\mu_{t|t}\\big),\\boldsymbol{\\Sigma}_{t|t}-\\mathbf{J}_{t}\\boldsymbol{\\Sigma}_{t+1|t}\\mathbf{J}_{t}^{T})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{J}_{t}\\,=\\,\\pmb{\\Sigma}_{t|t}\\mathbf{F}_{t+1}\\big[\\pmb{\\Sigma}_{t+1|t}\\big]^{-1}$ . We calculate the smoothed distribution for $t$ using the rules of iterated expectation and covariance: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mu_{t}}_{t}_{T}=\\mathbb{E}\\left[\\mathbb{E}\\big[\\mathbf{x}_{t}\\vert\\mathbf{x}_{t+1},\\mathbf{w}_{1:T}\\big]\\;\\vert\\mathbf{w}_{1:T}\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}\\left[\\mathbb{E}\\big[\\mathbf{x}_{t}\\vert\\mathbf{x}_{t+1},\\mathbf{w}_{1:t}\\big]\\;\\vert\\mathbf{w}_{1:T}\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}\\left[{\\mu_{t}}_{t}\\vert t+\\mathbf{J}_{t}\\big(\\mathbf{x}_{t+1}-\\mathbf{F}_{t+1}{\\mu_{t}}_{\\vert t}\\big)\\;\\vert\\mathbf{w}_{1:T}\\right]}\\\\ &{\\quad\\quad={\\mu_{t}}_{t}+\\mathbf{J}_{t}\\big({\\mu_{t+1}}_{\\vert T}-\\mathbf{F}_{t+1}{\\mu_{t}}_{\\vert t}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{t|T}=\\mathrm{cov}\\big[\\mathbb{E}[\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:T}]\\,\\,|\\mathbf{w}_{1:T}\\big]+\\mathbb{E}\\big[\\mathrm{cov}[\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:T}]\\,\\,|\\mathbf{w}_{1:T}\\big]}\\\\ &{\\qquad=\\mathrm{cov}\\big[\\mathbb{E}[\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:t}]\\,\\,|\\mathbf{w}_{1:T}\\big]+\\mathbb{E}\\big[\\mathrm{cov}[\\mathbf{x}_{t}|\\mathbf{x}_{t+1},\\mathbf{w}_{1:t}]\\,\\,|\\mathbf{w}_{1:T}\\big]}\\\\ &{\\qquad=\\mathrm{cov}\\big[\\mu_{t|t}+\\mathbf{J}_{t}\\big(\\mathbf{x}_{t+1}-\\mathbf{F}_{t+1}\\mu_{t}\\vert\\,\\,\\big\\vert\\mathbf{w}_{1:T}\\big]+\\mathbb{E}\\big[\\Sigma_{t|t}-\\mathbf{J}_{t}\\Sigma_{t+1|t}\\mathbf{J}_{t}^{T}\\,\\,\\big\\vert\\mathbf{w}_{1:T}\\big]}\\\\ &{\\qquad=\\mathbf{J}_{t}\\mathrm{cov}\\big[\\mathbf{x}_{t+1}-\\mathbf{F}_{t+1}\\mu_{t|t}\\,\\,\\big\\vert\\mathbf{w}_{1:T}\\big]\\mathbf{J}_{t}^{T}+\\Sigma_{t|t}-\\mathbf{J}_{t}\\Sigma_{t+1|t}\\mathbf{J}_{t}^{T}}\\\\ &{\\qquad=\\mathbf{J}_{t}\\Sigma_{t+1/T}\\mathbf{J}_{t}^{T}+\\Sigma_{t|t}-\\mathbf{J}_{t}\\Sigma_{t+1|t}\\mathbf{J}_{t}^{T}}\\\\ &{\\qquad=\\Sigma_{t|t}+\\mathbf{J}_{t}\\big(\\Sigma_{t+1|T}-\\Sigma_{t+1|t}\\big)\\mathbf{J}_{t}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Process Noise Matrix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As stated in (13), we can elaborate the process noise matrix at time $t$ in more details ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{Q}_{t}=\\pmb{\\Sigma}_{t|t-1}-\\mathbf{F}_{t}\\pmb{\\Sigma}_{t-1|t-1}\\mathbf{F}_{t}^{T}=\\pmb{\\Sigma}_{t|t-1}-\\mathbf{F}_{t}\\left[\\pmb{\\Sigma}_{t-1|t-2}-\\mathbf{K}_{t-1}\\big[\\mathbf{H}_{t-1}\\pmb{\\Sigma}_{t-1|t-2}\\mathbf{H}_{t-1}^{T}+\\mathbf{R}_{t-1}\\big]^{-1}\\mathbf{K}_{t-1}^{T}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "combining (13) into (22) results in ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}_{t}=\\boldsymbol{\\Sigma}_{t|t-1}-\\mathbf{F}_{t}\\big[[\\mathbf{F}_{t-1}\\boldsymbol{\\Sigma}_{t-2|t-2}\\mathbf{F}_{t-1}^{T}+\\mathbf{Q}_{t-1}]}\\\\ &{\\qquad\\qquad-\\mathbf{K}_{t-1}\\big[\\mathbf{H}_{t-1}[\\mathbf{F}_{t-1}\\boldsymbol{\\Sigma}_{t-2|t-2}\\mathbf{F}_{t-1}^{T}+\\mathbf{Q}_{t-1}]\\mathbf{H}_{t-1}^{T}+\\mathbf{R}_{t-1}\\big]^{-1}\\mathbf{K}_{t-1}^{T}\\big]\\mathbf{F}_{t}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a function of $\\mathbf{F}_{t}$ , $\\mathbf{Q}_{t-1}$ , $\\mathbf{F}_{t-1}$ and $\\mathbf{H}_{t-1}$ . In the GIN, $\\hat{\\mathbf{F}}_{t}$ and $\\hat{\\mathbf{H}}_{t}$ are learned by the Dynamic Network with the input of $\\mathbf{x}_{t-1}$ . From (15), $\\mu_{t-1|t-1}$ is derived as a function of both $\\mathbf{F}_{t-1}$ and $\\mathbf{H}_{t-1}$ , meaning the learned $\\hat{\\mathbf{F}}_{t}$ carries the information of both $\\mathbf{H}_{t-1}$ and $\\mathbf{F}_{t-1}$ . Therefore, one can rewrite the equation (23) as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{t}=\\mathbf{g}\\bigg(\\hat{\\mathbf{F}}_{t}\\big(\\mu_{t-1|t-1}\\big),\\mathbf{Q}_{t-1}\\bigg),\\mathrm{where}\\quad\\hat{\\mathbf{F}}_{t}=D y n a m i c\\;N e t\\bigg(\\mu_{t-1|t-1}\\big(\\mathbf{H}_{t-1},\\mathbf{F}_{t-1}\\big)\\bigg)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{g}$ is a nonlinear function mapping $\\mu_{t-1|t-1}$ and $\\mathbf{Q}_{t-1}$ to $\\mathbf{Q}_{t}$ and the graphical model for such choice of structure is in appendix figure 8b. It is possible to go one step further and simplify $\\mu_{t-1|t-1}$ more, as it has $\\Sigma_{\\mathbf{t-1|t-1}}$ term in (15), combining it with (13) results in: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t-1|t-1}=\\mu_{t-1|t-2}+\\big[\\mathbf{F}_{t-1}\\boldsymbol{\\Sigma}_{t-2|t-2}\\mathbf{F}_{t-1}^{T}+\\mathbf{Q}_{t-1}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\mathbf{H}_{t-1}^{T}\\big(\\mathbf{H}_{t-1}\\big[\\mathbf{F}_{t-1}\\boldsymbol{\\Sigma}_{t-2|t-2}\\mathbf{F}_{t-1}^{T}+\\mathbf{Q}_{t-1}\\big]\\mathbf{H}_{t-1}^{T}+\\mathbf{R}_{t-1}\\big)^{-1}\\big(\\mathbf{w}_{t}-\\mathbf{H}_{t-1}\\mu_{t-1|t-2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "indicating that not only $\\mathbf{F}_{t-1}$ and $\\mathbf{H}_{t-1}$ , but also $\\mathbf{Q}_{t-1}$ is included in $\\mu_{t-1}|t{-}1$ , meaning that $\\mathbf{Q}_{t}$ can be written solely as a function of $\\mu_{t-1|t-1}$ and the graphical model for such choice is in figure 8a. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{t}=\\mathbf{g}\\left(\\hat{\\mathbf{F}}_{t}\\left(\\mu_{t-1\\mid t-1}\\right)\\right),\\mathrm{where}\\quad\\hat{\\mathbf{F}}_{t}=D y n a m i c\\;N e t\\left(\\mu_{t-1\\mid t-1}\\left(\\mathbf{H}_{t-1},\\mathbf{F}_{t-1},\\mathbf{Q}_{t-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We refer to $\\mathbf{g}$ as the $Q$ Network, which can be modeled either by an MLP, as shown in (26), or by a recurrent network, as detailed in (24). ", "page_idx": 14}, {"type": "text", "text": "A.3 Output Distribution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the case of grayscale images, each pixel $y$ is either one or zero with the probability $p$ or $1-p$ , respectively, meaning that $P(Y=y)=p^{y}(1-p)^{1-y}$ . This probability equation can be rewritten in the form of an exponential family as follows: ", "page_idx": 14}, {"type": "image", "img_path": "z4duW3KzlD/tmp/4a452f12389c71af147754136a2b3ab23e3e827b5e13be966d67229e3eb296ec.jpg", "img_caption": ["(a) Without recurrent dependency on $\\mathbf{q}_{t}$ modeled with (b) With recurrent dependency on $\\mathbf{q}_{t}$ modeled with (26). (24). ", "Figure 8: Graphical models for different parameterizations of the process noise. "], "img_footnote": [], "page_idx": 15}, {"type": "equation", "text": "$$\nf_{\\theta}(y)=h(y).e x p\\big(\\theta.y-\\psi(\\theta)\\big)\\to e^{l o g(p^{y}(1-p)^{1-y})}=e^{y\\,l o g(\\frac{p}{1-p})+\\,l o g(1-p)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By choosing $\\begin{array}{r}{\\theta=l o g(\\frac{p}{1-p})}\\end{array}$ and $\\psi(\\theta)=l o g(1-p)$ and $h(y)=1$ , we can obtain $\\begin{array}{r}{p=\\frac{1}{1+e^{-\\theta}}}\\end{array}$ . This means that by considering $\\theta$ as the last layer of the decoder and applying a sigmoid layer, $p$ is obtained and one can optimize $p$ such that the likelihood is maximized as we did in in (11). ", "page_idx": 15}, {"type": "text", "text": "Similarly, consider $x$ as the ground truth state, $\\hat{x}_{\\theta}$ as the estimated state, and $\\theta$ as the model variables. The residual follows a Gaussian distribution: $x=\\hat{x}_{\\theta}+\\epsilon\\sim\\mathcal{N}(\\hat{x}_{\\theta},\\hat{\\sigma}_{\\theta})$ , where ${\\hat{\\sigma}}_{\\theta}$ is the estimated variance. Then, the negative log likelihood is given by (28) as we obtained it in(10). ", "page_idx": 15}, {"type": "equation", "text": "$$\n-l o g(\\mathcal{L})\\propto\\frac{1}{2}l o g(\\hat{\\sigma}_{\\theta})+\\frac{(x-\\hat{x}_{\\theta})^{2}}{2\\hat{\\sigma}_{\\theta}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.4 Proof of theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We restate the theorems for the ease of readability and then provide the proofs. ", "page_idx": 15}, {"type": "text", "text": "A.4.1 Proof of theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem. The log likelihood of the states given the original observations is determined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{Z}(\\mathbf{s}_{1:T}|\\mathbf{o}_{1:T})=\\frac{1}{N}\\sum_{i=1}^{N}\\log p(\\mathbf{s}_{1:T}|\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)},\\mathbf{o}_{1:T})=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\log N\\bigg(\\mathbf{s}_{t}\\Big|d_{\\mathfrak{m}}(\\hat{\\mu}_{t|T}^{(i)}),d_{\\mathfrak{c}}(\\hat{\\Sigma}_{t|T}^{(i)})\\bigg)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the $d_{\\mathrm{m}}(.)$ and $d_{\\mathrm{c}}(.)$ determines those parts of the decoder that obtain the state mean and variance, respectively. $N$ sequences of $(\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)})\\,\\sim\\,q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ are sampled for Monte Carlo integration estimation. ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad_{\\mathbf{S}}=\\partial_{\\mathbf{B}_{T}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)}\\\\ &{=\\int_{\\mathbf{R}_{(\\mathbf{J})}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)\\mathbf{D}_{\\mathbf{B}_{T}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)}\\\\ &{\\quad=\\int_{\\mathbf{R}_{(\\mathbf{J})}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)\\mathbf{D}_{\\mathbf{B}_{T}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)}\\\\ &{\\quad=\\int_{\\mathbf{R}_{(\\mathbf{J})}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)}\\\\ &{\\quad-\\left(\\mathbf{\\bar{\\bar{\\pi}}}_{\\mathbf{\\bar{\\bar{J}}}}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}\\right)}\\\\ &{\\quad=\\int_{\\mathbf{R}_{(\\mathbf{J})}}\\left(\\mathbf{S}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{R}_{(\\mathbf{J})}:\\mathbf{\\bar{H}}_{\\mathbf{\\bar{J}}}\\right)}\\\\ &{\\quad\\quad+\\int_{\\mathbf{R}_{(\\mathbf{J})}}\\left(\\mathbf{S}_{ $ (29) (30) (31) (32) (33) (34) (35 (36) o1:T ) (37)   \n= N1  log N st   dm(\u00b5\u02c6t(|i)T ), dc( \u02c6\u03a3t(|i)T ) (38)   \ni=1t=1   \n= L(s1:T |o1:T ). (39) ", "page_idx": 16}, {"type": "text", "text": "The inequality in (32) arises from the positivity of the Kullback-Leibler (KL) term. The approximation in (34) results from assuming equality between the prior and the approximated posterior, i.e., $p_{\\gamma_{1:T}}(\\mathbf{w}_{1:T},\\mathbf{x}_{1:T}|\\mathbf{o}_{1:T})\\,=\\,q_{\\phi}\\bigl(\\mathbf{x}_{1:T},\\mathbf{\\bar{w}}_{1:T}\\bigl|\\mathbf{o}_{1:T},\\mathbf{s}_{1:T}\\bigr)$ . This assumption is not very restrictive as explained further in [43]. The approximation in (35) is coming from the equality assumption of $q_{\\phi}\\big(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}\\big|\\mathbf{o}_{1:T},\\mathbf{s}_{1:T}\\big)$ and $q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ . This assumption implies that given the original observation $\\mathbf{o}_{1:T}$ , our latent representation $\\left(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}\\right)$ is independent of the ground truth states $\\mathbf{s}_{1:T}$ . In simpler terms, $\\mathbf{s}_{1:T}$ is concealed within $\\mathbf{o}_{1:T}$ , allowing us to disregard $\\mathbf{s}_{1:T}$ . The approximation in (37) is because of Monte Carlo integration estimation. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.4.2 Proof of theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem. The (lower bound of) log likelihood of the original images is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{\\Xi}}(\\mathbf{o}_{1:T})=\\frac{1}{N}\\sum_{i=1}^{N}\\log p(\\mathbf{o}_{1:T}|\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)})=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\sum_{k=1}^{D_{o}}\\mathbf{o}_{t}^{(k)}\\mathrm{log}\\big(d_{k}(\\hat{\\mu}_{t|T}^{(i)})\\big)+\\bigr(1-\\mathbf{o}_{t}^{(k)}\\bigr)\\mathrm{log}\\big(1-d_{k}(\\hat{\\mu}_{t|T}^{(i)})\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $N$ sequences of $(\\mathbf{x}_{1:T}^{(i)},\\mathbf{w}_{1:T}^{(i)})\\sim q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ for Mon Carlo integration estimation. $d_{k}(.)$   \ndefines the corresponding part of the decoder that maps the $k$ -th pixel of $\\mathbf{o}_{t}$ image. ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{lot~}p_{\\mathrm{{sin}}}(n_{1}),}\\\\ &{=\\int_{\\r_{0}}^{\\infty}\\mathrm{th}_{1}(n_{2},n_{1}^{\\prime},n_{2}^{\\prime},n_{3}^{\\prime})\\mathrm{lot~}\\ r_{0}^{\\prime}\\mathrm{lot~}\\ r_{1}^{\\prime}\\mathrm{lot~}\\ r_{2}^{\\prime}\\mathrm{lot~}\\ r_{1}^{\\prime}\\mathrm{lot~}}\\\\ &{-\\left\\int_{\\r_{0}}^{\\infty}\\mathrm{th}_{1}(n_{3},n_{1}^{\\prime},n_{1^{\\prime}})\\mathrm{lot~}\\overline{{p_{\\mathrm{{sin}}}(n_{1},n_{2}^{\\prime},n_{3}^{\\prime})}}+\\mathrm{Or}_{2}\\rightleftharpoons}\\\\ &{-\\left\\int_{\\r_{0}}^{\\infty}\\mathrm{th}_{1}(n_{2},n_{1}^{\\prime},n_{2}^{\\prime},n_{3}^{\\prime})\\mathrm{lot~}\\overline{{p_{\\mathrm{{sin}}(n_{1},n_{2},n_{3}^{\\prime})}}}+\\mathrm{Or}_{3}\\rightleftharpoons}\\\\ &{\\ \\ \\ \\ -\\mathrm{em~|h_{\\mathrm{{sin}}}-h_{\\mathrm{{am}}}-h_{\\mathrm{{am}}}|^{2}~t o~}\\overline{{p_{\\mathrm{{sin}}(n_{1},n_{2},n_{3}^{\\prime},n_{3}^{\\prime})}}}}\\\\ &{\\ \\ \\ \\ +\\mathrm{e}_{31}(n_{3},n_{1}^{\\prime},n_{1^{\\prime}},n_{1^{\\prime}})\\mathrm{lot~}\\overline{{p_{\\mathrm{{sin}}(n_{1},n_{2}^{\\prime},n_{3}^{\\prime},n_{3}^{\\prime})}}}}\\\\ &{-\\mathrm{id}\\ r_{0}^{\\infty}\\mathrm{th}_{1}(n_{3},n_{1^{\\prime}},n_{1^{\\prime}})\\mathrm{lot~}\\overline{{p_{\\mathrm{{sin}}(n_{1},n_{2}^{\\prime},n_{3}^{\\prime},n_{3}^{\\prime})}}}}\\\\ &{+\\mathrm{id}\\ r_{0}^{\\infty}\\mathrm{th}_{1}(n_{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The inequality in (44) arises from the positivity of the Kullback-Leibler (KL) term. The approximation in (46) results from assuming equality between the prior and the approximated posterior, i.e., $p_{\\gamma_{1:T}}(\\mathbf{w}_{1:T},\\mathbf{x}_{1:T})=q_{\\phi}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\bar{\\mathbf{o}}_{1:T})$ . Although it is feasible to optimize (44) directly, we found out that by setting prior $p_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T})$ equal to approximated posterior $q_{\\gamma_{1:T}}(\\mathbf{x}_{1:T},\\mathbf{w}_{1:T}|\\mathbf{o}_{1:T})$ , more stable results with concise improvement are achieved from the numerical results (similar to [19, 21, 43]). The approximation in (48) is because of Monte Carlo integration estimation. ", "page_idx": 17}, {"type": "text", "text": "A.4.3 Proof of theorem 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First we provide a brief review on the GRU cell equations from [35]. Then we introduce two lemmas, by which we can proof theorem 2. ", "page_idx": 17}, {"type": "text", "text": "GRU Cell Review. The GRU cell holds the following equations: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{t}=S i g m o i d(\\mathbf{U}_{x z}\\mathbf{x}_{t}+\\mathbf{U}_{z}\\mathbf{h}_{t-1}),}\\\\ &{\\mathbf{r}_{t}=S i g m o i d(\\mathbf{U}_{x r}\\mathbf{x}_{t}+\\mathbf{U}_{r}\\mathbf{h}_{t-1}),}\\\\ &{\\mathbf{h}_{t}=\\mathbf{z}_{t}\\odot\\mathbf{h}_{t-1}+(1-\\mathbf{z}_{t})\\odot\\hat{\\mathbf{h}}_{t},}\\\\ &{\\hat{\\mathbf{h}}_{t}=t a n h(\\mathbf{U}_{x h}\\mathbf{x}_{t}+\\mathbf{U}_{h}(\\mathbf{r}_{t}\\odot\\mathbf{h}_{t-1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{x}_{t}$ and $\\mathbf{h}_{t}$ are the input and the hidden state vectors, respectively. $\\mathbf{U}_{x z},\\mathbf{U}_{z},\\mathbf{U}_{x r},\\mathbf{U}_{r},\\mathbf{U}_{x h}$   \nand ${\\bf U}_{h}$ are all weight matrices. ", "page_idx": 17}, {"type": "text", "text": "Lemma 1. A GRU cell has a fixed point at $\\mathbf{h}^{*}=\\mathbf{0}$ , if the input variable ${\\bf x}_{t}={\\bf0}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. After substituting ${\\bf x}_{t}={\\bf0}$ and $\\mathbf h_{t-1}=\\mathbf0$ into (51) and (52), the update gate $\\mathbf{z}_{t}$ and reset gate $\\mathbf{r}_{t}$ are both $\\frac{1}{2}$ . Substituting these values along with ${\\bf x}={\\bf0}$ and $\\mathbf h_{t-1}=\\mathbf0$ into (54) gives the candidate ", "page_idx": 17}, {"type": "text", "text": "state $\\hat{\\mathbf{h}}_{t}=\\mathbf{0}$ . Finally, substituting $\\mathbf h_{t-1}=\\mathbf0$ , $\\mathbf{z}_{t}=\\textstyle{\\frac{1}{2}}$ , and $\\hat{\\mathbf{h}}_{t}=\\mathbf{0}$ into (53) results in the new state $\\mathbf{h}_{t}=\\mathbf{0}$ . Thus, $\\mathbf h_{t}=\\mathbf h_{t-1}=\\mathbf0$ , indicating that GRU has a fixed point $\\mathbf{h}^{*}=\\mathbf{0}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 2. Let I be an $h\\times h$ identity matrix, $\\lambda_{i}(.)$ shows the $i$ -th largest absolute eigenvalue, and $\\begin{array}{r}{{\\mathbf A}=\\frac{1}{4}{\\mathbf U}_{h}+\\frac{1}{2}{\\mathbf I}}\\end{array}$ . When the spectral radius $|\\lambda_{1}(\\mathbf{A})|<1$ , a GRU cell with input $\\mathbf{x}_{t}=\\mathbf{0}$ . can be approximated by the following linearized GRU near $\\mathbf h_{t}=\\mathbf0$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}=\\mathbf{A}\\mathbf{h}_{t-1}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the fixed point $\\mathbf{h}^{*}=\\mathbf{0}$ is locally stable. ", "page_idx": 18}, {"type": "text", "text": "Proof. By employing Taylor series expansion and linearizing $\\mathbf{h}_{t}$ with respect to $\\mathbf h_{t-1}$ at $\\mathbf{h}_{t-1}=$ $\\mathbf{x}_{t}\\,=\\,\\mathbf{0}$ , A in (55) is obtain as $\\begin{array}{r}{\\frac{1}{4}\\mathbf{U_{h}}+\\frac{1}{2}\\mathbf{I}}\\end{array}$ . Then, from (55) we get that $\\mathbf{h}_{t}\\,=\\,\\mathbf{A}^{t}\\mathbf{h}_{0}$ . Since $\\mathbf{A}^{t}$ is dependent on the $t$ -th powers of the eigenvalues of $\\mathbf{A}$ , the behavior of the linearized GRU is determined by the eigenvalues of A. According to the Hartman-Grobman theorem, the behavior of a dynamical system near a hyperbolic fixed point is homeomorphic to the behavior of the linearized system. Therefore, when $|\\lambda_{1}(\\mathbf{A})|<1$ , the fixed point $\\mathbf{h}^{*}=\\mathbf{0}$ becomes a hyperbolic fixed point. Thus, a GRU cell can be approximated as $\\mathbf{h}_{t}=\\bar{\\mathbf{A}}^{t}\\mathbf{h}_{0}$ . Then, local stability is determined by the spectral radius $|\\lambda_{1}(\\mathbf{A})|<\\overline{{1}}$ and then we have $|{\\bf h}_{t}-{\\bf h}_{0}|_{t\\rightarrow\\infty}={\\bf0}$ and the fixed point $\\mathbf{h}^{*}=\\mathbf{0}$ is locally stable. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Theorem 3. When $\\sigma_{1}(\\mathbf{U}_{h})<2$ , a GRU cell is locally stable at a fix point $\\mathbf{h}^{*}=\\mathbf{0}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Relying on Lemma1 and Lemma2, we need to satisfy $\\begin{array}{r}{|\\lambda_{1}(\\frac{1}{4}\\mathbf{U}_{h}+\\frac{1}{2}\\mathbf{I})|<1}\\end{array}$ . $\\begin{array}{r}{|\\lambda_{1}(\\frac{1}{4}{\\bf U}_{h}+{\\bf\\Delta}}\\end{array}$ $\\begin{array}{r}{\\frac{1}{2}\\mathbf{I})\\vert=\\vert\\frac{1}{4}\\lambda_{1}(\\mathbf{U}_{h})+\\frac{1}{2}\\vert}\\end{array}$ due to the properties of eigenvalues. From the triangle inequality we also have $\\begin{array}{r}{|\\frac14\\lambda_{1}^{-}({\\mathbf U}_{h})+\\frac12|\\stackrel{-}{\\le}\\frac14|\\lambda_{1}({\\mathbf U}_{h})|+\\frac12}\\end{array}$ . Using Weyl\u2019s inequality for its singular and eigenvalues, we have $\\left|\\lambda_{1}(\\mathbf{U}_{h})\\right|\\leq\\sigma_{1}(\\mathbf{U}_{h})$ and therefore $\\begin{array}{r}{\\frac{1}{4}|\\lambda_{1}(\\mathbf{U}_{h})|+\\frac{1}{2}\\leq\\frac{1}{4}|\\sigma_{1}(\\mathbf{U}_{h})|+\\frac{1}{2}}\\end{array}$ . Thus, we need to satisfy $\\begin{array}{r}{\\frac{1}{4}|\\sigma_{1}({\\mathbf U}_{h})|+\\frac{1}{2}<1}\\end{array}$ condition and conclude $\\begin{array}{r}{\\frac{1}{4}|\\lambda_{1}(\\mathbf{U}_{h})|+\\frac{1}{2}\\,\\leq\\,\\frac{1}{4}|\\sigma_{1}(\\mathbf{U}_{h})|+\\frac{1}{2}\\,<\\,1}\\end{array}$ . This can be achieved by $\\bar{\\sigma_{1}({\\mathbf{U}}_{h})}<2$ . ", "page_idx": 18}, {"type": "text", "text": "The GRU cell is considered as a nonlinear dynamical system with $\\dot{\\mathbf h}_{t}=f(\\mathbf h_{t},\\mathbf x_{t})$ model, where one can drive $f$ from (51)-(54). We have shown the conditions for local stability of $\\dot{\\mathbf{h}}=f(\\mathbf{h},\\mathbf{0})$ in the last paragraph. Now we show $\\dot{\\bf h}=f({\\bf h},{\\bf x})$ is locally stable if $\\mathbf{x}_{t}\\rightarrow\\mathbf{0}$ as $t\\to\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "As we have the stability condition for $\\dot{\\mathbf{h}}=f(\\mathbf{h},\\mathbf{0})$ , there exists ${\\mathbf V}({\\mathbf h})$ that is a Lyapunov function for this system such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{V}(\\mathbf{h})\\geq\\mathbf{0},\\mathrm{and}\\;\\;\\dot{\\mathbf{V}}(\\mathbf{h})=\\frac{d\\mathbf{V}}{d\\mathbf{h}}.f(\\mathbf{h},\\mathbf{0})\\leq-\\mathbf{W}(\\mathbf{h})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for a positive definite ${\\mathbf W}({\\mathbf h})$ . When the input variable $\\mathbf{x}$ is not zero, the system equation becomes $\\dot{\\bf h}=f({\\bf h},{\\bf x})$ and the derivative of $\\mathbf{V}(\\mathbf{h})$ becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\dot{\\mathbf V}(\\mathbf h)=\\frac{d\\mathbf V}{d\\mathbf h}.f(\\mathbf h,\\mathbf x)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "but for a small $\\mathbf{x}_{t}$ , we can write: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\mathbf{V}}(\\mathbf{h})\\leq-\\mathbf{W}(\\mathbf{h})+\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\epsilon=|\\frac{d\\mathbf{V}}{d\\mathbf{h}}.(f(\\mathbf{h},\\mathbf{x})-f(\\mathbf{h},\\mathbf{0}))|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\epsilon\\to0$ as $\\mathbf{x}_{t}\\rightarrow\\mathbf{0}$ , utilizing Taylor series expansion of $f(\\mathbf{h},\\mathbf{x})$ near ${\\bf x}={\\bf0}$ [44]. Then, a $\\mathbf{s}\\,\\mathbf{x}_{t}\\to\\mathbf{0}$ , for any $\\epsilon\\geq0$ , there exists a $T$ such that for all $t\\geq T$ , $|\\mathbf{x}_{t}|<\\epsilon$ and therefore we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\mathbf{V}}(\\mathbf{h})\\leq-\\mathbf{W}(\\mathbf{h})+\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for sufficiently small $\\epsilon$ , $\\dot{\\mathbf{V}}(\\mathbf{h})$ remains negative. ", "page_idx": 18}, {"type": "text", "text": "Therefore, we need to demonstrate that the inputs of the GRU cells in (6) and (8) approach 0 as $t\\to\\infty$ . As discussed in the main script, we apply a Wishart prior distribution on the covariance matrices, guiding them towards a scaled identity matrix. Over time, this scale diminishes towards zero, causing the covariance matrices\u2014and consequently, the zero bias $\\mathrm{Conv}(.)$ operators in (6) and (8)\u2014to converge to 0 as $t\\to\\infty$ . This implies that the system tends to behave deterministically after prolonged observation, while the stability of the GRU cell is guaranteed. Informally, Theorem 3 requires us to asymptotically shrink the input covariance matrices of our GRU cells toward zero. While this shrinkage is reasonable as we expect the system to behave deterministically in the long run, it imposes a limitation on posterior inference. We acknowledge this as an open problem for future research. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "A.4.4 Proof of theorem 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem. The modified weight matrix ${\\mathbf{U}}_{h}$ obtained from (iii) step above, is a solution of the following optimization problem: $m i n_{\\mathbf{U}_{h}}||\\hat{\\mathbf{U}}_{h}-\\mathbf{U}_{h}||_{F}^{2}$ , s.t. $\\sigma_{1}(\\mathbf{U}_{h})<2-\\delta$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Relying on [45], due to the matrices properties we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{h}\\left[\\sigma_{i}(\\hat{\\mathbf{U}}_{h})-\\sigma_{i}(\\mathbf{U}_{h})\\right]^{2}\\leq||\\hat{\\mathbf{U}}_{h}-\\mathbf{U}_{h}||_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Assuming that the first $s$ singular values of $\\hat{\\mathbf{U}}_{h}$ are greater than 2, we have $\\begin{array}{r}{\\sum_{i=1}^{h}\\left[\\sigma_{i}(\\hat{\\mathbf{U}}_{h})\\right.-}\\end{array}$ $\\begin{array}{r}{\\sigma_{i}(\\mathbf{U}_{h})\\big]_{-}=\\sum_{i=1}^{s}\\left[\\sigma_{i}(\\hat{\\mathbf{U}}_{h})-(2-\\delta)\\right]}\\end{array}$ . According to (61), we need to show that $\\begin{array}{r}{\\sum_{i=1}^{s}\\left[\\sigma_{i}(\\hat{\\mathbf{U}}_{h})-\\right.}\\end{array}$ $\\left(2-\\delta\\right)\\bigr]^{2}$ is equal to its upper bound $||\\hat{\\mathbf{U}}_{h}-\\mathbf{U}_{h}||_{F}^{2}$ , i.e. the optimum solution. ", "page_idx": 19}, {"type": "text", "text": "Using the singular value decomposition properties, we have $||\\hat{\\mathbf{U}}_{h}\\!-\\!\\mathbf{U}_{h}||_{F}^{2}=||\\mathbf{W}\\mathbf{A}\\mathbf{V}\\!-\\!\\mathbf{W}\\overline{{\\mathbf{A}}}\\mathbf{V}||_{F}^{2}=$ ||W $(\\mathbf{A}-\\overline{{\\mathbf{A}}})\\mathbf{V}|\\vert_{F}^{2}=t r(\\mathbf{V}^{*}(\\mathbf{A}-\\overline{{\\mathbf{A}}})\\mathbf{W}^{*}\\mathbf{W}(\\mathbf{A}-\\overline{{\\mathbf{A}}})\\mathbf{V})=t r(\\mathbf{V}^{*}(\\mathbf{A}-\\overline{{\\mathbf{A}}})(\\mathbf{A}-\\overline{{\\mathbf{A}}})\\mathbf{V})=t r(\\mathbf{V}\\mathbf{V}^{*}(\\mathbf{A}-\\overline{{\\mathbf{A}}})\\mathbf{V}).$ $\\begin{array}{r}{\\overline{{\\mathbf{A}}})(\\mathbf{A}-\\overline{{\\mathbf{A}}}))=t r\\big((\\mathbf{A}-\\overline{{\\mathbf{A}}})(\\mathbf{A}-\\overline{{\\mathbf{A}}}))=\\sum_{i=1}^{s}\\big[\\sigma_{i}(\\hat{\\bf U}_{h})-(2-\\delta)\\big]^{2}.}\\end{array}$ \u53e3 ", "page_idx": 19}, {"type": "text", "text": "A.4.5 Proof of theorem 5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem. We can satisfy the constraint introduced in Theorem 3 by applying the Gershgorin circle theorem as follows. ", "page_idx": 19}, {"type": "text", "text": "(i) In the $i$ -th row of $\\hat{\\mathbf{U}}_{h}$ , sum up the non diagonal elements as $\\begin{array}{r}{i_{d}=\\sum_{j=1,j\\neq i}^{h}|\\hat{\\mathbf{U}}_{h}(i,j)|}\\end{array}$ . Do (i) for the all rows. ", "page_idx": 19}, {"type": "text", "text": "(ii) For each $\\hat{\\mathbf{U}}_{h}(i,i)$ assign a circle cluster centered at $\\hat{\\mathbf{U}}_{h}(i,i)$ with $i_{d}$ radius in the complex plane. (iii) Merge the clusters which have intersection and repeat step (iii) until no cluster can be merged any more. Finally the set of clusters $\\mathcal{C}=\\{\\mathcal{C}_{1},...,\\mathcal{C}_{k}\\}$ with length of $k\\leq h$ is obtained. ", "page_idx": 19}, {"type": "text", "text": "(iv) Define the radius of $\\mathcal{C}_{j}$ cluster as $d_{\\mathcal{C}_{j}}=m a x(\\hat{\\mathbf{U}}_{h}(i,i)+i_{d}|\\forall\\hat{\\mathbf{U}}_{h}(i,i)\\in\\mathcal{C}_{j})$ . Calculate the radius of all $k$ clusters in step (iv). ", "page_idx": 19}, {"type": "text", "text": "(v) For each cluster $\\mathcal{C}_{j}$ , if $(\\hat{\\mathbf{U}}_{h}(i,i)+d_{\\mathcal{C}_{j}}|\\forall\\hat{\\mathbf{U}}_{h}(i,i)\\in\\mathcal{C}_{j})>2$ , shift $\\hat{\\mathbf{U}}_{h}(i,i)\\gets(2-d\\boldsymbol{c}_{j}-\\delta)$ for a small $\\delta$ . Otherwise, keep $\\hat{\\mathbf{U}}_{h}(i,i)$ unchanged. Do step (v) for all clusters and their elements, i.e. $\\hat{\\mathbf{U}}_{h}(i,i)$ . ", "page_idx": 19}, {"type": "text", "text": "These five steps, ensures the new $\\hat{\\mathbf{U}}_{h}$ follows the constraint introduced in Theorem 3. ", "page_idx": 19}, {"type": "text", "text": "Proof. The five steps elaborated above are direct results of the generalized Gershgorin circle theorem. This approach is faster than SVD method introduced in the paper, while it is not the best solution of the optimization problem introduced in the Theorem 4. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "A.5 Discussion About KL Term of Mode Collapse ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The term introduced in the mode collapse handling section is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-K L\\big(p_{i}(\\check{\\mathbf{F}})||p_{j}(\\check{\\mathbf{F}}).p r i_{j}(\\check{\\mathbf{F}})\\big)+K L\\big(p_{i}(\\check{\\mathbf{F}})||p r i_{i}(\\check{\\mathbf{F}})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $p_{i}(\\check{\\mathbf{F}})\\,\\sim\\,{\\mathcal{M N}}_{n\\times n}(\\check{\\mathbf{F}}^{i},{\\bf I},{\\bf I})$ and $p_{j}(\\check{\\mathbf{F}})\\,\\sim\\,\\mathcal{M N}_{n\\times n}(\\check{\\mathbf{F}}^{j},\\mathbf{I},\\mathbf{I})$ , are matrix normal distributions with priors $p r i_{i}(\\check{\\mathbf{F}})\\sim\\mathcal{M N}_{n\\times n}(\\mathrm{Diag}(\\mathbf{m}_{i}),\\mathbf{I},\\mathbf{I})$ and $p r i_{j}(\\check{\\mathbf{F}})\\sim\\mathcal{M N}_{n\\times n}(\\mathrm{Diag}(\\mathbf{m}_{j}),\\mathbf{I},\\mathbf{I})$ , respectively. We can expand (62) as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{~\\=~}\\displaystyle\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log\\frac{p_{j}(\\breve{\\mathbf{F}}).p r i_{j}(\\breve{\\mathbf{F}})}{p_{i}(\\breve{\\mathbf{F}})}+\\log\\frac{p_{i}(\\breve{\\mathbf{F}})}{p r i_{i}(\\breve{\\mathbf{F}})}\\bigg)\\ d\\breve{\\mathbf{F}}}&{\\quad{\\ \\mathrm{()}}\\displaystyle\\int(\\breve{\\mathbf{F}})^{2}d\\breve{\\mathbf{F}}}\\\\ &{\\mathrm{~\\=~}\\displaystyle\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log\\frac{p_{j}(\\breve{\\mathbf{F}}).p r i_{j}(\\breve{\\mathbf{F}})}{p r i_{i}(\\breve{\\mathbf{F}})}\\bigg)\\ d\\breve{\\mathbf{F}}}&{\\quad{\\ \\mathrm{()}}\\displaystyle\\int(\\breve{\\mathbf{F}})^{2}d\\breve{\\mathbf{F}}}\\\\ &{\\mathrm{~\\=~}\\displaystyle\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log p_{j}(\\breve{\\mathbf{F}})\\bigg)\\ d\\breve{\\mathbf{F}}+\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log p r i_{j}(\\breve{\\mathbf{F}})\\bigg)\\ d\\breve{\\mathbf{F}}-\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log p r i_{i}(\\breve{\\mathbf{F}})\\bigg)\\ d\\breve{\\mathbf{F}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Relying on [45], we can simplify each term in (65) as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log p_{j}(\\breve{\\mathbf{F}})\\bigg)\\ d\\breve{\\mathbf{F}}=-\\frac{1}{2}\\Big[\\big(\\mathrm{vec}(\\breve{\\mathbf{F}}^{i})-\\mathrm{vec}(\\breve{\\mathbf{F}}^{j})\\big)^{T}\\big(\\mathrm{vec}(\\breve{\\mathbf{F}}^{i})-\\mathrm{vec}(\\breve{\\mathbf{F}}^{j})\\big)+t r(\\mathbf{I})\\Big]+\\mathrm{const}}\\\\ &{\\qquad}&{\\mathrm{(66)}}\\\\ &{\\qquad}&{\\mathrm{(67)}}\\\\ &{\\qquad}&{\\qquad}\\\\ &{\\int p_{i}(\\breve{\\mathbf{F}})\\bigg(\\log p r i_{j}(\\breve{\\mathbf{F}})\\bigg)\\ d\\breve{\\mathbf{F}}=-\\frac{1}{2}\\Big[\\big(\\mathrm{vec}(\\breve{\\mathbf{F}}^{i})-\\mathrm{vec}(\\mathrm{Diag}(\\mathbf{m}_{j}))\\big)^{T}\\big(\\mathrm{vec}(\\breve{\\mathbf{F}}^{i})-\\mathrm{vec}(\\mathrm{Diag}(\\mathbf{m}_{j}))\\big)+t r}\\\\ &{\\qquad}&{\\mathrm{(68)}}\\\\ &{\\qquad}&{\\qquad=-\\frac{1}{2}||\\breve{\\mathbf{F}}^{i}-\\mathrm{Diag}(\\mathbf{m}_{j})||_{F}^{2}-\\frac{1}{2}t r(\\mathbf{I})+\\mathrm{const}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\int p_{i}(\\tilde{\\mathbf{F}})\\bigg(\\log p r i_{j}(\\tilde{\\mathbf{F}})\\bigg)\\ d\\tilde{\\mathbf{F}}=-\\frac{1}{2}\\Big[\\big(\\mathrm{vec}(\\tilde{\\mathbf{F}}^{i})-\\mathrm{vec}(\\mathrm{Diag}(\\mathbf{m}_{i}))\\big)^{T}\\big(\\mathrm{vec}(\\tilde{\\mathbf{F}}^{i})-\\mathrm{vec}(\\mathrm{Diag}(\\mathbf{m}_{i}))\\big)+t r(\\mathrm{vec}(\\tilde{\\mathbf{F}}^{i}))\\Big]\\ d\\mathbf{F}}&\\\\ &{}&{(70)}\\\\ &{}&{=-\\frac{1}{2}\\|\\tilde{\\mathbf{F}}^{i}-\\mathrm{Diag}(\\mathbf{m}_{i})\\|_{F}^{2}-\\frac{1}{2}t r(\\mathbf{I}).}&{\\qquad\\qquad\\qquad\\qquad\\qquad(71)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we can rewrite (62) as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac12||\\check{\\mathbf{F}}^{i}-\\mathrm{Diag}(\\mathbf{m}_{i})||_{F}^{2}-\\frac12||\\check{\\mathbf{F}}^{i}-\\mathrm{Diag}(\\mathbf{m}_{j})||_{F}^{2}-\\frac12||\\check{\\mathbf{F}}^{i}-\\check{\\mathbf{F}}^{j}||_{F}^{2}-\\frac12t r(\\mathbf{I})+\\mathrm{const}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is half of (12) plus a constant. ", "page_idx": 20}, {"type": "text", "text": "A.6 Noise Generation Process ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our experiments, we use a time-correlated noise generation scheme to demonstrate the system\u2019s noise robustness. This method introduces a sequence of factors, $f_{t}$ , of the same length as the data sequence, making the noise factors correlated over time. Let $f_{0}\\,\\sim\\,\\mathcal{U}(0,1)$ and $f_{t+1}\\;=$ $\\operatorname*{min}(\\operatorname*{max}(0,f_{t}+r_{t}),\\bar{1})$ with $r_{t}\\sim\\mathcal{U}(-0.2,0.2)$ , where $f_{0}$ is the initialized factor and $\\boldsymbol{\\mathcal{U}}$ is the uniform distribution. Two thresholds, $t_{1}\\sim\\mathcal{U}(0,0.25)$ and $t_{2}\\sim\\mathcal{U}(0.75,1)$ , are defined. Values of $f_{t}$ less than $t_{1}$ are set to 0, values greater than $t_{2}$ are set to 1, and the remaining values are linearly scaled within the range $[0,1]$ . The original observation at time $t$ , $\\mathbf{o}_{t}$ , is then given by $\\mathbf{o}_{t}=f_{t}\\mathbf{i}_{t}+(1-f_{t})\\mathbf{i}_{t}^{p n}$ , where $\\mathbf{i}_{t}$ is the true image at time $t$ and $\\mathbf{i}_{t}^{p n}$ is the pure noise generated for time $t$ . ", "page_idx": 20}, {"type": "text", "text": "A.7 Hyperparameters and training ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In all experiments, we used the Adam optimizer [46] on an NVIDIA GeForce GTX 1050 Ti with 16GB RAM. To ensure optimal hyperparameters, we conducted a grid search. We searched for the initial learning rate in the range of 0.001 to 0.2 with increments of 0.005, selecting the one that yielded the highest log-likelihood. We chose an initial learning rate of 0.006 with an exponential decay rate of 0.9 every 10 epochs. Backpropagation through time [47] was employed to compute gradients, given the use of GRU cells in our structure. The gradients are applied to GRU cells with the constraint explained in theorem 3, where we use the spectral theorem mentioned in the main script . We applied the layer normalization technique [48] to stabilize the dynamics within the recurrent structure and normalize the filter response. The $\\mathrm{Elu}+1$ activation function was used to ensure the positivity of the diagonal elements of the process noise and covariance matrices. ", "page_idx": 20}, {"type": "image", "img_path": "z4duW3KzlD/tmp/caef41bbfa8e2abf65d85eefce3dda2161f875a92ac34af61726a33e73d1a482.jpg", "img_caption": ["Figure 9: Proposed detailed architecture. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "To prevent the model from getting stuck in poor local minima, such as focusing excessively on reconstruction rather than learning the dynamics obtained through filtering-smoothing, we find it beneficial to employ two training techniques for end-to-end learning: ", "page_idx": 21}, {"type": "text", "text": "1- Generating time correlated noisy sequences as consecutive observations, forces the model to learn the dynamics instead of focusing on reconstruction, e.g. figures 11 and 13 in appendix. 2- During the initial epochs, the system focuses on learning the auto-encoder and globally learned parameters, such as $\\check{\\mathbf{F}}^{k}$ and $\\check{\\mathbf{H}}^{k}$ , while excluding the Dynamic Net parameters $\\alpha_{t}(\\mathbf{x}_{t-1}\\bar{)}$ . Afterwards, all parameters are jointly learned. This approach enables the system to initially acquire good embeddings and meaningful latent vectors before integrating the learning of $K$ different dynamics variables. ", "page_idx": 21}, {"type": "text", "text": "In our experiments, we set $K=15$ to accommodate the complexity of the latent space, with each latent representing a unique dynamical scenario. Generally, parameter tuning is not challenging when the GIN is sufficiently flexible, as it can learn to prune unused elements through the Dynamic Net. ", "page_idx": 21}, {"type": "text", "text": "A.8 Proposed architecture, qualitative comparison with the SOTA and empirical complexity analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A.8.1 Architectures ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The proposed detailed structure is shown in figure 9. To design the dynamic network, we use an MLP with 60 hidden units and a ReLU activation function, and a softmax activation in the last layer. The state mean, with a size of $n$ , is the input to the dynamic network, which outputs $k$ coefficients. The structures of the encoder and decoder are shown in Table 5. In this table, $m$ represents the transferred observation dimension, with various values considered in the results. For state estimation tasks, the output size (out) is 4, 4, 8 and 12 for the single-pendulum, bouncing ball, double-pendulum and KITTI visual odometry experiments, respectively. For the imputation task, the number of hidden units in the KG and SG networks is set to 40 and 30, respectively. The convolutional layer applied over the covariance matrix has 8 filters with a kernel size of 5 and zero bias. ", "page_idx": 21}, {"type": "table", "img_path": "z4duW3KzlD/tmp/143c0db16e608569c5bf79db7f2865d2e37ba23916efab7178891f16d759ae89.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Gradient Explosion Experiment. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Referring to table 4, higher values of $\\delta$ deteriorate performance because the convergence speed of the linearized GRU hidden state depends on $\\sigma_{1}(\\mathbf{U}_{h})$ (see Eq. (55) in the appendix), which is projected to $2-\\delta$ . Therefore, longer time dependencies in the data are eliminated for high $\\delta$ . On the other hand, smaller values of $\\delta$ close to zero make the model more prone to noise [10]. Therefore, it is necessary to tune an appropriate $\\delta$ within the range $0<\\delta<2$ , which is much easier than tuning the unbounded $\\theta$ . ", "page_idx": 22}, {"type": "text", "text": "A.8.2 Empirical running times and parameters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We present the number of parameters for the utilized cell structures in our experiments and their corresponding empirical running times for 1 epoch in Table 6. In the first row of each model structure, we set the number of parameters approximately equal to our GIN to demonstrate the GIN\u2019s superior performance with the same parameter count. The extra running time of EM-variational approaches, like KVAE, is due to the use of classic Bayesian equations, which significantly increase running time for higher-dimensional observations. However, the GIN avoids this issue. The number of parameters in the GIN is noticeably lower than in other memory cells, such as LSTM and GRU, and EM-variational methods. This efficiency is achieved by converting high-dimensional sparse covariance matrices into lower-dimensional covariance matrices using a convolutional operator. ", "page_idx": 22}, {"type": "text", "text": "A.8.3 Qualitative comparison of the GIN to recent related work. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Switching Linear Dynamical System models (SLDS) decompose complex, nonlinear time series data into sequences of simpler, reusable dynamical modes. Fitting an SLDS to data enables us to learn a flexible nonlinear generative model and parse data sequences into coherent discrete units. [9] is a SLDS model that incorporates additional latent variables to switch among different linear dynamics. However, this approach relies on Gibbs sampling for inferring the parameters, rendering it impractical for large datasets due to scalability issues. ", "page_idx": 22}, {"type": "text", "text": "Auto-regressive state space models (ARSSMs) are a class of dynamic models commonly used in time series analysis and forecasting. In these models, the evolution of a system over time is described by a set of states, by observing all data. Auto-Regressive (AR) Hidden Markov Models (AR-HMM) explain time series structures by defining a mapping from past observations to the current observation. [39] presents an ARHMM approach where target values are used directly as inputs. However, this reliance on target values makes the model more susceptible to noise. ", "page_idx": 22}, {"type": "text", "text": "Toward learning state space (system identification), a number of works including those by [49\u2013 53] have proposed algorithms for learning Gaussian Process SSMs (GPSSMs) through maximum likelihood estimation using the iterative EM algorithm. where flitering-smoothing with a set of fixed parameters $\\gamma$ is conducted (E step), and then updating the set of parameters $\\gamma$ is performed such that the obtained likelihood is maximized (M step). Frigola et al. [51] obtain sample trajectories from the smoothing distribution and then, conditioned on this trajectory, conduct the M step for the model\u2019s parameters. In the context of System Identification (SI), the GIN is akin to a Hammerstein-Wiener (HW) model [34], as it estimates system parameters directly from observations while transferring the observations through nonlinear functions. ", "page_idx": 22}, {"type": "table", "img_path": "z4duW3KzlD/tmp/43bd272db734ba7e7564907f146372f9f376ac2f7a2d2bca8f164123b94258e5.jpg", "table_caption": ["Table 6: Empirical running times and parameters of experiments. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "In Table 7, we compare various algorithms to evaluate their ability to handle high-dimensional observations, learn dynamics, estimate state accurately, provide uncertainty estimates while handling noisy data, manage missing data, and perform direct optimization. Classic LGSSMs, such as the EKF and UKF, rely on the linearization of transition and emission equations and apply classic Bayesian updates over the linearized system with respect to the states. In other words, classic LGSSMs are model-based. In contrast, the GIN uses a data-driven network to learn parameters, offering a different approach from the traditional model-based methods. ", "page_idx": 23}, {"type": "table", "img_path": "z4duW3KzlD/tmp/3a3751975993b28613392b1bc19aa53c63456dbff88c3d933f4be65b5fd7b422.jpg", "table_caption": ["Table 7: Learning the parameters in LGSSM is shown with $\\times/\\sqrt{}$ because general LGSSMs, e.g. UKF and EKF, are not able to learn the parameters. However, in our setting and parameterization we use a data driven-based network for obtaining $(\\mathbf{F},\\mathbf{H})$ to make LGSSMs comparable with the GIN for high dimensional observation experiments. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.9 Visualization and The Imputation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Graphical results of informed, uninformed and noisy observations for image imputation task for both single and double pendulum experiments can be found in 10, 11, 12 and 13 figures. Inference for the trained smoothing distribution of all experiments are in 14, 15, 16, , 17, 18, 19, 20, 21, 22, 23 and 24 figures, where we generated samples from the smoothing estimated states distribution, $\\mathbf{\\left(s}_{t}|\\mathbf{o}_{1:T}\\right)$ . Then we fti density on the generated samples. This visualization shows the effectiveness of the GIN in reducing the uncertainty of the estimates compare to LGSSM and KVAE. ", "page_idx": 24}, {"type": "image", "img_path": "z4duW3KzlD/tmp/59c6570ffa04dc22ded02b49af95f5eb353cd138cbb06eab43a05bdcd3013192.jpg", "img_caption": ["Figure 10: Informed(left column) and uninformed(right column) image imputation task for the single pendulum experiments. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "z4duW3KzlD/tmp/ef49b94f29109307dfc35f70bca3976ebc1800b0947f32ac178ec801d5e36dad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 11: Image imputation task for the single pendulum experiment exposed to the noisy observations, where the generated noise has correlation with the time. Each figure, beginning from top to bottom, indicates the ground truth, noisy observation and the imputation results of the GIN. ", "page_idx": 25}, {"type": "image", "img_path": "z4duW3KzlD/tmp/da1c5fa3f58a869ae5677d87b8f49537ffc1750cc31b81153ed20fefecf1feb3.jpg", "img_caption": ["Figure 12: Informed(left column) and uninformed(right column) image imputation task for the double pendulum experiments. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "z4duW3KzlD/tmp/7579a98ea94fe34dc4c4554b439a295c9039e87d2bc6508b9537bf59c97de298.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: Image imputation task for the double pendulum experiment exposed to the noisy observations, where the generated noise has correlation with the time. Each figure, beginning from top to bottom, indicates the ground truth, noisy observation and the imputation results of the GIN. ", "page_idx": 26}, {"type": "image", "img_path": "z4duW3KzlD/tmp/a3367d7d5b72df1ddd66e87d4cbd9d24ae50e3049bfb1f9545feed8a5ab417a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 14: Inference for the single pendulum $s1$ position at 100-th time step. Generated samples from smoothened distribution, $\\left(s1_{100}|\\mathbf{o}_{1:150}\\right)$ , trained by the GIN, LGSSM and KVAE, respectively. The dashed red line $p(g t1_{100})$ is the ground truth state with distribution of $\\delta(s1_{100}\\!-\\!0.7)$ . We calculate the sample mean and fti a distribution on the samples for further visualization and comparison purpose. ", "page_idx": 27}, {"type": "image", "img_path": "z4duW3KzlD/tmp/b0e4e839d63fcd911d896409993bf1a472d04dbad648e657fe618887e5a4fefb.jpg", "img_caption": ["Figure 15: Inference for the single pendulum $s2$ position at 100-th time step. Generated samples from smoothened distribution, $\\left(s2_{100}|\\mathbf{o}_{1:150}\\right)$ , trained by the GIN, LGSSM and KVAE, respectively. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "z4duW3KzlD/tmp/1134171468ee4543d7ca9b0107afc9195d09c839125edcbdeaeca94cd972578a.jpg", "img_caption": ["Figure 16: Generated samples from the trained smoothened joint distribution of the single pendulum position, $(s1,s2)$ , at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "z4duW3KzlD/tmp/700a5ebe8000a6012e84b761971976820c5e91c18ba0d70619a191d658e8d1ce.jpg", "img_caption": ["Figure 17: Inference for the double pendulum $s1$ position at 100-th time step. Generated samples from smoothened distribution, $\\left(s1_{100}|\\mathbf{o}_{1:150}\\right)$ , trained by the GIN, LGSSM and KVAE, respectively. The dashed red line $p(g t1_{100})$ is the ground truth state with distribution of $\\delta(s1_{100}-0.35)$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "z4duW3KzlD/tmp/204dfa0a4abd4668e5864e3070c34c80b7e120fd2a8360fdbd535ab0f11576bb.jpg", "img_caption": ["Figure 18: Inference for the double pendulum $s2$ position at 100-th time step. Generated samples from smoothened distribution, $(s2_{100}|\\mathbf{o}_{1:150})$ , trained by the GIN, LGSSM and KVAE, respectively. The dashed red line $p(g t2_{100})$ is the ground truth state with distribution of $\\delta(s2_{100}-0.35)$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "z4duW3KzlD/tmp/4fb61708909dfdd1222c49e5ee8e0a5ac236e7889384493e781332cfb1781c7c.jpg", "img_caption": ["Figure 19: Generated samples from the trained smoothened joint distribution of the double pendulum first joint position, $(s1,s2)$ , at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "z4duW3KzlD/tmp/02431421a0dc31b9d8f5eed647a47f703c9a7bcfbcc40d20aa3413b5b054a4fa.jpg", "img_caption": ["Figure 20: Inference for the double pendulum $s3$ position at 100-th time step. Generated samples from smoothened distribution, $(s3_{100}|\\mathbf{o}_{1:150})$ , trained by the GIN, LGSSM and KVAE, respectively. The dashed red line $p(g t3_{100})$ is the ground truth state with distribution of $\\delta(s3_{100}-1)$ . "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "z4duW3KzlD/tmp/7f6dee55d69d109b33abe561f6a95e811fd990a45c01a02dd302334c3942191e.jpg", "img_caption": ["Figure 21: Inference for the double pendulum $s4$ position at 100-th time step. Generated samples from smoothened distribution, $\\left(s4_{100}|\\mathbf{o}_{1:150}\\right)$ , trained by the GIN, LGSSM and KVAE, respectively. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "z4duW3KzlD/tmp/c37292217a9635b7befda36910f22ff45766b5646fdb0ddcdfe2dd3b598f1696.jpg", "img_caption": ["Figure 22: Generated samples from the trained smoothened joint distribution of the double pendulum second joint position, $(s3,s4)$ , at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "z4duW3KzlD/tmp/1f99bbf41232009b3770cf0a9cf220a47918bf71c2df3fb3e53abcc9bbd0735a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 23: Inference for the visual odometry $s1$ and $s2$ positions at 100-th time step. Generated samples from smoothened distribution, $\\left(s1_{100},s2_{100}|\\mathbf{o}_{1:500}\\right)$ , trained by the GIN, LGSSM and KVAE, respectively. The dashed red line $p(g t1_{100},g t2_{100})$ is the ground truth state with distribution of $(\\delta(s1_{100}+50),\\delta(s2_{100}-10))$ . ", "page_idx": 30}, {"type": "text", "text": "A.10 MSE Results for The State Estimation and Estimated KG-SG ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The MSE results for the single pendulum, double pendulum, and bouncing ball experiments are shown in Tables 8 and 9 and 10. In addition to the dynamics equation (5), where the $\\hat{\\mathbf{F}}$ matrix includes the effects of the process noise, two other solutions introduced in Section 5 are included in the MSE results. Parameterizing the process noise as in (24) with a GRU cell is indicated by $\\mathrm{{GRU}(\\mathbf{Q})}$ , while another parameterization, shown as MLP(Q), is done using (26). ", "page_idx": 30}, {"type": "image", "img_path": "z4duW3KzlD/tmp/76132724810d65acc4806fafddbb945cc8de6f1eb8dba138c620e18fed41a5a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 24: Generated samples from the trained smoothened joint distribution of the visual odometry joint position, $(s1,s2)$ , at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point. ", "page_idx": 31}, {"type": "text", "text": "Algorithm Training the GIN ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Input: Observations $\\mathbf{o}_{1:T}$ , last posteriors $\\left(\\hat{\\mu}_{0:T-1|0:T-1},\\hat{\\Sigma}_{0:T-1|0:T-1}\\right)$   \n$\\begin{array}{r}{\\big(\\mathbf{x}_{0:T-1|0:T-1}\\big)=S a m p l e(\\big(\\hat{\\mu}_{0:T-1|0:T-1},\\hat{\\Sigma}_{0:T-1|0:T-1}\\big))}\\end{array}$   \n$\\alpha_{1:T}=D$ ynamic Net $\\left(\\mathbf{x}_{0:T-1\\mid0:T-1}\\right)$   \nObtain $\\hat{\\mathbf{F}}_{1:T}$ and $\\hat{\\mathbf{H}}_{1:T}$ by (5)   \n$\\left(\\hat{\\mu}_{1:T|0:T-1},\\hat{\\Sigma}_{1:T|0:T-1}\\right)=P r e d i c t i o n\\left(\\hat{\\mu}_{0:T-1|0:T-1},\\hat{\\Sigma}_{0:T-1|0:T-1},\\hat{\\mathbf{F}}_{1:T}\\right)$   \n$\\left(\\mathbf{w}_{1:T},\\mathbf{r}_{1:T}\\right)=e$ ncoder $\\left(\\mathbf{o}_{1:T}\\right)$   \n$\\hat{\\mathbf{K}}_{1:T}=\\hat{\\pmb{\\Sigma}}_{1:T|0:T-1}\\hat{\\mathbf{H}}_{1:T}\\,\\mathbf{M}_{1:T}\\mathbf{M}_{1:T}^{T},\\ \\ \\mathbf{M}_{1:T}=G R U^{K G}(C o n v(\\hat{\\pmb{\\Sigma}}_{1:T|0:T-1}),\\mathbf{r}_{1:T}\\ )$   \n$\\begin{array}{r l}{\\hat{\\mathbf{J}}_{1:T}=\\hat{\\boldsymbol{\\Sigma}}_{1:T|1:T}\\hat{\\mathbf{F}}_{1:T}^{T}\\mathbf{N}_{1:T}\\mathbf{N}_{1:T}^{T},}&{\\overset{\\cdots}{\\mathbf{N}}_{1:T}=\\overset{\\cdots}{G}R U^{S G}\\left(C o n v(\\hat{\\mathbf{\\Sigma}}_{1:T|0:T-1})\\right)}\\end{array}$   \n$\\left(\\hat{\\mu}_{1:T|1:T},\\hat{\\Sigma}_{1:T|1:T}\\right)=F i l t e r i n g\\ensuremath{\\left(\\hat{\\mu}_{1:T|0:T-1},\\hat{\\Sigma}_{1:T|0:T-1},\\hat{\\mathbf{K}}_{1:T},\\mathbf{w}_{1:T},\\hat{\\mathbf{H}}_{1:T}\\right)}$   \n$\\left(\\hat{\\mu}_{1:T|T},\\hat{\\Sigma}_{1:T|T}\\right)=S m o o t h i n g\\ \\big(\\hat{\\mu}_{1:T|1:T},\\hat{\\Sigma}_{1:T|1:T},\\hat{\\mathbf{J}}_{1:T},\\hat{\\mathbf{F}}_{1:T}\\big)$   \nif (image imputation) then o1:T = decoder $\\big(\\hat{\\mu}_{1:T|T},\\hat{\\Sigma}_{1:T|T}\\big)$ $\\begin{array}{r}{\\mathcal{L}(\\mathbf{o}_{1:T})=\\mathbf{l}}\\end{array}$ Lower bound likelihood $\\left(\\mathbf{o}_{1:T}\\right)$ by using (10)   \nend if   \nif (state estimation) then $\\mathbf{s}_{1:T}=d e c o d e$ r $(\\hat{\\mu}_{1:T|T},\\hat{\\Sigma}_{1:T|T})$ $\\mathscr{L}(\\mathbf{s}_{1:T}|\\mathbf{o}_{1:T})=\\mathbf{L}$ ower bound likelihood $\\left(\\mathbf{s}_{1:T}\\right)$ by using (11)   \nend if   \nBackward Propagation by updating GRU cells with the steps in Theorem 3 or Theorem 4. ", "page_idx": 31}, {"type": "table", "img_path": "z4duW3KzlD/tmp/857de5530ec47385eab9611aab84d1b59d1a9d259b2a7ab21c74db155cf93e66.jpg", "table_caption": ["Table 8: MSE for single pendulum experiment. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "z4duW3KzlD/tmp/4cc6c49a2c5d7d6d910eac5ad3de1444d8745fb56bf7380258c93bd8fdb10363.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "z4duW3KzlD/tmp/e62411262eaf1ce7b1e6b9cacea2b5e19ce6ad75964c524f2c4b0439b2b246ae.jpg", "table_caption": ["Table 9: MSE for double pendulum experiment. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "z4duW3KzlD/tmp/2ba554f4fb67688330ab37608c4ed4f3ba1265fd970a391f620ac8ad195ea107.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "z4duW3KzlD/tmp/2fdf7f783c322dff7980226f28bb91592352749a055f0a6def179aabbbf9bcf1.jpg", "table_caption": ["Table 10: MSE for bouncing ball experiment. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "z4duW3KzlD/tmp/6fd8dd6eb8917ebd7c26f7c26b97b714a9c374ca6878620d161249961b2612fe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "To demonstrate the simplicity of our proposed GIN, we include intuitive inference code with Tensorflow library. The code runs with Python $3.6+$ . The entire code to reproduce the experiments are available in Github repository. ", "page_idx": 35}, {"type": "text", "text": "A.11.1 Python intuitive code. ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "# Inference with   \n2   \n3 import tensorflow.keras as k   \n4 import Prediction   \n5 import Filtering   \n6 import Smoothing   \nimport DynamicsNetwork   \n8 import Encoder   \n9 import Decoder   \n0 import DataGen   \n2 class GIN_CELL(k.layers.Layer):   \n3 def __init__(self , initial_states ):   \n4 self.mu_tm1_filt , self. Sigma_tm1_filt $=$ initial_states   \n15 self.filter_states $=$ [[ self.mu_tm1_filt , self. Sigma_tm1_filt ]]   \n16 def call(self , inputs):   \n17 w_1:T, r_1:T $=$ inputs   \n18 for w_t , r_t in (w_1:T, r_1:T):   \n19 x_tm1_filt $=$ sample(self.mu_tm1_filt , self. Sigma_tm1_filt )   \n20 F_hat_t , H_hat_t $=$ DynamicsNetwork (x_tm1_filt)   \n21 mu_t_pri , Sigma_t_pri $=$ Prediction(F_hat_t , H_hat_t ,   \n22 self.mu_tm1_filt , self. Sigma_tm1_filt )   \n23 mu_t_filt , Sigma_t_filt $=$ Filtering(mu_t_pri ,...   \n24 Sigma_t_pri , w_t , r_t , H_hat_t)   \n25 self.filter_states .append ([ mu_t_filt , Sigma_t_filt ])   \n26 self.mu_tm1_filt $=$ x_t_filt   \n27 self. Sigma_tm1_filt $=$ Sigma_t_filt   \n28 mu_1:T_smooth , Sigma_1:T_smooth $=$ Smoothing( F_hat_1:T ,   \n29 self.filter_states , Sigma_1:T_pri ,)   \n30 return mu_1:T_smooth , Sigma_1:T_smooth   \n31   \n32 class GIN(k.models.Model):   \n33 def _init__(self , initial_states ):   \n34 self.mu_0_filt , self. Sigma_0_filt $=$ initial_states   \n35 self.GIN_CELL_OBJ $=$ self.GIN_CELL(self.x_0_filt , self.   \nSigma_0_filt)   \n36 self.Encoder $=$ Encoder   \n37 self.Decoder $=$ Decoder   \n38   \n39 def call(self , o_1:T):   \n40 w_1:T, r_1:T $=$ self.Encoder(o_1:T)   \n41 mu_1:T_smooth , Sigma_1:T_smooth $=$ self. GIN_CELL_OBJ (w_1:T, r_1   \n:T)   \n42 o_1:T, s_1:T $=$ self.Decoder(mu_1:T_smooth , Sigma_1:T_smooth)   \n43 return o_1:T, s_1:T   \n44   \n45 initial_states $=$ mu_0_filt , Sigma_0_filt   \n46 GIN_OBJ $=\\tt G I N$ (initial_states )   \n47   \n48 Data $=$ DataGen ()   \n49 o_1:T, s_1:T $=$ GIN_OBJ(Data) ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The claims we have made in abstract and introduction are elaborated and achieved in the main paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We discuss the limitations and assumptions necessary for Theorems 3 and 4. In the appendix, where we provide the proofs, we reiterate the requirements that the algorithm must meet. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The proofs of the mentioned theorems and mathematical claims are all provided in appendix section. In the main text, we refer the readers to the proofs after introducing each theorem-claim. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the required information for running the experiments are provided in the supplementary materials and anonymous github repository. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide instructions to synthesis the data and run the experiments. All details are provided in anonymous github repository. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide a detailed section in appendix to argue about network architecture, hyper parameter optimization, experiment setting, etc.. This addresses the question you asked. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We include confidence interval in our numerical experiments to show the reliability of the results. Initialization approaches are explained with details in the main script. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?   \nAnswer: [Yes]   \nJustification: We provide detailed configuration of our machines for running the experiments. ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We are following all the ethics provided by NeurIPS. ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?   \nAnswer: [NA]   \nJustification: We believe this work does not have negative impact on the social communities. ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?   \nAnswer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}]