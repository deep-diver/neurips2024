[{"figure_path": "Mi853QaJx6/figures/figures_4_1.jpg", "caption": "Figure 2: The overlap rate of model-agnostic worst-k prompts across different models. The low result indicates a minimal occurrence of universally poor prompts.", "description": "This figure shows the overlap rate of the worst-performing prompts (k=1,2,3,4) across different large language models (LLMs).  The overlap rate is calculated as the proportion of worst-k prompts that are common to all models being compared. Low overlap rates, as observed in the figure, indicate a lack of universally bad prompts across all models. This suggests that prompts considered \"worst\" are often model-specific rather than universally poor.", "section": "4.1 Model-agnostic Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_5_1.jpg", "caption": "Figure 3: IoU fluctuation across varying sensitive case thresholds for diverse model sets. The IoU drops below 0.2 across all models, indicating a scarcity of model-agnostic traits.", "description": "This figure shows how the intersection over union (IoU) of sensitive cases changes with different thresholds for various model sets.  A sensitive case is defined as one where the difference between the best and worst model performance exceeds a certain threshold.  The IoU measures the overlap of sensitive cases between different model sets. The graph reveals that as the threshold increases, indicating more sensitivity, the IoU consistently drops below 0.2 for all model sets. This indicates that the prompts that cause the worst performance are mostly model-specific, highlighting a lack of model-agnostic traits in identifying consistently poor-performing prompts.", "section": "4.1 Model-agnostic Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_6_1.jpg", "caption": "Figure 4: Distribution of Pearson correlation coefficients between model performance and prompt perplexity (left) and prompt\u2019s Min-K% Prob (right) for Llama-family models across all cases. The absolute values of correlation in the ranges of (0, 0.3], (0.3, 0.6], and (0.6, 1] respectively denote weak/no correlation, moderate correlation, and strong correlation.", "description": "This figure shows the distribution of Pearson correlation coefficients between model performance and two different prompt features: prompt perplexity and prompt Min-K% Prob.  The analysis is performed for Llama family models across all cases in the benchmark. The x-axis represents the correlation coefficient, categorized into ranges indicating weak/no correlation, moderate correlation, and strong correlation.  The y-axis represents the percentage of cases falling into each correlation category.  The figure helps to understand if there's a relationship between these prompt characteristics and the model\u2019s performance.", "section": "4.2 Model-dependent Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_6_2.jpg", "caption": "Figure 5: (a) Visualization of Llama-2-7B-chat model\u2019s hidden states using 2-dimensional PCA. The color gradient, from light to dark, represents the ranking of model performance on each case\u2019s 11 prompts, from low to high. (b) Probing Llama-2-7B-chat model\u2019s hidden states for prompt scoring. The x-axis stands for training steps. The y-axis represents the accuracy of the model\u2019s predictions, quantified as the proportion of correctly judged prompt pairs out of all test pairs.", "description": "This figure presents a visualization of Llama-2-7B-chat model's hidden states using PCA. The left panel shows the 2D PCA plot, where the color gradient indicates the model's performance ranking for each case's 11 prompts. The right panel shows the results of probing the model's hidden states for prompt scoring, illustrating the accuracy of predictions over training steps.", "section": "4.2 Model-dependent Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_13_1.jpg", "caption": "Figure 6: Distribution of Pearson correlation coefficients between model performance and prompt perplexity (left) and prompt\u2019s Min-K% Prob (right) for Gemma family models and Mistral-7B model across all cases.", "description": "This figure displays the distribution of Pearson correlation coefficients between model performance and two prompt features: perplexity and Min-K% Prob.  It shows the correlation strength for each feature across various models (Gemma family and Mistral-7B), categorized into strong, moderate, weak, and no correlation.  The figure helps to analyze the relationship between prompt characteristics and model performance, illustrating whether these features are effective in predicting model performance on various prompts.", "section": "4.2 Model-dependent Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_13_2.jpg", "caption": "Figure 5: (a) Visualization of Llama-2-7B-chat model\u2019s hidden states using 2-dimensional PCA. The color gradient, from light to dark, represents the ranking of model performance on each case\u2019s 11 prompts, from low to high. (b) Probing Llama-2-7B-chat model\u2019s hidden states for prompt scoring. The x-axis stands for training steps. The y-axis represents the accuracy of the model\u2019s predictions, quantified as the proportion of correctly judged prompt pairs out of all test pairs.", "description": "This figure visualizes Llama-2-7B-chat model's hidden states using 2D PCA in (a). The color gradient shows the ranking of model performance across different prompts, darker being better.  Part (b) shows the results of probing the model's hidden states to predict prompt performance, plotting accuracy over training steps.", "section": "4.2 Model-dependent Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_14_1.jpg", "caption": "Figure 5: (a) Visualization of Llama-2-7B-chat model\u2019s hidden states using 2-dimensional PCA. The color gradient, from light to dark, represents the ranking of model performance on each case\u2019s 11 prompts, from low to high. (b) Probing Llama-2-7B-chat model\u2019s hidden states for prompt scoring. The x-axis stands for training steps. The y-axis represents the accuracy of the model\u2019s predictions, quantified as the proportion of correctly judged prompt pairs out of all test pairs.", "description": "This figure visualizes Llama-2-7B-chat model's hidden states using PCA, showing a color gradient representing performance rankings across 11 prompts per case.  A second part probes these hidden states to predict prompt quality based on training steps, with accuracy shown on the y-axis.", "section": "4.2 Model-dependent Analysis"}, {"figure_path": "Mi853QaJx6/figures/figures_14_2.jpg", "caption": "Figure 5: (a) Visualization of Llama-2-7B-chat model's hidden states using 2-dimensional PCA. The color gradient, from light to dark, represents the ranking of model performance on each case's 11 prompts, from low to high. (b) Probing Llama-2-7B-chat model's hidden states for prompt scoring. The x-axis stands for training steps. The y-axis represents the accuracy of the model's predictions, quantified as the proportion of correctly judged prompt pairs out of all test pairs.", "description": "This figure visualizes Llama-2-7B-chat model's hidden states using PCA and explores the potential of using hidden states for prompt scoring.  The left subplot (a) shows a 2D PCA visualization of the hidden states, where the color intensity reflects the model's performance ranking across different prompts. The right subplot (b) presents the accuracy of a reward model trained to predict prompt quality based on hidden states, plotted against the training steps.", "section": "4.2 Model-dependent Analysis"}]