[{"figure_path": "kPGNE4CrTq/figures/figures_9_1.jpg", "caption": "Figure 1: Numerical results on synthetic data. In short, each dot in the figure represents the average value of 10 independent trials (i.e., experiments) of compressed matrices \u03a6(1), ..., \u03a6(10) on a given tuple of parameters (K, d, n, SNR, m). The shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the ratio of training loss after and before compression, i.e., ||\u03a6Y \u2013 WX|||||Y \u2013 ZX|| versus the number of rows m. It is obvious that the ratio converges to one as m increases, which validates the result presented in Theorem 1. In the second row, we plot percision@3 versus the number of rows. As we can observe, the proposed algorithm outperforms CD and FISTA.", "description": "The figure shows numerical results on synthetic data to validate the theoretical findings and illustrate the efficiency and accuracy of the proposed algorithm. The first row presents the ratio of training loss after and before compression versus the number of rows (m), demonstrating convergence to one as m increases.  The second row shows precision@3 (the percentage of correctly identified supports in the predicted output) versus m, indicating the superiority of the proposed algorithm over other methods.", "section": "4 Numerical Experiments"}, {"figure_path": "kPGNE4CrTq/figures/figures_24_1.jpg", "caption": "Figure 1: Numerical results on synthetic data. In short, each dot in the figure represents the average value of 10 independent trials (i.e., experiments) of compressed matrices \u03a6(1), ..., \u03a6(10) on a given tuple of parameters (K, d, n, SNR, m). The shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the ratio of training loss after and before compression, i.e., ||\u03a6Y \u2013 WX|||||Y \u2013 ZX|| versus the number of rows m. It is obvious that the ratio converges to one as m increases, which validates the result presented in Theorem 1. In the second row, we plot percision@3 versus the number of rows. As we can observe, the proposed algorithm outperforms CD and FISTA.", "description": "The figure displays numerical results obtained from synthetic data to validate the proposed algorithm's performance.  It shows four sets of graphs. The first row shows the ratio of training loss before and after compression plotted against the number of rows (m) in the compressed matrix.  This demonstrates the relationship between compression and training loss, supporting Theorem 1. The second row displays precision@3 (a measure of the algorithm's accuracy) against the number of rows (m).  This comparison highlights the superiority of the proposed algorithm over existing methods (CD and FISTA).", "section": "4 Numerical Experiments"}, {"figure_path": "kPGNE4CrTq/figures/figures_25_1.jpg", "caption": "Figure 3: This figure reports the numerical results on real data \u2013 EURLex-4K. Each dot in the figure represents 10 independent trials (i.e., experiments) of compressed matrices \u03a6(1),...,\u03a6(10) on a given tuple of parameters (s,m). The curves in each panel correspond to the averaged values for the proposed Algorithm and baselines over 10 trials; the shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the output distance versus the number of rows. In the second row, we plot the precision versus the number of rows, and we cannot observe significant differences between these prediction methods.", "description": "This figure presents the results of numerical experiments on the EURLex-4K dataset, comparing the proposed algorithm to baselines (OMP, CD, FISTA) for different sparsity levels (s=1 and s=3).  The top row shows the output distance (a measure of prediction accuracy) against the number of rows (m) in the compressed matrix.  The bottom row shows precision (a measure of the number of correctly identified features) against the number of rows (m).  The results illustrate the performance of the proposed algorithm relative to the baselines. The shaded area represents the standard deviation over 10 trials, suggesting the stability of the algorithm's performance. While there are minor differences, the differences between the algorithms are relatively small.", "section": "4 Numerical Experiments"}, {"figure_path": "kPGNE4CrTq/figures/figures_26_1.jpg", "caption": "Figure 1: Numerical results on synthetic data. In short, each dot in the figure represents the average value of 10 independent trials (i.e., experiments) of compressed matrices \u03a6(1), ..., \u03a6(10) on a given tuple of parameters (K, d, n, SNR, m). The shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the ratio of training loss after and before compression, i.e., ||\u03a6Y \u2013 WX|||||Y \u2013 ZX|| versus the number of rows m. It is obvious that the ratio converges to one as m increases, which validates the result presented in Theorem 1. In the second row, we plot percision@3 versus the number of rows. As we can observe, the proposed algorithm outperforms CD and FISTA.", "description": "The figure presents numerical results obtained from experiments on synthetic data. The experiments assess the performance of the proposed algorithm and its comparison with other baseline algorithms by varying several parameters, including the number of rows (m), signal-to-noise ratio (SNR), and sparsity level (s). The plots show the ratio of training loss before and after compression and the precision@3 metric. The results indicate that the ratio of training losses converges to 1 as m increases, verifying Theorem 1.  The precision@3 metric demonstrates the proposed algorithm's superior performance compared to other baselines.", "section": "4 Numerical Experiments"}]