[{"type": "text", "text": "Solving Sparse & High-Dimensional-Output Regression via Compression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Renyuan Li Zhehui Chen Department of Industrial Systems Engineering & Management Google National University of Singapore zhehuichen@google.com renyuan.li@u.nus.edu ", "page_idx": 0}, {"type": "text", "text": "Guanyi Wang Department of Industrial Systems Engineering & Management National University of Singapore guanyi.w@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-Output Regression (MOR) has been widely used in scientific data analysis for decision-making. Unlike traditional regression models, MOR aims to simultaneously predict multiple real-valued outputs given an input. However, the increasing dimensionality of the outputs poses significant challenges regarding interpretability and computational scalability for modern MOR applications. As a first step to address these challenges, this paper proposes a Sparse & High-dimensional-Output REgression (SHORE) model by incorporating additional sparsity requirements to resolve the output interpretability, and then designs a computationally efficient twostage optimization framework capable of solving SHORE with provable accuracy via compression on outputs. Theoretically, we show that the proposed framework is computationally scalable while maintaining the same order of training loss and prediction loss before-and-after compression under arbitrary or relatively weak sample set conditions. Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-Output Regression (MOR) problem [8, 44] is a preponderant tool for factor prediction and decision-making in modern data analysis. Compared with traditional regression models that focus on a scalar output for each sample, MOR aims to predict multiple outputs $\\bar{\\pmb{y}}\\in\\mathbb{R}^{K}$ simultaneouslybased On a given input $\\pmb{x}\\in\\mathbb{R}^{d}$ ,i.e., ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\boldsymbol y:=\\underset{\\boldsymbol u\\in\\mathcal{Y}}{\\arg\\operatorname*{min}}~\\mathrm{dist}(\\boldsymbol u,\\widehat{\\boldsymbol g}(\\boldsymbol x))~~\\mathrm{with}~~\\widehat{\\boldsymbol g}:=\\underset{\\boldsymbol g\\in\\mathcal{G}}{\\arg\\operatorname*{min}}~\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol\\ell(\\boldsymbol y^{i},\\boldsymbol g(\\boldsymbol x^{i}))~~,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where we use $\\{(\\pmb{x}^{i},\\pmb{y}^{i})\\}_{i=1}^{n}$ to denote its given sample set with $\\pmb{x}^{i}\\in\\mathbb{R}^{d}\\textit{i}$ -th input feature vector and $\\pmb{y}^{i}\\in\\mathbb{R}^{K}$ corresponding output vector, define $\\boldsymbol{\\ell}:\\mathbb{R}^{K}\\times\\mathbb{R}^{K}\\rightarrow\\mathbb{R}$ as the loss function, dist : $\\mathbb{R}^{K}\\stackrel{\\cdot}{\\times}\\mathbb{R}^{K}\\rightarrow\\mathbb{R}$ as some prediction/distance metric, $\\boldsymbol{\\wp}$ as the structure/constraint set for multiple outputs, and $\\mathcal{G}$ as the candidate set for predicting model $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{K}$ . Hence, MOR and its variants have been used for numerous regression tasks with structure requirements on multi-dimensional outputs arising from real applications, such as simultaneous estimation of biophysical parameters from remote sensing images [40], channel estimation through the prediction of several received signals [35], the grounding (e.g., factuality check [16]) in the Large Language Model (LLM, [34, 11]) era, to name but a few. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we are interested in the interpretability issue of high-dimensional outputs obtained from modern MOR tasks. One typical example is raised from algorithmic trading. In particular, in algorithmic trading, MOR helps to construct the portfolio [31] from a large number of financial instruments (e.g., different stocks, futures, options, equities, etc [19]) based on given historical market and alternative data [22]. To be concise, a high-dimensional output in this example could be viewed as a \u201cdecision\", where every component denotes the investment for the corresponding financial instruments. Thus, other than accuracy, quantitative researchers prefer outputs with only a few instruments to enhance interpretability for the underlying decision-making reasons, which naturally introduces a sparse output condition. Similar scenarios apply to other applications, including offline reinforcement learning in robotics[33], discovering genetic variations based on genetic markers[25]. ", "page_idx": 1}, {"type": "text", "text": "As a result, the dramatic growth in output dimensions gives rise to two significant challenges: 1. High-dimensional-output impedes human interpretation for decision-making; 2. Approaches with better computational scalability are desired for training & predicting MOR. Upon these challenges, a conceptual question that motivates this research is: ", "page_idx": 1}, {"type": "text", "text": "How to design a framework that predicts output with enhanced interpretability, better computational scalability, and provable accuracy under a modern high-dimensional-output setting? ", "page_idx": 1}, {"type": "text", "text": "Generally speaking, this paper provides an affrmative answer as a first step to the above question. Before presenting the main contributions, let us first introduce the model that will be studied in this paper. Unlike the classical MOR model, we further assume that given outputs are of high-dimensional (i.e., $d\\,\\ll\\,K)$ , and to address the interpretability issue, these outputs have at most $s$ non-zero components, i.e., $\\lVert\\boldsymbol{y}^{i}\\rVert_{0}\\leq s$ , for all $i\\in[n]$ with some pre-determined sparsity-level $s(\\ll K)$ . Based on such given samples, this paper proposes the (uncompressed) Sparse & High-dimensional-Output REgression (SHORE) model that aims to predict an interpretable high-dimensional output $\\textit{\\textbf{y}}$ (i.e., $s$ -sparse in this paper) via any input feature vector $\\textbf{\\em x}$ . In particular, to be concise and still capture the essential relationship, the proposed (uncompressed) SHORE model predicts $\\textit{\\textbf{y}}$ from $\\textbf{\\em x}$ under a linear model, i.e., $\\begin{array}{r}{\\pmb{y}=\\arg\\operatorname*{min}_{\\|\\pmb{y}\\|_{0}\\leq s}\\mathbf{dist}(\\pmb{y},\\widehat{\\pmb{Z}}\\pmb{x})}\\end{array}$ for some distance metric (see Section 3.1, prediction stage) and the linear regression $\\hat{\\boldsymbol Z}$ is obtained by solving the following linear regression problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{Z}:=\\underset{Z\\in\\mathbb{R}^{K\\times d}}{\\arg\\operatorname*{min}}\\,\\widehat{\\mathcal{L}}_{n}(Z):=\\frac{1}{n}\\|Y-Z X\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ${\\pmb X}:=({\\pmb x}^{1}\\mid\\,\\cdot\\,\\cdot\\,\\mid{\\pmb x}^{n})\\in\\mathbb R^{d\\times n}$ is the input matrix and $\\pmb{Y}:=(\\pmb{y}^{1}\\mid\\,\\cdot\\,\\cdot\\,\\cdot\\,\\mid\\pmb{y}^{n})\\in\\mathbb{R}^{K\\times n}$ is the corresponding column-sparse output matrix. ", "page_idx": 1}, {"type": "text", "text": "1.1  Contributions and Paper Organization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This paper makes the following three main contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a two-stage computationally efficient framework for solving SHORE model. Specifically, the first training stage offers a computationally scalable reformulation on solving SHORE through compression in the output space. The second prediction stage then predicts high-dimensional outputs from a given input by solving a specific sparsity-constrained minimization problem via an efficient iterative algorithm. ", "page_idx": 1}, {"type": "text", "text": "2. We show that for arbitrarily given samples, the training loss in the first stage with compression isbounded by a $1+\\delta$ multiplicative ratio of the training loss for the original one (1) with some positiveconstant $\\delta$ . Additionally, the proposed iterative algorithm in the second stage exhibits global geometric convergence within a neighborhood of the ground-truth output, with a radius proportional to the given sample's optimal training loss. Furthermore, if all samples are drawn from a light-tailed distribution, the generalization error bound and sample complexity remain in the same order for SHORE with output compression. This finding indicates that the proposed framework achieves improved computational efficiency while maintaining the same order of generalization error bounds statistically. ", "page_idx": 1}, {"type": "text", "text": "3. We conduct rich numerical experiments that validate the theoretical findings and demonstrate the efficiency and accuracy of the proposed framework on both synthetic and real-world datasets. ", "page_idx": 1}, {"type": "text", "text": "In summary, this paper studies the SHORE model through computational and statistical lenses and provides a computationally scalable framework with provable accuracy. ", "page_idx": 1}, {"type": "text", "text": "The paper is organized as follows: Section 2 reviews related literature; Section 3 presents our proposed framework and provides theoretical results on sample complexity and generalization error bounds; Section 4 compares the proposed method with existing baselines in a suite of numerical experiments on both synthetic and real instances. Concluding remarks are given in Section 5. ", "page_idx": 2}, {"type": "text", "text": "Notation. Given a positive integer $n$ , we denote $[n]:=\\{1,\\ldots,n\\}$ . We use lowercase letters $a$ as scalars and bold lowercase letters $\\textbf{\\em a}$ as vectors, where $\\mathbf{a}_{i}$ is its $i$ -th component with $i\\in[d]$ , and bold upper case letters $\\pmb{A}$ as matrices. Without specific description, for a $m$ -by- ${\\cdot n}$ matrix $\\pmb{A}$ , we denote $A_{i,j}$ as its $(i,j)$ -th component, $A_{i,:}^{\\top}$ as its $i$ -th row, $A_{:,j}$ as its $j$ -th column. For a symmetric square matrix $\\pmb{A}$ , we denote $\\lambda_{\\operatorname*{max}}(A),\\lambda_{\\operatorname*{min}}(A)$ and $\\lambda_{i}({\\cal A})$ as its maximum, minimum and $i$ -th largest eigenvalue, respectively. We denote $||\\pmb{a}||_{1},||\\pmb{a}||_{2},||\\pmb{a}||_{\\infty},||\\pmb{A}||_{F},||\\pmb{A}||_{\\mathrm{op}}$ as the $\\ell_{1},\\ell_{2},\\ell_{\\infty}$ -norm of a vector $\\textbf{\\em a}$ , the Frobenius norm and the operator norm of a matrix $\\pmb{A}$ , respectively. We denote $\\mathbb{I}(\\cdot)$ as the indicator $\\begin{array}{r}{\\|\\pmb{a}\\|_{0}:=\\sum_{i=1}^{d}\\mathbb{I}(\\pmb{a}_{i}\\neq0)}\\end{array}$ $\\ell_{0}$ $\\mathrm{supp}(a):=\\{i\\in[d]\\mid a_{i}\\neq0\\}$ $\\mathcal{V}_{s}^{K}:=\\{\\pmb{y}\\in\\mathbb{R}^{K}\\mathrm{~}|\\mathrm{~}\\|\\pmb{y}\\|_{0}\\leq s\\}$ of $s$ -sparse vectors, $\\check{\\mathbb{B}}_{2}(\\pmb{c};\\pmb{\\rho}):=\\{\\pmb{y}\\in\\dot{\\mathbb{R}}^{\\mathcal{K}}\\ |\\ \\|\\pmb{y}-\\pmb{c}\\|_{2}\\leq\\rho\\}$ as a closed $\\ell_{2}$ -ball with center $^c$ and radius $\\rho,{\\mathcal{N}}(\\mu,\\sigma^{2})$ as a Gaussian distribution with mean $\\mu$ and covariance $\\sigma^{2}$ ", "page_idx": 2}, {"type": "text", "text": "For two sequences of non-negative reals $\\{f_{n}\\}_{n\\ge1}$ and $\\{g_{n}\\}_{n\\ge1}$ , we use $f_{n}\\lesssim g_{n}$ to indicate that there is a universal constant $C>0$ such that $f_{n}\\leq C g_{n}$ for all $n\\geq1$ . We use standard order notation $f_{n}=O(g_{n})$ to indicate that $f_{n}\\lesssim g_{n}$ and $f_{n}=\\widetilde{O}_{\\tau}(g_{n})$ to indicate that $f_{n}\\lesssim g_{n}\\ln^{c}(1/\\tau)$ for some universal constants $\\tau$ and $c$ .Throughout, we use $\\epsilon,\\delta,\\tau,c,c_{1},c_{2},\\ldots$ and $C,C_{1},C_{2},...$ to denote universal positive constants, and their values may change from line to line without specific comments. ", "page_idx": 2}, {"type": "text", "text": "2 Literature Review ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-output regression (MOR) and its variants have been studied extensively over the past decades.   \nIn this section, we focus on existing works related to our computational and statistical results. ", "page_idx": 2}, {"type": "text", "text": "Computational part. Existing computational methods for solving MOR can be, in general, classified into two categories [8], known as problem transformation methods and algorithm adaptation methods. Problem transformation methods (e.g., Binary Relevance (BR), multi-target regressor stacking (MTRS) method [37], regression chains method [37]) aim to transform MOR into multiple single-output regression problems. Thus, any state-of-the-art single-output regression algorithm can be applied, such as ridge regression [15], regression trees [9], and etc. However, these transformation methods ignore the underlying structures/relations between outputs, which leads to higher computational complexities. In contrast, algorithm adaptation methods focus more on the underlying structures/relations between outputs. For instance, [36] investigates input component selection and shrinkage in multioutput linear regression; [1] later couples linear regressions and quantile mapping and thus captures joint relationships among variables. However, the output dimension considered in these works is relatively small compared with modern applications, and their assumptions concerning low-dimensional structure of outputs are hard to verify. To overcome these shortages, we consider high-dimensional-output regression with only an additional sparsity requirement on outputs. ", "page_idx": 2}, {"type": "text", "text": "Statistical part. There are numerous works concerning statistical properties of traditional or multioutput regressions. [18] gives sharp results on \"out-of-sample\" (random design) prediction error for the ordinary least squares estimator of traditional linear regression. [45] proposes an empirical risk minimization framework for large-scale multi-label learning with missing outputs and provides excess risk generalization error bounds with additional bounded constraints. [28] investigates the generalization performance of structured prediction learning and provides generalization error bounds on three different scenarios, i.e., Lipschitz continuity, smoothness, and space capacity condition. [27] designs an efficient feature selection procedure for multiclass sparse linear classifiers (a special case for SHORE with sparsity-level $s=1$ ), and proves that the proposed classifiers guarantee the minimax generalization error bounds in theory. A recent paper [42] studies transfer learning via multi-task representation learning, a special case in MOR, which proves statistically optimistic rates on the excess risk with regularity assumptions on the loss function and task diversity. In contrast with these works, our contributions concentrate on how generalization error bounds changebefore and after thecompressionunderrelativelyweakconditions onthelossfunctionandunderlyingdistributions. ", "page_idx": 2}, {"type": "text", "text": "Specific results in MLC. MLC is an important and special case for MOR with $\\{0,1\\}$ valued output per dimension, i.e., $\\pmb{y}\\,\\in\\,\\{0,1\\}^{K}$ , and thus, in this paragraph, we use labels to replace outputs. Here, we focus on dimensionality reduction techniques on outputs, in particular, the compressed sensing and low-rank conditions on the output matrix $\\mathbf{Y}$ . The idea of compressed sensing rises from signal processing, which maps the original high-dimensional output space into a smaller one while ensuring the restricted isometry property (RIP). To the best of our knowledge, the compressed sensing technique is first used in [17] to handle a sparse expected output $\\mathbb{E}[{\\pmb y}|{\\pmb\\bar{x}}]$ Later, [39, 12] propose Principle Label Space Transformation (PLST) and conditional PLST through singular value decomposition and canonical component analysis respectively. More recently, many new compression approaches have been proposed, such as robust bloom filter [13], log time log space extreme classification [23], merged averaged classifiers via hashing [32], etc. Additionally, computational efficiency and statistical generalization bounds can be further improved when the output matrix $\\mathbf{Y}$ ensures a low-rank condition. Under such a condition, [45] provides a general empirical risk minimization framework for solving MLC with missing labels. Compared with the above works,this paper studies MOR under a sparse & high-dimensional-output setting without additional correlation assumptions or low-rank assumptions for output space, and then provides a completestory through a computational andstatistical lens. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Two-Stage Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This subsection presents a general framework for solving SHORE and then the computational complexity for the proposed framework with/without compression. Given a set of training samples $\\{(\\pmb{x}^{i},\\pmb{y}^{i})\\}_{i=1}^{n}$ as described in Section 1, the framework can be separated into two stages: (compressed) training stage & (compressed) prediction stage. ", "page_idx": 3}, {"type": "text", "text": "Training stage. In the first training stage, the framework finds a compressed regressor by solving a linear regression problem with compressed outputs. In particular, the framework compresses the original large output space ( $K$ -dim) to a smaller \"latent\" output space ( $m$ -dim) by left-multiplying a so-called \"compressed\" matrix $\\Phi\\in\\mathbb{R}^{m\\times K}$ to outputs. Thus, the compressed version of training stage in SHORE can be represented as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{W}}:=\\underset{\\pmb{W}\\in\\mathbb{R}^{m\\times d}}{\\arg\\operatorname*{min}}\\ \\widehat{\\mathcal{L}}_{n}^{\\Phi}(\\pmb{W}):=\\frac{1}{n}\\|\\Phi\\pmb{Y}-\\pmb{W}\\pmb{X}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We would like to point out that the idea of compressing the output space into some smaller intrinsic dimension has been used in many existing works, e.g., [17, 39, 12] mentioned in Section 2. ", "page_idx": 3}, {"type": "text", "text": "Prediction stage. In the second prediction stage, given any input $\\pmb{x}\\in\\mathbb{R}^{d}$ , the framework predicts a sparse output $\\hat{\\pmb y}$ by solving the following prediction problem based on the learned regressor $\\widehat{W}$ in the training stage, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{y}}(\\widehat{\\pmb{W}}):=\\arg\\operatorname*{min}_{\\pmb{y}}\\|\\pmb{\\Phi}\\pmb{y}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}^{2}\\;\\mathrm{~s.t.~}\\;\\pmb{y}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{V}_{s}^{K}$ is the set of $s$ -sparse vectors in $\\mathbb{R}^{K}$ ,_and $\\mathcal{F}$ is some feasible set to describe additional requirements of $\\textit{\\textbf{y}}$ . For example, by letting $\\mathcal{F}$ be $\\mathbb{R}^{K},\\mathbb{R}_{+}^{K},\\{0,1\\}^{K}$ , the intersection $\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}$ denotes the set of $s$ -sparse output, non-negative $s_{\\mathrm{{}}}$ -sparse output, $\\{0,1\\}$ -valued $s$ -sparse output, respectively. We use $\\widehat{\\pmb{y}}(\\widehat{\\pmb{W}})$ (shorthanded in $\\widehat{\\pmb{y}}.$ ) to specify that the predicted output is based on the regressor W. To solve the proposed prediction problem (3), we utilize the following projected gradient descent method (Algorithm 1), which could be viewed as a variant/generalization of existing iterative thresholding methods [6, 21] for nonconvex constrained minimization. In particular, step 4 incorporates additional constraints from $\\mathcal{F}$ other than sparsity into consideration, which leads to non-trivial modifications in designing efficient projection oracles and convergence analysis. Later, we show that the proposed Algorithm 1 ensures a near-optimal convergence (Theorem 2 and Theorem 4) while greatly reduces the computational complexity (Remark 2) of the prediction stage for solving compressed SHORE. ", "page_idx": 3}, {"type": "text", "text": "Before diving into theoretical analysis, we first highlight the differences between the proposed prediction stage (3), general sparsity-constrained optimization (SCO), and sparse regression in the following remark. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Proposed prediction stage v.s. General SCo: To be clear, the SCO here denotes the followingminimizationproblem $\\mathrm{min_{\\parallel\\alpha\\parallel_{0}\\leq k}}\\parallel\\!A\\alpha-\\beta\\|_{2}^{2}$ . Thus, the prediction stage is a special case of generalSCO problem.In particular, thepredicted stage takes a random projection matrix $\\Phi$ with restricted isometry property (RIP) to be its $\\pmb{A}$ and uses $\\widehat{\\pmb{W}}\\pmb{x}$ with $\\widehat{W}$ obtainedfromthecompressed training-stage to be its $\\beta$ .As a result (Theorem 2 and Theorem 4), the proposed Algorithm 1 for prediction stageensures a globally linear convergenceto a ball with center $\\hat{\\pmb y}$ (optimal solution of the prediction-stage) and radius $O(||\\Phi{\\widehat{\\pmb{y}}}-{\\widehat{W}}{\\pmb{x}}||_{2})$ which might not hold for general SCO problems. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Proposed prediction stage v.s. Sparse regression: Although the proposed prediction stage and sparse high-dimensional regression share a similar optimization formulation $\\mathrm{min}_{||\\boldsymbol{\\beta}||_{2}\\leq k}\\ \\ ||\\dot{\\mathbf{Y}}-\\boldsymbol{X}^{\\top}\\boldsymbol{\\beta}||_{2}^{2}$ the proposed prediction stage (3) is distinct from the sparse regression in the following parts: ", "page_idx": 4}, {"type": "text", "text": "$(I)$ Underlying Model: Most existing works about sparse high-dimensional regression assume that samplesarei.i.d.generatedfromthelinearrelationship $\\pmb{Y}^{-}=\\pmb{X}^{\\top}\\beta^{*}+\\pmb{\\epsilon}$ with underlying sparse ground truth $\\beta^{*}$ .In the proposed prediction stage,we do not assume additional underlying models on samples if there is no further specific assumption. The problem we studied in the predicted stage takestherandomprojectionmatrix $\\Phi$ with restricted isometry property $(R I P)$ as its $X^{\\top}$ (whereas $X^{\\top}$ in sparse regression does not ensure $R I P$ ), and uses $\\widehat{\\pmb{W}}\\pmb{x}$ with $\\widehat{W}$ obtained from the compressed training-stage as its $\\mathbf{Y}$ ", "page_idx": 4}, {"type": "text", "text": "(2) Problem Task: The sparse regression aims to recover the sparse ground truth $\\beta^{*}$ givenasample set $\\{(\\pmb{x}^{i},\\pmb{y}^{i})\\}_{i=1}^{n}$ with n ii.d. samples. In contrast, the task of the proposed prediction stage is to predict a sparse high-dimensional output $\\hat{\\pmb y}$ given a random projection matrix $\\Phi$ andasingleinput $\\textbf{\\em x}$ . As a quick summary, some typical and widely used iterative algorithms [38, 3, 4, 291 for sparse regression cannot be directly applied to the proposed prediction stage. ", "page_idx": 4}, {"type": "text", "text": "Then, we provide the computational complexity with and without the compression for the proposed two-stage framework to complete this subsection. ", "page_idx": 4}, {"type": "text", "text": "Remark 2. Training stage: Conditioned on $X X^{\\top}$ is invertible, the compressed regressor W has a closed form solution $\\widehat{\\pmb{W}}=\\Phi\\pmb{Y}\\pmb{X}^{\\top}(\\pmb{X}\\pmb{X}^{\\top})^{-1}$ with overall computational complexity ", "page_idx": 4}, {"type": "equation", "text": "$$\nO(K m n+m n d+n d^{2}+d^{3}+m d^{2})\\approx O(K m n).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compared with the computational complexity of finding $\\hat{\\boldsymbol Z}$ from the uncompressed SHORE (1) ", "page_idx": 4}, {"type": "equation", "text": "$$\nO(K n d+n d^{2}+d^{3}+K d^{2})\\approx O(K(n+d)d),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "solving. $\\widehat{W}$ enjoys a smaller computational complexity on the training stage if $m\\ll d$ In later analysis (see Section 3.2), $m=\\dot{\\mathrm{{O}}}(\\delta^{-2}\\cdot s\\log(\\frac{\\kappa}{\\tau}))$ with somepredetermined constants $\\delta,\\tau$ and sparsity-level $s\\ll d$ .thus in many applications with large output space, the condition $m\\ll d$ holds. ", "page_idx": 4}, {"type": "text", "text": "Prediction stage: The computational complexity of each step-3 in Algorithm $^{\\,l}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nO(K m+K+K m+K)\\approx O(K m).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theprojectioninstep- $^{.4}$ is polynomially solvablewithcomputational complexity $O(K\\operatorname*{min}\\{s,\\log K\\})$ (see proof in Appendix A.5.1). Thus, the overall computational complexity of Algorithm $^{\\,I}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nO(K(m+\\operatorname*{min}\\{s,\\log K\\})T).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Comparedwiththecomplexity $O(K(d+\\operatorname*{min}\\{s,\\log K\\}))$ ofpredicting $\\hat{\\pmb y}$ fromtheuncompressed SHORE (1), the compressed version enjoys a smaller complexity on the prediction stage $i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n(m+\\operatorname*{min}\\{s,\\log K\\})T\\ll d+\\operatorname*{min}\\{s,\\log K\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In later analysis (see Theorem 2), since $m=O(\\delta^{-2}\\cdot s\\log(\\frac{K}{\\tau}))$ with predetermined constants $\\delta,\\tau$ sparsir-level $s\\ll d$ and $\\begin{array}{r}{T=O(\\log[\\frac{\\|\\widehat{\\pmb{y}}-\\pmb{v}^{(0)}\\|_{2}}{\\|\\pmb{\\Phi}\\widehat{\\pmb{y}}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}}])}\\end{array}$ from inequality 5,wehave conditon holas. ", "page_idx": 4}, {"type": "text", "text": "Whole computational complexity: Based on the analysis of computational complexity above, we concludethatwhentheparameters $(K,d,m,T)$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nK>K^{1/3}>d\\gg O(\\delta^{-2}\\log(K/\\tau)\\cdot T)=m T,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the compressed SHORE enjoys a better computational complexity with respect to the original one (1). ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Projected Gradient Descent (for Second Stage) ", "page_idx": 5}, {"type": "text", "text": "Input: Regressor $\\widehat{W}$ , input sample $\\textbf{\\em x}$ , stepsize $\\eta$ , total iterations $T$ ", "page_idx": 5}, {"type": "text", "text": "1: Initialize point $\\pmb{v}^{(0)}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}$   \n2: for $t=0,1,\\ldots,T-1$ : do   \n3: Update $\\widetilde{\\pmb{v}}^{(t+1)}=\\pmb{v}^{(t)}-\\eta\\cdot\\pmb{\\Phi}^{\\top}(\\pmb{\\Phi}v^{(t)}-\\widehat{\\pmb{W}}\\pmb{x})$   \n4: Project $\\begin{array}{r}{\\pmb{v}^{(t+1)}=\\Pi(\\widetilde{\\pmb{v}}^{(t+1)}):=\\arg\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}}\\|\\pmb{v}-\\widetilde{\\pmb{v}}^{(t+1)}\\|_{2}^{2}.}\\end{array}$   \n5: end for ", "page_idx": 5}, {"type": "text", "text": "3.2  Worst-Case Analysis for Arbitrary Samples ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin this subsection by introducing the generalization method of the compressed matrix $\\Phi$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. Given an m-by. $K$ compressed matrix $\\Phi$ all components $\\Phi_{i,j}$ for $1\\leq i\\leq m$ and $1\\le j\\le K$ , are i.i.d. generated from a Gaussian distribution ${\\mathcal{N}}(0,1/m)$ ", "page_idx": 5}, {"type": "text", "text": "Before presenting the main theoretical results, let us first introduce the definition of restricted isometry property (RIP, [1o]), which is ensured by the generalization method (Assumption 1). ", "page_idx": 5}, {"type": "text", "text": "Definition 1. $(\\mathcal{V},\\delta)$ RIP: A $m$ -by- $K$ matrix $\\Phi$ is said to be $(\\mathcal{V},\\delta)$ -RIP over a given set of vectors $\\mathcal{V}\\subseteq\\mathbb{R}^{K}$ \uff0c $i f,$ for every $\\pmb{v}\\in\\mathcal{V}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n(1-\\delta)\\|\\pmb{v}\\|_{2}^{2}\\leq\\|\\pmb{\\Phi}\\pmb{v}\\|_{2}^{2}\\leq(1+\\delta)\\|\\pmb{v}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the rest of the paper, we use $(s,\\delta)$ -RIPto denote $(\\mathcal{V}_{s}^{K},\\delta)$ -RIP. Recall $\\mathcal{V}_{s}^{K}=\\{\\pmb{v}\\in\\mathbb{R}^{K}\\mathrm{~}|\\mathrm{~}||\\pmb{v}||_{0}\\leq s\\}$ istheset of $s$ -sparsevectors. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. From Johnson-Lindenstrauss Lemma [43], for any $\\delta\\in(0,1)$ ,any $\\tau\\in(0,1)$ , and any finite vector set $|\\nu|<\\infty$ if the number of rows $m\\ge O\\left(\\delta^{-2}\\cdot\\log(\\frac{|\\mathcal{V}|}{\\tau})\\right)$ , then the compressed matrix $\\Phi$ generated by Assumption $^{\\,I}$ satisfies $(\\mathcal{V},\\delta)$ -RIP with probability at least $1-\\tau$ ", "page_idx": 5}, {"type": "text", "text": "Now, we are poised to present the first result on training loss defined in (2). ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. For any $\\delta\\in(0,1)$ and $\\tau\\in(0,1)$ , suppose compressed matrix $\\Phi$ follows Assumption $^{\\,l}$ with $\\begin{array}{r}{m\\geq O(\\frac{1}{\\delta^{2}}\\cdot\\log(\\frac{K}{\\tau}))}\\end{array}$ . We have the following inequality for training loss ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\Phi Y-\\widehat{W}X\\|_{F}^{2}\\leq(1+\\delta)\\cdot\\|Y-\\widehat{Z}X\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "holds with probability at least $1-\\tau$ where $\\widehat{Z},\\widehat{W}$ are optimal solutions for the uncompressed (1) and compressed SHORE (2), respectively. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 1 is presented in Appendix A.1. In short, Theorem 1 shows that the optimal training loss for the compressed version is upper bounded within a $\\left(1+\\delta\\right)$ multiplicative ratio with respect to the optimal training loss for the uncompressed version. Intuitively, Theorem 1 implies that SHORE remains similar performances for both compressed and compressed versions, while the compressed version saves roughly $O(K n(d-m)+K\\bar{d}^{2})$ computational complexity in the training stage from Remark 2. Moreover, the lower bound condition on $\\begin{array}{r}{m\\geq O(\\frac{1}{\\delta^{2}}\\cdot\\log(\\frac{K}{\\tau}))}\\end{array}$ ensures that the generated compressed matrix $\\Phi$ is $(1,\\delta)$ -RIP with probability at least $1-\\tau$ . For people of independent interest, Theorem 1 only needs $(1,\\delta)$ -RIP (independent with the sparsity level) due to the unitary invariant property of $\\Phi$ from Assumption 1 (details in Appendix A.1). Additionally, due to the inverse proportionality between $m$ and $\\delta^{2}$ , for fixed $K$ and $\\tau$ , the result can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n||\\Phi Y-\\widehat{W}X||_{F}^{2}\\leq\\left(1+O(1/\\sqrt{m})\\right)\\cdot||Y-\\widehat{Z}X||_{F}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is verified in our experiments 4. ", "page_idx": 5}, {"type": "text", "text": "We then present the convergence result of the proposed Algorithm 1 for solving prediction problem (3). Theorem 2.For any $\\delta\\in(0,1)$ and $\\tau\\in(0,1)$ supposethecompressedmatrix $\\Phi$ follows Assumption $^{\\,I}$ with $\\begin{array}{r}{m\\geq O(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau}))}\\end{array}$ With a fixed stepsie $\\begin{array}{r}{\\eta\\in\\left(\\frac{1}{2-2\\delta},1\\right)}\\end{array}$ the following inequality ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\widehat{\\boldsymbol{y}}-\\boldsymbol{v}^{(t)}\\|_{2}\\leq c_{1}^{t}\\cdot\\|\\widehat{\\boldsymbol{y}}-\\boldsymbol{v}^{(0)}\\|_{2}+\\frac{c_{2}}{1-c_{1}}\\cdot\\|\\Phi\\widehat{\\boldsymbol{y}}-\\widehat{\\boldsymbol{W}}\\boldsymbol{x}\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "holds for all $t\\in[T]$ simultaneously with probability at least $1-\\tau_{i}$ where $c_{1}:=2-2\\eta+2\\eta\\delta<1$ .3 some positive constant strictly smaller than $^{\\,l}$ and $c_{2}:=2\\eta\\sqrt{1+\\delta}$ is some constant. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 2 is given in Appendix A.2. Here, the lower bound condition on the number of rows $m$ ensures that the generated compressed matrix $\\Phi$ is $(3s,\\delta)$ -RIP with probability at least $1-\\tau$ by considering a $\\delta/2$ -net cover of set $\\dot{\\mathcal{V}}=\\mathcal{V}_{3s}^{K}\\cap\\mathbb{B}_{2}(\\mathbf{0};1)$ from Johnson-Lindenstrauss Lemma [43]. Moreover, since the number of rows $m$ required in Theorem 2 is greater than the one required in Theorem 1, term $||\\Phi\\widehat{\\pmb y}-\\widehat{\\pmb W}\\pmb x||_{2}$ can be further upper bounded using the uncompressed version $(1+\\delta)\\|\\widehat{\\pmb y}-\\widehat Z\\pmb x\\|_{2}$ with probability at least $1-\\tau$ . Then we obtain a direct corollary of Theorem 2: suppose $\\|\\widehat{\\pmb{y}}-\\pmb{v}^{(0)}\\|_{2}>\\|\\pmb{\\Phi}\\widehat{\\pmb{y}}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}$ ,if ", "page_idx": 6}, {"type": "equation", "text": "$$\nt\\geq t_{*}:=O\\left(\\log\\left(\\|\\widehat{\\pmb{y}}-\\pmb{v}^{(0)}\\|_{2}/\\|\\Phi\\widehat{\\pmb{y}}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}\\right)\\right/\\log\\left(1/c_{1}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "the proposed Algorithm 1 guarantees a globally linear convergence to a ball $\\mathbb{B}(\\widehat{\\pmb{y}};O(\\|\\Phi\\widehat{\\pmb{y}}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}))$ In contrast with OMP used in [17] for multi-label predictions, Theorem 2 holds for arbitrary sample set without the so-called bounded coherence guarantee on $\\Phi$ . Moreover, as reported in Section 4, the proposed prediction method (Algorithm 1) has better computational efficiency than OMP. ", "page_idx": 6}, {"type": "text", "text": "3.3 Generalization Error Bounds for ID Samples ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This subsection studies a specific scenario when every sample $(x^{i},y^{i})$ is i.i.d. drawn from some underlying subGaussian distribution $\\mathcal{D}$ over sample space $\\mathbb{R}^{d}\\times\\mathcal{V}_{s}^{K}$ . Specifically, we use ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}}\\left[{\\binom{x}{y}}\\right]={\\binom{\\mu_{x}}{\\mu_{y}}}=:\\mu\\quad{\\mathrm{and}}\\quad\\operatorname{Var}_{\\mathcal{D}}\\left[{\\binom{x}{y}}\\right]=\\left(\\sum_{y=x}\\quad\\Sigma_{x y}\\right)=:\\Sigma\\succeq\\mathbf{0}_{(d+K)\\times(d+K)}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "to denote its mean and variance, respectively. Let $\\xi_{x}:=x-\\mu_{x},\\xi_{y}:=y-\\mu_{y},\\xi:=(\\xi_{x}^{\\top},\\xi_{y}^{\\top})^{\\top}$ be centered subGaussian random variables of $\\mathbf{x},\\mathbf{y},(\\mathbf{x}^{\\top},\\mathbf{y}^{\\top})^{\\top}$ , respectively. Let $\\pmb{a},\\pmb{b}\\in\\{\\pmb{x},\\pmb{y}\\}$ we use $\\begin{array}{r}{M_{a b}:=\\mathbb{E}_{\\mathcal{D}}[a b^{\\top}]=\\Sigma_{a b}+\\mu_{a}\\mu_{b}^{\\top},\\widehat{M}_{a b}:=\\frac{1}{n}\\sum_{i=1}^{n}a^{i}(b^{i})^{\\top}}\\end{array}$ to denote the population second (cross-)moments and empirical second (cross-)moments, respectively. Then, the population training loss is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{z}\\in\\mathbb{R}^{K\\times d}}\\mathcal{L}(\\pmb{Z}):=\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}}\\left[\\|\\pmb{y}-\\pmb{Z}\\pmb{x}\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with its optimal solution $Z_{*}:=M_{y x}M_{x x}^{-1}$ . Similarly, given a $\\Phi$ , the compressed training loss is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\Phi}(W):=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\left[||\\Phi\\pmb{y}-W\\pmb{x}||_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with optimal solution $W_{*}:=\\Phi M_{y x}M_{x x}^{-1}$ . We then define the following assumption: ", "page_idx": 6}, {"type": "text", "text": "Assumption 2. Let $\\mathcal{D}$ be $\\sigma^{2}$ -subGaussian for some positive constant $\\sigma^{2}\\,>\\,0,$ i.e, the inequality $\\mathbb{E}_{\\mathcal{D}}[\\mathrm{exp}(\\lambda\\pmb{u}^{\\top}\\pmb{\\xi})]\\leq\\mathrm{exp}\\left(\\lambda^{2}\\sigma^{2}/2\\right)$ holds for any $\\lambda>0$ and unitary vector $\\pmb{u}\\in\\mathbb{R}^{d+K}$ .Moreover, the covariance matrix $\\Sigma_{x x}$ is positive definite (i.e., its minimum eigenvalue $\\lambda_{\\operatorname*{min}}(\\Sigma_{x x})>0,$ ", "page_idx": 6}, {"type": "text", "text": "Remark 4. Assumption 2 ensures the light tail property of distribution $\\mathcal{D}$ .Note that in somereal applications, e.g., factuality check [31], algorithmic trading $[I6J,$ onecannormalizeinput and output vectortoensurebounded $\\ell_{2}$ -norm. Under such a situation, Assumption 2 is naturally satisfied. ", "page_idx": 6}, {"type": "text", "text": "Our first result in this subsection gives the generalization error bounds. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. For any $\\delta\\in(0,1)$ and $\\tau\\in(0,\\frac{1}{3})$ , suppose compressed matrix $\\Phi$ follows Assumption $^{\\,l}$ with $\\begin{array}{r}{m\\geq O(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau}))}\\end{array}$ , and Assumption 2 holds, for any constant $\\epsilon>0$ the following results hold: (Matrix Eror). The inequality for matrx eror $\\left\\Vert M_{x x}^{1/2}\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}\\right\\Vert_{o p}\\leq4$ holds with probability at least $1-2\\tau$ as the number of samples $n\\geq n_{1}$ with ", "page_idx": 6}, {"type": "equation", "text": "$$\nn_{1}:=\\operatorname*{max}\\left\\{\\frac{64C^{2}\\sigma^{4}}{9\\lambda_{\\operatorname*{min}}^{2}(M_{x x})}\\left(d+\\log(2/\\tau)\\right),\\,\\frac{32^{2}\\|\\mu_{x}\\|_{2}^{2}\\sigma^{2}}{\\lambda_{\\operatorname*{min}}^{2}(M_{x x})}\\left(2\\sqrt{d}+\\sqrt{\\log(1/\\tau)}\\right)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C$ is some fixed positive constant used in matrix concentration inequality of operator norm. ", "page_idx": 6}, {"type": "text", "text": "(Uncompressed). The generalization error bound for uncompressed SHORE satisfies $c(\\widehat{\\mathbf{Z}})\\leq$ $\\mathcal{L}(Z_{*})+4\\epsilon$ withprobability at least $1-3\\tau$ asthenumber of samples $n\\geq\\operatorname*{max}\\{n_{1},n_{2}\\}$ With ", "page_idx": 6}, {"type": "equation", "text": "$$\nn_{2}:=\\operatorname*{max}\\left\\{4(\\|Z_{*}\\|_{F}^{2}+K)\\cdot\\frac{d+2\\sqrt{d\\log(K/\\tau)}+2\\log(K/\\tau)}{\\epsilon},\\,4\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{\\epsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(Compressed). The generalization error bound for the compressed SHORE satisfies $\\mathcal{L}^{\\Phi}(\\widehat{W})\\leq$ $\\mathcal{L}^{\\Phi}(\\mathbf{\\dot{W}}_{*})+4\\epsilon$ with probability at least $1-3\\tau$ asthenumber of sample $n\\geq\\operatorname*{max}\\{n_{1},\\widetilde{n}_{2}\\}$ with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{i}_{2}:=\\operatorname*{max}\\left\\{4(\\|W_{*}\\|_{F}^{2}+\\|\\Phi\\|_{F}^{2})\\cdot\\frac{d+2\\sqrt{d\\log(m/\\tau)}+2\\log(m/\\tau)}{\\epsilon},\\,4\\|\\Phi\\mu_{y}-W_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{\\epsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 3 is presented in Appendix A.3. The proof sketch mainly contains three steps: InStep- $^{\\,l}$ ,we represent the difference $\\mathcal{L}(\\widehat{Z})-\\mathcal{L}(Z_{*})$ Or $\\mathcal{L}^{\\Phi}(\\widehat{W})-\\mathcal{L}^{\\Phi}(W_{*})$ asaproductbetween matrix error (in Theorem 3) and rescaled approximation error (see Appendix A.3 for definition), i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\widehat{Z})-\\mathcal{L}(Z_{*})\\;\\mathrm{~or}\\;\\;\\mathcal{L}^{\\oplus}(\\widehat{W})-\\mathcal{L}^{\\oplus}(W_{*})\\leq(\\mathrm{matrix~error})\\times(\\mathrm{rescaled~approximation~error});}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Step-2 controls the upper bounds for matrix error and rescaled approximation error separately, using concentration for subGuassian variables; for Step-3, we combine the upper bounds obtained in Step-2 and complete the proof. Based on the result of Theorem 3, ignoring the logarithm term for $\\tau$ ,the proposed generalization error bounds can be bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\widehat{Z})\\leq\\mathcal{L}(Z_{*})+\\widetilde{O}_{\\tau}\\left(\\operatorname*{max}\\{\\|Z_{*}\\|_{F}^{2},\\;\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{2}^{2},\\;K\\}\\cdot\\frac{d}{n}\\right),}\\\\ &{\\mathcal{L}^{\\Phi}(\\widehat{W})\\leq\\mathcal{L}^{\\Phi}(W_{*})+\\widetilde{O}_{\\tau}\\left(\\operatorname*{max}\\{\\|W_{*}\\|_{F}^{2},\\;\\|\\Phi\\mu_{y}-W_{*}\\mu_{x}\\|_{2}^{2},\\;\\|\\Phi\\|_{F}^{2}\\}\\cdot\\frac{d}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 5. To make a direct comparison between the generalization error bounds of the uncompressed and the compressed version, we further control the norms $\\|\\b{W}_{*}\\|_{F}^{2},\\|\\b{\\Phi}\\b{\\mu}_{y}-\\b{W}_{*}\\b{\\mu}_{x}\\|_{2}^{2},\\|\\b{\\Phi}\\|_{F}^{2}$ based on additional conditions on the compressed matrix $\\Phi$ .Recall the generalization method of the compressedmatrix $\\Phi$ asmentionedinAssumption $^{\\,I}$ ,wehave the following event ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{1}:=\\left\\lbrace\\Phi\\in\\mathbb{R}^{m\\times K}\\left|\\begin{array}{l}{\\|W_{*}\\|_{F}^{2}=\\|\\Phi Z_{*}\\|_{F}^{2}\\leq(1+\\delta)\\|Z_{*}\\|_{F}^{2}}\\\\ {\\|\\Phi\\mu_{y}-W_{*}\\mu_{x}\\|_{F}^{2}=\\|\\Phi(\\mu_{y}-Z_{*}\\mu_{x})\\|_{F}^{2}\\leq(1+\\delta)\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{F}^{2}}\\end{array}\\right.\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds with probability at least $1-\\tau$ due to the RIP property for a fixed matrix. Moreover, since every component $\\Phi_{i,j}$ is i.i.d. drawn from a Gaussian distribution ${\\mathcal{N}}(0,1/m)$ ,using the concentration tail bound for chi-squaredvariables(SeeLemma $^{\\,I}$ in $[26]_{.}$ ), we have the following event ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2}:=\\left\\{\\Phi\\in\\mathbb{R}^{m\\times K}\\,\\left|\\,\\|\\Phi\\|_{F}^{2}\\leq K+2\\sqrt{\\frac{K\\log(1/\\tau)}{m}}+\\frac{2\\log(1/\\tau)}{m}\\right.\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holdswith probability at least $1-\\tau$ .Conditioned on these two events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ thegeneralization error bound of the compressed version achieves the same order(ignoring the logarithm term of $\\tau$ )as the generalization error bound of the uncompressed version. That is to say, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\Phi}(\\widehat{W})\\leq(1+\\delta)\\cdot\\mathcal{L}(Z_{*})+\\widetilde{O}_{\\tau}\\left(\\operatorname*{max}\\{\\|Z_{*}\\|_{F}^{2},\\;\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{2}^{2},\\;K\\}\\cdot\\frac{d}{n}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds with probability at least $1-5\\tau$ ", "page_idx": 7}, {"type": "text", "text": "Comparing with existing results on generalization error bounds mentioned in Section 2, we would like to emphasize that Theorem 4 guarantees that the generalization error bounds maintain the order before and after compression. This result establishes on i.i.d. subGaussian samples for the SHORE model without additional regularity conditions on loss function and feasible set as required in [45]. Additionally, we obtained a $O(K d/n)$ generalization error bound for squared Frobenius norm loss function $\\mathcal{L}$ or ${\\mathcal{L}}^{\\Phi}$ , which is smaller than $O(K^{2}d/n)$ as presented in [Theorem 4, [45]]. ", "page_idx": 7}, {"type": "text", "text": "We then give results on prediction error bounds. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.For any $\\delta\\in(0,1)$ and any $\\tau\\in(0,1/3)$ ,supposethecompressedmatrix $\\Phi$ follows Assumption $^{\\,l}$ with $\\begin{array}{r}{m\\geq O(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau}))}\\end{array}$ , and Assumption 2 holds. Given any learned regressor $\\widehat{W}$ from training problem (2), let $\\left(x,y\\right)$ be a new sample drawn from the underlying distribution $\\mathcal{D}$ ,we have the following inequality holds with probability at least $1-\\tau$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}}[\\|\\widehat{\\pmb{y}}-\\pmb{y}\\|_{2}^{2}]\\leq\\frac{4}{1-\\delta}\\cdot\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}\\pmb{y}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\hat{\\pmb y}$ is the optimal solution from prediction problem (3) with input vector $\\textbf{\\em x}$ ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 4 is presented in Appendix A.4. Theorem 4 gives an upper bound of $\\ell_{2}$ norm distance between $\\hat{\\pmb y}$ and $\\textit{\\textbf{y}}$ Since $\\|v^{(\\bar{T})}-y\\|_{2}\\leq\\|v^{(T)}-\\widehat{\\pmb{y}}\\|_{2}+\\|\\widehat{\\pmb{y}}-\\pmb{y}\\|_{2}$ , combined with Theorem 2, we have $\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{v}^{(T)}-\\pmb{y}\\|_{2}^{2}]\\le\\ O(\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}\\pmb{y}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}])$ when $T\\geq t_{*}$ defined in (5) (see Appendix A.5.5), where the final inequality holds due to the optimality of $\\hat{\\pmb y}$ . Hence, we achieve an upper bound of $\\ell_{2}$ -norm distance between $\\bar{\\pmb{v}}^{(T)}$ and $\\textit{\\textbf{y}}$ as presented in Theorem 4, see Remark 6. ", "page_idx": 8}, {"type": "text", "text": "Remark 6. . For any $\\delta\\in(0,1)$ and $\\tau\\in(0,1/3)$ ,suppose compressed matrix $\\Phi$ follows Assumption $^{\\,l}$ with $m\\ge O\\big(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau})\\big)$ , and Assumption 2 holds, for any constant $\\epsilon>0$ the following inequality holds with probability at least $1-3\\tau$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{v}^{(T)}-\\pmb{y}\\|_{2}^{2}]\\le O(\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}\\pmb{y}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}])\\le O(\\mathcal{L}^{\\Phi}(\\pmb{W}_{*})+4\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "4  Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct numerical experiments on two types of instances (i.e., synthetic data sets and real data sets) to validate the theoretical results and illustrate both efficiency and accuracy of the proposed prediction method compared with typical existing prediction baselines, i.e., Orthogonal Matching Pursuit (OMP, [46]), Fast Iterative Shrinkage-Thresholding Algorithm (FISTA, [2]) and Elastic Net (EN, [47]). Due to the space limit, we put the implemented prediction method (Algorithm 2) in Appendix A.6.1, aforementioned existing prediction baselines in Appendix A.6.2, experiment setting details and results for real data in Appendix A.7. ", "page_idx": 8}, {"type": "text", "text": "Performance measures. Given a sample $\\left({\\pmb x},{\\pmb y}\\right)$ with input $\\textbf{\\em x}$ and corresponding true output $\\textit{\\textbf{y}}$ we use $\\pmb{v}$ to denote the predicted output obtained from any prediction method, and measure the numerical performances based on the following three metrics: ", "page_idx": 8}, {"type": "text", "text": "1. For a ground truth $\\textit{\\textbf{y}}$ with sparsity-level $s$ , the metric precision over selected supports, i.e., $\\begin{array}{r}{\\mathtt{P r e c i s i o n@s:}=\\frac{1}{s}|\\operatorname{supp}(v)\\cap\\operatorname{supp}(y)|}\\end{array}$ measures the percentage of correctly identified supports in the predicted output;   \n2. The metric output difference, i.e., $0u\\mathtt{t p u t}-\\mathtt{d i f f}\\ :=\\ \\|v-y\\|_{2}^{2}$ measures the $\\ell_{2}$ -norm distance between the predicted output and the ground-truth;   \n3. For any given MOR W and compressed matrix $\\Phi$ , the metric prediction loss, i.e., Prediction - Loss $:=||\\Phi v-\\widehat{W}x||_{2}^{2}$ computes the prediction loss with respect to $\\widehat{\\pmb{W}}\\pmb{x}$ ", "page_idx": 8}, {"type": "text", "text": "Synthetic data generation procedure. The synthetic data set is generated as follows: Every input $\\pmb{x}^{i}$ for $i\\in[n]$ is i.i.d. drawn from a Gaussian distribution $\\mathcal{N}(\\mu_{x},\\Sigma_{x x})$ , where its mean vector $\\pmb{\\mu}_{\\pmb{x}}$ and covariance matrix $\\Sigma_{x x}$ are selected based on the procedures given in Appendix A.6.3. For any given sparsity-level $s$ , underlying true regressor $\\mathbf{Z}_{\\ast}\\in\\mathbb{R}^{\\ast\\!}K\\!\\times\\!d$ , and Signal-to-Noise Ratio (SNR), the groundtruth $\\boldsymbol{y}^{i}$ (corresponding with its given input $\\pmb{x}^{i}$ ) is generated by $\\pmb{y}^{i}=\\Pi_{\\nu_{s}^{K}\\cap\\mathcal{F}}\\left(Z_{*}\\pmb{x}^{i}+\\pmb{\\epsilon}^{i}\\right)$ , where $\\epsilon^{i}\\in\\mathbb{R}^{K}$ is a i.i.d. random noise drawn from the Gaussian distribution $\\mathcal{N}(\\mathbf{0}_{K},\\mathbf{SNR}^{-2}\\|Z_{*}\\pmb{x}^{i}\\|_{\\infty}{\\cdot}I_{K})$ ", "page_idx": 8}, {"type": "text", "text": "Parameter setting. For synthetic data, we set input dimension $d=10^{4}$ , output dimension $K=$ $2\\times10^{4}$ , and sparsity-level $s=3$ . We generate in total $n=3\\times10^{4}$ , i.i.d. samples as described above, i. $S^{\\mathrm{syn}}:=\\{({\\pmb x}^{i},{\\pmb y}^{i})\\}_{i=1}^{3\\times10^{4}}$ With $\\mathrm{SNR}^{-1}\\in\\{1,0.32,0.032\\}$ to ensure the sigato-nise decibels (dB, [14]) takes values on d $\\mathbf{B}:=10\\log(\\mathbf{SNR^{2}})\\in\\{0,10,30\\}$ . We select the number of rows for compressed matrix $\\Phi$ by $m\\in\\{100,300,500,700,1000,2000\\}$ . For computing the empirical regressor $\\widehat{W}\\in\\mathbb{R}^{m\\times d}$ , we frst split the whole sample set $S^{\\mathrm{syn}}$ into two non-overlap subsets, i.e., a training set $S^{\\tt t r a}$ with $80\\%$ and a testing set $S^{\\tt t e s t}$ with rest $20\\%$ . The regressor W is therefore obtained by solving compressed SHORE (2) based on the training set $S^{\\tt t r a}$ with a randomly generated compressed matrix $\\Phi$ . For evaluating the proposed prediction method, Algorithm 2, we pick a fixed stepsize $\\eta=0.9$ $\\mathcal{F}=\\mathbb{R}_{+}^{K}$ , and set the maximum iteration number as $T=60$ and run prediction methods over the set Stest. ", "page_idx": 8}, {"type": "text", "text": "Hardware & Software. All experiments are conducted in Dell workstation Precision 7920 with a 3GHz 48Cores Intel Xeon CPU and 128GB 2934MHz DDR4 Memory. The proposed method and other methods are solved using PyTorch version 2.3.0 and scikit-learn version 1.4.2 in Python 3.12.3. ", "page_idx": 8}, {"type": "text", "text": "Numerical Results & Discussions. The results are demonstrated in Figure 1, which does not include the results from the Elastic Net and OMP due to relatively much longer running time. ", "page_idx": 8}, {"type": "image", "img_path": "kPGNE4CrTq/tmp/e2e147993752c62dde6cb95d076133edb2984447ef2d48317594acf6440482dd.jpg", "img_caption": ["Figure 1: Numerical results on synthetic data. In short, each dot in the figure represents the average value of 10 independent trials (i.e., experiments) of compressed matrices $\\bar{\\Phi}^{(1)},\\bar{\\mathbf{\\Omega}}.\\bar{\\mathbf{\\Omega}}.\\bar{\\mathbf{\\Omega}},\\bar{\\Phi}^{(10)}$ on a given tuple of parameters $(K,d,n,{\\mathsf{S N R}},m)$ . The shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the ratio of training loss after and before compression, i.e., $\\lVert\\Phi Y-\\widehat{W}X\\rVert_{F}^{2}/\\lVert Y-\\widehat{Z}X\\rVert_{F}^{2}$ versus the number of rows $m$ . It is obvious that the ratio converges to one as $m$ increases, which validates the result presented in Theorem 1. In the second row, we plot percision $@3$ versus the number of rows. As we can observe, the proposed algorithm outperforms CD and FISTA. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Based on Figure 1, we observe that the proposed algorithm enjoys a better computational cost and accuracy on most metrics. The running time for the proposed algorithm and baselines are reported in Table 2 (see in Appendix A.7), which further demonstrates the efficiency of the proposed algorithm. The implemented code could be found on Github https : //github. com/from-ryan/SolvingSHORE_via_compression. ", "page_idx": 9}, {"type": "text", "text": "5  Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, we propose a two-stage framework to solve Sparse & High-dimensional-Output REgression (SHORE) problem, the computational and statistical results indicate that the proposed framework is computationally scalable, maintaining the same order of both the training loss and prediction loss before and after compression under relatively weak sample set conditions, especially in the sparse and high-dimensional-output setting where the input dimension is polynomially smaller compared to the output dimension. In numerical experiments, SHORE provides improved optimization performance over existing MOR methods, for both synthetic data and real data. ", "page_idx": 9}, {"type": "text", "text": "We close with some potential questions for future investigation. The first is to extend our theoretical results to nonlinear/nonconvex SHORE frameworks [24]. The second direction is to improve existing variable reduction methods for better scalability while maintaining small sacrificing on prediction accuracy, e.g., new design and analysis on randomized projection matrices. The third direction is to explore general scenarios when high dimensional outputs enjoys additional geometric structures [30] from real applications in machine learning or operations management other than $s_{\\mathrm{{}}}$ sparsity and its variants as discussed in the paper. Taking our result for SHORE as an initial start, we expect a stronger follow-up work that applies to MOR with additional structures, which eventually benefits the learning community in both practice and theory. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Renyuan Li and Guanyi Wang were supported by the National University of Singapore under AcRF Tier-1 grant (A-8000607-00-00) 22-5539-A0001. Zhehui Chen would like to thank Google for its support in providing the research environment and supportive community that made this work possible. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Z. Abraham, P.-N. Tan, Perdinan, J. Winkler, S. Zhong, and M. Liszewska. Position preserving multi-output prediction. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part II 13, pages 320-335. Springer, 2013.   \n[2]  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.   \n[3]  D. Bertsimas and B. Van Parys. Sparse high-dimensional regression: Exact scalable algorithms and phase transitions (2017). arXiv preprint arXiv: 1709.10029, 2019.   \n[4]  D. Bertsimas and B. Van Parys. Sparse high-dimensional regression. The Annals of Statistics, 48(1):300-323, 2020.   \n[5] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classification repository: Multi-label datasets and code, 2016.   \n[6]  T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. Applied and computational harmonic analysis, 27(3):265-274, 2009.   \n[7] H. Boche, R. Calderbank, G. Kutyniok, and J. Vybiral. Compressed sensing and its applications: MATHEON workshop 2013. Springer, 2015.   \n[8] H. Borchani, G. Varando, C. Bielza, and P. Larranaga. A survey on multi-output regression. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 5(5):216-233, 2015.   \n[9] L. Breiman. Bagging predictors. Machine learning, 24:123-140, 1996.   \n[10]  E. J. Candes and T. Tao. Decoding by linear programming. IEEE transactions on information theory, 51(12):4203-4215, 2005.   \n[11] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3): 1-45, 2024.   \n[12]  Y.-N. Chen and H.-T. Lin. Feature-aware label space dimension reduction for multi-label classification. Advances in neural information processing systems, 25, 2012.   \n[13] M. M. Cisse, N. Usunier, T. Artieres, and P. Gallinari. Robust bloom filters for large multilabel classification tasks. Advances in neural information processing systems, 26, 2013.   \n[14] S. S. Dey, G. Wang, and Y. Xie. Approximation algorithms for training one-node relu neural networks. IEEE Transactions on Signal Processing, 68:6696-6706, 2020.   \n[15]  A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55-67, 1970.   \n[16] O. Honovich, R. Aharoni, J. Herzig, H. Taitelbaum, D. Kukliansy, V. Cohen, T. Scialom, I. Szpektor, A. Hassidim, and Y. Matias. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991, 2022.   \n[17] D. Hsu, S. M. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing, 2009.   \n[18] D. Hsu, S. M. Kakade, and T. Zhang. An analysis of random design linear regression. arXiv preprint arXiv:1106.2363, 6, 2011.   \n[19] J. C. Hull and S. Basu. Options, futures, and other derivatives. Pearson Education India, 2016.   \n[20] I. M. Jacobs and J. Wozencraft. Principles of communication engineering. 1965.   \n[21] P. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-dimensional m-estimation. Advances in neural information processing systems, 27, 2014.   \n[22]  S. Jansen. Machine Learning for Algorithmic Trading: Predictive models to extract signals from market and alternative data for systematic trading strategies with Python. Packt Publishing Ltd, 2020.   \n[23]  K. Jasinska and N. Karampatziakis. Log-time and log-space extreme classification. arXiv preprint arXiv: 1611.01964, 2016.   \n[24]  A. Kapoor, R. Viswanathan, and P. Jain. Multilabel classification using bayesian compressed sensing. Advances in neural information processing systems, 25, 2012.   \n[25] S. Kim and E. P. Xing. Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping. The Annals of Applied Statistics, 6(3):1095 -- 1117, 2012.   \n[26]  B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals of statistics, pages 1302-1338, 2000.   \n[27] T. Levy and F. Abramovich. Generalization error bounds for multiclass sparse linear classifers. Journal of Machine Learning Research, 24(151):1-35, 2023.   \n[28]  . Li and Y Liu. Towards sharper generalization bounds for structured prediction. Advances in Neural Information Processing Systems, 34:26844-26857, 2021.   \n[29] L. Liu, Y. Shen, T. Li, and C. Caramanis. High dimensional robust sparse regression. In International Conference on Artificial Inteligence and Statistics, pages 411-421. PMLR, 2020.   \n[30] S. Ludwig et al. Algorithms above the noise foor. PhD thesis, Massachusetts Institute of Technology, 2018.   \n[31]  Y. Ma, R. Han, and W. Wang. Porfolio optimization with return prediction using deep learning and machine learning. Expert Systems with Applications, 165:113973, 2021.   \n[32] T. K. R. Medini, Q. Huang, Y. Wang, V. Mohan, and A. Shrivastava. Extreme classification in log memory using count-min sketch: A case study of amazon search with $50\\mathrm{m}$ products. Advances in Neural Information Processing Systems, 32, 2019.   \n[33] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg. Learning by playing solving sparse reward tasks from scratch. In International conference on machine learning, pages 4344-4353. PMLR, 2018.   \n[34] A. Roberts, C. Raffel, and N. Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.   \n[35] M. Sanchez-Fernandez, M. de Prado-Cumplido, J. Arenas-Garcia, and F. P\u00e9rez-Cruz. Svm multiregression for nonlinear channel estimation in multiple-input multiple-output systems. IEEE transactions on signal processing, 52(8):2298-2307, 2004.   \n[36] T. Simila and J. Tikka. Input selection and shrinkage in multiresponse linear regression. Computational Statistics & Data Analysis, 52(1):406-422, 2007.   \n[37] E. Spyromitros-Xioufis, G. Tsoumakas, W. Groves, and I. Vlahavas. Multi-label classification methods for multi-target regression. arXiv preprint arXiv: 1211.6581, pages 1159-1168, 2012.   \n[38]  Q. Sun, H. Zhu, Y. Liu, and J. G. Ibrahim. Sprem: sparse projection regression model for highdimensional linear regression. Journal of the American Statistical Association, 110(509):289- 302, 2015.   \n[39]  F. Tai and H.-T. Lin. Multilabel classification with principal label space transformation. Neural Computation, 24(9):2508-2542, 2012.   \n[40] D. Tuia, J. Verrelst, L. Alonso, F. P\u00e9rez-Cruz, and G. Camps-Valls. Multioutput support vector regression for remote sensing biophysical parameter estimation. IEEE Geoscience and Remote Sensing Letters, 8(4):804-808, 2011.   \n[41] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[42]  A. Watkins, E. Ullah, T. Nguyen-Tang, and R. Arora. Optimistic rates for multi-task representation learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[43]  B. J. William and J. Lindenstrauss. Extensions of lipschitz mapping into hilbert space. Contemporary mathematics, 26(189-206):323, 1984.   \n[44] D. Xu, Y. Shi, I. W. Tsang, Y-S. Ong, C. Gong, and X. Shen. Survey on multi-output learning. IEEE transactions on neural networks and learning systems, 31(7):2409-2429, 2019.   \n[45]  H.-F. Yu, P. Jain, P. Kar, and I. Dhillon. Large-scale multi-label learning with missing labels. In International conference on machine learning, pages 593-601. PMLR, 2014.   \n[46] Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 41(12):3397-3415, 1993.   \n[47]  H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301-320, 2005. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall the following theorem in the main text. ", "page_idx": 13}, {"type": "text", "text": "Theorem 1.For any $\\delta\\in(0,1)$ and $\\tau\\in(0,1)$ , suppose the compressed matrix $\\Phi$ follows Assumption 1 with $\\begin{array}{r}{m\\geq O(\\frac{1}{\\delta^{2}}\\cdot\\log(\\frac{K}{\\tau}))}\\end{array}$ . We have the following inequality for training loss ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\Phi Y-\\widehat{W}X\\|_{F}^{2}\\leq(1+\\delta)\\cdot\\|Y-\\widehat{Z}X\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds with probability at least $1-\\tau$ where $\\widehat{Z},\\widehat{W}$ are optimal solutions for the uncompressed (1) and compressed SHORE (2), respectively. ", "page_idx": 13}, {"type": "text", "text": "Proof. Given a set of $n$ samples $\\{(\\pmb{x}^{i},\\pmb{y}^{i})\\}_{i=1}^{n}$ , and a matrix $\\Phi$ generated from Assumption 1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\Phi Y-\\widehat W X\\|_{F}^{2}\\leq\\|\\Phi Y-\\Phi\\widehat Z X\\|_{F}^{2}=\\|\\Phi(Y-\\widehat Z X)\\|_{F}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the inequality holds due to the optimality of $\\widehat{W}$ . Let $Y^{\\prime}=Y-\\hat{Z}X$ Consider the singular value decomposition: $Y^{\\prime}=U_{Y^{\\prime}}\\Sigma_{Y^{\\prime}}\\dot{V}_{Y^{\\prime}}^{\\top}$ , and then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\Phi(\\pmb{Y}-\\hat{\\pmb{Z}}\\pmb{X})\\|_{F}^{2}=\\|\\Phi\\pmb{Y}^{\\prime}\\|_{F}^{2}=\\|\\Phi\\pmb{U}_{Y^{\\prime}}\\pmb{\\Sigma}_{Y^{\\prime}}V_{Y^{\\prime}}^{\\top}\\|_{F}^{2}=\\|\\Phi\\pmb{U}_{Y^{\\prime}}\\pmb{\\Sigma}_{Y^{\\prime}}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Set $\\tilde{\\Phi}:=\\,\\Phi U_{Y^{\\prime}}$ . Since the generalization method for $\\Phi$ ensures its $(1,\\delta)$ -RIP with probability $1-\\tau$ (from the Johnson-Lindenstrauss Lemma), and ${\\cal U}_{Y^{\\prime}}$ is a real unitary matrix, then Lemma 1 for unitary invariant shows that $\\tilde{\\Phi}$ is also $(1,\\delta)$ -RIP with probability $1-\\tau$ . Now, using $\\tilde{\\Phi}$ is $(1,\\delta)$ -RIP and all columns in $\\pmb{\\Sigma}\\pmb{Y}^{\\prime}$ has at most one non-zero component, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Phi U_{Y^{\\prime}}\\Sigma_{Y^{\\prime}}\\|_{F}^{2}=\\|\\tilde{\\Phi}\\Sigma_{Y^{\\prime}}\\|_{F}^{2}\\leq(1+\\delta)\\|\\Sigma_{Y^{\\prime}}\\|_{F}^{2}=(1+\\delta)\\|Y^{\\prime}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining the above inequalities together implies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\Phi Y-\\widehat{W}X\\|_{F}^{2}\\leq(1+\\delta)\\|Y^{\\prime}\\|_{F}^{2}=(1+\\delta)\\cdot\\|Y-\\widehat{Z}X\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.2Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall the following theorem in the main text. ", "page_idx": 13}, {"type": "text", "text": "Theorem 2. For any $\\delta\\in(0,1)$ and $\\tau\\in(0,1)$ , suppose the compressed matrix $\\Phi$ follows Assumption 1 with $\\begin{array}{r}{m\\geq O(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau}))}\\end{array}$ With a fixed stepsize $\\begin{array}{r}{\\eta\\in(\\frac{1}{2-2\\delta},1)}\\end{array}$ , the following inequality ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\widehat{\\boldsymbol{y}}-\\boldsymbol{v}^{(t)}\\|_{2}\\leq c_{1}^{t}\\cdot\\|\\widehat{\\boldsymbol{y}}-\\boldsymbol{v}^{(0)}\\|_{2}+\\frac{c_{2}}{1-c_{1}}\\cdot\\|\\Phi\\widehat{\\boldsymbol{y}}-\\widehat{\\boldsymbol{W}}\\boldsymbol{x}\\|_{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds for all $t\\in[T]$ simultaneously with probability at least $1-\\tau$ where $c_{1}:=2-2\\eta+2\\eta\\delta<1$ is some positive constant strictly smaller than 1, and $c_{2}:=2\\eta\\sqrt{1+\\delta}$ is some constant. ", "page_idx": 13}, {"type": "text", "text": "Proof. Suppose Assumption 1 holds, then the randomized compressed matrix $\\Phi$ ensures $(3s,\\delta)$ -RIP with probability at least $1-\\tau$ (see Remark 3). Thus, to complete the proof, it is sufficient to show the above inequality holds for all $t\\in[T]$ under such a compressed matrix $\\Phi$ with $(3s,\\delta)$ -RIP. We conclude that the proof is, in general, separated into three steps. ", "page_idx": 13}, {"type": "text", "text": "Step-1.We establish an upper bound on the $\\ell_{2}$ -norm distance between the current point and the optimal solution. Due to the optimality of the projection (step-4 in Algorithm 1), we have the following inequality ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\boldsymbol{v}}^{(t+1)}-\\boldsymbol{v}^{(t+1)}\\|_{2}^{2}\\leq\\|\\widetilde{\\boldsymbol{v}}^{(t+1)}-\\boldsymbol{v}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds for all $\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}$ , which further implies that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde{\\boldsymbol{v}}^{(t+1)}-\\boldsymbol{v}+\\boldsymbol{v}-\\boldsymbol{v}^{(t+1)}\\|_{2}^{2}\\leq\\|\\widetilde{\\boldsymbol{v}}^{(t+1)}-\\boldsymbol{v}\\|_{2}^{2}}\\\\ &{\\Leftrightarrow\\|\\boldsymbol{v}-\\boldsymbol{v}^{(t+1)}\\|_{2}^{2}\\leq2\\langle\\boldsymbol{v}-\\widetilde{\\boldsymbol{v}}^{(t+1)},\\boldsymbol{v}-\\boldsymbol{v}^{(t+1)}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds for all $\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}$ ", "page_idx": 13}, {"type": "text", "text": "Step-2. We show one-iteration improvement based on the above inequality. Since $\\widehat{\\pmb{y}}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}$ we can replace $\\pmb{v}$ by $\\hat{\\pmb y}$ , which still ensures the above inequality. Set $\\pmb{\\Delta}^{(t)}:=\\widehat{\\pmb{y}}-\\pmb{v}^{(t)}$ for all $t\\in[T]$ Based on the updating rule (step-3 in Algorithm 1), $\\widetilde{\\pmb{v}}^{(t+1)}=\\pmb{v}^{(t)}-\\eta\\cdot\\pmb{\\Phi}^{\\top}(\\pmb{\\Phi}v^{(t)}-\\widehat{\\pmb{W}}\\pmb{x})$ . Thus, the above inequality can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta^{(t+1)}\\|_{2}^{2}\\le2\\langle\\widehat{y}-v^{(t)}+\\eta\\cdot\\Phi^{\\top}(\\Phi v^{(t)}-\\widehat{W}x),\\Delta^{(t+1)}\\rangle}\\\\ &{\\qquad\\qquad=2\\langle\\Delta^{(t)},\\Delta^{(t+1)}\\rangle+2\\eta\\langle\\Phi v^{(t)}-\\widehat{W}x,\\Phi\\Delta^{(t+1)}\\rangle}\\\\ &{\\qquad\\qquad=2\\langle\\Delta^{(t)},\\Delta^{(t+1)}\\rangle-2\\eta\\langle\\Phi\\Delta^{(t)},\\Phi\\Delta^{(t+1)}\\rangle+2\\eta\\langle\\Phi\\widehat{y}-\\widehat{W}x,\\Phi\\Delta^{(t+1)}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using Lemma 2 with ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\Delta^{(t)}}{\\|\\Delta^{(t)}\\|_{2}},\\quad\\frac{\\Delta^{(t+1)}}{\\|\\Delta^{(t+1)}\\|_{2}},\\quad\\frac{\\Delta^{(t)}}{\\|\\Delta^{(t)}\\|_{2}}+\\frac{\\Delta^{(t+1)}}{\\|\\Delta^{(t+1)}\\|_{2}},\\quad\\frac{\\Delta^{(t)}}{\\|\\Delta^{(t)}\\|_{2}}-\\frac{\\Delta^{(t+1)}}{\\|\\Delta^{(t+1)}\\|_{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "all $3s$ -sparse vectors, and $\\Phi$ a $(3s,\\delta)$ -RIP matrix, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n-2\\eta\\left\\langle\\Phi\\frac{\\Delta^{(t)}}{\\|\\Delta^{(t)}\\|_{2}},\\Phi\\frac{\\Delta^{(t+1)}}{\\|\\Delta^{(t+1)}\\|_{2}}\\right\\rangle\\le2\\delta\\eta-2\\eta\\left\\langle\\frac{\\Delta^{(t)}}{\\|\\Delta^{(t)}\\|_{2}},\\frac{\\Delta^{(t+1)}}{\\|\\Delta^{(t+1)}\\|_{2}}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n-2\\eta\\langle\\Phi\\Delta^{(t)},\\Phi\\Delta^{(t+1)}\\rangle\\leq2\\delta\\eta\\|\\Delta^{(t)}\\|_{2}\\|\\Delta^{(t+1)}\\|_{2}-2\\eta\\langle\\Delta^{(t)},\\Delta^{(t+1)}\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Inserting the above result into inequality (6) gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta^{(t+1)}\\|_{2}^{2}\\leq(2-2\\eta)\\langle\\Delta^{(t)},\\Delta^{(t+1)}\\rangle+2\\delta\\eta\\|\\Delta^{(t)}\\|_{2}\\|\\Delta^{(t+1)}\\|_{2}+2\\eta\\langle\\Phi\\widehat{y}-\\widehat{W}x,\\Phi\\Delta^{(t+1)}\\rangle}\\\\ &{\\qquad\\qquad\\overset{\\mathrm{(i)}}{\\leq}(2-2\\eta+2\\eta\\delta)\\|\\Delta^{(t)}\\|_{2}\\|\\Delta^{(t+1)}\\|_{2}+2\\eta\\|\\Phi\\widehat{y}-\\widehat{W}x\\|_{2}\\|\\Phi\\Delta^{(t+1)}\\|_{2}}\\\\ &{\\qquad\\qquad\\leq(2-2\\eta+2\\eta\\delta)\\|\\Delta^{(t)}\\|_{2}\\|\\Delta^{(t+1)}\\|_{2}+2\\eta\\sqrt{1+\\delta}\\|\\Phi\\widehat{y}-\\widehat{W}x\\|_{2}\\|\\Delta^{(t+1)}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the above inequality (i) requests $\\eta<1$ to ensurethe inequality $(2-2\\eta)\\langle\\Delta^{(t)},\\Delta^{(t+1)}\\rangle\\leq$ $(2-2\\eta)\\|\\Delta^{(t)}\\|_{2}\\|\\Delta^{(t+1)}\\|_{2}$ holds. Therefore, dividing $\\|\\Delta^{(t+1)}\\|_{2}$ onboth side implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\pmb{\\Delta}^{(t+1)}\\|_{2}\\leq(2-2\\eta+2\\eta\\delta)\\|\\pmb{\\Delta}^{(t)}\\|_{2}+2\\eta\\sqrt{1+\\delta}\\|\\pmb{\\Phi}\\pmb{\\widehat{y}}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which gives the one-step improvement. ", "page_idx": 14}, {"type": "text", "text": "Step-3. Combine everything together. To ensure contractions in every iteration, we pick stepsize $\\eta$ such that $2\\mathrm{~-~}2\\eta\\mathrm{~+~}2\\eta\\delta\\mathrm{~\\in~}(0,1)$ with $\\eta\\,<\\,1$ , which gives $\\begin{array}{r}{\\eta\\in\\;\\left(\\frac{1}{2(1-\\delta)},1\\right)}\\end{array}$ (2(1-s) 1). Using the above inequality (7) for one-step improvement, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\widehat{\\pmb{y}}-\\pmb{v}^{(t)}\\|_{2}\\leq(2-2\\eta+2\\eta\\delta)^{t}\\cdot\\|\\widehat{\\pmb{y}}-\\pmb{v}^{(0)}\\|_{2}+\\frac{2\\eta\\sqrt{1+\\delta}}{2\\eta-2\\eta\\delta-1}\\|\\pmb{\\Phi}\\widehat{\\pmb{y}}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which completes the proof. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall the following theorem in the main text. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3. For any $\\delta\\in(0,1)$ and $\\tau\\in(0,\\frac{1}{3})$ , suppose compressed matrix $\\Phi$ follows Assumption 1 with $\\begin{array}{r}{m\\geq O(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau}))}\\end{array}$ , and Assumption 2 holds, for any constant $\\epsilon>0$ , the following results hold: ", "page_idx": 14}, {"type": "text", "text": "(Matrix Error). The inequality for matrix error $\\left\\Vert M_{x x}^{1/2}\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}\\right\\Vert_{\\mathrm{op}}\\leq4$ holds with probability at least $1-2\\tau$ as the number of samples $n\\geq n_{1}$ with ", "page_idx": 14}, {"type": "equation", "text": "$$\nn_{1}:=\\operatorname*{max}\\left\\{\\frac{64C^{2}\\sigma^{4}}{9\\lambda_{\\operatorname*{min}}^{2}(M_{x x})}\\left(d+\\log(2/\\tau)\\right),\\,\\frac{32^{2}\\|\\mu_{x}\\|_{2}^{2}\\sigma^{2}}{\\lambda_{\\operatorname*{min}}^{2}(M_{x x})}\\left(2\\sqrt{d}+\\sqrt{\\log(1/\\tau)}\\right)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $C$ is some fixed positive constant used in matrix concentration inequality of operator norm. ", "page_idx": 15}, {"type": "text", "text": "(Uncompressed). The generalization error bound for uncompressed SHORE satisfies $c(\\widehat{\\mathbf{Z}})\\leq$ $\\mathcal{L}(Z_{*})+4\\epsilon$ with probability at least $1-3\\tau$ , as the number of samples $n\\geq\\operatorname*{max}\\{n_{1},n_{2}\\}$ With $n_{2}:=\\operatorname*{max}\\left\\{4(\\|Z_{*}\\|_{F}^{2}+K)\\cdot\\frac{d+2\\sqrt{d\\log(K/\\tau)}+2\\log(K/\\tau)}{\\epsilon},\\,4\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{\\epsilon}\\right\\}.$ ", "page_idx": 15}, {"type": "text", "text": "(Compressed). The generalization error bound for the compressed SHORE satisfies ${\\mathcal{L}}^{\\Phi}({\\widehat{W}})\\leq$ $\\mathcal{L}^{\\Phi}(\\dot{W}_{*})+4\\epsilon$ with probability at least $1-3\\tau$ , as the number of sample $n\\geq\\operatorname*{max}\\{n_{1},\\widetilde{n}_{2}\\}$ with $\\tilde{i}_{2}:=\\operatorname*{max}\\left\\{4(\\|W_{*}\\|_{F}^{2}+\\|\\Phi\\|_{F}^{2})\\cdot\\frac{d+2\\sqrt{d\\log(m/\\tau)}+2\\log(m/\\tau)}{\\epsilon},\\,4\\|\\Phi\\mu_{y}-W_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{\\epsilon}\\right\\}.$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Let us start with the uncompressed version. ", "page_idx": 15}, {"type": "text", "text": "Step-1. Note that the optimal solutions for population loss and empirical loss are ", "page_idx": 15}, {"type": "equation", "text": "$$\nZ_{*}=M_{y x}M_{x x}^{-1}\\quad\\mathrm{and}\\quad\\widehat{Z}=\\frac{Y X^{\\top}}{n}\\left(\\frac{X X^{\\top}}{n}\\right)^{-1}=:\\widehat{M}_{y x}\\widehat{M}_{x x}^{-1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "respectively. Thus, the generalization error bound is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{\\boldsymbol{Z}})-\\mathcal{L}(\\boldsymbol{Z}_{*})=\\Vert\\widehat{\\boldsymbol{Z}}-\\boldsymbol{Z}_{*}\\Vert_{M_{x x}}^{2}=\\Vert(\\widehat{\\boldsymbol{Z}}-\\boldsymbol{Z}_{*})M_{x x}^{1/2}\\Vert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\widehat{Z}-Z_{*})M_{x x}^{1/2}=\\left(\\widehat{M}_{y x}\\widehat{M}_{x x}^{-1}-M_{y x}M_{x x}^{-1}\\right)M_{x x}^{1/2}}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\widehat{M}_{y x}-M_{y x}M_{x x}^{-1}\\widehat{M}_{x x}\\right)\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}}\\\\ &{\\qquad\\qquad\\qquad=\\widehat{\\mathbb{E}}[y x^{\\top}-Z_{*}x x^{\\top}]\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}}\\\\ &{\\qquad\\qquad\\qquad=\\widehat{\\mathbb{E}}[(y-Z_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whereweuse $\\widehat{\\mathbb{E}}[\\cdot]$ to denote the empirical distribution. Then, the above generalization error bound can be upper-bounded as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(\\widehat{Z}-Z_{*})M_{x x}^{1/2}\\right\\|_{F}=\\left\\|\\widehat{\\mathbb{E}}[(y-Z_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\|\\widehat{\\mathbb{E}}[(y-Z_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{F}\\left\\|\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\|_{\\mathrm{op}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Step-2. Next, we provide upper bounds on these two terms $\\left\\|\\widehat{\\mathbb{E}}[(\\pmb{y}-\\pmb{Z}_{*}\\pmb{x})\\pmb{x}^{\\top}\\widehat{M}_{\\pmb{x}\\pmb{x}}^{-1/2}]\\right\\|_{F}$ and $\\left\\Vert\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\Vert_{\\mathrm{op}}$ in the right-hand-side separately. ", "page_idx": 15}, {"type": "text", "text": "For matrixerror term $\\left\\Vert\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\Vert_{o p}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\|_{\\mathrm{op}}^{2}=\\left\\|M_{x x}^{1/2}\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}\\right\\|_{\\mathrm{op}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Due to Assumption 2, the centralized feature vector $\\xi_{x}:=x-\\mu_{x}$ ensures the following inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}}\\left[\\exp\\left(\\lambda\\pmb{v}^{\\top}\\pmb{\\xi}_{\\pmb{x}}\\right)\\right]\\leq\\exp\\left(\\frac{\\lambda^{2}\\|\\pmb{v}\\|_{2}^{2}\\sigma^{2}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $\\pmb{v}\\in\\mathbb{R}^{d}$ . Consider the empirical second moment of $\\textbf{\\em x}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{M}_{x x}=\\sum_{i=1}^{n}\\frac{x^{i}(\\pmb{x}^{i})^{\\top}}{n}=\\sum_{i=1}^{n}\\frac{\\xi_{x}^{i}(\\xi_{x}^{i})^{\\top}}{n}+\\mu_{x}\\left(\\sum_{i=1}^{n}\\frac{\\xi_{x}^{i}}{n}\\right)^{\\top}+\\left(\\sum_{i=1}^{n}\\frac{\\xi_{x}^{i}}{n}\\right)\\mu_{x}^{\\top}+\\mu_{x}\\mu_{x}^{\\top}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\xi_{x}^{1},\\ldots,\\xi_{x}^{n}$ are i.d. $\\sigma^{2}$ subGaussian random vector with zeromean and covariancematix $\\Sigma_{x x}$ , then based on Lemma 3, there exists a positive constant $C$ such that for any $\\tau\\in(0,1)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\left\\|\\sum_{i=1}^{n}\\frac{\\xi_{x}^{i}(\\xi_{x}^{i})^{\\top}}{n}-\\Sigma_{x x}\\right\\|_{\\mathrm{op}}\\le C\\sigma^{2}\\operatorname*{max}\\left\\{\\sqrt{\\frac{d+\\log(2/\\tau)}{n}},\\frac{d+\\log(2/\\tau)}{n}\\right\\}\\right)\\ge1-\\tau,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and based on Lemma 4, for any $\\tau\\in(0,1)$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\left\\lVert\\sum_{i=1}^{n}\\frac{\\pmb{\\xi}_{x}^{i}}{n}\\right\\rVert_{2}\\leq\\frac{4\\sigma\\sqrt{d}+2\\sigma\\sqrt{\\log(1/\\tau)}}{\\sqrt{n}}\\right)\\geq1-\\tau.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\pmb{\\Delta}_{x x}:=\\sum_{i=1}^{n}\\frac{\\pmb{\\xi}_{x}^{i}(\\pmb{\\xi}_{x}^{i})^{\\top}}{n}-\\pmb{\\Sigma}_{x x}}\\end{array}$ $\\begin{array}{r}{\\bar{\\xi}:=\\sum_{i=1}^{n}\\frac{\\xi_{x}^{i}}{n}}\\end{array}$ then $\\widehat{M}_{x x}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{M}_{x x}=\\underbrace{\\Sigma_{x x}+\\mu_{x}\\mu_{x}^{\\top}}_{=:M_{x x}}+\\Delta_{x x}+\\mu_{x}\\bar{\\xi}^{\\top}+\\bar{\\xi}\\mu_{x}^{\\top},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and thus we have $\\begin{array}{r}{\\mathcal{M}_{x x}^{-1/2}\\widehat{M}_{x x}M_{x x}^{-1/2}=I_{d}+M_{x x}^{-1/2}\\left(\\Delta_{x x}+\\mu_{x}\\bar{\\xi}^{\\top}+\\bar{\\xi}\\mu_{x}^{\\top}\\right)M_{x x}^{-1/2}}\\end{array}$ Then the minimum eigenvalue of $M_{x x}^{-1/2}\\widehat{M}_{x x}M_{x x}^{-1/2}$ can be lower bounded as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{\\operatorname*{min}}\\left(M_{x x}^{-1/2}\\widehat{M}_{x x}M_{x x}^{-1/2}\\right)}\\\\ &{\\geq1-\\left\\Vert M_{x x}^{-1/2}\\left(\\Delta_{x x}+\\mu_{x}\\bar{\\xi}^{\\top}+\\bar{\\xi}\\mu_{x}^{\\top}\\right)M_{x x}^{-1/2}\\right\\Vert_{\\mathrm{op}}}\\\\ &{\\geq1-\\left\\Vert M_{x x}^{-1/2}\\Delta_{x x}M_{x x}^{-1/2}\\right\\Vert_{\\mathrm{op}}-\\left\\Vert M_{x x}^{-1/2}\\left(\\mu_{x}\\bar{\\xi}^{\\top}+\\bar{\\xi}\\mu_{x}^{\\top}\\right)M_{x x}^{-1/2}\\right\\Vert_{\\mathrm{op}}}\\\\ &{\\geq1-\\frac{C\\sigma^{2}}{\\lambda_{\\operatorname*{min}}\\left(M_{x x}\\right)}\\sqrt{\\frac{d+\\log(2/\\tau)}{n}}-\\frac{2\\left\\Vert\\mu_{x}\\right\\Vert_{2}}{\\lambda_{\\operatorname*{min}}\\left(M_{x x}\\right)}\\frac{4\\sigma\\sqrt{d}+2\\sigma\\sqrt{\\log(1/\\tau)}}{\\sqrt{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the final inequality holds with probability at least $1-2\\tau$ by inserting the above non-asymptotic bounds. Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|M_{x x}^{1/2}\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}\\right\\|_{\\mathrm{op}}}\\\\ &{=\\lambda_{\\operatorname*{min}}^{-1}\\left(M_{x x}^{-1/2}\\widehat{M}_{x x}M_{x x}^{-1/2}\\right)}\\\\ &{\\leq\\left[1-\\frac{C\\sigma^{2}}{\\lambda_{\\operatorname*{min}}(M_{x x})}\\sqrt{\\frac{d+\\log(2/\\tau)}{n}}-\\frac{2\\|\\mu_{x}\\|_{2}}{\\lambda_{\\operatorname*{min}}(M_{x x})}\\frac{4\\sigma\\sqrt{d}+2\\sigma\\sqrt{\\log(1/\\tau)}}{\\sqrt{n}}\\right]^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds with probability at least $1-2\\tau$ . It is easy to observe that as ", "page_idx": 16}, {"type": "equation", "text": "$$\nn\\geq n_{1}:=\\operatorname*{max}\\left\\{\\frac{64C^{2}\\sigma^{4}}{9\\lambda_{\\operatorname*{min}}^{2}(M_{x x})}\\left(d+\\log(2/\\tau)\\right),\\ \\frac{32^{2}\\|\\mu_{x}\\|_{2}^{2}\\sigma^{2}}{\\lambda_{\\operatorname*{min}}^{2}(M_{x x})}\\left(2\\sqrt{d}+\\sqrt{\\log(1/\\tau)}\\right)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have $\\left\\Vert M_{x x}^{1/2}\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}\\right\\Vert_{\\mathrm{op}}\\leq4$ holds with probability $1-2\\tau$ ", "page_idx": 16}, {"type": "text", "text": "For rescaled approximation eror term $\\left\\|\\widehat{\\mathbb{E}}[(\\pmb{y}-\\pmb{Z}_{*}\\pmb{x})\\pmb{x}^{\\top}\\widehat{M}_{\\pmb{x}\\pmb{x}}^{-1/2}]\\right\\|_{F}$ , we first compute variance proxy for the subGaussian vector $y-Z_{*}\\tilde{x}$ . Note that the $j$ -th component of the subGaussian vector ${\\pmb y}-{\\pmb Z}_{*}{\\pmb x}$ can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n[{\\pmb y}-{\\cal Z}_{*}{\\pmb x}]_{j}=(-[{\\pmb Z}_{*}]_{j,:}^{\\top}\\mid{\\pmb e}_{j}^{\\top})\\left({\\pmb x}\\atop{\\pmb y}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{[\\mathbf{Z}_{*}]_{j,}^{\\top}}\\end{array}$ is the $j$ -th row of $Z_{*},\\,e_{j}^{\\top}$ is a $K$ -dimensional vector with $j$ -thcomponent equals to one and rest components equal to zero. Thus, it is easy to observe that the $\\ell_{2}$ -norm square of $(-[Z_{*}]_{j,:}^{\\top}\\mid e_{j}^{\\top})$ is $\\|[\\mathbf{\\dot{Z}}_{*}]_{j,:}\\|_{2}^{2}+\\mathbf{\\dot{1}}$ , and therefore, based on the Assumption 2, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{D}}\\left[\\exp\\left(\\lambda[\\pmb{y}-\\pmb{Z}_{*}\\pmb{x}]_{j}-\\lambda[\\pmb{\\mu}_{\\pmb{y}}-\\pmb{Z}_{*}\\pmb{\\mu}_{\\alpha}]_{j}\\right)\\right]\\leq\\exp\\left(\\lambda^{2}\\cdot(\\|[\\pmb{Z}_{*}]_{j,:}\\|_{2}^{2}+1)\\sigma^{2}/2\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "i.e., a subGaussian with variance proxy $\\sigma_{j}^{2}:=(\\|[Z_{*}]_{j,:}\\|_{2}^{2}+1)\\sigma^{2}$ . Thus the rescaled approximation error can be upper-bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\widehat{\\mathbb{E}}[(y-Z_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{F}^{2}}\\\\ {\\displaystyle=\\sum_{j=1}^{K}\\left\\|\\widehat{\\mathbb{E}}[[y-Z_{*}x]_{j}x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}}\\\\ {\\displaystyle\\leq2\\sum_{j=1}^{K}\\left\\|\\widehat{\\mathbb{E}}[(y-Z_{*}x]_{j}-[\\mu_{y}-Z_{*}\\mu_{x}]_{j})x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}+2\\displaystyle\\sum_{j=1}^{K}\\left\\|\\widehat{\\mathbb{E}}[[\\mu_{y}-Z_{*}\\mu_{x}]_{j}x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}}\\\\ {\\displaystyle=:T_{j}^{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We control term $T_{j}^{1}$ for all $j\\in[K]$ separately using Lemma 5 as follows: For all $\\tau\\in(0,1)$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(T_{j}^{1}\\le\\frac{\\sigma_{j}^{2}(d+2\\sqrt{d\\log(K/\\tau)}+2\\log(K/\\tau))}{n}\\right)\\ge1-\\tau/K.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the term $T_{j}^{2}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T_{j}^{2}=\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}[\\mu_{y}-Z_{*}\\mu_{x}]_{j}(x^{i})^{\\top}\\widehat{M}_{x x}^{-1/2}\\right\\|_{2}^{2}}\\\\ {=\\left\\|\\frac{[\\mu_{y}-Z_{*}\\mu_{x}]_{j}}{\\sqrt{n}}\\left(\\widehat{M}_{x x}^{-1/2}\\frac{x^{1}}{\\sqrt{n}}\\mid\\cdots\\mid\\widehat{M}_{x x}^{-1/2}\\frac{x^{n}}{\\sqrt{n}}\\right)\\mathbf{1}_{n}\\right\\|_{2}^{2}}\\\\ {=\\frac{[\\mu_{y}-Z_{*}\\mu_{x}]_{j}^{2}}{n}\\left\\|\\widehat{M}_{x x}^{-1/2}\\left(\\frac{x^{1}}{\\sqrt{n}}\\mid\\cdots\\mid\\frac{x^{n}}{\\sqrt{n}}\\right)\\mathbf{1}_{n}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now let $\\left({\\frac{x^{1}}{\\sqrt{n}}}\\ |\\ \\cdot\\cdot\\cdot\\ |\\ {\\frac{\\mathbf{x}^{n}}{\\sqrt{n}}}\\right)\\ =\\ U_{x}D_{x}V_{x}^{\\top}$ be the singular ale dempositionof t maix $\\left({\\frac{x^{1}}{\\sqrt{n}}}\\mid\\,\\cdots\\,\\mid\\,{\\frac{x^{n}}{\\sqrt{n}}}\\right)$ , the above $\\ell_{2}$ -norm can be further written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\frac{\\left[\\mu_{y}\\mathrm{~-~}Z_{*}\\mu_{x}\\right]_{j}^{2}}{n}\\left\\|\\widehat{M}_{x x}^{-1/2}\\left(\\frac{x^{1}}{\\sqrt{n}}\\ensuremath{\\vert\\mathrm{~}\\cdots\\vert\\mathrm{~}\\frac{\\pmb{x}^{n}}{\\sqrt{n}}\\right)}\\mathbf{1}_{n}\\right\\|_{2}^{2}}\\\\ &{\\stackrel{\\mathrm{(i)}}{=}\\frac{\\left[\\mu_{y}\\mathrm{~-~}Z_{*}\\mu_{x}\\right]_{j}^{2}}{n}\\mathbf{1}_{n}^{\\top}V_{x}\\left(\\mathbf{0}_{(n-d)\\times d}\\quad\\mathbf{0}_{(n-d)\\times(n-d)}\\right)V_{x}^{\\top}\\mathbf{1}_{n}}\\\\ &{\\stackrel{\\mathrm{(ii)}}{=}\\frac{\\left[\\mu_{y}\\mathrm{~-~}Z_{*}\\mu_{x}\\right]_{j}^{2}}{n}\\cdot d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the equalty(i) holds dueto thedefintion ofempirial matrix $\\begin{array}{r}{\\widehat{M}_{x x}=\\frac{1}{n}\\sum_{i=1}^{n}\\pmb{x}^{i}(\\pmb{x}^{i})^{\\top}}\\end{array}$ equality (ii\uff09 holds due to the unitary property of matrix $V_{x}$ . Combining the above two parts implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\hat{\\mathbb{E}}[(y-Z_{*}x)x^{\\top}\\widehat{M_{x x}}^{-1/2}]\\right\\|_{F}^{2}}\\\\ &{=\\displaystyle\\sum_{j=1}^{K}\\left\\|\\hat{\\mathbb{E}}[|y-Z_{*}x|_{j}x^{\\top}\\widehat{M_{x x}}^{-1/2}]\\right\\|_{2}^{2}}\\\\ &{\\le2\\displaystyle\\sum_{j=1}^{K}T_{j}^{1}+2\\displaystyle\\sum_{j=1}^{K}T_{j}^{2}}\\\\ &{\\overset{(1\\mathrm{tit})}{\\le}2\\displaystyle\\sum_{j=1}^{K}\\frac{\\sigma_{j}^{2}(d+2\\sqrt{d\\log(K/\\tau)}+2\\log(K/\\tau))}{n}+2\\displaystyle\\sum_{j=1}^{K}\\displaystyle\\frac{\\left[\\mu_{y}-Z_{*}\\mu_{x}\\right]_{j}^{2}}{n}\\cdot d}\\\\ &{=2(\\|\\mathbf{z}_{*}\\|_{F}^{2}+K)\\cdot\\frac{d+2\\sqrt{d\\log(K/\\tau)}+2\\log(K/\\tau)}{n}+2\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with inequality (iii\uff09 holds with probability at least $1-\\tau$ . Still, it is easy to observe that for any positive constant $\\epsilon$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\ge n_{2}:=\\operatorname*{max}\\left\\{4(\\|Z_{*}\\|_{F}^{2}+K)\\cdot\\frac{d+2\\sqrt{d\\log(K/\\tau)}+2\\log(K/\\tau)}{\\epsilon},\\,4\\|\\mu_{y}-Z_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{\\epsilon}\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step-3. Combining two upper bounds together, if $n\\geq\\operatorname*{max}\\{n_{1},n_{2}\\}$ , the following inequality for generalization error bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{Z})-\\mathcal{L}(Z_{*})\\leq\\left\\|\\widehat{\\mathbb{E}}[(y-Z_{*}x)x^{\\top}\\widehat{M_{x x}}^{-1/2}]\\right\\|_{F}^{2}\\cdot\\left\\|M_{x x}^{1/2}\\widehat{M_{x x}}^{-1}M_{x x}^{1/2}\\right\\|_{\\mathrm{op}}\\leq4\\epsilon\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds with probability at least $1-3\\tau$ ", "page_idx": 18}, {"type": "text", "text": "We then study the compressed version. ", "page_idx": 18}, {"type": "text", "text": "Step-1'. Similarly, its optimal solutions for population loss and empirical loss are $\\begin{array}{r l}{W_{*}}&{{}=}\\end{array}$ $\\Phi M_{y x}M_{x x}^{-1}$ and $\\begin{array}{r}{\\widehat{W}=\\frac{\\Phi\\overline{{Y}}X^{\\top}}{n}\\left(\\frac{X X^{\\top}}{n}\\right)^{-1}=:\\Phi\\widehat{M}_{y x}\\widehat{M}_{x x}^{-1}}\\end{array}$ , respectively. Thus, the generalization errorbound is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\Phi}(\\widehat{W})-\\mathcal{L}^{\\Phi}(W_{*})=\\Vert\\widehat{W}-W_{*}\\Vert_{M_{x x}}^{2}=\\Vert(\\widehat{W}-W_{*})M_{x x}^{1/2}\\Vert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Stil. we have $(\\widehat{W}-W_{*})M_{x x}^{1/2}=\\widehat{\\mathbb{E}}[(\\Phi y-W_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2},$ Ir' , and therefore, the generalization error bound can be upper-bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|(\\widehat{W}-W_{*})M_{x x}^{1/2}\\right\\|_{F}^{2}\\leq\\left\\|\\widehat{\\mathbb{E}}[(\\Phi y-W_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{F}^{2}\\left\\|\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\|_{\\mathrm{op}}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step- $.2^{\\,\\bullet}$ . Next, we provide upper bounds on these two terms $\\left\\|\\widehat{\\mathbb{E}}\\big[(\\pmb{\\Phi}y-W_{*}\\pmb{x})\\pmb{x}^{\\top}\\widehat{M}_{\\pmb{x}\\pmb{x}}^{-1/2}\\big]\\right\\|_{F}$ and $\\left\\Vert\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\Vert_{\\mathrm{op}}$ in the right-hand-side separately. Note that for the matrix error term $\\left\\Vert\\widehat{M}_{x x}^{-1/2}M_{x x}^{1/2}\\right\\Vert_{\\mathrm{o_{f}}}$ , we could use the same upper bounded as mentioned in the proof of uncompressed version. ", "page_idx": 18}, {"type": "text", "text": "$\\left\\|\\widehat{\\mathbb{E}}\\big[(\\pmb{\\Phi}y-W_{*}\\pmb{x})\\pmb{x}^{\\top}\\widehat{M}_{\\pmb{x}\\pmb{x}}^{-1/2}\\big]\\right\\|_{F}$ vector $\\Phi y-W_{*}x$ , which is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{j}^{2}:=(\\|[\\boldsymbol{W}_{*}]_{j,:}\\|_{2}^{2}+\\|\\Phi_{j,:}\\|_{2}^{2})\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $j\\in[m]$ . Thus, the rescaled approximation error for the compressed version can be upper bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\widehat{\\mathbb{E}}[(\\Phi y-W_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\vert\\right\\vert_{F}^{2}=\\displaystyle\\sum_{j=1}^{m}\\left\\|\\widehat{\\mathbb{E}}[[\\Phi y-W_{*}x]_{j}x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\displaystyle\\sum_{j=1}^{m}\\underbrace{\\left\\|\\widehat{\\mathbb{E}}[([\\Phi y-W_{*}x]_{j}-[\\Phi\\mu_{y}-W_{*}\\mu_{x}]_{j})x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}}_{=:\\widehat{T}_{j}^{1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle2\\sum_{j=1}^{m}\\underbrace{\\left\\|\\widehat{\\mathbb{E}}[[\\Phi\\mu_{y}-W_{*}\\mu_{x}]_{j}x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}.}_{=:\\widehat{T}_{j}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Still using Lemma 5, for all $\\tau\\in(0,1)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\widetilde{T}_{j}^{1}\\leq\\frac{\\widetilde{\\sigma}_{j}^{2}(d+2\\sqrt{d\\log(m/\\tau)}+2\\log(m/\\tau))}{n}\\right)\\geq1-\\tau/m.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the term $\\widetilde{T}_{j}^{2}$ , following the same proof procedures of the uncompressed version implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widetilde{T}_{j}^{2}=\\frac{[\\Phi\\mu_{y}\\,-\\,W_{*}\\mu_{x}]_{j}^{2}}{n}\\,\\biggl\\|\\widehat{M}_{x x}^{-1/2}\\left(\\frac{x^{1}}{\\sqrt{n}}\\mid\\cdots\\mid\\frac{x^{n}}{\\sqrt{n}}\\right)\\mathbf{1}_{n}\\biggr\\|_{2}^{2}}\\\\ {=\\frac{[\\Phi\\mu_{y}\\,-\\,W_{*}\\mu_{x}]_{j}^{2}}{n}\\cdot d}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the rescaled approximation error for the compressed version is upper-bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\widehat{\\mathbb{E}}[(\\Phi y-W_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{F}^{2}}\\\\ &{\\le2(\\|W_{*}\\|_{F}^{2}+\\|\\Phi\\|_{F}^{2})\\cdot\\displaystyle\\frac{d+2\\sqrt{d\\log(m/\\tau)}+2\\log(m/\\tau)}{n}+2\\|\\Phi\\mu_{y}-W_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\displaystyle\\frac{d}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability at least $1-\\tau$ . Similarly, it is easy to get that for any positive constant $\\epsilon$ ,as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{n}_{2}:=\\operatorname*{max}\\left\\{4(\\|W_{*}\\|_{F}^{2}+\\|\\Phi\\|_{F}^{2})\\cdot\\frac{d+2\\sqrt{d\\log(m/\\tau)}+2\\log(m/\\tau)}{\\epsilon},\\,4\\|\\Phi\\mu_{y}-W_{*}\\mu_{x}\\|_{2}^{2}\\cdot\\frac{d}{\\epsilon}\\right\\},}\\\\ &{\\mathrm{ave}\\left\\|\\widehat{\\mathbb{E}}[(\\Phi y-W_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{F}^{2}\\leq\\epsilon\\mathrm{~holds~with~probability~at~least~}1-\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Step-3'. Combining two upper bounds together, if $n\\geq\\operatorname*{max}\\{n_{1},\\widetilde{n}_{2}\\}$ , the following inequality for generalization error bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\Phi}(\\widehat{W})-\\mathcal{L}^{\\Phi}(W_{*})\\leq\\left\\lVert\\widehat{\\mathbb{E}}[(\\Phi y-W_{*}x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\rVert_{F}^{2}\\cdot\\left\\lVert M_{x x}^{1/2}\\widehat{M}_{x x}^{-1}M_{x x}^{1/2}\\right\\rVert_{\\mathrm{op}}\\leq4\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds with probability at least $1-3\\tau$ ", "page_idx": 19}, {"type": "text", "text": "A.4Proof of Theorem 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall the following theorem in the main text. ", "page_idx": 19}, {"type": "text", "text": "Theorem 4 For any $\\delta\\,\\in\\,(0,1)$ and any $\\tau\\,\\in\\,(0,1/3)$ , suppose the compressed matrix $\\Phi$ follows Assumption 1 with $\\begin{array}{r}{m\\geq O(\\frac{s}{\\delta^{2}}\\log(\\frac{K}{\\tau}))}\\end{array}$ , and Assumption 2 holds. Given any learned regressor W from training problem (2), let $\\left({\\pmb x},{\\pmb y}\\right)$ be a new sample drawn from the underlying distribution $\\mathcal{D}$ ,we have the following inequality holds with probability at least $1-\\tau$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}}[\\|\\widehat{\\pmb{y}}-\\pmb{y}\\|_{2}^{2}]\\leq\\frac{4}{1-\\delta}\\cdot\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}\\pmb{y}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\hat{\\pmb y}$ is the optimal solution from prediction problem (3) with input vector $\\textbf{\\em x}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. Due to the optimality of $\\hat{\\pmb y}$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}\\leq\\left\\|\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}}\\\\ {\\Leftrightarrow}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}+\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}\\leq\\left\\|\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}}\\\\ {\\Leftrightarrow}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}\\right\\|_{2}^{2}\\leq2\\langle\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y},\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\rangle}\\\\ {\\Rightarrow}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}\\right\\|_{2}^{2}\\leq2\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}\\right\\|_{2}\\left\\|\\Phi\\boldsymbol{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}}\\\\ {\\Leftrightarrow}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}\\right\\|_{2}\\leq2\\left\\|\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}}\\\\ {\\Leftrightarrow}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}\\right\\|_{2}^{2}\\leq4\\left\\|\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}}\\\\ {\\Leftrightarrow}&{\\left\\|\\Phi\\hat{\\mathbf{y}}-\\Phi\\mathbf{y}\\right\\|_{2}^{2}\\leq4\\left\\|\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}}\\\\ {\\Rightarrow}&{(1-\\delta)\\|\\hat{\\mathbf{y}}-\\mathbf{y}\\|_{2}^{2}\\leq4\\left\\|\\Phi\\mathbf{y}-\\widehat{\\mathbf{W}}\\mathbf{x}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the final $\\Rightarrow{}$ holds due to the $(3s,\\delta)$ -RIP property of the compressed matrix $\\Phi$ with probability at least $1-\\tau$ . Taking expectations on both sides implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n(1-\\delta)\\mathbb{E}_{\\mathcal{D}}[\\|\\widehat{\\pmb{y}}-\\pmb{y}\\|_{2}^{2}]\\leq4\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}\\pmb{y}-\\widehat{\\pmb{W}}\\pmb{x}\\|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the story. ", "page_idx": 19}, {"type": "text", "text": "A.5 Technical Lemma ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.5.1 Proof of Claim Proposed in Remark 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Let us discuss the computational complexity for $\\mathcal{F}$ to be $\\mathbb{R}^{K},\\mathbb{R}_{+}^{K},\\{0,1\\}^{K}$ separately. Given a fixed $\\tilde{v}$ ", "page_idx": 20}, {"type": "text", "text": "\u00b7If $\\mathcal{F}\\,=\\,\\mathbb{R}^{K}$ , the projection method $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}}\\ \\|\\pmb{v}-\\pmb{\\tilde{v}}\\|_{2}^{2}}\\end{array}$ can be reformulate using the following mixed-integer programming (MIP), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(v_{*},z_{*})\\;:=\\;\\arg\\operatorname*{min}_{v,z}\\quad\\sum_{p=1}^{K}z_{p}(v_{p}-\\tilde{v}_{p})^{2}\\;\\;,}\\\\ {\\mathrm{s.t.}\\quad\\sum_{p=1}^{K}z_{p}\\leq s\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\pmb{v}_{*}$ the output of the projection method. Sorting the absolute values $\\{|\\widetilde{\\pmb{v}}_{p}|\\}_{p=1}^{K}$ in decreasing order such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widetilde{\\pmb{v}}_{(1)}|\\geq\\cdots\\geq|\\widetilde{\\pmb{v}}_{(K)}|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the output $\\pmb{v}_{*}$ of the proposed projection is ", "page_idx": 20}, {"type": "equation", "text": "$$\n[v_{*}]_{j}=\\left\\{\\begin{array}{l l}{\\tilde{v}_{j}}&{\\mathrm{if}\\;j\\in\\{(1),\\dots,(s)\\}}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with computational complexity $O(K\\operatorname*{min}\\{s,\\log K\\})$ ", "page_idx": 20}, {"type": "text", "text": "\u00b7If $\\mathcal{F}\\,=\\,\\mathbb{R}_{+}^{K}$ , the projection method $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}}~\\|\\pmb{v}-\\tilde{\\pmb{v}}\\|_{2}^{2}}\\end{array}$ can be reformulate using the following mixed-integer programming (MIP), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(v_{*},z_{*})\\;:=\\;\\arg\\operatorname*{min}_{v,z}}&{\\sum_{p=1}^{K}z_{p}(v_{p}-\\tilde{v}_{p})^{2}}\\\\ {\\mathrm{s.t.}}&{\\sum_{p=1}^{K}z_{p}\\leq s}\\\\ &{v_{p}\\geq0~\\forall\\:p\\in[K]}\\end{array}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Sorting $\\{\\tilde{v}_{p}\\}_{p=1}^{K}$ in decreasing order such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{v}}_{(1)}\\geq\\dots\\geq\\tilde{\\pmb{v}}_{(K)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the output $\\pmb{v}_{*}$ of the proposed projection is ", "page_idx": 20}, {"type": "equation", "text": "$$\n[\\pmb{v}_{*}]_{j}=\\left\\{\\begin{array}{l l}{\\tilde{\\pmb{v}}_{j}\\cdot\\mathbb{I}(\\tilde{\\pmb{v}}_{j}>0)}&{\\mathrm{if}\\;j\\in\\{(1),\\dots,(s)\\}}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with computation complexity $O(K\\operatorname*{min}\\{s,\\log K\\})$ ", "page_idx": 20}, {"type": "text", "text": "\u00b7If $\\mathcal{F}=\\{0,1\\}^{K}$ , the projection method $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}}\\;||\\pmb{v}-\\pmb{\\tilde{v}}||_{2}^{2}}\\end{array}$ presented in step-4 of Algorithm 1 can be represented as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\operatorname*{min}_{z}}&{\\sum_{p=1}^{K}(1-z_{p})\\tilde{v}_{p}^{2}+z_{p}(\\tilde{v}_{p}-1)^{2}\\qquad}&{\\operatorname*{min}_{z}}&{\\sum_{p=1}^{K}\\tilde{v}_{p}^{2}-z_{p}(2\\tilde{v}_{p}-1)}\\\\ {\\mathrm{s.t.}}&{\\sum_{p=1}^{K}z_{p}\\leq s}\\\\ &{z_{p}\\in\\{0,1\\}\\,\\,\\forall\\,p\\in[K]}\\end{array}.=\\begin{array}{r c l}&{\\mathrm{min}_{z}}&{\\sum_{p=1}^{K}z_{p}\\leq s}\\\\ &{\\mathrm{s.t.}}&{\\sum_{p=1}^{K}z_{p}\\leq s}\\end{array}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Sort $\\{2\\tilde{v}_{p}-1\\}_{p=1}^{K}$ in decreasing order such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n2\\tilde{v}_{(1)}-1\\ge2\\tilde{v}_{(2)}-1\\ge\\cdots\\ge2\\tilde{v}_{(K)}-1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then, the optimal $z^{*}$ can be set by ", "page_idx": 20}, {"type": "equation", "text": "$$\nz_{p}^{*}=\\left\\{\\begin{array}{l l}{\\mathbb{I}(2v_{p}-1>0)}&{\\mathrm{if}\\;p\\in\\{(1),\\dots,(s)\\}}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right.\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For computational complexity, computing the sequence $\\{2v_{p}-1\\}_{p=1}^{K}$ needs $O(K)$ , picking the top- $s$ elements of the above sequence requires $O(K\\operatorname*{min}\\{s,\\log K\\})$ , setting the optimal solution $z^{*}$ needs $O(s)$ , and thus the total computational complexity is $O(K)+O(K\\operatorname*{min}\\{s,\\log K\\})+O(s)=$ $O(K\\operatorname*{min}\\{s,\\log K\\})$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A.5.2 Lemma for the Proof of Theorem 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 1. (Unitary invariant). Let $\\Phi\\in\\mathbb{R}^{m\\times d}$ be a randomized compressed matrix as described in Assumption $^{\\,l}$ and $U\\in\\mathbb{R}^{d\\times d}$ be a real unitary matrix. Then we have $\\tilde{\\Phi}=\\Phi U$ is $(1,\\delta)$ -RIP with probability at least $1-\\tau$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Note that $(i,j)$ -th component of $\\tilde{\\Phi}$ can be represented as $\\begin{array}{r}{\\tilde{\\Phi}_{i,j}=\\Phi_{i,:}U_{:,j}=\\sum_{\\ell=1}^{d}\\Phi_{i,\\ell}U_{\\ell,j}}\\end{array}$ Since every component $\\Phi_{i,j}$ in $\\Phi$ is i.i.d. drawn from $\\mathcal{N}(0,1/m)$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}_{i,j}=\\sum_{\\ell=1}^{d}{\\Phi_{i,\\ell}U_{\\ell,j}}\\sim\\mathcal{N}\\left(0,\\sum_{\\ell=1}^{d}{\\frac{1}{m}U_{\\ell,j}^{2}}\\right)=\\mathcal{N}(0,1/m).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we need to show that any two distinct components $\\tilde{\\Phi}_{i_{1},j_{1}}$ and $\\tilde{\\Phi}_{i_{2},j_{2}}$ in $\\tilde{\\Phi}$ are independent. It is easy to observe that $\\tilde{\\Phi}_{i_{1},j_{1}}$ and $\\tilde{\\Phi}_{i_{2},j_{2}}$ are independent when $i_{1}\\neq i_{2}$ since $\\Phi_{i_{1},}$ : and $\\Phi_{i_{2},}$ : are independent. If $i_{1}=i_{2}=i$ , then the following random vector satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\tilde{\\Phi}_{i,j_{1}}\\right)=\\left(\\Phi_{i,:}U_{:,j_{1}}\\right)\\sim\\mathcal{N}\\left(\\binom{0}{0},\\binom{1/m}{0}\\begin{array}{c c}{0}\\\\ {1/m}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "That is to say, $\\tilde{\\Phi}_{i_{1},j_{1}}$ and $\\tilde{\\Phi}_{i_{2},j_{2}}$ are jointly Gaussian distributed and uncorrelated, which shows that $\\tilde{\\Phi}_{i_{1},j_{1}}$ and $\\tilde{\\Phi}_{i_{2},j_{2}}$ are independent. Combining the above together, we have $\\tilde{\\Phi}$ is a randomized matrix with component i.i.d. from $\\mathcal{N}(0,1/m)$ . Based on the existing result ([7], Theorem 1.5), when $m\\ge C_{1}\\cdot\\delta^{-2}[\\ln(e K)+\\ln(2/\\tau)]$ for any $\\delta>0$ and $\\tau\\in(0,1)$ , we have $\\tilde{\\Phi}$ ensures $(1,\\delta)$ -RIP with probability at least $1-\\tau$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A.5.3Lemma for the Proof of Theorem 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 2. For any integer parameter $s(\\leq d)$ and positive parameter $\\delta\\in(0,1)$ ,let $\\Phi\\in\\mathbb{R}^{m\\times d}$ be a: $(s,\\delta)$ -RIP matrix. For $\\mathbf{\\mathit{u}}_{1},\\mathbf{\\mathit{u}}_{2}$ ${\\bf\\nabla}^{\\prime}u_{1},u_{2},u_{1}+u_{2},u_{1}-u_{2}$ are all $s$ -sparse, then the following inequalityholds ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-2\\delta(\\|u_{1}\\|_{2}^{2}+\\|u_{2}\\|_{2}^{2})+4\\langle u_{1},u_{2}\\rangle\\leq4\\langle\\Phi u_{1},\\Phi u_{2}\\rangle\\leq2\\delta(\\|u_{1}\\|_{2}^{2}+\\|u_{2}\\|_{2}^{2})+4\\langle u_{1},u_{2}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Since ${\\pmb u}_{1},{\\pmb u}_{2},{\\pmb u}_{1}+{\\pmb u}_{2},{\\pmb u}_{1}-{\\pmb u}_{2}$ are $s$ -sparse, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\delta)(\\|u_{1}+u_{2}\\|_{2}^{2})\\le\\langle\\Phi(u_{1}+u_{2}),\\Phi(u_{1}+u_{2})\\rangle\\le(1+\\delta)(\\|u_{1}+u_{2}\\|_{2}^{2})}\\\\ &{(1-\\delta)(\\|u_{1}-u_{2}\\|_{2}^{2})\\le\\langle\\Phi(u_{1}-u_{2}),\\Phi(u_{1}-u_{2})\\rangle\\le(1+\\delta)(\\|u_{1}-u_{2}\\|_{2}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Subtracting (9) from (8) gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-2\\delta(\\|u_{1}\\|_{2}^{2}+\\|u_{2}\\|_{2}^{2})+4\\langle u_{1},u_{2}\\rangle\\leq4\\langle\\Phi u_{1},\\Phi u_{2}\\rangle\\leq2\\delta(\\|u_{1}\\|_{2}^{2}+\\|u_{2}\\|_{2}^{2})+4\\langle u_{1},u_{2}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the proof. ", "page_idx": 21}, {"type": "text", "text": "A.5.4Lemma for the Proof of Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 3. Let $\\xi^{1},\\dots,\\xi^{n}$ be $n$ i.i.d. $\\sigma^{2}$ -subGaussian random vectors with a zero mean and $a$ covariance matrix $\\Sigma$ .Then, there exists a positive constant $C$ such that for all $\\tau\\in(0,1)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{i=1}^{n}\\frac{\\xi^{i}(\\xi^{i})^{\\top}}{n}-\\Sigma\\right\\rVert_{o p}\\leq C\\sigma^{2}\\operatorname*{max}\\left\\{\\sqrt{\\frac{d+\\log(2/\\tau)}{n}},\\frac{d+\\log(2/\\tau)}{n}\\right\\}\\right)\\geq1-\\tau.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Lemma 3 is a direct corollary from [Theorem 6.5, [41]]. It is easy to observe that the proposed Lemma 3 holds by setting the parameter $\\delta$ listed in [Theorem 6.5, [41] as $\\operatorname*{min}\\{1,c\\sqrt{\\ln(2/\\tau)/n}\\}$ With $c$ some positive constant. ", "page_idx": 21}, {"type": "text", "text": "Lemma 4. Let $\\xi^{1},\\dots,\\xi^{n}$ be $n$ i.i.d. $\\sigma^{2}$ -subGaussian random vectors with a zero mean and $a$ covariance matrix $\\Sigma$ .Then for any $\\tau\\in(0,1)$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{i=1}^{n}\\frac{\\xi^{i}}{n}\\right\\rVert_{2}\\leq\\frac{4\\sigma\\sqrt{d}+2\\sigma\\sqrt{\\log(1/\\tau)}}{\\sqrt{n}}\\right)\\geq1-\\tau.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We show this lemma by discretizing the unit $\\ell_{2}$ -norm ball $\\mathbb{B}_{2}(\\mathbf{0};1)$ . Let $\\mathcal{N}_{1/2}$ be a $\\frac{1}{2}$ -minimum cover of $\\mathbb{B}_{2}(\\mathbf{0};1)$ with its cardinality $|\\mathcal{N}_{1/2}|\\leq5^{d}$ . Since for any vector $\\pmb{\\xi}\\in\\mathbb{R}^{d}$ , we always have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{\\xi}\\|_{2}=\\operatorname*{max}_{\\|\\pmb{v}\\|_{2}\\leq1}\\langle\\pmb{v},\\pmb{\\xi}\\rangle\\leq\\operatorname*{max}_{\\pmb{v}^{\\prime}\\in N_{1/2}}\\langle\\pmb{v}^{\\prime},\\pmb{\\xi}\\rangle+\\operatorname*{max}_{\\|\\pmb{v}^{\\prime\\prime}\\|_{2}\\leq1/2}\\langle\\pmb{v}^{\\prime\\prime},\\pmb{\\xi}\\rangle=\\operatorname*{max}_{\\pmb{v}^{\\prime}\\in N_{1/2}}\\langle\\pmb{v}^{\\prime},\\pmb{\\xi}\\rangle+\\frac{1}{2}\\operatorname*{max}_{\\|\\pmb{v}^{\\prime\\prime}\\|_{2}\\leq1}\\langle\\pmb{v}^{\\prime\\prime},\\pmb{\\xi}\\rangle,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then $\\|\\pmb{\\xi}\\|_{2}\\leq2\\operatorname*{max}_{\\pmb{v}^{\\prime}\\in\\mathcal{N}_{1/2}}\\langle\\pmb{v}^{\\prime},\\pmb{\\xi}\\rangle$ . Therefore, for any $\\sigma^{2}$ -subGaussian random vector, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\|\\pmb{\\xi}\\|_{2}\\ge t)\\ge\\mathbb{P}\\left(\\operatorname*{max}_{v^{\\prime}\\in{\\cal N}_{1/2}}\\langle v^{\\prime},\\pmb{\\xi}\\rangle\\ge t/2\\right)\\le|{\\cal N}_{1/2}|\\cdot\\exp\\left(-\\frac{t^{2}}{8\\sigma^{2}}\\right)\\le5^{d}\\cdot\\exp\\left(-\\frac{t^{2}}{8\\sigma^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies that $\\|\\pmb{\\xi}\\|_{2}\\,\\leq\\,4\\sigma\\sqrt{d}+2\\sigma\\sqrt{\\log(1/\\tau)}$ with probability at least $1-\\tau$ . Now, since $\\begin{array}{r}{\\bar{\\pmb{\\xi}}=\\sum_{i=1}^{n}\\frac{\\pmb{\\xi}^{i}}{n}}\\end{array}$ $\\sigma^{2}/n$ ", "page_idx": 22}, {"type": "text", "text": "Lemma 5. Let $\\eta({\\pmb x})$ be a zero-mean, $\\sigma_{\\eta}^{2}$ subGaussan random variable. Let $x^{1},\\ldots,x^{n}$ be n i.id. $\\sigma^{2}$ -subGaussian random vectors (may not zero-mean)as described in Assumption 2. Conditioned on $\\begin{array}{r}{\\widehat{M}_{x x}=\\frac{1}{n}\\sum_{i=1}^{n}x^{i}(x^{i})^{\\top}\\succ\\mathbf{0}_{d\\times d},}\\end{array}$ for any $\\tau\\in(0,1)$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|\\mathbb{\\widehat{E}}[\\eta(x)x^{\\top}\\widehat{M}_{x x}^{-1/2}]\\right\\|_{2}^{2}\\leq\\frac{\\sigma_{\\eta}^{2}(d+2\\sqrt{d\\log(1/\\tau)}+2\\log(1/\\tau))}{n}\\right)\\geq1-\\tau.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The proof of Lemma 5 can be found in [Lemma 5, [18]]. ", "page_idx": 22}, {"type": "text", "text": "A.5.5 Discussion after Theorem 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Since $\\|\\pmb{v}^{(T)}-\\pmb{y}\\|_{2}\\leq\\|\\pmb{v}^{(T)}-\\widehat{\\pmb{y}}\\|_{2}+\\|\\widehat{\\pmb{y}}-\\pmb{y}\\|_{2}.$ . combined with Theorem 2, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{D}}[\\|v^{(T)}-y\\|_{2}^{2}]\\le2\\mathbb{E}_{\\mathcal{D}}[\\|v^{(T)}-\\widehat{\\pmb{y}}\\|_{2}^{2}]+2\\mathbb{E}_{\\mathcal{D}}[\\|\\widehat{\\pmb{y}}-{\\pmb{y}}\\|_{2}^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\le O\\bigl(\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}\\widehat{\\pmb{y}}-\\widehat{\\pmb{W}}x\\|_{2}]\\bigr)+\\frac{8}{1-\\delta}\\cdot\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}y-\\widehat{\\pmb{W}}x\\|_{2}^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\le O\\bigl(\\mathbb{E}_{\\mathcal{D}}[\\|\\pmb{\\Phi}y-\\widehat{\\pmb{W}}x\\|_{2}]\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "A.6 Additional Numerical Experiments on Synthetic Data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "A.6.1  Implemented Prediction Method ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The implemented prediction method is presented as follows, see Algorithm 2. Comparing with Algorithm 1 proposed in the main content, it adds an additional stopping criteria ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\frac{\\|\\pmb{v}^{(t)}-\\pmb{v}^{(t-2)}\\|_{2}}{0.01+\\|\\pmb{v}^{(t)}\\|_{2}}}<10^{-6}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "to ensure an earlier stop than Algorithm 1. ", "page_idx": 22}, {"type": "text", "text": "Note, in later numerical experiments, we use the terminology \u2018early stopping\u2032 to denote that the iteration generated by the prediction algorithm satisfies the above additional stopping criteria within the maximum iteration number, i.e. $T=60$ (as listed in Section 4). ", "page_idx": 22}, {"type": "text", "text": "A.6.2 Discussions on Baselines ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Baselines. We compare our proposed prediction method with the following baselines. ", "page_idx": 22}, {"type": "text", "text": "Orthogonal Matching Pursuit. Orthogonal Matching Pursuit(OMP) is a greedy prediction algo. rithm. It iteratively chooses the most relevant output and then performs least-squares and updates the residuals. The built-in function \u201cOrthogonalMat chingPursuit\u2032 from Python package \u2018Sklearn.Linear_model' is used in the experiment. ", "page_idx": 22}, {"type": "text", "text": "Correlation Decoding. Correlation decoding is a standard decoding algorithm. It computes the multiplication of the transpose of compression matrix $\\Phi$ and the learned regressor W. For any test point $\\textbf{\\em x}$ , the algorithm predicts the top $s$ labels in \u03a6 W  orded by magnitude. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Implemented Projected Gradient Descent (for Second Stage) ", "page_idx": 23}, {"type": "text", "text": "Input: Regressor $\\widehat{W}$ , input sample $\\textbf{\\em x}$ , stepsize $\\eta$ , total iterations $T$ ", "page_idx": 23}, {"type": "text", "text": "1: Initialize point $\\pmb{v}^{(0)}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}$ and $t=0$   \n2: while $t<T$ and $\\|\\pmb{v}^{(t)}-\\pmb{v}^{(t-2)}\\|_{2}/(0.01+\\|\\pmb{v}^{(t)}\\|_{2})>10^{-3}$ do   \n3: Update $\\widetilde{\\pmb{v}}^{(t+1)}=\\pmb{v}^{(t)}-\\eta\\cdot\\pmb{\\Phi}^{\\top}(\\pmb{\\Phi}v^{(t)}-\\widehat{\\pmb{W}}\\pmb{x})$   \n4: Project $\\begin{array}{r}{\\pmb{v}^{(t+1)}=\\Pi(\\widetilde{\\pmb{v}}^{(t+1)}):=\\arg\\operatorname*{min}_{\\pmb{v}\\in\\mathcal{V}_{s}^{K}\\cap\\mathcal{F}}\\|\\pmb{v}-\\widetilde{\\pmb{v}}^{(t+1)}\\|_{2}^{2}.}\\end{array}$   \n5: Update $t:=t+1$   \n6: end while   \nN.----. -.(T) ", "page_idx": 23}, {"type": "text", "text": "Elastic Net. The elastic net is a regression method that combines both the $\\ell_{1}$ -norm penalty and the $\\ell_{2}$ -norm penalty to guarantee the sparsity and the stability of the prediction. The parameters for the $\\ell_{1}$ -norm penalty and $\\ell_{1}$ -norm penalty in Elastic Net are set to be 0.1 through the experiments. The built-in function \u2018ElasticNet\u2032 from Python package 'Sklearn.Linear _model' is used in the experiment. ", "page_idx": 23}, {"type": "text", "text": "Fast Iterative Shrinkage-Thresholding Algorithm. The fast iterative shrinkage-thresholding algorithm is an advanced optimization method designed to efficiently solve certain classes of unconstrained convex optimization problems. It utilizes momentum-like strategies to speed up convergence rates, which is particularly effective for minimizing functions that may be non-smooth. ", "page_idx": 23}, {"type": "text", "text": "A.6.3 Procedures on Generating Mean & Covariance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this paper, we give exact procedures on selecting $\\pmb{\\mu}_{\\pmb{x}}$ and $\\Sigma_{x x}$ as mentioned in Section 4. ", "page_idx": 23}, {"type": "text", "text": "\u00b7 The mean vector $\\pmb{\\mu}_{\\pmb{x}}$ is selected as follows. We first generate a $d$ -dimensional vector $\\pmb{\\mu}$ from the standard $d$ -dimensional Gaussian distribution $\\mathcal{N}(\\mathbf{0}_{d},I_{d})$ . Then we set ", "page_idx": 23}, {"type": "equation", "text": "$$\n[{\\pmb{\\mu}}_{x}]_{j}=|{\\pmb{\\mu}}|_{j}\\,\\,\\,\\mathrm{for\\,all}\\,\\,\\,j\\in[d]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "by taking absolute values over all components. ", "page_idx": 23}, {"type": "text", "text": "\u00b7 The covariance matrix $\\Sigma_{x x}$ is selected as follows. We first generate a $d$ -by- $d$ matrix $\\pmb{A}$ , where every component of $\\pmb{A}$ is i.i.d. generated from the standard Gaussian distribution ${\\mathcal{N}}(0,1)$ . Then the covariance matrix $\\Sigma_{x x}$ is set to be ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{x x}:=\\frac{1}{d}\\pmb{A}^{\\top}\\pmb{A}+\\frac{1}{2}\\pmb{I}_{d}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "A.7  Additional Numerical Experiments on Real Data ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we do experiments on real data and compare the performance of the proposed prediction method (see Algorithm 2) with four baselines, i.e., Orthogonal Matching Pursuit (OMP, [46]), Correlation Decoding (CD,[20]), Elastic Net (EN, [47]), and Fast Iterative Shrinkage-Thresholding Algorithm (FISTA,[2]) see Appendix A.6.2 for detailed explanations. ", "page_idx": 23}, {"type": "text", "text": "Real data.  We select two benchmark datasets in multi-label classification, Wiki10-31K and EURLex-4K[5] due to their sparsity property. Table 1 shows the details for the datasets. ", "page_idx": 23}, {"type": "table", "img_path": "kPGNE4CrTq/tmp/0efed189ba5cfed3c027fd4cbf1c998545276d26a33eead32c2209dcf0bb8aa8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 1: Statistics and details for training and test sets, where $\\overline{{d}},\\overline{{K}}$ denote their averaged non-zero components for input and output, respectively. ", "page_idx": 23}, {"type": "image", "img_path": "kPGNE4CrTq/tmp/fcfff691bc43bdb91616f9603d00344dca13f1b97956362909e5f537d65ba992.jpg", "img_caption": ["Figure 2: The figure reports the prediction running time (measured in seconds) on synthetic data with early stopping by the proposed algorithm under different compressed output dimensions. As we can observe, the running time first decreases dramatically, then increases almost linearly with respect to $m$ . Such a phenomenon has occurred since the max number of iterations is 60 in the implemented prediction method with early stopping, which is relatively large; As $m$ increases but is still less than 500, the actual number of iterations drops dramatically due to early stopping criteria; After passes 500, the actual number of iterations stays around 10, and then the running time grows linearly as dimension increases. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Parameter setting. In prediction stage, we choose $s\\,\\in\\,\\{1,3\\}$ for EURLex-4K and $s\\,\\in\\,\\{3,5\\}$ for Wiki10-31K. We choose the number of rows $m\\in\\{100,200,300,400,500,700,1000,2000\\}$ on both EURLex-4K and Wiki10-31. Ten independent trials of compressed matrices $\\Phi^{(1)},\\dots,\\Phi^{(10)}$ are implemented for each tuple of parameters $(s,m)$ on both datasets. ", "page_idx": 24}, {"type": "table", "img_path": "kPGNE4CrTq/tmp/f30a2c7c0c5f6cc779b601fd3b580f697a13d7c6360b7fc737c47fd28e2bae28.jpg", "table_caption": ["Empirical running time. Here, we report the running time of the proposed algorithm and baselines on both synthetic and real datasets, see Table 2. ", "Table 2: Time Complexity Comparison for each prediction "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Numerical Results & Discussions. Figure 2 further ilustrates that the computational complexity increases linearly with respect to the growth of compressed output dimension $m$ On synthetic data, when $m$ is greater than 500 to ensure the convergence of the prediction algorithm (see Remark 2). ", "page_idx": 24}, {"type": "text", "text": "For real data, Figure 3 and Figure 4 present the results of their accuracy performances. In particular, the accuracy grows relatively stable with respect to $m$ when the compression matrix satisfies the RIP-property with high probability. Besides, based on the results presented in Figure 3, Figure 4, and Table 2, we observe that the proposed algorithm slightly outperforms the baselines on precision as $s$ increases while enjoys a significant better computational efficiency, especially on large instances, which demonstrate the stability of the proposed algorithm. ", "page_idx": 24}, {"type": "image", "img_path": "kPGNE4CrTq/tmp/36c69ef129e217773143af262645daffcebf93fc095b7dc046b2b361d7d82ac2.jpg", "img_caption": ["Figure 3: This figure reports the numerical results on real data - EURLex-4K. Each dot in the figure represents 10 independent trials (i.e., experiments) of compressed matrices $\\Phi^{(1)},\\dots,\\Phi^{(10)}$ Ona given tuple of parameters $(s,m)$ . The curves in each panel correspond to the averaged values for the proposed Algorithm and baselines over 10 trials; the shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the output distance versus the number of rows. In the second row, we plot the precision versus the number of rows, and we cannot observe significant differences between these prediction methods. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "kPGNE4CrTq/tmp/14c63b47e3fa4cee16f4b66a0e55e34e71a3fc99bae0439897384be45bdd4ae3.jpg", "img_caption": ["Figure 4: This figure reports the numerical results on real data - Wiki10-31K. Similar to the plot reporting on EURLex-4K above, each dot in the figure represents 10 independent trials (i.e., experiments) of compressed matrices $\\Phi^{(1)},\\dots,\\Phi^{(10)}$ on a given tuple of parameters $(s,m)$ The curves in each panel correspond to the averaged values for the proposed algorithm and baselines over 10 trials; the shaded parts represent the empirical standard deviations over 10 trials. Similarly, in the first row, the precision of the proposed algorithm outperforms the FISTA especially when $s$ is small. In the second & third rows for output difference and prediction loss, there are only slight improvement on the proposed algorithm than CD of output difference. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper's abstract is the short highlight of our introduction, covering the problems we study, the theoretical results we establish, and the numerical experiments we conduct. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We mention assumptions and discuss the limitation of the proposed method, and compare our method with other existing methods. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Assumptions and complete proofs are presented in the Appendix with a short proof sketch in the main paper. All the theorems, used formulas, and proofs in the paper should be numbered and cross-referenced. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The numerical experiments are controlled by the default random seed, and it could be reproduced. Moreover, we not only provide the code but also include the detailed steps of the experiment in the main paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We follow the guidance to provide the code and data. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We add the details in the numerical experiment section. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide the confidence interval for the results, which shows the significant improvement. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The compute resources are mentioned in the numerical section. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: It is anonymous. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our method aims to solving challenges in the recent hype in large language model, for which hallucination is a big issue. Besides, our paper is also can be used in the fairness considerations. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper does not have this issue. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We cite all papers that are used in our numerical experiments. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]