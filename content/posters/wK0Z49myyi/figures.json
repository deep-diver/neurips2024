[{"figure_path": "wK0Z49myyi/figures/figures_1_1.jpg", "caption": "Figure 1: Our method, neural field optimization with camera ray matching (CRAYM), incorporates contextual information for per-ray processing and enforces color + geometric consistence between matched rays. Compared to SPARF [32] which utilizes dense pixel correspondences and the state-of-the-art, bundle-adjusting L2G-NeRF [5], both aimed at handling noisy camera poses, CRAYM produces superior results especially over fine details; see the zoom-ins on the right. Results are shown the Drums model from NeRF-Synthetic [26] on dense views.", "description": "This figure compares the results of three different methods for neural field optimization with noisy camera poses: SPARF, L2G-NeRF, and the proposed CRAYM method.  The results are shown for the \"Drums\" model from the NeRF-Synthetic dataset.  CRAYM is shown to produce superior results, particularly in the details. The zoom-in sections highlight the improved detail and color consistency of the CRAYM model compared to the others.", "section": "1 Introduction"}, {"figure_path": "wK0Z49myyi/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our CRAYM pipeline. After extracting keypoints (red dots) from input images and matching them using a pre-trained network, we train our CRAYM network to optimize a 3D feature volume V which encodes both geometric and photometric information about the target 3D object and can be queried by camera rays for both novel view synthesis (via the Texture Network) and 3D reconstruction (via the Geometry Network). The volume optimization is subject to photometric losses through rendering along camera rays passing through the keypoints (i.e., the key rays), which is enhanced (in the KRE) by integrating features from auxiliary rays, i.e., rays passing through nearby auxiliary points (yellow dots) in the images. Matched ray coherence (MRC) is imposed on matched key rays, in terms of color consistency, while potentially mismatched rays can be identified by comparing accumulated features along the key rays through V. On top of the standard photometric loss, we introduce two geometric losses, the epipolar loss and point-alignment loss, to explicitly optimize ray-to-ray coherency to maximize the reconstruction quality of the feature volume.", "description": "This figure illustrates the CRAYM pipeline, which optimizes a 3D feature volume using key and auxiliary rays extracted from input images.  The pipeline incorporates photometric and geometric losses to refine both the feature volume and camera poses.  The Key Rays Enrichment Module (KRE) improves robustness by incorporating contextual information from surrounding auxiliary rays. The Matched Rays Coherency Module (MRC) maintains coherence between matched rays, while identifying potential mismatches.  The overall goal is to enhance the quality of novel view synthesis and 3D reconstruction.", "section": "3 Method"}, {"figure_path": "wK0Z49myyi/figures/figures_5_1.jpg", "caption": "Figure 3: Illustrating of our geometric losses. The red lines in the left subfigure are epipolar lines. The epipolar loss constrains the relative transformations between cameras, so that the projection of a keypoint pk onto the image plane of the other camera should lie on the epipolar line e'x'. With the camera poses constrained by the epipolar loss, the point-alignment loss further constrains the depth of xk and xk, aiming to align pk and pk with P.", "description": "This figure illustrates the two geometric losses used in the CRAYM method: epipolar loss and point-alignment loss. The epipolar loss ensures that the projection of a keypoint in one image onto the epipolar line in another image is consistent with the camera poses.  The point-alignment loss further refines the accuracy by aligning the 3D points corresponding to matched keypoints, improving depth estimation and geometric consistency. Both losses work together to enhance the optimization process by enforcing geometric constraints and improving the accuracy of the camera poses and 3D scene reconstruction.", "section": "3.4 Loss Function"}, {"figure_path": "wK0Z49myyi/figures/figures_6_1.jpg", "caption": "Figure 4: Visualization of the initial and optimized camera poses for the LEGO scene in the NeRF-Synthetic dataset [26]. (Purple: ground-truth poses; blue: initial or optimized poses; red lines: translation errors.)", "description": "This figure visualizes the initial and optimized camera poses for the LEGO scene from the NeRF-Synthetic dataset.  Purple points represent the ground truth poses, while blue points show both the initial and optimized camera poses obtained through different methods (BARF, L2G-NeRF, and the authors' method). Red lines connect the initial and optimized poses, illustrating the translation errors between them. The visualization helps to understand the effectiveness of different methods in refining initial noisy camera pose estimates to better match the ground truth.", "section": "4 Results"}, {"figure_path": "wK0Z49myyi/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparison results of novel view synthesis and surfaces reconstruction on the synthetic objects.", "description": "The figure shows a qualitative comparison of novel view synthesis and surface reconstruction results on synthetic objects.  It compares the results of four different methods: BARF, SPARF, L2G-NeRF, and the authors' proposed method, CRAYM. For each method, the figure displays several novel views and their corresponding 3D surface reconstructions, allowing for a visual comparison of the quality and details achieved by each approach. The Ground Truth is also shown for reference.", "section": "4.2 Evaluation on Synthetic Objects"}, {"figure_path": "wK0Z49myyi/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison results of novel view synthesis and surfaces reconstruction on the synthetic objects.", "description": "This figure presents a qualitative comparison of novel view synthesis and surface reconstruction results between different methods (BARF, SPARF, L2G-NeRF, and CRAYM) on synthetic objects.  Each method is shown rendering various viewpoints of several objects. The visual comparison demonstrates CRAYM's ability to produce superior results, particularly regarding fine details and overall sharpness, in comparison to existing state-of-the-art methods.", "section": "4.2 Evaluation on Synthetic Objects"}, {"figure_path": "wK0Z49myyi/figures/figures_13_1.jpg", "caption": "Figure 1: Our method, neural field optimization with camera ray matching (CRAYM), incorporates contextual information for per-ray processing and enforces color + geometric consistence between matched rays. Compared to SPARF [32] which utilizes dense pixel correspondences and the state-of-the-art, bundle-adjusting L2G-NeRF [5], both aimed at handling noisy camera poses, CRAYM produces superior results especially over fine details; see the zoom-ins on the right. Results are shown the Drums model from NeRF-Synthetic [26] on dense views.", "description": "This figure compares the results of CRAYM against SPARF and L2G-NeRF on a drum model rendered from multiple views.  CRAYM's superior performance is highlighted by zoomed-in sections showing more detail and better handling of noisy camera poses.  The key advantage of CRAYM is its use of camera ray matching rather than pixel matching, allowing for the integration of both geometric and photometric information for improved accuracy.", "section": "1 Introduction"}, {"figure_path": "wK0Z49myyi/figures/figures_14_1.jpg", "caption": "Figure 8: Illustrating the Key Ray Enrichment Module. The yellow ray rk is the key ray and the gray rays {ra} are the neighboring auxiliary rays. The surface point (the blue point) captured by the associated keypoint (the red point) on the input image may not intersect with rk exactly due to the unreliable pose, while we can learn the contextual information of the surface point with the neighboring rays of point pk sampled on rk.", "description": "This figure illustrates the concept of Key Ray Enrichment Module (KRE). A key ray (yellow) and its neighboring auxiliary rays (gray) are shown.  The key ray originates from the camera's perspective and passes through a keypoint in the image. Due to potential inaccuracies in camera pose estimation, the intersection of the key ray with the 3D surface may not be perfectly accurate. To improve robustness, KRE incorporates features from the surrounding auxiliary rays, enriching the key ray's features and improving the stability and accuracy of the optimization process.", "section": "3.2 Key Rays Enrichment Module"}, {"figure_path": "wK0Z49myyi/figures/figures_14_2.jpg", "caption": "Figure 6: Qualitative results of novel view synthesis and surfaces reconstruction on real scenes captured by high-resolution cameras.", "description": "This figure showcases a qualitative comparison of novel view synthesis and 3D surface reconstruction results on real-world scenes.  The results are compared between ground truth images, results from NeuS, PET-NeuS, and the authors' CRAYM method. The comparison highlights the ability of CRAYM to produce superior results, especially in terms of detail and surface reconstruction, compared to existing methods.  The images show that CRAYM generates more accurate and detailed reconstructions with fewer artifacts.", "section": "4.3 Evaluation on Real Scenes"}, {"figure_path": "wK0Z49myyi/figures/figures_14_3.jpg", "caption": "Figure 6: Qualitative results of novel view synthesis and surfaces reconstruction on real scenes captured by high-resolution cameras.", "description": "This figure presents a qualitative comparison of novel view synthesis and surface reconstruction results on real-world scenes.  The results from three different methods (NeuS, PET-NeuS, and the authors' CRAYM method) are shown alongside the ground truth. Each column shows the ground truth image, a zoomed-in section, the 3D reconstruction from the respective method, and another zoomed-in section of the 3D reconstruction.  The comparison highlights the superior performance of the CRAYM method in capturing fine details and producing more accurate and realistic reconstructions compared to the other two methods.", "section": "4.3 Evaluation on Real Scenes"}, {"figure_path": "wK0Z49myyi/figures/figures_15_1.jpg", "caption": "Figure 5: Qualitative comparison results of novel view synthesis and surfaces reconstruction on the synthetic objects.", "description": "This figure shows a qualitative comparison of novel view synthesis and surface reconstruction results on synthetic objects.  It visually compares the results of four different methods: Ground Truth, BARF, SPARF, L2G-NeRF and CRAYM (the authors' method). For each object, the figure displays multiple views, allowing for a direct comparison of the quality and details of the generated images and 3D reconstructions.  The goal is to highlight the superior performance of the CRAYM method, particularly in terms of fine details and overall image quality.", "section": "4.2 Evaluation on Synthetic Objects"}]