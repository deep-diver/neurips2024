[{"type": "text", "text": "The Implicit Bias of Gradient Descent on Separable Multiclass Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hrithik Ravi1 Clayton Scott1 Daniel Soudry2 Yutong Wang3 ", "page_idx": 0}, {"type": "text", "text": "1University of Michigan 2Technion - Israel Institute of Technology 3Illinois Institute of Technology {hrithikr, clayscot}@umich.edu daniel.soudry@gmail.com ywang562@iit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Implicit bias describes the phenomenon where optimization-based training algorithms, without explicit regularization, show a preference for simple estimators even when more complex estimators have equal objective values. Multiple works have developed the theory of implicit bias for binary classification under the assumption that the loss satisfies an exponential tail property. However, there is a noticeable gap in analysis for multiclass classification, with only a handful of results which themselves are restricted to the cross-entropy loss. In this work, we employ the framework of Permutation Equivariant and Relative Margin-based (PERM) losses [Wang and Scott, 2024] to introduce a multiclass extension of the exponential tail property. This class of losses includes not only cross-entropy but also other losses. Using this framework, we extend the implicit bias result of Soudry et al. [2018] to multiclass classification. Furthermore, our proof techniques closely mirror those of the binary case, thus illustrating the power of the PERM framework for bridging the binary-multiclass gap. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Overparameterized models such as neural networks have shown state-of-the-art performance in many applications, despite having the potential to overfti. Zhang et al. [2021] demonstrate that this potential is indeed realizable by training real-world models to fit random noise. In recent years, there have been several research efforts that aim to understand the impressive performance of overparametrized models despite this ability to overfit. Both the model architecture and the training algorithms for selecting the weights have been investigated in this regard. ", "page_idx": 0}, {"type": "text", "text": "Work on implicit bias [Soudry et al., 2018, Ji et al., 2020, Vardi, 2022] has focused on the latter factor. Implicit bias is the hypothesis that gradient-based methods have a built-in preference for models with low-complexity. This hypothesis is perhaps best understood in the setting of (unregularized) empirical risk minimization for learning a linear model under the assumption of linearly separable data. Soudry et al. [2018] showed that in binary classification, implicit bias holds when the loss has the exponential tail property [Soudry et al., 2018, Theorem 3]. The same work also demonstrated implicit bias in the multiclass setting for the cross-entropy loss, but implicit bias for a more broadly defined class of losses in the multiclass case is left open. In this work, we extend the notion of the exponential tail property to multiclass losses and prove that the property is sufficient for implicit bias to occur in the multiclass setting. Toward this end, we employ the framework of permutation equivariant and relative margin-based (PERM) losses [Wang and Scott, 2024]. ", "page_idx": 0}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multiclass extension of the exponential tail property (Definition 2.2) It is unclear how the exponential tail property for binary margin losses should be extended to the multiclass setting. By using the PERM framework, we provide a multiclass extension that generalizes the exponential tail property to multiclass (Definition 2.2 in Section 2.3). We further verify that this property holds for some common losses. ", "page_idx": 1}, {"type": "text", "text": "Sufficiency of the exponential tail property for implicit bias (Theorem 3.4) We prove that the proposed multiclass exponential tail property is sufficient for implicit bias. More precisely, we show in Theorem 3.4 that for almost all linearly separable multiclass datasets, given a convex, ( $\\beta$ -smooth, strictly decreasing) PERM loss satisfying the exponential tail property in Definition 2.2, gradient descent exhibits directional convergence to the hard-margin multiclass SVM. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Soudry et al. [2018] show that gradient descent, applied to unregularized empirical risk minimization, converges to the hard-margin SVM solution at a slow logarithmic rate, provided the loss satisfies the exponential tail property (defined below). Nacson et al. [2019] improve the convergence rate using a specific step-size schedule. Ji and Telgarsky [2019] extend implicit bias to the setting of quasi-complete separation [Cand\u00e8s and Sur, 2020], where the two classes are linearly separated but with a margin of zero. Many works have also considered gradient-based methods beyond gradient descent. For example, Gunasekar et al. [2018] examine the implicit bias effects of mirror descent [Beck and Teboulle, 2003], steepest descent [Boyd and Vandenberghe, 2004], and adaptive gradient descent [Duchi et al., 2011, Kingma and Ba, 2015]. Cotter et al. [2012], Clarkson et al. [2012], Ji et al. [2021] study first order methods that are designed specifically to approach the hard-margin SVM as quickly as possible. ", "page_idx": 1}, {"type": "text", "text": "Results for the multiclass setting are more scarce, and are always specific to cross-entropy. Soudry et al. [2018] establish implicit bias for cross-entropy loss. Lyu and Li [2019] focus on homogeneous predictors and prove convergence of GD on cross-entropy loss to a KKT point of the marginmaximization problem. Lyu and Li [2019] proves convergence of gradient flow to a generalized maxmargin classifier for multiclass classification with cross-entropy loss using homogeneous models.1 In the special case when the model are linear classifiers, the generalized max-margin classifier reduces to the classical hard-margin SVM. Lyu et al. [2021] consider two-layer neural networks and prove convergence of GD on cross-entropy loss to the max-margin solution under an additional assumption on the data, that both $\\mathbf{x}$ and its negative counterpart $-\\mathbf{x}$ must belong to the dataset. Wang et al. [2023] prove that in certain overparameterized regimes, gradient descent on squared loss leads to an equivalent solution to gradient descent on cross-entropy loss. ", "page_idx": 1}, {"type": "text", "text": "Beyond work establishing (rate of) convergence to the max-margin classifier, there is also a separate line of work [Shamir, 2021, Schliserman and Koren, 2022, 2023] focusing on the generalization aspect of implicit bias. These works examine the binary classification setting, with the exception of Schliserman and Koren [2022] who consider cross-entropy. ", "page_idx": 1}, {"type": "text", "text": "1.3 Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $K\\ge2$ and $d\\geq1$ denote the number of classes and feature space dimension, respectively. Let $[K]\\,:=\\,\\{1,2,\\dots,K\\}$ . Vectors are denoted by boldface lowercase letters, e.g., $\\textbf{v}\\dot{\\in}\\ \\mathbb{R}^{K}$ whose entries are denoted by $v_{j}$ for $j\\in[K]$ . Likewise, matrices are denoted by boldface uppercase letters, e.g., $\\textbf{W}\\in\\mathbb{R}^{d\\times K}$ . The columns of $\\mathbf{W}$ are denoted $\\mathbf{w}_{1},\\dots,\\mathbf{w}_{K}$ . By ${\\bf0}_{n}$ and ${\\mathbf{1}}_{n}$ we denote the $n$ -dimensional vectors of all 0\u2019s and all 1\u2019s respectively. The $n\\times n$ identity matrix is denoted by ${\\mathbf{I}}_{n}$ . By $\\|\\mathbf{v}\\|$ we denote the Euclidean norm of vector $\\mathbf{v}$ . $||\\mathbf{A}||_{2}$ is the spectral norm of matrix A. Given two vectors $\\mathbf{w},\\mathbf{v}\\in\\mathbb{R}^{k}$ , we write $\\mathbf{w}\\succeq\\mathbf{v}$ (resp. $\\mathbf{w}\\succ\\mathbf{v},$ ) if $w_{j}\\geq v_{j}$ (resp. $w_{j}>v_{j})$ ) for all $j\\in[k]$ ; similarly we write $\\mathbf{w}\\preceq\\mathbf{v}$ (resp. w $\\prec\\mathbf{v}$ ) if $w_{j}\\leq v_{j}$ (resp. $w_{j}<v_{j}\\mathrm{~.~}$ ) for all $j\\in[k]$ . On the other hand, if $\\mathbf{A}$ and $\\mathbf{B}$ are equally-sized symmetric matrices, then by $\\mathbf A\\succeq\\mathbf B$ (resp. $\\mathbf{A}\\preceq\\mathbf{B})$ we mean that $\\mathbf{A}-\\mathbf{B}$ (resp. $\\mathbf{B}-\\mathbf{A})$ is positive semi-definite, i.e. $\\mathbf{A}-\\mathbf{B}\\succeq0$ (resp. $\\mathbf{B}-\\mathbf{A}\\succeq0,$ ). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "A bijection from $[k]$ to itself is called a permutation on $[k]$ . Denote by $\\mathtt{S y m}(k)$ the set of all permutations on $[k]$ . For each $\\sigma\\in\\mathtt{S y m}(k)$ , let $\\mathbf{S}_{\\sigma}$ denote the permutation matrix corresponding to $\\sigma$ . In other words, if v \u2208Rk is a vector, then [S\u03c3v]j = v\u03c3(j). ", "page_idx": 2}, {"type": "text", "text": "2 Multiclass Loss Functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In multiclass classification, a classifier is typically represented in terms of a class-score function $f=(f_{1},\\dots,f_{K}):\\mathbb{R}^{d}\\to\\mathbb{R}^{K}$ , which maps an input $\\dot{\\mathbf{x}}\\in\\mathbb{R}^{d}$ to a vector $\\mathbf{v}:=f(\\mathbf{x})$ of class scores. For instance, $f$ may be a feed-forward neural network and $\\mathbf{v}$ in this context is sometimes referred to as the logits. The label set is $[K]$ , and a label is predicted as ${\\mathrm{argmax}}_{j}f_{j}(\\mathbf{x})$ . A $K$ -ary multiclass loss function is a vector-valued function $\\mathcal{L}=(\\mathcal{L}_{1},\\ldots,\\mathcal{L}_{K}):\\mathbb{R}^{K}\\rightarrow\\overset{\\circ}{\\mathbb{R}}^{K}$ where $\\mathcal{L}_{y}(f(\\mathbf{x}))$ is the loss incurred for outputting $f(\\mathbf{x})$ when the ground truth label is $y$ . ", "page_idx": 2}, {"type": "text", "text": "In binary classification, a classifier is typically represented using a function $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ . The label set is $\\{-1,1\\}$ , and labels are predicted as $\\mathbf{x}\\stackrel{\\cdot}{\\mapsto}\\mathrm{sign}(g(\\mathbf{x}))$ . A binary margin loss is a function of the form $\\psi:\\mathbb{R}\\rightarrow\\mathbb{R}$ where $\\psi(y g(\\mathbf{x}))$ is the loss incurred for outputting $g\\mathbf{(x)}$ when the ground truth label is $y$ . Margin losses have been central to the development of the theory of binary classification, and the lack of a multiclass counterpart to binary margin losses may have impaired the development of corresponding theory for multiclass classification. To address this issue, Wang and Scott [2024] introduce PERM losses as a bridge between binary and multiclass classification. ", "page_idx": 2}, {"type": "text", "text": "2.1 Permutation equivariant and relative margin-based (PERM) losses ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume the label set is $[K]$ . Define 2 the matrix $\\mathbf{D}:=[-\\mathbf{I}_{K-1}\\quad\\mathbf{1}_{K-1}]\\in\\mathbb{R}^{(K-1)\\times K}$ . Observe that $\\mathbf{D}\\mathbf{v}=(v_{K}-v_{1},v_{K}-v_{2},\\ldots,v_{K}-v_{K-1})^{\\top}$ for all ${\\bf v}\\in\\mathbb{R}^{K}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (PERM loss [Wang and Scott, 2024]). Let $K\\geq2$ be an integer, and $\\mathcal{L}$ be a $K$ -ary multiclass loss function. We say that $\\mathcal{L}$ is ", "page_idx": 2}, {"type": "text", "text": "1. permutation equivariant $i f\\mathcal{L}(\\mathbf{S}_{\\sigma}\\mathbf{v})=\\mathbf{S}_{\\sigma}\\mathcal{L}(\\mathbf{v})$ for all ${\\bf v}\\in\\mathbb{R}^{K}$ and $\\sigma\\in\\operatorname{Sym}(K)$ , ", "page_idx": 2}, {"type": "text", "text": "2. relative margin-based if for each $y\\in[K]$ there exists a function $\\ell_{y}:\\mathbb{R}^{K-1}\\to\\mathbb{R}$ so that $\\mathcal{L}_{y}(\\mathbf{v})=\\ell_{y}(\\mathbf{Dv})=\\ell_{y}(v_{K}-v_{1},v_{K}-v_{2},\\ldots,v_{K}-v_{K-1})$ , for all ${\\bf v}\\in\\mathbb{R}^{K}$ . We refer to the vector-valued function $\\ell:=(\\ell_{1},\\dots,\\ell_{K})$ as the reduced form of $\\mathcal{L}$ . 3. PERM if $\\mathcal{L}$ is both permutation equivariant and relative margin-based. In this case, the function $\\psi:=\\ell_{\\cal K}$ is referred to as the template of $\\mathcal{L}$ . ", "page_idx": 2}, {"type": "text", "text": "Wang and Scott [2024] show that PERM losses are characterized by their template $\\psi$ . To show this, they introduce the matrix label code, an encoding of labels as matrices. Thus, for each $y\\in[K-1]$ , let $\\Upsilon_{y}$ be the $(K-1)\\times(K-1)$ identity matrix, but with the $y$ -th column replaced by all $-1\\,\\mathrm{`s}$ . For $\\bar{y}\\,=\\,K$ , let $\\Upsilon_{y}$ be the identity matrix. Note that when $K\\,=\\,2$ , this definition reduces to $\\mathbf{Y}_{y}=(-1)^{y}$ , the standard encoding of labels in the binary setting. Observe that (after permutation) $\\mathbf{Y}_{y}\\mathbf{D}\\mathbf{v}=(v_{y}-v_{1},v_{y}-v_{2},\\ldots,v_{y}-v_{K})^{\\top}\\in\\mathbb{R}^{K-1}$ , where the $v_{y}-v_{y}=0$ entry is omitted. Please see Wang and Scott [2024, Lemma B.2] for a simple proof. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 (Wang and Scott [2024]). Let $\\mathcal{L}:\\mathbb{R}^{K}\\rightarrow\\mathbb{R}^{K}$ be a PERM loss with template $\\psi$ , and let $\\boldsymbol{v}\\in\\mathbb{R}^{K}$ and $y\\in[K]$ be arbitrary. Then $\\psi$ is a symmetric function. Moreover, ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{y}\\left({\\bf v}\\right)=\\psi\\left({\\bf{\\hat{r}}}_{y}{\\bf{D}}{\\bf{v}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Conversely, let $\\psi\\,:\\mathbb{R}^{K-1}\\to\\mathbb{R}$ be a symmetric function. Define a multiclass loss function ${\\mathcal{L}}=$ $(\\mathcal{L}_{1},\\ldots,\\mathcal{L}_{k}):\\mathbb{R}^{K}\\rightarrow\\mathbb{R}^{K}$ according to Eqn. (1). Then $\\mathcal{L}$ is a PERM loss with template $\\psi$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 shows that a PERM loss is characterized by its template $\\psi$ . The right hand side of Eqn. (1) is referred to as the relative margin form of the loss, which extends binary margin losses to multiclass. As noted by Wang and Scott [2024], an advantage of the relative margin form is that it ", "page_idx": 2}, {"type": "image", "img_path": "JlWn80mTJi/tmp/2750fd0819bfc0f5d80290c4f4f4fcbf6a64ee23642cf1abda89359f18027673.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: An illustration of the exponential tail property for the cross entropy/multinomial logistic loss when $K=3$ . Panel a. Plot of $\\psi(\\mathbf{u})=\\log(1+\\exp(-u_{1})+\\exp(-u_{2}))$ , the template for the multinomial logistic loss. Note that the complement of the positive orthant in the domain $\\mathbf{\\bar{R}}^{2}$ is shown in gray. Panel b. and c. Plot of the upper bound (shown in black) and lower bounds (red) of \u2212\u2202\u2202u\u03c81 (blue) respectively. These bounds are from Appendix C.1.3 where $u_{\\pm}=0$ and $c=1$ . Note that the lower bound is valid in the positive orthant, i.e., the red surface is below the blue one there. ", "page_idx": 3}, {"type": "text", "text": "decouples the labels from the predicted scores, which facilitates analysis. Our results below support this understanding. ", "page_idx": 3}, {"type": "text", "text": "Many losses in the literature are PERM losses, including the cross-entropy loss whose template is $\\begin{array}{r}{\\dot{\\psi(\\mathbf{u})}\\,=\\,\\log(1+\\sum_{i=1}^{K-1}\\exp(-u_{i}))}\\end{array}$ , the multiclass exponential loss [Mukherjee and Schapire, 2013] whose template is $\\begin{array}{r}{\\psi(\\mathbf{u})=\\sum_{i=1}^{K-1}\\exp(-u_{i})}\\end{array}$ , and the PairLogLoss [Wang et al., 2022] whose template i $\\begin{array}{r}{\\mathrm{:=}\\,\\psi(\\mathbf{u})=\\sum_{i=1}^{K-1}\\log(1+\\exp(-u_{i}))}\\end{array}$ . See Wang and Scott [2024] for other examples. ", "page_idx": 3}, {"type": "text", "text": "2.2 Regularity assumptions on loss functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{L}$ be a PERM loss with differentiable template $\\psi$ . If ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial\\psi}{\\partial u_{i}}\\left(\\mathbf{u}\\right)<0,\\quad\\mathrm{~for~all~}i\\in\\{1,2\\ldots,K-1\\},\\mathbf{u}\\in\\mathbb{R}^{K-1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "i.e., the gradient of the template is entrywise strictly negative, then we say that the PERM loss $\\mathcal{L}$ is strictly decreasing. In this case, we write $\\nabla\\psi\\prec\\mathbf{0}$ , where 0 is the 0-vector. If the template is differentiable, then it is convex if: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi(\\mathbf{u}_{1})\\geq\\psi(\\mathbf{u}_{2})+\\nabla\\psi(\\mathbf{u}_{2})^{\\top}(\\mathbf{u}_{1}-\\mathbf{u}_{2}),\\quad{\\mathrm{~for~all~}}\\mathbf{u}_{1},\\mathbf{u}_{2}\\in\\mathbb{R}^{K-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If $\\psi$ is twice-differentiable, this is equivalent to saying that the Hessian is positive-semidefinite: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\succeq0\\quad\\mathrm{~for~all~}\\mathbf{u}\\in\\mathbb{R}^{K-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, the template is said to be $\\beta$ -smooth if its gradient is $\\beta$ -Lipschitz: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla\\psi\\left(\\mathbf{u}_{1}\\right)-\\nabla\\psi\\left(\\mathbf{u}_{2}\\right)\\|\\leq\\beta\\|\\mathbf{u}_{1}-\\mathbf{u}_{2}\\|,\\quad{\\mathrm{~for~all~}}\\mathbf{u}_{1},\\mathbf{u}_{2}\\in\\mathbb{R}^{K-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If $\\psi$ is twice-differentiable, this is equivalent to saying that the maximum eigenvalue of its Hessian is bounded by $\\beta$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\|_{2}\\leq\\beta\\quad\\mathrm{~for~all~}\\mathbf{u}\\in\\mathbb{R}^{K-1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lVert\\mathbf{A}\\rVert_{2}$ is the spectral norm of matrix A. ", "page_idx": 3}, {"type": "text", "text": "2.3 Multiclass analogue of exponential tail property ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the binary setting, the exponential tail property defined in prior work (Soudry et al. [2018], Nacson et al. [2019], Ji et al. [2020]) is assumed to hold for the negative derivative of the loss. Similarly, in the multiclass setting we are interested in bounding the negative gradient of the PERM loss template. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Multiclass exponential tail property). A multiclass PERM loss with template $\\psi$ : $\\mathbb{R}^{K-1}\\rightarrow\\mathbb{R}$ has the exponential tail (ET) property if there exist $u_{+},u_{-}\\in\\mathbb{R}$ and positive $c>0$ such ", "page_idx": 3}, {"type": "text", "text": "that for all $i\\in[K-1]$ the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\mathbf{u}\\ s.t.\\ \\displaystyle\\operatorname*{min}_{j\\in[K-1]}u_{j}>u_{+},\\ w e\\ h a v e\\ -\\ \\frac{\\partial\\psi}{\\partial u_{i}}\\left(\\mathbf{u}\\right)\\leq c\\exp(-u_{i}),\\ \\ \\ a n d}\\\\ &{\\forall\\mathbf{u}\\ s.t.\\ \\displaystyle\\operatorname*{min}_{j\\in[K-1]}u_{j}>u_{-},\\ w e\\ h a v e\\ -\\ \\frac{\\partial\\psi}{\\partial u_{i}}\\left(\\mathbf{u}\\right)\\geq c\\left(1-\\sum_{j\\in[K-1]}\\exp{(-u_{j})}\\right)\\exp(-u_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 2.2. We show in Appendix $C$ that cross-entropy $(C E)$ , multiclass exponential loss, and PairLogLoss all have this property. ", "page_idx": 4}, {"type": "text", "text": "3 Main Result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consider a dataset $\\{(\\mathbf{x}_{n},y_{n})\\}_{n=1}^{N}$ , with $\\mathbf{x}_{n}\\in\\mathbb{R}^{d}$ and class labels $y_{n}\\,\\in\\,[K]:=\\{1,\\ldots,K\\}$ . The class score function for class $k$ is $f_{k}(\\mathbf{x})=\\mathbf{w}_{k}^{T}\\mathbf{x}$ . Define $\\mathbf{X}\\in\\mathbb{R}^{d\\times N}$ to be the matrix whose nth column is ${\\bf x}_{n}$ . Define $\\mathbf{W}\\in\\mathbb{R}^{d\\times K}$ to be the matrix whose $k$ th column is $\\mathbf{w}_{k}$ . The learning objective is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(\\mathbf{W}\\right)=\\sum_{n=1}^{N}\\mathcal{L}_{y_{n}}\\left(\\mathbf{W}^{\\top}\\mathbf{x}_{n}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From Eqn. 1, if $\\mathcal{L}$ is a PERM loss, then ${\\mathcal{L}}_{y}\\left({\\bf v}\\right)=\\psi\\left({\\bf T}_{y}{\\bf D}{\\bf v}\\right)$ , and the learning objective becomes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{W})=\\sum_{i=1}^{N}\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Up to permuting the entries, $\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}$ is equal to the $(K-1)$ -dimensional vector of relativemargins $\\left[\\left(\\mathbf{w}_{y_{i}}-\\mathbf{w}_{1}\\right)^{\\top}\\mathbf{x}_{i}$ , $\\left(\\mathbf{w}_{y_{i}}-\\mathbf{w}_{2}\\right)^{\\top}\\mathbf{x}_{i}$ , . . . , $\\left(\\mathbf{w}_{y_{i}}-\\mathbf{w}_{K}\\right)^{\\top}\\mathbf{x}_{i}\\right]^{\\top}$ , where the 0-valued entry $(\\mathbf{w}_{y_{i}}-\\mathbf{w}_{y_{i}})^{\\top}\\mathbf{x}_{i}$ is omitted. This follows from Wang and Scott [2024, Lemma B.2]. ", "page_idx": 4}, {"type": "text", "text": "We are now ready to state our assumptions on the loss: ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1. The PERM loss\u2019s template $\\psi$ is convex, $\\beta$ -smooth, strictly decreasing and nonnegative. 3 4 ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. The PERM loss has exponential tail as defined in Definition 2.2. ", "page_idx": 4}, {"type": "text", "text": "To optimize Eqn. (2) we employ gradient descent with fixed learning rate $\\eta$ . Define $\\mathbf{w}:=\\mathsf{v e c}(\\mathbf{W})$ where vec denotes vectorization by column-stacking (See Definition B.1), and let the gradient descent iterate at time $t$ be w $(t)$ . Then: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}\\left(t+1\\right)=\\mathbf{w}\\left(t\\right)-\\eta\\nabla\\mathcal{R}\\left(\\mathbf{w}(t)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Define the \u201cmatrix-version\u201d of the trajectory $\\mathbf{W}(t)\\in\\mathbb{R}^{d\\times K}$ such that $\\mathbf{w}(t)=\\mathsf{v e c}(\\mathbf{W}(t))$ . Throughout this work, we frequently work with the risk as a matrix-input scalar-output function ${\\mathcal{R}}(\\mathbf{W})$ , and as a vector-input scalar-output function $\\mathcal{R}(\\mathbf{w})$ . ", "page_idx": 4}, {"type": "text", "text": "These two formulations will each be useful in different situations. For instances, adopting the matrix perspective can facilitate calculation of bounds, e.g., in Section 4.2. On the other hand, the vectorized formulation is easier for defining the Hessian of the risk $\\nabla^{2}\\mathcal{R}(\\mathbf{w})$ . See Appendix B for detail. ", "page_idx": 4}, {"type": "text", "text": "We focus on linearly separable datasets: ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3. The dataset is linearly separable, i.e. there exists w $\\in\\mathbb{R}^{d K}$ such that $\\forall n\\ \\in$ $[N],\\forall{\\bar{k}}\\in[K]\\backslash\\{y_{n}\\}:\\mathbf{w}_{y_{n}}^{\\top}\\mathbf{x}_{n}\\geq\\mathbf{w}_{k}^{\\top}{\\dot{\\mathbf{x}_{n}}}+{\\hat{1}}$ . Equivalently, there exists $\\mathbf{W}\\,\\in\\,\\mathbb{R}^{d\\times K}$ such that $\\forall n\\in[N]$ , $\\mathbf{\\Upsilon}\\Upsilon_{y_{n}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{n}\\succeq\\mathbf{1}$ . ", "page_idx": 4}, {"type": "text", "text": "Finally, let w\u02c6 be the multiclass hard-margin SVM solution for the linearly separable dataset: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{w}}=\\operatorname*{argmin}_{\\mathbf{w}}\\frac{1}{2}\\|\\mathbf{w}\\|^{2}\\mathrm{~s.t.~}\\forall n,\\forall k\\neq y_{n}:\\mathbf{w}_{y_{n}}^{\\top}\\mathbf{x}_{n}\\geq\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{n}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now we state the main result of the paper: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. For any PERM loss satisfying Assumptions 3.1 and 3.2, for all linearly separable datasets such that Assumption 4.1 holds, any sufficiently small learning rate $0<\\eta<2\\beta^{-1}\\bar{\\sigma}_{m a x}^{-2}\\left(\\right)$ X), and any initialization $\\mathbf{w}(0)$ , the iterates of gradient descent will behave as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}(t)=\\hat{\\mathbf{w}}\\log(t)+\\pmb{\\rho}(t)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the norm of the residual, $\\|\\rho(t)\\|$ , is bounded. This implies a directional convergence behavior: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{\\mathbf{w}\\left(t\\right)}{\\left\\Vert\\mathbf{w}\\left(t\\right)\\right\\Vert}=\\frac{\\hat{\\mathbf{w}}}{\\left\\Vert\\hat{\\mathbf{w}}\\right\\Vert}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix I, we show experimental results demonstrating implicit bias towards the hard margin SVM when using the PairLogLoss, in line with Theorem 3.4. ", "page_idx": 5}, {"type": "text", "text": "4 Proof Sketch ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we will overview the proof of the result. Along the way, we prove lemmas that extend to the multiclass setting results from Soudry et al. [2018]. The extensions are facilitated by the PERM framework, in particular the relative margin from of the loss. ", "page_idx": 5}, {"type": "text", "text": "We adopt the notation of Soudry et al. [2018] where possible throughout this proof. Recalling the notation and definitions from the paper: let us define the standard basis $\\mathbf{e}_{k}\\,\\in\\,\\mathbb{R}^{K}$ such that $(\\mathbf{e}_{k})_{i}=\\delta_{k i}$ (where $\\delta$ is the Kronecker-delta function), and the $d_{\\cdot}$ -dimension identity matrix $\\mathbf{I}_{d}$ . Define $\\mathbf{A}_{k}\\in\\mathbb{R}^{d K\\times d}$ as the Kronecker product between $\\mathbf{e}_{k}$ and $\\mathbf{I}_{d}$ , i.e. $\\mathbf{A}_{k}=\\mathbf{e}_{k}\\otimes\\mathbf{I}_{d}$ . We can then relate the original $k^{t h}$ -class predictor $\\mathbf{w}_{k}$ to the long column-vector w as follows: $\\mathbf{A}_{k}^{\\top}\\mathbf{w}=\\mathbf{w}_{k}$ . Next define $\\tilde{\\mathbf{x}}_{n,k}:=(\\mathbf{A}_{y_{n}}-\\mathbf{A}_{k})\\mathbf{x}_{n}$ . Using this notation, the multiclass SVM becomes ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{argmin}_{\\mathbf{w}}\\frac{1}{2}\\|\\mathbf{w}\\|^{2}~~~~\\mathrm{s.t.}~~~\\forall n,\\forall k\\neq y_{n}:\\mathbf{w}^{\\top}\\tilde{\\mathbf{x}}_{n,k}\\geq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For each $k\\in[K]$ , define $\\begin{array}{r}{S_{k}=\\arg\\operatorname*{min}_{n}(\\hat{\\mathbf{w}}_{y_{n}}-\\hat{\\mathbf{w}}_{k})^{\\top}\\mathbf{x}_{n}=\\{n:(\\hat{\\mathbf{w}}_{y_{n}}-\\hat{\\mathbf{w}}_{k})^{\\top}\\mathbf{x}_{n}=1\\}}\\end{array}$ , i.e., the $k^{t h}$ class support vectors. From the KKT optimality conditions for Eqn. (5), we have for some dual variables $\\alpha_{n,k}>0$ that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{w}}=\\sum_{n=1}^{N}\\sum_{k=1}^{K}\\alpha_{n,k}\\tilde{\\mathbf{x}}_{n,k}\\mathbb{1}_{n\\in S_{k}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{r}\\left(t\\right)=\\mathbf{w}\\left(t\\right)-\\log\\left(t\\right)\\hat{\\mathbf{w}}-\\tilde{\\mathbf{w}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\mathbf{w}}$ is a solution to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\forall n\\in\\mathcal{S}_{k}:\\,\\eta\\exp\\left(-\\mathbf{x}_{n}^{\\top}\\left(\\tilde{\\mathbf{w}}_{y_{n}}-\\tilde{\\mathbf{w}}_{k}\\right)\\right)=\\alpha_{n,k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Soudry et al. [2018], the existence of $\\tilde{\\mathbf{w}}$ is proven for the binary case for almost all datasets, and assumed in the multiclass case. Here, we also state the existence of $\\tilde{\\mathbf{w}}$ as an additional assumption: Assumption 4.1. Eqn. 8 has a solution, denoted w\u02dc. ", "page_idx": 5}, {"type": "text", "text": "We pose the problem of proving Assumption 4.1 for almost all datasets as a conjecture in Appendix H, where we also show experimentally that on a large number (100 instances for each choice of $d\\ \\in\\ \\{2,3,4,5,6\\}$ and $\\bar{K}\\;\\in\\;\\{3,4,5,6\\}\\rangle$ ) of synthetically generated linearly separable datasets, Assumption 4.1 indeed holds. ", "page_idx": 5}, {"type": "text", "text": "Note that $\\mathbf{r}(t)=\\pmb{\\rho}(t)-\\tilde{\\mathbf{w}}$ , and $\\tilde{\\bf w}$ is independent of $t$ , so bounding $\\mathbf{r}(t)$ is equivalent to bounding $\\rho(t)$ . Following the same steps as Soudry et al. [2018, Appendix E.3]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{r}\\left(t+1\\right)\\right\\|^{2}-\\left\\|\\mathbf{r}\\left(t\\right)\\right\\|^{2}=\\underbrace{\\left\\|\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right\\|^{2}}_{\\mathrm{First\\,Term}}+2\\underbrace{\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)}_{\\mathrm{Second\\,Term}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The high-level approach is to bound the two terms of the above expansion for $\\mathbf{r}(t)$ and then use a telescoping argument to bound $\\mathbf{r}(t)$ for all $t>0$ . Below we provide the main arguments; for a complete proof of the second term\u2019s bound, please refer to Appendix F. ", "page_idx": 5}, {"type": "text", "text": "4.1 Bounding the First Term ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Using $\\log(1+x)\\leq x$ for all $x>0$ , we expand the first term as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right\\|^{2}\\leq\\eta^{2}\\left\\|\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\right\\|^{2}+\\left\\|\\hat{\\mathbf{w}}\\right\\|^{2}t^{-2}+2\\eta\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\log\\left(1+t^{-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\eta^{2}\\left\\|\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\right\\|^{2}+\\left\\|\\hat{\\mathbf{w}}\\right\\|^{2}t^{-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Obtaining the second inequality requires proving that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2\\eta\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\log\\left(1+t^{-1}\\right)\\le0,\\mathrm{or}\\;\\mathrm{equivalently},\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We will spend the rest of this subsection going over the complete proof of this inequality. ", "page_idx": 6}, {"type": "text", "text": "First we state the following lemma (derived in Appendix B.2) that gives us a useful expression for the gradient of the risk w.r.t. W: ", "page_idx": 6}, {"type": "equation", "text": "$\\begin{array}{r}{\\nabla\\mathcal{R}(\\mathbf{W})=\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{T}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}.}\\end{array}$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This expression involves weight matrix W. However the inequality we set out to prove (Eqn. (10)) is in terms of $\\mathbf{w}=\\mathsf{v e c}(\\mathbf{W})$ . Throughout our main result proof, these two different forms \u2013 weight matrix versus vectorization of that matrix \u2013 will each be useful in different situations. Thus, to shuttle back and forth between these forms, the following well-known identity is useful: ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3. For equally sized matrices M and N, we have $\\mathsf{v e c}(\\mathbf{M})^{\\top}\\mathsf{v e c}(\\mathbf{N})=\\mathsf{t r}(\\mathbf{M}^{\\top}\\mathbf{N}).$ ", "page_idx": 6}, {"type": "text", "text": "Now we can prove our inequality of interest, i.e., Eqn. (10). ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4. (Multiclass generalization of Soudry et al. [2018, Lemma $I J$ ) For any PERM loss that is $\\beta$ -smooth, strictly decreasing, and non-negative, (Assumption 3.1) and Assumption 3.2, and for almost all linearly separable datasets (Assumption 3.3), we have w\u02c6 $\\tau^{\\top}\\nabla\\mathcal{R}(\\mathbf{w}(t))\\,\\bar{<}\\,0$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. Define matrix $\\hat{\\bf W}$ such that $\\hat{\\mathbf{w}}=\\mathsf{v e c}(\\hat{\\mathbf{W}})$ . Since $\\mathbf{w}(t)=\\mathsf{v e c}(\\mathbf{W}(t))$ , Lemma 4.3 implies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{w}(t))=\\sf{t r}(\\hat{\\mathbf{W}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{W}(t)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To see how the PERM framework allows for a simple generalization of binary results, we will compare our multiclass proof side-by-side with the binary proof discussed in Soudry et al. [2018, Lemma 1]. In the binary case, we have $\\begin{array}{r}{\\mathbf{\\mathcal{R}}\\left(\\mathbf{w}\\right)=\\sum_{i=1}^{N}\\psi\\left(y_{i}\\bar{\\mathbf{w}}^{\\top}\\mathbf{x}_{i}\\right)\\implies\\nabla\\mathcal{R}\\left(\\mathbf{w}\\right)=\\sum_{i=1}^{N}\\psi^{\\prime}\\left(y_{i}\\mathbf{w}^{\\top}\\mathbf{x}_{i}\\right)y_{i}\\mathbf{x}_{i}}\\end{array}$ . Thus $\\begin{array}{r}{\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}\\left(\\mathbf{w}\\right)=\\sum_{i=1}^{N}\\psi^{\\prime}\\left(y_{i}\\mathbf{w}^{\\top}\\mathbf{x}_{i}\\right)y_{i}\\hat{\\mathbf{w}}^{\\top}\\mathbf{x}_{i}}\\end{array}$ . In the multiclass case, the analogous quantity is $\\mathsf{t r}(\\hat{\\mathbf{W}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{W}(t)))$ which can be computed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}\\operatorname{tr}(\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D})=\\sum_{i=1}^{N}\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the multiclass proof we used the risk gradient from Lemma 4.2 as well as the cyclic property of the trace operator. Then we dropped the trace because $\\nabla\\psi\\left(\\mathbf{T}_{\\mathcal{Y}_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{\\mathcal{Y}_{i}}\\mathbf{D}\\bar{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}$ is a scalar (since $\\nabla\\psi\\left(\\cdot\\right)\\in\\mathbb{R}^{K-1}$ , $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y_{i}}\\mathbf{\\mathbf{D}}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\in\\mathbb{R}^{K-1})$ . For illustrative purpose, we place the rest of the proof, in both the binary and multiclass setting, side-by-side: ", "page_idx": 6}, {"type": "text", "text": "Binary: $\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{w}(t))$ . Focusing on just the $i$ -th term of this sum: ", "page_idx": 6}, {"type": "text", "text": "Multiclass: $\\mathsf{t r}(\\hat{\\mathbf{W}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{W}(t)))$ . Focusing on just the $i$ -th term of this sum: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi^{\\prime}\\left(y_{i}\\mathbf{w}(t)^{\\top}\\mathbf{x}_{i}\\right)y_{i}\\hat{\\mathbf{w}}^{\\top}\\mathbf{x}_{i}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla\\psi\\left(\\mathbf{T}_{\\mathcal{Y}_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{\\mathcal{Y}_{i}}\\mathbf{D}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\psi$ is assumed to be strictly decreasing, i.e. $\\psi^{\\prime}\\left(y_{i}\\mathbf{w}(t)^{\\top}\\mathbf{x}_{i}\\right)\\,<\\,0$ . The dataset is linearly separable, so $y_{i}\\hat{\\mathbf{w}}^{\\top}\\mathbf{x}_{i}\\geq1$ . Thus we obtain a sum (from $i=1$ to $N$ ) of negative terms. ", "page_idx": 6}, {"type": "text", "text": "$\\psi$ is assumed to be strictly decreasing, i.e. $\\nabla\\boldsymbol{\\psi}\\left(\\mathbf{\\mathcal{T}}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\ \\prec\\ \\mathbf{\\dot{0}}$ . The dataset is linearly separable, so $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\Upsilon_{y_{i}}\\mathbf{D}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\\;\\succeq\\;1$ . Thus we obtain a sum (from $i=1$ to $N$ ) of negative terms. ", "page_idx": 6}, {"type": "text", "text": "Thus we see how the PERM framework allows us to essentially mirror the binary proof. In Remark 4.5, we elaborate more on the necessity of the relative margin form here. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4 directly implies the auxiliary inequality we set out to prove (see Eqn. (10)). Thus we obtain: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right\\Vert^{2}\\leq\\eta^{2}\\left\\Vert\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\right\\Vert^{2}+\\left\\Vert\\hat{\\mathbf{w}}\\right\\Vert^{2}t^{-2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 4.5. Let us see what happens to our proof if we just used the general risk form in Eqn. (2) without the PERM framework. First, we need an expression for the gradient of the risk: $\\nabla\\mathcal{R}\\left(\\mathbf{W}\\right)=$ $\\begin{array}{r}{\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\mathcal{R}_{y_{i}}\\left(\\mathbf{W}^{\\top}\\mathbf{x_{i}}\\right)^{\\top}}\\end{array}$ . Proceeding similarly to the binary case, we focus on just the $i$ -th term of tr $\\left(\\hat{\\mathbf{W}}^{\\top}\\nabla\\mathcal{R}\\left(\\mathbf{W}\\right)\\right)$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\\nabla\\mathcal{R}_{y_{i}}\\left(\\mathbf{W}^{\\top}\\mathbf{x_{i}}\\right)^{\\top}\\right)=\\mathrm{tr}\\left(\\nabla\\mathcal{R}_{y_{i}}\\left(\\mathbf{W}^{\\top}\\mathbf{x_{i}}\\right)^{\\top}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\\right)=\\nabla\\mathcal{R}_{y_{i}}\\left(\\mathbf{W}^{\\top}\\mathbf{x_{i}}\\right)^{\\top}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From here it is not clear how to proceed. The linear separability condition (Assumption 3.3) is not useful anymore- it does not make a statement about the scores in the vector $\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}$ , but rather their relative margins (produced by the multiplication $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{x}_{i},$ ). ", "page_idx": 7}, {"type": "text", "text": "4.2 Bounding the Second Term ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the previous subsection we established a bound on the first term of Eqn. (9). Here we sketch the main arguments required to bound the second term, i.e. $\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)$ . For more details please refer to Appendix F. We state our final bound below as a lemma: ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.6. (Generalization of Soudry et al. [2018, Lemma 20]) Define $\\theta$ to be the minimum SVM margin across all datapoints and classes, i.e. $\\begin{array}{r}{\\theta=\\operatorname*{min}_{k}\\left[\\operatorname*{min}_{n\\notin S_{k}}\\tilde{\\mathbf{x}}_{n,k}^{\\top}\\hat{\\mathbf{w}}\\right]>1.}\\end{array}$ . Then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\exists C_{1},C_{2},t_{1}:\\forall t>t_{1}:\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)\\leq C_{1}t^{-\\theta}+C_{2}t^{-2}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "A remark is in order on the difference of the above result to Soudry et al. [2018, Lemma 20]: on a high-level, we are able to generalize the argument of Soudry et al. [2018, Lemma 20] to account for both binary and multiclass classification, as well as general PERM ET losses beyond just CE. ", "page_idx": 7}, {"type": "text", "text": "We now proceed with the proof sketch. The first step is to rewrite $\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(-\\eta\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)-\\hat{\\mathbf{w}}\\left[\\log\\left(t+1\\right)-\\log\\left(t\\right)\\right]\\right)^{\\top}\\mathbf{r}\\left(t\\right)\\qquad\\cdot\\mathrm{Definition~of~}\\mathbf{r}\\left(t\\right)\\mathrm{in~Equation~}(7)}\\\\ &{=\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\left(t^{-1}-\\log\\left(1+t^{-1}\\right)\\right)+\\mathrm{tr}\\left(\\left(-\\eta\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\right)^{\\top}\\mathbf{R}(t)\\right)}\\\\ &{\\qquad\\qquad-t^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\qquad\\quad\\cdot\\mathrm{Expression~for~}\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\mathrm{from~Lemma~}4.4}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We defer the bound on the first term $\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\left(t^{-1}-\\log\\left(1+t^{-1}\\right)\\right)$ of Equation (14) to the appendix, and instead focus on the second two terms. Using the cyclic property of the trace, the term in the above final line involving the trace can be further simplified as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that for each $i\\in[N]$ , a summand in Equation (15) is an inner product between two $(K-1)$ - dimensional vectors, i.e., $\\mathbf{\\bar{\\rho}}-\\nabla\\psi\\left(\\mathbf{\\mathcal{T}}_{\\mathcal{Y}_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)$ and $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}$ . To proceed, to expand this inner product out as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}\\mathbb{I}-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\mathbb{I}_{k}\\mathbb{I}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\\mathbb{I}_{k}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 4.7. Here, $\\underline{{\\mathbb{I}}}\\!\\!\\cdot\\!\\!\\!\\mathbb{I}_{k}\\ :\\ \\mathbb{R}^{K-1}\\ \\to\\ \\mathbb{R}$ is defined as the coordinate projection such that $\\[\\mathbf{\\mathcal{T}}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}]\\mathbf{\\Sigma}_{k}\\,=\\,\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{\\bar{w}}$ . Note that $[\\![\\cdot]\\!]_{k}$ implicitly depends on $i$ (the $\\tilde{\\mathbf{x}}_{i,y_{i}}$ $\\boldsymbol{O}$ -entry is omitted). But we abuse notation for brevity. Please see Appendix $D$ for a more precise definition. ", "page_idx": 7}, {"type": "text", "text": "Using Equation (6) and Equation (8) we express the last two terms in Equation (14) as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(\\displaystyle\\sum_{i=1}^{N}\\displaystyle\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}\\left\\|-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\right\\|_{k}\\left\\|\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\\right\\|_{k}\\Big)-t^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\displaystyle\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}\\left(\\left[-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\right\\|_{k}-t^{-1}\\exp(-\\tilde{\\mathbf{w}}^{\\top}\\tilde{\\mathbf{x}}_{i,k})\\mathbb{1}_{\\{i\\in S_{k}\\}}\\right)\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{r}(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Finally, to upper bound the above expression, we consider a single tuple $(i,k)$ case-by-case, depending on the sign of $\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{r}(t)$ . This is the step where the upper and lower bounds in Definition 2.2 come in. Lemma D.2 in the appendix essentially applies Definition 2.2 to the relative margins to yield ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[-\\nabla\\psi(\\mathbf{Y}_{y_{i}}\\mathbf{DW}(t)^{\\top}\\mathbf{x}_{i})]_{k}\\leq\\exp(-\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w}(t)),\\qquad\\mathrm{and}}\\\\ &{[-\\nabla\\psi(\\mathbf{Y}_{y_{i}}\\mathbf{DW}(t)^{\\top}\\mathbf{x}_{i})]_{k}\\geq(1-\\sum_{r\\in[K]\\backslash\\{y_{i}\\}}\\exp(-\\tilde{\\mathbf{x}}_{i,r}^{\\top}\\mathbf{w}(t)))\\exp(-\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w}(t))}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for all $k\\in[K]\\setminus\\{y_{i}\\}$ . We use Definition 2.2\u2019s exponential tail bounds by proving that the relative margins $\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w}(t)$ that appear in Lemma D.2 eventually become positive. This is true due to the following lemma (see Appendix E for the proof, which again mirrors the binary case): ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.8. (Multiclass generalization of Soudry et al. [2018, Lemma $I J$ ) Consider any linearly separable dataset, and any PERM loss with template $\\psi$ that is convex, $\\beta$ -smooth, strictly decreasing, and non-negative. For all $k\\in\\{1,...,K\\}$ , let $\\mathbf{w}_{k}(t)$ be the gradient descent iterates at iteration t for the $k^{t h}$ class. Then $\\forall i\\in\\{1,\\ldots,N\\},\\forall j\\in\\{1,\\ldots,K\\}\\backslash\\{y_{i}\\}:\\operatorname*{lim}_{t\\rightarrow\\infty}(\\mathbf{w}_{y_{i}}(t)-\\mathbf{w}_{j}(t))^{\\top}\\mathbf{x}_{i}\\rightarrow\\infty$ . ", "page_idx": 8}, {"type": "text", "text": "This lemma lets us use the exponential tail bounds with any finite $u_{\\pm}$ . To conclude, we apply the upper (18) and lower bounds (19) to the summation in Equation (17), and reduce the problem to that of Soudry et al. [2018, Appendix E], thereby proving Lemma 4.6. See our Appendix $\\boldsymbol{\\mathrm{F}}$ for details. ", "page_idx": 8}, {"type": "text", "text": "4.3 Tying It All Together ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We use the logic of Soudry et al. [2018, Appendix A.2] to conclude the analysis. Define ", "page_idx": 8}, {"type": "equation", "text": "$$\nC=\\sum_{t=0}^{\\infty}\\left\\|\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right\\|^{2}\\leq\\sum_{t=0}^{\\infty}\\eta^{2}\\left\\|\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\right\\|^{2}+\\left\\|\\hat{\\mathbf{w}}\\right\\|^{2}t^{-2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In the latter inequality we used Eqn. (12). Thus, $C$ is bounded because from Soudry et al. [2018, Lemma 10], we know that $\\begin{array}{r}{\\sum_{t=0}^{\\infty}\\left\\|\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\right\\|^{2}<\\infty}\\end{array}$ . Here we note that Soudry\u2032 et al. [2018, Lemma 10] requires the ER M objective $\\mathcal{R}\\left(\\mathbf{w}\\right)$ to be $\\beta^{\\prime}$ -smooth for some positive . It is easy to show that if the loss is $\\beta$ -smooth, then $\\mathcal{R}\\left(\\mathbf{w}\\right)$ is $\\beta\\sigma_{\\mathrm{max}}^{2}\\left(\\mathbf{X}\\right)$ -smooth. This explains the learning rate condition $\\eta<2/\\left(\\beta\\sigma_{\\mathrm{max}}^{2}\\left(\\mathbf{X}\\right)\\right)$ in our theorem. Also, a $t^{-p}$ power series converges for any $p>1$ . ", "page_idx": 8}, {"type": "text", "text": "Recalling the initial expansion of $\\|\\mathbf{r}(t+1)\\|$ from Eqn. (9): ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{r}\\left(t+1\\right)\\right\\|^{2}=\\left\\|\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right\\|^{2}+2\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)+\\left\\|\\mathbf{r}\\left(t\\right)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Combining the bounds in Eqn. (12) and Lemma 4.6 into Eqn. (9), we find ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{r}\\left(t\\right)\\right\\Vert^{2}-\\left\\Vert\\mathbf{r}\\left(t_{1}\\right)\\right\\Vert^{2}=\\sum_{u=t_{1}}^{t-1}\\left[\\left\\Vert\\mathbf{r}\\left(u+1\\right)\\right\\Vert^{2}-\\left\\Vert\\mathbf{r}\\left(u\\right)\\right\\Vert^{2}\\right]\\leq C+2\\sum_{u=t_{1}}^{t-1}\\left[C_{1}u^{-\\theta}+C_{2}u^{-2}\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Therefore, $\\left|\\left|\\mathbf{r}\\left(t\\right)\\right|\\right|$ is bounded, which proves our main theorem. ", "page_idx": 8}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here we describe some of our work\u2019s limitations/possible future research directions. We note that these questions have been analyzed for the binary classification setting, but not for multiclass. ", "page_idx": 8}, {"type": "text", "text": "Non-ET losses In our paper we only analyze multiclass implicit bias for losses with the ET property. Another possible line of future work is to analyze the gradient descent dynamics for non-ET losses. Nacson et al. [2019] and Ji et al. [2020] prove that in the binary setting, ET and well-behaved super-polynomial tailed losses ensure convergence to the maximum-margin direction, while other losses may converge to a different direction with poor margin. Is such a characterization possible in the multiclass setting? ", "page_idx": 9}, {"type": "text", "text": "Other gradient-based methods This paper only analyzes vanilla gradient descent. Another line of work involves exploring implicit bias effects of other gradient-based methods, such as those characterized in Gunasekar et al. [2018]. Nacson et al. [2022] uses similar proof techniques to prove results for SGD, which is prevalent in practice and often generalizes better than vanilla GD ([Amir et al., 2021]). ", "page_idx": 9}, {"type": "text", "text": "Non-asymptotic analysis Our result proves that the gradient descent predictors asymptotically do not overfti. However, in the binary classification case, Shamir [2021] goes one step further and proves that for gradient-based methods, throughout the entire training process (not just asymptotically), both the empirical risk and the generalization error decrease at an essentially optimal rate (or remain optimally constant). Does the same phenomenon occur in the multiclass setting? ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We use the permutation equivariant and relative margin-based (PERM) loss framework to provide an multiclass extension of the binary ET property. On a high level, while the binary ET bounds the negative derivative of the loss, our multiclass ET bounds each negative partial derivative of the PERM template $\\psi$ . We demonstrate our definition\u2019s validity for multinomial logistic loss, multiclass exponential loss, and PairLogLoss. We develop new techniques for analyzing multiclass gradient descent, and apply these to generalize binary implicit bias results to the multiclass setting. Our main result is that for almost all linearly separable multiclass datasets and a suitable ET PERM loss, the gradient descent iterates directionally converge towards the hard-margin multiclass SVM solution. ", "page_idx": 9}, {"type": "text", "text": "Our proof techniques in this paper demonstrate the power of the PERM framework to facilitate extensions of known binary results to multiclass settings and provide a unified treatment of both binary and multiclass classification. Thus it is possible that the binary results discussed in the Limitations section can also be extended using the PERM loss framework. In the future we would like to consider more complex settings that have been analyzed primarily for the binary case, such as non-separable data (Ji and Telgarsky [2019]) and two-layer neural nets (Lyu et al. [2021]). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "CS was supported in part by the National Science Foundation under award 2008074, and by the Department of Defense, Defense Threat Reduction Agency under award HDTRA1-20-2-0002. The research of DS was funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. YW was supported in part by the Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship, a Schmidt Futures program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Idan Amir, Tomer Koren, and Roi Livni. Sgd generalizes better than gd (and regularization doesn\u2019t help), 2021. ", "page_idx": 9}, {"type": "text", "text": "Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003. ", "page_idx": 9}, {"type": "text", "text": "Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004. ", "page_idx": 9}, {"type": "text", "text": "Emmanuel J. Cand\u00e8s and Pragya Sur. The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression. The Annals of Statistics, 48(1):27\u201342, 2020.   \nKenneth L Clarkson, Elad Hazan, and David P Woodruff. Sublinear optimization for machine learning. Journal of the ACM (JACM), 59(5):1\u201349, 2012.   \nAndrew Cotter, Shai Shalev-Shwartz, and Nathan Srebro. The kernelized stochastic batch perceptron. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 739\u2013746, 2012.   \nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.   \nSuriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832\u20131841. PMLR, 2018.   \nZiwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In Conference on Learning Theory, pages 1772\u20131798. PMLR, 2019.   \nZiwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In Conference on Learning Theory, pages 2109\u20132136. PMLR, 2020.   \nZiwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In International Conference on Machine Learning, pages 4860\u20134869. PMLR, 2021.   \nCG Khatri and C Radhakrishna Rao. Solutions to some functional equations and their applications to characterization of probability distributions. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages 167\u2013180, 1968.   \nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, volume 6, 2015.   \nShuangzhe Liu. Matrix results on the khatri-rao and tracy-singh products. Linear Algebra and its Applications, 289(1-3):267\u2013277, 1999.   \nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv preprint arXiv:1906.05890, 2019.   \nKaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. Advances in Neural Information Processing Systems, 34:12978\u201312991, 2021.   \nJan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics and econometrics. John Wiley & Sons, 2019.   \nIndraneel Mukherjee and Robert E Schapire. A theory of multiclass boosting. Journal of Machine Learning Research, 2013.   \nMor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420\u20133428. PMLR, 2019.   \nMor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate, 2022.   \nRobert R Phelps. Convex functions, monotone operators and differentiability, volume 1364. Springer, 2009.   \nMatan Schliserman and Tomer Koren. Stability vs implicit bias of gradient methods on separable data and beyond. In Conference on Learning Theory, pages 3380\u20133394. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "Matan Schliserman and Tomer Koren. Tight risk bounds for gradient descent on separable data. arXiv preprint arXiv:2303.01135, 2023. ", "page_idx": 11}, {"type": "text", "text": "Ohad Shamir. Gradient methods never overfit on separable data. The Journal of Machine Learning Research, 22(1):3847\u20133866, 2021. ", "page_idx": 11}, {"type": "text", "text": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1): 2822\u20132878, 2018. See arxiv.org/abs/1710.10345v7 for the most up-to-date version. ", "page_idx": 11}, {"type": "text", "text": "Gal Vardi. On the implicit bias in deep-learning algorithms. arXiv preprint arXiv:2208.12591, 2022. ", "page_idx": 11}, {"type": "text", "text": "Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overftiting in multiclass classification: All roads lead to interpolation, 2023. ", "page_idx": 11}, {"type": "text", "text": "Nan Wang, Zhen Qin, Le Yan, Honglei Zhuang, Xuanhui Wang, Michael Bendersky, and Marc Najork. Rank4class: A ranking formulation for multiclass classification, 2022. ", "page_idx": 11}, {"type": "text", "text": "Yutong Wang and Clayton Scott. Unified binary and multiclass margin-based classification. Accepted to Journal of Machine Learning Research, arXiv:2311.17778, 2024. ", "page_idx": 11}, {"type": "text", "text": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021. ", "page_idx": 11}, {"type": "text", "text": "A Discussion of Lyu and Li [2019] ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Lyu and Li [2019] allow the class score functions to be linear classifiers, e.g., $\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{i}$ , but also nonlinear, e.g., \u201ccubed\u201d linear classifier $(\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{i})^{3}$ . By shifting the cubing operation to the loss, we can view the implicit regularization result of Lyu and Li [2019] as a result for losses beyond the cross entropy. This resulting loss is rather exotic and we are not aware of it being used in the literature; it is interesting nevertheless. However, the optimization problem would become non-convex, so convergence would not necessarily be to a global minimum: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\frac{1}{2}\\|\\mathbf{w}\\|^{2}\\quad\\mathrm{s.t.}\\,\\left(\\mathbf{w}_{y_{i}}^{\\top}\\mathbf{x}_{i}\\right)^{3}-\\left(\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{i}\\right)^{3}\\geq1\\;\\mathrm{for}\\;\\mathrm{all}\\;i\\in[N],j\\in[K]\\backslash\\{y_{i}\\}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Moreover, the decision region for the $k$ -th class, i.e., the set of $\\mathbf{x}\\in\\mathbb{R}^{d}$ such that $(\\mathbf{w}_{k}^{\\top}\\mathbf{x})^{3}>(\\mathbf{w}_{j}^{\\top}\\mathbf{x})^{3}$ for all $j\\neq k$ , is an intersection of sets constructed via cubic hypersurfaces. ", "page_idx": 11}, {"type": "text", "text": "More precisely, the $k$ -th decision region can be written as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\{\\mathbf{x}\\in\\mathbb{R}^{d}:(\\mathbf{w}_{k}^{\\top}\\mathbf{x})^{3}=\\operatorname{argmax}_{j\\in[K]}(\\mathbf{w}_{j}^{\\top}\\mathbf{x})^{3}\\}=\\bigcap_{\\substack{j\\in[K]:j\\neq k}}\\{\\mathbf{x}\\in\\mathbb{R}^{d}:(\\mathbf{w}_{k}^{\\top}\\mathbf{x})^{3}>(\\mathbf{w}_{j}^{\\top}\\mathbf{x})^{3}\\}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Let us define $\\mathcal{H}_{j}:=\\{\\mathbf{x}\\in\\mathbb{R}^{d}:(\\mathbf{w}_{k}^{\\top}\\mathbf{x})^{3}=(\\mathbf{w}_{j}^{\\top}\\mathbf{x})^{3}\\}$ . Note that $\\mathcal{H}_{j}\\subseteq\\mathbb{R}^{d}$ is the zero set of degree 3 polynomials with variables in $\\mathbf{x}$ , hence, a cubic hypersurface. Now, the set $\\{\\mathbf{x}\\in\\mathbb{R}^{d}:(\\mathbf{w}_{k}^{\\top}\\mathbf{x})^{3}>$ $(\\mathbf{w}_{j}^{\\top}\\mathbf{x})^{3}\\}$ is a subset of the set-theoretic complement of $\\mathcal{H}_{j}$ in $\\mathbb{R}^{d}$ . Thus, the decision regions are complicated geometric objects, compared to the classical hard-margin SVM. ", "page_idx": 11}, {"type": "text", "text": "B Matrix Calculus ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "This section of the appendix establishes matrix identities that will be useful for us to calculate the gradient/Hessian of the empirical risk objective $\\mathcal{R}(\\mathbf{w})$ . ", "page_idx": 11}, {"type": "text", "text": "Vector-input scalar-output function. Suppose $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is a continuously differentiable function. Let $\\mathbf{x}=[\\mathsf{\\bar{x}}_{1},\\cdots\\,,x_{n}]^{\\top}\\,\\mathsf{\\bar{\\in}}\\,\\mathbb{R}^{n}$ be a vector of variables for differentiation. Define the (column) vector of partial derivatives w.r.t. x: \u2202x := \u2202\u2202xi i\u2208[n]. The gradient of $f$ , denoted $\\nabla f$ , is the function ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n},\\quad\\mathrm{where}\\quad\\nabla f(\\mathbf{x})=\\partial_{\\mathbf{x}}f(\\mathbf{x})=\\left[\\frac{\\partial f}{\\partial x_{1}}(\\mathbf{x})\\quad\\cdot\\cdot\\cdot\\quad\\frac{\\partial f}{\\partial x_{n}}(\\mathbf{x})\\right]^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Suppose that $f$ is twice continuously differentiable. The Hessian of $f$ , denoted $\\nabla^{2}f$ , is the function ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{2}f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n\\times n},\\quad\\mathrm{where}\\quad\\nabla^{2}f(\\mathbf{x})=\\left[\\frac{\\partial^{2}f}{\\partial x_{i}\\partial x_{j}}(\\mathbf{x})\\right]_{i,j\\in[n]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Matrix-input scalar-output function. Let $f\\,:\\,\\mathbb{R}^{m\\times n}\\,\\rightarrow\\,\\mathbb{R}$ be a differentiable function. Let $\\mathbf{X}=[x_{i j}]_{i\\in[m],j\\in[n]}^{-}\\in\\mathbb{R}^{\\bar{m}\\times n}$ be an arbitrary matrix. Define the matrix of partial derivatives w.r.t. $\\mathbf{X}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{\\mathbf{X}}:=\\left[\\frac{\\partial}{\\partial_{x}{}_{i j}}\\right]_{i\\in[m],j\\in[n]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Define the gradient of $f$ , denoted $\\nabla f$ , to be the function ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla f:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}^{m\\times n},\\quad\\mathrm{where}\\quad\\nabla(f)(\\mathbf{X}):=\\partial_{\\mathbf{X}}f(\\mathbf{X})=\\left[\\frac{\\partial}{\\partial_{x_{i j}}}f(\\mathbf{X})\\right]_{i\\in[m],j\\in[n]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We do not define the Hessian of a matrix-input scalar-output function $f:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}$ . Instead, we will define the Hessian for its vectorization v $\\mathfrak{z}(f):\\mathbb{R}^{m n}\\,\\overline{{\\to}}\\,\\mathbb{R}$ . ", "page_idx": 12}, {"type": "text", "text": "Definition B.1 (Vectorization operator). Let vec denote the vectorization operator by stacking the columns of a vector. In other words, $i f\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ is a matrix with columns $\\mathbf{a}_{1},\\dots,\\mathbf{a}_{n}\\in\\mathbb{R}^{m}$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(\\mathbf{A}):=\\left[\\mathbf{a}_{1}^{\\top}\\quad\\mathbf{a}_{2}^{\\top}\\quad\\cdot\\cdot\\quad\\mathbf{a}_{n}^{\\top}\\right]^{\\top}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Definition B.2 (Vectorization of a matrix-input function). Let $f:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}$ be a matrix-input function, we define vec $\\!\\cdot\\!(f):\\mathbb{R}^{m n}\\to\\mathbb{R}$ to be the vector-input function such that ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(\\mathbf{A})=\\mathsf{v e c}(f)(\\mathsf{v e c}(\\mathbf{A})).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In particular, if $f$ is already a vector-input function, then vec $(f)=f$ . ", "page_idx": 12}, {"type": "text", "text": "See [Magnus and Neudecker, 2019, Ch.5- $\\S15$ ]. Below in Lemma B.4, we give a convenient formula to calculate the Hessian of $\\mathcal{R}(\\mathbf{w})$ , the vectorization of ${\\mathcal{R}}(\\mathbf{W})$ . ", "page_idx": 12}, {"type": "text", "text": "The following relates the vectorization operator with the Kronecker product: ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. Let $\\mathbf{A}\\in\\mathbb{R}^{p\\times n}$ , $\\mathbf{B}\\in\\mathbb{R}^{n\\times m}$ , $\\mathbf{C}\\in\\mathbb{R}^{m\\times q}$ be matrices. Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(\\mathbf{ABC})=(\\mathbf{C}^{\\top}\\otimes\\mathbf{A})\\mathsf{v e c}(\\mathbf{B}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. This is [Magnus and Neudecker, 2019, Theorem 2.2]. ", "page_idx": 12}, {"type": "text", "text": "B.1 Special case of the chain rule for linear functions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proposition B.2. Let $\\mathbf{M}\\in\\mathbb{R}^{m\\times n}$ be a matrix. Let $f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ be a continuously differentiable function and define $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ by $g(\\mathbf{x}):=f(\\mathbf{M}\\mathbf{x})$ . Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla g(\\mathbf{x})=\\mathbf{M}^{\\top}\\nabla f(\\mathbf{M}\\mathbf{x}),\\quad a n d\\quad\\nabla^{2}g(\\mathbf{x})=\\mathbf{M}^{\\top}\\nabla^{2}f(\\mathbf{M}\\mathbf{x})\\mathbf{M}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. See [Magnus and Neudecker, 2019, Ch.9- $\\S13]$ for the first identity and [Magnus and Neudecker, 2019, Ch.10-\u00a78] for the second identity. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "The next two results will be referred to as the \u201cgradient formula\u201d and the \u201cHessian formula\u201d, respectively, for the function $g(\\mathbf{X}):=f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B})$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma B.3. Let $f:\\mathbb{R}^{p\\times q}\\rightarrow\\mathbb{R}$ be a matrix-input scalar-output differentiable function with Jacobian denoted $\\nabla f:\\mathbb{R}^{p\\times q}\\rightarrow\\mathbb{R}^{p\\times q}$ . Let $\\mathbf{A}\\in\\mathbb{R}^{p\\times n}$ , $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{m\\times n}$ , and $\\mathbf{B}\\in\\mathbb{R}^{m\\times q}$ . Define a function $g:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}$ by $g(\\mathbf{X}):=f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B})$ . Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla g(\\mathbf{X})=\\partial_{\\mathbf{X}}f(\\mathbf{AX^{\\top}B})=\\mathbf{B}\\nabla f(\\mathbf{AX^{\\top}B})^{\\top}\\mathbf{A}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma B.4. Let $f:\\mathbb{R}^{p}\\,\\rightarrow\\,\\mathbb{R}$ be a vector-input scalar-output twice differentiable function. Let $\\mathbf{A}\\in\\mathbb{R}^{p\\times n}$ , $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ be matrices and $\\mathbf{b}\\in\\mathbb{R}^{m}$ be a (column) vector. Let $\\mathbf{V}$ be another matrix with the same shape as $\\mathbf{X}$ . Le $t\\textbf{x}:=\\,\\mathsf{v e c}(\\mathbf{X})$ and $\\mathbf{v}\\,:=\\,\\mathsf{v e c}(\\mathbf{V})$ . Define $g(\\mathbf{X})\\ :=\\ f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{b})$ and let ${\\overline{{g}}}={\\mathsf{v e c}}(g)$ be the vectorization of $g$ . Then we have the following formula for computing $\\mathbf{v}^{\\top}\\nabla^{2}\\overline{{g}}(\\mathbf{x})\\mathbf{v}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\nabla^{2}\\overline{{g}}(\\mathbf{x})\\mathbf{v}=(\\mathbf{A}\\mathbf{V}^{\\top}\\mathbf{b})^{\\top}\\nabla^{2}f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{b})\\mathbf{A}\\mathbf{V}^{\\top}\\mathbf{b}^{\\top}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B.2 Proof of the gradient formula: Lemma 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the notation of Section 2.8.1 of the Matrix Cookbook, define matrix $\\mathbf{U}\\in\\mathbb{R}^{p\\times q}$ by $\\mathbf{U}:=\\mathbf{AX^{\\top}B}$ . Note that $\\mathbf{U}$ is a function of $\\mathbf{X}$ . Then by Eqn. (137) of the Matrix Cookbook, we have for each $(i,j)\\in[m]\\times[n]$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\frac{\\partial}{\\partial X_{i j}}}f(\\mathbf{AX^{\\top}B})={\\frac{\\partial}{\\partial X_{i j}}}f(\\mathbf{U})=\\operatorname{Tr}\\left[\\left({\\frac{\\partial f(\\mathbf{U})}{\\partial\\mathbf{U}}}\\right)^{\\top}{\\frac{\\partial\\mathbf{U}}{\\partial X_{i j}}}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that by definition, we have $\\begin{array}{r}{\\frac{\\partial f(\\mathbf{U})}{\\partial\\mathbf{U}}=\\nabla f(\\mathbf{U})}\\end{array}$ . Therefore ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial X_{i j}}f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B})=\\mathrm{Tr}\\left[\\nabla f(\\mathbf{U})^{\\top}\\frac{\\partial\\mathbf{U}}{\\partial X_{i j}}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, write $\\mathbf{U}=[U_{k\\ell}]_{k\\in[p],\\ell\\in[q]}$ in the \u201cmatrix-comprehension\u201d notation. Recall that $U_{k\\ell}$ , i.e., the $(k,\\ell)$ -th entry of $\\mathbf{U}$ , is precisely computed by $\\mathbf{A}[k,:](\\mathbf{X}^{\\top}\\mathbf{B})[:,\\ell]\\,=\\,\\mathbf{A}[k,:]\\mathbf{X}^{\\top}\\mathbf{B}[:,\\ell]$ . For each $\\dot{k},\\ell\\in[p]\\times[q]$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial U_{k\\ell}}{\\partial X_{i j}}=\\frac{\\partial(\\mathbf{A}[k,:]\\mathbf{X}^{\\top}\\mathbf{B}[:,\\ell])}{\\partial X_{i j}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where \u201c $\\cdot[k,:]^{\\,,}$ and \u201c $[:,\\ell]$ \u201d denote taking the $k$ -th row vector and $\\ell$ -th column vector, respectively. Now, by Eqn. (71) of the Matrix Cookbook, we have the following expression of the matrix-partial derivative as an outer product ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial U_{k\\ell}}{\\partial\\mathbf{X}}=\\frac{\\partial\\mathbf{A}[k,:]\\mathbf{X}^{\\top}\\mathbf{B}[:,\\ell]}{\\partial\\mathbf{X}}=\\mathbf{B}[:,\\ell]\\mathbf{A}[k,:].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From this, it follows that computing the entry-wise partial derivative at $X_{i j}$ is simply obtained by indexing at (i, j), i.e., \u2202\u2202UXkij\u2113 = B[i, \u2113]A[k, j] = A[k, j]B[i, \u2113] (we emphasize that this is just a product of two scalars). Thus,\u2202\u2202XUij = A[:, j]B[i, :]. Consequently, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial X_{i j}}f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B})=\\mathrm{Tr}\\left[\\boldsymbol{\\nabla}f(\\mathbf{U})^{\\top}\\mathbf{A}[:,j]\\mathbf{B}[i,:]\\right]=\\mathbf{B}[i,:]\\boldsymbol{\\nabla}f(\\mathbf{U})^{\\top}\\mathbf{A}[:,j].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In other words, $\\begin{array}{r}{\\frac{\\partial}{\\partial\\mathbf{X}}f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B})=\\mathbf{B}\\nabla f(\\mathbf{U})^{\\top}\\mathbf{A}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "For our purposes, we replace $f$ with $\\psi$ , A with $\\mathbf{\\Delta}\\mathbf{Y}_{y_{i}}\\mathbf{D},\\mathbf{X}$ with $\\mathbf{W}$ , and $\\mathbf{B}$ with $\\mathbf{x}_{i}$ . Thus we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{R}\\left(\\mathbf{W}\\right)=\\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\mathbf{W}}\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}\\right)=\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{\\Phi}^{\\top}\\mathbf{x}_{i}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as desired. ", "page_idx": 13}, {"type": "text", "text": "B.3 Proof of Hessian formula: Lemma B.4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our goal is to calculate the Hessian of ${\\mathsf{v e c}}(g)$ . First, we note that by definition ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(g)(\\mathbf{x})=\\mathsf{v e c}(g)(\\mathsf{v e c}(\\mathbf{X}))=\\mathsf{v e c}(f)(\\mathsf{v e c}(\\mathbf{AX^{\\top}b}))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the last equality is simply $\\mathsf{v e c}(f)(\\mathsf{v e c}(\\mathbf{AX^{\\top}b}))=f(\\mathbf{AX^{\\top}b})$ , but we work in the more general case of a matrix $\\mathbf{B}$ right now. We will need to simplify ve $\\mathbf{\\cdot}\\mathbf{c}(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{b})$ . It is more convenient during the first phase of the proof viewing $\\mathbf{b}$ as a $m\\times1$ matrix and denote it using uppercase letter B. First, applying Lemma B.1 to vec $(\\mathbf{A}\\bar{\\mathbf{X}}^{\\top}\\mathbf{B})$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(\\mathbf{AX}^{\\top}\\mathbf{B})=(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathsf{v e c}(\\mathbf{X}^{\\top})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "However, $\\mathsf{v e c}(\\mathbf{X}^{\\top})\\neq\\mathsf{v e c}(\\mathbf{X})$ in general. However, these two expressions are related using the commutation matrix: ", "page_idx": 13}, {"type": "text", "text": "Definition B.3 (Commutation matrix). Define $\\mathbf{K}_{m,n}$ to be the permutation matrix in $\\mathbb{R}^{m n\\times m n}$ such that ${\\mathbf{K}}_{m,n}{\\mathsf{v e c}}({\\mathbf{A}})={\\mathsf{v e c}}({\\mathbf{A}}^{\\top})$ for all matrices $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ . ", "page_idx": 13}, {"type": "text", "text": "See [Magnus and Neudecker, 2019, Ch.3-\u00a77]. Below, we drop the subscripts in Definition B.3 and simply write $\\mathbf{K}:=\\mathbf{K}_{m,n}$ . Now, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(\\mathbf{AX}^{\\top}\\mathbf{B})=(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathsf{v e c}(\\mathbf{X})=(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(g)(\\mathbf{x})=\\mathsf{v e c}(g)(\\mathsf{v e c}(\\mathbf{X}))=\\mathsf{v e c}(f)((\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Proposition B.2, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}\\mathsf{v e c}(g)(\\mathbf{x})}\\\\ &{\\mathbf{\\Omega}=((\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top})^{\\top}\\nabla^{2}\\mathsf{v e c}(f)((\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathbf{x})(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}}\\\\ &{\\mathbf{\\Omega}=((\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top})^{\\top}\\nabla^{2}\\mathsf{v e c}(f)(\\mathsf{v e c}(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B}))(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From this, we see that (recall that $\\mathbf{v}={\\mathsf{v e c}}(\\mathbf{V})!$ ) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\nabla^{2}\\mathsf{v e c}(g)(\\mathbf{x})\\mathbf{v}=((\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathbf{v})^{\\top}\\nabla^{2}\\mathsf{v e c}(f)(\\mathsf{v e c}(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{B}))(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathbf{v}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, by Lemma B.1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathbf{v}=(\\mathbf{B}^{\\top}\\otimes\\mathbf{A})\\mathbf{K}^{\\top}\\mathsf{v e c}(\\mathbf{V})=\\mathsf{v e c}(\\mathbf{A}\\mathbf{V}^{\\top}\\mathbf{B}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, since $\\mathbf{B}=\\mathbf{b}$ is just a vector, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{v e c}(\\mathbf{AV}^{\\top}\\mathbf{B})=\\mathbf{AV}^{\\top}\\mathbf{b}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla^{2}\\mathsf{v e c}(f)(\\mathsf{v e c}(\\mathbf{AX}^{\\top}\\mathbf{B}))=\\nabla^{2}f(\\mathbf{AX}^{\\top}\\mathbf{b})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting it all together, we get the desired equality. ", "page_idx": 14}, {"type": "text", "text": "C PERM Losses That Satisfy Assumptions 3.1 and 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Cross-Entropy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "By Wang and Scott [2024, Example 1], the cross-entropy loss $\\begin{array}{r}{\\mathcal{L}_{y}\\left(\\mathbf{v}\\right)=-\\log\\left(\\frac{\\exp\\left(v_{y}\\right)}{\\sum_{k=1}^{K}\\exp\\left(v_{k}\\right)}\\right)}\\end{array}$ has template $\\begin{array}{r}{\\psi\\left(\\mathbf{u}\\right)=\\log\\left(1+\\sum_{k=1}^{K-1}\\exp\\left(-u_{k}\\right)\\right)}\\end{array}$ . We calculate the partial derivatives: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial\\psi}{\\partial u_{i}}\\left(\\mathbf{u}\\right)}}\\\\ &{=-\\frac{\\exp\\left(-u_{i}\\right)}{1+\\sum_{k=1}^{K-1}\\exp\\left(-u_{k}\\right)}}\\\\ &{=-\\frac{1}{1+\\left(C_{i}+1\\right)\\exp\\left(u_{i}\\right)}\\,}&{\\mathrm{where~}C_{i}=\\sum_{k\\in\\left[K-1\\right]:k\\neq i}\\exp\\left(-u_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C.1.1 Convexity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let us analyze the entries of the Hessian of the template, i.e. $\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)$ . Let $[\\mathbf{A}]_{l,m}$ denote the element of $\\mathbf{A}$ at the $l$ -th row and $m$ -th column. We get for all $i,j\\in[K-1]$ where $j\\neq i$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\left[{{\\nabla^{2}}\\psi\\left({\\bf u}\\right)}\\right]_{i,i}=\\frac{{\\partial^{2}}\\psi\\left({\\bf u}\\right)}{{\\partial u_{i}^{2}}}=\\frac{{\\left({{C_{i}}+1}\\right){e^{-{u_{i}}}}}}{{\\left({1+\\sum_{k=1}^{K-1}{{e^{-{u_{k}}}}}}\\right)^{2}}}\\ \\ \\ \\ }\\\\ {{\\left[{{\\nabla^{2}}\\psi\\left({\\bf u}\\right)}\\right]_{i,j}={\\left[{{\\nabla^{2}}\\psi\\left({\\bf u}\\right)}\\right]_{j,i}=\\frac{{\\partial^{2}}\\psi\\left({\\bf u}\\right)}{{\\partial{u_{i}}{u_{j}}}}=\\frac{{-{e^{-{u_{i}}-{u_{j}}}}}}{{\\left({1+\\sum_{k=1}^{K-1}{{e^{-{u_{k}}}}}}\\right)^{2}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the definition of $C_{i}$ , this implies that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\right]_{i,i}=\\sum_{j\\in[K-1],j\\neq i}\\left|\\left[\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\right]_{i,j}\\right|+\\frac{e^{-u_{i}}}{\\left(1+\\sum_{k=1}^{K-1}e^{-u_{k}}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the Hessian is a symmetric diagonally dominant matrix, and hence is positive semi-definite. ", "page_idx": 14}, {"type": "text", "text": "C.1.2 $\\beta$ -smoothness ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For any diagonally dominant matrix $\\mathbf{B}$ , let $|\\mathbf{B}|$ be the matrix obtained by taking the absolute value of each element of $\\mathbf{B}$ , that is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\left\\vert\\mathbf{B}\\right\\vert\\right]_{l,m}=\\left\\vert\\left[\\mathbf{B}\\right]_{l,m}\\right\\vert\\quad\\mathrm{for}\\;\\mathrm{all}\\;l,m.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Additionally, let $\\mathsf{d i a g}(\\cdot):\\mathbb{R}^{p}\\,\\to\\,\\mathbb{R}^{p\\times p}$ (for any $p\\,\\in\\,\\mathbb{N},$ ) be the function that maps a vector to a diagonal matrix in the obvious way. ", "page_idx": 15}, {"type": "text", "text": "Then we have the following lemma: ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1. Let $\\mathbf{B}^{\\prime}:=\\mathsf{d i a g}\\left(\\left|\\mathbf{B}\\right|\\mathbf{1}\\right)$ where 1 is the appropriately-sized vector of all-1\u2019s. Then $\\mathbf{B}\\preceq\\mathbf{B^{\\prime}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. This can be proven simply by observing that $\\mathbf{B}^{\\prime}-\\mathbf{B}$ is symmetric and diagonally dominant (and thus positive semi-definite). \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1 can be directly applied to analyze the Hessian (and eventually bound its maximum eigenvalue). Define $\\mathbf{H}^{\\prime}=$ diag $\\left(\\left|\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\right|\\mathbf{1}\\right)$ . In other words, from Eqn. (25): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\mathbf{H}^{\\prime}\\right]_{i,i}=\\displaystyle\\sum_{k=1}^{K-1}\\left\\vert\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\right\\vert_{i,k}=\\frac{\\left(2C_{i}+1\\right)e^{-u_{i}}}{\\left(1+\\displaystyle\\sum_{k=1}^{K-1}e^{-u_{k}}\\right)^{2}}}\\\\ &{\\left[\\mathbf{H}^{\\prime}\\right]_{i,j}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, by directly applying Lemma C.1, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\preceq\\mathbf{H}^{\\prime}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So now since $\\mathbf{H}^{\\prime}$ is defined to be a diagonal matrix, all that\u2019s left to do is bound the diagonal entries by a positive constant. First note that from the definition of $C_{i}$ , it follows that $\\begin{array}{r l}{{1}+\\sum_{k=1}^{\\bar{K}-1}e^{-u_{k}}=}&{{}}\\end{array}$ $C_{i}+1+e^{-u_{i}}$ . Combining this with Eqn. (26), we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\left(2C_{i}+1\\right)e^{-u_{i}}}{\\left(\\left(C_{i}+1\\right)+e^{-u_{i}}\\right)^{2}}=\\frac{\\left(2C_{i}+1\\right)}{\\left(\\left(C_{i}+1\\right)+e^{-u_{i}}\\right)\\left(\\left(C_{i}+1\\right)e^{u_{i}}+1\\right)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can find a global minimum of the denominator of the above expression and thus arrive at an upper bound for the expression. Differentiating with respect to $u_{i}$ and setting to 0 yields a single critical point at $u_{i}=-\\mathrm{\\bar{log}}\\left(C_{i}+1\\right)$ , which produces a value of $4\\left(C_{i}+1\\right)$ when substituted in the denominator (this is a global minimum of the denominator expression). Thus, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\mathbf{H}^{\\prime}\\right]_{i,i}\\leq\\frac{\\left(2C_{i}+1\\right)}{4\\left(C_{i}+1\\right)}=\\frac{1}{2}-\\frac{1}{4\\left(C_{i}+1\\right)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the binary i.e. $K=2$ case, $C_{i}=0$ , so our bound is exactly $1/4$ . However, in the multiclass case (i.e. $K>2$ ), $C_{i}$ can be arbitrarily large. Setting $C_{i}=\\infty$ yields a final upper bound of $1/2$ . ", "page_idx": 15}, {"type": "text", "text": "So our final bound can be summarized as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}\\psi\\left(\\mathbf{u}\\right)\\|_{2}\\leq\\left[\\mathbf{H}^{\\prime}\\right]_{i,i}\\leq\\left\\{\\begin{array}{l l}{1/4}&{,\\,\\mathrm{if}\\,K=2}\\\\ {1/2}&{,\\,\\mathrm{if}\\,K>2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, $\\beta=1/4$ for binary cross-entropy (logistic loss), but $\\beta=1/2$ for $\\mathbf{K}$ -class cross-entropy. ", "page_idx": 15}, {"type": "text", "text": "C.1.3 Exponential Tail ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We claim that for the cross-entropy Definition 2.2 holds with $u_{\\pm}\\,=\\,0$ and $c\\,=\\,a\\,=\\,1$ . We are interested in analyzing the (negative) gradient of the template. From Eqn. 24: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\,\\frac{\\partial\\psi}{\\partial u_{i}}\\,({\\mathbf{u}})=\\frac{e^{-u_{i}}}{1+\\sum_{k=1}^{K-1}e^{-u_{k}}}}\\\\ {\\displaystyle\\leq e^{-u_{i}}}\\\\ {\\displaystyle\\geq e^{-u_{i}}\\left(1-\\sum_{k=1}^{K-1}e^{-u_{k}}\\right)\\quad\\cdot\\cdot\\forall x\\geq0,\\frac{1}{1+x}\\geq1-x}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This proves that the cross-entropy loss satisfies Definition 2.2 with $u_{\\pm}=0$ and $c=1$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Multiclass Exponential Loss [Mukherjee and Schapire, 2013] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The multiclass exponential loss $\\mathcal{L}:\\mathbb{R}^{K}\\rightarrow\\mathbb{R}$ can be written as $\\begin{array}{r}{\\mathcal{L}_{y}(\\mathbf{v})=\\sum_{k\\in[K]:k\\neq y}\\exp(-(v_{y}-}\\end{array}$ ${v_{k}})$ ). Thus, the template function $\\psi:\\mathbb{R}^{K-1}\\rightarrow\\mathbb{R}$ can be expressed as $\\begin{array}{r}{\\psi(\\mathbf{u})=\\sum_{i\\in[K-1]}e^{-u_{i}}}\\end{array}$ . The partial derivatives of the template are then simply: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial u_{i}}\\psi({\\mathbf{u}})=-e^{-u_{i}}\\quad\\mathrm{for\\,all\\,}i\\in[K-1].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2.1 Convexity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla^{2}\\psi({\\bf u})=\\mathsf{d i a g}(\\exp(-u_{i}):i=1,\\ldots,K-1).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The Hessian is a diagonal matrix with all diagonal entries positive. Hence it is positive definite. ", "page_idx": 16}, {"type": "text", "text": "C.2.2 $\\beta$ -\u201csmoothness\u201d ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall the identity derived in Lemma B.4: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\nabla^{2}\\overline{{g}}(\\mathbf{x})\\mathbf{v}=(\\mathbf{A}\\mathbf{V}^{\\top}\\mathbf{b})^{\\top}\\nabla^{2}f(\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{b})\\mathbf{A}\\mathbf{V}^{\\top}\\mathbf{b}^{\\top}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We are interested in the special case where $\\mathbf{X}\\leftarrow\\mathbf{W}$ is a linear classifier (represented as a matrix) and $\\mathbf{x}\\leftarrow\\mathsf{v e c}(\\mathbf{W})=\\mathbf{w}$ is its vectorization as in Section 3. Moreover, $g(\\mathbf{X})$ represents the risk ${\\mathcal{R}}(\\mathbf{W})$ , viewed as a matrix-input scalar-output function (defined in Appendix B), while $\\overline{{g}}(\\mathbf{x})$ represents the vectorized risk $\\mathcal{R}(\\mathbf{w})$ , viewed as a vector-input scalar-output function (defined in Appendix B). We will use the formula in Lemma B.4 to calculate $\\mathbf{v}^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathbf{v}$ , where we substitute in ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{A}\\gets\\mathbf{Y}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\in\\mathbb{R}^{(K-1)\\times K},\\qquad\\mathbf{b}\\gets\\mathbf{x}_{i}\\in\\mathbb{R}^{d},\\qquad f\\gets\\psi.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\left(\\mathbf{x}_{i},y_{i}\\right)$ is a training sample and $\\psi$ is the template of a PERM loss. Since $\\nabla^{2}$ is linear (i.e., distributive over additions), we have by Lemma B.4 that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{v}^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathbf{v}=\\mathsf{v e c}(\\mathbf{V})^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathsf{v e c}(\\mathbf{V})}\\\\ &{\\ =\\displaystyle\\sum_{i=1}^{N}(\\Upsilon_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{V}\\mathbf{x}_{i})^{\\top}\\nabla^{2}\\psi(\\Upsilon_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{W}\\mathbf{x}_{i})\\Upsilon_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{V}\\mathbf{x}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\|\\mathbf{v}\\|=\\|\\mathsf{v e c}(\\mathbf{V})\\|=\\|\\mathbf{V}\\|_{F}$ by the definitions of the Frobenius norm and vectorization. Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{v}\\in\\mathbb{R}^{d K}:\\|\\mathbf{v}\\|=1}\\mathbf{v}^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathbf{v}=\\operatorname*{max}_{\\mathbf{v}\\in\\mathbb{R}^{d\\times K}:\\|\\mathbf{V}\\|_{F}=1}\\mathsf{v e c}(\\mathbf{V})^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathsf{v e c}(\\mathbf{V}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that we never defined the Hessian of a matrix-input function, i.e., we do not work with $\\nabla^{2}\\mathcal{R}(\\mathbf{W})$ . Combining the two previous identities, we have proven ", "page_idx": 16}, {"type": "text", "text": "Corollary C.2. Let ${\\mathcal{R}}(\\mathbf{W})$ be the risk viewed as a matrix-input scalar-output function defined in Equation (3). Let $\\mathcal{R}(\\mathbf{w})$ be the vectorization of ${\\mathcal{R}}(\\mathbf{W})$ . Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{\\mathbf{v}\\in\\mathbb{R}^{d K}:\\|\\mathbf{v}\\|=1}{\\operatorname*{max}}\\mathbf{v}^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathbf{v}}\\\\ &{=\\displaystyle\\operatorname*{max}_{\\mathbf{v}\\in\\mathbb{R}^{d\\times K}:\\|\\mathbf{V}\\|_{F}=1}\\sum_{i=1}^{N}(\\Upsilon_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{V}\\mathbf{x}_{i})^{\\top}\\nabla^{2}\\psi(\\Upsilon_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{W}\\mathbf{x}_{i})\\Upsilon_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{V}\\mathbf{x}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla^{2}\\psi({\\bf u})=\\mathsf{d i a g}(\\exp(-u_{i}):i=1,\\ldots,K-1).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let vdiag $(\\cdot)$ be the \u201cinverse\u201d of $\\mathsf{d i a g}(\\cdot)$ , i.e., vdiag $(\\cdot)$ takes a diagonal matrix and returns the vector of the diagonal elements. ", "page_idx": 16}, {"type": "text", "text": "The max eigenvalue of the Hessian of $\\mathcal{R}(\\mathbf{w})$ , i.e., Equation (29), is computed below: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname*{max}}{\\mathrm{vetst\\'A}\\sqrt{\\|v\\|}\\|\\mathrm{r}=1}\\mathbf{Y}^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\mathbf{v}}\\\\ &{\\overset{\\mathrm{anc\\'{l}}}{=}\\underbrace{\\operatorname*{max}}_{\\mathbf{V}\\in\\mathbb{R}^{d+\\mathcal{N}}\\times\\mathbb{R}^{3}\\mathbb{V}^{\\top}=1}\\sum_{i=1}^{N}(\\mathbf{T}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i})^{\\top}\\nabla^{2}\\psi(\\mathbf{T}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i})\\mathbf{T}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i}\\quad\\colon\\mathrm{Corollay\\mathbb{C}\\itOmega}_{\\mathrm{2}}}\\\\ &{\\overset{\\mathrm{()}}{=}\\underbrace{\\operatorname*{max}}_{\\mathbf{V}\\in\\mathbb{R}^{d+\\mathcal{N}}\\times\\mathbb{R}^{3}\\mathbb{V}\\|\\mathrm{r}=1}\\sum_{i=1}^{N}\\mathrm{tr}\\Big(\\nabla^{2}\\psi(\\mathbf{T}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i})\\underbrace{\\mathbf{T}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i}(\\mathbf{Y}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i})^{\\top}}_{(K-1)\\cdot\\mathrm{by-(K-1)~outcr~product}}\\Big)}\\\\ &{\\overset{\\mathrm{3}}{=}\\underbrace{\\operatorname*{max}}_{\\mathbf{V}\\in\\mathbb{R}^{d+\\mathcal{N}}\\times\\mathbb{R}^{3}\\mathbb{V}\\|\\mathrm{r}=1}\\sum_{i=1}^{N}\\underbrace{\\mathrm{tr}\\mathrm{d}_{3}\\mathbf{g}(\\nabla^{2}\\psi(\\mathbf{T}_{\\mathfrak{p}},\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}))^{\\top}}_{\\mathrm{Vetard~diamal~custensted\\:}\\forall\\mathrm{~curous~stuce}}}\\\\ &{\\leq\\underbrace{\\operatorname*{max}}_{\\mathbf{V}\\in\\mathbb{R}^{d+\\mathcal{N}}\\times\\mathbb{R}^{3}\\mathbb{V}}\\underbrace{\\sum_{i=1}^{N}R(\\mathbf{W})^{\\top}(\\mathbf \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In equality 2 we took trace of a scalar (the expression in equality 1 is a scalar, so taking the trace of it will not change the value) and used the cyclic property. For equality 3: as per Equation (28), $\\nabla^{2}\\psi$ is a diagonal matrix. Finally, in the last inequality, we bound each element of the diagonal vector (i.e. $\\exp\\left(-u_{i}\\right)$ for all $i\\in[K^{-}1];$ ). Dropping the ${\\mathcal{R}}(\\mathbf{W})$ and $\\!\\!\\!\\!\\operatorname{\\mathrel{\\omega}}\\!\\!\\!\\operatorname{max}_{\\mathbf{V}\\in\\mathbb{R}^{d\\times K}:\\|\\mathbf{V}\\|_{F}=1}\\!\\!\\!^{*}$ from the front: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{N}\\mathbf{1}^{\\top}(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i})^{\\odot_{2}}}\\\\ &{\\displaystyle=\\sum_{i=1}^{N}(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i})^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i}=\\displaystyle\\sum_{i=1}^{N}\\mathrm{tr}\\left((\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i})^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i}\\right)}\\\\ &{\\displaystyle=\\sum_{i=1}^{N}\\mathrm{tr}\\Big(\\mathbf{D}^{\\top}\\mathbf{Y}_{y_{i}}^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{V}^{\\top}\\mathbf{x}_{i}\\mathbf{\\bar{x}}_{i}^{\\top}\\mathbf{V}\\Big)~~~\\mathrm{~Note~that~}\\mathbf{V}^{\\top}\\mathbf{x}_{i}\\in\\mathbb{R}^{K}}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{N}\\|\\mathbf{D}^{\\top}\\mathbf{Y}_{y_{i}}^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\|_{F}\\|\\mathbf{V}^{\\top}\\mathbf{x}_{i}\\mathbf{\\bar{x}}_{i}^{\\top}\\mathbf{V}\\|_{F}\\quad\\cdot\\mathrm{~:Cauchy\\mathrm{-}S c h w a r z~i n e q u a l i t y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that we applied Cauchy-Schwarz to the inner product space $\\mathbb{R}^{K\\times K}$ with inner product $\\left\\langle\\mathbf{A},\\mathbf{B}\\right\\rangle:=$ $\\mathbf{tr}(\\mathbf{A}^{\\top}\\mathbf{B})$ . Now, continuing with the calculation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{N}\\|\\mathbf{D}^{\\top}\\mathbf{Y}_{y_{i}}^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\|_{F}\\|\\mathbf{V}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{V}\\|_{F}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{N}\\|\\mathbf{Y}_{y_{i}}\\mathbf{D}\\|_{F}^{2}\\|\\mathbf{V}^{\\top}\\|_{F}\\|\\mathbf{X}_{i}\\mathbf{x}_{i}^{\\top}\\|_{F}\\|\\mathbf{V}\\|_{F}\\quad\\colon\\|\\mathbf{A}\\mathbf{B}\\|_{F}\\le\\|\\mathbf{A}\\|_{F}\\|\\mathbf{B}\\|_{F}}\\\\ &{\\le\\displaystyle(2K-2)\\displaystyle\\sum_{i=1}^{N}\\|\\mathbf{x}_{i}\\|^{2}\\quad\\colon\\mathrm{Lemma~C.3,~}\\|\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\|_{F}=\\|\\mathbf{x}_{i}\\|^{2},\\|\\mathbf{V}\\|_{F}=1}\\\\ &{\\le\\displaystyle(2K-2)\\left(\\displaystyle\\sum_{i=1}^{N}\\|\\mathbf{x}_{i}\\|\\right)^{2}\\quad\\colon\\mathrm{for~all~}a_{j}>0,M\\ge1,\\sum_{i=1}^{M}a_{j}^{2}\\le\\left(\\sum_{i=1}^{M}a_{j}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore we have proven that $\\|\\nabla^{2}\\mathcal{R}\\left(\\mathbf{w}\\right)\\|_{2}\\leq B^{2}\\mathcal{R}\\left(\\mathbf{w}\\right)$ , where ", "page_idx": 17}, {"type": "equation", "text": "$$\nB=\\sqrt{(2K-2)}\\sum_{i=1}^{N}\\|\\mathbf{x}_{i}\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we will also analyze the Euclidean norm of the gradient, for reasons that will become clear later. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla\\mathcal{R}\\left(\\mathbf{w}\\right)\\right|=\\left|\\nabla\\mathcal{R}\\left(\\mathbf{w}\\right)\\right|\\right|_{\\mathcal{F}}}\\\\ &{=\\left\\|\\displaystyle\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\mathbf{w}^{\\mathsf{T}}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\right\\|_{\\mathcal{F}}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{N}\\left\\|\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\mathbf{W}^{\\mathsf{T}}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\right\\|_{\\mathcal{F}}~~~\\because\\mathrm{trangle~inequality}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{N}\\left\\|\\mathbf{x}_{i}\\right\\|_{2}\\left\\|\\nabla\\psi\\left(\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\mathbf{W}^{\\mathsf{T}}\\mathbf{x}_{i}\\right)\\right\\|_{2}\\left\\|\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\right\\|_{F}~~~\\cdot\\left\\|\\mathbf{A}\\mathbf{B}\\right\\|_{F}\\le\\left\\|\\mathbf{A}\\right\\|_{F}\\left\\|\\mathbf{B}\\right\\|_{F}}\\\\ &{=\\sqrt{(2K-2)}\\displaystyle\\sum_{i=1}^{N}\\left\\|\\mathbf{x}_{i}\\right\\|_{2}\\left\\|\\nabla\\psi\\left(\\mathbf{T}_{\\mathbf{y}},\\mathbf{D}\\mathbf{W}^{\\mathsf{T}}\\mathbf{x}_{i}\\right)\\right\\|_{2}~~~\\cdot\\mathrm{U}\\mathrm{ama}~C,3}\\\\ &{\\le\\sqrt{(2K-2)}\\displaystyle\\sum_{i=1}^{N}\\left\\|\\mathbf{x}_{i}\\right\\|\\mathcal{R}\\left(\\mathbf{w}\\right)~~~\\cdot\\mathrm{[foral~}a_{j}>0,M\\ge1,\\sqrt{\\sum_{i=1}^{M}a_{j}^{2}}\\le\\sum_{i=1}^{M}a_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Gradient descent is a special case of steepest descent with the Euclidean norm [Boyd and Vandenberghe, 2004]. Thus, we can apply Gunasekar et al. [2018, Lemmas 11 & 12] to see that $\\nabla\\mathcal{R}\\left(\\mathbf{w}\\right)\\rightarrow0$ even for multiclass exponential loss. Elaborating on this: these lemmas from Gunasekar et al. [2018] assume a convex risk objective (which we have in the case of multiclass exponential loss). Additionally, they assume that $\\lVert\\nabla\\mathcal{R}\\left(\\mathbf{w}\\right)\\rVert\\leq B\\mathcal{R}\\left(\\mathbf{w}\\right)$ and $\\left\\|\\nabla^{2}\\mathcal{R}\\left(\\mathbf{w}\\right)\\right\\|_{2}\\leq B^{2}\\mathcal{R}\\left(\\mathbf{\\dot{w}}\\right)$ . In the above section we prove thes\u221ae exact results with $B$ as defined in Eqn. 31. Finally, Lemma C.3 below proves that $\\|\\mathbf{\\boldsymbol{\\Upsilon}}_{k}\\mathbf{\\mathbf{D}}\\|_{F}=\\sqrt{2K-2}$ for all $k\\in[K-1]$ . ", "page_idx": 18}, {"type": "text", "text": "In conclusion, by Gunasekar et al. [2018, Lemma 11], if our learning rate $\\begin{array}{r}{\\eta<\\frac{1}{B^{2}\\mathcal{R}(\\mathbf{w}(0))}}\\end{array}$ , we can use Soudry et al. [2018, Lemma 10]. ", "page_idx": 18}, {"type": "text", "text": "Finally, we calculate $\\|\\mathbf{\\boldsymbol{\\Upsilon}}_{k}\\mathbf{D}\\|_{F}$ which was used in several places above: ", "page_idx": 18}, {"type": "text", "text": "Lemma C.3. For each $k\\in[K]$ , we have $\\|\\mathbf{T}_{k}\\mathbf{D}\\|_{F}=\\sqrt{2(K-1)}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. First, if $k\\;=\\;K$ , then $\\mathbf{\\nabla}\\Upsilon_{K}$ is the identity matrix. In this case, we have $\\mathbf{\\Delta}\\mathbf{\\Upsilon}_{\\mathbf{\\Upsilon}_{k}}\\mathbf{D}\\;=\\;\\mathbf{D}\\;=\\;\\mathbf{\\Upsilon}$ $\\bigl[-\\mathbf{I}_{K-1}\\quad\\mathbf{1}_{K-1}\\bigr]$ is the negative $(K-1)$ -by- $(K-1)$ identity matrix concatenated with the all-ones vector. Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{Y}_{k}\\mathbf{D}\\|_{F}^{2}=\\|\\mathbf{D}\\|_{F}^{2}=\\|\\mathbf{I}_{K-1}\\|_{F}^{2}+\\|\\mathbf{1}_{K-1}\\|^{2}=2(K-1)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $k\\neq K$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{\\Upsilon}\\Upsilon_{k}\\mathbf{D}=\\left[-\\Upsilon_{k}\\right.\\ \\ \\Upsilon_{k}{\\mathbf{1}}_{K-1}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{\\mathcal{X}}_{k}\\mathbf{D}\\|_{F}^{2}=\\|\\mathbf{\\mathcal{Y}}_{k}\\|_{F}^{2}+\\|\\mathbf{\\mathcal{Y}}_{k}\\mathbf{1}_{K-1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we recall from the definition of $\\mathbf{\\boldsymbol{\\Upsilon}}_{\\ast}$ (Definition 2.4 of Wang and Scott [2024]) that ${\\bf\\nabla}{\\bf Y}_{k}$ is obtained by replacing the $k$ -th column of the identity matrix by the \u201call-negative-ones\u201d vector. Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{Y}_{k}\\|_{F}^{2}=2(K-1)-1,\\quad\\mathrm{and}\\quad\\|\\mathbf{Y}_{k}\\mathbf{1}_{K-1}\\|_{F}^{2}=\\|-\\mathbf{e}_{k}\\|_{F}^{2}=1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This proves Lemma C.3, as desired. ", "page_idx": 18}, {"type": "text", "text": "C.2.3 Exponential Tail ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From Eqn. (27), the negative partial derivative of the template is clearly always positive: ", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\frac{\\partial}{\\partial u_{i}}\\psi(\\mathbf{u})=e^{-u_{i}}\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the above, it is clear that the upper and lower bounds in Definition 2.2 hold when $u_{\\pm}=0$ and $c=1$ . ", "page_idx": 18}, {"type": "text", "text": "C.3 PairLogLoss [Wang et al., 2022] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall that the template of the PairLogLoss is $\\begin{array}{r}{\\psi\\left(\\mathbf{u}\\right)=\\sum_{k=1}^{K-1}\\log\\left(1+\\exp\\left(-u_{k}\\right)\\right)}\\end{array}$ . By elementary calculus, we see that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\psi(u)}{\\partial u_{k}}=-\\frac{e^{-u_{k}}}{1+e^{-u_{k}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.3.1 Convexity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla^{2}\\psi(\\mathbf{u})=\\mathsf{d i a g}\\left(e^{u_{i}}/\\left(1+e^{u_{i}}\\right)^{2}:i=1,\\ldots,K-1\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Hessian is a diagonal matrix with all diagonal entries positive. Hence it is positive definite. ", "page_idx": 19}, {"type": "text", "text": "C.3.2 $\\beta$ -smoothness ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Notice that the partial derivative of the template is exactly the same expression as the derivative of the logistic loss (i.e. binary cross-entropy). Thus, the exact same proof as logistic loss can be used to prove $\\beta$ -smoothness for the PairLogLoss as well. Thus, from Appendix C.1.2, $\\beta=1/4$ (logistic loss is simply the $K=2$ case for cross-entropy). ", "page_idx": 19}, {"type": "text", "text": "C.3.3 Exponential Tail ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n-\\frac{\\partial\\psi(u)}{\\partial u_{k}}=\\frac{e^{-u_{k}}}{1+e^{-u_{k}}}\\leq e^{-u_{k}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gives us the desired upper tail. As for the lower tail: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\,\\frac{\\partial\\psi({\\boldsymbol u})}{\\partial{\\boldsymbol u}_{k}}=\\frac{e^{-{\\boldsymbol u}_{k}}}{1+e^{-{\\boldsymbol u}_{k}}}}\\\\ {\\displaystyle\\geq e^{-{\\boldsymbol u}_{k}}\\left(1-e^{-{\\boldsymbol u}_{k}}\\right)\\quad\\cdot\\,\\frac{1}{1+x}\\geq1-x\\,\\mathrm{for\\,all}\\,x\\geq0}\\\\ {\\displaystyle\\geq e^{-{\\boldsymbol u}_{k}}\\left(1-\\sum_{i=1}^{K-1}e^{-{\\boldsymbol u}_{i}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, PairLogLoss satisfies Definition 2.2 with $u_{\\pm}=0$ and $c=a=1$ . ", "page_idx": 19}, {"type": "text", "text": "D Pseudo-index ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Note that $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}$ produces a $(K\\,-\\,1)$ -dimensional vector with entries of the form of $\\left(\\mathbf{r}_{y_{i}}(t)-\\mathbf{r}_{k}(t)\\right)^{\\top}\\mathbf{x}_{i},\\forall k\\,\\in\\,[K]\\backslash\\{y_{i}\\}$ . For $k\\,\\in\\,[K]\\backslash\\{y_{i}\\}$ , let us represent the corresponding entry of the vector as $\\mathbb{I}\\mathbf{T}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\\mathbb{I}_{k}$ . Note that this indexing is not the same as the $k^{t h}$ entry of the vectors, since the $y_{i}^{~t h}$ entry $\\left(\\mathbf{r}_{y_{i}}(t)-\\mathbf{r}_{y_{i}}(t)\\right)^{\\top}\\mathbf{x}_{i}$ is not present in the vector. Similarly, let us define $\\mathbb{[-\\nabla\\psi\\left(\\mathbf{T}_{\\mathcal{Y}_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)]}_{k}$ to be the corresponding entry of $-\\nabla\\psi$ . ", "page_idx": 19}, {"type": "text", "text": "This section makes this indexing trick rigorous. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.1. Let $\\mathbf{W}\\in\\mathbb{R}^{d\\times K}$ be arbitrary and $\\mathbf{w}:=\\mathsf{v e c}(\\mathbf{W})$ be its vectorization. Let $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ be $a$ training sample. Then there exists a bijection that depends only on $y_{i}$ that maps the entries of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}\\in\\mathbb{R}^{K-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "to the elements of the set of \u201c $y_{i}$ -versus- $k$ \u201d relative margins, i.e., $\\{\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w}\\in\\mathbb{R}:k\\in[K]\\setminus\\{y_{i}\\}\\}$ . ", "page_idx": 19}, {"type": "text", "text": "The following definition makes the bijection from Lemma D.1 concrete. ", "page_idx": 19}, {"type": "text", "text": "Definition D.1 (Pseudo-index). In the situation of Lemma $D.1$ , define $\\mathbb{I}\\!\\cdot\\!\\mathbb{I}_{i,k}:\\mathbb{R}^{K-1}\\to\\mathbb{R}$ to be the coordinate projection such that $\\[\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y_{i}}\\mathbf{D}\\mathbf{W}\\mathbf{x}_{i}]_{i,k}\\,=\\,\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w}$ . In othe r   words, $\\[\\cdot]_{i,k}$ selects the $y_{i}$ -versus- $k$ relative margin. When the sample index $i$ is clear from context, we drop $i$ from the subscript and simply write $[\\![\\cdot]\\!]_{k}$ . ", "page_idx": 19}, {"type": "text", "text": "The pseudo-index is useful for working with the exponential tail bounds: ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2. In the situation of Lemma $D.I$ , consider $\\nabla\\psi(\\mathbf{T}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i})$ which is the $(K-1)$ - dimensional vector of partial derivatives of the template evaluated at $\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i}$ . If $\\psi$ satisfies Definition 2.2, then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{[}-\\nabla\\psi(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i})\\mathbb{]}_{k}\\le\\exp(-\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big[-\\nabla\\psi(\\mathbf{T}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\mathbf{W}^{\\top}\\mathbf{x}_{i})\\big]\\|_{k}\\geq\\big(1-\\sum_{r\\in[K]\\backslash\\{\\boldsymbol{y}_{i}\\}}\\exp(-\\tilde{\\mathbf{x}}_{i,r}^{\\top}\\mathbf{w})\\big)\\exp(-\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all $k\\in[K]\\setminus\\{y_{i}\\}$ . ", "page_idx": 20}, {"type": "text", "text": "D.1 Proofs of Lemma D.1 and Lemma D.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In both lemmas, we work with a fixed sample, i.e., the index $i$ does not change. As such, we simply drop the index and write $y\\leftarrow y_{i}$ , $\\mathbf x\\gets\\mathbf x_{i}$ , $\\tilde{\\mathbf{x}}_{k}\\gets\\tilde{\\mathbf{x}}_{i,k}$ , and $[\\cdot]_{k}\\gets[\\cdot]_{i,k}$ . ", "page_idx": 20}, {"type": "text", "text": "Below, we fix $k\\in[K-1]$ throughout the proof. Let $\\mathbf{v}:=\\mathbf{w}^{\\top}\\mathbf{x}=[v_{1},\\,.\\,.\\,.\\,,\\,v_{K}]^{\\top}$ . Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{y}\\mathbf{D}^{\\top}\\mathbf{W}^{\\top}\\mathbf{x}=\\mathbf{Y}_{y}\\left[\\begin{array}{l}{v_{K}-v_{1}}\\\\ {\\qquad\\vdots}\\\\ {v_{K}-v_{K-1}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We prove Lemma D.1 by considering the case $y=K$ and $y\\ne K$ separately. First, let us consider the case when $y=K$ . Then $\\Upsilon_{y}$ is the identity matrix and so ", "page_idx": 20}, {"type": "equation", "text": "$$\nk{\\mathrm{-th~component~of~Equation~}}(32)~\\mathrm{is}\\,=v_{K}-v_{k}=\\left(\\mathbf{w}_{K}-\\mathbf{w}_{k}\\right)^{\\top}\\mathbf{x}=\\tilde{\\mathbf{x}}_{k}^{\\top}\\mathbf{w}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we\u2019ve proven Lemma D.1 when $y=K$ . In this case, $[\\![\\cdot]\\!]_{k}$ simply picks out the $k$ -th entry of the input $(K-1)$ -dimensional) vector. In other words, ", "page_idx": 20}, {"type": "equation", "text": "$$\n[\\mathbf{z}]_{k}=z_{k},\\quad{\\mathrm{for~all~}}\\mathbf{z}=[z_{1},\\,\\dots,z_{K-1}]^{\\top}\\in\\mathbb{R}^{K-1}{\\mathrm{~when~}}y=K.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By definition, we note that the $k$ -th row of $\\Upsilon_{y}$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Upsilon_{y}[k,:]=\\left\\{\\mathbf{e}_{k}-\\mathbf{e}_{y}\\quad:k\\neq y,\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, when $y\\ne K$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{k{\\mathrm{-th~component~of~Equation~}}(32)\\;=\\left\\{{\\bigl(}v_{K}-v_{n}{\\bigr)}\\;}&{:k\\neq y,}\\\\ {-{\\bigl(}v_{K}-v_{y}{\\bigr)}}&{:k=y.}\\end{array}\\right.}\\\\ {\\qquad=\\left\\{{\\bigl(}\\mathbf{w}_{y}-\\mathbf{w}_{k}{\\bigr)}^{\\top}\\mathbf{x}={\\tilde{\\mathbf{x}}}_{k}\\;}&{:k\\neq y,}\\\\ {\\qquad\\left(\\mathbf{w}_{y}-\\mathbf{w}_{K}\\right)^{\\top}\\mathbf{x}={\\tilde{\\mathbf{x}}}_{K}}&{:k=y.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we\u2019ve proven Lemma D.1 when $y\\ne K$ as well. In this case, $[\\![\\cdot]\\!]_{k}$ picks out the $k$ -th element of the input $(K-1)$ -dimensional) vector when $k\\neq y$ . Otherwise wh e n $k=y$ , we have that $[\\![\\cdot]\\!]_{k}$ picks out the $y$ -th element. More explicitly, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\mathbf{z}\\|_{k}=\\left\\{{\\boldsymbol{z}}_{k}\\ \\ :k\\neq y\\,\\,,\\quad{\\mathrm{for~all~}}\\mathbf{z}=[z_{1},\\,.\\,.\\,.\\,,z_{K-1}]^{\\top}\\in\\mathbb{R}^{K-1}{\\mathrm{~when~}}y\\neq K.\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we prove Lemma D.2 by considering the case $y=K$ and $y\\ne K$ separately. First, assume that we are in the $y=K$ case. From Equation (33), we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{\\left[{-\\nabla}\\psi({\\mathbf{T}_{\\mathcal{y}_{i}}}{\\mathbf{D}^{\\top}}{\\mathbf{W}^{\\top}}{\\mathbf{x}_{i}})\\right]}_{k}=-\\frac{\\partial\\psi}{\\partial u_{k}}({\\mathbf{\\Upsilon}{\\mathbf{Y}_{\\mathcal{y}_{i}}}{\\mathbf{D}^{\\top}}{\\mathbf{W}^{\\top}}{\\mathbf{x}_{i}}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, we have that for $\\mathbf{u}=[u_{1},\\,.\\,.\\,.\\,,\\,u_{K-1}]^{\\top}\\in\\mathbb{R}^{K-1}$ , the upper and lower exponential tail bounds are ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\frac{\\partial\\psi}{\\partial u_{k}}\\left(\\mathbf{u}\\right)\\leq c\\exp(-u_{k}),\\quad\\mathrm{and}\\quad-\\frac{\\partial\\psi}{\\partial u_{k}}\\left(\\mathbf{u}\\right)\\geq c\\left(1-\\sum_{r\\in[K-1]}\\exp{(-u_{r})}\\right)\\exp(-u_{k}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Letting $\\mathbf{u}:=\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y_{i}}\\mathbf{D}^{\\top}\\mathbf{W}^{\\top}\\mathbf{x}_{i}$ , using Lemma D.1 and Equation (33), we immediately prove Lemma D.2 in the case when $y=K$ . This is because we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{exp}(-u_{k})=\\mathrm{exp}(-[\\mathsf{\\Gamma}]\\mathsf{Y}_{\\boldsymbol{y}_{i}}\\mathbf{D}^{\\top}\\mathbf{W}^{\\top}\\mathbf{x}_{i}\\mathbb{\\]}_{k})=\\mathrm{exp}(-\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{r\\in[K-1]}\\exp\\left(-u_{r}\\right)=\\sum_{r\\in[K]\\backslash\\{y_{i}\\}}\\exp\\left(-u_{r}\\right)=\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}\\exp\\left(-\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{w}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $y=K$ , a similar argument proves Lemma D.2 using Equation (33) for the pseudo-index $[\\![\\cdot]\\!]_{k}$ . ", "page_idx": 21}, {"type": "text", "text": "E Proof of Lemma 4.8 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Re-stating the lemma: ", "page_idx": 21}, {"type": "text", "text": "Lemma 4.8. (Multiclass generalization of Soudry et al. [2018, Lemma $I J$ ) Consider any linearly separable dataset, and any PERM loss with template $\\psi$ that is convex, $\\beta$ -smooth, strictly decreasing, and non-negative. For all $\\dot{\\boldsymbol{k}}\\in\\{1,...,K\\}$ , let $\\mathbf{w}_{k}(t)$ be the gradient descent iterates at iteration $t$ for the $k^{t h}$ class. Then $\\forall i\\in\\{1,\\ldots,N\\},\\forall j\\in\\{1,\\ldots,K\\}\\backslash\\{y_{i}\\}:\\operatorname*{lim}_{t\\rightarrow\\infty}(\\mathbf{w}_{y_{i}}(t)-\\mathbf{w}_{j}(t))^{\\top}\\mathbf{x}_{i}\\rightarrow\\infty$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We know that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)=\\mathbf{0}}\\end{array}$ by Soudry et al. [2018, Lemma 10]. ", "page_idx": 21}, {"type": "text", "text": "This implies that $\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{w}(t))\\rightarrow\\mathbf{0}$ . Following the same steps as in the proof of Lemma 4.4, this is equivalent to saying: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{w}}^{\\top}\\nabla\\mathcal{R}(\\mathbf{w}(t))=\\operatorname{tr}(\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i})\\rightarrow0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "However, for linearly separable data we know that $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\Upsilon_{y_{i}}\\mathbf{D}\\hat{\\mathbf{W}}^{\\top}\\mathbf{x}_{i}\\succeq\\mathbf{1}$ (since $\\hat{\\bf W}$ here is the hardmargin SVM solution). Thus for the above limit to be true, the limit ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\nabla\\psi\\left(\\mathbf{T}_{\\boldsymbol{y}_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)=\\mathbf{0}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "must hold. By Proposition G.1, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}=\\infty\\qquad\\forall i\\in[N]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\infty$ is the \u201cvector\u201d whose entries are all equal to infinity. This is equivalent to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\rightarrow\\infty}{\\operatorname*{lim}}(\\mathbf w_{y_{i}}(t)-\\mathbf w_{j}(t))^{\\top}\\mathbf x_{i}=\\infty\\qquad\\forall i\\in[N],\\forall j\\in[K]\\backslash\\{y_{i}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathsf{c}_{y_{i}}\\mathbf D\\mathbf W(t)^{\\top}\\mathbf x_{i}\\mathbb J_{j}=(\\mathbf w_{y_{i}}(t)-\\mathbf w_{j}(t))^{\\top}\\mathbf x_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that in the binary case, the above \u201cconvergence-to-infinity\u201d condition is for a scalar quanity, where the assumption that the loss be strictly decreasing and non-negative suffices. In the multiclass setting, we must ensure that all entries of the vector $\\mathbf{\\check{T}}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{\\check{x}}_{i}$ converges to infinity. This is a nontrivial result and is addressed by our Proposition G.1. ", "page_idx": 21}, {"type": "text", "text": "F Proof of Lemma 4.6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let us first re-state the lemma we want to prove. ", "page_idx": 21}, {"type": "text", "text": "Lemma 4.6. (Generalization of Soudry et al. [2018, Lemma 20]) Define $\\theta$ to be the minimum SVM margin across all data points and classes, i.e., $\\theta=\\mathrm{min}_{k}\\left[\\mathrm{min}_{n\\notin S_{k}}\\,\\tilde{\\mathbf{x}}_{n,k}^{\\top}\\hat{\\mathbf{w}}\\right]>1$ . Then: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\exists C_{1},C_{2},t_{1}:\\forall t>t_{1}:\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)\\leq C_{1}t^{-\\theta}+C_{2}t^{-2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Proceeding the same way as Soudry et al. [2018], we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)}\\\\ &{=\\left(-\\eta\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)-\\hat{\\mathbf{w}}\\left[\\log\\left(t+1\\right)-\\log\\left(t\\right)\\right]\\right)^{\\top}\\mathbf{r}\\left(t\\right)}\\\\ &{=\\left(-\\eta\\nabla\\mathcal{R}\\left(\\mathbf{w}\\left(t\\right)\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)-\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\log\\left(1+t^{-1}\\right)}\\\\ &{=\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\left(t^{-1}-\\log\\left(1+t^{-1}\\right)\\right)+\\operatorname{tr}\\left(\\left(-\\eta\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\right)^{\\top}\\mathbf{R}(t)\\right)}\\\\ &{\\qquad-\\,t^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(3.5\\mathrm{~a~n~})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last equality is a new step required for our multiclass generalization, in which we used Lemma 4.2 and introduced the matrices $\\bar{\\bf W}(t)$ and $\\mathbf{R}(t)$ , where vec $(\\mathbf{W}(t))=\\mathbf{w}(t)$ and $\\mathsf{v e c}(\\mathbf{R}(t))=\\mathbf{r}(t)$ . Let us focus just on the second term of this expansion. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\left(\\left(-\\eta\\displaystyle\\sum_{i=1}^{N}\\mathbf{x}_{i}\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\right)^{\\top}\\mathbf{R}(t)\\right)}\\\\ &{\\stackrel{\\mathrm{(1)}}{=}\\eta\\mathrm{tr}\\left(\\displaystyle\\sum_{i=1}^{N}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\\left(-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\right)}\\\\ &{\\stackrel{\\mathrm{(2)}}{=}\\eta\\mathrm{tr}\\left(\\displaystyle\\sum_{i=1}^{N}\\left(-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\right)^{\\top}\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In step (1) we used the fact that for any square matrix $\\mathbf{M}$ , $\\mathsf{t r}(\\mathbf{M})=\\mathsf{t r}\\left(\\mathbf{M}^{\\top}\\right)$ . In step (2) we used the cyclic property of the trace. ", "page_idx": 22}, {"type": "text", "text": "Similar to in the proof of Lemma 4.4, the trace\u2019s cyclic property has enabled us to convert a matrixproduct into a simple dot product. Since dot products are scalars, we can now drop the trace and rewrite our expression in Eqn. (36) as a dot product: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta\\sum_{i=1}^{N}\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}[-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)]_{k}[\\mathbf{\\mathcal{T}}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}]_{k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using this form, we can rewrite Eqn. (35): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left(\\mathbf{r}\\left(t+1\\right)-\\mathbf{r}\\left(t\\right)\\right)^{\\top}\\mathbf{r}\\left(t\\right)=\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\left(t^{-1}-\\log\\left(1+t^{-1}\\right)\\right)}&{}&\\\\ {\\quad+\\underbrace{\\eta\\sum_{i=1}^{N}\\sum_{\\substack{k\\in[K]\\backslash\\{y_{i}\\}}}\\left[-\\nabla\\psi\\left(\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\right]_{k}\\left[\\mathbf{Y}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}\\right]_{k}}_{\\substack{-t^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)}}}&{}&{}\\\\ {\\quad-\\,t^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)}&{}&{(3)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first term $\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)\\left(t^{-1}-\\log\\left(t^{-1}\\right)\\right)$ is bounded in [Soudry et al., 2018, Eqn. (139)]. We will focus on the second and third terms. Recall by Equation (6) and Equation (8) that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{w}}=\\sum_{i=1}^{N}\\sum_{k\\in[K]\\setminus\\{y_{i}\\}}\\alpha_{i,k}\\mathbb{1}_{\\{i\\in S_{k}\\}}\\tilde{\\mathbf{x}}_{i,k}=\\sum_{i=1}^{N}\\sum_{k\\in[K]\\setminus\\{y_{i}\\}}\\eta\\exp(-\\tilde{\\mathbf{w}}^{\\top}\\tilde{\\mathbf{x}}_{i,k})\\mathbb{1}_{\\{i\\in S_{k}\\}}\\tilde{\\mathbf{x}}_{i,k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, the third term on the RHS of Equation (37) can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)=\\eta\\sum_{i=1}^{N}\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}t^{-1}\\exp(-\\tilde{\\mathbf{w}}^{\\top}\\tilde{\\mathbf{x}}_{i,k})\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{r}(t)\\mathbb{1}_{\\{i\\in S_{k}\\}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the last two terms on the RHS of Equation (37) can be written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{i=1}^{N}\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}[-\\nabla\\psi\\left(\\mathbf{\\mathcal{T}}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)]_{k}[\\mathbf{\\mathcal{T}}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}]_{k}-t^{-1}\\hat{\\mathbf{w}}^{\\top}\\mathbf{r}(t)}\\\\ &{\\ =\\eta\\Big(\\displaystyle\\sum_{i=1}^{N}\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}[-\\nabla\\psi\\left(\\mathbf{\\mathcal{T}}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)]_{k}[\\mathbf{\\mathcal{T}}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}]_{k}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -t^{-1}\\exp(-\\tilde{\\mathbf{w}}^{\\top}\\tilde{\\mathbf{x}}_{i,k})\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{r}(t)\\mathbb{1}_{\\{i\\in S_{k}\\}}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\eta>0$ is constant, we ignore it below and consider only the term inside the parenthesis: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{N}\\sum_{k\\in\\{N\\}(\\backslash\\{S\\})}\\left[-\\nabla\\psi\\left(\\mathbf{T}_{\\Re}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{k}\\right)\\right]\\mathbf{k}\\left[\\mathbf{T}_{\\Re}\\mathbf{D}\\mathbf{B}(t)^{\\top}\\mathbf{x}_{k}\\right]\\mathbf{k}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{N}\\sum_{k\\in\\{N\\}\\setminus\\{0\\}}\\quad-t^{-1}\\exp(-\\psi\\frac{\\mathbf{T}_{\\Re}}{k})\\mathbf{k}_{i}^{\\top}\\mathbf{x}_{k}^{\\top}\\mathbf{r}(t)\\mathbb{I}\\{(\\mathbf{z}_{k}\\mathbf{z}_{i})}}\\\\ &{\\displaystyle\\overset{(i)}{=}\\sum_{i=1}^{N}\\sum_{k\\in\\{N\\}\\setminus\\{\\mathbf{r}_{i}\\}}\\left([-\\nabla\\psi_{i}\\left(\\mathbf{T}_{\\Re}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{k}\\right)]\\mathbf{k}_{i}\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.-t^{-1}\\exp(-\\psi\\mathbf{T}_{\\Re})\\mathbf{I}\\{(\\mathbf{z}_{k}\\mathbf{z}_{i})\\}\\right)\\widecheck{\\mathbf{x}}_{k}^{\\top}\\mathbf{r}(t)}\\\\ &{\\displaystyle\\overset{(i)}{\\leq}\\sum_{i=1}^{N}\\sum_{k\\in\\{N\\}\\setminus\\{\\mathbf{r}_{i}\\}}\\left(\\exp\\left(-\\psi\\big(t)^{\\top}\\mathbf{x}_{k},\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-t^{-1}\\exp(-\\psi\\frac{\\mathbf{T}_{\\Re}}{k})\\mathbb{I}\\{(\\mathbf{z}_{k}\\mathbf{z}_{i}^{\\top}\\mathbf{x}_{i}^{\\top}\\mathbf{(t)}^{\\top}\\mathbf{z}_{i})\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.-t^{-1}\\exp(-\\psi\\frac{\\mathbf{T}_{\\Re}}{k})\\mathbb{I}\\{(\\mathbf{z}_{k}^{\\top}\\mathbf{x}_{i}^{\\top}\\mathbf{z}_{i})\\}\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.+\\sum_{i=1}^{N}\\sum_{k\\in\\{N\\}\\setminus\\{\\mathbf{r}_{i}\\}}\\left(\\exp \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In (1) we used Lemma D.1, which implies that $\\tilde{\\mathbf{x}}_{i,k}^{\\top}\\mathbf{r}(t)\\,=\\,[\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{y_{i}}\\mathbf{D}\\mathbf{R}(t)^{\\top}\\mathbf{x}_{i}]\\vert_{k}$ . For (2), from the exponential tail upper/lower bound and Lemma D.2, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(-\\mathbf{w}(t)^{\\top}\\tilde{\\mathbf{x}}_{i,k}\\right)\\geq\\|-\\nabla\\psi\\left(\\mathbf{\\mathcal{T}}_{y_{i}}\\mathbf{D}\\mathbf{W}(t)^{\\top}\\mathbf{x}_{i}\\right)\\|_{k}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\exp\\left(-\\mathbf{w}(t)^{\\top}\\tilde{\\mathbf{x}}_{i,k}\\right)\\Big(1-\\displaystyle\\sum_{k\\in[K]\\backslash\\{y_{i}\\}}\\exp(-\\mathbf{w}(t)^{\\top}\\tilde{\\mathbf{x}}_{i,k})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We note that Eqn. (38) above is identical to the right hand side of inequality (1) in [Soudry et al., 2018, Eqn. (141)]. Thus, the remainder of the analysis proceeds identically as in Soudry et al. [2018, Lemma 20]. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "G A structural result on symmetric and convex functions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition G.1. Let $\\psi:\\mathbb{R}^{K-1}\\to\\mathbb{R}$ be the template of a PERM loss that satisfies our Theorem 3.4. Let $\\mathbf{\\bar{u}}^{t}\\in\\mathbb{R}^{K-1}$ be any sequence, where $t=1,2,\\ldots$ , such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\nabla\\psi(\\mathbf{u}^{t})=\\mathbf{0}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is the zero vector. Then $\\operatorname*{lim}_{t\\to\\infty}u_{j}^{t}=\\infty$ for every $j\\in[K-1]$ . ", "page_idx": 23}, {"type": "text", "text": "We prove Proposition G.1 by first proving a structural result (Theorem G.2) concerning symmetric and convex function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ . The proof of Proposition G.1 will be presented in Appendix G.2 as an application of the structural result, where we take $f=\\psi$ , the template of a PERM loss, and $n=K-1$ , number of classes minus one. ", "page_idx": 23}, {"type": "text", "text": "Given a vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ and a real number $C\\in\\mathbb{R}$ , define $\\mathbf{x}\\vee C\\in\\mathbb{R}^{n}$ to be the vector such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n[\\mathbf{x}\\vee C]_{i}:=\\operatorname*{max}\\{x_{i},C\\},\\quad{\\mathrm{for~all~}}i\\in[n].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In other words, $\\mathbf{x}\\vee C$ \u201cboosts\u201d entries of $\\mathbf{x}$ up to $C$ if those entries are smaller than $C$ . Entries of $\\mathbf{x}$ larger than $C$ are kept as-is. ", "page_idx": 24}, {"type": "text", "text": "Define $\\mathrm{min}({\\bf x})=\\mathrm{min}_{j\\in[n]}\\,x_{j}$ and $\\operatorname{argmin}(\\mathbf{x}):=\\{i\\in[n]:x_{i}=\\operatorname*{min}(\\mathbf{x})\\}$ . We note the following easy-to-prove properties of the ${}^{66}\\bigvee{}^{7}$ operation: ", "page_idx": 24}, {"type": "text", "text": "1. $\\operatorname*{min}(\\mathbf{x}\\vee C)\\ge C$ with equality if $\\operatorname*{min}(\\mathbf{x})\\leq C$ ,   \n2. $\\operatorname{argmin}(\\mathbf{x}\\vee C)\\supseteq\\operatorname{argmin}(\\mathbf{x}).$ ", "page_idx": 24}, {"type": "text", "text": "Theorem G.2. Suppose that $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is a symmetric, convex, and differentiable function. Then for any real number $C\\in\\mathbb{R}$ and any $\\mathbf{x}\\in\\mathbb{R}^{n}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial f}{\\partial x_{i}}({\\mathbf{x}})\\leq\\frac{\\partial f}{\\partial x_{i}}({\\mathbf{x}}\\vee C),\\quad f o r\\,a n y\\,i\\in\\mathrm{argmin}({\\mathbf{x}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Before proceeding with the proof (which is in Appendix G.1), we first introduce some necessary preliminary notations and facts. Given a vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ , we define ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{v a l}(\\mathbf{x}):=\\{x_{i}:i=1,\\ldots,n\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "to be the set of values consistings of the entries of $\\mathbf{x}$ . For example, if $\\mathbf{x}$ is the all-ones vector, then $\\mathsf{v a l}(\\mathbf{x})=\\{1\\}$ . Given $v\\,\\in\\,\\mathsf{v a l}(\\mathbf{x})$ , we let $\\mathbf{id}\\mathbf{\\times}(v,\\mathbf{x})=\\{i\\in\\mathbf{x}:x_{i}=v\\}$ be the set of indices that attains the value $v$ . ", "page_idx": 24}, {"type": "text", "text": "Fact 1: For a convex and differentiable function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle\\geq0,\\quad{\\mathrm{for~all~}}\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This is a simple and well-known consequence of convexity. See this stackexchange answer for a short proof. When $n=1$ , Ineq. (39) is the fact that a convex differentiable function has nondecreasing derivative. Ineq. (39) is also a consequence of [Phelps, 2009, Theorem 3.24]. ", "page_idx": 24}, {"type": "text", "text": "Fact 2: For a symmetric and differentiable function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial f}{\\partial x_{i}}({\\bf x})=\\frac{\\partial f}{\\partial x_{j}}({\\bf x}),\\quad\\mathrm{whenever}\\;x_{i}=x_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This fact follows from the chain rule and the definition of a symmetric function. To be precise, let $\\mathbf{T}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ be the permutation matrix that switches the $i$ and $j$ -th coordinate. Then $f(\\mathbf{x})=f(\\mathbf{T}\\mathbf{x})$ and moreover $\\begin{array}{r}{\\frac{\\partial f}{\\partial x_{i}}(\\mathbf{x})=\\frac{\\partial f}{\\partial x_{i}}(\\mathbf{Tx})=[\\mathbf{T}\\nabla f(\\mathbf{Tx})]_{i}=[\\nabla f(\\mathbf{Tx})]_{j}=[\\nabla f(\\mathbf{x})]_{j}=\\frac{\\partial f}{\\partial x_{j}}(\\mathbf{x})}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "G.1 Proof of Theorem G.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Now, to prove the above theorem, we will use induction on \u201c $m^{\\circ}$ in the following lemma, which is simply a \u201cstratification\u201d of Theorem G.2 into cases indexed by the \u201cparameter\u201d $m$ : ", "page_idx": 24}, {"type": "text", "text": "Lemma G.3. Suppose that $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is a convex, symmetric, and differentiable function. Let $m\\,\\in\\,\\{0,1,\\dots,\\bar{n}\\}$ . Then for any real number $C\\,\\in\\,\\mathbb{R}$ and any $\\textbf{x}\\in\\mathbb{R}^{n}$ with the property that $|\\{v\\in\\mathsf{v a l}(\\mathbf{x}):v<C\\}|=m,$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial f}{\\partial x_{i}}({\\mathbf{x}})\\leq\\frac{\\partial f}{\\partial x_{i}}({\\mathbf{x}}\\vee C),\\quad f o r\\,a n y\\,i\\in\\mathrm{argmin}({\\mathbf{x}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that if we have proved Lemma G.3 for each $m\\in\\{0,1,\\ldots,n\\}$ , then Theorem G.2 holds. ", "page_idx": 24}, {"type": "text", "text": "The base step: we prove Lemma G.3 when $m=0$ and $m=1$ . Strictly speaking, the proof-byinduction technique typically only involve only the base case, which would be the $m=0$ case in this instance. But below, we will see that in the induction step, the $m=1$ case is helpful. ", "page_idx": 24}, {"type": "text", "text": "Note that the $m=0$ case holds vacuously, since $\\mathbf{x}\\vee C=\\mathbf{x}$ . Below, we focus on the $m=1$ case, where there exists a unique $v\\in\\mathsf{v a l}(\\mathbf{x})$ such that $v\\leq C$ . Let $i\\in\\mathrm{argmin}(\\mathbf{x})$ . Note that we have $\\mathsf{i d}\\mathsf{x}(v,\\mathbf{x})=\\mathrm{argmin}(\\mathbf{x})$ . Using Equation (39) (Fact 1), we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle\\nabla f(\\mathbf{x}\\vee C)-\\nabla f(\\mathbf{x}),(\\mathbf{x}\\vee C)-\\mathbf{x}\\rangle\\ge0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Definition G.1. Given any set $S\\subseteq[n]$ , we let $\\chi s\\in\\mathbb{R}^{n}$ denote the characteristic vector on $S{\\mathrm{:}}\\ \\chi_{S}$ is the vector whose jth entry $i s=1$ if $j\\in S$ and $=0$ otherwise. ", "page_idx": 25}, {"type": "text", "text": "By construction, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n(\\mathbf{x}\\vee C)-\\mathbf{x}=(C-v)\\chi_{\\mathrm{id}\\times(v,\\mathbf{x})}=(C-v)\\chi_{\\mathrm{argmin}(\\mathbf{x})}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The $\\cdot\\frac{\\partial f}{\\partial x_{i}}(\\cdot)^{,}\\rangle$ notation for partial derivatives is a bit cumbersome. Instead, we will write \u201c $[\\nabla f(\\cdot)]_{i}$ \u201d from now on. By Equation (40) (Fact 2), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f(\\mathbf{x})]_{j}=[\\nabla f(\\mathbf{x})]_{j^{\\prime}},\\quad{\\mathrm{for~all~}}j,j^{\\prime}\\in{\\mathfrak{i}}\\mathsf{d}\\mathsf{x}(v,\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and likewise ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f({\\mathbf x}\\vee C)]_{j}=[\\nabla f({\\mathbf x}\\vee C)]_{j^{\\prime}},\\quad{\\mathrm{for}}\\;{\\mathrm{all}}\\;j,j^{\\prime}\\in{\\mathrm{id}}{\\times}(v,{\\mathbf x}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, by Equation (40), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle\\nabla f(\\mathbf{x}\\vee C)-\\nabla f(\\mathbf{x}),(\\mathbf{x}\\vee C)-\\mathbf{x}\\rangle=|\\mathrm{argmin}(\\mathbf{x})|\\cdot(C-v)([\\nabla f(\\mathbf{x}\\vee C)]_{i}-[\\nabla f(\\mathbf{x})]_{i}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, since $C\\,>\\,v$ and $|\\mathrm{argmin}(\\mathbf{x})|\\,>\\,0$ , we must have that $[\\nabla f(\\mathbf{x}\\vee C)]_{i}-[\\nabla f(\\mathbf{x})]_{i}\\,\\ge\\,0$ , as desired. This proves the base step. ", "page_idx": 25}, {"type": "text", "text": "Induction step: Suppose Lemma G.3 holds for every integer $m$ where $0\\,\\leq\\,m\\,<\\,n$ , we must show that Lemma G.3 also holds for $m+1$ . To this end, let $\\textbf{x}\\in\\mathbb{R}^{n}$ and $C\\,\\in\\,\\mathbb{R}$ be such that $|\\{v\\in\\mathsf{v a l}(\\mathbf{x}):v<C\\}|=m\\!+\\!1$ . Let $v_{1},\\ldots,v_{m+1}\\in\\mathbb{R}$ be all the elements of $\\{v\\in\\mathsf{v a l}(\\mathbf{x})\\mid v<C\\}$ enumerated in increasing order, i.e., $v_{1}<\\dots<v_{m+1}$ . ", "page_idx": 25}, {"type": "text", "text": "Note by construction, we have that $\\left\\{v\\in\\mathsf{v a l}(\\mathbf{x})\\mid v<v_{m+1}\\right\\}=\\left\\{v_{1},\\ldots,v_{m}\\right\\}$ and so we immediately get that $\\big|\\{v\\,\\in\\,\\mathsf{v a l}(\\mathbf{x})\\,\\mid\\,v\\,<\\,v_{m+1}\\big\\}\\big|\\,=\\,m$ . By the $m$ -th case of Lemma G.3 (i.e., the induction hypothesis) using $v_{m+1}$ as $C$ , we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f({\\bf x}\\vee v_{m+1})]_{i}\\ge[\\nabla f({\\bf x})]_{i}\\quad\\mathrm{for}\\;\\mathrm{any}\\;i\\in\\mathrm{argmin}({\\bf x}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Below fix some $i\\in\\mathrm{argmin}(\\mathbf{x})$ arbitrarily. Let $\\mathbf{x}^{\\prime}:=\\mathbf{x}\\vee v_{m+1}$ . We note that by construction, all the entries of $\\mathbf{x}^{\\prime}$ that are less than $C$ are set to equal to $v_{m+1}$ . In other words, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\{v\\in\\mathsf{v a l}(\\mathbf{x}^{\\prime}):v<C\\}=\\{v_{m+1}\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is a singleton set. Thus, by the $m=1$ case of Lemma G.3 applied to $\\mathbf{x}^{\\prime}$ , we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f({\\mathbf x}^{\\prime}\\vee C)]_{i^{\\prime}}\\ge[\\nabla f({\\mathbf x}^{\\prime})]_{i^{\\prime}},\\quad\\mathrm{for\\;any}\\;i^{\\prime}\\in\\mathrm{argmin}({\\mathbf x}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\operatorname{argmin}(\\mathbf{x}^{\\prime})\\,\\supseteq\\,\\operatorname{argmin}(\\mathbf{x})$ , we have that $i\\,\\in\\,\\mathrm{argmin}(\\mathbf{x}^{\\prime})$ as well (recall that $i$ was chosen earlier from $\\mathrm{argmin}(\\mathbf{x})$ arbitrarily). Thus the above inequality implies in particular that ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f({\\mathbf{x}}^{\\prime}\\vee C)]_{i}\\geq[\\nabla f({\\mathbf{x}}^{\\prime})]_{i}=[\\nabla f({\\mathbf{x}}\\vee v_{m+1})]_{i}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combined with Equation (41), we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f(\\mathbf{x}^{\\prime}\\vee C)]_{i}\\geq[\\nabla f(\\mathbf{x})]_{i}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we note that $\\mathbf{x}^{\\prime}\\lor C=(\\mathbf{x}\\lor v_{m+1})\\lor C=\\mathbf{x}\\lor C$ . Thus, the above implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f(\\mathbf{x}\\vee C)]_{i}\\ge[\\nabla f(\\mathbf{x})]_{i}\\quad{\\mathrm{for~any~}}i\\in\\operatorname{argmin}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "since the choice of $i\\in\\mathrm{argmin}(\\mathbf{x})$ was arbitrary. ", "page_idx": 25}, {"type": "text", "text": "G.2 Application to our setting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The condition $\\operatorname*{lim}_{t\\to\\infty}u_{j}^{t}=\\infty$ by definition means that for every real number $M\\in\\mathbb{R}$ , there exists $T$ such that for all $t\\,\\geq\\,T$ we have $u_{j}^{t}\\,>\\,M$ . Thus, suppose that there exists $j\\,\\in\\,[K-1]$ such that $\\operatorname*{lim}_{t\\to\\infty}u_{j}^{t}\\neq\\infty$ , then there exists a real number $M\\in\\mathbb{R}$ such that for all $T=1,2,\\ldots$ there exists some $t\\geq T$ such that $u_{j}^{t}\\leq M$ . Passing to a subsequence, we assume that $u_{j}^{t}\\leq M$ (and so $\\operatorname*{min}(\\mathbf{u}^{t})\\leq M)$ for all $t=1,2,\\ldots$ . Note that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\nabla\\psi(\\mathbf{u}^{t})=\\mathbf{0}}\\end{array}$ continues to hold. ", "page_idx": 25}, {"type": "text", "text": "Below, whenever we say \u201cfor all/every $t^{\\bullet}$ , we mean \u201cfor all/every $t=1,2,\\ldots^{,}$ . ", "page_idx": 25}, {"type": "text", "text": "Onto the proof. First recall the lower bound portion of the exponential tail property: ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\left[\\nabla\\psi(\\mathbf{u})\\right]_{i}\\geq c\\Big(1-\\sum_{j=1}^{K-1}\\exp(-u_{j})\\Big)\\exp(-u_{i}),\\quad\\mathrm{for~all~}i\\in[K-1].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\begin{array}{r}{C:=\\operatorname*{max}\\lbrace\\ 2|u_{-}|,\\ M,\\ -\\log(\\frac{1}{2(K-1)})\\ \\rbrace}\\end{array}$ and $\\mathbf{v}^{t}:=\\mathbf{u}^{t}\\vee C$ for all $t$ . This choice of $C$ (and $\\mathbf{v}^{t}$ ) has the following consequences: ", "page_idx": 26}, {"type": "equation", "text": "$\\begin{array}{r}{\\Big(1-\\sum_{j=1}^{K-1}\\exp(-v_{j})\\Big)\\geq\\frac{1}{2}.}\\end{array}$ ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. If $\\mathbf{v}\\in\\mathbb{R}^{K-1}$ is such that $\\operatorname*{min}(\\mathbf{v})\\geq C$ , then we have by Equation (42) that ", "page_idx": 26}, {"type": "equation", "text": "$$\n-[\\nabla\\psi(\\mathbf{v})]_{i}\\ge\\frac12c\\exp(-v_{i}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "3. $\\operatorname*{min}(\\mathbf{u}^{t})\\leq C$ . This is true since $\\mathrm{min}({\\bf u}^{t})\\le M$ . ", "page_idx": 26}, {"type": "text", "text": "4. Choose $i_{t}\\in\\mathrm{argmin}(\\mathbf{u}^{t})$ for every $t$ . Then $v_{i_{t}}^{t}=\\mathrm{min}(\\mathbf{v}^{t})=C$ for every $t$ . This is simply a consequence of the fact that $\\mathrm{argmin}(\\mathbf{u}^{t})\\subseteq\\mathrm{argmin}(\\mathbf{v}^{t})$ . ", "page_idx": 26}, {"type": "text", "text": "5. We have $\\begin{array}{r l r}{\\operatorname*{lim}_{t\\rightarrow\\infty}[\\nabla\\psi(\\mathbf{u}^{t})]_{i_{t}}}&{{}=}&{0}\\end{array}$ . This follows from the assumption that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\nabla\\psi(\\mathbf{u}^{t})=\\mathbf{0}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "By Theorem G.2, we have for every $t$ that ", "page_idx": 26}, {"type": "equation", "text": "$$\n[\\nabla\\psi(\\mathbf{u}^{t})]_{i_{t}}\\leq[\\nabla\\psi(\\mathbf{v}^{t})]_{i_{t}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By plugging in $\\mathbf{v}^{t}$ for $\\mathbf{u}$ in Equation (43) above and the fact that $v_{i_{t}}^{t}=C$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\nabla\\psi({\\mathbf v}^{t})]_{i_{t}}\\le-\\frac{1}{2}c\\exp(-v_{i_{t}})=-\\frac{1}{2}c\\exp(-C)<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $-{\\textstyle{\\frac{1}{2}}}c\\exp(-C)$ is a constant that doesn\u2019t depend on $t$ , it is impossible for $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}[\\nabla\\psi(\\mathbf{v}^{t})]_{i_{t}}=}\\end{array}$ 0 to hold. This proves Proposition G.1. ", "page_idx": 26}, {"type": "text", "text": "H On the existence of w\u02dc ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The goal of this section is to explain the challenge and the current gap in the proof of the existence of w\u02dc that satisfies the condition in Equation (8) for almost all linearly separable datasets. To this end, recall Equation (4), the hard-margin SVM formulated as a constrained optimization: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{w}}=\\operatorname{argmin}_{\\mathbf{w}}\\frac{1}{2}\\|\\mathbf{w}\\|^{2}\\,\\,\\mathrm{s.t.}\\,\\forall n,\\forall k\\neq y_{n}:\\mathbf{w}_{y_{n}}^{\\top}\\mathbf{x}_{n}\\geq\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{n}+1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, recall that $\\scriptstyle S_{k}$ , the set of support vectors for each $k\\in[K]$ , is defined by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal S}_{k}:=\\{n:(\\hat{\\bf w}_{y_{n}}-\\hat{\\bf w}_{k})^{\\top}{\\bf x}_{n}=1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The Lagrangian of the objective in Equation (44) is ", "page_idx": 26}, {"type": "equation", "text": "$$\nL(\\mathbf{w},\\alpha)=\\frac{1}{2}\\sum_{r=1}^{K}\\left\\|\\mathbf{w}_{r}\\right\\|^{2}+\\sum_{n=1}^{N}\\sum_{r\\neq y_{n}}\\alpha_{n,r}\\left(\\mathbf{w}_{y_{n}}-\\mathbf{w}_{r}\\right)^{\\top}\\mathbf{x}_{n}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\alpha_{n,r}$ are the dual variables. Let $\\delta_{i,j}$ denote the Kronecker delta, i.e., $\\delta_{i,j}\\,=\\,1\\$ if $i=j$ and $\\delta_{i,j}=0$ otherwise. Taking the gradient of $\\bar{\\mathcal{L}}(\\mathbf{w},\\alpha)$ with respect to $\\mathbf{w}_{k}$ , we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{w}_{k}+\\sum_{n=1}^{N}\\sum_{r\\neq y_{n}}\\alpha_{n,k}\\left(\\delta_{r,y_{n}}-\\delta_{r,k}\\right)\\mathbf{x}_{n}=\\mathbf{w}_{k}+\\sum_{n=1}^{N}\\left(\\delta_{k,y_{n}}\\sum_{r\\neq y_{n}}\\alpha_{n,r}-\\alpha_{n,k}\\right)\\mathbf{x}_{n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So the KKT conditions satisfied by a stationary point w\u02c6 (hence globally optimal for Equation (44)) are ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall k\\in[K]:\\hat{\\mathbf{w}}_{k}=\\displaystyle\\sum_{n=1}^{N}\\left(\\alpha_{n,k}-\\delta_{k,y_{n}}\\displaystyle\\sum_{r\\neq k}\\alpha_{n,r}\\right)\\mathbf{x}_{n}}\\\\ &{\\forall k\\in[K]:\\forall n:\\mathrm{one~of~the~following~holds}\\left\\{\\alpha_{n,k}\\geq0\\;\\mathrm{~and~}\\;\\left(\\hat{\\mathbf{w}}_{y_{n}}-\\hat{\\mathbf{w}}_{k}\\right)^{\\top}\\mathbf{x}_{n}=1\\right\\}}\\\\ &{\\forall k\\in[K]:\\forall n:\\mathrm{one~of~the~following~holds}\\left\\{\\alpha_{n,k}=0\\;\\mathrm{~and~}\\;\\left(\\hat{\\mathbf{w}}_{y_{n}}-\\hat{\\mathbf{w}}_{k}\\right)^{\\top}\\mathbf{x}_{n}>1\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where Eqn. (47) (the second line) above is the complementary slackness condition. ", "page_idx": 27}, {"type": "text", "text": "The goal of this section is to prove the following result regarding the existence of $\\tilde{\\mathbf{w}}$ that satisfies the condition in Equation (8), which we restate below: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\forall n\\in\\mathcal{S}_{k}:\\,\\eta\\exp\\left(-\\mathbf{x}_{n}^{\\top}\\left(\\tilde{\\mathbf{w}}_{y_{n}}-\\tilde{\\mathbf{w}}_{k}\\right)\\right)=\\alpha_{n,k}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Conjecture H.1. For almost all linearly separable multiclass datasets, Assumption 4.1 holds, i.e., Eqn. (48) has a solution w\u02dc. ", "page_idx": 27}, {"type": "text", "text": "Below, we use the word \u201cgenerically\u201d to mean \u201cfor linearly separable datasets outside of a set of Lebesgue measure zero\u201d. In order for (48) to have a solution in w\u02dc generically, two conditions need to hold (generically). ", "page_idx": 27}, {"type": "text", "text": "Condition 1. $\\alpha_{n,k}>0$ for all $k$ and $n$ such that $\\begin{array}{r}{n\\in\\mathcal{S}_{k}:=\\{n:(\\hat{\\mathbf{w}}_{y_{n}}-\\hat{\\mathbf{w}}_{k})^{\\top}\\mathbf{x}_{n}=1\\}.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Condition 1 is already nontrivial and a gap in proving the Conjecture, as we will see below. For the sake of explaining Condition 2 below, let us assume Condition 1 holds. Then we can rewrite (48) as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\forall n\\in\\mathcal{S}_{k}:\\mathbf{x}_{n}^{\\top}\\left(\\tilde{\\mathbf{w}}_{y_{n}}-\\tilde{\\mathbf{w}}_{k}\\right)=\\log\\left(\\frac{\\eta}{\\alpha_{n,k}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Define the vector $\\mathbf{m}_{n,k}$ obtained by taking the difference between the $k$ -th and $y_{n}$ -th elementary basis vector in $\\mathbb{R}^{K}$ , i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{m}_{n,k}:=\\mathbf{e}_{k}-\\mathbf{e}_{y_{n}}\\in\\mathbb{R}^{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we can further rewrite (49) as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\forall n\\in\\mathcal{S}_{k}:(\\mathbf{m}_{n,k}\\otimes\\mathbf{x}_{n})^{\\top}\\tilde{\\mathbf{w}}=\\log\\left(\\frac{\\eta}{\\alpha_{n,k}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It is more convenient to pool all the class-specific support vectors $\\scriptstyle S_{k}$ into a single set: $\\begin{array}{r l}{S\\lefteqn{\\triangleq}}&{{}}\\end{array}$ $\\left\\{(n,k):\\left({\\hat{\\mathbf{w}}}_{y_{n}}-{\\hat{\\mathbf{w}}}_{k}\\right)^{\\top}\\mathbf{x}_{n}=1\\right\\}$ . For readability, we linearly order the tuples in $\\boldsymbol{S}$ , i.e., we assign to each $(n,k)\\in S$ a unique index $i\\in\\{1,...,|S|\\}$ . In other words, we define $n(1),\\dots,n(|S|)$ and $k(1),\\ldots,k(|S|)$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\nS=\\{(n(1),k(1)),\\,(n(2),k(2)),\\,\\dots,\\,(n(|S|),k(|S|))\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To reduce notational clutter in the subscript, define $\\overline{{\\mathbf{x}}}_{i}\\triangleq\\mathbf{x}_{n(i)}$ and $\\overline{{\\mathbf{m}}}_{i}\\triangleq\\mathbf{m}_{n(i),k(i)}$ . Finally, define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbf{M}}}\\triangleq\\left[\\overline{{\\mathbf{m}}}_{1},\\hdots,\\overline{{\\mathbf{m}}}_{|S|}\\right]\\mathbb{R}^{K\\times|S|},\\ \\overline{{\\mathbf{X}}}\\triangleq\\left[\\overline{{\\mathbf{x}}}_{1},\\hdots,\\overline{{\\mathbf{x}}}_{|S|}\\right]\\in\\mathbb{R}^{d\\times|S|},\\ \\mathrm{and}\\ \\mathbf{G}\\triangleq\\left(\\mathbf{M}\\circ\\mathbf{X}\\right)\\in\\mathbb{R}^{d K\\times|S|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\circ$ denoting the Khatri-Rao product, which is, by definition, the matrix obtained by taking the Kronecker product of corresponding columns [Khatri and Rao, 1968]. Note that the Khatri-Rao product is only defined for two matrices that have the same number of columns. See Liu [1999] for a reference. We now state ", "page_idx": 27}, {"type": "text", "text": "Condition 2. $\\mathrm{rank}(\\mathbf{G})=|S|$ generically. ", "page_idx": 27}, {"type": "text", "text": "Note that given Condition 2, Eqn. (50) has a solution in $\\tilde{\\mathbf{w}}$ , while Condition 1 is necessary for the logarithm in (50) to be valid in the first place. ", "page_idx": 27}, {"type": "text", "text": "The challenge in proving Condition 2 in the multiclass case is that the column vectors of $\\overline{{\\mathbf{X}}}$ may have repeats, i.e., it is possible for $n(i)=n(i^{\\prime})$ when $i\\neq i^{\\prime}$ . It is easy to generate synthetic linearly ", "page_idx": 27}, {"type": "image", "img_path": "JlWn80mTJi/tmp/d89cd9924fa60b282ce23be7a4f5016be516572a836c2056e8e719e723f204b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 2: Small simulation with $N=10$ , $d=2$ and $K=3$ . The loss used is the \u201cPairLogLoss\u201d. Top row. Decision regions of classifiers along the gradient path $\\mathbf{w}(t)$ at $t=100$ , 1000, and 100000, respectively from left to right. Bottom row. Decision regions of the hard-margin multiclass SVM. Note that most of the progress is made between iterations 100 and 1000. ", "page_idx": 28}, {"type": "text", "text": "separable multiclass datasets satisfying this condition. Nonetheless, we observe that even in such a case, the matrix $\\mathbf{G}$ has rank $|{\\mathcal{S}}|$ , i.e., Condition 2 holds. We verify this experimentally in the Python notebook checking_conjecture_in_Appendix_H.ipynb available at ", "page_idx": 28}, {"type": "text", "text": "https://github.com/YutongWangML/neurips2024-multiclass-IR-figures ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the binary case, linear classifiers are parametrized simply as a single vector, rather than the more cumbersome one-vector-per-class parametrization. Under the one-vector parametrization, the $\\overline{{\\bf M}}$ matrix becomes a 1-by- $|{\\cal S}|$ matrix consistings of only $\\pm1$ \u2019s, and $\\mathbf{G}$ reduces to $\\overline{{\\mathbf{X}}}$ . Moreover $\\overline{{\\mathbf{X}}}$ has no repeats. Thus, Condition 2 holds trivially. In both the multiclass and binary settings, given Condition 2, the proof for Condition 1 can proceed exactly as in Lemma 12 from Soudry et al. [2018] where their $\\mathbf{X}_{\\mathcal{S}}$ is replaced by our $\\mathbf{G}$ . ", "page_idx": 28}, {"type": "text", "text": "I Additional Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We provide additional experimental support for our main theoretical result for the PairLogLoss [Wang et al., 2022]. Code for recreating the figures can be found at ", "page_idx": 28}, {"type": "text", "text": "https://github.com/YutongWangML/neurips2024-multiclass-IR-figures The code can be ran on Google Colab with a CPU runtime in under one hour. ", "page_idx": 28}, {"type": "image", "img_path": "JlWn80mTJi/tmp/1b087097f08afcf3c957ccdb5ceaa9fe0f6bb3eca3f6d56a6804cc4bade441a0.jpg", "img_caption": ["Figure 3: Large simulations with $N=100$ , $d=10$ and $K=3$ . The loss used is the \u201cPairLogLoss\u201d. The curves are 10 independent runs with randomly sampled data and random initialization for gradient descent over 100000 iterations. Note that the convergence in direction of the gradient descent iterates to the hard-margin SVM slows down in log-log space. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our abstract clearly introduces the problem considered (implicit bias), identifies a gap in research (few multiclass results, which themselves are only for cross-entropy), and states our contributions (new ET property and implicit bias theorem for new losses). For the sake of brevity we do not state additional assumptions on the loss apart from ET (which we state later in the main text, i.e. smoothness, strictly decreasing, non-negative), because the ET property is a novel contribution and deserves to appear in the abstract. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have a separate section where we talk about our work\u2019s limitations. We highlight 2 natural questions one can ask: non-ET loss characterization, and non-asymptotic analysis (answering whether overftiting occurs after some finite number of (S)GD timesteps). Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All assumptions are stated clearly in bold at the beginning of section 3, and also re-iterated multiple times throughout the paper, The assumption on the learning rate being sufficiently small is mentioned in the theorem statement. Partial proofs are provided in the main text because they highlight salient features of our techniques (namely, simple generalization of binary proof techniques to multiclass). Complete proofs are provided in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We use a simple synthetic setup which can be reproduced easily with Google Colab. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: In Appendices H and I, we include a link to our GitHub repo which contains the complete code to reproduce the experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 31}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All details can be found in the GitHub repository. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our experiments are used to illustrate the main theoretical result, which is of mathematical nature. All experiments support the convergence behavior that we analyzed. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, the experiments can be run with a Google Colab CPU runtime as mentioned in the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]