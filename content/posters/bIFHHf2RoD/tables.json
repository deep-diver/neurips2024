[{"figure_path": "bIFHHf2RoD/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with the latest cultural specific LLMs.", "description": "This table compares the performance of the CulturePark model with other state-of-the-art cultural specific LLMs on content moderation tasks for Arabic, Chinese, and Korean cultures.  The results show that the CulturePark model outperforms other LLMs on average F1 score across various hate speech detection and offensive language identification tasks.  Specific tasks and baselines are listed for each culture.", "section": "4.1 Evaluation on content moderation tasks"}, {"figure_path": "bIFHHf2RoD/tables/tables_6_2.jpg", "caption": "Table 2: Results on ablation study of data generation and refinement.", "description": "This table presents the ablation study results on four different cultures (Arabic, Bengali, Chinese, and Portuguese).  It shows the impact of each step in the CulturePark data refinement process (Generate, Generate+Verify, Generate+Verify+Diversify) on the performance of the model.  The results demonstrate that each stage of refinement contributes to improved model performance.", "section": "3.2 Data refinement and fine-tuning"}, {"figure_path": "bIFHHf2RoD/tables/tables_7_1.jpg", "caption": "Table 3: Results on situated learning.", "description": "This table presents the results of a situated learning experiment for cultural education. Participants learned with either the CulturePark models or GPT-4, and then took a cultural understanding exam (VSM 2013). The table shows the Euclidean distance between the ground truth and participants' answers, representing learning performance.  A lower distance indicates better performance.  User experience is also measured on a scale of 1 to 5, with higher numbers indicating greater satisfaction. The results show that participants learning with the CulturePark models performed better and reported higher satisfaction than those learning with GPT-4.", "section": "4.3 Evaluation in situated learning for cultural education"}, {"figure_path": "bIFHHf2RoD/tables/tables_17_1.jpg", "caption": "Table 4: Interesting observation in Cross-cultural Dialogues. \"Extend Rate\" represents the ability of entending the topic. \"Understanding / Others\" represents the ratio of cross-cultural understanding statements and others.", "description": "This table presents the results of an analysis of cross-cultural dialogues generated by CulturePark.  It shows, for each of eight cultures, the percentage of dialogues that successfully extended the conversation topic beyond the initial prompt (Extend Rate) and the percentage of statements within those dialogues that demonstrated cross-cultural understanding (Understanding Ratio).  The average Extend Rate and Understanding Ratio across all eight cultures are also provided.", "section": "B.2 The dialogue dataset"}, {"figure_path": "bIFHHf2RoD/tables/tables_18_1.jpg", "caption": "Table 5: A brief introduction of the 8 evaluation tasks and 41 datasets. We list both the name and the size of test sets. For instance, \u201cOSACT5(2541) [Mubarak et al., 2022]\u201d denotes that there are 2541 test samples in the dataset OSACT5.", "description": "This table presents a summary of the eight content moderation tasks and the corresponding datasets used in the paper's experiments.  For each culture (Arabic, Bengali, Chinese, German, Korean, Portuguese, Spanish, and Turkish), the table lists the specific content moderation tasks (e.g., offensive language detection, hate speech detection, spam detection) and the datasets used for evaluation. The number of samples in each dataset is also provided in parentheses. The table is crucial for understanding the scope and scale of the experiments conducted in the paper.", "section": "C Details on the test sets"}, {"figure_path": "bIFHHf2RoD/tables/tables_22_1.jpg", "caption": "Table 6: Details on Fine-tuning GPT-3.5-turbo using OpenAI API.", "description": "This table shows the number of epochs used for fine-tuning the GPT-3.5-turbo model for each of the eight different cultures in the CulturePark project.  The epochs refer to the number of training iterations performed on the model. The variations in epoch numbers suggest that the optimal training duration varied depending on the specific language and cultural data used.", "section": "3.3 Design"}, {"figure_path": "bIFHHf2RoD/tables/tables_22_2.jpg", "caption": "Table 7: Information of agents in CulturePark", "description": "This table shows the names of the agents used in the CulturePark framework, categorized by gender and culture. The agents represent diverse cultural backgrounds to facilitate cross-cultural communication and data generation in the CulturePark model.", "section": "3.1 Design"}, {"figure_path": "bIFHHf2RoD/tables/tables_22_3.jpg", "caption": "Table 8: Number of generated data for different cultures", "description": "This table shows the number of seed data and generated data used for fine-tuning in each of the eight cultures included in the CulturePark experiment.  The seed data are the initial questions used to spark the conversations between the LLMs, while the generated data represent the resulting dialogues produced through the multi-agent communication process.  The table highlights the relatively even distribution of data across the different cultures.", "section": "3.2 Data refinement and fine-tuning"}, {"figure_path": "bIFHHf2RoD/tables/tables_23_1.jpg", "caption": "Table 7: Information of agents in CulturePark", "description": "This table shows the names and genders of the eight agents used in the CulturePark framework.  The agents represent different cultures and genders, with each one representing a distinct cultural background and communication style.", "section": "3 CulturePark"}, {"figure_path": "bIFHHf2RoD/tables/tables_24_1.jpg", "caption": "Table 5: A brief introduction of the 8 evaluation tasks and 41 datasets. We list both the name and the size of test sets. For instance, \u201cOSACT5(2541) [Mubarak et al., 2022]\u201d denotes that there are 2541 test samples in the dataset OSACT5.", "description": "This table presents a summary of the datasets used to evaluate the performance of the CulturePark models on eight different cultures: Arabic, Bengali, Chinese, German, Korean, Portuguese, Spanish, and Turkish.  For each culture, the table lists the specific content moderation tasks (e.g., hate speech, offensive language detection) and the corresponding datasets utilized for evaluation, along with the number of samples in each dataset. The table provides a concise overview of the evaluation resources and their scale for each target culture.", "section": "C Details on the test sets"}]