[{"figure_path": "rQYyWGYuzK/figures/figures_8_1.jpg", "caption": "Figure 1: CNN prediction on ReLU subset of Small CNN Zoo with different ranges of augmentations. Here the x-axis is the augment upper scale, presented in log scale. The metric used is Kendall's T.", "description": "This figure shows the performance of Monomial-NFN and other baseline models (NP, HNP, and STATNN) on the task of predicting CNN generalization using the ReLU subset of the Small CNN Zoo dataset.  The x-axis represents the upper bound of scaling augmentation applied to the weights (log scale), and the y-axis represents Kendall's Tau correlation, a measure of rank correlation between predicted and actual generalization performance. The figure demonstrates that the Monomial-NFN model maintains relatively stable performance across different levels of augmentation, while the other models show a significant drop in performance with increased augmentation.", "section": "6.1 Predicting CNN Generalization from Weights"}, {"figure_path": "rQYyWGYuzK/figures/figures_30_1.jpg", "caption": "Figure 2: Random qualitative samples of INR editing behavior on the Dilate (MNIST) and Contrast (CIFAR-10) editing tasks.", "description": "This figure shows the results of different INR editing methods on MNIST and CIFAR-10 datasets.  The \"Ground truth\" row displays the expected outcome of the image editing operations (dilation for MNIST digits and contrast enhancement for CIFAR-10 images). The subsequent rows show the results produced by different methods: the original Implicit Neural Representation (INR), a Multilayer Perceptron (MLP), the HNP and NP methods from previous work, and the Monomial-NFN method proposed in the paper. The images in the figure provide a visual comparison of the performance of each method in achieving the desired image editing effect.", "section": "6.2 Classifying implicit neural representations of images"}]