[{"figure_path": "NTkYSWnVjl/figures/figures_1_1.jpg", "caption": "Figure 1: Adversarial examples generated by RFPAR. The first column represents images from ImageNet (image classification), the second column from MS-COCO (object detection), and the third column from Argoverse (object detection). Each row represents a different condition: the first row shows clean images, the second row shows adversarially perturbed images, and the third row shows the perturbation levels with the ratio of attacked pixels to total pixels. Labels in the images indicate detected objects or classifications, such as \"Cock\" in ImageNet, \"2 Objects\" in MS-COCO, and \"5 Objects\" in Argoverse. In the adversarial row, labels are altered due to perturbations, resulting in misclassifications or undetected objects, such as \"Coil\" instead of \"Cock\" in ImageNet and no objects detected in MS-COCO and Argoverse. The perturbation row indicates the percentage of pixels attacked in the image. The percentages were 0.004% for ImageNet, 0.027% for MS-COCO, and 0.114% for Argoverse.", "description": "This figure displays example results of the RFPAR attack on three different datasets: ImageNet, MS-COCO, and Argoverse.  Each dataset is represented by a column. Each row shows clean images, the adversarial examples generated by RFPAR, and the percentage of pixels modified. The results illustrate that RFPAR can effectively perturb images to cause misclassifications in image classification and object removal in object detection, with only a small percentage of pixels altered.", "section": "1 Introduction"}, {"figure_path": "NTkYSWnVjl/figures/figures_2_1.jpg", "caption": "Figure 2: The model architecture of RFPAR: the Remember and Forget process. During the Remember process, the RL model generates perturbed images and corresponding rewards. Memory compares these with previously stored values and retains only the highest reward and its associated image. Once the rewards converge to a certain value, the Forget process starts and resets the RL agent and memory, then reintroduces the perturbed images that gained the highest reward to the Remember process. The process continues until an adversarial image is generated or a predefined number of cycles is reached, at which point it terminates.", "description": "This figure illustrates the workflow of the RFPAR attack, which consists of two main processes: Remember and Forget. The Remember process uses reinforcement learning to iteratively generate and refine adversarial images by maximizing rewards. The Forget process resets the model and memory, leveraging the best-performing adversarial image from the Remember process to start a new iteration. This cycle repeats until convergence or a maximum number of iterations is reached.", "section": "2 Remember and Forget Pixel Attack Using Reinforcement Learning"}, {"figure_path": "NTkYSWnVjl/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation study. The x and y axes show different victim models and the attack success rate, respectively. The notation 1 signifies the inclusion of the initialization step in the Forget process, and M denotes that the Remember process incorporates memory.", "description": "This ablation study analyzes the impact of Initialization (I) and Memory (M) on the RFPAR model's performance.  The four bars for each model represent: the baseline RFPAR; RFPAR without initialization (I); RFPAR without memory (M); and RFPAR with both initialization and memory. The results show that both Initialization and Memory significantly improve the attack success rate, indicating that the RL agent benefits from both resetting and storing high-reward image information.", "section": "Ablation study"}, {"figure_path": "NTkYSWnVjl/figures/figures_14_1.jpg", "caption": "Figure 4: Adversarial examples generated by RFPAR on the ImageNet dataset. The \"Original Image\" is the original unaltered image, the \"Delta\" represents the difference between the Original Image and the Adversarial Image, and the \"Adversarial Image\" is the image with the altered prediction. The predicted labels are shown below the Original Image and the Adversarial Image.", "description": "This figure shows several examples of adversarial attacks generated by the RFPAR method on the ImageNet dataset. Each row presents a clean image, the difference between the clean image and the adversarial image (delta), and the resulting adversarial image. The labels predicted by the model are shown below each image, demonstrating the impact of the adversarial attacks.", "section": "C Experimental Results on Image Classification"}, {"figure_path": "NTkYSWnVjl/figures/figures_15_1.jpg", "caption": "Figure 5: Adversarial examples generated by RFPAR on the MS-COCO dataset. The Original Image represents the unaltered image, and the Delta shows the difference between the Original Image and the Adversarial Image. The parameter  \u03b1 is a hyperparameter that determines the attack level; a higher value of \u03b1 attacks more pixels. We conducted experiments with \u03b1 ranging from 0.01 to 0.05. The Delta Image resulting from \u03b1 values of 0.01 to 0.05 is presented in columns 2 to 6, and the Adversarial Image generated from the same \u03b1 values is shown in columns 7 to 11. The Adversarial Image typically indicates an image with a changed prediction, but in this context, it also includes unsuccessful attacks. We present the results of Delta and Adversarial Images according to different values of \u03b1.", "description": "This figure shows the results of applying the RFPAR attack on the MS-COCO dataset for object detection. It demonstrates the impact of varying the attack intensity (controlled by parameter \u03b1) on both the generated perturbations (Delta Images) and the resulting adversarial images. The figure shows that as \u03b1 increases, more pixels are modified, leading to more significant changes in the detected objects.", "section": "D Experimental Results on Object Detection"}]