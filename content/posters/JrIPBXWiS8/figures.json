[{"figure_path": "JrIPBXWiS8/figures/figures_1_1.jpg", "caption": "Figure 1: The proposed Resfusion is a general framework for image restoration and can be easily expand to image generation (setting x0 = 0). We introduce the residual term (R = x0 - xT) into the forward process, redefine q(xt|xt\u22121) to q(xt|xt\u22121, R) (as shown by the orange arrow), and name this diffusion process as resnoise diffusion. Through employing a novel technique called \"smooth equivalence transformation\", we can directly use the degraded image x0 to obtain xT (as shown by the blue arrow). We bridge the gap between the input image and ground truth, unifying the training and inference processes.", "description": "This figure illustrates the Resfusion framework, which incorporates residual noise into the forward diffusion process, allowing the reverse process to start directly from the noisy degraded image. It highlights the smooth equivalence transformation that enables the use of the degraded image as input and unifies the training and inference processes.", "section": "1 Introduction"}, {"figure_path": "JrIPBXWiS8/figures/figures_3_1.jpg", "caption": "Figure 2: The working principle of Resfusion. x0 represents the distribution of the ground truth, while x0 represents the distribution of the degraded images. x0 - x0 represents the gap between them, defined as the residual term R in Eq. (1). Resfusion does not explicitly guide x0 to x0. Instead, it implicitly learns the distribution of R by doing resnoise-diffusion reverse process from xt to x0. The resnoise-diffusion reverse process can be imagined as doing diffusion reverse process from R + \u2208 to x0 (as shown by the violet arrow), guiding xt gradually towards x0 along this direction. Following the principles of similar triangles, the coefficient of R at step t is computed as 1 - \u221aat. At any step t during the training process, xt can be calculated based on x0 and R through Eq. (4).", "description": "This figure illustrates the core idea of Resfusion. It shows how the model bridges the gap between degraded images and ground truth by introducing a residual term into the forward diffusion process. The reverse process starts from the noisy degraded image and gradually approaches the ground truth by removing resnoise, a weighted sum of noise and residual terms.  The optimal acceleration step is determined through a smooth equivalence transformation, ensuring efficient sampling.", "section": "2 Methodology"}, {"figure_path": "JrIPBXWiS8/figures/figures_4_1.jpg", "caption": "Figure 3: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset.", "description": "This figure compares the results of several shadow removal methods on the ISTD dataset.  It shows the input image, the results from DSC (2019), DHAN (2020), DMTN (2023), and the proposed Resfusion method, along with the ground truth image.  The comparison highlights visual differences between the various methods in terms of shadow removal and overall image quality.  Red boxes show specific regions where differences are particularly noticeable.", "section": "3 Experiments"}, {"figure_path": "JrIPBXWiS8/figures/figures_6_1.jpg", "caption": "Figure 5: The analysis of the residual term and the noise term on the LOL dataset. Only removing noise will reconstruct the details of the degraded image without causing any semantic shift. Only removing residual can only accomplish the semantic shift (from low-light to normal-light) without reconstructing the details. Removing resnoise can achieve both the semantic shift and the detail reconstruction.", "description": "This figure demonstrates the individual contributions of noise and residual terms in the Resfusion model for low-light image enhancement. By selectively removing either noise or the residual component, or both, it illustrates that the noise term primarily recovers image details, while the residual term handles the semantic shift (transition from low-light to well-lit conditions). The combination of both (removing the resnoise) achieves both detail recovery and semantic correction.", "section": "4 Ablation Study"}, {"figure_path": "JrIPBXWiS8/figures/figures_7_1.jpg", "caption": "Figure 2: The working principle of Resfusion. x0 represents the distribution of the ground truth, while x0 represents the distribution of the degraded images. x0 - x0 represents the gap between them, defined as the residual term R in Eq. (1). Resfusion does not explicitly guide x0 to x0. Instead, it implicitly learns the distribution of R by doing resnoise-diffusion reverse process from xt to x0. The resnoise-diffusion reverse process can be imagined as doing diffusion reverse process from R + \u2208 to x0 (as shown by the violet arrow), guiding xt gradually towards x0 along this direction. Following the principles of similar triangles, the coefficient of R at step t is computed as 1 - \u221aat. At any step t during the training process, xt can be calculated based on x0 and R through Eq. (4).", "description": "This figure illustrates the working principle of the Resfusion model. It shows how the model bridges the gap between degraded input images and ground truth by introducing a residual term and employing a resnoise diffusion reverse process. The figure highlights the relationship between the residual term, noise term and optimal acceleration step, demonstrating the key aspects of Resfusion.", "section": "2 Methodology"}, {"figure_path": "JrIPBXWiS8/figures/figures_8_1.jpg", "caption": "Figure 7: Visual comparisons between DDPM and Resfusion on the CIFAR10 (32 \u00d7 32) dataset. We do not cherry-pick any results. With the same sampling steps, Resfusion outperforms DDPM in semantic generation and detail reconstruction.", "description": "This figure compares the image generation quality of DDPM and Resfusion on the CIFAR-10 dataset using different numbers of sampling steps (10, 20, 50, and 100).  It visually demonstrates that Resfusion produces higher-quality images than DDPM at all sampling step counts, showcasing improved semantic generation and detail reconstruction.", "section": "5 Discussion"}, {"figure_path": "JrIPBXWiS8/figures/figures_9_1.jpg", "caption": "Figure 8: Visualization of the five sampling steps, where the blue arrow represents the smooth equivalence transformation, and the red box represents the resnoise-diffusion reverse process. We select the LOL web Test dataset (which does not have ground truth) and the Raindrop-B test dataset (which is much more challenging than Raindrop-A) to showcase the effects of low-light enhancement and deraining. We directly use the pretrained models on LOL dataset and Raindrop dataset, demonstrating the strong robustness of Resfusion.", "description": "This figure visualizes the five sampling steps of the Resfusion model on the LOL web Test and Raindrop-B test datasets.  The blue arrow indicates the smooth equivalence transformation, while the red boxes highlight the resnoise-diffusion reverse process.  The figure showcases the model's ability to enhance low-light images and remove rain from images, using pre-trained models without needing ground truth data for the LOL dataset.", "section": "6 Conclusion"}, {"figure_path": "JrIPBXWiS8/figures/figures_16_1.jpg", "caption": "Figure 9: Visual results for image translation on the CelebA-HQ dataset and AFHQV2 dataset. The images are presented in pairs, with the translated image on the left and the target image on the right. We showcase the visual results of Resfusion for image translation tasks \"Dog \u2192 Cat\", \"Male \u2192 Cat\", \"Male \u2192 Female\", and \"Female \u2192 Male\".", "description": "This figure shows the results of image translation experiments using the Resfusion model.  Four different translation tasks are presented: Dog to Cat, Male to Cat, Male to Female, and Female to Male. For each task, multiple pairs of input and output images are shown, illustrating the model's ability to translate between different image domains.", "section": "A.3 Image translation"}, {"figure_path": "JrIPBXWiS8/figures/figures_19_1.jpg", "caption": "Figure 10: (a) Visualization of the relationship between the error coefficient and T. Technically, we can use the Truncated Schedule to eliminate this error when T is small. (b) Visual comparisons between Truncated Schedule and Original Schedule under five sampling steps (T'/T = 5/12). In terms of visual perception, the absence of Truncated Schedule will lead to residual shadows.", "description": "This figure demonstrates the effect of the Truncated Schedule on the error coefficient and the visual results. The left panel shows that the error coefficient decreases exponentially with T; when T is small, the error is not negligible. The right panel shows that the Truncated Schedule effectively removes the shadows which are left by the Original Schedule.", "section": "A.7 Truncated schedule"}, {"figure_path": "JrIPBXWiS8/figures/figures_19_2.jpg", "caption": "Figure 2: The working principle of Resfusion. x0 represents the distribution of the ground truth, while x0 represents the distribution of the degraded images. x0 - x0 represents the gap between them, defined as the residual term R in Eq. (1). Resfusion does not explicitly guide x0 to x0. Instead, it implicitly learns the distribution of R by doing resnoise-diffusion reverse process from xt to x0. The resnoise-diffusion reverse process can be imagined as doing diffusion reverse process from R + e to x0 (as shown by the violet arrow), guiding xt gradually towards x0 along this direction. Following the principles of similar triangles, the coefficient of R at step t is computed as 1 - \u221aat. At any step t during the training process, xt can be calculated based on x0 and R through Eq. (4).", "description": "This figure illustrates the working principle of the Resfusion model for image restoration.  It shows how Resfusion bridges the gap between the degraded input image and the ground truth by introducing a residual term and using a resnoise diffusion process.  The process is shown as a reverse diffusion from a weighted sum of the residual and noise terms (resnoise) toward the ground truth.  The optimal acceleration step is also determined graphically.", "section": "2 Methodology"}, {"figure_path": "JrIPBXWiS8/figures/figures_20_1.jpg", "caption": "Figure 3: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset.", "description": "This figure presents a visual comparison of shadow removal techniques applied to images from the ISTD dataset.  It shows the input image, results from several different methods (DSC (2019), DHAN (2020), DMTN (2023), and Resfusion (ours)), and the ground truth. The red boxes highlight areas where the differences between the methods are most apparent, allowing for a visual assessment of each technique's performance in shadow removal.", "section": "3 Experiments"}, {"figure_path": "JrIPBXWiS8/figures/figures_21_1.jpg", "caption": "Figure 3: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset.", "description": "This figure shows a visual comparison of shadow removal results from different methods on the ISTD dataset.  The figure displays the input image, results from DSC (2019), DHAN (2020), DMTN (2023), and the proposed Resfusion method, along with the ground truth. Each row represents a different image from the dataset, allowing a side-by-side comparison of each algorithm's performance in removing shadows while preserving image details. This visual comparison helps to assess the effectiveness of each method in removing shadows and maintaining image quality.", "section": "3 Experiments"}, {"figure_path": "JrIPBXWiS8/figures/figures_21_2.jpg", "caption": "Figure 4: Visual comparisons of the restored results by different image restoration methods on the LOL dataset and the Raindrop dataset.", "description": "This figure shows a visual comparison of image restoration results using different methods on the LOL and Raindrop datasets.  The images demonstrate the performance of each method in handling low-light conditions and rain streaks, highlighting the differences in detail preservation, color accuracy, and overall image quality.", "section": "4 Ablation Study"}, {"figure_path": "JrIPBXWiS8/figures/figures_22_1.jpg", "caption": "Figure 17: Visual comparisons of the restored results by different deraining methods on the Raindrop dataset. (failure cases)", "description": "This figure shows a comparison of deraining results from different methods on the Raindrop dataset, highlighting failure cases.  The figure presents several image triplets, each consisting of the input rainy image, the results from AttentiveGAN (2018), RaindropAttn (2019), WeatherDiff (2023), and the proposed Resfusion method, alongside the ground truth images. The red boxes in the figure indicate specific regions where the different models exhibit failures in the deraining process, allowing a visual analysis of the strengths and weaknesses of each technique in challenging scenarios.", "section": "A.10 More results and failure cases"}, {"figure_path": "JrIPBXWiS8/figures/figures_22_2.jpg", "caption": "Figure 3: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset.", "description": "This figure showcases a visual comparison of shadow removal results obtained using different methods on the ISTD dataset.  It highlights the differences in performance between various approaches, demonstrating how Resfusion compares to existing state-of-the-art methods (DSC, DHAN, DMTN) in removing shadows from images while preserving image quality and details.", "section": "3 Experiments"}, {"figure_path": "JrIPBXWiS8/figures/figures_22_3.jpg", "caption": "Figure 15: More visual comparisons of the restored results by different deraining methods on the Raindrop dataset.", "description": "This figure compares the results of different deraining methods on the Raindrop dataset, showing the input image and the results obtained by AttentiveGAN, RaindropAttn, WeatherDiff, and Resfusion, along with the ground truth. It showcases the visual differences between the methods in terms of rain removal and detail preservation.", "section": "A.10 More results and failure cases"}]