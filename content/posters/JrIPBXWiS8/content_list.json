[{"type": "text", "text": "Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhenning Shi 1 Haoshuai Zheng1 Chen $\\mathbf{Xu^{1}}$ Changsheng Dong1 Bin Pan2 Xueshuo Xie3 Along He1\u2217 Tao Li1,3\u2217 Huazhu Fu4 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science, Nankai University 2School of Statistics and Data Science, Nankai University 3Haihe Lab of ITAI 4Institute of High Performance Computing, A\\*STAR ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, research on denoising diffusion models has expanded its application to the field of image restoration. Traditional diffusion-based image restoration methods utilize degraded images as conditional input to effectively guide the reverse generation process, without modifying the original denoising diffusion process. However, since the degraded images already include low-frequency information, starting from Gaussian white noise will result in increased sampling steps. We propose Resfusion, a general framework that incorporates the residual term into the diffusion forward process, starting the reverse process directly from the noisy degraded images. The form of our inference process is consistent with the DDPM. We introduced a weighted residual noise, named resnoise, as the prediction target and explicitly provide the quantitative relationship between the residual term and the noise term in resnoise. By leveraging a smooth equivalence transformation, Resfusion determine the optimal acceleration step and maintains the integrity of existing noise schedules, unifying the training and inference processes. The experimental results demonstrate that Resfusion exhibits competitive performance on ISTD dataset, LOL dataset and Raindrop dataset with only five sampling steps. Furthermore, Resfusion can be easily applied to image generation and emerges with strong versatility. Our code and model are available at https://github.com/nkicsl/Resfusion. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Denoising diffusion models [1, 2] have emerged as powerful and effective conditional generative models, demonstrating remarkable success in synthesizing high-fidelity data for image generation. Saharia et al. [3] proved that these generative processes can be applied to image restoration by feeding degraded images as conditional input into the score network. SNIPS [4] combines annealed Langevin dynamics and Newton\u2019s method to arrive at a posterior sampling algorithm, exploring the generative diffusion processes to solve the general linear inverse problems. Based on these, many diffusion-based models were adapted for downstream image restoration tasks [5, 6, 7, 8, 9, 10, 11]. For traditional diffusion-based models, the reverse process begins with Gaussian white noise, considering only the degraded images as the conditional input. This results in an increased number of sampling steps. Image restoration tasks often focus on restoring and editing specific high-frequency details while preserving crucial low-frequency information, such as the image structure. The degraded images used as conditional input inherently contain the low-frequency information. Therefore, initiating the reverse process from Gaussian white noise for image restoration tasks appears unnecessary and inefficient. ", "page_idx": 0}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/d07210f8902c3024e107797e3102ee095502fb98fa0f5dd7de999ec7bbdcb4c2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The proposed Resfusion is a general framework for image restoration and can be easily expand to image generation (setting $\\hat{x}_{0}=0$ ). We introduce the residual term $(R=\\hat{x}_{0}-x_{0})$ into the forward process, redefine $q(x_{t}|x_{t-1})$ to $q(x_{t}|x_{t-1},R)$ (as shown by the orange arrow), and name this diffusion process as resnoise diffusion. Through employing a novel technique called \"smooth equivalence transformation\", we can directly use the degraded image $\\scriptstyle{\\hat{x}}_{0}$ to obtain $x_{T^{\\prime}}$ (as shown by the blue arrow). We bridge the gap between the input image and ground truth, unifying the training and inference processes. ", "page_idx": 1}, {"type": "text", "text": "Consequently, some works have proposed to generate clean images directly from degraded images or noisy degraded images. InDI [12] restores clean images through the reverse process of direct iteration to degraded images; DDRM [13] reformulate the image restoration tasks as inverse problems when the mapping between clean and degraded images is available; IR-SDE [14] directly models the image degradation process using mean-reverting SDE (Stochastic Differential Equations); $\\mathrm{I}^{2}\\mathrm{SB}$ [15] constructs a Schr\u00f6dinger bridge between clean and degraded data distributions; Resshift [16] shifts the residual term from degraded low-resolution images to high-resolution images, performing the recovery in the latent space. Liu et al. [17] introduced the Residual Denoising Diffusion Models (RDDM), generalizing the diffusion process of InDI and $\\mathrm{I}^{2}\\mathrm{SB}$ . RDDM points out that co-learning the residual term and the noise term can effectively improve the model performance. However, RDDM has some limitations. Firstly, RDDM predicts the residual term and the noise term separately, without explicitly specifying their quantitative relationship. Secondly, due to its forward process adopting an accumulation strategy for the residual term and the noise term, its forward and reverse processes are inconsistent with the DDPM [1], which results in poor generalization and interpretability. Thirdly, RDDM requires the design of a complex noise schedule, as utilizing existing noise schedules would result in performance loss. ", "page_idx": 1}, {"type": "text", "text": "To solve the problems mentioned above, we propose Resfusion, a general framework for image restoration and can be easily expand to image generation. By introducing the residual term into the diffusion forward process, we bridge the gap between the input image and the ground truth, starting the reverse process directly from the noisy degraded images. We calculate the quantitative relationship between the residual term and the noise term, naming their weighted sum as the resnoise. Through the smooth equivalence transformation, we determine the optimal acceleration step and unify the training and inference processes. As a versatile methodology for image restoration, Resfusion does not require any physical prior knowledge. Resfusion allows for the direct use of existing noise schedules, and the image restoration process can be completed in just five sampling steps. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 First, by introducing the residual term into the diffusion forward process, our resnoisediffusion process starts the reverse process directly from the noisy degraded images, closing the gap between the degraded input and the ground truth.   \n\u2022 Second, we explicitly provide the quantitative relationship between the residual term and the noise term in the loss function, and name the weighted residual noise as resnoise. Through transforming the learning of the noise term into the resnoise term, the form of our reverse inference process is consistent with the DDPM.   \n\u2022 Third, through the smooth equivalence transformation in resnoise-diffusion process, we determine the optimal acceleration step and unify the training and inference processes. The optimal acceleration step is non-trivial where the posterior probability distribution is ", "page_idx": 1}, {"type": "text", "text": "equivalent to the prior probability distribution at this step. Moreover, we can directly use the existing noise schedule instead of redesigning the noise schedule. ", "page_idx": 2}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Learning the resnoise ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we denote the input degraded image and the ground truth as $\\scriptstyle{\\hat{x}}_{0}$ and $x_{0}$ . In order to extend the diffusion process of Denoising Diffusion Probabilistic Models (DDPM) [1] to image restoration, we define the residual term as Eq. (1). ", "page_idx": 2}, {"type": "equation", "text": "$$\nR=\\hat{x}_{0}-x_{0}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then the forward process can be defined as Eq. (2) and Eq. (3). We provide a detailed explanation for why we introduce the residual term $R$ to Eq. (3) in this way in Sec. 2.2 and Figure 2. Consistent with the definition in DDPM, we use the notation $\\beta_{t}=1-\\alpha_{t}$ and $\\begin{array}{r}{\\overline{{\\alpha}}_{t}=\\prod_{s=1}^{t}\\alpha_{s}}\\end{array}$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(x_{1:T}|x_{0},R)=\\prod_{t=1}^{T}q(x_{t}|x_{t-1},R)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\nq(x_{t}|x_{t-1},R)=N(x_{t};\\sqrt{\\alpha_{t}}x_{t-1}+(1-\\sqrt{\\alpha_{t}})R,(1-\\alpha_{t})I)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then the redefined forward process can be formalized as Eq. (4), where $\\epsilon$ represents the Gaussian white noise. ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=\\sqrt{\\alpha_{t}}x_{t-1}+(1-\\sqrt{\\alpha_{t}})R+\\sqrt{1-\\alpha_{t}}\\epsilon,\\quad\\epsilon\\sim N(0,I)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "According to Eq. (4), $x_{t}$ can be reparameterized as Eq. (5). ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=\\sqrt{\\overline{{\\alpha}}_{t}}x_{0}+(1-\\sqrt{\\overline{{\\alpha}}_{t}})R+\\sqrt{1-\\overline{{\\alpha}}_{t}}\\epsilon,\\quad\\epsilon\\sim N(0,I)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We can easily incorporate this forward process into the vanilla DDPM. We introduce a residual noise, named as resnoise (symbolized as res\u03f5), to describe the gap between the current estimate $x_{t}$ and the ground truth $x_{0}$ , and the term to be minimized can be formulated as Eq. (6). Detailed proof can be found in Appendix A.1. ", "page_idx": 2}, {"type": "equation", "text": "$$\nr e s\\epsilon=\\epsilon+\\frac{(1-\\sqrt{\\alpha_{t}})\\sqrt{1-\\overline{{\\alpha}}_{t}}}{\\beta_{t}}R,\\quad\\mathbb{E}_{x_{0},\\epsilon,t}[||r e s\\epsilon-r e s\\epsilon_{\\theta}(x_{t},t)||^{2}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Through this process, we transform the learning of $\\epsilon_{\\theta}(x_{t},t)$ into $r e s\\epsilon_{\\theta}(x_{t},t)$ . $\\epsilon_{\\theta}(x_{t},t)$ represents the noise of the noisy ground truth, while $r e s\\epsilon_{\\theta}(x_{t},t)$ represents the residual noise between the input degraded images and the ground truth. We name this process resnoise-diffusion. ", "page_idx": 2}, {"type": "text", "text": "2.2 Smooth equivalence transformation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "According to Eq. (5) and Eq. (1), we can derive Eq. (7). It is worth mentioning that $x_{T}$ is uncomputable because the ground truth $x_{0}$ is unavailable in Eq. (7) during the reverse process, so we can not initialize $x_{T}$ directly. ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=(2\\sqrt{\\overline{{\\alpha}}_{t}}-1)x_{0}+(1-\\sqrt{\\overline{{\\alpha}}_{t}})\\hat{x}_{0}+\\sqrt{1-\\overline{{\\alpha}}_{t}}\\epsilon,\\epsilon\\sim N(0,I)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Fortunately, the weighted coefficient of $x_{0}$ , which is $\\left(2\\sqrt{\\alpha}_{t}-1\\right)$ in Eq. (7), can be very close to zero. Since the input degraded image $\\scriptstyle{\\hat{x}}_{0}$ is available, \u221awe can find a time step $T^{\\prime}$ where $x_{T^{\\prime}}$ is computable. When $T$ is sufficiently large, the variation of ${\\sqrt{\\overline{{\\alpha}}_{t}}}(t\\leq T)$ with respect to time $t$ is smooth. We call this technique smooth equivalence transformation. Therefore, we can derive $T^{\\prime}$ as Eq. (8) and obtain $x_{T^{\\prime}}$ in Eq. (9) with a small bias. This bias can also be eliminated through the Truncated Schedule technique that we propose next. ", "page_idx": 2}, {"type": "equation", "text": "$$\nT^{\\prime}=\\arg\\operatorname*{min}_{i=1}^{T}{\\left|\\sqrt{\\overline{{\\alpha}}_{i}}-\\frac{1}{2}\\right|}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{T^{\\prime}}\\approx(1-\\sqrt{\\overline{{\\alpha}}_{T^{\\prime}}})\\hat{x}_{0}+\\sqrt{1-\\overline{{\\alpha}}_{T^{\\prime}}}\\epsilon\\approx\\sqrt{\\overline{{\\alpha}}_{T^{\\prime}}}\\hat{x}_{0}+\\sqrt{1-\\overline{{\\alpha}}_{T^{\\prime}}}\\epsilon\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thus we only need to minimize Eq. (6) when $t\\leq T^{\\prime}$ since $x_{T^{\\prime}}$ is available, as shown in Eq. (10). ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{\\theta}(x_{0})=\\int_{x_{1}:x_{T^{\\prime}}}P_{d a t a}(x_{T^{\\prime}})\\prod_{t=1}^{T^{\\prime}}P_{\\theta}(x_{t-1}|x_{t})d x_{1}:x_{T^{\\prime}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/ccdd08eb5e8113b5f3e4ff04881aab4af34e6c500c170751d1c678f3447d679c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The working principle of Resfusion. $x_{0}$ represents the distribution of the ground truth, while $\\scriptstyle{\\hat{x}}_{0}$ represents the distribution of the degraded images. $\\hat{x}_{0}-x_{0}$ represents the gap between them, defined as the residual term $R$ in Eq. (1). Resfusion does not explicitly guide $\\scriptstyle{\\hat{x}}_{0}$ to $x_{0}$ . Instead, it implicitly learns the distribution of $R$ by doing resnoise-diffusion reverse process from $x_{t}$ to $x_{0}$ . The resnoise-diffusion reverse process can be imagined as doing diffusion reverse process from $R+\\epsilon$ to $x_{0}$ (as shown by the violet arrow), guiding $x_{t}$ gradually towards $x_{0}$ along this dire\u221action. Following the principles of similar triangles, the coefficient of $R$ at step $t$ is computed as $1-{\\sqrt{\\alpha}}_{t}$ . At any step $t$ during the training process, $x_{t}$ can be calculated based on $x_{0}$ and $R$ through Eq. (4). ", "page_idx": 3}, {"type": "text", "text": "The resnoise-diffusion reverse process can be formulated as Eq. (11) and Eq. (12). Consistent with the definition in DDPM, the $\\Sigma_{\\theta}$ is taken fixed as $\\begin{array}{r}{\\widetilde{\\beta}_{t}=\\frac{1-\\overline{{\\alpha}}_{t-1}}{1-\\overline{{\\alpha}}_{t}}\\bar{\\beta}_{t}}\\end{array}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{\\theta}(x_{0:T^{\\prime}-1}|x_{T^{\\prime}})=\\prod_{t=1}^{T^{\\prime}}P_{\\theta}(x_{t-1}|x_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{\\theta}(x_{t-1}|x_{t})=N(x_{t-1};\\mu_{\\theta}(x_{t},t),\\Sigma_{\\theta}(x_{t},t)),\\quad P_{\\theta}(x_{0}|x_{1})=N(x_{0};\\mu_{\\theta}(x_{1},1))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The mean $\\mu_{\\theta}(x_{t},t)$ of resnoise-diffusion reverse process can be formalized as Eq. (13), which is demonstrated in Appendix A.1 from Eq. (19) to Eq. (23). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(x_{t},t)=\\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}r e s\\epsilon_{\\theta})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similar to previous work [5, 17], our method enhances the diffusion model by incorporating a conditioning function. This function integrates latent representation from both the current estimate $x_{t}$ and the input degraded image $\\scriptstyle{\\hat{x}}_{0}$ . Then Eq. (6) can be modified to Eq. (14). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{0},\\epsilon,t}[||r e s\\epsilon-r e s\\epsilon_{\\theta}((x_{t},\\hat{x}_{0},t)||^{2}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Just like the vanilla DDPM, Resfusion gradually fit the current estimate $x_{t}$ to the ground truth $x_{0}$ , implicitly reducing the residual term $R$ between $\\scriptstyle{\\hat{x}}_{0}$ and $x_{0}$ with the resnoise. When we define $\\hat{x}_{t}$ as Eq. (15) with the same Gaussian noise $\\epsilon$ in $x_{t}$ , $\\hat{x}_{T^{\\prime}}$ can be seen as an intermediate result in an implicit DDPM process with the input degraded image $\\scriptstyle{\\hat{x}}_{0}$ as the target distribution. We essentially quantitatively computed the accelerated step $T^{\\prime}$ , on which step $x_{T^{\\prime}}$ is closed to $\\hat{x}_{T^{\\prime}}$ . When $\\mathrm{T}$ is large enough, the approximate equal sign will become an equal sign. The implicit DDPM reverse process ( $\\epsilon$ to $\\scriptstyle{\\hat{x}}_{0}$ ) is deterministic because $\\scriptstyle{\\hat{x}}_{0}$ is available during the inference. The determination of step $T^{\\prime}$ corresponds to the point where the posterior probability distribution (resnoise-diffusion reverse process) becomes consistent with the prior probability distribution (implicit DDPM reverse process). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{x}_{t}=\\sqrt{\\overline{{\\alpha}}_{t}}\\hat{x}_{0}+\\sqrt{1-\\overline{{\\alpha}}_{t}}\\epsilon,\\quad\\sqrt{\\overline{{\\alpha}}_{T^{\\prime}}}\\approx1-\\sqrt{\\overline{{\\alpha}}_{T^{\\prime}}}\\approx\\frac{1}{2},\\quad x_{T^{\\prime}}\\approx\\hat{x}_{T^{\\prime}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As shown in Fig. 2, the resnoise-diffusion reverse process $\\boldsymbol{R}+\\boldsymbol{\\epsilon}$ to $\\scriptstyle x_{0}$ ) intersects with implicit DDPM reverse process ( $\\mathbf{\\chi}_{\\epsilon}$ to $\\scriptstyle{\\hat{x}}_{0}$ ), this intersection corresponds to step $T^{\\prime}$ . The intersection of two diagonals of a parallelogram is the midpoint of them (corresponding to 0.5 in Eq. (11)), but due to the discrete nature of the diffusion process, the acceleration point actually falls on the point closest to the intersection, which is step $T^{\\prime}$ as Eq. (8). Meanwhile, since $x_{T^{\\prime}}$ is available, resnoise-diffusion steps after step $T^{\\prime}$ are not necessary according to Eq. (10). Therefore, Resfusion can directly start from step $T^{\\prime}$ for both inference and training process. Because the $\\alpha$ coefficient of resnoise-diffusion is exactly the same as vanilla DDPM, Resfusion can directly use any existing noise schedule. In practical implementation, we utilized a technique called Truncated Schedule to control the offset between $x_{T^{\\prime}}$ and $\\hat{x}_{T^{\\prime}}$ when $T$ is small. Further details can be found in Appendix A.7. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Table 1: Quantitative comparisons with other shadow removal methods. We report PSNR, SSIM [18] and MAE in the shadow region (S), the non-shadow region (NS) and all image (ALL). The best and second-best results are highlighted in bold and underlined. \u201c\u2191\" (resp. \u201c\u2193\") means the larger (resp. smaller), the better. We use the symbol \u201c-\" to indicate models or results that are unavailable. ", "page_idx": 4}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/0be3334ec1210b46543d68ed6ae5205282056e75614ed2c61ac1186d82aa582b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/9cfbb587b71bf72c49c06e077127587628ffabae71204714d15f16c0d68e23fd.jpg", "img_caption": ["Figure 3: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "To verify the performance of Resfusion, we conducted experiments on three image restoration tasks, including shadow removal, low-light enhancement and deraining. For fair comparisons, we use an U-net [28] structure which is the same as RDDM as the backbone. We simply concatenate $x_{t}$ and $\\scriptstyle{\\hat{x}}_{0}$ in the channel dimension and feed them into the network. For all the tasks, we only employ one U-net to predict resnoise. Furthermore, we simply utilize a truncated linear schedule [29] and perform only five sampling steps for all datasets. The experimental setting details are provided in the Appendix A.4. ", "page_idx": 4}, {"type": "text", "text": "ISTD dataset [19] is a dataset designed for shadow removal, comprising 1870 sets of image triplets consisting of shadow image, shadow mask, and shadow-free image. It consists of 1330 image triplets for training and 540 image triplets for quantitative evaluations. We compare the proposed method with the popular shadow removal methods, i.e., ST-CGAN [20], DSC [21], ARGAN [26], DHAN [22], ", "page_idx": 4}, {"type": "text", "text": "Table 3: Quantitative comparisons with other deraining methods. We report PSNR and SSIM. The best and second-best results are highlighted in bold and underlined. \u201c\u2191\" means the larger, the better. ", "page_idx": 5}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/aa49767fb26afeed1de00228f2af163b50b4da5d754b957aa58ece05bf41fcfa.jpg", "table_caption": ["Table 2: Quantitative comparisons with other lowlight enhancement methods. We report PSNR, SSIM and LPIPS [38]. The best and second-best results are highlighted in bold and underlined. \u201c\u2191\" (resp. \u201c\u2193\") means the larger (resp. smaller), the better. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "", "img_caption": ["Figure 4: Visual comparisons of the restored results by different image restoration methods on the LOL dataset and the Raindrop dataset. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "FusionNet [23], CANet [27], UnfoldingNet [24], DMTN [25], and RDDM(SM-Res-N) [17]. In order to ensure a fair comparison, we conducted experiments on two settings for the ISTD dataset, following the methods used in DMTN [25] and DHAN [22]: (1) The results are evaluated at a resolution of $256\\times256$ after being resized. (2) The original image resolutions ( $640\\times480)$ are maintained for evaluation. ", "page_idx": 5}, {"type": "text", "text": "LOL dataset [30] comprises 500 pairs of images, consisting of both low-light and normal-light versions, which are further divided into 485 training pairs and 15 evaluation pairs. The low-light images contain noise produced during the photo capture process. We compare the proposed method with the popular low-light enhancement methods, i.e., RetinexNet [30], KinD [31], $\\mathrm{KinD++}$ [32], Zero-DCE [33], EnlightenGAN [34], Restormer [35], LLFormer [19], RDDM (SM-Res-N) [17], and RDDM (SM-Res) [17]. Some existing methods [8, 36, 37] calculate metrics by adjusting the overall brightness based on reference images (called as using GT-mean). However, this approach can introduce biases and potential unfairness. In accordance with LLFormer [19], we compute metrics without utilizing any reference information. To ensure a fair comparison, we conducted experiments on two settings for the LOL dataset, following the methods employed in RDDM [17] and LLFormer [19]: (1) The results are evaluated at a resolution of $256\\times256$ after being resized. PSNR and SSIM are evaluated based in YCbCr color space. (2) The original image resolutions $(600\\times400)$ are maintained for evaluation. PSNR and SSIM are evaluated in RGB color space. ", "page_idx": 5}, {"type": "text", "text": "Raindrop dataset [39] is a dataset designed for deraining, comprising 861 training image pairs for training, and 58 image pairs dedicated for quantitative evaluations, denoted in [39] and [5] as Raindrop-A. We compare the proposed method with the popular deraining methods, i.e., pix2pix [40], ", "page_idx": 5}, {"type": "text", "text": "AttentiveGAN [39], DuRN [41], RaindropAttn [42], All-in-One [43], IDT [44], WeatherDiff [5], RainDropDiff [5], and RDDM(SM-Res-N)[17]. We conduct experiments on two settings for the Raindrop dataset for fairness, following the methods employed in RDDM [17] and WeatherDiff [5]: (1) The results are evaluated at a resolution of $256\\times256$ after being resized. (2) The original image resolutions are maintained for evaluation. ", "page_idx": 6}, {"type": "text", "text": "Quantitative comparison. As shown in the Tables 1, 2 & 3, We provide the quantitative evaluation results on ISTD dataset, LOL dataset and Raindrop dataset. Our methods clearly outperform all competing methods by significant margins in terms of PSNR, SSIM, MAE and LPIPS across all three datasets. The current experimental results demonstrate that Resfusion achieves highly competitive results under these conditions: (1) utilizing only one U-net to predict resnoise. (2) simply concatenating $x_{t}$ and $\\scriptstyle{\\hat{x}}_{0}$ in the channel dimension. (3) employing a simple truncated linear schedule. (4) conducting only five sampling steps for all datasets. In contrast, alternative methods often rely on intricate network architectures, including multi-stage [19, 23, 24], multi-scale [45], multi-branch [22] and prior knowledge of physics [9, 30, 33, 42], complex noise schedules [16, 17] and patch-overlapping strategy [5]. For the ISTD dataset and Raindrop dataset, we only employ one U-net to predict resnoise, outperforming RDDM with two U-nets to predict the residual term and the noise separately in terms of PSNR and SSIM. Resfusion use half the number of parameters of RDDM and achieved better quantitative evaluation metrics. For the LOL dataset, under the same parameters, Resfusion outperforms RDDM in terms of PSNR $(+18\\%)$ and LPIPS $(-40\\%)$ significantly. Furthermore, for all datasets, we employed a simple truncated linear schedule, while RDDM utilized a complex custom noise schedule. ", "page_idx": 6}, {"type": "text", "text": "4 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 The analysis of the residual term and the noise term ", "text_level": 1, "page_idx": 6}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/b4cb58f4d4b7029dc7d771a34dad917c4b77049367e04659a0bb0b1d882b9b8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: The analysis of the residual term and the noise term on the LOL dataset. Only removing noise will reconstruct the details of the degraded image without causing any semantic shift. Only removing residual can only accomplish the semantic shift (from low-light to normal-light) without reconstructing the details. Removing resnoise can achieve both the semantic shift and the detail reconstruction. ", "page_idx": 6}, {"type": "text", "text": "Resfusion and DDPM share the consist form of the reverse inference process. The DDPM reverse process restores the original image distribution by gradually removing the noise $\\left(\\epsilon_{\\theta}\\right)$ from Gaussian white noise, while resnoise-diffusion reverse process restores clean image by gradually removing the resnoise $(r e s\\epsilon_{\\theta})$ from the noisy degraded images. Mathematically, the resnoi\u221ase term $(r e s\\epsilon_{\\theta})$ and the noise term $\\left(\\epsilon_{\\theta}\\right)$ only differ in the form of a weighted residual term ( (1\u2212 \u03b1t\u03b2) 1\u2212\u03b1tR\u03b8). In other words, Resfusion subtracts an extra weighted residual term while removing the noise term during each step of the reverse process. ", "page_idx": 6}, {"type": "text", "text": "According to the Green\u2019s theorem [46], when the neural network is sufficiently robust, the components of the resnoise should be path independent. Based on this belief, we trained two separate neural networks on the LOL dataset. One network predicts only $\\epsilon_{\\theta}$ and removes only the noise term during the resnoise-diffusion reverse process. The other network predicts only $\\begin{array}{r}{\\frac{(1-\\dot{\\sqrt{\\alpha_{t}}})\\sqrt{1-\\overline{{\\alpha}}_{t}}}{\\beta_{t}}R_{\\theta}}\\end{array}$ and removes only the weighted residual term during the resnoise-diffusion reverse process. As shown in Fig 5, we qualitatively determine the functionalities of each component in the loss function of Resfusion. The weighted residual term are responsible for semantic shift, while the noise term handles detail reconstruction. Predicting resnoise $r e s\\epsilon_{\\theta}$ achieves both semantic shifts and detail reconstruction, bridging the gap between the input degraded image and the ground truth, enabling effective image restoration. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Equivalent representations of the loss function ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/9406ece621902b13d795570d88915873d09c432610607d19733ce70638c27547.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: Visual comparisons of the restored results generated by other equivalent loss functions on Raindrop dataset and LOL dataset. Using $r e s\\epsilon_{\\theta}$ as the loss function allows for better restoration of details while completing semantic shifting. ", "page_idx": 7}, {"type": "text", "text": "Based on Eq. (1), since $\\scriptstyle{\\hat{x}}_{0}$ is available, once we acquire either $x_{0}$ or $R$ , we can determine the other. Moreover, according to Eq. (18), since $x_{t}$ is obtainable and considering the equivalence between $x_{0}$ and $R$ , we can also obtain the equivalent reverse process of resnoise-diffusion by predicting either $x_{0\\theta}$ or $R_{\\theta}$ . Similar to DDPM, the fundamental purpose of Resfusion is also to predict $x_{0}$ (which is equivalent to predicting $R$ ). In contrast, DDPM reconstructs the distribution of the original image $x_{0}$ by incrementally reducing noise from the Gaussian white noise, whereas Resfusion skips the generation of low-frequency information by utilizing $\\scriptstyle{\\hat{x}}_{0}$ for initialization. Therefore, in addition to the noise removal, Resfusion also involves removing a weighted residual term to accomplish the semantic shifting. It is worth mentioning that, unlike RDDM, predicting the noise term $\\epsilon_{\\theta}$ is not an equivalent loss function. Due to the truncated schedule we adopt, the starting point $x_{T^{\\prime}}$ will only consist of weighted $\\scriptstyle{\\hat{x}}_{0}$ and $\\epsilon$ . As $\\scriptstyle{\\hat{x}}_{0}$ is obtainable and serves as a conditional input to the neural network, directly predicting the noise term would lead to the neural network learning a simple pattern, resulting in training failure. ", "page_idx": 7}, {"type": "text", "text": "We compare the quantitative performance obtained by using different equivalent prediction targets as loss functions on ISTD dataset, LOL dataset and Raindrop dataset. We use the same backbone as described in Sec. 3 and employ the same truncated linear schedule, performing five sampling steps for all datasets. The original image resolutions are maintained for evaluation. As shown in Table 4, predicting $r e s\\epsilon_{\\theta}$ outperformed predicting $x_{0\\theta}$ and $R_{\\theta}$ in terms of PSNR and SSIM on all three datasets. Predicting $r e s\\epsilon_{\\theta}$ resulted in better reconstruction of the fine details, as illustrated in Fig 6. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Quantitative comparisons with other equivalent loss functions on ISTD dataset, LOL dataset and Raindrop dataset. We report PSNR, SSIM, MAE and LPIPS. The best and second-best results are highlighted in bold and underlined. \u201c\u2191\" (resp. \u201c\u2193\") means the larger (resp. smaller), the better. ", "page_idx": 7}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/4c02980eb741312b176f4fffabb2ecf5c6372746ae324cf25a1780fa817d3861.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/72ba8994b96fcaca2dc530b370810003ab5cefc9c8df1f63b8a4382ce6dc0beb.jpg", "img_caption": ["Figure 7: Visual comparisons between DDPM and Resfusion on the CIFAR10 $\\mathrm{32\\times32})$ dataset. We do not cherry-pick any results. With the same sampling steps, Resfusion outperforms DDPM in semantic generation and detail reconstruction. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Resfusion is not limited to image restoration. In fact, It is a versatile framework which can be applied to any general image generation domain. For image generation tasks, as it is impossible to obtain any additional information, we redefine $\\scriptstyle{\\hat{x}}_{0}$ as a zero matrix. Thus we can obtain a new definition of the res\u221aidual term $R\\,=\\,-x_{0}$ for image generation. Therefore, res\u03f5 is redefined as $\\begin{array}{r}{r e s\\epsilon=\\epsilon-\\frac{(1-\\sqrt{\\alpha_{t}})\\sqrt{1-\\overline{{\\alpha}}_{t}}}{\\beta_{t}}x_{0}}\\end{array}$ . Applied with the redefined residual term and the resnoise, Resfusion\u2019s forward and reverse process for image generation are completely consistent with the original resnoisediffusion process for image retoration. ", "page_idx": 8}, {"type": "text", "text": "The redefined res\u03f5 further explains why Resfusion can complete the reverse inference process in fewer sampling steps than DDPM, under the same noise schedule. Due to consistency between the reverse processes, both of Resfusion and DDPM require the removal of the noise term. However, compared to DDPM, Resfusion additionally add a weighted $x_{0}$ instead of simply removing the noise. This is the key factor that allows Resfusion to diffuse faster. ", "page_idx": 8}, {"type": "text", "text": "We train DDPM, Resfusion, and DDIM [2] on the CIFAR10 $(32\\times32)$ dataset [47] with the same backbone. The experimental details are provided in the Appendix A.4. A truncated linear schedule is used for Resfusion, and a linear schedule is used for DDPM and DDIM. We employ the Frechet Inception Distance (FID) [48] as the quantitative metric. As shown in Table 5, Resfusion significantly outperforms DDPM with the same sampling steps. At nearly half of the sampling steps, Resfusion achieves a similar FID with DDPM. Interestingly, when using a truncated linear schedule, the value of $T^{\\prime}/T$ is also closed to 0.5, further validating Resfusion\u2019s accelerated sampling property. Consistent with DDPM, Resfusion also performs stochastic steps during the reverse process. Due to its stochastic nature, similar to DDPM, Resfusion\u2019s performance is lower than the deterministic DDIM. ", "page_idx": 8}, {"type": "text", "text": "Table 5: Quantitative comparisons with DDPM and DDIM on CIFAR10 $(32\\times32)$ dataset. We report FID under different sampling steps. \u201c\u2193\" means the smaller, the better. ", "page_idx": 8}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/bc873f1c4559b0acb1e4f4f18bf79dede334d31d7b734dd51cfff4f9731d3a3e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/415ce33445edad5f732baeb568d3beaea3db91f3a9802102801c406b65819a2f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 8: Visualization of the five sampling steps, where the blue arrow represents the smooth equivalence transformation, and the $r e d$ box represents the resnoise-diffusion reverse process. We select the LOL web Test dataset (which does not have ground truth) and the Raindrop-B test dataset (which is much more challenging than Raindrop-A) to showcase the effects of low-light enhancement and deraining. We directly use the pretrained models on LOL dataset and Raindrop dataset, demonstrating the strong robustness of Resfusion. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose the Resfusion, a general framework for image restoration. We explicitly provide the quantitative relationship between the residual term and the noise term, named as resnoise. Through employing the smooth equivalence transformation, we unify the training and inference process. Resfusion does not require any prior physical knowledge and can directly utilize existing noise schedules. Experimental results shows that Resfusion exhibits competitive performance for shadow removal, low-light enhancement, and deraining tasks, with only five sampling steps. It is important to note that Resfusion is not limited to image restoration and can be applied to any image generation domain. The versatility of our framework lies in its ability to simultaneously model the residual term and the noise term. Our subsequent experiments have demonstrated that Resfusion can be easily applied to various image generation tasks and exhibits strong competitiveness. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by the National Natural Science Foundation of China (62272248), the Natural Science Foundation of Tianjin (23JCZDJC01010, 23JCQNJC00010) and the Key Program of Science and Technology Foundation of Tianjin (24HHXCSS00004). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[2] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [3] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):4713\u20134726, 2022. [4] Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochastically. Advances in Neural Information Processing Systems, 34:21757\u201321769, 2021.   \n[5] Ozan \u00d6zdenizci and Robert Legenstein. Restoring vision in adverse weather conditions with patch-based denoising diffusion models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [6] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis, and Peyman Milanfar. Deblurring via stochastic refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16293\u201316303, 2022. [7] Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo, Bjorn Stenger, Tae-Kyun Kim, Wei Liu, and Hongdong Li. Lldiffusion: Learning degradation representations in diffusion models for low-light image enhancement. arXiv preprint arXiv:2307.14659, 2023.   \n[8] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structure-aware diffusion process for low-light image enhancement. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, and Bihan Wen. Shadowdiffusion: When degradation prior meets diffusion model for shadow removal. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14049\u201314058, 2023.   \n[10] Chunming He, Chengyu Fang, Yulun Zhang, Kai Li, Longxiang Tang, Chenyu You, Fengyang Xiao, Zhenhua Guo, and Xiu Li. Reti-diff: Illumination degradation image restoration with retinex-based latent diffusion model. arXiv preprint arXiv:2311.11638, 2023.   \n[11] Chunming He, Yuqi Shen, Chengyu Fang, Fengyang Xiao, Longxiang Tang, Yulun Zhang, Wangmeng Zuo, Zhenhua Guo, and Xiu Li. Diffusion models in low-level vision: A survey. arXiv preprint arXiv:2406.11138, 2024.   \n[12] Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. arXiv preprint arXiv:2303.11435, 2023.   \n[13] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022.   \n[14] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, and Thomas B Sch\u00f6n. Image restoration with mean-reverting stochastic differential equations. arXiv preprint arXiv:2301.11699, 2023.   \n[15] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. $\\mathrm{I}^{2}\\mathrm{sb}$ : Image-to-image schr\u00f6dinger bridge. arXiv preprint arXiv:2302.05872, 2023.   \n[16] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, and Liangqiong Qu. Residual denoising diffusion models. arXiv preprint arXiv:2308.13712, 2023.   \n[18] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.   \n[19] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn Stenger, and Tong Lu. Ultra-high-definition low-light image enhancement: A benchmark and transformer-based method. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2654\u20132662, 2023.   \n[20] Jifeng Wang, Xiang Li, and Jian Yang. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1788\u20131797, 2018.   \n[21] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin, and Pheng-Ann Heng. Direction-aware spatial context features for shadow detection and removal. IEEE transactions on pattern analysis and machine intelligence, 42(11):2795\u20132808, 2019.   \n[22] Xiaodong Cun, Chi-Man Pun, and Cheng Shi. Towards ghost-free shadow removal via dual hierarchical aggregation network and shadow matting gan. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10680\u201310687, 2020.   \n[23] Lan Fu, Changqing Zhou, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Wei Feng, Yang Liu, and Song Wang. Auto-exposure fusion for single-image shadow removal. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10571\u201310580, 2021.   \n[24] Yurui Zhu, Zeyu Xiao, Yanchi Fang, Xueyang Fu, Zhiwei Xiong, and Zheng-Jun Zha. Efficient modeldriven network for shadow removal. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 3635\u20133643, 2022.   \n[25] Jiawei Liu, Qiang Wang, Huijie Fan, Wentao Li, Liangqiong Qu, and Yandong Tang. A decoupled multi-task network for shadow removal. IEEE Transactions on Multimedia, 2023.   \n[26] Bin Ding, Chengjiang Long, Ling Zhang, and Chunxia Xiao. Argan: Attentive recurrent generative adversarial network for shadow detection and removal. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10213\u201310222, 2019.   \n[27] Zipei Chen, Chengjiang Long, Ling Zhang, and Chunxia Xiao. Canet: A context-aware network for shadow removal. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4743\u20134752, 2021.   \n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[30] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018.   \n[31] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light image enhancer. In Proceedings of the 27th ACM international conference on multimedia, pages 1632\u20131640, 2019.   \n[32] Yonghua Zhang, Xiaojie Guo, Jiayi Ma, Wei Liu, and Jiawan Zhang. Beyond brightening low-light images. International Journal of Computer Vision, 129:1013\u20131037, 2021.   \n[33] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1780\u20131789, 2020.   \n[34] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. IEEE transactions on image processing, 30:2340\u20132349, 2021.   \n[35] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5728\u20135739, 2022.   \n[36] Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, and Alex Kot. Low-light image enhancement with normalizing flow. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 2604\u20132612, 2022.   \n[37] Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. arXiv preprint arXiv:2305.10028, 2023.   \n[38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[39] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial network for raindrop removal from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2482\u20132491, 2018.   \n[40] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.   \n[41] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. Dual residual networks leveraging the potential of paired operations for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7007\u20137016, 2019.   \n[42] Yuhui Quan, Shijie Deng, Yixin Chen, and Hui Ji. Deep learning for seeing through window with raindrops. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2463\u20132471, 2019.   \n[43] Ruoteng Li, Robby T Tan, and Loong-Fah Cheong. All in one bad weather removal using architectural search. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3175\u20133185, 2020.   \n[44] Jie Xiao, Xueyang Fu, Aiping Liu, Feng Wu, and Zheng-Jun Zha. Image de-raining transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[45] Lanqing Guo, Siyu Huang, Ding Liu, Hao Cheng, and Bihan Wen. Shadowformer: global context helps shadow removal. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 710\u2013718, 2023.   \n[46] Bernhard Riemann. Grundlagen fur eine allgemeine Theorie der Functionen einer ver\u00e4nderlichen complexen Gr\u00f6sse. Adalbert Rente, 1867.   \n[47] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[48] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[49] Calvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970, 2022.   \n[50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \n[51] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. arXiv preprint arXiv:2307.12348, 2023.   \n[53] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \n[54] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.   \n[55] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188\u20138197, 2020.   \n[56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[58] Hieu Le and Dimitris Samaras. Shadow removal via shadow image decomposition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8578\u20138587, 2019.   \n[59] Yurui Zhu, Jie Huang, Xueyang Fu, Feng Zhao, Qibin Sun, and Zheng-Jun Zha. Bijective mapping network for shadow removal. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5627\u20135636, 2022.   \n[60] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738, 2015.   \n[61] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE Transactions on Image Processing, 30:2072\u20132086, 2021.   \n[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[63] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, and Thomas B Sch\u00f6n. Refusion: Enabling large-size realistic image restoration with latent-space diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1680\u20131691, 2023.   \n[64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix Section ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Detailed proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "According to Eq. (4) and Eq. (5), the forward process can be written as Eq. (16) and Eq. (17). ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(x_{t}|x_{t-1},R)=\\mathcal{N}(x_{t};\\sqrt{\\alpha_{t}}x_{t-1}+(1-\\sqrt{\\alpha_{t}})R,(1-\\alpha_{t})I)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nq(x_{t}|x_{0},R)=\\mathcal{N}(x_{t};\\sqrt{\\overline{{\\alpha}}_{t}}x_{0}+(1-\\sqrt{\\overline{{\\alpha}}_{t}})R,(1-\\overline{{\\alpha}}_{t})I)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By simply performing a change of variable $\\langle x_{0}\\to x_{0}-R$ , $x_{t}\\rightarrow x_{t}-R$ , $x_{t-1}\\to x_{t-1}-R)$ , the derivation of Eq. (18) is identical in form to (71) - (84) in the reference [49], where line $6\\textup{-}7$ of Eq. (18) corresponds to (73). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(x_{i-1}|x_{t},x_{t},R)}\\\\ &{=\\frac{q(x_{i}|x_{t},x_{t},R)}{2(1-R)},\\qquad}\\\\ &{=\\frac{X\\left(x_{i}+\\sqrt{q_{i}}\\right)R\\left(x_{i-1}|\\eta,R)}{X\\left(x_{i},\\sqrt{q_{i}},x_{t},R\\right)\\left(R+1-\\sqrt{q_{i}}\\right)R\\left(x_{i-1}|\\eta,R\\right)\\left(R+1-\\sqrt{q_{i-1}}x_{t}+\\left(1-\\sqrt{q_{i-1}}\\right)R,\\left(1-\\widetilde{q}_{i-1}\\right)\\right)}}\\\\ &{=\\frac{X\\left(x_{i}+\\sqrt{q_{i}}\\right)\\left(R-1-\\sqrt{q_{i}}\\right)R\\left(1-\\sqrt{q_{i}}\\right)\\left(R-1\\right)}{X\\left(x_{i},\\sqrt{q_{i}},x_{t}+\\left(1-\\sqrt{q_{i}}\\right)R,\\left(R-1-\\widetilde{q}_{i}\\right)\\right)}}\\\\ &{\\times\\exp\\left(-\\left[\\frac{\\left(R-1\\right)}{2}-\\left(\\sqrt{q_{i}}x_{i-1}+\\left(1-\\sqrt{q_{i}}\\right)R\\right)\\right]\\right)^{2}+\\frac{\\left(R-1-\\sqrt{\\left(q_{i-1}\\right)}R\\right)\\left(1-\\sqrt{q_{i-1}}\\right)\\left(1-\\sqrt{q_{i-1}}\\right)R\\left(1-\\sqrt{q_{i-1}}\\right)R\\left(1-\\widetilde{q}_{i-1}\\right)}{2\\left(1-R-\\widehat{q}_{i-1}\\right)}}\\\\ &{-\\frac{\\left[R-1\\right]\\sqrt{q_{i}}x_{i}\\left(1-\\sqrt{q_{i}}\\right)\\left(1-\\sqrt{q_{i}}x_{i}\\right)\\left(1\\right)}{2\\left(1-R\\right)}}\\\\ &{=\\exp\\left(-\\frac{\\left(R-1\\right)}{2}-\\sqrt{q_{i}}\\left(x_{i-1}-R\\right)\\right)^{2}+\\frac{\\left(\\left(x_{i-1}-R\\right)-\\sqrt{q_{i-1}}\\right)\\left(1-\\sqrt{q_{i-1}}\\right)\\left(R-R\\right)}{2\\left(1-R-1\\right)}}\\\\ &{-\\frac{\\left(\\left(R_{i}-R\\right)-\\sqrt{q_{i}}\\left(x_{i}-R\\right)\\right)^{2}}{2\\left(1-R\\right)}}\\\\ &{\\times X(x_{i-1}-R)\\frac{\\sqrt{q_{i}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we can derive $\\widetilde{\\mu}(x_{t},x_{0},R)$ as Eq. (19). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}(x_{t},x_{0},R)=\\frac{\\sqrt{\\overline{{\\alpha}}_{t-1}}\\beta_{t}}{1-\\overline{{\\alpha}}_{t}}(x_{0}-R)+\\frac{\\sqrt{\\alpha_{t}}(1-\\overline{{\\alpha}}_{t-1})}{1-\\overline{{\\alpha}}_{t}}(x_{t}-R)+R\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can derive Eq. (20) through Eq. (5). ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t}-R=\\sqrt{\\overline{{\\alpha_{t}}}}(x_{0}-R)+\\sqrt{1-\\overline{{\\alpha_{t}}}}\\epsilon,\\quad\\epsilon\\sim N(0,I)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we can derive Eq. (21) through Eq. (19) and Eq. (20). By simply performing a change of variables $[x_{0}\\to x_{0}-\\bar{R}$ , $x_{t}\\to x_{t}-R)$ , the derivation process becomes exactly identical in form to the derivation of equations (115) - (124) in the reference [49], where Eq. (20) corresponds to (115) and Eq. (19) corresponds to (116). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{t-1}-{\\cal C}=\\mathbb E_{x_{0},\\epsilon,t}[\\frac{1}{2\\sigma_{t}^{2}}||\\tilde{\\mu}(x_{t},x_{0},R)-\\mu_{\\theta}(x_{t}(x_{0},\\epsilon,R),t)||^{2}]}}\\\\ {~~~~~~~~~~~~~~~~~=\\mathbb E_{x_{0},\\epsilon,t}[\\frac{1}{2\\sigma_{t}^{2}}||\\{\\frac{1}{\\sqrt{\\alpha_{t}}}[(x_{t}(x_{0},\\epsilon,R)-R)-{\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}}\\epsilon]+R\\}-\\mu_{\\theta}(x_{t}(x_{0},\\epsilon,R),t)||^{2}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Eq. (22), we can modify Eq. (21) as Eq. (23). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t}-R-\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}\\epsilon)+R=\\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t}-R+\\sqrt{\\alpha_{t}}R-\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}\\epsilon)}&{}\\\\ {=\\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t}-(1-\\sqrt{\\alpha_{t}})R-\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}\\epsilon)}&{}\\\\ {=\\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t}-\\frac{(1-\\sqrt{\\alpha_{t}})\\sqrt{1-\\overline{{\\alpha}}_{t}}}{\\beta_{t}}\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}R-\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}\\epsilon)}&{}\\\\ {=\\frac{1}{\\sqrt{\\alpha_{t}}}[x_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}(\\epsilon+\\frac{(1-\\sqrt{\\alpha_{t}})\\sqrt{1-\\overline{{\\alpha}}_{t}}}{\\beta_{t}}R)]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to [1], the minimize term become Eq. (6). ", "page_idx": 15}, {"type": "text", "text": "A.2 Comparison with other methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The main difference is in how the denoising diffusion, score, flow, or Schr\u00f6dinger\u2019s bridge are adapted to image restoration. Different methods select various elements as prediction targets: the noise term (Shadow Diffusion [9], SR3 [3], WeatherDiffusion [5]), the residual term (DvSR [6], Rectified Flow [50]), the target image (ColdDiffusion [51], InDI [12]), or its linear transformation term $\\mathrm{T}^{2}\\mathrm{SB}$ [15]). Similar to RDDM [17], Resfusion simultaneously predicts both the residual term and the noise term, and provides the quantitive relationship between them. ", "page_idx": 15}, {"type": "text", "text": "Comparison with traditional diffusion-based methods. Traditional diffusion-based image restoration methods [5, 6, 7, 8, 9] adapt the diffusion model for image restoration tasks with degraded images as conditional input to implicitly guide the reverse generation process, without altering the original denoising diffusion process [1, 2]. Starting the reverse process from Gaussian white noise, traditional diffusion-based models consider only the degraded images as conditional input, resulting in an increased number of sampling steps. Meanwhile, these models are often task-specific, requiring the design of different model structures based on different scenarios. By introducing the residual term into the diffusion forward process, Resfusion bridge the gap between the input degraded images and ground truth, starting the reverse process directly from the noisy degraded images. As a versatile methodology for image restoration, Resfusion does not require any physical prior knowledge, and the image restoration can be completed in just five sampling steps. ", "page_idx": 15}, {"type": "text", "text": "Comparison with RDDM [17]. RDDM can be seen as a diffusion process from the noisy input degraded image to the ground truth, while Resfusion represents a diffusion process from the noisy residual term to the ground truth. RDDM predicts the residual term and the noise term separately without specifying their weighted relationship, using a complex Automatic Objective Selection Algorithm (AOSA) to learn them. In contrast, Resfusion calculates the quantitative relationship between the residual term and the noise term, naming their weighted sum as resnoise. RDDM\u2019s forward process accumulates the residual term and the noise term, making its forward and backward processes inconsistent with DDPM, leading to poor generalization and interpretability. By transforming the learning of the noise term into the resnoise term, Resfusion\u2019s reverse inference process becomes consistent with DDPM, unifying the training and inference processes. Lastly, RDDM requires a customized noise schedule, as using existing noise schedules results in performance loss. Through the smooth equivalence transformation in resnoise-diffusion process, Resfusion can directly use the existing noise schedule. ", "page_idx": 15}, {"type": "text", "text": "Comparison with Resshift [52]. Similar to RDDM, Resshift\u2019s forward process also adopts an accumulation strategy for the residual term and the noise term. Therefore, Resshift also requires the design of a complex noise schedule, which is formulated as equation (10) in Resshift [52]. Resfusion can directly use the existing noise schedule instead of redesigning the noise schedule. The reverse process of Resshift is inconsistent with DDPM. The form of Resfusion\u2019s reverse inference process is consistent with the DDPM, leading to better generalization and interpretability. The prediction target of Resshift is $x_{0}$ , while the prediction target of Resfusion is res\u03f5. Given that the essence of res\u03f5 is the noise term with an offset, and LDM models mainly predict the noise term, the loss function of Resfusion is extremely friendly to fine-tuning techniques such as Lora, which helps further scale up. Resshift diffuses in the latent space, utilizing the powerful encoding capability of models like VQ-GAN [53]. Resfusion, on the other hand, directly diffuses in the RGB space. Resshift only explores fixed degradations such as image super-resolution. Resfusion explores more complex scenarios, including shadow removal, low-light enhancement, and deraining. ", "page_idx": 15}, {"type": "text", "text": "Comparison with DvSR [6]. DvSR predicts clean images from input degraded images using a traditional (non-diffusion) network and calculates the residual term between the ground truth and the predicted clean images. DvSR employs denoising-based diffusion models to predict the noise term like DDPM, generating the residual term from Gaussian white noise. Unlike DvSR, Resfusion does not directly learn the residual term. Instead, it indirectly learns the distribution of the residual term through resnoise-diffusion process. ", "page_idx": 15}, {"type": "text", "text": "Comparison with ColdDiffusion [51]. ColdDiffusion aims to completely remove random noise from the diffusion model, replacing it with other transformations like blurring and masking. In contrast, Resfusion still incorporates noise diffusion. As shown in our ablation study, Resfusion requires the noise term for detail recovery. Since ColdDiffusion discards random noise, it needs additional degradation injection to enhance generation diversity. To simulate degradation processes for various restoration tasks, ColdDiffusion uses Gaussian blur for deblurring, snowification transform for snow removal, etc. These specific explorations might lose generality. Resfusion employs the residual term for directed diffusion from the ground truth to the noisy residual term, eliminating the need for task-specific degradation operators. Additionally, Resfusion provides solid theoretical derivation, whereas ColdDiffusion lacks a theoretical basis. ", "page_idx": 16}, {"type": "text", "text": "A.3 Image translation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Resfusion can also be implemented in image-to-image distribution transformation. By redefining $\\scriptstyle{\\hat{x}}_{0}$ as the translated image and $x_{0}$ as the target image, we can easily transition Resfusion from image restoration to image translation. We train Resfusion with a truncated linear schedule on CelebA-HQ $(64\\times64)$ dataset [54] and AFHQV2 $(64\\times64)$ dataset [55] for image translation with 50 sampling steps. We selected the following image translation tasks: $\"\\mathrm{Dog}\\rightarrow\\mathrm{Cat\"}$ , \"Male $\\rightarrow\\mathrm{{Cat}\"}$ , \"Male $\\rightarrow$ Female\", and \"Female $\\rightarrow\\mathrm{Male^{\\prime\\prime}}$ . As shown in Fig. 9, Resfusion can effectively model the shift between image domains, making it a unified methodology for a wider range of image generation tasks. ", "page_idx": 16}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/bbe80edbc08cc248e61bc12adac77d76ef6ca2edd1498723b680ecf7e47842a2.jpg", "img_caption": ["Figure 9: Visual results for image translation on the CelebA-HQ dataset and AFHQV2 dataset. The images are presented in pairs, with the translated image on the left and the target image on the right. We showcase the visual results of Resfusion for image translation tasks $\\mathrm{^{\\cdot}D o g\\rightarrow C a t^{\\circ}}$ , \u201cMale $\\rightarrow\\mathrm{{Cat^{\\circ}}}$ , \u201cMale $\\rightarrow$ Female\u201d, and \u201cFemale $\\rightarrow\\mathrm{Male^{;}}$ . "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/327bbd001f9c674f6778a71e72c68550b9f38f3353c0bbe04af20915803d906c.jpg", "table_caption": ["Table 6: Experimental settings for our Resfusion during the training stage. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.4 Experimental setting details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use the PyTorch Lightning framework to train all the models, utilizing the AdamW [56] optimizer with the default settings of PyTorch [57]. Following Liu et al. [17], we utilize the THOP to compute the number of parameters (Params) and multiply-accumulate operations (MACs). We only employ one U-net to predict resnoise and simply utilize a truncated Linear schedule across all tasks. For all the tasks, we implement a regular MSE loss as the loss function. The detailed experimental settings are provided in Table 6. All experiments listed in Table 6 can be carried out with 8 NVIDIA RTX A6000 GPUs. ", "page_idx": 17}, {"type": "text", "text": "Image restoration. We evaluate our method on several image restoration tasks, including shadow removal, low-light enhancement, and image deraining on 3 different datasets. For fair comparisons, the results of other image restoration methods are referenced from previously published papers [5, 7, 17, 19, 45] whenever possible. For all image restoration tasks, we used an identical U-net as the backbone, which is the same as RDDM [17]. We take the shadow images and shadow masks together as the input condition (similar to [17, 58, 59]) for the ISTD dataset and only degraded images as the input condition for other datasets. We simply concatenate $x_{t}$ and $\\scriptstyle{\\hat{x}}_{0}$ (and shadow masks) together in the channel dimension and feed them into the network. For the LOL dataset, we do not use pre-processing and post-processing techniques like Histogram Equalization and GT-mean. For the Raindrop dataset, we evaluate PSNR and SSIM based on the luminance channel $\\mathrm{\\bfY}$ of the YCbCr color space in accordance with previous work [5, 17, 39]. We employ a patch size of $256\\times256$ for all the datasets during the training stage. We use the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), learned perceptual image patch similarity (LPIPS), and mean absolute error (MAE) as quantitative metrics. ", "page_idx": 17}, {"type": "text", "text": "Image generation and image translation. For image generation on the CIFAR10 $(32\\times32)$ dataset, we utilize the same U-net structure as DDIM [2]. In contrast to DDIM which employs a linear schedule with $T=1000$ and a quadratic selection procedure to select sub-sampling steps, we use a linear schedule with $T=100$ and a linear selection procedure to select sub-sampling steps (for a fair comparison with truncated linear schedule) while training DDPM and DDIM. We use the Frechet Inception Distance (FID) as the quantitative metric. For image translation tasks, we employ a U-net structure with the same configuration as used in DDIM for CelebA [60] as the backbone. To increase the diversity of the generated images, the translated images are not fed into the network as conditional input. ", "page_idx": 17}, {"type": "text", "text": "A.5 Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Based on the derivations from the Sec. 2.1 and Sec. 2.2, the training and inference processes of Resfusion can be represented as Algorithm 1 and Algorithm 2. We highlight the modifications in our training and inference algorithms compared to DDPM in red. Just like the vanilla Denoising Diffusion Probabilistic Models (DDPM), Resfusion gradually fit $x_{t}$ to $x_{0}$ , implicitly reducing the residual term between $\\scriptstyle{\\hat{x}}_{0}$ and $x_{0}$ with the resnoise during the reverse inference process. Through transforming the learning of the noise term into the resnoise term, the form of resnoise-diffusion reverse inference process is consist with DDPM, leading to excellent interpretability. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Training Algorithm for ResfusionAlgorithm 2 Inference Algorithm for Resfusion   \nRequire: total diffusion steps $T$ , degradedRequire: total diffusion steps $T$ , degraded image .d groun\u221ad truth dataset $\\begin{array}{r l r}{D}&{{}=}&{\\hat{x}_{0}}\\end{array}$ , =p r1et1\u2212r\u2212a\u03b1it\u03b1n\u2212te1 d\u03b2 tResf\u221ausion model $r e s\\epsilon_{\\theta}$ . $(\\hat{x}_{0}^{n},x_{0}^{n})_{n}^{N}$ $\\begin{array}{r}{T^{\\prime}=\\arg\\operatorname*{min}_{i=1}^{T}\\left|\\sqrt{\\overline{{\\alpha}}_{i}}-\\frac{1}{2}\\right|}\\end{array}$ $\\begin{array}{r}{T^{\\prime}=\\arg\\operatorname*{min}_{i=1}^{T}\\left|\\sqrt{\\overline{{\\alpha}}_{i}}-\\frac{1}{2}\\right|}\\end{array}$ repeat Sample \u221a\u03f5 \u223cN(0, I\u221a) Sample $(\\hat{x}_{0}^{i},x_{0}^{i})\\sim D,\\epsilon\\sim N(0,I)$ $x_{T^{\\prime}}=\\sqrt{\\overline{{\\alpha}}_{T^{\\prime}}}\\hat{x}_{0}\\dot{+}\\sqrt{1-\\overline{{\\alpha}}_{T^{\\prime}}}\\epsilon$ Sample $t\\sim U n i f o r m(1,...,T^{\\prime})$ for $t=T^{\\prime},T^{\\prime}-1,...,2$ do R = x\u02c6\u221a0 \u2212x0 Sample $z\\sim N(0,I)$ $x_{t}=\\sqrt{\\overline{{\\alpha}}_{t}}x_{0}+\\big(1-\\sqrt{\\overline{{\\alpha}}_{t}}\\big)R+\\sqrt{1-\\overline{{\\alpha}}_{t}}\\epsilon$ xt\u22121 =\u221a1\u03b1t (xt \u2212\u221a1\u03b2\u2212t\u03b1t (res\u03f5\u03b8(xt, x\u02c60, t)) + $\\begin{array}{r}{r e s\\epsilon=\\epsilon+\\frac{(1-\\sqrt{\\alpha_{t}})\\sqrt{1-\\overline{{\\alpha}}_{t}}}{\\beta_{t}}R}\\end{array}$ take gradient step o\u03b2tn \u03b2tz until $\\nabla_{\\theta}^{\\check{\\mathbf{\\theta}}}||r e s\\epsilon-r\\dot{e}s\\epsilon_{\\theta}(x_{t},\\hat{x}_{0},t)||^{2}$ end for $\\begin{array}{r}{x_{0}=\\frac{1}{\\sqrt{\\alpha_{1}}}(x_{1}-\\frac{\\beta_{1}}{\\sqrt{1-\\overline{{\\alpha}}_{1}}}r e s\\epsilon_{\\theta}(x_{1},\\hat{x}_{0},1))}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "A.6 Resource efficiency ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We compare the parameters, multiply-accumulate operation (MACs) and inference time with other image restoration methods on ISTD [20] dataset, LOL [30] dataset and Raindrop [39] dataset by THOP, using $256\\times256$ images as the input. For ISTD dataset, PSNR and SSIM are evaluated at a resolution of $256\\times256$ after being resized. For LOL dataset and Raindrop dataset, the original image resolutions are maintained for the evaluation of PSNR and SSIM. The experimental results are quoted from the results of previous papers as well as our implementation based on open source code. ", "page_idx": 18}, {"type": "text", "text": "As shown in Table 7, for the ISTD dataset, compared to Shadow Diffusion [9], Resfusion has $5\\times$ fewer parameters, $5\\times$ fewer sampling steps, and $20\\times$ fewer MACs. For the LOL dataset, compared to LLDiffusion [7], Resfusion has $6\\times$ fewer sampling steps. For the Raindrop dataset, compared to RainDiff $\\mathrm{\\dot{\\Omega}_{128}}$ [5], Resfusion has $10\\times$ fewer parameters, $10\\times$ fewer sampling steps, and $50\\times$ fewer MACs. Experiments in shadow removal, low-light enhancement, and deraining demonstrate the effectiveness of Resfusion, enabling computationally constrained researchers to utilize our model for image restoration tasks. ", "page_idx": 18}, {"type": "text", "text": "Table 7: Resource efficiency and performance analysis by THOP on ISTD dataset, LOL dataset and Raindrop dataset. \u201cMAC\u201d means multiply-accumulate operation. The best and second-best results are highlighted in bold and underlined. \u201c\u2191\" (resp. \u201c\u2193\") means the larger (resp. smaller), the better. We use the symbol \u201c-\" to indicate models or results that are unavailable. ", "page_idx": 18}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/91a2b10dca593129a00e9eba7ffba3a1998b30ceeeed2b4f0c5807ea1ae8b1f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/5e7bb9de7eb1640681e2ba8bd94fc1b9fcda32909397d304c059af013c16154f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 10: (a) Visualization of the relationship between the error coefficient and T. Technically, we can use the Truncated Schedule to eliminate this error when $T$ is small. (b) Visual comparisons between Truncated Schedule and Original Schedule under five sampling steps $(T^{\\prime}/T=5/12)$ . In terms of visual perception, the absence of Truncated Schedule will lead to residual shadows. ", "page_idx": 19}, {"type": "text", "text": "We observed that in the actual diffusion forward process, the noise addition steps are uniformly spaced and discrete. The discontinuity of diffusion steps implies that when we approximate the acceleration point using Eq. (8), the offset of this approximate acceleration point relative to the ideal acceleration point is unavoidable, because ensuring the existence of intersection point with no offset requires that the gray arrow and the violet arrow in Fig. 2 must be continuous. This offset actually quantifies the confidence level of the approximate equivalence $x_{T^{\\prime}}\\approx\\hat{x}_{T^{\\prime}}$ in Eq. (15). When $\\mathrm{T}$ is small, the diffusion steps are divided sparsely, and the offset can be unacceptable. The absolute value of the offset can be derived as Eq. (24). ", "page_idx": 19}, {"type": "equation", "text": "$$\n||x_{T^{\\prime}}-\\hat{x}_{T^{\\prime}}||=||(2\\sqrt{\\bar{\\alpha}_{T^{\\prime}}}-1)x_{0}+(1-2\\sqrt{\\bar{\\alpha}_{T^{\\prime}}})\\hat{x}_{0}||=||(1-2\\sqrt{\\bar{\\alpha}_{T^{\\prime}}})R||\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As shown in Figure 10 (a), the absolute offset $||(1-2\\sqrt{\\overline{{\\alpha}}_{T^{\\prime}}})R||$ exponentially decreases with the increase of $T$ . When $T$ is relatively small, this error is not negligible. However, this potential instability can be avoided in practical experiments, with a noise schedule named Truncated Schedule based on the existing noise schedules. In order to control the offset, we define an offset threshold $h$ with a default value of 0.01. When \u221adecreasing $\\sqrt{\\overline{{\\alpha}}}_{t}$ , the first element less than \u221a0.5 is denoted as $\\sqrt{\\overline{{\\alpha}}_{r}}$ . If the difference between 0.5 and $\\sqrt{\\overline{{\\alpha}}_{r}}$ is greater than the offset threshold $h$ , $\\sqrt{\\overline{{\\alpha}}_{r}}$ will be reassigned to 0.5 and the following elements will be truncated from here. Since the diffusion steps after the acceleration point is not involved in the actual diffusion process, direct truncation can avoid potential risks. Taking the truncated linear schedule [29] and $T=25$ as an example, Fig. 11 demonstrates how to achieve the acceleration point $T^{\\prime}=10$ . As shown in Figure 10 (b), Truncated Schedule can effectively eliminate \"residual shadows\". ", "page_idx": 19}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/5a08fbf6b968281c9a99c5d0f592c6182dbfe4d1818432e2c8d9737307da3825.jpg", "img_caption": ["Figure 11: The schematic diagram of our truncated linear schedule. Taking $T\\,=\\,25$ , the left figure shows the 10 diffusion steps obtained by truncated linear schedule; the right figure shows the comparison of the truncated linear schedule and the linear schedule [29]. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.8 LOL-v2-real dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 8: Quantitative comparisons with other low-light enhancement methods on LOL-v2-real dataset. We report PSNR, SSIM and LPIPS. The best and second-best results are highlighted in bold and underlined. \u201c\u2191\" (resp. $\\downarrow\"$ ) means the larger (resp. smaller), the better. ", "page_idx": 20}, {"type": "table", "img_path": "JrIPBXWiS8/tmp/9aac3fae46c5ebcbba14487d7d777c46d347fc9bc09187903b66777d103466a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "The LOL-v2-real dataset [61] includes visual degradations such as decreased visibility, intensive noise, and biased color. It contains 689 image pairs of both low-light and normal-light versions for training and 100 image pairs for evaluation. All experimental settings are exactly the same as the LOL dataset. As shown in Figure 12, compared to Histogram Equalization, Resfusion can significantly reduce noise, while also achieving a better color offset, demonstrating strong denoising capabilities. We provide results in terms of PSNR, SSIM, and LPIPS on LOL-v2-real dataset in Table 8. ", "page_idx": 20}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/65ede7f07177947e4ba4333804a02a940199eded62e55a58f8d6b0742cc68a9f.jpg", "img_caption": ["Figure 12: Visual comparisons of the restored results by different image restoration methods on the LOL-v2-real dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.9 Limitations and Future work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Task specific. Our main effort has been directed towards creating a general prototype model for image restoration and generation. This approach may lead to some performance limitations when compared to task-specific state-of-the-art methods [9, 10]. To enhance performance for particular tasks, potential strategies include employing task-specific backbones, incorporating physical prior knowledge, and utilizing customized noise schedules. ", "page_idx": 20}, {"type": "text", "text": "Feature fusion. In the reverse process, we simply concatenate the noisy image $x_{t}$ at time step $t$ with the conditional input $\\scriptstyle{\\hat{x}}_{0}$ in the channel dimension. It is worth exploring more efficient feature fusion strategies, such as cross-attention, multi-stage, multi-scale, and multi-branch. ", "page_idx": 20}, {"type": "text", "text": "Latent space. The diffusion process in Resfusion is conducted in the original pixel space. Some studies [52, 62, 63] have shown that conducting diffusion process in the latent space [53, 64] can significantly reduce computational complexity while ensuring the quality of generated images, which is worth exploring in the future. ", "page_idx": 20}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/f94f86ea419d4669bf78e1af08a2709c72f554e65a723d8b97c02c6b36573c6b.jpg", "img_caption": ["Figure 13: More visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/c6cb74e4eee3f04918ff32e4f53d78c2b58ce1b10ee7739b09a5bf4286c1f4e3.jpg", "img_caption": ["Figure 14: More visual comparisons of the restored results by different low-light enhancement methods on the LOL dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/3b0ebeba69ea45e30e7653471297f8e0ba2784d43d0028dfe33a06a4a7d1e69c.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 15: More visual comparisons of the restored results by different deraining methods on the Raindrop dataset. ", "page_idx": 22}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/1bb0b97cce6b6dbcaaab7acdc234ca25a644027eba2bc2c7f7356fd2125aadc3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 16: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset. (failure cases) ", "page_idx": 22}, {"type": "image", "img_path": "JrIPBXWiS8/tmp/f34687d54114ad3802f6829e023605565d7385ae565caa4bc50c7137e7550fbb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 17: Visual comparisons of the restored results by different deraining methods on the Raindrop dataset. (failure cases) ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The principal assertions in the abstract and introduction accurately represent the paper\u2019s contributions and scope. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Appendix A.9. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: As shown in section 2 and Appendix A.1, we provide the full set of assumptions and a complete (and correct) proof. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide detailed information about our experimental setting in section 3 and Appendix A.4. We provide the anonymous inference results link in our supplementary material\u2019s README.md. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide our code and anonymous inference results link in our supplementary material. We used only open-source datasets for all our experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: As shown in section 3 and Appendix A.4, we specify all the training and test details. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: As shown in Appendix A.4 and Appendix A.6, we provide sufficient information on the computer resources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The tasks we selected are all existing public problems, and the datasets we used are widely utilized and publicly available. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited, and the licenses and terms of use are explicitly mentioned and properly respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The code and inference results introduced in the paper are well documented. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}]