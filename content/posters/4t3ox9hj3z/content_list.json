[{"type": "text", "text": "When are dynamical systems learned from time series data statistically accurate? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nicole Tianjiao Yang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jeongjin (Jayjay) Park School of Computational Science and Engineering Georgia Institute of Technology Atlanta, GA 30332 jpark3141@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics Emory University Atlanta, GA 30322   \ntianjiao.yang@emory.edu ", "page_idx": 0}, {"type": "text", "text": "Nisha Chandramoorthy\u2217 Department of Statistics The University of Chicago Chicago, IL 60637 nishac@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conventional notions of generalization often fail to describe the ability of learned models to capture meaningful information from dynamical data. A neural network that learns complex dynamics with a small test error may still fail to reproduce its physical behavior, including associated statistical moments and Lyapunov exponents. To address this gap, we propose an ergodic theoretic approach to generalization of complex dynamical models learned from time series data. Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures. Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training. We verify our results on a number of ergodic chaotic systems and neural network parameterizations, including MLPs, ResNets, Fourier Neural layers, and RNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning a dynamical system from time series data is a pervasive challenge across scientific domains. Such data come from expensive experiments and high-fidelity numerical models that simulate the underlying nonlinear, often chaotic, processes. The learning challenge is to train on available data to produce output models that i) provably preserve known symmetries and invariances (e.g., conservation principles); and ii) are inexpensive surrogates for use in downstream computations such as optimization and uncertainty quantification. The field of physics-guided machine learning $[\\mathrm{vdGSB^{+}}20$ , FO22, LK22, KKLL21, RPK19, $\\mathrm{KKL}^{+}21]$ has emerged in response, rapidly integrating neural networks into data-driven modeling and prediction workflows for a wide variety of complex dynamics, from geophysical fluid flows to phase transitions in materials (see $[\\mathrm{KMA}^{+}21$ , $\\mathrm{CCC^{+}19]}$ for surveys). Yet rigorous generalization analyses of neural parameterizations in these applications, wherein the underlying dynamics can exhibit chaotic behavior, have been underexplored. ", "page_idx": 0}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/3817cc7f99b1fdd4dfa3381648d261cbaeeaca8d480adcfc539625b58350da25.jpg", "img_caption": ["Figure 1: A random orbit on the x-z plane obtained from RK4 integration of the Lorenz vector field ([Lor63] (first column), Neural ODE, \u2018MSE_MLP\u2019, trained with mean-squared loss (second column) and Neural ODE, \u2018JAC_MLP\u2019, trained with Jacobian loss (third column). The last column shows the empirical PDF of the orbit generated by the true (gray), \u2018MSE_MLP\u2019 (red) and \u2018JAC_MLP\u2019 (blue) models. Experimental settings and additional results are in Appendices B and C respectively. Gist: A model trained well with MSE can produce atypical orbits but when Jacobian information is added to the training, it reproduces the long-term/statistical behavior accurately. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Here we investigate data-driven neural parameterizations of chaotic ODEs/PDEs and maps (discretetime dynamical systems) motivated by the following observation: a neural representation that is learned well, i.e., with a small generalization error, can still produce unphysical long-term or ensemble behavior. Figure 1 (left) shows the classical Lorenz $^{\\,\\!}63$ chaotic attractor [Lor63] plotted using a long random trajectory/orbit (time integration of the Lorenz equation vector field starting from a random initial condition, which is indicated by a $\"+\"$ sign). The second column shows an orbit from a neural network model, \u2018MSE_MLP\u2019, which minimizes the mean-squared error in the represented vector field at 10,000 training points and shows high accuracy $\\zeta<5\\%$ average relative error) over 8000 test points on the attractor. Surprisingly, Figure 1(column 2) shows that the learned \u2018MSE_MLP\u2019 Neural ODE [CRBD18] model produces an atypical orbit \u2013 an orbit different from almost every orbit of the true system \u2013 for the same randomly chosen initial condition. As a result, the learned empirical distribution is not close to the physical distribution \u2013 that of almost every true orbit, as shown in Figure 1 (column 4). On the other hand, the \u2018JAC_MLP\u2019 model, which is obtained by minimizing the mean-squared error in the vector field and its first derivative (Jacobian matrix), reproduces the Lorenz \u201963 attractor and the physical distribution on the attractor (Figure 1, column 4). The \u2018JAC_MLP\u2019 also captures all the Lyapunov exponents (LEs) \u2013 measures of asymptotic stability to perturbations, which are invariants in ergodic systems \u2013 accurately, while the \u2018MSE_MLP\u2019 only obtains the leading LE accurately. ", "page_idx": 1}, {"type": "text", "text": "Naturally, we ask about the wider applicability of these observations. For any ground truth dynamical system, how can we quantify the probability of success, including obtaining sample complexity results, of learning its physical or typical behavior? That is, how do we redefine and extend generalization analyses to neural representations of complex dynamical systems? To answer these questions, we start with a deceivingly simple supervised learning setup: given $m$ samples from a time series, $\\{(x_{t},x_{t+1})\\}_{t}$ , $0\\leq t\\leq(m-1)$ , how can we learn a model, $F_{\\mathrm{nn}}$ , such that, i) $x_{t+1}\\approx F_{\\mathrm{nn}}(x_{t})$ , for all time $t$ , and ii) the underlying distribution of the states $x_{t}$ and other dynamical invariants such as Lyapunov exponents are reproduced by orbits of $F_{\\mathrm{nn}}$ ? It is widely accepted that learning such an $F_{\\mathrm{nn}}$ involves matching orbits of $F_{\\mathrm{nn}}$ with $x_{t}$ over large $t$ during training. However, small errors propagate over orbits, by definition, in a chaotic system, leading to training instabilities. In response, a vast literature has been dedicated to developing sophisticated training models based on RNNs $\\mathrm{[PLH^{+}17}$ , $\\mathrm{PWF^{+}}18\\$ , RM21], operator learning $[\\bar{\\mathrm{LLSK}^{+}}22]$ and regularizations [LG22, FJNO20]. ", "page_idx": 1}, {"type": "text", "text": "We focus instead on the empirical risk minimization (ERM) for $F_{\\mathrm{nn}}$ that does not explicitly use the temporal correlations/dynamical structure in the data, avoiding training instabilities. Thus, we attempt to characterize when an elementary regression problem $F_{\\mathrm{nn}}$ can still lead to learning a physical representation, leading to a practical theory of learning chaotic systems from data. Our specific contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Motivated by extensive empirical results, we develop useful notions of generalization that characterize a model\u2019s ability to reproduce dynamical invariants.   \n\u2022 We develop new dynamics-aware generalization bounds for minimization of errors in the $C^{r}$ , $r=0,1$ , topology.   \n\u2022 We leverage shadowing theory from dynamical systems to rigorously characterize failure modes in learning statistically accurate models. ", "page_idx": 1}, {"type": "text", "text": "2 Generalization of parameterized ergodic dynamics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we motivate, through illustrative examples, the need for a dynamics-aware definition of generalization in the context of learning from time series data. We introduce concepts from dynamical systems as needed for a self-contained presentation. ", "page_idx": 2}, {"type": "text", "text": "Dynamics.: A map, or a discrete-time dynamical system, $F.$ , is a function on a closed and bounded (compact) set, $\\boldsymbol{M}\\dot{\\subset}\\mathbb{R}^{d}$ , which is called the state/phase space of the dynamics. We exclude scenarios where the dynamics can be unbounded, and focus on settings where $\\dot{F}\\in C^{1}(M)$ is a differentiable function on $M$ . We denote by $F^{t}$ , $t\\in\\mathbb{Z}^{+}$ , the iterates of the dynamics, or the compositions of $F$ with itself $t$ times, i.e., $F^{t}=\\dot{F}\\circ F^{t-1}$ . An orbit or trajectory of ${\\overline{{F}}}^{t}$ starting at an initial state $x\\in M$ is the sequence $\\{F^{t}(x)\\}_{t\\in\\mathbb{Z}^{+}}$ . In practice, $F$ may be a numerical ODE solver that approximates the continuous-time solutions, $\\bar{\\varphi^{t}},t\\,\\in\\,\\mathbb{R}^{+}$ , of the ODE (written in dynamical systems notation2 ): $d\\varphi^{t}(x)/d t=v(\\varphi^{t}(x))$ , where $v:M\\to T M$ is the true vector field describing the governing equations. Fixing some $\\bar{\\tau}\\in\\mathbb{R}^{+}$ , $F:=\\varphi^{\\tau}$ . We say a map $F$ is chaotic if there exists a subbundle of $T M$ , called the unstable subbundle, where infinitesimal perturbations grow exponentially under the linearization (Jacobian map), $d F^{t}$ , of the dynamics. The true map $F$ generates a deterministic, autonomous system (see Appendix A). ", "page_idx": 2}, {"type": "text", "text": "Physical measures. A probability measure $\\mu\\,:\\,M\\,\\rightarrow\\,\\mathbb{R}^{+}$ is a $p$ hysical measure [You02] for the dynamics $F$ if it is a) $F$ -invariant, b) ergodic and c) observable through $F$ . A measure $\\mu$ is observable if time-averages along any orbit starting from a randomly chosen initial point converge to constants that are expectations (phase space average) with respect to $\\mu$ . That is, for any $f\\in\\mathcal{C}(M)$ , $\\begin{array}{r}{(1/T)\\sum_{t\\leq T}f(F^{t}(x))\\xrightarrow{T\\rightarrow\\infty}\\mathbb{E}f(x)}\\end{array}$ , for any initial point $x$ chosen Lebesgue a.e. on a set $U\\subseteq M$ . The orbits starting almost everywhere on the basin of attraction, $U.$ , asymptotically enter a set, $\\Lambda$ , called the attractor. In dissipative chaotic systems, the attractor, $\\Lambda$ , which is the compact support of the physical measure $\\mu$ , has Lebesgue measure 0. Consequently, $\\mu$ may not have a probability density, that is, $\\mu$ is not absolutely continuous, or is singular, with respect to Lebesgue measure. ", "page_idx": 2}, {"type": "text", "text": "Neural ODE. Introduced in [CRBD18], a Neural ODE, denoted here by, $v_{\\theta}\\,:\\,M\\,\\rightarrow\\,\\mathbb{R}^{d}$ , with parameters, $\\theta$ , is a vector field represented by a neural network. The vector field can be time integrated to obtain solutions $\\varphi_{\\theta}^{t}:\\overline{{\\mathbb{R}^{d}}}\\rightarrow\\mathbb{R}^{d},t\\in\\mathbb{R}^{+}$ to the ODE, $d\\varphi_{\\theta}^{t}(x)/d t\\,=\\,v_{\\theta}(\\varphi_{\\theta}^{t}(x))$ . As noted in the introduction, suppose we have $n$ distinct pairs $S=\\{(x_{i},F(x_{i}))\\}_{i\\in[n]}$ , which could come from a single orbit of $F$ , as our training data. We train the Neural ODE by solving an ERM for the loss, $\\ell$ , that can be chosen to be a square loss, e.g., $\\ell(x,\\varphi_{\\theta}^{\\tau})=\\|\\varphi_{\\theta}^{\\tau}(x)-F(x)\\|^{\\tilde{2}}$ . That is, we solve for $\\theta$ that minimizes the training loss, $\\textstyle(1/n)\\sum_{x\\in S}{\\dot{\\ell}}(x,{\\\\\\dot{\\theta}})$ . Note that the true ODE or vector field is not explicitly used in training, only the solution map at some time intervals, $F$ . We refer to the map, $x\\rightarrow\\varphi^{\\tau}(x)$ , as $F_{\\mathrm{nn}}$ , or neural representation of the target map, $F$ . ", "page_idx": 2}, {"type": "text", "text": "Data and optimization. Suppose we use an $m$ -length orbit as our training data, i.e., $x_{i+1}=F(x_{i})$ , then, the data are not, strictly speaking, independent. However, a feature of chaotic systems is an exponential decay of correlations, and so training data of the form, $\\{(x_{i},F^{\\omega}(x_{i}))\\}$ starting from an initial condition $x_{0}\\sim\\mu$ , can be thought of as iid, for a large enough $\\omega\\in\\mathbb{N}$ . In that case, the learned map $F_{\\mathrm{nn}}$ represents the function $F^{\\omega}$ . During optimization, infinitesimal linear perturbations, will have to be evolved for time $\\omega$ . Since adjoint solutions blow up exponentially, a longer $\\omega$ will lead to training instabilities. To avoid numerical difficulties in training and focus on issues surrounding generalization, we choose $\\tau:=\\delta t$ , a small time step, to define the target map $F$ and learn a neural network representation of this function. When viewed this way, the generalization of Neural ODEs can be analyzed through the conventional lens of supervised learning with the loss, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(x,F_{\\mathrm{nn}})=\\|F_{\\mathrm{nn}}(x)-F(x)\\|^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $F_{\\mathrm{nn}}:=\\varphi^{\\delta t}$ is a neural network representing the map. For a map $h:M\\to M$ , we define the training and generalization errors in the usual way: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{R}_{S}(h)=(1/m)\\sum_{i=1}^{m}\\ell(x_{i},h),\\quad R(h)=\\mathbb{E}\\ell(x,h),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/7bb96b959492472a1e8eaf65e3d3917a033febfe705a52cad46b00c4587e0630.jpg", "img_caption": ["Figure 2: Learned and true LEs computed over 30,000 time steps using the QR algorithm of Ginelli et al [GCLP13] starting from 10,000 random initial states. The \u201cMSE\u201d and \u201cJAC\u201d labels indicate computations using the Neural ODE models trained with the loss functions in (1) and (3) respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where the expectation is over the data distribution, which may be $\\mu$ or any initial probability distribution of the states. ", "page_idx": 3}, {"type": "text", "text": "Example. To illustrate our conceptual findings surrounding generalization, we use the canonical Lorenz $^{\\prime}63$ system, which is a 3-variable reduced order model of atmospheric convection [Lor63]. The ODE can be written as $d\\varphi^{\\mathrm{t}}(x)/d t\\,=\\,v(\\varphi^{\\mathrm{t}}(x))$ , where the vector field $v$ is given by $v(x)=$ $[\\sigma(\\mathrm{y-x}),\\mathrm{x}(\\rho-\\mathrm{z})-\\mathrm{y},\\mathrm{xy-\\beta}\\mathrm{z}]^{\\top}$ , and $\\boldsymbol{x}=[\\mathbf{x},\\mathbf{y},\\mathbf{z}]^{\\top}$ are the coordinate functions of the state $x$ . We use the standard values of the parameters $\\sigma=10,\\rho=28,\\beta=8/3$ , at which the solutions are chaotic. We use the Runge-Kutta 4-stage time integrator with a time step size of 0.01 to define the map $F.$ . That is, $F(x)$ is the solution $\\varphi^{0.01}(x)$ approximated by an RK4 time-integrator. Our Neural ODE map, $F_{\\mathrm{nn}}$ , is learned to approximate $F$ by solving the above optimization with $n=10,000$ training points along an orbit. We illustrate our numerical results on various Neural ODE models and architectures that have fully connected layers, ResNet blocks with convolutional layers, and Fourier neural operators $[\\mathrm{LKA}^{+}20]$ . Many such models learn accurate representations of the true vector field $v$ , as evidenced by small training and test errors (sample average approximation of the generalization error in (2) over 8,000 points from the data distribution). These are shown in Figure 3 (Appendix C), while other hyperparameter and optimization settings are in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Statistical measures and Lyapunov exponents. Our numerical results test the accuracy of models beyond generalization error as defined in (2). In particular, we compute time-averages using the Neural ODE models and compare against expectations with respect to $\\mu$ obtained from the true equations. For the Lorenz system, we note that time averages obtained from the models with small generalization errors can be inaccurate. That is, even if the errors in the vector field are small (see also Figure 4 in Appendix C), the time-averages can match poorly. This is described for the best performing Neural ODE model in Table 3. The discrepancy in the learned distribution is shown in terms of Wasserstein distance computed using empirical distributions on long orbits (of length 50,000). We find that the Neural ODE model does not learn the ground truth statistics even if the training data include transient dynamics off the attractor. ", "page_idx": 3}, {"type": "text", "text": "Lyapunov exponents, roughly speaking, measure the asymptotic exponential growth/decay of infinitesimal perturbations under the Jacobian map $d F$ . In an ergodic system, they are independent of the initial state and can be written as an expectation with respect to $\\mu$ . In this work, Lyapunov exponents are yet another dynamical invariant (statistical quantity) in ergodic systems that we use to evaluate the statistical fidelity of learned models. In a chaotic system, there is at least one positive Lyapunov exponent, and the number of positive Lyapunov exponents is the dimension of the unstable manifold. The Lorenz system has one positive Lyapunov exponent (LE), one zero LE (corresponding to a center direction, or the vector field $v$ itself) and one negative LE. The ground truth map $F.$ , which is a time $\\delta t$ approximation of the flow $\\varphi^{t}$ has a two-dimensional center-unstable manifold and a one-dimensional stable manifold. ", "page_idx": 3}, {"type": "text", "text": "In Figure 2, we plot the Lyapunov exponents obtained using a classical QR iteration-based algorithm [GCLP13]. The Neural ODE model, marked \u2018MSE\u2019, produces a reasonably close approximation of the ground truth value $(\\approx0.9)$ for the positive LE but obtains an incorrect approximation of the stable LE (true value $\\approx-14.5)$ . We note also that standard deviations in the LE values computed over different orbits is quite large, indicating some orbits with atypical behaviors. We show the $\\ell_{2}$ error in the computed LEs by the Neural ODE model in Table 3, and full details in Table 5. ", "page_idx": 3}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/4e88e752dd85b5adba9d5db964a4ef020a5f196d94a3447e2986de0e0b4f093f.jpg", "table_caption": ["Table 1: True vs. learned Lorenz system: comparison of statistics. $W^{1}$ : Wasserstein-1 Distance, \u039b: set of LEs, $\\hat{\\mu}_{T}$ : empirical distribution of an orbit $T$ . The subscript NN indicates quantities computed using NN models trained with different loss functions (MSE (1), JAC (3)). "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Jacobian-matching. We now consider Neural ODE models trained with the following loss function. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{\\lambda}(x,F_{\\mathrm{nn}})=\\|F_{\\mathrm{nn}}(x)-F(x)\\|^{2}+\\lambda\\|d F_{\\mathrm{nn}}(x)-d F(x)\\|^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $F_{\\mathrm{nn}}:M\\to M$ is a Neural ODE map, $d F_{\\mathrm{nn}}(x):T_{x}M\\to T_{x}M$ its $(d\\times d)$ Jacobian matrix at $x$ , and $\\lambda>0$ is a hyperparameter that scales the relative importance of the two terms. We train in the usual way, by solving an ERM problem to minimize the training error, $\\hat{R}_{S,\\lambda}$ , which is defined as before, but with the new Jacobian-matching loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{R}_{S,\\lambda}(h)=(1/m)\\sum_{i=1}^{m}\\ell_{\\lambda}(x_{i},h),\\quad R_{\\lambda}(h)=\\mathbb{E}\\ell_{\\lambda}(x,h).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We train Neural ODE models with similar architectures as above, and find best performing models in terms of the test error (approximation of the generalization error) in (4). As in the training with the loss function $\\ell$ , we find several accurate neural representations of the Lorenz map (see training and test loss plots in Appendix C). Their performance on statistical measures and Lyapunov exponent predictions is remarkably different, however. In Figure 1, a Neural ODE trained with the Jacobianmatching loss produces an attractor (column 3) that is visually similar to the Lorenz attractor. The empirical distributions match closely with the ground truth, as shown in column 5 of Figure 1 and in Table 3, where the model is marked with a \u2018JAC\u2019, while one trained with the loss $\\ell$ in (1) is indicated with an \u2018MSE\u2019. The LEs and statistical averages of the state are all accurately represented, although the model is not explicitly designed to learn temporal patterns in the data. The MSE models learn vector fields with comparable accuracy with the Jacobian-matching models. That is, they learn accurate representations of $F$ with high probability (over the data distribution), but failing to learn $d F$ accurately leads to statistical inaccuracy. Due to the presence of atypical orbits, the attractor and the physical measure are not reproduced. Even though our formulation of Neural ODEs is simply stated as an ERM for regression, the generalization errors $R(F_{\\mathrm{nn}})$ and $R_{\\lambda}(F_{\\mathrm{nn}})$ cannot determine whether $F_{\\mathrm{nn}}$ is a physical representation of the dynamics. ", "page_idx": 4}, {"type": "text", "text": "3 When generalization implies statistical accuracy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we observed that adding information about the Jacobian in the training process led to statistically-accurate learning. Does this observation apply more broadly to other ergodic systems? How can one further improve generalization? In this section, we provide answers to these questions by proving dynamics-aware generalization bounds for Neural ODEs. ", "page_idx": 4}, {"type": "text", "text": "Our ultimate goal is to minimize a statistical loss function rather than (1) or (3) since this measures how accurately a model $h$ can reproduce ergodic averages associated with $F$ . For instance, $\\begin{array}{r}{\\ell^{\\mathrm{stat}}(x,h)=\\operatorname*{sup}_{f\\in\\mathrm{Lip}_{1}}|\\mathbb{E}[f(x)]-\\operatorname*{lim}_{T\\rightarrow\\infty}(1/T)\\sum_{t\\leq T}f(h^{\\overline{{t}}}(x))|}\\end{array}$ . This is the Wasserstein $(W^{1})$ distance (expressed in its dual form) between the ergodic measure $\\mu$ and the ergodic measure associated with the orbit of $h$ , a learned model, starting at $x$ , $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}\\dot{\\mathrm{Unif}}\\{x,h(x),h^{2}(x),\\cdot\\cdot\\cdot\\cdot,h^{T}(x)\\}}\\end{array}$ . As noted, ERMs for this loss function do not have a straightforward implementation when the map $h$ sought is chaotic. Hence, we seek conditions under which solving ERMs defined with losses (1) and (3) can still minimize $\\ell^{\\mathrm{stat}}$ . To understand when solving regression problems for $F_{\\mathrm{nn}}$ lead to statistically accurate physical representations, we first assume that a notion of \u201cshadowing\u201d applies to $F$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 1. We say that the shadowing property applies to a map $F$ if for any $\\delta>0$ , there exists an $\\epsilon=\\mathcal{O}(\\delta)$ so that for any map $G$ with $\\begin{array}{r}{\\|G-F\\|_{1}:=\\operatorname*{sup}_{x\\in M}(\\|G(x)-F(x)\\|+\\|d G(x)-d F(x)\\|)\\leq}\\end{array}$ $\\epsilon$ , there exists a map $\\tau:M\\to M$ close to the identity such that $\\|G^{t}(\\tau(x))-F^{t}(x)\\|\\leq\\delta$ for all $t$ . ", "page_idx": 5}, {"type": "text", "text": "Intuitively, the shadowing property means that an orbit of a nearby dynamical system can closely follow a true orbit \u2013 called a shadowing orbit \u2013 of $F$ for all time. This kind of uniform-in-time shadowing is a classical result for a mathematically ideal class of chaotic systems, called uniformly hyperbolic systems (see Katok and Hasselblatt [KKH95] Ch 18; Appendix A). For a textbook presentation and extension of shadowing for dynamical systems with some hyperbolicity, see [Pil06]. For a uniformly hyperbolic $F$ (see Appendix A), we now assume that a neural representation $F_{\\mathrm{nn}}$ of $F$ trained with $n$ samples generalizes well in terms of $C^{1}$ -distance. That is, an ERM solution for the loss 3 (\u2018JAC\u2019 models in sections 1 and 2) generalizes so that $R_{\\lambda}(F_{\\mathrm{nn}})$ is small. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 $\\textstyle C^{1}$ generalization). Given $\\delta>0$ , there exist $\\mathcal{E}_{0},\\mathcal{E}_{1}>0$ and a function $(\\delta,\\mathcal{E}_{0},\\mathcal{E}_{1})\\rightarrow$ $\\tau(\\delta,\\mathcal{E}_{0},\\mathcal{E}_{1})\\in\\mathbb{N}$ such that $\\mathbb{E}_{x\\sim\\mu}\\|F(x)-F_{\\mathrm{nn}}(x)\\|\\le\\mathcal{E}_{0}$ and $\\begin{array}{r}{\\mathbb{E}_{x\\sim\\mu}\\|d F(x)-d F_{\\mathrm{nn}}(x)\\|\\leq\\mathcal{E}_{1}.}\\end{array}$ for all $m\\geq\\tau$ with probability $\\geq1-\\delta$ over the randomness of the training data from $\\mu^{m}$ . ", "page_idx": 5}, {"type": "text", "text": "We now make an optimistic assumption on a learned model, $F_{\\mathrm{nn}}$ , that satisfies the above definition of $C^{1}$ generalization. Using Hoeffding\u2019s inequality, we know that for any $\\delta_{0}\\;>\\;0$ , with probability at least $1\\,-\\,\\delta_{0}$ over the randomness of $x$ , $\\begin{array}{r}{\\|\\dot{F}(x)-F_{\\mathrm{nn}}(x)\\|\\;\\le\\;\\mathcal{E}_{0}\\,+\\,(\\operatorname*{sup}_{x\\in M}\\|\\bar{F(x)}\\,-\\,$ $F_{\\mathrm{nn}}(x)\\Vert)\\sqrt{\\log(2/\\delta_{0})}$ and $\\begin{array}{r}{\\|d F(x)-d F_{\\mathrm{nn}}(x)\\|\\leq\\mathcal{E}_{1}+(\\operatorname*{sup}_{x\\in M}\\|d F(x)-d F_{\\mathrm{nn}}(x)\\|)\\sqrt{\\log(2/\\delta_{0})}}\\end{array}$ . Given $\\delta>0$ , let $\\epsilon_{0}:=2\\mathcal{E}_{0}$ and $\\epsilon_{1}:=2\\mathcal{E}_{1}$ . Fixing $\\delta_{0}>0$ , suppose that the trained model $F_{\\mathrm{nn}}$ is such that $\\begin{array}{r}{(\\operatorname*{sup}_{x\\in M}\\|F(x)-F_{\\mathrm{nn}}(x)\\|)<\\epsilon_{0}/(2\\sqrt{\\log(2/\\delta_{0})})}\\end{array}$ and $\\begin{array}{r}{(\\operatorname*{sup}_{x\\in M}\\|d F(x)-d F_{\\mathrm{nn}}(x)\\|)\\leq}\\end{array}$ $\\epsilon_{1}/(2\\sqrt{\\log(2/\\delta_{0})})$ . Taking a union bound, with probability $>1-\\left(\\delta+\\delta_{0}\\right)$ , $\\|F(x)-F_{\\mathrm{nn}}(x)\\|\\leq\\epsilon_{0}$ and $\\|d F(x)-d F_{\\mathrm{nn}}(x)\\|\\leq\\epsilon_{1}$ . We enhance this inequality to obtain a stronger assumption on $F_{\\mathrm{nn}}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 $C^{1}$ strong generalization). Given $\\delta\\,>\\,0$ , there exist $\\epsilon_{0},\\epsilon_{1}\\,>\\,0$ and a function $(\\epsilon_{0},\\epsilon_{1},m)\\,\\rightarrow\\,n(\\epsilon_{0},\\epsilon_{1},m)\\,\\in\\,\\mathbb{N}$ such that $\\|F(F_{\\mathrm{nn}}^{t}(x))\\,-\\,F_{\\mathrm{nn}}^{t+1}(x)\\|\\;\\le\\;\\epsilon_{0}$ and $\\|d F(F_{\\mathrm{nn}}^{i}(x))-$ $d F_{\\mathrm{nn}}(F_{\\mathrm{nn}}^{t}(x))\\dag\\le\\epsilon_{1}$ for all $t\\leq n$ , $m\\ge\\tau(\\delta,\\epsilon)$ , and $n\\to\\infty$ as $m\\rightarrow\\infty$ , with probability $\\geq1-\\delta$ over the initial state $x$ . ", "page_idx": 5}, {"type": "text", "text": "That is, we assume that, with high probability, the trained model makes a small error at each time. This stronger notion of generalization can be satisfied when the true model shows a smooth linear response in its statistics [Bal14], or in practice, training is performed with points sampled at random near the attractor, as opposed to with a spin-off time to achieve a state on the attractor. Given a tuple, $(\\epsilon_{0},\\epsilon_{1})$ , an orbit with initial condition $x$ that satisfies, $\\|F(F_{\\mathrm{nn}}^{t}(x))-F_{\\mathrm{nn}}^{t+1}(x)\\|\\,\\le\\,\\epsilon_{0}$ and $\\begin{array}{r}{\\|\\bar{d}F(F_{\\mathrm{nn}}^{t}(x))-d F_{\\mathrm{nn}}(F_{\\mathrm{nn}}^{t}(x))\\|\\le\\epsilon_{1}}\\end{array}$ for all $t\\leq m$ will be referred to as an $(\\epsilon_{0},\\epsilon_{1})$ orbit. That is, at each time, the neural representation $F_{\\mathrm{nn}}$ is $(\\epsilon_{0},\\epsilon_{1})$ -close to the true map, $F.$ . Under this assumption, we can follow the proof of the Shadowing lemma (see e.g., Ch 18 of [KKH95]) for hyperbolic maps to show that a true orbit (of $F$ ) shadows every $(\\epsilon_{0},\\epsilon_{1})$ orbit. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (Shadowing). Let $F_{\\mathrm{nn}}$ be an approximation of $F$ that satisfies the $C^{1}$ strong generalization (Assumption $^{\\,l}$ ). Given any $\\delta>0$ , there exist $\\epsilon_{0},\\epsilon_{1},n$ such that every $(\\epsilon_{0},\\epsilon_{1})$ orbit is $\\delta$ -shadowed by an orbit of $F$ . That is, there is a true orbit, say, $\\{F^{t}(x)\\}$ , corresponding to every orbit, $\\{G^{t}(x^{\\prime})\\}$ such that $\\|F^{t}(x)-G^{t}(x^{\\prime})\\|\\leq\\delta_{}$ , for all $t\\leq n$ . ", "page_idx": 5}, {"type": "text", "text": "See section A.1 for the proof. Let $\\{x_{t}^{\\mathrm{nn}}\\}_{t\\leq n}$ be an $n$ -length orbit of $F_{\\mathrm{nn}}$ , i.e., $x_{t+1}^{\\mathrm{nn}}=F_{\\mathrm{nn}}(x_{t}^{\\mathrm{nn}})$ . We use $T_{n,\\mathrm{nn}}M$ to denote the direct sum $\\scriptstyle\\oplus_{i=1}^{n}T_{x_{i}^{\\mathrm{nn}}}M$ of tangent spaces along the orbit, $\\boldsymbol x_{t}^{\\mathrm{{nn}}}$ . The proof follows Theorem 18.1.3 of [KKH95] to apply contraction mapping on a compact ball in $T_{n,\\mathrm{nn}}M$ . ", "page_idx": 5}, {"type": "text", "text": "The above result defines, for each $(\\epsilon_{0},\\epsilon_{1})$ -orbit, $\\{x_{t}^{\\mathrm{nn}}\\}_{t}$ , a shadowing orbit, $x^{\\mathrm{sh}}:=\\{F(x_{t}^{\\mathrm{nn}}+v_{t})\\}_{t}$ , where $v=\\oplus_{t}v_{t}$ is the fixed point of the contraction map in the proof (section A.1). Let $\\mu_{n}^{\\mathrm{sh}}(x_{0}^{\\mathrm{nn}})$ be the empirical measure defined on the shadowing orbit corresponding to an $(\\epsilon_{0},\\epsilon_{1})$ -orbit, $\\{x_{t}^{\\mathrm{nn}}\\}_{t}$ , i.e., $\\mu_{n}^{\\mathrm{sh}}(x_{0}^{\\mathrm{nn}})=\\mathrm{Unif}\\{x_{0}^{\\mathrm{sh}},\\cdot\\cdot\\cdot,x_{t}^{\\mathrm{sh}},\\cdot\\cdot\\cdot,x_{n-1}^{\\mathrm{sh}}\\}.$ . A shadowing orbit is indeed an orbit of the true map $F$ , but, unexpectedly, it may be atypical for the physical measure, $\\mu$ . That is, for an atypical shadowing orbit, the time average, $\\left(1/n\\right)\\overleftarrow{\\sum}_{t\\leq n}f(x_{t}^{\\mathrm{sh}})$ does not converge to the expected value $\\mathbb{E}_{x\\sim\\mu}f(x)$ , as $n\\to\\infty$ . This means that the Wasserstein distance, $W^{1}(\\mu_{n}^{\\mathrm{sh}}(x_{0}^{\\mathrm{nn}}),\\mu)$ , does not converge to 0 as $n\\to\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "Given an initial condition, $x_{0}^{\\mathrm{nn}}$ , of an $(\\epsilon_{0},\\epsilon_{1})$ -orbit, the corresponding shadowing orbit may be typical with some probability (over the distribution of $x_{0}^{\\mathrm{nn}}$ ), and this probability of finding typical shadowing orbits is a property of the true dynamics, $F.$ . When $\\mu$ , the physical measure of $F.$ , is highly sensitive to perturbations of $F$ , (see [Rue09, Bal14] for surveys on linear response theory, the study of perturbations of statistics) this probability may be small. Although the connection between the sensitivity of statistics and the atypicality of shadowing orbits is not completely known, this can justify the differences in the statistical accuracy of neural parameterizations with good $C^{1}$ generalization (see Definition 2). A neural model $F_{\\mathrm{nn}}$ which generalizes well in the $C^{1}$ sense (Definition 2) can be thought of as a $C^{1}$ -smooth perturbation of the true dynamics $F$ . If smooth perturbations of $F$ can cause a large change in $\\mu$ , then even when the training size $m\\rightarrow\\infty$ , and the orbit length $n\\to\\infty$ , $W^{1}(\\mu_{n}^{\\mathrm{sh}}(x_{0}^{\\mathrm{in}}),\\mu)$ , may not converge to zero, resulting in a model that does not preserve the physical behavior of $F$ . On the other hand, for typical shadowing, this possibility is excluded and gives us a characterization of statistically accurate learning. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Statistically accurate learning). Let $F_{\\mathrm{nn}}$ be a model of $F$ that satisfies $C^{1}$ strong generalization. In addition, let $F_{\\mathrm{nn}}$ and $F$ be such that for any $\\delta>0$ , there exists an $\\epsilon_{2}>0$ so that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}W^{1}(\\mu_{n}^{\\mathrm{sh}}(x),\\mu)\\leq\\epsilon_{2}}\\end{array}$ with probability (over the randomness of $\\mathrm{~c~})\\geq1-\\delta$ . Then, for any $\\delta>$ 0, there exists an $\\epsilon>0$ such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}W^{1}(\\mathrm{Unif}\\{x,F_{\\mathrm{nn}}(x),\\cdot\\cdot\\cdot\\;,F_{\\mathrm{nn}}^{t}(x),\\cdot\\cdot\\cdot\\;,F_{\\mathrm{nn}}^{n}(x)\\},\\mu)\\leq\\epsilon}\\end{array}$ with probability $\\geq1-\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "Proof: Let $\\mu_{n}^{\\mathrm{nn}}(x):=\\mathrm{Unif}\\{x,F_{\\mathrm{nn}}(x),\\cdot\\cdot\\cdot\\,,F_{\\mathrm{nn}}^{t}(x),\\cdot\\cdot\\cdot\\,,F_{\\mathrm{nn}}^{n-1}(x)\\}$ be the empirical measure of an $n$ -length orbit of $F_{\\mathrm{nn}}$ starting at $x$ . Given a $\\delta\\ >\\ 0$ , we choose $(\\epsilon_{0},\\epsilon_{1})$ so that $C^{1}$ strong generalization is satisfied with probability $\\geq~1-\\delta/2$ . Thus, an initial condition $x$ is such that $\\overline{{\\{F_{\\mathrm{nn}}^{t}(x)\\}}}_{t\\leq n}$ is an $(\\epsilon_{0},\\epsilon_{1})$ orbit, where the tuple $(\\epsilon_{0},\\epsilon_{1})$ is as defined in Proposition 1, with probability $\\geq\\bar{1}-\\delta/2$ . Applying Proposition 1, we have, for any 1-Lipschitz function $f\\,:\\,M\\,\\rightarrow\\,\\mathbb{R}$ , $\\begin{array}{r}{(1/n)\\sum_{t\\leq n}|f(\\dot{F}_{\\mathrm{nn}}^{t}(\\bar{x_{}}))\\stackrel{}{-}\\bar{f}(F^{t}(\\bar{x}))|\\leq(1/n)\\sum_{t\\leq n}\\|F_{\\mathrm{nn}}^{t}(\\bar{x}))-\\bar{F}^{t}(x)\\|\\leq\\delta/2}\\end{array}$ . Taking a supremum over $f$ , $W^{1}(\\mu_{n}^{\\mathrm{sh}}(x),\\mu_{n}^{\\mathrm{nn}}(x))\\;\\leq\\;\\delta/2$ , with probability $\\geq~1~-~\\delta/2$ . By assumption, there exists some $\\epsilon_{2}~>~0$ such that $\\mathrm{lim}_{n\\rightarrow\\infty}\\,W^{1}(\\mu,\\mu_{n}^{\\mathrm{sh}}(x))\\;<\\;\\epsilon_{2}$ with probability $1\\,-\\,\\delta/2$ . Hence, $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}W^{1}(\\mu,\\mu_{n}^{\\mathrm{nn}}(x))\\leq\\operatorname*{lim}_{n\\to\\infty}W^{1}(\\mu_{n}^{\\mathrm{sh}}(x),\\mu_{n}^{\\mathrm{nn}}(x))+W^{1}(\\mu,\\mu_{n}^{\\mathrm{sh}}(x))\\leq\\epsilon_{2}+\\delta/2:=\\epsilon_{1}\\,.}\\end{array}$ , with probability $>1-\\delta$ , using triangle inequality and taking union bound. ", "page_idx": 6}, {"type": "text", "text": "This result explains why training to minimize Jacobian-matching loss (3) can lead to statistically accurate models, even though, long-time temporal patterns in the data are not learned explicitly by regression for the one-time map $F$ . Since $C^{0}$ generalization (i.e., small errors in(2)) is insufficient for learning shadowing orbits, and thus for Proposition 1 and Theorem 1 to hold, the models trained on MSE loss 1 are not expected to learn ergodic/statistical averages with respect to $\\mu$ . ", "page_idx": 6}, {"type": "text", "text": "When shadowing orbits are atypical with high probability, we observe numerically that $C^{1}$ generalization, i.e., training with Jacobian-matching loss, still does not produce statistically accurate dynamics, in line with the above theorem. For instance, for maps with atypical shadowing described in [CW21], we find that learned neural representations with Jacobian-matching do have good $C^{1}$ generalization (Figure 6), but do not exhibit good statistical accuracy and learn incorrect Lyapunov exponents (see Table 5, Plucked Tent map). ", "page_idx": 6}, {"type": "text", "text": "4 Dynamic generative models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "So far, we have focused on understanding the statistical accuracy of supervised learning of dynamical systems. Without any minimization of distances on the space of probability measures, we proved sufficient conditions under which regression with Jacobian-matching information can yield samples from $\\mu$ with high probability. A generative method is an unsupervised learning technique to train on samples from a target distribution to produce more samples (provably) from the target. Naturally, we can use several popular generative models for our target physical measure here, including scorebased methods [SE19, $\\mathrm{SSDK}^{+}20]$ , Variational Autoencoders (VAE) [RM15] or normalizing flows [RM15, $\\mathbf{P}\\mathbf{N}\\mathbf{R}^{+}21^{\\scriptscriptstyle+}$ ]. However, these methods neglect the dynamical relationships in the input samples. In other words, from a vanilla generative model of a physical measure, we cannot also recover the true dynamics. Thus, we focus here on Latent SDEs models from [LWCD20], which combine neural representations of dynamics with generative models. We reinterpret them as dynamic generative models, and analyze their ability to faithfully represent both the underlying dynamics as well as the physical measure. ", "page_idx": 6}, {"type": "text", "text": "In a dynamic generative model, we approximate $F^{t}$ with a stochastic map, that can written as, $F_{\\mathrm{ls}}:=\\dot{f}_{\\theta}\\circ\\Phi_{\\phi}^{t}\\circ g_{\\phi}$ , where the subscript ls stands for \u201clatent SDE\u201d. Here, the function $g_{\\phi}:\\mathbb{R}^{d}\\rightarrow$ $\\mathbb{R}^{d_{l}}$ , with learnable parameters, $\\phi$ , is a (possibly stochastic) embedding from the data to latent space $(\\mathbb{R}^{d_{l}})$ , such that, $g_{\\phi\\sharp}\\mu\\:=\\:q_{\\phi,0}$ . Recall the pushforward notation (\u266f), i.e., if $x~\\sim~\\mu$ , then, $g_{\\phi}(x)\\,\\sim\\,q_{\\phi,0}$ . The dynamical system $\\Phi_{\\phi}^{t}\\,:\\,\\mathbb{R}^{d_{l}}\\,\\rightarrow\\,\\mathbb{R}^{d_{l}}$ acts on the latent space, and defines a sequence of pushforward distributions $\\Phi_{\\phi\\sharp}^{t}q_{\\phi,0}=q_{\\phi,t}$ . A special case of this setup is a \u201clatent ODE\u201d, where $\\Phi_{\\phi}^{t}$ is a deterministic map instead. With a stochastic latent SDE model instead, we observe, in line with [LWCD20], that the multimodal distribution of the Lorenz $^{\\,\\!}63$ attractor is reproduced better. That is, $\\Phi_{\\phi}^{t}$ is a solution map of a Neural SDE: $d\\Phi_{\\phi}^{t}(z)=w_{\\phi}(t,\\Phi_{\\phi}^{t}(z))d t+\\sigma_{\\phi}(t,\\Phi_{\\phi}^{t}(z))\\circ d W_{t}$ . Here the drift term, $w_{\\phi}$ , and the diffusion term, $\\sigma_{\\phi}$ , are represented as neural networks. The decoder $f_{\\theta}:\\mathbb{R}^{d_{l}}\\,\\rightarrow\\,\\mathbb{R}^{d}$ is a deterministic map that defines the conditional, $f_{\\theta\\sharp}q_{\\phi,t}\\,=\\,p_{\\theta}(\\cdot|Z_{t})$ . This dynamic VAE approach has been found to be expressive for chaotic systems (see Chapters 3 and 5 of [Kid22], [KMFL20]), wherein the conditional distribution, $z\\,\\rightarrow\\,q_{\\phi,0}\\big(z|X_{1:m}\\big)$ , of $Z_{0}$ given $X_{1:m}\\,:=\\,\\{F^{t}(x)\\}_{t\\leq m}$ is modeled as a Gaussian distribution whose learnable parameters are also denoted by $\\phi$ . Similarly, the conditional $p_{\\theta}\\big(\\cdot|Z_{t}\\big)$ is again modeled as a Gaussian with parameters $\\theta$ . The parameters, $\\phi$ and $\\theta$ respectively, of the encoder and the decoder are trained by maximizing the following dynamic version of the evidence lower bound (ELBO): $\\ell_{\\mathrm{ls}}(X_{1:m},(\\phi,\\theta)):=$ $\\begin{array}{r}{\\sum_{t=1}^{m}\\mathbb{E}_{z_{t}\\sim q_{\\phi,t}(\\cdot|X_{1:m})}[-\\operatorname{log}p_{\\theta}(x_{t}|z_{t})]+\\mathrm{KL}(q_{\\phi,0}(\\cdot|X_{1:m})||p_{Z_{0}})],}\\end{array}$ where the prior $p_{Z_{0}}$ follows a standard Gaussian distribution in the latent dimension. For alternatives to the ELBO objective above, such as the Wasserstein-GAN objective, we refer the reader to [Kid22]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We now evaluate both the learned dynamics, $F_{\\mathrm{ls}}$ , and the learned generative model, $p_{\\theta}$ , which approximates $\\mu$ . In Figure 11, we present the empirical distribution of a generated orbit against that of a typical orbit of the Lorenz system $(\\mu)$ . We observe that the distributions match well, nevertheless the vector field is not well-approximated. First, even though the system is deterministic, the learned $\\Phi_{\\phi}^{t}$ with minimum generalization error $(\\mathbb{E}\\ell^{\\mathrm{{stat}}})$ encountered in the hyperparameter search (Appendix C.8) is not, i.e, the diffusion term $\\sigma_{\\phi}$ is not zero. The learned stochastic map, $F_{\\mathrm{ls}}$ , produces an incorrect stable LE for the Lorenz system $(\\approx-11.8)$ , while the leading unstable LE matches reasonably well. ", "page_idx": 7}, {"type": "text", "text": "Since the map $F_{\\mathrm{ls}}$ , or its underlying vector field, on the latent space is not unique, we may obtain maps that do not preserve dynamical structure or invariants, even if the generated samples from $p_{\\theta}$ approximately capture $\\mu$ . As noted in section 2, the physical measure $\\mu$ is often singular, but absolutely continuous on lower-dimensional manifolds, leading to lack of theoretical guarantees for vanilla generative models [Pid22]. Finally, the sample complexity (and tight generalization bounds) of generative models, the above variational optimization, are not fully understood theoretically, especially for singular distributions (that satisfy the manifold hypothesis [BCV13]). We remark that since the minimax rates for approximating distributions have an exponential dependence on the dimension, exploiting the intrinsic dimension (unstable dimension) associated with the support of $\\mu$ will be key to tractable generative models for $\\mu$ in high-dimensional chaotic systems. Even in the Lorenz $^{\\,\\!}63$ system, we require $\\mathcal{O}(10^{6})$ samples for training reasonably accurate model in Figure 11, while the Jacobian-matching (Figure 1 column 5) produces smaller Wasserstein distances with fewer samples $(10^{4})$ , and a simpler regression problem as opposed to variational optimization above. ", "page_idx": 7}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments with the MSE and JAC losses in (1) and (3) respectively on many canonical chaotic systems: 1D tent maps, 2D Bakers map, 3D Lorenz $^{\\prime}63$ system and the Kuramoto Sivanshinsky equation (127 dimensional system after discretization) (Appendix C). In each system, we identify the Neural ODE model with the lowest generalization errors $R$ and $R_{\\lambda}$ in (2) and (4) respectively, by an architecture and optimization hyperparameter search (Appendix B) . On these models that generalize well, we perform tests of statistical accuracy and LE computations (as described in section 2 for the Lorenz $^{\\prime}63$ system). Consistent with Theorem 1, although most of the considered systems are not uniformly hyperbolic, we find that the JAC models are statistically accurate and reproduce the LEs, while the MSE models with $R$ comparable to $R_{\\lambda}$ are not statistically accurate. For the KS system for instance, the MSE models even overpredict the number of positive LEs. Interestingly, the best JAC model can learn more than half of the first 64 LEs, compared to the best MSE model that can learn only 2 out of $64\\:\\mathrm{LEs}$ , with $<10\\%$ relative error. The Python code is available at https://github.com/ni-sha-c/stacNODE. ", "page_idx": 7}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Neural ODEs and generalization. Introduced as continuous-time analogues [Wei17, HR17, CRBD18] of Residual neural networks (ResNets) [HZRS16], Neural ODEs [CRBD18, HR17] offer a vector field parameterization that can be time-integrated with an ODE solver. Augmented Neural ODEs [DDT19, RCD19, DA22] demonstrate improved expressivity for complex dynamics, and some universal approximation results appear in [ZGUA20, Kid22]. Neural ODEs [CRBD18] as normalizing flows [RM15] and for density estimation have been tackled in $[\\mathrm{GCB^{+}19}]$ . Training and regularization techniques that allow handling long time series are the subject of numerous works $[\\mathbf{RMM}^{+}20$ , $\\mathrm{KJD}^{+}21$ , FJNO20, PER23a, PER23b, PMSR21, $\\mathrm{GBD}^{+}20$ , MSKF21]. Our focus is different: we characterize when an elementary Jacobian-matching regularization serves as an inductive bias toward learning physical representations from irregular, even chaotic data. ", "page_idx": 8}, {"type": "text", "text": "Data-driven surrogates of complex systems. The vast and growing literature in the field of physicsinformed machine learning[VAUK22, LKB22, $\\mathrm{PABT}^{+}21$ , $\\mathbf{\\bar{H}}Z\\mathbf{B}^{+}\\mathbf{\\bar{2}}1$ , $\\mathrm{vdGSB}^{+}20$ , WY21, RPK19, WY21, KKLL21, LK22] has encouraged the adoption of physical machine learning models that preserve physical properties, symmetries and conservation laws, and are yet applicable in scientific problems [HKUT20, $\\bar{\\mathrm{PABT}}^{+}\\bar{2}1$ , $\\mathrm{HZB}^{+}21$ , RBL22, VAUK22, $\\mathrm{vdGSB}^{+}2\\mathrm{\\dot{0}}$ , $\\mathrm{YHP}^{+}23]$ . At the same time, several purely data-driven methods for complex systems [CNH20, $\\mathrm{PSH}^{+}22]$ , which do not require an expensive high-fidelity solver, have gained attention for their impressive prediction skill $[\\bar{\\mathbf{RC}}\\mathbf{V}\\mathbf{S}^{+}19\\$ , SLST17, $\\bar{\\mathrm{LSGW}}^{+}\\bar{2}3]$ ] and both faster training and inference making them suitable for optimization [BB21, RBL22, LP21, JD21] and inverse problems [HVT23, AEOV23]. Since the fundamental regression problem we study underlies both hybrid and data-driven methods, we provide insight into dynamics-aware generalization (e.g., reproducing ergodic behavior, Lyapunov exponents etc) applicable to different surrogate modeling approaches. Several innovative approaches for ensuring training stability [MMD22, $\\bar{\\mathrm{SWP}}^{+}24$ , HMBD23, JLOW24] in chaotic systems when using recurrent architectures have been proposed recently. The failure of generalization notions based on mean-squared error have also been noted in $[\\mathbf{S}\\mathbf{W}\\mathbf{P}^{\\downarrow}24$ , JLOW24] and empirical strategies and new definitions of generalization suitable of non-ergodic systems have been introduced in $[\\bar{\\mathrm{GHB}}^{+}24]$ . For ease of theoretical analysis, we do not consider these more sophisticated training approaches, choosing instead to learn short-term dynamics which obviates the need for stabilization strategies. Encouragingly, we observe low sample complexity of vanilla regression with Jacobian information to learn physical measures, when compared to the generative modeling approaches (which can be comparable to RNNs as well). A computational analysis of the Jacobian loss training compared to generative modeling/stablized recurrent training is deferred to a future work. ", "page_idx": 8}, {"type": "text", "text": "Ergodic theory and shadowing. Hyperbolic dynamics and ergodic theory (see e.g. the textbook [KKH95]) lay the foundation for understanding the long-time/statistical physical behavior [You02] of complex systems. The scientific computing community has leveraged ergodic theory and shadowing [Ano67, Bow75, Pil06] for rigorous computations that use high-fidelity numerical simulations of chaotic systems [GL24, Wan13, Ni21] and to analyze the correctness of numerical simulations [HYG87, CW21, Lia17, Sau05, GHYS90]. The novelty of our work lies in introducing shadowing as the basis for generalization, thus providing new analysis tools to understand the correctness of learned chaotic systems $[\\mathrm{LLSK}^{+}22]$ . An interesting direction for future work is to extend dynamics-aware generalization bounds similar to Theorem 1 to operator learning with Sobolev norms introduced in $\\bar{[\\mathrm{LLSK}^{+}22]}$ . ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our dynamics-aware generalization (Theorem 1) and empirical results provide a new characterization of statistical accuracy in models learned from dynamical data. These results open many avenues for improving mechanistic understanding and bridging the theory-practice gap in physical neural modeling of complex systems: ", "page_idx": 8}, {"type": "text", "text": "Understanding learning attractors. By exposing foundational problems in the elementary and fairly general setting of regression of a dynamical system, our analytical tools in section 3 broaden our conceptual understanding of learning from time series data in more complicated models and paradigms. ", "page_idx": 8}, {"type": "text", "text": "Learning dynamical representations vs. generative models. We show that physical measures can be produced by accurate neural representations of the dynamics, which can be more data and time-efficient compared with generative modeling for time series generated by chaotic systems. ", "page_idx": 9}, {"type": "text", "text": "Dynamics-aware learning of scientific models. Our results imply that generalization does not imply statistical accuracy and preservation of dynamical invariants and properties such as Lyapunov exponents, which is crucial for trustworthy ML for science. Thus, we reinforce the need to move toward a context-aware theory of generalization, organically unifying complex dynamics with learning theory. ", "page_idx": 9}, {"type": "text", "text": "Limitations: A limitation of the theoretical results about $C^{1}$ generalization is that we need to assume the typicality of shadowing. Empirically, the Jacobian can be expensive to estimate for high-dimensional scientific applications. We leave the study of statistical accuracy for learning atypical shadowing orbits, and the extension of an efficient algorithm for Jacobian information during training as a future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments We are grateful to the anonymous reviewers for their useful feedback and their suggestions of additional numerical results, which have strengthened the paper. NC acknowledges support from the James C. Edenfield faculty fellowship provided by the College of Computing at Georgia Tech. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[AEOV23] Harbir Antil, Howard C Elman, Akwum Onwunta, and Deepanshu Verma. A deep neural network approach for parameterized pdes and bayesian inverse problems. Machine Learning: Science and Technology, 4(3):035015, 2023.   \n[Ano67] D. V. Anosov. Geodesic flows on closed riemannian manifolds of negative curvature. Trudy Mat. Inst. Steklov., 90, 1967. [Bal14] Viviane Baladi. Linear response, or else. arXiv preprint arXiv:1408.2937, 2014. [BB21] William Bradley and Fani Boukouvala. Two-stage approach to parameter estimation of differential equations using neural odes. Industrial & Engineering Chemistry Research, 60(45):16330\u201316344, 2021.   \n[BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013. [BJS24] Oscar F Bandtlow, Wolfram Just, and Julia Slipantschuk. A numerical study of rigidity of hyperbolic splittings in simple two-dimensional maps. Nonlinearity, 37(4):045007, mar 2024. [Bow75] Rufus Bowen. $\\omega$ -limit sets for axiom a diffeomorphisms. Journal of differential equations, 18(2):333\u2013339, 1975. [BW14] Patrick J Blonigan and Qiqi Wang. Least squares shadowing sensitivity analysis of a modified kuramoto\u2013sivashinsky equation. Chaos, Solitons & Fractals, 64:16\u201325, 2014.   \n$[\\mathsf{C C C}^{+}19]$ Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborov\u00e1. Machine learning and the physical sciences. Rev. Mod. Phys., 91:045002, Dec 2019.   \n[CNH20] Ashesh Chattopadhyay, Ebrahim Nabizadeh, and Pedram Hassanzadeh. Analog forecasting of extreme-causing weather patterns using deep learning. Journal of Advances in Modeling Earth Systems, 12(2):e2019MS001958, 2020.   \n[CRBD18] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. [CW21] Nisha Chandramoorthy and Qiqi Wang. On the probability of finding nonphysical solutions through shadowing. Journal of Computational Physics, 440:110389, 2021. [CW22] Nisha Chandramoorthy and Qiqi Wang. Efficient computation of linear response of chaotic attractors with one-dimensional unstable manifolds. SIAM Journal on Applied Dynamical Systems, 21(2):735\u2013781, 2022.   \n[CZH21] Michele Coti Zelati and Martin Hairer. A noise-induced transition in the lorenz system. Communications in Mathematical Physics, 383:2243\u20132274, 2021. [DA22] Thai Duong and Nikolay Atanasov. Adaptive control of se (3) hamiltonian dynamics with learned disturbance features. IEEE Control Systems Letters, 6:2773\u20132778, 2022.   \n[DDT19] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. Advances in neural information processing systems, 32, 2019.   \n[EHL04] Gregory L Eyink, Tom WN Haine, and Daniel J Lea. Ruelle\u2019s linear response formula, ensemble adjoint schemes and l\u00e9vy flights. Nonlinearity, 17(5):1867, 2004.   \n[FJNO20] Chris Finlay, J\u00f6rn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In International conference on machine learning, pages 3154\u20133164. PMLR, 2020. [FO22] Alexander G Fletcher and James M Osborne. Seven challenges in the multiscale modeling of multicellular tissues. WIREs mechanisms of disease, 14(1):e1527, 2022.   \n$[\\mathrm{GBD}^{+}20]$ Arnab Ghosh, Harkirat Behl, Emilien Dupont, Philip Torr, and Vinay Namboodiri. Steer: Simple temporal regularization for neural ode. Advances in Neural Information Processing Systems, 33:14831\u201314843, 2020. [GC95] Giovanni Gallavotti and Ezechiel Godert David Cohen. Dynamical ensembles in nonequilibrium statistical mechanics. Physical review letters, 74(14):2694, 1995.   \n$[\\mathrm{GCB^{+}19}]$ Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. 7th International Conference on Learning Representations, ICLR 2019, 2019.   \n[GCLP13] Francesco Ginelli, Hugues Chat\u00e9, Roberto Livi, and Antonio Politi. Covariant lyapunov vectors. Journal of Physics A: Mathematical and Theoretical, 46(25):254005, 2013.   \n$[\\mathrm{GHB}^{+}24]$ Niclas G\u00f6ring, Florian Hess, Manuel Brenner, Zahra Monfared, and Daniel Durstewitz. Out-of-domain generalization in dynamical systems reconstruction. arXiv preprint arXiv:2402.18377, 2024. [GHL20] Bernard J Geurts, Darryl D Holm, and Erwin Luesink. Lyapunov exponents of two stochastic lorenz 63 systems. Journal of statistical physics, 179:1343\u20131365, 2020.   \n[GHYS90] Celso Grebogi, Stephen M. Hammel, James A. Yorke, and Tim Sauer. Shadowing of physical trajectories in chaotic dynamics: Containment and refinement. Phys. Rev. Lett., 65:1527\u20131530, Sep 1990. [GL24] Rhys E Gilbert and Davide Lasagna. On the application of optimal control techniques to the shadowing approach for time averaged sensitivity analysis of chaotic systems. SIAM Journal on Applied Dynamical Systems, 23(1):505\u2013552, 2024.   \n[HKUT20] Philipp Holl, Vladlen Koltun, Kiwon Um, and Nils Thuerey. phiflow: A differentiable pde solving framework for deep learning via physical simulations. In NeurIPS workshop, volume 2, 2020.   \n[HMBD23] Florian Hess, Zahra Monfared, Manuel Brenner, and Daniel Durstewitz. Generalized teacher forcing for learning chaotic dynamics. arXiv preprint arXiv:2306.04406, 2023. [HR17] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse problems, 34(1):014004, 2017. [HVT23] Benjamin Holzschuh, Simona Vegetti, and Nils Thuerey. Solving inverse physics problems with score matching. Advances in Neural Information Processing Systems, 36, 2023.   \n[HYG87] Stephen M Hammel, James A Yorke, and Celso Grebogi. Do numerical orbits of chaotic dynamical processes represent true orbits? Journal of Complexity, 3(2):136\u2013 145, 1987.   \n$[\\mathrm{HZB}^{+}21]$ Changnian Han, Peng Zhang, Danny Bluestein, Guojing Cong, and Yuefan Deng. Artificial intelligence for accelerating time integrations in multiscale modeling. Journal of Computational Physics, 427:110053, 2021.   \n[HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. [JD21] Elnaz Jamili and Vivek Dua. Parameter estimation of partial differential equations using artificial neural network. Computers & Chemical Engineering, 147:107221, 2021.   \n[JLOW24] Ruoxi Jiang, Peter Y Lu, Elena Orlova, and Rebecca Willett. Training neural operators to preserve invariant measures of chaotic attractors. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[Kid22] Patrick Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435, 2022. ", "page_idx": 12}, {"type": "text", "text": "$[\\mathrm{KJD}^{+}21]$ Suyong Kim, Weiqi Ji, Sili Deng, Yingbo Ma, and Christopher Rackauckas. Stiff neural ordinary differential equations. Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(9), 2021. [KKH95] Anatole Katok, AB Katok, and Boris Hasselblatt. Introduction to the modern theory of dynamical systems. Number 54. Cambridge university press, 1995.   \n$[\\mathrm{KKL}^{+}21]$ George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013 440, 2021.   \n[KKLL21] Sung Wook Kim, Iljeok Kim, Jonghwan Lee, and Seungchul Lee. Knowledge integration into deep learning in dynamical systems: an overview and taxonomy. Journal of Mechanical Science and Technology, 35:1331\u20131342, 2021.   \n$[\\mathrm{KMA}^{+}21]$ Karthik Kashinath, M Mustafa, Adrian Albert, JL Wu, C Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, R Wang, Ashesh Chattopadhyay, A Singh, et al. Physicsinformed machine learning: case studies for weather and climate modelling. Philosophical Transactions of the Royal Society A, 379(2194):20200093, 2021.   \n[KMFL20] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696\u20136707, 2020. [LG22] Alec J. Linot and Michael D. Graham. Data-driven reduced-order modeling of spatiotemporal chaos with neural ordinary differential equations. Chaos: An Interdisciplinary Journal of Nonlinear Science, 32(7):073110, 07 2022. [LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [Lia17] Shi-jun Liao. On the clean numerical simulation (cns) of chaotic dynamic systems. Journal of Hydrodynamics, Ser. B, 29(5):729\u2013747, 2017. [LK22] Yi Heng Lim and Muhammad Firmansyah Kasim. Unifying physical systems\u2019 inductive biases in neural ode using dynamics constraints. arXiv preprint arXiv:2208.02632, 2022.   \n$[\\mathrm{LKA}^{+}20]$ Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020. [LKB22] Yuying Liu, J. Nathan Kutz, and Steven L. Brunton. Hierarchical deep learning of multiscale differential equation time-steppers. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 380(2229):20210200, 2022.   \n[LLSK $^{+}22$ ] Zongyi Li, Miguel Liu-Schiaffini, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Learning chaotic dynamics in dissipative systems. Advances in Neural Information Processing Systems, 35:16768\u201316781, 2022. [Lor63] Edward N Lorenz. Deterministic nonperiodic flow. Journal of atmospheric sciences, 20(2):130\u2013141, 1963. [LP21] Kookjin Lee and Eric J Parish. Parameterized neural ordinary differential equations: Applications to computational physics problems. Proceedings of the Royal Society A, 477(2253):20210162, 2021.   \n[LSGW $^{+}23$ ] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global weather forecasting. Science, 382(6677):1416\u20131421, 2023.   \n[LWCD20] Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients for stochastic differential equations. In International Conference on Artificial Intelligence and Statistics, pages 3870\u20133882. PMLR, 2020.   \n[MMD22] Jonas Mikhaeil, Zahra Monfared, and Daniel Durstewitz. On the difficulty of learning chaotic dynamics with rnns. Advances in Neural Information Processing Systems, 35:11297\u201311312, 2022.   \n[MSKF21] James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pages 7829\u20137838. PMLR, 2021. [Ni21] Angxiu Ni. Approximating linear response by nonintrusive shadowing algorithms. SIAM Journal on Numerical Analysis, 59(6):2843\u20132865, 2021.   \n[PABT $^{+}21$ ] Grace CY Peng, Mark Alber, Adrian Buganza Tepole, William R Cannon, Suvranu De, Savador Dura-Bernal, Krishna Garikipati, George Karniadakis, William W Lytton, Paris Perdikaris, et al. Multiscale modeling meets machine learning: What can we learn? Archives of Computational Methods in Engineering, 28:1017\u20131037, 2021. [PER23a] Avik Pal, Alan Edelman, and Chris Rackauckas. Continuous deep equilibrium models: Training neural odes faster by integrating them to infinity. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 1\u20139. IEEE, 2023. [PER23b] Avik Pal, Alan Edelman, and Christopher Vincent Rackauckas. Locally regularized neural differential equations: some black boxes were meant to remain closed! In International Conference on Machine Learning, pages 26809\u201326819. PMLR, 2023. [Pid22] Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35852\u201335865. Curran Associates, Inc., 2022. [Pil06] Sergei Yu Pilyugin. Shadowing near an invariant set. Shadowing in dynamical systems, pages 1\u2013101, 2006.   \n$[\\mathrm{PLH^{+}}17]$ Jaideep Pathak, Zhixin Lu, Brian R. Hunt, Michelle Girvan, and Edward Ott. Using machine learning to replicate chaotic attractors and calculate lyapunov exponents from data. Chaos, 27(12), 2017. Cited by: 364; All Open Access, Hybrid Gold Open Access.   \n[PMSR21] Avik Pal, Yingbo Ma, Viral Shah, and Christopher V Rackauckas. Opening the blackbox: Accelerating neural differential equations by regularizing internal solver heuristics. In International Conference on Machine Learning, pages 8325\u20138335. PMLR, 2021.   \n$[\\mathbf{P}\\mathbf{N}\\mathbf{R}^{+}21]$ George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1\u201364, 2021. $[\\mathrm{PSH}^{+}22]$ Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.   \n$[\\mathbf{P}\\mathbf{W}\\mathbf{F}^{+}18]$ Jaideep Pathak, Alexander Wikner, Rebeckah Fussell, Sarthak Chandra, Brian R. Hunt, Michelle Girvan, and Edward Ott. Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model. Chaos, 28(4), 2018. Cited by: 206; All Open Access, Green Open Access. [RBL22] Johann Rudi, Julie Bessac, and Amanda Lenzi. Parameter estimation with dense and convolutional neural networks applied to the fitzhugh\u2013nagumo ode. In Mathematical and Scientific Machine Learning, pages 781\u2013808. PMLR, 2022. [RCD19] Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. Advances in neural information processing systems, 32, 2019.   \n$[\\mathrm{RCVS}^{+}19]$ Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno Carvalhais, and fnm Prabhat. Deep learning and process understanding for datadriven earth system science. Nature, 566(7743):195\u2013204, 2019. [RM15] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015. [RM21] Alberto Racca and Luca Magri. Robust optimization and validation of echo state networks for learning chaotic dynamics. Neural Networks, 142:252\u2013268, 2021.   \n$[{\\mathrm{RMM}}^{+}20]$ Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385, 2020. [R\u00f6s76] Otto E R\u00f6ssler. An equation for continuous chaos. Physics Letters A, 57(5):397\u2013398, 1976. [RPK19] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686\u2013707, 2019. [Rue09] David Ruelle. A review of linear response theory for general differentiable dynamical systems. Nonlinearity, 22(4):855, 2009. [Sau05] Tim Sauer. Computer arithmetic and sensitivity of natural measure. Journal of Difference Equations and Applications, 11(7):669\u2013676, 2005. [SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [SLST17] Tapio Schneider, Shiwei Lan, Andrew Stuart, and Jo\u00e3o Teixeira. Earth system modeling 2.0: A blueprint for models that learn from observations and targeted high-resolution simulations. Geophysical Research Letters, 44(24):12\u2013396, 2017.   \n$[\\mathrm{SSDK}^{+}20]$ Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n$[\\mathrm{SWP}^{+}24]$ Yair Schiff, Zhong Yi Wan, Jeffrey B Parker, Stephan Hoyer, Volodymyr Kuleshov, Fei Sha, and Leonardo Zepeda-N\u00fa\u00f1ez. Dyslim: Dynamics stable learning by invariant measure for chaotic systems. arXiv preprint arXiv:2402.04467, 2024.   \n[VAUK22] Pantelis R Vlachas, Georgios Arampatzis, Caroline Uhler, and Petros Koumoutsakos. Multiscale simulations of complex systems by learning their effective dynamics. Nature Machine Intelligence, 4(4):359\u2013366, 2022.   \nvdGSB $^+20^{\\circ}$ ] Erik van der Giessen, Peter A Schultz, Nicolas Bertin, Vasily V Bulatov, Wei Cai, G\u00e1bor Cs\u00e1nyi, Stephen M Foiles, M G D Geers, Carlos Gonz\u00e1lez, Markus H\u00fctter, Woo Kyun Kim, Dennis M Kochmann, Javier LLorca, Ann E Mattsson, J\u00f6rg Rottler, Alexander Shluger, Ryan B Sills, Ingo Steinbach, Alejandro Strachan, and Ellad B Tadmor. Roadmap on multiscale materials modeling. Modelling and Simulation in Materials Science and Engineering, 28(4):043001, mar 2020. [Wan13] Qiqi Wang. Forward and adjoint sensitivity computation of chaotic dynamical systems. Journal of Computational Physics, 235:1\u201313, 2013. [Wei17] Ee Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics and Statistics, 1(5):1\u201311, 2017. [WG18] Caroline L Wormell and Georg A Gottwald. On the validity of linear response theory in high-dimensional deterministic dynamical systems. Journal of Statistical Physics, 172:1479\u20131498, 2018. [WY21] Rui Wang and Rose Yu. Physics-guided deep learning for dynamical systems: A survey. arXiv preprint arXiv:2107.01272, 2021.   \n$[\\mathrm{YHP}^{+}23]$ Sungduk Yu, Walter Hannah, Liran Peng, Jerry Lin, Mohamed Aziz Bhouri, Ritwik Gupta, Bj\u00f6rn L\u00fctjens, Justus C. Will, Gunnar Behrens, Julius Busecke, Nora Loose, Charles Stern, Tom Beucler, Bryce Harrop, Benjamin Hillman, Andrea Jenney, Savannah L. Ferretti, Nana Liu, Animashree Anandkumar, Noah Brenowitz, Veronika Eyring, Nicholas Geneva, Pierre Gentine, Stephan Mandt, Jaideep Pathak, Akshay Subramaniam, Carl Vondrick, Rose Yu, Laure Zanna, Tian Zheng, Ryan Abernathey, Fiaz Ahmed, David Bader, Pierre Baldi, Elizabeth Barnes, Christopher Bretherton, Peter Caldwell, Wayne Chuang, Yilun Han, YU HUANG, Fernando Iglesias-Suarez, Sanket Jantre, Karthik Kashinath, Marat Khairoutdinov, Thorsten Kurth, Nicholas Lutsko, Po-Lun Ma, Griffin Mooers, J. David Neelin, David Randall, Sara Shamekh, Mark Taylor, Nathan Urban, Janni Yuval, Guang Zhang, and Mike Pritchard. Climsim: A large multi-scale dataset for hybrid physics-ml climate emulation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 22070\u201322084. Curran Associates, Inc., 2023. [You02] Lai-Sang Young. What are srb measures, and which dynamical systems have them? Journal of statistical physics, 108:733\u2013754, 2002. [You17] Lai-Sang Young. Generalizations of srb measures to nonautonomous, random, and infinite dimensional systems. Journal of Statistical Physics, 166:494\u2013515, 2017.   \n[ZGUA20] Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural odes and invertible residual networks. In International Conference on Machine Learning, pages 11086\u201311095. PMLR, 2020. [Zha17] Lingmei Zhang. A novel 4-d butterfly hyperchaotic system. Optik, 131:215\u2013220, 2017. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proofs and assumptions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our results in section 3 make the assumption that the map $F$ is uniformly hyperbolic. Roughly speaking, this means the uniform (on the attractor $\\Lambda\\subseteq M$ ) expansion and contraction of infinitesimal perturbations under the differential $d F$ . More precisely, in a uniformly hyperbolic system, there exists a decomposition of the tangent bundle, $T M\\,=\\,E^{u}\\oplus E^{s}$ , into an unstable $(E^{u})$ and stable $(E^{s})$ subbundle such that these subbundles are $d F$ -invariant, and moreover, for any vector field $v\\in E^{s}$ , $\\|d F^{t}v\\|\\leq C\\,\\lambda^{t}\\,\\|v\\|$ , $t\\,\\in\\,\\mathbb{Z}^{+}$ . Similarly, an infinitesimal perturbation along vector field $\\boldsymbol{v}\\in E^{u}$ shows a uniform exponential decay in norm backward in time, $\\|d F^{t}v\\|\\,\\leq\\,C\\,\\,\\lambda^{|t|}\\,\\,\\|v\\|$ , $t\\,\\in\\,\\mathbb{Z}^{-}$ . Here the constants $C,\\lambda$ are uniform over $\\Lambda$ . There are several relaxations of the uniformity in the splitting to obtain nonuniform and partial hyperbolicity; there are also several known examples of non-hyperbolic chaotic systems. Assumptions of hyperbolicity are common in dynamical systems analyses due to well-understood ergodic theoretic results, including existence of physical measures of the SRB-type [You02, You17]. Such assumptions are also widespread in computational dynamics, wherein algorithms derived rigorously for uniformly hyperbolic systems are found to be applicable [WG18] to turbulent fluids and chaotic systems in practice [GC95, EHL04, CW22], where a rigorous verification [BJS24] of these assumptions is not feasible. ", "page_idx": 16}, {"type": "text", "text": "We remark that we only require high-probability finite-time shadowing, and thus, our results can possibly be extended under relaxations of uniform hyperbolicity. Despite being mathematically convenient, the class of uniformly hyperbolic systems do contain examples of the pathological behaviors we address: atypicality of shadowing and large linear responses. Moreover, our numerical examples, including our primary one \u2013 the Lorenz $^{\\circ3}$ system, which is only singular hyperbolic \u2013 are chosen to not all be uniformly hyperbolic systems, in order for our inferences to have wider applicability. ", "page_idx": 16}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we complete a proof sketch that we begin in section 3. An element $v\\;\\in\\;T_{n,\\mathrm{{nn}}}M$ with $v=\\oplus_{t\\leq n}v_{t}$ can be identified as $t\\to v_{t}\\in T_{x_{t}^{\\mathrm{nn}}}M$ . We define a function $\\mathcal{F}_{\\mathrm{nn}}:T_{n,\\mathrm{nn}}M\\to T_{n,\\mathrm{nn}}M$ as $\\mathcal{F}_{\\mathrm{nn}}(v)_{\\underline{{t}}+1}\\,=\\,x_{t+1}^{\\mathrm{nn}}\\,-\\,F(x_{t}^{\\mathrm{nn}}+v_{t})$ . If $v$ t is a fixed point of ${\\mathcal{F}}_{\\mathrm{nn}}$ , that is, ${\\mathcal{F}}_{\\mathrm{nn}}(v)\\;=\\;v$ , then, $\\{x_{t}^{\\mathrm{nn}}+v_{t}\\}_{t\\leq n}$ is an orbit of $F$ . Writing $\\begin{array}{r}{\\mathcal{F}_{\\mathrm{nn}}(v)=\\mathcal{F}_{\\mathrm{nn}}(\\bar{0})+d\\mathcal{F}_{\\mathrm{nn}}(0)v+N_{\\mathrm{nn}}(v).}\\end{array}$ , where $N$ is the nonlinear part of ${\\mathcal{F}}_{\\mathrm{nn}}$ , a fixed point $v$ satisfies, $(\\mathrm{Id}-d\\mathcal{F}_{\\mathrm{nn}}(0))v=\\mathcal{F}_{\\mathrm{nn}}(0)+N_{\\mathrm{nn}}(v)$ . To show that $v$ exists, we follow Theorem 18.1.3 of [KKH95] and prove that $\\begin{array}{r}{\\dot{T}_{\\mathrm{nn}}(w)=(\\operatorname{Id}-d\\mathcal{F}_{\\mathrm{nn}}(0))^{-1}(N_{\\mathrm{nn}}(w)+}\\end{array}$ ${\\mathcal{F}}_{\\mathrm{nn}}(0))$ is a contraction on a compact subset of $T_{n,\\mathrm{nn}}M$ . We have $\\lVert\\mathcal{T}_{\\mathrm{nn}}(v)-\\mathcal{T}_{\\mathrm{nn}}(w)\\rVert\\leq\\lVert(\\mathrm{Id}-$ $d\\mathcal{F}_{\\mathrm{nn}}(0))^{-1}\\||N_{\\mathrm{nn}}(v)-N_{\\mathrm{nn}}(w)\\|\\leq C(\\epsilon_{0},\\epsilon_{1})\\|N_{\\mathrm{nn}}(v)-N_{\\mathrm{nn}}(w)\\|$ with probability $>1-\\delta$ . To see this, note that $d\\mathcal{F}_{\\mathrm{nn}}(0)(v)=d F\\,v$ , and $F$ is a hyperbolic map, by assumption, and hence Lemma 18.1.4 of [KKH95] applies. Next, assuming that $\\textstyle\\operatorname{sup}_{x\\in M}\\|d^{2}F({\\dot{x}})\\|$ is bounded (i.e., $d F$ is Lipschitz), we obtain that $d N_{\\mathrm{nn}}$ is Lipschitz, and hence, $\\begin{array}{r}{\\|\\mathcal{T}_{\\mathrm{nn}}(v)-\\mathcal{T}_{\\mathrm{nn}}(w)\\|\\le\\|(\\mathrm{Id}-d\\mathcal{F}_{\\mathrm{nn}}(0))^{-1}\\|\\|N_{\\mathrm{nn}}(v)-}\\end{array}$ $N_{\\mathrm{nn}}(w)\\|\\;\\leq\\;C(\\epsilon_{0},\\epsilon_{1})\\bar{K}\\delta_{0}\\|v\\,-\\,w\\|$ with probability $>\\;1\\:-\\:\\delta$ . Here, $\\operatorname*{max}_{t}\\{\\|v_{t}\\|,\\|w_{t}\\|\\}\\,\\leq\\,\\delta_{0},$ which is chosen independent of $n,\\epsilon_{0},\\epsilon_{1}$ and $\\delta$ , and $K$ is the Lipschitz constant of $d N_{\\mathrm{nn}}$ . Thus, ${\\mathcal{T}}_{\\mathrm{nn}}$ is a contraction on a $\\delta_{0}$ ball around $0\\in T_{n,\\mathrm{nn}}M$ when $C(\\epsilon)K\\delta_{0}<1-\\epsilon_{2}$ , for some $\\epsilon_{2}>0$ . Since $\\|\\mathcal{T}_{\\mathrm{nn}}(0)\\|\\,\\le\\,C(\\epsilon_{0},\\epsilon_{1})\\epsilon_{0}$ , and for any $v$ in a $\\delta_{0}$ ball around in 0 in $T_{n,\\mathrm{nn}}M$ , $\\|\\tau_{\\mathrm{nn}}(v)\\|\\le$ $C(\\epsilon_{0},\\epsilon_{1})\\epsilon_{0}+(1-\\epsilon_{2})\\delta_{0}$ , when $\\epsilon_{0},\\epsilon_{1}$ and $\\delta_{0}$ are such that $C(\\epsilon_{0},\\epsilon_{1})\\epsilon_{0}+(1-\\epsilon_{2})\\delta_{0}<\\delta_{0}$ , we have that ${\\mathcal{T}}_{\\mathrm{nn}}$ maps a $\\delta_{0}$ ball around in 0 in $T_{n,\\mathrm{nn}}M$ to itself. Thus, the unique fixed point (from contraction mapping theorem on the $\\delta_{0}$ ball) lies within the ball. ", "page_idx": 16}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this paper, the time series data from different chaotic systems are generated by simulating the ODEs and iterated function systems below. The ODEs were numerically integrated using the fourth-order Runge-Kutta solver from the torchdiffeq3 library [CRBD18], with an absolute and relative error tolerance of $10^{-8}$ . ", "page_idx": 16}, {"type": "text", "text": "We split the simulated trajectories into training and test datasets. The first 10,000 data points are used as the training data and the last 8,000 points are the test data. The time step size of the simulation is included in Table 2. ", "page_idx": 17}, {"type": "text", "text": "1D: Tent maps As our first examples, we choose three types of tent maps defined in [CW21], that are shown to be examples exhibiting atypical shadowing orbits: the tilted map (5), the pinched map (6), and the plucked map (7). We use $n=3$ and $s=0.2$ and $s=0.8$ in the plucked map following [CW21]. ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(x;s)=\\left\\{\\frac{2}{1+s}x,\\qquad\\qquad x<1+s,\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nF(x;s)=\\left\\{\\frac{4x}{1+s+\\sqrt{(1+s)^{2}-4s x}},\\qquad\\qquad x<1,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{s,n}(x)=\\operatorname*{min}(\\lambda_{s,n}(x),\\lambda_{s,n}(2-x)),\\qquad0<x<2,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{s}(x)=\\operatorname*{min}(\\frac{2x}{1-s},2-\\frac{2(1-x)}{1+s}),\\qquad x<1,}\\\\ {o_{s}(x)=\\left\\{\\frac{f_{s}(2x)}{2},\\qquad\\qquad x<0.5,\\right.}\\\\ {\\left.2-\\frac{f_{s}(2-2x)}{2},\\qquad x\\geq0.5,\\right.}\\\\ {\\left.\\lambda_{s,n}(x)=\\frac{o_{s}\\left(2^{n}x-\\lfloor2^{n}x\\rfloor\\right)}{2^{n}}+2\\frac{\\lfloor2^{n}x\\rfloor}{2^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2D: Baker\u2019s map We use the following perturbation of the classical Baker\u2019s map from [CW22]. ", "page_idx": 17}, {"type": "equation", "text": "$$\nF([\\mathrm{x},\\mathrm{y}]^{T};s)=\\left[\\frac{2\\mathrm{x}-\\lfloor\\mathrm{y}/\\pi\\rfloor2\\pi}{2}+\\lfloor\\mathrm{x}/\\pi\\rfloor2\\pi\\right]\\bmod2\\pi.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "3D: Lorenz $\\bf{563}$ We conduct extensive numerical experiments in this paper with the Lorenz \u201963 system [Lor63], with $\\sigma=10$ , $\\beta=8/3$ , and $\\rho=28$ , which we describe in section 2. ", "page_idx": 17}, {"type": "text", "text": "3D: R\u00f6ssler We use the parameter setting $a\\,=\\,0.2$ , $b=0.2$ , and $c=5.7$ in the R\u00f6ssler system [R\u00f6s76] below, which is in the chaotic regime. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d\\varphi^{\\mathrm{t}}}{d t}([\\mathrm{x},\\mathrm{y},\\mathrm{z}]^{\\top})=\\left[\\begin{array}{c}{-\\mathrm{y}-\\mathrm{z}}\\\\ {\\mathrm{x}\\!+\\!a\\mathrm{y}}\\\\ {b\\!+\\!z(\\mathrm{x}\\!-\\!c).}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "4D: Hyperchaos Another test case we consider is the hyperchaotic system below in 4 dimensions from [Zha17], where the parameter values are set as $a=16,b=40,c=20,d=8$ for the system to show chaotic behavior. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d\\varphi^{\\mathrm{t}}}{d t}(\\left[\\mathrm{x},\\mathrm{y},\\mathrm{z},\\mathrm{w}\\right]^{\\top})=\\left[\\begin{array}{c}{a\\mathrm{x}+d\\mathrm{z}-\\mathrm{yz}}\\\\ {\\mathrm{xz}-b\\mathrm{y}}\\\\ {c(\\mathrm{x}-\\mathrm{z})+\\mathrm{xy}}\\\\ {c(\\mathrm{y}-\\mathrm{w})+\\mathrm{xz}.}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "127D: Kuramoto-Sivashinsky To test Neural ODE\u2019s performance in learning high dimensional chaotic system, we generate the modified Kuramoto-Sivashinsky (KS) system\u2019s solution defined below with a second order finite difference scheme used in [BW14]. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial u}{\\partial t}=-(u+c)\\frac{\\partial u}{\\partial\\mathbf{x}}-\\frac{\\partial^{2}u}{\\partial\\mathbf{x}^{2}}-\\frac{\\partial^{4}u}{\\partial\\mathbf{x}^{4}}}\\\\ &{\\quad\\mathbf{x}\\in[0,L],\\quad t\\in[0,\\infty)}\\\\ &{\\displaystyle\\quad u(0,t)=u(L,t)=0}\\\\ &{\\displaystyle\\left.\\frac{\\partial u}{\\partial\\mathbf{x}}\\right|_{\\mathbf{x}=0}=\\frac{\\partial u}{\\partial\\mathbf{x}}\\right|_{\\mathbf{x}=L}=0}\\\\ &{\\displaystyle\\left.u(\\mathbf{x},0)=u_{0}(\\mathbf{x})\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.2 Architecture ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We try two neural network architectures for learning the vector field of the systems: a simple MultiLayer Perceptron (MLP) model and a ResNet [HZRS16]. For the MLP model, we use $\\mathrm{GELU^{4}}$ as the activation function, and ReLU for ResNet model. In addition, we experiment with adding Fourier layers $[\\mathrm{LKA}^{+}20]$ to represent the solution operator. We use the Latent SDE code from [LWCD20] for results in section 4. ", "page_idx": 18}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/e246d3d85ce01773b74ec6061aba30c013d9ba7d19682fb978d4d056dd2621d5.jpg", "table_caption": ["Table 2: Hyperparameter choices "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 Hyperparameter Search ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "When $\\ensuremath{\\boldsymbol{v}}_{F}(\\ensuremath{\\boldsymbol{{x}}})$ is a true vector field and ${v}_{h}(x)$ is a learned vector field by a neural network, $h$ , given solution $x$ , relative error can be defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathrm{relative~error}}(x)={\\frac{\\|v_{F}(x)-v_{h}(x)\\|}{\\|v_{F}(x)\\|}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using a grid search, hyperparameter values that yield the lowest relative error in the vector field as per (8) were chosen. Hyperparameter search results are shown in Tables 7, 8, and 9. Hyperparameter values that are different for each system are in the Table 2. We use the AdamW [LH17] optimization algorithm implemented in the PyTorch library for all our experiments. ", "page_idx": 18}, {"type": "text", "text": "In addition to MLP and Resnet, for Lorenz $^{\\ast}63$ , we train with FNOs $[\\mathrm{LKA}^{+}20]$ and latent SDE [LWCD20]. In the FNO network, we fix the number of modes to 4, with a batch size of 200 with 4 Fourier Neural layers. Further details on latent SDE are discussed in section C.8. ", "page_idx": 18}, {"type": "text", "text": "B.4 Computing ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Numerical experiments were conducted using Tesla A100 GPUs with 80GB and 40GB memory capacities. All experiments were completed in under one hour, with the exception of those involving the KS system. ", "page_idx": 18}, {"type": "text", "text": "C Additional numerical results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we present results that are described in section 2 for testing the statistical accuracy and generalization of learned models of the Lorenz $^{\\,\\!}63$ system. We also present additional results for the tent maps and the KS equation, described in the previous section. ", "page_idx": 19}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/43e44935a754186632d5aefa781f1fc5b0099e51539b481b56c6fab020e41567.jpg", "img_caption": ["C.1 Loss ", "Figure 3: Training and test loss of a representative Neural ODE trained with MSE (1) and Jacobianmatching loss (3). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2 Relative Error ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figures 4 and 5 illustrate the comparison of relative error trends, as defined in Eq. (8), for vector fields learned by a Neural ODE (ResNet) trained with mean squared loss and Jacobian-matching loss (as defined in Eq. (3)), for two distinct dynamical systems. ", "page_idx": 19}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/610bdad5237abd904bab54e01d82b2b9b5d4f7dc7bb142b64c4946385be1056f.jpg", "img_caption": ["Figure 4: Comparison of relative errors in the vector fields of the Lorenz $^{\\circ3}$ system produced by a Neural ODE with a ResNet, trained using mean squared loss (Left), and Jacobian-matching loss (Right) as defined in (1) and (3) respectively. The vector field is evaluated on a random true orbit. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 Tent map ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 6, we show learned models of representative tent map perturbations from the preceding section. We observe that the Jacobian-matching loss leads to reasonably accurate representations of the maps. Yet, the LE computed along learned orbits differ significantly at $s=0.8$ , as shown in Table 5. For various other perturbed tent maps, we show the computed LEs alongside the true LEs in Table 5; for most of the maps, the Jacobian-matching leads to accurate LE predictions, consistent with the results of Theorem 1. ", "page_idx": 19}, {"type": "text", "text": "C.4 Lorenz \u201863 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we show identical results to Figure 1 but with ResNet-based Neural ODE models as opposed to MLPs used in Figure 1. We find that Jacobian-matching training leads to superior performance in ", "page_idx": 19}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/6d2cf4ca3a1d33eddb78837f150c2d42eaaaf4f41f8f301e7eceb1b56dac8c50.jpg", "img_caption": ["Figure 5: Comparison of relative errors in the vector fields of the R\u00f6ssler system produced by a Neural ODE with a ResNet, trained using mean squared loss (Left), and Jacobian-matching loss (Right) as defined in (1) and (3) respectively. The vector field is evaluated on a random true orbit. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/d83b5fb2c888985bdeff69075dca4c78c7211ec8479c6d1b8f09f2a3f590b328.jpg", "img_caption": ["Figure 6: Comparison of true plucked tent map with generated one with Neural ODE trained with Jacobian-matching loss defined in Equation (3). When $s=0.8$ , we observe a failure mode of training with Jacobian-matching loss, possibly due to atypicality of shadowing orbits observed in [CW21]. Details on hyperparameter setting and other statistical results can be found in Table 2 and 5. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "terms of distributional match with $\\mu$ , when compared to MLPs. Furthermore, our overall inference about MSE models leading to atypical orbits still holds, agnostic to modeling and architectural choices. ", "page_idx": 20}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/0bb0806e5332f0df5171eca65f4b93bf522bf2cf2fa1e888bbf7f5c2c65cdae2.jpg", "img_caption": ["Figure 7: The first 3 columns show orbits on x-z plane obtained from RK4 integration of the Lorenz vector field ([Lor63], the Neural ODE, \u2018MSE_Res\u2019, trained with mean-square loss, and the Neural ODE, \u2018JAC_Res\u2019, trained with Jacobian-regularized loss, respectively (see section 2). The last two columns show the probability distribution of orbits generated by the true (gray), \u2018MSE_Res\u2019 (red) and \u2018JAC_Res\u2019 (blue) models. Experimental settings are in Appendix B. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.5 KS equation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "After spatial discretization and following the scheme of [BW14], we obtain a 127-dimensional dynamical system representing the KS equation, which we consider to be the ground truth map $F$ . Figure 8 shows the solutions of the KS system over physical space x and time (T). Since the original model and the learned models are all chaotic, we expect two solutions of even slightly different models to diverge along the time axis, even when starting with identical initial conditions. We observe that this divergence is minimal for the JAC model, while the errors in the MSE model grow strikingly quickly. ", "page_idx": 20}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/b02ff472d4fb8c20a79d4ad2e45d3eaffb0b570105dc75659304ba46b3dfb863.jpg", "img_caption": ["Figure 8: Solution plot of Kuramoto-Sivashinksy system when number of inner nodes is 127 and $c{=}0.4$ (see [BW14] for the parameter c). True solution (left), solution of the Neural ODE with mean squared loss (1) (center column), solution of the Neural ODE trained with Jacobian-matching loss defined in (3) (right) "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.6 Learning Lyapunov exponents ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We report Lyapunov exponents learned with Neural ODE trained with mean squared loss and Jacobianmatching loss in Table 5. To compute Lyapunov exponents, we use a QR algorithm of Ginelli et al [GCLP13]. We find that, for most systems considered, the LEs computed by JAC models match very well with the ground truth, while the MSE models sometimes capture the leading LEs but are inaccurate in the rest of them. ", "page_idx": 21}, {"type": "text", "text": "C.7 Comparison of Jacobian-matching loss training with unrolling dynamics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we report experiments on the Lorenz $^{\\,\\!}63$ system with an unrolled loss function: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{u}}(x):={\\frac{1}{k}}\\sum_{t\\leq k}|v_{t}(x)|^{2},{\\mathrm{where~}}v_{t}(x)=F_{\\mathrm{nn}}^{t}(x)-F^{t}(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With $k=10$ timesteps of unrolling time, we see that the attractor is reproduced well, as shown in Figure 9. However, atypical orbits are still produced for random initializations (Figure 10). ", "page_idx": 21}, {"type": "text", "text": "We experiment using two types of $v_{\\mathrm{nn}}$ , MLP and Resnet, and varying sequence lengths, $k$ . As shown in Table 3, we observe that the learned negative Lyapunov exponent plateaued for $k\\geq40$ , remaining between -9 and -10 (the true value being $\\sim-14.5)$ , with no further improvement. Also, as $k$ increases, we observe that Neural ODEs overestimate the positive Lyapunov exponent. Overall, unrolling seems to learn more accurate representations than the MSE model but less accurate representations than the JAC models. We also observe that the unrolling time needs to be fine-tuned as a hyperparameter to achieve good generalization; a small perturbation can lead to training instabilities. ", "page_idx": 21}, {"type": "text", "text": "To understand these results, for short times, when compared to the Lyapunov time, $v_{t}$ are in tangent spaces along the orbit. This yields the recursive relationship, $v_{t}\\approx d F(x_{t-1})v_{t-1}$ , when $\\mathcal{O}(\\|v_{t}\\|^{2})$ is negligible. Thus, the unrolled loss does contain Jacobian information implicitly although it does not enforce the learned trajectory to be close to the true trajectory in $\\mathcal{C}^{\\bar{1}}$ -distance. We remark, speculatively, that Theorem 2 gives a possible explanation for why the unrolling loss performs better than a one-step loss (even in some practical climate emulators, e.g., FourcastNet $[\\mathrm{PSH}^{+}22]$ ) at learning the physical measure. In other words, our numerical results lend support to the central thesis of this paper: adding Jacobian information improves statistical accuracy. Table 4 shows the norm difference between the reproduced invariant statistics and the true statistics of Lorenz $^{\\,\\!}63$ . ", "page_idx": 21}, {"type": "text", "text": "C.8 Latent SDE: experimental details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here we present the experimental details and results of learning the Lorenz $^{\\prime}63$ system with latent SDEs [LWCD20] that are described in Section 4. ", "page_idx": 21}, {"type": "text", "text": "As training data, we use 1024 time series of the interval [0, 6] and timestep size 0.01. With a smaller sample size, we empirically observe a large distributional mismatch (see Figure 11). We also observe that increasing or decreasing the time length of the trajectories can lead to unstable training. As in the supervised learning methods, the time series are simulated using a fourth-order Runge-Kutta scheme (RK4). We follow the same architecture 5 as used in [LWCD20], with a GRU encoder, a linear decoder, and the drift and diffusion functions in the prior and posterior processes modelled by MLPs. ", "page_idx": 21}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/92a0b35392dcb3490f6b648793104177e763d459dfcdafe0e6f71f467e327474.jpg", "table_caption": ["Table 3: Lyapunov Spectra learned by Neural ODE models trained on the MSE (1) for multi-step prediction. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/3619a4d77c1c45feebfbc26d71873b2d80ca50b9504095683ff66ce0b63bd682.jpg", "table_caption": ["Table 4: True vs. learned Lorenz $^{\\,\\!}63$ system: comparison of statistics. $W^{1}$ : Wasserstein-1 Distance, $\\boldsymbol{\\Lambda}=[\\lambda_{1},\\lambda_{2},\\lambda_{3}]^{\\top}$ , set of LEs, $\\hat{\\mu}_{T}$ : empirical distribution of an orbit of length $T$ . The subscript NN indicates quantities computed using NN models. The variable $k$ refers to the sequence length used for training. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "We present the learned Lyapunov exponents in Table 6, and a comparison of the learned and true empirical measures in Figure 11. When learning a deterministic system with a latent SDE, the diffusion coefficient of the learned system is small but not exactly 0, and this results in a slight difference in the Lyapunov exponents ([GHL20]) compared to only using the learned drift term. The latent SDE model was also tested on the stochastic Lorenz attractor [CZH21], where it reproduced the \u2018bimodal\u2019 distribution of the trajectories. ", "page_idx": 22}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/d86ec862f3e25d9968cea8f35a35b36f4cc037262ce5921bd51739f2a2761258.jpg", "img_caption": ["Figure 10: Comparison of the true phase plots of Lorenz $^{\\,\\!}63$ with phase plots of Neural ODE trained with the unrolled loss function (9). Initial condition of the long orbits is at $[-15,-15,5]$ . The first row shows the orbits on the xy, xz, and the yz plane obtained from RK4 integration of the Lorenz $^{\\prime}63$ system. The second row shows the orbits on the xy, xz, and the yz plane generated from Neural ODE trained with the loss (9) with $k=50$ . "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/d583bd15a6eb9c3a8855080a599a1e076176c10b15d1a9fa07b9281f35787dfe.jpg", "table_caption": ["Table 5: Chaotic systems and the true Lyapunov Spectra, the learned Lyapunov Spectra from Neural ODEs with MSE loss (1), and the learned Lyapunov Spectra from Neural ODEs with Jacobianmatching loss (3). "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 6: Lyapunov spectra computed from the learned latent SDE model evaluated on EulerMaruyama and RK4 solvers. Both the learned drift and the near-zero diffusion vector fields are used in Euler-Maruyama, and only the drift vector field is used in RK4. ", "page_idx": 24}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/f8182bde081c4a857527a2e85406eb131af76de1699c3424cf6ec9bd3e11f5f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/4a2f5ec700965cf78ce4227a3a1709661473d65faa82d84fbd810f44c575c19e.jpg", "table_caption": ["Table 7: Results of search over hyperparameters (batch size, weight decay, hidden layer depth and width) in training Neural ODEs with MLPs (with fully connected and convolution layers). We train with mean squared loss using the AdamW optimization algorithm, with two values of weight decay: $10^{-3}$ and $10^{\\dot{-}4}$ , and an adaptive learning rate with an initial value of 0.001. For each hyperparameter combination, we show the test loss and the relative error in the one-timestep predictions averaged over 8000 samples; we choose the hyperparameter combination that results in the least relative error. The time step of the maps (both true and NNs) are set at 0.01. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "4t3ox9hj3z/tmp/2defab4f4ac05c849209bf15a499b89bdacfbae42d1eb37d72515086217cde10.jpg", "img_caption": ["Figure 11: Comparison of empirical distributions of the x, y, and z coordinates of the true orbits of the Lorenz $^{\\,\\!}63$ system (black) against those of the latent SDE (red). Top row: The latent SDE model (see section 4) is trained with 614,400 sample points. Bottom row: The latent SDE model is trained with the same sample size, 10,000, as the JAC_MLP and MSE_MLP (Neural ODE) models. We generate a trajectory over the interval [0, 50] for both the Lorenz $^{\\prime}63$ model and the learned latent SDE system. Gist: we empirically observe that, even when training with $\\mathcal{O}(100)$ , the latent SDE model results in a worse prediction of the physical distribution compared to supervised learning based on Jacobian-matching loss (3). "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/3e9af74d75a1bc887566ba989dfbf8b4f61f7820448dca7ba75b0041a5f1dd2c.jpg", "table_caption": ["Table 8: Results of search over hyperparameters (batch size, weight decay, hidden layer depth and width) in training Neural ODEs with MLP_skip (ResNets). We train with mean squared loss using the AdamW optimization algorithm, with two values of weight decay: $10^{-3}$ and $10^{-4}$ , and an adaptive learning rate with an initial value of 0.001. For each hyperparameter combination, we show the test loss and the relative error in the one-timestep predictions averaged over 8000 samples; we choose the hyperparameter combination that results in the least relative error. The time step of the maps (both true and NNs) are set at 0.01. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "4t3ox9hj3z/tmp/e6d913314bf1483fe148e6beb1bd2c9fc236cd620103de246efd59e45854536b.jpg", "table_caption": ["Table 9: Results of search over hyperparameters (batch size, hidden layer depth and width) in training Neural ODEs with MLP_skip (ResNets). We train with the Jacobian-matching loss (3) using the AdamW optimization algorithm, with a weight decay of $5\\times10^{-4}$ , and an adaptive learning rate with an initial value of 0.001. For each hyperparameter combination, we show the test loss and the relative error in the one-timestep predictions averaged over 8000 samples; we choose the hyperparameter combination that results in the least relative error. The time step of the maps (both true and NNs) are set at 0.01. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims, which match with the presented experimental results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Limitations are discussed under Related Work, Conclusion and Appendix A. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All theorems and proofs are clearly documented in the main text along with supplementary proofs in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details on experiment setting and results are thoroughly documented under the Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: To preserve anonymity, we submit main part of the code, but not the full repository which has the all the pretrained model. We aim to publish the full code with pretrained model in public later on. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have written all the necessary details under the Appendix B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discuss all of the details on the metrics we used under the Appendix B. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Details on computing resources are written under the Appendix subsection B.4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All of the authors have reviewed the NeurIPS Code of Ethics and the research conducted in the paper conform with the Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Discussion on broader impacts are under the Conclusion. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not pose any such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Citation and versions are properly cited throughout the paper. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Details are given under the Appendix and the main text as well. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects nor crowedsourcing. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects nor crowdsourcing. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]