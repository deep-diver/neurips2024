{"references": [{"fullname_first_author": "A. S. Manne", "paper_title": "Linear programming and sequential decisions", "publication_date": "1960-00-00", "reason": "This paper introduces the linear programming formulation for Markov Decision Processes (MDPs), a fundamental concept underlying the algorithm presented in the current paper."}, {"fullname_first_author": "R. Bellman", "paper_title": "Dynamic programming", "publication_date": "1966-00-00", "reason": "Bellman's work on dynamic programming provides the theoretical foundation for solving sequential decision-making problems, which is central to the offline reinforcement learning approach discussed."}, {"fullname_first_author": "E. V. Denardo", "paper_title": "On linear programming in a Markov decision problem", "publication_date": "1970-00-00", "reason": "This paper further develops the connection between linear programming and MDPs, specifically focusing on the optimal control problem, which is directly relevant to the algorithm's design."}, {"fullname_first_author": "C. Jin", "paper_title": "Provably efficient reinforcement learning with linear function approximation", "publication_date": "2020-00-00", "reason": "This paper establishes the linear MDP framework and sample complexity bounds for offline RL with linear function approximation, a direct theoretical antecedent to this paper's results."}, {"fullname_first_author": "G. Gabbianelli", "paper_title": "Offline primal-dual reinforcement learning for linear MDPs", "publication_date": "2024-00-00", "reason": "This paper introduces a saddle-point reparametrization for linear MDPs, which is a core component of the new algorithm proposed in the current paper."}]}