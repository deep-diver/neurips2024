[{"type": "text", "text": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yihong Guo1, Yixuan Wang1, Yuanyuan $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{2}$ , Pan $\\mathbf{X}\\mathbf{u}^{3}$ , Anqi Liu1 ", "page_idx": 0}, {"type": "text", "text": "1Johns Hopkins University 2University of California San Diego 3Duke University {yguo80,ywang830,aliu.cs}@jhu.edu, yyshi@ucsd.edu, pan.xu@duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training a policy in a source domain for deployment in the target domain under a dynamics shift can be challenging, often resulting in performance degradation. Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories. However, pure modified rewards only ensure the behavior of the learned policy in the source domain resembles trajectories produced by the target optimal policies, which does not guarantee optimal performance when the learned policy is actually deployed to the target domain. In this work, we propose to utilize imitation learning to transfer the policy learned from the reward modification to the target domain so that the new policy can generate the same trajectories in the target domain. Our approach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL), utilizes the reward modification for domain adaptation and follows the general framework of generative adversarial imitation learning from observation (GAIfO) by applying a reward augmented estimator for the policy optimization step. Theoretically, we present an error bound for our method under a mild assumption regarding the dynamics shift to justify the motivation of our method. Empirically, our method outperforms the pure modified reward method without imitation learning and also outperforms other baselines in benchmark off-dynamics environments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The objective of reinforcement learning (RL) is to learn an optimal policy that maximizes rewards through interaction and observation of environmental feedback. However, in domains such as medical treatment [1] and autonomous driving [2], we cannot interact with the environment freely as the errors are too costly or the amount of access to the environment is limited. Instead, we might have access to a simpler or similar source domain. This requires domain adaptation in reinforcement learning. In this paper, we study a specific problem of domain adaptation in reinforcement learning (RL), where only the dynamics (transition probability) are different in two domains. This is called off-dynamics RL [3\u20135]. Specifically, we focus on a problem setting in which we have limited access to rollout data from the target domain, but we do not have access to the target domain reward, following the previous off-dynamics work [3\u20135]. ", "page_idx": 0}, {"type": "text", "text": "Previous work on off-dynamics RL, such as Domain Adaptation with Rewards from Classifiers (DARC) [3] and [6, 5], focuses on training the policy in the source domain with a modified reward function that compensates for the dynamics differences. The reward modification is derived so that the distribution of the learning policy\u2019s experience in the source domain matches that of the optimal trajectories in the target domain. As a result, their experience in the source domain will produce a trajectory distribution close to the target domain\u2019s optimal one. However, deploying the resulting policy in the target domain usually causes performance degradation compared to its training performance in the source domain. Figure 1 (a) shows the experiment result of DARC under a broken source environment setting, where the broken source environment means the value of 0-index in the action of the source domain is frozen to 0, and the target environment remains intact. Consequently, existing reward modification methods will only obtain a sub-optimal policy in the target domain. Details of DARC and its suboptimality in the target domain will be introduced in Section 3.1. More details about why DARC fails in more general dynamics shift cases are in Appendix C.6. ", "page_idx": 0}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/2fe775466b73249f43602b2a0e470f1939c825830c3ef17a5c99494b4c5eaebf.jpg", "img_caption": ["Figure 1: (a) Training reward in the source domain, i.e. $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{src}}}}\\bigl[\\sum_{t}r\\bigl(s_{t},a_{t}\\bigr)\\bigr]$ , evaluation reward in the target domain, i.e. $\\begin{array}{r}{\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{trg}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]}\\end{array}$ and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC\u2019s objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we present an off-dynamics reinforcement learning algorithm described in Figure 1 (b). Our method, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) consists of two components. Following previous work like DARC [3] on off-dynamics RL, we first obtain the source domain trajectories that resemble the target domain\u2019s optimal ones. We then transfer the policy\u2019s behavior from the source to the target domain through imitation learning from observation [7], which can mimic the policy\u2019s behavior from the state space. ", "page_idx": 1}, {"type": "text", "text": "In particular, we consider the dynamics shift in the framework of generative adversarial imitation from observation (GAIfo) [8], and propose a novel and practical reward estimator called the reward augmented estimator $(R_{A E})$ for the policy optimization step in imitation learning. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose the Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) algorithm by transferring the learned policy of reward modification approaches from the source domain to the target domain via mimicking state-space trajectories in the source domain. We propose reward augmented estimator $(R_{A E})$ to leverage the reward from the source domain to stabilize the learning. \u2022 We recognize limitations in the existing DARC algorithm and off-dynamics reinforcement learning algorithms with similar reward modification, which is directly deploying the learned policy to the target domain results in significant performance degradation. Our proposed algorithm mitigates this issue with an imitation learning component that transfers DARC policy to the target. \u2022 We introduce an error bound for DARAIL that relaxes the assumption made in previous works that the optimal policy will receive a similar reward in both domains. Specifically, with our imitation learning from the observation component, we can show the convergence of DARAIL with a mild assumption on the magnitude of the dynamics shift. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We conducted experiments on four Mujoco environments, namely, HalfCheetah, Ant, Walker2d, and Reacher on modified gravity/density configurations and broken action environments. A comparative analysis between DARAIL and baseline methods is performed, demonstrating the effectiveness of our approach. Our method exhibits superior performance compared to the pure modified reward method without imitation learning and outperforms other baselines in these environments. Code is available at https://github.com/guoyihonggyh/Off-Dynamics-Reinforcement-Learning-via-DomainAdaptation-and-Reward-Augmented-Imitation. ", "page_idx": 2}, {"type": "text", "text": "2 Backgrounds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Off-dynamics reinforcement learning We consider two Markov Decision Processes (MDPs): one is the source domain $\\mathcal{M}_{\\mathrm{src}}$ , defined by $(S,\\mathcal{A},\\mathcal{R},p_{\\mathrm{src}},\\gamma)$ , and the other one is the target domain $\\mathcal{M}_{\\mathrm{trg}}$ , defined by $(S,\\mathcal{A},\\mathcal{R},p_{\\mathrm{trg}},\\gamma)$ . The difference between them is the dynamics $p$ , also known as transition probability, i.e., $p_{\\mathrm{src}}\\neq p_{\\mathrm{trg}}$ or $p_{\\mathrm{src}}(s_{t+1}|s_{t},a_{t})\\neq p_{\\mathrm{trg}}(s_{t+1}|s_{t},a_{t})$ . In our paper, we experiment with two types of dynamics shift: 1) broken environment [3], in which the 0-th index value is set to be 0 in action, and 2) modifying the gravity/density setting of the target environment [9]. The source and the target domain share the same reward function, i.e., $r_{\\mathrm{src}}(s_{t},s_{t+1})=r_{\\mathrm{trg}}\\big(s_{t},s_{t+1}\\big)$ . All other settings, including state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , and the discounting factor $\\gamma$ , are the same. We will use $\\gamma=1$ in the derivation and analysis in our paper. ", "page_idx": 2}, {"type": "text", "text": "We aim to learn a policy $\\zeta(a|s)$ using interaction from the source domain together with a small amount of data from the target domain $(s_{t},a_{t},s_{t+1})_{\\mathrm{trg}}$ to maximize the expected discounted sum of reward $\\begin{array}{r}{\\mathbb{E}_{\\zeta,p_{\\mathrm{trg}}}[\\sum_{t}\\gamma^{t}r(s_{t},a_{t})]}\\end{array}$ in the target domain. Note that we assume we only have limited access to the target domain transition, namely $(s_{t},a_{t},s_{t+1})_{\\mathrm{trg}}$ , in the whole process and we do not utilize the target domain reward. ", "page_idx": 2}, {"type": "text", "text": "Imitation learning (from Observation) Imitation Learning $\\operatorname{(IL)}$ trains a policy to mimic an expert policy $\\pi_{E}$ with expert demonstration $\\{(s_{0},a_{0}),(s_{1},a_{1}),...\\}$ or $\\{(s_{0},s_{1}),\\stackrel{\\bullet}{,}(s_{1},\\stackrel{\\bullet}{s}_{2}),\\ldots\\}$ . Generative adversarial imitation learning (GAIL) [7] uses an objective similar to Generative adversarial networks (GANs) that minimizes the distribution generated by the policy and the expert demonstration. It alternatively trains a discriminator $D_{\\omega}$ and a policy $\\pi_{\\theta}$ to solve the min-max problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\pi_{\\theta}}\\operatorname*{max}_{D_{\\omega}}\\mathbb{E}_{(s,s^{\\prime})\\sim\\pi_{E}}\\big[\\log D_{\\omega}(s,s^{\\prime})\\big]+\\mathbb{E}_{(s,s^{\\prime})\\sim\\pi_{\\theta}}\\big[\\log(1-D_{\\omega}(s,s^{\\prime}))\\big]-\\lambda\\mathcal{H}(\\pi_{\\theta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s^{\\prime}$ is the next state and $\\mathcal{H}(\\pi_{\\theta})$ is the entropy of the policy $\\pi_{\\theta}$ . Note that in our problem, we mimic the state-only expert demonstrations $\\{(s_{0},s_{1}),(s_{1},s_{2}),\\ldots\\}$ instead of the expert\u2019s actions. This setting is also called imitation learning from observation [8]. We will further discuss why we use state observation instead of action in section 3.2. $D_{\\omega}$ is the classifier that discriminates whether the state pair is from the expert $\\pi_{E}$ or generated by the policy $\\pi_{\\theta}$ . Then, the policy is trained with the RL algorithm using reward estimation $-\\log D_{\\omega}(s,s^{\\prime})$ as the reward. The optimization of the Eq. (2.1) involves alternatively training the policy and the discriminator. ", "page_idx": 2}, {"type": "text", "text": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our algorithm, DARAIL, under the off-dynamics RL problem setting. First, we introduce DARC [3] in Section 3.1, which provides the distribution of target optimal trajectories in the source domain to mimic. Then, in Section 3.2, we introduce the imitation learning component through which we utilize the trajectories provided by DARC and transfer the DARC policy to the target domain. We aim to learn a policy that generates the same distribution of trajectories in the target domain as the DARC trajectories in the source domain. ", "page_idx": 2}, {"type": "text", "text": "3.1 Off-dynamics RL via Modified Reward ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "DARC is proposed to solve the off-dynamics RL through a modified reward that compensates for the dynamics shift [3]. Here, we first introduce DARC and its drawbacks. DARC seeks to match the policy\u2019s experiences in the source domain and optimal trajectories in the target domain. We ", "page_idx": 2}, {"type": "text", "text": "define $\\tau=\\{(s_{1},a_{1}),(s_{2},a_{2}),...,(s_{t},a_{t}),...\\}$ as a trajectory. We use $\\tau_{\\pi_{\\theta}}^{\\mathrm{src}}$ to represent the trajectories generated by $\\pi_{\\theta}$ in the source domain. The policy\u2019s distribution over trajectories in the source domain is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\tau_{\\pi_{\\theta}}^{\\mathrm{src}})=p_{1}(s_{1})\\prod_{t}p_{\\mathrm{src}}(s_{t+1}\\vert s_{t},a_{t})\\pi_{\\theta}(a_{t}\\vert s_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\begin{array}{r}{\\pi^{*}\\,=\\,\\mathrm{argmax}_{\\pi}\\,\\mathbb{E}_{\\pi,p_{\\mathrm{trg}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]\\,}\\end{array}$ be the policy maximizing the cumulative reward in the target domain. We use $\\tau_{\\pi^{*}}^{\\mathrm{trg}}$ to represent the trajectories generated by $\\pi^{*}$ in the target domain. Given the assumption that the optimal policy $\\pi^{*}$ in the target domain is proportional to the exponential reward, i.e., $\\begin{array}{r}{\\pi^{*}(a_{t}|s_{t})\\propto\\mathrm{\\bar{exp}}(\\sum_{t}r(s_{t},a_{t}))}\\end{array}$ , the desired distribution over trajectories in the target domain is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\tau_{\\pi^{*}}^{\\mathrm{trg}})\\propto p_{1}(s_{1})\\prod_{t}p_{\\mathrm{trg}}(s_{t+1}|s_{t},a_{t})\\times\\exp\\Big(\\sum_{t}r(s_{t},a_{t})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "DARC policy can be obtained by minimizing the reverse KL divergence of $p(\\tau_{\\pi^{*}}^{\\mathrm{trg}})$ and $q(\\tau_{\\pi_{\\theta}}^{\\mathrm{src}})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\pi_{\\theta}}\\mathcal{D}_{\\mathrm{KL}}(q||p)=-\\operatorname*{min}\\mathbb{E}_{p_{\\mathrm{xx}}}\\sum_{t}r(s_{t},a_{t})+\\Delta r(s_{t},a_{t},s_{t+1})+\\mathcal{H}_{\\pi_{\\theta}}[a_{t}|s_{t}]+c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta r(s_{t},a_{t},s_{t+1}):=\\log p_{\\mathrm{trg}}(s_{t+1}|s_{t},a_{t})-\\log p_{\\mathrm{src}}(s_{t+1}|s_{t},a_{t})$ and $c$ is a partition function of $p(\\tau_{\\pi^{*}}^{\\mathrm{trg}})$ , which is independent of the dynamics and policy. The $\\Delta r(s_{t},a_{t},s_{t+1})$ can be calculated through the following procedure: i), train two classifiers $p(\\mathrm{trg}|s_{t},a_{t})$ and $p(\\Psi|s_{t},a_{t},s_{t+1})$ with cross-entropy loss $\\mathcal{L}_{C E}$ ; ii), Use Bayes\u2019 rules to obtain the $\\begin{array}{r}{\\log\\left(\\frac{p_{\\mathrm{trg}}\\left(s_{t+1}\\vert s_{t},a_{t}\\right)}{p_{\\mathrm{src}}\\left(s_{t+1}\\vert s_{t},a_{t}\\right)}\\right)}\\end{array}$ . Details are in Appendix C.1. Eq. (3.3) shows that $\\pi_{\\mathrm{DARC}}$ can be obtained via maximum entropy algorithm with a modified reward $r_{\\mathrm{modified}}=r(s_{t},a_{t})+\\Delta r(s_{t},a_{t},s_{t+1})$ at every step. ", "page_idx": 3}, {"type": "text", "text": "However, DARC matches the distribution of $\\tau_{\\pi^{*}}^{\\mathrm{trg}}$ and $\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}}$ . As the dynamics shift exists, $\\pi_{\\mathrm{DARC}}$ will not recover the optimal policy , and deploying the DARC in the target domain will usually suffer from performance degradation due to the dynamics shift, as shown in Figure 1(a) and Figure 9 in Appendix. However, in the source domain $\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}}$ resembles those optimal trajectories in the target domain. Given the property of \u03c4 \u03c0srDcARC, we propose to use imitation learning from observation with $\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}}$ as expert demonstrations to transfer DARC to the target domain. The new policy in the target domain should behave similarly (generate similar trajectories) as DARC in the source domain. ", "page_idx": 3}, {"type": "text", "text": "3.2 Imitation Learning from Observation with Reward Augmentation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) method, which mitigates the problem of DARC via imitation learning from observation. As described in Section 3.1, $\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{sr}\\overline{{\\mathrm{c}}}}$ resembles the target optimal trajectories, and we want to transfer DARC\u2019s behavior to the target domain. A natural way to tackle it is utilizing imitation learning to mimic the expert demonstration $\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}}$ . Following [7, 8], the objective can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\zeta}\\operatorname*{max}_{D_{\\omega}}\\left\\{\\mathbb{E}_{p_{\\mathrm{rp},\\zeta}}\\Big[\\sum_{t}\\log D_{\\omega}\\big(s_{t},s_{t+1}\\big)\\Big]+\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\pi_{\\mathrm{DaKC}}}^{\\mathrm{ar}}}\\Big[\\sum_{t}\\log(1-D_{\\omega}\\big(s_{t},s_{t+1}\\big))\\Big]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D_{\\omega}$ is the discriminator in the generative adversarial imitation learning and $\\zeta$ is the policy to be learned in the target domain. In the objective function Eq. (3.4), the $\\left(s_{t},s_{t+1}\\right)$ pairs are from the target domain, while we do not have much access to the target domain. Alternatively, we can use the $(s_{t},s_{t+1})$ pairs from the source domain and re-weight the transition with the importance sampling method to account for the dynamics shift. The objective with data rolled out from the source domain, and the importance sampling is as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\zeta}\\operatorname*{max}_{D_{\\omega}}\\left\\{\\mathbb{E}_{p_{\\mathrm{sc}},\\zeta}\\left[\\sum_{t}\\rho(s_{t},s_{t+1})\\log D_{\\omega}(s_{t},s_{t+1})\\right]+\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\mathrm{rom},\\mathrm{bic}}^{\\mathrm{arc}}}\\left[\\sum_{t}\\log(1-D_{\\omega}(s_{t},s_{t+1}))\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho\\big(s_{t},s_{t+1}\\big)=\\frac{p_{\\mathrm{{tr}}}(s_{t+1}|s_{t},a_{t})}{p_{\\mathrm{src}}(s_{t+1}|s_{t},a_{t})}}\\end{array}$ is the importance weight. Note that we do the generative adversarial imitation learning from only state observations $(G A I L f o)$ with $\\left(s_{t},s_{t+1}\\right)$ [9\u201311] instead of $(s_{t},a_{t})$ . This is because we aim to learn a policy $\\zeta$ to produce the same trajectory distributions in the target as the ones $\\pi_{\\mathrm{DARC}}$ produces in the source domain, despite the dynamics shift, rather than mimicking the policy. Mimicking the $\\left({{s}_{t}},{{a}_{t}}\\right)$ pairs will recover the same policy as DARC, and deploying it to the target domain will not recover the expert trajectories due to the dynamics shift. ", "page_idx": 3}, {"type": "text", "text": "This objective Eq. (3.5) can be interpreted as training the discriminator $D_{\\omega}$ to discriminate whether the $\\left(s_{t},s_{t+1}\\right)$ generated by $\\zeta$ in the target domain matches the distribution of DARC trajectories in the source domain using data rolled out from the source domain with $\\zeta$ and importance weight. Then, after the discriminator is fitted, the policy can be trained with the reward estimator $R_{A E}$ with model-free RL. The objective is: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{\\zeta}\\mathbb{E}_{p_{\\mathrm{src}},\\zeta}\\left[\\sum_{t}R_{A E}\\big(s_{t},s_{t+1}\\big)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $R_{A E}$ is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{A E}(s_{t},s_{t+1})=-\\log D_{\\omega}(s_{t},s_{t+1})+\\rho(s_{t},s_{t+1})(r_{\\mathrm{src}}(s_{t},s_{t+1})+\\log D_{\\omega}(s_{t},s_{t+1})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here the $r_{\\mathrm{src}}(s_{t},s_{t+1})$ is the reward obtained from the source domain, which is the same as the reward from the source domain, i.e. $r_{\\mathrm{trg}}(s_{t},s_{t+1})$ . In imitation learning, the $-\\log D_{\\omega}(s_{t},s_{t+1})$ can be viewed as a local reward function for the policy optimization step and the objective is $\\begin{array}{r}{\\operatorname*{max}_{\\zeta}\\mathbb{E}_{p_{\\mathrm{src}},\\zeta}[\\sum_{t}-\\log D_{\\omega}(s_{t},s_{t+1})]}\\end{array}$ . So Eq.(3.5) can be viewed as learning a reward function for the training of $\\zeta$ . However, as the dynamics shift exists, the estimation of the $-\\log D_{\\omega}(s_{t},s_{t+1})$ could be biased, which is similar to the case in off-policy evaluation (OPE) [12\u201316] when training a reward estimation on biased data. As we have access to the source domain and can obtain the reward from the rollout, we are motivated to use both the reward estimation $-\\log D_{\\omega}(s_{t},s_{t+1})$ and the ground truth reward in the source domain $r_{\\mathrm{src}}(s_{t},s_{t+1})$ so that we could have a better reward estimation than $-\\log D_{\\omega}(s_{t},s_{t+1})$ under dynamics shift. The $R_{A E}$ here can be viewed as using $-\\log D_{\\omega}(s_{t},s_{t+1})$ as a base estimator of the reward and use $r_{\\mathrm{src}}(s_{t},s_{t+1})$ and importance weight $\\rho\\big(s_{t},s_{t+1}\\big)$ to correct it. This correction idea is similar to the doubly robust estimator (DR) [12] in OPE. The DR estimator combines the reward estimation $\\hat{r}$ and the importance-weighted difference between true reward $r$ and $\\hat{r}$ . Specifically, the DR method takes the reward estimation $\\hat{r}$ as a base estimator and applies the importance weighting to the difference between true reward $r$ and $\\hat{r}$ , which is $\\rho(r-\\hat{r})$ term, to correct the bias of the $\\hat{r}$ , where $\\rho$ is the importance weight. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Initialize: source and target environments $\\mathcal{M}_{\\mathrm{src}}$ and $\\mathcal{M}_{\\mathrm{trg}}$ ; replay buffers for source and target transitions, $(\\mathcal{D}_{\\mathrm{src}}^{\\pi_{\\mathrm{DARC}}}$ , $\\mathcal{D}_{\\mathrm{trg}}^{\\zeta},\\mathcal{D}_{\\mathrm{src}}^{\\zeta})$ ; initial parameters for the two classifiers $\\theta=(\\theta_{\\mathrm{SA}},\\theta_{\\mathrm{SAS}})$ ; initial policy $\\left(\\pi_{\\mathrm{DARC}},\\zeta\\right)$ ; initial discriminator $D_{\\omega}$ , ratio r of experience from source vs. target, ratio $\\boldsymbol{\\mathrm{k}}$ of update frequency of generator vs. discriminator.   \n2: $\\pi_{\\mathrm{DARC}}\\leftarrow\\mathrm{Call}$ DARC [3] $\\triangleright$ training expert policy ", "page_idx": 4}, {"type": "text", "text": "Reward Augmented Imitation Learning   \n3: $\\mathcal{D}_{\\mathrm{src}}^{\\pi_{\\mathrm{DARC}}}\\leftarrow\\mathcal{D}_{\\mathrm{src}}^{\\pi_{\\mathrm{DARC}}}\\bigcup\\mathrm{ROLLOUT}(\\pi_{\\mathrm{DARC}},\\mathcal{M}_{\\mathrm{src}})$   \n4: for $t=0,...T$ do   \n5: $\\mathcal{D}_{\\mathrm{src}}^{\\zeta}\\leftarrow\\mathcal{D}_{\\mathrm{src}}^{\\zeta}\\bigcup\\mathrm{ROLLOUT}(\\zeta,\\mathcal{M}_{\\mathrm{src}})$   \n6: if $t$ mod $r=0$ then   \n7: $\\mathcal{D}_{\\mathrm{trg}}^{\\zeta}\\leftarrow\\mathcal{D}_{\\mathrm{trg}}^{\\zeta}\\bigcup\\mathrm{ROLLOUT}(\\zeta,\\mathcal{M}_{\\mathrm{trg}})$   \n8: end if   \n9: if $t$ mod $k=0$ then   \n10: $D_{\\omega}\\gets\\mathrm{IL}(\\mathcal{D}_{\\mathrm{src}}^{\\pi_{\\mathrm{DARC}}},\\mathcal{D}_{\\mathrm{src}}^{\\zeta},\\mathcal{L})$ , where $\\mathcal{L}$ is from Eq. (3.5) \u25b7update discriminator   \n11: end if   \n12: $\\vec{\\theta}\\leftarrow\\mathrm{argmin}\\,\\mathcal{L}_{\\mathrm{CE}}(\\mathcal{D}_{\\mathrm{src}}^{\\zeta},\\mathcal{D}_{\\mathrm{trg}}^{\\zeta})$ $\\triangleright$ update classifiers by cross-entropy loss   \n13: Calculate $R_{A E}$ from Eq.(3.7) $\\triangleright$ reward augmented estimator   \n14: $\\zeta\\gets\\mathsf{S A C}(\\zeta,\\mathcal{D}_{\\mathrm{src}}^{\\zeta},R_{A E})$ \u25b7update generator   \n15: end for   \n16: Output: $\\zeta$ ", "page_idx": 4}, {"type": "text", "text": "Our Algorithm The DARAIL is shown in Algorithm 1, which consists of two steps: the first step, Line 2 in Algorithm 1, is the training of $\\pi_{\\mathrm{DARC}}$ , and the second step is imitation learning with the reward estimator in Eq. (3.7). In Lines 6-8, we roll out the target domain transition $(s_{t},a_{t},s_{t+1})$ to calculate the importance weight. Here, we will not collect the target domain reward. In Lines 9-11, we update the discriminator based on Eq. (3.5). In Line 12, we train the two classifiers $p(\\mathrm{trg}|s_{t},a_{t})$ and $p(\\Psi|s_{t},a_{t},s_{t+1})$ with cross-entropy loss $\\mathcal{L}_{C E}$ and Bayes\u2019 rules similar to $\\Delta r(s_{t},a_{t},s_{t+1})$ in DARC as mentioned in Section 3.1. The details are in Appendix C.1. Lastly, we calculate the $R_{A E}$ in Line 13 and update the generator (Soft Actor-Critic (SAC) [17]) with $R_{A E}$ in Line 14. ", "page_idx": 4}, {"type": "text", "text": "Note that in Lines 6-7, we roll out from the target domain, but the amount of it is significantly smaller than the source rollouts. In our experiments, we roll out from the target domain every 100 steps of source domain rollouts, which is $1\\%$ of the source domain rollouts. Further, even though DARAIL requires more target domain rollouts than DARC as it is required to train DARC first and then perform the imitation learning step, the advantage of DARAIL does not solely come from the more target samples. Because, in DARC, increasing the training step or target domain rollouts will not further improve its performance due to its inherent suboptimality, which is shown in table 11 and 12 in Appendix with the same amount of target domain rollouts. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis of DARAIL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $\\begin{array}{r}{\\pi^{*}=\\operatorname*{argmax}_{\\pi}\\mathbb{E}_{\\pi,p_{\\mathrm{trg}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]}\\end{array}$ be the optimal policy maximizing the cumulative reward in the target domain and $\\hat{\\zeta}$ be the policy learned from DARAIL. Now, we provide an error bound for DARAIL. Details of the proof are deferred to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Let $m$ be the number of the expert demonstration and $\\begin{array}{r l}{\\hat{\\mathcal{R}}_{\\pi}^{(m)}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}_{\\sigma}\\left[\\operatorname*{sup}_{D\\in\\mathcal{D}}\\frac{1}{m}\\sum_{i=1}^{m}\\sigma_{i}D(s_{t},s_{t+1})\\right]}\\end{array}$ be the empirical Rademacher complexity. Let $B$ be the error bound of DARC in the source domain, i.e. $\\begin{array}{r l}{\\mathbb{E}_{p_{s r c},\\pi_{D A R C}^{*}}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]\\;-}&{{}}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}_{p_{s r c},\\pi_{D A R C}}\\left[\\sum_{t}r(s_{t},a_{t})\\right]\\leq B}\\end{array}$ n aatnod $W$ abses e  buep ap -bboouunnd doefd t fhue nicmtipoonr,t ai.nec. $\\rho(s_{t},s_{t+1})\\leq$ $W$ $\\forall(s_{t},s_{t+1})$ $\\mathcal{D}$ $\\Delta$ $|D_{\\omega}(s_{t},s_{t+1})|\\leq\\Delta$ any $\\left(s_{t},s_{t+1}\\right)$ . $\\|r\\|_{\\mathcal{D}}$ measures the richness of the discriminator to represent the ground truth reward as defined in Appendix B.2. $d_{D}$ is a defined neural network distance between the $\\left(s_{t},s_{t+1}\\right)$ distributions generated by the \u03c0DARC and $\\pi_{\\widehat{\\zeta}}$ defined in Appendix B.1. Given the empirical training error of the imitation learning, i.e. $\\begin{array}{r}{d_{\\mathcal{D}}\\big(\\hat{\\tau}_{\\pi_{D A R C}}^{s r c},\\hat{\\tau}_{\\hat{\\zeta}}^{t r g}\\big)-\\operatorname*{inf}_{\\zeta}d_{\\mathcal{D}}\\big(\\hat{\\tau}_{\\pi_{D A R C}}^{s r c},\\hat{\\tau}_{\\zeta}^{t r g}\\big)\\leq\\hat{\\epsilon},\\,\\forall\\,\\delta\\in(0,1),}\\end{array}$ , with probability at least , we have ", "page_idx": 5}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/e5a2f5bd8e63148027a7e06b54c60ab258107ffa49ac4eacee18afef124aebce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Remark 4.2. Our error bound depends on (1) the DARC error bound in the source domain and (2) the imitation learning generalization error, where (2) is further decomposed into (2.1) approximation error and (2.2) estimation error. This bound demonstrates how the two important components in our proposed approach contribute to a good performance. Firstly, we would want a well-trained policy on the source to reduce (1), which can be achieved by a good policy learning algorithm and well-trained classifiers for reward modification. Secondly, we utilize imitation learning from observation to transfer the experience to the source. (2.1) depends on the upper bound of the importance weight, which can be decreased with a richer policy class or when the dynamics shift becomes smaller. Additionally, a better imitation can be also achieved by increasing the complexity of the discriminator function class and the number of samples, which pushes (2.2) to be smaller. ", "page_idx": 5}, {"type": "text", "text": "4.1 Comparison with the Analysis of DARC ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we discussed in Section 3.1, the DARC algorithm [3] trains a policy $\\pi_{\\mathrm{DARC}}$ on the source domain via matching the distribution of trajectories generated by $\\pi_{\\mathrm{DARC}}$ in the source and the distribution of the optimal trajectory in the target domain. Consequently, the learned policy $\\pi_{\\mathrm{DARC}}$ will be suboptimal if it is directly deployed in the target domain. ", "page_idx": 5}, {"type": "text", "text": "In the DARC analysis, it is assumed that the optimal policy for the target domain $\\pi^{*}$ lies in the no exploit set defined as follows [3, Assumption 1]. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Pi_{\\mathrm{no\\;exploit}}\\triangleq\\Bigl\\{\\mathbb{E}_{a\\sim\\pi(a\\vert s)}\\bigl[\\sum_{t}\\mathcal{D}_{\\mathrm{KL}}\\bigl(p_{\\mathrm{src}}\\bigl(s_{t+1}\\vert s_{t},a_{t}\\bigr)\\bigr\\vert\\vert p_{\\mathrm{trg}}\\bigl(s_{t+1}\\vert s_{t},a_{t}\\bigr)\\bigr)\\Bigr]\\leq\\epsilon\\Bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, the no exploit set means that the experiences for any policy in this set are similar in the source and target domains. Consequently, any two policies in this no exploit set also receive similar expected rewards in the two domains, and thus the reward received by $\\pi^{*}$ in the target domain is similar to ", "page_idx": 5}, {"type": "text", "text": "that received by $\\pi_{\\mathrm{DARC}}$ in the target domain. Further, the objective function Eq. (3.3) of DARC is equivalent to the following constrained optimization. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{\\pi\\in\\Pi_{\\mathrm{no\\;exploit}}}\\mathbb{E}_{p_{\\mathrm{src}},\\pi}\\left[\\,\\sum_{t}r\\bigl(s_{t},a_{t}\\bigr)+\\mathcal{H}[a_{t}|s_{t}]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, deploying the policy $\\pi_{\\mathrm{DARC}}$ will not receive a huge performance degradation. However, the assumption that $\\pi^{*}\\in\\Pi_{\\mathrm{no\\;exploit}}$ is stringent and might not always be satisfied when the dynamics shift is large. When this assumption is violated, $\\pi^{*}$ is not a good policy in the source domain, though it is the optimal policy in the target domain. Thus, the DARC policy which only optimizes the modified reward in the source domain will have significant performance degradation, as we have empirically shown in Figure 1 (a) and Figure 9. We also demonstrate this performance gap in Lemma A.1 in Appendix A when their assumption is not satisfied. ", "page_idx": 6}, {"type": "text", "text": "In contrast, our algorithm DARAIL does not assume the performance of $\\pi_{\\mathrm{DARC}}$ in the source domain to be close to the performance of $\\pi^{*}$ in the target domain. Instead, we only assume that the importance weight is somehow bounded, meaning that the dynamics shift is bounded. The error bound of our algorithm presented in Theorem 4.1 is controlled by imitation learning, which transfers the performance of $\\pi_{\\mathrm{DARC}}$ in the source domain to that of $\\pi^{*}$ in the target domain without assuming $\\pi^{*}\\in\\Pi_{\\mathfrak{n}\\mathfrak{o}}$ exploit. Therefore, our algorithm can work well even in the cases shown in Figure 1 (a) and Figure 9 where the experience of $\\pi_{\\mathrm{DARC}}$ is very distinctive in the source and target domains. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct experiments on off-dynamics reinforcement learning settings on four OpenAI environments: HalfCheetah- $_{\\nu2}$ , Ant- $_{\\nu2}$ , Walker2d- $_{\\cdot\\nu2}$ , and Reacher- $\\nu2$ . We compare our method with seven baselines and demonstrate the superiority of the proposed DARAIL. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experiments Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dynamics Shifts: We examine our algorithm with two types of dynamics shift. 1) Broken environment. Following previous work [3], we freeze the 0-index value to 0 in action: zero torque is applied to this joint, regardless of the commanded torque. Different from DARC [3], who only test their method in intact source and broken target environment, we further test our algorithm in the broken source and intact target environment, where the source has less support than the target domain. As discussed in Section 4.1, violating the $\\pi^{*}\\in\\Pi_{\\mathrm{no\\;exploit}}$ assumption leads to significant performance degradation for DARC and similar methods. When the source domain is intact, this assumption is more likely to hold and DARC can achieve a near-optimal policy in the target domain. So, besides the setting in DARC, we focus on a harder problem for off-dynamics RL where DARC is prone to failure due to the violation of the assumptions in Section 4.1. Further, for the Ant and Walker2d, the source environment is broken with $p_{f}=0.8$ probability, which means that with 0.8 probability, the 0-index will be set to be 0, and 0.2 probability remains the original value. More details about the broken environment will be introduced in the Appendix C.3. 2) Modify parameters of the environment. Besides the broken environment, we create dynamics shifts by modifying MuJoCo\u2019s configuration flies for the target domain. Specifically, we modify one of the coefficients of {gravity, density} from 1.0 to one of the value $\\lbrace0.5,1.5\\rbrace$ . ", "page_idx": 6}, {"type": "text", "text": "Baselines: We first compare our method with DARC performance in the source and target domains. DARC Training and DARC Evaluation, defined as $\\begin{array}{r}{\\mathbb{E}_{p_{\\mathrm{src}},\\pi_{\\mathrm{DARC}}}\\!\\left[\\sum_{t}r\\!\\left(s_{t},a_{t}\\right)\\right]}\\end{array}$ and $\\mathbb{E}_{p_{\\mathrm{trg}},\\pi_{\\mathrm{DARC}}}[\\sum_{t}r(s_{t},a_{t})]$ respectively, represent DARC performance in the two domains. We compare DARAIL with DARC training performance as we mimic the DARC behavior in the source domain, which should receive a similar reward as the DARC training reward in the source domain. We compare with DARC Evaluation to show that our method mitigates the problem of DARC and outperforms DARC in the target domain. Further, we compare our method DARAIL with several baselines that we describe as follows. Importance Sampling for Reward (IS-R) re-weights the reward in the transition with ptrg(st+1|st,at), and update the policy with reward pptrg((sst+1|sst,,aat))r [18]. Importance Sampling for SAC Actor and Critic Loss (IS-ACL) [18] re-weights the transitions in the SAC actor and critic loss. DAIL is a reduction of DARAIL without reward augmentation. Model-based RL method MBPO [19] uses short model rollouts branched from real data to reduce the compounding errors of inaccurate models and decouple the model horizon from the task horizon. MATL [20] uses different modified rewards and is similar to our problem setting, except that they have access to rewards in the target domain. Finally, we compare with generative adversarial reinforced action transformation (GARAT) [10], a grounded action transformation method that uses imitation learning to modify the action that is executed in the source domain to simulate the target transitions. More details of the baselines are in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Experimental Details: We perform weight clipping to all methods that use the importance weight, including the DARAIL, DAIL, IS-R, and IS-ACL, and select the [0.01, 100] as the clipping interval for fair comparison, which works well for all methods. We also show that DARAIL is less sensitive to the importance of weight clipping in the next section. We conduct fair parameter tuning for our method and baselines, including learning rate, Gaussian noise scale, and learning frequency of the importance weight. We also tune the parameter for the imitation learning component in DARAIL and DAIL and notice that the higher update frequency tends to perform better, and experiment results are in Appendix D.2. More details are in Appendix D.4. ", "page_idx": 7}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We show the results of DARAIL and DARC in Table 1 and 2 for broken source and 1.5 gravity setting, respectively. And the results of other baselines are in Table 3 and 4. We refer to the results on other settings in the Appendix, including the intact source and broken target environment setting and the modification of different scales of the parameters in the configuration file. We will also empirically discuss why DARC works well in the broken target setting while fails in the broken source setting in Appendix C.6. ", "page_idx": 7}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/9270dbeb449e96939042bb3732928e1090c76bebdb57ec4fe0b3fc5b99e6f5a4.jpg", "table_caption": ["Table 1: Comparison of DARAIL with DARC, broken source environment. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "The Suboptimality of DARC and DARAIL outperforms DARC By comparing DARC Training and DARC Evaluation in Table 1 and 2 we demonstrate that there is a performance degradation of $\\pi_{\\mathrm{DARC}}$ deployed in the target domain on all four environments. \u03c0DARC reward in the target domain is about $40\\%$ lower than $\\pi_{\\mathrm{DARC}}$ reward in the source domain on average for broken source setting, and the degradation can be more severe in the changing gravity and density setting. Also, \u03c0DARC reward in the target domain is significantly lower than the target optimal reward. The training reward curves of DARC of the broken source environment setting are in Appendix C.5, clearly showing performance degradation when deployed in the target domain. Further, DARAIL outperforms the DARC evaluation performance. ", "page_idx": 7}, {"type": "text", "text": "DARAIL Outperforms Baselines We show the result of DARAIL and baselines in Table 3, 4. The training curves of other settings are in Appendix C.4. In all four environments, DARAIL outperforms the $\\pi_{\\mathrm{DARC}}$ reward in the target domain. DARAIL also achieves better performance or the same level of rewards compared to the $\\pi_{\\mathrm{DARC}}$ in the source domain as shown in Table 1 and 2, which is our expert policy for the imitation step. Compared with the DAIL, DARAIL has a much better performance, which demonstrates the effectiveness of the reward estimator $R_{A E}$ . Compared with the two important weighting methods, IS-R and IS-ACL, in broken source settings, DARAIL outperforms IS-R in four environments and IS-ACL in Ant and Walker2d. IS-ACL and DARAIL achieve similar rewards in HalfCheetah and Reacher. And in modifying configuration settings, DARAIL outperforms IS-R and IS-ACL. Our method outperforms MBPO, MATL, and GARAT in all environments. ", "page_idx": 7}, {"type": "text", "text": "DARAIL is Less Sensitive to Extreme Values in Importance Weights Though IS-ACL achieves comparable performance with DARAIL on some tasks shown in Table 3, it is highly sensitive to the clipping interval of importance weight. In Figure 2, we show the performance of DARAIL and IPS-ACL on different importance weight clipping intervals in the broken source setting, and DARAIL outperforms IPS-ACL on all tasks. If the clipping interval is too large, IPS-ACL suffers from high variance, thus harming the performance. If the clipping interval is too small, the effective information about the dynamics shift is lost. On the other hand, DARAIL is less sensitive to it, which is an inherent property of our $R_{A E}$ . Furthermore, in Figure 2, for IPS-ACL, the training curve for [0.001, 1000] clipping interval has a much larger variance than [0.1, 10] clipping interval, while our method does not suffer from such a high variance. This also demonstrates that our proposed reward estimator $R_{A E}$ is a more robust estimator and less affected by the importance weight. ", "page_idx": 7}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/c04eded53da8b3e251cc049ae22e51dd9118f61af6cee7306268e1751ac8c230.jpg", "table_caption": ["Table 3: Comparison of DARAIL with baselines in off-dynamics RL, broken source environment. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/6b43a287bc5dc7d0909fd5a6056e6ab0b1bbd6c70a46cdd7d5b4abcf7c327248.jpg", "img_caption": ["Figure 2: Performance of DARAIL and IPS-ACL on HalfCheetah and Walker2d under different importance weight clipping intervals. DARAIL outperforms IPS-ACL on all tasks. In Table 3, IPS-ACL receives comparable performance with DARAIL with the clipping interval [0.01,100], while the performance decreases significantly with different intervals. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "DARAIL\u2019s Performance on Different Magnitudes of Shifts In our broken action environments, as we create the off-dynamics shift by (probabilistically) freezing one action dimension in the source domain, we can control the off-dynamics shift magnitudes by controlling the broken probability. For the same environment, the larger the $p_{f}$ is, the higher the probability of freezing the 0-index action, thus a larger dynamics shift. We consider $p_{f}=[0.2,0.5,0.8]$ for Ant, respectively and the experiment results is shown in Figure 3. From left to right, as the dynamics shift increases, we observe that the DARC performance decreases, and DARAIL outperforms DARC on all tasks. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Off-dynamics RL Off-dynamics RL [3] is a specific domain adaptation [21, 22] and transfer learning problem in the RL domain [23] where the goal is to learn a policy from a source domain to adapt to a target domain where the dynamics are different. Similar to many works in off-policy evaluation (OPE) [12] in bandit and offilne/off-policy RL [13, 24], an importance weight approach can be used to account for the difference between the transition dynamics with pptsrrgc((sstt++11|sstt,,aatt)). However, this method can easily suffer from high variance due to the estimation bias of $p_{\\mathrm{src}}\\big(s_{t+1}\\big|s_{t},a_{t}\\big)$ [12]. Another line of method for the off-dynamics RL is through reward shaping [3, 5]. DARC [3] learns a policy from a modified reward function that accounts for the dynamics shifts through a trajectories distribution matching objective. [6] proposed an unsupervised domain adaptation method with KL regularized objective, which uses the same reward modification techniques trajectories distribution matching objective in DARC [3]. These reward-shaping methods all face the same problem: they will not recover the optimal policy in the target domain and will suffer from performance degradation in the target domain, but the policy\u2019s experience in the source domain is similar to the optimal policy in the target domain. Similarly, [25] proposes a state-regularized policy optimization method that constrains the state distribution to be similar in the source and target domain by adding a constraint term in the reward. However, this will also lead to suboptimal policy in the target domain like DARC. Different from DARC, Mutual Alignment Transfer Learning (MATL) [20] uses different modified rewards with GAN [26] to align the trajectories generated in the source and the target domain, but it requires access to the target domain reward. There is also work [27] that solves the off-dynamics RL problem by training a distributionally robust policy in the source domain by assuming that the target domain\u2019s transition probability is in an ambiguity set defined around the transition probability of the source domain. Our method builds on DARC, inspired by its property in the source domain, overcoming the issues in DARC and similar methods by mimicking the $\\pi_{\\mathrm{DARC}}$ behavior in the source domain. ", "page_idx": 8}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/5584950ce7ab6affda0ff73fb0f5a6afa88e1eaed77f86c30e8b8e03b2df279f.jpg", "img_caption": ["Figure 3: Performance of DARC and DARAIL under different off-dynamics shifts on Ant. Action 0 is frozen (set to be 0) with probability $p_{f}$ in the source domain. From left to right, the off-dynamics shift becomes larger. As the shift becomes larger, the gap between DARC Training and DARC Evaluation is larger. Our method outperforms DARC on different dynamics shift. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Imitation Learning Imitation learning (IL) is another line of work that can be applied to off-dynamics problems by mimicking the expert demonstration in the target domain. Generative adversarial imitation learning, [7, 28\u201330, 8, 31, 32], frames IL as an occupancy-measure matching or divergence minimization problem, which minimizes the divergence of the generated trajectories and the expert demonstration. Building on GAN [26], it uses the RL algorithm as a generator and a classifier as a discriminator to achieve this. Imitation learning from observation (Ifo) [33\u201335] is recently proposed to mimic the expert\u2019s behavior without knowing which actions the expert took. In the off-dynamics RL setting, recent work on IL under dynamics mismatch [11, 10, 36] can transfer a policy learned in the source to the target domain with minimal interaction with the target domain. However, these methods require high-quality and sufficient expert demonstrations and also the expert demonstrations might not be the optimal trajectories for the target domain, resulting in a suboptimal policy. Our method, DARAIL, transfers the DARC policy\u2019s behavior in the source to the target domain through imitation learning from observation so that the new policy will behave like the optimal policy in the target domain. Furthermore, we propose a novel and practical reward estimator with the signal from the discriminator and the reward from the source domain for the policy optimization. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) for off-dynamics RL. We recognize the drawbacks of DARC and its following work with the same modified rewards function. We demonstrate that DARC or similar reward modification methods can only obtain a near-optimal policy in the target domain. We then propose to mimic the trajectory distribution generated by DARC in the source domain. Specifically, we propose a reward-augmented estimator for the policy optimization step in imitation learning from observation. Theoretically, we established the finite sample upper bounds of rewards for the proposed method, relaxing the restrictive assumption about the optimal policy in the previous work. Empirically, we conducted experiments on four Mujoco environments, demonstrating the superiority of our method. From the safety perspective, our method avoids directly training a policy in a high-risk environment. Our future work includes investigating off-dynamics reinforcement learning under safety constraints and more severe domain gaps in reinforcement learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their helpful comments. YG was supported by the Center for Digital Health and Artificial Intelligence (CDHAI) of the Johns Hopkins University. PX was supported in part by the National Science Foundation (DMS-2323112) and the Whitehead Scholars Program at the Duke University School of Medicine. AL was partially supported by the Amazon Research Award, the Discovery Award of the Johns Hopkins University, and a seed grant from the JHU Institute of Assured Autonomy. The views and conclusions in this paper are those of the authors and should not be interpreted as representing any funding agency. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ying Liu, Brent Logan, Ning Liu, Zhiyuan Xu, Jian Tang, and Yangzhi Wang. Deep reinforcement learning for dynamic treatment regimes on medical registry data. In 2017 IEEE international conference on healthcare informatics (ICHI), pages 380\u2013385. IEEE, 2017.   \n[2] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \n[3] Benjamin Eysenbach, Swapnil Asawa, Shreyas Chaudhari, Sergey Levine, and Ruslan Salakhutdinov. Off-dynamics reinforcement learning: Training for transfer with domain classifiers. arXiv preprint arXiv:2006.13916, 2020.   \n[4] Junda Wu, Zhihui Xie, Tong Yu, Qizhi Li, and Shuai Li. Sim-to-real interactive recommendation via off-dynamics reinforcement learning. In 2rd Offline Reinforcement Learning Workshop Advances at NeurIPS, 2021.   \n[5] Jinxin Liu, Hongyin Zhang, and Donglin Wang. Dara: Dynamics-aware reward augmentation in offline reinforcement learning. arXiv preprint arXiv:2203.06662, 2022.   \n[6] Jinxin Liu, Hao Shen, Donglin Wang, Yachen Kang, and Qiangxing Tian. Unsupervised domain adaptation with dynamics-aware rewards in reinforcement learning. Advances in Neural Information Processing Systems, 34:28784\u201328797, 2021.   \n[7] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.   \n[8] Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. arXiv preprint arXiv:1807.06158, 2018.   \n[9] Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecified simulator. Advances in neural information processing systems, 33:8510\u20138520, 2020.   \n[10] Siddharth Desai, Ishan Durugkar, Haresh Karnan, Garrett Warnell, Josiah Hanna, and Peter Stone. An imitation from observation approach to transfer learning with dynamics mismatch. Advances in Neural Information Processing Systems, 33:3917\u20133929, 2020.   \n[11] Tanmay Gangwani and Jian Peng. State-only imitation with transition dynamics mismatch. arXiv preprint arXiv:2002.11879, 2020.   \n[12] Miroslav Dud\u00edk, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv preprint arXiv:1103.4601, 2011.   \n[13] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, pages 652\u2013661. PMLR, 2016.   \n[14] Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dud\u00edk. Doubly robust off-policy evaluation with shrinkage. In International Conference on Machine Learning, pages 9167\u20139176. PMLR, 2020.   \n[15] Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang. Doubly robust off-policy actor-critic: Convergence and optimality. In International Conference on Machine Learning, pages 11581\u201311591. PMLR, 2021.   \n[16] Nathan Kallus, Xiaojie Mao, Kaiwen Wang, and Zhengyuan Zhou. Doubly robust distributionally robust off-policy evaluation and learning. In International Conference on Machine Learning, pages 10598\u201310632. PMLR, 2022.   \n[17] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[18] Joshua Arvind Holla. On the off-dynamics approach to reinforcement learning. McGill University (Canada), 2021.   \n[19] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019.   \n[20] Markus Wulfmeier, Ingmar Posner, and Pieter Abbeel. Mutual alignment transfer learning. In Conference on Robot Learning, pages 281\u2013290. PMLR, 2017.   \n[21] Thomas Carr, Maria Chli, and George Vogiatzis. Domain adaptation for reinforcement learning on the atari. arXiv preprint arXiv:1812.07452, 2018.   \n[22] Jinwei Xing, Takashi Nagata, Kexin Chen, Xinyun Zou, Emre Neftci, and Jeffrey L Krichmar. Domain adaptation in reinforcement learning via latent unified state representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10452\u201310459, 2021.   \n[23] Zhuangdi Zhu, Kaixiang Lin, Anil K Jain, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[24] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[25] Zhenghai Xue, Qingpeng Cai, Shuchang Liu, Dong Zheng, Peng Jiang, Kun Gai, and Bo An. State regularized policy optimization on data with dynamics shift. Advances in neural information processing systems, 36, 2024.   \n[26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[27] Zhishuai Liu and Pan Xu. Distributionally robust off-dynamics reinforcement learning: Provable efficiency with linear function approximation. In International Conference on Artificial Intelligence and Statistics, pages 2719\u20132727. PMLR, 2024.   \n[28] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.   \n[29] Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[30] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow. arXiv preprint arXiv:1810.00821, 2018.   \n[31] Mingxuan Jing, Xiaojian Ma, Wenbing Huang, Fuchun Sun, and Huaping Liu. Task transfer by preference-based cost learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 2471\u20132478, 2019.   \n[32] Faraz Torabi, Garrett Warnell, and Peter Stone. Imitation learning from video by leveraging proprioception. arXiv preprint arXiv:1905.09335, 2019.   \n[33] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1118\u20131125. IEEE, 2018.   \n[34] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint arXiv:1805.01954, 2018.   \n[35] Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from observation. arXiv preprint arXiv:1905.13566, 2019.   \n[36] Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Domain adaptive imitation learning. In International Conference on Machine Learning, pages 5286\u20135295. PMLR, 2020.   \n[37] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International conference on machine learning, pages 224\u2013232. PMLR, 2017.   \n[38] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. Advances in Neural Information Processing Systems, 33:15737\u201315749, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Analysis of DARC ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 DARC Objective ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 4 shows the objective of DARC, which minimizes the reverse KL divergence of the trajectories generated by the $\\pi_{\\mathrm{DARC}}$ in the source domain and $\\pi^{*}$ in the target domain. Note that the optimal policy is assumed to be proportional to the exponential form of the reward, i.e. $\\pi^{*}\\propto\\exp(r(s_{t},a_{t}))$ . Given this assumption, the reverse KL divergence can be re-formulated to Eq. (3.3) with modified reward. So, the $\\pi_{\\mathrm{DARC}}$ will not be optimal in the target domain but can generate trajectories in the source domain that resemble the optimal trajectories given the objective. ", "page_idx": 13}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/d5d15b0f32ebb7b797aa987711074086432871aea03d173c76a0f9267e01dd20.jpg", "img_caption": ["Minimize Reverse KL Divergence "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 4: Optimization objective of DARC. DARC minimizes the reverse KL divergence of the trajectories generated by the $\\pi_{\\mathrm{DARC}}$ and optimal policy $\\pi^{*}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 DARC Error Bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Now, we show that without the assumption of $\\pi^{*}\\in\\prod_{n o\\;e x p l o i t}$ in [3], the error of $\\pi_{\\mathrm{DARC}}$ cannot be trivially bounded. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. $t f\\pi^{*}\\notin\\Pi_{n o\\;e x p l o i t}$ , the error bound of the \u03c0DARC is in the following form: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}_{p_{n},\\pi^{*}}\\bigg[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\bigg]-\\mathrm{E}_{p_{n},\\pi_{B\\delta t\\mathcal{C}}}\\bigg[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\bigg]}\\\\ &{\\leq2R_{m a x}\\sqrt{\\displaystyle\\frac{1}{2}D_{K L}\\big(p_{t r,\\pi^{*}}(\\tau),p_{s r,\\pi^{*}}(\\tau)\\big)}+\\displaystyle\\sum_{t}T V\\big(\\pi_{D A R C}(\\cdot|s_{t}),\\pi^{*}(\\cdot|s_{t})\\big)\\operatorname*{max}_{s_{t},a_{t},s_{t+1}}\\Delta r(s_{t},a_{t},s_{t})}\\\\ &{\\phantom{\\leq}+2R_{m a x}\\sqrt{\\epsilon/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. In [3] Lemma B.2, they show that for any policy $\\pi\\,\\in\\,\\Pi_{n o\\;e x p l o i t}$ , the following inequality holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\operatorname{E}_{p_{\\mathrm{sv}},\\pi}\\biggl[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}_{\\pi}[a_{t}|s_{t}]\\biggr]-\\operatorname{E}_{p_{\\mathrm{up}},\\pi}\\biggl[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}_{\\pi}[a_{t}|s_{t}]\\biggr]\\biggr|\\leq2R_{m a x}\\sqrt{\\epsilon/2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $R_{m a x}$ refers to the maximum entropy-regularized return of any trajectories. However, the inequality Eq. (A.1) only holds for $\\pi_{\\mathrm{DARC}}$ , not for $\\pi^{*}$ . Now, we show that without the assumption $\\pi^{*}\\in\\prod_{n o\\;e x p l o i t}$ , the error could not be bounded trivially. ", "page_idx": 13}, {"type": "text", "text": "We start with the same decomposition. Therefore, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{E}_{p_{\\mathrm{tg}},\\pi^{*}}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]-\\mathrm{E}_{p_{\\mathrm{tg}},\\pi_{\\mathrm{DARC}}}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\zeta_{p_{m_{\\tau},\\pi^{*}}}\\left\\vert\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}]s_{t}\\right\\vert}_{I_{1}}\\,-\\,\\mathrm{E}_{p_{m_{\\tau},\\pi^{*}}}\\left\\vert\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}]s_{t}\\right\\vert\\Bigg.}\\\\ &{\\quad+\\underbrace{\\mathrm{E}_{p_{m_{\\tau},\\pi^{*}}}\\left\\vert\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}]s_{t}\\right\\vert\\right\\vert-\\mathrm{E}_{p_{m_{\\tau},\\pi_{0},\\mathrm{Re}}}\\left[\\sum_{t}r(s_{t},a_{t})+H_{\\pi^{*}}[a_{t}]s_{t}\\right]}_{I_{2}}}\\\\ &{\\quad+\\underbrace{\\mathrm{E}_{p_{m_{\\tau},\\pi_{0},\\mathrm{Re}}}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}]s_{t}\\right]}_{I_{1}}\\,-\\mathrm{E}_{p_{m_{\\tau},\\pi_{0},\\mathrm{Re}}}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}]s_{t}\\right]\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the original proof of [3], they bound the three terms based on the following idea: ", "page_idx": 14}, {"type": "text", "text": "For the term $I_{1}$ , they directly assume $\\pi^{*}\\;\\in\\;\\Pi_{n o\\;e x p l o i t}$ and obtain $I_{1}\\ \\leq\\ 2R_{m a x}\\sqrt{\\epsilon/2}$ based on inequality Eq. (A.1). However, without the $\\pi^{*}\\in\\prod_{n o\\;e x p l o i t}$ , the upper bound is not valid. A valid upper bound should be: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}=\\mathrm{E}_{p_{\\mathrm{w}},\\pi^{*}}\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]-\\mathrm{E}_{p_{\\mathrm{sr}},\\pi^{*}}\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{\\tau}(p_{\\mathrm{trg},\\pi^{*}}(\\tau)-p_{\\mathrm{src},\\pi^{*}}(\\tau))\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]}\\\\ &{\\quad\\le R_{m a x}\\|p_{\\mathrm{trg},\\pi^{*}}(\\tau)-p_{\\mathrm{src},\\pi^{*}}(\\tau)\\|_{\\infty}}\\\\ &{\\quad\\le2R_{m a x}\\sqrt{\\displaystyle\\frac{1}{2}D_{K L}(p_{\\mathrm{trg},\\pi^{*}}(\\tau),p_{\\mathrm{src},\\pi^{*}}(\\tau))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $\\pi^{*}\\in\\Pi_{n o\\;e x p l o i t}$ holds, we have $D_{K L}(p_{\\mathrm{trg},\\pi^{*}}(\\tau),p_{\\mathrm{src},\\pi^{*}}(\\tau))\\leq\\epsilon,$ , which recovers the inequality Eq.   \n(A.1). If it doesn\u2019t, we cannot trivially bound the $D_{K L}(p_{\\mathrm{trg},\\pi^{*}}(\\tau),p_{\\mathrm{src},\\pi^{*}}(\\tau))$ . ", "page_idx": 14}, {"type": "text", "text": "For the term $I_{2}$ , in the proof of [3], they also assume $\\pi^{*}\\in\\Pi_{n o\\;e x p l o i t}$ and obtain the $I_{2}\\leq0$ based on the objective $\\pi_{\\mathrm{DARC}}$ maximizes the reward in the source domain with $\\pi_{\\mathrm{DARC}}\\,\\in\\,\\Pi_{n o\\;e x p l o i t}$ . If $\\pi^{*}\\in\\prod_{n o\\;e x p l o i t}$ doesn\u2019t hold, we can bound this term by the following inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}_{p_{\\mathrm{sre},\\pi_{\\mathrm{DARC}}}}\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\Delta r(s_{t},a_{t},s_{t+1})+\\mathcal{H}[a_{t}|s_{t}]\\right]}\\\\ &{\\geq\\mathrm{E}_{p_{\\mathrm{sre},\\pi^{*}}}\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\Delta r(s_{t},a_{t},s_{t+1})+\\mathcal{H}[a_{t}|s_{t}]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}_{p_{\\mathrm{sre},\\pi^{*}}}\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]-\\mathrm{E}_{p_{\\mathrm{sre}},\\pi_{\\mathrm{BAK}}}\\left[\\displaystyle\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]}\\\\ &{\\le\\mathrm{E}_{p_{\\mathrm{sre},\\pi^{*}}}\\left[\\displaystyle\\sum_{t}\\Delta r(s_{t},a_{t},s_{t+1})\\right]-\\mathrm{E}_{p_{\\mathrm{sre},\\pi_{\\mathrm{BAK}}}}\\left[\\displaystyle\\sum_{t}\\Delta r(s_{t},a_{t},s_{t+1})\\right]}\\\\ &{\\le\\displaystyle\\sum_{t}T V(\\pi_{\\mathrm{DAKC}}(\\cdot|s_{t}),\\pi^{*}(\\cdot|s_{t}))\\operatorname*{max}_{s_{t},a_{t},s_{t+1}}\\Delta r(s_{t},a_{t},s_{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "And the total variation of the two policies cannot be trivially bound as well. For the term $I_{3}$ , we can easily bound it by applying the inequality Eq. (A.1) as $\\pi_{\\mathrm{DARC}}\\in\\Pi_{n o\\;e x p l o i t}$ . ", "page_idx": 14}, {"type": "text", "text": "In summary, the bound without assuming $\\pi^{*}\\in\\Pi_{n o\\;e x p l o i t}$ will be: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{E}_{p_{\\mathrm{trg}},\\pi^{*}}\\left[\\sum_{t}r(s_{t},a_{t})+H[a_{t}|s_{t}]\\right]-\\mathrm{E}_{p_{\\mathrm{trg}},\\pi_{\\mathrm{DARC}}}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}[a_{t}|s_{t}]\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\leq2R_{m a x}\\sqrt{\\frac{1}{2}D_{K L}(p_{\\mathrm{trg},\\pi^{*}}(\\tau),p_{\\mathrm{src},\\pi^{*}}(\\tau))}+\\sum_{t}T V(\\pi_{\\mathrm{DARC}}(\\cdot|s_{t}),\\pi^{*}(\\cdot|s_{t}))\\operatorname*{max}_{s_{t},a_{t},s_{t+1}}\\Delta r(s_{t},a_{t},s_{t+1})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "B Theoretical Analysis of DARAIL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove our theoretical results. ", "page_idx": 15}, {"type": "text", "text": "Definition B.1. (Neural Network Distance [37, 38]) For a class of neural networks $\\mathcal{D}$ , the neural network distance between two state-next state distributions, $\\tau_{\\pi_{D A R C}}^{s r c}$ and $\\tau_{\\zeta}^{t r g}$ , is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l_{D}(\\tau_{\\pi_{D i R C}}^{s r c},\\tau_{\\zeta}^{t p})=\\underset{D\\in D}{\\operatorname*{sup}}\\left\\{\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\pi_{D i R C}}^{n r}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\zeta}^{t p}}[D(s_{t},s_{t+1})]\\right\\}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{D\\in D}{\\operatorname*{sup}}\\left\\{\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\pi_{D i R C}}^{n r}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\zeta}^{s r}}[\\rho(s_{t},s_{t+1})D(s_{t},s_{t+1})]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition B.2. (Empirical Rademacher Complexity) Given a function class $\\mathcal{F}$ , a dataset $X\\,=$ $(x_{1},x_{2},...,x_{n})$ , i.i.d drawn from distribution $\\mu$ and random variable $\\sigma$ defined as $P(\\sigma\\,=\\,1)\\,=$ $\\begin{array}{r}{P(\\sigma=-1)=\\frac{1}{2}}\\end{array}$ , the empirical Rademacher complexity is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{\\mu}^{(n)}=\\mathbb{E}_{\\sigma}[\\operatorname*{sup}_{f\\in\\mathcal{F}}]\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}f(x_{i}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition B.3. (Linear Span of the Discriminator) Consider a span of the discriminator class: $s p a n(D)=\\{c_{0}+\\textstyle\\sum_{i}^{k}c_{i}D_{i}:c_{0}\\in\\mathbb{R},D_{i}\\in\\mathcal{D},n\\in\\mathbb{N}\\}$ . Assuming the ground truth reward function $r$ lies in the span $(\\mathcal{D})$ , then the compatible coefficient is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|r\\|_{\\mathcal{D}}=\\operatorname*{inf}\\left\\{\\sum_{i}^{k}|c_{i}|:r=c_{0}+\\sum_{i}^{k}c_{i}D_{i},c_{0}\\in\\mathbb{R},D_{i}\\in\\mathcal{D},n\\in\\mathbb{N}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The compatible coefficient represents the minimum number of functions in $\\mathcal{D}$ required to the reward function $r$ , which means the complexity of the reward function $r$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma B.4. (GAIL Generalization). Let \u03c0DARC be the expert policy and $\\hat{\\zeta}$ be the solution of the imitation learning algorithm. Let discriminator class $\\mathcal{D}$ be a $\\Delta$ -bounded function, i.e. $|D(s_{t},s_{t+1})|\\leq$ $\\Delta$ . Suppose reward function r lies in the span of the discriminator class. Given $d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{D A R C}}^{s r c},\\hat{\\tau}_{\\zeta}^{t r g})\\;-$ $\\operatorname*{inf}_{\\zeta}d_{\\mathcal{D}}\\big(\\hat{\\tau}_{\\pi_{D A R C}}^{s r c},\\hat{\\tau}_{\\zeta}^{t r g}\\big)\\leq\\hat{\\epsilon}$ (empirical neural network distance achieved by imitation learning), the importance weight $\\rho(s,s_{t+1})$ is bounded by $W$ , $m$ is the number of the expert data, then $\\forall\\,\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{s r},\\pi_{B a K C}}\\left[\\underset{t}{\\sum}r(s_{t},a_{t})\\right]-\\mathbb{E}_{p_{r r g},\\hat{\\zeta}}\\left[\\underset{t}{\\sum}r(s_{t},a_{t})\\right]}\\\\ &{\\leq\\|r_{D}\\|\\left[\\operatorname*{inf}\\left(\\overset{\\mathrm{inf}}{\\zeta}^{\\hat{x}^{p_{C}}}\\hat{\\tau}_{\\mathcal{C}}^{t r g}\\right)+2\\hat{\\mathcal{R}}_{\\tau_{\\pi D A K C}^{(m)}}^{(m)}+2W\\hat{\\mathcal{R}}_{\\tau_{\\zeta}^{p n}}^{(m)}+(6W+1)\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}+\\hat{\\epsilon}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Given $d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\zeta}^{\\mathrm{trg}})-\\operatorname*{inf}_{\\zeta}d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}^{\\mathrm{src}}},\\hat{\\tau}_{\\zeta}^{\\mathrm{trg}})\\leq\\hat{\\epsilon}$ , we can have ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{\\mathcal{D}}(\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\tau_{\\zeta}^{\\mathrm{tg}})\\leq d_{\\mathcal{D}}(\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\tau_{\\zeta}^{\\mathrm{tg}})-d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\zeta}^{\\mathrm{tg}})+\\operatorname*{inf}_{\\zeta}d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\zeta}^{\\mathrm{tg}})+\\hat{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We prove that $d_{\\mathcal{D}}(\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\tau_{\\zeta}^{\\mathrm{trg}})-d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\zeta}^{\\mathrm{trg}})$ has an upper bound. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathcal{D}}(\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\tau_{\\zeta}^{\\mathrm{tg}})-d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\zeta}^{\\mathrm{tg}})}\\\\ &{\\ =\\displaystyle\\operatorname*{sup}_{D\\in\\mathcal{D}}\\Big[\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\zeta}^{\\mathrm{tg}}}[D(s_{t},s_{t+1})]\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]\\right]}\\\\ &{\\le\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]\\right]}\\\\ &{\\quad+\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]\\right]}\\\\ &{=\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]\\right]}\\\\ &{\\quad+\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[\\rho(s_{t},s_{t+1})D(s_{t},s_{t+1})]\\right]}\\\\ &{\\le\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim r_{\\operatorname*{sup}}^{\\pi}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "According to McDiarmid \u2019s inequality, with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ , the following inequality holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{P}\\in\\mathbb{P}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\mathrm{pnac}}^{\\pi}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\hat{\\pi}_{\\mathrm{pnac}}^{\\pi}}[D(s_{t},s_{t+1})]\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\underset{{D}\\in\\mathcal{D}}{\\operatorname*{sup}}|\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau\\pi_{\\mathrm{pnac}}^{\\pi}\\times\\left[D(s_{t},s_{t+1})\\right]}-\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\hat{\\pi}_{\\mathrm{pnac}}^{\\pi}}[D(s_{t},s_{t+1})]\\right]+2\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}}\\\\ &{\\leq2\\mathbb{E}_{\\sigma,\\tau_{\\mathrm{mac}}^{\\pi}}\\left[\\underset{{D}\\in\\mathcal{D}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\frac{1}{m}\\sigma_{i}D(s_{i},s_{i}^{\\prime})\\right]+2\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}}\\\\ &{\\leq2\\mathcal{R}_{\\tau_{\\mathrm{mac}}^{\\pi}}^{(m)}+2\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}}\\\\ &{\\leq2\\hat{\\mathcal{R}}_{\\tau_{\\mathrm{mac}}^{\\pi}}^{(m)}+6\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By a similar derivation, we can have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W\\underset{D\\in\\mathcal{D}}{\\operatorname*{sup}}\\left[\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\zeta}^{\\mathrm{sc}}}[D(s_{t},s_{t+1})]-\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\hat{\\tau}_{\\zeta}^{\\mathrm{sc}}}[D(s_{t},s_{t+1})]\\right]}\\\\ &{\\leq2W\\hat{\\mathcal{R}}_{\\tau_{\\zeta}^{\\mathrm{sc}}}^{(m)}+6W\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathcal{D}}(\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\tau_{\\zeta}^{\\mathrm{trg}})-d_{\\mathcal{D}}(\\hat{\\tau}_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\zeta}^{\\mathrm{trg}})}\\\\ &{\\leq2\\hat{\\mathcal{R}}_{\\tau_{\\pi_{\\mathrm{DARC}}}^{\\mathrm{3rc}}}^{(m)}+2W\\hat{\\mathcal{R}}_{\\tau_{\\zeta}^{\\mathrm{tr}}}^{(m)}+(6W+1)\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, based on Theorem 2 in [38], we can conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{\\mathrm{sce}},\\pi_{\\mathrm{BAC}}}\\left[\\underset{\\textit{t}}{\\sum}r(s_{t},a_{t})\\right]-\\mathbb{E}_{p_{\\mathrm{rg},\\hat{\\zeta}}}\\left[\\underset{\\textit{t}}{\\sum}r(s_{t},a_{t})\\right]}\\\\ &{\\leq\\|r_{D}\\|\\big[\\operatorname*{inf}\\big(\\pounds_{\\hat{\\mathcal{C}}}(\\hat{\\tau}_{\\pi_{\\mathrm{BAC}}}^{\\mathrm{src}},\\hat{\\tau}_{\\hat{\\mathcal{C}}}^{\\mathrm{rg}})+2\\hat{\\mathcal{R}}_{\\tau_{\\pi_{\\mathrm{DAK}}}^{(\\infty)}}^{(m)}+2W\\hat{\\mathcal{R}}_{\\tau_{\\hat{\\mathcal{C}}}^{\\mathrm{sr}}}^{(m)}+(6W+1)\\Delta\\sqrt{\\frac{l o g(4/\\delta)}{2m}}+\\hat{\\epsilon}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.5. Let $\\begin{array}{r}{\\pi^{*}\\ =\\ \\mathrm{argmax}_{\\pi}\\,\\mathbb{E}_{\\pi,p_{t r g}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]}\\end{array}$ be the policy maximizing the cumulatbievre  roef wthaer de ixnp etrhte  dtearmgoetn sdtroamtiaoinn  aanndd $\\hat{\\zeta}$ $\\begin{array}{r}{\\hat{\\mathcal{R}}_{\\pi}^{(m)}\\;=\\;\\mathbb{E}_{\\sigma}\\left[\\operatorname*{sup}_{D\\in\\mathcal{D}}\\frac{1}{m}\\sum_{i=1}^{m}\\sigma_{i}D(s_{t},s_{t+1})\\right]}\\end{array}$ bbe et hteh en uemm-- $B$ be the error bound of $D A R C$ in the source domain, i.e. $\\begin{array}{r}{\\mathbb{E}_{p_{s r},\\pi_{D A R C}^{*}}\\left[\\sum_{t}r(s_{t},a_{t})\\right.\\bar{+}\\left.\\mathcal{H}[a_{t}|s_{t}]\\right]-\\mathbb{E}_{p_{s r},\\pi_{D A R C}}\\left[\\sum_{t}r(s_{t},a_{t})\\right]\\leq B}\\end{array}$ and $W$ be the upper bound of the importance weight, i.e. $\\rho(s_{t},s_{t+1})\\;\\leq\\;W,$ , $\\forall(s_{t},s_{t+1})$ . Let discriminator class $\\mathcal{D}$ be $a$ $\\Delta$ -bounded function, i.e. $|D_{\\omega}(s_{t},s_{t+1})|\\;\\leq\\;\\Delta$ given any $\\left(s_{t},s_{t+1}\\right)$ . $\\|r\\|_{\\mathcal{D}}$ measures the richness of the discriminator to represent the ground truth reward as defined in Appendix B.2. $d_{D}$ is a defined neural network distance between the $\\left(s_{t},s_{t+1}\\right)$ distributions generated by the \u03c0DARC and $\\pi_{\\widehat{\\zeta}}$ defined in Appendix B.1. Given the empirical training error of the imitation learning, i.e. $\\begin{array}{r}{d_{\\mathcal{D}}\\big(\\hat{\\tau}_{\\pi_{D A R C}}^{s\\bar{r}c},\\hat{\\tau}_{\\hat{\\zeta}}^{t r g}\\big)-\\operatorname*{inf}_{\\zeta}d_{\\mathcal{D}}\\big(\\hat{\\tau}_{\\pi_{D A R C}}^{s r c},\\hat{\\tau}_{\\zeta}^{t r g}\\big)\\leq\\hat{\\epsilon},\\,\\forall\\,\\delta\\in(0,1)\\,}\\end{array}$ , with probability at least $1-\\delta$ , we have ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/91da790a881cc513574dbdcb1589e91333ca1b33e05523aabd31d08ff595c1cf.jpg", "img_caption": ["IMITATION LEARN I NG ERROR BOUND"], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Proof. We can first decompose it into three terms: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{0},\\tau^{*}}\\left[\\sum_{t}r(s_{t},a_{t})\\right]-\\mathbb{E}_{p_{0},\\xi}\\left[\\sum_{t}r(s_{t},a_{t})\\right]}\\\\ &{=\\underbrace{\\mathbb{E}_{p_{0},\\tau^{*}}\\left[\\sum_{t}r(s_{t},a_{t})\\right]-\\mathbb{E}_{p_{0},\\tau^{*}}\\sum_{t=0}^{\\tau}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}(a_{t}|s_{t})\\right]}_{I_{1}}}\\\\ &{\\qquad+\\underbrace{\\mathbb{E}_{p_{0},\\tau^{*}}\\sum_{t=0}^{\\tau}\\left[\\sum_{t}r(s_{t},a_{t})+\\mathcal{H}(a_{t}|s_{t})\\right]-\\mathbb{E}_{p_{0},\\tau_{0},\\mathrm{nose}}\\left[\\sum_{t}r(s_{t},a_{t})\\right]}_{I_{2}}}\\\\ &{\\qquad+\\underbrace{\\mathbb{E}_{p_{0},\\tau_{0},\\mathrm{nose}}\\left[\\sum_{t}r(s_{t},a_{t})\\right]-\\mathbb{E}_{p_{0},\\tau_{0}}\\left\\{\\sum_{t}r(s_{t},a_{t})\\right\\}}_{I_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Based on the formulation, $\\pi_{D A R C}^{*}$ can generate optimal trajectories for the target domain in the source domain so that $I_{1}=\\bar{0}$ . Also, the $I_{2}$ term is the training error of the DARC and the entropy term of the optimal DARC policy, and we can assume together they are bounded by $B$ . Then, we only need to bound the $I_{3}$ terms. Combining Lemma B.4, we have ", "page_idx": 17}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/ad474b7cd1c6be1738625b3394b6e852a5ea1b8c4b43d0c395b075670cb7f243.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Additional Experimental Details and Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Code is available at https://github.com/guoyihonggyh/Off-Dynamics-Reinforcement-Learning-viaDomain-Adaptation-and-Reward-Augmented-Imitation. ", "page_idx": 18}, {"type": "text", "text": "C.1 Estimation of $\\Delta r(s_{t},a_{t},s_{t+1})$ and importance weight $\\frac{p_{\\mathbf{trg}}(s_{t+1}|s_{t},a_{t})}{p_{\\mathbf{src}}(s_{t+1}|s_{t},a_{t})}$ ", "page_idx": 18}, {"type": "text", "text": "Following the DARC [3], the importance weight can be estimated with the following two binary classifiers $p(\\mathrm{trg}|s_{t},a_{t})$ and $p(\\Psi|s_{t},a_{t},s_{t+1})$ with Bayes\u2019 rules: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p({\\mathrm{trg}}|s_{t},a_{t},s_{t+1})=p_{{\\mathrm{trg}}}(s_{t+1}|s_{t},a_{t})p(s_{t},a_{t}|{\\mathrm{trg}})p({\\mathrm{trg}})/p(s_{t},a_{t},s_{t+1}),}\\\\ &{\\qquad\\qquad\\qquad p(s_{t},a_{t}|{\\mathrm{trg}})=p({\\mathrm{trg}}|s_{t},a_{t})p(s_{t},a_{t})/p({\\mathrm{trg}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Replacing the $p(s_{t},a_{t}|\\mathbf{trg})$ in Eq. (C.1) with Eq. (C.2), we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{\\mathrm{trg}}(s_{t+1}|s_{t},a_{t})=\\frac{p(\\mathrm{trg}|s_{t},a_{t},s_{t+1})p(s_{t},a_{t},s_{t+1})}{p(\\mathrm{trg}|s_{t},a_{t})p(s_{t},a_{t})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we can obtain the psrc(st+1|st, at) = p(src|spt(,sract,sst,+a1))pp((sst,,aat,)st+1). ", "page_idx": 18}, {"type": "text", "text": "We can calculate the $\\Delta r(s_{t},a_{t},s_{t+1})$ following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Gamma}_{9}(s_{t},s_{t+1})=\\log\\left(\\frac{p_{\\mathrm{trg}}\\big(s_{t+1}\\big|s_{t},a_{t}\\big)}{p_{\\mathrm{src}}\\big(s_{t+1}\\big|s_{t},a_{t}\\big)}\\right)}\\\\ &{\\qquad\\qquad=\\log p(\\mathrm{trg}|s_{t},a_{t},s_{t+1})-\\log p(\\mathrm{trg}|s_{t},a_{t})+\\log p(\\mathrm{src}|s_{t},a_{t},s_{t+1})-\\log p(\\mathrm{src}|s_{t},a_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\rho\\big(s_{t},s_{t+1}\\big)$ can be obtained from $\\rho(s_{t},s_{t+1})=\\exp\\left[\\Delta r(s_{t},a_{t},s_{t+1})\\right]$ ", "page_idx": 18}, {"type": "text", "text": "Training the classifier $p(\\mathbf{trg}|s_{t},a_{t})$ and $p(\\mathbf{trg}|s_{t},a_{t},s_{t+1})$ The two classifiers are parameterized bu $\\theta_{\\mathrm{SA}}$ and $\\theta_{\\mathrm{SAS}}$ . To update the two classifiers, we sample one mini-batch of data from the source replay buffer $D_{\\mathrm{src}}^{\\zeta}$ and the target replay buffer $D_{\\mathrm{src}}^{\\zeta}$ respectively. Imbalanced data is considered here as each time we sample the same amount of data from the source and target domain buffer. Then, the parameters are learned by minimizing the standard cross-entropy loss: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{SAS}}=-\\mathbb{E}_{\\mathcal{D}_{\\mathrm{sc}}^{\\zeta}}\\left[\\log p_{\\theta_{\\mathrm{sas}}}(\\mathrm{trg}\\big|s_{t},a_{t},s_{t+1})\\right]-\\mathbb{E}_{\\mathcal{D}_{\\mathrm{tg}}^{\\zeta}}\\left[\\log p_{\\theta_{\\mathrm{sas}}}(\\mathrm{trg}\\big|s_{t},a_{t},s_{t+1})\\right],}\\\\ &{\\mathcal{L}_{\\mathrm{SA}}=-\\mathbb{E}_{\\mathcal{D}_{\\mathrm{sc}}^{\\zeta}}\\left[\\log p_{\\theta_{\\mathrm{sa}}}(\\mathrm{trg}\\big|s_{t},a_{t},s_{t+1})\\right]-\\mathbb{E}_{\\mathcal{D}_{\\mathrm{tg}}^{\\zeta}}\\left[\\log p_{\\theta_{\\mathrm{sa}}}(\\mathrm{trg}\\big|s_{t},a_{t},s_{t+1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, $\\theta=(\\theta_{\\mathrm{SAS}},\\theta_{\\mathrm{SA}})$ is obtained from: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\theta=\\operatorname*{argmin}_{\\theta}\\mathcal{L}_{C E}(\\mathcal{D}_{\\mathrm{src}}^{\\zeta},\\mathcal{D}_{\\mathrm{trg}}^{\\zeta})}}\\\\ &{=\\operatorname*{argmin}_{\\theta}[\\mathcal{L}_{\\mathrm{SAS}}+\\mathcal{L}_{\\mathrm{SA}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.2 Description of Baseline Methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Importance Sampling for Reward (IS-R) With $(s_{t},a_{t},s_{t+1})$ from the source domain, the IS-R directly re-weight the reward in each transition. We can view IS-R as learning the SAC with rewards pptsrrgc((sstt++11||sstt,,aatt))rt(st, at) and seeking to maximize the following objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{(s_{t},a_{t},s_{t+1})\\sim\\pi(\\cdot|s_{t})\\times p_{\\mathrm{sr}}(\\cdot|s_{t},a_{t})}\\left[\\sum_{t}\\frac{p_{\\mathrm{trg}}(s_{t+1}|s_{t},a_{t})}{p_{\\mathrm{src}}(s_{t+1}|s_{t},a_{t})}r_{t}(s_{t},a_{t})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Importance Sampling for SAC Actor and Critic Loss (IS-ACL) Another way of doing importance sampling is by re-weighting the actor and critic loss in SAC. The loss for the Q-network in SAC becomes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{\\phi}{\\operatorname*{min}}\\mathbb{E}_{(s_{t},a_{t},s_{t+1})\\sim\\pi(\\cdot|s_{t})\\times p_{\\mathrm{src}}(\\cdot|s_{t},a_{t})}\\left[\\frac{p_{\\mathrm{trg}}\\big(s_{t+1}|s_{t},a_{t}\\big)}{p_{\\mathrm{src}}\\big(s_{t+1}|s_{t},a_{t}\\big)}\\big(Q_{\\phi}\\big(s_{t},a_{t}\\big)-y\\big(s_{t},a_{t},d\\big)\\big)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $d$ is the done signal, and the target is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\ny(s_{t},a_{t},d)=r+\\gamma(1-d)\\left[\\operatorname*{min}_{j=1,2}Q_{\\mathrm{trg},j}(s_{t+1},a^{\\prime})-\\alpha\\log\\pi(a^{\\prime}|s_{t+1})\\right],a^{\\prime}\\sim\\pi(a|s_{t+1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The actor loss is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{a\\sim\\pi}\\frac{p_{\\mathrm{trg}}(s_{t+1}|s_{t},a_{t})}{p_{\\mathrm{src}}(s_{t+1}|s_{t},a_{t})}\\left[Q^{\\pi}(s,a)-\\alpha\\log\\pi(a|s)\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "DAIL In DARAIL, the policy is optimized with the reward estimator $R_{A E}$ with the true reward from the source domain. We also want to compare the vanilla imitation learning with importance weight. The objective is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\zeta}\\operatorname*{max}_{D_{\\omega}}\\Bigg\\{\\mathbb{E}_{p_{\\mathrm{sc}},\\zeta}\\Bigg[\\sum_{t}\\rho(s_{t},s_{t+1})\\log D_{\\omega}(s_{t},s_{t+1})\\Bigg]+\\mathbb{E}_{(s_{t},s_{t+1})\\sim\\tau_{\\pi_{\\mathrm{BRK}}}^{\\mathrm{av}}}\\Bigg[\\sum_{t}\\log(1-D_{\\omega}(s_{t},s_{t+1}))\\Bigg]\\Bigg\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, following the Eq.(C.3), the objective of policy optimization without the reward estimator is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\zeta}\\mathbb{E}_{p_{\\mathrm{src}},\\zeta}\\Bigg[\\sum_{t}-\\rho(s_{t},s_{t+1})\\log D_{\\omega}\\big(s_{t},s_{t+1}\\big)\\Bigg].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can view it as a reduced version of our proposed method, which uses the reward function provided by the discriminator and importance weight. ", "page_idx": 19}, {"type": "text", "text": "MBPO [19]. MBPO is a model-based RL method. We train the MBPO in the source domain and deploy it to the target domain. ", "page_idx": 19}, {"type": "text", "text": "MATL [20]. MATL modified the reward on both the source and target domains and aligned the trajectories on both domains. Unlike our method, they need access to the reward from the target domain. ", "page_idx": 19}, {"type": "text", "text": "GARAT[10] GARAT is a grounded action transformation approach that simulates target transitions $(s_{t},a_{t},s_{t+1},r)$ in the source domain with modified action, where the modified action is learned from imitation learning. ", "page_idx": 19}, {"type": "text", "text": "C.3 Broken with probability $p_{f}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As discussed, we use the broken with probability for Ant and Walker2d. The dynamics shift created by freezing one action varies across environments. For instance, in the Ant robot, the 0-index controls the rotor between the torso and front left hip, while in the HalfCheetah, the 0-index controls the back thigh rotor. So, the broken Ant experiences a larger shift than the broken HalfCheetah if we break the 0-index for both environments. Also, the broken environment in Walker2d and Ant creates such a large dynamics shift that it is overly difficult to adapt from the source domain, i.e., DARC cannot obtain the optimal reward in the source domain. We then introduce the broken with probability $p_{f}$ to better control the magnitude of dynamics shift. Broken with probability $p_{f}$ means the 0-index action is frozen with probability $p_{f}$ and follows the commanded torque with probability $1-p_{f}$ . In Reacher and HalfCheetah, the source environment is broken with probability 1. Ant and Walker2d\u2019s source domain is broken with a probability of 0.8. ", "page_idx": 19}, {"type": "text", "text": "Figure 5 shows the performance of DARC in Ant and Walker2d under different broken probability $p_{f}$ in the source domain. We can observe that when $p_{f}=1.0$ , the performance degradation of evaluating in the target domain is larger than the $p_{f}=0.8$ case. Also, when $p_{f}=1.0$ , the DARC evaluation performance in the target domain is close to 0. Moreover, we notice that in the $p_{f}=1.0$ case, DARC training performance in the source domain receives a much lower reward than the $p_{f}=0.8$ case. However, we want to mimic the DARC behavior in the imitation learning, so we want DARC to be able to receive optimal reward in the source domain. Thus, for the Ant and Walker2d environment, we choose $p_{f}=0.8$ for the source domain. ", "page_idx": 19}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/a809ead7d95f09154c61cbbbd3152f3266ec218ed2c9b3337a3d84f59f594f0a.jpg", "img_caption": ["Figure 5: Training reward in the source domain, i.e. $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{src}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]$ , and evaluation reward in the target domain , i.e. $\\begin{array}{r}{\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{trg}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]}\\end{array}$ , for DARC in Ant and Walker2d with different broken probability $p_{f}$ in the source domain. (a) and (c) shows the performance of DARC under $p_{f}=0.8$ , and (a) and (c) shows the performance of DARC under $p_{f}=1.0$ . The performance of DARC under $p_{f}=1.0$ is much worse than the case $p_{f}=0.8$ , and the performance gap between DARC in the source and target is larger, showing that the dynamics shift is overly large to adapt and learn a good expert demonstration. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.4 Training Curve of the DARAIL and Baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We show the training curve of DARAIL and baselines in different environments under the broken source environment setting in Figure 6 corresponding to the result in Table 3. We also show the training curve of modifying the configuration in Figure 7 and 8. ", "page_idx": 20}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/72b41eba5b3c2dbaa72519e3fe07ecfad2fd85e263b93395e2136764b11a76c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 6: Upper horizon line: DARC reward in the source domain. Lower horizon line: DARC reward in the target domain. The figures show the mean value of multiple runs and the standard deviation. The figure shows that our proposed method performs better than DARC in the target domain and other baseline methods. ", "page_idx": 20}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/028a11ab8d4e63deeccf454ff5576792458543b5e885a8cfe0a1226d3111d073.jpg", "table_caption": ["Table 5: Comparison of DARAIL with DARC, 0.5 gravity. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/5c967b96fcc4d6ea89232f112e71309386dafa4462926c3e01c4d4b6cb554835.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 7: Training Curve of changing gravity setting. Top: target domain gravity $\\times0.5$ , button: target domain gravity $\\times1.5$ . Upper horizon line: DARC reward in the source domain. Lower horizon line: DARC reward in the target domain. The figures show the mean value of multiple runs and the standard deviation. The figure shows that our proposed method performs better than DARC in the target domain and other baseline methods. ", "page_idx": 21}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/ac5f5bc5ce52e36f98dcf5a36b1b0f2cefcda8d7a7b25965a23a572bec185246.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 8: Training Curve of changing density setting. Top: target domain density $\\times0.5$ , button: target domain density $\\times1.5$ . Upper horizon line: DARC reward in the source domain. Lower horizon line: DARC reward in the target domain. The figures show the mean value of multiple runs and the standard deviation. The figure shows that our proposed method performs better than DARC in the target domain and other baseline methods. ", "page_idx": 21}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/dd1b39eabe5447d3c50a7703b23795355568f10e42da0ac0735116b1541dddd0.jpg", "img_caption": ["Figure 9: Training reward in the source domain, i.e. $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{src}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]$ , and evaluation reward in the target domain , i.e. E\u03c0DARC,ptrg [ $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{trg}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]$ , for DARC in four environments. Deploying trained DARC policy to the target domain will cause performance degradation. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/d83283a7691bd81e640708c7c117d0ed8a08f60c6bdc38a9a5f297a3912b9be4.jpg", "table_caption": ["Table 6: Comparison of DARAIL with baselines in off-dynamics RL, 0.5 gravity. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/6144cd1f6cd4de7af0c014f5bce8429b3fe42d09e6ba72426cee8cf60b2478e7.jpg", "table_caption": ["Table 7: Comparison of DARAIL with DARC, 0.5 density "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/9b066b4d75af19369e7f3f03c63996d5ff3063ad9746cadc9ff8cbf36e34a5ef.jpg", "table_caption": ["Table 8: Comparison of DARAIL with baselines in off-dynamics RL, 0.5 density. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/1012439c66b0c8172b35b7c590420d113b94f354cfc626b4ac26caab2e2b4ba1.jpg", "table_caption": ["Table 9: Comparison of DARAIL with DARC, 1.5 density. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/a2ec2436a017f76fd6178b84d77c45d7448045e80800f5b79c035815f1d7e3a5.jpg", "table_caption": ["Table 10: Comparison of DARAIL with baselines in off-dynamics RL, 1.5 density. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/68b6f71885fc8ce6588298b964ede806178810b734d9d184b98d4d40125538c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 10: Experiments of DARC and DARAIL on the intact source and broken target setting. We observe that the DARC does not have significant performance degradation. Also, we show that DARAIL can perform similarly to DARC in this setting. ", "page_idx": 23}, {"type": "text", "text": "C.5 DARC training and evaluation performance on broken source setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figure 9 shows the performance of DARC trained in the source and evaluated in the target domain under broken source environment setting. The training reward is the reward obtained in the source domain, i.e. E\u03c0DARC,psrc [ $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{src}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]$ and the evaluation is the reward deployed in the target domain, i.e. $\\begin{array}{r}{\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{trg}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]}\\end{array}$ . We observe the performance degradation in the figure 9. Empirically, we notice that the DARC policy performance in the source domain, E\u03c0DARC,psrc [ $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{src}}}}[\\sum_{t}r(s_{t},a_{t})]$ , is close to the optimal reward in the target domain which matches with the DARC objective that DARC can generate target optimal trajectories in the source domain. However, deploying it to the target domain will result in performance degradation and a suboptimal reward due to the dynamics shift. ", "page_idx": 23}, {"type": "text", "text": "C.6 Performance of DARAIL on broken target environment ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We show the performance of DARAIL in the intact source and broken target environment setting in Figure 10 (the setting in DARC paper [3]). We observe that our method outperforms the DARC reward in the target domain, $\\mathbb{E}_{\\pi_{\\mathrm{DARC},p_{\\mathrm{trg}}}}\\left[\\sum_{t}r\\big(s_{t},a_{t}\\big)\\right]$ . Also, we see that the performance of DARC in the source domain and target domain are very similar. Compared with the performance gap when the source environment is broken in Figure 9. As discussed, DARC works well when the assumption that the target optimal policy performs well in the source domain is satisfied. In the broken target setting, the target optimal policy can perform the same in the source domain. ", "page_idx": 23}, {"type": "text", "text": "Further, empirically, in the broken target setting, the DARC policy learns a near 0 value for the broken joint, which guarantees that the policy can generate similar trajectories in the two domains. Also, maximizing the adjusted cumulative reward in the source domain with a policy with a near 0 value for the broken joint is equivalent to maximizing the cumulative reward in the target domain. Thus, DARC perfectly suits the broken target setting. However, in the broken source setting and other more general dynamics shift cases, the target optimal policy might not perform well in the source domain. For example, in the broken source setting, the target optimal policy will perform poorly in the source domain as it loses one joint in the source domain. Another way to understand why DARC fails is that it learns an arbitrary value for the broken joint, which becomes detrimental in the target domain. However, this is just an artifact of the particular setting. As we discussed above, the intrinsic reason that DARC fails is the violation of the assumption. ", "page_idx": 23}, {"type": "text", "text": "C.7 Performance of mimicking source optimal trajectories ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Figure 11, We compare our DARAIL, which uses DARC trajectories in the source domain as expert demonstrations and mimicking source optimal trajectories regardless of the target domain. ", "page_idx": 23}, {"type": "text", "text": "C.8 Access to the target domain data compared to DARC. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Both DARC and DARAIL require some limited access to the target rollouts. In DARAIL, the imitation learning step only rolls out data from the target domain every 100 steps of the source domain rollouts, which is $1\\%$ of the source domain rollouts. We claim that more target domain rollouts will not improve DARC\u2019s performance due to its suboptimality, and DARAIL is better not because of having more target domain rollouts. We verify it by comparing DARC and DARAIL with the same amount of rollouts from the target domain in the broken source environment setting in Tables 11 and 12. Specifically, we examine DARAIL with 5e4 target rollouts alongside DARC with 2e4 and 5e4 target rollouts. DARAIL has 5e3 target rollouts for the Reacher environment, while DARC has 3e3 and 5e3 rollouts. From the results, we see that increasing the target rollouts from 2e4 to 5e4 (or from 3e3 to 5e3 in the case of Reacher) does not yield a significant improvement in DARC\u2019s performance due to its inherent suboptimality. Notably, DARAIL consistently outperforms DARC when given comparable levels of target rollouts. ", "page_idx": 23}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/89a8a29880aafd07d7544310b26734d895cf3d80037135ac0557313f6b34cb48.jpg", "img_caption": ["Figure 11: Experiments on using source optimal policy as the expert demonstration instead of the DARC policy as the expert demonstration. Mimicking the source optimal trajectories will not receive a similar performance as mimicking DARC performance, and there is a big performance gap between the source optimal reward and imitation learning performance in the target domain. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Table 11: Comparison with DARC with the same amount of rollout from the target. The number in the columns represents the amount of rollout from the target. More target domain rollout will not improve the DARC\u2019s performance further. Experiment on broken source setting. ", "page_idx": 24}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/bb20861b829e30751883d4b9e81e929d4e0c25b7676fd0f20038d25f2e95a3b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 12: Comparison with DARC with the same amount of rollout from target, on Reacher. The number in the columns represents the amount of rollout from the target. More target domain rollout will not improve the DARC\u2019s performance further. Experiment on broken source setting. ", "page_idx": 24}, {"type": "table", "img_path": "k2hS5Rt1N0/tmp/8ac2a8caba98284175e95e7ee01862d1c94ae26cdf67b1b205a3f8284a3f6484.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/a4e84a45d096960c9c3691f16918f9591c8ccff2c0be5182136047e69a98f0ab.jpg", "img_caption": ["Figure 12: Experiment on how cumulative n-step importance weight performs on DARAIL. Per-step importance weight significantly outperforms using the last n-step multiplication of the importance weight. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/e7620d9064b1155309b86bbfdaf4e22973058222297e285da19ca8412dae3c3c.jpg", "img_caption": ["Figure 13: Experiment on how cumulative n-step importance weight performs on IS-R in broken source setting. Per-step importance weight significantly outperforms using the last n-step multiplication of the importance weight. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "D Ablation Study ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Per-Step Importance Weight v.s Cumulative Importance weight ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In our paper, to reduce the variance, we use the per-step importance weight $\\frac{p_{\\mathrm{trg}}\\big(s_{t},s_{t+1}\\big)}{p_{\\mathrm{src}}\\big(s_{t},s_{t+1}\\big)}$ for the importance sampling method and DARAIL. Here, we compare the per-step importance weight with the cumulative $\\mathbf{n}$ -step importance weight, which is the multiplication of the weight before time step $t$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho_{n}\\big(s_{t},s_{t+1}\\big)=\\prod_{i=t-n}^{t}\\frac{p_{\\mathrm{trg}}\\big(s_{i+1}|s_{i},a_{i}\\big)}{p_{\\mathrm{src}}\\big(s_{i+1}|s_{i},a_{i}\\big)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that here, the importance weight is the multiplication of the last n steps weight instead of the multiplication from $i=0$ to $i=t$ . Because the cumulative importance weight might have a $N a N$ value due to the product. Thus, the optimization step for the imitation learning of DARAIL is as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\zeta}\\mathbb{E}_{p_{\\mathrm{xr}},\\zeta}\\Big[\\sum_{t}\\rho_{n}(s_{t},s_{t+1})r(s_{t},s_{t+1})-(1-\\rho_{n}(s_{t},s_{t+1}))\\log D_{\\omega}(s_{t},s_{t+1})\\Big].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, the objective of IS-R is: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{p_{\\mathrm{src}},\\pi}\\left[\\sum_{t}\\rho_{n}(s_{t},s_{t+1})r(s_{t},s_{t+1})\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We compare the per-step importance weight and the cumulative n-step importance weight on DARAIL and IS-R. Specifically, we consider $n=[10,50]$ for HalfCheetah and Walker2d, respectively. We show the results of DARAIL in Figure 12 and the results of IS-R in Figure 13. We see that the cumulative importance weight doesn\u2019t perform well on both methods and environments. In HalfCheetah, we can observe that the 10-step cumulative importance weight performs better than the 50-step one. And similar patterns appear in the Walker2d. Thus, we can conclude that per-step importance weight will have a lower variance and be more favorable in our experiment. ", "page_idx": 26}, {"type": "text", "text": "D.2 Update Steps of Discriminator ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/945661c772464c266b389e7a6b61ffb73488946acdc2f2fd2959a57c4b678cd4.jpg", "img_caption": ["Figure 14: Experiment on the performance of DARAIL under different update steps of the discriminator in broken source setting. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "In imitation learning, we alternatively update the generator and discriminator. In practice, we normally update the generator several steps and then update the discriminator once. The update steps, updating the discriminator every how many training steps, is a hyperparameter and is important in GAN training. The smaller the update steps are, the higher the update frequencies are. We tune the update steps and show the result of it in different environments. The best discriminator update step in HalfCheetah, Walker2d, and Reacher are 50, 50, and 1000, respectively. We varied the discriminator update steps in HalfCheetah and Walker2d in [10, 50, 500, 1000] steps, and the update steps in Reacher are [50, 100, 1000, 2000] steps. Figure 14 shows the effects of different discriminator update steps in the final performance. As we can see, for all three environments, a smaller update step (higher update frequency) is preferred as it can learn a better reward estimation. However, as we noticed, for example, for HalfCheetah and Walker2d, when the update step is 50, decreasing it to 10 will not further improve the performance. ", "page_idx": 26}, {"type": "text", "text": "D.3 Increase the weight on the modified reward of DARC. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We tested DARC algorithm with modified reward $r(s_{t},a_{t})+\\eta\\Delta(s_{t},a_{t},s_{t+1})$ with $\\eta>1$ instead of $\\eta=1$ . And the $\\eta=1$ is derived from the distribution matching objective in Eq.(3.3). We show the results in Figure 15 under the broken source environment setting. We can see that increasing $\\eta$ will not increase the DARC performance in the target domain but will hurt the performance of DARC in the target domain. ", "page_idx": 26}, {"type": "text", "text": "D.4 Hyperparameters ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For a fair comparison, we tune the parameters of baselines and our method. The hidden layers of the policy and value network are [256,256] for the HalfCheetah, Ant, and Walker2d and [64,64] for Reacher. And the hidden layer of the two classifiers is [64] for the HalfCheetah, Ant, and Walker2d and [32] for Reacher. The batch size is set to be 256. We regularize the state by adding the running average of the state. We fairly tune the learning rate from $[3e-4,1e-4,5e-5,1e-5]$ . For those methods that require the importance weight $\\rho$ , we tune the update steps of the two classifiers trained to obtain the importance weight from [10, 50, 100]. We also add Gaussian noise $\\displaystyle\\epsilon\\sim N(0,1)$ to the input of the classifiers for regularization, and the noise scale is selected from [0.1, 0.2, 1.0]. For the imitation learning component, the number of expert trajectories is 20. We further tune the update steps of the discriminator and add Gaussian noise to the input of the discriminator. ", "page_idx": 26}, {"type": "image", "img_path": "k2hS5Rt1N0/tmp/0b4ccdb4750fa6c951756dddc1e4ad4219700c4b85936cf56fb1ba8336da09b4.jpg", "img_caption": ["Figure 15: Experiment of different $\\eta$ in the modified reward $r(s_{t},a_{t})+\\eta\\!+\\!\\Delta\\!\\left(s_{t},a_{t},s_{t+1}\\right)$ for DARC in broken source environment setting. Top row: $\\eta=1$ , middle row: $\\eta=1.5$ and button row: $\\eta=2$ . We observe that increasing the $\\eta$ will reduce the performance degradation in most cases, but it will also harm the performance of DARC in the target domain as increasing $\\eta$ focuses more on making the DARC perform more similarly in both domains instead of maximizing the cumulative reward. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "D.5 Computation Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We run the experiment on a single GPU: NVIDIA RTX A5000-24564MiB with 8-CPUs: AMD Ryzen Threadripper 3960X 24-Core. Each experiment requires 12GB RAM and require 20GB available disk space for storage of the data. ", "page_idx": 27}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "A potential limitation will be that we rely on DARC or similar methods to generate state pairs. An overly large dynamics shift, or data limitation may prevent us from obtaining high-quality state space data to imitate in the source domain. We do the experiment on the Mujoco environment instead of the real-world sim-2-real problem. We leave the investigation of this to future work. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Abstract and Introduction section states the contribution. And in the introduction section, we have a contribution list. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We talk about the limitation of our method in the Appendix E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes. We present our theoretical result in Section 4 and the proof is in Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide details about how we create the dynamics shift and the hyperparameters that we used in the experiments in Appendix D.4 and release the code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide a GitHub repository in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the details of the experiment setting, including how to create the dynamics shift in the Experiment section. We also describe the hyperparameter tuning in the Appendix D.4. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have multiple runs of each experiment and report the mean value and standard deviation in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the GPU/CPU as well as the RAM and storage information for each experiment. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more computing than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our data is open source benchmarks in the RL research field. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the conclusion, we briefly mentioned that our method avoids directly training a policy in a high-risk environment in safety-critical tasks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We run the experiment on the simulated RL benchmarks; thus, no such issue exists. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide citations to all the data and related work in our paper. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We include the code in our paper. Also, details about the implementation are included in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our experiments are conducted on the RL benchmarks and thus do not involve any crowdsourcing or research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our research and experiment don\u2019t require IRB as we conducted experiments on simulated RL benchmarks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]