[{"figure_path": "k2hS5Rt1N0/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of DARAIL with DARC, broken source environment.", "description": "This table compares the performance of DARAIL and DARC in four MuJoCo environments with a broken source environment.  It shows the DARC evaluation reward (performance in the target domain), the DARC training reward (performance in the source domain), the optimal reward achievable in the target domain, and the DARAIL reward (the proposed method's performance in the target domain). The results demonstrate that DARAIL significantly outperforms DARC in the target domain, mitigating the performance degradation observed when DARC is directly deployed in the target domain.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of DARAIL with baselines in off-dynamics RL, broken source environment.", "description": "This table presents a comparison of the DARAIL algorithm's performance against several baseline methods in off-dynamics reinforcement learning settings with a broken source environment.  The results are shown for four MuJoCo environments (HalfCheetah, Ant, Walker2d, and Reacher), each with performance measured by the average cumulative reward and standard deviation.  The baseline methods include DAIL, IS-R, IS-ACL, MBPO, MATL, and GARAT. The table allows for a direct comparison of DARAIL's performance with state-of-the-art methods under challenging conditions.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_20_1.jpg", "caption": "Table 1: Comparison of DARAIL with DARC, broken source environment.", "description": "This table compares the performance of DARAIL and DARC in four MuJoCo benchmark environments (HalfCheetah, Ant, Walker2d, and Reacher) with a broken source environment.  The \"DARC Evaluation\" column shows DARC's performance in the target domain (where the 0-index in the action is intact). The \"DARC Training\" column shows DARC's performance in the source domain (where the 0-index in the action is broken). The \"Optimal in Target\" column indicates the optimal reward achievable in the target domain. Finally, the \"DARAIL\" column shows the performance of the proposed DARAIL algorithm in the target domain. The results demonstrate DARAIL's superior performance compared to DARC when the source domain has limited functionality compared to the target domain.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_22_1.jpg", "caption": "Table 3: Comparison of DARAIL with baselines in off-dynamics RL, broken source environment.", "description": "This table presents a comparison of the DARAIL algorithm's performance against several baseline methods in off-dynamics reinforcement learning scenarios.  The experiments were conducted in environments with a broken source setting, meaning the source environment has limitations. The table shows the average reward obtained by each algorithm across four different MuJoCo environments (HalfCheetah, Ant, Walker2d, and Reacher).  The results demonstrate DARAIL's superior performance compared to the baselines. The metrics reported allow a clear assessment of the relative performance improvements achieved by DARAIL.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_22_2.jpg", "caption": "Table 1: Comparison of DARAIL with DARC, broken source environment.", "description": "This table compares the performance of DARAIL and DARC in four MuJoCo environments (HalfCheetah, Ant, Walker2d, and Reacher) when the source domain has a broken environment (0-index action frozen to 0).  It shows the DARC Evaluation (performance in the target domain), DARC Training (performance in the source domain), the optimal reward achievable in the target domain, and the DARAIL performance in the target domain. The results highlight DARAIL's improved performance over DARC in the target domain, particularly when facing challenging off-dynamics scenarios.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_22_3.jpg", "caption": "Table 3: Comparison of DARAIL with baselines in off-dynamics RL, broken source environment.", "description": "This table presents a comparison of the DARAIL algorithm's performance against several baseline methods in off-dynamics reinforcement learning scenarios with a broken source environment.  The table shows the average reward achieved by each algorithm across four different MuJoCo environments (HalfCheetah, Ant, Walker2d, and Reacher).  The results demonstrate DARAIL's superior performance compared to the baselines, highlighting its effectiveness in handling dynamics shifts between source and target domains when the source domain is compromised.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_22_4.jpg", "caption": "Table 2: Comparison of DARAIL with DARC, 1.5 gravity.", "description": "This table compares the performance of DARAIL and DARC in MuJoCo environments with a 1.5 times gravity setting.  It shows the DARC Evaluation (performance of DARC in the target domain), DARC Training (performance of DARC in the source domain), the optimal reward achievable in the target domain, and the performance of DARAIL (the proposed method) in the target domain.  The results demonstrate the improvement of DARAIL over DARC, especially considering the significant performance drop of DARC when deployed to the target domain.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_22_5.jpg", "caption": "Table 3: Comparison of DARAIL with baselines in off-dynamics RL, broken source environment.", "description": "This table presents a comparison of the DARAIL algorithm's performance against several baseline methods in off-dynamics reinforcement learning settings.  The source environment is modified to be 'broken', meaning a specific action is constrained. The table shows the average reward achieved by each algorithm across four different Mujoco benchmark environments (HalfCheetah, Ant, Walker2d, and Reacher).  The results highlight DARAIL's superior performance compared to the baselines, demonstrating its effectiveness in handling dynamics shifts between source and target domains.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_24_1.jpg", "caption": "Table 11: Comparison with DARC with the same amount of rollout from the target. The number in the columns represents the amount of rollout from the target. More target domain rollout will not improve the DARC's performance further. Experiment on broken source setting.", "description": "This table compares the performance of DARAIL and DARC when provided with the same number of rollouts from the target domain.  The results show that increasing the number of target domain rollouts does not significantly improve DARC's performance, highlighting its inherent sub-optimality. In contrast, DARAIL consistently outperforms DARC, even with comparable target rollouts. This demonstrates the effectiveness of DARAIL's imitation learning component in mitigating the limitations of DARC in handling dynamics shifts.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/tables/tables_24_2.jpg", "caption": "Table 12: Comparison with DARC with the same amount of rollout from target, on Reacher. The number in the columns represents the amount of rollout from the target. More target domain rollout will not improve the DARC's performance further. Experiment on broken source setting.", "description": "This table compares the performance of DARAIL and DARC on the Reacher environment with varying amounts of target domain rollouts.  It demonstrates that increasing the number of target rollouts does not significantly improve DARC's performance because of its inherent limitations.  DARAIL consistently outperforms DARC even with comparable target rollouts, highlighting its effectiveness in mitigating the sub-optimality of DARC.", "section": "5.2 Results"}]