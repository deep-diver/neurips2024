[{"figure_path": "k2hS5Rt1N0/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. \u0395DARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0DARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward and evaluation reward of the DARC algorithm in the source and target domains respectively. It also illustrates the performance degradation of DARC when deployed in the target domain due to the dynamics shift.  The figure also presents the learning framework of the proposed DARAIL algorithm, showing the training steps of DARC, discriminator, and generator. DARAIL uses reward modification and imitation learning to transfer a policy from the source to the target domain, addressing the suboptimality issues of the reward-only DARC approach.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_5_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward of DARC in the source domain and the evaluation reward in the target domain for the Ant environment. It also illustrates the learning framework of the proposed method, DARAIL, which consists of DARC training, discriminator training, and generator training steps. The figure highlights the performance degradation of DARC when deployed in the target domain and how DARAIL utilizes imitation learning to transfer the policy learned from DARC to the target domain.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_8_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the performance degradation of DARC when deployed to the target domain and the framework of DARAIL, which addresses this issue by using imitation learning to transfer the policy learned from reward modification to the target domain.  Panel (a) illustrates DARC's sub-optimal performance in the target domain despite resembling target optimal trajectories in its source domain training. Panel (b) details DARAIL's training process:  first training DARC in the source domain with modified rewards, then using DARC trajectories as expert data for discriminator and generator training in an imitation learning framework that refines the policy in the target domain.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_9_1.jpg", "caption": "Figure 3: Performance of DARC and DARAIL under different off-dynamics shifts on Ant. Action 0 is frozen (set to be 0) with probability pf in the source domain. From left to right, the off-dynamics shift becomes larger. As the shift becomes larger, the gap between DARC Training and DARC Evaluation is larger. Our method outperforms DARC on different dynamics shift.", "description": "This figure compares the performance of DARC and DARAIL algorithms on the Ant environment under different levels of dynamics shift, controlled by the probability (pf) of freezing action 0 in the source domain.  As pf increases, the dynamics shift between source and target domains grows larger.  The figure shows that the reward obtained by DARC in the target domain significantly degrades as the dynamics shift increases, whereas DARAIL consistently outperforms DARC, maintaining a higher reward in the target domain across various shift magnitudes.", "section": "5 Experiment"}, {"figure_path": "k2hS5Rt1N0/figures/figures_13_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. \u0395\u03c0DARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0DARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure illustrates the performance degradation of DARC in the target domain and introduces the DARAIL framework. (a) shows the training and evaluation reward curves for DARC on the Ant environment, highlighting the performance gap between the source and target domains. (b) presents a detailed breakdown of the DARAIL algorithm, including the DARC training, discriminator training, and generator training phases, emphasizing the use of imitation learning to transfer the policy from the source to the target domain.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_17_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward of DARC in the source domain and its evaluation reward in the target domain for the Ant environment.  It demonstrates performance degradation when deploying the DARC policy in the target domain, highlighting the limitations of DARC.  The second part illustrates the DARAIL framework, which addresses these limitations by using imitation learning to transfer the policy learned from reward modification to the target domain.  DARAIL comprises DARC training with a modified reward, discriminator training to classify data and provide a local reward function, and generator training using an augmented reward estimator.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_17_2.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. \u0395DARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0DARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the comparison of training and evaluation rewards for DARC in the Ant environment and illustrates the framework of the proposed DARAIL algorithm.  Panel (a) highlights the performance degradation of DARC when deployed in the target domain, showing a significant gap between the training reward (close to optimal) and the evaluation reward. Panel (b) details the DARAIL framework, outlining the DARC training phase, discriminator training to classify source and target data, and generator training to optimize the policy using augmented reward estimation from source domain reward and discriminator feedback.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_20_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. \u0395DARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0DARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward and evaluation reward of DARC (Domain Adaptation with Rewards from Classifiers) in the Ant environment. It illustrates the performance degradation of DARC in the target domain when compared to its training reward.  The figure also presents the learning framework of DARAIL (Domain Adaptation and Reward Augmented Imitation Learning), highlighting the DARC training, discriminator training, and generator training phases. DARAIL uses imitation learning to address the suboptimality issues of the pure modified reward methods.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_20_2.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows a comparison of the training reward of DARC in the source domain and its evaluation reward in the target domain, highlighting the performance degradation when deploying DARC in a new environment with a dynamics shift. It also illustrates the learning framework of DARAIL, which incorporates DARC training, discriminator training, and generator training to improve policy transfer and performance in the target domain.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_21_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. \u0395\u03c0DARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0DARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward in the source domain and evaluation reward in the target domain for the DARC (Domain Adaptation with Rewards from Classifiers) algorithm.  It also illustrates the performance degradation of DARC when deployed in the target domain and the framework of the proposed DARAIL (Domain Adaptation and Reward Augmented Imitation Learning) algorithm.  DARAIL utilizes reward modification for domain adaptation and generative adversarial imitation learning from observation, incorporating a reward augmented estimator for policy optimization.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_21_2.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the performance degradation of DARC when deployed in the target domain due to dynamics shift.  It also illustrates the DARAIL framework, which addresses this issue by incorporating imitation learning. DARAIL first trains a DARC policy in the source domain with a modified reward. Then, it uses imitation learning to transfer this policy to the target domain, improving performance.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_21_3.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward of DARC in the source domain and evaluation reward in the target domain for the Ant environment.  It highlights the performance degradation when a DARC policy trained in the source domain is directly deployed to the target domain. It also illustrates the learning framework of the proposed DARAIL method, which consists of three stages: DARC training with modified rewards to match target trajectory distributions, discriminator training to distinguish between expert (DARC) and generated trajectories, and generator training (policy update) using augmented reward estimation incorporating both source domain rewards and discriminator feedback.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_23_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the performance degradation of DARC when deployed to the target domain and introduces the DARAIL framework.  Panel (a) compares the training reward of DARC in the source domain to its evaluation reward in the target domain, demonstrating performance degradation. Panel (b) illustrates the DARAIL method, which uses DARC to generate initial trajectories, then trains a discriminator and generator to improve policy performance in the target domain via imitation learning.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_24_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. \u0395\u03c0DARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0DARC, ptrg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "The figure shows the training reward in the source domain and evaluation reward in the target domain for the DARC algorithm. It also illustrates the learning framework for the DARAIL algorithm, detailing the steps involved in DARC training, discriminator training, and generator training.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_25_1.jpg", "caption": "Figure 2: Performance of DARAIL and IPS-ACL on HalfCheetah and Walker2d under different importance weight clipping intervals. DARAIL outperforms IPS-ACL on all tasks. In Table 3, IPS-ACL receives comparable performance with DARAIL with the clipping interval [0.01,100], while the performance decreases significantly with different intervals.", "description": "This figure compares the performance of DARAIL and IPS-ACL on two different environments (HalfCheetah and Walker2d) under various importance weight clipping intervals.  The results show that DARAIL consistently outperforms IPS-ACL across all conditions. Notably, IPS-ACL's performance is sensitive to the choice of clipping interval, achieving comparable results to DARAIL only under a specific interval ([0.01, 100]), while its performance degrades significantly with other intervals.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/figures/figures_25_2.jpg", "caption": "Figure 2: Performance of DARAIL and IPS-ACL on HalfCheetah and Walker2d under different importance weight clipping intervals. DARAIL outperforms IPS-ACL on all tasks. In Table 3, IPS-ACL receives comparable performance with DARAIL with the clipping interval [0.01,100], while the performance decreases significantly with different intervals.", "description": "The figure compares the performance of DARAIL and IPS-ACL algorithms on HalfCheetah and Walker2d environments under various importance weight clipping intervals.  It demonstrates that DARAIL consistently outperforms IPS-ACL across all tested intervals.  The results highlight DARAIL's robustness to the choice of clipping interval, unlike IPS-ACL which shows significant performance degradation when the interval is poorly chosen.", "section": "5.2 Results"}, {"figure_path": "k2hS5Rt1N0/figures/figures_26_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the comparison of DARC and DARAIL. Subfigure (a) illustrates the training and evaluation reward of DARC in the Ant environment, highlighting the performance degradation when deploying the policy trained in the source domain to the target domain. Subfigure (b) presents the learning framework of DARAIL, which leverages DARC for domain adaptation and incorporates generative adversarial imitation learning for policy transfer, aiming to improve performance in the target domain.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}, {"figure_path": "k2hS5Rt1N0/figures/figures_27_1.jpg", "caption": "Figure 1: (a) Training reward in the source domain, i.e. EDARC, psrc [\u2211tr(st, at)], evaluation reward in the target domain, i.e. \u0395\u03c0\u03c1ARC, Purg [\u2211tr(st, at)] and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARC's objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.", "description": "This figure shows the training reward of DARC in the source domain and its evaluation reward in the target domain. It also shows the learning framework of DARAIL, which consists of three stages: DARC training, discriminator training, and generator training.  The results highlight the performance degradation of DARC when deployed to the target domain and illustrates how DARAIL aims to mitigate this issue by using imitation learning to transfer the policy learned in the source domain to the target domain.", "section": "3 Off-dynamics RL via Domain Adaptation and Reward Augmented Imitation Learning"}]