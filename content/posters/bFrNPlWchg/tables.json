[{"figure_path": "bFrNPlWchg/tables/tables_6_1.jpg", "caption": "Table 1: Decoder masking strategies on short-videos (32 frames) on EK100. We report fine-tuning top-1 action classification performance of MAE pre-trained models using pixel (RGB) or token (FSQ-MagViT) reconstruction targets. For decoder masking, we consistently use 15% as the token budget. We find that (1) compared to no decoder mask (None) with 100% budget, uniform masking scheme (Uniform [14]), random masking scheme (Random), and decoder masking using Optical Flow (Flow) perform competitively with the lower token budget, and (2) we obtain best results with our proposed Adaptive decoder masking scheme (Adaptive). Note that fine-tuning performance is reported on 1 temporal crop.", "description": "This table compares different decoder masking strategies for short videos (32 frames) on the EK100 dataset.  It evaluates the top-1 action classification performance after fine-tuning MAE pre-trained models using either pixel (RGB) or token (FSQ-MagViT) reconstruction targets.  A consistent token budget of 15% is used for decoder masking. The results show that the proposed adaptive masking strategy outperforms other methods, including those that leverage uniform, random, or optical flow-based masking.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_7_1.jpg", "caption": "Table 2: Decoder masking enables training over long-video (128 frames). We report fine-tuning top-1 action classification performance of long-video MAE pre-trained models using token (FSQ-MagViT) as reconstruction target. Note that 128 frames fine-tuning is performed using random-tube masking with 20% masking and evaluation is reported on 1 temporal crop. Refer to Appendix A.5.3 for the fine-tuning details.", "description": "This table compares the performance of the proposed long-video masked autoencoder (LVMAE) model with different pre-training strategies on two datasets (EK100 and D48).  It shows the top-1 action classification accuracy after fine-tuning models pretrained using either 128 frames or 32 frames. The table highlights that using 128-frame pre-training with the adaptive decoder masking strategy leads to significantly better results compared to the short video counterparts, demonstrating the effectiveness of the LVMAE approach on long-range video understanding.  Note that the fine-tuning stage also uses different frame numbers according to the pre-training setting.", "section": "4.3 Adaptive decoder masking enables pre-training over long videos"}, {"figure_path": "bFrNPlWchg/tables/tables_8_1.jpg", "caption": "Table 3: Comparison to State-of-the-art (SOTA). In these tables we compare to a broad set of approaches, many of which use additional (labeled) data in pre-training or specialized modules.", "description": "This table compares the proposed LVMAE model's performance with other state-of-the-art models on EPIC-Kitchens-100 and Diving48 datasets.  It highlights that many SOTA models utilize additional labeled data or specialized architectures during pre-training, whereas LVMAE achieves competitive results using only unlabeled video data and a standard ViT architecture.  The table showcases the action, verb, and noun classification accuracy of each model, demonstrating LVMAE's strong performance, particularly on verb classification.", "section": "4.4 Comparison with prior state-of-the-art works"}, {"figure_path": "bFrNPlWchg/tables/tables_8_2.jpg", "caption": "Table 3: Comparison to State-of-the-art (SOTA). In these tables we compare to a broad set of approaches, many of which use additional (labeled) data in pre-training or specialized modules.", "description": "This table compares the proposed LVMAE model's performance with other state-of-the-art models on EPIC-Kitchens-100 and Diving48 datasets.  It highlights that many SOTA models leverage additional labeled data or specialized architectures, while LVMAE achieves competitive results using a simpler architecture and video-only pre-training.", "section": "4.4 Comparison with prior state-of-the-art works"}, {"figure_path": "bFrNPlWchg/tables/tables_9_1.jpg", "caption": "Table 1: Decoder masking strategies on short-videos (32 frames) on EK100. We report fine-tuning top-1 action classification performance of MAE pre-trained models using pixel (RGB) or token (FSQ-MagViT) reconstruction targets. For decoder masking, we consistently use 15% as the token budget. We find that (1) compared to no decoder mask (None) with 100% budget, uniform masking scheme (Uniform [14]), random masking scheme (Random), and decoder masking using Optical Flow (Flow) perform competitively with the lower token budget, and (2) we obtain best results with our proposed Adaptive decoder masking scheme (Adaptive). Note that fine-tuning performance is reported on 1 temporal crop.", "description": "This table compares different decoder masking strategies on short videos (32 frames) for the EK100 dataset.  It shows the top-1 action classification accuracy after fine-tuning MAE models pretrained with either pixel (RGB) or token (FSQ-MagViT) reconstruction targets.  A consistent 15% token budget was used for decoder masking.  The results demonstrate that the proposed adaptive decoder masking scheme outperforms other methods, including content-agnostic (uniform, random) and content-informed (optical flow, EVEREST) approaches, even with a significantly reduced token budget.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_9_2.jpg", "caption": "Table 1: Decoder masking strategies on short-videos (32 frames) on EK100. We report fine-tuning top-1 action classification performance of MAE pre-trained models using pixel (RGB) or token (FSQ-MagViT) reconstruction targets. For decoder masking, we consistently use 15% as the token budget. We find that (1) compared to no decoder mask (None) with 100% budget, uniform masking scheme (Uniform [14]), random masking scheme (Random), and decoder masking using Optical Flow (Flow) perform competitively with the lower token budget, and (2) we obtain best results with our proposed Adaptive decoder masking scheme (Adaptive). Note that fine-tuning performance is reported on 1 temporal crop.", "description": "This table compares different decoder masking strategies for short videos (32 frames) on the EK100 dataset.  The experiment uses two reconstruction targets: RGB pixels and FSQ-MagViT tokens. The token budget is consistently set to 15%.  The table shows that the proposed 'Adaptive' masking strategy outperforms other methods, including uniform, random, and optical flow-based masking, even when the latter methods leverage content information.  The results highlight the efficacy of the adaptive masking approach in improving fine-tuning performance on a single temporal crop.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_9_3.jpg", "caption": "Table 4c: Number of frames ablation.", "description": "This table shows the ablation study on the number of frames used for training the model. The experiment uses the adaptive masking strategy and FSQ-MagViT tokens as reconstruction targets. The results show that increasing the number of frames from 16 to 32 to 64 improves the performance significantly, but the improvement diminishes when moving from 64 to 128 frames, suggesting diminishing returns for longer video contexts.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_9_4.jpg", "caption": "Table 2: Decoder masking enables training over long-video (128 frames). We report fine-tuning top-1 action classification performance of long-video MAE pre-trained models using token (FSQ-MagViT) as reconstruction target. Note that 128 frames fine-tuning is performed using random-tube masking with 20% masking and evaluation is reported on 1 temporal crop. Refer to Appendix A.5.3 for the fine-tuning details.", "description": "This table compares the performance of different decoder masking strategies when training a Masked Autoencoder (MAE) model on long videos (128 frames).  It shows the top-1 accuracy achieved on downstream action classification tasks (EK100 and D48) after fine-tuning the model pretrained with different masking schemes (None, Adaptive, etc.).  The table highlights that the adaptive masking strategy, which selects important tokens for reconstruction, enables efficient training on long videos and outperforms other methods.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_9_5.jpg", "caption": "Table 3e: Comparison to SOTA on EPIC-Kitchens-100 Verbs at different video lengths.", "description": "This table compares the performance of the proposed LVMAE model against the state-of-the-art (SOTA) methods on the EPIC-Kitchens-100 Verbs benchmark.  It focuses specifically on the performance across different video lengths (0-4s, 4-8s, 8-16s, 16-32s, >32s) to highlight the model's capability of handling long-range temporal dependencies. The relative difference column indicates the percentage improvement of LVMAE over the Avion model for each length category. This demonstrates the effectiveness of LVMAE in handling longer videos.", "section": "4.4 Comparison with prior state-of-the-art works"}, {"figure_path": "bFrNPlWchg/tables/tables_14_1.jpg", "caption": "Table 5: LFQ vs FSQ. We compare tokenizers trained with different quantization schemes and report their reconstruction quality (PSNR, FVD) on Kinetics600 [37] benchmark and the corresponding MAE model\u2019s EPIC-Kitchens-100 top-1 accuracy.", "description": "This table compares the performance of two different quantization methods (LFQ and FSQ) with varying codebook sizes on the Kinetics600 benchmark.  It shows PSNR (Peak Signal-to-Noise Ratio), FVD (Fr\u00e9chet Video Distance), and top-1 accuracy on the EPIC-Kitchens-100 dataset for MAE models trained using each quantizer. This helps determine which quantizer and codebook size performs best for video masked autoencoder (MAE) model training.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_14_2.jpg", "caption": "Table 6: Something-Something-V2 benchmark. We report top-1 performance of our proposed MAE pre-training with decoder masking & FSQ-MagViT as targets while varying the number of frames.", "description": "This table presents the results of experiments on the Something-Something-V2 dataset, which tests the model's ability to understand actions using different lengths of video clips.  The performance is measured using the top-1 accuracy metric, and the table shows how the performance changes as the number of frames used in the pre-training increases from 16 to 96. The decoder masking strategy and the FSQ-MagViT targets remain constant across different frame lengths.", "section": "4. Experiments"}, {"figure_path": "bFrNPlWchg/tables/tables_15_1.jpg", "caption": "Table 7: SOTA comparison on FineGym288. We report results on the FineGym288 benchmark compared to current state-of-the-art methods.", "description": "This table compares the performance of the proposed LVMAE model against other state-of-the-art models on the FineGym288 benchmark.  FineGym288 is a video classification benchmark focusing on gymnastics, and it tests the ability to categorize multi-second sports action sequences consisting of fine-grained motion. The table shows that LVMAE achieves the highest per-video accuracy, outperforming existing methods.", "section": "4 Experiments"}, {"figure_path": "bFrNPlWchg/tables/tables_15_2.jpg", "caption": "Table 8: Varying decoder budget. We report Diving48 top-1 performance and relative memory usage of our proposed MAE pre-training with decoder masking & FSQ-MagViT as targets.", "description": "This table presents the results of an ablation study on the decoder masking strategy. It shows how varying the decoder token budget affects the top-1 accuracy on the Diving48 dataset and the relative memory usage compared to the baseline (15% token budget). The results indicate that a 15% token budget yields the best performance, achieving a top-1 accuracy of 89.7 while maintaining reasonable memory consumption.", "section": "4.2 Adaptive decoder masking strategy outperforms on short videos"}, {"figure_path": "bFrNPlWchg/tables/tables_16_1.jpg", "caption": "Table 9: Model size vs frames. We report top-1 performance of our proposed MAE pre-training with decoder masking & FSQ-MagViT with different model sizes and maximum frames for that model size given a fixed memory budget.", "description": "This table shows the results of an experiment designed to evaluate the impact of model size on the maximum number of frames that can be processed while maintaining a fixed memory budget.  Three different model sizes (Small, Base, Large) were used, each with varying computational complexity (GFLOPs). The table demonstrates that larger models achieve higher accuracy but are limited to processing fewer frames due to memory constraints. The experiment was performed on the EPIC-Kitchens-100 dataset.", "section": "4.1 Decoder is the most memory intensive stage in long-video MAE"}, {"figure_path": "bFrNPlWchg/tables/tables_16_2.jpg", "caption": "Table 10: Adaptive Tokenizer Model Card", "description": "This table details the hyperparameters and architecture used for training the adaptive tokenizer model.  It includes information about the number of frames, spatial resolution, model size, channel multipliers, latent shape, vocabulary size, embedding dimension, top-k selection, batch size, learning rate schedule, optimizer, loss functions, and other relevant training details. This tokenizer plays a crucial role in the adaptive masking strategy of the proposed method.", "section": "3.3 Adaptive Finite Scalar Quantized VAE for Saliency and Reconstruction Targets"}, {"figure_path": "bFrNPlWchg/tables/tables_17_1.jpg", "caption": "Table 11: Model Card with detailed model architecture and training setups for Base size experiments on Diving48 and EPIC-Kitchens.", "description": "This table details the model architecture and training hyperparameters used in the experiments. It shows the configuration for both pre-training and fine-tuning stages, specifying the model type (ViT-B), number of layers, heads, MLP dimension, input and output shapes, optimizer, learning rate schedule, augmentation techniques, batch size, label smoothing, and dropout rate for both the encoder and decoder.  The table also indicates the type of target used for the pre-training stage (pixels or tokens).  The differences in hyperparameters between the pre-training and fine-tuning phases highlight the adaptation strategy employed during the downstream task.", "section": "A.5 Implementation details"}]