[{"figure_path": "QzvWyggrYB/tables/tables_3_1.jpg", "caption": "Table 3: ECE and AUROC on livecodebench_generation_lite (LeetCode easy subset). ECE is shown after temperature scaling on a small hold-out set of the original dataset mixture Appendix C.2. Acc is task accuracy (proportion of coding solutions that are correct). Supervised training (LoRA + Prompt) seems to always improve selective prediction, although supervised training only heavily improves calibration for Mistral-7B and in fact slightly degrades calibration for the two other models.", "description": "This table presents the results of evaluating different methods for estimating uncertainty on a subset of the LeetCode coding challenge.  It compares the Expected Calibration Error (ECE) and Area Under the ROC Curve (AUROC) of three language models (LLaMa-2-7B, Mistral-7B, and Mistral-7B-Instruct) using both zero-shot methods and methods fine-tuned using LoRA. The table shows that fine-tuning generally improves selective prediction (AUROC), but the impact on calibration (ECE) is model-dependent.", "section": "5 How Should We Use Labeled Examples?"}, {"figure_path": "QzvWyggrYB/tables/tables_5_1.jpg", "caption": "Table 1: Regularization improves calibration. Numbers show the mean over six base models models. See Appendix C.1 for discussion.", "description": "This table presents the results of an ablation study comparing the calibration and area under the receiver operating characteristic curve (AUROC) of a model with and without Kullback-Leibler (KL) divergence regularization.  The results show that adding KL divergence regularization significantly improves calibration (lower ECE) while only slightly improving AUROC. The table also notes that the data represents the mean of results over six different base models and refers the reader to Appendix C.1 for further discussion.", "section": "Training and regularization"}, {"figure_path": "QzvWyggrYB/tables/tables_15_1.jpg", "caption": "Table 3: ECE and AUROC on livecodebench_generation_lite (LeetCode easy subset). ECE is shown after temperature scaling on a small hold-out set of the original dataset mixture Appendix C.2. Acc is task accuracy (proportion of coding solutions that are correct). Supervised training (LoRA + Prompt) seems to always improve selective prediction, although supervised training only heavily improves calibration for Mistral-7B and in fact slightly degrades calibration for the two other models.", "description": "This table presents the results of evaluating the performance of different uncertainty estimation methods on a subset of the LiveCodeBench dataset, specifically focusing on LeetCode easy questions.  It compares the Expected Calibration Error (ECE), Area Under the Receiver Operating Characteristic curve (AUROC), and accuracy of three different models (LLaMa-2-7B, Mistral-7B, and Mistral-7B-Instruct) using two approaches: Zero-Shot and LoRA + Prompt.  The results demonstrate the impact of fine-tuning on the calibration and predictive power of uncertainty estimation, especially highlighting the varying effects across different models.", "section": "Experiments with open-source models"}, {"figure_path": "QzvWyggrYB/tables/tables_16_1.jpg", "caption": "Table 2: Absolute differences in accuracy % for the different grading methods vs human estimated accuracy. A lower value corresponds to an accuracy estimate closer to the human estimate.", "description": "This table compares the accuracy of three different methods for grading the correctness of large language model (LLM) answers against human-provided grades. The methods compared are:  (1) Substring Match (checking if the correct answer is a substring of the generated answer), (2) GPT 3.5 Turbo (using a large language model to judge the correctness), and (3) GPT 4 (similar to GPT 3.5 Turbo, but using a more powerful model). The table shows the absolute difference between the accuracy of each method and the human accuracy across different subsets of the MMLU dataset (World Religions, Philosophy, Anatomy, Chemistry, and Math).  A lower percentage indicates closer agreement between the method's grading and human grading, showing better performance by the LLM-based methods compared to the simple substring-matching approach.", "section": "A Evaluation Methods"}, {"figure_path": "QzvWyggrYB/tables/tables_20_1.jpg", "caption": "Table 3: ECE and AUROC on livecodebench_generation_lite (LeetCode easy subset). ECE is shown after temperature scaling on a small hold-out set of the original dataset mixture Appendix C.2. Acc is task accuracy (proportion of coding solutions that are correct). Supervised training (LoRA + Prompt) seems to always improve selective prediction, although supervised training only heavily improves calibration for Mistral-7B and in fact slightly degrades calibration for the two other models.", "description": "This table shows the performance of different methods for estimating uncertainty on the livecodebench_generation_lite dataset, which is a subset of LeetCode easy questions.  The table compares the expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) for two methods: Zero-Shot Classifier and Lora + Prompt.  Accuracy (Acc) is also provided.  The results show that using Lora + Prompt consistently improves selective prediction, but its effect on calibration varies depending on the model.", "section": "5 How Should We Use Labeled Examples?"}, {"figure_path": "QzvWyggrYB/tables/tables_23_1.jpg", "caption": "Table 3: ECE and AUROC on livecodebench_generation_lite (LeetCode easy subset). ECE is shown after temperature scaling on a small hold-out set of the original dataset mixture Appendix C.2. Acc is task accuracy (proportion of coding solutions that are correct). Supervised training (LoRA + Prompt) seems to always improve selective prediction, although supervised training only heavily improves calibration for Mistral-7B and in fact slightly degrades calibration for the two other models.", "description": "This table presents the performance of different methods for estimating uncertainty in a coding task (LeetCode easy subset).  It shows expected calibration error (ECE), Area Under the Receiver Operating Characteristic curve (AUROC), and accuracy (Acc) for three language models: LLaMA-2 7B, Mistral 7B, and Mistral 7B Instruct. Two approaches are compared: zero-shot classification and fine-tuning using LoRA and prompts. The results indicate the impact of supervised training on calibration and selective prediction in this out-of-distribution task.", "section": "Experiments with open-source models"}, {"figure_path": "QzvWyggrYB/tables/tables_25_1.jpg", "caption": "Table 3: ECE and AUROC on livecodebench_generation_lite (LeetCode easy subset). ECE is shown after temperature scaling on a small hold-out set of the original dataset mixture Appendix C.2. Acc is task accuracy (proportion of coding solutions that are correct). Supervised training (LoRA + Prompt) seems to always improve selective prediction, although supervised training only heavily improves calibration for Mistral-7B and in fact slightly degrades calibration for the two other models.", "description": "This table presents the results of evaluating the performance of different uncertainty estimation methods on a subset of LeetCode easy questions. The methods evaluated include zero-shot classifiers, probes, LoRA, and LoRA+Prompt. The metrics used to evaluate performance are expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC). The table also reports the task accuracy for each method. The results show that supervised training (LoRA + Prompt) generally improves selective prediction but has mixed effects on calibration, depending on the model used.", "section": "5 How Should We Use Labeled Examples?"}]