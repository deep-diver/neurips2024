[{"figure_path": "QzvWyggrYB/figures/figures_0_1.jpg", "caption": "Figure 1: Large language models struggle to assign reliable confidence estimates to their generations. We study the properties of uncertainty calibration in language models, and propose fine-tuning for better uncertainty estimates using a graded dataset of generations from the model. We evaluate our methods on a new open-ended variant of MMLU [18]. We show that fine-tuning improves expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) compared to commonly-used baselines. Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.", "description": "This figure illustrates the challenges of obtaining reliable uncertainty estimates from large language models (LLMs). It introduces a method for fine-tuning LLMs using a graded dataset of generations to improve their calibration and AUROC.  The results show that fine-tuning outperforms baseline methods for uncertainty estimation on open-ended questions from the MMLU benchmark, demonstrating improved calibration and better discrimination between correct and incorrect answers.", "section": "1 Introduction"}, {"figure_path": "QzvWyggrYB/figures/figures_1_1.jpg", "caption": "Figure 1: Large language models struggle to assign reliable confidence estimates to their generations. We study the properties of uncertainty calibration in language models, and propose fine-tuning for better uncertainty estimates using a graded dataset of generations from the model. We evaluate our methods on a new open-ended variant of MMLU [18]. We show that fine-tuning improves expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) compared to commonly-used baselines. Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.", "description": "This figure illustrates the challenge of large language models (LLMs) in accurately estimating their uncertainty.  The authors propose using a graded dataset of LLM-generated answers to fine-tune the model, improving the accuracy of uncertainty estimation. The evaluation uses a new open-ended version of the MMLU benchmark, demonstrating that fine-tuning leads to better calibration (lower ECE) and discrimination (higher AUROC) compared to existing methods.", "section": "1 Introduction"}, {"figure_path": "QzvWyggrYB/figures/figures_4_1.jpg", "caption": "Figure 2: (Left) We compare common uncertainty estimates for multiple-choice questions (max softmax probability) and open-ended generation (perplexity). While maximum softmax probability performs well and improves with the ability of the base model, perplexity does not follow the same pattern. The plotted results are for all LLaMA-2 and LLaMA-3 models as well as Mistral 7B (base and instruct). (Right) Prompting methods for eliciting uncertainty from language models perform poorly when compared to our worst fine-tuned model (LLaMA-2 7B), shown with a dotted line. ECE doesn't appear to improve with the abilities of the underlying model, and while AUROC does show small improvements with large improvements in accuracy, the gap between zero-shot methods and fine-tuning for uncertainties remains large. Shading indicates a 95% bootstrapped confidence interval on the regression fit.", "description": "The figure compares different methods for estimating uncertainty in large language models (LLMs).  The left panel shows that a likelihood-based approach works well for multiple-choice questions but poorly for open-ended generation. The right panel demonstrates that fine-tuning significantly improves the accuracy and calibration of uncertainty estimates compared to simple prompting methods.  Different LLMs are evaluated and the results show that the benefits of fine-tuning are consistent across them.", "section": "Do We Get Good Uncertainties Out-of-the-Box?"}, {"figure_path": "QzvWyggrYB/figures/figures_6_1.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "The figure compares the performance of different uncertainty estimation methods on multiple-choice and open-ended MMLU tasks.  The left panel shows that supervised fine-tuning methods (Probe, LORA, LORA + Prompt) generally improve calibration and AUROC compared to baseline methods (Zero-Shot, Sampling).  The right panel demonstrates that even a small number of labeled examples (around 1000) significantly improves the uncertainty estimation of large language models, exceeding the performance of unsupervised methods.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_7_1.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "This figure shows the comparison of different methods for estimating uncertainty in large language models (LLMs) for both multiple-choice and open-ended question answering tasks.  The left panel demonstrates the effectiveness of fine-tuning with various parameterizations in improving both Expected Calibration Error (ECE) and Area Under the Receiver Operating Characteristic curve (AUROC).  The right panel illustrates how the amount of labeled data used in fine-tuning impacts performance on open-ended tasks, suggesting that 1000 examples are sufficient to outperform baseline methods.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_8_1.jpg", "caption": "Figure 5: (Left) We ablate the correspondence between questions and answers by training LORA + Prompt on a dataset with correctness labels from the model's generations but with the actual generations swapped with incorrect answers. In this case, the only relationships that can be extracted by the model are between the correctness labels and the questions. The model trained on incorrect answers generalizes surprisingly well but is much worse than a model trained on the original answers. Error bars show two standard deviations over three instruction-tuned models. (Center) We test how well models can learn to predict the correctness of a different model (in terms of AUROC), and we find that mistral models are often better at estimating the correctness of LLaMA models than LLaMA can on their own generations. (Right) We show that generic sentence embeddings can also perform on par with frozen language model representations (MMLU-OE), but training through a model is much better. sSBERT and OAIEmb refer to training a classifier on top of sBERT [44] or OpenAI sentence embeddings. Error bars show two standard deviations over tasks in MMLU.", "description": "This figure presents ablation studies on the model's ability to learn uncertainty. The left panel shows that training on incorrect answers leads to poorer performance compared to training on correct answers, suggesting that the model learns more than just simple patterns in the question and answer pairs. The central panel demonstrates that models can generalize uncertainty estimation to other models, with Mistral models performing better at estimating uncertainty for LLaMA models than LLaMA models can for themselves. The right panel shows that simple sentence embeddings can achieve comparable performance to frozen language models in uncertainty estimation, but training through the model's features significantly enhances the performance.", "section": "6 When and Why Do These Estimates Generalize?"}, {"figure_path": "QzvWyggrYB/figures/figures_9_1.jpg", "caption": "Figure 6: We compare the distribution of LLM confidence (for Mistral 7B Instruct) on its answers, and whether the users (N = 20 per variant) agree with the answer generated by the model or not. (a) For the zero-shot prompt, we find that the model provides little signal since most mass is similarly clustered. However, (b) improving the calibration of the model reveals an increased reliance on the LLM for more confident answers, and decreased reliance for less confident answers. Evidently, the users are sensitive to calibrated confidence scores. (c) For reference, we verify that uniformly confidence scores do not provide meaningful signal, rendering users unable to modulate their decision to rely on the LLM. All variants are compared at approximately the same average participant accuracy.", "description": "This figure shows the distribution of model confidence scores and whether users agreed with the model's prediction for three different scenarios: zero-shot prompting, fine-tuning with LoRA and prompting, and random confidence scores. The results show that calibrated confidence scores (LoRA + Prompt) influence user reliance on the model's predictions, unlike zero-shot and random confidence.", "section": "7 Does Calibrated Confidence Improve Collaboration with AI Assistants?"}, {"figure_path": "QzvWyggrYB/figures/figures_19_1.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "This figure compares the performance of different methods for estimating uncertainty in LLMs on both multiple-choice and open-ended question-answering tasks. The left panel shows that fine-tuning significantly improves both calibration and the ability to discriminate between correct and incorrect answers. The right panel illustrates that using as few as 1000 labeled examples is sufficient for good performance, and increasing the number of examples beyond that yields only marginal improvement.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_24_1.jpg", "caption": "Figure 2: (Left) We compare common uncertainty estimates for multiple-choice questions (max softmax probability) and open-ended generation (perplexity). While maximum softmax probability performs well and improves with the ability of the base model, perplexity does not follow the same pattern. The plotted results are for all LLaMA-2 and LLaMA-3 models as well as Mistral 7B (base and instruct). (Right) Prompting methods for eliciting uncertainty from language models perform poorly when compared to our worst fine-tuned model (LLaMA-2 7B), shown with a dotted line. ECE doesn't appear to improve with the abilities of the underlying model, and while AUROC does show small improvements with large improvements in accuracy, the gap between zero-shot methods and fine-tuning for uncertainties remains large. Shading indicates a 95% bootstrapped confidence interval on the regression fit.", "description": "This figure compares different methods for estimating uncertainty in large language models (LLMs). The left panel shows that using maximum softmax probability for multiple-choice questions is a good indicator of uncertainty and improves with better models, unlike perplexity for open-ended generation.  The right panel demonstrates that simple prompting methods are far less effective at estimating uncertainty compared to fine-tuning a model on a small dataset of graded examples.", "section": "Do We Get Good Uncertainties Out-of-the-Box?"}, {"figure_path": "QzvWyggrYB/figures/figures_26_1.jpg", "caption": "Figure 2: (Left) We compare common uncertainty estimates for multiple-choice questions (max softmax probability) and open-ended generation (perplexity). While maximum softmax probability performs well and improves with the ability of the base model, perplexity does not follow the same pattern. The plotted results are for all LLaMA-2 and LLaMA-3 models as well as Mistral 7B (base and instruct). (Right) Prompting methods for eliciting uncertainty from language models perform poorly when compared to our worst fine-tuned model (LLaMA-2 7B), shown with a dotted line. ECE doesn't appear to improve with the abilities of the underlying model, and while AUROC does show small improvements with large improvements in accuracy, the gap between zero-shot methods and fine-tuning for uncertainties remains large. Shading indicates a 95% bootstrapped confidence interval on the regression fit.", "description": "The left panel compares uncertainty estimation methods for multiple choice and open-ended questions.  It shows that maximum softmax probability is a good indicator of uncertainty for multiple choice questions, while perplexity is not suitable for open-ended generation. The right panel compares prompting methods against a fine-tuned model.  It highlights that fine-tuning significantly improves uncertainty estimation, outperforming zero-shot prompting methods in both expected calibration error (ECE) and area under the ROC curve (AUROC).", "section": "Do We Get Good Uncertainties Out-of-the-Box?"}, {"figure_path": "QzvWyggrYB/figures/figures_27_1.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "This figure compares the performance of different uncertainty estimation methods on both multiple-choice and open-ended MMLU datasets. The left panel shows that supervised fine-tuning methods generally improve both calibration (ECE) and discrimination (AUROC), with LORA+Prompt outperforming other methods.  The right panel demonstrates that fine-tuning performance improves with more labeled data, but that using 1000 examples already yields substantial gains.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_28_1.jpg", "caption": "Figure 4: (Left) We compare the composition of the fine-tuning dataset with MMLU. Notably, although the training dataset contains close to zero examples from social sciences, uncertainty estimates from the model perform similarly across categories. (Center) Testing the generalization of supervised methods by taking models trained on one setting (MCQA or OE) and evaluating them on the other setting. The MCQA or OE labels denote the evaluation setting, with the method labels indicate whether the model was trained on the same or different setting. Fine-tuning through the model's features (LORA + Prompt) performs almost as well in transfer as on in-distribution data. Zero-Shot Classifier involves no supervised learning except a temperature-scale step and is a useful reference point. Error bars show two standard deviations over six fine-tuned models. (Right) Fine-tuning leads to lower confidence on unanswerable questions, taken from the SelfAware dataset [61]. Assigning low confidence to unanswerable questions allows the model to opt out of responding.", "description": "This figure demonstrates the generalization of uncertainty estimation methods across different distribution shifts.  The left panel compares the composition of the fine-tuning dataset with the MMLU benchmark, showing the robustness of the method to differences in subject matter distribution. The center panel evaluates generalization across question formats (multiple-choice vs. open-ended), highlighting the superior performance of LORA+Prompt. The right panel assesses performance on unanswerable questions, illustrating the ability of the fine-tuned model to express appropriate uncertainty.", "section": "When and Why Do These Estimates Generalize?"}, {"figure_path": "QzvWyggrYB/figures/figures_28_2.jpg", "caption": "Figure 6: We compare the distribution of LLM confidence (for Mistral 7B Instruct) on its answers, and whether the users (N = 20 per variant) agree with the answer generated by the model or not. (a) For the zero-shot prompt, we find that the model provides little signal since most mass is similarly clustered. However, (b) improving the calibration of the model reveals an increased reliance on the LLM for more confident answers, and decreased reliance for less confident answers. Evidently, the users are sensitive to calibrated confidence scores. (c) For reference, we verify that uniformly confidence scores do not provide meaningful signal, rendering users unable to modulate their decision to rely on the LLM. All variants are compared at approximately the same average participant accuracy.", "description": "This figure shows the density plots of the model's reported confidence and whether the user chose to agree with the model's prediction across three different conditions: zero-shot prompt, calibrated confidence (LORA+Prompt), and random confidence.  The results demonstrate that users are sensitive to calibrated confidence scores and tend to agree with high-confidence predictions from a calibrated model more often, while showing indifference to random confidence scores.", "section": "7 Does Calibrated Confidence Improve Collaboration with AI Assistants?"}, {"figure_path": "QzvWyggrYB/figures/figures_28_3.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "This figure compares the performance of different uncertainty estimation methods on multiple-choice and open-ended MMLU datasets. The left panel shows that supervised training methods (Probe, LORA, LORA+Prompt) generally improve both calibration (ECE) and the ability to discriminate between correct and incorrect answers (AUROC). The right panel demonstrates that even a small number (1000) of labeled examples is sufficient to improve uncertainty estimation performance, with diminishing returns for larger datasets.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_29_1.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "This figure compares the performance of different uncertainty estimation methods (zero-shot, probing, LoRA, LoRA+Prompt) on both multiple-choice and open-ended MMLU datasets. The left panel shows the expected calibration error (ECE) and area under the ROC curve (AUROC) for each method, highlighting the improvements achieved through fine-tuning, particularly with the LoRA+Prompt approach.  The right panel illustrates how the performance of fine-tuning changes as the size of the labeled training dataset increases, revealing that even 1000 examples are effective and that the marginal gain from additional data is relatively small after 5000 examples.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_29_2.jpg", "caption": "Figure 6: We compare the distribution of LLM confidence (for Mistral 7B Instruct) on its answers, and whether the users (N = 20 per variant) agree with the answer generated by the model or not. (a) For the zero-shot prompt, we find that the model provides little signal since most mass is similarly clustered. However, (b) improving the calibration of the model reveals an increased reliance on the LLM for more confident answers, and decreased reliance for less confident answers. Evidently, the users are sensitive to calibrated confidence scores. (c) For reference, we verify that uniformly confidence scores do not provide meaningful signal, rendering users unable to modulate their decision to rely on the LLM. All variants are compared at approximately the same average participant accuracy.", "description": "The figure shows the distribution of LLM confidence scores and whether users agree with the model's prediction, comparing zero-shot prompting, calibrated and random confidence scores.  It demonstrates that users are sensitive to calibrated confidence scores, adjusting their reliance on the LLM based on the reliability of the confidence estimates.", "section": "7 Does Calibrated Confidence Improve Collaboration with AI Assistants?"}, {"figure_path": "QzvWyggrYB/figures/figures_30_1.jpg", "caption": "Figure 1: Large language models struggle to assign reliable confidence estimates to their generations. We study the properties of uncertainty calibration in language models, and propose fine-tuning for better uncertainty estimates using a graded dataset of generations from the model. We evaluate our methods on a new open-ended variant of MMLU [18]. We show that fine-tuning improves expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) compared to commonly-used baselines. Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.", "description": "This figure shows that large language models (LLMs) have difficulty assigning reliable confidence estimates to their generated text.  The authors propose a method of fine-tuning the LLMs on a graded dataset of correct and incorrect answers to improve calibration and uncertainty estimation. They evaluate their method on a modified version of the MMLU benchmark and demonstrate improved performance (lower expected calibration error, higher AUROC) compared to existing approaches. The error bars represent the standard deviation across three different LLM models and their chat variants.", "section": "1 Introduction"}, {"figure_path": "QzvWyggrYB/figures/figures_32_1.jpg", "caption": "Figure 3: (Left) ECE and AUROC on both multiple choice (MC) and open-ended (OE) MMLU. ECE is shown after temperature scaling on a small hold-out set. Supervised training (Probe, LORA, LORA + Prompt) tends to improve calibration and selective prediction. Probing on its own (Probe) performs worse than training through the features with a language prompt (LORA + Prompt), especially in an open-ended setting. Error bars show two standard deviations over six base models. Extended results in Appendix D. (Right) Effect of varying number of labeled datapoints on OE MMLU. In the most extreme case, we train on only 200 examples. Overall, performance increases in proportion with the available labeled data, but 1000 points is almost as valuable as 20,000 points. Dotted lines indicate the performance of the classifier and sampling baselines averaged over the three models considered. Shaded regions show one standard deviation over subsets of MMLU.", "description": "The figure presents results of calibration and discrimination for different methods of uncertainty estimation on multiple choice and open-ended MMLU question answering datasets. The left panel displays the expected calibration error (ECE) and area under the ROC curve (AUROC) for several methods: zero-shot classifier, sampling, probe, LORA, and LORA+Prompt. The right panel compares the performance of supervised learning with different amounts of training data, showing that 1000 examples achieve nearly the same performance as 20000 examples.", "section": "Fine-tuning results"}, {"figure_path": "QzvWyggrYB/figures/figures_33_1.jpg", "caption": "Figure 1: Large language models struggle to assign reliable confidence estimates to their generations. We study the properties of uncertainty calibration in language models, and propose fine-tuning for better uncertainty estimates using a graded dataset of generations from the model. We evaluate our methods on a new open-ended variant of MMLU [18]. We show that fine-tuning improves expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) compared to commonly-used baselines. Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.", "description": "The figure illustrates the challenge of assigning reliable confidence estimates to Large Language Model (LLM) generations.  It introduces a method to improve these estimates by fine-tuning the model on a dataset of graded generations, comparing the performance against zero-shot methods and highlighting the improvement in both Expected Calibration Error (ECE) and Area Under the Receiver Operating Characteristic curve (AUROC).", "section": "1 Introduction"}]