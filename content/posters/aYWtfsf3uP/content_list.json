[{"type": "text", "text": "Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Miao Lu1 \\* Han Zhong2 \\* Tong Zhang3 Jose Blanchetl ", "page_idx": 0}, {"type": "text", "text": "'Department of Management Science and Engineering, Stanford University 2Center for Data Science, Peking University 3Department of Computer Science, University of linois Urbana-Champaign ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributionally robust reinforcement learning (DRRL), often framed as a robust Markov decision process (RMDP), seeks to find a robust policy that achieves good performance under the worst-case scenario among all environments within a prespecified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing the distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation distance robust set, postulating that the minimal value of the optimal robust value function is zero. Such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and we present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for sample-efficient algorithms with sharp sample complexity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) serves as a framework for addressing complex decision-making problems through iterative interactions with environments. The advancements in deep reinforcement learning have enabled the successful application of the general RL framework across various domains, including mastering strategic games, such as Go (Silver et al., 2017), robotics (Kober et al., 2013), and tuning large language models (LLMs; Ouyang et al. (2022)). The critical factors contributing to these successes encompass not only the potency of deep neural networks and modern deep RL algorithms but also the availability of substantial training data. However, there are scenarios, such as healthcare (Wang et al., 2018) and autonomous driving (Kiran et al., 2021), among others, where collecting data in the target domain is challenging, costly, or even unfeasible. ", "page_idx": 0}, {"type": "text", "text": "In such cases, the sim-to-real transfer (Kober et al., 2013; Sadeghi and Levine, 2016; Peng et al., 2018; Zhao et al., 2020) becomes a remedy - a process in which the RL agents are trained in some simulated environment and subsequently deployed in real-world settings. Nevertheless, the training environment may differ from the real-world environment. Such a discrepancy, also known as the simto-real gap, will typically result in suboptimal performance of RL agents in real-world applications. One promising strategy to control the impact in performance degradation due to the sim-to-real gap is robust RL (Iyengar, 2005; Pinto et al., 2017; Hu et al., 2022), which aims to learn policies exhibiting strong (i.e. robust) performance under environmental deviations from the training environment, effectively hedging the epistemological uncertainty arising from the differences between the training environment and the unknown testing environments. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A robust RL problem is often formulated within a robust Markov decision process (RMDP) framework, with various types of robust sets characterizing different environmental perturbations. In this robust RL context, prior works have developed algorithms with provable sample complexity guarantees. However, these algorithms typically rely on either a generative model ( Yang et al., 2022; Panaganti and Kalathil, 2022; Xu et al., 2023; Shi et al., 2023) or offline data with good coverage of deployment environments (Zhou et al., 2021b; Panaganti et al., 2022; Shi and Chi, 2022; Ma et al., 2022; Blanchet et al., 2023). Notably, the current literature does not explicitly address the exploration problem, which stands as one of the fundamental challenges in reinforcement learning through trial-and-error (Sutton and Barto, 2018). Meanwhile, the empirical success of robust RL methods (Pinto et al., 2017; Kuang et al., 2022; Moos et al., 2022) typically relies on reinforcement learning through interactive data collection in the training environment, where the agent iteratively and actively interacts with the environment, collecting data, optimizing and robustifying its policy. Given that all the existing literature on robust RL theory relies on a generative model or pre-collected data, it is natural to ask: ", "page_idx": 1}, {"type": "text", "text": "Can we design a provably sample-effcient robust RL algorithm that relies on interactivedatacollectioninthetrainingenvironment? ", "page_idx": 1}, {"type": "text", "text": "Answering the above question faces a fundamental challenge, namely, that during the interactive data collection process, the learner no longer has the oracle control over the training data distributions that are induced by the policy learned through the interaction process. In particular, it could be the case that certain data patterns that are crucial for the policy to be robust across all testing environments are not accessible through interactive data collection, even through a sophisticated design of an exploration mechanism during the interaction process. For example, specific states may not be accessible within the training environment dynamics but could be reached in the testing environment dynamics. ", "page_idx": 1}, {"type": "text", "text": "In contrast, previous work has demonstrated that robust RL through a generative model or a precollected offline dataset with good coverage does not face such difficulty. In the generative model setup, fortunately, the learner can directly query any state-action pair and obtain the sampled next state from the generator. Intuitively, once the states that could appear in the testing environment trajectory are queried enough times, it is possible to guarantee the performance of the learned policy in testing environments. The situation is similar if one has a pre-collected ofline dataset that possesses good coverage of the testing environment. This motivates us to take the initial steps towards answering the above questions regarding robust RLwith interactive data collection. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we study robust RL in a finite-horizon RMDP with an ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular total-variation distance (TV) robust set (see Assumption 2.1 and Definition 2.4) through interactive data collection. We give both a fundamental hardness result in the general case and a sample-efficient algorithm within tractable settings. More specifically, our contributions are three folds. ", "page_idx": 1}, {"type": "text", "text": "Fundamental hardness. We construct a class of hard-to-learn RMDPs (see Example 3.1) and demonstrate that any learning algorithm inevitably incurs an $\\Omega(\\rho\\cdot H K)$ -online regret (Theorem 3.2) under at least one RMDP instance. Here, $\\rho$ signifies the radius of the TV robust uncertainty set, $H$ is the horizon, and $K$ is the number of interactive episodes. This linear regret lower bound underscores the impossibility of sample-efficient robust RL via interactive data collection in general. ", "page_idx": 1}, {"type": "text", "text": "Identifying a tractable case. Upon close examination of the challenging instance, we recognize that the primary obstacle to achieving sample-efficient learning lies in the curse of support shift, i.e., the disjointedness of distributional support between the training environment and the testing environments. In a broader sense, the curse of support shift also refers to the situation when the states appearing in testing environments are extremely hard to arrive in the training environment. ", "page_idx": 1}, {"type": "text", "text": "Table 1: Comparison between OPROVI-TV and prior results on RMDP with $s\\times A$ -rectangular TV robust sets under various settings (generative model/offline dataset/interactive data collection). For the infinite horizon $\\gamma\\cdot$ discounted RMDPs, we denote $H_{\\gamma}:=(1-\\gamma)^{-1}$ as the effective horizon length. In the offine setting, $\\mathcal{C^{\\star}}_{\\mathrm{rob}}$ and $\\mathcal{C}_{\\mathrm{full}}$ represent the robust partial coverage coeffcient and full coverage coefficient, respectively. In the general case, our lower bound reads intractable, meaning that there exist hard instances where it is impossible to learn the nearly optimal robust policy via a finite number of interactive samples. ", "page_idx": 2}, {"type": "table", "img_path": "aYWtfsf3uP/tmp/cf83bb3dc7038609e3477817d4bff425b72808be03ab10d0338f3b59f73dda4e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "To rule out these pathological instances, we propose the vanishing minimal value assumption (Assumption 4.1), positing that the optimal robust value function reaches zero at a specific state. Such an assumption naturally applies to the sparse reward RL paradigm and offers a broader scope compared to the \u201cfail-state\u201d assumption utilized in prior studies on offline RMDP with function approximation (Panaganti et al., 2022). For a comprehensive discussion on this comparison, please see Remark B.3. On the theoretical front, we establish that the vanishing minimal value assumption effectively mitigates the support shift issues between the training and the testing environments (Proposition 4.2), rendering robust RL with interactive data collection feasible for RMDPs with TV robust sets. ", "page_idx": 2}, {"type": "text", "text": "Efficient algorithm with sharp sample complexity. Under the vanishing minimal value assumption, we develop an algorithm named OPtimistic RObust Value Iteration for TV Robust Set (OPROVI-TV, Algorithm 1), that is capable of finding an $\\varepsilon$ -optimal robust policy with a total number of ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{\\tilde{O}}\\big(\\operatorname*{min}\\{H,\\rho^{-1}\\}\\cdot H^{2}S A/\\varepsilon^{2}\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "interactive samples (Theorem 4.3). Here $S$ and $A$ denote the number of states and actions, $\\rho$ represents the radius of the TV robust set, and $H$ is the horizon length of each episode. To our best knowledge, this is the first provably sample-efficient algorithm for robust RL with interactive data collection. ", "page_idx": 2}, {"type": "text", "text": "According to (1.1), the sample complexity of finding an $\\varepsilon$ -optimal robust policy decreases as the radius $\\rho$ of the robust set increases. When the radius $\\rho=0$ , an RMDP reduces to a standard MDP, and the sample complexity (1.1) recovers the minimax-optimal sample complexity for online RL in standard MDPs up to logarithm factors, i.e., $\\widetilde{\\mathcal{O}}(H^{3}S A/\\bar{\\varepsilon^{2}})$ ", "page_idx": 2}, {"type": "text", "text": "In the end, we further extend our algorithm and theory to a new type of RMDPs, ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular discounted RMDP equipped with robust sets consisting of transition probabilities with bounded ratio to the nominal kernel (See Appendix B.4.2). This newly identified class of RMDPs naturally does not suffer from the support shift issue. It is equivalent to the $s\\times A$ -rectangular RMDP with TV robust set and vanishing minimal value assumption in an appropriate sense due to Proposition 4.2. Consequently, by a clever usage of Algorithm 1, we can also solve this new model sample-efficiently, as shown in Corollary B.5. Such a result echoes our intuition on the curse of support shift. ", "page_idx": 2}, {"type": "text", "text": "Comparison to related works. Due to the space limit, we only compare with the most related works through Table 1. A detailed discussion of related works is in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. For a set $\\mathcal{X}$ we denote $\\Delta(\\mathcal{X})$ as the set of probability distributions on $\\mathcal{X}$ For a distribution $p\\in\\Delta(\\mathcal{X})$ , we define the shorthand for expectation and variance as $\\mathbb{E}_{p(\\cdot)}[f]:=\\mathbb{E}_{X\\sim p(\\cdot)}[f(X)]$ and $\\mathbb{V}_{p(\\cdot)}[f]\\,=\\,\\mathbb{E}_{p(\\cdot)}[f^{2}]\\,-\\,(\\mathbb{E}_{p(\\cdot)}[f])^{2}$ . Given any set $\\mathcal{Q}\\subseteq\\Delta(\\mathcal{X})$ , we define the robust expectation ", "page_idx": 2}, {"type": "text", "text": "operator as $\\mathbb{E}_{\\mathcal{Q}}[f]:=\\operatorname*{inf}_{p(\\cdot)\\in\\mathcal{Q}}\\mathbb{E}_{X\\sim p(\\cdot)}[f(X)]$ . For any $x,a\\in\\mathbb{R}$ , we denote $(x)_{+}=\\operatorname*{max}\\{x,0\\}$ and $x\\vee a=\\operatorname*{max}\\{x,a\\}$ . We use $O(\\cdot)$ to hide absolute constant factors and use $\\widetilde O$ tofurther hide logarithmic factors. For a positive integer $H\\in\\mathbb{N}_{+}$ ,wedenote theset $\\{1,2,\\ldots,H\\}$ by $[H]$ ", "page_idx": 3}, {"type": "text", "text": "2.1 Robust Markov Decision Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first introduce our underlying model for doing robust RL, the episodic robust Markov decision process (RMDP), denoted by a tuple $(S,{\\mathcal{A}},H,P^{\\star},R,\\Phi)$ . Here the set $\\boldsymbol{S}$ is the state space and the set $\\boldsymbol{\\mathcal{A}}$ $P^{\\star}=\\{P_{h}^{\\star}\\}_{h=1}^{H}$ $H$ $P_{h}^{\\star}:S\\times A\\mapsto\\Delta(S)$ The set $R=\\{R_{h}\\}_{h=1}^{H}$ is the collection of reward functions where $R_{h}:S\\times A\\mapsto[0,1]$ For simplicity, we denote $\\mathcal{P}\\overset{\\leftarrow}{=}\\{\\boldsymbol{P}(\\cdot|\\cdot,\\cdot):\\mathcal{S}\\times\\mathcal{A}\\mapsto\\Delta(\\mathcal{S})\\}$ as the space of all possible transition kernels, and we denote $S=|S|$ and $A=|{\\mathcal{A}}|$ . Most importantly and different from standard MDPs, the RMDP is equipped with a mapping $\\dot{\\Phi}:\\mathcal{P}\\mapsto2^{\\mathcal{P}}$ that characterizes the robust set of any transition kernel in $\\mathcal{P}$ Formally, for any transition kernel $P\\in\\mathcal P$ , we call $\\Phi(P)$ the robust set of $P$ . One could interpret the nominal transition kernel $P_{h}^{\\star}$ as the transition of the training environment, while $\\Phi(P_{h}^{\\star})$ contains all possible transitions of the testing environments. ", "page_idx": 3}, {"type": "text", "text": "Given an RMDP $(S,{\\mathcal{A}},H,P^{\\star},R,\\Phi)$ , we consider using a Markovian policy to make decisions. A Markovian decision policy (or simply, policy) is defined as $\\pi=\\{\\pi_{h}\\}_{h=1}^{H}$ With $\\pi_{h}:S\\mapsto\\Delta(A)$ for each step $h\\in[H]$ . To measure the performance of a policy $\\pi$ in the RMDP, we introduce its robust value function, defined as for any $(\\bar{s},a)\\in S\\times A$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle V_{h,P^{*},\\Phi}^{\\pi}(s):=\\operatorname*{inf}_{\\tilde{P}_{h}\\in\\Phi(P_{h}^{*}),1\\leq h\\leq H}\\mathbb{E}_{\\{\\tilde{P}_{h}\\}_{h=1}^{H},\\{\\pi_{h}\\}_{h=1}^{H}}\\left[\\displaystyle\\sum_{i=h}^{H}R_{i}(s_{i},a_{i})\\right]s_{h}=s\\right],}\\\\ &{Q_{h,P^{*},\\Phi}^{\\pi}(s,a):=\\operatorname*{inf}_{\\tilde{P}_{h}\\in\\Phi(P_{h}^{*}),1\\leq h\\leq H}\\mathbb{E}_{\\{\\tilde{P}_{h}\\}_{h=1}^{H},\\{\\pi_{h}\\}_{h=1}^{H}}\\left[\\displaystyle\\sum_{i=h}^{H}R_{i}(s_{i},a_{i})\\right]s_{h}=s,a_{h}=a\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here the expectation is taken w.r.t. the state-action trajectories induced by policy $\\pi$ under the transition $\\widetilde{P}$ . One can also extend the definition of the robust value functions in terms of any collection of transition kernel $P=\\{P_{h}\\}_{h=1}^{H}\\subset\\mathcal{P}$ as $V_{h,P,\\Phi}^{\\pi}$ and $Q_{h,P,\\Phi}^{\\pi}$ , which we usually use in the sequel. ", "page_idx": 3}, {"type": "text", "text": "Among all the policies, we define the optimal robust policy $\\pi^{\\star}$ as the policy that can maximize the robust value function at the initial time step $h=1$ i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi^{\\star}\\in\\operatorname*{\\argmax}_{\\pi=\\{\\pi_{h}\\}_{h=1}^{H}}V_{1,P^{\\star},\\Phi}^{\\pi}(s_{1}),\\quad\\forall s_{1}\\in\\mathcal{S}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, the optimal robust policy $\\pi^{\\star}$ maximizes the worst case expected total rewards in all possible testing environments. For simplicity and without loss of generality, we assume in the sequel that the initial state $s_{1}\\in\\mathcal{S}$ is fixed. Our results could be directly generalized to $s_{1}\\sim p_{0}(\\cdot)\\in\\Delta(S)$ Similarly, we can also define the optimal robust policy associated with a given stochastic process definedthrough any colletion of transtion kermels $P=\\{P_{h}\\}_{h=1}^{H}\\subset\\mathcal{P}$ inthe same way as (2.1.We denote the optimal robust value functions associated with $P$ $V_{h,P,\\Phi}^{\\star}$ and $Q_{h,P,\\Phi}^{\\star}$ respectivly. ", "page_idx": 3}, {"type": "text", "text": "$s\\times A$ -rectangularity and robust Bellman equations. We consider robust sets $\\Phi$ that have the $s\\times A$ -rectangular structure (Iyengar, 2005). which requires that the robust set is decoupled and independent across different $(s,a)$ -pairs. This kind of structure results in a dynamic programming representation of the robust value functions (efficient planning), and is thus commonly adopted in the literature of distributionally robust RL. More specifically, we assume the fllowing. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 $(S\\times A$ -rectangularity). We assume that the mapping $\\Phi$ satisfies for any transition kernel $P\\in\\mathcal P$ ,the robust set $\\Phi(P)$ is in the form of ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi(P)=\\bigotimes_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\mathcal{P}(s,a;P),\\quad w h e r e\\quad\\mathcal{P}(s,a;P)\\subseteq\\Delta(S).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Under above Assumption 2.1, we have the so-called robust Bellman equation (Iyengar, 2005; Blanchet et al., 2023) which gives a dynamic programming representation of robust value functions. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.2 (Robust Bellman equation). Under Assumption 2.1, for any transition ${\\cal P}\\;=\\;$ $\\{P_{h}\\}_{h=1}^{H}\\subseteq\\mathcal{P}$ and anypolicy $\\pi=\\{\\pi_{h}\\}_{h=1}^{H}$ With $\\pi_{h}:S\\mapsto\\Delta(A)$ itholds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{h,P,\\Phi}^{\\pi}(s)=\\mathbb{E}_{\\pi_{h}(\\cdot\\vert s)}\\big[Q_{h,P,\\Phi}^{\\pi}(s,\\cdot)\\big],\\quad Q_{h,P,\\Phi}^{\\pi}(s,a)=R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}(s,a;P_{h})}\\big[V_{h+1,P,\\Phi}^{\\pi}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Regarding the robust value functions of the optimal robust policy, we also have the following dynamic programming solution which plays a key role in our algorithm design and theoretical analysis. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.3 (Robust Bellman optimal equation). Under Assumption 2.1, for any ${\\cal P}\\;=\\;$ $\\{P_{h}\\}_{h=1}^{H}\\subseteq\\mathcal{P}$ , the robust value functions of any optimal robust policy of $P$ satisfiesthat, ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{h,P,\\Phi}^{\\star}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{h,P,\\Phi}^{\\star}(s,a),\\quad Q_{h,P,\\Phi}^{\\star}(s,a)=R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}(s,a;P_{h})}\\bigl[V_{h+1,P,\\Phi}^{\\star}\\bigr].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Taking $\\begin{array}{r}{\\pi_{h}^{\\star}(\\cdot|s)=\\operatorname*{argmax}_{a\\in\\cal A}Q_{h,P,\\Phi}^{\\star}(s,a),}\\end{array}$ .then $\\pi^{\\star}=\\{\\pi_{h}^{\\star}\\}_{h=1}^{H}$ is optimalrobust policyunder $P$ ", "page_idx": 4}, {"type": "text", "text": "Total-variation distance robust set. In Assumption 2.1, the robust set $\\mathcal{P}(s,a;P)$ is often modeled as a \u201cdistribution ball' centered at $P(\\cdot|s,a)$ . In this paper, we mainly consider this type of robust sets specified by a total-variation distance ball. We put it in the following definition. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 (Total-variation distance robust set). Total-variation distance robust set is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{\\rho}(s,a;P):=\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(S):D_{\\mathrm{TV}}\\big(\\widetilde{P}(\\cdot)\\big|\\big|P(\\cdot|s,a)\\big)\\leq\\rho\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "forsome $\\rho\\in[0,1)$ where $D_{\\mathrm{TV}}(\\cdot\\|\\cdot)$ denotes the total variation distance defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\big(p(\\cdot)\\|q(\\cdot)\\big):=\\frac{1}{2}\\sum_{s\\in\\mathcal{S}}\\big|p(s)-q(s)\\big|,\\quad\\forall p(\\cdot),q(\\cdot)\\in\\Delta(\\mathcal{S}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The TV robust set has recently been extensively studied by Yang et al. (2022); Panaganti and Kalathil (2022); Panaganti et al. (2022); $\\mathrm{Xu}$ et al. (2023); Blanchet et al. (2023); Shi et al. (2023), which all focus on robust RL with a generative model or with a pre-collected offline dataset. More importantly, we emphasize that by (2.2) in Definition 2.4, we do not define the TV distance through the notion of $f$ -divergence which requires that the distribution $p$ is absolute continuous w.r.t. $q$ , as is generally adopted by the above previous works on learning RMDPs with TV robust sets. According to (2.2), we allow p to have a different support than $q$ . That is, there might exist an $s\\in S$ such that $p(s)>0$ and $q(s)=0$ . Given that, the TV robust set in Definition 2.4 could contain transition probabilities that have different supports than the nominal transition probability $P^{\\star}(\\cdot|s,a)$ ", "page_idx": 4}, {"type": "text", "text": "An essential property of the TV robust set is that the robust expectation involved in the robust Bellman equations (Propositions 2.2 and 2.3) has a duality representation that only uses the expectation under the nominal transition kernel, as is shown in the following theorem and proved in Appendix C.1. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.5 (Strong duality representation). Under Definition 2.4, the following duality representation for the robust expectation holds,for any $V:S\\mapsto[0,H]$ and $P_{h}:S\\times{\\mathcal{A}}\\mapsto\\Delta(S),$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h})}\\big[V\\big]=\\operatorname*{sup}_{\\eta\\in[0,H]}\\Big\\{-\\mathbb{E}_{P_{h}(\\cdot\\vert s,a)}\\big[(\\eta-f)_{+}\\big]-\\frac{\\rho}{2}\\cdot\\Big(\\eta-\\operatorname*{min}_{s\\in\\mathcal{S}}V(s)\\Big)_{+}+\\eta\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Value gap between maximum and minimum. Finally, another useful property of the robust value functions of an RMDP with TV robust sets is a fine characterization of the gap between the maximum and the minimum of the robust value function, which is first identified and utilized by Shi et al. (2023) for an infinite horizon RMDP with TV robust sets. In this work, we prove and use a similar result for the finite horizon case, concluded in the following proposition. The proof is in Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.6 (Gap between maximum and minimum). Under Assumption 2.1 with the robust set specified by Definition 2.4, the robust value functions satisfies that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(s,a)\\in S\\times A}{\\operatorname*{max}}Q_{h,P,\\Phi}^{\\pi}(s,a)-\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}Q_{h,P,\\Phi}^{\\pi}(s,a)\\leq\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\},}\\\\ {\\underset{s\\in S}{\\operatorname*{max}}V_{h,P,\\Phi}^{\\pi}(s)-\\underset{s\\in S}{\\operatorname*{min}}V_{h,P,\\Phi}^{\\pi}(s)\\leq\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for any transition $P=\\{P_{h}\\}_{h=1}^{H}\\subset\\mathcal{P}$ any policy $\\pi$ and any step $h\\in[H]$ ", "page_idx": 4}, {"type": "text", "text": "2.2 Robust RL with Interactive Data Collection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We study how to learn the optimal robust policy $\\pi^{\\star}$ in (2.1) from interactive data collection. Specifically, the learner is required to interact with only the training environment, i.e., $P^{\\star}$ , for some $K\\in\\mathbb N$ episodes. In each episode $k$ , the learner adopts a policy $\\pi^{k}$ to interact with the training environment $P^{\\star}$ and to collect data. When the $k$ -th episode ends, the learner updates its policy to $\\bar{\\pi}^{k+1}$ based on historical data and proceeds to the subsequent $(k+1)$ -th episode. The process ends after $K$ episodes. ", "page_idx": 5}, {"type": "text", "text": "Sample complexity. We use the notion of sample complexity as the key evaluation metric. For any given algorithm and predetermined accuracy level $\\varepsilon>0$ , the sample complexity is the minimum number of episodes $K$ required for the algorithm to output an $\\varepsilon$ -optimal robust policy $\\widehat{\\pi}$ satisfying $V_{1,P^{\\star},\\Phi}^{\\star}(s_{1})^{\\star}-V_{1,P^{\\star},\\Phi}^{\\widehat\\pi}(s_{1}^{\\star})\\le\\varepsilon$ The goal is to design algorithms whose sample complexity has small or even optimal dependence on the problem parameters $S,A,H,\\rho$ and $1/\\varepsilon$ ", "page_idx": 5}, {"type": "text", "text": "Online regret. Another evaluation metric that is related to the minimization of sample complexity is the online regret, which is the cumulative difference between the optimal robust policy $\\pi^{\\star}$ and the executed policies $\\{\\pi^{k}\\}_{k=1}^{K}$ Formally, wedefine $\\begin{array}{r}{\\mathrm{Regret}_{\\Phi}(K):=\\sum_{k=1}^{K}V_{1,P^{\\star},\\Phi}^{\\star}\\big(s_{1}\\big)-V_{1,P^{\\star},\\Phi}^{\\pi^{k}}\\big(s_{1}\\big)}\\end{array}$ The goal is to design algorithms that can achieve a sublinear-in- $K$ regret with small dependence on $S,A,H,\\rho$ . It turns out that any sublinear-regret algorithm can be easily converted to a polynomialsample complexity algorithm by applying the standard online-to-batch conversion (Jin et al., 2018). ", "page_idx": 5}, {"type": "text", "text": "3  A Hardness Result: The Curse of Support Shift ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Unfortunately, we show in this section that in general such a problem of robust RL with online data collection is impossible - there exists a simple class of two RMDPs such that an $\\Omega(K)$ -onlineregret lower bound exists. However, previous works on robust RL with a generative model or ofine data with good coverage do provide sample-efficient ways to find the optimal robust policy for this class of RMDPs. This is a separation between robust RL with interactive data collection and generative model/offline data. Please see also Figure 1 for an illustration of the example. ", "page_idx": 5}, {"type": "text", "text": "Example 3.1 (Hard example of robust RL with interactive data collection). Consider two RMDPs $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$ which only differ in their nominal transition kernels.The state space is $\\mathcal{S}=\\{s_{\\mathrm{good}},s_{\\mathrm{bad}}\\}$ andtheactionspaceis $A=\\{0,1\\}$ Thehorizonlength $H=3$ Therewardfunction $R$ always is 1 at the good state $s_{\\mathrm{good}}$ andisOatthebadstate $s_{\\mathrm{bad}}$ i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{h}(s,a)={\\binom{1,}{0,}}\\quad s=s_{\\mathrm{good}}\\;,\\quad\\forall(a,h)\\in{\\cal A}\\times[{\\cal H}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For thegood state $s_{\\mathrm{good}}$ the next state is always $s_{\\mathrm{good}}$ For thebad state $s_{\\mathrm{bad}}$ there is a chance to get to the good state $s_{\\mathrm{good}}$ , with the transition probability depending on the action it takes. Formally, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{h}^{\\star,\\mathcal{M}_{\\theta}}(s_{\\mathrm{good}}|s_{\\mathrm{good}},a)=1,\\quad\\forall(a,h)\\in\\mathcal{A}\\times\\{1,2\\},\\quad\\forall\\theta\\in\\{0,1\\},}\\\\ &{P_{2}^{\\star,\\mathcal{M}_{\\theta}}(s_{\\mathrm{good}}|s_{\\mathrm{bad}},a)=\\left\\{p,\\quad a=\\theta\\right.}\\\\ &{\\quad\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p,q$ are two constants satisfying $0<q<p<1$ .Intuitively, when at the bad state, the optimal actionwould result in a highertransition probability $p$ to the good state than the transition probability $q$ induced by the other action. Finally, we consider the robust set being specified by a total-variation distance ball centered at thenominal transition kernel, that is,for any $P$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Phi(P)=\\bigotimes_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\mathcal{P}_{\\rho}(s,a;P),~\\mathcal{P}_{\\rho}(s,a;P)=\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(S):D_{\\mathrm{TV}}\\big(\\widetilde{P}(\\cdot)\\big|\\big|P(\\cdot|s,a)\\big)\\leq\\rho\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\rho\\in[0,q]$ is the parameter characterizing the size of the robust set. We set $s_{1}=s_{\\mathrm{good}}$ ", "page_idx": 5}, {"type": "text", "text": "For this class of RMDPs, we have the following hardness result for doing robust RL with interactive data collection, an $\\Omega(\\rho\\cdot K)$ -online regret lower bound. The proof is in Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Hardness result (based on Example 3.1). There exists two RMDPs $\\{\\mathcal{M}_{0},\\mathcal{M}_{1}\\}$ the followingregret lower bound holds, ", "page_idx": 5}, {"type": "image", "img_path": "aYWtfsf3uP/tmp/7bc65d9b0497d4de617b859054bcfcd160e7d474311a13fa89c7a0945eec0735.jpg", "img_caption": ["where Regret $\\mathbf{\\mathcal{M}}_{\\Phi}^{\\mathcal{M}_{\\theta},\\mathcal{A}\\mathcal{L}\\mathcal{G}}(K)$ refrstoteregretfalgoritfP $\\mathcal{M}_{\\theta}$ "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "The reason why any algorithm fails for this class of RMDPs is the support shift of the worst-case transition kernel. In robust RL, the performance of a policy $\\pi$ is evaluated via the robust expected total rewards, or equivalently, the expected return under the most adversarial transition kernel $P^{\\dagger,\\pi}$ In such an example, as we explicitly show in the proof, when in the good state $s_{\\mathrm{good}}$ , the worst-case transition kernel $P^{\\dagger,\\pi}$ would transit the state to $s_{\\mathrm{bad}}$ with a constant probability $\\rho>0$ . But the state $s_{\\mathrm{bad}}$ is out of the scope of the data collection process because starting from $s_{1}=s_{\\mathrm{good}}$ the nominal transition kernel always transits the state to $s_{\\mathrm{good}}$ . As a result, the performance of the learned policy at the bad state $s_{\\mathrm{bad}}$ is not guaranteed, and inevitably incurs an ${\\bar{\\Omega}}(\\rho\\cdot K)$ -lower bound of regret, a hardness result. Furthermore, by strategically constructing RMDPs with the horizon $3H$ based on Example 3.1, we can derive a lower bound of $\\Omega(\\rho\\cdot H K)$ . See Appendix B.3 for more discussions. ", "page_idx": 6}, {"type": "text", "text": "4 A Solvable Case, Efficient Algorithm, and Sharp Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Motivated by the hard instance (Example 3.1), we now investigate a special subclass of RMDPs with ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular total variation robust set that we show allows for doing sample-efficient robust RL through interactive data collection. In Section 4.1, we introduce the assumption we impose on the RMDP. We propose our algorithm design in Section 4.2, with theoretical analysis in Section 4.3. ", "page_idx": 6}, {"type": "text", "text": "4.1  Vanishing Minimal Value: Eliminating Support Shift ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To overcome the difficulty of support shift identified in Section 3, we make the following vanishing minimal value assumption on the underlying RMDP. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1 (Vanishing minimal value). We assume that the underlying RMDP satisfies that $\\mathrm{min}_{s\\in{\\cal S}}\\,V_{1,P^{\\star},\\Phi}^{\\star}(s)=0$ Also, WLOG, we assume that the initial state $s_{1}\\not\\in\\mathrm{argmin}_{s\\in S}\\,V_{1,P^{\\star},\\Phi}^{\\star}(s)$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1 imposes that the minimal robust expected total rewards over all possible initial states is 0.Assuing thatheii s $s_{1}\\not\\in\\operatorname*{argmin}_{\\cdot}\\bar{s}\\bar{V}_{1,P_{\\star}^{\\star},\\Phi}^{\\star}(s)$ avoids making the problem trivial. A close look at Assumption 4.1 actually gives that the minimal robust value function of any policy $\\pi$ at any step is zero, that is, $\\mathrm{min}_{s\\in\\mathcal{S}}\\,V_{h,P^{\\star},\\Phi}^{\\pi}(s)=0$ for any policy $\\pi$ and any step $h\\in[H]$ . With this observation, the following proposition explains why this assumption helps to overcome the difficulty, with the proof of the proposition in Appendix C.3. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 (Equivalent expression of TV robust set with vanishing minimal value). For any function $V:S\\mapsto[0,H]$ With $\\begin{array}{r}{\\operatorname*{min}_{s\\in{\\cal S}}V(s)=0,}\\end{array}$ wehavethat ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\left[V\\right]=\\rho^{\\prime}\\cdot\\mathbb{E}_{\\mathcal{B}_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})}[V],\\quad w i t h\\quad\\rho^{\\prime}=1-\\frac{\\rho}{2}>0,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the TV robust set $\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})$ is defined in (3.1) and the set $B_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})$ is defined $a s^{2}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nB_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})=\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(S):\\operatorname*{sup}_{s^{\\prime}\\in S}\\frac{\\widetilde{P}(s^{\\prime})}{P_{h}^{\\star}(s^{\\prime}|s,a)}\\leq\\frac{1}{\\rho^{\\prime}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2Here we implicitly define ${\\begin{array}{l}{{\\frac{0}{0}}}\\end{array}}=0$ and $\\textstyle{\\frac{a}{0}}=\\infty$ for any $a>0$ ", "page_idx": 6}, {"type": "text", "text": "1: Initialize: dataset $\\mathbb{D}=\\emptyset$ 2: for episode $k=1,\\cdots\\,,K$ do 3: Training environment transition estimation: 4: Update the count functions $N_{h}^{k}(s,a,s^{\\prime})$ and $N_{h}^{k}(s,a)$ based on $\\mathbb{D}$ 5: Calculate the transition kernel estimator $\\widehat{P}_{h}^{k}$ as $N_{h}^{k}(s,a,s^{\\prime})/(N_{h}^{k}(s,a)\\vee1)$ 67 $\\overline{{V}}_{H+1}^{k}=\\underline{{V}}_{H+1}^{k}=0.$ 8: for step $\\boldsymbol{h}=\\boldsymbol{H},\\cdots,\\boldsymbol{1}$ do 9: Set $\\overline{{Q}}_{h}^{k}(\\cdot,\\cdot)$ and $\\underline{{Q}}_{h}^{k}(\\cdot,\\cdot)$ as (4.2) and (4.3), with bonus $\\mathsf{s}_{h}^{k}(\\cdot,\\cdot)$ defined in (4.5). 10: Set $\\begin{array}{r l r}{\\pi_{h}^{k}(\\cdot|\\cdot)}&{{}=}&{\\mathrm{argmax}_{a\\in\\mathcal{A}}\\;\\overline{{Q}}_{h}^{k}(\\cdot,a)}\\end{array}$ $\\overline{{V}}_{h}^{k}(\\cdot)\\;\\;=\\;\\;\\mathbb{E}_{\\pi_{h}^{k}(\\cdot|\\cdot)}[\\overline{{Q}}_{h}^{k}(\\cdot,\\cdot)]$ \u3001and $\\begin{array}{r l}{V_{h}^{k}(\\cdot)}&{{}=}\\end{array}$ $\\mathbb{E}_{\\pi_{h}^{k}(\\cdot|\\cdot)}[\\underline{{Q}}_{h}^{k}(\\cdot,\\cdot)]$ 11: end for 12: Execute the policy in training environment and collect data: 13: Receive the initial state $s_{1}^{k}\\in\\mathcal{S}$ 14: for step $h=1,\\cdots\\,,H$ do 15: Take action $a_{h}^{k}\\sim\\pi_{h}^{k}(\\cdot|s_{h}^{k})$ bserve reward $R_{h}(s_{h}^{k},a_{h}^{k})$ and the next state $s_{h+1}^{k}$ 16: end for 17: Sset $\\mathbb{D}$ $\\mathbb{D}\\cup\\{(s_{h}^{k},a_{h}^{k},s_{h+1}^{k})\\}_{h=1}^{H}$ 18: end for 19: Output: Randomly (uniformly) return a policy from $\\{\\pi^{k}\\}_{k=1}^{K}$ ", "page_idx": 7}, {"type": "text", "text": "As Proposition 4.2 indicates, under Assumption 4.1, the robust Bellman equations (Propositions 2.2 and 2.3) at step $h\\in[H]$ is equivalent to taking an infimum over another robust set $\\bar{B}_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})$ that shares the same support as the nominal transition kernel $P^{\\star}(\\cdot|s,a)$ , discounted by a constant $\\rho^{\\prime}<1$ . Intuitively, this new robust set rules out the difficulty originated in unseen states in training environments and the discount factor $\\rho^{\\prime}$ hedges the difficulty from prohibitively small probability of reaching certain states that may appear often in the testing environments. This renders robust RL with interactive data collection possible. See Appendix B.4.1 for discussions/examples of Assumption 4.1. ", "page_idx": 7}, {"type": "text", "text": "4.2 Algorithm Design: OPROVI-TV ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we propose our algorithm that solves robust RL with interactive data collection forRMDPswith $s\\times A$ -rectangular total-variation (TV) robust sets (Assumption 2.1 and Definition 2.4) and satisfying the vanishing minimal value assumption (Assumption 4.1). Our algorithm, OPtimistic RObust Value Iteration for TV Robust Set (OPROVI-TV, Algorithm 1), can automatically balance exploitation and exploration during the interactive data collecting process while managing the distributional robustness of the learned policy. The full algorithm OPROVI-TV is given in Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "Step I: Training Environment Transition Estimation (Line 3 to 5). At the beginning of each episode $k\\in[K]$ , we maintain an estimate of the transition kernel $P^{\\star}$ of the training environment by using the historical data $\\mathbb{D}\\;=\\;\\{(s_{h}^{\\tau},a_{h}^{\\tau},s_{h+1}^{\\tau})\\}_{\\tau=1,h=1}^{k-1,H}$ colleted from the interaetion with the training environment. Specifically, we simply adopt a vanilla empirical estimator, defined as $\\widehat{P}_{h}^{k}(s^{\\prime}|s,a)\\:=\\:N_{h}^{k}(s,a,s^{\\prime})/(N_{h}^{k}(s,a)\\vee1)$ for any $(s,a,h,s^{\\prime})\\,\\in\\,\\mathcal{S}\\,\\times\\,\\mathcal{A}\\,\\times\\,\\mathcal{S}\\,\\times\\,[H]$ where the count functions $N_{h}^{k}(s,a,s^{\\prime})$ and $N_{h}^{k}(s,a)$ are calculated based on the current dataset $\\mathbb{D}$ by $\\begin{array}{r}{N_{h}^{k}(s,a,s^{\\prime})=\\sum_{\\tau=1}^{k-1}\\mathbf{1}\\big\\{(s_{h}^{\\tau},a_{h}^{\\tau},s_{h+1}^{\\tau})=(s,a,s^{\\prime})\\big\\}}\\end{array}$ and $\\begin{array}{r}{N_{h}^{k}(s,a)=\\sum_{s^{\\prime}\\in S}N_{h}^{k}(s,a,s^{\\prime})}\\end{array}$ for any $(s,a,h,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\times[H]$ . This just coincides with the transition estimator adopted by existing non-robust online RL algorithms (Auer et al., 2008; Azar et al., 2017; Zhang et al., 2021). ", "page_idx": 7}, {"type": "text", "text": "Step Il: Optimistic Robust Planning (Line 6 to 11). Given $\\widehat{P}^{k}(\\cdot|\\cdot,\\cdot)$ that estimates the training environment, we perform an optimistic robust planning to construct the policy $\\pi^{k}$ to execute.Basically, the optimistic robust planning follows the robust Bellman optimal equation (Proposition 2.3) to approximate the optimal robust policy, but differs in that it maintains an upper bound and a lower bound of the optimal robust value function and chooses the policy that maximizes the optimistic estimate to incentivize exploration during data collection. Here the purpose of maintaining the lower bound estimate is to facilitate the construction of the variance-aware optimistic bonus (see following), which helps to sharpen our theoretical analysis. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "$\\triangleright$ Simplifying the robust expectation. To utilize the vanishing minimal value condition (Assumption 4.1), we take a closer look into the robust Bellman equation. By strong duality (Proposition 2.5), the robust expectation $\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P)}[V]$ for any $V\\in[0,H]$ satisfying $\\begin{array}{r}{\\operatorname*{min}_{s\\in\\bar{\\cal S}}V(s)\\stackrel{.}{=}0}\\end{array}$ isequivalent to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P)}\\left[V\\right]=\\operatorname*{sup}_{\\eta\\in[0,H]}\\bigg\\{-\\mathbb{E}_{P(\\cdot|s,a)}\\left[\\left(\\eta-V\\right)_{+}\\right]+\\left(1-\\frac{\\rho}{2}\\right)\\cdot\\eta\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Consequently, with a slight abuse of the notation, in the remaining of the paper, we re-define the operator $\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P)}[V]$ as the right hand side of (4.1). Due to Assumption 4.1, the robust Bellman (optimal) equation (Proposition 2.2 and Proposition 2.3) still holds under this new definition. ", "page_idx": 8}, {"type": "text", "text": "$\\triangleright$ Optimistic robust planning. With this in mind, the optimistic robust planning goes as follows. Starting rom $\\overline{{V}}_{H+1}^{k}=\\underline{{V}}_{H+1}^{k}=0$ we recursvly define that frany $(s,a)\\in S\\times A$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{Q}}_{h}^{k}(s,a)=\\operatorname*{min}\\left\\{R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]+\\mathfrak{b o n u s}_{h}^{k}(s,a),\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\right\\},}\\\\ &{\\underline{{Q}}_{h}^{k}(s,a)=\\operatorname*{max}\\left\\{R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathfrak{b o n u s}_{h}^{k}(s,a),0\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the robust expectation $\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}$ follows thedefinition in the right hand side of (4.1) and the bonus function b $\\mathsf{o n u s}_{h}^{k}(s,a)\\geq0$ is defined later. Here we truncate the optimistic estimate $\\overline{{Q}}_{h}^{k}$ via the upper bound $\\operatorname*{min}\\{H,\\rho^{-1}\\}$ of the true optimal robust value function $Q_{h,P^{\\star},\\Phi}^{\\star}$ This truncation arises from the combined implication of Proposition 2.6 and the fact that $\\mathrm{min}_{(s,a)\\in S\\times A}\\,Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)=0$ under Assumption 4.1. As we establish in Lemma E.2, $\\overline{{Q}}_{h}^{k}$ and $\\underline{o}_{h}^{k}$ form upper and lower bounds for $Q_{h,P^{\\star},\\Phi}^{\\star}$ and $Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}$ under a proper choice of the bonus. After performing (4.2) and (4.3), we choose the data collection policy $\\pi_{h}^{k}$ to be the optimal policy with respect to the optimistic estimator $\\overline{{Q}}_{h}^{k}$ and define $\\overline{{V}}_{h}^{k}$ and $\\underline{{V}}_{h}^{k}$ accordingly by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\pi_{h}^{k}(\\cdot|\\cdot)=\\operatorname*{argmax}_{a\\in\\mathcal{A}}\\overline{{Q}}_{h}^{k}(\\cdot,a),\\quad\\overline{{V}}_{h}^{k}(s)=\\mathbb{E}_{\\pi_{h}^{k}(\\cdot|s)}\\Big[\\overline{{Q}}_{h}^{k}(s,\\cdot)\\Big],\\quad\\underline{{V}}_{h}^{k}(s)=\\mathbb{E}_{\\pi_{h}^{k}(\\cdot|s)}\\Big[\\underline{{Q}}_{h}^{k}(s,\\cdot)\\Big].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We remark that the purpose of maintaining the lower bound estimate (4.3) is to facilitate the construction of the bonus and to help sharpen our theoretical analysis. The construction of the policy $\\pi^{k}$ is still based on the optimistic estimator, which is why we call it optimistic robust planning. As indicated by theory, the optimistic robust planning can effectively guide the policy to explore uncertain robust value function estimates, striking a balance between exploration and exploitation while managing distributional robustness. ", "page_idx": 8}, {"type": "text", "text": "$\\triangleright$ Bonus function. The bonus function bonus $_{h}^{k}(s,a)$ is a Bernstein-style bound defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{2\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]}{H}+\\frac{c_{2}H^{2}S\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ \uff0c $c_{1},c_{2}>0$ are absolute constants, and $\\delta$ signifies a pre-selected fail probability. Under (4.5), $\\overline{{Q}}_{h}^{k}$ and $\\underline{o}_{h}^{k}$ become upper and lower bounds of the optimal robust value functions (Lemma E.2). More importantly, the bonus (4.5) is carefully designed for robust value functions such that the summation of this bonus term (especially the leading variance term in (4.5)) over time steps is well controlled, for which we also develop new analysis methods. This is critical for obtaining a sharp sample complexity of Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "4.3  Theoretical Guarantees ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section establishes the online regret and the sample complexity of OPROVI-TV (Algorithm 1). Our main result is the following, upper bounding the online regret of Algorithm 1, proved in AppendixE. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.3 (Online regret of OPROVI-TV). Given an RMDP with ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular total-variation robust set of radius $\\rho\\,\\in\\,[0,1)$ (Assumption 2.1 and Definition 2.4) satisfying Assumptions 4.1, choosing the bonus function as (4.5) with suffciently large $c_{1},c_{2}>0$ then with probability at least $1-\\delta$ Algorithm $^{\\,l}$ satisfies ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{Regret}_{\\Phi}(K)\\leq\\mathcal{O}\\Bigl(\\sqrt{\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}H^{2}S A K\\iota^{\\prime}}\\Bigr),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\iota^{\\prime}=\\log^{2}(S A H K/\\delta)$ and $O(\\cdot)$ hides absolute constants and lower order terms in $K$ ", "page_idx": 9}, {"type": "text", "text": "Theorem 4.3 shows that Algorithm 1 enjoys a sublinear online regret of $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ , meaning that it is able to approximately find the optimal robust policy through interactive data collection. This is in contrast with the general hardness result in Section 3 where sample-efficient learning is impossible in the worst case. Thus we show the effectiveness of the minimal value assumption for robust RL with interactive data collection. As a corollary, we have the following sample complexity bound for Algorithm 1. It is obtained directly from Theorem 4.3 and a standard online to batch conversion. ", "page_idx": 9}, {"type": "text", "text": "Corollary 4.4 (Sample complexity of OPROvI-TV). Under the same setup and conditions as in Theorem4.3,with probability at least $1-\\delta$ Algorithm $^{\\,l}$ canoutputan $\\varepsilon$ -optimalpolicywithin ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}H^{2}S A^{\\prime\\prime}/\\varepsilon^{2}\\right)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "episodes, where $\\iota^{\\prime\\prime}=\\log(S A H/\\varepsilon\\delta)$ and $O(\\cdot)$ hides absolute constants. The valid range of $\\varepsilon$ satisfies $\\varepsilon\\in(0,c\\cdot\\operatorname*{min}\\{1,1/(\\rho H)\\}]$ for someconstant $c>0$ ", "page_idx": 9}, {"type": "text", "text": "We compare the sample complexity (4.6) with prior arts on non-robust online RL and robust RL with a generative model. On the one hand, (4.6) with $\\rho=0$ equalsto $\\widetilde{\\mathcal{O}}(H^{3}S A/\\varepsilon^{2})$ , matching the minimax sample complexity lower bound for online RL in non-robust MDPs (Azar et al., 2017). This means that our algorithm design can naturally handle non-robust MDPs as a special case (please also see Remark B.4 for why one can reduce Algorithm 1 to general non-robust MDPs under Assumption 4.1). On the other hand, the previous work of Shi et al. (2023) for robust RL in infinite horizon RMDPs with a TV robust set and a generative model showcases a minimax optimal sample complexity of ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\operatorname*{min}\\left\\{H_{\\gamma},\\rho^{-1}\\right\\}H_{\\gamma}^{2}S A/\\varepsilon^{2}\\right),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for $\\rho\\in[0,1)$ ,wherewe $H_{\\gamma}:=1/(1-\\gamma)$ is the effective horizon of the infinite $\\gamma$ discountedRMDPs. As a result, the sample complexity (4.6) of Algorithm 1 matches their result. We highlight that our algorithm does not rely on a generative model and operates purely through interactive data collection. ", "page_idx": 9}, {"type": "text", "text": "Extensions of Algorithm 1 and its theory. In Appendix B.4.2, we extend Algorithm 1 to solve a new type of RMDPs whose robust set consists of transition probabilities with bounded ratio to the nominal kernel. The intuition is because it is equivalent to the ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular RMDPwith a TV robust set and vanishing minimal value assumption in an appropriate sense (Proposition 4.2) Consequently, by a clever usage of Algorithm 1, we can also solve this new model sample-efficiently, as is shown in Corollary B.5. Such a result echoes our intuition on the curse of support shift. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and future works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work shows that in the absence of any structural assumptions, robust RL via interactive data collection necessarily induces a linear regret lower bound in the worst case due to the curse of support shift. Under the vanishing minimal value assumption, an assumption that is able to effectively rule out the potential support shift issues for RMDPs with a TV robust set, we propose a sample-efficient robust RL algorithm for those RMDPs with sharp analysis. Potential future works include extending to function approximation settings and other types of robust sets. See discussion in Appendix B.5. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 1915967, 2118199, 2229012, 2312204. The authors would like to thank the anonymous reviewers for their helpful comments. The authors would also like to thank Pan Xu and Zhishuai Liu for their feedback on an early draft of this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "AGARWAL, A., JIANG, N., KAKADE, S. M. and SUN, W. (2019). Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 10-4. 44   \nAGARWAL, A., JIN, Y. and ZHANG, T. (2023). Vo q l: Towards optimal regret in model-free rl with nonlinear function approximation. In The Thirty Sixth Annual Conference on Learning Theory. PMLR. 16, 17   \nAUER, P., JAKSCH, T. and ORTNER, R. (2008). Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems 21. 8   \nAYOUB, A., JIA, Z., SZEPESVARI, C., WANG, M. and YANG, L. (2020). Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning. PMLR. 16   \nAZAR, M. G., OsBAND, I. and MUNOs, R. (2017). Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning. PMLR. 8, 10, 16, 41   \nBADRINATH, K. P. and KALATHIL, D. (2021). Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In International Conference on Machine Learning. PMLR. 16   \nBLANCHET, J., LU, M., ZHANG, T. and ZHONG, H. (2023). Double pessimism is provably effcient for distributionally robust offine reinforcement learning: Generic algorithm and robust partial coverage. arXiv preprint arXiv:2305.09659 . 2, 3, 4, 5, 16, 17, 18, 21, 24   \nCLAVIER, P., PENNEC, E. L. and GEIST, M. (2023). Towards minimax optimality of model-based robust reinforcement learning. arXiv preprint arXiv:2302.05372 . 16   \nDANN, C., LATTIMORE, T. and BRUNSKILL, E. (2017). Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems 30.16   \nDING, W., SHI, L., CHI, Y. and ZHAO, D. (2024). Seeing is not believing: Robust reinforcement learning against spurious correlation. Advances in Neural Information Processing Systems 36. 16   \nDONG, J., L1, J., WANG, B. and ZHANG, J. (2022). Online policy optimization for robust mdp. arXiv preprint arXiv:2209.13841 . 17   \nDU, S., KAKADE, S., LEE, J., LOVETT, S., MAHAJAN, G., SUN, W. and WANG, R. (2021). Bilinear clases: A structural framework for provable generalization in rl. In International Conference on Machine Learning. PMLR. 16   \nEL GHA0UI, L. and NILIM, A. (2005). Robust solutions to markov decision problems with uncertain transition matrices. Operations Research 53 780-798. 16   \nFOSTER, D. J., KAKADE, S. M., QIAN, J. and RAKHLIN, A. (2021). The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487 . 16   \nHE, J., ZHA0, H., ZHOU, D. and GU, Q. (2023). Nearly minimax optimal reinforcement learning for linear markov decision processes. In International Conference on Machine Learning. PMLR. 16   \nHU, J., ZHONG, H., JIN, C. and WANG, L. (2022). Provable sim-to-real transfer in continuous domain with partial observations. arXiv preprint arXiv:2210.15598 . 2   \nHUANG, J., ZHONG, H., WANG, L. and YANG, L. F. (2023a). Horizon-free and instance-dependent regret bounds for reinforcement learning with general function approximation. arXiv preprint arXiv:2312.04464 . 17   \nHUANG, J., ZHONG, H., WANG, L. and YANG, L. F. (2023b). Tackling heavy-tailed rewards in reinforcement learning with function approximation: Minimax optimal and instance-dependent regret bounds. arXiv preprint arXiv:2306.06836 . 16   \nIYENGAR, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research 30 257-280. 2,4,16, 17   \nJIANG, N., KRISHNAMURTHY, A., AGARWAL, A., LANGFORD, J. and SCHAPIRE, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In International Conference on Machine Learning. PMLR. 16   \nJIN, C., ALLEN-ZHU, Z., BUBECK, S. and JORDAN, M. I. (2018). Is q-learning provably effcient? Advances in neural information processing systems 31. 6, 16   \nJIN, C., LIU, Q. and MIRYOOsEFI, S. (2021). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. Advances in neural information processing systems 34 13406-13418. 16   \nJIN, C., YANG, Z., WANG, Z. and JoRDAN, M. 1. (2020). Provably effcient reinforcement learning with linear function approximation. In Conference on Learning Theory. PMLR. 16   \nKIRAN, B. R., SOBH, I., TALPAERT, V., MANNION, P., AL SALLAB, A. A., YOGAMANI, S. and PEREZ, P. (2021). Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems 23 4909-4926. 1   \nKOBER, J., BAGNELL, J. A. and PETERs, J. (2013). Reinforcement learning in robotics: A survey. The International Journal of Robotics Research 32 1238-1274. 1   \nKUANG, Y., LU, M., WANG, J., ZHOU, Q., L1, B. and L1, H. (2022). Learning robust policy against disturbance in transition dynamics via state-conservative policy optimization. In Proceedings of the AAAI Conference on Artificial Inteligence, vol. 36. 2, 16   \nL1, G., CA1, C., CHEN, Y., WEI, Y. and CHI, Y. (2023). Is q-learning minimax optimal? a tight sample complexity analysis. Operations Research . 16   \nL1, Y. and LAN, G. (2023). First-order policy optimization for robust policy evaluation. arXiv preprint arXiv:2307.15890 . 16   \nLIU, Z., LU, M., WANG, Z., JORDAN, M. and YANG, Z. (2022). Welfare maximization in competitive equilibrium: Reinforcement learning for markov exchange economy. In International Conference on Machine Learning. PMLR. 17   \nLIU, Z.,LU, M., XIONG, W., ZHONG, H., HU, H., ZHANG, S., ZHENG, S., YANG, Z. and WANG, Z. (2023). One objective to rule them all A maximization objective fusing estimation and planning for exploration. arXiv preprint arXiv:2305.18258 . 17   \nLIU, Z. and Xu, P. (2024a). Distributionally robust off-dynamics reinforcement learning: Provable efficiency with linear function approximation. arXiv preprint arXiv:2402.15399 . 16   \nLIU, Z. and Xu, P. (2024b). Minimax optimal and computationally effcient algorithms for distributionally robust offline reinforcement learning. arXiv preprint arXiv:2403.09621 . 16   \nLYKOURIS, T., SIMCHOWITZ, M., SLIVKINS, A. and SUN, W. (2021). Corruption-robust exploration in episodic reinforcement learning. In Conference on Learning Theory. PMLR. 17   \nMA, X., LIANG, Z., XIA, L., ZHANG, J., BLANCHET, J., LIU, M., ZHAO, Q. and ZHOU, Z. (2022). Distributionally robust offline reinforcement learning with linear function approximation. arXiv preprint arXiv:2209.06620 . 2, 16   \nMAURER, A. and PoNTIL, M. (2009). Empirical bernstein bounds and sample variance penalization. arXiv preprint arXiv:0907.3740 . 29   \nMENARD, P., DOMINGUES, O. D., SHANG, X. and VALKO, M. (2021). Ucb momentum q-learning: Correcting the bias without forgetting. In International Conference on Machine Learning. PMLR. 16   \nMOOS, J., HANSEL, K., ABDULSAMAD, H., STARK, S., CLEVER, D. and PETERS, J. (2022). Robust reinforcement learning: A review of foundations and recent advances. Machine Learning and Knowledge Extraction 4 276-315. 2   \nOUYANG, L., WU, J., JIANG, X., ALMEIDA, D., WAINWRIGHT, C., MISHKIN, P., ZHANG, C., AGARWAL, S., SLAMA, K., RAY, A. ET AL. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 27730- 27744.1   \nPANAGANTI, K. and KALATHIL, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In International Conference on Artijficial Intelligence and Statistics. PMLR. 2,5, 16, 17, 18   \nPANAGANTI, K., XU, Z., KALATHIL, D. and GHAVAMZADEH, M. (2022). Robust reinforcement learning using offine data. arXiv preprint arXiv:2208.05129 . 2, 3, 5, 16, 17, 18, 19   \nPENG, X. B., ANDRYCHOWICZ, M., ZAREMBA, W. and ABBEEL, P. (2018). Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA). IEEE. 1   \nPINTO, L., DAVIDSON, J., SUKTHANKAR, R. and GUPTA, A. (2017). Robust adversarial reinforcement learning. In International Conference on Machine Learning. PMLR. 2   \nSADEGHI, F and LEVINE, S. (2016). Cad2rl: Real single-image fight without single real image. arXiv preprint arXiv:1611.04201 . 1   \nSHI, L. and CHI, Y. (2022). Distributionally robust model-based offine reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767 . 2, 16, 21   \nSHI, L., LI, G., WEI, Y., CHEN, Y, GEIST, M. and CHI, Y. (2023). The curious price of distributional robustness in reinforcement learning with a generative model. arXiv preprint arXiv:2305.16589 . 2, 3, 5, 10, 16, 17, 18, 21, 23   \nS1, N.,ZHANG, F., ZHOU, Z. and BLANCHET, J. (2023). Distributionally robust batch contextual bandits. Management Science . 16   \nSILVER, D., SCHRITTWIESER, J., SIMONYAN, K., ANTONOGLOU, I., HUANG, A., GUEZ, A., HUBERT, T., BAKER, L., LAI, M., BOLTON, A. ET AL. (2017). Mastering the game of go without human knowledge. nature 550 354-359. 1   \nSUN, W., JIANG, N., KRISHNAMURTHY, A., AGARWAL, A. and LANGFORD, J. (2019). Modelbased rl in contextual decision processes: Pac bounds and exponential improvements over modelfree approaches. In Conference on learning theory. PMLR. 16   \nSUTTON, R. S. and BARTO, A. G. (2018). Reinforcement learning: An introduction. MIT press. 2   \nWANG, H., SHI, L. and CHI, Y. (2024). Sample complexity of offline distributionally robust linear markov decision processes. arXiv preprint arXiv:2403.12946 . 16   \nWANG, L., ZHANG, W., HE, X. and ZHA, H. (2018). Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1   \nWANG, Q., Ho, C. P. and PETRIK, M. (2022). On the convergence of policy gradient in robust mdps. arXiv preprint arXiv:2212.10439 . 16   \nWANG, Q., Ho, C. P. and PETRIK, M. (2023a). Policy gradient in robust MDPs with global convergence guarantee. In Proceedings of the 4Oth International Conference on Machine Learning (A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato and J. Scarlett, eds.), vol. 202 of Proceedings of Machine Learning Research. PMLR. 16   \nWANG, S., SI, N., BLANCHET, J. and ZHOU, Z. (2023b). A finite sample complexity bound for distributionally robust q-learning. In International Conference on Artificial Intelligence and Statistics. PMLR. 16   \nWANG, S., SI, N., BLANCHET, J. and ZHOU, Z. (2023c). On the foundation of distributionally robust reinforcement learning. arXiv preprint arXiv:2311.09018 . 16   \nWANG, S., SI, N., BLANCHET, J. and ZHOU, Z. (2023d). Sample complexity of variance-reduced distributionally robust q-learning. arXiv preprint arXiv:2305.18420 . 16   \nWANG, Y. and ZoU, S. (2021). Online robust reinforcement learning with model uncertainty. Advances in Neural Information Processing Systems 34 7193-7206. 16   \nWANG, Y. and ZoU, S. (2022). Policy gradient method for robust reinforcement learning. In International Conference on Machine Learning. PMLR. 16   \nWEI, C.-Y., DANN, C. and ZIMMERT, J. (2022). A model selection approach for corruption robust reinforcement learning. In International Conference on Algorithmic Learning Theory. PMLR. 17   \nWIESEMANN, W., KUHN, D. and RUSTEM, B. (2013). Robust markov decision processes. Mathematics of Operations Research 38 153-183. 16   \nWU, T, YANG, Y., ZHONG, H., WANG, L., DU, S. and JIAO, J. (2022). Nearly optimal policy optimization with stable at any time guarante. In International Conference on Machine Learning. PMLR. 16   \nXU, H. and MANNOR, S. (2010). Distributionally robust markov decision processes. Advances in Neural Information Processing Systems 23. 16   \nXu, Y. and ZEEV, A. (2023). Bayesian dsin principles for frequentist squential learning. In International Conference on Machine Learning. PMLR. 17   \nXU, Z., PANAGANTI, K. and KALATHIL, D. (2023). Improved sample complexity bounds for distributionally robust reinforcement learning. In International Conference on Artificial Intelligence and Statistics. PMLR. 2, 3, 5, 16, 17, 18, 21   \nYANG, R., ZHONG, H., XU, J., ZHANG, A., ZHANG, C., HAN, L. and ZHANG, T. (2023a). Towards robust offine reinforcement learning under diverse data corruption. arXiv preprint arXiv:2310.12955. 17   \nYANG, W., WANG, H., KOZUNO, T., JORDAN, S. M. and ZHANG, Z. (2023b). Avoiding model estimation in robust markov decision processes with a generative model. arXiv preprint arXiv:2302.01248. 16   \nYANG, W., ZHANG, L. and ZHANG, Z. (2022). Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics. The Annals of Statistics 50 3223-3248. 2,5,16,17,18,21   \nYE, C., HE, J, Gu, Q. and ZHANG, T. (2024). Towards robust model-based reinforcement learning against adversarial corruption. arXiv preprint arXiv:2402.08991 . 17   \nYE, C., XIONG, W., GU, Q. and ZHANG, T. (2023a). Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In International Conference on Machine Learning. PMLR. 17   \nYE, C., YANG, R., GU, Q. and ZHANG, T. (2023b). Corruption-robust offine reinforcement learning with general function approximation. arXiv preprint arXiv:2310.14550 . 17   \nYU, Z., DA1, L., XU, S., GAO, S. and Ho, C. P. (2023). Fast bellman updates for wasserstein distributionally robust mdps. In Thirty-seventh Conference on Neural Information Processing Systems. 16   \nZANETTE, A. and BRUNsKILL, E. (2019). Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning. PMLR. 16   \nZHANG, X., CHEN, Y., ZHU, X. and SUN, W. (2022). Corruption-robust offine reinforcement learning. In International Conference on Artificial Intelligence and Statistics. PMLR. 17   \nZHANG, Z., CHEN, Y., LEE, J. D. and DU, S. S. (2023). Settling the sample complexity of online reinforcement learning. arXiv preprint arXiv:2307.13586 . 16 ZHANG, Z., J1, X. and DU, S. (2021). Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory. PMLR.   \n8, 16 ZHANG, Z., ZHOU, Y. and J1, X. (2020). Almost optimal model-free reinforcement learningvia reference-advantage decomposition. Advances in Neural Information Processing Systems 33   \n15198-15207. 16 ZHAO, W., QUERALTA, J. P. and WESTERLUND, T. (202O). Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE. 1 ZHONG, H., XIONG, W., ZHENG, S., WANG, L., WANG, Z., YANG, Z. and ZHANG, T. (2022). Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond. arXiv preprint arXiv:2211.01962.17 ZHONG, H. and ZHANG, T. (2023). A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes. arXiv preprint arXiv:2305.08841 . 16 ZHOU, D., GU, Q. and SZEPEsVARI, C. (2021a). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory. PMLR. 16 ZHOU, R., LIU, T., CHENG, M., KALATHIL, D., KUMAR, P. and TIAN, C. (2023). Natural actorcritic for robust reinforcement learning with function approximation. In Thirty-seventh Conference on Neural Information Processing Systems. 16 ZHOU, Z., ZHOU, Z., BAI, Q., QIU, L., BLANCHET, J. and GLYNN, P. (2021b). Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In International Conference on Artificial Intelligence and Statistics. PMLR. 2, 16 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We give a detailed discussion on the related works in this section. ", "page_idx": 15}, {"type": "text", "text": "Robust reinforcement learning in robust Markov decision processes. Robust RL is usually framed as a robust Markov decision process (RMDP) (Iyengar, 2005; El Ghaoui and Nilim, 2005; Wiesemann et al., 2013). There is a long line of work dedicated to the problem of how to solve for the optimal robust policy of a given RMDP, i.e., planning (Iyengar, 2005; El Ghaoui and Nilim, 2005; Xu and Mannor, 2010; Wang and Zou, 2022; Wang et al., 2022; Kuang et al., 2022; Wang et al., 2023a; Yu et al., 2023; Zhou et al., 2023; Li and Lan, 2023; Wang et al., 2023c; Ding et al., 2024). Recently, the community has also witnessed a growing body of work on sample-efficient robust RL in RMDPs with different data collection oracles, including the generative model setup (Yang et al., 2022; Panaganti and Kalathil, 2022; Si et al., 2023; Wang et al., 2023b; Yang et al., 2023b; Xu et al., 2023; Clavier et al., 2023; Wang et al., 2023d; Shi et al., 2023), offline setting (Zhou et al., 2021b; Panaganti et al., 2022; Shi and Chi, 2022; Ma et al., 2022; Blanchet et al., 2023; Liu and Xu, 2024b; Wang et al., 2024), and interactive data collection setting (Badrinath and Kalathil, 2021; Wang and Zou, 2021; Liu and Xu, 2024a). ", "page_idx": 15}, {"type": "text", "text": "Our work falls into the paradigm of sample-efficient robust RL with interactive data collection. Wang and Zou (2021) and Badrinath and Kalathil (2021) propose efficient online learning algorithms to obtain the optimal robust policy of an infinite horizon RMDP, but none of them handle the challenge of exploration in online RL by assuming the access to explorative policies. This assumption enables the learner to collect high-quality data essential for effective learning and decision-making. In contrast, our work focuses on developing efficient algorithms for the fully online setting, where there is no predefined exploration policy to use. Under this more challenging setting, we address the exploration challenge through algorithmic design rather than relying on assumed access to explorative policies. ", "page_idx": 15}, {"type": "text", "text": "During the preparation of this work, we are aware of several concurrent and independent works (Liu and Xu, 2024a,b; Wang et al., 2024), which study a different type of RMDPs known as $d$ -rectangular linear MDPs (Ma et al., 2022; Blanchet et al., 2023). In particular, Liu and $\\mathrm{\\DeltaXu}$ (2024b) and Wang et al. (2024) consider the offline setting, while Liu and $\\mathrm{\\DeltaXu}$ (2024a) investigate robust RL through interactive data collection (off-dynamics learning), thus bearing closer relevance to our work. More specifically, under the existence of a \u201cfail-state\", the algorithm in Liu and Xu (2024a) can learn an $\\varepsilon$ -optimal robust policy with provable sample efficiency. In contrast, our work first explicitly uncovers the fundamental hardness of doing robust RL in RMDPs with a TV distance based robust set and without additional assumptions. To overcome the inherent difficulty, we adopt a vanishing minimal value assumption that strictly generalizes the \u201cfail-state\u201d assumption used in Liu and Xu (2024a). Moreover, our focus is on tabular ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular RMDPs, with customized algorithmic design and theoretical analysis which allow us to obtain a sharp sample complexity bound. ", "page_idx": 15}, {"type": "text", "text": "Finally, in Table 1, we compare the sample complexity of our algorithm with prior work on robust RL for RMDPswith ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular TV robust sets under various settings (generative model/offline dataset). We remark that the works of Panaganti and Kalathil (2022) and Blanchet et al. (2023) are in the paradigm of function approximation, and here we reduce their general sample complexity result to the tabular setup we consider. ", "page_idx": 15}, {"type": "text", "text": "Sample-efficient online non-robust reinforcement learning.  Our work is also closely related to online non-robust RL, which is often formulated as a Markov decision process (MDP) with online data collection. For non-robust online RL, the key challenge is the exploration-exploitation tradeoff. There has been a long line of work (Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Zanette and Brunskill, 2019; Zhang et al., 2020, 2021; M\u00e9nard et al., 2021; Wu et al., 2022; Li et al., 2023; Zhang et al., 2023) addressing this challenge in the context of tabular MDPs, where the state space and action space are finite and also relatively small. In particular, many algorithms (e.g., UCBvI in Azar et al. (2017)) have been proven capable of finding an $\\varepsilon$ optimal policywithin $\\widetilde{\\mathcal{O}}(H^{3}S A/\\varepsilon^{2})$ sample complexity. Notably, a standard MDP corresponds to an RMDP with a TV robust set and $\\rho=0$ , suggesting that OPROvI-TV can naturally achieve nearly minimax-optimality for non-robust RL. Moving beyond the tabular setups, recent works also investigate online non-robust RL with linear function approximation (Jin et al., 2020; Ayoub et al., 2020; Zhou et al., 2021a; Zhong and Zhang, 2023; Huang et al., 2023b; He et al., 2023; Agarwal et al., 2023) and even general function approximations (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021; Jin et al., 2021; Foster et al., ", "page_idx": 15}, {"type": "text", "text": "2021; Liu et al., 2022; Zhong et al., 2022; Liu et al., 2023; Huang et al., 2023a; Xu and Zeevi, 2023;   \nAgarwal et al., 2023). ", "page_idx": 16}, {"type": "text", "text": "Corruption robust reinforcement learning. Generally speaking, our research is also related to another form of robust RL, namely corruption robust RL (Lykouris et al., 2021; Wei et al., 2022; Zhang et al., 2022; Ye et al., 2023a,b; Yang et al., 2023a; Ye et al., 2024). This branch of researches on robust RL addresses scenarios where training data is corrupted, presenting a distinct challenge from distributionally robust RL. The latter concerns testing time robustness, where the agent is evaluated in a perturbed environment after being trained on nominal data. These two forms of robust RL, while sharing the overarching goal to enhance agent resilience, operate within different contexts and confront distinct challenges. Thus, a direct comparison between these two types of robust RL is difficult because each addresses unique aspects of resilience. ", "page_idx": 16}, {"type": "text", "text": "B  Further Discussions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section complements the main part of the paper by further commenting and discussing several aspects of the paper. Due to space limits, these important remarks are provided here. ", "page_idx": 16}, {"type": "text", "text": "B.1 Discussions on Introduction (Section 1) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "About a generative model and a simulator.  A generative model here means a mechanism that when queried at some state, action, and time step, returns a sample of next state. Here we distinguish this notion with the notion of simulator or simulated environment which generally refers to a humanmade training environment that mimics the real-world environment. With a generative model on hand, interactive data collection is no longer needed, but in a simulated environment, it is common to train a robust policy through interactive data collection in practice. ", "page_idx": 16}, {"type": "text", "text": "The definition of total-variation robust set. We notice that all of the previous work on sampleefficient robust RL in RMDPs with TV robust sets (Yang et al., 2022; Panaganti and Kalathil, 2022; Panaganti et al., 2022; Xu et al., 2023; Blanchet et al., 2023; Shi et al., 2023) relies on defining the TV distance through the general $f$ -divergence so that a strong duality representation holds. But this implicitly requires the testing environment transition probability is absolute continuous w.r.t. the training environment transition probability. In this paper, we do not make such a restriction. We prove the same strong duality even if the absolute continuity does not hold. In fact, all the previous work can be directly extended to such TV distance definition via our more general strong duality result. ", "page_idx": 16}, {"type": "text", "text": "An existing work. We note that an existing work (Dong et al., 2022) also studies the problem of robust RL with interactive data collection. They study ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular RMDPs with a TV robust set, assuming that the support of the training environment transition is the full state space $\\boldsymbol{S}$ .They claim the existence of an algorithm that enjoys a $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ -online regret. We point out that their proof exhibits an essential faw (misuse of Lemma 12 therein) and therefore the regret they claim is invalid. ", "page_idx": 16}, {"type": "text", "text": "The range of the robust set size $\\rho$ .We do not signify the situation when $\\rho=1$ since in that case the TV robust set contains all possible transition probabilities, making the problem statistically trivial. In that case, no sample is needed. ", "page_idx": 16}, {"type": "text", "text": "B.2 Discussions on Preliminaries (Section 2) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.2.1 Robust Markov Decision Processes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Robust Bellman equations. We remark that the original version of the robust Bellman equation (Iyengar, 2005) is for infinite horizon RMDPs and a customized proof of robust Bellman equation for finite horizon RMDPs (Proposition 2.2) can be found in Appendix A.1 of Blanchet et al. (2023). The robust Bellman optimal equation (Proposition 2.3) is then a corollary or can be proved similarly. ", "page_idx": 16}, {"type": "text", "text": "Strong duality under TV distance robust set. An essential property of the TV robust set is that the robust expectation involved in the robust Bellman equations (Propositions 2.2 and 2.3) has a duality representation that only uses the expectation under the nominal transition kernel. Previous works, e.g., Yang et al. (2022), have proved such a result when the TV distance is defined through $f$ -divergence. Here we extend such a result to the TV distance defined directly though (2.2) that allows a difference support between $p$ and $q$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Remark B.1. Despite all previous works on RMDPs with TV robust sets relying on the definition ofTvdistance $D_{\\mathrm{TV}}(p(\\cdot)\\|\\bar{q}(\\cdot))$ with absolute continuity of pwithrespect toq to obtain the strong duality representation in the form of (2.3), their results can be directly extended to TV distance that allowsfordifferentsupportbetween $p$ andqthankstoProposition2.5. ", "page_idx": 17}, {"type": "text", "text": "Value gap between maximum. We note that in the proof of Proposition 2.6, we actually show a tighter form of bound of the gap between the maximum and minimum as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\rho}\\cdot\\left(1-(1-\\rho)^{H}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "But in the sequel, we mainly use the form of $\\operatorname*{min}\\{H,\\rho^{-1}\\}$ for its brevity and the fact of $(1-(1-$ $\\rho)^{H})/\\rho=\\Theta(\\operatorname*{min}\\{H,\\rho^{-1}\\})$ in the sense that ", "page_idx": 17}, {"type": "equation", "text": "$$\nc\\cdot\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\leq(1-(1-\\rho)^{H})/\\rho\\leq\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $H\\geq H_{0}\\in\\mathbb{N}_{+}$ and $\\rho\\in[0,1]$ with some constant $c>0$ that is independent of $(H,\\rho)$ ", "page_idx": 17}, {"type": "text", "text": "In contrast with a crude bound of $H$ , such a fine upper bound decreases when $\\rho$ is large, which is essential to understanding the statistical limits of doing robust RL in RMDPs with TV robust sets. ", "page_idx": 17}, {"type": "text", "text": "B.2.2 Robust RL with Interactive Data Collection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sample complexity. The metric of sample complexity is connected with the sample complexity used in robust RL with generative models and offline settings (see related works for the references), wherein the sample complexity means the minimum number of generative samples or pre-collected offine data required to achieve $\\varepsilon$ -optimality. In contrast, here the sample complexity is measuring the least number of interactions with the training environment needed to learn $\\pi^{\\star}$ ,where nogenerative or offline sample is available. Such a learning protocol casts unique challenges on the algorithmic design and theoretical analysis to get the optimal sample complexity. ", "page_idx": 17}, {"type": "text", "text": "B.3Discussions on Hardness Result: The Curse of Support Shift (Section 3) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In contrast to the interactive data collection setting we consider, doing robust RL with a generative model or an offine dataset with good coverage properties does not face the difficulty we displayed through Example 3.1. It turns out that any RMDP with ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular total-variation robust set (including Example 3.1) can be solved in a sample-efficient manner therein, see Yang et al. (2022); Panaganti and Kalathil (2022); Panaganti et al. (2022); $\\mathrm{Xu}$ et al. (2023); Blanchet et al. (2023); Shi et al. (2023) and Remark B.1. The intuitive reason is that, for the generative model setting, the learner can directly query any state-action pair to estimate the nominal transition kernel $P^{\\star}$ , and thus no support shift happens. The same reason holds for the ofline setup with a good-coverage dataset. ", "page_idx": 17}, {"type": "text", "text": "There is a broader understanding of the curse of support shift that hinders the tractability of robust RL via interactive data collection. The concept of support shift could be comprehended within a broader context beyond the disjointness of certain parts of the support sets of the training and testing environments. Instead, ensuring a \u201chigh probability of disjointness\" is enough to maintain the integrity of the hardness result. For instance, we can modify the state $s_{\\mathrm{good}}$ in Example 3.1 so that it is no longer an absorbing state. Rather, $s_{\\mathrm{good}}$ could transit to $s_{\\mathrm{bad}}$ with a small probability, such as $2^{-H}$ . This modification expands the support of the training environment to encompass the entire state space. Nevertheless, acquiring information about $s_{\\mathrm{bad}}$ necessitates exponential samples, thereby preserving the hardness result. ", "page_idx": 17}, {"type": "text", "text": "B.4  Discussions on A Solvable Case, Efficient Algorithm, and Sharp Analysis (Section 4) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.4.1  Vanishing Minimal Value Assumption ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Another understanding of Assumption 4.1. To understand this from another perspective, it could be shown that under the conclusions of Proposition 4.2, the robust value function of any ", "page_idx": 17}, {"type": "text", "text": "policy $\\pi$ is equivalent to the robust value function of this policy under another discounted RMDP $\\bar{({\\cal S},\\dot{\\cal A},{\\cal H},{P^{\\star}},{\\cal R}^{\\prime},\\Phi^{\\prime})}$ With $R_{h}^{\\prime}(s,a)=(\\rho^{\\prime})^{h-1}R_{h}(s,a)$ and $\\Phi^{\\prime}$ givenby ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi^{\\prime}(P)=\\bigotimes_{(s,a)\\in S\\times A}B_{\\rho^{\\prime}}(s,a;P).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And therefore we are equivalently considering this new type of RMDPs. Please refer to Section B.4.2 for more discussions on the connections between the two types of RMDPs. ", "page_idx": 18}, {"type": "text", "text": "Examples of Assumption 4.1. In the sequel, we provide a concrete condition that makes Assumption 4.1 hold, which imposes that the state space of the RMDP has a \u201cclosed\"\u2019 subset of \u201cfail-states\" withzerorewards. ", "page_idx": 18}, {"type": "text", "text": "Condition B.2 (Fail-states). There exists a subset $\\mathcal{S}_{f}\\subset\\mathcal{S}$ of fail states such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{h}(s,a)=0,\\quad P_{h}^{\\star}(S_{f}|s,a)=1,\\quad\\forall(s,a,h)\\in S_{f}\\times\\mathcal{A}\\times[H].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This type of \u201cfail-states\" condition is first proposed by Panaganti et al. (2022) (with $|S_{f}|=1]$ Oto handle the computational issues for robust offline RL under function approximations (out of the scope of our work). In contrast, here we make the vanishing minimal value assumption in order to tackle the support shift or extrapolation issue for the interactive data collection setup. The comparison between the vanishing minimal value assumption (Assumption 4.1) and the \u201cfail-states\"\u201d\u2019 condition (Condition B.2) is given below. ", "page_idx": 18}, {"type": "text", "text": "Remark B.3 (Comparison between Assumption 4.1 and Condition B.2). We first observe that ConditionB.2 implies that $\\mathrm{min}_{s\\in{\\cal S}}\\,V_{h,P^{\\star},\\Phi}^{\\pi}(s)=0$ for any policy $\\pi$ and step $h\\in[H]$ therefore satisfying the minimal value assumption (Assumption 4.1). Conversely, thevanishing minimal value assumption in Assumption 4.1 is strictly more general than the fail-state condition in Condition B.2. To illustrate, one can consider an RMDP characterized by the state space $S=\\{s_{1},s_{2}\\}$ , action space ${\\mathcal{A}}=\\{a_{1}\\}$ time horizon $H=2$ reward function $R_{h}(s,\\dot{a})=\\mathbf{1}\\big\\{s=\\dot{s}_{2}\\big\\}$ andtransitionprobabilities definedasfollows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{1}^{\\star}(s_{1}|s_{1},a_{1})=1-\\rho,\\quad P_{1}^{\\star}(s_{2}|s_{1},a_{1})=\\rho,\\quad P_{1}^{\\star}(s_{1}|s_{2},a_{1})=0,\\quad P_{1}^{\\star}(s_{2}|s_{2},a_{1})=1,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\rho$ is the radius of the robust set.It is evident that no fail-state emerges within such an RMDP structure. However, this RMDP satisfies the vanishing minimal value assumption since $V_{1,P^{\\star},\\Phi}^{\\star}(s_{1})=0$ ", "page_idx": 18}, {"type": "text", "text": "Remark B.4 (Reduction to non-robust MDP without loss of generality). It is noteworthy that assuming thevanishing minimal value(Assumption 4.1) or the presence of fail-states(Condition B.2) in thenon-robustcase $\\mathit{\\Pi}^{\\prime}\\rho=0.$ )is without loss of generality. This is achievable by expanding the prior statespace $\\boldsymbol{S}$ ofMDPto includean additional state $s_{f}$ ,denoted as the fail-state.More importantly, this augmentation does not alter the optimal value or the optimal valuefunction of the original MDP. Consequently, it becomes sufficient to seek the optimal policy within the augmented MDP, which satisfiestheconditionsofvanishingminimalvalue(Assumption4.1)ortheexistenceoffail-states (Condition B.2). This indicates that our algorithm and theoretical analysis in the sequel can be directly reduced to non-robust MDPs without additional assumptions. ", "page_idx": 18}, {"type": "text", "text": "B.4.2 Extensions to Robust Set with Bounded Transition Probability Ratio ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we show that our algorithm design (Algorithm 1) can also be applied to $s\\times A$ rectangular discounted RMDPs with robust sets given by (B.1) (i.e., bounded ratio between training and testing transition probabilities). We establish that our main theoretical result in Section 4.3 can imply a sublinear regret upper bound for this model, which means that this type of RMDPs can also be solved sample-efficiently by a clever usage of Algorithm 1. This coincides with our intuition on support shift in Section 4.1. ", "page_idx": 18}, {"type": "text", "text": "$s\\times A$ -rectangular discounted RMDPs with robust set (B.1). We first formally define the model we consider. We define a finite-horizon discounted RMDP as a finite-horizon RMDP $\\mathcal{M}_{\\gamma}=$ $(S,\\mathcal{A},H,P^{\\star},R_{\\gamma},\\Phi^{\\prime})$ where therobust set $\\Phi^{\\prime}$ is given by (B.1),i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi^{\\prime}(P)=\\bigotimes_{(s,a)\\in\\mathcal{S}\\times A}\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(S):\\operatorname*{sup}_{s^{\\prime}\\in S}\\frac{\\widetilde{P}(s^{\\prime})}{P_{h}^{\\star}(s^{\\prime}|s,a)}\\leq\\frac{1}{\\rho^{\\prime}}\\right\\}:=\\bigotimes_{(s,a)\\in\\mathcal{S}\\times A}B_{\\rho^{\\prime}}(s,a;P^{\\star}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This robust set contains transition probabilities that share the same support as the nominal transition kermel.Thereward function $R_{\\gamma}\\stackrel{\\cdot}{=}\\{\\gamma^{h-1}\\cdot R_{h}\\}_{h=1}^{H}$ wWhere $\\gamma\\in\\mathsf{\\bar{(0,1)}}$ isthediscount factor and $R_{h}\\,\\in\\,[0,1]$ is the true reward at step $h$ . That is, the robust value function is now the worst case expected discounted total reward. ", "page_idx": 19}, {"type": "text", "text": "Algorithm and regret bound. \u03b2 Now we show that we can apply Algorithm 1 to solve robust RL in ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular discounted RMDPs with robust set (B.2) via interactive data collection. ", "page_idx": 19}, {"type": "text", "text": "As motivated by the discussions under Proposition 4.2, we define an auxiliary finite-horizon TVRMDP $\\widetilde{\\mathcal{M}}$ as $\\mathcal{\\widetilde{M}}=(\\widetilde{\\mathcal{S}},\\mathcal{A},H,\\widetilde{P^{\\star}},\\widetilde{R},\\widetilde{\\Phi})$ which include an additional \u201cfail-state' $s_{f}$ . More specifically, the state space $\\widetilde{S}=S\\cup\\{s_{f}\\}$ . The transition kernel $\\widetilde{P}^{\\star}$ is defined as, for any step $h\\in[H]$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{P}_{h}^{\\star}(\\cdot|s,a)=P_{h}^{\\star}(\\cdot|s,a),\\quad\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\quad\\mathrm{and}\\quad\\widetilde{P}_{h}^{\\star}(\\cdot|s_{f},a)=\\delta_{s_{f}}(\\cdot),\\quad\\forall a\\in\\mathcal{A}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The reward function $\\widetilde{R}$ is defined as, for any step $h\\in[H]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{R}_{h}(s,a)=\\left(\\frac{\\gamma}{\\rho^{\\prime}}\\right)^{h-1}\\cdot R_{h}(s,a),\\quad\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\quad\\mathrm{and}\\quad\\widetilde{R}_{h}(s_{f},a)=0,\\quad\\forall a\\in\\mathcal{A}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We suppose that the discount factor $\\gamma\\le\\rho^{\\prime}$ so that the reward function $\\widetilde{R}_{h}\\,\\in\\,[0,1]$ . The robust mapping $\\widetilde{\\Phi}$ is defined as, for any $\\widetilde{P}:\\widetilde{S}\\times A\\mapsto\\Delta(\\widetilde{S})$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\Phi}(\\widetilde{P})=\\underset{(s,a)\\in\\widetilde{S}\\times A}{\\bigotimes}\\Big\\{\\widetilde{P}(\\cdot)\\in\\Delta(\\widetilde{S}):D_{\\mathrm{TV}}\\big(P(\\cdot)\\big\\|\\widetilde{P}(\\cdot|s,a)\\big)\\leq\\rho\\Big\\}}\\\\ &{\\qquad:=\\underset{(s,a)\\in\\widetilde{S}\\times A}{\\bigotimes}\\ \\widetilde{\\mathcal{P}}_{\\rho}(s,a;\\widetilde{P}),\\quad\\rho=2-2\\rho^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, $\\widetilde{\\mathcal{M}}$ is an RMDP with $s\\times A$ -rectangular TV robust set of radius $\\rho$ and satisfying Assumption 4.1 (because it satisfies the \u201cfail-state' Condition B.2). Furthermore, for any initial state $\\boldsymbol{s}_{1}\\in\\tilde{\\mathcal{S}}\\setminus\\{\\boldsymbol{s}_{f}\\}=\\mathcal{S}$ , the interaction with the transition kernel P\\* is equivalent to the interaction with the transition kernel $P^{\\star}$ of the original RMDP $\\mathcal{M}_{\\gamma}$ , since by the definition (B.3), starting from any $s\\neq s_{f}$ the agent would follow the same dynamics as $P^{\\star}$ . What's more, for any policy $\\widetilde{\\pi}_{h}:\\widetilde{S}\\mapsto\\Delta(\\mathcal{A})$ for $\\widetilde{\\mathcal{M}}$ , it naturally induces the unique policy $\\widetilde{\\pi}_{S,h}:S\\mapsto\\Delta(A)$ for the original RMDP $\\mathcal{M}_{\\gamma}$ ", "page_idx": 19}, {"type": "text", "text": "Therefore, we can run Algorithm 1 on the auxiliary RMDP M, starting from the initial state $s_{1}\\in\\widetilde{S}\\setminus\\{s_{f}\\}$ , which only needs the interaction with $P^{\\star}$ . Suppose the output policy by the algorithm $\\{\\widetilde{\\pi}^{k}\\}_{k=1}^{K}$ thenthefolowingorolarowsthindudpoliy $\\{\\widetilde{\\pi}_{S}^{k}\\}_{k=1}^{K}$ for theorigialRMDP $\\mathcal{M}_{\\gamma}$ enjoys a sublinear regret. ", "page_idx": 19}, {"type": "text", "text": "Corollary B.5 (Online regret of Algorithm 1 for discounted RMDPs with robust sets (B.2)). Consider an ${\\mathcal{S}}\\times{\\mathcal{A}}$ -rectangular $\\gamma$ -discounted RMDP with robust set (B.2) satisfying $0\\leq\\gamma\\leq\\rho^{\\prime}\\in(1/2,1]$ There exists an algorithm $\\mathcal{A}\\mathcal{L}\\mathcal{G}$ (specified by the above discussion) such that its online regret for this RMDP is bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Regret}_{\\Phi^{\\prime}}^{A\\mathcal{L G}}(K)\\leq\\mathcal{O}\\biggl(\\sqrt{\\operatorname*{min}\\left\\{H,(2-2\\rho^{\\prime})^{-1}\\right\\}H^{2}S A K\\iota^{\\prime}}\\biggr),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\iota^{\\prime}=\\log^{2}(S A H K/\\delta)$ and $O(\\cdot)$ hides absolute constants and lower order terms in $K$ ", "page_idx": 19}, {"type": "text", "text": "Proof of Corollary B.5. See Appendix F.1 for a detailed proof of Corollary B.5. ", "page_idx": 19}, {"type": "text", "text": "Corollary B.5 shows that besides $s\\times A$ -rectangular RMDPs with TV robust set and vanishing minimal value assumption, the $s\\times A$ -rectangular discounted RMDP with robust set of bounded transition probability ratio (B.2) can also be solved sample-efficiently by robust RL via interactive data collection. This also echoes our intuition on the support shift issue in Section 4.1. Furthermore, the regret decays as $\\rho^{\\prime}$ decays in which case the transition probability ratio bound becomes higher, i.e., the robust set becomes larger. ", "page_idx": 19}, {"type": "text", "text": "Remark B.6. The upper bound in Corollary B.5 does not depend on the discount factor $\\gamma$ since Algorithm1adoptsacoarseboundof $\\tilde{R}_{h}\\leq1$ .Theupperbound canbedirectlyimproved tobe $\\gamma$ -dependent using a tighter truncation in step (4.2) of Algorithm 1. ", "page_idx": 19}, {"type": "text", "text": "B.5 Discussions of Limitations and Future Works ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this work, we show that in the absence of any structural assumptions, robust RL through interactive data collection necessarily induces a linear regret lower bound in the worst case due to the curse of support shift. Meanwhile, under the vanishing minimal value assumption, an assumption that is able to effectively rule out the potential support shift issues for RMDPs with a TV robust set, we propose a sample-efficient robust RL algorithm for those RMDPs. We discuss some potential extensions here and the associated challenges next. ", "page_idx": 20}, {"type": "text", "text": "Extension to function approximation setting.  The vanishing minimal value assumption also suffices for developing sample-efficient algorithms for $S\\times A$ -rectangular TV-robust-set RMDPs with linear or even general function approximation (Blanchet et al., 2023). Nonetheless, achieving the nearly optimal rate under general function approximation remains elusive. ", "page_idx": 20}, {"type": "text", "text": "Extension to other types of robust set. Beyond the TV distance based robust set we consider, recent literature on robust RL also investigate other types of $\\phi$ -divergence based robust set including KL divergence, $\\chi^{2}$ distance ( Yang et al., 2022; Shi and Chi, 2022; Blanchet et al., 2023; Xu et al., 2023; Shi et al., 2023). An interesting direction of future work is to investigate is it also possible and, if possible, can we design provably sample-efficient robust RL algorithms with interactive data collection for RMDPs with those types of robust sets. Notably, the KL divergence based robust set naturally does not suffer from the curse of support shifts that gives rise to the hardness for the TV robust set case. However, we find that there are other difficulties for robust RL in KL divergence based RMDPs through interactive data collection. Meanwhile, the optimal sample complexity for robust RL in RMDPs with KL divergence robust set is still elusive even in the offine learning setup (Shi and Chi, 2022). We leave the study of RMDPs with KL divergence robust set for future work. ", "page_idx": 20}, {"type": "text", "text": "C Proofs for Properties of RMDPs with TV Robust Sets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1  Proof of Proposition 2.5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To simplify the notations, we present the following lemma, which directly implies Proposition 2.5. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1 (Strong duality for TV robust set). The following duality for total variation robust set holds,for $f:S\\mapsto[0,H]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{Q(\\cdot):D_{\\tau\\vee}(Q(\\cdot)\\|Q^{*}(\\cdot))\\leq\\sigma}}\\mathbb{E}_{Q(\\cdot)}[f]=\\operatorname*{sup}_{\\eta\\in[0,H]}\\left\\{-\\mathbb{E}_{Q^{*}(\\cdot)}\\big[(\\eta-f)_{+}\\big]-\\frac{\\sigma}{2}\\cdot\\left(\\eta-\\operatorname*{min}_{s\\in\\mathcal{S}}f(s)\\right)_{+}+\\eta\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\sigma\\in[0,1]$ and the TV distance $D_{\\mathrm{TV}}(Q(\\cdot)\\|Q^{\\star}(\\cdot))$ is defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}(Q(\\cdot)\\|Q^{\\star}(\\cdot))=\\frac{1}{2}\\sum_{s\\in{\\cal S}}|Q(s)-Q^{\\star}(s)|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma C.1. First, we note that when $Q^{\\star}(s)>0$ for any $s\\in S$ , i.e., any $Q(\\cdot)\\in\\Delta(S)$ is absolute continuous w.r.t. $Q^{\\star}(\\cdot)$ , it has been proved by Yang et al. (2022) that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{Q(\\cdot):D_{\\mathrm{TV}}(Q(\\cdot)\\|Q^{\\star}(\\cdot))\\le\\sigma}}\\mathbb{E}_{Q(\\cdot)}[f]=\\operatorname*{sup}_{\\eta\\in\\mathbb{R}}\\left\\{-\\mathbb{E}_{Q^{\\star}(\\cdot)}\\big[(\\eta-f)_{+}\\big]-\\frac{\\sigma}{2}\\cdot\\left(\\eta-\\operatorname*{min}_{s\\in\\mathcal{S}}f(s)\\right)_{+}+\\eta\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, as is shown in Lemma H.8 in Blanchet et al. (2023), the optimal dual variable $\\eta^{\\star}$ lies in $[0,H]$ when $f\\in[0,H]$ . Therefore, for $Q^{\\star}(\\cdot)$ such that $Q^{\\star}(s)>0$ for any $s\\in S$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{Q(\\cdot):D_{\\operatorname{TV}}(Q(\\cdot)\\|Q^{*}(\\cdot))\\leq\\sigma}}\\mathbb{E}_{Q(\\cdot)}[f]=\\operatorname*{sup}_{\\eta\\in[0,H]}\\left\\{-\\mathbb{E}_{Q^{*}(\\cdot)}\\left[(\\eta-f)_{+}\\right]-\\frac{\\sigma}{2}\\cdot\\left(\\eta-\\operatorname*{min}_{s\\in\\mathcal{S}}f(s)\\right)_{+}+\\eta\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now for any $Q^{\\star}(\\cdot)\\;\\in\\;\\Delta(S)$ , we can prove the same result by averaging $Q^{\\star}(\\cdot)$ with a uniform distribution and taking the limit. More specifically, denote $U(\\cdot)\\,\\dot{\\in}\\,\\Delta(S)$ as the uniform distribution ", "page_idx": 20}, {"type": "text", "text": "on $\\boldsymbol{S}$ ,i.e., $U(s)=1/|S|$ for any $s\\in S$ . Consider the following distributionally robust optimization problem, for any $\\epsilon\\in[0,1]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{P}\\big(\\epsilon\\big):=\\operatorname*{inf}_{Q(\\cdot):D_{\\mathrm{TV}}\\big(Q(\\cdot)\\|(1-\\epsilon)Q^{\\star}(\\cdot)+\\epsilon\\cdot U(\\cdot)\\big)\\leq\\sigma}\\mathbb{E}_{Q(\\cdot)}[f].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By our previous discussions, since $(1-\\epsilon)Q^{\\star}(s)+\\epsilon\\cdot U(s)>0$ for any $s\\in S$ and $\\epsilon>0$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{P}(\\epsilon)=\\mathsf{D}(\\epsilon),\\quad\\forall\\epsilon\\in(0,1],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the function $\\mathsf{D}(\\cdot):[0,1]\\mapsto\\mathbb{R}_{+}$ is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\left(\\epsilon\\right):=\\operatorname*{sup}_{\\eta\\in\\left[0,H\\right]}\\left\\{-(1-\\epsilon)\\cdot\\mathbb{E}_{Q^{\\star}(\\cdot)}\\big[\\big(\\eta-f\\big)_{+}\\big]-\\epsilon\\cdot\\mathbb{E}_{U(\\cdot)}\\big[\\big(\\eta-f\\big)_{+}\\big]-\\frac{\\sigma}{2}\\cdot\\left(\\eta-\\operatorname*{min}_{s\\in S}f\\left(s\\right)\\right)_{+}+\\eta\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the definition of $\\mathsf{P}(\\cdot)$ and $\\mathbb{D}(\\cdot)$ , our goal is to prove that $\\mathbb{P}(0)=\\mathbb{D}(0)$ . To this end, it suffices to prove that (i) $\\scriptstyle\\operatorname*{lim}_{\\epsilon\\to0+}\\mathsf{D}(\\epsilon)$ exists and $\\begin{array}{r}{\\operatorname*{lim}_{\\epsilon\\to0+}\\tt D(\\epsilon)=\\tt D(0)}\\end{array}$ ; and (i) $\\mathrm{lim}_{\\epsilon\\to0+}\\,\\mathsf{P}(\\epsilon)=\\mathsf{P}(0)$ To prove (i), consider that for any $\\epsilon>0$ , by the definition of $\\mathbb{D}(\\cdot)$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\mathtt{D}(0)-\\mathtt{D}(\\epsilon)|\\leq\\operatorname*{sup}_{\\eta\\in[0,H]}\\Big\\{\\epsilon\\cdot\\mathbb{E}_{Q^{\\star}(\\cdot)}\\big[(\\eta-f)_{+}\\big]+\\epsilon\\cdot\\mathbb{E}_{U(\\cdot)}\\big[(\\eta-f)_{+}\\big]\\Big\\}\\leq\\epsilon\\cdot2H.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the right hand side tends to O as $\\epsilon$ tends to O, we know that $\\scriptstyle\\operatorname*{lim}_{\\epsilon\\to0+}\\mathbb{D}(\\epsilon)$ exists, $\\begin{array}{r}{\\operatorname*{lim}_{\\epsilon\\to0+}\\mathsf{D}(\\epsilon)=}\\end{array}$ $\\mathtt{D}(0)$ . This also indicates that $\\textstyle\\operatorname*{lim}_{\\epsilon\\to0+}\\operatorname{P}(\\epsilon)$ exists due to (C.1). This proves (i). Now we prove (i). Notice that since the set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\{Q(\\cdot)\\in\\Delta(S):D_{\\mathrm{TV}}\\left(Q(\\cdot)\\|(1-\\epsilon)Q^{\\star}(\\cdot)+\\epsilon\\cdot U(\\cdot)\\right)\\leq\\sigma\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is a closed subset of $\\mathbb{R}^{|S|}$ , and $\\mathbb{E}_{Q(\\cdot)}[f]$ is a continuous function of $Q(\\cdot)\\in\\mathbb{R}^{|S|}$ w.rt. the $\\|\\cdot\\|_{2}$ -norm, we can denote the optimal solution to the optimization problem involved in $\\mathtt{P}(\\epsilon)$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{\\epsilon}^{\\dagger}(\\cdot)=\\underset{Q(\\cdot):D_{\\mathrm{TV}}\\left(Q(\\cdot)\\|(1-\\epsilon)Q^{\\star}(\\cdot)+\\epsilon\\cdot U(\\cdot)\\right)\\leq\\sigma}{\\mathrm{arginf}}\\mathbb{E}_{Q(\\cdot)}[f],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which also gives that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{P}(\\epsilon)=\\mathbb{E}_{Q_{\\epsilon}^{\\dagger}(\\cdot)}[f]=\\sum_{s\\in S}Q_{\\epsilon}^{\\dagger}(s)f(s).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With these preparations, we are able to prove (i). On the one hand, consider for any $\\epsilon\\in(0,1]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\big((1-\\epsilon)\\cdot Q_{0}^{\\dagger}(\\cdot)+\\epsilon\\cdot U(\\cdot)\\big|\\big|(1-\\epsilon)\\cdot Q^{\\star}(\\cdot)+\\epsilon\\cdot U(\\cdot)\\big)\\leq(1-\\epsilon)\\cdot\\sigma\\leq\\sigma.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, for any $\\epsilon\\in(0,1]$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}(\\epsilon)=\\operatorname*{inf}}}\\\\ &{}&{Q(\\cdot){\\colon\\cal D}_{\\mathrm{TV}}\\left(Q(\\cdot)\\|(1-\\epsilon)Q^{\\star}(\\cdot)+\\epsilon{\\cdot}U(\\cdot)\\right)\\le\\sigma}\\\\ &{}&{\\le\\mathbb{E}_{(1-\\epsilon)\\cdot Q_{0}^{\\dagger}(\\cdot)+\\epsilon\\cdot U(\\cdot)}[f]=(1-\\epsilon)\\cdot\\mathbb{E}_{Q_{0}^{\\dagger}}[f]+\\epsilon\\cdot\\mathbb{E}_{U(\\cdot)}[f],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0+}\\mathtt{P}(\\epsilon)\\le\\mathbb{E}_{Q_{0}^{\\dagger}}[f]=\\mathtt{P}(0).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, for any $\\epsilon\\in(0,1]$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma\\geq\\displaystyle\\frac{1}{2}\\sum_{s\\in S}\\Big|Q_{\\epsilon}^{\\dagger}(s)-(1-\\epsilon)\\cdot Q^{\\star}(s)-\\epsilon\\cdot U(s)\\Big|}\\\\ &{\\quad\\geq(1-\\epsilon)\\cdot D_{\\mathrm{TV}}(Q_{\\epsilon}^{\\dagger}(\\cdot)\\|Q^{\\star}(\\cdot))-\\epsilon\\cdot D_{\\mathrm{TV}}(Q_{\\epsilon}^{\\dagger}(\\cdot)\\|U(\\cdot)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and by using $D_{\\mathrm{TV}}(Q_{\\epsilon}^{\\dagger}(\\cdot)\\|U(\\cdot))\\leq1$ , we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}(Q_{\\epsilon}^{\\dagger}(\\cdot)\\|Q^{\\star}(\\cdot))\\leq\\frac{\\sigma+\\epsilon}{1-\\epsilon}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider a sequence of $\\{\\epsilon_{i}\\}_{i=1}^{\\infty}$ converging to O, i.e., $\\mathrm{lim}_{i\\to0+}\\,\\epsilon_{i}\\;=\\;0$ . Since $\\{Q_{\\epsilon_{i}}^{\\dagger}(\\cdot)\\}_{i=1}^{\\infty}$ is a sequence contained in a compact subset of $\\mathbb{R}^{|S|}$ , it has a converging (w.r.t. $\\|\\cdot\\|_{2})$ subsequence denoted by $\\{Q_{\\epsilon_{i_{k}}}^{\\dagger}(\\cdot)\\}_{k=1}^{\\infty}$ whose limit is denoted as $Q^{\\dagger}(\\cdot)\\in\\Delta(S)$ . By (C.3), we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}(Q_{\\epsilon_{i_{k}}}^{\\dagger}(\\cdot)\\|Q^{\\star}(\\cdot))\\leq\\frac{\\sigma+\\epsilon_{i_{k}}}{1-\\epsilon_{i_{k}}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking limit on both sides of (C.4) (limit of LHS exists since the TV distance is a continuous function (w.r.t. $\\|\\cdot\\|_{2})$ of its first entry and the limit of RHS obviously exists), we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}(Q^{\\dagger}(\\cdot)\\|Q^{\\star}(\\cdot))\\leq\\sigma.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we can arrive at the following. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\epsilon\\to0+}{\\operatorname*{lim}}\\,\\mathbb{P}(\\epsilon)=\\underset{\\epsilon\\to0+}{\\operatorname*{lim}}\\,\\mathbb{E}_{Q_{\\epsilon}^{\\dagger}(\\cdot)}[f]=\\underset{k\\to0+}{\\operatorname*{lim}}\\,\\mathbb{E}_{Q_{\\epsilon_{i_{k}}}^{\\dagger}(\\cdot)}[f]=\\mathbb{E}_{Q^{\\dagger}(\\cdot)}[f]}\\\\ &{\\qquad\\qquad\\qquad\\underset{Q(\\cdot):D_{\\mathrm{TV}}(Q(\\cdot)\\|Q^{\\star}(\\cdot))\\leq\\sigma}{\\operatorname*{inf}}\\mathbb{E}_{Q(\\cdot)}[f]=\\mathbb{P}(0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first and the last equality follows from the definition of $\\mathsf{P}(\\cdot)$ , the second equality follows from the choice of the sequence $\\{\\epsilon_{i_{k}}\\}_{k=1}^{\\infty}$ that converges to O, the third equality is due to the continuity of $\\mathbb{E}_{Q(\\cdot)}[f]$ of $Q(\\cdot)$ (w.r.t. $\\|\\cdot\\|_{2})$ , and the inequality follows from (C.5). Finally, with (C.2) and (C.6), we conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0+}\\mathsf{P}(\\epsilon)=\\mathsf{P}(0),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which proves (ii). Consequently, by (i) and (i) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}(0)=\\operatorname*{lim}_{\\epsilon\\rightarrow0+}\\mathbb{P}(\\epsilon)=\\operatorname*{lim}_{\\epsilon\\rightarrow0+}\\mathbb{D}(\\epsilon)=\\mathbb{D}(0).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recalling the definitions of $\\mathsf{P}(\\cdot)$ and $\\mathbb{D}(\\cdot)$ , we conclude the proof of Lemma C.1. ", "page_idx": 22}, {"type": "text", "text": "C.2  Proof of Proposition 2.6 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Proposition 2.6. Here we prove a stronger result that for any policy $\\pi$ and step $h\\in[H]$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(s,a)\\in S\\times A}{\\operatorname*{max}}Q_{h,P,\\Phi}^{\\pi}(s,a)-\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}Q_{h,P,\\Phi}^{\\pi}(s,a)\\leq\\frac{1}{\\rho}\\cdot\\Big(1-(1-\\rho)^{H-h+1}\\Big),}\\\\ {\\underset{s\\in S}{\\operatorname*{max}}V_{h,P,\\Phi}^{\\pi}(s)-\\underset{s\\in S}{\\operatorname*{min}}V_{h,P,\\Phi}^{\\pi}(s)\\leq\\frac{1}{\\rho}\\cdot\\Big(1-(1-\\rho)^{H-h+1}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, we note that for the last step $h=H$ (C.7) and (C.8) naturally hold since $R_{H}\\in[0,1]$ Now suppose that (C.8) hold for some step $h+1$ . By robust Bellman equation (Proposition 2.2), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)=R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq1+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big],\\quad\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality uses the fact that $R_{h}\\leq1$ . Now we denote the state with the least robust value as ", "page_idx": 22}, {"type": "equation", "text": "$$\ns_{0}\\in\\underset{s\\in\\mathcal{S}}{\\mathrm{argmin}}\\,V_{h+1,P^{\\star},\\Phi}^{\\pi}(s).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Inspired by Shi et al. (2023), we choose a transition kernel ${\\widetilde P}_{h}$ satisfying that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{P}_{h}(\\cdot|s,a)\\right\\|_{1}=1-\\rho,\\quad P_{h}^{\\star}(s^{\\prime}|s,a)\\geq\\widetilde{P}_{h}(s^{\\prime}|s,a)\\geq0,\\quad\\forall(s,a,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\left(\\widetilde{P}_{h}(\\cdot|s,a)+\\rho\\cdot\\delta_{s_{0}}(\\cdot)\\,\\Big\\Vert\\,P_{h}^{\\star}(\\cdot|s,a)\\right)\\leq\\rho,\\quad\\forall(s,a)\\in S\\times A.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here $\\delta_{s_{0}}(\\cdot)$ is the point measure centered at $s_{0}$ defined in (C.10). Combined with (C.9), we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)\\leq1+\\mathbb{E}_{\\widetilde{P}_{h}(\\cdot\\vert s,a)+\\rho\\cdot\\delta_{s_{0}}(\\cdot)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]}\\\\ &{\\qquad\\qquad\\qquad=1+\\mathbb{E}_{\\widetilde{P}_{h}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]+\\rho\\cdot V_{h+1,P^{\\star},\\Phi}^{\\pi}(s_{0})}\\\\ &{\\qquad\\qquad\\leq1+(1-\\rho)\\cdot\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)+\\rho\\cdot\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consequently from (C.11), we further obtain that for any $(s,a)\\in S\\times A$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)-\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)}}\\\\ &{}&{\\leq1+(1-\\rho)\\cdot\\underset{s\\in S}{\\operatorname*{max}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)+\\rho\\cdot\\underset{s\\in S}{\\operatorname*{min}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)-\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)}\\\\ &{}&{=1+(1-\\rho)\\cdot\\left(\\underset{s\\in S}{\\operatorname*{max}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)-\\underset{s\\in S}{\\operatorname*{min}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)\\right)}\\\\ &{}&{\\quad+\\underset{s\\in S}{\\operatorname*{min}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)-\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)}\\\\ &{}&{\\leq1+(1-\\rho)\\cdot\\left(\\underset{s\\in S}{\\operatorname*{max}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)-\\underset{s\\in S}{\\operatorname*{min}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s)\\right),\\ \\ \\ \\ \\ }\\end{array}\\quad\\quad\\mathrm{(C.)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality uses (C.11) and the last inequality uses the following fact, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)=\\underset{(s,a)\\in S\\times A}{\\operatorname*{min}}\\left\\lbrace R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\right]\\right\\rbrace}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\underset{s\\in S}{\\operatorname*{min}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now applying the assumption that (C.8) holds at step $h+1$ to the right hand side of (C.12), we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{(s,a)\\in S\\times A}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)-\\operatorname*{min}_{(s,a)\\in S\\times A}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)\\leq1+\\displaystyle\\frac{1-\\rho}{\\rho}\\cdot\\Big(1-(1-\\rho)^{H-h}\\Big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{\\rho}\\cdot\\Big(1-(1-\\rho)^{H-h+1}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus given (C.8) at step $h+1$ , we can derive (C.7) at step $h$ . Now by noticing that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(s,a)\\in{\\mathcal{S}}\\times A}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)\\le\\operatorname*{min}_{s\\in{\\mathcal{S}}}V_{h,P^{\\star},\\Phi}^{\\pi}(s)\\le\\operatorname*{max}_{s\\in{\\mathcal{S}}}V_{h,P^{\\star},\\Phi}^{\\pi}(s)\\le\\operatorname*{max}_{(s,a)\\in{\\mathcal{S}}\\times A}Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we can conclude that (C.8) also holds at step $h$ . As a result, by an induction argument, we finish the proof of Proposition 2.6. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.3Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Proposition 4.2. We consider some fixed $(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]$ throughout proof. By Lemma C.1, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\left[V\\right]=\\underset{\\eta\\in\\mathbb{R}}{\\operatorname*{sup}}\\left\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[(\\eta-V)_{+}\\right]-\\frac{\\rho}{2}\\cdot\\bigg(\\eta-\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V(s)\\bigg)_{+}+\\eta\\right\\}}\\\\ &{\\quad=\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\left\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[(\\eta-V)_{+}\\right]-\\frac{\\rho}{2}\\cdot\\bigg(\\eta-\\underset{s\\in\\mathcal{S}}{\\operatorname*{min}}V(s)\\bigg)_{+}+\\eta\\right\\}}\\\\ &{\\quad=\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\bigg\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[(\\eta-V)_{+}\\right]+\\left(1-\\frac{\\rho}{2}\\right)\\cdot\\eta\\bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second equality follows from the fact the optimal dual variable $\\eta^{\\star}$ is in $[0,H]$ when $V\\in[0,H]$ (see e.g., Lemma $\\mathrm{H.8}$ in Blanchet et al. (2023)), and the last equality is obtained by the fact that $\\mathrm{min}_{s\\in{\\cal S}}\\:V(s)=0$ ", "page_idx": 23}, {"type": "text", "text": "Part (i). For any $\\eta\\in[0,H]$ and $Q\\in B_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\big[(\\eta-V)_{+}\\big]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\le\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\Big(-\\mathbb{E}_{Q(\\cdot)}\\big[(\\eta-V)_{+}\\big]+\\eta\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\Big(-\\mathbb{E}_{Q(\\cdot)}\\big[\\eta-V\\big]+\\eta\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\mathbb{E}_{Q(\\cdot)}\\big[V\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality uses the definition of $B_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})$ , the second equality follows from the fact that $(x)_{+}\\geq x$ . Furthermore, by (C.14) we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\eta\\in[0,H]}\\Bigg\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\big[(\\eta-V)_{+}\\big]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\Bigg\\}\\leq\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\operatorname*{inf}_{Q\\in B_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})}\\mathbb{E}_{Q(\\cdot)}\\big[V\\big]\\mathrm{C}_{P_{h}^{\\star}(Q_{\\star})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining (C.13) and (C.15), we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\left[V\\right]\\leq\\rho^{\\prime}\\cdot\\mathbb{E}_{\\mathcal{B}_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})}\\left[V\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Part (ii).Since $\\rho\\in[0,1]$ , we know that there exists a $\\widetilde{\\eta}\\in[0,H]$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{s^{\\prime}:V(s^{\\prime})<\\tilde{\\eta}}P_{h}^{\\star}(s^{\\prime}|s,a)\\leq1-\\frac{\\rho}{2}\\leq\\sum_{s^{\\prime}:V(s^{\\prime})\\leq\\tilde{\\eta}}P_{h}^{\\star}(s^{\\prime}|s,a),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which further implies that we have the following interpolation for some $\\lambda\\in[0,1]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-\\frac\\rho2=\\lambda\\sum_{s^{\\prime}:V(s^{\\prime})<\\widetilde{\\eta}}P_{h}^{\\star}(s^{\\prime}|s,a)+(1-\\lambda)\\sum_{s^{\\prime}:V(s^{\\prime})\\leq\\widetilde{\\eta}}P_{h}^{\\star}(s^{\\prime}|s,a).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We define a probability measure $\\widetilde{P}^{\\star}\\in\\Delta(S)$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{P}_{h}^{\\star}=\\frac{\\lambda P_{h}^{\\star}(s^{\\prime}|s,a)\\cdot\\mathbf{1}\\{V(s^{\\prime})>\\widetilde{\\eta}\\}+(1-\\lambda)P_{h}^{\\star}(s^{\\prime}|s,a)\\cdot\\mathbf{1}\\{V(s^{\\prime})\\geq\\widetilde{\\eta}\\}}{1-\\frac{\\rho}{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is not difficult to verify that $\\widetilde{P}_{h}^{\\star}\\in B_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})$ . Hence, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\displaystyle\\frac{\\rho}{2}\\right)\\cdot\\mathbb{E}_{\\mathcal{B}_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})}[V]\\leq\\left(1-\\displaystyle\\frac{\\rho}{2}\\right)\\cdot\\mathbb{E}_{\\widetilde{P}_{h}^{\\star}(\\cdot)}\\big[V\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left(1-\\displaystyle\\frac{\\rho}{2}\\right)\\cdot\\mathbb{E}_{\\widetilde{P}_{h}^{\\star}(\\cdot)}\\big[V-\\widetilde{\\eta}\\big]+\\left(1-\\displaystyle\\frac{\\rho}{2}\\right)\\cdot\\widetilde{\\eta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\big[(\\widetilde{\\eta}-V)_{+}\\big]+\\left(1-\\displaystyle\\frac{\\rho}{2}\\right)\\cdot\\widetilde{\\eta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality uses the definition of $\\widetilde{P}_{h}^{\\star}$ in (C.16). Furthermore, by (C.17) we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho^{\\prime}\\cdot\\mathbb{E}_{\\mathcal{B}_{\\rho^{\\prime}}(s,a;P_{h}^{\\star})}\\left[V\\right]\\leq\\operatorname*{sup}_{\\eta\\in[0,H]}\\bigg\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[(\\eta-V)_{+}\\right]+\\left(1-\\frac{\\rho}{2}\\right)\\cdot\\eta\\bigg\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\left[V\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the equality follows from (C.13). ", "page_idx": 24}, {"type": "text", "text": "Combining Part (i) and Part (i).  Finally, combining (C.15) and (C.18), we prove Proposition 4.2. ", "page_idx": 24}, {"type": "text", "text": "D Proofs for Hardness Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 3.2. We first explicitly give the expressions of the robust value functions in Example 3.1, based on which we derive the desired online regret lower bound. ", "page_idx": 24}, {"type": "text", "text": "Robust value function. Firstly, we can explicitly write down the expression of the robust value functions for anypolicy $\\pi$ under Example 3.1, i.e., $V_{h,P^{\\star}}^{\\pi},\\!M_{\\theta},\\Phi$ and h,p+,Me,\u03a6. From now on we fix a policy $\\pi$ ", "page_idx": 25}, {"type": "text", "text": "Forstep $h\\,=\\,3$ , the robust value function is the reward received. We can directly obtain for any $a\\in A$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{3,P^{\\star,\\mathcal{M}_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{good}},a)=V_{3,P^{\\star,\\mathcal{M}_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{good}})=1,}\\\\ &{\\ Q_{3,P^{\\star,\\mathcal{M}_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{bad}},a)=V_{3,P^{\\star,\\mathcal{M}_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{bad}})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Forstep $h=2$ , by the robust Bellman equation (Proposition 2.2), we have that for the good state $s_{\\mathrm{good}}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{2,P^{\\star},\\mathcal{M}_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}},a)}\\\\ &{\\qquad=1+\\operatorname*{inf}_{\\substack{P\\in\\mathcal{P}_{\\rho}(s_{\\mathrm{good}},a;P_{2}^{\\star},M_{\\theta})}}\\mathbb{E}_{P(\\cdot)}\\big[V_{3,P^{\\star},M_{\\theta},\\Phi}^{\\pi}\\big]=1+(1-\\rho),\\quad\\forall a\\in\\mathcal{A},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last equality is because V,p\\*.Mg ,4 takes the minimal value O at the bad state Sbad and thus the most adversarial transition distribution is achieved at ", "page_idx": 25}, {"type": "equation", "text": "$$\nP^{\\dagger}(s^{\\prime})=(1-\\rho)\\cdot{\\bf1}\\{s^{\\prime}=s_{\\mathrm{good}}\\}+\\rho\\cdot{\\bf1}\\{s^{\\prime}=s_{\\mathrm{bad}}\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, we have that for the bad state $s_{\\mathrm{bad}}$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{2,P^{\\star},\\mathcal{M}_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{bad}},a)=0+\\operatorname*{inf}_{\\substack{P\\in\\mathcal{P}_{\\rho}(s_{\\mathrm{bad}},a;P_{2}^{\\star,\\mathcal{M}_{\\theta}})}}\\mathbb{E}_{P(\\cdot)}\\big[V_{3,P^{\\star,\\mathcal{M}_{\\theta}},\\Phi}^{\\pi}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\bigg\\{\\!\\!\\!\\begin{array}{r l}{\\!\\!\\!p-\\rho,}&{\\ a=\\theta}\\\\ {\\!\\!\\!q-\\rho,}&{\\ a=1-\\theta}\\end{array}\\!\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally by the robust Bellman equation again, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2,P^{\\star},\\mathcal{M}_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}})=1+(1-\\rho),}\\\\ &{V_{2,P^{\\star},\\mathcal{M}_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{bad}})=\\pi_{2}(\\theta|s_{\\mathrm{bad}})\\cdot(p-\\rho)+\\pi_{2}(1-\\theta|s_{\\mathrm{bad}})\\cdot(q-\\rho).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notice that by $q<p$ we know that $V_{2,P^{\\star},M_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{bad}})<p-\\rho<1+(1-\\rho)<V_{2,P^{\\star},M_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}}).$ ", "page_idx": 25}, {"type": "text", "text": "Forstep $h=1$ , we consider the robust values on the initial state $s_{1}=s_{\\mathrm{good}}$ , by robust Bellman equation, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Q_{1,P^{\\star},M_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}},a)=1+\\operatorname*{inf}_{\\substack{P\\in\\mathcal{P}_{\\rho}(s_{\\mathrm{good}},a;P_{1}^{\\star},M_{\\theta})}}\\mathbb{E}_{P(\\cdot)}\\big[V_{2,P^{\\star},M_{\\theta},\\Phi}^{\\pi}\\big]}&{}&{\\mathrm{(D.4)}}\\\\ {=1+(1-\\rho)\\cdot\\big[1+(1-\\rho)\\big]}\\\\ &{}&{+\\,\\rho\\cdot\\big[\\pi_{2}(\\theta|s_{\\mathrm{bad}})\\cdot(p-\\rho)+\\pi_{2}(1-\\theta|s_{\\mathrm{bad}})\\cdot(q-\\rho)\\big],}&{\\forall a\\in\\mathcal{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By robust Bellman equation, we also derive $V_{1,P^{\\star},\\mathcal{M}_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}})=Q_{1,P^{\\star},\\mathcal{M}_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}},a)$ for $\\forall a\\in A$ ", "page_idx": 25}, {"type": "text", "text": "Lower bound the online regret under Example 3.1. With all the previous preparation, we can lower bound the online regret for robust RL with interactive data collection in Example 3.1. But first, we present the following general lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.1 (Performance difference lemma for robust value function). For any RMDP satisfying Assumption2.1andanypolicy $\\pi$ ,thefollowinginequalityholds, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{1,P^{\\star},\\Phi}^{\\pi^{\\star}}(s)-V_{1,P^{\\star},\\Phi}^{\\pi}(s)}}\\\\ {{\\qquad\\ge\\mathbb{E}_{(P^{\\pi^{\\star},\\dagger},\\pi^{\\star})}\\left[\\displaystyle\\sum_{h=1}^{H}\\sum_{a\\in\\mathcal{A}}\\left(\\pi_{h}^{\\star}(a|s_{h})-\\pi_{h}(a|s_{h})\\right)\\cdot Q_{h,P^{\\star},\\Phi}^{\\pi}(s_{h},a)\\middle|s_{1}=s\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the expectation is taken with respect to the trajectories induced by policy $\\pi^{\\star}$ ,transitionkernel $P^{\\pi^{\\star},\\dagger}$ .Here the transition kernel $P^{\\pi^{\\star},\\dagger}$ isdefined as ", "page_idx": 25}, {"type": "equation", "text": "$$\nP_{h}^{\\pi^{\\star},\\dagger}(\\cdot|s,a)=\\underset{P\\in\\mathcal{P}(s,a;P_{h}^{\\star})}{\\arg\\operatorname*{inf}}\\mathbb{E}_{P(\\cdot)}\\big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{\\star}}\\big],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathcal{P}(s,a;P_{h}^{\\star})$ is the robust set for state-action pair $(s,a)$ (see Assumption 2.1). ", "page_idx": 25}, {"type": "text", "text": "Now back to Example 3.1, our previous calculation actually shows that, by (D.1) for step $h=3$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{a\\in A}\\big(\\pi_{3}^{\\star,M_{\\theta}}(a|s_{3})-\\pi_{3}(a|s_{3})\\big)\\cdot Q_{3,P^{\\star,M_{\\theta}},\\Phi}^{\\pi}(s_{3},a)=0,\\quad\\forall s_{3}\\in\\{s_{\\mathrm{good}},s_{\\mathrm{bad}}\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and by (D.4) we also have that for step $h=1$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\mathcal{A}}\\big(\\pi_{1}^{\\star,M_{\\theta}}(a\\vert s_{1})-\\pi_{1}(a\\vert s_{1})\\big)\\cdot Q_{1,P^{\\star,M_{\\theta}},\\Phi}^{\\pi}(s_{1},a)=0,\\quad\\mathrm{where}\\quad s_{1}=s_{\\mathrm{good}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, let's consider step $h=2$ . By (D.2), we have that for the good state, it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\mathcal{A}}\\big(\\pi_{2}^{\\star,\\mathcal{M}_{\\theta}}(a|s_{\\mathrm{good}})-\\pi_{2}(a|s_{\\mathrm{good}})\\big)\\cdot Q_{2,P^{\\star,\\mathcal{M}_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{good}},a)=0,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Meanwhile, by (D.3), we have that for the bad state, it holds that (recall that $q<p$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{\\alpha\\in A}\\left(\\pi_{2}^{\\star,M_{\\theta}}(a|s_{\\mathrm{bad}})-\\pi_{2}(a|s_{\\mathrm{bad}})\\right)\\cdot Q_{2,P^{\\star,M_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{bad}},a)}\\\\ &{\\qquad=\\operatorname*{max}\\big\\{p-\\rho,q-\\rho\\big\\}-\\bigg(\\pi_{2}(\\theta|s_{\\mathrm{bad}})\\cdot(p-\\rho)+\\pi_{2}(1-\\theta|s_{\\mathrm{bad}})\\cdot(q-\\rho)\\bigg)}\\\\ &{\\qquad=p-\\rho-\\Big(\\pi_{2}(\\theta|s_{\\mathrm{bad}})\\cdot(p-\\rho)+\\pi_{2}(1-\\theta|s_{\\mathrm{bad}})\\cdot(q-\\rho)\\Big)}\\\\ &{\\qquad=\\frac{p-q}{2}\\cdot\\bigg(\\left|\\pi_{2}^{\\star,M_{\\theta}}(\\theta|s_{\\mathrm{bad}})-\\pi_{2}(\\theta|s_{\\mathrm{bad}})\\right|+\\left|\\pi_{2}^{\\star,M_{\\theta}}(1-\\theta|s_{\\mathrm{bad}})-\\pi_{2}(1-\\theta|s_{\\mathrm{bad}})\\right|\\bigg)}\\\\ &{\\qquad=(p-q)\\cdot D_{\\mathrm{TV}}\\left(\\pi_{2}^{\\star,M_{\\theta}}(\\cdot|s_{\\mathrm{bad}})\\right|\\bigg|\\pi_{2}(\\cdot|s_{\\mathrm{bad}})\\bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where according to (D.3) the optimal policy of $\\mathcal{M}_{\\theta}$ $h=2$ and $s_{\\mathrm{bad}}$ $\\pi_{2}^{\\star,\\mathcal{M}_{\\theta}}(\\theta|s_{\\mathrm{bad}})=1$ Now combining (D.5), (D.6), (D.7), and (D.8) with Lemma D.1, we can conclude that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{V_{1,P^{*},M\\theta,\\Phi}^{\\pi^{*},M_{\\theta}}(s_{\\mathrm{good}})-V_{1,P^{*},M_{\\theta},\\Phi}^{\\pi}(s_{\\mathrm{good}})}\\quad}&{}\\\\ &{\\geq\\mathbb{E}_{a_{1}\\sim\\pi_{1}^{*},M_{\\theta}}(.|s_{\\mathrm{good}}),s_{2}\\sim P_{1}^{\\pi^{*},M_{\\theta},\\dagger}(.|s_{\\mathrm{good}},a_{1})\\left[\\sum_{a\\in A}(\\pi_{2}^{\\star}(a|s_{2})-\\pi_{2}(a|s_{2}))\\cdot Q_{2,P^{\\star},M_{\\theta},\\Phi}^{\\pi}(s_{2},a_{1})\\right]}\\\\ &{=P_{1}^{\\pi^{*},M_{\\theta},\\dagger}(s_{\\mathrm{bad}}|s_{\\mathrm{good}},0)\\cdot(p-q)\\cdot D_{\\mathrm{TV}}\\left(\\pi_{2}^{\\star},M_{\\theta},.|s_{\\mathrm{bad}}\\right)\\bigg|\\bigg|\\pi_{2}(\\cdot|s_{\\mathrm{bad}})\\bigg)\\,,\\qquad\\qquad\\qquad\\quad(\\mathrm{D}.\\,)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the adversarial ransitionkel $P_{1}^{\\pi^{\\star,M_{\\theta}},\\dag}$ Mo ,t is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{1}^{\\pi^{\\star,M_{\\theta}},\\dagger}(\\cdot|s_{\\mathrm{good}},0)=\\underset{P\\in\\mathcal{P}(s_{\\mathrm{good}},0;P_{1}^{\\star,M_{\\theta}})}{\\mathrm{argmin}}\\mathbb{E}_{P(\\cdot)}\\Big[V_{2,P^{\\star,M_{\\theta}},\\Phi}^{\\pi^{\\star,M_{\\theta}}}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=(1-\\rho)\\cdot\\mathbf{1}\\{\\cdot=s_{\\mathrm{good}}\\}+\\rho\\cdot\\mathbf{1}\\{\\cdot=s_{\\mathrm{bad}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consequently, taking (D.10) back into (D.9), we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\nV_{1,P^{\\star,M_{\\theta}},\\Phi}^{\\pi^{\\star,M_{\\theta}}}(s_{\\mathrm{good}})-V_{1,P^{\\star,M_{\\theta}},\\Phi}^{\\pi}(s_{\\mathrm{good}})\\geq\\rho\\cdot(p-q)\\cdot D_{\\mathrm{TV}}\\left(\\pi_{2}^{\\star,M_{\\theta}}(\\cdot|s_{\\mathrm{bad}})\\Big|\\Big|\\pi_{2}(\\cdot|s_{\\mathrm{bad}})\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This implies that for any algorithm executing $\\pi^{1},\\cdots,\\pi^{K}$ , its online regret is lower bounded by the following, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}_{\\Phi}^{\\mathcal{M}_{\\theta},\\mathcal{A}\\mathcal{L}\\mathcal{G}}(K)=\\displaystyle\\sum_{k=1}^{K}{V_{1,P^{\\star},M_{\\theta}}^{\\pi^{\\star},M_{\\theta}}},\\Phi^{(s_{\\mathrm{good}})}-{V_{1,P^{\\star},M_{\\theta}}^{\\pi^{k}}},\\Phi^{(s_{\\mathrm{good}})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\rho\\cdot(p-q)\\cdot\\displaystyle\\sum_{k=1}^{K}D_{\\mathrm{TV}}\\left(\\pi_{2}^{\\star,M_{\\theta}}(\\cdot|s_{\\mathrm{bad}})\\right|\\Big|\\pi_{2}^{k}(\\cdot|s_{\\mathrm{bad}})\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "However, since in RMDPs of Example 3.1, the online interaction process is always kept in $s_{\\mathrm{good}}$ and there is no information on $\\theta$ which can only be accessed at $\\bar{(s,h)}=(s_{\\mathrm{bad}},2)$ . As a result, the estimates $\\pi_{2}^{k}(\\cdot|s_{\\mathrm{bad}})$ $\\pi_{2}^{\\star,\\mathcal{M}_{\\theta}}(\\cdot|s_{\\mathrm{bad}})=\\mathbf{1}\\{\\cdot=\\theta\\}$ can do no better than a random guess. Put it formally, consider that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\theta\\in\\{0,1\\}}\\mathbb{E}_{\\mathcal{M}_{\\theta},\\mathcal{A}\\mathcal{L}\\mathcal{G}}\\left[\\mathrm{Regret}_{\\Phi}^{\\mathcal{M}_{\\theta},\\mathcal{A}\\mathcal{L}\\mathcal{G}}(K)\\right]}\\\\ &{\\displaystyle\\qquad\\geq\\rho\\cdot(p-q)\\cdot\\operatorname*{sup}_{\\theta\\in\\{0,1\\}}\\mathbb{E}_{\\mathcal{M}_{\\theta},\\mathcal{A}\\mathcal{L}\\mathcal{G}}\\left[\\displaystyle\\sum_{k=1}^{K}D_{\\mathrm{TV}}\\left(\\pi_{2}^{\\star,\\mathcal{M}_{\\theta}}(\\cdot|s_{\\mathrm{bad}})\\right|\\Big|\\pi_{2}^{k}(\\cdot|s_{\\mathrm{bad}})\\right)\\right]}\\\\ &{\\displaystyle\\qquad=\\rho\\cdot(p-q)\\cdot\\operatorname*{sup}_{\\theta\\in\\{0,1\\}}\\sum_{k=1}^{K}\\mathbb{E}_{\\mathcal{A}\\mathcal{L}\\mathcal{G}}\\left[\\pi_{2}^{k}(1-\\theta|s_{\\mathrm{bad}})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here in the last equality we can drop the subscription of $\\mathcal{M}_{\\theta}$ because the algorithm outputs $\\pi_{2}^{k}$ independentofthe $\\theta$ due to our previous discussion. Notice that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\{0,1\\}}\\sum_{k=1}^{K}\\mathbb{E}_{A\\mathcal{L}\\mathcal{G}}\\left[\\pi_{2}^{k}(1-\\theta|s_{\\mathrm{bad}})\\right]=\\sum_{k=1}^{K}\\sum_{\\theta\\in\\{0,1\\}}\\mathbb{E}_{A\\mathcal{L}\\mathcal{G}}\\left[\\pi_{2}^{k}(1-\\theta|s_{\\mathrm{bad}})\\right]=\\sum_{k=1}^{K}1=K,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which further indicates that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\{0,1\\}}\\sum_{k=1}^{K}\\mathbb{E}_{\\mathcal{A L G}}\\left[\\pi_{2}^{k}(1-\\theta|s_{\\mathrm{bad}})\\right]\\geq\\frac{K}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, by combining (D.11) and (D.12), we conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\mathcal{L G}}\\operatorname*{sup}_{\\theta\\in\\{0,1\\}}\\mathbb{E}_{\\mathcal{M}_{\\theta},\\mathcal{A L G}}\\left[\\mathrm{Regret}_{\\Phi}^{\\mathcal{M}_{\\theta},\\mathcal{A L G}}(K)\\right]\\geq(p-q)\\cdot\\frac{\\rho K}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This is the desired online regret lower bound of $\\Omega(\\rho\\cdot K)$ for the RMDPs presented in Example 3.1. Furthermore, we can construct two RMDPs $\\{\\widetilde{\\mathcal{M}}_{0},\\widetilde{\\mathcal{M}}_{1}\\}$ with horizon $3H$ by concatenating $H$ RMDPs $\\{\\mathcal{M}_{0},\\mathcal{M}_{1}\\}$ presented in Examle 3.1. Notably,at any stes $\\{3i+1\\}_{i=0}^{H-1}$ , we define ", "page_idx": 27}, {"type": "equation", "text": "$$\nR_{3i+1}(s_{\\mathrm{bad}},a)=1,\\qquad P_{3i+1}^{\\star,\\widetilde{\\mathcal{M}}_{\\theta}}(s_{\\mathrm{good}}|s_{\\mathrm{bad}},a)=1,\\quad\\forall(a,\\theta)\\in\\mathcal{A}\\times\\{0,1\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\mathcal{L G}}\\operatorname*{sup}_{\\theta\\in\\{0,1\\}}\\mathbb{E}_{\\widetilde{\\mathcal{M}}_{\\theta},A\\mathcal{L G}}\\left[\\mathrm{Regret}_{\\Phi}^{\\widetilde{\\mathcal{M}}_{\\theta},A\\mathcal{L G}}(K)\\right]\\geq H\\cdot\\Omega(\\rho\\cdot K)=\\Omega(\\rho\\cdot H K),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which completes the proof of Theorem 3.2 ", "page_idx": 27}, {"type": "text", "text": "D.2  Proof of Lemma D.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Lemma $D.I$ . For any step $h\\,\\in\\,[H]$ , we have that by robust Bellman equation (Proposition 2.2), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{h,P^{\\star},\\Phi}^{\\pi^{\\star}}(s,a)-Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)=\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{\\star}}\\big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By the definition of the transition kernel $P^{\\pi^{\\star},\\dagger}$ in Lemma D.1 and the property of infmum, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{h,P^{\\star},\\Phi}^{\\pi^{\\star}}(s,a)-Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)\\geq\\mathbb{E}_{P_{h}^{\\pi^{\\star},\\dag}(\\cdot\\vert s,a)}\\big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{\\star}}\\big]-\\mathbb{E}_{P_{h}^{\\pi^{\\star},\\dag}(\\cdot\\vert s,a)}\\big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{P_{h}^{\\pi^{\\star},\\dag}(\\cdot\\vert s,a)}\\big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{\\star}}-V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By robust Bellman equation (Proposition 2.2) and (D.13), we further obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h,P^{\\star},\\Phi}^{\\pi^{\\star}}(s)-V_{h,P^{\\star},\\Phi}^{\\pi}(s)=\\mathbb{E}_{\\pi_{h}^{\\star}(\\cdot\\vert s)}\\bigl[Q_{h,P^{\\star},\\Phi}^{\\pi^{\\star}}(s,\\cdot)\\bigr]-\\mathbb{E}_{\\pi_{h}(\\cdot\\vert s)}\\bigl[Q_{h,P^{\\star},\\Phi}^{\\pi}(s,\\cdot)\\bigr]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\pi_{h}^{\\star}(\\cdot\\vert s)}\\bigl[Q_{h,P^{\\star},\\Phi}^{\\pi}(s,\\cdot)\\bigr]-\\mathbb{E}_{\\pi_{h}(\\cdot\\vert s)}\\bigl[Q_{h,P^{\\star},\\Phi}^{\\pi}(s,\\cdot)\\bigr]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\mathbb{E}_{\\pi_{h}^{\\star}(\\cdot\\vert s)}\\bigl[Q_{h,P^{\\star},\\Phi}^{\\pi^{\\star}}(s,\\cdot)\\bigr]-\\mathbb{E}_{\\pi_{h}^{\\star}(\\cdot\\vert s)}\\bigl[Q_{h,P^{\\star},\\Phi}^{\\pi}(s,\\cdot)\\bigr]}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\sum_{a\\in A}\\Big(\\pi_{h}^{\\star}(a\\vert s)-\\pi_{h}(a\\vert s)\\Big)\\cdot Q_{h,P^{\\star},\\Phi}^{\\pi}(s,a)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathbb{E}_{a\\sim\\pi_{h}^{\\star}(\\cdot\\vert s),P_{h}^{\\pi^{\\star},\\P}(\\cdot\\vert s,a)}\\bigl[V_{h,P^{\\star},\\Phi}^{\\pi}-V_{h,P^{\\star},\\Phi}^{\\pi}\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus by recursively applying (D.14) over $h\\in[H]$ , we can conclude that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{1,P^{\\star}\\star,\\Phi}^{\\pi^{\\star}}(s)-V_{1,P^{\\star},\\Phi}^{\\pi}(s)}}\\\\ {{\\qquad\\ge\\mathbb{E}_{(P^{\\pi^{\\star},\\dagger},\\pi^{\\star})}\\left[\\displaystyle\\sum_{h=1}^{H}\\sum_{a\\in\\mathcal{A}}\\left(\\pi_{h}^{\\star}(a|s_{h})-\\pi_{h}(a|s_{h})\\right)\\cdot Q_{h,P^{\\star},\\Phi}^{\\pi}(s_{h},a)\\middle|s_{1}=s\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which completes the proof of Lemma D.1. ", "page_idx": 28}, {"type": "text", "text": "E Proofs for Theoretical Analysis of OPROVI-TV ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we prove our main theoretical results (Theorem 4.3). In Appendix E.1, we outline the proof of the theorem. In Appendix E.2, we list all the key lemmas used in the proof of the theorem. We defer the proof of all the lemmas to subsequent sections (Appendices E.3 to E.8). ", "page_idx": 28}, {"type": "text", "text": "Before presenting all the proofs, we define the typical event $\\mathcal{E}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}=\\Bigg\\{\\Bigg|\\left(\\mathbb{E}_{P_{h}^{k}(\\cdot\\vert s,a)}-\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\right)\\left[(\\eta-V_{h+1,P^{*},\\Phi}^{*})_{+}\\right]\\Bigg|}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{\\mathbb{V}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[(\\eta-V_{h+1,P^{*},\\Phi}^{*})_{+}\\right]\\cdot c_{1}l}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}H t}{N_{h}^{k}(s,a)\\vee1},}\\\\ &{\\qquad\\Big\\vert P_{h}^{*}(s^{\\prime}\\vert s,a)-\\hat{P}_{h}(s^{\\prime}\\vert s,a)\\Big\\vert\\leq\\sqrt{\\frac{\\operatorname*{min}\\left\\{P_{h}^{*}(s^{\\prime}\\vert s,a),\\hat{P}_{h}^{k}(s^{\\prime}\\vert s,a)\\right\\}\\cdot c_{1}l}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}\\ell}{N_{h}^{k}(s,a)\\vee1},}\\\\ &{\\qquad\\qquad\\forall(s,a,s^{\\prime},h,k)\\in\\mathcal{S}\\times A\\times\\mathcal{S}\\times[H]\\times[K],\\ \\forall\\eta\\in\\mathcal{N}_{1/(S\\times\\mathcal{H})}([0,H])\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta).$ $c_{1},c_{2}>0$ are two absolute constants, $\\mathcal{N}_{1/S\\sqrt{K}}([0,H])$ denotes an $1/S\\sqrt{K}$ -cover of the interval $[0,H]$ ", "page_idx": 28}, {"type": "text", "text": "Lemma E.1 (Typical event). For the typical event $\\mathcal{E}$ defined in (E.35), it holds that $\\mathbb{P}(\\boldsymbol{\\mathcal{E}})\\ge1-\\boldsymbol{\\delta}$ ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma E. 1. This is a direct application of Bernstein inequality and its empirical version (Maurer and Pontil, 2009), together with a union bound over $(s,a,s^{\\bar{\\prime}},h,\\dot{k},\\eta)\\in\\mathcal{S}\\times\\bar{\\mathcal{A}}\\times\\mathcal{S}\\times[H]\\times$ $[K]\\times\\mathcal{N}_{1/(S\\sqrt{K})}([0,H])$ . Note that the size of $\\mathcal{N}_{1/(S\\sqrt{K})}([0,H])$ is of order $S H\\sqrt{K}$ \u53e3 ", "page_idx": 28}, {"type": "text", "text": "In this section, we always let the event $\\mathcal{E}$ hold, which by Lemma E.1 is of probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "text", "text": "E.1 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 4.3. With Lemma E.2 (optimism and pessimism), we upper bound the regret as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Regret}_{\\Phi}(K)=\\sum_{k=1}^{K}V_{1,P^{\\star},\\Phi}^{\\star}(s_{1})-V_{1,P^{\\star},\\Phi}^{\\pi^{k}}(s_{1})\\leq\\sum_{k=1}^{K}\\overline{{V}}_{1}^{k}(s_{1})-\\underline{{V}}_{1}^{k}(s_{1}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the sequel, we break our proof into three steps. ", "page_idx": 28}, {"type": "text", "text": "Step 1: uper bouding (E1).Acording to the choic f $\\overline{{Q}}_{h}^{k},\\underline{{Q}}_{h}^{k},\\overline{{V}}_{h}^{k},\\underline{{V}}_{h}^{k}$ in (4.2), 4.3),and (4.4), let's consider that for any $(h,k)\\in[H]\\times[K]$ and $(s,a)\\in S\\times{\\dot{A}}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{Q}}_{h}^{k}(s,a)-\\underline{{Q}}_{h}^{k}(s,a)}\\\\ &{\\qquad=\\operatorname*{min}\\bigg\\{R_{h}(s,a)+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]+\\mathfrak{b o n u s}_{h}^{k}(s,a),\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\}\\bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\\\\\\\\\}-\\operatorname*{max}\\bigg\\{R_{h}(s,a)+{\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\overline{{V}}_{h+1}^{k}\\bigg]-\\mathtt{b o n u s}_{h}^{k}(s,a),\\,0\\bigg\\}}\\\\ &{\\le{\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\overline{{V}}_{h+1}^{k}\\bigg]-{\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\underline{{V}}_{h+1}^{k}\\bigg]+2\\cdot\\mathtt{b o n u s}_{h}^{k}(s,a)}\\\\ &{={\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\overline{{V}}_{h+1}^{k}\\bigg]-{\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{k})}\\bigg[\\overline{{V}}_{h+1}^{k}\\bigg]+{\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{k})}\\bigg[\\underline{{V}}_{h+1}^{k}\\bigg]-{\\mathbb E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\underline{{V}}_{h+1}^{k}\\bigg]\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{+\\underbrace{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]}_{\\mathrm{Term~(ii)}}\\,+\\,2\\cdot\\mathfrak{b o n u s}_{h}^{k}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Step 1.1: upper bounding Term (i). _ By using a Bernstein-style concentration argument customized for TV robust expectations (Lemma E.3), we can bound Term (i) by the bonus function, i.e., ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Term}\\;(\\mathrm{i})\\leq2\\cdot\\mathsf{b o n u s}_{h}^{k}(s,a).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Step 1.2: upper bounding Term (i). By our defnition of the operator $\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}[V]$ in (4.1), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Term~(ii)}=\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Bigg[\\Big(\\eta-\\overline{{V}}_{h+1}^{k}\\Big)_{+}\\Bigg]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\Bigg\\}}\\\\ &{\\qquad\\quad\\quad-\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Bigg[\\Big(\\eta-\\underline{{V}}_{h+1}^{k}\\Big)_{+}\\Bigg]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\Bigg\\}}\\\\ &{\\quad\\leq\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Bigg[\\Big(\\eta-\\underline{{V}}_{h+1}^{k}\\Big)_{+}-\\Big(\\eta-\\overline{{V}}_{h+1}^{k}\\Big)_{+}\\Bigg]\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma E.2 which showsthat $\\overline{{V}}_{h+1}^{k}\\geq\\underline{{V}}_{h+1}^{k}$ and the fct that $(\\eta-x)_{+}-(\\eta-y)_{+}\\leq y-x$ for any $y>x$ , we can further upper bound the right hand side of (E.4) by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Term~(ii)}\\leq\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Step 1.3: combining the upper bounds. Now combining (E.3) and (E.5) with (E.2), we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{Q}}_{h}^{k}(s,a)-\\underline{{Q}}_{h}^{k}(s,a)\\leq\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]+4\\cdot\\mathsf{b o n u s}_{h}^{k}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma E.4, we can upper bound the bonus function, and after rearranging terms we further obtain that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{Q}}_{h}^{k}(s,a)-\\underline{{Q}}_{h}^{k}(s,a)\\leq\\bigg(1+\\displaystyle\\frac{12}{H}\\bigg)\\cdot\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle4\\sqrt{\\frac{\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\displaystyle\\frac{4c_{2}H^{2}S\\iota}{N_{h}^{k}(s,a)\\vee1}+\\displaystyle\\frac{4}{\\sqrt{K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $c_{1},c_{2}>0$ are two absolute constants. For the sake of brevity, we introduce the following notations of differences, for any $(h,k)\\in[H]\\times[K]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{h}^{k}:=\\overline{{V}}_{h}^{k}(s_{h}^{k})-\\underline{{V}}_{h}^{k}(s_{h}^{k}),}\\\\ &{\\zeta_{h}^{k}:=\\Delta_{h}^{k}-\\Big(\\overline{{Q}}_{h}^{k}(s_{h}^{k},a_{h}^{k})-\\underline{{Q}}_{h}^{k}(s_{h}^{k},a_{h}^{k})\\Big),}\\\\ &{\\xi_{h}^{k}:=\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[\\overline{{V}}_{h}^{k}-\\underline{{V}}_{h}^{k}\\Big]-\\Delta_{h+1}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If we further define the fltration {Fh,k}(h,k)E[H]x[K] as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{F}_{h,k}=\\sigma\\left(\\{\\left(s_{i}^{\\tau},a_{i}^{\\tau}\\right)\\}_{(i,\\tau)\\in[H]\\times[k-1]}\\bigcup\\{\\left(s_{i}^{k},a_{i}^{k}\\right)\\}_{i\\in[h-1]}\\bigcup\\{s_{h}^{k}\\}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "then we can find that $\\left\\{\\zeta_{h}^{k}\\right\\}_{(h,k)\\in[H]\\times[K]}$ is a martingale difference sequence with respect to $\\{{\\mathcal{F}}_{h,k}\\}_{(h,k)\\in[H]\\times[K]}$ and $\\left\\{\\xi_{h}^{k}\\right\\}_{(h,k)\\in[H]\\times[K]}$ is a martingale difference sequence with respect to $\\{{\\mathcal{F}}_{h,k}\\cup\\{a_{h}^{k}\\}\\}_{(h,k)\\in[H]\\times[K]}$ Also, we further have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{h}^{k}=\\zeta_{h}^{k}+\\left(\\overline{{Q}}_{h}^{k}(s_{h}^{k},a_{h}^{k})-\\underline{{Q}}_{h}^{k}(s_{h}^{k},a_{h}^{k})\\right)}\\\\ &{\\quad\\leq\\zeta_{h}^{k}+\\left(1+\\frac{12}{H}\\right)\\cdot\\mathbb{E}_{P_{h}^{k}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}\\\\ &{\\qquad\\quad+4\\sqrt{\\frac{\\mathbb{V}_{P_{h}^{k}(\\cdot\\vert s,a)}\\Big[{V}_{h+1,P^{k},\\Phi}^{\\varepsilon^{k}}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}}+\\frac{4c_{2}H^{2}S t}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}+\\frac{4}{\\sqrt{K}}}\\\\ &{\\quad=\\zeta_{h}^{k}+\\left(1+\\frac{12}{H}\\right)\\cdot\\xi_{h}^{k}+\\left(1+\\frac{12}{H}\\right)\\cdot\\Delta_{h+1}^{k}+}\\\\ &{\\qquad\\quad+\\frac{\\mathbb{V}_{P_{h}^{k}(\\cdot\\vert s,a)}\\Big[{V}_{h+1,P^{k},\\Phi}^{\\varepsilon^{k}}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}+\\frac{4c_{2}H^{2}S t}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}+\\frac{4}{\\sqrt{K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the inequality applies (E.6). Recursively applying (E.9) and using the fact that $\\begin{array}{r}{(1+\\frac{12}{H})^{h}\\leq}\\end{array}$ $\\begin{array}{r}{(1+\\frac{12}{H})^{H}\\leq c}\\end{array}$ for some absolute constant $c>0$ , we can upper bound the right hand side of (E.1) as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{Regret}_{\\Phi}(K)\\leq\\displaystyle\\sum_{k=1}^{K}\\Delta_{1}^{k}\\leq C_{1}\\cdot\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}(\\zeta_{h}^{k}+\\xi_{h}^{k})}\\\\ {\\qquad\\qquad\\qquad\\qquad+\\sqrt{\\frac{\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\cdot\\iota}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}+\\frac{H^{2}S l}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}+\\frac{1}{\\sqrt{K}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $C_{1}>0$ is an absolute constant. ", "page_idx": 30}, {"type": "text", "text": "Step 2: controlling the summation of variance terms. In view of (E.10), it suffices to upper bound its right hand side. The key difficulty is the analysis of the summation of the variance terms, which we focus on now. By Cauchy-Schwartz inequality, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sqrt{\\frac{\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}}}\\\\ &{}&{\\leq\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\cdot\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "On the right hand side of (E.11), the summation of the inverse of the count function is a well bounded term (Lemma E.13). So the key is to upper bound the the summation of the variance of the robust value functions to obtain a sharp bound. To this end, we invoke Lemma E.5 to obtain that with probability atleast $1-\\delta$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\!\\!\\nabla_{P_{h}^{\\star}(\\cdot|s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]}\\quad}&{}\\\\ &{\\leq C_{2}\\cdot\\Big(\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\}\\cdot H K+\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\}^{3}\\cdot H\\iota\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $C_{2}>0$ is an absolute constant. With inequality (E.12) and Lemma E.13 that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}\\leq C_{2}^{\\prime}\\cdot H S A\\iota,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With $C_{2}^{\\prime}>0$ being another constant, we can upper bound the summation of the variance terms (E.11) as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sqrt{\\frac{\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\leq C_{3}\\sqrt{\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\cdot H^{2}S A K\\iota+\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}^{3}\\cdot H^{2}S A\\iota^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{3}>0$ is also an absolute constant. ", "page_idx": 31}, {"type": "text", "text": "Step 3: finishing the proof. With (E.10) and (E.13), it suffices to control the remaining terms. For the summation of the martingale difference terms, notice that by the definitions in (E.7) and (E.8),both $\\zeta_{h}^{k}$ and $\\xi_{h}^{k}$ are bounded by $\\operatorname*{min}\\{H,\\rho^{-1}\\}$ according to (4.2)and Lemma E.2(optimism and pessimism). As a result, using Azuma-Hoeffding inequality, with probability at least $1-\\delta$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}(\\zeta_{h}^{k}+\\xi_{h}^{k})\\leq C_{4}\\cdot\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\cdot\\sqrt{H K\\iota},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{4}>0$ is an absolute constant. For the summation of the inverse of the count function in (E.10), it suffices to invoke again Lemma E.13. Combining all together, with probability at least $1-3\\delta$ ,wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}_{\\Phi}(K)\\leq C_{5}\\cdot\\left(\\sqrt{\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\cdot H^{2}S A K\\iota^{2}+\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}^{3}\\cdot H^{2}S A\\iota^{3}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\cdot\\sqrt{H K\\iota}+H^{3}S^{2}A\\iota^{2}+H\\sqrt{K}\\right)}\\\\ &{\\qquad=\\mathcal{O}\\left(\\sqrt{\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}\\cdot H^{2}S A K\\iota^{\\prime}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{5}~>~0$ is an absolute constant and $\\iota^{\\prime}\\,=\\,\\log^{2}(S A H K/\\delta)$ . This completes the proof of Theorem 4.3. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "E.2  Key Lemmas ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma E.2 (Optimistic and pessimistic estimation of the robust values). By setting the bonus $\\mathbf{\\Sigma}_{h}^{k}$ as in (4.5), then under the typical event $\\mathcal{E}$ itholdsthat ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underline{{Q}}_{h}^{k}(s,a)\\leq Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a)\\leq Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)\\leq\\overline{{Q}}_{h}^{k}(s,a),}\\\\ &{\\underline{{V}}_{h}^{k}(s)\\leq V_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s)\\leq V_{h,P^{\\star},\\Phi}^{\\star}(s)\\leq\\overline{{V}}_{h}^{k}(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for any $(s,a,h,k)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]\\times[K]$ ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $E.2$ .See Appendix E.3 for a detailed proof. ", "page_idx": 31}, {"type": "text", "text": "Lemma E.3 (Proper bonus for TV robust sets and optimistic and pessimistic value estimators). By Setting thebonus $\\mathbf{\\Psi}_{h}^{\\hat{k}}$ as in (4.5), then under the typical event $\\mathcal{E}$ it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]}\\\\ &{\\qquad\\leq2\\cdot\\mathrm{bonus}_{h}^{k}(s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $E.3$ .See Appendix E.4 for a detailed proof. ", "page_idx": 31}, {"type": "text", "text": "Lemma E.4 (Control of the bonus term). Under the typical event $\\mathcal{E}$ thebonus $\\mathbf{\\Sigma}_{h}^{k}$ in (4.5) is bounded by ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "bonush(s, a) $$\n\\begin{array}{r l}&{\\mathsf{0o n u s}_{h}^{\\mathsf{\\Delta}}(s,a)}\\\\ &{\\qquad\\le\\sqrt{\\frac{\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{\\star}}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{4\\cdot\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]}{H}+\\frac{c_{2}H^{2}S\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ and $c_{1},c_{2}>0$ are absolute constants. ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $E_{\\cdot}$ 4. See Appendix E.5 for a detailed proof. ", "page_idx": 31}, {"type": "text", "text": "Lemma E.5 (Total variance law for robust MDP with TV robust sets). With probability at least $1-\\delta$ the following inequality holds ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\le c_{3}\\cdot\\Big(\\operatorname*{min}\\{H,\\rho^{-1}\\}\\cdot H K+\\operatorname*{min}\\{H,\\rho^{-1}\\}^{3}\\cdot H\\iota\\Big).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ and $c_{3}>0$ is an absolute constant. ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma E.5. See Appendix E.6 for a detailed proof. ", "page_idx": 32}, {"type": "text", "text": "E.3Proof of Lemma E.2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof of Lemma E.2. We prove Lemma E.2 by induction. Suppose the conclusion (E.14) holds at step $h+1$ .For step $h$ , let's first consider the robust $Q$ function part. Specifically, by using the robust Bellman optimal equation (Proposition 2.3) and (4.2), we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)-\\overline{Q}_{h}^{k}(s,a)}\\\\ &{\\quad\\leq\\operatorname*{max}\\bigg\\{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathsf{b o n u s}_{h}^{k}(s,a),}\\end{array}}\\\\ &{\\begin{array}{r l}&{Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)-\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\}\\bigg\\}}\\\\ &{\\quad\\leq\\operatorname*{max}\\bigg\\{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]-\\mathsf{b o n u s}_{h}^{k}(s,a),\\,0\\bigg\\},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality follows from the induction of $V_{h+1,P^{\\star},\\Phi}^{\\star}\\le\\overline{{V}}_{h+1}^{k}$ atstep $h+1$ and the fact that $Q_{h,P^{\\star},\\Phi}^{\\star}\\leq\\operatorname*{min}\\{H,\\rho^{-1}\\}$ (by Proposition 2.6 and Assumption 4.1). By Lemma E.7, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]}\\\\ &{\\quad\\quad\\leq\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{c_{2}H\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now by further applying Lemma E.11 to the variance term in the above inequality, we can obtain that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ \\ \\frac{\\mathbb{E}_{P_{k}}(s,t,s_{k})\\mathbb{E}_{P_{k}}\\left[V_{k+1}^{k},r_{k}\\ e^{-\\mathrm{i}}\\mathbb{E}_{P_{k}(s,t,\\mathcal{F}_{k}^{k})}\\left[V_{k+1}^{k},r_{k},e^{-\\mathrm{i}}\\right]\\right.}{\\left.\\times\\left.\\sqrt{\\frac{V_{k}}{R_{k}}(s,t)}\\sqrt{\\left(V_{k+1}^{k}+\\frac{V_{k}^{k}}{V_{k+1}^{k}}\\right)/2}\\right]+4H\\cdot\\mathbb{E}_{P_{k}(t,s)}\\left[V_{k+1}^{k}-\\frac{V_{k+1}^{k}}{V_{k+1}^{k}}\\right]\\right)\\cdot c_{k}\\eta}}\\\\ {\\leq}&{\\sqrt{\\frac{C\\eta\\mathbb{E}_{P_{k}}}{N_{k}^{1}(s,t)}+\\frac{1}{V_{k}^{k}(s,t)}}}\\\\ &{\\ \\ \\ \\ +\\frac{C\\eta\\mathbb{E}_{P_{k}}}{N_{k}^{1}(s,t)}+\\frac{\\sqrt{W}}{\\sqrt{N_{k}}}}\\\\ {\\leq}&{\\sqrt{\\frac{\\mathbb{V}_{P_{k}(s,t)}\\left[\\left(V_{k+1}^{k}+\\frac{V_{k+1}^{k}}{V_{k}}\\right)/2\\right]\\cdot c_{k}\\eta}{N_{k}^{1}(s,t)\\sqrt{1}}+\\sqrt{\\frac{\\mathbb{E}_{P_{k}(t,s)}\\left[V_{k+1}^{k}-\\frac{V_{k+1}^{k}}{V_{k}}\\right]\\cdot4H_{C_{k}}}{N_{k}^{1}(s,t)\\sqrt{1}}}}}\\\\ &{\\ \\ \\ \\ +\\frac{C\\eta\\tilde{H}}{N_{k}^{1}(s,t)}+\\frac{1}{\\sqrt{N}}}\\\\ {\\leq}&{\\sqrt{\\frac{\\mathbb{V}_{P_{k}(s,t)}\\left[\\left(V_{k+1}^{k}+\\frac{V_{k+1}^{k}}{V_{k}}\\right)/2\\right]\\cdot c_{k}\\eta}{N_{k}^{1}(s,t)\\sqrt{1}}+\\frac{\\mathbb{E}_{P_{k}(t,s)}\\left[V_{k+1}^{k}-\\frac{V_{k+1}^{k}}{V_{k}}\\right]}{H}}}\\\\ &{\\ \\ \\ \\ + \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the first inequality is due to Lemma E.11, the second inequality is due to ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}},$ and the last inequality is from ${\\sqrt{a b}}\\,\\leq\\,a+b$ where $c_{2}^{\\prime}\\,>\\,0$ is an absolute constant. Therefore, combining (E.15) and (E.16), and the choice of bonus $_{h}^{k}(s,a)$ in (4.5), we can conclude that ", "page_idx": 33}, {"type": "equation", "text": "$$\nQ_{h,P^{\\star},\\Phi}^{\\star}(s,a)\\leq\\overline{{{Q}}}_{h}^{k}(s,a).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore, it holds that $Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a)\\,\\leq\\,Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)$ . Thus it reduces to prove $\\underline{{Q}}_{h}^{k}(s,a)\\leq$ $Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a)$ . Again, by using the robust Bellman equation (Proposition 2.2) and (4.3), we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underline{{Q}}_{h}^{k}(s,a)-Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a)}\\\\ &{\\qquad\\leq\\operatorname*{max}\\bigg\\{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[{V}_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]-\\mathsf{b o n u s}_{h}^{k}(s,a),0\\bigg\\}}\\\\ &{\\qquad\\quad0-Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a)\\bigg\\}}\\\\ &{\\qquad\\leq\\operatorname*{max}\\bigg\\{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[{V}_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[{V}_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]-\\mathsf{b o n u s}_{h}^{k}(s,a),0\\bigg\\}\\mathbb{E}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the second inequality follows from th inductionf $\\underline{{V}}_{h+1}^{k}\\leq V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}$ at step $h+1$ and the fact that $Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}\\geq0$ . By Lemma E.8, we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\Big[V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{*})}\\Big[V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\Big]}\\\\ &{\\qquad\\le\\sqrt{\\frac{\\mathbb{V}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{*},\\Phi}^{\\star}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}{H}+\\frac{c_{2}^{\\prime}H^{2}S t}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now by applying Lemma E.11 to the variance term, with an argument similar to (E.16), we can obtainthat ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]}\\\\ &{\\qquad\\le\\sqrt{\\frac{\\mathbb{V}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{2\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}{H}}\\\\ &{\\qquad\\qquad+\\,\\frac{c_{2}^{\\prime\\prime}H^{2}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus by combining (E.17) and (E.18), and the choice of bonus $_{h}^{k}(s,a)$ in (4.5), we can conclude that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\underline{{Q}}_{h}^{k}(s,a)\\leq Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we have proved that at step $h$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\underline{{Q}}_{h}^{k}(s,a)\\leq Q_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,a)\\leq Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)\\leq\\overline{{Q}}_{h}^{k}(s,a).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally for the robust $V$ function part, consider that by robust Bellman equation (Proposition 2.2) and (4.4), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{V}}_{h}^{k}(s)=\\mathbb{E}_{\\pi_{h}^{k}(\\cdot\\vert s)}\\Big[\\underline{{Q}}_{h}^{k}(s,\\cdot)\\Big]\\leq\\mathbb{E}_{\\pi_{h}^{k}(\\cdot\\vert s)}\\Big[\\underline{{Q}}_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s,\\cdot)\\Big]=V_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and that by robust Bellman optimal equation (Proposition 2.3), the choice of $\\pi^{k}$ , and (4.4), ", "page_idx": 33}, {"type": "equation", "text": "$$\nV_{h,P^{\\star},\\Phi}^{\\star}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{h,P^{\\star},\\Phi}^{\\star}(s,a)\\le\\operatorname*{max}_{a\\in\\mathcal{A}}\\overline{{Q}}_{h}^{k}(s,a)=\\overline{{V}}_{h}^{k}(s),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which proves that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{V}}_{h}^{k}(s)\\leq V_{h,P^{\\star},\\Phi}^{\\pi^{k}}(s)\\leq V_{h,P^{\\star},\\Phi}^{\\star}(s)\\leq\\overline{{V}}_{h}^{k}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since the conclusion (E.14) holds for the $V$ function part at step $H+1$ , an induction proves Lemma E.2. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "E.4Proof of Lemma E.3 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proofof Lemma $E.3$ .We upper bound the differences by a concentration inequality Lemma E.9, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]}\\\\ &{\\qquad\\le2\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{2\\cdot\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}{H}}\\\\ &{\\qquad\\qquad+\\:\\frac{2c_{2}^{\\prime}H^{2}S t}{N_{h}^{k}(s,a)\\vee1}+\\frac{2}{\\sqrt{K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $c_{1},c_{2}^{\\prime}>0$ are absolute constants. Then applying Lemma E.11 to the variance term in (E.19), with an argument the same as (E.16) in the proof of Lemma E.2, we can obtain that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]}\\\\ &{\\qquad\\le2\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]\\cdot c_{1}t}{N_{h}^{k}(s,a)\\vee1}}+\\frac{4\\cdot\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}{H}}\\\\ &{\\qquad\\qquad+\\:\\frac{2c_{2}^{\\prime\\prime}H^{2}t}{N_{h}^{k}(s,a)\\vee1}+\\frac{2}{\\sqrt{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, by looking into the choice of bonus $_{h}^{k}(s,a)$ in (4.5), we can conclude that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\overline{{V}}_{h+1}^{k}\\Big]+\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[\\underline{{V}}_{h+1}^{k}\\Big]}\\\\ &{\\qquad\\leq2\\cdot\\mathrm{bonus}_{h}^{k}(s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This finishes the proof of Lemma E.3. ", "page_idx": 34}, {"type": "text", "text": "E.5 Proof of Lemma E.4 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Lemma E.4. Recall that the bonus $_{h}^{k}(s,a)$ is defined as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{b o n u s}_{h}^{k}(s,a)=\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{2\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]}{H}+\\frac{c_{2}H^{2}S t}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The main thing we need to consider is to control the first term and the second term. We first deal with the second term of bonus ${\\mathbf{\\zeta}}_{h}^{k}(s,a)$ by invoking Lemma E.10, which gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{2\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-{\\cal V}_{h+1}^{k}\\right]}{H}\\leq\\bigg(\\frac{2}{H}+\\frac{2}{H^{2}}\\bigg)\\cdot\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-{\\cal V}_{h+1}^{k}\\Big]+\\frac{c_{2}^{\\prime}H S\\iota}{N_{h}^{k}(s,a)\\vee1}}\\\\ &{}&{\\leq\\frac{3\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-{\\cal V}_{h+1}^{k}\\right]}{H}+\\frac{c_{2}^{\\prime}H S t}{N_{h}^{k}(s,a)\\vee1},\\qquad\\qquad(\\mathrm{E})^{\\star}(\\cdot\\vert s,a)\\vee(\\mathrm{E})^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the second inequality is from $H\\ge2$ . Then we deal with the first term (variance term) of bonus $_{h}^{k}(s,a)$ by invoking Lemma E.12, which gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\frac{\\Psi_{R_{k}^{n}(\\{*,*\\})}\\left[\\left(\\overline{{V}}_{h_{k}}^{k}+1\\right)/2\\right]\\cdot c_{1}\\eta}{N_{k}^{k}(s,a)\\vee1}\\right.}\\\\ &{\\qquad\\leq\\sqrt{\\frac{\\left(\\Psi_{P_{k}^{n}(\\{*,*\\})}\\prod_{i=1}^{N}\\jmath_{i}\\right)-4\\eta\\cdot\\mathbb{E}_{P_{k}^{n}(\\{*,*\\})}\\left[\\overline{{V}}_{h_{k}^{k}-1}^{k}-\\frac{V_{k}^{k}}{N_{k}^{i}+1}\\right]+\\frac{c_{1}^{2}\\eta\\cdot R_{k}^{5}\\eta}{N_{k}^{i}(s,0)\\vee1}+1}{N_{k}^{k}(s,a)}}}\\\\ &{\\qquad\\leq\\sqrt{\\frac{\\Psi_{P_{k}^{n}(\\{*,*\\})}\\left[\\overline{{V}}_{h_{k}^{n},\\eta}^{k}\\mathrm{,*}\\right]\\cdot c_{1}\\eta}{N_{k}^{k}(s,a)\\vee1}+\\sqrt{\\frac{4H\\cdot\\mathbb{E}_{P_{k}^{n}(\\cdot\\}|\\omega_{0}|)\\left[\\overline{{V}}_{h_{k}^{k}}^{k}(s,a)\\vee1\\right]\\cdot c_{1}\\eta}{N_{k}^{k}(s,a)\\vee1}}}}\\\\ &{\\qquad\\left.+\\sqrt{\\frac{\\Gamma(c_{1}^{2}\\tilde{\\sigma}\\tilde{S})^{2}H^{2}}{N_{k}^{k}(s,a)\\vee1}}+\\sqrt{\\frac{c_{1}\\tilde{\\sigma}}{N_{k}^{n}(s,a)\\vee1}}\\right.}\\\\ &{\\qquad\\leq\\sqrt{\\frac{\\Psi_{P_{k}^{n}(\\{*,*\\})}\\left[\\overline{{V}}_{h_{k}^{n},\\eta}^{k}\\mathrm{,*}\\right]\\cdot c_{1}^{\\prime}}{N_{k}^{k}(s,a)\\vee1}}+\\frac{\\mathbb{E}_{P_{k}^{n}(\\cdot\\}|\\omega_{0}|\\left[\\overline{{V}}_{h_{k}^{k}}^{k}-\\frac{V_{k}^{k}}{N_{k}^{i}+1}\\right]}{H}+\\frac{\\left(4c_{1}+\\sqrt{c_{1}c_{2}^{2}\\tilde{S}}\\right)H^{2}\\eta}{\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus by combining (E.20) and (E.21) with the choice of bonush, we can conclude the proof of Lemma E.4. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "E.6Proof of Lemma E.5 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof of Lemma E.5. The key idea is to relate the visitation distribution (w.r.t. $P^{\\star}$ )and thevariance (w.r.t. $P^{\\star}$ )to the value function of $\\pi^{k}$ , after which we can derive an upper bound for the total variance. Throughout this proof, we use the shorthand that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\overline{{H}}=\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "According to Proposition 2.6 and Assumption 4.1, for any policy $\\pi$ and any step $h$ , the robust value function of $\\pi$ holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\in S}V_{h,P^{\\star},\\Phi}^{\\pi}(s)\\leq\\overline{{H}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which we usually apply in the sequel. Also, to facilitate our analysis, we define ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\widetilde{T}_{h}^{k}(\\cdot|s,a)=\\underset{P(\\cdot)\\in\\mathcal{P}_{h}(s,a;P_{h}^{\\star})}{\\mathrm{argmin}}~\\mathbb{E}_{P(\\cdot)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right],\\quad\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H],\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and set $\\widetilde{T}^{k}=\\{\\widetilde{T}_{h}^{k}\\}_{h=1}^{H}$ whchis st erarialaitst lf $\\pi^{k}$ ", "page_idx": 35}, {"type": "text", "text": "Now consider the following decomposition of our target, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{*}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P}^{\\pi^{k}},\\Phi\\Big]}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{*}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P}^{\\pi^{k}},\\Phi\\Big]-\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(P^{*},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{*}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P}^{\\pi^{k}},\\Phi\\Big]\\right]}\\\\ &{\\displaystyle\\qquad+\\sum_{k=1}^{K}\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(P^{*},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{*}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P}^{\\pi^{k}},\\Phi\\Big]\\right]}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{*}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P}^{\\pi^{k}},\\Phi\\Big]-\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(P^{*},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{*}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P}^{\\pi^{k}},\\Phi\\Big]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n+\\sum_{k=1}^{K}\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(\\widetilde{T}^{k},\\pi^{k})}\\left[\\sum_{h=1}^{H}\\mathbb{V}_{\\widetilde{T}_{h}^{k}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\right]_{,}\\quad\\cdot\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n+\\sum_{k=1}^{K}\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(P^{\\star},\\pi^{k})}\\left[\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\right]-\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(\\widetilde{T}^{k},\\pi^{k})}\\left[\\sum_{h=1}^{H}\\mathbb{V}_{\\widetilde{T}_{h}^{k}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\right]\\Big[\\sum_{h=1}^{H}\\mathbb{E}_{\\tau^{k}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[\\Big.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the sequel, we upper bound each of the three terms respectively. ", "page_idx": 36}, {"type": "text", "text": "Term (i): martingale difference term. This is a summation of martingale difference term (with respect to filtration $\\mathcal{G}_{k}\\,=\\,\\sigma\\big(\\{\\big(s_{h}^{\\tau},a_{h}^{\\tau}\\big)\\big\\}_{(h,\\tau)\\in[H]\\times[k]}\\big))$ . By Azuma-Hoeffding's inequality, with probability at least $1-\\delta$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{Term\\;(i)}\\leq c\\cdot H\\cdot\\overline{{H}}^{2}\\cdot\\sqrt{K\\iota},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $c>0$ is an absolute constant. We have utilized the fact of (E.22) to obtain the upper bound $H\\overline{{H}}^{2}$ on each martingale difference term in the summation. ", "page_idx": 36}, {"type": "text", "text": "Term (i): total variance law. The upper bound of this term is the core part of the analysis, for which we summarize it in the following lemma. ", "page_idx": 36}, {"type": "text", "text": "Lemma E.6 (Total variance law). Under the same setup as Theorem 4.3, given any deterministic policy $\\pi$ definethat ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{T}_{h}(\\cdot|s,a)=\\underset{P(\\cdot)\\in\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}{\\mathrm{argmin}}~\\mathbb{E}_{P(\\cdot)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big],\\quad\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H],\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and set $\\widetilde{T}=\\{\\widetilde{T}_{h}\\}_{h=1}^{H}$ . Then we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(s_{h},a_{h})\\sim(\\widetilde{T},\\pi)}\\left[\\sum_{h=1}^{H}\\mathbb{V}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]\\right]\\le2H\\cdot\\overline{{H}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We defer the proof of Lemma E.6 to Appendix E.7. With Lemma E.6, we consider taking policy $\\pi=\\pi^{k}$ for $k\\in[K]$ therein (which are deterministic policies), and obtain that the Term (i is upper boundedby ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{Term}\\;(\\mathrm{ii})\\leq2H\\cdot\\overline{{H}}\\cdot K.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Term (ii): error from $P^{\\star}$ to $\\widetilde{T}^{k}$ . We first relate the visitation distribution under $P^{\\star}$ to that under $\\widetilde{T}^{k}$ . On the one hand, by the choice of the adversarial transition kernel $\\widetilde{T}_{h}^{k}$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\mathrm{TV}}\\left(P_{h}^{\\star}(\\cdot|s,a)\\left\\|\\widetilde{T}_{h}^{k}(\\cdot|s,a)\\right)\\leq\\rho,\\quad\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "On the other hand, by (E.22), we can upper bound the variance term by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\le\\overline{{H}}^{2},\\quad\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, by combining (E.26) and (E.27), we can conclude that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(P_{h}^{\\star},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P^{\\star}(\\cdot|s_{h}^{k},a_{h}^{k})}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\right]}}\\\\ &{\\quad\\le\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(\\widetilde{T}^{k},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{\\star}(\\cdot|s_{h}^{k},a_{h}^{k})}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,H\\cdot\\left(\\underset{(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]}{\\operatorname*{sup}}D_{\\mathrm{TV}}\\left(P_{h}^{\\star}(\\cdot\\mid s,a)\\right\\Vert\\widetilde{T}_{h}^{k}(\\cdot\\mid s,a)\\right)}\\\\ &{\\quad\\quad\\quad\\cdot\\underset{(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]}{\\operatorname*{sup}}\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\mid s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\Bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\cdot\\underset{(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]}{\\operatorname*{sup}}\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\mid s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\Bigg]+\\rho H\\cdot\\overline{{H}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We then relate the variance term under $P^{\\star}$ to that under $\\widetilde{T}^{k}$ . Specifically, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]=\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\left(V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right)\\right]^{2}-\\left(\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\right)^{2}}\\\\ &{\\phantom{=}\\leq\\mathbb{E}_{\\widetilde{T}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right)\\right]^{2}-\\left(\\mathbb{E}_{\\widetilde{T}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1}^{\\pi^{k}}(s^{\\prime})\\right]\\right)^{2}}\\\\ &{\\phantom{=}\\quad+2\\cdot\\underset{\\left(s,a\\right)\\in{\\mathcal{S}}\\times A}{\\operatorname*{sup}}D_{\\mathrm{TV}}\\left(P_{h}^{\\star}(\\cdot\\vert s,a)\\left\\Vert\\widetilde{T}_{h}^{k}(\\cdot\\vert s,a)\\right)\\cdot\\left(\\underset{s^{\\prime}\\in{\\mathcal{S}}}{\\operatorname*{max}}V_{h+1,P^{\\star},\\Phi}^{\\pi}(s^{\\prime})\\right)^{2}}\\\\ &{\\phantom{=}\\leq\\mathbb{V}_{\\widetilde{T}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]+2\\rho\\cdot\\overline{{H}}^{2},\\quad\\forall(s,a,h)\\in S\\times A\\times[H],\\quad\\mathrm{(E.29)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality follows from the definition of $\\widetilde{T}^{k}$ and (E.22). Combining (E.28) and (E.29), we can upper bound Term (i) by the following, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Term~(iii)}=\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(P^{k},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{s}\\times\\{\\cdot|s_{h}^{k},a_{h}^{k}\\}}\\Big[V_{h+1,P^{s},\\Phi}^{\\pi^{k}}\\Big]\\right]}&{}\\\\ {\\quad-\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(\\widetilde{T}^{k},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{s}\\times\\{\\cdot|s_{h}^{k},a_{h}^{k}\\}}\\Big[V_{h+1,P^{s},\\Phi}^{\\pi^{k}}\\Big]\\right]}&{}\\\\ {\\quad+\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(\\widetilde{T}^{k},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{s}\\times\\{\\cdot|s_{h}^{k},a_{h}^{k}\\}}\\Big[V_{h+1,P^{s},\\Phi}^{\\pi^{k}}\\Big]\\right]}&{}\\\\ {\\quad-\\mathbb{E}_{(s_{h}^{k},a_{h}^{k})\\sim(\\widetilde{T}^{k},\\pi^{k})}\\left[\\displaystyle\\sum_{h=1}^{H}\\nabla_{\\widetilde{T}_{h}^{s}(\\cdot|s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{s},\\Phi}^{\\pi^{k}}\\Big]\\right]}&{}\\\\ {\\leq3\\rho H\\cdot\\overline{{H}}^{2}\\cdot K\\leq3H\\cdot\\overline{{H}}\\cdot\\mathcal{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where in the last inequality we use the fact that for any $\\rho\\in[0,1]$ , it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\rho\\overline{{H}}=\\rho\\cdot\\operatorname*{min}\\left\\{H,\\rho^{-1}\\right\\}=\\operatorname*{min}\\left\\{\\rho H,1\\right\\}\\leq1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finishing the proof. Finally, combining the upper bounds for Terms (i), (i), and (ii), i.e., (E.23), (E.25), and (E.30), we conclude that with probability at least $1-\\delta$ , it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\le c\\cdot H\\cdot\\overline{{H}}^{2}\\cdot\\sqrt{K\\iota}+2H\\cdot\\overline{{H}}\\cdot K+3H\\cdot\\overline{{H}}\\cdot K}&{}\\\\ {\\le c^{\\prime}\\cdot H\\cdot\\overline{{H}}\\cdot K+c^{\\prime\\prime}\\cdot H\\cdot\\overline{{H}}^{3}\\cdot\\iota,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where in the last inequality we use ${\\sqrt{a b}}\\,\\leq\\,a+b$ forany $a,b\\;>\\;0$ .Plug in the notation that $\\overline{{H}}=\\operatorname*{min}\\{H,\\rho^{-1}\\}$ and finish the proof of Lemma E.5. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "E.7 Proof of Lemma E.6 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Proof of Lemma $E.6.$ Using the property of variance, we have that for any $\\left(s_{h},a_{h}\\right)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\left[\\big(V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big)^{2}\\right]-\\left(\\mathbb{E}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By robust Bellman equation (Proposition 2.2) and the definition of $\\widetilde{T}_{h}$ in (E.24), we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\nV_{h,P^{\\star},\\Phi}^{\\pi}(s_{h})=R_{h}(s_{h},\\pi_{h}(s_{h}))+\\mathbb{E}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},\\pi_{h}(s_{h}))}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, by (E.31) and (E.32), we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},\\pi_{h}(s_{h}))}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},\\pi_{h}(s_{h}))}\\left[\\big(V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big)^{2}\\right]-\\Big(V_{h,P^{\\star},\\Phi}^{\\pi}\\big(s_{h}\\big)-R_{h}\\big(s_{h},\\pi_{h}(s_{h})\\big)\\Big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the second term in (E.33), we can calculate it as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\left(V_{h,P^{\\star},\\Phi}^{\\pi}(s_{h})-R_{h}(s_{h},\\pi_{h}(s_{h}))\\right)^{2}}\\\\ &{\\qquad=-\\left(V_{h,P^{\\star},\\Phi}^{\\pi}\\right)^{2}(s_{h})+2\\cdot V_{h,P^{\\star},\\Phi}^{\\pi}(s_{h})\\cdot R_{h}(s_{h},\\pi_{h}(s_{h}))-R_{h}^{2}(s_{h},\\pi_{h}(s_{h}))}\\\\ &{\\qquad\\leq-\\left(V_{h,P^{\\star},\\Phi}^{\\pi}\\right)^{2}(s_{h})+2\\overline{{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality utilizes the facts that $0\\leq R_{h}(s_{h},\\pi_{h}(s_{h}))\\leq1$ $R_{h}^{2}(s_{h},\\pi_{h}(s_{h}))\\geq0$ , and (E.22) that $V_{h,P^{\\star},\\Phi}^{\\pi}(s_{h})\\leq\\overline{{H}}$ . Combining (E.33) and (E.34), we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\widetilde{T}_{h}\\left(\\cdot\\vert s_{h},\\pi_{h}\\left(s_{h}\\right)\\right)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]\\le\\mathbb{E}_{\\widetilde{T}_{h}\\left(\\cdot\\vert s_{h},\\pi_{h}\\left(s_{h}\\right)\\right)}\\left[\\left(V_{h+1,P^{\\star},\\Phi}^{\\pi}\\right)^{2}\\right]-\\left(V_{h,P^{\\star},\\Phi}^{\\pi}\\right)^{2}\\left(s_{h}\\right)+2\\overline{{H}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which further implies that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(s_{h},a_{h})\\sim(\\widetilde{T},\\pi)}\\bigg[\\mathbb{V}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]\\bigg]}\\\\ &{\\qquad=\\mathbb{E}_{s_{h}\\sim(\\widetilde{T},\\pi)}\\bigg[\\mathbb{V}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},\\pi_{h}(s_{h}))}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]\\bigg]}\\\\ &{\\qquad\\le\\mathbb{E}_{s_{h}\\sim(\\widetilde{T},\\pi)}\\bigg[\\mathbb{E}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},\\pi_{h}(s_{h}))}\\left[\\big(V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big)^{2}\\right]-\\big(V_{h,P^{\\star},\\Phi}^{\\pi}\\big)^{2}+2\\overline{{H}}\\bigg]}\\\\ &{\\qquad=\\mathbb{E}_{s_{h+1}\\sim(\\widetilde{T},\\pi)}\\left[\\big(V_{h+1,P^{\\star},\\Phi}^{\\pi}\\big)^{2}\\right]-\\mathbb{E}_{s_{h}\\sim(\\widetilde{T},\\pi)}\\left[\\big(V_{h,P^{\\star},\\Phi}^{\\pi}\\big)^{2}\\right]+2\\overline{{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Taking summation over $h\\in[H]$ gives that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(s_{h},a_{h})\\sim(\\tilde{T},\\pi),h\\in[H]}\\left[\\sum_{h=1}^{H}\\mathbb{V}_{\\widetilde{T}_{h}(\\cdot\\vert s_{h},a_{h})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi}\\Big]\\right]\\le2H\\cdot\\overline{{H}}=2H\\cdot\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which concludes the proof of Lemma E.6. ", "page_idx": 38}, {"type": "text", "text": "E.8Other Technical Lemmas ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Before presenting all lemmas, we recall that the typical event $\\mathcal{E}$ is defined as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}=\\left\\{\\bigg|\\left(\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}-\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\right)\\bigg[(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star})_{+}\\bigg]\\right|}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{\\mathbb{V}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star})_{+}\\right]\\cdot c_{1}t}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}H t}{N_{h}^{k}(s,a)\\vee1},}\\\\ &{\\qquad\\Big\\vert P_{h}^{\\star}(s^{\\prime}\\vert s,a)-\\hat{P}_{h}(s^{\\prime}\\vert s,a)\\Big\\vert\\leq\\sqrt{\\frac{\\operatorname*{min}\\left\\{P_{h}^{\\star}(s^{\\prime}\\vert s,a),\\hat{P}_{h}^{k}(s^{\\prime}\\vert s,a)\\right\\}\\cdot c_{1}t}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}t}{N_{h}^{k}(s,a)\\vee1},}\\\\ &{\\qquad\\qquad\\forall(s,a,s^{\\prime},h,k)\\in S\\times A\\times S\\times[H]\\times[K],\\ \\forall\\eta\\in\\mathcal{N}_{1/(S\\times\\mathcal{H})}([0,H])\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ \uff0c $c_{1},c_{2}>0$ are two absolute constants, $\\mathcal{N}_{1/S\\sqrt{K}}([0,H])$ denotes an $1/S\\sqrt{K}$ -cover of the interval $[0,H]$ . where $c_{1},c_{2}>0$ are two absolute constants, $\\mathcal{N}_{1/S\\sqrt{K}}([0,H])$ denotes an $1/S\\sqrt{K}$ -cover of the interval $[0,H]$ ", "page_idx": 38}, {"type": "text", "text": "E.8.1  Concentration Inequalities ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma E.7 (Bernstein bound for TV robust sets and the optimal robust value function). Under event $\\mathcal{E}$ in (E.35),itholds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]\\bigg|}\\\\ &{\\quad\\quad\\leq\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{c_{2}H\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma $E.7.$ Byour denition of the operator $\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}[V_{h+1,P^{\\star},\\Phi}^{\\star}]$ in (4. 1), we can arrive that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\Big[V_{h+1,P^{\\star},\\Phi}^{k}\\Big]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{k}\\Big]\\Bigg|}\\\\ &{\\quad\\quad=\\left|\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\left\\{-\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\big(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}\\big)_{+}\\Big]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\right\\}\\right.}\\\\ &{\\quad\\quad\\quad\\left.-\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\;\\Bigg\\{-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[\\big(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}\\big)_{+}\\Big]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\Bigg\\}\\right|}\\\\ &{\\quad\\quad\\quad\\leq\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{\\Bigg\\vert\\left(\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\right)\\Big[\\big(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}\\big)_{+}\\Bigg]\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now according to the first inequality of event $\\mathcal{E}$ , we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\left(\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}-\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\right)\\bigg[\\big(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}\\big)_{+}\\bigg]\\bigg|}\\\\ &{\\quad\\quad\\leq\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\big(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}\\big)_{+}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{c_{2}H\\iota}{N_{h}^{k}(s,a)\\vee1}}}\\\\ &{\\quad\\quad\\leq\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}H\\iota}{N_{h}^{k}(s,a)\\vee1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for any $\\eta\\in\\mathcal{N}_{1/(S\\sqrt{K})}([0,H])$ . Here the second inequality is because $\\operatorname{Var}[(a-X)_{+}]\\leq\\operatorname{Var}[X]$ Therefore, by a covering argument, for any $\\eta\\in[0,H]$ , it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\left(\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}-\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\right)\\left[\\left(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}\\right)_{+}\\right]\\bigg|}\\\\ &{\\qquad\\le\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{c_{2}H\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This finishes the proof of Lemma E.7. ", "page_idx": 39}, {"type": "text", "text": "Lemma E.8 (Bernstein bound for TV robust sets and the robust value function of $\\pi^{k}$ ).Underevent $\\mathcal{E}$ in (E.35), suppose that the optimism and pessimism (E.14) holds at $(h+1,k)$ ,thenitholdsthat ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\widehat{P}_{h}^{k})}\\!\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\!\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\right|}\\\\ &{\\qquad\\le\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]}{H}+\\frac{c_{2}^{\\prime}H^{2}S c}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ and $c_{1}$ $c_{2}^{\\prime}$ are absolute constants. ", "page_idx": 39}, {"type": "text", "text": "$E.8.$ $\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P)}[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{P_{\\rho}(s,\\alpha_{t};\\hat{P}_{h})}\\left[V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\right]-\\mathbb{E}_{P_{\\rho}(s,\\alpha_{t},P_{h}^{*})}\\left[V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\right]\\right|}\\\\ &{\\qquad=\\left|\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\,\\Bigg\\{-\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot_{1},s,a)}\\Big[\\big(\\eta-V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\big)_{+}\\Big]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\Bigg\\}\\right.}\\\\ &{\\qquad\\qquad\\left.-\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{-\\mathbb{E}_{P_{h}^{k}(\\cdot_{1},s,a)}\\Big[\\big(\\eta-V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\big)_{+}\\Big]+\\Big(1-\\frac{\\rho}{2}\\Big)\\cdot\\eta\\Bigg\\}\\right|}\\\\ &{\\qquad\\le\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{\\Bigg|\\left(\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot_{1},s,a)}-\\mathbb{E}_{P_{h}^{k}(\\cdot_{1},s,a)}\\right)\\Big[\\big(\\eta-V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\big)_{+}\\Bigg]\\Bigg|\\Bigg\\}}\\\\ &{\\qquad\\le\\underset{\\eta\\in[0,H]}{\\operatorname*{sup}}\\Bigg\\{\\Bigg|\\left(\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot_{1},s,a)}-\\mathbb{E}_{P_{h}^{k}(\\cdot_{1},s,a)}\\right)\\Big[\\big(\\eta-V_{h+1,P^{*},\\Phi}^{\\pi^{k}}\\big)_{+}\\Bigg]\\Bigg|\\Bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We deal with Term (i) and Term (i) respectively. For Term (i), this is exactly the same as the right hand side of (E.36). Therefore, applying the same argument as Lemma E.7 gives the following upper bound, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{Term~(i)}\\leq\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}\\left(\\cdot\\vert s,a\\right)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{c_{2}H\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For Term (i), we first apply the second inequality of event $\\mathcal{E}$ to obtain that, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\le\\operatorname*{sup}_{\\eta\\in[0,H]}\\left\\{\\sum_{s^{\\prime}\\in S}\\left(\\sqrt{\\frac{\\widehat{P}_{h}^{k}\\left(s^{\\prime}|s,a\\right)\\cdot c_{1}l}{N_{h}^{k}\\left(s,a\\right)\\vee1}}+\\frac{c_{2}\\iota}{N_{h}^{k}\\left(s,a\\right)\\vee1}\\right)\\right.}}\\\\ &{}&{\\left.\\cdot\\left|\\left(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}(s^{\\prime})\\right)_{+}\\!\\!-\\left(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}(s^{\\prime})\\right)_{+}\\right|\\right\\}\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By the assumption that (E.14) holds at $(h+1,k)$ , we can upper bound the absolute value above by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}(s^{\\prime})\\right)_{+}-\\left(\\eta-V_{h+1,P^{\\star},\\Phi}^{\\star}(s^{\\prime})\\right)_{+}\\right|\\le\\left|V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}(s^{\\prime})-V_{h+1,P^{\\star},\\Phi}^{\\star}(s^{\\prime})\\right|}&{}\\\\ {\\le\\overline{{V}}_{h+1}^{k}(s^{\\prime})-\\underline{{V}}_{h+1}^{k}(s^{\\prime}).}&{\\qquad0\\le s^{\\prime}\\le0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the first inequality is due to the 1-Lipschitz continuity of $\\psi_{\\eta}(x)=(\\eta-x)_{+}$ and the second inequality is due to (E.14). Thus combining (E.38) and (E.39), we know that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{Term~(ii)}\\leq\\sum_{s^{\\prime}\\in\\mathcal{S}}\\left(\\sqrt{\\frac{\\widehat{P}_{h}^{k}(s^{\\prime}|s,a)\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}\\iota}{N_{h}^{k}(s,a)\\vee1}\\right)\\cdot\\left(\\overline{{V}}_{h+1}^{k}(s^{\\prime})-\\underline{{V}}_{h+1}^{k}(s^{\\prime})\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now following the argument first identified by Azar et al. (2017), we proceed to upper bound (E.40) as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{Term~(ii)}\\leq\\sum_{s^{\\prime}\\in\\mathcal{S}}\\left(\\frac{\\widehat{P}_{h}^{k}(s^{\\prime}|s,a)}{H}+\\frac{c_{1}H\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{c_{2}\\iota}{N_{h}^{k}(s,a)\\vee1}\\right)\\cdot\\left(\\overline{{V}}_{h+1}^{k}(s^{\\prime})-\\underline{{V}}_{h+1}^{k}(s^{\\prime})\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\leq\\frac{\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}{H}+\\frac{c_{2}^{\\prime}H^{2}S\\iota}{N_{h}^{k}(s,a)\\vee1},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $c_{2}^{\\prime}\\ >\\ 0$ is another absolute constant. The first inequality is by ${\\sqrt{a b}}~\\leq~a+b$ and the second inequlity is due to $\\overline{{V}}_{h+1}^{k},\\underline{{V}}_{h+1}^{k}\\in[0,H]$ .Finally, combining (E.37) and (E.41), we prove Lemma E.8. ", "page_idx": 41}, {"type": "text", "text": "Lemma E.9 (Bernstein bounds for TV robust sets and optimistic and pessimistic robust value estimators).Under event $\\mathcal{E}$ in (E.35), suppose that the optimism and pessimism (E.14) holds at $(h+1,k)$ itholdsthat ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{nax}\\left\\{\\,\\bigg|\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\overline{{V}}_{h+1}^{k}\\bigg]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\bigg[\\overline{{V}}_{h+1}^{k}\\bigg]\\,\\bigg|\\,,\\bigg|\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;\\hat{P}_{h}^{k})}\\bigg[\\underline{{V}}_{h+1}^{k}\\bigg]-\\mathbb{E}_{\\mathcal{P}_{\\rho}(s,a;P_{h}^{\\star})}\\bigg[\\underline{{V}}_{h+1}^{k}\\bigg]\\,\\bigg|\\,\\right\\}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\leq\\sqrt{\\frac{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big]\\cdot c_{1}\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]}{H}+\\frac{c_{2}^{\\prime}H^{2}S\\iota}{N_{h}^{k}(s,a)\\vee1}+\\frac{1}{\\sqrt{K}}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\iota=\\log(S^{3}A H^{2}K^{3/2}/\\delta)$ and $c_{1},c_{2}^{\\prime}$ are absolute constants. ", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma E.9. This follows from the same proof as Lemma E.8 and is thus omitted. ", "page_idx": 41}, {"type": "text", "text": "Lemma E.10 (Non-robust concentration). Under event $\\mathcal{E}$ in (E.35), suppose that the optimism and pessimism (E.14) holds at $(h+1,k)$ thenitholdsthat ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\bigg|\\left(\\mathbb{E}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a)}-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\right)\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]\\bigg|\\leq\\frac{1}{H}\\cdot\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big]+\\frac{c_{2}^{\\prime}H^{2}S t}{N_{h}^{k}(s,a)\\vee1}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\iota=\\log(S^{2}A H^{2}K^{3/2}/\\delta)$ and $c_{2}^{\\prime}$ is an absolute constant. ", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma $E.I O$ .According to the second inequality of event $\\mathcal{E}$ , we have that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\left(\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\right)\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]\\bigg|}\\\\ &{\\qquad\\le\\displaystyle\\sum_{s^{\\prime}\\in S}\\left(\\sqrt{\\frac{P_{h}^{\\star}(s^{\\prime}\\vert s,a)\\cdot c_{1}l}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}\\iota}{N_{h}^{k}(s,a)\\vee1}\\right)\\cdot\\left(\\overline{{V}}_{h+1}^{k}(s^{\\prime})-\\underline{{V}}_{h+1}^{k}(s^{\\prime})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we als apply E 14) that $\\overline{{V}}_{h+1}^{k}(s^{\\prime})\\geq\\underline{{V}}_{h+1}^{k}(s^{\\prime})$ Now using the same argument as E.41) in the proof of Lemma E.8, we can arrive at ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\left(\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}-\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\right)\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]\\bigg|}\\\\ &{\\qquad\\le\\frac{\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}(s^{\\prime})-\\underline{{V}}_{h+1}^{k}(s^{\\prime})\\right]}{H}+\\frac{c_{2}^{\\prime}H^{2}S\\iota}{N_{h}^{k}(s,a)\\vee1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which finishes the proof of Lemma E.10. ", "page_idx": 41}, {"type": "text", "text": "E.8.2 Variance Analysis ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma E.11 (Variance analysis 1). Suppose that the optimism and pessimism (E.14) holds at $(h+1,k)$ ,thenthefollowinginequalityholds, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\!\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]-\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\!\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\right|\\leq4H\\cdot\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\!\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma $E.I I$ . Directly consider that the left hand side can be upper bounded by the following, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]-\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\right|}\\\\ &{\\qquad\\leq\\left|\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)^{2}/4\\right]-\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(V_{h+1,P^{\\star},\\Phi}^{\\star}\\right)^{2}\\right]\\right|}\\\\ &{\\qquad\\qquad+\\left|\\left(\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]\\right)^{2}-\\left(\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\right)^{2}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since ll of $\\overline{{V}}_{h+1}^{k},\\underline{{V}}_{h+1}^{k},V_{h+1,P^{\\star},\\Phi}^{\\star}\\in[0,H]$ bythcoeftdef $\\overline{{V}}_{h+1}^{k},\\underline{{V}}_{h+1}^{k})$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\middle[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]-\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\middle[V_{h+1,P^{\\star},\\Phi}^{\\star}\\right]\\right|}\\\\ &{\\qquad\\leq4H\\cdot\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\bigg[\\Big\\vert\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2-V_{h+1,P^{\\star},\\Phi}^{\\star}\\Big\\vert\\bigg]}\\\\ &{\\qquad\\leq4H\\cdot\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last inequality is due to the correctness of (E.14) at $(h\\!+\\!1,k)$ . This proves Lemma E.11. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Lemma E.12 (Variance analysis 2). Under event $\\mathcal{E}$ in (E.35), suppose that optimism and pessimism (E.14) holds at $(h+1,k)$ ,thenit holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]-\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\right|}\\\\ &{\\qquad\\leq4H\\cdot\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right]+\\frac{c_{2}^{\\prime}H^{4}S_{l}}{N_{h}^{k}\\left(s,a\\right)}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof of Lemma E.12. We first relate the variane on $\\widehat{P}_{h}^{k}$ to the variance on $P_{h}^{\\star}$ . Specificll, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]-\\mathbb{V}_{P_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]\\Bigg\\vert}\\\\ &{\\quad\\quad=\\Bigg\\vert\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Bigg[\\bigg(\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2-\\mathbb{E}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]\\bigg)^{2}\\Bigg]\\Bigg\\vert}\\\\ &{\\quad\\quad\\quad+\\Bigg\\vert\\mathbb{E}_{P_{h}^{k}(\\cdot\\vert s,a)}\\left[\\bigg(\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2-\\mathbb{E}_{P_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]\\bigg)^{2}\\right]\\Bigg\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since $(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k})/2\\in[0,H]$ we can ftherupper boud Eby ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]-\\mathbb{V}_{P_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]\\Bigg]}\\\\ &{\\quad\\quad\\leq H^{2}\\cdot\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}\\left\\vert P_{h}^{*}(s^{\\prime}\\vert s,a)-\\widehat{P}_{h}(s^{\\prime}\\vert s,a)\\right\\vert}\\\\ &{\\quad\\quad\\leq H^{2}\\cdot\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}\\left(\\sqrt{\\frac{P_{h}^{k}(\\cdot\\vert s,a)}{N_{h}^{k}(s,a)}\\cdot c_{1}t}+\\frac{c_{2}t}{N_{h}^{k}(s,a)\\vee1}\\right)}\\\\ &{\\quad\\quad\\leq H^{2}\\cdot\\left(\\sqrt{\\frac{c_{1}S t}{N_{h}^{k}(s,a)\\vee1}}+\\frac{c_{2}S t}{N_{h}^{k}(s,a)\\vee1}\\right)}\\\\ &{\\quad\\quad\\leq1+\\frac{c_{2}H^{4}S t}{N_{h}^{k}(s,a)\\vee1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the second inequality is by the second inequality in event $\\mathcal{E}$ , the third inequality is by CauchySchwartz inequality and the probability distribution sums up to 1, and the last inequality is from ${\\sqrt{a b}}\\leq a+b$ . Thus by (E.44), we can bound our target as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]-\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\,}\\\\ &{\\qquad\\le\\Big\\vert\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[\\Big(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\Big)/2\\Big]-\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\Big]\\,\\Big\\vert}\\\\ &{\\qquad\\qquad+\\,\\frac{c_{2}^{\\prime}H^{4}S\\iota}{N_{h}^{k}(s,a)\\vee1}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now by the same proof of Lemma E.11, using the correctness of (E.14) at $(h+1,k)$ ,wecanshow that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\left(\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}\\right)/2\\right]-\\mathbb{V}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[V_{h+1,P^{\\star},\\Phi}^{\\pi^{k}}\\right]\\right|}\\\\ &{\\quad\\quad\\leq4H\\cdot\\mathbb{E}_{P_{h}^{\\star}(\\cdot\\vert s,a)}\\left[\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Combining (E.45) and (E.46), we can finish the proof of Lemma E.12. ", "page_idx": 43}, {"type": "text", "text": "E.8.3Other Auxiliary Lemmas ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Lemma E.13(Lemma 7.5 in Agarwal et al. (2019). For the sequences of $\\{s_{h}^{k},a_{h}^{k}\\}_{h,k=1}^{H,K}$ it holds that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})\\vee1}\\leq c\\cdot H S A\\log(K).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $c>0$ is an absoluteconstant. ", "page_idx": 43}, {"type": "text", "text": "Proof of Lemma E.13. See Lemma 7.5 in Agarwal et al. (2019) for a detailed proof. ", "page_idx": 43}, {"type": "text", "text": "F Proofs for Extensions in Section B.4.2 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we prove the theoretical results in Section B.4.2. ", "page_idx": 43}, {"type": "text", "text": "F.1 Proof of Corollary B.5 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Proof of Corollary B.5. We consider applying Algorithm 1 on the auxiliary $S\\!\\times\\!A$ -rectangular RMDP with a TV robust set M (see Section B.4.2) which satisfies the vanishing minimal value assumption (Assumption 4.1). Suppose the algorithm outputs $\\widetilde{\\pi}^{1},\\cdots,\\widetilde{\\pi}^{K}$ for the $K$ episodes. Then Theorem 4.3 shows that by a proper choice of the hyperparameters, with probability at least $1-\\delta$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}_{\\widetilde{\\Phi}}(K)=\\displaystyle\\sum_{k=1}^{K}\\underset{\\widetilde{\\pi}}{\\operatorname*{max}}\\,V_{1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}(s_{1})-V_{1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}^{k}}(s_{1})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{O}\\bigg(\\sqrt{\\operatorname*{min}\\big\\{H,\\rho^{-1}\\big\\}H^{2}(S+1)A K\\iota^{\\prime}}\\bigg),.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\iota^{\\prime}=\\log^{2}(S A H K/\\delta)$ and $\\rho=2-2\\rho^{\\prime}\\in[0,1)$ . In the sequel, we prove that for any policy $\\widetilde{\\pi}$ of $\\widetilde{\\mathcal{M}}$ and its induced policy $\\widetilde{\\pi}_{S}$ of $\\mathcal{M}_{\\gamma}$ , their robust value functions coincide at the initial state $s_{1}\\in\\mathcal{S}$ , that is, ", "page_idx": 43}, {"type": "equation", "text": "$$\nV_{1,\\widetilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}}(s_{1})=V_{1,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}}(s_{1}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where V\\* $V_{1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}$ s is the robust alue function of \u03c0 in M = (S, A, H, P\\*, R,\u03a6), and V V1,P\\*,\u03a6 is the robust value function of $\\widetilde{\\pi}_{S}$ in $\\mathcal{M}_{\\gamma_{\\mathrm{-}}}=(S,A,H,P^{\\star},R_{\\gamma},\\Phi^{\\prime})$ . To this end, we actually prove a stronger result that for any step $h\\in[H]$ , it holds that ", "page_idx": 43}, {"type": "equation", "text": "$$\n(\\rho^{\\prime})^{h-1}\\cdot V_{h,\\tilde{P}^{\\star},\\mathfrak{F}}^{\\tilde{\\pi}}(s)=V_{h,P^{\\star},\\Phi^{\\prime}}^{\\tilde{\\pi}_{S}}(s),\\quad\\forall s\\in\\mathcal{S}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We prove (F.2) by induction. For step $H$ , by robust Bellman equation, we have that, for any $(s,a)\\in S\\times A$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n(\\rho^{\\prime})^{H-1}\\cdot Q_{H,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}(s,a)=(\\rho^{\\prime})^{H-1}\\cdot\\left(\\frac{\\gamma}{\\rho^{\\prime}}\\right)^{H-1}\\cdot R_{H}(s,a)=R_{\\gamma,H}(s,a)=Q_{H,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}}(s,a),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and thus for any $s\\in S$ \uff0c ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\rho^{\\prime})^{H-1}\\cdot V_{h,\\tilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}}(s)=\\mathbb{E}_{\\widetilde{\\pi}(\\cdot\\vert s)}\\Big[(\\rho^{\\prime})^{H-1}\\cdot Q_{H,\\tilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}}(s,\\cdot)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathbb{E}_{\\widetilde{\\pi}_{S}(\\cdot\\vert s)}\\Big[Q_{H,P^{\\star},\\Phi}^{\\widetilde{\\pi}_{S}}(s,\\cdot)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\quad=V_{H,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This proves (F.2) for step $H$ . Suppose that (F.2) holds at some step $h+1$ , that is, ", "page_idx": 44}, {"type": "equation", "text": "$$\n(\\rho^{\\prime})^{h}\\cdot V_{h+1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}(s)=V_{h+1,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}}(s),\\quad\\forall s\\in\\mathcal{S}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then for step $h$ , by robust Bellman equation and Proposition 4.2, we have that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(\\rho^{\\prime})^{h-1}\\cdot Q_{h,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}(s,a)=(\\rho^{\\prime})^{h-1}\\cdot\\left(\\frac{\\gamma}{\\rho^{\\prime}}\\right)^{H-1}\\cdot R_{h}(s,a)+(\\rho^{\\prime})^{h-1}\\cdot\\mathbb{E}_{\\widetilde{P}_{\\rho}(s,a;\\widetilde{P}_{h}^{\\star})}\\Big[V_{h+1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}\\Big]}\\\\ &{}&{=R_{\\gamma,h}(s,a)+(\\rho^{\\prime})^{h-1}\\cdot\\rho^{\\prime}\\cdot\\mathbb{E}_{\\widetilde{B}_{\\rho}(s,a;\\widetilde{P}_{h}^{\\star})}\\Big[V_{h+1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}\\Big],\\qquad\\qquad\\quad\\mathrm{(F.~)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$\\mathrm{min}_{s\\in\\widetilde{\\cal S}}\\,V_{h+1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}(s)=0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\widetilde{B}_{\\rho}(s,a;\\widetilde{P}_{h}^{\\star})=\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(\\widetilde{S}):\\operatorname*{sup}_{s^{\\prime}\\in\\widetilde{S}}\\frac{\\widetilde{P}(s^{\\prime})}{\\widetilde{P}_{h}^{\\star}(s^{\\prime}|s,a)}\\leq\\frac{1}{\\rho^{\\prime}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Notice that by the definition (B.3), we know for $(s,a)\\in S\\times A$ it holds that $\\widetilde{P}_{h}^{\\star}(\\cdot|s,a)=P_{h}^{\\star}(\\cdot|s,a)$ which is supported on $\\boldsymbol{S}$ . Therefore, we can equivalently write ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{B}_{\\rho}(s,a;\\widetilde{P}_{h}^{\\star})=\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(\\widetilde{S}):\\operatorname*{sup}_{s^{\\prime}\\in\\mathcal{S}}\\frac{\\widetilde{P}(s^{\\prime})}{\\widetilde{P}_{h}^{\\star}(s^{\\prime}|s,a)}\\leq\\frac{1}{\\rho^{\\prime}}\\right\\}}\\\\ &{\\qquad\\qquad=\\left\\{\\widetilde{P}(\\cdot)\\in\\Delta(S):\\operatorname*{sup}_{s^{\\prime}\\in\\mathcal{S}}\\frac{\\widetilde{P}(s^{\\prime})}{P_{h}^{\\star}(s^{\\prime}|s,a)}\\leq\\frac{1}{\\rho^{\\prime}}\\right\\}}\\\\ &{\\qquad\\qquad=B_{\\rho}(s,a;P_{h}^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus by (F.4) and (F.5) and the induction hypothesis (F.3), we obtain that for any $(s,a)\\in S\\times A$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\rho^{\\prime})^{h-1}\\cdot Q_{h,\\tilde{P}^{\\star},\\tilde{\\Phi}}^{\\tilde{\\pi}}(s,a)=R_{\\gamma,h}(s,a)+(\\rho^{\\prime})^{h}\\cdot\\mathbb{E}_{\\mathcal{B}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,\\tilde{P}^{\\star},\\tilde{\\Phi}}^{\\tilde{\\pi}}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=R_{\\gamma,h}(s,a)+\\mathbb{E}_{\\mathcal{B}_{\\rho}(s,a;P_{h}^{\\star})}\\Big[V_{h+1,P^{\\star},\\Phi}^{\\tilde{\\pi}_{S}}\\Big]=Q_{h,P^{\\star},\\Phi}^{\\tilde{\\pi}_{S}}(s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the second equality applies (F.3) and the last equality is from robust Bellman equation. Consequently,for any $s\\in S$ ,wehavethat ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\rho^{\\prime})^{h-1}\\cdot V_{h,\\tilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}}(s)=\\mathbb{E}_{\\widetilde{\\pi}(\\cdot\\vert s)}\\Big[(\\rho^{\\prime})^{h-1}\\cdot Q_{h,\\tilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}}(s,\\cdot)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\widetilde{\\pi}_{S}(\\cdot\\vert s)}\\Big[Q_{h,P^{\\star},\\Phi}^{\\widetilde{\\pi}_{S}}(s,\\cdot)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\quad=V_{h,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}}(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which finishes the induction argument, proving our claim (F.2). By taking $h=1$ , we can derive that for any initial state $s_{1}\\in\\mathcal{S}$ , it holds that for any policy $\\widetilde{\\pi}$ of $\\widetilde{\\mathcal{M}}$ and its induced policy $\\widetilde{\\pi}_{S}$ of $\\mathcal{M}_{\\gamma}$ ", "page_idx": 44}, {"type": "equation", "text": "$$\nV_{1,\\widetilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}}(s_{1})=V_{1,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}}(s_{1}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This indicates two facts: the first is that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\widetilde{\\pi}}V_{1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}\\big(s_{1}\\big)=\\operatorname*{max}_{\\pi}V_{1,P^{\\star},\\Phi^{\\prime}}^{\\pi}\\big(s_{1}\\big),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where on the right hand side the maximization is with respect to all the policies for $\\mathcal{M}_{\\gamma}$ ; thesecond is that ", "page_idx": 45}, {"type": "equation", "text": "$$\nV_{1,\\widetilde{P}^{\\star},\\tilde{\\Phi}}^{\\widetilde{\\pi}^{k}}(s_{1})=V_{1,P^{\\star},\\Phi^{\\prime}}^{\\widetilde{\\pi}_{S}^{k}}(s_{1}),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for each $k\\in[K]$ , where recall that $\\widetilde{\\pi}^{k}$ is the policy output by Algorithm 1 for episode $k$ . As a result, the $k$ policies $\\{\\overset{\\bar{\\pi}}{\\widetilde{\\pi}}_{S}^{k}\\}_{k=1}^{K}$ $\\mathcal{M}_{\\gamma}$ during ineractiveatacollection satis with prbabiltyatast $1-\\delta$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}_{\\Phi^{\\prime}}(K)=\\displaystyle\\sum_{k=1}^{K}\\underset{\\pi}{\\mathrm{max}}\\,V_{1,P^{\\star},\\Phi^{\\prime}}^{\\pi}(s_{1})-V_{1,P^{\\star},\\Phi^{\\prime}}^{\\pi_{s}^{k}}(s_{1})}\\\\ &{\\phantom{\\mathrm{Regret}_{\\Phi^{\\prime}}(K)=}\\displaystyle\\sum_{k=1}^{K}\\underset{\\widetilde{\\pi}}{\\mathrm{max}}\\,V_{1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}}(s_{1})-V_{1,\\widetilde{P}^{\\star},\\widetilde{\\Phi}}^{\\widetilde{\\pi}^{k}}(s_{1})}\\\\ &{\\phantom{\\mathrm{Regret}_{\\Phi^{\\prime}}(K)=}\\displaystyle\\mathcal{O}\\bigg(\\sqrt{\\operatorname*{min}\\big\\{H,(2-2\\rho^{\\prime})^{-1}\\big\\}H^{2}S A K\\iota^{\\prime}}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where in the second equality we apply the facts (F.6) and (F.7), and the last inequality follows from (F.1) and that $\\rho=2-2\\rho^{\\prime}$ . This completes the proof of Corollary B.5. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 46}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 46}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 46}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g.,\"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 46}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. . Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Sections 2, 3, 4. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Section B.5 ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Sections 3, 4, and the Appendix. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 47}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This is a theoretical work and we do not conduct experiments. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: This is a theoretical work and we do not conduct experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips. cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^\u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 48}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical work and we do not conduct experiments. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical work and we do not conduct experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical work and we do not conduct experiments. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The research in this work conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 50}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]