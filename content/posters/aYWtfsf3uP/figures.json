[{"figure_path": "aYWtfsf3uP/figures/figures_6_1.jpg", "caption": "Figure 1: Illustration of the hard example in Example 3.1. The solid lines represent possible transitions of the nominal transition kernel. The dashed lines represent the transitions induced by the worst case transition kernel in the robust set. The red solid line represents the transition where the two RMDP instances differ in that different actions lead to higher transition probability from sbad to sgood. We notice that when starting from s1 = sgood, the nominal transition kernel keeps the agent at sgood and no information at sbad is revealed.", "description": "This figure illustrates Example 3.1 from the paper, which presents a hard instance for robust reinforcement learning with interactive data collection.  It shows two Markov Decision Processes (MDPs), M0 and M1, differing only in their transition probabilities from the 'bad' state (sbad) to the 'good' state (sgood).  The solid lines represent the nominal transition probabilities in M0, while dashed lines depict the worst-case transition probabilities within a specified uncertainty set. The red line highlights the crucial difference:  In M1, a specific action leads to a higher probability of transitioning from sbad to sgood, a transition that is highly unlikely in M0, starting from s1 = sgood.  This exemplifies the 'curse of support shift' \u2013 crucial information about parts of the state space relevant for robust policy learning might be hard to obtain via interactive data collection in the training environment.", "section": "A Hardness Result: The Curse of Support Shift"}]