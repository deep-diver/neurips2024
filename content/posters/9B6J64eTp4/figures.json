[{"figure_path": "9B6J64eTp4/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Our method learns the geometry and appearance of an articulated object by first fitting a NeRF from (source) images of an object in a fixed articulation. Then, from another set of (target) images of the object in another articulation, we distill the relative articulation and part labels. Green lines show the gradient path during this distillation. (b) Using the part geometry and appearance from NeRF, we render the target images by compositing the parts after applying the predicted articulations to the segmented parts. The photometric error provides the required supervision for learning the parts and their articulation without groundtruth labels.", "description": "This figure illustrates the two main stages of the proposed method. (a) shows the pipeline overview.  First, a NeRF is trained on images of the object in a single articulation state. Then, using images of the object in a different articulation state, the model learns the part segmentation and articulation parameters.  (b) shows the details of the composite rendering process. The model uses the learned part geometry and appearance to render the target images by applying the predicted articulations to the segmented parts. The photometric error between the rendered and target images provides the supervision for learning.", "section": "4 Method"}, {"figure_path": "9B6J64eTp4/figures/figures_4_1.jpg", "caption": "Figure 2: Voxel initialization: identify the voxels belonging to moved parts based on pixel opacity difference.", "description": "This figure illustrates the process of initializing a voxel grid to identify the parts of an object that move between two different articulation states.  First, the static NeRF renders a view of the object in the second articulation state. The difference between this rendered view and the actual second-state image (target view) highlights the areas of change. These areas are then used to identify voxels corresponding to the moving parts in the 3D space of the object's NeRF representation. This provides an initial estimate for optimizing the positions of the moving parts during subsequent steps of the articulated object modeling process.  The voxel grid's coordinates are then passed to a process that refines the part segmentation and articulations in later steps.", "section": "4 Method"}, {"figure_path": "9B6J64eTp4/figures/figures_4_2.jpg", "caption": "Figure 3: Illustration for optimization of M. The green dotted line shows the gradient flow.", "description": "This figure illustrates the optimization process for the pose-change tensor M.  The left side shows the collection of 2D coordinates (U') from the overlap region between rendered opacity and the target image I'. These are concatenated with the part-specific matrices U_e, resulting in the matrix U.  The right side shows the projection of 3D coordinates X_e for part l onto the image plane to get U_e. The goal is to minimize the Chamfer distance between U and the target coordinates F (obtained from I's foreground mask). The green dotted lines indicate the gradient flow during this optimization.", "section": "4 Method"}, {"figure_path": "9B6J64eTp4/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative 2D part segmentation results. Pixels in green denotes the movable parts. Our method demonstrates consistent performance across all tested objects while PARIS failed for Blade, Laptop and Scissor.", "description": "This figure shows a qualitative comparison of 2D part segmentation results between the proposed method and the baseline method (PARIS) on seven different articulated objects. Each object is shown in two articulation states. The green pixels denote movable parts. The results demonstrate that the proposed method achieves consistent performance across various object categories while the PARIS method fails to accurately segment parts for some objects such as the Blade, Laptop, and Scissor.", "section": "5.2 Results"}, {"figure_path": "9B6J64eTp4/figures/figures_6_2.jpg", "caption": "Figure 5: Qualitative results for 2D multi-part segmentation. The pink color denotes the static part, while other colors denote the moving parts.", "description": "This figure displays the qualitative results of 2D multi-part segmentation.  The ground truth (GT) segmentations are compared to the segmentations produced by the proposed method. Four objects are shown: Box, Glasses, Oven, and Storage. Each object has multiple parts, some static (pink) and some moving (various other colors). The figure visually demonstrates the accuracy of the model's ability to segment different parts of articulated objects in 2D.", "section": "5.2 Results"}, {"figure_path": "9B6J64eTp4/figures/figures_7_1.jpg", "caption": "Figure 6: Qualitative evaluation for novel articulation synthesis. The ground truth axis is denoted in green and the predicted axis is denoted in red. Please refer to the supplementary for more visualizations.", "description": "This figure shows a qualitative comparison of novel articulation synthesis results between the proposed method and the baseline method (PARIS).  For several objects (blade, stapler, and box), multiple views are displayed, showing the ground truth articulation axis (green) and the predicted articulation axis (red) generated by each method. The results visually demonstrate the superior performance of the proposed method in accurately predicting the articulation of objects with various shapes and moving parts.  The supplementary materials contain additional visualizations for a more comprehensive evaluation.", "section": "5.2 Results"}, {"figure_path": "9B6J64eTp4/figures/figures_8_1.jpg", "caption": "Figure 7: Results on real world examples, the red line indicates the estimated joint axis direction. Green and purple color denotes the moving car door, while pink denotes the body of the toy car. Please refer to Fig. 12 for more qualitative evaluation.", "description": "This figure shows the results of applying the ArticulateYourNeRF method to real-world examples.  Specifically, it showcases the model's ability to estimate the joint axis direction (shown in red) for a toy car with an openable door. The ground truth is shown in green and purple for the door and pink for the car body.  Additional qualitative evaluations can be found in Figure 12.", "section": "5.4 Real-world example"}, {"figure_path": "9B6J64eTp4/figures/figures_13_1.jpg", "caption": "Figure 2: Voxel initialization: identify the voxels belonging to moved parts based on pixel opacity difference.", "description": "This figure illustrates the voxel grid initialization process.  It shows how the system identifies voxels corresponding to moving parts by comparing rendered opacity (from the static NeRF) with the foreground masks from target images. Pixels that appear in the rendered opacity but not in the target foreground mask are identified as part of the moving parts, and their corresponding 3D coordinates are used to initialize the voxel grid.  This grid serves as an initial estimate for the locations of moving parts, helping guide the subsequent optimization process.", "section": "4 Method"}, {"figure_path": "9B6J64eTp4/figures/figures_13_2.jpg", "caption": "Figure 8: Failure cases for foldchair, from left to right: groundtruth RGB, rendered RGB, part segmentation.", "description": "This figure shows three images of a folding chair. The leftmost image is the ground truth RGB image. The middle image is the rendered RGB image from the model, showing artifacts and inaccuracies in the rendering. The rightmost image shows the part segmentation produced by the model.  The part segmentation highlights areas where the model struggled to accurately separate the parts of the chair, particularly the seat and legs. This serves as an example of a failure case for the method in handling more complex objects.", "section": "A.3 Limitations"}, {"figure_path": "9B6J64eTp4/figures/figures_13_3.jpg", "caption": "Figure 8: (c) Artifacts for thin parts, the left one is the groundtruth, the right rendering result.", "description": "This figure shows a comparison between the ground truth image and the rendered image of a pair of glasses with thin arms.  The ground truth image on the left shows the glasses clearly. The rendered image on the right shows artifacts, particularly around the thin arms of the glasses, indicating challenges in accurately rendering fine details with the proposed method.", "section": "A.3 Limitations"}, {"figure_path": "9B6J64eTp4/figures/figures_14_1.jpg", "caption": "Figure 9: We can see in the Fig. 9(b) that the corner of the laptop screen is missing in the novel articulation rendering. While it looks perfect when we check the segmentation in the original pose. Thus, we suspect it is the proposal network than failed to estimate the density distribution for the screen from certain viewpoints.", "description": "This figure shows a comparison between the ground truth image, the rendered image from the model, and the part segmentation. It highlights a limitation of the model where the corner of the laptop screen is missing in the novel articulation rendering, despite the segmentation appearing accurate in the original pose. This suggests a potential issue with the proposal network's ability to estimate density distributions accurately from specific viewpoints.", "section": "A.5 Ablations"}, {"figure_path": "9B6J64eTp4/figures/figures_15_1.jpg", "caption": "Figure 6: Qualitative evaluation for novel articulation synthesis. The ground truth axis is denoted in green and the predicted axis is denoted in red. Please refer to the supplementary for more visualizations.", "description": "This figure shows a qualitative comparison of novel articulation synthesis between the proposed method and the baseline method (PARIS).  For multiple objects with different articulation types, the ground truth and predicted axes of movement are visualized. The green arrows represent the ground truth, and the red arrows show the predicted axes of movement.  The figure demonstrates the superior accuracy and robustness of the proposed method in estimating the articulation parameters for different object categories. More detailed visualizations and quantitative results are available in the supplementary materials.", "section": "5.2 Results"}, {"figure_path": "9B6J64eTp4/figures/figures_15_2.jpg", "caption": "Figure 6: Qualitative evaluation for novel articulation synthesis. The ground truth axis is denoted in green and the predicted axis is denoted in red. Please refer to the supplementary for more visualizations.", "description": "This figure displays qualitative results for novel articulation synthesis.  It shows several objects in different articulation states (poses). For each object, there are ground truth poses, the predicted poses from the model, and images generated from the model. The green arrows indicate the ground truth axis of rotation, while red arrows represent the predicted axis.  The results demonstrate the model's ability to accurately predict the pose changes of an object's parts, leading to realistic novel view synthesis. More visualizations can be found in the supplementary material.", "section": "5.2 Results"}, {"figure_path": "9B6J64eTp4/figures/figures_17_1.jpg", "caption": "Figure 7: Results on real world examples, the red line indicates the estimated joint axis direction. Green and purple color denotes the moving car door, while pink denotes the body of the toy car. Please refer to Fig. 12 for more qualitative evaluation.", "description": "This figure shows qualitative results of applying the proposed method to real-world objects. The top row displays novel articulation synthesis for a toy car with its door open and closed.  The red lines indicate the predicted joint axis. The bottom row shows the corresponding part segmentation results. The colors represent different parts of the car.  This demonstrates the method's ability to handle real-world scenarios.", "section": "5.4 Real-world example"}, {"figure_path": "9B6J64eTp4/figures/figures_18_1.jpg", "caption": "Figure 13: Here we show qualitative results for a \u2018door\u2019 instance along with its frame. In contrast to the instances in our submission, the static part (frame) is smaller than the moving part (door). Given two sets of views in the articulation P and P\u2019, we provide the original input images and their rendering in the respective articulations, ground-truth (GT) and predicted part segmentation results. Our method achieves faithful rendering results in different articulations with minor artifacts and accurate part segmentation.", "description": "This figure shows a qualitative comparison of the proposed method's performance on a door object.  The top row displays the input images and the rendered images for the door in two different articulation states (P and P'). The bottom row shows the ground truth and predicted part segmentations for each state. This visualization demonstrates the model's ability to accurately render the door's appearance and segment its parts in different poses.", "section": "5.2 Results"}, {"figure_path": "9B6J64eTp4/figures/figures_18_2.jpg", "caption": "Figure 11: Articulation interpolation for multiple moving part objects.", "description": "This figure shows qualitative results for novel articulation synthesis on objects with multiple moving parts.  The top row displays a robotic arm with multiple joints. The bottom row shows a different object with multiple moving parts. For each object, the ground truth (GT) poses in articulations P and P\u2019 are shown alongside the rendered images produced by the method in articulations P and P\u2019.  Part segmentation results are also displayed for both articulations.", "section": "5.2 Results"}]