[{"type": "text", "text": "AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature Space ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We study the intriguing connection between visual data, deep networks, and the   \n2 brain. Our method creates a universal channel alignment by using brain voxel   \n3 fMRI response prediction as the training objective. We discover that deep net  \n4 works, trained with different objectives, share common feature channels across   \n5 various models. These channels can be clustered into recurring sets, correspond  \n6 ing to distinct brain regions, indicating the formation of visual concepts. Tracing   \n7 the clusters of channel responses onto the images, we see semantically meaning  \n8 ful object segments emerge, even without any supervised decoder. Furthermore,   \n9 the universal feature alignment and the clustering of channels produce a picture   \n10 and quantification of how visual information is processed through the different   \n11 network layers, which produces precise comparisons between the networks. ", "page_idx": 0}, {"type": "text", "text": "12 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "13 Introducing a novel approach, Yang et al. (2024) has successfully established a method of computing   \n14 a mapping between the brain and deep-nets, effectively linking two black boxes. The brain fMRI   \n15 prediction task allows for visualizing information flow from layer to layer, using the brain as an   \n16 analysis tool.   \n17 If a picture is worth a thousand words, the   \n18 main idea is that the brain\u2019s thousands of voxels   \n19 can be thought of as alphabets for these words   \n20 that describe an image. Just as alphabets must   \n21 be combined to form words and phrases with   \n22 meanings, we need to find the grouping of brain   \n23 voxels and their network channel counterparts   \n24 to understand their meaning (Figure 1).   \n25 Our main discovery is that while the network   \n26 layer structure differs, channel feature corre  \n27 spondence exists across networks with a shared   \n28 encoding of reoccurring visual concepts. This paper builds upon the idea of \u2018Rosetta stone\u2019 neurons   \n29 (Dravid et al., 2023), which find channels across networks that share similar image responses in bi  \n30 nary segmentation. If channels are alphabets, \u2018Rosetta stone\u2019 provides an alphabet-level translation   \n31 between networks.   \n32 Individual channel-level analysis could miss feature correspondence across networks at finer and   \n33 coarser levels. On a finer level, because the channels are invariant up to a linear transformation, we   \n34 might miss a reconstituted feature constructed from a composition of existing channels. On a coarse   \n35 level, the channels can be combined and clustered to form a bigger \u2018Rosetta\u2019 concept. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "opdiIAHfBr/tmp/9c2b4368f517e5dd62b5558ca5d2a1e07fa27f4143e64a5a376cbc52a1dce900.jpg", "img_caption": ["Figure 1: Transform the hidden channel activation of deep-nets into visual brain voxels\u2019 response. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "36 To address fine-level channel analysis, we use brain voxel response as a reference signal and linearly transform channels for each network into a shared space sufficient for brain fMRI prediction. This 38 process produces a universal feature space that aligns channel features across the layers and models. To find bigger visual concepts, one can start with Neuroscience knowledge of brain regions (ROIs) 40 with specific brain functionality, i.e., V1, V4, and EBA. While tracing the mapping of the ROIs to 41 channels can produce visual concepts (Figure 2), brain regions don\u2019t function in isolation. ", "page_idx": 1}, {"type": "image", "img_path": "opdiIAHfBr/tmp/67344b9cc024eb23d7d65da8f84c9b4cf6e8fa235b8b744e0cd89a573edeefc0.jpg", "img_caption": ["Figure 2: From the 768D feature on CLIP layer-6, we extract different levels of segmentation by restricting the use of a subset of channels. Left: Channel activation on example image patches. The ordering of channels is sorted from the early brain to the late brain by their weights for brain voxels. Right: Spectral clustering on each subset of channels filtered by each brain ROI (V1, V4, EBA), image pixels colored by 3D spectral-tSNE of top 10 eigenvectors. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "42 Instead of searching through all possible channel grouping combinations, our first insight is that   \n43 we can create a channel grouping hypothesis by examining channels from each pixel\u2019s perspective.   \n44 Think of the pixels and channels forming a bipartite graph; each channel produces a per-pixel re  \n45 sponse (image activation map), defining the graph edge between the pixels and channels. Taking the   \n46 perspective of pixels, one can collect graph edges incident on each pixel into a vector, which can be   \n47 thresholded to produce a hypothesis grouping over channels.   \n48 Our second insight is that if a channel grouping hypothesis repeats across images, layers, and mod  \n49 els, it is highly unlikely to be accidental and, therefore, signals meaningful visual concepts.   \n50 We formulate this clustering problem as a graph partition task. The graph nodes are the product   \n51 space of pixels and layers. We apply spectral clustering to produce $\\mathbf{k}$ -top eigenvectors. We take   \n52 advantage of two properties of spectral clustering: it makes 1) soft-cluster embedding space in the   \n53 form of eigenvectors and 2) hierarchical clustering by varying the number of eigenvectors.   \n54 We made the following discoveries. First, shared channel sets, reoccurring across layers and models,   \n55 predict response in distinct brain regions. By tracing the channel activation to the known brain ROI   \n56 properties, we observe that the channel cluster encodes visual concepts at various levels of visual   \n57 abstraction.   \n58 Second, meaningful object segments can emerge by tracing the channel cluster responses onto each   \n59 image. We observed that some channel clusters produce figure/ground separation while others pro  \n60 duce fine-grained category classification. Our image segmentation requires no additional segmenta  \n61 tion decoder and uses only a simple distance measure over the eigenvectors.   \n62 Finally, the universal feature alignment and the spectral clustering of channels produce a picture and   \n63 quantification of how visual information is processed through the different network layers.   \n64 While these discoveries are promising, there are two main technical hurdles to overcome to verify   \n65 them on a large scale. Our method rests upon a crucial assumption: the channels across the different   \n66 layers and models can be mapped into a shared space. While brain prediction over thousands of   \n67 voxels can provide strong guidance for this alignment, an additional constraint would be needed   \n68 when the shared space has a large dimension (suitable for expressiveness). We use clustering as   \n69 a constraint, ensuring alignment linear transformation preserves spectral clustering eigenvectors.   \n70 Furthermore, the graph size is enormous as it is a product space over pixels, layers, images, and   \n71 models; therefore, computing eigenvectors over their pairwise affinity matrix can be computationally   \n72 infeasible. We developed a Nystrom-like approximation to ensure efficient computation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "73 In summary, our key contributions are: ", "page_idx": 2}, {"type": "text", "text": "74 1. We constructed a universal channel-aligned space using brain encoding as supervision and spec  \n75 tral clustering eigenvector constraints to ensure minimal channel signal loss. Brain encoding asso  \n76 ciates the aligned channel space to brain regions and gives them meanings.   \n77 2. Models trained with different objectives learned similar visual concepts: corresponding channel   \npatterns exist across different models. The resulting visual concepts can be validated by unsuper  \n79 vised segmentation benchmarks on ImageNet-segmentation and PASCAL VOC.   \n80 3. Models show divergent computation paths over the visual concept space formed by the top- $\\cdot\\mathbf{k}$   \n81 spectral eigenvectors. Different models differ in trajectories and pace of movement layer-to-layer. ", "page_idx": 2}, {"type": "text", "text": "82 2 Methods: AlignedCut ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "83 Just as human languages might consist of distinct   \n84 alphabets, features across different models appear   \n85 superficially in embedding spaces as almost mutu  \n86 ally orthogonal (Figure 3). However, the underly  \n87 ing information that they represent can be similar.   \n88 To jointly analyze features across models and lay  \n89 ers, we proposed the channel align transform that   \n90 linearly projects features to a universal space.   \n91 The learning signal for the channel align transform   \n92 is provided by brain response prediction. Learn  \n93 ing from brain prediction offers two advantages.   \n94 First, brain response covers rich representations from all levels of semantics; the channel alignment   \n95 removes irrelevant information while preserving the necessary and sufficient visual image features.   \n96 Second, knowledge of brain regions provides an interpretable understanding of their corresponding   \n97 channels derived from the alignment.   \n98 Our visual concept discovery is formulated as a graph partitioning task using spectral clustering.   \n99 We term our approach for this channel align and graph partitioning as AlignedCut. Furthermore, a   \n100 major challenge in applying spectral clustering to large graphs is the complexity scaling issue. To   \n101 address this, we developed a Nystrom-like approximation to reduce the computational complexity. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "opdiIAHfBr/tmp/28d8415973e408e9ffe9b82c3794a4440bf59cc50cd7ddc2f229d55f260993f4.jpg", "img_caption": ["Figure 3: Cosine similarity of channel activation on the same image inputs. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "102 2.1 Brain-Guided Universal Channel Align ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "103 Brain Dataset We used the Algonauts competition (Gifford et al., 2023) release of Nature Scenes   \n104 Dataset (NSD) (Allen et al., 2022). Briefly, NSD provides an fMRI brain scan when watching   \n105 COCO images. Each subject viewed 10,000 images over 40 hours of scanning. We used the first   \n106 subject\u2019s publicly shared pre-processed and denoised (Prince et al., 2022) data.   \n107 Channel Align Let $\\mathcal{V}=\\left\\{V_{1},V_{2},\\cdot\\cdot\\cdot\\,,V_{n}|V_{i}\\,\\in\\,\\mathbb{R}^{P\\times D_{i}}\\right\\}$ be the set of image features, extracted   \n108 from each layer of pre-trained ViT models, where $P=(H{\\times}W{+}1)$ is image patches and class token,   \n109 $D_{i}$ is the hidden dimension. In particular, we used the attention layer output for each $V_{i}$ without   \n110 adding residual connections from previous layers. Let $\\mathcal{V}^{\\prime}$ be the channel-aligned features; the goal   \n111 of channel alignment is to learn a set of linear transform $\\mathcal{W}=\\{W_{1},W_{2},\\cdots,W_{n}|W_{i}\\in\\mathbb{R}^{D_{i}\\times\\bar{D}^{\\prime}}\\}$ .   \n112 In the new $D^{\\prime}$ dimensional space, channels are aligned. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{V}^{\\prime}=\\mathcal{V}\\odot\\mathcal{W}=\\{V_{1}W_{1},V_{2}W_{2},\\cdot\\cdot\\cdot\\cdot,V_{n}W_{n}|V_{i}W_{i}\\in\\mathbb{R}^{P\\times D^{\\prime}}\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "113 Brain Prediction To produce a learning signal for channel align $\\mathcal{W}$ , features from $\\mathcal{V}^{\\prime}$ are summed   \n114 (not concatenated) to do brain prediction. Let $\\boldsymbol{Y}\\in\\mathbb{R}^{1\\times N}$ be the brain prediction target, where $N$ is   \n115 the number of flattened 3D brain voxels, and 1 indicates that each voxel\u2019s response is a scalar value.   \n116 Let $F_{\\theta}:\\mathbb{R}^{P\\times D^{\\prime}}\\Rightarrow\\mathbb{R}^{1\\times N}$ be the learned brain encoding model; without loss of generalizability, we   \n117 set $F_{\\theta}$ as global average pooling then linear weight $\\bar{\\beta}_{\\theta}\\overset{\\bar{\\mathbf{\\alpha}}}{\\in}\\mathbb{R}^{D^{\\prime}\\times N}$ and bias $\\epsilon_{\\theta}\\in\\mathbb{R}^{1\\times N}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left[\\operatorname*{Avg\\,Poo1}_{p\\in P}(\\frac1n\\sum_{i=1}^{n}(V_{i}W_{i}))\\times\\beta_{\\theta}+\\epsilon_{\\theta}\\right]\\Rightarrow Y\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "118 Channel in the Brain\u2019s Space Let $\\mathcal{B}=\\{B_{1},B_{2},\\cdot\\cdot\\cdot,B_{n}|B_{i}\\in\\mathbb{R}^{P\\times N}\\}$ be the set of channel   \n119 activations in the brain\u2019s space. By defining $B_{i}:=V_{i}W_{i}\\times\\beta_{\\theta}$ , we have the brain response prediction   \n120 $\\begin{array}{r}{Y=\\operatorname{Avg}\\operatorname{Pool}_{p\\in P}(\\frac{1}{n}\\sum_{i=1}^{\\dot{n}}B_{i})\\dot{+}\\;\\epsilon_{\\theta}}\\end{array}$ (Eq. (2)). Intuitively, we linearly transformed the activation   \n121 to the brain\u2019s space, such that the activation from all slots sum up to the brain response prediction. ", "page_idx": 3}, {"type": "text", "text": "122 2.2 Graph Spectral Clustering ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "123 Spectral Clustering We use spectral clustering for visual concepts discovery and image-channel   \n124 analysis; it provides 1) soft-cluster embedding space and 2) unsupervised hierarchical image seg  \n125 mentation. Normalized Cut (Shi and Malik, 2000) partitions the graph into sub-graphs with minimal   \n126 cost of breaking edges. It embeds the graph into a lower dimensional eigenvector representation,   \n127 where each eigenvector is a hierarchical sub-graph assignment.   \n128 Let $A\\,\\in\\,\\mathbb{R}^{M\\times M}$ be the symmetric affinity matrix, where $M$ denotes the total number of image   \n129 patches. Given channel aligned features $\\dot{V^{\\prime}}\\in\\mathbb{R}^{M\\times D^{\\prime}}$ , we define $\\pmb{A}_{i j}:=\\exp(\\cos(V_{i}^{\\prime},V_{j}^{\\prime})-1)$   \n130 such that $A_{i j}>0$ measures the similarity between data $i$ and $j$ . The spectral clustering embedding   \n131 $\\boldsymbol{X}\\in\\mathbb{R}^{M\\times\\dot{C}}$ is solved by the top $C$ eigenvectors of the following generalized eigenproblem: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n(D^{-1/2}A D^{-1/2})X=X\\Lambda\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where $_{D}$ is the diagonal degree matrix $\\begin{array}{r}{D_{i i}=\\sum_{j}A_{i j}.}\\end{array}$ , $\\Lambda$ is diagonal eigenvalue matrix. ", "page_idx": 3}, {"type": "text", "text": "133 Nystrom-like Approximation Computing eigenvectors for $A\\in\\mathbb{R}^{M\\times M}$ is prohibitively expen  \n134 sive for enormous $M$ with a time complexity of $O(M^{3})$ . The original Nystrom approximation   \n135 method (Fowlkes et al., 2004) reduced the time complexity to $O(m^{3}\\,\\bar{+}\\,m^{2}M)$ by solving eigenvec  \n136 tors on sub-sampled graph $A^{\\prime}\\in\\mathbb{R}^{m\\times m}$ , where $m\\ll M$ . In particular, the orthogonalization step   \n137 of eigenvectors introduced the time complexity of $O(m^{2}M)$ . Because our Nystrom-like approxi  \n138 mation trades the $O(m^{2}M)$ orthogonalization term with the $\\mathbf{K}$ -nearest neighbor, our Nystrom-like   \n139 approximation reduced the time complexity to $O(m^{3}+m M)$ .   \n140 Our Nystrom-like Approximation first solves the eigenvector $\\pmb{X}^{\\prime}\\in\\mathbb{R}^{m\\times C}$ on a sub-sampled graph   \n141 $\\pmb{A}^{\\prime}\\ \\in\\ \\mathbb{R}^{m\\times m}$ using Equation (3), then propagates the eigenvector from the sub-graph $m$ nodes   \n142 to the full-graph $M$ nodes. Let $\\tilde{\\boldsymbol{X}}\\:\\in\\:\\mathbb{R}^{M\\times C}$ be the approximation $\\tilde{X}\\,\\approx\\,X$ . The eigenvector   \n143 approximation $\\tilde{\\boldsymbol{X}}_{i}$ of full-graph node $i\\leq M$ is assigned by averaging the top $\\mathbf{K}$ -nearest neighbors\u2019   \n144 eigenvector $X_{k}^{\\prime}$ from the sub-graph nodes $k\\leq m$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal K}_{i}=K N N({\\cal A}_{*i};m,K)=\\arg\\operatorname*{max}_{k\\le m}\\sum_{k=1}^{K}{\\cal A}_{k i}}}\\\\ {{\\displaystyle\\tilde{\\cal X}_{i}=\\frac{1}{\\sum_{k\\in{\\cal K}_{i}}{\\cal A}_{k i}}\\sum_{k\\in{\\cal K}_{i}}{\\cal A}_{k i}{\\cal X}_{k}^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 where $K N N({\\bf{A}}_{*i};m,K)$ denotes KNN from full-graph node $i\\leq M$ to sub-graph nodes $k\\leq m$ . ", "page_idx": 3}, {"type": "text", "text": "146 2.3 Affinity Eigen-constraints as Regularization for Channel Align ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "147 While brain prediction can provide strong supervi  \n148 sion for the learned channel align operation, we ob  \n149 served that the quality of unsupervised segmentation   \n150 dropped after the channel alignment. To address this   \n151 issue, a regularization term is added: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{e i g e n}=\\|\\pmb{X}_{b}\\pmb{X}_{b}^{T}-\\pmb{X}_{a}\\pmb{X}_{a}^{T}\\|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "152 where $X_{b}$ and $\\boldsymbol{X}_{a}\\;\\;\\in\\;\\;\\mathbb{R}^{\\tilde{m}\\times c}$ are affinity matrix   \n153 eigenvectors before and after channel alignment, re  \n154 spectively; $\\tilde{m}\\,=\\,100$ are randomly sampled nodes   \n155 in a mini-batch and $c\\,=\\,6$   \n156 tering eigenvectors in dot-product space, invariant to random rotations in eigenvectors. We found ", "page_idx": 3}, {"type": "table", "img_path": "opdiIAHfBr/tmp/441c652bcaedf0470ba67306259026e613f4d71eed520d63d596567201e76875.jpg", "table_caption": ["Table 1: Affinity eigen-constraints improved brain score ( $\\mathit{R}^{2}$ : variance explained). "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "are the top eigenvectors. The eigen-constraint preserves spectral clus157 adding eigen-constraints improved both the performance of segmentation (Figure 5) and the brain 158 prediction score (Table 1). ", "page_idx": 3}, {"type": "text", "text": "159 3 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "160 Our spectral clustering analysis aims to discover visual concepts that share the same pattern of   \n161 channel activation across different models and layers. However, implementing spectral clustering   \n162 analysis comes with two main challenges. First, the models sit in different feature spaces, so direct   \n163 clustering will not reveal their overlap and similarities. Second, when scaling up to a large graph,   \n164 spectral clustering is computationally expensive.   \n165 To address the first challenge, we developed our channel align transform to align features into a   \n166 universal space. We extracted features from all 12 layers of the CLIP (ViT-B, OpenAI) (Radford   \n167 et al., 2021), DINOv2 (ViT-B with registers) (Darcet et al., 2024), and MAE (ViT-B) (He et al.,   \n168 2022) and then transformed features from each layer into the universal feature space.   \n169 To address the second challenge, we developed our Nystrom-like approximation to reduce the com  \n170 putational complexity. We extracted features from 1000 ImageNet (Deng et al., 2009) images, with   \n171 each image consisting of 197 patches per layer. The entire product space of all images and fea  \n172 tures totaled $M=7\\mathrm{e}{+}6$ nodes, from which we applied our Nystrom-like approximation with sub  \n173 sampled $m=5\\mathrm{e}{+4}$ nodes and KNN $K=100$ , computing the top 20 eigenvectors.   \n174 To visualize the affinity eigenvectors, the top 20 eigenvectors were reduced to a 3-dimensional space   \n175 by t-SNE, and a color value was assigned to each node by the RGB cube. We call this approach   \n176 AlignedCut color. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "opdiIAHfBr/tmp/d9d506abaf73afe5543aafc4795584095461f16043b40436b3abd355a25dd5a7.jpg", "img_caption": ["Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "177 In Figure 4, we displayed the analysis, AlignedCut color, and made the following observations: ", "page_idx": 4}, {"type": "text", "text": "178 1. In CLIP layer-5, DINO layer-6, and MAE layer-8, there is class-agnostic figure-ground sepa  \n179 ration, with foreground objects from different categories grouped into the same AlignedCut color.   \n180 2. In CLIP layer-9, there is a class-specific separation of foreground objects, with foreground   \n181 objects grouped into AlignedCut colors with associated semantic categories.   \n182 3. Before layer-3, CLIP and DINO produce the same AlignedCut color regardless of the image   \n183 input. From layer-4 onwards, the AlignedCut color smoothly changes over layers.   \n185 In this section, we benchmark each layer in CLIP with unsupervised segmentation. The key findings   \n186 from this benchmarking are: 1) The figure-ground representation emerges at CLIP layer-4 and is   \n187 preserved in subsequent layers; 2) Categories emerge over layers, peaking at layer-9 and layer-10. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "opdiIAHfBr/tmp/9992223aab4439bb500401a663b5283bf0d41551acb51e795e25023a6ddf19c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 5: Unsupervised segmentation scores from spectral clustering on each CLIP layer. ImageNetsegmentation dataset is used with binary figure-ground labels, and the mIoU score peaks plateau from layer-4 to layer-10. In PASCAL VOC with 20 class labels, the mIoU score peaks at layer-9. ", "page_idx": 5}, {"type": "text", "text": "188 From which layers did the figure-ground and category representations emerge? We conducted   \n189 experiments that compared the unsupervised segmentation scores across layers, tracing how well   \n190 each representation is encoded at each layer. We used two datasets: a) ImageNet-segmentation   \n191 (Guillaumin et al., 2014) with binary figure-ground labels, and b) PASCAL VOC (Everingham et al.,   \n192 2010) with 20 category labels. The results are presented in Figure 5. On the ImageNet-segmentation   \n193 benchmark, the score peaks at layer-4 $\\mathrm{(mIoU=}0.6)$ ) and plateaus in subsequent layers, suggesting that   \n194 the figure-ground representation is encoded and preserved from layer-4 onwards. On the PASCAL   \n195 VOC benchmark, the score peaks at layer-9 and layer-10 $\\scriptstyle{\\mathrm{mIoU}}=0.5\\$ ) even though it is low at layer-4   \n196 $(\\mathrm{mIoU}{=}0.2)$ ), indicating that category information is encoded at layer-9 and layer-10. Overall, we   \n197 conclude that the figure-ground representation emerges before the category representation. ", "page_idx": 5}, {"type": "text", "text": "198 3.2 Visual concepts: class-agnostic figure-ground ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 In this section, we use brain activation heatmaps and image similarity heatmaps to describe figure  \n200 ground visual concepts. The key findings from these heatmaps are: 1) The figure vs. ground pixels   \n201 activate different channels; 2) The figure-ground visual concept is class-agnostic; 3) The figure  \n202 ground visual concept is consistent across models. ", "page_idx": 5}, {"type": "image", "img_path": "opdiIAHfBr/tmp/853be95783e6c505ee42e253c4094a3ad091282396cdeaa5acc19fd991e349fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 6: The figure-ground visual concepts in CLIP layer-5. Left: Mean activation of foreground or background pixels, linearly transformed to the brain\u2019s space. Right: Cosine similarity from one reference pixel marked. The figure-ground visual concepts are agnostic to image categories. ", "page_idx": 5}, {"type": "text", "text": "203 How can the channel activation patterns of the figure-ground visual concept be described? We   \n204 averaged the channel activations from foreground and background pixels, using the ground-truth   \n205 labels from the ImageNet-segmentation dataset. The averaged channel activations were transformed   \n206 into the brain\u2019s space. In Figure 6, foreground pixels exhibit positive activation in early visual brain   \n207 ROIs (V1 to V4) and the face-selective ROI (FFA), while negatively activating place-selective ROIs   \n208 (OPA and PPA). Interestingly, background pixels activate the reverse pattern compared to foreground   \n209 pixels. Overall, the figure and ground pixels activate distinct brain ROIs.   \n210 Is the figure-ground visual concept class-agnostic? We manually selected one pixel and computed   \n211 the cosine similarity to all of the other image pixels. In Figure 6, the results demonstrate that one   \n212 pixel (on the human) could segment out foreground objects from all other classes (shark, dog, cat,   \n213 rabbit). The same result holds true for one background pixel. We conclude that the figure-ground   \n214 visual concept is class-agnostic. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "opdiIAHfBr/tmp/cf3f602e9ef7cd7388c7c3e5e393acee7596fa4e100fbc7715584b55c9ee1e7f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 7: The same figure-ground visual concepts are found in CLIP, DINO and MAE. Left: Mean activation of all foreground (top) and background (bottom) pixels; the three models exhibit similar activation patterns. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors; the three models show similar grouping colors for foreground pixels. ", "page_idx": 6}, {"type": "text", "text": "215 Is the figure-ground visual concept consistent across models? We performed the channel analysis   \n216 for CLIP, DINO, and MAE. In Figure 7, the foreground or background pixels activates similar   \n217 brain ROIs across the three models. Additionally, spectral clustering grouped the representations of   \n218 foreground objects into similar colors for CLIP and DINO (light blue), the grouping for MAE is less   \n219 similar (dark blue). Overall, the figure-ground visual concept is consistent across models. ", "page_idx": 6}, {"type": "text", "text": "220 3.3 Visual concepts: categories ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "221 In this section we use AlignedCut to discover category visual concepts. The key findings from the   \n222 category visual concepts are: 1) Class-specific visual concepts activate diverse brain regions; 2)   \n223 Visual concepts with higher channel activation values are more consistent.   \n224 How does each class-specific concept activate the channels? To answer this question, we sam  \n225 pled class-specific concepts from CLIP layer-9. First, we used farthest point sampling to identify   \n226 candidate centers in the 3D spectral-tSNE space. Then, each candidate center was grouped with its   \n227 neighboring pixels within an Euclidean sphere in the spectral-tSNE space. Finally, the channel acti  \n228 vations of the grouped pixels were averaged to produce the mean channel activation for each visual   \n229 concept. In Figure 8, Concept 1 (duck, goose) negatively activates late brain regions; Concept 2   \n230 (snake, turtle) positively activates early brain regions and also FFA; Concept 3 (dog) negatively ac  \n231 tivates early brain regions. Overall, category-specific visual concepts activate diverse brain regions.   \n232 How do we quantify the consistency of each visual concept? Qualitatively, Concept 1 exhibits more   \n233 consistent coloring (Figure 8, pink) than Concept 3 (purple). To further quantify this observation, we   \n234 computed the mean and standard deviation of channel activations for each Euclidean sphere centered   \n235 on a concept. In Figure 8, there is a reverse U-shape relation between magnitude and standard   \n236 deviation. The reverse U-shape implies that larger absolute mean channel activation corresponds   \n237 to lower standard deviation. Overall, higher channel activation magnitudes suggest more consistent   \n238 visual concepts. ", "page_idx": 6}, {"type": "image", "img_path": "opdiIAHfBr/tmp/47f8ffce0354daea0b0ce39f9b85383afb49358c29a347b855f1201182c3b9af.jpg", "img_caption": ["Figure 8: Category visual concepts in CLIP layer-9. Left: Mean activation of all pixels within an Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate different brain regions. Middle: The standard deviation negatively correlates with absolute mean activations. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "239 3.4 Transition of visual concepts over layers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "240 In this section, instead of using 3D spectral-tSNE, we use 2D spectral-tSNE to trace the layer-to  \n241 layer feature computation. The key findings of spectral-tSNE in 2D are: 1) The figure vs. ground   \n242 pixels are encoded in separate spaces in late layers; 2) The representations for foreground and back  \n243 ground bifurcate at CLIP layer-4 and DINO layer-5.   \n244 How does the network encode figure and ground pixels in each layer? We performed spectral   \n245 clustering and 2D t-SNE on the top 20 eigenvectors to project all layers into a 2D spectral-tSNE   \n246 space. In Figure 9, we found that all foreground and background pixels are grouped together in   \n247 each early layer. Each early layer (dark dots) forms an isolated cluster separate from other layers,   \n248 while late layers (bright dots) are grouped in the center. In the late layers, there is a separation   \n249 where foreground pixels occupy the upper part of 2D spectral-tSNE space, while background pixels   \n250 occupy the middle part. Overall, foreground and background pixels are encoded in separate spaces   \n251 in late layers.   \n252 How does the network process each pixel from layer to layer? In the 2D spectral-tSNE plot,   \n253 we traced the trajectory for each pixel from layer-3 to the last layer. In Figure 9, we found that   \n254 the trajectories for foreground and background pixels bifurcate: foreground pixels (person, horse,   \n255 car) traverse to the upper side and remain within the upper side; background pixels (grass, road,   \n256 sky) jump between the middle right and left sides. The same bifurcation is consistently observed   \n257 for CLIP from layer-3 to layer-4 and DINO from layer-4 to layer-5. Furthermore, to quantify the   \n258 bifurcation for foreground and background pixels, we first sampled 5 visual concepts from CLIP   \n259 layer-3 and layer-4. Then, we measured the transition probability between visual concepts, defined   \n260 as the proportion of pixels that transited from an Euclidean circle around concept A to a circle   \n261 around concept B. In Figure 10, the transition probability of foreground pixels to the upper side   \n262 (A1 to B0) is higher than that of background pixels (0.44 vs. 0.16), while the transition probability   \n263 of background pixels to the right side (A4 to B4) is higher than that of foreground pixels (0.36 vs.   \n264 0.06). Overall, this suggests a bifurcation of figure and ground pixel representations at the middle   \n265 layers of both CLIP and DINO. ", "page_idx": 7}, {"type": "image", "img_path": "opdiIAHfBr/tmp/3847f50e5a4abdc306dd109f4d56db8baae9b12a9060b254d475cc2743725fff.jpg", "img_caption": ["Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmentation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "opdiIAHfBr/tmp/cef9b754b9be52638ddabb45c0e7c823766ab2e2e8a414d731637691adb5cc47.jpg", "img_caption": ["Figure 10: Transition probability of visual concepts from CLIP layer-3 to layer-4. Left: Five visual concepts sampled from CLIP layer-3 and layer-4. Right: Transition probability measured separately for foreground and background pixels; a bifurcation occurs where foreground pixels have more traffic to concept B0, while background pixels have more traffic to concepts B3 and B4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "266 4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "267 Mechanistic Interpretability is a field of study that intends to understand and explain the inner   \n268 working mechanisms of deep networks. One approach is to interpret individual neurons (Bau et al.,   \n269 2017; Dravid et al., 2023) and circuit connections between neurons (Olah et al., 2020). Another ap  \n270 proach is to interpret transformer attention heads (Gandelsman et al., 2024) and circuit connections   \n271 between attention heads (Wang et al., 2023a). Other approaches also looked into the role of patch   \n272 tokens (Sun et al., 2024). These approaches made the assumption that channels are aligned within   \n273 the same model; we compare across models by actively aligning the channels to a universal space.   \n274 Spectral Clustering is a graphical method to analyze data grouping in the eigenvector space. Spec  \n275 tral methods have been widely used for unsupervised image segmentation (Shi and Malik, 2000;   \n276 von Luxburg, 2007; Wu et al., 2018; Wang et al., 2023b). One major challenge for applying spectral   \n277 clustering to large graphs is the complexity scaling issue. To solve the scaling issue, the Nystrom   \n278 approximation (Fowlkes et al., 2004) approaches solve eigenvectors on sub-sampled graphs and then   \n279 propagate to the full graph. Another approach is the gradient-based eigenvector solver (Zhang et al.,   \n280 2023), which solves the eigenvectors in mini-batches. Our proposed Nystrom-like approximation   \n281 achieves a computational speedup over the original Nystrom approximation, albeit at the expense of   \n282 weakened orthogonality of the eigenvectors.   \n283 Brain Encoding Model is widely used by the computational neuroscience community (Kriegeskorte   \n284 and Douglas, 2018). They have been using deep nets to explain the brain\u2019s function. One approach   \n285 is to use the gradient of the brain encoding model to find the most salient image features (Sarch   \n286 et al., 2023). Another approach generate text caption for brain activation (Luo et al., 2024). Other   \n287 approaches compare brain prediction performance for different models (Schrimpf et al., 2020). The   \n288 field focused on using deep nets as a tool to explain the brain\u2019s function; we go in the opposite   \n289 direction by using the brain to explain deep nets. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "290 5 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "291 We present a novel approach to interpreting deep neural networks by leveraging brain data. Our   \n292 fundamental innovation is twofold: First, we use brain prediction as guidance to align channels from   \n293 different models into a universal feature space; Second, we developed a Nystrom-like approximation   \n294 to scale up the spectral clustering analysis. Our key discovery is that recurring visual concepts exist   \n295 across networks and layers; such concepts correspond to different levels of objects, ranging from   \n296 figure-ground to categories. Additionally, we quantified the information flow from layer to layer,   \n297 where we found a bifurcation of figure-ground visual concepts.   \n298 Limitations. While the learned channel align transformation projects all features onto a universal   \n299 feature space, the nature of learned transformation does not preserve all the information. There   \n300 is a small drop in unsupervised segmentation performance after channel alignment, which is not   \n301 fully addressed by our proposed eigen-constraint regularization. Secondly, as a trade-off for faster   \n302 computation, our Nystrom-like approximation does not produce strictly orthogonal eigenvectors. To   \n303 produce expressive eigenvectors, our approximation relies on using larger sub-sample sizes than the   \n304 original Nystrom method. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "305 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "306 Allen, E. J., St-Yves, G., Wu, Y., Breedlove, J. L., Prince, J. S., Dowdle, L. T., Nau, M., Caron,   \n307 B., Pestilli, F., Charest, I., Hutchinson, J. B., Naselaris, T., and Kay, K. (2022). A massive 7T   \n308 fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience,   \n309 25(1):116\u2013126. Number: 1 Publisher: Nature Publishing Group. 3   \n310 Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. (2017). Network Dissection: Quantifying   \n311 Interpretability of Deep Visual Representations. In Computer Vision and Pattern Recognition. 9   \n312 Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. (2024). Vision Transformers Need Registers.   \n313 In The Twelfth International Conference on Learning Representations. 5   \n314 Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large  \n315 scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern   \n316 Recognition, pages 248\u2013255. 5   \n317 Dravid, A., Gandelsman, Y., Efros, A. A., and Shocher, A. (2023). Rosetta Neurons: Mining the   \n318 Common Units in a Model Zoo. In Proceedings of the IEEE/CVF International Conference on   \n319 Computer Vision (ICCV), pages 1934\u20131943. 1, 9   \n320 Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2010). The Pascal   \n321 Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):303\u2013   \n322 338. 6   \n323 Fowlkes, C., Belongie, S., Chung, F., and Malik, J. (2004). Spectral grouping using the Nystrom   \n324 method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214\u2013225. 4, 9   \n325 Gandelsman, Y., Efros, A. A., and Steinhardt, J. (2024). Interpreting CLIP\u2019s Image Representation   \n326 via Text-Based Decomposition. In The Twelfth International Conference on Learning Represen  \n327 tations. 9   \n328 Gifford, A. T., Lahner, B., Saba-Sadiya, S., Vilas, M. G., Lascelles, A., Oliva, A., Kay, K., Roig, G.,   \n329 and Cichy, R. M. (2023). The Algonauts Project 2023 Challenge: How the Human Brain Makes   \n330 Sense of Natural Scenes. arXiv:2301.03198 [cs, q-bio]. 3   \n331 Guillaumin, M., Ku\u00a8ttel, D., and Ferrari, V. (2014). ImageNet Auto-Annotation with Segmentation   \n332 Propagation. International Journal of Computer Vision, 110(3):328\u2013348. 6   \n333 He, K., Chen, X., Xie, S., Li, Y., Dolla\u00b4r, P., and Girshick, R. (2022). Masked Autoencoders Are   \n334 Scalable Vision Learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n335 Pattern Recognition (CVPR), pages 16000\u201316009. 5   \n336 Kriegeskorte, N. and Douglas, P. K. (2018). Cognitive computational neuroscience. Nature Neuro  \n337 science, 21(9):1148\u20131160. Publisher: Nature Publishing Group. 9   \n338 Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollar, P. (2017). Focal Loss for Dense Object   \n339 Detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV). 14   \n340 Luo, A., Henderson, M. M., Tarr, M. J., and Wehbe, L. (2024). BrainSCUBA: Fine-Grained Natural   \n341 Language Captions of Visual Cortex Selectivity. In The Twelfth International Conference on   \n342 Learning Representations. 9   \n343 Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020). Zoom In: An   \n344 Introduction to Circuits. Distill, 5(3):e00024.001. 9   \n345 Prince, J. S., Charest, I., Kurzawski, J. W., Pyles, J. A., Tarr, M. J., and Kay, K. N. (2022). Im  \n346 proving the accuracy of single-trial fMRI response estimates using GLMsingle. eLife, 11:e77599.   \n347 Publisher: eLife Sciences Publications, Ltd. 3   \n348 Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,   \n349 Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning Transferable Visual Models   \n350 From Natural Language Supervision. In Meila, M. and Zhang, T., editors, Proceedings of the 38th   \n351 International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning   \n352 Research, pages 8748\u20138763. PMLR. 5   \n353 Sarch, G. H., Tarr, M. J., Fragkiadaki, K., and Wehbe, L. (2023). Brain Dissection: fMRI-trained   \n354 Networks Reveal Spatial Selectivity in the Processing of Natural Images. In Thirty-seventh Con  \n355 ference on Neural Information Processing Systems. 9   \n356 Schrimpf, M., Kubilius, J., Lee, M. J., Murty, N. A. R., Ajemian, R., and DiCarlo, J. J. (2020). Inte  \n357 grative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence. Neuron.   \n358 9   \n359 Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on   \n360 Pattern Analysis and Machine Intelligence, 22(8):888\u2013905. 4, 9   \n361 Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. (2024). Massive Activations in Large Language Models.   \n362 arXiv:2402.17762 [cs]. 9   \n363 von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and Computing, 17(4):395\u2013416.   \n364 9   \n365 Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. (2023a). Interpretabil  \n366 ity in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small. In The Eleventh   \n367 International Conference on Learning Representations. 9   \n368 Wang, X., Girdhar, R., Yu, S. X., and Misra, I. (2023b). Cut and learn for unsupervised object   \n369 detection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer   \n370 Vision and Pattern Recognition, pages 3124\u20133134. 9   \n371 Wu, Z., Xiong, Y., Stella, X. Y., and Lin, D. (2018). Unsupervised Feature Learning via Non  \n372 Parametric Instance Discrimination. In Proceedings of the IEEE Conference on Computer Vision   \n373 and Pattern Recognition. 9   \n374 Yang, H., Gee, J., and Shi, J. (2024). Brain Decodes Deep Nets. arXiv:2312.01280 [cs]. 1   \n375 Zhang, X., Yunis, D., and Maire, M. (2023). Deciphering \u2019What\u2019 and \u2019Where\u2019 Visual Pathways   \n376 from Spectral Clustering of Layer-Distributed Neural Representations. arXiv:2312.06716 [cs]. 9 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "377 A Appendix overview ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "378 1. Appendix B summarizes background of brain ROIs.   \n379 2. Appendix C is implementation details   \n380 2.1. Additional regularization terms   \n381 2.2. Brain encoding model training loss function   \n382 2.3. Unsupervised segmentation evaluation pipeline   \n383 2.4. Nystrom-like approximation for t-SNE   \n384 3. Appendix D lists more image examples from the 3D spectral-tSNE.   \n385 4. Appendix E lists figure-ground channel activation for every model and layer.   \n386 5. Appendix F lists more example category-specific visual concepts.   \n387 6. Appendix G lists more example pixels from the 2D spectral-tSNE information flow. ", "page_idx": 11}, {"type": "image", "img_path": "opdiIAHfBr/tmp/e16b708b7bec564d4468acaf6b89f9e82dbd7aa1f0ef02a09e18645a458b3be3.jpg", "img_caption": ["Figure 11: Brain Region of Interests (ROIs). V1v: ventral stream, V1d: dorsal stream. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Table 2: Known function and selectivity of brain region of interests (ROIs). ROI name V1 V2 V3 V4 EBA FBA OFA FFA OPA PPA OWFA VWFA Known Function/Selectivity primary visual mid-level body face navigation scene words ", "page_idx": 12}, {"type": "text", "text": "389 This section briefly summarizes the known functions of key brain regions of interest (ROIs). Fig  \n390 ure 11 provides an overview of these brain ROIs. Table 2 lists the known functions and selectivities   \n391 for each ROI.   \n392 In brief, V1 to V3 are the primary visual stream, which is further divided into ventral (lower) and dor  \n393 sal (upper) streams. V4 is a mid-level visual area. EBA (extrastriate body area) and FBA (fusiform   \n394 body area) are selectively responsive to bodies, while FFA (fusiform face area) and OFA (occipital   \n395 face area) show selectivity for faces. OWFA (occipital word form area) and VWFA (visual word   \n396 form area) are selective for written words. PPA (parahippocampal place area) exhibits selectivity for   \n397 scenes and places, and OPA (occipital place area) is involved in navigation and spatial reasoning.   \n398 Visual information processing in the brain follows a hierarchical, feedforward organization. Be  \n399 ginning in the primary visual cortex (V1) and progressing through higher visual areas like V2, V3,   \n400 and V4, neurons exhibit increasingly large receptive fields and represent increasingly abstract visual   \n401 concepts. While neurons in V1 encode low-level features like edges and orientations within a small   \n402 portion of the visual field, neurons in V4 synthesize more complex patterns and object representa  \n403 tions across a larger area of the visual input. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "404 C Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "405 C.1 Additional Regularization for Channel Align Transformation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "406 Additional Regularization are added to the channel align transform to ensure good properties of the   \n407 aligned features: 1) zero-centered, 2) small covariance between channels, and 3) focal loss.   \n408 Zero-centered regularization. We did not apply ${\\bf Z}$ -score normalization to the extracted features;   \n409 instead, we added a regularization term to ensure the transformed features are zero-centered. Recall   \n410 that the channel-aligned transformed feature $V^{\\prime}\\in\\mathbb{R}^{M\\times D^{\\prime}}$ , where $M$ is the number of data points   \n411 and $D^{\\prime}$ is the hidden dimension. The zero-center loss is defined as: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{zero}}=\\frac{1}{D^{\\prime}}\\frac{1}{M}\\sum_{i\\leq M,j\\leq D^{\\prime}}v_{i j}^{\\prime}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "412 Covariance regularization. We used the covariance loss to minimize the off-diagonal elements in   \n413 the covariance matrix of the transformed feature $C(V^{\\prime})$ , aiming to bring them close to 0. Recall   \n414 that channel align transformed feature $V^{\\prime}\\in\\mathbb{R}^{M\\times D^{\\prime}}$ , where $M$ is number of data, $D^{\\prime}$ is the hidden   \n415 dimension. The covariance loss is defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{cov}}=\\frac{1}{D^{\\prime}}\\sum_{i\\neq j}[C(V^{\\prime})]_{i,j}^{2},\\ \\mathrm{where}\\ C(V^{\\prime})=\\frac{1}{M-1}\\sum_{i=1}^{M}\\left(v_{i}^{\\prime}-\\bar{v^{\\prime}}\\right)\\left(v_{i}^{\\prime}-\\bar{v^{\\prime}}\\right)^{T},\\bar{v^{\\prime}}=\\frac{1}{M}\\sum_{i=1}^{M}v_{i}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "416 Focal Loss. Lin et al. (2017) introduced focal loss, which dynamically assigns smaller weights to   \n417 the loss function for hard-to-classify classes. In our scenario, we apply spectral clustering on the   \n418 affinity matrix ${\\bf A}_{a}\\in\\mathbb{R}^{M\\times M}$ after performing the channel alignment transform, where $M$ represents   \n419 the number of data points. Due to the characteristics of spectral clustering, disconnected edges   \n420 play a more critical role than connected edges. Adding an edge between disconnected clusters   \n421 significantly reshapes the eigenvectors, while adding edges to connected clusters has only a minor   \n422 impact. Therefore, we aim to assign larger weights to disconnected edges in the loss function: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{e i g e n}=\\big\\|(\\pmb{X}_{b}\\pmb{X}_{b}^{T}-\\pmb{X}_{a}\\pmb{X}_{a}^{T})*\\exp(-\\pmb{A}_{b})\\big\\|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "423 where $\\pmb{A}_{b}\\in\\mathbb{R}^{M\\times M}$ is the affinity matrix before the channel alignment transform, element wise dot  \n424 product to $\\exp(-A_{b})$ assigned larger wights for disconnected edges. $\\pmb{X}_{b}\\in\\mathbb{R}^{M\\times C}$ , $X_{a}\\in\\mathbb{R}^{M\\times C}$   \n425 are eigenvectors before and after channel align transform, respectively. ", "page_idx": 13}, {"type": "text", "text": "426 C.2 Brain Encoding Model Training Loss ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "427 Let $\\boldsymbol{Y}\\in\\mathbb{R}^{1\\times N}$ represent the brain prediction target, where $N$ is the number of flattened 3D brain   \n428 voxels, and the 1 indicates that each voxel\u2019s response is a scalar value. $\\hat{Y}$ is the model\u2019s predicted   \n429 brain response. The brain encoding model training loss is the L1 loss: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{b r a i n}=\\|Y-\\hat{Y}\\|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "430 C.3 Total Training Loss ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "431 The total training loss is a combination of the following components: 1) brain encoding model loss,   \n432 2) eigen-constraint regularization, 3) zero-centered regularization, and 4) covariance regularization: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{b r a i n}+\\lambda_{e i g e n}\\mathcal{L}_{e i g e n}+\\lambda_{z e r o}\\mathcal{L}_{z e r o}+\\lambda_{c o v}\\mathcal{L}_{c o v}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "433 where we set $\\lambda_{e i g e n}=1,\\lambda_{z e r o}=0.01,\\lambda_{c o v}=0.01.$ ", "page_idx": 13}, {"type": "text", "text": "434 C.4 Oracle-based Unsupervised Segmentation Evaluation Pipeline ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "435 Our unsupervised segmentation pipeline aims to benchmark and compare the performance across   \n436 each single layer of the CLIP model. The evaluation pipeline is oracle-based: ", "page_idx": 14}, {"type": "text", "text": "437 1. Apply spectral clustering jointly across all images, taking the top 10 eigenvectors. 2. For each class of object (plus one background class), use ground-truth labels from the dataset   \n439 to mask out the pixels and their eigenvectors, and then use the mean of the eigenvectors to define a   \n440 center for each class.   \n441 3. Compute the cosine similarity of each pixel to all class centers. 4. For each pixel, if the maximum similarity to all classes is less than a threshold value, assign this pixel to the background class. 5. Assign pixels (with a similarity greater than the threshold value) to the class with the maximum   \n445 similarity.   \n446 There\u2019s one hyper-parameter, the threshold value that requires different optimal value for each layer   \n447 of CLIP. To ensure a fair comparison across all layers, the threshold value is grid-searched from 10   \n448 evenly spaced values between 0 and 1, the maximum mIoU score in the grid search is taken for each   \n449 layer. ", "page_idx": 14}, {"type": "text", "text": "450 C.5 Nystrom-like approximation for t-SNE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "451 To visualize the eigenvectors, we applied t-SNE to the eigenvectors $\\boldsymbol{X}\\in\\mathbb{R}^{M\\times C}$ , where the number   \n452 of data points $M$ span the product space of models, layers, pixels, and images. Due to the enormous   \n453 size of $M=7\\mathrm{e}{+6}$ nodes, t-SNE suffered from complexity scaling issues. We again applied our   \n454 Nystrom-like approximation to t-SNE, with sub-sampled $m=10\\mathrm{e}{+4}$ nodes and KNN $K=1$ .   \n455 It\u2019s worth noting that, since the non-linear distance adjustment in t-SNE, it\u2019s crucial to use only one   \n456 nearest neighbor $K=1$ for t-SNE. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "457 C.6 Computation Resource ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "458 All of our experiments are performed on one consumer-grade RTX 4090 GPU. The brain encoding   \n459 model training took 3 hours on 4GB of VRAM, spectral clustering eigen-decomposition on large   \n460 graph took 10 minutes on 10GB of VRAM and 60GB of CPU RAM. ", "page_idx": 14}, {"type": "text", "text": "461 C.7 Code Release ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "462 Our code will be publicly released upon publication. ", "page_idx": 14}, {"type": "image", "img_path": "opdiIAHfBr/tmp/03d9dd26fd1c578bf4e92f7a4a751b7bc1a5664862f61752ee7838c36fb0de4e.jpg", "img_caption": ["463 D 3D spectral-tSNE ", "Figure 12: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "opdiIAHfBr/tmp/f500554ff8ab055a2bb3ce858147c03d3a5b9b5cda9f65a9184c9eb8fb517b76.jpg", "img_caption": ["Figure 13: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "opdiIAHfBr/tmp/4245cc08059b9540113d4318951ea82b0d568a532b170f82af7deb70440082e6.jpg", "img_caption": ["Figure 14: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "opdiIAHfBr/tmp/dfda3ed03f751d9d87a63c7a6d29764b15f300c5388a0d50f7907c04a2d7ae1c.jpg", "img_caption": ["Figure 15: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "opdiIAHfBr/tmp/eda1a1196253ff771fc1f6fecc8396d7da265fbed2bcd36d45ef1926f0c9d059.jpg", "img_caption": ["464 E Figure-ground Channel Activation from All Layers and Models "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 16: Mean activation of foreground or background pixels at each layer of CLIP, DINO and MAE. Channel activations are linearly transformed to the brain\u2019s space. Large absolute activation value means more consistent visual concepts. ", "page_idx": 19}, {"type": "image", "img_path": "opdiIAHfBr/tmp/f810b07997c0ad14481a3102b595acc8d14cf35f9b9693e72d8c78d47869c627.jpg", "img_caption": ["465 F Visual Concepts: Categories "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 17: Category visual concepts in CLIP Layer 9. Left: Mean activation of all pixels within an Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate different brain regions. Middle: The standard deviation negatively correlates with absolute mean activations. Right: Spectral clustering, colored by 3D spectral-tSNE of the top 20 eigenvectors. ", "page_idx": 20}, {"type": "text", "text": "466 G Layer-to-Layer Feature Computation Flow in 2D spectral-tSNE space ", "page_idx": 21}, {"type": "image", "img_path": "opdiIAHfBr/tmp/56b321c5817a8e5d93a40f83ced143daabef0d29d6cc9c25e4b067b955dc0ab2.jpg", "img_caption": ["Figure 18: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space. Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsupervised segmentation. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "opdiIAHfBr/tmp/deea8791eee41fcf13cbdaff7b72832efd994c0a911f39026fcde01b3fce3ffd.jpg", "img_caption": ["Figure 19: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space. Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsupervised segmentation. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "opdiIAHfBr/tmp/8d461a6528bf504101b5311f773d4bd883b4a84e1eab336fe2010d13e6b289f7.jpg", "img_caption": ["Figure 20: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space. Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsupervised segmentation. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "opdiIAHfBr/tmp/5e085a7cafd7d21d7368f1e3c9273e2caafeb3dc21cfeec786144cea5192436e.jpg", "img_caption": ["Figure 21: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space. Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsupervised segmentation. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "opdiIAHfBr/tmp/af7b3e678edd87ce2b3268acd319d4e019561487b872305bf9ac2c43ad8f6909.jpg", "img_caption": ["Figure 22: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space. Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsupervised segmentation. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "467 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "468 1. Claims   \n469 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n470 paper\u2019s contributions and scope?   \n471 Answer: [Yes]   \n472 Justification:   \n473 Guidelines:   \n474 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n475 made in the paper.   \n476 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n477 contributions made in the paper and important assumptions and limitations. A No or   \n478 NA answer to this question will not be perceived well by the reviewers.   \n479 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n480 much the results can be expected to generalize to other settings.   \n481 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these   \n482 goals are not attained by the paper.   \n483 2. Limitations   \n484 Question: Does the paper discuss the limitations of the work performed by the authors?   \n485 Answer: [Yes]   \n486 Justification:   \n487 Guidelines:   \n488 \u2022 The answer NA means that the paper has no limitation while the answer No means   \n489 that the paper has limitations, but those are not discussed in the paper.   \n490 \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n491 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n492 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n493 model well-specification, asymptotic approximations only holding locally). The au  \n494 thors should reflect on how these assumptions might be violated in practice and what   \n495 the implications would be.   \n496 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n497 only tested on a few datasets or with a few runs. In general, empirical results often   \n498 depend on implicit assumptions, which should be articulated.   \n499 \u2022 The authors should reflect on the factors that influence the performance of the ap  \n500 proach. For example, a facial recognition algorithm may perform poorly when image   \n501 resolution is low or images are taken in low lighting. Or a speech-to-text system might   \n502 not be used reliably to provide closed captions for online lectures because it fails to   \n503 handle technical jargon.   \n504 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n505 and how they scale with dataset size.   \n506 \u2022 If applicable, the authors should discuss possible limitations of their approach to ad  \n507 dress problems of privacy and fairness.   \n508 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n509 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n510 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n511 judgment and recognize that individual actions in favor of transparency play an impor  \n512 tant role in developing norms that preserve the integrity of the community. Reviewers   \n513 will be specifically instructed to not penalize honesty concerning limitations.   \n514 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n515   \n516 a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "531 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Experimental details are in the appendix Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "570 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n571 tions to faithfully reproduce the main experimental results, as described in supplemental   \n572 material?   \n573 Answer: [No]   \n574 Justification: The data is provided as open-source from another study; our code is not yet   \n575 released, it will be released upon publication.   \n576 Guidelines:   \n577 \u2022 The answer NA means that paper does not include experiments requiring code.   \n578 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n579 public/guides/CodeSubmissionPolicy) for more details.   \n580 \u2022 While we encourage the release of code and data, we understand that this might not   \n581 be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n582 including code, unless this is central to the contribution (e.g., for a new open-source   \n583 benchmark).   \n584 \u2022 The instructions should contain the exact command and environment needed to run to   \n585 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n586 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n587 \u2022 The authors should provide instructions on data access and preparation, including how   \n588 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n589 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n590 proposed method and baselines. If only a subset of experiments are reproducible, they   \n591 should state which ones are omitted from the script and why.   \n592 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n593 versions (if applicable).   \n594 \u2022 Providing as much information as possible in supplemental material (appended to the   \n595 paper) is recommended, but including URLs to data and code is permitted.   \n596 6. Experimental Setting/Details   \n597 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n598 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n599 results?   \n600 Answer: [Yes]   \n601 Justification: Experimental details are in the appendix   \n602 Guidelines:   \n603 \u2022 The answer NA means that the paper does not include experiments.   \n604 \u2022 The experimental setting should be presented in the core of the paper to a level of   \n605 detail that is necessary to appreciate the results and make sense of them.   \n606 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n607 material.   \n608 7. Experiment Statistical Significance   \n609 Question: Does the paper report error bars suitably and correctly defined or other appropri  \n610 ate information about the statistical significance of the experiments?   \n611 Answer: [Yes]   \n612 Justification: We provided standard deviation in Table 1, measured over training with 3   \n613 random seed.   \n614 Guidelines:   \n615 \u2022 The answer NA means that the paper does not include experiments.   \n616 \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confi  \n617 dence intervals, or statistical significance tests, at least for the experiments that support   \n618 the main claims of the paper.   \n619 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n620 example, train/test split, initialization, random drawing of some parameter, or overall   \n621 run with given experimental conditions).   \n622 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n623 call to a library function, bootstrap, etc.)   \n624 \u2022 The assumptions made should be given (e.g., Normally distributed errors). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "636 Question: For each experiment, does the paper provide sufficient information on the com  \n637 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n638 the experiments? ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "4 Justification:   \n55 Guidelines:   \n56 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n57 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n58 deviation from the Code of Ethics.   \n59 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n60 eration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "661 10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "662 Question: Does the paper discuss both potential positive societal impacts and negative   \n663 societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Pape   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "06 12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. \u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to ", "page_idx": 31}, {"type": "text", "text": "727 the asset\u2019s creators.   \n728 13. New Assets   \n729 Question: Are new assets introduced in the paper well documented and is the documenta  \n730 tion provided alongside the assets?   \n731 Answer: [NA]   \n732 Justification:   \n733 Guidelines:   \n734 \u2022 The answer NA means that the paper does not release new assets.   \n735 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n736 submissions via structured templates. This includes details about training, license,   \n737 limitations, etc.   \n738 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n739 asset is used.   \n740 \u2022 At submission time, remember to anonymize your assets (if applicable). You can   \n741 either create an anonymized URL or include an anonymized zip file.   \n742 14. Crowdsourcing and Research with Human Subjects   \n743 Question: For crowdsourcing experiments and research with human subjects, does the pa  \n744 per include the full text of instructions given to participants and screenshots, if applicable,   \n745 as well as details about compensation (if any)?   \n746 Answer: [NA]   \n747 Justification:   \n748 Guidelines:   \n749 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n750 with human subjects.   \n751 \u2022 Including this information in the supplemental material is fine, but if the main contri  \n752 bution of the paper involves human subjects, then as much detail as possible should   \n753 be included in the main paper.   \n754 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, cura  \n755 tion, or other labor should be paid at least the minimum wage in the country of the   \n756 data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "759 Question: Does the paper describe potential risks incurred by study participants, whether   \n60 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n761 approvals (or an equivalent approval/review based on the requirements of your country or   \n62 institution) were obtained?   \n63 Answer: [NA]   \n64 Justification:   \n65 Guidelines:   \n66 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n67 with human subjects.   \n68 \u2022 Depending on the country in which research is conducted, IRB approval (or equiva  \n69 lent) may be required for any human subjects research. If you obtained IRB approval,   \n70 you should clearly state this in the paper.   \n71 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n72 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n73 guidelines for their institution.   \n774 \u2022 For initial submissions, do not include any information that would break anonymity   \n75 (if applicable), such as the institution conducting the review. ", "page_idx": 31}]