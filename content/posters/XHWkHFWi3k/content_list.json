[{"type": "text", "text": "Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nikil Roashan Selvam Amil Merchant Stefano Ermon Department of Computer Science Stanford University {nrs,amil,ermon}@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In diffusion models, samples are generated through an iterative refinement process, requiring hundreds of sequential model evaluations. Several recent methods have introduced approximations (fewer discretization steps or distillation) to trade off speed at the cost of sample quality. In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute. We take inspiration from the Parareal algorithm, a popular numerical method for parallel-in-time integration of differential equations. In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations. SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining. As we demonstrate for pre-trained diffusion models, the early convergence of this refinement procedure drastically reduces the number of steps required to produce a sample, speeding up generation for instance by up to $1.7\\mathbf{x}$ on a 25-step StableDiffusion-v2 benchmark and up to $4.3\\mathrm{x}$ on longer trajectories. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep generative models based on diffusion processes have showcased the capability to produce high-fidelity samples in a wide-range of applications [28, 11, 39, 27]. From their origins in image and audio generation [31, 33, 10], diffusion models have enabled robotic applications as well as scientific discovery via drug design [1]. Despite this promise, sampling from diffusion models can still be prohibitively slow. Early Denoising Diffusion Probabilistic Models [10] required a thousand sequential model evaluations (steps), and state-of-the-art models such as StableDiffusion [27] can still require up to 100s of iterations for high-quality generations. This large number of sampling steps leads to high-latencies associated with diffusion models, limiting applications such as real-time image or music editing and trajectory planning in robotics [13, 12]. ", "page_idx": 0}, {"type": "text", "text": "As sampling involves solving an ordinary differential equation (ODE), a prominent body of research \u2014 including works such as Denoising Diffusion Implicit Models (DDIM, [32]), Diffusion Exponential Integrator Sampler (DEIS, [42]), and DPM-Solver [19] \u2014 has tried to reduce the number of model evaluations required by introducing various approximations. For example, progressive distillation [29] requires re-training models to approximate the solution to the ODE at larger timesteps. However, such approaches trade-off speed at the cost of sample quality. ", "page_idx": 0}, {"type": "text", "text": "In this work, we instead take an orthogonal approach: we focus on additional parallel compute and show how this can be used to reduce latencies while still providing accurate solutions to the original ODE, thereby preserving sample quality. Recently Shih et al. [30] leveraged a parallel-in-time integration method to introduce the first highly-parallelizable algorithm for diffusion model sampling. The presented ParaDiGMs algorithm builds on Picard iterations [26] to perform denoising steps across the trajectory in parallel, leading to state-of-the-art sampling speeds on popular benchmarks [30]. Despite the promising results, ParaDiGMs can be highly memory bound due to the use of sliding window methods and also has a sequential convergence criteria requiring communicationexpensive cumulative sums across devices to coordinate parallel-sampling. The method also has limited controllability over the accuracy of the final solution, assuming convergence per step rather than in the final generation. It is also worth noting that in concurrent work, Tang et al. [37] take a completely different approach to accelerating parallel sampling: special techniques in solving triangular nonlinear systems through fixed point iteration. Instead, in this paper, we turn to multipleshooting methods from the parallel-in-time ODE integration literature [14, 3, 6] and aim to improve parallelization of diffusion sampling by utilizing multiple resolutions across the time domain [21]. ", "page_idx": 0}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/1e65eed3c416462cc0ac15595f44372d0456e421818d11c57768f057ebdc9e39.jpg", "img_caption": ["Figure 1: A visualization of the iterative refinement provided by the SRDS algorithm on a sample from StableDiffusion with the prompt \u2018a beautiful castle, matte painting.\u2019 The initial coarse solve (left) via limited steps provides a rough estimate of the sample, which iteratively refined through iterations of our algorithm. Early convergence is observed as the 3rd output nearly matches, a key feature that enables efficient generation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Specifically, we present Self-Refining Diffusion Samplers (SRDS) that start with a quick but rough solve of the diffusion trajectory, achieved by limiting the number of total steps taken (for instance, using a few-step DDIM solver). The trajectory can then be simulated to higher fidelity via a highlyparallel algorithm that updates the final generation iteratively until convergence. At a high level, each refinement step of SRDS partitions the current guess of the trajectory into blocks, and simulates each of these blocks at the desired (high) resolution. The running guess for the overall trajectory is then updated via a predictor-corrector mechanism based on the Parareal algorithm to accelerate convergence. This iterative refinement allows us to efficiently interpolate in parallel between a coarse solution corresponding to a low-fidelity sample and an accurate solution corresponding to a high-fidelity sample. The key benefits of SRDS are three-fold: ", "page_idx": 1}, {"type": "text", "text": "Approximation-Free: By design, SRDS computes an accurate solution to the reverse process (as defined by the practitioner\u2019s choice of diffusion solver), thereby maintaining high quality of samples. Importantly, as it is purely a sampling algorithm, it does so without incurring any retraining cost. ", "page_idx": 1}, {"type": "text", "text": "Extensive Control and Compatibility: By serving as an efficient interpolation method between the coarse and fine-grained solvers, SRDS provides the practitioner with flexible control of the tradeoff between sample quality and speed. For instance, one could first start with a rough solve (corresponding to, say, few-step DDIM). Then, if desired, one can add a budget-appropriate number of parallel SRDS iterations (instead of sequential) to refine the obtained sample. Furthermore, SRDS is compatible with most existing off-the-shelf solvers (such as Euler, Heun, DPM etc), thereby providing direct benefits to virtually any diffusion workflow. ", "page_idx": 1}, {"type": "text", "text": "Low Latency: Most importantly, we find that the number of iterations required by SRDS for convergence is quite low, leading to drastic improvements in latency for sampling. We present results using the Self-Refining Diffusion Samplers on a wide range of benchmarks, starting with pixel-based image diffusion models and further exploring latent-methods where SRDS leads to a $1.7\\mathbf{x}$ improvement in the sampling speed on the 25-step StableDiffusion-v2 benchmark and up to $4.3\\mathrm{x}$ on longer trajectories. 1 Through enabling faster sampling, SRDS aims to unlock capabilities for real-time interaction with diffusion models. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models are a general class of generative models that rely on a noising procedure that converts the data distribution into noise via a series of latent variables updates. For the purposes of this work, we will consider the continuous-time generalization presented by Song [35] and Denoising Diffusion Implicit Models [10] that formulate sampling as solving the initial value problem characterized by the probability flow ordinary differential equation (ODE): ", "page_idx": 2}, {"type": "equation", "text": "$$\nd{\\pmb x}=\\underbrace{\\bigg[{\\pmb f}({\\pmb x},t)-\\frac{1}{2}g(t)^{2}s_{\\theta}({\\pmb x},t)\\bigg]}_{h_{\\theta}}d t;\\quad{\\pmb x}(t=0)={\\pmb x}_{0}\\sim\\mathcal{N}({\\pmb0},{\\pmb I})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s_{\\theta}(x,t)$ is a time-conditional prediction of the score function $\\nabla_{x}\\log{p_{t}(\\pmb{x})}$ from the diffusion model. To be consistent with prior work on parallelized diffusion sampling Shih et al. [30], we use a reversed time index (from traditional notation) where $\\scriptstyle x_{0}$ refers to pure Gaussian noise, and ${\\mathbf{}}x_{T}$ refers to the denoised image after $T$ denoising steps. ", "page_idx": 2}, {"type": "text", "text": "2.1 Solving the Differential Equation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given the dynamics governing the differential equation, our goal is to provide accurate solutions to: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}_{T}=\\pmb{x}_{0}+\\int_{t=0}^{T}h_{\\theta}(\\pmb{x},t)d t\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in order to produce a sample from the diffusion model. Common approaches discretize the time interval $[0,T]$ into $N$ pieces $(t_{0}{=}0,t_{1},t_{2},...,t_{N}{=}T)$ and solve a sequence of initial value problems to yield an approximation $({\\pmb x}_{0},{\\pmb x}_{1},...,{\\pmb x}_{N}{=}{\\pmb x}_{T})$ to the trajectory. ", "page_idx": 2}, {"type": "text", "text": "Formally, a solver is a function $\\mathcal{F}(\\pmb{x}_{s t a r t},t_{s t a r t},t_{e n d})$ that propagates $\\textbf{\\em x}$ from $t=t_{s t a r t}$ with initial value $\\mathbf{\\deltax}_{s t a r t}$ to $t=t_{e n d}$ . Solving the differential equation corresponds to approximating the solution $x_{T}$ to the given initial value problem by a sequence of $N$ solves: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i+1}={\\mathcal{F}}\\left(x_{i},t_{i},t_{i+1}\\right)\\ \\ \\forall i\\in[0,N-1];\\ \\ \\ {\\mathrm{given~initial~value}}\\ x_{0}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The choice of solver $\\mathcal{F}$ dictates the sampling speed and accuracy of the solution. In practice, solvers which are accurate are often slow (due to high number of evaluations of $h_{\\theta}$ ), whereas solvers that are fast tend to have reduced accuracy. Initial works on diffusion models used the classical Euler method as choice of $\\mathcal{F}$ , and it can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb x}_{i+1}=\\mathcal{F}_{e u l e r}({\\pmb x}_{i},t_{i},t_{i+1})={\\pmb x}_{i}+h_{\\theta}({\\pmb x}_{i},t_{i})*(t_{i+1}-t_{i})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, DDIM [32] quickly became a popular choice of $\\mathcal{F}$ for its improved efficiency. Other recent works have tried to rely on approximations or leverage various ideas from the numerical methods literature to design solvers $\\mathcal{F}$ that require fewer denoising steps. For instance, Diffusion Exponential Integrator Sampler (DEIS, [42]), and DPM-Solver [19] exploit the special structure of the probability flow ODE to design special solvers where the linear part of the ODE is solved analytically and the non-linear part is solved by incorporating ideas from exponential integrators in the numerical methods literature. Karras et al. [13] leverage the Heun\u2019s second order method and demonstrate a favorable tradeoff between number of model evaluations and quality of generated samples for a small number of denoising steps. In this work, SRDS presents an orthogonal improvement to these methods via parallelization, and by default we will assume all our solvers to be DDIM. ", "page_idx": 2}, {"type": "text", "text": "3 Self-Refining Diffusion Samplers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Attempts to reduce the number of steps in diffusion samplers can provide speedups in sample generation [29, 23], but unfortunately often lead to lower-sample quality. While low-frequency components (in the Fourier sense) of the images may be well-established, the generations miss the high-frequency details that leads to good generations [40]. To fix sample quality while maintaining the latency beneftis of reducing the number of steps, we turn to numerical methods introduced in the parallel-in-time integration literature where dynamics with different components having different rates of convergence has been extensively studied. Specifically, multiple-shooting and multigrid methods have seen success in a wide range of domains from convection-diffusion equations to eigenvalue problems [2, 22, 24, 4] by creating a rough but efficient solve of the prescribed differential equation that can then be iteratively updated via a highly parallelizable simulation. One such algorithm \u2013 Parareal [18] \u2013 serves as the backbone for our Self-Refining Diffusion Samplers that we describe below. ", "page_idx": 2}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/6edd9f64a4a7f8cf738d9683f5b404af0ba8e2aec31f01efac5001ee013d8ae1.jpg", "img_caption": ["Figure 2: First iteration of the parareal algorithm to solve an example ODE. The black curve represents the desired solution from the fine solver. The magenta dots indicate the running solution after one iteration of predictor-corrector updates. Figure inspiration from Pentland et al. [25]. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 Parareal Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Parareal makes use of two solvers: solver $\\mathcal{F}$ (called the \u2019fine solver\u2019) provides accurate solutions but is slow to evaluate, and $\\mathcal{G}$ (called the \u2019coarse solver\u2019) provides rough solutions but is much quicker. ", "page_idx": 3}, {"type": "text", "text": "Parareal targets general purpose initial value problems of forms similar to Equation 1. Consider a partition $(t_{0},t_{1},...,t_{N}=T)$ of the time axis $[t_{0},T]$ into $_\\mathrm{N}$ intervals of equal width. Using the same solver notation as above, the goal is to approximate the solution $x_{N}$ to the initial value problem that would be produced using a sequence of fine solves: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{i+1}=\\mathcal{F}\\left(x_{i},t_{i},t_{i+1}\\right),\\forall i\\in\\left[0,N-1\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The key insight of parareal is that we can first use the coarse solver $\\mathcal{G}$ to quickly produce a rough trajectory, and this rough solution can be iteratively refined using parallel calls to the fine solver $\\mathcal{F}$ . ", "page_idx": 3}, {"type": "text", "text": "Formally, the parareal algorithm begins with a rough estimate of the trajectory, initialzied via a series of coarse solves from $\\mathcal{G}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{0}^{0}=x_{0}\\qquad x_{i+1}^{0}=\\mathcal{G}\\left(x_{i}^{0},t_{i},t_{i+1}\\right)\\forall i\\in[0,N-1]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the notation $\\boldsymbol{x}_{i}^{0}$ denotes the initial estimate of the trajectory from the coarse solver (orange curve in Figure 2). ", "page_idx": 3}, {"type": "text", "text": "Parareal then proceeds in iterations until convergence, where each iteration corresponds to a refinement of the trajectory. At each iteration, we solve the differential equation in each of the $N$ time intervals at a higher resolution using the fine solver $\\mathcal{F}$ , where the initial value for each interval is given by the estimate of the trajectory from the previous iteration. Crucially, these fine solves (blue in Figure 2) can be performed in parallel. Lastly, at the end of each iteration, we perform another coarse sequential solve through the trajectory (magenta in Figure 2) and incorporate the results of the fine solves into the running solution for the trajectory using a predictor-corrector method, where the coarse solver \u2018predictions\u2019 are \u2018corrected\u2019 via the updates from the parallel fine solves. Formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{i+1}^{p+1}=\\mathcal{F}\\left(x_{i}^{p},t_{i},t_{i+1}\\right)+\\left(\\mathcal{G}\\left(x_{i}^{p+1},t_{i},t_{i+1}\\right)-\\mathcal{G}\\left(x_{i}^{p},t_{i},t_{i+1}\\right)\\right),\\quad i=0,\\ldots,N-1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the notation $\\boldsymbol{x}_{i}^{p}$ denotes the running estimate of the trajectory at Parareal iteration number $p$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Self-Refining Diffusion Samplers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Now, we turn our attention back to drawing a sample from our diffusion model, which as discussed corresponds to estimating a solution to the initial value problem as defined in Equation 1. ", "page_idx": 3}, {"type": "text", "text": "Require: Diffusion model $p_{\\theta}$ with denoising steps $N$ , tolerance $\\tau$ , and corresponding DDIM solver   \n$\\mathbf{\\bar{\\Phi}}_{h\\left(\\mathbf{x},\\,t_{s t a r t},\\,t_{e n d},\\,s t e p s\\right)}$   \nEnsure: A sample from $p_{\\theta}$   \n1: $\\pmb{x}_{0}^{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\bar{{I}}})$ // Sample initial condition for Initial Value Problem from prior   \n2: for $i\\gets1$ to $\\sqrt{N}$ do   \n3: $p r e v_{i}\\gets h\\left(\\pmb{x}_{i-1}^{0},t_{i-1},t_{i},1\\right)$   \n4: $\\pmb{x}_{i}^{0}\\gets p r e\\pmb{v}_{i}$ // Initialize x with a coarse solve   \n5: $p\\gets1$ // SRDS refinement iteration number   \n6: while $p\\leq\\sqrt{N}$ do   \n7: for $i\\gets1$ to $\\sqrt{N}$ in parallel do   \n8: $\\pmb{{y}}_{i}\\gets h\\left(\\pmb{{x}}_{i-1}^{p-1},t_{i-1},\\bar{t}_{i},\\sqrt{N}\\right)$ // Perform fine solves in parallel   \n9: for i \u21901 to $\\sqrt{N}$ do // Perform a coarse sweep   \n10: $c u r_{i}\\gets h\\left(\\pmb{x}_{i-1}^{p},t_{i-1},t_{i},1\\right)$   \n11: $\\pmb{x}_{i}^{p}\\leftarrow\\pmb{y}_{i}+\\pmb{c}\\pmb{\\dot{u}}\\pmb{r}_{i}-\\pmb{p}\\pmb{r}e\\pmb{v}_{i}\\$ // Take predictor-corrector step   \n12: $p r e v_{i}\\leftarrow c u r_{i}$   \n13: if $|{\\bf x}_{\\sqrt{N}}^{p}-{\\bf x}_{\\sqrt{N}}^{p-1}|<\\tau$ then // Check for convergence   \n14: break   \n15: p \u2190p + 1   \n16: return xp\u221a\u22121 ", "page_idx": 4}, {"type": "text", "text": "We leverage the Parareal algorithm in order to parallelize sampling. Our ide\u221aa is to compute a solution on the $N$ -discretization of the in\u221aterval by instead considering a coarser $\\sqrt{N}$ -dis\u221acretization2 of the interv\u221aal $[0,T]$ . Let $\\Delta T\\,=\\,T/\\sqrt{N}$ , $t_{i+1}\\,=\\,t_{i}+\\Delta T$ , partitioning $[0,T]$ into $\\sqrt{N}$ intervals. We p\u221aick $\\sqrt{N}$ -step DDIM solver 3 as our fine solver $\\mathcal{F}$ . In other words, $\\mathcal{F}(\\pmb{x}_{i},t_{i},t_{i+1})$ is the result of a $\\sqrt{N}$ -step DDIM solve propagating $\\textbf{\\em x}$ from $t=t_{i}$ with initial value $\\pmb{x}_{i}$ to $t=t_{i+1}$ . We pick 1-step DDIM solver as our coarse solver $\\mathcal{G}$ . That is, $\\mathscr{G}(\\pmb{x}_{i},t_{i},t_{i+1})$ denotes the result of the corresponding 1-step DDIM solve propagating $\\textbf{\\em x}$ from $t=t_{i}$ with initial value $\\pmb{x}_{i}$ to $t\\,=\\,t_{i+1}$ (\"step\" refers to denoising step involving an $h_{\\theta}$ evaluation). ", "page_idx": 4}, {"type": "text", "text": "The SRDS algorithm proceeds as follows. We start wit\u221ah the coarse-solve to generate a rough estim\u221aate of the trajectory and final sample, achieved by taking $\\sqrt{N}$ DDIM steps in total. T\u221ahen, each of the $\\sqrt{N}$ coarse predictions in the trajectory is simulated at higher resolution with further $\\sqrt{N}$ DDIM-iterations in parallel, each with an effective time step corresponding to the original $N$ -step discretization of the diffusion model. Iterative updates to the running solution then proceed in a manner equivalent to Parareal updates until convergence, as measured by the change in the outputted generation. Our SRDS algorithm is summarized in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "3.3 Convergence Guarantee ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The ideal result for diffusion sampling is to get the solution arising from $N$ sequential d\u221aenoising score steps. SRDS however only starts with a rough solve of the diffusion trajectory taking $\\sqrt{N}$ sequential denoising steps. Nevertheless, we can show that each iteration of SRDS (line 6 of Algorithm 1) refines the generated sample and leads us closer to the ideal solution. ", "page_idx": 4}, {"type": "text", "text": "Proposition\u221a 1. The sample output by SRDS converges to the output of the $N$ -step sequential solver in at most $\\sqrt{N}$ refinement iterations. ", "page_idx": 4}, {"type": "text", "text": "A key property of our algorithm is that after $i$ iterations (refinements to the diffusion trajectory) of SRDS, the first $i$ steps of the running trajectory exactly matches the trajectory generated by the sequential solver for \u221athe corresponding intervals. Consequently, the algorithm is guaranteed to converge in at most $\\sqrt{N}$ iterations. We defer the formal proof to Appendix A. It is also worth noting that this worst case guarantee is similar in spirit to Proposition 1 in [30] and its generalization (Theorem 3.6 in [37]). ", "page_idx": 4}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/5d23444a0bdc09fcb21a733189f64f16f14fa703b992a553c89d840182bda2a4.jpg", "img_caption": [], "img_footnote": ["Figure 3: Computation graph for \u221adiffusi\u221aon sam\u221apling. For SRDS, the red arrows correspond to fine solves, and each block \u2014 $\\cdot\\left[0,\\sqrt{N}\\right],[\\sqrt{N},2\\sqrt{N}]$ , and so on \u2014 can be perform the fine solves independently in parallel. The blue arrows correspond to 1-step coarse solves. "], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "It is worth noting, however, that in practice we observe that the number of refinement iterations required till convergence \u2014 which is defined as difference in consecutive s\u221aample generations not exceeding a chosen threshold \u2014 is much less than the worst case bound of $\\sqrt{N}$ . This early convergence is critical to speedups from SRDS. The exact choice of threshold $\\tau$ in line 13 of Algorithm 1, is a hyperparameter that is empirically chosen so as to avoid measurable degradation in sample quality. ", "page_idx": 5}, {"type": "text", "text": "3.4 Batched Inference and Pipelining ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "SRDS benefits from two key features to reduce latencies: batched inference and pipelining. ", "page_idx": 5}, {"type": "text", "text": "First, the fine solves that are used in order to refine the trajectories implementation-equivalent DDIM-steps, which means that they can be performed in a batched manner even for a single sample generation. This parallelization allows for a single sample generation to incur the beneftis of batched inference, introducing higher device utilization or device parallelism. ", "page_idx": 5}, {"type": "text", "text": "Secondly, we observe that the dependency graph for SRDS enables pipelined parallelism. As outlined in Figure 3, we find that $\\mathcal{F}(x_{i}^{p},\\bar{t}_{i},t_{i+1})$ and $\\bar{\\boldsymbol{\\mathcal{G}}}\\left(\\mathbf{\\boldsymbol{x}}_{i}^{p},t_{i},t_{i+1}\\right)$ both only depend on $\\pmb{x}_{i}^{p}$ . The tasks for computing $\\mathcal{F}\\left(\\pmb{x}_{j}^{i},t_{i},t_{i+1}\\right)$ and $\\mathcal{G}\\left(\\pmb{x}_{i}^{p},t_{i},t_{i+1}\\right)$ can be spawned as soon as $\\pmb{x}_{j}^{i}$ is computed, without waiting for the entire predictor-corrector mechanism to finish updating the SRDS solution for iteration $i$ . This leads to an efficiently pipelined version of the algorithm, further speeding up the sampling process by a factor of two. See Figure 4 for an illustration of this pipelined algorithm with $N=16$ . Pipelining furthers the beneftis of batched inference as the coarse solver is simply a DDIM-step with a larger time-step, so it can be batched with fine solves when applicable. ", "page_idx": 5}, {"type": "text", "text": "3.5 Sampling Latency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Proposition 2. [Worst-Case Behavior] Ignoring GPU overhead, the worst case wall-clock time of generating a sample through SRDS is no worse than that of generating through sequential sampling. ", "page_idx": 5}, {"type": "text", "text": "Referring to the pipelined implementation of SRDS, it is easy to see that the fine solve $\\mathcal{F}\\left(\\pmb{x}_{i}^{i},t_{i},t_{i+1}\\right)$ starts immediately after $\\mathcal{F}\\left(\\mathbf{\\boldsymbol{x}}_{i}^{i-1},t_{i-1},t_{i}\\right)$ . Subsequently, from Proposition 1, it then follows that in the worst case, the final sample of SRDS x\u221aNN i s computed at time ${\\sqrt{N}}\\cdot{\\sqrt{N}}=N$ as desired. A formal argument can be found in Appendix A. It is worth noting, however, that this property of SRDS comes at the cost of much higher parallel compute compared to sequential sampling. ", "page_idx": 5}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/f212df0574435c01a84fcac56ff432fdcb4e1f11484fe8f1738d67c527db26fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Illustration of the pipelined version of the SRDS algorithm on $N=16$ denoising steps, which results in a direct $2\\mathbf{x}$ speedup compared to the vanilla version. ", "page_idx": 6}, {"type": "text", "text": "3.6 Memory and Communication Overhead ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Proposition 3. [Memory] SRDS requires memory corresponding to $\\mathcal{O}(\\sqrt{N})$ model evaluations. ", "page_idx": 6}, {"type": "text", "text": "Once again referring to the pipelined implementation of SRDS, it is easy to see tha\u221at at any given time there is at most one model evaluation corresponding to a coarse solve, and up to $\\sqrt{N}$ parallel model evaluations corresponding to the fine solves. It is worth contrasting this with the quadratically higher $\\mathcal{O}(N)$ memory requirement of the full ParaDiGMs algorithm in [30], necessitating the use of sliding window tricks to reverse the process in a piece-wise fashion. ", "page_idx": 6}, {"type": "text", "text": "It is finally worth noting that there is minimal inter-GPU communication in SRDS. In particular, at most one sample is passed between adjacent GPUs in each SRDS iteration. Once again, it is worth contrasting this with ParaDiGMs algorithm, which \u2013 by its use of parallel prefix sum operations to sync the solutions at each Picard iteration \u2013 incurs greater GPU communication overhead. See Appendix D for more discussion. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments on Diffusion Image Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To showcase the capabilities of the prescribed SRDS algorithm, we apply the diffusion sampler to pretrained diffusion models and present the difference in sample time and quality to ensure that applied convergence criteria do not reduce generation metrics. We start with pixel-based diffusion before expanding experiments applied to latent methods such as StableDiffusion-v2. Across the range of tasks, we show consistent speedups while maintaining quality of sample generation. ", "page_idx": 6}, {"type": "text", "text": "In this section, we perform an extensive comparison with ParaDiGMs [30] as our baseline. Nonetheless, we provide a high level empirical comparison to our concurrent work ParaTAA[37] in Appendix E, where we demonstrate the superiority of SRDS. ", "page_idx": 6}, {"type": "text", "text": "4.1 Pixel Diffusion $^{\\ast}$ Image Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We start with pixel-space diffusion models. In particular, we test our SRDS algorithm and demonstrate capabilities in performing diffusion directly on the pixel space of 128x128 LSUN Church and Bedroom [41], 64x64 Imagenet [5], and $32\\mathtt{x32}$ CIFAR [16] using pretrained diffusion models [29], which all use $N=1024$ length diffusion trajectories. ", "page_idx": 6}, {"type": "text", "text": "We measure the convergence via $l_{1}$ norm in pixel space with values [0, 255]. We conservatively set $\\tau=0.1$ , meaning that convergence occurs when on average each pixel in the generation differs by only 0.1 after a refinement step (see Appendix F for an ablation on choice of $\\tau$ ). Through our experiments, we quantitatively showcase how the SRDS algorithm can provide signficant speedups in generation without degrading model quality (as measured by FID score [9] on 5000 samples). As seen in Table 1, SRDS remarkably converges in 4-6 iterations across all datasets; this corresponds to roughly $150-200$ effective serial steps (counting all model evaluations simultaneously performed in parallel as one evaluation), which is only $15-2\\bar{0}\\%$ of the serial steps required by a sequential solve ", "page_idx": 6}, {"type": "text", "text": "Table 1: Evaluating FID score (lower is better) of SRDS on various datasets using 5000 samples generated using a DDIM solver. Effective serial evals refers to the number of serial model evaluations taken by the pipelined SRDS algorithm (counting all model evaluations simultaneously performed in parallel as one evaluation) . Total evals refers to the total number of model evaluations. ", "page_idx": 7}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/b552b0a22dd7ea7ed8901f5891a2619193bd5b39c866c52d582a4485206692d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/6a7ccb66374b0152b2e5a99fda371c2b0a36bccb2aefb68c6133a528afce12e5.jpg", "table_caption": ["Table 2: CLIP scores of SRDS on StableDiffusion-v2 over 1000 samples from the COCO2017 captions dataset, with classifier guidance $w=7.5$ , evaluated on ViT- $\\mathbf{\\vec{g}}$ -14. Time is measured on 4 A100 GPUs without pipeline parallelism, showcasing speedups with early convergence of the SRDS sample. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "$\\langle N=1024\\rangle$ ). We clarify that effective serial evaluations is referred to as Parallel Iters in [30] and Steps in [37]. ", "page_idx": 7}, {"type": "text", "text": "While we are pretty conservative above in measuring convergence through distance in pixel space, we can also simply limit the number of SRDS iterations to $1-2$ and achieve further speedups without measurable degradation in sample quality. See Appendix F for more details. ", "page_idx": 7}, {"type": "text", "text": "It is once again worth noting that this improved latency from parallelization comes at the cost of greater number of total model evaluations compared to a regular sequential solver. However, this tradeoff enables the diffusion models for many other use cases such as real-time image or music editing and trajectory planning in robotics. Moreover, we often empirically observe that SRDS provides reasonable predictions within a single Parareal iteration; here, the total number of model evaluations is only slightly larger than the serial approach (increasing from $n$ to $n+2{\\sqrt{n}})$ . Lastly, it is also worth noting that many users are often agnostic to inference time GPU compute costs as they are orders of magnitude lower than training compute costs anyway. ", "page_idx": 7}, {"type": "text", "text": "4.2 Latent Diffusion $^{\\ast}$ Image Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finally, we turn to latent diffusion models, in particular StableDiffusion-v2 [27], where evaluations of the CLIP score over 1000 random samples show how SRDS maintains sample quality while improving the number of parallel iterations required per sample, with summary metrics presented in Table 2. As the SRDS algorithm has small GPU overhead, we achieve measured wallclock time improvements with a Diffusers compatible implementation [38]. It is worth nothing that while we focus on DDIM here (as in the rest of the writing), we show speedups by readily incorporating other solvers into SRDS in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "For the test bed of latent diffusion models, we explore the convergence properties of our SRDS algorithm, with the average CLIP score plotted against the number of iterations in Figure 5. For shorter sequences of length 25 (left), the corresponding SRDS sampler converges after approximately 3 iterations. However, for longer sequences of length 100 (right) the sampler has converged after a single SRDS iteration, showcasing the capabilities of our algorithm improves with longer trajectories. ", "page_idx": 7}, {"type": "text", "text": "Next, we demonstrate the additional speedup that pipeline parallelism can bring to SRDS. We implement a slightly suboptimal version of pipelined SRDS for StableDiffusion and already observe ", "page_idx": 7}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/f5881ed33622d4d9133d62e70301de7c28881a126b915b0e4201c3dcb6e91287.jpg", "img_caption": ["Figure 5: Convergence of the SRDS algorithm for a trajectory of length 25 (left) and 100 (right) showcase how early termination of the algorithm can yield equivalent sample quality. In particular, longer trajectories with increased parallelism appear to converge faster. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/313ff900496063fa7776447e19a522a82f5ea95c06c4625657e0d9d7968dbba5.jpg", "table_caption": ["Table 3: Evaluation of additional speedup offered by pipelined version of SRDS. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Comparison of wallclock speedups offered by Pipelined SRDS and ParaDiGMS with various thresholds, with respect to Serial image generation. These StableDiffusion experiments are performed on identical machines (4 40GB A100 GPUs) for a fair comparison. ", "page_idx": 8}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/4e363f5c29bc4fe5d4ceeb48c801b66f0c0c299abd09b79561adefd5553dcb7a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "significant speedups as seen in Table 3; with some more engineering effort4, we can further push towards extracting the full potential of pipelining. However, as this already beats the baselines, this sufficiently demonstrates the benefits of $\\mathsf{\\dot{S}R D S}^{5}$ . ", "page_idx": 8}, {"type": "text", "text": "Furthermore, for our main baseline ParaDiGMs, we also perform more extensive evaluation to evaluate both methods on equal hardware to more clearly demonstrate the beneftis of SRDS. In Table 4, we demonstrate that SRDS consistently beats ParaDiGMS on wallclock speedups. Though the authors of [30] uses a convergence threshold of $1e-3$ , we show that SRDS can provide impressive speedups even when compared to significantly relaxed ParaDiGMS thresholds of $1e-1$ . ", "page_idx": 8}, {"type": "text", "text": "Finally, we finally provide a few sample generations on standard text prompts from DrawBench [28] in Figure 6 and additional examples of convergence similar to Figure 1 in the Appendix G. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Recent literature on diffusion models has focused heavily on reducing the cost of sampling. Techniques such as higher order methods [13] and exponential integrators [42] have been proposed as strategies for reducing the model evaluations required in order to build high-quality samples without any additional training. When additional training is possible, other works have proposed distillation [29], quantization [17], and consistency [34] as alternate objectives to further speed up sample generation. For the purposes of this paper, we view these approaches as orthogonal, as the resultant models could be simulated with SRDS for potential benefits from combining methods. ", "page_idx": 8}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/6650c386732472be9e60cd04552987ca2cda847e23f4d7073db5ff06092168d2.jpg", "img_caption": ["Figure 6: Sample generation from StableDiffusion-v2 with the SRDS algorithm with text prompts based on examples from DrawBench. We plot the early converged SRDS figure (top) and the result of the serial trajectory (bottom); the two rows are essentially indistinguishable, highlighting the approximation-free nature of SRDS. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "As discussed throughout the paper, this work is most cloesly related to the ParaDiGMS sampler method developed by Shih et al. [30] for parallel sampling of diffusion models. The two works take a similar approach by building off popular parallel-in-time integration methods in order to achieve lower latencies in simulation. In particular, ParaDiGMS builds on Picard iterations to converge on trajectories; we, however, build on Parareal method that performs multiresolution along the time axis for faster sampling. Parareal has been well-explored [18, 25, 20, 7] though with limited theoretical guarantees only spanning certain cases such as the heat equation and Navier-Stokes equation [8, 36]; our work is the first to apply this algorithm to diffusion models. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations: Similar to previous iterations of parallel-in-time integration methods, SRDS make use of additional compute that can be used in parallel in exchange for faster latencies of sampling. That is to say, the total number of model evaluations in comparison to standard diffusion modelling increases in exchange for lower latencies. The additional compute may be reasonable in applications such as small-batch sampling where the additional cost can be hidden through better device utilization (e.g. sampling of a single image or trajectory in robotics). Alternatively, the responsiveness of real-time image editing may make parallel sampling an appealing option for cost-insensitive users. ", "page_idx": 9}, {"type": "text", "text": "Future Directions: This work opens up a ton of interesting open questions for future exploration. Firstly, while fast convergence of parareal-style algorithms has only been proven for very limited settings, it will be extremely interesting to derive convergence guarantees specifically for the diffusion process. This has the potential to further our understanding of the nature of the ODE/SDE that governs the reverse process. Another natural direction is to explore the effects of employing higher levels of discretization and other multigrid methods such as $F$ -cycles and $W$ -cycles. As alluded to in Section 3.2, one could not only further study the optimal choice of second level of discretization, but also consider novel schedules that involve partitioning the diffusion trajectory into intervals of varying sizes. Lastly, it is worth highlighting that by serving a highly modular and interoperable framework, SRDS unlocks a vast landscape of interesting coarse/fine solver combinations. For instance, one can use a DDIM solver to perform the parallel refinement steps, while using a progressively distilled model [29] or consistency trajectory model [15] as the coarse solver in SRDS. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the reviewers for their thoughtful feedback towards improving this paper. We also thank Aryaman Arora, Harshit Joshi, Ken Liu, Rajeshwari Jadhav, Rohit Nema, and Yanzhe Zhang for their helpful discussions and support during various stages of the project. This project was funded in part by ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and the CZ Biohub. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1\u20133, 2024.   \n[2] Hans Georg Bock and K. J. Plitt. A multiple shooting algorithm for direct solution of optimal control problems. IFAC Proceedings Volumes, 17:1603\u20131608, 1984.   \n[3] R Bulirsch. The multiple shooting method for the numerical solution of nonlinear boundaryvalue problems and of optimal control problems. Report of the Carl\u2013Cranz\u2013Gesellschaft eV, Oberpfaffenhofen, Germany, 1971.   \n[4] Hongtao Chen, Hehu Xie, and Fei Xu. A full multigrid method for eigenvalue problems. J. Comput. Phys., 322:747\u2013759, 2014.   \n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[6] Peter Deuflhard. Recent advances in multiple shooting techniques. 1980.   \n[7] Paul F Fischer, Fr\u00e9d\u00e9ric Hecht, and Yvon Maday. A parareal in time semi-implicit approximation of the navier-stokes equations. In Domain decomposition methods in science and engineering, pages 433\u2013440. Springer, 2005.   \n[8] Martin J. Gander and Stefan Vandewalle. Analysis of the parareal time-parallel timeintegration method. SIAM Journal on Scientific Computing, 29(2):556\u2013578, 2007. doi: 10.1137/05064607X.   \n[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n[12] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.   \n[13] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 26565\u201326577, 2022.   \n[14] M. Kiehl. Parallel multiple shooting for the solution of initial value problems. Parallel Computing, 20(3):275\u2013295, 1994. ISSN 0167-8191. doi: https://doi.org/10.1016/S0167-8191(06) 80013-X.   \n[15] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. ", "page_idx": 10}, {"type": "text", "text": "[16] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. ", "page_idx": 11}, {"type": "text", "text": "[17] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17535\u201317545, 2023.   \n[18] J.-L. Lions, Yvon Maday, and Gabriel Turinici. A \"parareal\" in time discretization of PDE\u2019s. Comptes Rendus de l\u2019Acad\u00e9mie des Sciences - Series I - Mathematics, 332:661\u2013668, 2001. doi: 10.1016/S0764-4442(00)01793-6.   \n[19] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[20] Yvon Maday and Olga Mula. An adaptive parareal algorithm. Journal of computational and applied mathematics, 377:112915, 2020.   \n[21] Yvon Maday and Gabriel Turinici. A parareal in time procedure for the control of partial differential equations. Comptes Rendus Mathematique, 335(4):387\u2013392, 2002.   \n[22] Oliver A. McBryan, Johannes Linden, A. Schuller, Karl Solchenbach, and K. Stuben. Multigrid methods on parahel computers - a survey on recent developments. 2014.   \n[23] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \n[24] Galina V. Muratova and Evgeniya M. Andreeva. Multigrid method for solving convectiondiffusion problems with dominant convection. Journal of Computational and Applied Mathematics, 226(1):77\u201383, 2009. ISSN 0377-0427. Special Issue: The First International Conference on Numerical Algebra and Scientific Computing (NASC06).   \n[25] Kamran Pentland, Massimiliano Tamborrino, Debasmita Samaddar, and Lynton C Appel. Stochastic parareal: An application of probabilistic methods to time-parallelization. SIAM Journal on Scientific Computing, 45(3):S82\u2013S102, 2022.   \n[26] \u00c9mile Picard. Memoire sur la theorie des equations aux derivees partielles et la methode des approximations successives. Journal de Math\u00e9matiques pures et appliqu\u00e9es, 6:145\u2013210, 1890.   \n[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[29] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \n[30] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[32] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[33] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[34] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, 2023.   \n[35] Yang et al. Song. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[36] Johannes Steiner, Daniel Ruprecht, Robert Speck, and Rolf Krause. Convergence of parareal for the navier-stokes equations depending on the reynolds number. In Numerical Mathematics and Advanced Applications-ENUMATH 2013: Proceedings of ENUMATH 2013, the 10th European Conference on Numerical Mathematics and Advanced Applications, Lausanne, August 2013, pages 195\u2013202. Springer, 2015.   \n[37] Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, and Tsung-Hui Chang. Accelerating parallel sampling of diffusion models, 2024. URL https://arxiv.org/abs/2402.09970.   \n[38] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022.   \n[39] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[40] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22552\u201322562, 2023.   \n[41] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \n[42] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 1. [Convergence G\u221auarantee] The sample from SRDS converges to the output of the slow sequential solver in at most $\\sqrt{N}$ refinement iterations. ", "page_idx": 13}, {"type": "text", "text": "Proof. We show, by induction, that $\\pmb{x}_{i}^{p}$ converges in $i$ iterations of SRDS for all $i\\,\\in\\,[0,N\\,-\\,1]$ . Further, $\\pmb{x}_{i}^{p}=\\mathcal{F}\\left(\\pmb{x}_{i-1}^{p-1},t_{i-1},t_{i}\\right)$ for all $p\\geq i$ , implying that the final sample indeed corresponds to the desired sample from $\\mathcal{F}$ . The base case of $i=0$ follows trivially from the initialization (initial condition). To prove the second base case of $i=1$ , notice that $\\pmb{x}_{0}^{\\bar{p}}=\\pmb{x}_{0}$ for all $p$ , implying that $\\mathcal{G}\\left(\\pmb{x}_{0}^{p-1},t_{0},t_{1}\\right)$ is constant for all $p\\geq1$ . Consequently, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{1}^{p}=\\mathcal{F}\\left(x_{0}^{p-1},t_{0},t_{1}\\right)+\\left(\\mathcal{G}\\left(x_{0}^{p},t_{0},t_{1}\\right)-\\mathcal{G}\\left(x_{0}^{p-1},t_{0},t_{1}\\right)\\right),\\quad\\forall p\\geq1}\\\\ &{\\quad=\\mathcal{F}\\left(x_{0},t_{0},t_{1}\\right),\\quad\\forall p\\geq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as desired. ", "page_idx": 13}, {"type": "text", "text": "Assume by the induction hypothesis that for some fixed $i$ , $\\pmb{x}_{i}^{p}=\\mathcal{F}\\left(\\pmb{x}_{i-1}^{i-1},t_{i-1},t_{i}\\right),\\forall p\\geq i$ . Then, $\\forall p\\geq i$ , we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i+1}^{p+1}={\\mathcal F}\\left(x_{i}^{p},t_{i},t_{i+1}\\right)+\\left({\\mathcal G}\\left(x_{i}^{p+1},t_{i},t_{i+1}\\right)-{\\mathcal G}\\left(x_{i}^{p},t_{i},t_{i+1}\\right)\\right)}\\\\ &{\\qquad={\\mathcal F}\\left(x_{i}^{i},t_{i},t_{i+1}\\right)+\\left({\\mathcal G}\\left({\\mathcal F}\\left(x_{i-1}^{i-1},t_{i-1},t_{i}\\right),t_{i},t_{i+1}\\right)-{\\mathcal G}\\left({\\mathcal F}\\left(x_{i-1}^{i-1},t_{i-1},t_{i}\\right),t_{i},t_{i+1}\\right)\\right)}\\\\ &{\\qquad={\\mathcal F}\\left(x_{i}^{i},t_{i},t_{i+1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as desired. ", "page_idx": 13}, {"type": "text", "text": "Proposition 2. [Worst-Case Sampling Latency] Ignoring GPU overhead, the worst case wallclock time of generating a single sample through SRDS is no worse than that of generating a single sample through sequential sampling. ", "page_idx": 13}, {"type": "text", "text": "Proof. Consider the unit of time to be the time taken for one denoising step (or one model evaluation). Referring to the pipelined implementation of SRDS, it is easy to see via a straightforward inductive argument that the $\\sqrt{N}$ -step fine solve $\\mathcal{F}\\left(\\pmb{x}_{\\sqrt{N}}^{p},t_{i},t_{i+1}\\right)$ ends at time $\\scriptstyle{\\frac{N}{\\sqrt{N}}}p\\,+\\,{\\sqrt{N}}\\,-\\,p$ . From Proposition 1, it then follows that in the worst case, the final sample of SRDS $\\pmb{x}_{\\sqrt{N}}^{\\sqrt{N}}$ is computed at time $\\begin{array}{r}{\\frac{N}{\\sqrt{N}}\\sqrt{N}+\\sqrt{N}-\\sqrt{N}=N}\\end{array}$ as desired. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proposition 3. [Memory] SRDS requires memory corresponding to $\\mathcal{O}(\\sqrt{N})$ denoising model evaluations. ", "page_idx": 13}, {"type": "text", "text": "Proof. In the pipelined implementation of SRDS, it is easy to see that at any given timestep there is at most one model evaluation corresponding to a coarse solve. Further, the number of parallel model evaluations corresponding to th\u221ae fine solves is upper bounded by the coarse discretization (or the number of \"blocks\"), which is $\\sqrt{N}$ . Thus, the memory used by SRDS corresponds to at most $\\sqrt{N}+1=\\mathcal{O}(\\sqrt{N})$ model evaluations. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B Choice of Coarse Resolution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The choice of resolution for the coarse solve is not arbitrary. For practical impleme\u221antations, since we use the same denoiser (say, DDIM) for both the coarse and fine solves, we choose $\\sqrt{N}$ as an optimal choice in the runtime sense (assuming constant number of iterations till convergence6). At a high level, this choice stems from the fact that we want to balance out the time the it takes to run all the fine solves in parallel and the time it takes perform one set of sequential predictor-corrector steps through the trajectory. ", "page_idx": 14}, {"type": "text", "text": "Propo\u221asition 4. [Optimal Coarse Resolution] The speed of an SRDS iteration is maximized for $B\\approx\\sqrt{N}$ . ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Let $k$ denote the number of SRDS iterations until convergence, let $\\tau$ denote the cost of one denoising step or model evaluation, and let $1<B<N$ denote the \"block-size\": that is, the second scale of discretization. For the 1-step coarse solve, each SRDS iteration incurs a runtime cost of $1\\cdot\\textstyle\\left\\lceil{\\frac{N}{B}}\\right\\rceil\\cdot\\tau$ . For the $B$ -step fine solves, as each of the $\\lceil{\\frac{N}{B}}\\rceil$ fine solves are independently executed in parallel, each SRDS iteration incurs a runtime cost of $B^{-}\\!\\cdot\\!1\\cdot\\tau$ . The baseline runtime for sequentially sampling from the diffusion model is $N\\cdot\\tau$ . Thus, the runtime speedup (ignoring parallelization overhead) is $\\begin{array}{r}{\\frac{N\\cdot\\tau}{k\\left(\\lceil\\frac{N}{B}\\rceil\\cdot\\tau+B\\cdot\\tau\\right)}=\\frac{N}{k\\left(\\lceil\\frac{N}{B}\\rceil+B\\right)}}\\end{array}$ . For a fixed value of $k$ , it is easy to see that this quantity is \u00b7   \nconcave in $B$ and is maximized by choosing $B\\approx\\sqrt{N}$ . ", "page_idx": 14}, {"type": "text", "text": "It is worth noting, however, that if we use solvers of different latencies for the coarse and fine steps, a modifed analys\u221ais is required to incorporate differences in denoising step times for the two solvers. Consequently, N might no longer be the optimal choice of coarse resolution. ", "page_idx": 14}, {"type": "text", "text": "C Incorporation of other Solvers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "It is worth emphasizing again that SRDS provides an orthogonal improvement when compared to the other lines of research on accelerating diffusion model sampling. In particular, while the main experiments (and writing) were focused on DDIM, SRDS is compatible with the other solvers and they can be readily incorporated into SRDS to speed up diffusion sampling. For example, below we show that SRDS is directly compatible with other solvers such as DDPM (often requiring more steps than DDIM) and DPMSolver (often requiring fewer steps than DDIM) and can efficiently accelerate sampling in both cases. We demonstrate this on StableDiffusion in Table 5. We also highlight that the Diffuser-compatible implementation requires only minor modification to the arguments of the solver, suggesting that SRDS will also be easy to extend out-of-the-box to other methods that the community develops. ", "page_idx": 14}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/6bd6a051e0a38baa6c9819cce6e0796f005067522082281e40beeac0d3116019.jpg", "table_caption": ["Table 5: Evaluation of SRDS with various off-the-shelf solvers. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Memory Utilization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a $T$ step denoising process, ParaDiGMS needs to perform T model evaluations in parallel with subseque\u221ant computations needing information about all previous evaluations, while SRDS only requires $\\sqrt{T}$ parallel evaluations (which ftis comfortably in GPU memory) and requires much lesser communication between GPUs. While the prohibitively large memory requirement can be combated with a sliding window method, the significantly larger communication overhead remains because at every step of Paradigms, an AllReduce over all devices must be performed in order to calculate updates to the sliding window. (For instance, even when ParaDiGMS reduces Eff. Serial Steps by $20\\mathrm{x}$ , the obtained speedup is only 3.4x). This is in contrast to the independent fine-solves in parareal that only need to transfer information for the coarse solve. ", "page_idx": 15}, {"type": "text", "text": "Below in Table 6, we demonstrate how the minimal memory and communication overhead of SRDS shines through as we are able to achieve better device utilization as we increase the number of available GPUs. The following experiment was performed on 40GB A100s and used a generous 1e-2 threshold for ParaDiGMS. ", "page_idx": 15}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/281c3c92bc9875bf2d6879962635f97c4db9fe03584668e73becffbb3dd5c22c.jpg", "table_caption": ["Table 6: Evaluating the device utilization of SRDS in comparison to ParaDiGMS. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Comparison to ParaTAA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We demonstrate the superiority of SRDS to baselines ParaDiGMS [30] and ParaTAA [37]. Here, we demonstrate the high-level superiority of SRDS solely by using the results published by the authors in [30] (Table 5) and [37] (Table 1). ", "page_idx": 15}, {"type": "text", "text": "In the table 7 below, we show that SRDS offers better wall-clock speedups (over sequential) in sample generation time for StableDiffusion when compared to [30] and [37]. We clarify that the reported speedup for each method is with respect to sequential solve on the same machine that the corresponding parallel method was evaluated. Our results are particularly impressive given that the authors of [30] used 8x 80GB A100s for the evaluation and the authors of [37] used 8x 80GB A800 for the same, while we (SRDS) only used 4x 40GB A100 for the evaluation due to computational constraints. (For interpretation purposes, recall that a sequential solve is not compute/memory bound and doesn\u2019t benefti significantly from additional GPU compute, whereas the parallel methods certainly do!) We would also like to highlight the superiority of SRDS over the baselines in the regime of small number of denoising steps (25) as being particularly impactful. ", "page_idx": 15}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/ed837f2666cf1184a8b7daa4444de1b9e6f0bf938fbc263f132ea800017440ff.jpg", "table_caption": ["Table 7: Speedup in wallclock time for single sample generation offered by SRDS compared to ParaDiGMS and ParaTAA "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "F Additional Convergence Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we further evaluate the convergence properties of SRDS. First, in Table 8, we analyze how the sample quality varies as we vary the tolerance threshold $\\tau$ . ", "page_idx": 16}, {"type": "text", "text": "Table 8: Evaluating the effect of SRDS tolerance parameter $\\tau$ on sample quality for a pretrained diffusion model on 128x128 LSUN Church. The metric used is Kernel Inception Distance (KID, lower is better) over 1000 random samples generated using DDIM solvers. ", "page_idx": 16}, {"type": "table", "img_path": "XHWkHFWi3k/tmp/5e50aa4df80d247556d696d3cdb3da7d6f3e62990c225ea5ec435dca13bf56ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Next, we analyze how the FID score of generated samples varies as a function of the number of SRDS iterations. As shown in Figure 7, we observe rapid convergence of the FID score to the value obtained by sequential sampling (12.8) within a few SRDS iterations. ", "page_idx": 16}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/ae63c10999753afb0a92ae610128d6edb853e9ac4a123274dd8e27371e79408b.jpg", "img_caption": ["Figure 7: Convergence of the SRDS algorithm on LSUN Church. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "G Additional Samples from SRDS ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Various samples from Drawbench prompts are provided in Figure 8. ", "page_idx": 17}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/7043c041385920ef34503d76540947268e8c09d0cba6381fa7292feb26826425.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "35mm macro shot a kitten licking a baby duck, studio lighting. ", "page_idx": 17}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/4c045a6f34488beb878e08f2da8f43857da639c5419c9b3f373459f01830e7a0.jpg", "img_caption": ["A blue cup and a green cell phone. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "XHWkHFWi3k/tmp/3df227e2d3fce89dc2b654fdf0cb365bbc7f0ffa57239d7e5482e042046d9414.jpg", "img_caption": ["Figure 8: Convergence of the SRDS algorithm on various samples from DrawBench. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our novel algorithm and its related claims made in the abstract and introduction are discussed thoroughly in Section 3 with supporting empirical results in Section 4. The relation to existing work in the literature is presented in Sections 2 and 5. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Section 6 ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Complete proofs of all theoretical claims are presented in Appendix A. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes, all the experimental settings are thoroughly described in Section 4. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: There is a public link to our repository containing complete code needed to reproduce the main experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: All relevant experimental details are thoroughly described in Section 4. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Each evaluation of sample quality (using FID score) requires thousands of samples from the model; unfortunately, however, sampling from diffusion models is computationally very expensive. Due to computational constraints, we were unable to perform multiple rounds of our experiments to provide error bars for the FID scores. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Computer resource details are provided in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The research presented conforms to the NeurIPS Code of Ethics including aspects such as disclosing essential elements for reproducibility. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper presents foundational research: a novel fast sampling algorithm for diffusion models in an attempt unlock capabilities for real-time interaction with diffusion models. While there could be potential positive and negative societal impacts of this work stemming from applications that use this (say, real-time generation of deepfakes), we believe it is not a direct consequence of this work. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper presents a sampling algorithm and hence poses no such risks. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper properly credits and attributes the creators of all the models and datasets used. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]