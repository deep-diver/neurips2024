{"importance": "This paper is important because **it addresses the critical issue of explanation robustness in machine learning**, a crucial aspect for building trust and reliability in AI systems. By introducing a novel metric and training method, it offers a practical solution for enhancing the stability of explanations, which is highly relevant to current research trends in explainable AI and adversarial robustness.  The findings **open up avenues for future research**, including the development of new explanation methods, improved attack models and defenses, and further explorations into the theoretical underpinnings of explanation stability.", "summary": "R2ET: training for robust ranking explanations by an effective regularizer.", "takeaways": ["A novel ranking-based metric, \"ranking explanation thickness,\" is proposed to assess the robustness of explanations.", "R2ET, a new training method using an efficient regularizer, is introduced to enhance the stability of explanations.", "Theoretical analysis and extensive experiments demonstrate R2ET's superior stability and generalizability across various data and model types."], "tldr": "Existing methods for evaluating explanation robustness rely on lp distance, which may not align with human perception and can lead to an \"arms race\" between attackers and defenders.  Moreover, adversarial training (AT) methods, commonly used to improve robustness, are computationally expensive. This paper addresses these limitations. \nThe paper proposes a novel metric, \"ranking explanation thickness,\" which better captures human perception of stability. It also introduces R2ET, a training method that uses an efficient regularizer to promote robust explanations.  R2ET avoids the drawbacks of AT by directly optimizing the ranking stability of salient features and is theoretically shown to be robust against various attacks. Extensive experiments across different data modalities and model architectures confirm R2ET's superior performance.", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "AI Theory", "sub_category": "Interpretability"}, "podcast_path": "HYa3eu8scG/podcast.wav"}