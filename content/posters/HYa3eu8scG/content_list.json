[{"type": "text", "text": "Training for Stable Explanation for Free ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chao Chen1 Chenghua Guo2 Rufeng Chen3 Guixiang Ma4 Ming Zeng5 Xiangwen Liao6 Xi Zhang2 Sihong Xie3\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), China 2Key Laboratory of Trustworthy Distributed Computing and Service (MoE), Beijing University of Posts and Telecommunications, China ", "page_idx": 0}, {"type": "text", "text": "3Artificial Intelligence Thrust, The Hong Kong University of Science and Technology (Guangzhou), China 4 Intel, USA 5Carnegie Mellon University, USA 6College of Computer and Data Science, Fuzhou University, China cha01nbox@gmail.com {chenghuaguo,zhangx}@bupt.edu.cn rchen514@connect.hkust-gz.edu.cn jean.maguixiang@gmail.com ming.zeng@sv.cmu.edu liaoxw@fzu.edu.cn \u2217Corresponding to: xiesihong1@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To foster trust in machine learning models, explanations must be faithful and stable for consistent insights. Existing relevant works rely on the $\\ell_{p}$ distance for stability assessment, which diverges from human perception. Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race. To address these challenges, we introduce a novel metric to assess the stability of top- $k$ salient features. We introduce R2ET which trains for stable explanation by efficient and effective regularizer, and analyze R2ET by multiobjective optimization to prove numerical and statistical stability of explanations. Moreover, theoretical connections between R2ET and certified robustness justify R2ET\u2019s stability in all attacks. Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods. The code can be found at https://github.com/ccha005/R2ET. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks have proven their strengths in many real-world applications, and their explainability is a fundamental requirement for humans\u2019 trust [25, 62]. Explanations usually attribute predictions to human-understandable basic elements, such as input features and patterns under network neurons [35]. Given the inherent limitations of human cognition [69], only the most relevant top- $k$ features are presented to the end-users [89]. Among many existing explanation methods, gradient-based methods [55, 24] are widely adopted due to their inexpensive computation and intuitive interpretation. However, the gradients can be significantly manipulated with neglected changes in the input [16, 23]. The susceptibility of explanations compromises their integrity as trustworthy evidence when explanations are legally mandated [25], such as credit risk assessments [4]. ", "page_idx": 0}, {"type": "text", "text": "Challenges. As shown in Fig. 1, novel explanation methods [75, 77] and training methods [13, 17, 88] are proposed to promote the robustness of explanations. We focus on training methods since novel explanation methods require extra inference (explanation) time and may fail the sanity check [1]. Existing works [17, 88, 13, 16] study the explanation stability (robustness) through $\\ell_{p}$ distance. However, as shown in the right part of Fig. 1, a perturbed explanation with a small $\\ell_{p}$ distance to the original can exhibit notably different top salient features. Therefore, relying on $\\ell_{p}$ distance as a metric for optimizing the explanation stability fails to attain desired robustness, indicating the necessity for a novel ranking-based distance metric. Second, unlike robustness of prediction and $\\ell_{p}$ distance based explanation, analyses of robustness on ranking-based explanations are challenging due to multiple objectives (feature pairs), which are dependent since features are from the same model. Finally, adversarial training (AT) [86, 74] has been adopted for robust explanations, yet it potentially leads to an attack-defense arms race. AT conditions models on adversarial samples, which intrinsically rely on the objectives of the employed attacks. When defenders resist well against specific attacks, attackers will escalate attack intensity, which iteratively compels defenders to defend against fiercer attacks. Besides that, AT is time-intensive due to the search for adversarial samples per training iteration. ", "page_idx": 0}, {"type": "image", "img_path": "HYa3eu8scG/tmp/6f003a5502ee3e1ddaa3a49100e6528f4fe20cf43f4aafd7bc4dd932b9325ed7.jpg", "img_caption": ["Figure 1: Left: Green $\\left(\\bigoplus\\right)$ : Model training. Yellow $\\textcircled{2}{-}\\textcircled{4})$ : Explanation generation for a target input. Red $\\left(\\odot\\!\\cdot\\!\\left(6\\right)\\right)$ : Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller $\\ell_{p}$ distances do not imply similar top salient features. $\\mathbf{\\dot{\\mathcal{Z}}}(\\mathbf{x}^{\\prime\\prime})$ has a smaller $\\ell_{2}$ distance from the original explanation $\\mathcal{T}(\\mathbf{x})$ , but manipulates the explanation more significantly (by top- $k$ metric) shown in blue dashed boxes. Statistically, $\\mathcal{T}(\\mathbf{x}^{\\prime})$ has a $67\\%$ top-3 overlap in the tabular case, and $36\\%$ top-50 overlap in the image, compared with $\\mathcal{T}(\\mathbf{x}^{\\prime})$ \u2019s $100\\%$ and $92\\%$ top- $^{\\textit{k}}$ overlap, respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions. We center our contributions around a novel metric called \u201cranking explanation thickness\u201d that precisely measures the robustness of the top- $k$ salient features. Based on the bounds of thickness, we propose R2ET, a training method, for robust explanations by an effective and efficient regularizer. More importantly, R2ET comes with a theoretically certified guarantee of robustness to obstruct all attacks (Prop. 4.5), avoiding the arms race. Besides certified robustness, we theoretically prove that R2ET shares the same goal with AT but avoids intensive computation (Prop. 4.6), and the connection between R2ET with constrained optimization (Prop. 4.7). We also analyze the problem by multi-objective optimization to prove explanations\u2019 numerical and statistical stability in Sec. 5. ", "page_idx": 1}, {"type": "text", "text": "Results. We experimentally demonstrate that: $i$ ) Prior $\\ell_{p}$ norm attack cannot manipulate explanations as effectively as ranking-based attacks. R2ET achieves more ranking robustness than state-of-theart methods; ii) Strong evidence indicates a substantial correlation between explanation thickness and robustness; iii) R2ET proves its generalizability by showing its superiority across diverse data modalities, model structures, and explanation methods such as concept-based and saliency map-based. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Robust Explanations. Gradient-based methods are widely used due to their simplicity and efficiency [55], while they lack robustness against small perturbations [23]. Adversarial training (AT) is used to improve the explanation robustness [13, 74]. Alternatively, some works propose replacing ReLU function with softplus [17], training with weight decay [16], and incorporating gradient- and Hessianrelated terms as regularizers [17, 88, 91]. Besides, some works propose post-hoc explanation methods, such as SmoothGrad [75]. More relevant works are discussed in Appendix C. ", "page_idx": 1}, {"type": "text", "text": "Ranking robustness in IR. Ranking robustness in information retrieval (IR) is well-studied [106, 26, 104]. Ranking manipulations in IR and explanations are different since 1) In IR, the goal is to distort the ranking of candidates by manipulating one candidate or the query. We manipulate input to swap any pairs of salient and non-salient features. 2) Ranking in IR is based on the model predictions. However, explanations rely on gradients, and studying their robustness requires second or higher-order derivatives, necessitating an efficient regularizer to bypass intensive computations. ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Saliency map explanations. Let a classification model with parameters w be $f(\\mathbf{x},\\mathbf{w})\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow$ $[0,1]^{C}$ , and $f_{c}(\\mathbf{x},\\mathbf{w})$ is the probability of class $c$ for the input $\\mathbf{x}$ . Denote a saliency map explanation [1] for $\\mathbf{x}$ concerning class $c$ as $\\mathcal{T}(\\mathbf{x},c;f)=\\nabla_{\\mathbf{x}}f_{c}(\\mathbf{x},\\mathbf{w})$ . Since $f$ is fixed for explanations, we omit w and fix $c$ to the predicted class. We denote $\\mathcal{T}(\\mathbf{x},c;f)$ by $\\mathcal{T}(\\mathbf{x})$ , and the $i$ -th feature\u2019s importance score, e.g., the $i$ -th entry of $\\mathcal{T}(\\mathbf{x})$ , by ${\\mathcal{T}}_{i}(\\mathbf{x})$ . The theoretical analysis following will mainly based on saliency explanations, and more explanation methods, such as Grad $\\times$ Inp, SmoothGrad [75], and IG [77], are studied in Sec. 6 and Appendix A.1.2. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Threat model. The adversary solves the following problem to find the optimal perturbation $\\delta^{*}$ to distort the explanations without changing the predictions [17]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta^{*}=\\mathop{\\mathrm{arg\\,max}}_{\\delta:\\|\\delta\\|_{2}\\leq\\epsilon}~\\mathrm{Dist}(\\mathbb{Z}(\\mathbf{x}),\\mathbb{Z}(\\mathbf{x}+\\delta)),\\quad\\mathrm{~s.t.~}\\mathop{\\mathrm{arg\\,max}}_{c}f_{c}(\\mathbf{x})=\\mathop{\\mathrm{arg\\,max}}_{c}f_{c}(\\mathbf{x}+\\delta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\delta$ is the perturbation whose $\\ell_{2}$ norm is not larger than a given budget $\\epsilon$ . $\\mathrm{{Dist}}(\\cdot,\\cdot)$ evaluates how different the two explanations are. We tackle constraints by constrained optimization (see B.2.3) and will not explicitly show the constraints. ", "page_idx": 2}, {"type": "text", "text": "4 Explanation Robustness via Thickness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first present the rationale and formal definition of thickness in Sec. 4.1, then propose R2ET algorithm to promote the thickness grounded on theoretical analyses in Sec. 4.2. All omitted proofs and more theoretical discussions are provided in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "4.1 Ranking explanation thickness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Quantify the gap. Given a model $f$ and the associated original explanation $\\mathcal{T}(\\mathbf{x})$ with respect to an input $\\mathbf{x}\\in\\mathbb{R}^{n}$ , we denote the gap between the $i$ -th and $j$ -th features\u2019 importance by $h(\\mathbf{x},\\bar{i},j)=$ $\\mathbb{Z}_{i}(\\mathbf{x})-\\mathcal{T}_{j}(\\mathbf{x})$ . Clearly, $h(\\mathbf{x},i,j)>0$ if and only $i f$ the $i$ -th feature has a more positive contribution to the prediction than the $j$ -th feature. The magnitude of $h$ reflects the disparity in their contributions. Although the feature importance order varies across different $\\mathbf{x}$ , for notation brevity, we label the features in a descending order such that $h(\\mathbf{x},i,j)>0,\\forall i<j$ , holds for any original input $\\mathbf{x}$ . This assumption will not affect the following analysis. ", "page_idx": 2}, {"type": "text", "text": "Pairwise thickness. The adversary in Eq. (1) searches for a perturbed input $\\mathbf{x}^{\\prime}$ to flip the ranking between features $i$ and $j$ so that $h(\\mathbf{x}^{\\prime},i,j)<0$ for some $i<j$ . A simple evaluation metric, such as $\\theta=\\mathbb{1}[h(\\mathbf{x}^{\\prime},i,j)\\ge0]$ , can only indicate if the ranking flips at $\\mathbf{x}^{\\prime}$ , failing to quantify the extent of disarray. Moreover, it is greatly impacted by the choice of $\\mathbf{x}^{\\prime}$ . Conversely, the explanation thickness is defined by the expected gap of the relative ranking of the feature pair $(i,j)$ in a neighborhood of $\\mathbf{x}$ ", "page_idx": 2}, {"type": "text", "text": "Definition 4.1 (Pairwise ranking thickness). Given a model $f$ , an input $\\mathbf{x}\\in\\mathcal{X}$ and a distribution $\\mathcal{D}$ of perturbed input $\\mathbf{x}^{\\prime}\\sim\\mathcal{D}$ , the pairwise ranking thickness of the pair of features $(i,j)$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Theta(f,\\mathbf{x},\\mathcal{D},i,j)\\overset{\\mathrm{def}}{=}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathcal{D}}\\left[\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t\\right],\\quad\\mathrm{where}~\\mathbf{x}(t)=(1-t)\\mathbf{x}+t\\mathbf{x}^{\\prime},t\\in[0,1].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The integration computes the average gap between two features from $\\mathbf{x}$ to $\\mathbf{x}^{\\prime}$ . The expectation over $\\mathbf{x}^{\\prime}\\sim\\mathcal{D}$ makes an overall estimation. For example, $\\mathbf{x}^{\\prime}$ can be from a Uniform distribution [88] or the adversarial samples local to $\\mathbf{x}$ [97]. The relevant work [97] proposes the boundary thickness to evaluate a model\u2019s prediction robustness by the distance between two level sets. ", "page_idx": 2}, {"type": "text", "text": "Top- $k$ thickness. Existing works in general robust ranking [105] propose maintaining the ranking between every two entities (features). However, as shown in Fig. 1, only the top- $k$ important features in $\\mathcal{T}(\\mathbf{x})$ attract human perception the most, and will be delivered to end-users. Thus, we focus on the relative ranking between a top- ${\\cdot k}$ salient feature and any other non-salient one. ", "page_idx": 2}, {"type": "text", "text": "Definition 4.2 (Top- $k$ ranking thickness). Given a model $f$ , an input $\\mathbf{x}\\in\\mathcal{X}$ , and a distribution $\\mathcal{D}$ of $\\mathbf{x}^{\\prime}\\sim\\mathcal{D}$ , the ranking thickness of the top- ${\\cdot k}$ features is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Theta(f,\\mathbf{x},\\mathcal{D},k)\\overset{\\mathrm{def}}{=}\\frac{1}{k(n-k)}\\sum_{i=1}^{k}\\sum_{j=k+1}^{n}\\Theta(f,\\mathbf{x},\\mathcal{D},i,j).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Variants and discussions. We consider and discuss the following three major variants of thickness. ", "page_idx": 2}, {"type": "text", "text": "\u2022 One variant of thickness in Eq. (2) is in a probability version: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{Pr}}(f,\\mathbf{x},\\mathcal{D},i,j)\\stackrel{\\mathrm{def}}{=}\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathcal{D}}\\left[\\int_{0}^{1}\\mathbb{1}[h(\\mathbf{x}(t),i,j)\\geq0]d t\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, the inherent non-differentiability of the indicator function hinders effective analysis and optimization of thickness. One may replace it with a sigmoid to alleviate the non-differentiability issue. Yet, the use of a sigmoid can potentially lead to vanishing gradient problems. ", "page_idx": 3}, {"type": "text", "text": "\u2022 By considering the relative positions of the top- $k$ features, one can define an average precision $@k$ like thickness $\\begin{array}{r}{\\Theta_{a p}\\stackrel{\\mathrm{def}}{=}\\frac{1}{K}\\sum_{k=1}^{K}\\Theta(f,\\mathbf{x},\\mathcal{D},k)}\\end{array}$ , or a discounted cumulative gain [33] like thickness \u0398dcg d=ef  ik=1 jn=k+1\u0398lo(gf(,jx,\u2212Di,+i,1j)) . Other variants for specific properties or purposes can be derived similarly, and we will study these variants deeply in future work. \u2022 Many non-salient features are less likely to be confused with the top- $k$ salient features (see Fig. 8), and treating them equally may complicate optimization. Thus, one may approximate the top- $k$ thickness by $k^{\\prime}$ i npcalirusd oe ft fheea tcuarsees $\\begin{array}{r}{\\sum_{i=1}^{k^{\\prime}}h(\\mathbf{x},k\\!-\\!i,k\\!+\\!i)}\\end{array}$ w, itwhh tichhe  sseulfefcixts $k^{\\prime}$ m dims\u201dti innc tt hpea iersx pweirtih mmeinntis.mal $h(\\mathbf{x},i,j)$ . We   when $k^{\\prime}=k$ denoted  \u201c- ", "page_idx": 3}, {"type": "text", "text": "4.2 R2ET: Training for robust ranking explanations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will theoretically (in Sec. 5) and experimentally (in Sec. 6) demonstrate that maximizing the ranking explanation thickness in Eq. (3) can make attacks more difficult and thus the explanation more robust. Thus, a straightforward way is to add $\\Theta(f,\\mathbf{x},\\mathcal{D},k)$ as a regularizer during training: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\mathcal{L}=\\mathbb{E}_{\\mathbf{x}}\\left[\\mathcal{L}_{c l s}(\\boldsymbol{f},\\mathbf{x})-\\lambda\\Theta(\\boldsymbol{f},\\mathbf{x},\\mathcal{D},k)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}_{c l s}$ is the classification loss. However, direct optimization of $\\Theta$ requires $M_{1}\\times M_{2}\\times2$ backward propagations per training sample. $M_{1}$ is the number of $\\mathbf{x}^{\\prime}$ sampled from $\\mathcal{D}$ ; $M_{2}$ is the number of interpolations ${\\mathbf x}(t)$ between $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ ; and evaluating the gradient of $h(\\mathbf{x},i,j)$ requires at least 2 backward propagations [59]. In response, we consider the bounds of $\\Theta$ in Prop. 4.4, and propose R2ET which requires only 2 backward propagations each time. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.3 (Locally Lipschitz continuity). A function $f$ is $L$ -locally Lipschitz continuous if $\\|f(\\mathbf{x})-f(\\mathbf{x}^{\\prime})\\|_{2}\\leq L\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}$ holds for all $\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{2}(\\mathbf{x},\\epsilon)=\\{\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{n}:\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}\\leq\\epsilon\\}$ . Proposition 4.4. (Bounds of thickness) Given an $L$ -locally Lipschitz model $f$ , for some $L>0$ , pairwise ranking thickness $\\Theta(f,\\mathbf{x},\\mathcal{D},i,j)$ is bounded by ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(\\mathbf{x},i,j)-\\epsilon\\ast\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}\\leq\\Theta(f,\\mathbf{x},\\mathcal{D},i,j)\\leq h(\\mathbf{x},i,j)+\\epsilon\\ast(L_{i}+L_{j}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $H_{i}(\\mathbf{x})$ is the derivative of ${\\mathcal{T}}_{i}(\\mathbf{x})$ with respect to $\\mathbf{x}$ , and $\\begin{array}{r}{L_{i}=\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in B_{2}(\\mathbf{x},\\epsilon)}\\|H_{i}(\\mathbf{x}^{\\prime})\\|_{2}.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Noticing $\\begin{array}{r}{\\operatorname*{lim}_{\\|H(\\mathbf{x})\\|_{2}\\rightarrow0}\\Theta(f,\\mathbf{x},\\mathcal{D},i,j)=h(\\mathbf{x},i,j)}\\end{array}$ , the objective of the proposed R2ET, Robust Ranking Explanation via Thickness, is to maximize the gap and minimize Hessian norm: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\mathcal{L}=\\mathbb{E}_{\\mathbf{x}}\\left[\\mathcal{L}_{c l s}-\\lambda_{1}\\sum_{i=1}^{k}\\sum_{j=k+1}^{n}h(\\mathbf{x},i,j)+\\lambda_{2}\\|H(\\mathbf{x})\\|_{2}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda_{1},\\lambda_{2}\\geq0$ . R2ET with $\\lambda_{1}=0$ recovers Hessian norm minimization for smooth curvature [17]. The difference between R2ET in Eq. (6) and vanilla model are two regularizers. $h(\\mathbf{x},i,j)=$ $\\mathbb{Z}_{i}(\\mathbf{x})-\\mathcal{T}_{j}(\\mathbf{x})$ can be calculated by one time backward propagation over $\\mathcal{T}(\\mathbf{x})$ , and the summations are done by assigning different weights to ${\\mathcal{T}}_{i}(\\mathbf{x})$ , e.g., $(n-k)$ for $i\\ <=k$ , and $-k$ for $i\\,>\\,k$ $\\|H(\\mathbf{x})\\|_{2}$ is estimated by the finite difference (calculating the difference of the gradient of $\\mathcal{T}(\\mathbf{x})$ and $\\boldsymbol{\\mathcal{T}}(\\mathbf{x}+\\boldsymbol{\\delta}))$ , which costs two times backward propagation [52]. Thus, R2ET is at an extra cost of 2 times backward propagation. For comparison, AT (in PGD-style) searches the adversarial samples by $M_{2}$ (being 40 in [13, 70]) iterations for each training sample in every training epoch. ", "page_idx": 3}, {"type": "text", "text": "We connect R2ET to three established robustness paradigms, including certified robustness, AT, and constrained optimization. Intuitively, three methods assess robustness from the adversary\u2019s perspective by identifying the worst-case samples and restricting model responses to these cases. Conversely, R2ET is defender-oriented, which preserves models\u2019 behavior stably by imposing restrictions on the rate of change, avoiding costly adversarial sample searches. ", "page_idx": 3}, {"type": "text", "text": "Connection to certified robustness. Prop. 4.5 delineates the minimal budget required for a successful attack. The defense strategy of maximizing the budget can effectively mitigate the arms race by obstructing all attacks. The strategy essentially aligns with R2ET, but is empirically less stable and harder to converge due to the second-order term in the denominator. The proof is based on [28]. ", "page_idx": 3}, {"type": "text", "text": "Proposition 4.5. For all $\\delta$ with $\\begin{array}{r}{\\|\\delta\\|_{2}\\leq\\operatorname*{min}_{\\{i,j\\}}\\frac{h(\\mathbf{x},i,j)}{\\operatorname*{max}_{\\mathbf{x}^{\\prime}}\\|H_{i}(\\mathbf{x}^{\\prime})-H_{j}(\\mathbf{x}^{\\prime})\\|_{2}}}\\end{array}$ , it holds $\\mathbb{1}[h(\\mathbf{x},i,j)>0]$ $\\mathbf{\\varepsilon}=\\mathbb{1}[h(\\mathbf{x}+\\delta,i,j)>0].$ for all $(i,j)$ pair, that is all the feature rankings do not change. ", "page_idx": 4}, {"type": "text", "text": "Connection to adversarial training (AT). Prop. 4.6 implies that R2ET shares the same goal as AT but employs regularizers to avoid costly computations. The proof in Appendix A.3 is based on [94]. Proposition 4.6. The optimization problem in Eq. (6) is equivalent to the following min-max problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}{\\operatorname*{max}_{(\\delta_{1,k+1},\\dots,\\delta_{k,n})\\in\\mathcal{M}}\\mathbb{E}_{\\mathbf{x}}}\\left[\\mathcal{L}_{c l s}-\\sum_{i=1}^{k}\\sum_{j=k+1}^{n}h(\\mathbf{x}+\\delta_{i,j},i,j)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\delta_{i,j}\\in\\mathbb{R}^{n}$ is a perturbation to $\\mathbf{x}$ targeting at the $(i,j)$ pair of features. $\\mathcal{M}$ is the feasible set of perturbations where each $\\delta_{i,j}$ is independent of each other, with $\\|\\sum_{i,j}\\delta_{i,j}\\|\\leq\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "Connection to constrained optimization. Prop. 4.7 shows that R2ET aims to keep the feature in the \u201ccorrect\u201d positions, e.g., $l_{i}\\mathbb{Z}_{i}\\geq G_{i}$ , for the local vicinity of original inputs, thus promoting robustness. First, Eq. (6) can be considered a Lagrange function for the constrained problem in Eq. (8): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{s}}\\quad\\mathbb{E}_{\\mathbf{x}}\\left[\\mathcal{L}_{c l s}+\\lambda_{2}\\|H(\\mathbf{x})\\|_{2}\\right],\\qquad\\mathrm{s.t.}\\quad l_{i}\\mathcal{L}_{i}(\\mathbf{x})\\geq G_{i},\\,\\forall i\\in\\{1,\\ldots,n\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $G_{i}\\geq0$ is a predefined margin. $\\boldsymbol{l}_{i}=(\\boldsymbol{n}-\\boldsymbol{k})$ for $i\\leq k$ , and $l_{i}=-k$ otherwise, which labels features at the top as positive and those at the bottom as negative. The proof is based on [34]. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.7. The optimization problem in Eq. (8) is equivalent to the following problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\quad\\mathbb{E}_{\\mathbf{x}}\\left[\\mathcal{L}_{c l s}\\right]\\qquad\\mathrm{s.t.}\\quad\\operatorname*{min}_{\\delta_{i}:\\|\\delta_{i}\\|_{2}\\leq\\epsilon}l_{i}\\mathcal{Z}_{i}(\\mathbf{x}+\\delta_{i})\\geq G_{i},\\:\\forall i\\in\\{1,\\ldots,n\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5 Analyses of numerical and statistical robustness ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Numerical stability of explanations. Different from prior work, we characterize the worst-case complexity of explanation robustness using iterative numerical algorithm for constrained multiobjective optimization. Threat model. The attacker aims to swap the ranking of any pair of a salient feature $i$ and a non-salient feature $j$ , so that the gap $h(\\mathbf{x},i,j)<\\mathbf{\\bar{\\alpha}}$ , while does not change the input $\\mathbf{x}$ and the predictions $f(\\mathbf{x})$ significantly for stealthiness. We assume that the attacker has access to the parameters and architecture of the model $f$ to conduct gradient-based white-box attacks. ", "page_idx": 4}, {"type": "text", "text": "Optimization problem for the attack. Each $h(\\mathbf{x},i,j)$ can be treated as an objective function for the attacker, who however does not know which objective is the easiest to attack. Furthermore, attacking one objective can make another easier or harder to attack due to their dependency on the same model $f$ and the input $\\mathbf{x}$ . As a result, an attack against a specific target feature pair does not reveal the true robustness of the feature ranking. We quantify the hardness of a successful attack (equivalently, explanation robustness) by an upper bound on the number of iterations for the attacker to flip the first (unknown) feature pair. The longer it takes to flip the first pair, the more robust the explanations of $f(\\mathbf{x})$ are. Formally, for any given initial input $\\mathbf{x}^{(0)}$ ( $(p)$ in the superscript means the $p$ -th attacking iteration), the attacker needs to iteratively manipulate $\\mathbf{x}$ to reduce the gaps $h(\\mathbf{x},i,j)$ for all $i\\in\\{1,\\ldots,k\\}$ and $j\\in\\{k+1,\\ldots,n\\}$ packed in a vector objective function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{MOO-Attack}(\\mathbf x,\\epsilon);\\,\\operatorname*{min}_{\\mathbf x}[h_{1}(\\mathbf x),\\dots,h_{m}(\\mathbf x)],\\quad\\mathbf s.\\mathbf t.\\,\\,\\|f(\\mathbf x)-f(\\mathbf x^{(0)})\\|\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h_{\\ell}(\\mathbf{x})\\ {\\stackrel{\\mathrm{def}}{=}}\\ h(\\mathbf{x},i,j)$ , with $\\ell=1,\\ldots,m$ indexing the $m$ pairs of $(i,j)$ . ", "page_idx": 4}, {"type": "text", "text": "Comments: $(i)$ The scalar function $\\begin{array}{r}{\\sum_{i}\\sum_{j}h(\\mathbf{x},i,j)}\\end{array}$ is unsuitable for theoretical analyses, since there can be no pair of features flipped at a stationary point of the sum. Simultaneously minimizing multiple objectives identifies when the first pair of features is flipped and gives a pessimistic bound for defenders (and an optimistic bound for attackers). $(i i)$ We could adopt the Pareto criticality for convergence analysis [21], but a Pareto critical point indicates that no critical direction (moving in which does not violate the constraint) leads to a joint descent in all objectives, rather than that an objective function $h_{\\ell}$ has been sufficiently minimized. $(i i i)$ Different from the unconstrained case in [19], we constrain the amount of changes in $\\mathbf{x}$ and $f(\\mathbf{x})$ . ", "page_idx": 4}, {"type": "text", "text": "We propose a trust-region method, Algorithm 1, to solve MOO-Attack (A.4 for more details). First, we define the merit function $\\phi_{\\ell}$ that combines the $\\ell_{}$ -th objective and the constraint in Eq. (10): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi_{\\ell}(\\mathbf{x},t)\\stackrel{\\mathrm{def}}{=}\\|f(\\mathbf{x})\\|+|h_{\\ell}(\\mathbf{x})-t|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As an approximating model for the trust-region method, the linearization of $\\phi(\\mathbf{x},t)$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\nl_{\\phi_{\\ell}}(\\mathbf{x},t,\\mathbf{d})=\\|f(\\mathbf{x})+J(\\mathbf{x})^{\\top}\\mathbf{d}\\|+|h_{\\ell}(\\mathbf{x})+g_{\\ell}(\\mathbf{x})^{\\top}\\mathbf{d}-t|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $J(\\mathbf{x})$ is the Jacobian of $f$ at $\\mathbf{x}$ and $g_{\\ell}(\\mathbf{x})$ is the gradient of $h_{\\ell}$ . The maximal reduction in $l_{\\phi_{\\ell}}(\\mathbf{x},t,0)$ within a radius of $\\Delta>0$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\chi_{\\ell}(\\mathbf{x},t)\\overset{\\mathrm{def}}{=}l_{\\phi_{\\ell}}(\\mathbf{x},t,0)-\\operatorname*{min}_{\\mathbf{d}:\\|\\mathbf{d}\\|\\leq\\Delta}l_{\\phi_{\\ell}}(\\mathbf{x},t,\\mathbf{d}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\mathbf{x}$ is called a critical (approximately critical) point of $\\phi_{\\ell}$ when $\\chi_{\\ell}(\\mathbf{x},t)=0$ (or $\\leq\\epsilon$ ). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Attacking a pair of features ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: target model $f$ , input $\\mathbf{x}$ , explanation $\\mathcal{T}(\\mathbf{x})$ , trust-region method parameters $0<\\gamma,\\eta<1$ .   \n2: Set $k=1,\\ \\ \\mathbf{x}^{(p)}=\\mathbf{x},\\ \\ t_{\\ell}^{(p)}=\\|f(\\mathbf{x}^{(p)})\\|+h_{\\ell}(\\mathbf{x}^{(p)})-\\epsilon^{(p)}$ .   \n3: while $\\begin{array}{r}{\\operatorname*{min}_{1\\leq\\ell\\leq m}\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big)\\geq\\epsilon\\,\\mathbf{do}}\\end{array}$   \n4: Solve TR- $\\cdot{\\bf M O O}({\\bf x}^{(p)},\\Delta^{(p)})$ to obtain a joint descent direction $\\mathbf{d}^{(p)}$ for all $l_{\\phi_{\\ell}}$ .   \n5: Update $\\rho_{\\ell}^{(p)}$ using $\\mathbf{d}^{(p)}$ for each $\\ell=1,\\ldots,m$ .   \n6: if min\u2113\u03c1\u2113 $\\rho_{\\ell}^{(p)}>\\eta$ then   \n7: Update $t_{\\ell}^{(p+1)}$ and $\\mathbf{x}^{(p+1)}$ .   \n8: else   \n9: Update $\\Delta^{(p+1)}=\\gamma\\Delta^{(p)}$ .   \n10: end if   \n11: end while ", "page_idx": 5}, {"type": "text", "text": "The core of Algorithm 1 is TR- $\\mathbf{-MOO}(\\mathbf{x},\\Delta)$ , whose optimal solution provides a descent direction d for all linearized merit functions $l_{\\phi_{\\ell}}$ (not $h_{\\ell}$ or $\\phi_{\\ell}$ ): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TR-MOO}(\\mathbf{x},\\Delta)\\left\\{\\begin{array}{l l}{\\operatorname*{min}_{\\alpha,\\mathbf{d}}}&{\\alpha}\\\\ {\\mathrm{s.t.}}&{l_{\\phi_{\\ell}}(\\mathbf{x},t,\\mathbf{d})\\leq\\alpha,\\forall1\\leq\\ell\\leq m,}\\\\ &{\\|\\mathbf{d}\\|\\leq\\Delta.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ is the minimal amount of descent and $\\Delta$ is the current radius of search for $\\mathbf{d}$ . ", "page_idx": 5}, {"type": "text", "text": "Different from the local convergence results [20], we provide a global rate of convergence for Algorithm 1 to justify thickness maximization in Theorem 5.1. Specifically, it implies that increasing gaps between salient and non-salient features tend to result in larger $h_{\\mathrm{up}}-h_{\\mathrm{low}}$ , which makes it harder for attackers to alter the rankings, leading to stronger explanation robustness. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1. Suppose that $f(\\mathbf{x})$ and $h_{\\ell}(\\mathbf{x})$ are continuously differentiable and $h_{\\ell}$ is bounded. Then Algorithm 1 generates an \u03f5-first-order critical point for problem Eq. (10) in at most $\\begin{array}{r}{\\left\\lceil\\left(h_{\\mathrm{up}}-h_{\\mathrm{low}}\\right)\\frac{\\kappa}{\\epsilon^{2}}\\right\\rceil}\\end{array}$ iterations, where $\\kappa$ is a constant independent of \u03f5 but depending on $\\gamma$ and $\\eta$ in Algorithm $^{\\,l}$ , $h_{\\mathrm{up}}\\ {\\stackrel{d e f}{=}}$ $\\begin{array}{r}{\\operatorname*{max}_{\\ell}\\{\\operatorname*{max}_{\\mathbf{x}}h_{1}(\\mathbf{x}),\\dots,\\operatorname*{max}_{\\mathbf{x}}h_{m}(\\mathbf{x})\\},}\\end{array}$ , and $\\begin{array}{r}{h_{\\mathrm{low}}\\overset{d e f}{=}\\operatorname*{min}_{\\ell}\\{\\operatorname*{min}_{\\mathbf{x}}h_{1}(\\mathbf{x}),\\ldots,\\operatorname*{min}_{\\mathbf{x}}h_{m}(\\mathbf{x})\\}.}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Statistical stability of explanations. We use McDairmid\u2019s inequality for dependent random variables and the covering number of the space $\\mathcal{F}$ of saliency maps $\\mathcal{T}$ in Theorem 5.2. The theorem justifies maximizing the gap by showing a bound on how likely a model can rank some non-salient features higher than some salient features due to perturbation to $\\mathbf{x}$ . Specifically, for a consistent empirical risk level $R_{0,1,u}^{\\mathrm{emp}}$ , a larger gap corresponds to a larger $u$ , which leads to a smaller covering number $\\textstyle N(\\mathcal{F},\\frac{\\epsilon u}{8})$ . It indicates a reduced probability of high true risk $R_{0,1}^{\\mathrm{true}}$ , thereby implying more explanations robustness against perturbations. Importantly, this insight is universal regardless of the approach to achieving the larger gaps. See Appendix A.5 for relevant definitions and details. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.2. Given model $f_{;}$ , input x, surrogate loss $\\phi_{u},$ , and a constant $\\epsilon>0,$ , for arbitrary saliency map $\\mathcal{T}\\in\\mathcal{F}$ and any distribution $\\mathcal{D}$ surrounding $\\mathbf{x}$ that preserves the salient features in $\\mathbf{x}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left\\{R_{0,1}^{\\mathrm{true}}({\\mathbb{Z}},{\\bf x})\\geq R_{0,1,u}^{\\mathrm{emp}}({\\mathbb{Z}},{\\bf x})+\\epsilon\\right\\}\\leq\\exp\\left(-\\frac{2m\\epsilon^{2}}{\\chi}\\right){\\cal N}\\left({\\mathcal F},\\frac{\\epsilon u}{8}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Table $1\\colon\\mathbf{P}\\mathcal{@}k$ (shown in percentage) of different models (rows) under ERAttack / MSE attack. $k\\,=\\,8$ for the first three dataset, and $k=50$ for the rest. Bold (and underlines) highlight the winner (and runner-up), $^\\dagger$ indicates the significant superiority between R2ET winner and non-R2ET winner (pairwise t-test at a $5\\%$ significance level). $^*$ Est-H has about $4\\%$ lower clean AUC than others on BP. Exact-H and SSR only apply to tabular datasets, since computing the exact Hessian and its eigenvalues is extremely expensive. ", "page_idx": 6}, {"type": "table", "img_path": "HYa3eu8scG/tmp/c0fe536f2c2327d7eb87fb7f6da5968a2f039e53c8ac192c47a2949b7b08523d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{N}$ is the covering number of the space $\\mathcal{F}$ with radius $\\frac{\\epsilon u}{8}$ [102]. $\\chi$ is the chromatic number of a dependency graph of all pairs of salient and non-salient features [81] for McDairmid\u2019s inequality. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section experimentally validates the proposed defense strategies, R2ET, from various aspects, and Appendix B provides more details and results. For a fair comparison, R2ET operates without any prior knowledge of the \u201ctrue\u201d feature ranking. At each training iteration, R2ET aims to preserve the feature ranking determined in the immediately preceding iteration. ", "page_idx": 6}, {"type": "text", "text": "Datasets. We adopt DNNs for three tabular datasets: Bank [53], Adult, and COMPAS [54], and two image datasets, CIFAR-10 [36] and ROCT [22]. ROCT consists of real-world medical images having 771x514 pixels on average, making it comparable in scale to CelebA (178x218), ImageNet $(469\\mathrm{x}387)$ , and MS COCO $(640\\mathrm{x}480)$ . Besides, dual-input Siamese networks [48] are adopted on MNIST [40] and two graph datasets, BP [48] and ADHD [49]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. Precision $@k$ $(\\mathrm{P}@k)$ [88] is used to quantify the similarity between explanations before and after attacks. We adopt three faithfulness metrics [72, 15] in Appendix B.2.6 to show that explanations from R2ET are faithful. ", "page_idx": 6}, {"type": "text", "text": "All the models have comparable prediction performance. Specifically, the vanilla models achieve AUC of 0.87, 0.64, 0.83, 0.99, 0.87, 0.82, 0.69, and 0.76 on Adult, Bank, COMPAS, MNIST, CIFAR10, ROCT, BP, and ADHD, respectively. Correspondingly, all other models reported only when their AUC is no less than 0.86, 0.63, 0.83, 0.99, 0.86, 0.82, 0.67, and 0.74, respectively. ", "page_idx": 6}, {"type": "text", "text": "Explanation methods. Gradient (Grad for short) is used as the explanation method if not specified. We also share findings when adopting robust explanation method, e.g., SmoothGrad (SG) [75] and Integrated Gradient (IG) [77], and concept-based explanation methods (on ROCT datasets). ", "page_idx": 6}, {"type": "text", "text": "6.1 Compared methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct two attacks in the PGD manner [50]: Explanation Ranking attack (ERAttack) and MSE attack. ERAttack minimizes $\\begin{array}{r}{\\sum_{i=1}^{k}\\sum_{j=k+1}^{n}h(\\mathbf{x}^{\\prime},i,j)}\\end{array}$ to manipulate the ranking of featur\u2032es in explanation , and MSE attack maximizes the MSE (i.e., distance) between and . The proposed defense strategy R2ET is compared with the following baselines. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Vanilla: provides the basic ReLU model trained without weight decay or any regularizer term. \u2022 Curvature smoothing based methods: Weight decay (WD) [17] implicitly binds Hessian norm by weight decay during training, and Softplus (SP) [16, 17] replaces ReLU with Softplus $(x;\\rho)=$ $\\textstyle{\\frac{1}{\\rho}}\\ln(1+e^{\\rho x})$ . Est-H [17] and Exact- $\\mathbf{\\nabla}\\cdot\\mathbf{H}$ consider the estimated (by the finite difference [52]) and exact Hessian norm as the regularizer, respectively. SSR [88] sets the largest eigenvalue of the Hessian matrix as the regularizer. ", "page_idx": 6}, {"type": "table", "img_path": "HYa3eu8scG/tmp/cd7d1728934a6aabf8aac421b868e3ace2c490e5058ad830137cbd30419fffbe.jpg", "table_caption": ["Table $2\\colon\\mathbf{P}@k$ of models under ERAttack when adopting Grad / SG / IG as the explanation method. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u2022 Adversarial Training (AT) [31, 92]: finds $f$ by $\\operatorname*{min}_{f}\\mathcal{L}_{c l s}(f;\\mathbf{x}\\,+\\,\\delta^{*},y)$ , where $\\delta^{*}=$ $\\begin{array}{r}{\\arg\\operatorname*{max}_{\\delta}-\\sum_{i=1}^{k}\\sum_{j=k+1}^{n}h(\\mathbf{x}+\\delta,i,j)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 Variants of R2ET: R2ET-mm selects multiple $(k)$ distinct $i,j$ with minimal $h(\\mathbf{x},i,j)$ as discussed in Sec. 4.1. $\\mathbf{R}2\\mathbf{E}\\mathbf{T}_{\\backslash\\mathbf{H}}$ and $\\mathbf{R}2\\mathbf{E}\\mathbf{T}\\mathbf{-mm}_{\\backslash\\mathbf{H}}$ are the ablation variants of R2ET and R2ET-mm, respectively, without optimizing the Hessian-related term in Eq. (6) $\\lambda_{2}\\,=\\,0)$ ). Est-H can be considered an ablation variant of R2ET $\\lambda_{1}=0$ ). ", "page_idx": 7}, {"type": "text", "text": "6.2 Overall robustness results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Attackability of ranking-based explanation. Table 1 reports the explanation robustness of different models against ERAttack and MSE attacks on all datasets. More than $50\\%$ of models achieve at least $90\\%$ $\\mathrm{P}@\\bar{k}$ against MSE attacks, concluding that MSE attack cannot effectively alter the rankings of salient features, even without extra defense (row Vanilla). The ineffective attack method can give a false (over-optimistic) impression of explanation robustness. In contrast, ERAttack can displace more salient features from the top- $k$ positions for most models and datasets, leading to significantly lower $\\mathrm{P}@k$ values than MSE attack. Intuitively, R2ET performs better against attacks since the attackers (by either MSE attack or ERAttack) are supposed to keep the predictions unchanged. R2ET maintains consistency between model explanations and predictions, which ensures that significant manipulation in the top salient features leads to a detectable change in the model\u2019s predictions. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of R2ET against ERAttacks. We evaluate the effectiveness of different defense strategies against ERAttack, and similar conclusions can be made with MSE attacks case. (1) The best (highest) top- $k$ of R2ET and its variants across most datasets indicate their superiority in preserving the top salient features. (2) It is counter-intuitive that $\\mathrm{R}2\\mathrm{E}\\mathrm{T}_{\\backslash H}$ , as an ablation version of R2ET, outperforms R2ET on Adult and Bank. The reason is that $\\mathrm{R}2\\mathrm{E}\\mathrm{T}_{\\backslash H}$ has the highest thickness on these datasets (see Table 3 in Sec. 6.3). (3) Overall, the curvature smoothing-based baselines without considering the gaps among feature importance perform unstably and usually badly across datasets. Their inferior performance is exactly consistent with the discussion in Sec 4.2. Specifically, solely minimizing Hessian-related terms may marginally contribute to the ranking robustness. (4) We do not compare with many AT baselines [74, 70] but Fast-AT [92], which provides a fairer basis for comparing with R2ET and baselines since they require similar training time and resources. However, AT suffers from unstable robust performance and cannot perform well on most datasets. ", "page_idx": 7}, {"type": "text", "text": "Apply R2ET to other explanation methods. We demonstrate the efficacy and generalizability of R2ET by adopting the concept-based explanation method, SG and IG, respectively. ", "page_idx": 7}, {"type": "text", "text": "Concept-based explanation shows the neurons\u2019 maximal activations in a specific model layer. We concentrate on the 512 neurons in the penultimate layer of a ResNet [27] on ROCT. To this end, the most stimulated neurons must be stable under perturbations. We define ${\\mathcal{T}}_{i}$ as the activation map of the $i$ -th penultimate neuron, and derive the objective analogous to Eq. (6). Empirical results in Table 1 (col ROCT) illustrate that R2ET and its variants, again, perform best against ERAttack. ", "page_idx": 7}, {"type": "text", "text": "Table 2 reports the results of models trained using Grad but evaluated by SG and IG. The following conclusions can be drawn: (1) Compared to Grad, SG and IG generally promote explanation robustness. Notably, regardless of the explanation method used, R2ET and its variants consistently achieve the highest $\\mathrm{P}@k$ . (2) Applying Grad to R2ETs usually results in greater robustness than adopting SG/IG to baselines. For example in MNIST, R2ET with Grad attains a $\\mathrm{P}@k$ of $85\\%$ , while applying SG to the baselines (except Est-H) fails to exceed a $\\mathrm{P}@k$ of $70\\%$ . (3) R2ET can generalize and transfer the robustness to explanation methods divergent from those utilized during training. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Understanding thickness and attackability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Assessing model vulnerability: critical role of thickness. As discussed in Section 5, the number of attack iterations required to reach the first successful flip between salient and non-salient features characterizes the ranking explanation robustness. In Fig. 2, each dot in subplots represents an individual sample x. The left subplot demonstrates a high correlation between the sample\u2019s thickness and the attacker\u2019s required iterations to manipulate the rankings. This high correlation signifies that samples with greater thickness demand a larger attack budget for a successful manipulation, thereby justifying thickness as a more precise metric of explanation ranking robustness. ", "page_idx": 8}, {"type": "text", "text": "To understand why R2ET does not always outperform other models, Table 3 juxtaposes $\\mathrm{P}@k$ with dataset-level thickness, defined as the average thickness of all samples across a dataset. Notably, the model exhibiting optimal explanation robustness $(\\mathrm{P}@k)$ consistently displays the greatest thickness, irrespective of the method employed, aligning with Theorem 5.2. ", "page_idx": 8}, {"type": "text", "text": "Optimal Method Selection. Thickness has proven its efficacy as an indicator for evaluating explanation ranking robustness, rendering it an apt criterion for method selection. Table 1 indicates a more straightforward way to pick a model: deploy $\\mathrm{R}2\\mathrm{E}\\mathrm{T}_{\\backslash H}$ (and $\\mathrm{R}2\\mathrm{ET}\\mathrm{-mm}_{\\setminus H})$ for datasets with a limited feature set, and R2ET (and R2ET-mm) for datasets for datasets with more features. This is because distinguishing salient and non-salient features is easier in datasets with fewer features. Conversely, maintaining such distinctions becomes more complex and potentially less efficient as the feature count increases, where reducing the Hessian norm is advantageous, as it complicates the manipulation of feature magnitude. ", "page_idx": 8}, {"type": "image", "img_path": "HYa3eu8scG/tmp/40f162d4ae04addbc71d478907a8328f67eecd1c0ab08d0b3c0bc855879d6b40.jpg", "img_caption": ["Figure 2: The number of iterations to first filp versus sample-level thickness (left) and Hessian norm (right) for R2ET on COMPAS. Each dot represents an individual sample x. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "HYa3eu8scG/tmp/5397e6cc2b62320366b3b626d5cc085e56c5454cb813c32527ce3b3c4a2e0cd2.jpg", "table_caption": ["Table 3: $\\mathrm{P}@k$ under ERAttack / model-level thickness. Refer to Table 5 for more results. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.4 Case study: saliency maps visualization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For visual evaluation, Fig. 3 displays models\u2019 saliency maps on the ideal testbeds, MNIST and CIFAR-10. On MNIST, Vanilla and WD perform poorly, where about $50\\%$ of the top 50 important pixels fell out of top positions under attack. Even worse, the salient pixels identified by Vanilla and WD fail to highlight the digit\u2019s spatial patterns (e.g., pixels covering the digits). In contrast, R2ET and R2ET-mm maintain over $90\\%$ of salient features encoding the digits\u2019 recognizable spatial patterns on the top. Similar trends are observed in CIFAR-10, exemplified by the ship. Vanilla and WD exhibit inferior $\\mathrm{P}@k$ scores, whereas R2ET and R2ET-mm achieve around $90\\%$ $\\mathrm{P}@k$ . Interestingly, all four models identify the front hull of the ship as the significant region for the predictions. However, ERAttack manipulates the explanations of Vanilla and WD to include another region, the wheelhouse, while the critical explanation regions of R2ET and R2ET-mm under attacks remain unaffected. The wheelhouse could be a reason for the ship class, but the inconsistency in explanations due to subtle perturbations raises confusion and mistrust. Fig. 5 provides more results concerning all models. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We proposed \u201cexplanation ranking thickness\u201d to measure the robustness of the top-ranked salient features to align with human cognitive capability. We theoretically disclosed the connection between thickness and a min-max optimization problem, and a global convergence rate of a constrained multi-objective attacking algorithm against the thickness. The theory leads to an efficient training algorithm $R2E T$ . On 8 datasets (vectors, images, and graphs), we compared 7 state-of-the-art baselines and 3 variants of R2ET, and consistently confirmed that explanation ranking thickness is a strong indicator of the stability of the salient features. However, R2ET is based on the surrogate loss of thickness, rather than exact thickness, which prevents it from outperforming others all the time. Besides, the theoretical analysis and discussions are based on gradient-based explanation methods. In the future, we plan to further apply R2ET to a broader spectrum of explanation methods. We also plan to investigate scenarios involving highly variable and noisy data and further adjust R2ET to ensure robustness and reliability in more diverse and challenging environments. This paper goals to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel are negative and must be specifically highlighted here. ", "page_idx": 8}, {"type": "image", "img_path": "HYa3eu8scG/tmp/4a64d9b46d4f3eb037fea89ac52e54647dba2082c83412d96458580dc95d47dd.jpg", "img_caption": ["Figure 3: Explanations of original (ori.) and perturbed (pert.) images against ERAttack from MNIST (class digit 3, $k{=}50$ ) and CIFAR-10 (class ship, $k{=}100$ ). The top $k$ salient pixels are highlighted, and darker colors indicate higher importance. $\\mathbf{P}@k$ is reported within each subplot. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This material is based upon work supported by the National Science Foundation under Grant Number 2008155. Chao Chen was supported by the National Key Research and Development Program of China (No. 2023YFB3106504), Pengcheng-China Mobile Jointly Funded Project (No. 2024ZY2B0050), and the Natural Science Foundation of China (No. 62476060). Chenghua Guo and Xi Zhang were supported by the Natural Science Foundation of China (No. 62372057). Rufeng Chen and Sihong Xie were supported in part by the National Key R&D Program of China (No. 2023YFF0725001), the Guangzhou-HKUST(GZ) Joint Funding Program (No. 2023A03J0008), and Education Bureau of Guangzhou Municipality. Xiangwen Liao was supported by the Natural Science Foundation of China (No. 62476060). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. NeurIPS, 31, 2018. [2] Shivani Agarwal. The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list. In SDM, 2011. [3] Marco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In ICML, 2019. [4] Christopher Anders, Plamen Pasliev, Ann-Kathrin Dombrowski, Klaus-Robert M\u00fcller, and Pan Kessel. Fairwashing explanations with off-manifold detergent. In ICML, 2020. [5] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and applications. Theory of Computing, 2012. [6] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 2015. [7] Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks. ICML, 2019.   \n[8] Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Smooth loss functions for deep top-k classification. ICLR, 2018.   \n[9] Coralia Cartis, Nicholas I M Gould, and Philippe L Toint. On the complexity of finding first-order critical points in constrained nonlinear optimization. Mathematical Programming, 2014.   \n[10] Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian. Neural network attributions: A causal perspective. In ICML, 2019.   \n[11] Chao Chen, Yifan Shen, Guixiang Ma, Xiangnan Kong, Srinivas Rangarajan, Xi Zhang, and Sihong Xie. Self-learn to explain siamese networks robustly. ICDM, 2021.   \n[12] Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. L-shapley and c-shapley: Efficient model interpretation for structured data. ICLR, 2019.   \n[13] Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha. Robust attribution regularization. NeurIPS, 2019.   \n[14] Zhun Deng, Linjun Zhang, Amirata Ghorbani, and James Zou. Improving adversarial robustness via unlabeled out-of-domain data. In AISTATS, 2021.   \n[15] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. ACL, 2019.   \n[16] Ann-Kathrin Dombrowski, Maximilian Alber, Christopher J Anders, Marcel Ackermann, Klaus-Robert M\u00fcller, and Pan Kessel. Explanations can be manipulated and geometry is to blame. NeurIPS, 2019.   \n[17] Ann-Kathrin Dombrowski, Christopher J Anders, Klaus-Robert M\u00fcller, and Pan Kessel. Towards robust explanations for deep neural networks. Pattern Recognition, 2021.   \n[18] Ricardo Dominguez-Olmedo, Amir H Karimi, and Bernhard Sch\u00f6lkopf. On the adversarial robustness of causal algorithmic recourse. In ICML, 2022.   \n[19] J Fliege, A I F Vaz, and L N Vicente. Complexity of gradient descent for multiobjective optimization. Optimization Methods and Software, 2019.   \n[20] J\u00f6rg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical Methods of Operations Research, 2000.   \n[21] J\u00f6rg Fliege and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on sqp techniques. SIAM Journal on Optimization, 2016.   \n[22] Peyman Gholami, Priyanka Roy, Mohana Kuppuswamy Parthasarathy, and Vasudevan Lakshminarayanan. Octid: Optical coherence tomography image database. Computers & Electrical Engineering, 2020.   \n[23] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In AAAI, 2019.   \n[24] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic conceptbased explanations. NeurIPS, 2019.   \n[25] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decisionmaking and a \u201cright to explanation\u201d. AI magazine, 2017.   \n[26] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. Ranking robustness under adversarial document manipulations. In SIGIR, 2018.   \n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[28] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. NeurIPS, 2017.   \n[29] Juyeon Heo, Sunghwan Joo, and Taesup Moon. Fooling neural network interpretations via adversarial model manipulation. NeurIPS, 2019.   \n[30] Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable model explanations for graph neural networks. TKDE, 2022.   \n[31] Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv\u00e1ri. Learning with a strong adversary. ICLR, 2016.   \n[32] Adam Ivankay, Ivan Girardi, Chiara Marchiori, and Pascal Frossard. Far: A general framework for attributional robustness. BMVC, 2020.   \n[33] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. Ir evaluation methods for retrieving highly relevant documents. In SIGIR, 2017.   \n[34] Shuichi Katsumata and Akiko Takeda. Robust cost sensitive support vector machine. In AISTATS, 2015.   \n[35] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In ICML, 2018.   \n[36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[37] Maksim Lapin, Matthias Hein, and Bernt Schiele. Top-k multiclass svm. NeurIPS, 2015.   \n[38] Maksim Lapin, Matthias Hein, and Bernt Schiele. Loss functions for top-k error: Analysis and insights. In CVPR, 2016.   \n[39] Maksim Lapin, Matthias Hein, and Bernt Schiele. Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification. PAMI, 2017.   \n[40] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.   \n[41] Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng. Are data-driven explanations robust against out-of-distribution data? In CVPR, 2023.   \n[42] Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui Wang, Hang Su, and Hui Xue. Qair: Practical query-efficient black-box attacks for image retrieval. In CVPR, 2021.   \n[43] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In ICML, 2020.   \n[44] Ao Liu, Xiaoyu Chen, Sijia Liu, Lirong Xia, and Chuang Gan. Certifiably robust interpretation via r\u00e9nyi differential privacy. Artificial Intelligence, 2022.   \n[45] Yifei Liu, Chao Chen, Yazheng Liu, Xi Zhang, and Sihong Xie. Shapley values and metaexplanations for probabilistic graphical model inference. In CIKM, 2020.   \n[46] Yang Young Lu, Wenbo Guo, Xinyu Xing, and William Stafford Noble. Dance: Enhancing saliency maps using decoys. In ICML, 2021.   \n[47] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. NeurIPS, 2017.   \n[48] Guixiang Ma, Nesreen K Ahmed, Theodore L Willke, Dipanjan Sengupta, Michael W Cole, Nicholas B Turk-Browne, and Philip S Yu. Deep graph similarity learning for brain data analysis. In CIKM, 2019.   \n[49] Guixiang Ma, Lifang He, Bokai Cao, Jiawei Zhang, Philip S Yu, and Ann B Ragin. Multi-graph clustering based on interior-node topology with applications to brain networks. In ECML PKDD, 2016.   \n[50] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ICLR, 2017.   \n[51] Piyushi Manupriya, Tarun Ram Menta, Sakethanath N Jagarlapudi, and Vineeth N Balasubramanian. Improving attribution methods by learning submodular functions. In AISTATS, 2022.   \n[52] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In CVPR, 2019.   \n[53] S\u00e9rgio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 2014.   \n[54] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In FAccT, 2020.   \n[55] Ian E Nielsen, Dimah Dera, Ghulam Rasool, Ravi P Ramachandran, and Nidhal Carla Bouaynaya. Robust explainability: A tutorial on gradient-based attribution methods for deep neural networks. IEEE Signal Processing Magazine, 2022.   \n[56] \u00c1lvaro Parafita and Jordi Vitri\u00e0. Explaining visual models by causal attribution. In ICCVW, 2019.   \n[57] Remigijus Paulavi\u02c7cius and Julius \u017dilinskas. Analysis of different norms and corresponding lipschitz constants for global optimization. Technol. Econ. Dev, 2006.   \n[58] Judea Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. WSDM, 2018.   \n[59] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 1994.   \n[60] Felix Petersen, Hilde Kuehne, Christian Borgelt, and Oliver Deussen. Differentiable top- $\\cdot\\mathbf{k}$ classification learning. In ICML, 2022.   \n[61] Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, and Hamed Pirsiavash. Consistent explanations by contrastive learning. In CVPR, 2022.   \n[62] Pearl Pu and Li Chen. Trust building with explanation interfaces. In IUI, 2006.   \n[63] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. ICML, 2020.   \n[64] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In SIGKDD, 2016.   \n[65] Laura Rieger and Lars Kai Hansen. A simple defense against adversarial attacks on heatmap explanations. WHI, 2020.   \n[66] J\u00e9r\u00f4me Rony, Eric Granger, Marco Pedersoli, and Ismail Ben Ayed. Augmented lagrangian adversarial attacks. ICCV, 2020.   \n[67] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. Adversarial training is a form of datadependent operator norm regularization. NeurIPS, 2020.   \n[68] Cynthia Rudin. The P-Norm Push: A Simple Convex Ranking Algorithm That Concentrates at the Top of the List. JMLR, 2009.   \n[69] Thomas L Saaty and Mujgan S Ozdemir. Why the magic number seven plus or minus two. Mathematical and computer modelling, 2003.   \n[70] Anindya Sarkar, Anirban Sarkar, and Vineeth N Balasubramanian. Enhanced regularizers for attributional robustness. In AAAI, 2021.   \n[71] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradientbased localization. In ICCV, 2017.   \n[72] Sofia Serrano and Noah A Smith. Is attention interpretable? ACL, 2019.   \n[73] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In ICML, 2017.   \n[74] Mayank Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, Vineeth N Balasubramanian, and Balaji Krishnamurthy. Attributional robustness training using input-gradient spatial alignment. In ECCV, 2020.   \n[75] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.   \n[76] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. ICLR workshop, 2015.   \n[77] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, 2017.   \n[78] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. ICLR, 2018.   \n[79] Zhuozhuo Tu, Jingwei Zhang, and Dacheng Tao. Theoretical analysis of adversarial learning: A minimax approach. NeurIPS, 2019.   \n[80] Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards robust and reliable algorithmic recourse. NeurIPS, 2021.   \n[81] Nicolas Usunier, Massih R Amini, and Patrick Gallinari. Generalization error bounds for classifiers trained with interdependent data. In NeurIPS, 2005.   \n[82] Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. NeurIPS, 2020.   \n[83] Fan Wang and Adams Wai-Kin Kong. Exploiting the relationship between kendall\u2019s rank correlation and cosine similarity for attribution protection. NeurIPS, 2022.   \n[84] Fan Wang and Adams Wai-Kin Kong. A practical upper bound for the worst-case attribution deviations. CVPR, 2023.   \n[85] Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, and Liang Lin. Transferable, controllable, and inconspicuous adversarial attacks on person re-identification with deep mis-ranking. In CVPR, 2020.   \n[86] Lidan Wang, Paul N Bennett, and Kevyn Collins-Thompson. Robust ranking models via risk-sensitive optimization. In SIGIR, 2012.   \n[87] Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence and robustness of adversarial training. ICML, 2021.   \n[88] Zifan Wang, Haofan Wang, Shakul Ramkumar, Matt Fredrikson, Piotr Mardziel, and Anupam Datta. Smoothed geometry for robust attribution. NeurIPS, 2020.   \n[89] Tianjun Wei, Tommy WS Chow, Jianghong Ma, and Mingbo Zhao. Expgcn: Review-aware graph convolution network for explainable recommendation. Neural Networks, 2022.   \n[90] Yuxin Wen, Shuai Li, and Kui Jia. Towards understanding the regularization of adversarial robustness on neural networks. In ICML, 2020.   \n[91] Matthew Wicker, Juyeon Heo, Luca Costabello, and Adrian Weller. Robust explanation constraints for neural networks. ICLR, 2023.   \n[92] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. ICLR, 2020.   \n[93] Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks and defenses in images, graphs and text: A review. Int. J. Autom. Comput., 2020.   \n[94] Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. JMLR, 2009.   \n[95] Forest Yang and Sanmi Koyejo. On the consistency of top-k surrogate losses. In ICML, 2020.   \n[96] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. NeurIPS, 2020.   \n[97] Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez, Kannan Ramchandran, and Michael W Mahoney. Boundary thickness and robustness in learning models. NeurIPS, 2020.   \n[98] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.   \n[99] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In ICML, 2021.   \n[100] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014.   \n[101] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.   \n[102] Tong Zhang. Covering Number Bounds of Certain Regularized Linear Function Classes. JMLR, 2002.   \n[103] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016.   \n[104] Mo Zhou, Zhenxing Niu, Le Wang, Qilin Zhang, and Gang Hua. Adversarial ranking attack and defense. In ECCV, 2020.   \n[105] Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning Zheng, and Gang Hua. Practical relative order attack in deep ranking. In ICCV, 2021.   \n[106] Yun Zhou and W Bruce Croft. Ranking robustness: a novel framework to predict query performance. In CIKM, 2006. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix A provides theoretical proofs for the propositions and lemmas. Appendix B reports experimental settings and results. Appendix C shows relevant works concerning explanation robustness, ranking robustness, and top- ${\\cdot k}$ intersection. ", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proofs concerning Prop. 4.4, 4.5, and 4.6 are in A.1, A.2 and A.3, respectively. As the supplementary of Sec. 5, theoretical analyses on numerical and statistical robustness are in A.4 and A.5, respectively. ", "page_idx": 15}, {"type": "text", "text": "A.1 Bounds of Ranking Explanation Thickness ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1.1 Bounds of Pairwise Thickness ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We denote $\\mathbf{x}$ as the target sample, $\\lVert\\delta\\rVert_{2}\\leq\\epsilon$ as the perturbation, $\\mathbf{x}^{\\prime}=\\mathbf{x}+\\delta$ as the perturbed input, and ${\\bf x}(t)\\,=\\,(1-t){\\bf x}+t{\\bf x}^{\\prime},t\\,\\in\\,[0,1]$ . Here, $\\mathcal{T}(\\mathbf{x})$ is not specifically defined as $\\nabla_{\\mathbf{x}}f(\\mathbf{x})$ , but an arbitrary explanation method. ", "page_idx": 15}, {"type": "text", "text": ",o fsoitri osno mAe. wexhpelrae $L$ p, stchheit zr amnkoidnegl $f(\\mathbf{x})$ $\\begin{array}{r}{L\\;\\geq\\;\\frac{\\|\\mathbf x-\\mathbf x^{*}\\|_{2}\\cdot\\operatorname*{max}_{i}L_{i}}{2}}\\end{array}$ $\\begin{array}{r}{\\mathbf{x}^{*}\\,=\\,\\arg\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{2}(\\mathbf{x},\\epsilon)}\\frac{||\\mathcal{Z}(\\mathbf{x})-\\mathcal{Z}(\\mathbf{x}^{\\prime})||_{2}}{||\\mathbf{x}-\\mathbf{x}^{\\prime}||}}\\end{array}$ \u2225I(x)\u2212I(\u2032x\u2032)\u22252 explanation thickness for the $(i,j)$ feature pair of a target x is bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\iota(\\mathbf{x},i,j)-\\epsilon\\ast\\frac{1}{2}\\|\\nabla_{\\mathbf{x}}\\mathcal{Z}_{i}(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\mathcal{Z}_{j}(\\mathbf{x})\\|_{2}\\leq\\mathbb{E}_{\\mathbf{x}^{\\prime}}\\left[\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t\\right]\\leq h(\\mathbf{x},i,j)+\\epsilon\\ast(L_{i}+L_{j}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $H_{i}(\\mathbf{x})$ is the $i$ -th column of Hessian matrix of $f$ with respect to the input $\\mathbf{x}_{\\mathrm{:}}$ , and $L_{i}\\,=$ $\\mathrm{max}_{\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{2}(\\mathbf{x},\\epsilon)}\\,\\|\\nabla_{\\mathbf{x}}\\mathcal{T}_{i}(\\mathbf{x}^{\\prime})\\|_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "Lower bound. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We start from the definition of ranking thickness between $(i,j)$ -th features of $\\mathbf{x}$ in Eq. (2). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{0}^{1}h({\\mathbf x}(t),i,j)d t}\\\\ &{=\\int_{0}^{1}Z_{i}({\\mathbf x}+t\\delta)-Z_{j}({\\mathbf x}+t\\delta)d t}\\\\ &{\\approx\\int_{0}^{1}Z_{i}({\\mathbf x})+t\\delta^{\\top}\\nabla_{\\mathbf x}Z_{i}({\\mathbf x})-Z_{j}({\\mathbf x})-t\\delta^{\\top}\\nabla_{\\mathbf x}Z_{j}({\\mathbf x})d t}\\\\ &{=\\!Z_{i}({\\mathbf x})-Z_{j}({\\mathbf x})+\\frac{\\delta^{\\top}}{2}\\nabla_{\\mathbf x}\\bar{Z}_{i}({\\mathbf x})-\\frac{\\delta^{\\top}}{2}\\nabla_{\\mathbf x}\\bar{Z}_{j}({\\mathbf x})}\\\\ &{\\geq\\!Z_{i}({\\mathbf x})-Z_{j}({\\mathbf x})-{\\epsilon}\\ast\\frac{1}{2}\\|\\nabla_{\\mathbf x}\\bar{Z}_{i}({\\mathbf x})-\\nabla_{\\mathbf x}\\bar{Z}_{j}({\\mathbf x})\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We approximate the gradient at intermediate point $\\mathbf{x}+t\\delta$ by the Taylor Expansion on line-3. Then the minimum is found with \u03b4\u2217= arg min\u2225\u03b4\u2225\u2264\u03f512\u03b4\u22a4(\u2207xIi(x) \u2212\u2207xIj(x)) = \u2212\u03f5 \u2225\u2207\u2207xxIIii((xx))\u2212\u2212\u2207\u2207xxIIjj((xx))\u2225 on line-5. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Upper bound. We introduce two lemmas from [57] and [88] before the proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. If a function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is $L$ -locally Lipschitz within $B_{p}(\\mathbf{x},\\epsilon)$ , such that $|f(\\mathbf{x})-$ $f(\\mathbf{x}^{\\prime})|\\leq L\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{p}$ , $\\forall\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{p}(\\mathbf{x},\\epsilon)=\\{\\mathbf{x}^{\\prime}:\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\|_{p}\\leq\\epsilon\\},$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\nL=\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{p}(\\mathbf{x},\\epsilon)}\\|\\nabla_{\\mathbf{x}^{\\prime}}f(\\mathbf{x}^{\\prime})\\|_{q},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\textstyle{\\frac{1}{p}}+{\\frac{1}{q}}=1,1\\leq p,q\\leq\\infty$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. If a function $f(\\mathbf{x})$ is $L$ -locally Lipschitz continuous in $B_{2}(\\mathbf{x},\\epsilon)$ , then $\\mathcal{T}(\\mathbf{x})$ is $K$ -locally Lipschitz as well, where $\\begin{array}{r}{K\\leq\\frac{2L}{\\|\\mathbf x-\\mathbf x^{*}\\|}}\\end{array}$ and $\\begin{array}{r}{\\mathbf{x}^{*}=\\arg\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{2}(\\mathbf{x},\\epsilon)}\\frac{\\|\\mathcal{Z}(\\mathbf{x})-\\mathcal{Z}(\\mathbf{x}^{\\prime})\\|_{2}}{\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Given an $L$ -locally Lipschitz $f(\\mathbf{x})$ , Lemma A.3 further indicates that $\\mathcal{T}_{i}(\\mathbf{x}):\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is locally Lipschitz as well. By adopting $p=q=2$ on Lemma A.2, the Lipschitz constant is specified by $\\begin{array}{r}{L_{i}=\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{2}(\\mathbf{x},\\epsilon)}\\|\\nabla_{\\mathbf{x}^{\\prime}}\\mathcal{T}_{i}(\\mathbf{x}^{\\prime})\\|_{2}}\\end{array}$ . Formally, ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\mathcal{Z}_{i}(\\mathbf{x})-\\mathcal{Z}_{i}(\\mathbf{x}^{\\prime})|\\leq\\|\\mathcal{Z}(\\mathbf{x})-\\mathcal{Z}(\\mathbf{x}^{\\prime})\\|_{2}\\leq L_{i}\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t}\\\\ &{=h(\\mathbf{x}_{0}^{\\prime},i,j)}\\\\ &{=\\!(\\mathbb{Z}_{i}(\\mathbf{x}_{0}^{\\prime})-\\mathbb{Z}_{i}(\\mathbf{x}))-(\\mathbb{Z}_{j}(\\mathbf{x}_{0}^{\\prime})-\\mathbb{Z}_{j}(\\mathbf{x}))+(\\mathbb{Z}_{i}(\\mathbf{x})-\\mathbb{Z}_{j}(\\mathbf{x}))}\\\\ &{\\le\\!|\\mathbb{Z}_{i}(\\mathbf{x}_{0}^{\\prime})-\\mathbb{Z}_{i}(\\mathbf{x})|+|\\mathbb{Z}_{j}(\\mathbf{x}_{0}^{\\prime})-\\mathbb{Z}_{j}(\\mathbf{x})|+(\\mathbb{Z}_{i}(\\mathbf{x})-\\mathbb{Z}_{j}(\\mathbf{x}))}\\\\ &{\\le\\!L_{i}|\\mathbf{\\|x}_{0}^{\\prime}-\\mathbf{x}|_{2}+L_{j}|\\mathbf{\\|x}_{0}^{\\prime}-\\mathbf{x}|_{2}+(\\mathbb{Z}_{i}(\\mathbf{x})-\\mathbb{Z}_{j}(\\mathbf{x}))}\\\\ &{\\le\\!\\epsilon*(L_{i}+L_{j})+(\\mathbb{Z}_{i}(\\mathbf{x})-\\mathbb{Z}_{j}(\\mathbf{x})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Based on first mean value theorem, there exists $\\ensuremath{\\mathbf{x}}_{0}^{\\prime}$ within the line segment $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ such that $\\begin{array}{r}{h(\\mathbf{x}_{0}^{\\prime},i,j)=\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t}\\end{array}$ on line-2. The Lipschitz constant $L$ of $f$ satisfies $\\begin{array}{r}{L\\geq\\frac{\\|\\mathbf{x}-\\mathbf{x}^{*}\\|}{2}\\ast}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{i}L_{i}=\\frac{\\Vert\\mathbf{x}-\\mathbf{x}^{*}\\Vert}{2}*\\operatorname*{max}_{i}\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in B_{2}(\\mathbf{x},\\epsilon)}\\Vert\\nabla_{\\mathbf{x}}\\mathcal{Z}_{i}(\\mathbf{x}^{\\prime})\\Vert_{2}.}\\end{array}$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.1.2 Instantiation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Notice that the inequality in Eq. (13) holds for arbitrary explanation methods. Here, we consider different explanation methods (Grad, Grad $\\times$ Inp, SG, and IG) to specify $\\mathcal{T}(\\mathbf{x})$ and corresponding bounds. We leave DeepLIFT [73] and $L R P$ [6] to further study, and we do not consider Grad-CAM [71] and Guided Backpropagation [76] since they are designed only for specific networks. ", "page_idx": 16}, {"type": "text", "text": "Grad. When adopting Grad as explanation method, where $\\mathcal{T}(\\mathbf{x})=\\nabla f(\\mathbf{x})$ and $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x})=H(\\mathbf{x})$ , it recovers Prop. 4.4. ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\nabla f(\\mathbf{x}))_{i}-(\\nabla f(\\mathbf{x}))_{j}-\\epsilon\\ast{\\frac{1}{2}}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}\\leq\\Theta(f,\\mathbf{x},\\mathcal{D},i,j)\\leq h(\\mathbf{x},i,j)+\\epsilon\\ast(L_{i}+L_{j}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $H(\\mathbf{x})$ is the Hessian matrix, and specifically $\\begin{array}{r}{L_{i}=\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in B_{2}(\\mathbf{x},\\epsilon)}\\|H_{i}(\\mathbf{x}^{\\prime})\\|_{2}.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "SmoothGrad. When adopting SmoothGrad, where $\\begin{array}{r}{\\mathcal{Z}(\\mathbf{x})\\;=\\;\\frac{1}{M}\\sum_{m}\\nabla f(\\mathbf{x}+\\beta_{m})}\\end{array}$ and $\\beta_{m}\\,\\sim$ $\\mathcal{N}(0,\\sigma^{2}I)$ , we have $\\begin{array}{r}{\\nabla_{\\mathbf{x}}\\mathcal{Z}(\\mathbf{x})=\\frac{1}{M}\\sum_{m}H(\\mathbf{x}+\\beta_{m})}\\end{array}$ . We derive the lower bounds analogy to the derivations on Eq. (13): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t}\\\\ &{\\displaystyle=\\cfrac{1}{M}\\sum_{m}\\int_{0}^{1}(\\nabla f(\\mathbf{x}+t\\delta+\\beta_{m}))_{i}-(\\nabla f(\\mathbf{x}+t\\delta+\\beta_{m}))_{j}d t}\\\\ &{\\displaystyle\\geq\\cfrac{1}{M}\\sum_{m}\\left((\\nabla f(\\mathbf{x}))_{i}-(\\nabla f(\\mathbf{x}))_{j}-\\epsilon\\ast\\frac{1}{2}\\ast\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}+\\beta_{m}^{T}(H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x}))\\right)}\\\\ &{\\displaystyle=(\\nabla f(\\mathbf{x}))_{i}-(\\nabla f(\\mathbf{x}))_{j}-\\epsilon\\ast\\frac{1}{2}\\ast\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}+(\\frac{1}{M}\\sum_{m}\\beta_{m})^{T}\\left(H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note: when $M$ is large enough, $\\begin{array}{r}{\\frac{1}{M}\\sum_{m}\\beta_{m}\\rightarrow\\mathbf{0}}\\end{array}$ since $\\beta_{m}$ is drawn from $\\mathcal{N}(0,\\sigma^{2}I)$ . ", "page_idx": 16}, {"type": "text", "text": "$\\bf G r a d\\,\\times\\,I n p$ . When adopting $\\mathrm{Grad}~\\times~\\mathrm{Inp}$ , where ${\\mathcal{Z}}({\\mathbf{x}})\\;=\\;\\nabla f({\\mathbf{x}})\\odot{\\mathbf{x}}$ , we have $\\nabla_{\\mathbf{x}}\\mathcal{I}(\\mathbf{x})\\;=$ $\\mathrm{diag}((\\nabla f)_{1},\\bar{\\dots}...,(\\nabla f)_{n})+H(\\mathbf{x})$ . The lower bounds derived from Eq. (13) can be further specified: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{Z}_{i}(\\mathbf{x})-{Z}_{j}(\\mathbf{x})-\\epsilon\\frac{1}{2}\\|\\nabla_{\\mathbf{x}}{Z}_{i}(\\mathbf{x})-\\nabla_{\\mathbf{x}}{Z}_{j}(\\mathbf{x})\\|_{2}}\\\\ &{={Z}_{i}(\\mathbf{x})-{Z}_{j}(\\mathbf{x})-\\epsilon\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})+[0,\\dots,(\\nabla f)_{i},\\dots,-(\\nabla f)_{j},\\dots,0)\\|_{2}}\\\\ &{\\le{Z}_{i}(\\mathbf{x})-{Z}_{j}(\\mathbf{x})-\\epsilon\\frac{1}{2}\\frac{1}{2}\\big(\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}+\\|[0,\\dots,(\\nabla f)_{i},\\dots,-(\\nabla f)_{j},\\dots,0)\\|_{2}\\big)}\\\\ &{={Z}_{i}(\\mathbf{x})-{Z}_{j}(\\mathbf{x})-\\epsilon\\frac{1}{2}\\sqrt{(\\nabla f)_{i}^{2}+(\\nabla f)_{j}^{2}}-\\epsilon\\frac{1}{2}\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}}\\\\ &{\\ge(\\nabla f(\\mathbf{x}))_{i}x_{i}-(\\nabla f(\\mathbf{x}))_{j}x_{j}-\\frac{\\epsilon}{2}(\\nabla f(\\mathbf{x}))_{i}-\\frac{\\epsilon}{2}(\\nabla f(\\mathbf{x}))_{j}-\\epsilon\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}}\\\\ &{=(\\nabla f(\\mathbf{x}))_{i}\\left(x_{i}-\\frac{\\epsilon}{2}\\right)-(\\nabla f(\\mathbf{x}))_{j}\\left(x_{j}+\\frac{\\epsilon}{2}\\right)-\\epsilon\\ast\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}}\\\\ &{\\approx\\!(\\nabla f(\\mathbf{x}))_{i}x_{i}-(\\nabla f(\\mathbf{x}))_{j}x_{j}-\\epsilon\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note: $x_{i}$ is the $i$ -th feature of the input $\\mathbf{x}$ , and $\\epsilon\\ll x_{i}$ since the perturbation is supposed to be neglectable. ", "page_idx": 17}, {"type": "text", "text": "Integrated Grad. When adopting Integrated Grad, $\\begin{array}{r}{\\mathcal{Z}(\\mathbf{x})=(\\mathbf{x}-\\mathbf{x}^{0})\\odot\\int_{\\alpha=0}^{1}\\nabla f(\\mathbf{x}^{0}+\\alpha(\\mathbf{x}-\\mathbf{x}^{0}))d\\alpha}\\end{array}$ . By setting $\\mathbf{x}^{0}=\\mathbf{0}$ , we have $\\begin{array}{r}{\\mathcal{Z}(\\mathbf{x})=\\mathbf{x}\\odot\\int_{\\alpha=0}^{1}\\nabla f(\\alpha\\mathbf{x})d\\alpha}\\end{array}$ . Similar to the ${\\mathrm{Grad}}\\times{\\mathrm{Inp}}$ case, the lower bound of IG can be derived: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{Z}_{i}(\\mathbf{x})-\\mathcal{Z}_{j}(\\mathbf{x})-\\epsilon\\ast\\frac{1}{2}\\|\\nabla_{\\mathbf{x}}\\mathcal{Z}_{i}(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\mathcal{Z}_{j}(\\mathbf{x})\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\displaystyle\\int_{\\alpha=0}^{1}(\\nabla f(\\alpha\\mathbf{x}))_{i}x_{i}-(\\nabla f(\\alpha\\mathbf{x}))_{j}x_{j}-\\epsilon\\ast\\frac{1}{2}\\|H_{i}(\\alpha\\mathbf{x})-H_{j}(\\alpha\\mathbf{x})\\|_{2}d\\alpha}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{2}\\left((\\nabla f(\\mathbf{x}))_{i}x_{i}-(\\nabla f(\\mathbf{x}))_{j}x_{j}-\\epsilon\\ast\\frac{1}{2}\\|H_{i}(\\mathbf{x})-H_{j}(\\mathbf{x})\\|_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.1.3 Generalization to Top- $k$ Thickness ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Since the inequalities in Eq. (13) and (15) hold for any choice of $\\mathbf{x}^{\\prime}\\in B_{2}(\\mathbf{x},\\epsilon)$ with a specific $\\epsilon$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\iota(\\mathbf{x},i,j)-\\epsilon\\ast\\frac{1}{2}\\|\\nabla_{\\mathbf{x}}\\mathcal{I}_{i}(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\mathcal{I}_{j}(\\mathbf{x})\\|_{2}\\leq\\mathbb{E}_{\\mathbf{x}^{\\prime}}\\left[\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t\\right]\\leq h(\\mathbf{x},i,j)+\\epsilon\\ast(L_{i}+L_{j}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, the bounds of the top- $k$ ranking thickness hold for any choice of comparison pairs: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i,j}\\left[h(\\mathbf{x},i,j)-\\epsilon\\ast\\frac{1}{2}\\|\\nabla_{\\mathbf{x}}\\mathcal{L}_{i}(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\mathcal{L}_{j}(\\mathbf{x})\\|_{2}\\right]\\leq\\mathbb{E}_{\\mathbf{x}^{\\prime}}\\left[\\sum_{i,j}\\int_{0}^{1}h(\\mathbf{x}(t),i,j)d t\\right]\\leq\\sum_{i,j}\\left[h(\\mathbf{x},i,j)+h(\\mathbf{x}(t),i,j)\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Instantiation for the top- $k$ thickness can be derived similarly to those in A.1.2. ", "page_idx": 17}, {"type": "text", "text": "A.2 Connection between R2ET and Certified Robustness ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proposition A.4. For all $\\delta$ with $\\begin{array}{r}{\\|\\delta\\|_{2}\\leq\\operatorname*{min}_{\\{i,j\\}}\\frac{h(\\mathbf{x},i,j)}{\\operatorname*{max}_{\\mathbf{x}^{\\prime}}\\|H_{i}(\\mathbf{x}^{\\prime})-H_{j}(\\mathbf{x}^{\\prime})\\|_{2}}}\\end{array}$ , it holds $\\mathbb{1}[h(\\mathbf{x},i,j)>0]$ $=\\mathbb{1}[h(\\mathbf{x}+\\delta,i,j)>0]$ for all $(i,j)$ pair, that is all the feature rankings do not change. ", "page_idx": 17}, {"type": "text", "text": "Proof. The proof is adapted from the work [28]. We consider the minimal budget required to achieve $h(\\mathbf{x}+\\delta,i,j)<0$ , e.g., $\\begin{array}{r}{\\mathcal{Z}_{i}(\\mathbf{x}+\\delta)-\\mathcal{Z}_{j}(\\mathbf{x}+\\delta)<0}\\end{array}$ , given $h(\\mathbf{x},i,j)>0$ . The same proof can be done for $h(\\mathbf{x},i,j)<0$ . Based on the calculus, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal{T}_{i}(\\mathbf{x}+\\delta)=\\mathcal{T}_{i}(\\mathbf{x})+\\int_{0}^{1}\\langle H_{i}(\\mathbf{x}+t\\delta),\\delta\\rangle d t,}}\\\\ {\\displaystyle{\\mathcal{T}_{j}(\\mathbf{x}+\\delta)=\\mathcal{T}_{j}(\\mathbf{x})+\\int_{0}^{1}\\langle H_{j}(\\mathbf{x}+t\\delta),\\delta\\rangle d t.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To achieve $\\begin{array}{r}{\\mathcal{T}_{i}(\\mathbf{x}+\\delta)-\\mathcal{T}_{j}(\\mathbf{x}+\\delta)<0}\\end{array}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bigg(\\mathcal{Z}_{i}(\\mathbf{x})+\\int_{0}^{1}\\langle H_{i}(\\mathbf{x}+t\\delta),\\delta\\rangle d t\\bigg)-\\bigg(\\mathcal{Z}_{j}(\\mathbf{x})+\\int_{0}^{1}\\langle H_{j}(\\mathbf{x}+t\\delta),\\delta\\rangle d t\\bigg)<0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{Z}}_{i}({\\bf x})-{\\mathcal{Z}}_{j}({\\bf x})<\\int_{0}^{1}\\langle H_{j}({\\bf x}+t\\delta),\\delta\\rangle d t-\\int_{0}^{1}\\langle H_{i}({\\bf x}+t\\delta),\\delta\\rangle d t.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Based on the H\u00f6lder\u2019s inequality, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{i}(\\mathbf{x})-\\mathcal{Z}_{j}(\\mathbf{x})<\\|\\delta\\|_{2}\\int_{0}^{1}\\|H_{i}(\\mathbf{x}+t\\delta)-H_{j}(\\mathbf{x}+t\\delta)\\|_{2}d t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the minimal budget required to flip the ranking between the feature pair $(i,j)$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\delta\\|_{2}>\\frac{\\mathcal{Z}_{i}(\\mathbf{x})-\\mathcal{Z}_{j}(\\mathbf{x})}{\\int_{0}^{1}\\|H_{i}(\\mathbf{x}+t\\delta)-H_{j}(\\mathbf{x}+t\\delta)\\|_{2}d t},}\\\\ {>\\frac{\\mathcal{Z}_{i}(\\mathbf{x})-\\mathcal{Z}_{j}(\\mathbf{x})}{\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in\\mathcal{B}_{2}(\\mathbf{x}^{\\prime},\\epsilon)}\\|H_{i}(\\mathbf{x}^{\\prime})-H_{j}(\\mathbf{x}^{\\prime})\\|_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the minimal budget required to flip any ranking is min{i,j}maxx\u2032 \u2225Hhi((xx,\u2032i),j\u2212)Hj(x\u2032)\u22252 . ", "page_idx": 18}, {"type": "text", "text": "A.3 Connection between AT and R2ET ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following Sec. 3, given a model $f:\\mathbb{R}^{n}\\rightarrow[0,1]^{C}$ and an input x, $\\boldsymbol{\\mathcal{T}}(\\mathbf{x},c;f)=\\boldsymbol{\\nabla}_{\\mathbf{x}}f_{c}(\\mathbf{x})$ is the explanation and ${\\mathcal{T}}_{i}(\\mathbf{x})$ is the score for the $i$ -th feature. We assume that $\\mathcal{T}(\\mathbf{x})$ is sorted and top- $k$ ones are salient features. The proof is based on the prior work [94]. ", "page_idx": 18}, {"type": "text", "text": "The objective of Adversarial Training (AT) for training a model by a min-max game in Sec. 4.2 is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}{\\displaystyle\\operatorname*{max}_{(\\delta_{1,k+1},\\ldots,\\delta_{k,n})\\in\\mathcal{M}}{\\mathcal{L}_{c l s}}-\\sum_{i=1}^{k}\\sum_{j=k+1}^{n}h(\\mathbf{x}+\\delta_{i,j},i,j)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We explicitly show the weight of each $\\mathcal{T}_{i}$ , which is $\\boldsymbol{l}_{i}=(\\boldsymbol{n}-\\boldsymbol{k})$ if $i\\leq k$ , and $l_{i}=-k$ , otherwise, and rewrite AT\u2019s goal: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\operatorname*{max}_{(\\delta_{1},\\dots,\\delta_{n})\\in\\mathcal{M}}\\mathcal{L}_{c l s}-\\sum_{i=1}^{n}l_{i}\\mathcal{T}_{i}(\\mathbf{x}+\\delta_{i}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "or ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{w}}\\operatorname*{min}_{(\\delta_{1},\\dots,\\delta_{n})\\in\\mathcal{M}}-\\mathcal{L}_{c l s}+\\sum_{i=1}^{n}l_{i}\\mathcal{Z}_{i}(\\mathbf{x}+\\delta_{i}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\delta_{i}$ is specific to the $i$ -th feature of input $\\mathbf{x}$ . ", "page_idx": 18}, {"type": "text", "text": "We will prove the equivalence between R2ET in Eq. (6) and AT in Eq. (21) by proving that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\delta_{1},\\ldots,\\delta_{n})\\in\\mathcal{M}}\\sum_{i=1}^{n}l_{i}\\mathcal{T}_{i}(\\mathbf{x}+\\delta_{i})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nu=\\sum_{i=1}^{n}l_{i}\\mathcal{T}_{i}(\\mathbf{x})-\\epsilon\\operatorname*{max}_{t}l_{t}\\|H_{t}(\\mathbf{x})\\|_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Given an $\\epsilon$ norm ball $\\mathcal{M}_{0}\\stackrel{\\mathrm{def}}{=}\\left\\{\\delta\\in\\mathbb{R}^{n}:\\lVert\\delta\\rVert\\leq\\epsilon\\right\\}$ , we consider a perturbation set $\\mathcal{M}$ where perturbations on each feature are independent, but the aggregation of perturbations is controlled. Formally, $\\mathcal{M}$ satisfies $\\mathcal{M}^{-}\\subseteq\\mathcal{M}\\subseteq\\bar{\\mathcal{M}}^{+}$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}^{-}\\stackrel{\\mathrm{def}}{=}\\bigcup_{i=1}^{n}\\mathcal{M}_{i}^{-},\\quad\\mathrm{where~}\\mathcal{M}_{i}^{-}\\stackrel{\\mathrm{def}}{=}\\{\\big(\\delta_{1},\\ldots,\\delta_{n}\\big)\\big|\\delta_{i}\\in\\mathcal{M}_{0};\\delta_{t\\ne i}=\\mathbf{0}\\};}\\\\ &{\\mathcal{M}^{+}\\stackrel{\\mathrm{def}}{=}\\big\\{\\big(\\alpha_{1}\\delta_{1},\\ldots,\\alpha_{n}\\delta_{n}\\big)\\big|\\displaystyle\\sum_{i=1}^{n}\\alpha_{i}=1;\\alpha_{i}\\geq0,\\delta_{i}\\in\\mathcal{M}_{0},i=1,\\ldots,n\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\mathcal{M}^{-}\\subseteq\\mathcal{M}\\subseteq\\mathcal{M}^{+}$ naturally implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\delta_{1},\\ldots,\\delta_{n})\\in{\\cal M}^{+}}\\sum_{i=1}^{n}l_{i}\\mathbb{Z}_{i}({\\bf x}+\\delta_{i})\\le\\operatorname*{min}_{(\\delta_{1},\\ldots,\\delta_{n})\\in{\\cal M}}\\sum_{i=1}^{n}l_{i}\\mathbb{Z}_{i}({\\bf x}+\\delta_{i})\\le\\operatorname*{min}_{(\\delta_{1},\\ldots,\\delta_{n})\\in{\\cal M}^{-}}\\sum_{i=1}^{n}l_{i}\\mathbb{Z}_{i}({\\bf x}+\\delta_{i}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will prove that $\\nu$ is not smaller than the rightmost term and not larger than the leftmost term in Eq. (26). ", "page_idx": 19}, {"type": "text", "text": "To prove $\\begin{array}{r}{\\nu\\geq\\operatorname*{min}_{(\\delta_{1},\\ldots,\\delta_{n})\\in\\mathcal{M}^{-}}\\sum_{i=1}^{n}l_{i}\\mathcal{T}_{i}(\\mathbf{x}+\\delta_{i}).}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{(\\delta_{1},\\dots,\\delta_{n})\\in\\mathcal{A}}{\\mathrm{min}}\\sum_{i=1}^{n}l_{i}\\overline{{L_{i}(\\mathbf{x}+\\delta_{i})}}}\\\\ &{\\le\\displaystyle\\sum_{(\\delta_{1},\\dots,\\delta_{n})\\in\\mathcal{A}}\\sum_{i=1}^{n}l_{i}\\overline{{L_{i}(\\mathbf{x}+\\delta_{i})}}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}l_{i}L_{i}(\\mathbf{x})+\\sum_{i=1}^{n}\\frac{\\operatorname*{min}}{(\\delta_{1},\\dots,\\delta_{n})\\in\\mathcal{A}}\\ l_{i}H_{i}(\\mathbf{x})^{\\top}\\delta_{i}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}l_{i}\\overline{{L_{i}(\\mathbf{x})+\\sum_{i\\in\\{1,\\dots,n\\}}}}\\ \\underset{\\delta_{i}\\in\\mathcal{A}}{\\mathrm{min}}\\ l_{i}H_{i}(\\mathbf{x})^{\\top}\\delta_{i}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}l_{i}\\overline{{L_{i}(\\mathbf{x})-\\epsilon_{\\mathrm{~t~max~}}}}\\ l_{i}H_{i}(\\mathbf{x})\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The inequality on line-2 holds since $\\mathcal{M}_{i}^{-}\\subseteq\\mathcal{M}^{-}$ . The equality on line-4 holds due to the definition of $\\mathbf{\\mathcal{M}}_{i}^{-}$ , where only one $\\delta_{t}$ of $(\\delta_{1},\\ldots,\\delta_{n})$ is from $\\mathcal{M}_{\\mathrm{0}}$ and the rest are all zeros. The equality on line-5 holds by picking \u03b4t = \u2212\u03f5 \u2225HHtt((xx))\u2225. ", "page_idx": 19}, {"type": "text", "text": "To prove $\\begin{array}{r}{\\nu\\leq\\operatorname*{min}_{(\\delta_{1},\\ldots,\\delta_{n})\\in\\mathcal{M}^{+}}\\sum_{i=1}^{n}l_{i}\\mathcal{Z}_{i}(\\mathbf{x}+\\delta_{i}).}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{(b_{1},\\ldots,\\delta_{n})\\in{\\cal M}+\\sum_{i=1}^{n}\\atop1\\leq i_{1}\\leq i_{2}\\leq n_{i}}{\\operatorname*{min}}\\sum_{i,\\ell_{i}}^{n}({\\bf x}+\\delta_{i})}\\\\ &{=\\underset{\\sum_{i_{1}\\in\\cal A}=1,\\alpha_{i}\\geq0,\\hat{\\lambda}_{i}\\in\\cal A}{\\operatorname*{min}}\\underset{i_{1}\\leq i_{1}\\leq n_{i}}{\\sum}l_{i_{1}}\\alpha_{i}Z_{i}({\\bf x}+\\hat{\\delta}_{i})}\\\\ &{=\\underset{\\sum_{i=1}^{n}\\cal L_{i}}{\\sum}l_{i_{2}}({\\bf x})+\\underset{\\sum_{i_{1}\\in\\cal A}-1,\\alpha_{i}\\geq0,i_{2}}{\\operatorname*{min}}\\underset{i_{1}\\leq i_{1}}{\\sum}\\alpha_{i_{1}\\bmod{i}}\\;l_{i}H_{i}({\\bf x})^{\\top}\\delta_{i}}\\\\ &{=\\underset{{i_{1}\\in\\cal A}}{\\sum}l_{i_{1}}({\\bf x})+\\underset{t\\in\\{1,\\ldots,n\\}}{\\operatorname*{min}}\\;\\underset{{\\bar{\\lambda}}_{i}\\in{\\cal A}}{\\operatorname*{min}}\\;l_{i}H_{i}({\\bf x})^{\\top}\\delta_{t}}\\\\ &{=\\underset{\\sum_{i_{1}\\in\\cal A}}{\\sum}l_{i_{1}}({\\bf x})-\\epsilon\\;\\underset{t\\in\\{1,\\ldots,n\\}}{\\operatorname*{max}}l_{t}\\Vert H_{i}({\\bf x})\\Vert_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.4 A multi-objective attacking algorithm and its analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present an algorithm that will terminate in finite iterations, and flip the first pair of salient and non-salient features, or claim a failed attack. The algorithm is based on a trust-region method designed for single non-convex but smooth objective with nonlinear and non-convex equality constraints [9]. ", "page_idx": 19}, {"type": "text", "text": "Let the output vector $f(\\mathbf{x})=[f_{1}(\\mathbf{x}),\\dots,f_{C}(\\mathbf{x})]$ . Given $n$ features and top- $k$ salient features, there are $m=k\\times(n-k)$ objectives that can be indexed by subscripts $\\ell$ so that the objective vector becomes $[h_{1}(\\mathbf{x}),\\allowbreak\\dots,h_{\\ell}(\\mathbf{x}),\\allowbreak\\dots,h_{m}(\\mathbf{x})]$ . Let $\\bigstar|||\\bigstar\\bigstar$ be a convex norm with Lipschitz constant 1. ", "page_idx": 19}, {"type": "text", "text": "Input: initial input $\\mathbf{x}$ , target model $f$ , current explanation $\\mathcal{T}(\\mathbf{x})$ , tolerance $\\epsilon>0$ , trust-region   \nmethod parameters $1>\\eta>0$ and $1>\\gamma>0$ .   \nSet $k=1$ , $\\mathbf{x}^{(p)}=\\mathbf{x}$ , $t_{\\ell}^{(p)}=\\|f(\\mathbf{x}^{(p)})\\|+h_{\\ell}(\\mathbf{x}^{(p)})-\\epsilon^{(p)}$ for each objective $h_{\\ell}$ .   \nwhile min1\u2264\u2113\u2264m $\\chi_{\\ell}(\\mathbf{x}^{(p)},t^{(p)})\\geq\\epsilon\\,\\mathbf{d}$ o Solve TR- $\\cdot{\\bf M O O}({\\bf x}^{(p)},\\Delta^{(p)})$ to obtain a joint descent direction $\\mathbf{d}^{(p)}$ for all linearized merit functions $l_{\\phi_{\\ell}}$ . \u03d5\u2113(x(p),t((pp)))\u2212\u03d5\u2113(x(p)+d((pp)),t(p)) for each \u2113= 1, . . . , m. ${\\bf i f}\\operatorname*{min}_{\\ell}\\rho_{\\ell}^{(p)}>\\eta$ then $\\mathbf{x}^{(p+1)}=\\mathbf{x}^{(p)}+\\mathbf{d}^{(p)}$ . $\\Delta^{(p+1)}=\\Delta^{(p)}.\\;\\;\\;\\;$ if $h_{\\ell}(\\mathbf{x}^{(p)})\\geq t_{\\ell}^{(p)}$ then el $\\begin{array}{r l}&{t_{\\ell}^{(p+1)}=\\overline{{t_{\\ell}^{(p)}}}-\\phi_{\\ell}(\\mathbf{x}^{(p)},t_{\\ell}^{(p)})+\\phi_{\\ell}(\\mathbf{x}^{(p+1)},t_{\\ell}^{(p)}).}\\\\ &{s\\mathbf{e}}\\\\ &{t_{\\ell\\ldots}^{(p+1)}=2h_{\\ell}(\\mathbf{x}^{(p+1)})-t^{(p)}-\\phi_{\\ell}(\\mathbf{x}^{(p)},t_{\\ell}^{(p)})+\\phi_{\\ell}(\\mathbf{x}^{(p+1)},t_{\\ell}^{(p)}).}\\end{array}$ end if else $\\mathbf{x}^{(p+1)}=\\mathbf{x}^{(p)}$ . \u2206(p+1) = \u03b3\u2206(p). t(\u2113p+1)= t(\u2113p)for \u2113= 1, . . . , m. end if ", "page_idx": 20}, {"type": "text", "text": "end while ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since we are working with numerical algorithms, an approximately feasible set will be appropriate. Define the following constrained MOO problem ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x}}{\\operatorname*{min}}~~[h_{1}(\\mathbf{x}),\\dots,h_{\\ell}(\\mathbf{x}),\\dots,h_{m}(\\mathbf{x})],}\\\\ &{\\mathrm{~s.t.~}\\,\\|f(\\mathbf{x})-f(\\mathbf{x}^{(0)})\\|\\le\\epsilon_{f},}\\\\ &{\\|\\mathbf{x}-\\mathbf{x}^{(0)}\\|_{2}\\le\\epsilon_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the sequel, the two constraints can be combined such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\tilde{f}(\\mathbf{x})-\\tilde{f}(\\mathbf{x}^{(0)})\\|\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $\\epsilon>0$ , with $\\tilde{f}(\\mathbf{x})=[f(\\mathbf{x}),\\mathbf{x}]$ . Therefore, we let $f$ denote $\\tilde{f}$ to simplify the notation. Define the domain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{1}\\stackrel{\\mathrm{def}}{=}\\{\\mathbf{x}:\\|f(\\mathbf{x})-f(\\mathbf{x}^{(0)})\\|\\leq\\epsilon\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assumption A.1: The constraint function $f(\\mathbf{x})$ is continuously differentiable on the domain $\\mathbb{R}^{n}$ , and the objective functions $h_{\\ell}$ , $\\ell=1,\\ldots,m_{},$ , are continuously differentiable in the set ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{C}_{2}\\overset{d e f}{=}\\mathcal{C}_{1}+\\mathcal{B}(0,\\delta\\Delta^{(1)}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\delta>1$ is a constant, $\\Delta^{(1)}$ is the initial radius argument for the trust-region method for multiobjective optimization $T R{-}M O O(\\mathbf{x},\\Delta)$ to be defined below, and $B(0,\\delta\\Delta^{(1)})$ is an open ball centered at $\\boldsymbol{O}$ of radius $\\delta\\Delta^{(1)}$ . ", "page_idx": 20}, {"type": "text", "text": "Assumption A.2: The objective functions $h_{\\ell}$ , $\\ell=1,\\ldots,m$ are bounded in the set $\\mathcal{C}_{1}$ . Specifically, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathrm{low}}\\overset{\\mathrm{def}}{=}\\underset{\\ell}{\\operatorname*{min}}\\{\\underset{\\mathbf{x}}{\\operatorname*{min}}\\,h_{1}(\\mathbf{x}),\\ldots,\\underset{\\mathbf{x}}{\\operatorname*{min}}\\,h_{m}(\\mathbf{x})\\},}\\\\ &{h_{\\mathrm{up}}\\overset{\\mathrm{def}}{=}\\underset{\\ell}{\\operatorname*{max}}\\{\\underset{\\mathbf{x}}{\\operatorname*{max}}\\,h_{1}(\\mathbf{x}),\\ldots,\\underset{\\mathbf{x}}{\\operatorname*{max}}\\,h_{m}(\\mathbf{x})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We give an attacking algorithm MOO-attack that can either find the first feature pair to flip in an explanation, or claim that it is impossible to flip any feature pair. The superscript $(p)$ $\\mid,\\,p=1,2,\\ldots$ indicates the number of iterations. ", "page_idx": 20}, {"type": "text", "text": "The attacking algorithm may not be able to filp any pair of features when $\\begin{array}{r}{\\operatorname*{min}_{1\\leq\\ell\\leq m}\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big)<}\\end{array}$ $\\epsilon$ , but there can be other objective functions that still have $\\chi_{\\ell}(\\mathbf{x}^{(p)},t^{(p)})\\bar{\\geq}\\;\\;\\epsilon$ and there is a chance to flip the corresponding pairs of features. We will remove any objective function $h_{\\ell}$ with $\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big)<\\epsilon$ and return to the while loop to try to flip other pairs of features. If all objective functions are removed at the end, the attacker fails to attack the explanation. ", "page_idx": 21}, {"type": "text", "text": "We adapt the theoretical results from [9] to the above multi-objective optimization algorithm to show a global convergence rate for the attacker. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.5. Suppose that assumption A.1 holds. If $\\mathbf{x}^{(p)}\\,\\in\\,\\mathcal{C}_{1}$ , then for each linearized merit function $l_{\\phi_{\\ell}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nl_{\\phi_{\\ell}}\\big(\\mathbf{x}^{(p)},t_{\\ell}^{(p)},0\\big)-l_{\\phi_{\\ell}}\\big(\\mathbf{x}^{(p)},t_{\\ell}^{(p)},\\mathbf{d}^{(p)}\\big)\\geq\\operatorname*{min}(\\Delta^{(p)},1)\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma A.5 shows sufficient descent in the linearized merit functions in the direction $\\mathbf{d}^{(p)}$ . The proof can be found in Lemma 2.1 in [9]. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.6. Suppose that assumption A.1 holds. In each iteration for $k\\geq1$ in Algorithm $^{\\,l}$ , the following properties hold: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{h_{\\ell}({\\mathbf x}^{(p)})-t_{\\ell}^{(p)}>0,\\ell=1,\\dots,m,}\\\\ &{}&{\\phi_{\\ell}({\\mathbf x}^{(p)},t_{k})=\\epsilon,\\ell=1,\\dots,m,\\ \\ \\ }\\\\ &{}&{|h_{\\ell}({\\mathbf x}^{(p)})-t_{\\ell}^{(p)}|\\leq\\epsilon,\\ell=1,\\dots,m,}\\\\ &{}&{\\|f({\\mathbf x}^{(p)})\\|\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma A.6 shows that from iteration to iteration during the loop in Algorithm 1, the invariants will be maintained. The proof can be obtained by applying Lemma 2.2 in [9] to each of the objective functions independently. ", "page_idx": 21}, {"type": "text", "text": "The last inequality indicates that during the attack, the manipulated input $\\mathbf{x}^{(p)}$ remains in the approximate feasible set $\\mathcal{C}_{1}$ and that the prediction by $f$ is not changed. This is important to make the attack stealthy. The second last inequality shows that each objective $h_{\\ell}$ will chase the corresponding target t(\u2113p over the iterations, so that if $\\bar{t_{\\ell}^{(p)}}$ can be shown to be decreasing sufficiently fast, we can show the convergence of $h_{\\ell}$ . Note that different targets $t_{\\ell}^{(p)}$ will move at different speeds, as they are updated independently in the algorithm. Also, a target is not guaranteed to be reduced below zero for $h_{\\ell}$ to become negative and the $\\ell\\cdot$ -th pair of features will be flipped. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.7. Suppose that assumptions A.1 and A.2 hold. Then with min\u2113 $\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big)\\geq\\epsilon$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta^{(p)}\\leq\\operatorname*{min}_{\\ell}\\frac{(1-\\eta)\\epsilon}{L_{g_{\\ell}}+\\frac{1}{2}L_{J}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the condition min\u2113\u03c1(\u2113p $\\operatorname*{min}_{\\ell}\\rho_{\\ell}^{(p)}>\\eta$ in Algorithm $^{\\,l}$ will hold true and $\\Delta^{(p+1)}=\\Delta^{(p)}$ . Furthermore, with min\u2113 $\\begin{array}{r}{\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big)\\geq\\epsilon,}\\end{array}$ , we have, for all $p\\geq1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta^{(p)}\\geq\\operatorname*{min}\\left(\\Delta^{(1)},\\operatorname*{min}_{\\ell}\\frac{(1-\\eta)\\gamma}{L_{g_{\\ell}}+\\frac{1}{2}L_{J}}\\right)\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma A.7 shows that when the radius $\\Delta^{(p)}$ used in $\\Gamma\\mathrm{R-}\\mathrm{MOO}(\\mathbf{x}^{(p)},\\,\\Delta^{(p)})$ is small enough, the radius won\u2019t be further reduced. Together with Lemma A.5, the linearized merit functions are reduced sufficiently per iteration. The proof is a modification to the proof of Lemma 3.2 in [9], with the derivation done for each of the objective functions $h_{\\ell}$ to guarantee sufficient descent in all merit functions. An interesting observation is that the search radius $\\Delta^{(p)}$ is restricted by the objective that has the most rapid change in its gradient $g_{\\ell}$ (characterized by $L_{g_{\\ell}}$ ). If all objectives are smooth (with small $L_{g_{\\ell}}$ ), the search radius can be larger and the reduction in the merit functions is larger. The second inequality of the above lemma says that the search radius will have a lower bound across all iterations. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.8. There are at most $O(\\lceil\\log(\\epsilon)\\rceil)$ number of times that $\\Delta^{(p)}$ will be reduced by the factor of $\\gamma$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma A.8 characterizes the times to reduce the search radius. It is because, starting from $\\Delta^{(1)}$ , once \u2206(p) falls below min\u2113L(g1\u2212+ \u03b71)L\u03f5J at iteration $p$ , there will be no more reduction in future iterations. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.9. Suppose that assumptions A.1 and A.2 hold. Whenever $\\mathbf{x}^{(p)}$ is updated in Algorithm $^{\\,l}$ , for each objective function $h_{\\ell}$ , both the reductions $\\phi_{\\ell}\\bigl(\\mathbf{x}^{(p)},t_{\\ell}^{(p)},0\\bigr)-\\phi_{\\ell}\\bigl(\\mathbf{x}^{(p)},t_{\\ell}^{(p)},\\mathbf{d}^{(p)}\\bigr)\\,a n d\\,t_{\\ell}^{(p)}-t_{\\ell}^{(p)}$ are at least ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left(\\Delta^{(1)},\\operatorname*{min}_{\\ell}\\frac{(1-\\eta)\\gamma}{L_{g_{\\ell}}+\\frac{1}{2}L_{J}}\\right)\\epsilon^{2}\\eta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma A.9 shows a sufficient reduction in the merit functions and the targets. The proof is to use the condition that $\\rho_{\\ell}^{(p)}>\\eta$ for all $\\ell=1,\\ldots,m$ , Eq. (33), the condition that $\\begin{array}{r}{\\operatorname*{min}_{1\\leq\\ell\\leq m}\\chi_{\\ell}\\big(\\mathbf{x}^{(p)},t^{(p)}\\big)\\geq}\\end{array}$ $\\epsilon$ , and Eq. (39). ", "page_idx": 22}, {"type": "text", "text": "Lastly, we present the main global convergence results. ", "page_idx": 22}, {"type": "text", "text": "Theorem A.10. Suppose assumptions A.1-A.2 hold. Then Algorithm 2 generates an \u03f5\u2212first-order critical point for problem Eq. (27) in at most ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\lceil\\left(h_{\\mathrm{up}}-h_{\\mathrm{low}}\\right)\\frac{\\kappa}{\\epsilon^{2}}\\right\\rceil\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "iterations of the while loop in the algorithm, where $\\kappa$ is a constant independent of \u03f5 but depending on $\\gamma,\\,\\eta,\\,L_{h_{\\ell}}$ , and $L_{J}$ . ", "page_idx": 22}, {"type": "text", "text": "The proof hinge on the following inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathrm{low}}\\leq h_{\\ell}(\\mathbf{x}^{(p)})}\\\\ &{\\qquad\\leq t_{\\ell}^{(p)}+\\epsilon}\\\\ &{\\qquad\\leq t_{\\ell}^{(1)}-i_{p}\\kappa_{2}\\epsilon^{2}+\\epsilon}\\\\ &{\\qquad\\leq h_{\\ell}(\\mathbf{x}^{(1)})-i_{p}\\kappa_{2}\\epsilon^{2}+\\epsilon}\\\\ &{\\qquad\\leq h_{\\mathrm{up}}-i_{p}\\kappa_{2}\\epsilon^{2}+\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $i_{p}$ is the number of iterations between 1 and $p$ where $\\mathrm{min}_{\\ell}\\,\\rho_{\\ell}^{(p^{\\prime})}>\\eta,1\\le p\\le p^{\\prime}$ , is true. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\kappa_{2}=\\operatorname*{min}\\left(\\Delta^{(1)},\\operatorname*{min}_{\\ell}\\frac{(1-\\eta)\\gamma}{L_{g_{\\ell}}+\\frac{1}{2}L_{J}}\\right)\\eta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\ni_{p}\\leq\\Big\\lceil\\frac{h_{\\mathrm{up}}-h_{\\mathrm{low}}+\\epsilon}{\\kappa_{2}\\epsilon^{2}}\\Big\\rceil.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $O(\\lceil\\log(\\epsilon)\\rceil)$ grows slower than $1/\\epsilon^{2}$ , the overall number of iterations is in the order of $\\begin{array}{r}{\\left\\lceil\\left(h_{\\mathrm{up}}-h_{\\mathrm{low}}\\right)\\frac{\\kappa}{\\epsilon^{2}}\\right\\rceil}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "A.5 Statistical robustness ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We will use tools for generalization error analysis on any single input $\\mathbf{x}$ to prove that with a high probability, the empirical error of ranking salient features of $\\mathbf{x}$ for a fixed model $f$ is not too far away from the true error of ranking the same set of salient features, with respect to random sampling of $\\mathbf{x}$ from an arbitrary distribution $\\mathcal{D}$ on a support set around $\\mathbf{x}$ where the salient features are preserved. ", "page_idx": 22}, {"type": "text", "text": "Roadmap: We first adopt the McDairmid\u2019s inequality for dependent variables to show a concentration inequality for ranking errors of a fixed feature ranking function based on the salient score function $\\bar{\\mathcal{Z}}(\\bar{\\bf x})$ . Then we will adopt the standard covering number argument as in [2, 68] to prove uniform convergence that depends on the margin in the empirical ranking loss, therefore justifying the maximization of the gap/thickness between the scores of salient and non-salient features in R2ET. ", "page_idx": 22}, {"type": "text", "text": "Basic definitions. Given $\\mathcal{T}$ and $\\mathbf{x}$ , define the true and empirical 0-1 risks ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{0,1}^{\\mathrm{true}}({\\mathcal{Z}},{\\mathbf{x}})=\\mathbb{E}_{{\\mathbf{x}}^{\\prime}\\sim{\\mathcal{D}}}\\left[\\frac{1}{m}\\sum_{i=1}^{k}\\sum_{j=1}^{n}\\mathbb{1}[{\\mathcal{Z}}_{i}({\\mathbf{x}}^{\\prime})<{\\mathcal{Z}}_{j}({\\mathbf{x}}^{\\prime})]\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{0,1}^{\\mathrm{emp}}(\\mathbb{Z},\\mathbf{x})=\\frac{1}{m}\\sum_{i=1}^{k}\\sum_{j=1}^{n}\\mathbb{1}[\\mathbb{Z}_{i}(\\mathbf{x})<\\mathbb{Z}_{j}(\\mathbf{x})].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To relate the risks to the thickness, consider the loss $\\phi_{u}(z),u>0$ , which is similar to the hinge loss ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi_{u}(z)=\\left\\{\\!\\!\\begin{array}{l l}{{1}}&{{z<0,}}\\\\ {{1-z/u}}&{{0\\leq z<u,}}\\\\ {{0}}&{{z\\geq u.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Based on $\\phi_{u}$ , we define the surrogate true and empirical risks ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{\\phi,u}^{\\mathrm{true}}(\\mathbb{Z},\\mathbf{x})=\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathcal{D}}\\left[\\frac{1}{m}\\sum_{i=1}^{k}\\sum_{j=1}^{n}\\phi_{u}\\big(\\mathbb{Z}_{i}(\\mathbf{x}^{\\prime})-\\mathbb{Z}_{j}(\\mathbf{x}^{\\prime})\\big)\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{\\phi,u}^{\\mathrm{emp}}(\\mathbb{Z},\\mathbf{x})=\\frac{1}{m}\\sum_{i=1}^{k}\\sum_{j=1}^{n}\\phi_{u}(\\mathbb{Z}_{i}(\\mathbf{x})-\\mathbb{Z}_{j}(\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lastly, an upper bound of $\\phi_{u}(z)$ can be defined as a large-margin 0-1 loss: if $z\\geq u$ , the loss is 0, and otherwise, the loss is 1. The corresponding empirical risk $\\begin{array}{r}{R_{0,1,u}^{\\mathrm{emp}}(\\mathcal{T},\\mathbf{x})=\\frac{1}{m}\\sum_{i=1}^{k}\\sum_{j=1}^{n}\\mathbb{1}[\\mathcal{T}_{i}(\\mathbf{x})<}\\end{array}$ $\\mathbb{Z}_{j}(\\mathbf{x})+u]$ counts how many pairs of salient and non-salient features that are the salient feature is $u$ less salient than the non-salient feature according to $\\mathcal{T}$ . ", "page_idx": 23}, {"type": "text", "text": "Generalization bound of $R_{\\phi,u}^{\\mathrm{emp}}(\\mathcal{T},\\mathbf{x})$ for a specific $\\mathcal{T}$ . As we randomly sample $\\mathbf{x}^{\\prime}$ from $\\mathcal{D}$ , the terms $\\mathcal{T}_{i}(\\mathbf{x}^{\\prime})$ and the associated 0-1 and $\\phi_{u}$ losses. The typical McDiarmid\u2019s inequality bounds the probability of a function of multiple independent random variables from the expectation of the function. The elements in $R_{\\phi,u}^{\\mathrm{emp}}$ are not independent. The saliency scores of different features are dependent since they are function of the same model $f$ , the input $\\mathbf{x}$ , and the mechanism that calculate the gradients for $\\mathcal{T}$ . In [81], the authors generalized the inequality to dependent variables, and we adopt their conclusion as follows: ", "page_idx": 23}, {"type": "text", "text": "Lemma A.11. Given prediction model $f$ , saliency map $\\mathcal{T}$ , input $\\mathbf{x}_{:}$ , distribution $\\mathcal{D}$ surrounding x, surrogate loss $\\phi_{u}$ , and a constant $\\epsilon>0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\mathbf{x}^{\\prime}\\sim\\mathcal{D}}\\left\\{R_{\\phi,u}^{\\mathrm{true}}(\\mathcal{Z},\\mathbf{x})\\geq R_{\\phi,u}^{\\mathrm{emp}}(\\mathcal{Z},\\mathbf{x})+\\epsilon\\right\\}\\leq2\\exp\\left(-\\frac{2m\\epsilon^{2}}{\\chi}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\chi$ is the chromatic number of the dependency graph of the pairs of random variables $(\\mathbb{Z}_{i},\\mathbb{Z}_{j})$ for any $1\\leq i\\leq k$ and $k+1\\leq j\\leq n$ . ", "page_idx": 23}, {"type": "text", "text": "Comments: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The above dependency graph describes when two pairs of random variables $(\\mathcal{T}_{i},\\mathcal{T}_{j})$ (regarded as a random vector) and $({\\mathcal{T}}_{i^{\\prime}},{\\mathcal{T}}_{j^{\\prime}})$ (regarded as another random vector) are dependent. In particular, the   \ntwo nodes $(\\mathbb{Z}_{i},\\mathbb{Z}_{j})$ and $(\\bar{Z_{i^{\\prime}}},\\bar{Z_{j^{\\prime}}})$ are linked if they are dependent. This graph depends on $\\mathcal{T},\\ensuremath{\\mathbf{x}}$ , and $\\mathcal{D}$ and is in general unknown.   \n\u2022 The chromatic number of the dependency graph is upper-bounded by 1 plus the maximum degree of nodes on the dependency graph, which is $1{+}m$ . In the worst case, the bounds in Eq. (54) approximates $2\\exp(-2\\epsilon^{2})$ as $m$ becomes sufficiently large. ", "page_idx": 23}, {"type": "text", "text": "Uniform convergence of the class of saliency score functions. Since R2ET searches the optimal $\\mathcal{T}$ function from a class $\\mathcal{F}$ of such functions given $f,{\\mathcal{T}},\\mathbf{x},$ , and $\\mathcal{D}$ , $u>0$ , with the preference over $\\mathcal{T}$ that has a large gap $u$ so that $\\mathcal{T}_{i}(\\mathbf{x})>\\mathcal{T}_{j}(\\mathbf{x})+u$ for salient feature $i$ and non-salient feature $j$ . ", "page_idx": 23}, {"type": "text", "text": "Theorem A.12. Given prediction model $f$ , input $\\mathbf{x}$ , surrogate loss $\\phi_{u}$ , and a constant $\\epsilon>0$ , for arbitrary saliency map $\\mathcal{T}\\in\\mathcal{F}$ and any distribution $\\mathcal{D}$ surrounding $\\mathbf{x}$ that preserves the salient features in $\\mathbf{x}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\mathbf{x}^{\\prime}\\sim\\mathcal{D}}\\big\\{R_{0,1}^{\\mathrm{tue}}(\\mathbb{Z},\\mathbf{x})\\ge R_{0,1,u}^{\\mathrm{emp}}(\\mathbb{Z},\\mathbf{x})+\\epsilon\\big\\}\\le2\\mathcal{N}(\\mathcal{F},\\frac{\\epsilon u}{8})\\exp\\left(-\\frac{2m\\epsilon^{2}}{\\chi}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\textstyle N(\\mathcal{F},\\frac{\\epsilon u}{8})$ is the covering number of the functional space $\\mathcal{F}$ with radius $\\begin{array}{r}{\\frac{\\epsilon u}{8}\\;[I O2J.}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "\u2022 The proof is standard and a similar proof can be found in [68].   \n\u2022 nWoint-hs aal ileanrtg efre $u\\,>\\,0$ ,w itlhl en oetm hpaivriec tahl eriirs ska $R_{0,1,u}^{\\mathrm{emp}}(\\mathcal{T},\\mathbf{x})$ lawriglle ri nthcraena tshei sa sg amp.o rOe np tahire s otohf esra lhiaenndt , atnhde covering number $\\textstyle N(\\mathcal{F},\\frac{\\epsilon u}{8})$ will decrease as $u$ increases as a larger radius can cover more $\\mathcal{T}\\in\\mathcal{F}$ This represents a model selection problem.   \n\u2022 Last but not least important, with a larger gap $u$ that R2ET can optimize $\\mathcal{T}$ , with a random perturbation of $\\mathbf{x}$ to $\\mathbf{x}^{\\prime}$ that has the same saliency features, the true risk (representing how bad the true explanations can be perturbed) will be low. ", "page_idx": 24}, {"type": "text", "text": "B Details of Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide detailed experimental settings in B.1, and comprehensive results in B.2. Specifically, correlation exploration (B.2.1), case study (B.2.2), constrained optimizations (B.2.3), ablation studies (B.2.5), accuracy and explanation robustness trade-off (B.2.4), more explanation methods (??), and faithfulness evaluations (B.2.6). ", "page_idx": 24}, {"type": "text", "text": "B.1 Experimental Settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "B.1.1 Datasets and Target Model Structure ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Tabular data. Three tabular datasets are included: Adult, Bank [53] and COMPAS [54]. We divide each tabular dataset into training, validation, and test portions at a ratio of $70:15:15$ , respectively. We follow [11] to binarize and map the original inputs mixing with strings and numbers to 28-dim, 18-dim, and 16-dim feature spaces, respectively. ", "page_idx": 24}, {"type": "text", "text": "Image data. We adopt ResNet18 [27] for CIFAR-10 [36]. As for MNIST [40], we adopt an SN, where the embedding model is a classic CNN, consisting of two convolutional layers with $3{^{*}\\!\\cdot}3$ kernels followed by max-pooling and three fully-connected layers. Then We use cosine similarity as the similarity metric, and classifier is a single-layer linear model taking the outputs from the embedding models as inputs. To construct pairing samples, we randomly sample 2400 pairs of images with digits 3 and 8 from training images as the training set, and 300 and 600 pairs from test images as the validation and test sets, respectively. ", "page_idx": 24}, {"type": "text", "text": "Graph data. We apply SNs to two graph datasets, BP [48] and ADHD [49], where each brain network comprises 82 and 116 nodes, respectively. We pair any two training graphs as the training set. To simulate real medical diagnosis (by comparing a new sample with those in the database), each validation (testing) pair consists of a training graph and a validation (test) graph. The embedding model consists of a two-layer GCN, where the first layer maps inputs to a 256-dimension hidden space following ReLU, and then maps to a 128-dimension embedding space. Then it adopts a mean pooling to aggregate node features to graph-level one. The cosine similarity is used to measure the similarity between two embeddings. The classifier is a single-layer linear model taking the outputs from the corresponding embedding models as inputs. ", "page_idx": 24}, {"type": "text", "text": "B.1.2 Methodology of Training Target Models ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We train all the models from scratch, except that ResNet is retrained from the Vanilla model. The model with a relatively high cAUC and the highest $\\mathrm{P}@k$ on the validation set will be adopted as the outstanding model for further evaluation. Specifically, We choose the last model with cAUC higher than a threshold on the validation set. Then the outstanding model is the one with the highest $\\mathrm{P}@k$ out of these high cAUC models on the validation set. ", "page_idx": 24}, {"type": "text", "text": "Details of training by R2ET. While incorporating a priori information could enhance R2ET\u2019s performance, we do not furnish the model with such information to ensure a fair comparison among training methods. 1) Training from scratch: The model incrementally develops an understanding of feature importance. At the $t$ -th training iteration, it aims to preserve the feature ranking from the $\\left(t-1\\right)$ iteration. This iterative refinement stabilizes the model\u2019s perception of feature importance without any predefined ranking knowledge. 2) Training from a Vanilla Model: We leverage the established model\u2019s feature ranking as a baseline. Given that the vanilla model has achieved satisfactory performance (AUC), its explanation ranking is a reliable reference. The goal of R2ET is to maintain this inherited feature ranking during retraining. ", "page_idx": 24}, {"type": "table", "img_path": "HYa3eu8scG/tmp/edf3cc4501a62465b98f7796ab1ad70131af757266614f7631bb873ebcaf6a8a.jpg", "table_caption": [], "table_footnote": ["Table 4: (Hyper)-parameters used in experiments. $\\overline{{{*}^{\\bullet}\\mathbf{S}\\mathbf{N}^{\\circ}}}$ means Siamese Networks for dual input. "], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Selections of hyperparamters and parameters. Table 4 shows the default (hyper)-parameters adopted in the experiments. Attacks are conducted in a PGD-style [50], and the infinity norm of the perturbations in each iteration is restricted to no more than 5 for images and 0.2 for graphs. For R2ET, $\\lambda_{1}$ and $\\lambda_{2}$ are selected as the same as the best $\\lambda$ for $\\mathrm{R}2\\mathrm{ET}_{\\backslash H}$ and est-H, respectively. Alternatively, we simply set $\\lambda_{1}=\\lambda_{2}=\\lambda$ , and $\\lambda$ can be drawn from $\\{0.01,0.1,1,5,10,100\\}$ . ", "page_idx": 25}, {"type": "text", "text": "When adopting SmoothGrad (SG) [75] as explanation method, where $\\begin{array}{r}{\\mathcal{Z}_{S G}(\\mathbf{x})=\\frac{1}{M}\\sum_{m}^{M}\\mathcal{Z}(\\mathbf{x}+\\boldsymbol{\\beta}_{m})}\\end{array}$ and $\\beta_{m}\\sim\\mathcal{N}(0,\\sigma^{2}I)$ , $M$ is set 50 for all datasets, and $\\sigma^{2}=0.5,25.5^{2}$ , 0.01 for tabular, images, and graphs, respectively. When adopting Integrated Gradients (IG) [77], where $\\mathcal{Z}_{I G}(\\mathbf{x})=(\\mathbf{x}-\\mathbf{x}^{0})\\odot$ $\\begin{array}{r}{\\int_{\\alpha=0}^{1}\\nabla_{\\mathbf{x}}f(\\mathbf{x}^{0}+\\alpha(\\mathbf{x}-\\mathbf{x}^{0}))d\\alpha}\\end{array}$ . In practice, we set $\\mathbf{x}^{0}=\\mathbf{0}$ , and approximate the integration by interpolating 100 samples between $\\mathbf{x}^{0}$ and $\\mathbf{x}$ . ", "page_idx": 25}, {"type": "text", "text": "Running environment. We mainly conduct experiments for three tabular data and CIFAR-10 on the following two machines. Both come with a 16-core Intel Xeon processor and four TITAN X GPUs. One installs 16.04.3 Ubuntu with 3.8.8 Python and 1.7.1 PyTorch, and the other installs 18.04.6 Ubuntu with 3.7.6 Python and 1.8.1 PyTorch. The MNIST and graph datasets are run on a machine with two 10-core Intel Xeon processors and five GeForce RTX 2080 Ti GPUs, which installs 18.04.3 Ubuntu with 3.9.5 Python and 1.9.1 PyTorch. ", "page_idx": 25}, {"type": "text", "text": "B.1.3 Saliency Maps of Target Models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use the absolute gradient values as the explanation for tabular datasets. For MNIST, we use the normalized absolute gradient values as the explanations. For CIFAR-10, the sum of the absolute gradient values is used as the explanation. For graph data, we use element-wise multiplication of the gradient and the input as the explanation [1]. ", "page_idx": 25}, {"type": "text", "text": "B.1.4 Evaluation Metrics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We adopt precision $@k$ to evaluate the explanation robustness by quantifying the similarity between $\\mathcal{T}(\\mathbf{x})$ and $\\mathcal{T}(\\mathbf{x}^{\\prime})$ ; AUCs and sensitivity to evaluate the model\u2019s classification performance (see B.2.3 and B.2.4); and DFFOT, COMP, and SUFF to evaluate the explanation faithfulness (see B.2.6). ", "page_idx": 25}, {"type": "text", "text": "\u2022 Precision $@k$ $(\\mathbf{P}@k)$ . It is widely used to evaluate the robustness of explanations [23]. Formally, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{P}@k(\\mathbb{Z}(\\mathbf{x}),\\mathbb{Z}(\\mathbf{x}^{\\prime}))=\\frac{|\\mathcal{Z}(\\mathbf{x})_{[k]}\\cap\\mathcal{Z}(\\mathbf{x}^{\\prime})_{[k]}|}{k},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbb{Z}(\\mathbf{x})_{[k]}$ is the set of the $k$ most important features of the explanation ${\\mathcal{T}}(\\mathbf{x}).\\mid\\cdot\\mid$ counts the number of elements. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Clean AUC (cAUC) and adversarial AUC (aAUC). Besides robust explanations, the model needs robust predictions. Thus, we adopt cAUC and aAUC to measure the model\u2019s classification performance before and after the attack, respectively. \u2022 Sensitivity (Sen). Since attacks can be detected by checking the consistency of predictions [93], Sen measures the ratio at which classification results change after an attack for models. \u2022 Decision Flip - Fraction of Tokens (DFFOT) [72] measures the minimum fraction of important features to be removed to filp the prediction. A lower DFFOT indicates a more faithful explanation. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{DFFOT}=\\operatorname*{min}_{k}\\frac{k}{n},\\quad\\mathrm{s.t.~}\\arg\\operatorname*{max}_{c}f_{c}(\\mathbf{x})\\neq\\arg\\operatorname*{max}_{c}f_{c}(\\mathbf{x}_{[\\setminus k]}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $n$ is the number of features, and $\\mathbf{x}_{[\\backslash k]}$ is the perturbed input whose top- $k$ important features are removed. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Comprehensiveness (COMP) [15] measures the changes of predictions before and after removing the most important features. A higher COMP means a more faithful explanation. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{COMP}=\\frac{1}{\\|K\\|}\\sum_{k\\in K}|f_{c}(\\mathbf{x})-f_{c}(\\mathbf{x}_{[\\backslash k]})|,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $K$ is $\\{1,\\ldots,n\\}$ for tabular data, and $\\{1\\%*n,5\\%*n,10\\%*n,20\\%*n,50\\%*n\\}$ for images and graphs. The same setting of $K$ is adopted for SUFF. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Sufficiency (SUFF) [15] measures the change of predictions if only the important tokens are preserved. A lower SUFF means a more faithful explanation. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{SUFF}=\\frac{1}{\\|K\\|}\\sum_{k\\in K}|f_{c}(\\mathbf{x})-f_{c}(\\mathbf{x}_{[k]})|,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ${\\bf x}_{[k]}$ is the perturbed input with only top- $k$ important features. ", "page_idx": 26}, {"type": "text", "text": "B.1.5 Introduction to Siamese Network ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "An SN predicts whether two samples, $\\mathbf{x}^{s}$ and $\\mathbf{x}^{t}$ , are from the same class. The SN uses a network to embed each input, respectively, and measures their similarity by a similarity function. The prediction for class 1 (two samples from the same class), $\\mathrm{Pr}(y=1|\\mathbf{x}^{\\bar{s}},\\mathbf{\\dot{x}}^{t})$ , is ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{1}^{S N}(\\mathbf{x}^{s},\\mathbf{x}^{t};\\mathbf{w})=\\sin(\\mathbf{emb}(\\mathbf{x}^{s};\\mathbf{w}_{0}),\\mathbf{emb}(\\mathbf{x}^{t};\\mathbf{w}_{0});\\mathbf{w}_{1}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and $f_{0}^{S N}=1-f_{1}^{S N}$ for class 0. $\\sin(\\cdot,\\cdot;\\mathbf{w}_{1})$ is a similarity function, such as the cosine similarity, and $\\mathbf{emb}(\\cdot;\\mathbf{w}_{0})$ is an embedding network, such as GNN. $\\dot{\\mathbf{w}}=\\{\\mathbf{w}_{0},\\mathbf{w}_{1}\\}$ is the SN\u2019s parameter set. The ground truth used in SN is $y^{s t}=\\mathbb{1}[y^{s}=y^{t}]$ , where $y^{s}$ is the label of $\\mathbf{x}^{s}$ . ", "page_idx": 26}, {"type": "text", "text": "To predict a single input, one can further train a classifier $f^{C L}$ based on the embedding part of the SN. For example, $f^{C L}(\\mathbf{x})=\\mathrm{cls}(\\mathbf{emb}(\\mathbf{x};\\mathbf{w}_{0}^{\\prime});\\mathbf{w}_{2})$ , where $\\ensuremath{\\mathbf{w}}_{0}^{\\prime}$ can be the same as or retrained from $\\mathbf{w}_{\\mathrm{0}}$ in Eq. (56), and $\\mathrm{cls}(\\cdot;\\mathbf{w}_{\\mathrm{2}})$ is a classifier such as a linear model with the parameter set $\\mathbf{w}_{2}$ . ", "page_idx": 26}, {"type": "text", "text": "B.2 Additional Experimental Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "B.2.1 Settings for Correlation Experiments and More Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We explore the correlation between the manipulation epochs, thickness, and Hessian norm concerning different models on various datasets. Specifically, as shown in Fig. 4, we consider different datasets and methods, such as Vanilla, est- $\\boldsymbol{\\cdot}\\boldsymbol{\\mathrm{H}}$ , and R2ET. Since images and graphs have more features than the three tabular datasets, almost all adversarial samples will swap feature pairs under the first epoch of attack. Alternatively, we set the manipulation epoch metric for graph and image datasets to record the first epoch where $\\mathrm{P}@k$ drops below 0.8. We do not plot the results on Bank, where R2ET achieves $\\bar{100\\%}$ $\\mathrm{P}@k$ . Fig. 4 presents the correlations between manipulation epochs and the other four metrics. Besides thickness measured by adversarial samples and Hessian norm as used in Sec. 6.3, we additionally show the thickness measured by either the average or minimal of a few Gaussian samples to mimic random noise [106]. Although the thickness evaluated by Gaussian distribution is not specific to adversarial attacks, the corresponding correlations are much higher than between the manipulation epoch and Hessian norm. ", "page_idx": 26}, {"type": "text", "text": "The dataset-level thickness is defined as the average thickness over all the samples. Table 5 reports $\\mathrm{P}@k$ and dataset-level thickness. Clearly, the models with high $\\mathrm{P}@k$ performance usually have a relatively large thickness as well, and vice versa. ", "page_idx": 26}, {"type": "image", "img_path": "HYa3eu8scG/tmp/95aa7801817fd01f32be918177c846896c79e89fb61524bca23079a60416d3d3.jpg", "img_caption": ["Figure 4: We show the correlation between the manipulation epoch and other metrics, including thickness evaluated by adversarial samples, Hessian norm, and thickness evaluated by (mean and min of) Gaussian samples for different models in various datasets. From top to bottom: Vanilla, est-H models on Adult, respectively. R2ET models on Adult, COMPAS, MNIST, ADHD, and BP, respectively. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "HYa3eu8scG/tmp/948ca95a2129400f90924def9d4e749895a82f8add6669bc1607336f4c3aa9de.jpg", "table_caption": ["Table 5: $\\mathbf{P}@k$ (shown in percentage) of different robust models (rows) under ERAttack and the values of dataset-level thickness. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "B.2.2 More Results for Case Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Fig. 5 shows the saliency maps for case studies concerning all the baselines and R2ETs. ", "page_idx": 28}, {"type": "text", "text": "B.2.3 Constrained Optimization for Attacks ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We show the general form of adversarial attacks by a constrained optimization framework for explanations in Sec. 3. Since defenders can catch adversarial attacks if there are any changes in predictions, the attackers must keep all $S$ predictions unchanged during the attacks, where $S=3$ for SNs and $S\\,=\\,1$ for DNNs. We denote the primary objective in Eq. (1) manipulating the explanations as $g_{0}$ , and constraints for small prediction changes (measured by KL-divergence) as $g_{s},1\\,\\leq\\,s\\,\\leq\\,S$ . Naturally, we construct a Lagrangian function with the non-negative multiplier $\\pmb{\\gamma}=[\\gamma_{0},\\gamma_{1},\\dots,\\gamma_{S}]\\in\\mathbb{R}_{+}^{\\dot{S}+1}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{x},\\gamma)=\\gamma_{0}g_{0}(\\mathbf{x})+\\sum_{s=1}^{S}\\gamma_{s}g_{s}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Some previous works manually set $\\gamma$ as a hyperparameter by experience [16], dismissing the relatives among $g_{s}$ . Instead, we care about the unsatisfied constraints with higher weights by uplifting $\\gamma_{s}$ with larger $g_{s}$ . Thus, we adopt the following methods to update both primal variables $\\mathbf{x}$ and dual variables $\\gamma$ [66, 11]: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Gradient descent ascent (GDA) solves the nonconvex-concave minimax problems [43] by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}\\leftarrow\\mathbf{x}-\\eta_{x}\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}},\\ \\ \\ \\ \\ \\gamma\\leftarrow\\gamma+\\eta_{\\gamma}\\frac{\\partial\\mathcal{L}}{\\partial\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\eta_{x}$ and $\\eta_{\\gamma}$ are the learning rates. Notice that $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}(\\mathbf{x},\\gamma)}{\\partial\\gamma_{s}}\\,=\\,g_{s},\\forall s\\,\\in\\,\\{1,\\ldots,S\\}}\\end{array}$ , and $\\gamma_{0}$ is passively updated by the normalization $\\begin{array}{r}{\\sum_{s=0}^{S}\\gamma_{s}=1}\\end{array}$ at the end of each iteration. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Hedge is an incarnation of Multiplicative Weights algorithm that updates $\\gamma$ using exponential factors [5]. In each iteration, we normalize $\\gamma$ such that $\\begin{array}{r}{\\sum_{s=0}^{S}\\gamma_{s}=1}\\end{array}$ , and then update $\\gamma$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma\\leftarrow\\gamma\\odot\\exp^{\\eta_{H e d g e}[g_{0},g_{1},\\dots,g_{S}]},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\eta_{H e d g e}$ is the learning rate, $\\odot$ is the element-wise multiplication, and exp is exponential operation. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Another way to update weights $\\gamma$ is to solve a quadratic programming (QP) problem ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\gamma}-\\frac{1}{2}\\|\\sum_{s=0}^{S}\\gamma_{s}\\nabla g_{s}(\\mathbf{x})\\|^{2},\\qquad\\mathrm{s.t.}\\sum_{s=0}^{S}\\gamma_{s}=1,\\quad\\gamma_{s}\\geq0,\\forall s\\in\\{0,\\ldots,S\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We also include unconstrained attack without any constraints, and no update that fixes the weights $\\begin{array}{r}{\\gamma=\\frac{1}{S+1}\\mathbf{1}}\\end{array}$ ", "page_idx": 28}, {"type": "image", "img_path": "HYa3eu8scG/tmp/d16cd6d4ebee75f371f4f1b7b88d20da877a2bdae3200c343f9f6e9231422644.jpg", "img_caption": ["Figure 5: Saliency maps concerning the original image pair and the image pair perturbed under ERAttack for all methods. The red pixels are the top 50 important features in saliency maps, with darker colors meaning more important. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Scatter plots in Fig. 6 show different constrained attack methods in terms of $\\mathrm{P}@k$ and sensitivity. The attacker looks for lower sensitivity and $\\mathrm{P}@k$ : lower sensitivity means that the constraints are better satisfied, and lower $\\mathrm{P}@k$ means that more top- $k$ important features in the explanation are distorted. The asterisks highlight the attack methods having the smallest $\\mathrm{P}@k$ and no more than $2\\%$ sensitivity. These attack methods are used to evaluate the defense strategies in Table 1. We do not use constrained attacks on three tabular datasets and MNIST because their sensitivity is $0\\%$ . Besides, Table 6 shows that the difference between cAUC and aAUC for all methods is no more than 0.01, indicating the success of the selected constrained attack to retain the predictions. ", "page_idx": 30}, {"type": "image", "img_path": "HYa3eu8scG/tmp/0bfbff01f214027b30c163731547fa929f27031cbad07ad23266ffa9ced0499f.jpg", "img_caption": ["Figure 6: Performance of different constrained ERAttack and MSE attacks on BP and ADHD for Vanilla. Points in different colors represent different constraint attack methods, and the same color represents the same method with different step sizes. The red line implies the Pareto front, and the asterisk marks the selected attack method. "], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "HYa3eu8scG/tmp/6fd38b059a59c806c876d8b784449f987dbd85d2341c590f1fa9ed71433e5e87.jpg", "table_caption": ["Table 6: cAUC/aAUC of SNs trained by different methods under ERAttack and MSE attack on ADHD and BP. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "B.2.4 Trade-off between Accuracy and Explanation Robustness ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "A good defensive strategy should be both robust and accurate. Authors in [78] find the trade-off between prediction robustness and prediction performance. We explore the trade-off between the explanation robustness $(\\mathbf{P}@k)$ and prediction performance (cAUC/aAUC). A defender looks for a higher $\\mathrm{P}@k$ and AUCs. In Fig. 7, at least one R2ET and its variants are on the Pareto front on all datasets, demonstrating that R2ET and its variants are more advantageous in the trade-off between robustness and accuracy. Furthermore, on MNIST, compared with other methods on the Pareto front, ", "page_idx": 30}, {"type": "text", "text": "R2ET variants on the Pareto front sacrifice less AUC (less than $2\\%$ ) but gain significant improvements on $\\mathrm{P}@k$ $(20\\%\\sim40\\%)$ ). On BP, Est-H improves $\\mathrm{P}@k$ by 0.01 but loses about $4\\%$ AUC. R2ET on ADHD with both high AUC and the highest $\\mathrm{P}@k$ indicates the possibility that a model can be precise and explanation-robust at the same time. ", "page_idx": 31}, {"type": "image", "img_path": "HYa3eu8scG/tmp/e5c2e6c1e76b057259cc1ce4ebc0f0626a7937273779915b41cae520921fd38c.jpg", "img_caption": ["Figure 7: Trade-off between explanation robustness $(\\mathrm{P}@k)$ and prediction performance (cAUC/aAUC) on MNIST, BP and ADHD. The red lines imply the Pareto front, and the triangles present R2ET and its variants. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "B.2.5 Sensitivity Analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Table 7: $\\mathrm{P}@k$ (shown in percentage) of models trained by different methods under ERAttack. Three numbers on tabular datasets present the results when $k$ is 2, 5, and 8, respectively. $k$ is set to 10, 30, and 50 on the rest datasets, respectively. ", "page_idx": 31}, {"type": "table", "img_path": "HYa3eu8scG/tmp/e2778c9d6784974ca7198d7016ff89110514baf4a9d601f68adbd0eddf51cdd6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "In this section, we consider the impacts of three hyperparameters or settings. ", "page_idx": 31}, {"type": "text", "text": "Impacts of $^k$ . Table 7 shows that R2ET and its variants stay at the top compared with other baselines for various $k$ . Besides, we further explore the more fundamental reasons why models perform differently for various $k$ . As mentioned in Eq. (5), the gaps of gradients of original inputs positively contribute to the thickness. In Fig. 8, we sort the gradients of Vanilla models in descending order where their gaps can be inferred. Vanilla achieves about $100\\%$ $\\mathrm{P}@k$ when $k=2$ on Adult and Bank, indicating that even ERAttack cannot effectively manipulate the ranking in such scenarios. The reason is that the top 2 features are much more significant than the rest as shown in Fig. 8. However, there is a narrow margin between the top 2 and top 3 features on COMPAS, and attackers can easily flip their relative rankings, thus Vanilla\u2019s $\\mathrm{P}@k$ reduces to around 2/3. On MNIST, BP and ADHD, Vanilla models\u2019 $\\mathrm{P}@k$ increases as $k$ grows. It seems that models are more efficient against ERAttack with larger $k$ . However, the absolute number of success manipulations for top- $k$ features increases for larger $k$ . Take Vanilla on MNIST as an example, ERAttack distorts the model\u2019s 5 features when $k{=}10$ , and about $40\\%*50{=}20$ features when $k{=}50$ . Since ERAttack uses the same budget for different $k$ , it becomes harder for ERAttack to manipulate more features simultaneously (for larger $k$ ). However, it indeed kicks off more features from the top positions due to narrower margins on the long tail. ", "page_idx": 31}, {"type": "image", "img_path": "HYa3eu8scG/tmp/f7d362c4c674f3942dc869643ad846c81c8ab8faa57a4b861970b5159947974d.jpg", "img_caption": ["Figure 8: We order the features based on the Vanilla model\u2019s gradient magnitude with respect to each original input on different datasets. Notice that these figures would differ for various models. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "HYa3eu8scG/tmp/a7e84a278f4105c652e64ad4834fe8f08d389c2f364c41fc9761693b795e9709.jpg", "img_caption": ["Figure 9: Sensitivity analysis on the number of selected pairs. $\\mathrm{R}2\\mathrm{ET}_{\\backslash H}$ and $\\mathsf{R}2\\mathrm{ET}\\mathrm{-mm}_{\\backslash H}$ are examined in Bank dataset. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Impacts of the number of selected pairs $k^{\\prime}$ . As discussed in Sec. 4.2, fewer $(k^{\\prime})$ compared feature pairs for R2ET and its variants alleviate optimization issues. Specifically, R2ET takes $\\begin{array}{r}{\\sum_{i=k-k^{\\prime}+1}^{k-1}h(\\mathbf x,i,k)+\\sum_{j=k+1}^{k+k^{\\prime}+1}h(\\mathbf x,k,j)}\\end{array}$ nads uocbtj eac tsievnes.i tSiivnitcye $\\mathrm{R}2\\mathrm{E}\\mathrm{T}_{\\backslash H}$ oafn tdh $\\mathsf{R}2\\mathrm{ET}\\mathrm{-mm}_{\\backslash H}$ oardes  tohne Bank. We set $k=8$ , and change $k^{\\prime}$ from 1 to $k$ . Fig. 9 indicates that $\\mathrm{R}2\\mathrm{E}\\mathrm{T}_{\\backslash H}$ and $\\mathsf{R}2\\mathrm{ET}\\mathrm{-mm}_{\\backslash H}$ perform much better when $k^{\\prime}>4$ than those with $k^{\\prime}\\leq4$ . Apparently, when more pairs of features are taken into consideration, R2ET and its variants have a more comprehensive view of feature rankings to maintain the rankings better. Besides that, $\\mathbf{R}2\\mathbf{E}\\mathbf{T}_{\\backslash H}$ and $\\mathrm{R}2\\mathrm{ET}\\mathrm{-mm}_{\\backslash H}$ outperform almost all baselines, except for SP, for all $k^{\\prime}$ . ", "page_idx": 32}, {"type": "text", "text": "Impacts of pretrain / retrain. Lastly, we explore how good these methods are when applying them in the retrain schema, and each model is retrained from the Vanilla model for at most 10 epochs. Since the Vanilla model has already converged and reached a good AUC, we assume that the Vanilla model\u2019s explanation ranking is an excellent reference. Thus these robust methods try to maintain the Vanilla model\u2019s rankings. The retraining will be terminated if $\\mathrm{P}@k$ between Vanilla and retrain models\u2019 rankings significantly drops, or the retrain model\u2019s cAUC drops a lot. Table 8 presents the results for comparing two training schemas. Since SP changes the models\u2019 structure (activation function), we do not consider it here. Instead, we add one more baseline, CL [28], because retraining demands much fewer training epochs, thus its time complexity is acceptable. More details for CL can be found in Sec. 4.2. Besides that, none of retrain models by Exact-H and SSR can maintain Vanilla model\u2019s explanation rankings and cAUC at the same time in Bank dataset, and thus both are not applicable. ", "page_idx": 32}, {"type": "text", "text": "Table 8: $\\mathrm{P}@k$ (shown in percentage) of models trained by different methods under ERAttack $(k=8)$ ). Two numbers indicate the performance of models being trained from scratch or retrained from Vanilla models. ", "page_idx": 33}, {"type": "table", "img_path": "HYa3eu8scG/tmp/fecde21710dad211b36a1784ffd90ad27e8c02fd4b6a7d03889aebaa3e4abda9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "B.2.6 Faithfulness of explanations on different models ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We report the faithfulness of explanations evaluated by three widely used metrics, DFFOT, COMP and SUFF, in Table 9. ", "page_idx": 33}, {"type": "table", "img_path": "HYa3eu8scG/tmp/c645b26d1e73b96e1cb79ca1a2013ef2006e5794199cadf2a717f3f078ba44f6.jpg", "table_caption": ["Table 9: Faithfulness of explanations evaluated by DFFOT (\u2193) / COMP (\u2191) / SUFF (\u2193). "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "C Related Work ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Explainable machine learning and explanation robustness. Recent post-hoc explanation methods for deep networks can be categorized into gradient-based [103, 71, 7, 75, 77, 73, 100, 98, 35], surrogate model based [64, 30, 82], Shapley values [47, 12, 45, 99, 3], and causality [58, 10, 56]. The gradient-based methods are widely used in practice due to their simplicity and efficiency [55], while they are found to lack robustness against small perturbations [23, 29]. Some works [13, 74, 32, 83, 70, 80] propose to improve the explanation robustness by time-consuming adversarial training (AT). To bypass the high time complexity of AT, some works propose replacing ReLU function with softplus [17], training with weight decay [16], and incorporating gradient- and Hessianrelated terms as regularizers [17, 88, 91]. Some works propose new explanation methods, rather than training methods, to enhance explanation robustness [46, 44, 11, 51, 65, 75]. Besides, many works [50, 79, 87, 52, 67, 63, 97, 28, 96, 14, 78, 90, 101, 18] for improving adversarial robustness focus on prediction robustness, instead of explanation robustness, and thus are different from our work. The work [97] proposes the prediction thickness by measuring the distance between two decision boundaries, while our explanation thickness qualifies the expected gaps between two features\u2019 importance. Furthermore, the explanation thickness is inherently more complicated to optimize due to the nature of gradient-based explanations being defined as first-order derivative relevant terms. ", "page_idx": 33}, {"type": "text", "text": "Ranking robustness in IR. Ranking robustness in information retrieval (IR) concerning noise [106] and adversarial attacks [26, 104, 105, 42, 85] is well-studied. Ranking manipulations in IR and explanations are different because 1) In IR, authors either manipulate the position of one candidate [104, 26], or manipulate a query to distort the ranking of candidates [104, 105, 42]. We manipulate input to swap any pairs of salient and non-salient features. 2) Ranking in IR is based on the model predictions. However, explanations are defined by gradient or its variants, and studying their robustness requires second or higher-order derivatives, and it motivates us to design an efficient regularizer to bypass costly computations. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Top- $k$ intersection. Top- $k$ intersection is widely used to evaluate explanation robustness [13, 88, 74, 32, 83, 70, 23]. However, most existing works study the explanation robustness through $\\ell_{p}$ norm [13, 16, 17, 88], cosine similarity [83, 84], Pearson correlation [32], Kendall tau [83], or KL-divergence [70]. These correlation metrics measure the explanation similarity using all features, and a high similarity can have entirely different top salient features. To optimize the top- $k$ intersection in classification tasks, some works propose various surrogate losses based on the upper bounds of top- $k$ intersection [2, 37, 38, 39, 8, 95, 60]. The upper bounds hold for predictions whose gaps are not larger than one, while it is not always true for gradient-based explanations with unbounded values, which prevents adopting similar methods to explanation tasks. ", "page_idx": 34}, {"type": "text", "text": "Distributional shift. The distributional shifts [61, 41] and our work are distinct concerning the \u201cperturbation budget\u201d between the original and perturbed inputs: In contrast, the distributional shifts ensure reasonable predictions (and explanations) for out-of-distribution samples, where a substantial deviation from the in-distribution samples is expected, naturally leading to different explanations and rankings. The input perturbations are intentionally neglectable in ours, and other adversarial robustness works, for stealthiness. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The abstract and introduction outline the primary contributions, which include the thickness metric (in Sec. 4.1) and a corresponding training method R2ET (in Sec. 4.2), alongside their theoretical analyses (in Sec. 5) and experimental results (in Sec. 6). ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The only noticed limitation of the work is discussed in Sec. 7. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The complete proofs for the theoretical analyses are provided in Appendix A. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All the information needed to reproduce all the experimental results are disclosed in Appendix B. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Code is public at https://github.com/ccha005/R2ET. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The relevant information is disclosed in appendix B.1.2. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Main result in Table 1 includes the statistical significance tests. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The relevant information is disclosed in appendix B.1.2. ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] Justification: The research conforms with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The original papers for the datasets are cited properly. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}]