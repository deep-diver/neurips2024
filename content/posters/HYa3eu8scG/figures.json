[{"figure_path": "HYa3eu8scG/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of explanation generation and adversarial attacks. The left side shows a schematic of model training and explanation generation, while the right side presents two examples showing that smaller L_p distances between explanations do not guarantee similar top salient features. This highlights the limitations of using L_p distance as a metric for explanation stability.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_8_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure demonstrates the process of generating explanations and how adversarial attacks can manipulate them, even with small lp-distances. The left side shows the model training and explanation generation process. The right side uses two examples to show that smaller lp distances between explanations do not guarantee that their top salient features will be similar.  This highlights a weakness in using lp distances to assess explanation stability, which motivates the paper's proposed method.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_9_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of explanation generation and adversarial attacks. The left side shows the steps involved in model training and explanation generation, highlighting adversarial attacks that manipulate the input to distort the explanation. The right side uses two examples to demonstrate that a small L_p distance between the original explanation and a perturbed explanation does not guarantee similarity in top-k salient features.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_27_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of explanation generation and adversarial attacks. The left side shows the training of a model and the subsequent generation of explanations for a target input, followed by adversarial attacks that manipulate the input to distort the explanation. The right side provides two examples to demonstrate that small lp distances between explanations do not necessarily imply that the top salient features are similar. It highlights the limitations of relying on lp distance as a metric for evaluating the stability of explanations.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_29_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of explanation generation and adversarial attacks. The left side shows a model training process followed by explanation generation from a given input using an explanation method. Then, adversarial attacks manipulate the input, aiming to change the explanation while preserving the original prediction. The right side shows two examples of explanation maps, demonstrating that small lp distances between explanations do not always indicate similar top salient features. This highlights the limitation of using lp distance as a metric for assessing explanation stability.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_30_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of generating explanations and adversarial attacks. The left side shows the model training and explanation generation, while the right side uses two examples to demonstrate that a small lp distance between explanations does not necessarily mean that the top salient features are similar.  This highlights the limitations of using lp distance as a metric for evaluating explanation stability.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_31_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure demonstrates the process of generating explanations and how adversarial attacks can manipulate them.  The left side illustrates the training process and generation of explanations for a target input, followed by adversarial attacks. The right side uses examples to highlight that a small lp distance between two explanations does not guarantee that they have similar top-k features. This is a key point to the paper's argument about the limitations of using lp distance as a metric for evaluating explanation stability.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_32_1.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of generating explanations and the impact of adversarial attacks. The left side shows the steps involved in model training and explanation generation, while the right side demonstrates how small L_p distance between explanations does not guarantee similarity in top salient features.  Two examples highlight how a perturbed explanation can significantly differ from the original explanation, even with a small L_p distance.", "section": "1 Introduction"}, {"figure_path": "HYa3eu8scG/figures/figures_32_2.jpg", "caption": "Figure 1: Left: Green (1): Model training. Yellow (2-4): Explanation generation for a target input. Red (5-6): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller lp distances do not imply similar top salient features. I(x\") has a smaller l2 distance from the original explanation I(x), but manipulates the explanation more significantly (by top-k metric) shown in blue dashed boxes. Statistically, I(x') has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with I(x')'s 100% and 92% top-k overlap, respectively.", "description": "This figure illustrates the process of explanation generation and adversarial attacks. The left side shows the model training and the generation of explanations using various methods. The right side shows two examples of saliency maps, demonstrating that a small L_p distance between perturbed and original explanations does not guarantee similarity in the top-k salient features.  This highlights the limitations of using L_p distance alone to assess explanation stability.", "section": "1 Introduction"}]