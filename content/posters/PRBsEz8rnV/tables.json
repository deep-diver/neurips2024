[{"figure_path": "PRBsEz8rnV/tables/tables_5_1.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table presents the kNN classification accuracy results on 11 datasets using two different vision transformer backbones pre-trained on ImageNet-1K and ImageNet-21K.  It compares the performance of using only the model's embeddings as features versus using the FUNGI features (embeddings combined with self-supervised gradients). The results are shown separately for both the \"full dataset\" and a \"5-shot\" scenario.  The table aims to demonstrate that the FUNGI features consistently improve the classification accuracy across multiple datasets and pre-training strategies.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_6_1.jpg", "caption": "Table 2: Performance improves as more gradients are used. Accuracy in image classification using kNN with embeddings and FUNGI features, averaged across 11 datasets for 7 backbones, for standard and few shot setups. Results for additional backbones are shown in Table 8. \"K\", \"D\" and \"S\" stand for KL, DINO and SimCLR, respectively.", "description": "This table shows the performance of image classification using k-nearest neighbor with embeddings and FUNGI features.  The results are averaged across 11 datasets and 7 different backbones.  It compares the performance with different combinations of gradients from three self-supervised learning objectives (KL, DINO, and SimCLR) for both standard and few-shot settings.  Additional backbones' results are presented in Table 8.", "section": "5.1 Image Classification"}, {"figure_path": "PRBsEz8rnV/tables/tables_6_2.jpg", "caption": "Table 7: FUNGI features improve in-context semantic segmentation on ADE20K. We report the mIoU for retrieval-based semantic segmentation on ADE20K, comparing a DINO baseline against FUNGI features and the self-supervised HummingBird model. Results from Balazevic et al. (2024) are marked with \u2021. We resize each image to 512 \u00d7 512 and extract 322 = 1024 patch features.", "description": "This table compares the performance of different feature extraction methods (Embeddings, FUNGI, HummingBird) for the task of in-context semantic segmentation on the ADE20K dataset.  The results are presented for three different memory bank sizes (1024x102, 1024x103, 1024x104).  The improvement in mIoU (mean Intersection over Union) using FUNGI features over DINO embeddings is highlighted.  The results are also compared to the state-of-the-art HummingBird model.", "section": "5.2 In-Context Scene Understanding"}, {"figure_path": "PRBsEz8rnV/tables/tables_7_1.jpg", "caption": "Table 4: Data-efficient semantic segmentation. mIoU scores for data-efficient retrieval-based semantic segmentation on Pascal VOC 2012 and ADE20K, using DINO backbones and their FUNGI features and embeddings. We also compare FUNGI to end-to-end fine-tuning and find our method to perform best for VOC. Results from Balazevic et al. (2024) are marked with \u2021.", "description": "This table presents the results of data-efficient semantic segmentation on the Pascal VOC 2012 and ADE20K datasets.  It compares the mean Intersection over Union (mIoU) scores achieved using different methods: end-to-end fine-tuning (E2E FT), using only embeddings (Emb.), and using the proposed FUNGI features.  The table shows results for various dataset sizes and DINO backbones (ViT-S/16 and ViT-B/16).  The results highlight that FUNGI consistently outperforms using embeddings alone and is competitive with or even surpasses end-to-end fine-tuning, particularly for Pascal VOC.", "section": "5.2 In-Context Scene Understanding"}, {"figure_path": "PRBsEz8rnV/tables/tables_7_2.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table presents the accuracy achieved by kNN classification using both standard embeddings and FUNGI features.  The results are broken down by dataset and are shown for two different pre-trained models (IN1K and IN21K), demonstrating the consistent improvement of FUNGI across various datasets.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_8_1.jpg", "caption": "Table 6: Impact of the projection head configuration. Top-1 accuracy of gradients on ImageNet-100 in k-nearest neighbor classification versus the projection head configuration for KL, DINO and SimCLR gradients. \u201cnorm\u201d indicates whether the features are L2-normalized before being projected. As features are always L2-normalized for the SimCLR objective, the \u201cempty\u201d head configuration is not applicable. The default setup is marked in cyan.", "description": "This table presents an ablation study on the impact of different projection head configurations (with or without L2 normalization) on the accuracy of gradients obtained using three self-supervised losses (KL, DINO, and SimCLR) for ImageNet-100. The results show that the best configuration consistently yields the highest accuracy.", "section": "5.4 Ablation Studies"}, {"figure_path": "PRBsEz8rnV/tables/tables_16_1.jpg", "caption": "Table 7: FUNGI features improve in-context semantic segmentation on ADE20K. We report the mIoU for retrieval-based semantic segmentation on ADE20K, comparing a DINO baseline against FUNGI features and the self-supervised HummingBird model. Results from Balazevic et al. (2024) are marked with \u2021. We resize each image to 512 \u00d7 512 and extract 32<sup>2</sup> = 1024 patch features.", "description": "This table presents the results of in-context semantic segmentation on the ADE20K dataset.  It compares the mean Intersection over Union (mIoU) scores achieved by using DINO embeddings, FUNGI features (which combine DINO embeddings with gradients from self-supervised losses), and the HummingBird model (a state-of-the-art method for this task). The comparison is done across three different memory bank sizes (1024 x 10<sup>2</sup>, 1024 x 10<sup>3</sup>, 1024 x 10<sup>4</sup>).  The results show that FUNGI consistently improves upon the baseline DINO method, and even achieves comparable performance to the HummingBird model.", "section": "5.2 In-Context Scene Understanding"}, {"figure_path": "PRBsEz8rnV/tables/tables_16_2.jpg", "caption": "Table 8: Additional backbones. Average accuracy of embeddings and FUNGI features in k-nearest neighbor classification across 11 datasets for CLIP (Radford et al., 2021; Sun et al., 2023), AugReg (Steiner et al., 2022), DeIT III (Touvron et al., 2022) and masked autoencoder (He et al., 2022) models. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively.", "description": "This table shows the average accuracy of embeddings and FUNGI features in k-nearest neighbor classification across 11 datasets for several different backbones.  The backbones included are CLIP, AugReg, DeIT III, and MAE.  It also indicates the performance when using only KL, KL+DINO, and KL+DINO+SimCLR gradients. The results are shown for both full dataset and few-shot settings.", "section": "B Additional Experimental Results"}, {"figure_path": "PRBsEz8rnV/tables/tables_17_1.jpg", "caption": "Table 6: FUNGI works across backbones. Accuracy in k-nearest neighbor classification using embeddings and FUNGI features from various ViT backbones, both for full dataset and few shot setups, averaged over 11 datasets. For the FUNGI features we chose the best performing combination across datasets. \"AR\" indicates backbones trained with the AugReg strategy (Steiner et al., 2022).", "description": "This table presents the accuracy of k-nearest neighbor classification using both the original model embeddings and the enhanced FUNGI features. Results are shown for various Vision Transformer (ViT) backbones, across different sizes and training strategies.  Both full-dataset and few-shot settings are included, averaged across 11 diverse datasets.  The \"AR\" designation indicates backbones trained using the AugReg strategy.", "section": "5.1 Image Classification"}, {"figure_path": "PRBsEz8rnV/tables/tables_17_2.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table presents the accuracy of k-Nearest Neighbor (kNN) classification on eleven datasets using two different feature sets:  embeddings from pre-trained models and FUNGI features (which are embeddings augmented with gradients).  Two different pre-trained models are used, one trained on ImageNet-1K and the other on ImageNet-21K. The table shows that using FUNGI features consistently improves classification accuracy over embeddings alone, demonstrating the effectiveness of the FUNGI method.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_18_1.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table compares the performance of embeddings and FUNGI features in k-Nearest Neighbor (kNN) classification across 11 datasets.  Two different Vision Transformer (ViT) models, pretrained on ImageNet-1K (IN1K) and ImageNet-21K (IN21K), are used. The results show that FUNGI features generally improve the accuracy compared to using embeddings alone, indicating that incorporating gradients from self-supervised learning enhances the model's representations.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_18_2.jpg", "caption": "Table 12: FUNGI works for audio. Top-1 accuracies in k-nearest neighbor audio classification of embeddings and FUNGI features obtained from a SSAST backbone (Gong et al., 2022, 2021). \u201cK\u201d and \u201cS\u201d stand for KL and SimCLR, respectively.", "description": "This table presents the results of the k-nearest neighbor audio classification task using the SSAST backbone.  It compares the top-1 accuracy of using just the embeddings with the results of adding features derived from the KL and SimCLR gradients (FUNGI features).  The performance is shown for both the full dataset and a 5-shot scenario, indicating the method's efficacy in low-data settings.  The arrows in the table indicate whether the addition of gradients improved or decreased the accuracy.", "section": "B.2 Audio Classification"}, {"figure_path": "PRBsEz8rnV/tables/tables_18_3.jpg", "caption": "Table 2: Performance improves as more gradients are used. Accuracy in image classification using kNN with embeddings and FUNGI features, averaged across 11 datasets for 7 backbones, for standard and few shot setups. Results for additional backbones are shown in Table 8. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively.", "description": "This table shows the accuracy of image classification using k-Nearest Neighbors (kNN) with different combinations of model embeddings and FUNGI features (features from unsupervised gradients).  It compares the performance using only embeddings against using embeddings combined with gradients from one, two, or three different self-supervised learning objectives (KL, DINO, SimCLR).  Results are averaged across 11 different datasets and are shown for 7 different vision transformer backbones.  The table also shows results for \"few-shot\" scenarios, using a limited number of training examples.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_19_1.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table presents a comparison of the performance of embeddings and FUNGI features on 11 image datasets using two different pre-trained Vision Transformer models (ViT-B/16).  The models were pre-trained using ImageNet-1K (IN1K) and ImageNet-21K (IN21K). For each dataset, it shows the accuracy achieved by k-Nearest Neighbors (kNN) classification using both the original embeddings and the embeddings augmented with FUNGI features.  The table highlights the improvements in kNN accuracy achieved by using FUNGI features over the original embeddings.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_19_2.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table presents the accuracy of kNN classification using embeddings and FUNGI features on eleven datasets.  Two different backbones, pretrained on ImageNet-1k and ImageNet-21k, are used. The table shows that FUNGI features consistently improve the accuracy compared to embeddings across a variety of datasets, indicating the method's generalizability and effectiveness.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_20_1.jpg", "caption": "Table 2: Performance improves as more gradients are used. Accuracy in image classification using kNN with embeddings and FUNGI features, averaged across 11 datasets for 7 backbones, for standard and few shot setups. Results for additional backbones are shown in Table 8. \"K\", \"D\" and \"S\" stand for KL, DINO and SimCLR, respectively.", "description": "This table shows the performance of k-nearest neighbor image classification using embeddings and FUNGI features across 11 datasets.  Results are provided for 7 different backbones (pre-trained models) and for both full datasets and few-shot scenarios (where only a small amount of labeled data is used).  The table demonstrates that incorporating more gradients from different self-supervised learning objectives leads to improved accuracy, showcasing the effectiveness of the FUNGI method.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_20_2.jpg", "caption": "Table 8: Additional backbones. Average accuracy of embeddings and FUNGI features in k-nearest neighbor classification across 11 datasets for CLIP (Radford et al., 2021; Sun et al., 2023), AugReg (Steiner et al., 2022), DeIT III (Touvron et al., 2022) and masked autoencoder (He et al., 2022) models. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively.", "description": "This table presents the average accuracy results across 11 datasets for different backbones (CLIP, EVA-CLIP, AugReg, DeIT III, and MAE) using k-nearest neighbor classification.  It shows the performance of both embeddings alone and embeddings enhanced with FUNGI features derived from three self-supervised learning objectives (KL, DINO, SimCLR). The results are presented for both full datasets and few-shot scenarios.", "section": "B Additional Experimental Results"}, {"figure_path": "PRBsEz8rnV/tables/tables_20_3.jpg", "caption": "Table 17: FUNGI improves language in-context learning. Classification accuracy of GPT 40 mini in language in-context learning with examples retrieved using embeddings or FUNGI features, both extracted from a BERT backbone.", "description": "This table shows the results of in-context learning experiments using a GPT 40 mini model.  It compares the classification accuracy when using either embeddings or FUNGI features (combining embeddings with gradients from KL and SimCLR losses).  Two datasets, Banking-77 and SST, were used for evaluation.", "section": "5.3 Other Modalities"}, {"figure_path": "PRBsEz8rnV/tables/tables_21_1.jpg", "caption": "Table 18: DINO head configuration and data augmentation. Top-1 accuracy on ImageNet-100 in k-nearest neighbor classification for the DINO gradients using shared or independent teacher and student heads (left) and with respect to the data augmentation policy (right).", "description": "This table presents ablation studies on the DINO gradients, comparing different head configurations (shared vs. independent) and data augmentation strategies (standard DINO vs. random crops) for ImageNet-100 classification.  The results show that using independent heads and random crops significantly improves the accuracy of the DINO gradients.", "section": "C Experimental Details"}, {"figure_path": "PRBsEz8rnV/tables/tables_21_2.jpg", "caption": "Table 19: The random projection initialization has little impact on performance. Comparison of the downstream accuracy of FUNGI features built with gradients projected using matrices with different initializations. We report the mean and one standard deviation measured across three runs using the Flowers102 dataset and a DeIT ViT-B/16 backbone. No further dimensionality reduction was applied to the concatenated features.", "description": "This table shows the results of an ablation study on the initialization method for random projections used in the FUNGI method.  It compares three different initializations (Binary, Gaussian, and Sparse) on the Flowers102 dataset using a DeIT ViT-B/16 backbone.  The table shows that the choice of initialization has a minimal impact on the final accuracy, with Gaussian showing slightly better results than binary and sparse.", "section": "B Additional Experimental Results"}, {"figure_path": "PRBsEz8rnV/tables/tables_22_1.jpg", "caption": "Table 20: PCA does not degrade performance. PCA output dimensionalities with respect to the backbone architecture (left) and its impact on k-nearest neighbor image classification accuracy averaged across 11 datasets using a DeIT ViT-B/16 backbone (right).", "description": "This table displays the dimensionality reduction (PCA) impact on the performance of the k-NN image classification experiments.  It shows the PCA dimensions used for different model architectures (ViT-S/16, ViT-B/16, ViT-L/16, BERT, T5, SSAST) and the resulting accuracy with and without PCA, averaged across 11 datasets. The results demonstrate that using PCA doesn't negatively affect the accuracy and even shows a minor improvement in some cases.", "section": "C Experimental Details"}, {"figure_path": "PRBsEz8rnV/tables/tables_22_2.jpg", "caption": "Table 21: PCA is the best dimensionality reduction method. The mean-per-class accuracy of embeddings and FUNGI features from a DeIT ViT-16/B backbone on Flowers102, transformed with PCA and random projections. We report the mean and one standard deviation across three seeds.", "description": "This table compares the performance of different dimensionality reduction techniques on the Flowers102 dataset using a DeIT ViT-16/B backbone.  The methods compared include no dimensionality reduction (No Reduction), Principal Component Analysis (PCA), and three types of random projections (Binary, Gaussian, and Sparse).  The table shows the mean per-class accuracy and standard deviation for each method, with the embeddings and various combinations of gradients (K, D, and S). PCA achieves the highest accuracy. ", "section": "B Additional Experimental Results"}, {"figure_path": "PRBsEz8rnV/tables/tables_22_3.jpg", "caption": "Table 7: FUNGI features improve in-context semantic segmentation on ADE20K. We report the mIoU for retrieval-based semantic segmentation on ADE20K, comparing a DINO baseline against FUNGI features and the self-supervised HummingBird model. Results from Balazevic et al. (2024) are marked with \u2021. We resize each image to 512 \u00d7 512 and extract 322 = 1024 patch features.", "description": "This table compares the performance of different methods for in-context semantic segmentation on the ADE20K dataset.  It shows the mean Intersection over Union (mIoU) scores for embeddings, FUNGI features, and the HummingBird model, across various memory bank sizes.  The results demonstrate that FUNGI significantly improves upon the DINO baseline.", "section": "5.2 In-Context Scene Understanding"}, {"figure_path": "PRBsEz8rnV/tables/tables_24_1.jpg", "caption": "Table 7: FUNGI features improve in-context semantic segmentation on ADE20K. We report the mIoU for retrieval-based semantic segmentation on ADE20K, comparing a DINO baseline against FUNGI features and the self-supervised HummingBird model. Results from Balazevic et al. (2024) are marked with \u2021. We resize each image to 512 \u00d7 512 and extract 322 = 1024 patch features.", "description": "This table compares the performance of different methods on the ADE20K dataset for in-context semantic segmentation using retrieval-based approach.  The methods compared are: DINO embeddings, FUNGI features using DINO embeddings, and the HummingBird model.  The results demonstrate that FUNGI significantly enhances DINO's performance across all memory bank sizes.  Results are broken down by backbone (ViT-S/16 and ViT-B/16) and memory bank size (1024 x 10<sup>2</sup>, 1024 x 10<sup>3</sup>, 1024 x 10<sup>4</sup>).", "section": "5.2 In-Context Scene Understanding"}, {"figure_path": "PRBsEz8rnV/tables/tables_24_2.jpg", "caption": "Table 24: Text classification experimental details. Parameters used to extract text encoder gradients for the LKL (left) and LSimCLR (right) objectives.", "description": "This table lists the hyperparameters used for the text modality experiments.  Specifically, it shows the parameters used to extract gradients from text encoders for two different self-supervised learning objectives: KL Divergence and SimCLR.  The parameters include the number of positive and negative views, projection dimensions, batch size, temperature, and the probability of word deletion for the SimCLR objective.", "section": "C.3 Text Classification Experimental Details"}, {"figure_path": "PRBsEz8rnV/tables/tables_25_1.jpg", "caption": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.", "description": "This table compares the performance of embeddings and FUNGI features in k-Nearest Neighbor classification across 11 datasets.  It shows the accuracy for two different ViT-B/16 models pretrained on either ImageNet-1K or ImageNet-21K, demonstrating that FUNGI features consistently improve accuracy across various datasets.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_25_2.jpg", "caption": "Table 2: Performance improves as more gradients are used. Accuracy in image classification using kNN with embeddings and FUNGI features, averaged across 11 datasets for 7 backbones, for standard and few shot setups. Results for additional backbones are shown in Table 8. \"K\", \"D\" and \"S\" stand for KL, DINO and SimCLR, respectively.", "description": "This table presents the accuracy of image classification using k-Nearest Neighbors (kNN) with different combinations of features (embeddings and gradients from KL, DINO and SimCLR losses).  It compares the performance using only embeddings against results when one or more gradient types are added. The results are averaged across 11 datasets for 7 different backbones and are shown for both standard (full dataset) and few-shot (limited data) scenarios.  Additional backbones are included in Table 8.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/tables/tables_26_1.jpg", "caption": "Table 27: FUNGI introduces a speed overhead. Embeddings and gradients extraction speed measured in images/second on an NVIDIA A100 GPU for a DeIT ViT-B/16 backbone. The gradients speed include the random projection step. The performance column reports the accuracy averaged across 11 datasets for the combination of a single gradient with the model embeddings. \u2020 indicates k-nearest neighbor inference on CPU.", "description": "This table shows the speed of generating embeddings and gradients using an NVIDIA A100 GPU for a DeIT ViT-B/16 backbone.  It also shows the impact on accuracy when using gradients from different self-supervised learning objectives in a k-nearest neighbor classification task across 11 image datasets. The table demonstrates that using gradients, although improving accuracy, significantly reduces the speed at which features can be generated.", "section": "E Compute Resources"}]