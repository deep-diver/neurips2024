[{"figure_path": "PRBsEz8rnV/figures/figures_1_1.jpg", "caption": "Figure 1: Gradient-augmented features: given a pretrained backbone f* and its embeddings, we apply a family of SSL losses, extract their gradients, and project and concatenate them. These new features are used to build a k-nearest neighbor index, which can be used for classification or retrieval.", "description": "This figure illustrates the FUNGI method. It shows how gradients from various self-supervised losses are computed for a given input from a pretrained backbone. These gradients are projected to a lower dimension and concatenated with the model's output embedding. The resulting gradient-enhanced features are then used to build a k-nearest neighbor index, which is used for classification or retrieval.", "section": "1 Introduction"}, {"figure_path": "PRBsEz8rnV/figures/figures_2_1.jpg", "caption": "Figure 2: Combining diverse features (embeddings, gradients) leads to large improvements. Pairwise CKA similarity of features (top) and the KNN accuracy of their combination (bottom).", "description": "This figure demonstrates that combining embeddings with gradients from different self-supervised learning objectives leads to significantly improved performance in k-nearest neighbor (kNN) classification.  The top part shows a pairwise Centered Kernel Alignment (CKA) similarity matrix, which measures the similarity between different feature sets (embeddings and gradients from SimCLR, DINO, and KL-divergence losses).  The heatmap indicates that the features are quite different and therefore complementary. The bottom part displays the kNN accuracy achieved by using different combinations of these features. It illustrates that combining embeddings with gradients consistently results in higher accuracy than using embeddings alone.", "section": "3 Gradients as Features"}, {"figure_path": "PRBsEz8rnV/figures/figures_2_2.jpg", "caption": "Figure 3: Gradients encode different information. Delta in per-class kNN accuracy of gradients from different objectives compared to the embeddings, indicated as \"Emb.\" in the plot.", "description": "This figure compares the per-class kNN accuracy improvement of using gradients from different self-supervised learning objectives (KL, DINO, SimCLR) against using only the model embeddings.  The x-axis represents the class index, and the y-axis represents the change in accuracy. Positive values indicate an improvement in accuracy when using gradients compared to embeddings, and negative values indicate a decrease in accuracy. The plot visually demonstrates that different self-supervised objectives result in gradients that contain different information and affect the accuracy of different classes differently.  This highlights the potential benefit of combining gradients from various objectives, as they seem to provide complementary information.", "section": "Gradients as Features"}, {"figure_path": "PRBsEz8rnV/figures/figures_3_1.jpg", "caption": "Figure 4: Gradients extraction using a SimCLR loss. Given a pretrained backbone f and a randomly initialized projection head h, we first patchify an image, obtain the latent representations of patches (1), calculate the SimCLR loss by maximizing the pairwise cosine similarity of patches, and minimizing their similarity to a fixed negatives batch and backpropagate (2), extract the per-sample gradients (3) and finally project the gradients to the same dimensionality as the embeddings (4).", "description": "This figure illustrates the process of extracting gradients from a pretrained model using the SimCLR loss.  First, an input image is patchified (divided into smaller patches). These patches are fed through the pretrained backbone (f) to generate latent representations.  A projection head (h) further processes these representations. The SimCLR loss is then computed by maximizing similarity between patches from the same image and minimizing similarity between patches from different images (a \"fixed negative batch\" is used for comparison).  Backpropagation calculates the gradients with respect to the weights and biases of a specific layer within the backbone. Finally, these gradients are projected down to the same dimensionality as the model's output embeddings.", "section": "4 Method"}, {"figure_path": "PRBsEz8rnV/figures/figures_5_1.jpg", "caption": "Figure 6: FUNGI works across backbones. Accuracy in k-nearest neighbor classification using embeddings and FUNGI features from various ViT backbones, both for full dataset and few shot setups, averaged over 11 datasets. For the FUNGI features we chose the best performing combination across datasets. \"AR\" indicates backbones trained with the AugReg strategy (Steiner et al., 2022).", "description": "This figure demonstrates the consistent performance improvement of FUNGI across various Vision Transformer (ViT) backbones.  The results are shown for both full datasets and few-shot learning scenarios (averaged over 11 datasets). The best-performing combination of FUNGI features is used for each backbone.  The \"AR\" designation indicates backbones that were trained with the AugReg strategy.", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/figures/figures_5_2.jpg", "caption": "Figure 5: Better data-efficiency. KNN accuracy of embeddings and FUNGI (using only KL and SimCLR gradients) on ImageNet-100 using a DeIT-B/16 backbone when only k shots are used.", "description": "This figure shows the k-Nearest Neighbor accuracy on the ImageNet-100 dataset using a DeIT-B/16 backbone for different numbers of training shots (few-shot learning).  It compares the accuracy achieved using only the model embeddings against the accuracy achieved when augmenting those embeddings with features derived from KL and SimCLR gradients (FUNGI). The plot demonstrates that the FUNGI features improve accuracy, particularly in low-data scenarios (few-shot learning).", "section": "5.1 Image Classification"}, {"figure_path": "PRBsEz8rnV/figures/figures_6_1.jpg", "caption": "Figure 7: FUNGI produces sharper and more complete segmentation masks. Segmentation masks produced via nearest neighbor retrieval using DINO features (left), FUNGI (center) and the ground truth (right). Both methods use a memory bank of 1024 \u00d7 104 patches.", "description": "This figure shows a comparison of semantic segmentation results obtained using DINO features and FUNGI features.  The leftmost column displays results from DINO, the middle column shows the improvement gained by using FUNGI features, and the rightmost column shows the ground truth segmentation.  Both methods utilize a large memory bank (1024 x 104 patches) for nearest neighbor retrieval to generate these results. The images visually demonstrate that FUNGI produces better, more complete, and sharper segmentation masks than DINO.", "section": "5.2 In-Context Scene Understanding"}, {"figure_path": "PRBsEz8rnV/figures/figures_8_1.jpg", "caption": "Figure 8: Gradients from deeper layers are more predictive. Top-1 accuracy of gradients obtained from every layer of a supervised DeIT ViT-B/16 in k-nearest neighbor classification on ImageNet-100 for the KL, DINO, and SimCLR objectives. The default setup (last layers) is marked in cyan.", "description": "This figure displays the accuracy results of using gradients from different layers of a DeIT ViT-B/16 model for k-nearest neighbor classification on the ImageNet-100 dataset.  Three different self-supervised learning objectives (KL, DINO, and SimCLR) were used, and gradients from four layers within each transformer block (attn.qkv, attn.proj, mlp.fc1, and mlp.fc2) were evaluated.  The results demonstrate that gradients from deeper layers generally produce more accurate features, indicating that these layers contain more predictive information. The cyan-colored lines highlight the accuracy obtained when using gradients from the last layers (default setup), showing they're competitive with the other layer choices.", "section": "5.4 Ablation Studies"}, {"figure_path": "PRBsEz8rnV/figures/figures_19_1.jpg", "caption": "Figure 6: FUNGI works across backbones. Accuracy in k-nearest neighbor classification using embeddings and FUNGI features from various ViT backbones, both for full dataset and few shot setups, averaged over 11 datasets. For the FUNGI features we chose the best performing combination across datasets. \"AR\" indicates backbones trained with the AugReg strategy (Steiner et al., 2022).", "description": "This figure compares the performance of k-Nearest Neighbor (kNN) classification using two different feature sets: embeddings from various Vision Transformer (ViT) backbones and FUNGI features (which augment the embeddings with gradients from self-supervised losses).  The results are averaged across 11 datasets and are shown for both full datasets and low-data (\"few-shot\") scenarios.  The figure demonstrates the consistent performance improvement achieved by using FUNGI features across a range of ViT architectures, pretrained with different strategies, including those using the AugReg strategy. ", "section": "5 Experiments"}, {"figure_path": "PRBsEz8rnV/figures/figures_21_1.jpg", "caption": "Figure 10: Not all objectives produce good predictive gradients. Top-1 accuracy in k-nearest neighbor classification of gradients obtained from different self-supervised objectives using a DeIT ViT-B/16 backbone. \u201cMIM\u201d stands for masked image modeling.", "description": "This figure compares the performance of different self-supervised learning objectives in predicting the class of an image using its gradients as features.  The k-nearest neighbor classification accuracy on ImageNet-102 is shown for six different objectives: DeepCluster, DINO, KL, iBOT (No MIM), iBOT (MIM), and SimCLR.  The results indicate that not all self-supervised objectives produce equally effective gradients for this task, highlighting the impact of objective selection on gradient quality and downstream classification accuracy. MIM stands for Masked Image Modeling.", "section": "B Additional Experimental Results"}, {"figure_path": "PRBsEz8rnV/figures/figures_21_2.jpg", "caption": "Figure 11: Gains across backbone sizes. Accuracy in k-nearest neighbor image classification averaged across 11 datasets using the model embeddings and FUNGI features extracted from AugReg backbones of increasing size.", "description": "This figure shows the scalability of FUNGI across different sizes of Vision Transformers (ViTs).  The x-axis represents the ViT size (ViT-S, ViT-B, ViT-L), and the y-axis shows the accuracy achieved using both embeddings alone and FUNGI-enhanced features. The results demonstrate consistent improvements in accuracy with FUNGI across all ViT sizes, suggesting the method's generalizability and effectiveness regardless of model capacity.", "section": "5.1 Image Classification"}, {"figure_path": "PRBsEz8rnV/figures/figures_23_1.jpg", "caption": "Figure 12: SimCLR is sensitive to the number of views. The SimCLR gradients mean-per-class accuracy on Flowers102 with respect to the number of patches (left) and the images/s versus the number of patches (right) using a supervised DeIT ViT-B/16 backbone.", "description": "This figure shows two plots. The left plot shows the relationship between the number of patches used in the SimCLR loss and the resulting accuracy on the Flowers102 dataset.  The accuracy increases as the number of patches increases, but the rate of increase slows down. The right plot shows the relationship between the number of patches and the speed at which images can be processed. The speed decreases as the number of patches increases.  Both plots demonstrate a trade-off between accuracy and processing speed when using the SimCLR loss.  This highlights the importance of considering computational efficiency alongside accuracy when tuning hyperparameters in self-supervised learning.", "section": "C.1 Vision Nearest Neighbor Classification Experimental Details"}]