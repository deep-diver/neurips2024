[{"type": "text", "text": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jinlin Lai, Daniel Sheldon, Justin Domke Manning College of Information and Computer Sciences University of Massachusetts Amherst {jinlinlai,sheldon,domke}@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC). A common approach is to write the model in a probabilistic programming language and then sample via Hamiltonian Monte Carlo (HMC). However, there are many ways a user can transform a model that make inference more or less efficient. In particular, marginalizing some variables can greatly improve inference but is difficult for users to do manually. We develop an algorithm to easily marginalize random effects in LMMs. A naive approach introduces cubic time operations within an inference algorithm like HMC, but we reduce the running time to linear using fast linear algebra techniques. We show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian hierarchical models account for complicated relationships in data by introducing hierarchical structures [23]. Among hierarchical models, linear mixed effects models (LMMs) are widely used in various scientific disciplines, including ecology [31], medicine [7], psychology [41], neuroscience [77] and cognitive science [47]. Solving LMMs involves inferring latent variables, such as fixed and random effects, based on the observed data. Fixed effects are shared by all observations, while random effects vary across different groups within the data. LMMs are often implemented using probabilistic programming languages (PPLs), which isolate inference from modeling: users write a program representing the model and the PPL automatically executes a suitable inference algorithm. Variants of Hamiltonian Monte Carlo (HMC) [15] are dominant in many PPLs today and are widely used for LMMs. For example, BRMS [8] is an influential R package that allows users to write regression-style formulas that are automatically translated to Stan programs [9] representing an LMM, and then Stan\u2019s HMC implementation is called to generate posterior samples. ", "page_idx": 0}, {"type": "text", "text": "We develop techniques that allow users to easily transform their models to analytically marginalize random effect variables from LMMs to improve the efficiency of HMC. Marginalization has several benefits. First, there are often pathologies in LMMs that hinder efficient HMC sampling. A notable one is the \u201cfunnel\u201d shape created by correlation between variance parameters and parameters for fixed or random effects [45]. Marginalization [35] and other program transformations [26] have been shown to be useful in addressing such pathologies. Second, marginalization reduces the number $H$ of latent variables for HMC. The complexity of HMC is about $\\bar{O}(H^{5/4})$ [11, 46], so it is desirable to run HMC on a subset of variables if marginalization can be done efficiently. Our methods enable marginalization of random effects in LMMs with a linear Gaussian structure, which includes models with normal and log-normal likelihoods as well as other likelihoods for continuous data based on transforming a normal distribution. Note that our methods are not limited to HMC, and could be applied to many inference algorithms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "There are several challenges to efficient marginalization. The automatic marginalization algorithm of [35] can be applied to LMMs but is limited to scalar random variables, so it requires users to construct the LMM as a graphical model with separate variables for each effect and observation. Another alternative is to model the relationships between effects and observations with a design matrix and marginalize effects using properties of multivariate normal distributions. We call this the \u201cvectorized approach\u201d since it can leverage vectorization to accelerate computations. Unfortunately, vectorized marginalization leads to a dense covariance matrix over the observations and thus cubic time for evaluating the log-density within HMC, when the log-density of the original could be evaluated in linear time. Our main technical contribution is to accelerate vectorized marginalization for LMMs using fast linear algebra: we show that marginalization for a single random effect can be achieved with linear time complexity and can significantly accelerate HMC compared to both the original model and non-vectorized marginalization. ", "page_idx": 1}, {"type": "text", "text": "We implement vectorized marginalization for LMMs in NumPyro [5, 54] via simple classes users can use to express their models. We evaluate our approach on a variety of real LMMs from past scientific investigations, including nine models and datasets from cognitive sciences, and find that marginalization is always beneficial. Our findings suggest that practitioners should marginalize group-level effects whenever applicable in Bayesian hierarchical inference. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To motivate our problem, we present an example model. In [72], a set of experiments were run to examine the relationship between human pupil and attention load. A total of $N=2228$ measurements of pupil sizes from $M=20$ subjects were taken under different attention load levels. Specifically, in the ith measurement, the pupil size $y_{i}\\in\\mathbb{R}^{+}$ of subject $g_{i}\\in\\{1,2,...,k\\}$ under attention load $c_{i}\\in$ $\\{0,1,2,3,4,5\\}$ was recorded. Pupil size can be assumed to have linear relationship $y_{i}\\approx\\theta_{0}+\\theta_{1}c_{i}$ with respect to the attention load $c_{i}$ , where both the slope $\\theta_{1}$ and intercept $\\theta_{0}$ split into fixed and random effects: ", "page_idx": 1}, {"type": "equation", "text": "$$\ny_{i}=\\alpha+u_{g_{i},1}+c_{i}(\\beta+u_{g_{i},2})+\\epsilon,\\;\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\alpha,\\beta$ are variables for fixed effects and $u._{,}$ \u00b7 are variables for subject-specific random effects. Bayesian hierarchical modeling assigns priors to each unknown variable: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha\\sim\\mathcal{N}(1000,500^{2}),\\ \\beta\\sim\\mathcal{N}(0,100),\\ \\sigma\\sim\\mathcal{N}^{+}(0,1000),\\ \\mathbf{T}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(1000^{2},1000^{2})),}\\\\ &{\\quad\\quad\\quad\\mathbf{L}_{u}\\sim\\mathrm{LKJCholesky}(2,1),\\ [u_{j,1},u_{j,2}]\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{TL}_{u}\\mathbf{L}_{u}^{T}\\mathbf{T}),\\ j=1,2,...,k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "A half-normal distribution $(\\mathcal{N}^{+})$ and an LKJ distribution (LKJCholesky) [36] are used as a prior on the covariance matrix. Inference for the unknown parameters determining the relationship between pupil size and attention load can be performed by writing a probabilistic program and running HMC. For example, in NumPyro, the regression model for all measurements may be implemented as below. ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left|\\ n u m p y\\mathbf{ro}\\cdot\\mathbf{samp1e}\\left(\\mathbf{\\Phi}^{\\dagger}\\mathbf{y}^{\\prime}\\mathbf{\\Phi},\\mathbf{dist}\\cdot\\mathbf{Normal}\\left(\\mathbf{alpha}+\\mathbf{u}\\left[\\mathbf{g}\\right]\\mathbf{\\Phi}\\left[:\\mathbf{\\Lambda},\\mathbf{0}\\right]+\\mathbf{c}\\ast\\left(\\mathbf{beta}\\mathbf{+}\\mathbf{u}\\left[\\mathbf{g}\\right]\\mathbf{\\Phi}\\left[:\\mathbf{\\Lambda},\\mathbf{1}\\right]\\right)\\mathbf{\\Phi},\\mathbf{sigma}\\right)\\,,\\mathbf{obs}-\\mathbf{y}\\right)\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The code above uses advanced indexing and vectorization techniques in numpy, where ${\\mathrm{u}}\\,,{\\mathrm{g}}\\,,{\\mathsf{c}}\\,,{\\mathsf{y}}$ are all vectors or matrices. We further observe that, conditioned on $\\alpha,\\beta,\\sigma,{\\bf T},{\\bf L}_{u}$ , the distribution of all $\\mathbf{u}_{j}$ and all $y_{i}$ form a multivariate normal distribution. Theoretically it is possible to analytically integrate u out from the model to improve inference efficiency. But it is not straightforward for users to transform the probabilistic program to do so, and, as we will see, if done in the most obvious way, may not make the model more efficient for HMC. ", "page_idx": 1}, {"type": "text", "text": "To be more clear about how marginalization can be implemented, we rearrange the model into a canonical form that focuses on the random effects. All observations are collected into the vector $\\mathbf{y}=[y_{1},...,y_{N}]_{-}^{T}$ and random effects into the vector u = [u1,1, u1,2, ..., uk,1, uk,2]T Then, we can write ", "page_idx": 1}, {"type": "image", "img_path": "uXuObobJHO/tmp/3ea6274cfc677d380f94ce0ddb96283d0726c6e9bb194e689fa021fb7dea0a94.jpg", "img_caption": ["Figure 1: A tree-structured model conditioned on $\\Theta$ . "], "img_footnote": [], "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{u}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}_{\\mathbf{u}}),\\ \\mathbf{y}\\sim\\mathcal{N}(\\mathbf{A}\\mathbf{u}+\\mathbf{b},\\boldsymbol{\\Sigma}_{\\mathbf{y}}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mu,\\Sigma_{\\mathbf{u}},\\mathbf{A},\\mathbf{b},\\Sigma_{\\mathbf{y}}$ are functions of $\\alpha,\\beta,\\sigma,{\\bf T},{\\bf L}_{u},g_{i},c_{i}$ . Note that $y_{i}$ only depends on the entry $\\mathbf{u}_{g_{i}}$ of $\\mathbf{u}$ . The corresponding graphical model has a tree structure, as demonstrated in Figure 1. This tree structure has several benefits: first, matrix multiplications like Au and $\\mathbf A^{T}\\mathbf y$ can be done efficiently; second, we will see that it leads to a block-diagonal structure that facilitates efficient inversion in a key matrix that appears later. ", "page_idx": 2}, {"type": "text", "text": "For more general LMMs with more than one class of random effects we generalize the canonical form as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Theta\\sim p(\\Theta),\\quad{\\bf u}_{i}|\\Theta\\sim\\mathcal{N}(\\mu_{i}(\\Theta),\\Sigma_{{\\bf u}_{i}}(\\Theta)),\\quad i=1,2,...,L}}\\\\ {{\\displaystyle\\mathbf{y}|\\Theta,{\\bf u}_{1},{\\bf u}_{2},...{\\bf u}_{L}\\sim\\mathcal{N}\\left(\\sum_{i=1}^{L}{\\bf A}_{i}(\\Theta){\\bf u}_{i}+{\\bf b}(\\Theta),\\Sigma_{{\\bf y}}(\\Theta)\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p(\\Theta)$ is the distribution for global variables (including fixed effects), $p(\\mathbf{u}_{i}|\\pmb{\\Theta})$ is the distribution for random effects and $p(\\mathbf{y}|\\Theta,\\mathbf{u}_{1},...,\\mathbf{u}_{L})$ is the distribution for observations. Notationally this generalization further adds an index to each random effect to specify its class. A user might specify the model directly in this canonical form, or in another syntax (e.g., the formula syntax of BRMS) that is compiled to this form. Each pair $(\\mathbf{u}_{i},\\mathbf{A}_{i})$ specifies a class of random effects for a particular classification of the observations (e.g., by subject, age, gender, etc.). Each classification contains multiple groups and different classifications are distinct from one another. Each observation belongs to one group for each classification. The vector $\\mathbf{u}_{i}=[\\mathbf{u}_{i,1}^{T},\\mathbf{u}_{i,2}^{T},...,\\mathbf{u}_{i,k_{i}}^{T}]^{T}$ contains random effects for the ith classification (e.g., subject, age, or gender), consisting of $k_{i}$ groups (e.g., one subject, age, or gender), with ${\\bf{u}}_{i,j}$ containing the random effects (e.g., slope and intercept) for the $j$ th group. We denote the number of observations as $\\dim(\\mathbf{y})=N$ , and the number of random effects per group as $\\mathrm{dim}(\\mathbf{u}_{i,j})=d$ . Any covariates\u2014such as $c_{i}$ in the pupil size example\u2014are considered constants and not represented in the notation. In LMMs, the number $d$ is related to the number of covariates and is usually small. The total number of random effects for $\\mathbf{u}_{i}$ is denoted as $\\mathrm{dim}(\\mathbf{u}_{i})=M_{i}=k_{i}d$ . The matrix ${\\bf A}_{i}$ therefore has size $N\\times M_{i}$ , and encodes the group structure for $\\mathbf{u}_{i}$ by mapping random effects (together with covariates) to observations. Each row of $\\mathbf{A}_{i}$ encodes the assignment of an observation to one group, so it has at most $d$ nonzero elements. Therefore, the complexity of computing ${\\bf A}_{i}{\\bf u}_{i}$ is ${\\mathcal{O}}(N d)$ , as A has at most $N d$ nonzero elements. Henceforth, we omit the dependence on $\\Theta$ for $\\pmb{\\mu}$ , $\\pmb{\\Sigma_{\\mathbf{u}}}$ , A, b, $\\Sigma_{\\mathbf{y}}$ for simplicity. ", "page_idx": 2}, {"type": "text", "text": "Marginalizing $\\mathbf{u}_{i}$ It is possible to analytically marginalize variables in this model: since the mean of $\\mathbf{y}$ is linear in each $\\mathbf{u}_{i}$ and all of these variables are normally distributed, the joint distribution of $(\\mathbf{y},\\mathbf{u}_{1},\\dots,\\mathbf{u}_{L})$ is also multivariate normal. We will focus for most of the paper on marginalizing the random effects $\\mathbf{u}_{i}$ for a single $i$ in order to leverage the tree structure mentioned earlier, but return in Section 4 to the idea of marginalizing many effects. Locally, $\\mathbf{u}_{i}$ and y form the conditional distribution $p(\\mathbf{u}_{i},\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})=p(\\mathbf{\\bar{u}}_{i}|\\Theta)p(\\mathbf{\\bar{y}}|\\Theta,\\mathbf{\\bar{u}}_{-i},\\mathbf{u}_{i})$ . Marginalized MCMC rewrites this conditional distribution as $p(\\mathbf{u}_{i},\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})\\,=\\,p(\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})p(\\mathbf{u}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ , which reverses the dependence between $\\mathbf{u}_{i}$ and y [35]. During sampling, $\\mathbf{u}_{i}$ is marginalized from the HMC procedure by using $p(\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})$ as the likelihood function and $p(\\Theta,{\\mathbf{u}}_{-i})$ as the distribution of latent variables. After HMC sampling, $\\mathbf{u}_{i}$ is recovered through ancestral sampling from $p(\\mathbf{u}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ given posterior samples of $(\\Theta,\\mathbf{u}_{-i})$ . The reversal requires analytical forms of $p(\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})$ and $p(\\mathbf{u}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ , which can be obtained via standard marginalization and conditioning operations on multivariate normal distributions [e.g., 6] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{y}|\\boldsymbol{\\Theta},\\mathbf{u}_{-i}\\sim\\mathcal{N}\\left(\\sum_{j\\neq i}\\mathbf{A}_{j}\\mathbf{u}_{j}+\\mathbf{A}_{i}\\mu_{i}+\\mathbf{b},\\mathbf{A}_{i}\\Sigma_{\\mathbf{u}_{i}}\\mathbf{A}_{i}^{T}+\\Sigma_{\\mathbf{y}}\\right),}\\\\ &{\\displaystyle\\mathbf{u}_{i}|\\boldsymbol{\\Theta},\\mathbf{y},\\mathbf{u}_{-i}\\sim\\mathcal{N}\\left(\\mu_{i}+\\mathbf{M}\\left(\\mathbf{y}-\\sum_{j\\neq i}\\mathbf{A}_{j}\\mathbf{u}_{j}-\\mathbf{A}_{i}\\mu_{i}-\\mathbf{b}\\right),(\\mathbf{I}-\\mathbf{M}\\mathbf{A}_{i})\\Sigma_{\\mathbf{u}_{i}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{M}=\\Sigma_{\\mathbf{u}i}\\mathbf{A}_{i}^{T}(\\mathbf{A}_{i}\\Sigma_{\\mathbf{u}i}\\mathbf{A}_{i}^{T}+\\Sigma_{\\mathbf{y}})^{-1}$ . Marginalization introduces the benefit of sampling in a lower dimensional space, but the cost depends on the complexity of evaluating the log-density functions of these two distributions in order to run HMC. ", "page_idx": 2}, {"type": "table", "img_path": "uXuObobJHO/tmp/2296ba8a59dfbe5f69a6c1a17751674aa05864c4c6cd26f7590cd19dbea5735e.jpg", "table_caption": ["Table 1: Time complexities of different HMC approaches for the submodel involved in marginalization. Initialization is done once before the HMC loop. The log density is computed within each step of the leapfrog integrator. Recovery is performed for each sample from HMC. $N$ is the number of observations, $M$ is the dimension for one class of random effects, $D$ is the dimension for all classes of random effects, $L$ is the number of classes, $d$ is the dimension for an effect of a group in a class. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.1 Challenges of multivariate marginalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, the original model usually has structure that makes evaluating its density very efficient, which is lost by naive marginalization. For example, the observations in y are usually conditionally independent, making $\\Sigma_{\\mathbf{y}}$ diagonal; also, $\\pmb{\\Sigma}_{\\mathbf{u}i}$ is usually block diagonal with blocks of size $d\\times d$ . So evaluating the density $p(\\mathbf{u}_{i},\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})\\,=\\,p(\\mathbf{u}_{i}|\\Theta)p(\\mathbf{y}|\\Theta,\\mathbf{u}_{1:L})$ requires $\\mathcal{O}(k_{i}d^{3}+N L d)=$ $\\mathcal{O}(M_{i}d^{2}+\\bar{N}L d)$ time with the main operations being (1) inverting and computing the determinant of $\\pmb{\\Sigma_{\\mathbf{u}}}$ and $\\Sigma_{\\mathbf{y}}$ ; (2) computing the mean parameter of $\\mathbf{y}$ . When $\\pmb{\\Sigma}_{\\mathbf{u}i}$ is diagonal, the complexity goes down to $\\mathcal{O}(\\bar{M_{i}}d+N L d)$ . However, it is more expensive to evaluate the density of the reversed model in Equation (2). Computing $p(\\mathbf{y}|\\mathbf{\\Theta}\\mathbf{,u}_{-i})$ and $p(\\mathbf{\\bar{u}}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ requires the inverting and computing the determinant of the $N\\times N$ matrix $\\mathbf{A}_{i}\\pmb{\\Sigma}_{\\mathbf{u}_{i}}\\mathbf{A}_{i}^{T}+\\pmb{\\Sigma}_{\\mathbf{y}}$ , which we denote by $\\mathbf{E}$ for simplicity. For the log likelihood, we need to compute $\\begin{array}{r}{\\log p(\\mathbf{y}|\\Theta,\\mathbf{u}_{-i}^{-})=-\\frac{1}{2}\\operatorname*{det}\\left(\\mathbf{E}\\right)-\\frac{1}{2}\\mathbf{z}^{T}\\mathbf{E}^{-1}\\mathbf{z}+C,}\\end{array}$ , where $\\begin{array}{r}{\\mathbf{z}\\;=\\;{\\bar{\\mathbf{y}}}\\,-\\,\\sum_{j\\neq i}\\mathbf{A}_{j}\\mathbf{u}_{j}\\,-\\,\\mathbf{A}_{i}\\mu_{i}\\,-\\,{\\bar{\\mathbf{b}}}}\\end{array}$ . $\\mathbf{E}$ is not diagonal and without using additional structure will trigger $\\mathcal{O}(N^{3})$ operations within each step of the leapfrog integrator within HMC. For the recovery distribution $p(\\mathbf{u}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ , $\\mathbf{E}$ will be inverted when calculating M. Also, a Cholesky decomposition for the covariance $(\\mathbf{I}-\\mathbf{M}\\mathbf{A}_{i})\\pmb{\\Sigma}_{\\mathbf{u}i}$ should be computed for sampling, which takes $\\mathcal{O}(M_{i}^{3})$ time. These cubic time operations are prohibitively expensive for large datasets. We summarize the complexities of different approaches in Table 1. In Section 3, we discuss how to marginalize one group of random effects with lemmas from linear algebra. In Section 4, we discuss how to marginalize all random effects with additional assumptions. ", "page_idx": 3}, {"type": "text", "text": "3 Marginalization with fast linear algebra ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now show how to speed up calculations with the marginalized model using fast linear algebra methods. In particular, we use the matrix inversion lemma and matrix determinant lemma together with special structure in the relevant matrices. In this section, we sometimes omit the subscript $i$ such as for $\\mathbf{A}_{i}$ and $\\pmb{\\Sigma}_{\\mathbf{u}i}$ for simplicity. The steps in log density evaluation and recovery are summarized in Algorithm 1, and in Algorithm 2 in the appendix, with comments about their implementation and cost. We mainly use sparsity and tree-structure in $\\mathbf{A}$ to make operations faster. As an overview, computing ${\\bf z}$ takes $\\mathcal{O}(N L d)$ time for $L$ sparse matrix multiplications of time ${\\mathcal{O}}(N d)$ each. Also, evaluating As and $\\mathbf{A}^{T}\\mathbf{t}$ both take $O(N d)$ for any $\\mathbf{s}\\in\\mathbb{R}^{M}$ and any $\\mathbf{t}\\in\\mathbb{R}^{N}$ . With tree-structure, we will see that $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ is block-diagonal and can be computed efficiently. ", "page_idx": 3}, {"type": "text", "text": "3.1 Matrix inversion and determinant lemmas in marginalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The two main bottlenecks when evaluating $\\log p(\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})$ are computing $\\operatorname*{det}(\\mathbf{E})$ and ${\\bf z}^{T}{\\bf E}^{-1}{\\bf z}$ With the matrix determinant lemma [32], we have that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{det}(\\mathbf{E})=\\operatorname*{det}(\\mathbf{A}\\Sigma_{\\mathbf{u}}\\mathbf{A}^{T}+\\Sigma_{\\mathbf{y}})=\\operatorname*{det}(\\Sigma_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A})\\operatorname*{det}(\\Sigma_{\\mathbf{u}})\\operatorname*{det}(\\Sigma_{\\mathbf{y}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By the matrix inversion lemma or the Woodbury formula [53] we have that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{E}^{-1}=(\\mathbf{A}\\boldsymbol{\\Sigma}_{\\mathbf{u}}\\mathbf{A}^{T}+\\boldsymbol{\\Sigma}_{\\mathbf{y}})^{-1}=\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}-\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big(\\boldsymbol{\\Sigma}_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big)^{-1}\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}^{T}\\mathbf{E}^{-1}\\mathbf{z}=\\mathbf{z}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{z}-\\mathbf{z}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big(\\boldsymbol{\\Sigma}_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big)^{-1}\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{z}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "uXuObobJHO/tmp/f8b901f79cdf9858b5f900a8d954635182315e92c0c6e9a0b42ed134300a539a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "By using the facts that $\\pmb{\\Sigma_{\\mathbf{u}}}$ is block-diagonal, $\\Sigma_{\\mathbf{y}}$ is diagonal, and $\\mathbf{A}$ has $N d$ nonzero elements, the quantities $\\operatorname*{det}(\\Sigma_{\\mathbf{u}}),\\,\\operatorname*{det}(\\Sigma_{\\mathbf{y}}),\\,{\\mathbf{z}}^{T}\\Sigma_{\\mathbf{y}}^{-{\\mathbf{1}}}{\\mathbf{z}}$ , and $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{z}$ can each be calculated in $\\mathcal{O}(M d^{2}+N d)$ time. Equations (3) and (4) contain the expressions ${\\bf F}^{-1}$ or $\\operatorname*{det}(\\mathbf{F})$ for the $M\\times M$ matrix $\\mathbf{F}:=\\Sigma_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ , which both require $\\mathcal{O}(\\bar{M}^{3})$ time when done naively. The following theorem shows that these quantities can be computed in $\\mathcal{O}((M+N)d^{2})$ for LMMs. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. If $\\Sigma_{\\mathbf{y}}$ is diagonal, $\\pmb{\\Sigma_{\\mathbf{u}}}$ is block-diagonal with blocks of size $d\\times d,$ , then $\\mathbf{F}=\\Sigma_{\\mathbf{u}}^{-1}+$ $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ is also block-diagonal with $d\\times d$ blocks and computing $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ takes $O(N d^{2})$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. The proof uses the tree-structure in A. For details, see Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "Therefore, it is $\\mathcal{O}((M+N)d^{2})$ to compute $\\operatorname*{det}(\\mathbf{F})$ and ${\\bf F}^{-1}$ . Combined with other parts in the formulas, the overall complexity is $\\mathcal{O}(\\bar{M}d^{2}+N\\dot{L}\\dot{d}+N d^{2})$ . In LMMs, $d$ is usually small, so the complexity with marginalization can be viewed as the same as the complexity without marginalization. ", "page_idx": 4}, {"type": "text", "text": "3.2 Speeding up the recovery step ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Different from evaluating $\\log p(\\mathbf{y}|\\Theta,\\mathbf{u}_{-i})$ , ancestral sampling from $p(\\mathbf{u}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ is only performed once for each posterior sample. When sampling from $p(\\mathbf{u}_{i}|\\Theta,\\mathbf{y},\\mathbf{u}_{-i})$ , computing $\\mathbf{M}$ directly is also costly. With the matrix inversion lemma, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}=\\pmb{\\Sigma_{\\mathbf{u}}}\\mathbf{A}^{T}(\\mathbf{A}\\pmb{\\Sigma_{\\mathbf{u}}}\\mathbf{A}^{T}+\\pmb{\\Sigma_{\\mathbf{y}}})^{-1}}\\\\ &{\\quad=\\pmb{\\Sigma_{\\mathbf{u}}}\\mathbf{A}^{T}\\pmb{\\Sigma_{\\mathbf{y}}^{-1}}-\\pmb{\\Sigma_{\\mathbf{u}}}\\mathbf{A}^{T}\\pmb{\\Sigma_{\\mathbf{y}}^{-1}}\\mathbf{A}(\\pmb{\\Sigma_{\\mathbf{u}}^{-1}}+\\mathbf{A}^{T}\\pmb{\\Sigma_{\\mathbf{y}}^{-1}}\\mathbf{A})^{-1}\\mathbf{A}^{T}\\pmb{\\Sigma_{\\mathbf{y}}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With this expression, the mean variable $\\pmb{\\mu}+\\mathbf{M}\\mathbf{z}$ , then is evaluated in $\\mathcal{O}((M+N)d^{2})$ , by computing $\\pmb{\\Sigma_{\\mathbf{u}}^{-1}}\\!+\\!\\mathbf{A}^{T}\\pmb{\\dot{\\Sigma}_{\\mathbf{y}}^{-1}}\\mathbf{A}$ in the same way as Line 2 of Algorithm 1. For the covariance variable $(\\mathbf{I}{-}\\mathbf{M}\\mathbf{A})\\Sigma_{\\mathbf{u}}$ , we have from substituting Equation 5 that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{I}-\\mathbf{MA})\\boldsymbol{\\Sigma}_{\\mathbf{u}}=\\boldsymbol{\\Sigma}_{\\mathbf{u}}-\\boldsymbol{\\Sigma}_{\\mathbf{u}}\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\boldsymbol{\\Sigma}_{\\mathbf{u}}+\\boldsymbol{\\Sigma}_{\\mathbf{u}}\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big(\\boldsymbol{\\Sigma}_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big)^{-1}\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\boldsymbol{\\Sigma}_{\\mathbf{u}}}\\\\ {=\\boldsymbol{\\Sigma}_{\\mathbf{u}}\\big(\\boldsymbol{\\Sigma}_{\\mathbf{u}}^{-1}-\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}+\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big(\\boldsymbol{\\Sigma}_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big)^{-1}\\mathbf{A}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}\\big)\\boldsymbol{\\Sigma}_{\\mathbf{u}}.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $\\Sigma_{\\mathbf{u}}^{-1}$ , $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ , and $\\mathbf{F}\\,=\\,\\Sigma_{\\mathbf{u}}^{-1}+\\mathbf{A}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ are all block diagonal, so the result of adding and multiplying them is also block diagonal. For a block diagonal matrix with $k$ blocks of size $d\\times d$ , the time complexity for a Cholesky decomposition is $\\mathcal{O}(k d^{3})=\\mathcal{O}(M d^{2})$ . Combined with the complexity of computing ${\\bf z}$ , the recovery step takes $\\mathcal{O}(M d^{2}+N L d+N d^{2})$ time. ", "page_idx": 4}, {"type": "text", "text": "4 Marginalizing multiple effects with additional assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We have shown that it is efficient to marginalize one class of random effects. With additional practical assumptions, it is possible to marginalize all classes of random effects for efficient HMC inference. Instead of separating different classes of random effects, LMMs can also be written as $\\mathbf{v}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},\\pmb{\\Sigma}_{\\mathbf{v}})$ , $\\mathbf{y}\\sim\\mathcal{N}(\\mathbf{B}\\bar{\\mathbf{v}}+\\mathbf{b},\\pmb{\\Sigma}_{\\mathbf{y}})$ , where $\\mathbf{B}=[\\mathbf{A}_{1},...,\\mathbf{A}_{L}]$ and $\\mathbf{v}=[\\mathbf{u}_{1}^{T},...,\\mathbf{u}_{L}^{T}]^{T}$ . We define that $\\textstyle D\\,=\\,\\sum_{i=1}^{L}M_{i}$ . The matrix inversion and determinant lemmas can still be applied to marginalize $\\mathbf{v}$ out, but the combined matrix does not have the special structure of $\\mathbf{A}_{i}$ we exploited in Section 3. More specifically, the computation of $\\operatorname*{det}(\\mathbf{F})$ and the evaluation of ${\\bf F}^{-1}$ for $\\begin{array}{r}{\\mathbf{F}\\dot{=}\\,\\pmb{\\Sigma}_{\\mathbf{v}}^{-1}+\\mathbf{B}^{T}\\pmb{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{B}}\\end{array}$ both become non-trivial. We introduce additional assumptions to show that they can be solved faster in some special cases. For the general case, see the discussion section. ", "page_idx": 4}, {"type": "text", "text": "The assumption we make is that $\\Sigma_{\\mathbf{v}}=\\tau_{\\mathbf{v}}\\mathbf{I}$ and $\\Sigma_{\\mathbf{y}}=\\tau_{\\mathbf{y}}\\mathbf{I}$ , where $\\tau_{\\mathbf{v}},\\tau_{\\mathbf{y}}$ are scalars that either belong to $\\Theta$ or are fixed non-random parameters. This means that all effects share the same variance and all observations share the same noise scale. These assumptions are not as restrictive as it may appear. If the underlying distribution is $\\mathbf{u}_{i}\\sim\\mathcal{N}(\\pmb{\\mu}_{i},\\pmb{\\sigma}_{i}^{2}\\mathbf{I})$ where $\\sigma_{i}$ is a fixed parameter, it is possible to reparameterize this distribution as $\\mathbf{u}_{i}^{\\prime}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , $\\mathbf{A}_{i}^{\\prime}=\\sigma_{i}\\mathbf{A}_{i}$ , $\\mathbf{b}^{\\prime}=\\mathbf{b}+\\mathbf{\\bar{B}}\\mu_{i}$ , and use $\\ensuremath{\\mathbf{u}}_{i}^{\\prime}$ , $,\\mathbf{A}_{i}^{\\prime},\\mathbf{b}^{\\prime}$ in place of $\\mathbf{u}_{i},\\mathbf{A}_{i},$ b. Then $\\Sigma_{\\mathbf{v}}$ becomes a scaled identity matrix. Also, in many models, the noise scale for different observations is the same, making $\\Sigma_{\\mathbf{y}}$ a scaled identity matrix as well. ", "page_idx": 5}, {"type": "text", "text": "In practice, if the assumptions are satisfied, marginalization can be done in $\\mathcal{O}(D^{2}+N d)$ time with $\\mathcal{O}(D^{3}+N L^{2}d^{2})$ preprocessing. Details are provided in Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While many works aim to improve HMC directly [71, 30, 58, 73], a number of other works focus on model transformation. Non-centered parameterization [49] is a widely used trick among MCMC users to alleviate slow sampling in difficult posterior distributions. However, there is no general way to know whether a non-centered parameterization will be beneficial [76]. Variationally inferred parameterization [26] proposes to learn a model parameterization from a specified family that will lead to effective sampling. In Parno and Marzouk [52] and Hoffman et al. [33], preconditioners for HMC are learned to transform the model to be approximately isotropic Gaussians. Marginalization differs from reparameterization in that it reduces the problem dimension as well as potentially alleviating difficult characteristics such as funnels, so it has two mechanisms to improve MCMC efficiency. The Laplace approximation (LA) is one way to approximately marginalize variables in MCMC [59, 40, 65], but it may be difficult to quantify the error or recover the marginalized variables. ", "page_idx": 5}, {"type": "text", "text": "Marginalization, or Rao-Blackwellization, has been an important topic in Bayesian inference and probabilistic programming. In Gibbs sampling, marginalization is usually called collapsing [37]. Collapsed Gibbs sampling has been developed for latent Dirichlet allocation [56] and LMMs [50]. We explore marginalization in the context of HMC, which induces different considerations. Methods with HMC do not have to make the conditional distributions of the marginalized model tractable. Marginalization is also related to symbolic inference in probabilistic programming. Hakaru [44] and PSI [21, 22] are systems for performing exact Bayesian inference by symbolically marginalizing all latent variables. To marginalize discrete variables, Gorinova et al. [27] propose an information flow type system. Another line of related work is delayed sampling [43, 3], which automates marginalization of variables within Rao-Blackwellized particle fliters [42]. Lai et al. [35] developed an automatic system for marginalizing variables in HMC, but is limited to scalar variables so cannot leverage vectorization and forces users to write models with univariate distributions. ", "page_idx": 5}, {"type": "text", "text": "Linear algebra tricks have been widely utilized in various machine learning algorithms, such as ridge regression [68], Gaussian processes [61] and Kalman fliters [60]. Recently, frameworks [62, 20, 57] have been proposed to ease the implementation of fast linear algebras in machine learning algorithms. Marginalization in Bayesian models may be an interesting application of those frameworks. ", "page_idx": 5}, {"type": "text", "text": "Fast and scalable inference for LMMs has been studied in the context of maximum likelihood estimation [19], variational EM [24], Gibbs sampling [51] and numerical integration [28]. We are the first to consider speeding up the inference of LMMs with HMC. There is also a recent trend in integrating random effects into deep neural networks for correlated data [67] or personalization [66, 64, 74] with parameters estimated by maximum likelihood. ", "page_idx": 5}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments on LMMs from various disciplines using the default no-U-turn sampler (NUTS) [34] from NumPyro [5, 54], which has an adaptive step size with dual averaging, adaptive and diagonal mass matrix, target acceptance probability of 0.8, and maximum tree depth of 10. For the ETH instructor evaluation model, we set the maximum tree depth to 12 to overcome difficulties performing inference without marginalization in preliminary experiments. For all models, we use weakly informative priors unless specified. In general, our conclusion is insensitive to the choice of hyperparameters and priors. For all experiments, we collect 10,000 warm up samples for tuning, and 100,000 samples for evaluation, and evaluate performance via effective sample size (ESS) and running time. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Running time in seconds for HMC, with or without marginalization. Mean and standard deviation over 5 independent runs are reported. Experiments are run on NVIDIA A40. ", "page_idx": 6}, {"type": "table", "img_path": "uXuObobJHO/tmp/73a71d893403d0f80eb69c3a7d3289bc059a77f6f427708b67699fa90f699d52.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "uXuObobJHO/tmp/a29662504e40512f4c3abcefe46fd6366127d5e047c9c0de849028e34bbebcf1.jpg", "img_caption": ["Figure 2: Average ESS for each variable on the instruction evaluation model with different HMC strategies. Numbers above the sample size 100,000 indicate effective sampling. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "6.1 Marginalization in cross-effects models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Cross-effects models are a type of LMM that have more than one class of random effects (i.e. $L>1$ ). Usually each observation belongs to one subject group (e.g. individuals, animals) and one item group (e.g. questions, objects). The correlation among latent variables can create severely challenging geometry that slows down the sampling of HMC. With our idea, it is possible to marginalize one or more group of effects from the model, reducing the dimension of latent space for faster sampling and better geometry. ", "page_idx": 6}, {"type": "text", "text": "ETH instructor evaluations An example cross-effects model describes university lecture evaluations by students at ETH [4]. The dataset records $N\\,=\\,73421$ ratings, where each rating $y_{n}$ comes from student $s_{n}$ for professor $p_{n}$ teaching a course from department $d_{n}$ , with $t_{n}$ indicating whether the professor is teaching outside their own department. There are a total of $M_{1}=2972$ students, $M_{2}=1128$ professors and $M_{3}=14$ departments. We use a version of the model from the document of Tensorflow probability [13]. The model is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Likelihood}:y_{n}\\sim{\\mathcal{N}}(u_{1,s_{n}}+u_{2,p_{n}}+u_{3,d_{n}}+\\alpha+\\beta t_{n},\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\nu_{1,i}\\sim\\mathcal{N}(0,1),\\,\\,u_{2,j}\\sim\\mathcal{N}(0,1),\\,\\,u_{3,k}\\sim\\mathcal{N}(0,1),\\,\\,\\alpha\\sim\\mathcal{N}(0,5),\\,\\,\\beta\\sim\\mathcal{N}(0,1),\\,\\,\\sigma\\sim\\mathcal{N}^{+}(0,1).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $1\\leq i\\leq M_{1}$ , $1\\leq j\\leq M_{2}$ and $1\\leq k\\leq M_{3}$ . Given the dataset, we wish to learn about the latent variables $\\mathbf{u}_{1}$ , $\\mathbf{u}_{2}$ , u3, $\\alpha$ , $\\beta$ and $\\sigma$ . HMC is the most direct way to sample those variables, but the dimension and complicated relations make it inefficient. Marginalization can be applied to one of the effects, $\\mathbf{u}_{1}$ , $\\mathbf{u}_{2}$ or $\\mathbf{u}_{3}$ . We report the running time of sampling from the model with and without marginalization in Table 2. We found that marginalizing any group of random effects improves the sampling speed of HMC. However, the improvements are not necessarily predicted by the dimension of marginalized variable: HMC is faster when marginalizing $\\mathbf{u}_{3}$ than when marginalizing $\\mathbf{u}_{1}$ even though $\\mathbf{u}_{1}$ has 200-times higher dimension than $\\mathbf{u}_{3}$ . In Figure 2, the ESS for each variable is reported. Without marginalization, sampling $\\mathbf{u}_{2}$ and $\\mathbf{u}_{3}$ are both difficult compared to sampling $\\mathbf{u}_{1}$ , and HMC becomes more efficient when marginalizing either of these variables, so we conjecture that $\\mathbf{u}_{2}$ and $\\mathbf{u}_{3}$ are responsible for the difficulty for sampling in the original model. In this model, all random effects are independent and have the same variance, so $\\pmb{\\Sigma_{\\mathbf{u}}}$ is a scaled identity matrix and we can marginalize all random effects efficiently. This approach is observed to be the most efficient in our experiments, despite having quadratic complexity in $D$ . Overall, marginalization never hurts ESS, and runs faster. We expect that any marginalization strategy works better than HMC in the original model, a finding which will be consistent across experiments. Additional results of this experiment, including trace plots and R\u02c6 diagnosis, are included in Figure 6 and Table 5 in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "6.2 Marginalization vs reparameterization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To tackle bad geometry in statistical models, another model transformation is non-centered parameterization, or reparameterization [49]. Reparameterization converts the distribution of $z\\sim\\dot{\\mathcal{N}}(\\mu,\\sigma^{2})$ into $\\epsilon\\sim\\mathcal{N}(0,\\bar{1})$ and $z\\,=\\,\\epsilon\\sigma\\,+\\,\\mu$ . Reparameterization is especially useful for funnel shapes in hierarchical models. We note that when applicable, marginalization is able to solve a broader class of problems. We compare marginalization and reparameteriation on the grouse ticks model. ", "page_idx": 6}, {"type": "image", "img_path": "uXuObobJHO/tmp/7b383865ebd4a12f94810697ba5159193767df72ceaf6e2ee8651b2f62e58d93.jpg", "img_caption": ["Figure 3: Distribution of 10,000 samples for variable pairs $(\\sigma_{1},u_{1,1})$ and $(\\sigma_{2},u_{2,61})$ on the grouseticks model with different methods. We use M1 to represent marginalizing $\\mathbf{u}_{1}$ , M2 to represent marginalizing $\\mathbf{u}_{2}$ , R1 to represent reparameterizing $\\mathbf{u}_{1}$ , R2 to represent reparameterizing $\\mathbf{u}_{2}$ . The number of divergences for each case are reported, with locations shown as red dots. We choose $u_{2,61}$ to demonstrate the distribution of divergences when reparameterizing $\\mathbf{u}_{2}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "uXuObobJHO/tmp/efee2de79a5422756b12158d2e2d6ccb66a8fd593077c3c23b9a02f15fd551fc.jpg", "table_caption": ["Table 3: Compilation time $T_{c}$ and running time $T_{r}$ in seconds for marginalized MCMC [35], with or without vectorization. Mean and std across 5 independ runs are reported. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Grouse ticks The dataset [4] contains observations $\\mathbf{y}$ of the the number of ticks on the heads of red grouse chicks in the field. Each observation $y_{k}$ comes from brood $b_{k}$ in location $l_{k}$ during year $e_{k}$ at altitude $a_{k}$ , where year and altitude give fixed effects, and there are random effects $\\mathbf{u}_{1}$ and $\\mathbf{u}_{2}$ corresponding to brood and location. There are $N=403$ observations, $M_{1}=118$ broods and $M_{2}=63$ locations. We define the hierarchical model as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Likelihood:~}y_{k}\\sim\\mathcal{N}(u_{1,b_{k}}+u_{2,l_{k}}+\\beta_{e}e_{k}+\\beta_{a}a_{k},\\sigma_{t}^{2})\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{}&{\\mathrm{Prior:~}\\mu_{1}\\sim\\mathcal{N}(0,1),\\ \\sigma_{1}\\sim\\mathrm{HalfCauchy(5),~}\\mu_{2}\\sim\\mathcal{N}(0,1),\\ \\sigma_{2}\\sim\\mathrm{HalfCauchy~}\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{}&{\\beta_{e}\\sim\\mathcal{N}(0,1),\\ \\beta_{a}\\sim\\mathcal{N}(0,1),\\ u_{1,i}\\sim\\mathcal{N}(\\mu_{1},\\sigma_{1}^{2}),\\ u_{2,j}\\sim\\mathcal{N}(\\mu_{2},\\sigma_{2}^{2}),\\sigma_{t}\\sim\\mathrm{Half}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $i=1,...,M_{1}$ , $j=1,...,M_{2}$ , $k=1,...,N$ and each $y_{k}$ is observed. The correlation between $\\sigma$ and $\\mathbf{u}$ creates the funnel shape that makes vanilla HMC inefficient. Nevertheless, it is possible to apply either marginalization or reparameterization to each random effect. In Figure 3, we plot the distributions of samples for variable pairs $(\\sigma_{1},u_{1,1})$ and $(\\sigma_{2},u_{2,1})$ with different combinations of marginalization and reparameterization. There is a difficult correlation between $\\sigma_{2}$ and $\\mathbf{u}_{2}$ . After applying marginalization or reparameterization to $\\mathbf{u}_{2}$ , HMC manages to explore the funnel region (at low values of $\\sigma_{1}$ ). However, we find that only samplers that marginalize $\\mathbf{u}_{2}$ report zero divergent transitions after warm-up. Such behavior is consistent with different random seeds. See Table 6 in the Appendix. Also, the distribution of divergent samples is related to specific parameters when reparameterizing $\\mathbf{u}_{2}$ , implying that reparameterization introduces pathologies that create challenges for HMC inference. In addition, we find that reparameterization does not improve the running time of HMC, while marginalizing $\\mathbf{u}_{2}$ speeds up sampling by about $20\\%$ . ", "page_idx": 7}, {"type": "text", "text": "6.3 Benefits from vectorization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In theory, marginalization with LMMs can be done by constructing a graphical model for scalar random variables and performing automatic marginalization as in [35]. But it is more efficient to marginalize in a vectorized way. We demonstrate the benefits from vectorization in Table 3. Both marginalization strategies are performed on two hierarchical linear regression models, the electric company model [23] and the pulmonary fibrosis model [63]. We find that vectorized marginalization is much more efficient for sampling from the two models. ", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 4: Specifications of the datasets from cognitive sciences. Details of each model are provided in Appendix D. GPU models run on an NVIDIA RTX 2080ti GPU. CPU models run on one Intel Xeon Gold 6148 processor. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "uXuObobJHO/tmp/f5f00d83dab5688ba9f3fc84564d23f60164ea7a4d586a0f8500dfd13ced033c.jpg", "img_caption": ["Figure 4: Experimental results for the 9 cognitive science datasets with and without marginalization. Each experiment is performed 5 times with different random seeds. Marginalization usually improves sampling speed measured by iterations per second (iter/s) and sample efficiency measured by ESS per iteration (ESS/iter). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.4 Applications in cognitive sciences ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Hierarchical Bayesian inference with LMMs has wide applications in cognitive science [47]. We highlight the effectiveness of marginalization with 9 datasets from cognitive science (Table 4). They cover various settings, with one or two random effects, normal or log-normal likelihoods, on CPU or GPU. Experiments that are slow on CPU are performed on GPU. Each dataset corresponds to an LMM where both the intercept and the coefficient include random effects. Details of all the models can be found in Appendix D. Results are summarized in Figure 4. Marginalization usually improves the sampling speed of HMC and consistently improves efficiency measured by ESS per iteration. ", "page_idx": 8}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There are several promising directions for future work. ", "page_idx": 8}, {"type": "text", "text": "7.1 Marginalization vs Rao-Blackwellization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Marginalization is related to Rao-Blackwellization. This paper focuses on marginalization, which improves the speed of obtaining samples from the remaining variables by improving mixing times, reducing the cost per iteration, or both. Combining marginalization with Rao-Blackwellization is an interesting avenue for future work. More formally, if one is interested in some expectation $\\mathbb{E}_{(\\Theta,\\mathbf{u})\\sim p(\\Theta,\\mathbf{u}|\\mathbf{y})}\\bar{[f(\\Theta,\\mathbf{u})]}$ in an LMM, there is a Monte Carlo estimator ", "page_idx": 8}, {"type": "equation", "text": "$$\nE_{1}=\\frac{1}{N}\\sum_{i=1}^{N}f(\\boldsymbol{\\Theta}^{i},\\mathbf{u}^{i}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $(\\Theta^{i},{\\bf u}^{i})\\sim p(\\Theta,{\\bf u}|{\\bf y})$ and $N$ is the sample size. Marginalization is a trick to improve the efficiency of the posterior sampling, so that we can achieve the same estimation variance with smaller $N$ or less runtime . At the same time, we also have access to a conditional distribution that is useful for Rao-Blackwellization. If the effects variable u can be marginalized we have both an approximate posterior for $p(\\Theta|\\mathbf{y})$ and an analytical conditional distribution $p(\\mathbf{u}|\\boldsymbol{\\Theta},\\mathbf{y})$ . With Rao-Blackwellization we have that $\\mathbb{E}_{(\\mathbf{e},\\mathbf{u})\\sim p(\\mathbf{e},\\mathbf{u}|\\mathbf{y})}[f(\\mathbf{\\Theta}\\mathbf{\\Theta},\\mathbf{u})]=\\mathbb{E}_{\\mathbf{\\Theta}\\sim p(\\mathbf{\\Theta}|\\mathbf{y})}[\\mathbb{E}_{\\mathbf{u}\\sim p(\\mathbf{u}|\\mathbf{\\Theta},\\mathbf{y})}[f(\\mathbf{\\Theta}\\mathbf{\\Theta},\\mathbf{u})]]$ . In such case, another Monte Carlo estimator can be constructed: ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\nE_{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}_{\\mathbf{u}\\sim p(\\mathbf{u}|\\mathbf{\\Theta}\\mathbf{,y})}\\left[f\\big(\\mathbf{\\Theta}\\mathbf{,u}\\big)\\right],\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\Theta^{i}\\sim p(\\Theta|\\mathbf{y})$ . For some functions, such as those that are polynomial in $\\mathbf{u}$ , the inner expectation can be computed exactly using properties of Gaussians. In other cases, the inner expectation can be estimated cheaply via Monte Carlo using exact samples from $p(\\mathbf{u}|\\pmb{\\Theta}_{i},\\mathbf{y})$ . ", "page_idx": 9}, {"type": "text", "text": "7.2 Marginalizing multiple effects in general models ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Section 4, we proposed to marginalize multiple classes of random effects by assuming a scaled identity covariance matrix. To marginalize multiple effects in general models, a possibility is to compute ${\\bf z}^{T}{\\bf E}^{-1}{\\bf z}$ and estimate $\\operatorname*{det}(\\mathbf{E})$ and the corresponding gradients with conjugate gradient (CG) solvers [14, 20]. However, this approach uses stochastic estimators for the determinant and gradients, which introduce bias into the HMC dynamics. These biases can be corrected through pseudo-marginalization [2], but it is unclear how significantly the extra stochasticity will affect the sampling. Another possible way to marginalize multiple effects for LMMs is to introduced the balanced levels assumption [50]. We leave these ideas for future exploration. ", "page_idx": 9}, {"type": "text", "text": "7.3 Beyond normal likelihoods ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we only consider normal or log-normal likelihoods, but our method can be easily generalized to other deterministic transformation of normal likelihood. This implies that marginalization can benefti regression with most continuous predictors given proper link functions. Another potential future direction is to marginalize classification models with probit regressions [1]. Marginalization will turn probit models into multivariate probit models as $\\mathbf{\\dot{A}}\\Sigma\\mathbf{u}\\mathbf{A}^{\\mathcal{\\tilde{T}}}+\\Sigma\\mathbf{y}$ is a dense covariance matrix, which may require a simulation-based method [10] or variational Bayes [39]. It will be interesting to see how ideas from multivariate probit regression could be fti into an HMC pipeline. In a broader context, marginalization is related to data augmentation techniques that \"create\" conjugacy for non-normal likelihoods or non-normal effects. Those techniques were developed for Gibbs sampling, e.g. [18, 55], but may also be useful for HMC. ", "page_idx": 9}, {"type": "text", "text": "7.4 Integration with probabilistic programming ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have developed a tool to speed up the HMC inference for LMMs. In our implementation, the marginalized likelihood $p(\\mathbf{y}|\\mathbf{\\Theta}\\mathbf{,u}_{-i})$ is defined as a special type of parametric distribution available to the user, and the recovery distribution $p(\\mathbf{u}_{i}|\\Theta,\\mathbf{u}_{-i},\\mathbf{y})$ is a function called after sampling. In our experiments, marginalization never hurt sampling efficiency measured by $\\mathrm{ESS/s}$ , and usually helped. Thus, it would be desirable to always marginalize one group of random effects when the model is an LMM. Future work could aim to automatically apply such transformations to user-specified LMMs. There are two possible high-level approaches. The first is to perform marginalization starting with a model described using a high-level abstraction such as an R formula. Then, when compiling the high-level model description into a concrete model (e.g., a probabilistic program), we can marginalize one or more of the effects using our methods. The second is to perform marginalization starting with a user-written probabilistic program representing an LMM. In this case, some compilation or program tracing technique will be needed to convert the user\u2019s program to a model representation suitable for manipulation. For example, Lai et al. [35] used program tracing to construct a graphical model representation that could be programmatically analyzed and transformed. To apply this methodology to LMMs, a special parser would also be needed to match the models to LMMs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Yuling Yao and the anonymous reviewers for comments that greatly improved the manuscript. This material is based upon work supported by the National Science Foundation under Grants #1749854, #2045900. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alan Agresti. Foundations of linear and generalized linear models. John Wiley & Sons, 2015.   \n[2] Christophe Andrieu and Gareth O Roberts. The pseudo-marginal approach for efficient Monte Carlo computations. 2009. [3] Eric Atkinson, Charles Yuan, Guillaume Baudart, Louis Mandel, and Michael Carbin. Semisymbolic inference for efficient streaming probabilistic programming. Proceedings of the ACM on Programming Languages, 6(OOPSLA2):1668\u20131696, 2022. [4] Douglas Bates, Martin M\u00e4chler, Ben Bolker, and Steve Walker. Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1):1\u201348, 2015. doi: 10.18637/jss.v067. i01.   \n[5] Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman. Pyro: Deep universal probabilistic programming. Journal of machine learning research, 20(28):1\u20136, 2019. [6] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.   \n[7] Helen Brown and Robin Prescott. Applied mixed models in medicine. John Wiley & Sons, 2014.   \n[8] Paul-Christian B\u00fcrkner. BRMS: An R package for Bayesian multilevel models using Stan. Journal of statistical software, 80:1\u201328, 2017.   \n[9] Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus A Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of statistical software, 76, 2017.   \n[10] Siddhartha Chib and Edward Greenberg. Analysis of multivariate probit models. Biometrika, 85(2):347\u2013361, 1998.   \n[11] Michael Creutz. Global Monte Carlo algorithms for many-fermion systems. Physical Review D, 38(4):1228, 1988.   \n[12] Brian Dillon, Alan Mishler, Shayne Sloggett, and Colin Phillips. Contrasting intrusion proflies for agreement and anaphora: Experimental and modeling evidence. Journal of Memory and Language, 69(2):85\u2013103, 2013.   \n[13] Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. arXiv preprint arXiv:1711.10604, 2017.   \n[14] Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318\u2013326. PMLR, 2012.   \n[15] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid Monte Carlo. Physics letters B, 195(2):216\u2013222, 1987.   \n[16] Charles R Ebersole, Olivia E Atherton, Aimee L Belanger, Hayley M Skulborstad, Jill M Allen, Jonathan B Banks, Erica Baranski, Michael J Bernstein, Diane BV Bonfiglio, Leanne Boucher, et al. Many labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67:68\u201382, 2016.   \n[17] Stefan L Frank, Thijs Trompenaars, and Shravan Vasishth. Cross-linguistic differences in processing double-embedded relative clauses: Working-memory constraints or language statistics? Cognitive science, 40(3):554\u2013578, 2016.   \n[18] Sylvia Fr\u00fchwirth-Schnatter, Rudolf Fr\u00fchwirth, Leonhard Held, and H\u00e5vard Rue. Improved auxiliary mixture sampling for hierarchical models of non-Gaussian data. Statistics and Computing, 19:479\u2013492, 2009.   \n[19] Katelyn Gao and Art B Owen. Estimation and inference for very large linear mixed effects models. Statistica Sinica, 30(4):1741\u20131771, 2020.   \n[20] Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. Advances in neural information processing systems, 31, 2018.   \n[21] Timon Gehr, Sasa Misailovic, and Martin Vechev. PSI: Exact symbolic inference for probabilistic programs. In Computer Aided Verification: 28th International Conference, CAV 2016, Toronto, ON, Canada, July 17-23, 2016, Proceedings, Part I 28, pages 62\u201383. Springer, 2016.   \n[22] Timon Gehr, Samuel Steffen, and Martin Vechev. \u03bbPSI: exact inference for higher-order probabilistic programs. In Proceedings of the 41st acm sigplan conference on programming language design and implementation, pages 883\u2013897, 2020.   \n[23] Andrew Gelman and Jennifer Hill. Data analysis using regression and multilevel/hierarchical models. Cambridge university press, 2006.   \n[24] Disha Ghandwani, Swarnadip Ghosh, Trevor Hastie, and Art B Owen. Scalable solution to crossed random effects model with random slopes. arXiv preprint arXiv:2307.12378, 2023.   \n[25] Edward Gibson and James Thomas. Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical. Language and Cognitive Processes, 14(3):225\u2013248, 1999.   \n[26] Maria Gorinova, Dave Moore, and Matthew Hoffman. Automatic reparameterisation of probabilistic programs. In International Conference on Machine Learning, pages 3648\u20133657. PMLR, 2020.   \n[27] Maria I Gorinova, Andrew D Gordon, Charles Sutton, and Matthijs V\u00e1k\u00e1r. Conditional independence by typing. ACM Transactions on Programming Languages and Systems (TOPLAS), 44(1):1\u201354, 2021.   \n[28] Philip Greengard, Jeremy Hoskins, Charles C Margossian, Jonah Gabry, Andrew Gelman, and Aki Vehtari. Fast methods for posterior inference of two-group normal-normal models. Bayesian Analysis, 18(3):889\u2013907, 2023.   \n[29] Daniel Grodner and Edward Gibson. Consequences of the serial nature of linguistic input for sentenial complexity. Cognitive science, 29(2):261\u2013290, 2005.   \n[30] Richard Grumitt, Biwei Dai, and Uros Seljak. Deterministic Langevin Monte Carlo with normalizing flows for Bayesian inference. Advances in Neural Information Processing Systems, 35:11629\u201311641, 2022.   \n[31] Xavier A Harrison, Lynda Donaldson, Maria Eugenia Correa-Cano, Julian Evans, David N Fisher, Cecily ED Goodwin, Beth S Robinson, David J Hodgson, and Richard Inger. A brief introduction to mixed effects modelling and multi-model inference in ecology. PeerJ, 6:e4794, 2018.   \n[32] David A Harville. Matrix algebra from a statistician\u2019s perspective, 1998.   \n[33] Matthew Hoffman, Pavel Sountsov, Joshua V Dillon, Ian Langmore, Dustin Tran, and Srinivas Vasudevan. Neutra-lizing bad geometry in Hamiltonian Monte Carlo using neural transport. arXiv preprint arXiv:1903.03704, 2019.   \n[34] Matthew D Hoffman, Andrew Gelman, et al. The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1):1593\u20131623, 2014.   \n[35] Jinlin Lai, Javier Burroni, Hui Guan, and Daniel Sheldon. Automatically marginalized MCMC in probabilistic programming. In International Conference on Machine Learning, pages 18301\u2013 18318. PMLR, 2023.   \n[36] Daniel Lewandowski, Dorota Kurowicka, and Harry Joe. Generating random correlation matrices based on vines and extended onion method. Journal of multivariate analysis, 100(9): 1989\u20132001, 2009.   \n[37] Jun S Liu. The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem. Journal of the American Statistical Association, 89(427):958\u2013966, 1994.   \n[38] Colin M MacLeod. Half a century of research on the Stroop effect: an integrative review. Psychological bulletin, 109(2):163, 1991.   \n[39] Stephan Mandt, Florian Wenzel, Shinichi Nakajima, John Cunningham, Christoph Lippert, and Marius Kloft. Sparse probit linear mixed model. Machine Learning, 106:1621\u20131642, 2017.   \n[40] Charles Margossian, Aki Vehtari, Daniel Simpson, and Raj Agrawal. Hamiltonian Monte Carlo using an adjoint-differentiated Laplace approximation: Bayesian inference for latent Gaussian models and beyond. Advances in Neural Information Processing Systems, 33:9086\u20139097, 2020.   \n[41] Lotte Meteyard and Robert AI Davies. Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112:104092, 2020.   \n[42] Kevin Murphy and Stuart Russell. Rao-Blackwellised particle filtering for dynamic Bayesian networks. In Sequential Monte Carlo methods in practice, pages 499\u2013515. Springer, 2001.   \n[43] Lawrence Murray, Daniel Lund\u00e9n, Jan Kudlicka, David Broman, and Thomas Sch\u00f6n. Delayed sampling and automatic Rao-Blackwellization of probabilistic programs. In International Conference on Artificial Intelligence and Statistics, pages 1037\u20131046. PMLR, 2018.   \n[44] Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. Probabilistic inference by program transformation in Hakaru (system description). In Functional and Logic Programming: 13th International Symposium, FLOPS 2016, Kochi, Japan, March 4-6, 2016, Proceedings 13, pages 62\u201379. Springer, 2016.   \n[45] Radford M Neal. Slice sampling. The annals of statistics, 31(3):705\u2013767, 2003.   \n[46] Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov chain Monte Carlo, 2(11):2, 2011.   \n[47] Bruno Nicenboim, Daniel Schad, and Shravan Vasishth. An introduction to Bayesian data analysis for cognitive science. Under contract with Chapman and Hall/CRC statistics in the social and behavioral sciences series, 2021.   \n[48] Mante S Nieuwland, Stephen Politzer-Ahles, Evelien Heyselaar, Katrien Segaert, Emily Darley, Nina Kazanina, Sarah Von Grebmer Zu Wolfsthurn, Federica Bartolozzi, Vita Kogan, Aine Ito, et al. Large-scale replication study reveals a limit on probabilistic prediction in language comprehension. ELife, 7:e33468, 2018.   \n[49] Omiros Papaspiliopoulos, Gareth O Roberts, and Martin Sk\u00f6ld. A general framework for the parametrization of hierarchical models. Statistical Science, pages 59\u201373, 2007.   \n[50] Omiros Papaspiliopoulos, Gareth O Roberts, and Giacomo Zanella. Scalable inference for crossed random effects models. Biometrika, 107(1):25\u201340, 2020.   \n[51] Omiros Papaspiliopoulos, Timoth\u00e9e Stumpf-F\u00e9tizon, and Giacomo Zanella. Scalable Bayesian computation for crossed and nested hierarchical models. Electronic Journal of Statistics, 17(2): 3575\u20133612, 2023.   \n[52] Matthew D Parno and Youssef M Marzouk. Transport map accelerated Markov chain Monte Carlo. SIAM/ASA Journal on Uncertainty Quantification, 6(2):645\u2013682, 2018.   \n[53] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University of Denmark, 7(15):510, 2008.   \n[54] Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flexible and accelerated probabilistic programming in NumPyro. arXiv preprint arXiv:1912.11554, 2019.   \n[55] Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models using P\u00f3lya\u2013Gamma latent variables. Journal of the American statistical Association, 108(504): 1339\u20131349, 2013.   \n[56] Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic Smyth, and Max Welling. Fast collapsed Gibbs sampling for latent Dirichlet allocation. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 569\u2013577, 2008.   \n[57] Andres Potapczynski, Marc Finzi, Geoff Pleiss, and Andrew G Wilson. CoLA: Exploiting compositional structure for automatic and efficient numerical linear algebra. Advances in Neural Information Processing Systems, 36, 2024.   \n[58] Jakob Robnik, G Bruno De Luca, Eva Silverstein, and Uro\u0161 Seljak. Microcanonical Hamiltonian Monte Carlo. The Journal of Machine Learning Research, 24(1):14696\u201314729, 2023.   \n[59] H\u00e5vard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society Series B: Statistical Methodology, 71(2):319\u2013392, 2009.   \n[60] Simo S\u00e4rkk\u00e4 and Lennart Svensson. Bayesian flitering and smoothing, volume 17. Cambridge university press, 2023.   \n[61] Matthias Seeger. Gaussian processes for machine learning. International journal of neural systems, 14(02):69\u2013106, 2004.   \n[62] Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Eric Meissner, and Neil D Lawrence. Autodifferentiating linear algebra. arXiv preprint arXiv:1710.08717, 2017.   \n[63] Ahmed Shahin, Carmela Wegworth, David, Elizabeth Estes, Julia Elliott, Justin Zita, SimonWalsh, Slepetys, and Will Cukierski. OSIC pulmonary fibrosis progression, 2020.   \n[64] Jun Shi, Chengming Jiang, Aman Gupta, Mingzhou Zhou, Yunbo Ouyang, Qiang Charles Xiao, Qingquan Song, Yi Wu, Haichao Wei, and Huiji Gao. Generalized deep mixed models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3869\u20133877, 2022.   \n[65] Justin D Silverman, Kimberly Roche, Zachary C Holmes, Lawrence A David, and Sayan Mukherjee. Bayesian multinomial logistic normal models through marginally latent matrix-T processes. Journal of Machine Learning Research, 23(7):1\u201342, 2022.   \n[66] Giora Simchoni and Saharon Rosset. Using random effects to account for high-cardinality categorical features and repeated measures in deep neural networks. Advances in Neural Information Processing Systems, 34:25111\u201325122, 2021.   \n[67] Giora Simchoni and Saharon Rosset. Integrating random effects in deep neural networks. Journal of Machine Learning Research, 24(156):1\u201357, 2023.   \n[68] Wessel N van Wieringen. Lecture notes on ridge regression. arXiv preprint arXiv:1509.09169, 2015.   \n[69] Shravan Vasishth, Katja Suckow, Richard L Lewis, and Sabine Kern. Short-term forgetting in sentence comprehension: Crosslinguistic evidence from verb-final structures. Language and Cognitive Processes, 25(4):533\u2013567, 2010.   \n[70] Shravan Vasishth, Zhong Chen, Qiang Li, and Gueilan Guo. Processing Chinese relative clauses: Evidence for the subject-relative advantage. PloS one, 8(10):e77006, 2013.   \n[71] Greg Ver Steeg and Aram Galstyan. Hamiltonian dynamics with non-Newtonian momentum for rapid sampling. Advances in Neural Information Processing Systems, 34:11012\u201311025, 2021.   \n[72] Basil Wahn, Daniel P Ferris, W David Hairston, and Peter K\u00f6nig. Pupil sizes scale with attentional load and task experience in a multiple object tracking task. PloS one, 11(12): e0168087, 2016.   \n[73] Jun-Kun Wang and Andre Wibisono. Accelerating Hamiltonian Monte Carlo via Chebyshev integration time. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023.   \n[74] Torsten W\u00f6rtwein, Nicholas B Allen, Lisa B Sheeber, Randy P Auerbach, Jeffrey F Cohn, and Louis-Philippe Morency. Neural mixed effects for nonlinear personalized predictions. In Proceedings of the 25th International Conference on Multimodal Interaction, pages 445\u2013454, 2023.   \n[75] HI Wu and E Gibson. Processing Chinese relative clauses in context. In Poster presented at the 21st CUNY Conference on Sentence Processing, University of North Carolina at Chapel Hill, 2008.   \n[76] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but did it work?: Evaluating variational inference. In International Conference on Machine Learning, pages 5581\u20135590. PMLR, 2018.   \n[77] Zhaoxia Yu, Michele Guindani, Steven F Grieco, Lujia Chen, Todd C Holmes, and Xiangmin Xu. Beyond t test and ANOVA: applications of mixed-effects models for more rigorous statistical analysis in neuroscience research. Neuron, 110(1):21\u201335, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Notation table ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the important symbols used in the paper. ", "page_idx": 14}, {"type": "table", "img_path": "uXuObobJHO/tmp/29fa709f8cefdc8a89b0be20c8b273b126f929e4823846dd2abbe7beca839fc5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Proofs and details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first review the tree structure of the matrix A. A is an $N\\times M$ matrix where every block of $d$ columns corresponds to the effects for one group (e.g., an individual subject, age, school, or gender). For example, if $N=3$ , $k=2$ and $d=2$ , one possible graphical model is as below. ", "page_idx": 15}, {"type": "image", "img_path": "uXuObobJHO/tmp/ffd4fc99dd85403db73c10523aeaea59e821257e38a289a72d5db04eb819465c.jpg", "img_caption": ["Figure 5: A tree-structured model conditioned on $\\Theta$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Each $\\mathbf{u}_{j}\\in\\mathbb{R}^{2}$ . If the coefficients are all 1s, then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}={\\left(\\begin{array}{l l l l}{1}&{1}&{0}&{0}\\\\ {1}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{1}\\end{array}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To generalize, if for $y_{i}$ , the grouping variable is $g_{i}$ , then in the $i$ th row of $\\mathbf{A}$ , only $\\mathbf{A}_{i,j:k}$ can be nonzero for $j=(g_{i}-1)d+1$ and $k=g_{i}d$ . We consider three representations of the matrix A. By rows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\left({\\begin{array}{l}{\\mathbf{r}_{1}}\\\\ {\\mathbf{r}_{2}}\\\\ {\\dots}\\\\ {\\mathbf{r}_{N}}\\end{array}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by columns, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\left(\\mathbf{c}_{1}\\quad\\mathbf{c}_{2}\\quad\\ldots\\quad\\mathbf{c}_{k d}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and by blocks of $d$ columns, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}=\\left(\\mathbf{A}_{:,1:d}\\quad\\mathbf{A}_{:,d+1:2d}\\quad...\\quad\\mathbf{A}_{:,(k-1)d+1:k d}\\right)}\\\\ &{\\quad=(\\mathbf{C}_{1}\\quad\\mathbf{C}_{2}\\quad...\\quad\\mathbf{C}_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where each ${\\bf C}_{i}\\;(i=1,2,...,k)$ ) is $N\\times d$ . Now we restate and prove Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "Theorem 1. If $\\Sigma_{\\mathbf{y}}$ is diagonal, $\\pmb{\\Sigma_{\\mathbf{u}}}$ is block-diagonal with blocks of size $d\\times d,$ , then $\\mathbf{F}=\\Sigma_{\\mathbf{u}}^{-1}+$ $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ is also block-diagonal with $d\\times d$ blocks and computing $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ takes $O(N d^{2})$ time. ", "page_idx": 15}, {"type": "text", "text": "Proof. The theorem has two parts: (a) the property of $\\mathbf{F}$ , and (b) the computation of $\\mathbf{F}$ . We address them with the three representations of $\\mathbf{A}$ . ", "page_idx": 15}, {"type": "text", "text": "(a) $\\mathbf{F}$ is block-diagonal. Because $\\pmb{\\Sigma_{\\mathbf{u}}}$ is block-diagonal, $\\pmb{\\Sigma_{\\mathbf{u}}^{-1}}$ is also block-diagonal with the same sizes. Also, $\\Sigma_{\\mathbf{y}}$ is diagonal, so the block-diagonality of $\\mathbf{A}^{T}\\dot{\\Sigma_{\\mathbf{y}}}^{-1}\\mathbf{A}$ is the same as $\\mathbf{\\hat{A}}^{T}\\mathbf{A}$ . We consider the column representation of $\\mathbf{A}$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}^{T}\\mathbf{A}=\\left(\\begin{array}{l}{\\mathbf{C}_{1}^{T}}\\\\ {\\mathbf{C}_{2}^{T}}\\\\ {\\cdots}\\\\ {\\mathbf{C}_{k}^{T}}\\end{array}\\right)\\left(\\mathbf{C}_{1}\\quad\\mathbf{C}_{2}\\quad\\ldots\\quad\\mathbf{C}_{k}\\right)}\\\\ &{\\quad\\quad=\\left(\\begin{array}{l l l l}{\\mathbf{C}_{1}^{T}\\mathbf{C}_{1}}&{\\mathbf{C}_{1}^{T}\\mathbf{C}_{2}}&{\\ldots}&{\\mathbf{C}_{1}^{T}\\mathbf{C}_{k}}\\\\ {\\mathbf{C}_{2}^{T}\\mathbf{C}_{1}}&{\\mathbf{C}_{2}^{T}\\mathbf{C}_{2}}&{\\ldots}&{\\mathbf{C}_{2}^{T}\\mathbf{C}_{k}}\\\\ {\\mathbf{C}_{k}^{\\cdots}\\mathbf{C}_{1}}&{\\mathbf{C}_{k}^{T}\\mathbf{C}_{2}}&{\\ldots}&{\\mathbf{C}_{k}^{T}\\mathbf{C}_{k}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $1\\leq i\\leq k$ , ${\\bf C}_{i}^{T}{\\bf C}_{i}$ is $d\\times d$ . For $1\\leq i<j\\leq k$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{C}_{i}^{T}\\mathbf{C}_{j}=\\left(\\mathbf{c}_{(i-1)d+1}^{T}\\right)\\left(\\mathbf{c}_{(j-1)d+1}\\quad\\mathbf{c}_{(j-1)d+2}\\quad...\\quad\\mathbf{c}_{j d}\\right).}\\\\ {\\mathbf{c}_{i d}^{T}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The following lemma shows that ${\\bf C}_{i}^{T}{\\bf C}_{j}={\\bf0}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. For any $1\\leq i<j\\leq k$ and $1\\leq s,t\\leq d,$ it holds that $\\mathbf{c}_{(i-1)d+s}^{T}\\mathbf{c}_{(j-1)d+t}=0.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The lemma can be proved by contradiction. Suppose $\\mathbf{c}_{(i-1)d+s}^{T}\\mathbf{c}_{(j-1)d+t}\\neq0$ . Then there exists an index $n$ such that $\\mathbf{c}_{(i-1)d+s}[n]\\neq0$ and $\\mathbf{c}_{(j-1)d+t}[n]\\neq0$ . This means that in the $n$ th row of $\\mathbf{A}$ , both $\\mathbf{A}_{n,(i-1)d+s}$ and ${\\bf A}_{n,(j-1)d+t}$ are non-zero. This contradicts with the tree-structure where only one group of $d$ elements can be non-zero in a row. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "With the lemma, we have that ${\\mathbf A}^{T}{\\mathbf A}$ is block-diagonal, thus $\\pmb{\\Sigma_{\\mathbf{u}}^{-1}}+\\mathbf{A}^{T}\\pmb{\\Sigma_{\\mathbf{y}}^{-1}}\\mathbf{A}$ is also block-diagonal and each block is $d\\times d$ . ", "page_idx": 16}, {"type": "text", "text": "(b) The computation of $\\mathbf{A}^{T}\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{A}$ is $O(N d^{2})$ . Since $\\Sigma_{\\mathbf{y}}$ is diagonal, $\\mathbf{A}^{\\prime}=\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{A}$ has the same pattern of zeros and nonzeros as A. We consider the row representations such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{A}^{\\prime}=\\left({\\begin{array}{l}{\\mathbf{r}_{1}^{\\prime}}\\\\ {\\mathbf{r}_{2}^{\\prime}}\\\\ {\\dots}\\\\ {\\mathbf{r}_{N}^{\\prime}}\\end{array}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{A}^{T}\\mathbf{A}^{\\prime}=\\left(\\mathbf{r}_{1}^{T}\\quad\\mathbf{r}_{2}^{T}\\quad...\\quad\\mathbf{r}_{N}\\right)\\left(\\mathbf{r}_{2}^{r_{1}^{\\prime}}\\right)=\\sum_{i=1}^{N}\\mathbf{r}_{i}^{T}\\mathbf{r}_{i}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "note that each of $\\mathbf{r}_{i}$ and $\\mathbf{r}_{i}^{\\prime}$ has $d$ non-zero elements. So computing $\\mathbf{A}^{T}\\mathbf{A}^{\\prime}$ is $O(N d^{2})$ . ", "page_idx": 16}, {"type": "table", "img_path": "uXuObobJHO/tmp/c64c8860d09dca6f1e990f6a6d6a158b548d7815d605b50fbf07be5b1aa34d29.jpg", "table_caption": ["B.2 Pseudocode for recovery after marginalizing one group of random effects "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.3 Details of scaled identity covariance matrices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "With the assumptions of scaled identity covariance matrices, all effects can be marginalized with a preprocessing of the eigendecomposition of BTB. ", "page_idx": 16}, {"type": "text", "text": "(a) Preprocessing before HMC. We compute ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{B}^{T}\\mathbf{B}=\\mathbf{Q}\\Lambda\\mathbf{Q}^{T}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In LMMs, the computation of $\\mathbf{B}^{T}\\mathbf{B}$ is $\\mathcal{O}(N L^{2}d^{2})^{2}$ , and the eigendecomposition of it is $\\mathcal{O}(D^{3})$ . So the overall complexity for preprocessing is $\\mathcal{O}(D^{3}+N L^{2}d^{2})$ . Compared with the HMC sampling ", "page_idx": 16}, {"type": "text", "text": "loop that takes thousands of steps and visits the model hundreds of times each step, the cost of preprocessing is not expensive. In our attempt to marginalize all random effects for the instructor evaluation model in Section 6.1, this step takes less than 10 seconds. ", "page_idx": 17}, {"type": "text", "text": "(b) Marginalized likelihood during HMC. During HMC sampling, the log density $\\log p(\\mathbf{y}|\\mathbf{\\Theta}\\Theta)$ would be calculated, which is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log p(\\mathbf{y}|\\boldsymbol{\\Theta})=-\\frac{1}{2}\\operatorname*{det}\\left(\\mathbf{E}\\right)-\\frac{1}{2}\\mathbf{z}^{T}\\mathbf{E}^{-1}\\mathbf{z}+C\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{z}=\\mathbf{y}-\\mathbf{B}\\mu-\\mathbf{b}$ and $\\mathbf{E}=\\mathbf{B}\\pmb{\\Sigma}_{\\mathbf{v}}\\mathbf{B}^{T}+\\pmb{\\Sigma}_{\\mathbf{y}}$ . The computation of $\\mathbf{z}$ takes $\\mathcal{O}(N L d)$ time. With the two lemmas, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{det}(\\mathbf{E})=\\operatorname*{det}(\\Sigma_{\\mathbf{v}}^{-1}+\\mathbf{B}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{B})\\operatorname*{det}(\\Sigma_{\\mathbf{v}})\\operatorname*{det}(\\Sigma_{\\mathbf{y}}),\\quad\\quad}\\\\ {\\mathbf{z}^{T}\\mathbf{E}^{-1}\\mathbf{z}=\\mathbf{z}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{z}-\\mathbf{z}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{B}(\\Sigma_{\\mathbf{v}}^{-1}+\\mathbf{B}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{B})^{-1}\\mathbf{B}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A shared matrix in the formulas is $\\mathbf{F}=\\pmb{\\Sigma}_{\\mathbf{v}}^{-\\mathbf{1}}+\\mathbf{B}^{T}\\pmb{\\Sigma}_{\\mathbf{y}}^{-\\mathbf{1}}\\mathbf{B}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{F}=\\pmb{\\Sigma}_{\\mathbf{v}}^{-1}+\\mathbf{B}^{T}\\pmb{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{B}=\\mathbf{Q}\\left(\\frac{1}{\\tau_{\\mathbf{v}}}\\mathbf{I}+\\frac{1}{\\tau_{\\mathbf{y}}}\\mathbf{A}\\right)\\mathbf{Q}^{T}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the trick, evaluating $\\operatorname*{det}(\\mathbf{F})$ reduced to $\\mathcal{O}(D)$ time as $\\operatorname*{det}(\\mathbf{Q})=1$ . Also ${\\bf z}^{T}{\\bf E}^{-1}{\\bf z}$ becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{z}^{T}\\mathbf{E}^{-1}\\mathbf{z}=\\mathbf{z}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{z}-\\mathbf{z}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{B}\\mathbf{Q}\\left(\\frac{1}{\\tau_{\\mathbf{v}}}\\mathbf{I}+\\frac{1}{\\tau_{\\mathbf{y}}}\\mathbf{A}\\right)^{-1}\\mathbf{Q}^{T}\\mathbf{B}^{T}\\boldsymbol{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{z}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\mathbf{B}^{T}\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{\\mathbf{y}}^{-1}\\mathbf{z}\\in\\mathbb{R}^{D}$ can be computed in $\\mathcal{O}(N L d)$ time, but its multiplication with $\\mathbf{Q}^{T}$ takes $\\mathcal{O}(D^{2})$ time. Given that $\\begin{array}{r}{\\frac{1}{\\tau_{\\mathbf{v}}}\\mathbf{I}+\\frac{1}{\\tau_{\\mathbf{y}}}\\mathbf{A}}\\end{array}$ is diagonal, the complexity of evaluating $\\log p(\\mathbf{y}|\\mathbf{\\Theta}\\mathbf{\\Theta})$ once is then $\\mathcal{O}(D^{2}+N L d)$ . ", "page_idx": 17}, {"type": "text", "text": "(c) Ancestral sampling after HMC. In the recovery step, we perform ancestral sampling from $p(\\mathbf{v}|\\boldsymbol{\\Theta},\\mathbf{y})$ . To efficiently generate samples, we give the following theorem. ", "page_idx": 17}, {"type": "text", "text": "Theorem 2. If $\\begin{array}{r}{\\Sigma_{\\mathbf{v}}=\\tau_{\\mathbf{v}}\\mathbf{I},}\\end{array}$ , $\\Sigma_{\\mathbf{y}}=\\tau_{\\mathbf{y}}\\mathbf{I}$ and $\\mathbf{B}^{T}\\mathbf{B}=\\mathbf{Q}\\Lambda\\mathbf{Q}^{T}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{v}|\\Theta,\\mathbf{y}\\sim\\mathcal{N}(\\mu_{\\mathbf{v}|\\Theta,\\mathbf{y}},\\Sigma_{\\mathbf{v}|\\Theta,\\mathbf{y}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mu_{\\mathbf{v}|\\Theta,\\mathbf{y}}=\\mu+\\frac{\\tau_{\\mathbf{v}}}{\\tau_{\\mathbf{y}}}\\left(\\mathbf{B}^{T}-\\frac{1}{\\tau_{\\mathbf{y}}}\\mathbf{Q}\\Lambda\\left(\\frac{1}{\\tau_{\\mathbf{v}}}+\\frac{\\Lambda}{\\tau_{\\mathbf{y}}}\\right)^{-1}\\mathbf{Q}^{T}\\mathbf{B}^{T}\\right)\\mathbf{z},}\\\\ {\\displaystyle\\sum_{\\mathbf{v}|\\Theta,\\mathbf{y}}=\\mathbf{Q}\\left(\\tau_{\\mathbf{v}}\\mathbf{I}-\\frac{\\tau_{\\mathbf{v}}}{\\tau_{\\mathbf{y}}}\\mathbf{\\Lambda}\\mathbf{A}+\\frac{\\tau_{\\mathbf{v}}}{\\tau_{\\mathbf{y}}^{2}}\\left(\\frac{1}{\\tau_{\\mathbf{v}}}+\\frac{\\Lambda}{\\tau_{\\mathbf{y}}}\\right)^{-1}\\mathbf{\\Lambda}\\mathbf{A}^{2}\\right)\\mathbf{Q}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In Theorem 2, from $\\mathbf{z}$ , we can apply matrix multiplications from right to left to get $\\mu_{\\mathbf{v}|\\Theta,\\mathbf{y}}$ . The whole computation takes $\\mathcal{O}(D^{2}+N L d)$ . To generate normal samples a Cholseky factorization for \u03a3v|\u0398,y is required. But \u03c4vI \u2212\u03c4\u03c4v2 \u039b + \u03c4\u03c4v22 \u03c41 + \u03c4\u039b is diagonal, so it can be obtained in $\\mathcal{O}(D^{2})$ time as well. Now we prove Theorem 2. ", "page_idx": 17}, {"type": "text", "text": "Proof. $\\mu_{\\mathbf{v}|\\Theta,\\mathbf{y}}$ and $\\Sigma_{\\mathbf{v}\\mid\\Theta,\\mathbf{y}}$ can both be derived algebraically. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathrm{v}|\\mathbf{\\boldsymbol{\\Theta}},\\mathbf{\\boldsymbol{y}}}=\\mu+\\mathbf{M}\\mathbf{z}}\\\\ &{\\hphantom{=}=\\mu+\\boldsymbol{\\Sigma}_{\\mathbf{v}}\\mathbf{B}^{T}(\\mathbf{B}\\Sigma_{\\mathbf{v}}\\mathbf{B}^{T}+\\Sigma_{\\mathbf{y}})^{-1}\\mathbf{z}}\\\\ &{\\hphantom{=}=\\mu+\\boldsymbol{\\Sigma}_{\\mathbf{v}}\\mathbf{B}^{T}(\\Sigma_{\\mathbf{y}}^{-1}-\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{B}(\\Sigma_{\\mathbf{v}}^{-1}+\\mathbf{B}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{B})^{-1}\\mathbf{B}^{T}\\Sigma_{\\mathbf{y}}^{-1})\\mathbf{z}}\\\\ &{\\hphantom{=}=\\mu+\\frac{7\\mathbf{v}}{7\\mathbf{y}}(\\mathbf{B}^{T}-\\frac{1}{7\\mathbf{y}}\\mathbf{B}^{T}\\mathbf{B}(\\Sigma_{\\mathbf{v}}^{-1}+\\mathbf{B}^{T}\\Sigma_{\\mathbf{y}}^{-1}\\mathbf{B})^{-1}\\mathbf{B}^{T})\\mathbf{z}}\\\\ &{\\hphantom{=}=\\mu+\\frac{7\\mathbf{v}}{7\\mathbf{y}}(\\mathbf{B}^{T}-\\frac{1}{7\\mathbf{y}}\\mathbf{Q}\\mathbf{A}\\mathbf{Q}^{T}\\mathbf{Q}\\left(\\frac{1}{7\\mathbf{v}}\\mathbf{I}+\\frac{1}{7\\mathbf{y}}\\mathbf{A}\\right)^{-1}\\mathbf{Q}^{T}\\mathbf{B}^{T})\\mathbf{z}}\\\\ &{\\hphantom{=}=\\mu+\\frac{7\\mathbf{v}}{7\\mathbf{y}}(\\mathbf{B}^{T}-\\frac{1}{7\\mathbf{y}}\\mathbf{Q}\\mathbf{A}\\left(\\frac{1}{7\\mathbf{v}}\\mathbf{I}+\\frac{1}{7\\mathbf{y}}\\mathbf{A}\\right)^{-1}\\mathbf{Q}^{T}\\mathbf{B}^{T})\\mathbf{z}}\\\\ &{\\hphantom{=}=\\mu+\\frac{7\\mathbf{v}}{7\\mathbf{y}}(\\mathbf{B}^{T}-\\frac{1}{7\\mathbf{y}}\\mathbf{Q}\\mathbf{A}\\left(\\frac{1}{7\\mathbf{v}}\\mathbf{I}+\\frac{1}{7\\mathbf{y}}\\mathbf{A}\\right)^{-1}\\mathbf{Q}^{T}\\mathbf{B}^{T})\\mathbf{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde\\Sigma_{\\gamma}(g,\\gamma=(1-\\mathrm{MB})\\widetilde\\Sigma_{\\gamma}}\\\\ &{\\qquad=(1-\\sum_{\\widetilde\\gamma=1}^{N}\\mathrm{R}^{T}(\\mathbb{B}\\Sigma_{\\gamma}\\mathbb{B}^{T}+\\Sigma_{\\gamma})^{-1}\\mathbb{B})\\Sigma_{\\gamma}}\\\\ &{\\qquad=\\tau_{1}-\\tau_{1}\\overline{\\mathrm{R}}^{T}(\\mathbb{B}\\Sigma_{\\gamma}\\mathbb{B}^{T}+\\Sigma_{\\gamma})^{-1}\\mathbb{B}}\\\\ &{\\qquad=\\tau_{1}\\overline{\\mathrm{I}}-\\tau_{\\gamma}\\overline{\\mathrm{R}}^{T}(\\mathbb{B}\\Sigma_{\\gamma}^{-1}-\\Sigma_{\\gamma}^{-1}\\mathbb{B}(\\Sigma_{\\gamma}^{-1}+\\mathbb{B}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B})^{-1}\\mathbb{B}^{T}\\Sigma_{\\gamma}^{-1})\\mathbb{B}}\\\\ &{\\qquad=\\tau_{1}\\overline{\\mathrm{I}}-\\tau_{\\gamma}\\overline{\\mathrm{B}}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B}+\\tau_{\\gamma}^{-1}\\overline{\\mathrm{R}}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B}(\\Sigma_{\\gamma}^{-1}+\\mathbb{B}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B})^{-1}\\mathbb{B}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B}}\\\\ &{\\qquad=\\tau_{1}\\overline{\\mathrm{I}}-\\tau_{\\gamma}\\overline{\\mathrm{R}}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B}+\\frac{\\tau_{1}}{\\gamma_{\\gamma}^{2}}\\mathrm{B}^{T}\\Delta\\Sigma_{\\gamma}^{-1}+\\mathbb{B}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B})^{-1}\\mathbb{B}^{T}\\Sigma_{\\gamma}^{-1}\\mathbb{B}}\\\\ &{\\qquad=\\tau_{1}\\overline{\\mathrm{I}}-\\frac{\\tau_{\\gamma}}{\\gamma_{\\gamma}}^{-1}\\mathbb{B}^{T}\\Delta\\underline{\\mathrm{I}}^{\\gamma}+\\frac{\\tau_{\\gamma}^{-1}}{\\gamma_{\\gamma}^{2}}\\mathrm{Q}\\mathrm{M}^{T}\\mathbb{Q}\\Bigg(\\frac{1}{\\gamma_{\\gamma}}\\mathrm{I}+\\frac{1}{\\gamma_{\\gamma}}\\Big)^{-1}\\mathbb{B}^{T}\\mathbb{Q \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C Additional experimental results ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "uXuObobJHO/tmp/49e64d907aac8f89d0403d0224476bd05d3ac5f63527b18db3904e7840d39d49.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Trace plots for $u_{2,1}$ , $u_{3,1}$ and $\\alpha$ of an interval of 1,000 sampling steps after warmup on the ETH instructor evaluation model, using the same data as Figure 2 in the paper. ", "page_idx": 19}, {"type": "text", "text": "Table 5: Number of parameters (out of 4117) whose R\u02c6 exceed a threshold for 1,000 samples from HMC, with or without marginalization. Mean and standard deviation over 5 independent runs are reported. ", "page_idx": 19}, {"type": "table", "img_path": "uXuObobJHO/tmp/49e94e83428e4361a3f0e574cc69a559a5ae9f8c4b400797cc6b72bdac04a14a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 6: Divergence (mean and standard deviation) out of 10,000 samples with different strategies on the grouseticks model across 5 random seeds under different target probabilities. We use M1 to represent marginalizing $\\mathbf{u}_{1}$ , M2 to represent marginalizing $\\mathbf{u}_{2}$ , R1 to represent reparameterizing $\\mathbf{u}_{1}$ , R2 to represent reparameterizing $\\mathbf{u}_{2}$ . ", "page_idx": 19}, {"type": "table", "img_path": "uXuObobJHO/tmp/3d0a11effe01124d135a1e0d25bb44081410aa39aa787e02992f1b3190d39001.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Models and example probabilistic programs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide the details of the nine cognitive science datasets and their corresponding models and probabilistic programs. We follow [47] and use maximal models with correlated varying intercept and slopes for each of the datasets. The model for the pupil dataset is described in Section 2. ", "page_idx": 20}, {"type": "text", "text": "D.1 Agreement attraction in comprehension ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The dataset (dillonE1) studies the effect of the agreement attraction phenomenon when reading a noun with the auxiliary verb [12]. The predictor is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log(y_{i})=\\alpha+u_{1,g_{1,i},1}+u_{2,g_{2,i},1}+t_{i}(\\beta+u_{1,g_{1,i},2}+u_{2,g_{2,i},2})+\\epsilon,\\epsilon\\sim{\\cal N}(0,\\sigma^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Each experiment result $y_{i}$ is from subject $g_{1,i}$ on sentence $g_{2,i}$ , with $t_{i}$ being the interference level $(t_{i}\\in\\{0,1\\})$ ). Bayesian hierarchical modeling assigns prior to the variables. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{T}_{1}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(5^{2},5^{2})),\\ \\mathbf{L}_{1}\\sim\\mathrm{LKICholesky}(2,1),\\ \\mathbf{T}_{2}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(5^{2},5^{2})),\\ \\mathbf{L}_{2}\\sim\\mathrm{LKICholesky~}}\\\\ &{\\alpha\\sim\\mathcal{N}(0,10^{2}),\\ \\beta\\sim\\mathcal{N}(0,5^{2}),\\ \\sigma\\sim\\mathcal{N}^{+}(0,5^{2}),\\ \\mathbf{u}_{1,j}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf T_{1}\\mathbf{L}_{1}\\mathbf{L}_{1}^{T}\\mathbf{T}_{1}),\\ \\mathbf{u}_{2,k}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf T_{2}\\mathbf{L}_{2}\\mathbf{u}_{3,k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The probabilistic program in NumPyro is then ", "page_idx": 20}, {"type": "image", "img_path": "uXuObobJHO/tmp/bfba621a1df36ab68dfd09d29f0a335eef04b8e09d719c5044f521d5e3c33f33.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "We use u and $\\mathtt{v}$ in the codes to represent the two random effects. The probabilistic program with marginalization is similar. Suppose we marginalize u, our probabilistic program becomes ", "page_idx": 20}, {"type": "image", "img_path": "uXuObobJHO/tmp/8e36e6dce9485b53c9c58106d642f66fa24190107960a5a34a698c2f7ecb9951.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "To marginalize $\\mathtt{v}$ , the probabilistic program is ", "page_idx": 20}, {"type": "image", "img_path": "uXuObobJHO/tmp/3aec1a2a1147ab6099773776da923c0448d5c7ef7f0c414d2f8312b659ae519e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "The probabilistic programs for the other models will be similar and we omit them for simplicity. ", "page_idx": 20}, {"type": "text", "text": "D.2 English and Dutch Grammaticality illusion ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The datasets (english [69], dutch [17]) study the VP-forgetting hypothesis [25] for different languages. They use the same predictor and priors. The predictor is ", "page_idx": 21}, {"type": "equation", "text": "$$\ny_{i}=\\alpha+u_{1,g_{1,i},1}+u_{2,g_{2,i},1}+t_{i}(\\beta+u_{1,g_{1,i},2}+u_{2,g_{2,i},2})+\\epsilon,\\epsilon\\sim{\\cal N}(0,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $t_{i}$ is the treatment variable and $t_{i}\\in\\{-1,1\\}$ . And the prior is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{0},\\mathrm{diag}(1^{2},1^{2})),\\ \\mathbf{L}_{1}\\sim\\mathrm{LKJCholesky}(2,1),\\ \\mathbf{T}_{2}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(1^{2},1^{2})),\\ \\mathbf{L}_{2}\\sim\\mathrm{LKJCholesky}(2,1),}\\\\ &{10^{2}),\\ \\beta\\sim\\mathcal{N}(0,5^{2}),\\ \\sigma\\sim\\mathcal{N}^{+}(0,5^{2}),\\ \\mathbf{u}_{1,j}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf T_{1}\\mathbf{L}_{1}\\mathbf{L}_{1}^{T}\\mathbf{T}_{1}),\\ \\mathbf{u}_{2,k}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf T_{2}\\mathbf{L}_{2}\\mathbf{L}_{2}^{T}\\mathbf{T}_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.3 Electrophysiological responses with N400 effect ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the study of language, the electroencephalography (EGG) responses with N400 effect is studied [47]. Experimental results of subjects from the Edinburgh lab are collected [48]. The predictor is ", "page_idx": 21}, {"type": "equation", "text": "$$\ny_{i}=\\alpha+u_{1,g_{1,i},1}+u_{2,g_{2,i},1}+t_{i}(\\beta+u_{1,g_{1,i},2}+u_{2,g_{2,i},2})+\\epsilon,\\epsilon\\sim{\\cal N}(0,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $t_{i}$ is the treatment variable and $t_{i}\\in[0,1]$ . And the prior is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{r}_{1}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(20^{2},20^{2})),\\,\\,\\mathbf{L}_{1}\\sim\\mathrm{LKJCholesky}(2,1),\\,\\,\\mathbf{T}_{2}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(20^{2},20^{2})),\\,\\,\\mathbf{L}_{2}\\sim\\mathrm{LKJCholesky}(20^{2},10^{2}),}\\\\ &{\\imath\\sim\\mathcal{N}(0,10^{2}),\\,\\,\\beta\\sim\\mathcal{N}(0,10^{2}),\\,\\,\\sigma\\sim\\mathcal{N}^{+}(0,50^{2}),\\,\\,\\mathbf{u}_{1,j}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf T_{1}\\mathbf{L}_{1}\\mathbf{L}_{1}^{T}\\mathbf{T}_{1}),\\,\\,\\mathbf{u}_{2,k}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf T_{2}\\mathbf{L}_{2}\\mathbf{L}_{2}\\mathbf{T}_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.4 Subjective and objective relatives ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Grodner and Gibson [29] $(\\mathrm{gg05})$ studies the processing time difference between object relative clause and subject relative clause sentences. The predictor is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{og}(y_{i})=\\alpha+u_{1,g_{1,i},1}+u_{2,g_{2,i},1}+u_{3,g_{3,i},1}+t_{i}(\\beta+u_{1,g_{1,i},2}+u_{2,g_{2,i},2}+u_{3,g_{3,i},1})+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the treatment variable $t_{i}\\,\\in\\,\\{-1,1\\}$ . The third effect $\\mathbf{u}_{3}$ is related to different repeats of the experiment and has only two groups. We consider the first two effects for marginalization to match the other experiments. The prior for the variables is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf T}_{1}\\sim\\mathcal{N}^{+}({\\bf0},\\mathrm{diag}(5^{2},5^{2})),\\ {\\bf T}_{2}\\sim\\mathcal{N}^{+}({\\bf0},\\mathrm{diag}(5^{2},5^{2})),\\ {\\bf T}_{3}\\sim\\mathcal{N}^{+}({\\bf0},\\mathrm{diag}(5^{2},5^{2})),}\\\\ &{\\quad{\\bf L}_{1}\\sim\\mathrm{LKJCholesky}(2,1),\\ {\\bf L}_{2}\\sim\\mathrm{LKJCholesky}(2,1),\\ {\\bf L}_{3}\\sim\\mathrm{LKJCholesky}(2,1),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\alpha\\sim\\mathcal{N}(0,10^{2}),\\ \\beta\\sim\\mathcal{N}(0,5^{2}),\\ \\sigma\\sim\\mathcal{N}^{+}(0,5^{2}),}\\\\ &{{\\bf u}_{1,j}\\sim\\mathcal{N}({\\bf0},{\\bf T}_{1}{\\bf L}_{1}{\\bf L}_{1}^{T}{\\bf T}_{1}),\\ {\\bf u}_{2,k}\\sim\\mathcal{N}({\\bf0},{\\bf T}_{2}{\\bf L}_{2}{\\bf L}_{2}^{T}{\\bf T}_{2}),\\ {\\bf u}_{3,l}\\sim\\mathcal{N}({\\bf0},{\\bf T}_{3}{\\bf L}_{3}{\\bf L}_{3}^{T}{\\bf T}_{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.5 Relative clause processing in Mandarin Chinese ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The datasets (mandarin [75], mandarin2 [70]) are collected from experiments to study the effect of relative clause type on reading time of Mandarin Chinese. In our model, the predictor is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log(y_{i})=\\alpha+u_{1,g_{1,i},1}+u_{2,g_{2,i},1}+t_{i}(\\beta+u_{1,g_{1,i},2}+u_{2,g_{2,i},2})+\\epsilon,\\epsilon\\sim{\\cal N}(0,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $t_{i}$ is the treatment variable and $t_{i}\\in\\{-0.5,0.5\\}$ . And the prior is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varsigma^{+}(0,\\mathrm{diag}(5^{2},5^{2})),\\ \\mathbf{L}_{1}\\sim\\mathrm{LKJCholesky}(2,1),\\ \\mathbf{T}_{2}\\sim\\mathcal{N}^{+}(0,\\mathrm{diag}(5^{2},5^{2})),\\ \\mathbf{L}_{2}\\sim\\mathrm{LKJCholesky}(2,1),\\ \\mathbf{T}_{3}\\sim\\mathcal{N}^{+}(0,1),}\\\\ &{(0,10^{2}),\\ \\beta\\sim\\mathcal{N}(0,5^{2}),\\ \\sigma\\sim\\mathcal{N}^{+}(0,5^{2}),\\ \\mathbf{u}_{1,j}\\sim\\mathcal{N}(0,\\mathbf T_{1}\\mathbf L_{1}\\mathbf L_{1}^{T}\\mathbf T_{1}),\\ \\mathbf{u}_{2,k}\\sim\\mathcal{N}(0,\\mathbf T_{2}\\mathbf L_{2}\\mathbf L_{2}^{T}\\mathbf T_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.6 The Stroop effect ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Stroop effect describes the change of response time between congruent and incongruent stimuli [38]. The dataset is from Ebersole et al. [16]. Different from the other models, the noise scale for each observation is also grouped. In our model, the predictor is ", "page_idx": 21}, {"type": "text", "text": "$\\log(y_{i})=\\alpha+u_{g_{i},1}+t_{i}(\\beta+u_{g_{i},2})+\\epsilon,\\,\\epsilon\\sim\\mathcal{N}(0,\\sigma_{i}^{2}),\\,\\sigma_{i}=\\exp(\\sigma_{\\alpha}+s_{g_{i},1}+t_{i}(\\sigma_{\\beta}+s_{g_{i},2})),$ and the treatment variable is $t_{i}\\in\\{-1,1\\}$ . Priors for the model are ", "page_idx": 21}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{+}(\\mathbf{0},\\mathrm{diag}(1,1)),\\ \\mathbf{L_{u}}\\sim\\mathrm{LKJCholesky}(2,1),\\ \\mathbf{T}_{\\sigma}\\sim\\mathcal{N}^{+}(\\mathbf{0},\\mathrm{diag}(1,1)),\\ \\mathbf{L}_{\\sigma}\\sim\\mathrm{LKJCholesky}(2,1),}\\\\ &{\\quad\\quad\\quad\\alpha\\sim\\mathcal{N}(\\mathbf{6},1.5^{2}),\\ \\beta\\sim\\mathcal{N}(0,0.01^{2}),\\ \\sigma_{\\alpha}\\sim\\mathcal{N}(0,1),\\ \\sigma_{\\beta}\\sim\\mathcal{N}(0,1),}\\\\ &{\\quad\\quad\\quad\\quad\\mathbf{u}_{j}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{T_{u}}\\mathbf{L_{u}}\\mathbf{L_{u}^{T}}\\mathbf{T_{u}}),\\ \\mathbf{s}_{j}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{T}_{\\sigma}\\mathbf{L_{\\sigma}}\\mathbf{L_{\\sigma}^{T}}\\mathbf{T}_{\\sigma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 22}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 22}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 22}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 22}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 22}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our major claim is the faster algorithm for marginalization and applications in various models, which are supported by the theories and experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The discussion section covers three future directions to improve over. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The details and proofs are included in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The algorithm we propose can be directly implemented and we use a publicly available HMC. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide the data and code in the supplemental material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 24}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The hyperparameters for all models are included in the appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We reproduce all experiments five times and report error bar or std if possible. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report both the type of machines and the time in our experiments. The memory usage is below 32GB in all experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have reviewed and verified that it confirms to the CoE of NeurIPS. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper is methodological in nature. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The data and model are all publicly available. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have cited all the frameworks and datasets we use. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have documented our submitted codes. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We use public datasets and do not conduct research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We use public datasets and do not conduct research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]