[{"type": "text", "text": "Stochastic Optimal Control for Diffusion Bridges in Function Spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Byoungwoo Park1, Jungwon Choi1, Sungbin Lim2,3,\u2217 Juho Lee1\u2217 KAIST1, Korea University2, LG AI Research3 {bw.park, jungwon.choi, juholee}@kaist.ac.kr, sungbin@korea.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in diffusion models and diffusion bridges primarily focus on finite-dimensional spaces, yet many real-world problems necessitate operations in infinite-dimensional function spaces for more natural and interpretable formulations. In this paper, we present a theory of stochastic optimal control (SOC) tailored to infinite-dimensional spaces, aiming to extend diffusion-based algorithms to function spaces. Specifically, we demonstrate how Doob\u2019s $h$ -transform, the fundamental tool for constructing diffusion bridges, can be derived from the SOC perspective and expanded to infinite dimensions. This expansion presents a challenge, as infinite-dimensional spaces typically lack closed-form densities. Leveraging our theory, we establish that solving the optimal control problem with a specific objective function choice is equivalent to learning diffusion-based generative models. We propose two applications: 1) learning bridges between two infinite-dimensional distributions and 2) generative models for sampling from an infinite-dimensional distribution. Our approach proves effective for diverse problems involving continuous function space representations, such as resolution-free images, time-series data, and probability density functions. Code is available at https://github.com/bw-park/DBFS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic Optimal Control (SOC) is designed to steer a noisy system toward a desired state by minimizing a specific cost function. This methodology finds extensive applications across various fields in science and engineering, including rate event simulation [33, 35], stochastic flitering and data assimilation [47, 55, 72], non-convex optimization [9], modeling population dynamics [8, 43]. SOC is also related to diffusion-based sampling methods that are predominant in machine learning literature. Specifically, if we choose the terminal cost of a control problem as the log density ratio between a target distribution and a simple prior distribution, solving the optimal control reduces to learning a diffusion-based generative models [56, 73, 77] built upon the Schro\u00a8dinger bridge problem [12, 54]. ", "page_idx": 0}, {"type": "text", "text": "While SOC associated diffusion-based generative models have been well-established for finitedimensional spaces [5, 10, 11, 44], their theoretical foundations and practical algorithms for infinitedimensional spaces remain underexplored. There is a growing interest in developing generative models in function spaces. Examples include learning neural operators for partial differential equations (PDEs) [37, 39, 40], interpreting images as discretized functions through implicit neural representations (INRs) [21, 63], and operating in function spaces for Bayesian neural networks (BNNs) [65, 74]. Models that operate in function spaces are more parameter-efficient as they avoid resolution-specific parameterizations. In response to this demand, several extensions of diffusion-based generative models for infinite-dimensional function spaces have been proposed [3, 25, 32, 41, 42, 53]. However, SOC theory for building diffusion bridges in function spaces is still demanding in the community of generative modeling. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these challenges, this work introduces an extension of SOC for diffusion bridges in infinite-dimensional Hilbert spaces, particularly focusing on its applications in sampling problems. Specifically, we demonstrate the idea of Doob\u2019s $h$ -transform [13, 59] can be derived from SOC theory and extend its formulation into Hilbert spaces. Due to the absence of a density with respect to the Lebesgue measure in infinite-dimensional spaces, building a diffusion bridge between function spaces is a nontrivial task separated from the finite-dimensional cases [51, 61]. To this end, we propose a Radon-Nikodym derivative relative to a specified Gaussian reference measure in Hilbert space. Leveraging the infinite-dimensional Doob\u2019s $h$ -transform and SOC, we then formulate diffusion bridgebased sampling algorithms in function spaces. While the infinite-dimensional Doob\u2019s $h$ -transform has already been derived in [2, 26, 31], our main goal is not merely to derive it. Instead, we aim to generalize various finite-dimensional sampling problems [51, 61, 73, 77] into the infinite-dimensional space by exploiting the theory of infinite-dimensional SOC. ", "page_idx": 1}, {"type": "text", "text": "To demonstrate the applicability of our theory, we present learning algorithms for two representative problems. First, we introduce an infinite-dimensional bridge-matching algorithm as an extension of previous methods [51, 61] into Hilbert spaces, which learns a generative model to bridge two distributions defined in function spaces. As an example, we show that our framework can learn smooth transitions between two image distributions in a resolution-free manner. Second, we propose a simulation-based Bayesian inference algorithm [73, 77] that operates in function space. Instead of directly approximating the target Bayesian posterior, our algorithm learns a stochastic transition from the prior to the posterior within the function space. We demonstrate the utility of this approach by inferring Bayesian posteriors of stochastic processes, such as Gaussian processes. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Based on the SOC theory, we derive the Doob\u2019s $h$ -transform in Hilbert spaces. We propose a $h$ function as a Randon-Nikodym derivative with respect to a Gaussian measure in infinitedimensional space. \u2022 Based on the infinite-dimensional extension of the Doob\u2019s $h$ -transform, we present the diffusion bridge and simulation-based Bayesian inference algorithm in function spaces. \u2022 We demonstrate our method for various real-world problems involving function spaces, including resolution-free image translation and posterior sampling for stochastic processes. ", "page_idx": 1}, {"type": "text", "text": "Notation. Consider a real and separable Hilbert space $\\mathcal{H}$ with the norm and inner product denoted by $\\left\\|\\cdot\\right\\|_{\\mathcal{H}}$ and $\\langle\\cdot,\\cdot\\rangle_{\\mathcal{H}}$ . Throughout the paper, we consider a path measure denoted by $\\mathbb{P}^{(\\cdot)}$ on the space of all continuous mappings $\\Omega=\\bar{C}([0,T],\\mathcal{H})$ . The stochastic processes associated with this path measure $\\mathbb{P}^{(\\cdot)}$ are denoted by $\\mathbf{X}^{(\\cdot)}$ , and their time-marginal distribution at time $t\\in[0,T]$ as push-forward measure $\\mu_{t}^{(\\cdot)}:=(\\mathbf{X}_{t}^{(\\cdot)})_{\\#}\\mathbb{P}^{(\\cdot)}$ . Furthermore, for a function $\\nu:[0,T]\\times\\mathcal{H}\\to\\mathbb{R}$ , we define $D_{\\mathbf{x}}\\nu,D_{\\mathbf{x}\\mathbf{x}}\\nu$ as the first and second order Fr\u00b4echet derivatives with respect to the variable $\\mathbf{x}\\in{\\mathcal{H}}$ , respectively, and $\\partial_{t}\\mathcal{V}$ as the derivative with respect to the time variable $\\bar{t}\\in[0,T]$ . ", "page_idx": 1}, {"type": "text", "text": "2 A Foundation on Stochastic Optimal Control for Diffusion Bridges ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we first present a brief introduction to the theory of stochastic optimal control (SOC) in infinite dimensions and the Verification Theorem (Lemma 2.1), which is the key to understanding the theoretical connection between SOC and the diffusion bridges. Then we propose Doob\u2019s $h$ -transform in Hilbert spaces based on the SOC theory (Theorems 2.2 and 2.3). ", "page_idx": 1}, {"type": "text", "text": "2.1 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Gaussian Measure and Cameron-Martin Space. Let $(\\Omega,{\\mathcal{F}},\\mathbb{Q})$ and $(\\mathcal{H},\\mathcal{B}(\\mathcal{H}))$ be two measurable spaces and consider $\\mathcal{H}$ -valued random variable $\\mathbf{X}:\\Omega\\rightarrow\\mathcal{H}$ such that the push-forward measure $\\mu:=\\mathbf{X}_{\\#}\\mathbb{Q}$ induced by $\\mathbf{X}$ is Gaussian, i.e., a real-valued random variable $\\langle u,\\mathbf{X}\\rangle_{\\mathcal{H}}$ follows Gaussian distribution on $\\mathbb{R}$ for any $u\\in\\mathcal H$ . Then, there exist a unique mean $m_{\\mathbf{X}}\\in\\mathcal{H}$ given by $\\langle m_{\\mathbf{X}},u\\rangle_{\\mathcal{H}}\\,=\\,{\\mathbf{E}}_{\\mu}\\left[\\langle{\\mathbf{X}},u\\rangle_{\\mathcal{H}}\\right]$ and a nonnegative, symmetric, and self-adjoint covariance operator $Q:\\mathcal{H}\\rightarrow\\mathcal{H}$ defined by $\\stackrel{\\triangledown}{\\langle{u,Q v}\\rangle}_{\\mathcal{H}}\\,=\\,\\bar{\\mathbb{E}_{\\mu}}\\left[\\langle{\\mathbf{X}}-m{\\mathbf{x}},u\\rangle_{\\mathcal{H}},\\langle{\\mathbf{X}}-m{\\mathbf{x}},\\bar{v}\\rangle_{\\mathcal{H}}\\right]$ for any $u,v\\,\\in\\,{\\mathcal{H}}$ . ", "page_idx": 1}, {"type": "text", "text": "We write $\\mu=\\mathcal{N}(m\\mathbf{x},Q)$ and say $\\mathbf{X}$ is centered if $m_{\\mathbf{X}}\\,=\\,0$ . For a covariance operator $Q$ , we assume $\\mathcal{H}$ -valued centered $\\mathbf{X}$ follows the distribution $\\mathcal{N}(0,Q)$ supported on $\\mathcal{H}$ so that $Q$ is guaranteed to be compact. Hence, there exists an eigen-system $\\{(\\lambda^{(k)},\\phi^{(k)})\\in\\mathbb{R}\\times\\mathcal{H}:k\\in\\mathbb{N}\\}$ such that ${\\cal Q}(\\phi^{(k)})\\,=\\,\\bar{\\lambda}^{(k)}\\phi^{(k)}$ holds and $\\begin{array}{r}{\\mathrm{Tr}(Q)=\\sum_{k=1}^{\\infty}\\lambda^{(k)}\\,<\\,\\infty}\\end{array}$ . We define the Cameron-Martin space by $\\mathcal{H}_{0}:=Q^{1/2}(\\mathcal{H})$ , $\\mathcal{H}_{0}\\subseteq\\mathcal{H}$ is a separable Hilbert space endowed with the inner product $\\langle u,v\\rangle_{\\mathcal{H}_{0}}:=\\langle Q^{-1/2}u,Q^{-1/2}v\\rangle_{\\mathcal{H}}$ . ", "page_idx": 2}, {"type": "text", "text": "Stochastic Differential Equations in Hilbert Spaces. The standard $\\mathbb{R}^{d}$ -valued Wiener process has independent increments $\\bar{w_{t+\\Delta_{t}}}-w_{t}\\sim\\mathcal{N}(0,\\bar{\\Delta_{t}}\\mathbf{\\bar{I}}_{d})$ . In the case of infinite-dimensional Hilbert space $\\mathcal{H}_{0}$ , however, such identity covariance may not be a trace class. We consider a larger space $\\mathcal{H}_{0}\\subset\\mathcal{H}_{1}$ such that $\\mathcal{H}_{0}$ is embedded into $\\mathcal{H}_{1}$ with a Hilbert-Schmidt embedding $J:\\mathcal{H}_{0}\\to\\mathcal{H}_{1}$ . Let $Q:=J J^{\\ast}$ and we define $\\mathcal{H}_{1}$ -valued $Q$ -Wiener process [17, Proposition 4.7] as $\\begin{array}{r}{\\mathbf{W}_{t}^{Q}=\\sum_{k=1}^{\\infty}Q^{1/2}\\phi^{(k)}w_{t}^{(k)}}\\end{array}$ . We focus on the Cameron-Martin space $\\mathcal{H}_{0}$ where the Wiener process has increments $\\mathcal{N}(0,\\Delta_{t}\\mathbf{I}_{\\mathcal{H}})$ , and a larger space $\\mathcal{H}_{1}=\\mathcal{H}$ , where the Wiener process has increments $\\mathcal{N}(0,\\Delta_{t}Q)$ . Then we define a path measure $\\mathbb{P}$ and associated stochastic differential equation (SDE) in $\\mathcal{H}$ as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{t}=\\mathcal{A}\\mathbf{X}_{t}+\\sigma d\\mathbf{W}_{t}^{Q},\\quad\\mathbf{X}_{0}\\in\\mathcal{H},\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A:\\mathcal{H}\\to\\mathcal{H}$ is a linear operator, a constant $\\sigma>0$ and a $Q$ -Wiener process ${\\bf W}^{Q}$ defined on a probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq0},\\mathbb{P})$ . For a more comprehensive understanding, see [17, 29]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Stochastic Optimal Control in Hilbert Spaces ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "From the uncontrolled SDE introduced in equation (1), various sampling problems in $\\mathbb{R}^{d}$ including density sampling [5, 73, 77] and generative modeling [10, 11] can be solved by adjusting the SDE with proper drift (control) function $\\alpha\\in\\mathbb{R}^{d}$ . Motivated by these approaches, introducing stochastic optimal control (SOC) to solve real-world sampling problems, we aim to introduce SOC to the infinite-dimensional Hilbert space $\\mathcal{H}$ . We consider that a controlled path measures $\\mathbb{P}^{\\alpha}$ is induced by following infinite-dimensional SDE defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{t}^{\\alpha}=\\left[A\\mathbf{X}_{t}^{\\alpha}+\\sigma Q^{1/2}\\alpha_{t}\\right]d t+\\sigma d\\tilde{\\mathbf{W}}_{t}^{Q},\\quad\\mathbf{X}_{0}^{\\alpha}=\\mathbf{x}_{0},\\quad t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha_{(\\cdot)}:[0,T]\\times\\mathcal{H}\\to\\mathcal{H}$ is infinite-dimensional Markov control (see Section A.1.2 for more details). We refer to the SDE in equation (2) as controlled SDE. The controlled SDE can be exploited to the various problems. In general, it can be done by finding the optimal control function that minimizes the objective functional with suitably chosen cost functionals depending on the problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{I}(t,\\mathbf{x}_{t},\\alpha)=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\int_{t}^{T}\\left[R(\\alpha_{s})\\right]d s+G(\\mathbf{X}_{T}^{\\alpha})\\big\\vert\\mathbf{X}_{t}^{\\alpha}=\\mathbf{x}_{t}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $R:\\mathcal{H}\\to\\mathbb{R}$ are running cost and $G:\\mathcal{H}\\rightarrow\\mathbb{R}$ is terminal cost. The measurable function $\\mathcal{I}(t,\\mathbf{x},\\alpha)$ , representing the total cost incurred by the control $\\alpha$ over the interval $[t,T]$ , given that the control strategy from the interval $[0,t]$ has resulted in $\\mathbf{X}_{t}^{\\alpha}=\\mathbf{x}$ . The objective is to minimize the objective functional in (3) over all admissible control policies $\\alpha\\in\\mathcal{U}$ , where $\\boldsymbol{\\mathcal{U}}$ is the Hilbert space of all square-integrable $\\mathcal{H}$ -valued processes adapted to ${\\bf W}^{Q}$ defined on $[0,T]$ . Then we define the value function $\\mathcal{V}(t,\\mathbf{x})=\\operatorname*{inf}_{\\alpha\\in\\mathcal{U}}\\mathcal{I}(t,\\mathbf{x},\\alpha)$ , the optimal costs conditioned on $(t,\\mathbf{x})\\in[0,T]\\times\\mathcal{H}$ . By using the dynamic programming [23], we can solve the Hamilton-Jacobi-Bellman (HJB) equation, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{t}\\mathcal{V}_{t}+\\mathcal{L}\\mathcal{V}_{t}+\\operatorname*{inf}_{\\alpha\\in\\mathcal{U}}\\left[\\langle\\alpha,\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}_{t}\\rangle+R\\right]=0,\\quad\\mathcal{V}(T,\\mathbf{x})=G(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{L}\\mathcal{V}_{t}:=\\langle\\mathbf{X}_{t}^{\\alpha},\\mathcal{A}D_{\\mathbf{x}}\\mathcal{V}_{t}\\rangle_{\\mathcal{H}}+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}Q D_{\\mathbf{xx}}\\mathcal{V}_{t}\\right]}\\end{array}$ . We demonstrate that with specific choices of cost functionals $R$ and $G$ in (3), the optimal control $\\alpha^{\\star}$ of the minimization problem aligns with the proper drift function for sampling problems we will discuss later. To do so, we start with how the HJB equation can characterize the optimal controls. ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.1 (Verification Theorem). Let $\\nu$ be a solution of HJB equation (4) with $\\begin{array}{r}{R(\\alpha):=\\frac{1}{2}\\left\\lVert\\alpha\\right\\rVert_{\\mathcal{H}}^{2}}\\end{array}$ satisfying the assumptions in $A.I$ . Then, we have $\\mathcal{V}(t,\\mathbf{x})\\;\\leq\\;\\mathcal{I}(t,x,\\alpha)$ for every $\\alpha\\,\\in\\,{\\mathcal{U}}$ and $(t,\\mathbf{x})\\in[0,T]\\times\\mathcal{H}$ . Let $(\\alpha^{*},\\mathbf{X}^{\\alpha^{*}})$ be an admissible pair such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha_{s}^{*}=\\underset{\\alpha\\in\\mathcal{U}}{\\arg\\operatorname*{inf}}\\left[\\left\\langle\\alpha_{s},\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}_{t}\\right\\rangle+\\frac{1}{2}\\left\\Vert\\alpha_{s}\\right\\Vert_{\\mathcal{H}}^{2}\\right]=-\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha^{*}})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for almost every $s\\in[t,T]$ and $\\mathbb{P}$ -almost surely. Then $(\\alpha^{*},\\mathbf{X}^{\\alpha^{*}})$ satisfying $\\mathcal{V}(t,\\mathbf{x})=\\mathcal{I}(t,\\mathbf{x},\\alpha^{*})$ . ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.1 demonstrate that with specific choices of running costs, the solution to the HJB equation in (4) is the optimal cost of the minimization problem in (3) with the closed-form optimal control $\\alpha_{t}^{*}=-\\sigma Q^{1/2}D\\mathcal{V}_{t}$ . In the subsequent subsection, we reveal the connection between the optimal controlled process, characterized by the controlled SDE with optimal control $\\alpha^{\\star}$ , and the conditioned SDE in $\\mathcal{H}$ . Additionally, we show various problems depending on the choice of the terminal cost $G$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Doob\u2019s $h$ -transform for Diffusion Bridges in Hilbert Spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The conditioned SDE is a stochastic process that is guaranteed to satisfy a set constraint defined over the interval $[0,T]$ . For instance, a Diffusion Bridge is a stochastic process that satisfies both terminal constraints $\\mathbf{X}_{0}=\\mathbf{x}_{0}$ , $\\mathbf{X}_{T}=\\mathbf{x}_{T}$ for any $\\mathbf{x}_{0},\\mathbf{x}_{T}\\in\\mathcal{H}$ . We will show that with a proper choice of the terminal cost $G$ , the conditioned SDE is a special case of controlled SDE. For this, let us define the function $h:[0,T]\\times\\mathcal{H}\\to\\mathbb{R}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(t,\\mathbf{x})=\\int_{\\mathcal{H}}\\tilde{G}(\\mathbf{z})\\mathcal{N}_{e^{(T-t).A}\\mathbf{x},Q_{T-t}}(d\\mathbf{z})=\\mathbb{E}_{\\mathbb{P}}\\left[\\tilde{G}(\\mathbf{X}_{T})\\vert\\mathbf{X}_{t}=\\mathbf{x}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{N}_{e^{t\\mathcal{A}}{\\bf x},Q_{t}}$ is a Gaussian measure with mean function $e^{t\\mathcal{A}_{\\mathbf{X}}}$ and covariance operator $Q_{t}\\,=$ $\\begin{array}{r}{\\int_{0}^{t}e^{(t-s)\\mathcal{A}}Q e^{(t-s)\\mathcal{A}}d s}\\end{array}$ and $\\tilde{G}:=e^{-G}$ for the function $G$ in (3). The function $h$ evaluates the future states $\\mathbf{X}_{T}$ using $\\tilde{G}$ which propagated by (1) for a given initial $\\mathbf{x}$ at time $t\\in[0,T]$ . It can be shown that the function $h$ satisfies the Kolmogorov-backward equation [17] with terminal condition, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}h_{t}+\\mathscr{L}h_{t}=0,\\quad h(T,\\mathbf{x})=\\tilde{G}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now, we employ the Hopf-Cole transformation [24] to establish an inherent connection between two classes of PDEs, the linear PDE in (7) and the HJB equation in (4), which provide us a key insight to deriving the Doob\u2019s $h$ -transform in function spaces utilizing the SOC theory. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.2 (Hopf-Cole Transform). Let $\\mathcal{V}_{t}=-\\log h_{t}$ . Then $\\mathcal{V}_{t}$ satisfies the HJB equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}\\mathcal{V}_{t}+\\mathscr{L}\\mathcal{V}_{t}-\\frac{1}{2}\\left\\|\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}_{t}\\right\\|_{\\mathcal{H}}^{2}=0,\\quad\\mathcal{V}(T,\\mathbf{x})=G(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "According to Theorem 2.2, the solution of linear PDE in equation (7) is negative exponential to the solution of the HJB equation in (8). Given that it already verified that the optimal control $\\alpha^{\\star}$ results the value function $\\mathcal{V}_{t}$ , which has explicit form as described in (8), we find that the relationship of two PDEs through $\\mathcal{V}_{t}=-\\log h_{t}$ leads to a distinct form of optimal control $\\alpha^{\\star}=-\\sigma Q^{1/2}D_{\\bf x}\\bar{\\nu}=$ $\\sigma Q^{1/2}D_{\\mathbf{x}}\\log h$ , where $D_{\\mathbf{x}}\\log h:=D_{\\mathbf{x}}h/h$ . Consequently, it yields another class of SDE as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{t}^{h}=\\left[A\\mathbf{X}_{t}^{h}d t+\\sigma^{2}Q D_{\\mathbf{x}}\\log h(t,\\mathbf{X}_{t}^{h})\\right]d t+\\sigma d\\hat{\\mathbf{W}}_{t}^{Q},\\quad\\mathbf{X}_{0}^{h}=\\mathbf{x}_{0},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mathbf{W}}_{t}^{Q}$ is a $Q$ -Wiener process on $\\mathbb{P}^{h}$ . This representation is consistent with infinite-dimensional conditional SDE [2, 26] which induces an expansion of Doob\u2019s $h$ -transform [59] in Hilbert space $\\mathcal{H}$ . ", "page_idx": 3}, {"type": "text", "text": "The Doob\u2019s $h$ -transform in finite-dimensional spaces is well-established to construct the diffusion bridge process under the assumption that $h(t,\\bar{\\mathbf{x}_{}})=\\mathbb{P}(\\mathbf{X}_{T}\\in d\\mathbf{x}_{T}|\\mathbf{X}_{t}=\\mathbf{x})$ has an explicit RadonNikodym density function [46, 60], enable to simulate the bridge SDEs. In contrast, although the choice of $\\tilde{G}(\\mathbf{x})=\\mathbf{1}_{d\\mathbf{x}_{T}}(\\mathbf{x}_{T})$ in (6) yields same representation of $h(t,\\mathbf{x})=\\mathbb{P}(\\mathbf{X}_{T}\\in d\\mathbf{x}_{T}|\\mathbf{X}_{t}=\\mathbf{x})$ , we cannot easily define the Radon-Nikodym density in $\\mathcal{H}$ due to the absence of an equivalent form of Lesbesgue measure which hinder the computation of $D_{\\mathbf{x}}\\log h(t,{\\mathbf{X}}_{t}^{h})$ in (9) explicitly. Hence, to define the diffusion bridge process in $\\mathcal{H}$ , we need to identify an explicit density form of $h(t,\\mathbf{x})=\\mathbb{P}(\\mathbf{X}_{T}\\in d\\mathbf{x}_{T}|\\mathbf{X}_{t}=\\mathbf{x})$ . The following theorem reveals the explicit form of $h$ function and becomes a key ingredient in deriving infinite dimensional diffusion bridge processes. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.3 (Explicit Representation of $h$ ). For any $t>0$ and any $\\mathbf{x}\\in{\\mathcal{H}}$ , the measure $\\mathcal{N}_{e^{t\\mathcal{A}}\\mathbf{x},Q_{t}}$ and $\\mathcal{N}_{0,Q_{\\infty}}$ are equivalent, where $\\mathcal{N}_{0,Q_{\\infty}}$ is an invariant measure of $\\mathbb{P}$ in (1) as $t\\,\\rightarrow\\,\\infty$ where $\\begin{array}{r}{Q_{\\infty}=-\\frac{1}{2}Q A^{-1}}\\end{array}$ . Moreover, for any $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{H}}$ , the Radon-Nikodym density ddNNetAx,Qt(\u00b7) = qt(x, \u00b7) is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}({\\bf x},{\\bf y})=d e t(1-\\Theta_{t})^{-1/2}\\exp\\bigg[-\\frac{1}{2}\\langle(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}e^{t A}{\\bf x},Q_{\\infty}^{-1/2}e^{t A}{\\bf x}\\rangle_{\\mathcal{H}}}\\\\ &{\\quad\\quad\\quad+\\langle(1-\\Theta_{t})^{-1}e^{t A}Q_{\\infty}^{-1/2}{\\bf x},Q_{\\infty}^{-1/2}{\\bf y}\\rangle_{\\mathcal{H}}-\\frac{1}{2}\\langle\\Theta_{t}(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}{\\bf y},Q_{\\infty}^{-1/2}{\\bf y}\\rangle_{\\mathcal{H}}\\bigg],}\\\\ &{h e r e\\,\\Theta_{t}=Q_{\\infty}^{1/2}(Q_{t}^{-1/2}e^{t A})^{*}(Q_{\\infty}^{-1/2}Q_{t}^{1/2})^{*}(Q_{\\infty}^{1/2}(Q_{t}^{-1/2}e^{t A})^{*}(Q_{\\infty}^{-1/2}Q_{t}^{1/2})^{*})^{*},t\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 2.3 states that the marginal distribution of certain classes of SDEs described in (1) has an explicit Radon-Nikodym density with respect to their invariant measure. Therefore, by the time-homogeneity of the process in (1), it allows us to define the $h$ function explicitly: ", "page_idx": 4}, {"type": "equation", "text": "$$\nh(t,\\mathbf{x})=\\int_{\\mathcal{U}}\\tilde{G}(\\mathbf{z})\\mathcal{N}_{e^{(T-t).A}\\mathbf{x},Q_{T-t}}(d\\mathbf{z})=\\int_{\\mathcal{U}}\\tilde{G}(\\mathbf{z})q_{T-t}(\\mathbf{x},\\mathbf{z})\\mathcal{N}_{0,Q_{\\infty}}(d\\mathbf{z}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This framework enables the construction of an infinite-dimensional bridge process, by selecting $\\tilde{G}$ in (12) properly. Below, we demonstrate the infinite-dimensional diffusion bridge. ", "page_idx": 4}, {"type": "text", "text": "Example 2.4 (Diffusion Bridge in $\\mathcal{H}$ ). Let $\\{(\\lambda^{(k)},\\phi^{(k)})\\in\\mathbb{R}\\times\\mathcal{H}:k\\in\\mathbb{N}\\}$ be an eigen-system of $\\mathcal{H}$ . Then for each $k\\in\\mathbb{N}$ , the SDE system in equation (1) can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{t}^{(k)}=-a_{k}\\mathbf{X}_{t}^{(k)}d t+\\sigma\\sqrt{\\lambda^{(k)}}d\\mathbf{W}_{t}^{(k)},\\quad\\mathbf{X}^{(k)}(0)=\\mathbf{x}_{0}^{(k)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{A}\\phi^{(k)}=-a_{k}\\phi^{(k)}$ , $Q\\phi^{(k)}=\\lambda^{(k)}\\phi^{(k)}$ , $\\mathbf{X}_{t}^{(k)}=\\langle\\mathbf{X}_{t},\\boldsymbol{\\phi}^{(k)}\\rangle_{\\mathcal{H}}$ and $\\mathbf{W}_{t}^{(k)}=\\langle\\mathbf{W}_{t},\\phi^{(k)}\\rangle_{\\mathcal{H}}$ . Then, for any $\\mathbf{x}_{T}\\in\\mathcal{H}$ , the conditional law of $\\mathbf{x}_{T}^{(k)}$ given $\\mathbf{X}_{t}^{(k)}$ is a Gaussian $\\mathcal{N}(\\mathbf{m}_{T|t}^{(k)}\\mathbf{X}_{t}^{(k)},\\Sigma_{T|t}^{(k)})$ with ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\bf m}_{T|t}^{(k)}=e^{-a_{k}(T-t)},\\quad\\Sigma_{T|t}^{(k)}=\\sigma^{2}\\frac{\\lambda_{k}}{2a_{k}}\\left(1-e^{-2a_{k}(T-t)}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Now, by setting the terminal condition in (6) as $\\tilde{G}(\\mathbf{x})\\,:=\\,\\mathbf{1}_{\\mathbf{x}_{T}}(\\mathbf{x})$ (i.e., Dirac delta of $\\mathbf{x}_{T}$ ) then $h(t,{\\bf x})=q_{T-t}({\\bf x},{\\bf x}_{T})$ . Thus, for each coordinate $k$ , we get following representation: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{t}^{(k)}=\\left[-a_{k}\\mathbf{X}_{t}^{(k)}+\\frac{2a_{k}e^{-a_{k}(T-t)}}{1-e^{-2a_{k}(T-t)}}(\\mathbf{x}_{T}^{(k)}-e^{-a_{k}(T-t)}\\mathbf{X}_{t}^{(k)})\\right]d t+\\sigma\\sqrt{\\lambda^{(k)}}d\\mathbf{W}_{t}^{(k)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with two end points conditions $\\mathbf{X}_{0}^{(k)}=\\mathbf{x}_{0}^{(k)}$ )and X(Tk) $\\mathbf{X}_{T}^{(k)}=\\mathbf{x}_{T}^{(k)}$ ", "page_idx": 4}, {"type": "text", "text": "2.4 Approximating path measures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the function $h$ is intractable for a general terminal cost $\\tilde{G}$ in (12), simulating the conditioned SDEs in (9) requires some approximation techniques. As observed in Theorem 2.2, finding the function $h$ is equal to learning the control function $\\alpha$ such that $\\mathbb{P}^{\\alpha}$ is equal to $\\mathbb{P}^{\\star}:=\\mathbb{P}^{\\alpha^{\\star}}\\stackrel{\\mathcal{\\Lambda}}{=}\\mathbb{P}^{h}$ . Therefore, with a parametrization $\\alpha:=\\alpha(\\cdot,\\theta)$ the approximation can be done by neural network parameterization $i.e.,\\alpha^{\\star}\\approx\\alpha^{\\theta^{\\star}}$ with local minimum $\\theta^{\\star}=\\arg\\operatorname*{min}_{\\theta}D(\\mathbb{P}^{\\alpha}||\\mathbb{P}^{\\star})$ , where $D(\\mathbb{P}^{\\alpha}||\\mathbb{P}^{\\star})$ is a divergence between $\\mathbb{P}^{\\alpha}$ and $\\mathbb{P}^{\\star}$ . For example, the cost functional described in equation (3) can be represented as relative-entropy loss $\\begin{array}{r}{D_{\\mathrm{rel}}(\\mathbb{P}^{\\alpha}\\overset{\\cdot}{\\vert}\\,\\mathbb{P}^{\\star})=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\log\\frac{d\\mathbb{P}^{\\alpha}}{d\\mathbb{P}^{\\star}}\\right]^{2}}\\end{array}$ . Therefore, the training loss for $\\theta$ can be estimated by first simulating the parameterized  controPl path and then calculating (3) for a specified cost functional $R,G$ . Moreover, if we can access to the $\\mathbb{P}^{\\star}$ , one can define the variational optimization [69] where the loss is defined as cross-entropy loss $\\begin{array}{r}{D_{\\mathrm{cross}}(\\mathbb{P}^{\\alpha}||\\mathbb{P}^{\\star})=\\mathbb{E}_{\\mathbb{P}^{\\star}}\\left[\\log\\frac{d\\mathbb{P}^{\\star}}{d\\mathbb{P}^{\\alpha}}\\right]}\\end{array}$ . See [20, 48] for more details about the approximation technique and other loss functions. ", "page_idx": 4}, {"type": "text", "text": "3 Simulating Diffusion Bridges in Infinite Dimensional Spaces ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Leveraging the SOC theory within $\\mathcal{H}$ , we show how our approach generalizes existing diffusionbased sampling methods. Specifically, incorporating the relation between controlled SDEs (2) and conditioned SDEs (9), we introduce two learning algorithms that allow us to simulate various diffusion bridge-based sampling algorithms. ", "page_idx": 4}, {"type": "text", "text": "3.1 Infinite Dimensional Bridge Matching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, our objective is to learn a control $\\alpha$ that yields $\\mathbb{P}^{\\alpha}$ such that $\\{\\mathbf{X}_{t}^{\\alpha}\\}_{t\\in[0,T]}$ satisfies $\\mu_{t}^{\\alpha}\\,\\approx\\,\\mu_{t}^{\\star}$ for all pre-specified $\\mu_{t}^{\\star}$ over the interval $t\\,\\in\\,[0,T]$ . Specifically, we assume that the end-point marginals $\\mu_{\\mathrm{0}}^{\\star}$ and $\\mu_{T}^{\\star}$ follow the laws of two data distributions $\\pi_{0}$ and $\\pi_{T}$ , respectively, and the intermediate marginals $\\{\\mu_{t}^{\\star}\\}_{t\\in(0,T)}$ are defined as a mixture of diffusion bridge paths. This learning problem is referred to as the Bridge Matching (BM) algorithm [45, 50, 61] and can be expressed as a solution of the SOC problem structured as ", "page_idx": 4}, {"type": "table", "img_path": "WyQW4G57Zd/tmp/f276fe5a872e685e6d79f1291c8b71258b27cddc9f2cfd366caa41c909c442f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\alpha}D(\\mathbb{P}^{\\alpha}|\\mathbb{P}^{\\star}),\\mathrm{~such~that~}d{\\mathbf{X}}_{t}^{\\alpha}=\\left[A{\\mathbf{X}}_{t}^{\\alpha}+\\sigma Q^{1/2}\\alpha_{t}\\right]d t+\\sigma d\\tilde{\\mathbf{W}}_{t}^{Q},\\quad{\\mathbf{X}}_{0}^{\\alpha}\\sim\\pi_{0}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In (16), various divergences can be chosen for the same learning problem, as discussed in Section 2.4. Here, we will choose the cross-entropy because the relative entropy requires the appropriate selection of the terminal cost $G$ in (3), which is intractable since we do not have access to the distributional form of $\\pi_{0},\\pi_{T}$ [44]. Furthermore, keeping the entire computational graph of $\\mathbb{P}^{\\alpha}$ with parameterized $\\alpha$ can become resource-intensive, especially for higher-dimensional datasets like images [11]. ", "page_idx": 5}, {"type": "text", "text": "Now, we specify the optimal path measure $\\mathbb{P}^{\\star}$ for a problem in (16). Let $\\mathbb{P}_{|0,T}$ be a path measure induced by (15) and $\\mu_{t|0,T}$ be a marginal distribution of $\\mathbb{P}_{|0,T}$ . Moreover, let $\\mathbb{P}^{\\star}=\\mathbb{P}_{|0,T}\\Pi_{0,T}$ for an independent coupling $\\Pi_{0,T}=\\pi_{0}\\otimes\\pi_{T}$ . Then the optimal path measure $\\mathbb{P}^{\\star}$ is defined as Mixture of bridge. Under regular assumptions, the optimal control $\\alpha^{\\star}$ that induces the optimal path measure $\\mathbb{P}^{\\star}$ can be constructed as a mixture of functions $h$ in (9) by choosing $G(\\mathbf{x})=\\mathbf{1}_{\\mathbf{x}_{T}}^{\\overline{{}}}(\\mathbf{x})$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Mixture of Bridges in $\\mathcal{H}$ ). Let us consider a marginal distribution of $\\mathbb{P}^{\\star}$ at $t\\in[0,T]$ , $\\begin{array}{r}{\\mu_{t}^{\\star}(d\\mathbf{x}_{t})=\\int\\mu_{t|0,T}(d\\mathbf{x}_{t})\\Pi_{0,T}\\bar{(}d\\mathbf{x}_{0},d\\mathbf{\\dot{x}}_{T})}\\end{array}$ has density $p_{t}^{\\star}$ with respect to some Gaussian reference measure $\\mu_{r e f}\\,i.e.,\\mu_{t}^{\\star}(d\\mathbf{x}_{t})/\\mu_{r e f}(d\\mathbf{x}_{t})=p_{t}^{\\star}(\\mathbf{x}_{t})$ . Then the optimal path measure $\\mathbb{P}^{\\star}$ associated with: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\mathbf{X}_{t}^{\\star}=\\left[A\\mathbf{X}_{t}^{\\star}+\\mathbb{E}_{\\mathbf{x}_{T}\\sim\\mathbb{P}^{\\star}(d\\mathbf{x}_{T}|\\mathbf{X}_{t}^{\\star})}\\left[\\sigma^{2}Q D_{\\mathbf{x}}\\log\\mathcal{N}(\\mathbf{x}_{T};\\mathbf{m}_{T|t}\\mathbf{X}_{t}^{\\star},\\Sigma_{T|t})\\right]\\right]d t+\\sigma d\\hat{\\mathbf{W}}_{t}^{Q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Objective Functional for Bridge Matching. With the structure of $\\mathbb{P}^{\\star}$ specified in (17) we can estimate the divergence between the optimal target path measure $\\mathbb{P}^{\\star}$ and a path measure $\\mathbb{P}^{\\alpha}$ . To accomplish this, we first define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma(t,\\mathbf{x};\\theta)=Q^{1/2}\\left[\\mathbb{E}_{\\mathbf{x}_{T}\\sim\\mathbb{F}^{\\star}(d\\mathbf{x}_{T}\\vert\\mathbf{x})}\\left[\\sigma Q^{1/2}D_{\\mathbf{x}}\\log N(\\mathbf{x}_{T};\\mathbf{m}_{T\\vert t}\\mathbf{X}_{t}^{\\star},\\boldsymbol{\\Sigma}_{T\\vert t})\\right]-\\alpha(t,\\mathbf{x};\\theta)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, by applying the Girsanov theorem3 which provides us the Radon-Nikodym derivative between $\\mathbb{P}^{\\star}$ and $\\mathbb{P}^{\\alpha}$ , we can derive the cross-entropy loss in equation (16): ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{\\mathrm{cross}}(\\mathbb{P}^{\\alpha^{\\theta}}|\\mathbb{P}^{\\star})=\\mathbb{E}_{\\mathbb{P}^{\\star}}\\left[\\log\\frac{d\\mathbb{P}^{\\star}}{d\\mathbb{P}^{\\alpha^{\\theta}}}\\right]=\\mathbb{E}_{\\mathbb{P}^{\\star}}\\left[\\int_{0}^{T}\\frac{1}{2}\\left\\lVert\\gamma(t,\\mathbf{X}_{t}^{\\star};\\theta)\\right\\rVert_{\\mathcal{H}_{0}}^{2}d s\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, under the neural network parameterization of control function $\\alpha^{\\theta}$ , we can reformulate the SOC problem in (16) as a learning problem with the training loss function represented by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{BM}}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}_{0,T}}\\mathbb{E}_{\\mathbb{P}^{\\star}(\\mathbf{x}_{T}\\sim d\\mathbf{x}_{T}\\mid\\mathbf{X}_{t}^{\\star})}\\left[\\frac{1}{2}\\left\\|\\sigma Q^{1/2}D_{\\mathbf{x}}\\log\\mathcal{N}(\\mathbf{x}_{T};\\mathbf{m}_{T\\mid t}\\mathbf{X}_{t}^{\\star},\\boldsymbol{\\Sigma}_{T\\mid t})-\\alpha(t,\\mathbf{X}_{t}^{\\star};\\theta)\\right\\|_{\\mathcal{U}_{\\star}}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The ${\\mathcal{L}}_{\\mathrm{BM}}(\\theta)$ in (20) yields the infinite-dimensional BM summarized in $\\mathrm{Alg~l~}$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Bayesian Learning in Function Space ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous section, we observed that by appropriately defining a terminal cost functional $G$ in (3), the SOC problem aligns with the sampling problem, where optimal control effectively steers the distribution from $\\pi_{0}$ to the target distribution $\\pi_{T}$ , where we can access samples from $\\pi_{0}$ and $\\pi_{T}$ . However, accessing samples from unknown target distribution $\\pi_{T}$ is generally not feasible. For instance, for a $\\pi_{T}$ , a posterior distribution over function. In this case, although direct samples from $\\pi_{T}$ are unattainable, its distributional representation is given as [3, 66]: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{d\\pi_{T}}{d\\mu_{\\mathrm{prior}}}(\\mathbf{X}_{T})\\propto\\exp\\left(-\\mathcal{U}(\\mathbf{X}_{T})\\right),\\quad\\mu_{\\mathrm{prior}}=\\mathcal{N}(\\mathbf{m}_{\\mathrm{prior}},Q_{\\mathrm{prior}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{U}}$ is a energy function. Here, our primary objective is to sample from a distribution over function $\\pi_{T}:=\\mu_{T}^{\\star}$ by simulating the controlled diffusion process $\\{\\mathbf{X}_{t}^{\\bar{\\alpha}}\\}_{t\\in[0,T]}$ over finite horizon $[0,T]$ with $T<\\infty$ . It can be represented as a solution of the following SOC problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\alpha}D(\\mathbb{P}^{\\alpha}|\\mathbb{P}^{\\star}),\\mathrm{~such~that~}d{\\mathbf{X}}_{t}^{\\alpha}=\\left[A{\\mathbf{X}}_{t}^{\\alpha}+\\sigma Q^{1/2}\\alpha_{t}\\right]d t+\\sigma d\\tilde{\\mathbf{W}}_{t}^{Q},\\quad{\\mathbf{X}}_{0}^{\\alpha}={\\mathbf{x}}_{0}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following theorem implies that with a suitable terminal cost functional $G$ in (3), it is possible to achieve ${\\bf X}_{T}^{\\alpha^{\\star}}\\sim\\pi_{T}$ as an expansion of [54, 71] for infinite dimensional space $\\mathcal{H}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 (Exact sampling in $\\mathcal{H}$ ). Consider that the initial distribution $\\mu_{0}$ is given as the Dirac measure $\\delta_{\\mathbf{x}_{0}}$ for some $\\mathbf{x}_{0}\\in\\mathcal{H}$ and the following objective functional ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{J}(\\alpha)=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\int_{0}^{T}\\frac{1}{2}\\left\\lVert\\alpha_{s}\\right\\rVert_{\\mathcal{H}}^{2}d s-\\log\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T}^{\\alpha})\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu_{T}=\\mathcal{N}(e^{T\\mathcal{A}}\\mathbf{x}_{0},Q_{T})$ as a marginal distribution of $\\mathbf{X}_{T}$ in (1) with a well-defined terminal cost dd\u03c0\u00b5T by Theorem 2.3. Then, XT\u03b1\u22c6 \u223c\u03c0T . ", "page_idx": 6}, {"type": "text", "text": "Objective Functional for Bayesian Learning. Unlike problem in (16) where we can access to the target path measure $\\mathbb{P}^{\\star}$ directly, it is not feasible here because $\\begin{array}{r}{h(t,\\mathbf{x})=\\mathbb{E}_{\\mathbb{P}}\\left[-\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T})\\vert\\mathbf{X}_{t}=\\mathbf{x}\\right]}\\end{array}$ does not have an explicit solution. Therefore, we will use the relative-entropy loss as our training loss function: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Bayes}}(\\theta)=D_{\\mathrm{rel}}(\\mathbb{P}^{\\alpha^{\\theta}}|\\mathbb{P}^{\\star})=\\mathbb{E}_{\\mathbb{P}^{\\alpha^{\\theta}}}\\left[\\int_{0}^{T}\\frac{1}{2}\\left\\lVert\\alpha(s,\\mathbf{X}_{s}^{\\alpha^{\\theta}};\\theta)\\right\\rVert_{\\mathcal{H}}^{2}d s-\\log\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T}^{\\alpha^{\\theta}})\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The key difference from previous algorithms [73, 77] is that the Radon-Nikodym derivative $\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T}^{\\alpha})$ may not be well-defined on $\\mathcal{H}$ due to the absence of the Lesbesgue measure. However, the Theorem 2.3 suggests that by choosing certain classes of Gaussian measure i.e., $\\mu_{\\mathrm{prior}}:=\\mathcal{N}(e^{t A}\\mathbf{x},Q_{t})$ , the Radon-Nikodym density of d\u03c0T and $\\frac{d\\mu_{T}}{d\\mu_{\\mathrm{prior}}}$ has explicit form since $\\mu_{\\mathrm{prior}}$ and $\\mathcal{N}(0,Q_{\\infty})$ are equivalent. Thus, using the chain rule, the terminal cost for any $\\mathbf{x}\\in{\\mathcal{H}}$ can be computed as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{x})=-\\mathcal{U}(\\mathbf{x})-\\log\\frac{d\\mu_{\\mathrm{prior}}}{d\\mathcal{N}(0,Q_{\\infty})}(\\mathbf{x})+\\log\\frac{d\\mu_{T}}{d\\mathcal{N}(0,Q_{\\infty})}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With $\\mathcal{L}_{\\mathrm{Bayes}}(\\theta)$ in (24), the infinite-dimensional bayesian learning algorithm is summarized in $\\mathrm{Alg~}2$ ", "page_idx": 6}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Most diffusion models operate within the framework of time-reversal [1], where the generation process is learned from its corresponding time-reversed SDEs [64]. In contrast, diffusion models based on conditioned SDEs, such as diffusion bridges, built upon the theory of Doob\u2019s $h$ -transform, offer a conceptually simpler approach as they solely rely on a forward process. [50] proposes generative models with this concept, showing that the mixture of forward diffusion bridge processes effectively transports between couplings of two distributions. [76] introduces a family of first hitting diffusion models that generate data with a forward diffusion process at a random first hitting time based on Doob\u2019s $h$ -transform. Combining time-reversal with the $h$ -transform, [46] proposes a diffusion bridge process on constrained domains. Moreover, [51, 61] presented that the Schro\u00a8dinger bridge problem can be solved by an iterative algorithm, which is improved by [18] to enhance efficiency. Furthermore, [44] generalizes the Schro\u00a8dinger bridge matching algorithm by introducing an approximation scheme with a non-trivial running cost. Compared to prior works, which primarily focus on finite-dimensional spaces, our work extends the formulation of Doob\u2019s $h$ -transform into Hilbert space, enabling the development of various sampling algorithms in function spaces. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section details the experimental setup and the application of the proposed Diffusion Bridges in Function Spaces (DBFS) for generating functional data. We interpret the data from a functional perspective, known as field representation [75, 79], where data are seen as a finite collection of function evaluations $\\{\\dot{\\mathbf{Y}}[\\mathbf{p}_{i}],\\mathbf{p}_{i}\\}_{i}^{N}$ . Here, a function $\\mathbf{Y}$ maps points $\\mathbf{p}_{i}$ from a coordinate space $\\mathcal{X}$ to a signal space $\\boldsymbol{\\wp}$ , i.e., $\\mathbf{Y}:\\mathcal{X}\\rightarrow\\mathcal{Y}$ . Additional experimental details are provided in Appendix A.8. ", "page_idx": 7}, {"type": "text", "text": "5.1 Bridge Matching ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we present empirical results for the infinite-dimensional BM algorithm discussed in Sec 3.1, applied to 1D and 2D data. For 1D data, we consider $\\mathcal{X}=\\mathbb{R}$ and $\\mathcal{V}=\\mathbb{R}$ . For 2D data, we assume $\\Dot{\\mathcal{X}}=\\mathbb{R}^{2}$ and $\\ y=\\mathbb R$ for probability density or grayscale images, and $\\mathcal{V}=\\mathbb{R}^{3}$ for RGB images. ", "page_idx": 7}, {"type": "text", "text": "Bridging Field. We begin by validating our bridge matching Algorithm in $\\mathrm{Alg~l~}$ on bridging probability density function within $\\mathcal{H}$ . Specifically, we set $\\pi_{0}~:=~\\delta_{p_{0}}$ with a ring-shaped density function $p_{0}$ and $\\pi_{T}:=\\delta_{p_{T}}$ characterized by a Gaussian mixture density function $p_{T}$ . The functions map each grid points $\\mathbf{p}_{i}$ to the probability in $\\mathcal{V}=\\mathbb{R}$ . Therefore, both density functions can be represented as their field representations $\\{p_{0}[\\mathbf{p}_{i}],\\mathbf{p}_{i}\\}_{i}^{N},\\{p_{T}[\\mathbf{p}_{i}],\\mathbf{p}_{i}\\}_{i}^{N}$ , respectively. Figure 1 illustrates the progressive propagation of the target optimal bridge process $\\mathbb{P}^{\\star}$ from $p_{0}$ to $p_{T}$ . Despite the $\\alpha^{\\star}$ is trained on ", "page_idx": 7}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/67176678a5885a2bf58a48b5b1bc7fe75eefc6c4cc5348a64adfe90245c4b510.jpg", "img_caption": ["Figure 1: (Top) Diffusion Bridge $\\mathbb{P}^{\\star}$ evaluated on $32^{2}$ . (Bottom) Learned process $\\mathbb{P}^{\\alpha^{\\star}}$ evaluated on $256^{2}$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "the functions generated from $\\mathbb{P}^{\\star}$ which are evaluated on a coarse grid $\\{{\\bf p}_{i}\\}_{i}^{32^{2}}$ , $\\mathbb{P}^{\\alpha^{\\star}}$ is capable of producing accurate functional evaluations on a finer grid {pi}i2562. This resolution-invariance property indicates that our method is adept at learning continuous functional representations, rather than merely memorizing the discrete evaluations. ", "page_idx": 7}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/8cfbe7620c39291449381c51e475583a61152eb6fc289e150c3557c658e454e0.jpg", "img_caption": ["Figure 2: Results on 1D function generation. (Left) Real data and (Right) generated samples from our model. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "1D function generation. We conducted an experiment on a 1D function generation task, comparing our baseline methods [22, 52] on three datasets: Quadratic, Melbourne, and Gridwatch, following the setup from [52]. For gen", "page_idx": 7}, {"type": "table", "img_path": "WyQW4G57Zd/tmp/2ad75d6ba47da25d315b19e4876763ed502f4719eaa5763984cc3e7a213d97bf.jpg", "table_caption": ["Table 1: A Power $(\\%)$ of a kernel two-sample test. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "erative modeling, we set the initial distribution $\\pi_{0}$ as ${\\mathcal{N}}(0,Q)$ with RBF kernel for the covariance operator $Q$ and the terminal distribution as data distribution $\\pi_{T}$ , respectively. We employing the bridge matching algorithm in $\\mathrm{Alg~}1$ . For quantitative evaluation, we used the power of a kernel two-sample hypothesis test to distinguish between generated and ground-truth samples. Table 1 shows that our method performs comparably to baseline infinite-dimensional methods. Additionaly, The generated samples compared to the ground-truth for each dataset are provided in Figure 2. ", "page_idx": 7}, {"type": "text", "text": "Unpaired Image Transfer. We compare our proposed model with a finite (fixed)-dimensional baseline through an experiment on unpaired image transfer between the MNIST and EMNIST datasets at $32^{\\bar{2}}$ resolution, as well as wild and cat images from the AFHQ dataset [14], downsampled to $64^{2}$ resolution (AFHQ-64). Specifically, we evaluate the performance of [51, 61] and our DBFS model. For a fair comparison, we follow the iterative training scheme of [51] based on the public repository4, where two forward and backward control networks are trained alternately. For quantitative evaluation, we estimate the FID score between the generated samples and real datasets. We set $\\sigma=1$ for both [51] and our method, while FID scores for [61] are taken from [18]. Table 2 shows that our method performs comparably to the finite- Table 2: Test FID on undimensional method. Additionally, we provide generated samples at vari- paired image transfer task. ous unseen resolutions in Figure 3 to demonstrate the resolution-invariant (A) EMNIST $\\rightarrow$ MNIST, property of our infinite-dimensional models. We note that our method may (B) AFHQ-64 Wild $\\to\\mathrm{Cat}$ . ", "page_idx": 7}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/ef760bbad0125f1a52cf4b1f8dc06b112c1c1a53aa6a95e6db9e1662bb56f068.jpg", "img_caption": ["Figure 3: Results on Unpaired image transfer task. (Up) EMNIST $\\rightarrow$ MNIST (Down) AFHQ-64 Wild $\\rightarrow$ Cat. (Left) Real data and (Right) generated samples from our model. For generation at unseen resolutions, the images within the red and blue boxed initial conditions were upsampled (using bi-linear transformation) from the observed resolution $(32^{2})$ for EMNIST and $(64^{2})$ for AFHQ-64 Wild, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "have slightly lower FID scores compared to finite-dimensional baselines, which may align with the observation in [79] that resolution-agnostic methods tend to have lower FID scores compared to resolution-specific ones. This could be because resolution-specific methods can incorporate domain-specific design features in their score networks. Samples generated from the reverse direction can be found in Figure A.2. ", "page_idx": 8}, {"type": "table", "img_path": "WyQW4G57Zd/tmp/b9ca3234000f89600d66f69882d5d4b34c81a8cf230b906716842c7aeeb41d76.jpg", "table_caption": [], "table_footnote": ["\u2020 result from [18]. "], "page_idx": 8}, {"type": "text", "text": "5.2 Bayesian Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We validate our Bayesian learning algorithm for modeling functional data. Specifically, we will consider the temporal data as a function. We denote $\\mathbf{Y}[\\mathbf{O}]=\\{\\mathbf{Y}[\\mathbf{p}_{i}]\\}_{i=1}^{|\\mathbf{O}|}$ as a collection of a function evaluation on a set of 1-dimensional observation grid ${\\bf O}=\\{{\\bf p}_{i}\\}_{i}^{|0|}$ where $0\\le{\\bf p}_{0}<\\cdot\\cdot\\cdot<{\\bf p}_{|{\\bf O}|}\\le I$ We assume that each observed time series approximates a corresponding underlying continuous function X : R \u2192Rd as the number of observations increases i.e., {Y[pi]}|iO=|1\u2192 \u221e\u2248X. For given observations $\\mathbf Y[\\mathbf O]$ , our goal is to infer the posterior distribution on some set of unobserved grid ${\\bf T}=[0,I]-{\\bf O}$ i.e., $\\mathbb{P}(\\bar{\\mathbf{Y}}[\\mathbf{T}]|\\mathbf{Y}[\\mathbf{O}])$ and therefore modeling distribution over $\\mathbf{X}$ on $[0,I]$ . Please refer to Section A.8.2 for further details. ", "page_idx": 8}, {"type": "text", "text": "Functional Regression To verify the effectiveness of the proposed DBFS in generating functions in the 1D domain, we conducted regression experiments using synthetic data generated from the Gaussian Process (GP) by following the experimental settings in [38]. Figure 4 shows the sampled trajectories of a controlled dynamics $\\mathbf{X}_{t}^{\\alpha}$ for $t\\in[0,\\frac{T}{2},T]$ trained on data generated from GP with RBF covariance kernel. The stochastic process begins from the deterministic function $\\mathbf{X}_{0}^{\\alpha}=\\mathbf{x}_{0}$ at $t=0$ and propagates towards the conditional posterior distribution ${\\bf X}_{T}^{\\alpha}\\sim\\mathbb{P}({\\bf Y}[{\\bf T}]|{\\bf Y}[{\\bf O}])$ at $t=T$ ", "page_idx": 8}, {"type": "table", "img_path": "WyQW4G57Zd/tmp/924e61c18c7f28113c5adfcd1a467f664bc208127719a14e6b612c72ee30d1f3.jpg", "table_caption": ["Table 4: Regression results. \u201ccontext\u201d and \u201ctarget\u201d refer to the log-likelhoods at $\\mathbf{O}$ and $\\mathbf{T}$ , respectively. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/7d0850e1f24f2c665f838e79010cd59772ea9be73085b6e422be6eea0cb73022.jpg", "img_caption": ["Figure 4: Sampled functions from a learned stochastic process $\\mathbf{X}_{t}^{\\alpha}$ evaluated on $[0,I]$ for $\\textstyle t\\in[0,{\\frac{T}{2}},T]$ . The grey line represents the mean function $\\mathbb{E}[\\mathbf{X}_{t}^{\\alpha}]$ and the blue-shaded region represents the confidence interval. (Left) GP with RBF kernel. (Right) Physionet. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Imputation. We evaluate our method against recent diffusion-based imputation method where the goal is to infer the conditional distribution $\\bar{p}(\\mathbf Y[\\mathbf T]|\\mathbf Y[\\mathbf O])$ of unobserved grid $\\mathbf{Y}[\\mathbf{T}]$ give observations $\\mathbf Y[\\mathbf O]$ . CSDI [68] utilizes DDPM [34] to learn the reverse process by treating the temporal data $\\mathbf Y[\\mathbf O]$ as a $\\mathbb{R}^{|\\mathbf{O}|\\times d}$ dimensional feature. Extending this, DSDP-GP [6] ", "page_idx": 9}, {"type": "table", "img_path": "WyQW4G57Zd/tmp/5980d47bea8fc1a326be4111910b0f5ac63dd958371326442b171f126d2b28af.jpg", "table_caption": ["Table 3: Test imputation RMSE on Physionet. "], "table_footnote": ["\u2020 results from [6]. "], "page_idx": 9}, {"type": "text", "text": "enhances CSDI by incorporating noise derived from a stochastic process, instead of simple Gaussian noise. We maintained the same training setup as these models, including random seeds and the model architecture for control $\\alpha^{\\theta}$ in (2). Consistent with their methodology, we employed the Physionet dataset [30], which comprises medical time-series data collected on an hourly rate. Since the dataset inherently contains missing values, we selected certain degrees of observed values to create an imputation test set for evaluation. We then reported the results on this test set, varying the degrees of missingness. Table 3 shows that we outperform the previous methods even though it solely relies on forward propagation of controlled SDEs in (9) without denoising procedure. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we shed light on the application of the infinite-dimensional Doob\u2019s $h$ -transform, exploiting SOC theory in infinite-dimensional spaces. By developing an explicit Radon-Nikodym density, we address the challenge posed by the absence of an equivalent to the Lebesgue measure. With specified cost functions for control objectives, it enables us to extend previous algorithm based on the finite-dimensional Doob\u2019s $h$ -transform into infinite-dimensional function spaces, such as resolution-free unpaired image transfer and functional Bayesian posterior sampling. ", "page_idx": 9}, {"type": "text", "text": "Compared to the recent infinite-dimensional score-based diffusion model [42], our work restricts the coefficients for the stochastic dynamics to be time-independent. This limitation prevents us from defining a noise schedule for the diffusion model [78], which may hinder performance improvements. Additionally, in Bayesian learning, computing the gradient of the proposed training loss function (24) can be computationally demanding. Thus, developing a more scalable algorithm would be an interesting direction for future work. Furthermore, as our model can be applied to any functional domain, we have limited our experiments to regular 1D and 2D domains, leaving the extension to more general domains for future work. ", "page_idx": 9}, {"type": "text", "text": "Broader Societal Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Similar to other works in the literature, our proposed method holds the potential for both beneficial outcomes, such as automated data synthesis, and adverse implications, such as the deep fakes, depending on how it is used. We adhere to ethical standards for using our model in generative AI. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partly supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST), No.2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics, No. 2022-0-00612, Geometric and Physical Commonsense Reasoning based Behavior Intelligence for Embodied AI) and the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (NRF2021M3E5D9025030, NRF-2022R1A5A708390812, RS-2024-00410082). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[2] Elizabeth Louise Baker, Gefan Yang, Michael L Severinsen, Christy Anna Hipsley, and Stefan Sommer. Conditioning non-linear and infinite-dimensional diffusion processes. arXiv preprint arXiv:2402.01434, 2024.   \n[3] Lorenzo Baldassari, Ali Siahkoohi, Josselin Garnier, Knut Solna, and Maarten V de Hoop. Conditional score-based diffusion models for bayesian inference in infinite dimensions. Advances in Neural Information Processing Systems, 36, 2024. [4] Y.I. Belopolskaya and Y.L. Dalecky. Stochastic Equations and Differential Geometry. Mathematics and its Applications. Springer Netherlands, 2012.   \n[5] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusionbased generative modeling. Transactions on Machine Learning Research, 2024.   \n[6] Marin Bilo\u02c7s, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan G\u00a8unnemann. Modeling temporal data as continuous functions with stochastic process diffusion. In International Conference on Machine Learning, pages 2452\u20132470. PMLR, 2023.   \n[7] Vladimir Bogachev, Giuseppe Da Prato, and Michael R\u00a8ockner. Uniqueness for solutions of fokker\u2013planck equations on infinite dimensional spaces. Communications in Partial Differential Equations, 36(6):925\u2013939, 2011.   \n[8] R. Carmona and F. Delarue. Probabilistic Theory of Mean Field Games with Applications I: Mean Field FBSDEs, Control, and Games. Probability Theory and Stochastic Modelling. Springer International Publishing, 2018.   \n[9] Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and Guillaume Carlier. Deep relaxation: partial differential equations for optimizing deep neural networks. Research in the Mathematical Sciences, 5:1\u201330, 2018.   \n[10] Tianrong Chen, Jiatao Gu, Laurent Dinh, Evangelos Theodorou, Joshua M. Susskind, and Shuangfei Zhai. Generative modeling with phase stochastic bridge. In The Twelfth International Conference on Learning Representations, 2024.   \n[11] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schro\u00a8dinger bridge using forward-backward SDEs theory. In International Conference on Learning Representations, 2022.   \n[12] Yongxin Chen, Tryphon T. Georgiou, and Michele Pavon. Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schro\u00a8dinger bridge. SIAM Review, 63(2):249\u2013313, 2021.   \n[13] Raphae\u00a8l Chetrite and Hugo Touchette. Nonequilibrium markov processes conditioned on large deviations. In Annales Henri Poincar\u00b4e, volume 16, pages 2005\u20132057. Springer, 2015.   \n[14] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[15] G. Da Prato and J. Zabczyk. Differentiability of the feynman-kac semigroup and a control application. Atti della Accademia Nazionale dei Lincei. Classe di Scienze Fisiche, Matematiche e Naturali. Rendiconti Lincei. Matematica e Applicazioni, 8(3):183\u2013188, 10 1997.   \n[16] G. Da Prato and J. Zabczyk. Second Order Partial Differential Equations in Hilbert Spaces. London Mathematical Society Lecture Note Series. Cambridge University Press, 2002.   \n[17] G. Da Prato and J. Zabczyk. Stochastic Equations in Infinite Dimensions. Encyclopedia of Mathematics and its Applications. Cambridge University Press, 2014.   \n[18] Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, and Arnaud Doucet. Schr\\\u201d odinger bridge flow for unpaired data translation. arXiv preprint arXiv:2409.09347, 2024.   \n[19] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schro\u00a8dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[20] Carles Domingo-Enrich, Jiequn Han, Brandon Amos, Joan Bruna, and Ricky TQ Chen. Stochastic optimal control matching. arXiv preprint arXiv:2312.02027, 2023.   \n[21] Emilien Dupont, Adam Goli\u00b4nski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. Coin: Compression with implicit neural representations. arXiv preprint arXiv:2103.03123, 2021.   \n[22] Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes. In International Conference on Machine Learning, pages 8990\u20139012. PMLR, 2023.   \n[23] Giorgio Fabbri, Fausto Gozzi, and Andrzej Swiech. Stochastic optimal control in infinite dimension. Probability and Stochastic Modelling. Springer, 2017.   \n[24] Wendell H Fleming and Halil Mete Soner. Controlled Markov processes and viscosity solutions, volume 25. Springer Science & Business Media, 2006.   \n[25] Giulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi. Continuous-time functional diffusion processes. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Marco Fuhrman. A class of stochastic optimal control problems in hilbert spaces: Bsdes and optimal control laws, state constraints, conditioned processes. Stochastic processes and their applications, 108(2):263\u2013298, 2003.   \n[27] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional neural processes. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1704\u20131713. PMLR, 10\u201315 Jul 2018.   \n[28] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye Teh. Neural processes, 2018.   \n[29] L. Gawarecki and V. Mandrekar. Stochastic Differential Equations in Infinite Dimensions: with Applications to Stochastic Partial Differential Equations. Probability and Its Applications. Springer Berlin Heidelberg, 2010.   \n[30] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation, 101(23):e215\u2013e220, 2000 (June 13). Circulation Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi: 10.1161/01.CIR.101.23.e215.   \n[31] Ben Goldys and Bohdan Maslowski. The ornstein\u2013uhlenbeck bridge and applications to markov semigroups. Stochastic processes and their applications, 118(10):1738\u20131767, 2008.   \n[32] Paul Hagemann, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, and Nicole Tianjiao Yang. Multilevel diffusion: Infinite dimensional score-based diffusion models for image generation. arXiv preprint arXiv:2303.04772, 2023.   \n[33] Carsten Hartmann, Omar Kebiri, Lara Neureither, and Lorenz Richter. Variational approach to rare event simulation using least-squares regression. Chaos: An Interdisciplinary Journal of Nonlinear Science, 29(6), 2019.   \n[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[35] Lars Holdijk, Yuanqi Du, Priyank Jaini, Ferry Hooft, Bernd Ensing, and Max Welling. Path integral stochastic optimal control for sampling transition paths. In ICML 2022 2nd AI for Science Workshop, 2022.   \n[36] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A general architecture for structured inputs & outputs. In International Conference on Learning Representations, 2022.   \n[37] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):1\u201397, 2023.   \n[38] Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Bootstrapping neural processes. Advances in neural information processing systems, 33:6606\u2013 6615, 2020.   \n[39] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations\u2019 operator learning. Transactions on Machine Learning Research, 2023.   \n[40] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021.   \n[41] Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaif,i Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion models in function space. arXiv preprint arXiv:2302.07400, 2023.   \n[42] Sungbin Lim, Eunbi Yoon, Taehyun Byun, Taewon Kang, Seungwoo Kim, Kyungjae Lee, and Sungjoon Choi. Score-based generative modeling through stochastic evolution equations in hilbert spaces. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[43] Guan-Horng Liu, Tianrong Chen, Oswin So, and Evangelos Theodorou. Deep generalized schr\u00a8odinger bridge. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[44] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou, and Ricky T. Q. Chen. Generalized schr\u00a8odinger bridge matching. In The Twelfth International Conference on Learning Representations, 2024.   \n[45] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. $\\mathrm{I^{2}s b}$ : Image-to-image schr\u00a8odinger bridge. arXiv preprint arXiv:2302.05872, 2023.   \n[46] Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In The Eleventh International Conference on Learning Representations, 2023.   \n[47] S.K. Mitter. Filtering and stochastic control: a historical perspective. IEEE Control Systems Magazine, 16(3):67\u201376, 1996.   \n[48] Nikolas Nu\u00a8sken and Lorenz Richter. Solving high-dimensional hamilton\u2013jacobi\u2013bellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space. Partial differential equations and applications, 2:1\u201348, 2021.   \n[49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[50] Stefano Peluchetti. Non-denoising forward-time diffusions, 2022.   \n[51] Stefano Peluchetti. Diffusion bridge mixture transports, schr\u00a8odinger bridge problems and generative modeling. Journal of Machine Learning Research, 24(374):1\u201351, 2023.   \n[52] Angus Phillips, Thomas Seror, Michael Hutchinson, Valentin De Bortoli, Arnaud Doucet, and Emile Mathieu. Spectral diffusion processes. arXiv preprint arXiv:2209.14125, 2022.   \n[53] Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, and Sven Wang. Infinite-dimensional diffusion models for function spaces. arXiv e-prints, pages arXiv\u20132302, 2023.   \n[54] Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. Applied Mathematics and Optimization, 23:313\u2013329, 1991.   \n[55] Sebastian Reich. Data assimilation: the schro\u00a8dinger perspective. Acta Numerica, 28:635\u2013711, 2019.   \n[56] Lorenz Richter. Solving high-dimensional PDEs, approximation of path space measures and importance sampling of diffusions. PhD thesis, BTU Cottbus-Senftenberg, 2021.   \n[57] Lorenz Richter and Julius Berner. Improved sampling via learned diffusions. In The Twelfth International Conference on Learning Representations, 2024.   \n[58] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In The Eleventh International Conference on Learning Representations, 2023.   \n[59] L Chris G Rogers and David Williams. Diffusions, Markov processes and martingales: Volume 2, It\u02c6o calculus, volume 2. Cambridge university press, 2000.   \n[60] S. S\u00a8arkk\u00a8a and A. Solin. Applied Stochastic Differential Equations. Institute of Mathematical Statistics Textbooks. Cambridge University Press, 2019.   \n[61] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schro\u00a8dinger bridge matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[62] Isabel Sim\u02dcao. Regular transition densities for infinite dimensional diffusions. Stochastic Analysis and Applications, 11(3):309\u2013336, 1993.   \n[63] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:7462\u20137473, 2020.   \n[64] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[65] Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS. In International Conference on Learning Representations, 2019.   \n[66] Taiji Suzuki. Generalization bound of globally optimal non-convex neural network training: Transportation map estimation by infinite dimensional langevin dynamics. Advances in Neural Information Processing Systems, 33:19224\u201319237, 2020.   \n[67] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren $\\mathrm{Ng}$ . Fourier features let networks learn high frequency functions in low dimensional domains. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7537\u20137547. Curran Associates, Inc., 2020.   \n[68] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. In Advances in Neural Information Processing Systems, 2021.   \n[69] Evangelos A. Theodorou, George I. Boutselis, and Kaivalya Bakshi. Linearly solvable stochastic optimal control for infinite-dimensional systems. In 2018 IEEE Conference on Decision and Control (CDC), pages 4110\u20134116, 2018.   \n[70] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. Expert Certification.   \n[71] Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In COLT, 2019.   \n[72] Ramon Van Handel. Stochastic calculus, filtering, and stochastic control. Course notes., URL http://www. princeton. edu/rvan/acm217/ACM217. pdf, 14, 2007.   \n[73] Francisco Vargas, Andrius Ovsianas, David Fernandes, Mark Girolami, Neil D Lawrence, and Nikolas N\u00a8usken. Bayesian learning via neural schr\u00a8odinger\u2013f\u00a8ollmer flows. Statistics and Computing, 33(1):3, 2023.   \n[74] Ziyu Wang, Tongzheng Ren, Jun Zhu, and Bo Zhang. Function space particle optimization for Bayesian neural networks. In International Conference on Learning Representations, 2019.   \n[75] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. Computer Graphics Forum, 41(2):641\u2013676, 2022.   \n[76] Mao Ye, Lemeng Wu, and qiang liu. First hitting diffusion models for generating manifold, graph and categorical data. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[77] Qinsheng Zhang and Yongxin Chen. Path integral sampler: A stochastic control approach for sampling. In International Conference on Learning Representations, 2022.   \n[78] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. In The Twelfth International Conference on Learning Representations, 2024.   \n[79] Peiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M. Susskind, and Miguel A\u00b4ngel Bautista. Diffusion probabilistic fields. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Verification Theorem and Markov control ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the derivation, we will use the following assumption ", "page_idx": 15}, {"type": "text", "text": "Assumption A.1. The function $\\mathcal{V}:[0,T]\\,\\times\\,\\mathcal{H}\\,\\rightarrow\\,\\mathbb{R}$ and its derivatives $D_{\\mathbf{x}}\\nu,D_{\\mathbf{xx}}\\nu,\\partial_{t}\\nu$ are uniformly continuous on bounded subsets of $[0,T]\\times\\mathcal{H}$ and $(0,T)\\times\\mathcal{H}$ , respectively. Moreover, for all $(t,\\mathbf{x})\\in(0,T)\\times\\mathcal{H},$ , there exists $C_{1},C_{2}>0$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{V}(t,\\mathbf{x})|+|D_{\\mathbf{x}}\\mathcal{V}(t,\\mathbf{x})|+|\\partial_{t}\\mathcal{V}(t,\\mathbf{x})|+\\|D_{\\mathbf{xx}}\\mathcal{V}(t,\\mathbf{x})\\|+|A^{\\star}D_{\\mathbf{x}}\\mathcal{V}(t,\\mathbf{x})|\\leq C_{1}(1+|\\mathbf{x}|)^{C_{2}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{A}^{\\star}$ is adjoint operator of $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 15}, {"type": "text", "text": "The HJB equation (4) can be derived by the following theorem. ", "page_idx": 15}, {"type": "text", "text": "Theorem A.2. Let assumptions A.1 hold and let the function $\\nu$ with $\\mathcal{V}(T,\\mathbf{x})=G(\\mathbf{x})$ satisfying the dynamic programming principle for every $0<t<t^{\\prime}<T$ , $x\\in\\mathcal{H}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{V}(t,\\mathbf{x})=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\int_{t}^{t^{\\prime}}\\left[l(s,\\mathbf{X}_{t}^{\\alpha})+\\psi(\\alpha_{s})\\right]d s+\\mathcal{V}(t^{\\prime},\\mathbf{X}_{t^{\\prime}})|\\mathbf{X}_{t}^{\\alpha}=\\mathbf{x}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then $\\mathcal{V}$ is a solution of the following equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}\\nu+\\mathcal{L}\\nu+\\operatorname*{inf}_{\\alpha\\in\\mathcal{U}}\\left[\\left\\langle\\sigma Q_{\\mathbf{x}}^{1/2}\\mathcal{V},\\alpha\\right\\rangle+\\frac{1}{2}\\left\\lVert\\alpha\\right\\rVert^{2}\\right]=0,\\quad\\mathcal{V}(T,\\mathbf{x})=G(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. The proof can be found in [23, Theorem 2.34]. ", "page_idx": 15}, {"type": "text", "text": "A.1.1 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. To begin the proof, we can formally compute the minimum of $F(D_{\\mathbf{x}}\\mathcal{V})$ . Since [15] ", "page_idx": 15}, {"type": "equation", "text": "$$\nF(\\mathbf{x})=\\operatorname*{inf}_{\\alpha\\in\\mathcal{U}}\\left[\\left\\langle\\mathbf{x},\\alpha\\right\\rangle+\\frac{1}{2}\\left\\|\\alpha\\right\\|^{2}\\right]=-\\frac{1}{2}\\left\\|\\mathbf{x}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $F(\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V})$ takes the infimum at $\\alpha^{*}\\;=\\;-\\sigma Q^{1/2}D_{\\mathbf{x}}\\nu$ . Next, applying It\u02c6o\u2019s formula [23, Proposition 1.165] to $\\mathcal{V}$ and taking expectation on both sides, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{L}_{\\mathbb{P}\\alpha}^{t,\\mathbf{x}}\\left[\\mathcal{V}(T,\\mathbf{X}_{T}^{\\alpha})\\right]=\\mathcal{V}(t,\\mathbf{x})+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\left(\\partial_{t}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})+\\mathcal{L}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})+\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha}),\\alpha_{s}\\rangle\\right)d s\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we denote $\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\cdot\\right]=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\cdot|\\mathbf{X}_{t}=\\mathbf{x}\\right]$ By incorporating the fact that $\\nu$ satisfies the equation in (A.3), we can derive the following by adding $\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\frac{1}{2}\\left\\lVert\\alpha_{s}\\right\\rVert^{2}d s\\right]}\\end{array}$ to both terms. The LHS of the equation (A.5) becomes: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\underbrace{\\mathcal{V}(T,\\mathbf{X}_{T}^{\\alpha})}_{=G(\\mathbf{X}_{T}^{\\alpha})}\\right]+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\frac{1}{2}\\left\\lVert\\alpha_{s}\\right\\rVert^{2}d s\\right]=\\mathcal{I}(t,\\mathbf{x},\\alpha).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And for the RHS of the equation (A.5): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle^{\\gamma}(t,\\mathbf{x})+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\frac{1}{2}\\left\\|\\alpha_{s}\\right\\|^{2}d s\\right]+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\left(\\partial_{t}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})+\\mathcal{L}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})+\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})\\rangle,\\alpha_{s}\\right)\\right.}\\\\ &{\\displaystyle=\\mathcal{V}(t,\\mathbf{x})+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\left(\\partial_{t}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})+\\mathcal{L}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})+\\left[\\left\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha}),\\alpha_{s}\\right\\rangle+\\frac{1}{2}\\left\\|\\alpha_{s}\\right\\|^{2}\\right]\\right)d s\\right]}\\\\ &{\\displaystyle=\\mathcal{V}(t,\\mathbf{x})+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\left(\\left[\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha}),\\alpha_{s}\\right\\rangle+\\frac{1}{2}\\left\\|\\alpha_{s}\\right\\|^{2}\\right]-F(\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha}))\\right)d s\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last equation can be derived by adding and subtracting $\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}F(D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha})d s\\right]}\\end{array}$ and incorporating the fact that $\\mathcal{V}$ satisfies the equation in (A.3) again. Hence, we get the following equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{I}(t,\\mathbf{x},\\alpha)=\\mathcal{V}(t,\\mathbf{x})+\\mathbb{E}_{\\mathbb{P}^{\\alpha}}^{t,\\mathbf{x}}\\left[\\int_{t}^{T}\\left(\\left[\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\rangle\\gamma(s,\\mathbf{X}_{s}^{\\alpha}),\\alpha_{s}\\rangle+\\frac{1}{2}\\left\\lVert\\alpha_{s}\\right\\rVert^{2}\\right]-F(\\sigma Q^{1/2}D_{\\mathbf{x}}\\rangle\\gamma(s,\\mathbf{X}_{s}^{\\alpha}))\\right)\\right]\\,d s,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since, by definition ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha}),\\alpha_{s}\\rangle+\\frac{1}{2}\\left\\lVert\\alpha_{s}\\right\\rVert^{2}\\right]-F(D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{\\alpha}))\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by taking the infimum over $\\alpha\\in\\mathcal{U}$ in the RHS of (A.8), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{I}(t,\\mathbf{x},\\alpha)\\geq\\mathcal{V}(t,\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, since we already verified that the $F(\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V})$ has infimum at $\\alpha^{*}=-\\sigma Q^{1/2}D_{\\mathbf{x}}\\nu$ . Therefore, by choosing $u=\\alpha^{*}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\langle\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{u}),u_{s}\\rangle+\\frac{1}{2}\\left\\Vert u_{s}\\right\\Vert^{2}\\right]-F(\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}(s,\\mathbf{X}_{s}^{u}))=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{J}(t,\\mathbf{x},u)=\\mathcal{V}(t,\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, together with (A.8), this implies that $(\\alpha^{*},\\mathbf{X}^{\\alpha^{*}})$ is optimal at $(t,\\mathbf{x})\\in\\left[0,T\\right]\\times\\mathcal{H}$ This concludes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.1.2 Markov Control Formulation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Now, we introduce the following corollary that states the Markov control formulation. ", "page_idx": 16}, {"type": "text", "text": "Corollary A.3 (Markov Control [23]). Let us consider the measurable function $\\phi_{t}:(t,T)\\times\\mathcal{H}\\to\\mathcal{U}$ which admit a mild solution $\\mathbf{X}^{\\phi_{t}}$ of the following closed-loop equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{s}^{\\phi_{t}}=\\left[A\\mathbf{X}_{s}^{\\phi_{s}}+\\sigma Q^{1/2}\\phi_{s}(s,\\mathbf{X}_{s}^{\\phi_{s}})\\right]d s+d\\mathbf{W}_{t}^{Q},\\ \\mathbf{X}_{t}=\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then the pair $\\left(\\alpha^{\\phi_{t}},\\mathbf{X}^{\\phi_{t}}\\right)$ , where the control $\\alpha^{\\phi_{t}}$ is defined by the Markov feedback law $\\alpha_{s}^{\\phi_{t}}\\,=$ $\\phi\\big(s,\\mathbf{X}_{s}^{\\phi_{t}}\\big)$ is admissible and it is optimal at $(t,\\bf{x})$ for all $s\\in[t,T]$ . ", "page_idx": 16}, {"type": "text", "text": "Therefore, in the context of the initial value problem, such as in our case, we consider the form of the Markov control $\\alpha_{s}:=\\alpha_{s}^{\\phi_{0}}=\\phi(s,\\mathbf{X}_{s}^{\\phi_{0}})$ for $s\\in[0,T]$ . The proof and details can be found in [23, Chap 2.5.1]. ", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Let us consider the function $\\mathcal{V}(t,v)=-\\log h(t,v)$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{t}h=-h\\partial_{t}\\mathcal{V},\\quad D h=-h D_{\\mathbf{x}}\\mathcal{V},\\quad D^{2}h=h D_{\\mathbf{x}}\\mathcal{V}\\otimes D_{\\mathbf{x}}\\mathcal{V}-h D_{\\mathbf{xx}}\\mathcal{V},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that $h$ satisfy the KBE in equation (7): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{t}h+\\mathscr{L}h=0,\\quad h_{T}=\\tilde{G}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $h=e^{-\\nu}$ , hence $\\partial_{t}h=-\\mathscr{L}h=\\partial_{t}e^{-\\mathscr{V}}=-\\partial_{t}\\mathscr{V}h$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{O_{t}\\nu n=\\mathcal{L}n}}\\\\ &{=\\langle D_{\\mathbf{x}}h,\\mathcal{A}\\mathbf{X}_{t}\\rangle+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}D_{\\mathbf{x}\\mathbf{x}}h Q\\right]}\\\\ &{=-\\langle h D_{\\mathbf{x}}\\mathcal{V},\\mathcal{A}\\mathbf{X}_{t}\\rangle+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}\\left(h D_{\\mathbf{x}}\\mathcal{V}\\otimes D_{\\mathbf{x}}\\mathcal{V}-h D_{\\mathbf{x}\\mathbf{x}}\\mathcal{V}\\right)Q\\right]}\\\\ &{=-\\langle h D_{\\mathbf{x}}\\mathcal{V},\\mathcal{A}\\mathbf{X}_{t}\\rangle+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}\\left(h D_{\\mathbf{x}}\\mathcal{V}\\otimes D_{\\mathbf{x}}\\mathcal{V}\\right)Q\\right]-\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}h D_{\\mathbf{x}\\mathbf{x}}\\mathcal{V}Q\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can simplify the last equation as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{t}\\mathcal{V}=-\\langle D_{\\mathbf{x}}\\mathcal{V},A\\mathbf{X}_{t}\\rangle+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}\\left(D_{\\mathbf{x}}\\mathcal{V}\\otimes D_{\\mathbf{x}}\\mathcal{V}\\right)Q\\right]-\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}D_{\\mathbf{xx}}\\mathcal{V}Q\\right]}\\\\ {\\displaystyle\\qquad=-\\mathcal{L}\\mathcal{V}+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}\\left(D_{\\mathbf{x}}\\mathcal{V}\\otimes D_{\\mathbf{x}}\\mathcal{V}\\right)Q\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Following [69], the second term of RHS can be derived as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\mathbb{T}\\left[\\sigma^{2}\\left(D_{\\mathbf{x}}\\gamma\\otimes D_{\\mathbf{x}}\\gamma\\right)Q\\right]=\\frac{1}{2}\\underset{k\\in\\mathbb{N}}{\\sum_{i}}(\\sigma^{2}(D_{\\mathbf{x}}\\gamma\\otimes D_{\\mathbf{x}}\\gamma)Q\\phi^{(k)},\\phi^{(k)}),}\\\\ &{\\qquad=\\frac{1}{2}\\underset{k\\in\\mathbb{N}}{\\sum_{i}}\\sigma^{2}D_{\\mathbf{x}}\\gamma\\langle D_{\\mathbf{x}}\\gamma,Q\\phi^{(k)}\\rangle,\\phi^{(k)})}\\\\ &{\\qquad=\\frac{1}{2}\\underset{k\\in\\mathbb{N}}{\\sum_{i}}(\\sigma^{2}D_{\\mathbf{x}}\\gamma,Q\\phi^{(k)})\\langle D_{\\mathbf{x}}\\gamma,\\phi^{(k)}\\rangle}\\\\ &{\\qquad=\\frac{1}{2}\\underset{k\\in\\mathbb{N}}{\\sum_{i}}(\\sigma^{2}Q D_{\\mathbf{x}}\\gamma,\\phi^{(k)})\\langle D_{\\mathbf{x}}\\gamma,\\phi^{(k)}\\rangle}\\\\ &{\\qquad=\\frac{1}{2}(\\sigma^{2}D_{\\mathbf{x}}\\gamma,Q D_{\\mathbf{x}}\\gamma)}\\\\ &{\\qquad=\\frac{1}{2}\\left|\\sigma^{2}D_{\\mathbf{x}}\\gamma,Q D_{\\mathbf{x}}\\gamma\\right|_{M}^{2}}\\\\ &{\\qquad=\\frac{1}{2}\\left|\\sigma^{2}\\Gamma^{2}D_{\\mathbf{x}}\\gamma\\right|_{M}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, combining the above results, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\partial_{t}\\mathcal{V}+\\mathcal{L}\\mathcal{V}-\\frac{1}{2}\\left\\|\\sigma Q^{1/2}D_{\\mathbf{x}}\\mathcal{V}\\right\\|_{\\mathcal{H}}^{2},\\quad\\mathcal{V}_{T}=G.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since (A.28) coincides with (4) with $\\begin{array}{r}{\\psi(\\cdot):=\\frac{1}{2}\\left\\|\\cdot\\right\\|_{\\mathcal{H}}^{2}}\\end{array}$ , this concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Theorem 2.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. A proof of Theorem 2.3 is based on [16, Chap. 10.3]. Since $\\begin{array}{r}{Q_{\\infty}=-\\frac{1}{2}Q A^{-1}}\\end{array}$ is a trace class, we define a trace class operator $\\Theta_{t}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Theta_{t}=Q_{\\infty}^{1/2}(Q_{t}^{-1/2}e^{t.4})^{*}(Q_{\\infty}^{-1/2}Q_{t}^{1/2})^{*}(Q_{\\infty}^{1/2}(Q_{t}^{-1/2}e^{t.4})^{*}(Q_{\\infty}^{-1/2}Q_{t}^{1/2})^{*})^{*},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $t\\geq0$ . Since $Q_{t}=Q_{\\infty}-e^{t A}Q_{\\infty}e^{t A^{*}}$ , we can rewrite $Q_{t}$ in terms of $\\Theta_{t}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t}=Q_{\\infty}-e^{t A}Q_{\\infty}e^{t A^{*}}}\\\\ &{\\quad=Q_{\\infty}^{1/2}\\left[1-(Q_{\\infty}^{-1/2}e^{t A})Q_{\\infty}(Q_{\\infty}^{-1/2}e^{t A})^{*}\\right]Q_{\\infty}^{1/2}}\\\\ &{\\quad=Q_{\\infty}^{1/2}(1-\\Theta_{t})Q_{\\infty}^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we have $(1-\\Theta_{t})\\mathbf{x}=Q_{\\infty}^{-1/2}Q_{t}Q_{\\infty}^{-1/2}\\mathbf{x}$ for all $\\mathbf{x}\\in\\mathcal{H}_{0}$ . It implies that $\\langle(1-\\Theta_{t})\\mathbf{x},\\mathbf{x}\\rangle_{\\mathcal{H}_{0}}\\ge0$ , the non-negativity of $(1-\\Theta_{t})$ . Moreover it also implies that $(1-\\bar{\\Theta}_{t})^{-1}$ is invertible: ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1-\\Theta_{t})^{-1}=(Q_{t}^{-1/2}Q_{\\infty}^{1/2})^{*}Q_{t}^{-1/2}Q_{\\infty}^{1/2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, it yields the following formula [16, Proposition. 1.3.11] ", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{t}(0,\\mathbf{y})=d e t(1-\\Theta)^{-1/2}\\exp\\left[-\\frac{1}{2}\\langle\\Theta_{t}(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}\\mathbf{y},Q_{\\infty}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, for the general case, by using the chain rule, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{t}(\\mathbf{x},\\mathbf{y})=\\frac{d\\mathcal{N}_{e^{t}A}_{\\mathbf{x},Q_{t}}}{d\\mathcal{N}_{0,Q_{t}}}\\frac{d\\mathcal{N}_{0,Q_{t}}}{d\\mathcal{N}_{0,Q_{\\infty}}}(\\mathbf{y})=\\frac{d\\mathcal{N}_{e^{t}A}_{\\mathbf{x},Q_{t}}}{d\\mathcal{N}_{0,Q_{t}}}(\\mathbf{y})q_{t}(0,\\mathbf{y}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and utilizing Cameron-Martin theorem [16, Theorem. 1.3.6], we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\mathcal{N}_{e^{t},A_{\\mathbf{x},Q_{t}}}}{d\\mathcal{N}_{0,Q_{t}}}(\\mathbf{y})=\\exp\\left[\\langle Q_{t}^{-1/2}e^{t A_{\\mathbf{x},}}Q_{t}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}-\\frac{1}{2}\\left\\|Q_{t}^{-1/2}e^{t A_{\\mathbf{x}}}\\right\\|_{\\mathcal{H}}^{2}\\right]}\\\\ &{=\\exp\\left[\\langle Q_{t}^{-1/2}e^{t A_{\\mathbf{x},}}Q_{t}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}-\\frac{1}{2}\\langle Q_{t}^{-1/2}e^{t A_{\\mathbf{x},}}Q_{t}^{-1/2}e^{t A_{\\mathbf{x}}}\\rangle_{\\mathcal{H}}\\right]}\\\\ &{=\\exp\\left[\\langle Q_{\\infty}^{1/2}Q_{t}^{-1}e^{t A_{\\mathbf{x},}}Q_{\\infty}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}-\\frac{1}{2}\\langle Q_{\\infty}^{1/2}Q_{t}^{-1}e^{t A_{\\mathbf{x},}}Q_{\\infty}^{-1/2}e^{t A_{\\mathbf{x}}}\\rangle_{\\mathcal{H}}\\right]}\\\\ &{\\overset{(i)}{=}\\exp\\left[\\langle(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}e^{t A_{\\mathbf{x},}}Q_{\\infty}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}-\\frac{1}{2}\\langle(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}e^{t A_{\\mathbf{x}}},Q_{\\infty}^{-1/2}e^{t A_{\\mathbf{x}}}\\rangle_{\\mathcal{H}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(i)$ follows from (A.33), $(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}=(Q_{t}^{-1/2}Q_{\\infty}^{1/2})^{*}Q_{t}^{-1/2}=Q_{\\infty}^{1/2}Q_{t}^{-1}$ . Thus, by substituting (A.34) and (A.39) into (A.35), we obtain the following result: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}(\\mathbf{x},\\mathbf{y})=d e t(1-\\Theta)^{-1/2}\\exp\\bigg[-\\frac{1}{2}\\langle\\Theta_{t}(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}\\mathbf{y},Q_{\\infty}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}\\qquad\\qquad\\qquad(\\mathbf{A}.4\\mathrm{~})}\\\\ &{\\qquad\\qquad+\\langle(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}e^{t.A}\\mathbf{x},Q_{\\infty}^{-1/2}\\mathbf{y}\\rangle_{\\mathcal{H}}-\\frac{1}{2}\\langle(1-\\Theta_{t})^{-1}Q_{\\infty}^{-1/2}e^{t.A}\\mathbf{x},Q_{\\infty}^{-1/2}e^{t.A}\\mathbf{x}\\rangle_{\\mathcal{H}}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "A.4 Derivation of Example 2.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For a diffusion bridge process, let us define the $h$ function as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nh(t,T,\\mathbf{x}_{t},\\mathbf{x}_{T})=\\mathbb{E}_{\\mathbb{P}}\\left[\\tilde{G}(\\mathbf{X}_{T},\\mathbf{x}_{T})|\\mathbf{X}_{t}=\\mathbf{x}_{t}\\right]=\\int\\tilde{G}(\\mathbf{z},\\mathbf{x}_{T})d\\mathbf{\\hat{\\calN}}_{e^{(T-t){\\cal A}}\\mathbf{x}_{t},Q_{T-t}}(\\mathbf{z})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If we choose $\\tilde{G}(\\mathbf{x},\\mathbf{y})=\\mathbf{1}_{d\\mathbf{y}}(\\mathbf{x})$ . Then, for any $\\mathbf{y}\\in\\mathcal{H}$ and $t\\in[0,T]$ , Theorem 2.3 implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(0,t,{\\mathbf x}_{0},{\\mathbf y})=\\displaystyle\\int\\tilde{G}({\\mathbf z},{\\mathbf y})d\\ensuremath{{\\mathcal N}}_{e^{t,A}{\\mathbf x}_{0},Q_{t}}({\\mathbf z})}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\int\\tilde{G}({\\mathbf z},{\\mathbf y})\\frac{d\\ensuremath{{\\mathcal N}}_{e^{t,A}{\\mathbf x}_{0},Q_{t}}}{d\\ensuremath{{\\mathcal N}}_{0,Q_{\\infty}}}({\\mathbf z})d\\ensuremath{{\\mathcal N}}_{0,Q_{\\infty}}({\\mathbf z})}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\int\\tilde{G}({\\mathbf z},{\\mathbf y})q_{t}({\\mathbf x},{\\mathbf z})d\\ensuremath{{\\mathcal N}}_{0,Q_{\\infty}}({\\mathbf z})}\\\\ &{\\qquad\\qquad\\quad=q_{t}({\\mathbf x},{\\mathbf y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, with an eigen-system of $\\mathcal{H}$ , $\\{(\\lambda^{(k)},\\phi^{(k)})\\,\\in\\,\\mathbb{R}\\times\\mathcal{H}\\,:\\,k\\,\\in\\,\\mathbb{N}\\}$ , $q_{t}$ can be represented as [62]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t}(\\mathbf{x},\\mathbf{y})=\\prod_{k\\in\\mathbb{N}}q_{t}^{(k)}(\\mathbf{x}^{(k)},\\mathbf{y}^{(k)}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where for each coordinated $k$ , $q_{t}^{(k)}(\\mathbf{x}^{(k)},\\mathbf{y}^{(k)})$ has following representation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}^{(k)}(\\mathbf{x}^{(k)},\\mathbf{y}^{(k)})=\\bigg(\\displaystyle\\frac{\\lambda^{(k)}}{2a_{k}}(1-e^{-2a_{k}t})\\bigg)^{-1/2}\\exp\\left[-\\frac{(\\mathbf{y}^{(k)}-e^{-a_{k}t}\\mathbf{x}^{k}))^{2}}{2\\lambda^{(k)}(1-e^{-2a_{k}t})}+\\frac{(\\mathbf{y}^{(k)})^{2}}{2\\lambda^{(k)}}\\right]}\\\\ &{A\\phi^{(k)}=-a_{k}\\phi^{(k)},\\quad Q\\phi^{(k)}=\\lambda^{(k)}\\phi^{(k)},\\quad\\mathbf{x}^{(k)}=\\langle\\mathbf{x},\\phi^{(k)}\\rangle_{\\mathcal{H}},\\quad\\mathbf{y}_{k}=\\langle\\mathbf{y},\\phi^{(k)}\\rangle_{\\mathcal{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, since $D_{\\mathbf{x}}\\log h(t,T,\\mathbf{x},\\mathbf{x}_{T})=D_{\\mathbf{x}}\\log q_{T-t}(\\mathbf{x}_{t},\\mathbf{x}_{T})$ , by projecting $D_{\\bf x}\\log q_{T-t}({\\bf x}_{t},{\\bf x}_{T})$ to each coordinate $\\phi^{(k)}$ , we obtain the following results: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d\\mathbf{x}^{(k)}}\\log q_{T-t}^{(k)}(\\mathbf{x}_{t}^{(k)},\\mathbf{x}_{T}^{(k)})=\\frac{2a_{k}e^{-a_{k}(T-t)}}{\\lambda^{(k)}(1-e^{-2a_{k}(T-t)})}(\\mathbf{x}_{T}^{(k)}-e^{-a_{k}(T-t)}\\mathbf{x}^{(k)})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.5 Deriving Divergence Between Path Measures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we present an infinite-dimensional generalization of Girsanov\u2019s theorem [17, Theorem 10.14], which plays a crucial role in estimating the divergence between two path measures discussed in Sec 2.4. The theorem is formulated as follows: ", "page_idx": 19}, {"type": "text", "text": "Theorem A.4 (Girsanov\u2019s Theorem in $\\mathcal{H}$ ). Let $\\gamma$ be a $\\mathcal{H}_{0}$ -valued $\\mathcal{F}_{t}$ -predictable process such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\int_{0}^{T}\\|\\gamma_{s}\\|_{\\mathcal{H}_{0}}^{2}\\,d s<\\infty\\right)=1,\\quad\\mathbb{E}\\left[\\exp\\left(\\int_{0}^{t}\\langle\\gamma_{s},d\\mathbf{W}_{t}^{Q}\\rangle_{\\mathcal{H}_{0}}-\\frac{1}{2}\\int_{0}^{T}\\|\\gamma_{s}\\|_{\\mathcal{H}_{0}}^{2}\\,d t\\right)\\right]=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then the process $\\begin{array}{r}{\\tilde{\\mathbf{W}}_{t}^{Q}=\\mathbf{W}_{t}^{Q}-\\int_{0}^{T}\\gamma_{s}d s}\\end{array}$ is a $Q$ -Wiener process with respect to $\\{\\mathcal{F}_{t}\\}_{t\\ge0}$ on the probability space $(\\Omega,{\\mathcal{F}},\\mathbb{Q})$ where ", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\mathbb{Q}=\\exp\\left(\\int_{0}^{t}\\langle\\gamma_{s},d\\mathbf{W}_{t}^{Q}\\rangle_{\\mathcal{H}_{0}}-\\frac{1}{2}\\int_{0}^{T}\\|\\gamma_{s}\\|_{\\mathcal{H}_{0}}^{2}\\,d t\\right)d\\mathbb{P}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Or we can derive alternative formulation, by substituting $\\begin{array}{r}{\\mathbf{W}_{t}^{Q}=\\tilde{\\mathbf{W}}_{t}^{Q}+\\int_{0}^{T}\\gamma_{s}d s}\\end{array}$ to (A.52), ", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\mathbb{Q}=\\exp\\left(\\int_{0}^{t}\\langle\\gamma_{s},d\\tilde{\\mathbf{W}}_{t}^{Q}\\rangle_{\\mathcal{H}_{0}}+\\frac{1}{2}\\int_{0}^{T}\\|\\gamma_{s}\\|_{\\mathcal{H}_{0}}^{2}\\,d t\\right)d\\mathbb{P}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we will apply Girsanov\u2019s theorem to path measures related to (1) and (9). Let $\\mathbb{Q}:=\\mathbb{P}^{\\alpha}$ in (A.53). Then $\\gamma_{s}:=Q^{1/2}\\alpha_{s}$ and we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d\\mathbb{P}^{\\alpha}}{d\\mathbb{P}}=\\exp\\left(\\int_{0}^{t}\\langle\\alpha,d\\tilde{\\mathbf{W}}_{t}^{Q}\\rangle_{\\mathcal{H}_{0}}+\\frac12\\int_{0}^{T}\\left\\|Q^{1/2}\\alpha_{s}\\right\\|_{\\mathcal{H}_{0}}^{2}d t\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Proof can be founded in [17, Theorem 10.14] ", "page_idx": 19}, {"type": "text", "text": "Since our goal is find an optimal control $\\alpha^{\\star}$ such that ${\\bf X}_{T}^{\\alpha^{\\star}}$ satisfying terminal constraints which is represented by function $\\tilde{G}$ in (6), we may define our target path measure $\\mathbb{P}^{\\star}$ as $\\begin{array}{r}{\\frac{d\\mathbb{P}^{\\star}}{d\\mathbb{P}}=\\frac{1}{\\mathcal{Z}}\\tilde{G}(\\cdot)}\\end{array}$ [57], where $\\mathcal{Z}=\\mathbb{E}_{\\mathbb{P}}\\left[\\tilde{G}(\\mathbf{X}_{T})\\right]$ Then, we can compute the logarithm of Radon Nikodym derivative as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\log\\frac{d\\mathbb{P}^{\\alpha}}{d\\mathbb{P}^{\\star}}=\\log\\frac{d\\mathbb{P}^{\\alpha}}{d\\mathbb{P}}+\\log\\frac{d\\mathbb{P}}{d\\mathbb{P}^{\\star}}-\\mathcal{Z}}}\\\\ &{}&{\\approx\\displaystyle\\int_{0}^{t}\\langle\\alpha,d\\tilde{\\mathbf{W}}_{t}^{Q}\\rangle_{\\mathcal{H}_{0}}+\\frac{1}{2}\\displaystyle\\int_{0}^{T}\\left\\|Q^{1/2}\\alpha_{s}\\right\\|_{\\mathcal{H}_{0}}^{2}d t+G(\\cdot).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\tilde{\\mathbf{W}}_{t}^{Q}$ is $Q$ -Wiener process on $\\mathbb{P}^{\\alpha}$ , we can compute the relative entropy loss in Sec 2.4: ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\mathrm{ref}}(\\mathbb{P}^{\\alpha}|\\mathbb{P}^{\\star})=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\log\\frac{d\\mathbb{P}^{\\alpha}}{d\\mathbb{P}^{\\star}}\\right]=\\mathbb{E}_{\\mathbb{P}^{\\alpha}}\\left[\\frac12\\int_{0}^{T}\\|\\alpha_{s}\\|_{\\mathcal{H}}^{2}\\,d t+G(\\mathbf{X}_{T}^{\\alpha})\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we denote $\\|(\\cdot)\\|_{\\mathcal{H}_{0}}^{2}\\;=\\;\\bigl\\|Q^{-1/2}(\\cdot)\\bigr\\|_{\\mathcal{H}}^{2}$ This representation matches with (3) when $R(\\cdot)\\;:=$ $\\frac{1}{2}\\left\\|\\cdot\\right\\|_{\\mathcal{H}}^{2}$ . Similarly, the cross entropy loss in (19) can be derived by set $\\mathbb{Q}:=\\mathbb{P}^{\\star}$ and $\\mathbb{P}:=\\mathbb{P}^{\\alpha}$ in (A.53). Then $\\gamma_{s}$ can be defined as (18) and $\\begin{array}{r}{\\hat{\\mathbf{W}}_{t}^{Q}=\\tilde{\\mathbf{W}}_{t}^{Q}+\\int_{0}^{T}\\gamma_{s}(\\theta)d s}\\end{array}$ is a $Q$ -Wiener process on $\\mathbb{P}^{\\star}$ where $\\mathbb{P}^{\\alpha}$ satisfies the Radon-Nikodym derivative: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d\\mathbb{P}^{\\star}}{d\\mathbb{P}^{\\alpha}}=\\exp\\left(\\int_{0}^{t}\\langle\\gamma_{s}(\\theta),d\\hat{\\mathbf{W}}_{t}^{Q}\\rangle_{\\mathcal{H}_{0}}+\\frac{1}{2}\\int_{0}^{T}\\|\\gamma_{s}(\\theta)\\|_{\\mathcal{H}_{0}}^{2}\\,d t\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, the cross-entropy loss can be computed as in (19). ", "page_idx": 19}, {"type": "text", "text": "A.6 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Let us consider that the marginal distributions $\\{\\mu_{t}^{\\star}\\}_{t\\in0,T}$ satisfying the following relation in a weak sense [4]: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{t}\\int f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t})=\\int f(\\mathbf{x}_{t})\\mathcal{L}_{t}^{\\star}\\mu_{t}^{\\star}(d\\mathbf{x}_{t})=\\int\\mathcal{L}_{t}f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{L}_{t}^{\\star}$ is adjoint operator of $\\mathcal{L}_{t}$ . Now, let us denote $\\mu_{t|0,T}(\\mathbf{x}_{t})=\\mu_{t}^{\\star}(\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T})$ and consider factorizable marginal distribution $\\begin{array}{r}{\\mu_{t}^{\\star}(d{\\bf x}_{t})=\\int_{\\Pi}\\mu_{t}^{\\star}({\\bf x}_{t}|{\\bf x}_{0},{\\bf x}_{T})\\Pi(d{\\bf x}_{0},d{\\bf x}_{T})}\\end{array}$ , where $\\mu_{t}^{\\star}\\big({\\bf x}_{t}|{\\bf x}_{0},{\\bf x}_{T}\\big)$ satisfying the following relation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}\\displaystyle\\int_{\\mathcal{H}}f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T})=\\displaystyle\\int_{\\mathcal{H}}\\mathcal{L}_{t|0,T}f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T})}\\\\ &{=\\displaystyle\\int_{\\mathcal{H}}\\left[\\langle\\varLambda\\mathbf{x}_{t},D_{\\mathbf{x}}f(\\mathbf{x}_{t})\\rangle\\boldsymbol{\\pi}+\\langle h_{t|0,T}(\\mathbf{x}_{t}),D_{\\mathbf{x}}f(\\mathbf{x}_{t})\\rangle_{\\mathcal{H}}+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}Q D_{\\mathbf{x}\\mathbf{x}}f(\\mathbf{x}_{t})\\right]\\right]\\mu_{t}^{\\star}(d\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we denote $h_{t|0,T}(\\mathbf{x}_{t}):=\\sigma^{2}Q D_{\\mathbf{x}}\\log h(t,\\mathbf{x_{t}})$ . Now, we can obtain the Kolmogorov operator associated with the diffusion process associated with the marginal distributions $\\mu_{t}^{\\star}(d\\mathbf{x}_{t})$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}\\displaystyle\\int_{\\mathcal{H}}f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t})=\\partial_{t}\\displaystyle\\int_{\\Pi}\\int_{\\mathcal{H}}f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T})\\Pi(d\\mathbf{x}_{0},d\\mathbf{x}_{T})}\\\\ &{=\\displaystyle\\int_{\\mathcal{H}}\\left[\\langle A\\mathbf{x}_{t},D_{\\mathbf{x}}f(\\mathbf{x}_{t})\\rangle_{\\mathcal{H}}+\\langle h_{t}^{\\star}(\\mathbf{x}_{t}),D_{\\mathbf{x}}f(\\mathbf{x}_{t})\\rangle_{\\mathcal{H}}+\\frac{1}{2}\\mathrm{Tr}\\left[\\sigma^{2}Q D_{\\mathbf{xx}}f(\\mathbf{x}_{t})\\right]\\right]\\mu_{t}^{\\star}(d\\mathbf{x}_{t})}\\\\ &{=\\displaystyle\\int_{\\mathcal{H}}\\mathcal{L}_{t}f(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t})=\\displaystyle\\int f(\\mathbf{x}_{t})\\mathcal{L}_{t}^{\\star}\\mu_{t}^{\\star}(d\\mathbf{x}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have defined $\\begin{array}{r}{\\int_{\\mathcal{H}}h_{t}^{\\star}(\\mathbf{x}_{t})\\mu_{t}^{\\star}(d\\mathbf{x}_{t}):=\\int_{\\Pi}\\int_{\\mathcal{H}}h_{t|0,T}\\mu_{t}^{\\star}(d\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T})\\Pi(d\\mathbf{x}_{0},d\\mathbf{x}_{T})}\\end{array}$ . It implies that the diffusion dynamics $d\\mathbf{X}_{t}^{\\star}$ associated with the Kolmogorov operator $\\mathcal{L}:=\\langle\\mathcal{A}\\mathbf{x}_{t},D_{\\mathbf{x}}f(\\mathbf{x}_{t})\\rangle_{\\mathcal{H}}+$ $\\begin{array}{r}{\\langle h_{t}^{\\star}({\\bf x}_{t}),D_{\\bf x}f({\\bf x}_{t})\\rangle_{\\mathcal{H}}\\,+\\,\\frac{1}{2}\\mathrm{Tr}\\left[\\bar{\\sigma}^{2}Q D_{\\bf x x}f({\\bf x}_{t})\\right]}\\end{array}$ also associated with Fokker-Planck equation [7] $\\mathscr{L}_{t}^{*}\\mu_{t}^{\\star}(d\\mathbf{x}_{t})\\,=\\,0$ , meaning $\\mathbf{X}_{t}^{\\star}\\sim\\mu_{t}^{\\star}$ for all $t\\ \\in\\ [0,T]$ . Now, for some reference measure $\\mu_{\\mathrm{ref}}$ where the Radon-Nikodym derivatives dd\u00b5\u00b5rtef (x $\\begin{array}{r}{\\frac{d\\mu_{t}^{\\star}}{d\\mu_{\\mathrm{ref}}}(\\mathbf x_{t})\\,=\\,p_{t}(\\mathbf{\\dot{x}}_{t}),\\frac{d\\mu_{t|0,T}}{d\\mu_{\\mathrm{ref}}}(\\mathbf x_{t})\\,=\\,p_{t|0,T}(\\mathbf x_{t})}\\end{array}$ ),d\u00b5dt\u00b5|r0e,fT (xt) = pt|0,T (xt) exist. Then, under $\\mu_{\\mathrm{ref}},\\,h_{t}^{\\star}$ can be defined as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\nh_{t}^{\\star}(\\mathbf x_{t})=\\frac{\\int_{\\Pi}h_{t|0,T}(\\mathbf x_{t})p_{t|0,T}(\\mathbf x_{t})\\Pi(d\\mathbf x_{0},d\\mathbf x_{T})}{p_{t}(\\mathbf x_{t})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, the diffusion process associated with marginal distributions $\\mu_{t}^{\\star}({\\bf x}_{t})$ has following representation: ", "page_idx": 20}, {"type": "equation", "text": "$$\nd\\mathbf{X}_{t}^{\\star}=\\left[A\\mathbf{X}_{t}^{\\star}+\\sigma^{2}Q\\mathbb{E}_{\\mathbf{x}_{T}\\sim\\mathbb{P}^{h}(d\\mathbf{x}_{T}|\\mathbf{X}_{t}^{h})}\\left[q_{T-t}(\\mathbf{X}_{t}^{\\star},\\mathbf{x}_{T})\\right]\\right]d t+\\sigma d\\mathbf{W}_{t}^{Q},\\quad\\mathbf{X}_{0}^{\\star}\\sim\\mu_{0}^{\\star}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "A.7 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Let us assume the initial condition is fixed to deterministic point $\\mathbf{x}_{0}\\in\\mathcal{H}$ and define a reference measure $\\mu_{\\mathrm{ref}}\\,=\\mathcal{N}(0,Q_{\\infty})$ . Since our goal is ${\\bf X}_{T}^{\\alpha}$ satisfying the terminal condition $G$ , define $\\mathbb{P}^{\\star}$ as $\\begin{array}{r}{\\frac{d\\mathbb{P}^{\\star}}{d\\mathbb{P}}=\\frac{1}{\\mathcal{Z}}\\tilde{G}(\\cdot)}\\end{array}$ , where $\\mathcal{Z}=\\mathbb{E}_{\\mathbb{P}}[\\tilde{G}(\\mathbf{X}_{T})]$ . Therefore, we have the following relation for marginal distributions $d\\mu_{T}^{\\star}\\,=\\,\\textstyle{\\frac{1}{Z}}\\tilde{G}(\\mathbf{X}_{T})d\\mu_{T}$ . Moreover, since we have defined $\\begin{array}{r}{G(\\mathbf{X}_{T})\\,=\\,-\\log\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T})}\\end{array}$ in (23), then it result $\\begin{array}{r}{h(t,\\mathbf{x})=\\mathbb{E}_{\\mathbb{P}}\\left[\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T})\\vert\\mathbf{X}_{t}=\\mathbf{x}\\right]}\\end{array}$ by following Theorem 2.2, we get $\\tilde{G}(\\mathbf{X}_{T})=$ $h(T,\\mathbf{X}_{T})$ . Hence, we have for any Borel set $B\\in B(\\bar{\\mathcal{H}})$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{B}d\\mu_{T}^{\\star}(\\boldsymbol{v})=\\int_{B}\\frac{h(T,\\mathbf{X}_{T})}{h(0,\\mathbf{x}_{0})}d\\mu_{T}}}\\\\ &{=\\int_{B}\\frac{\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T})}{\\int_{\\mathcal{H}}\\frac{d\\pi_{T}}{d\\mu_{T}}(\\mathbf{X}_{T})d\\mu_{T}}d\\mu_{T}=\\int_{B}d\\pi_{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, given that we have confirmed that the conditioned SDE in (9) correspond to the controlled process (9) with optimal control, we can establish the result $\\mathbb{P}(\\mathbf{X}_{T}^{\\alpha^{\\star}}\\in B)=\\pi(B)$ . This concludes the proof. \u53e3 ", "page_idx": 20}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/a142cf970265496880dddc4a6eae571868f9140ef8646fe161cfd0a93c9df34e.jpg", "img_caption": ["Figure A.1: Transformer-based network architecture. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.8 Experimental Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A.8.1 Experiment on 2D-Domain ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Synthetic Experiment. In a synthetic experiment, we computed the log-probability of $p_{0}$ and $p_{T}$ across a uniformly sampled grid of $64^{2}$ points, with each point $\\mathbf{p}_{i}$ ranging within $[-7,7]^{2}$ . For $p_{0}$ , the log-probability was generated using an 8-Gaussian mixture model as specified in [70]. For $p_{T}$ , we employed the following log-density function: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log p_{T}(\\mathbf{p})=-\\frac{\\operatorname*{min}(\\left\\|\\mathbf{p}-1\\right\\|^{2},\\left\\|\\mathbf{p}-3\\right\\|^{2},\\left\\|\\mathbf{p}-5\\right\\|^{2})}{0.05}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Training was conducted using the Adam optimizer with a learning rate of $1e-3$ . The network was trained with a batch size of 24 for a total of 1000 iterations. We set $\\sigma=0.2$ in (1) for this experiment and set 100 discretization steps. We use a single A6000 GPU for this experiment. ", "page_idx": 21}, {"type": "text", "text": "The control function was parameterized using a 4-layer FNO-2D [40], with the cutoff number of Fourier modes set at 8 and each convolution layer having a width of 32. ", "page_idx": 21}, {"type": "text", "text": "Simulation of DBFS. We follow the simulation scheme introduced in [42, 58]. For ${\\bf x}=(x_{1},x_{2})\\in$ $\\mathbb{R}^{2}$ , we use the discrete cosine transformation (DCT) for projection. Specifically, the eigenvector $\\phi^{(k)}(\\mathbf{x})$ and eigenvalue ${\\lambda}^{(k)}$ of the negative Laplacian operator $-\\Delta$ , which is positive definite and Hermitian, and satisfies the zero Neumann boundary condition, are given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-\\,\\Delta\\phi^{(k)}({\\bf x})=\\lambda^{(k)}\\phi^{(k)}({\\bf x})}~}\\\\ {{\\displaystyle\\frac{\\partial\\phi^{(k)}({\\bf x})}{\\partial x_{1}}=\\frac{\\partial\\phi^{(k)}({\\bf x})}{\\partial x_{2}}=0,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It implies that the orthonormality of $\\phi^{(k)}(\\cdot)$ with respect to the associated inner product, thereby enables computations in (A.47-A.50). Now, considering a rectangular domain with Cartesian coordinates, where pixels in the image are sampled from an underlying regular domain, the eigenbasis is given as a separable cosine basis: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\phi^{(n,m)}(x_{1},x_{2})\\sim\\cos\\left(\\displaystyle\\frac{\\pi n x_{1}}{W}\\right)\\cos\\left(\\displaystyle\\frac{\\pi m x_{2}}{H}\\right)}}\\\\ {{\\lambda^{(n,m)}=\\pi^{2}\\left(\\displaystyle\\frac{n^{2}}{W^{2}}+\\displaystyle\\frac{m^{2}}{H^{2}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, the negative Laplacian $-\\Delta$ can then by represented by an eigen decomposition $-\\Delta=\\mathbf{EDE}^{T}$ , where ${\\bf E}^{T}$ is the projection matrix for the DCT $i.e.,\\tilde{\\mathbf{X}}_{t}=\\mathbf{E}^{T}\\mathbf{X}_{t}=\\complement\\complement\\mathrm{{DCT}}(\\mathbf{X}_{t})$ , and $\\mathbf{D}$ is a digonal ", "page_idx": 21}, {"type": "text", "text": "Input: Linear operator $\\mathcal{A}$ , trace-class operator $Q$ , learned control $\\alpha$ , initial distribution $\\pi_{0}$ , discrete and   \ninverse discrete cosine transforms DCT, iDCT, target resolution grid $\\mathbf{T}^{2}$ , trained resolution grid $\\mathbf{O}^{2}$ .   \nSample the initial condition $\\mathbf{X}_{0}^{\\alpha}=\\mathbf{x}_{0}\\sim\\pi_{0}$   \nif $\\mathbf{T}^{\\hat{2}}\\neq\\mathbf{O}^{2}$ then Upsample $\\mathbf{X}_{0}^{\\alpha}=\\{\\mathbf{X}_{0}^{\\alpha}[\\mathbf{p}_{i}]\\}_{i=1}^{\\mathbf{O}^{2}}$ to $\\{\\mathbf{X}_{0}^{\\alpha}[\\mathbf{p}_{i}]\\}_{i=1}^{\\mathbf{T}^{2}}$   \nend if   \nfor $t=0,\\cdots\\,,T$ do Estimate the control $\\alpha_{t}^{\\star}=\\alpha(t,{\\mathbf{X}}_{t}^{\\alpha};\\theta^{\\star})$ Sample Gaussian noise $\\xi\\sim\\mathcal{N}(0,\\mathbf{I})$ Discrete cosine transform the initial condition, estimated control, Gaussian noise (k)}kT=21 = DC2T (Xt\u03b1 ), {\u03b1\u02dct(k)}kT=21 = DCT(\u03b1t\u22c6 ), {\u03be\u02dc(k)}kT=21 for $k=1,\\cdots\\,,^{\\mathbf{T}^{2}}$ do in parallel $\\tilde{\\mathbf{X}}_{t+\\Delta_{t}}^{(k)}=\\left[-a_{k}\\tilde{\\mathbf{X}}_{t}^{(k)}+\\sigma\\sqrt{\\lambda_{k}}\\tilde{\\alpha}_{t}^{(k)}\\right]\\Delta_{t}+\\sigma\\sqrt{\\lambda_{k}\\Delta_{t}}\\tilde{\\xi}^{(k)}$ end for Inv\u03b1erse di\u03b1screte cosine transform, $\\mathbf{X}_{t+\\Delta_{t}}^{\\alpha}=\\pounds{\\tt i D C T}(\\{\\tilde{\\mathbf{X}}_{t+\\Delta_{t}}^{(k)}\\}_{k=1}^{\\mathbf{T}^{2}})$ Xt = Xt+\u2206t   \nend for   \nOutput: ${\\bf X}_{T}^{\\alpha}\\sim\\pi_{T}$ ", "page_idx": 22}, {"type": "text", "text": "matrix containing the eigenvalues $\\lambda^{(n,m)}$ . Hence the controlled SDEs (2) with $Q=\\mathbf{I}$ can be rewritten as ", "page_idx": 22}, {"type": "equation", "text": "$$\nd\\tilde{\\mathbf{X}}_{t}^{\\alpha}=\\left[-\\mathbf{D}\\tilde{\\mathbf{X}}_{t}^{\\alpha}+\\sigma\\tilde{\\alpha_{t}}\\right]d t+\\sigma d\\tilde{\\mathbf{W}}_{t},\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\tilde{\\mathbf{X}}_{t}=\\mathbf{E}^{T}\\mathbf{X}_{t},\\tilde{\\alpha}_{t}=\\mathbf{E}^{T}\\alpha_{t}$ , and $\\tilde{\\mathbf{W}}_{t}\\overset{d}{=}\\mathbf{W}_{t}$ for all $t\\in[0,T]$ . ", "page_idx": 22}, {"type": "text", "text": "Sampling Algorithm. The sampling algorithm for DBFS in bridge matching, discussed in Section 5.1, is provided in detail in Algorithm 3. ", "page_idx": 22}, {"type": "text", "text": "Unpaired dataset Transfer Experiment. For the experiment involving transfer between dataset, we followed the setup described in [51]. ", "page_idx": 22}, {"type": "text", "text": "(A) For the EMNIST and MNIST datasets, the initial distribution, $\\pi_{0}$ , was set as the MNIST dataset, while for the terminal distribution, $\\pi_{T}$ , we used the EMNIST dataset with the first five lowercase and uppercase characters, as outlined by [19]. The iterative training scheme proposed by [51]. was adopted, which involved two neural networks, $\\alpha_{t}(t,\\mathbf{x},\\theta)$ and $\\alpha_{t}(t,{\\bf x},\\psi)$ , each with around 20.7 million parameters. These networks approximate mixtures of bridges for the forward $(\\pi_{0}\\rightarrow\\pi_{T})$ ) and reverse $(\\pi_{T}\\rightarrow\\pi_{0})$ ) directions, respectively. The SDE was discretized into 30 steps without a noise schedule. The DBFS model was trained for 60 iterations, with each iteration comprising 5,000 gradient updates. Additionally, 2,560 cached images were used for training each network, updated every 250 steps. We used the Adam optimizer with a learning rate of 1e-4 and a batch size of 128, with the EMA rate set to 0.999. The complete DBFS training for the MNIST experiment took approximately 15 hours on a single A6000 GPU. ", "page_idx": 22}, {"type": "text", "text": "(B) For the AFHQ dataset $[14]^{5}$ , we evaluated DBFS between the wild and cat classes on a $64^{2}$ grid, with each class containing approximately 5,000 samples. The control networks each contained about 120.3 million parameters. We discretized the SDE into 100 steps without using a noise schedule. The Adam optimizer was used with a learning rate of 1e-4, and the EMA rate was set to 0.999. We used a batch size of 64 and trained for 20 iterations, with a total of 400,000 gradient steps. Additionally, 2,560 cached images were used for training each network, updated every 1,000 steps. The training took approximately 8 days, using 8 A6000 GPUs for the experiment. ", "page_idx": 22}, {"type": "text", "text": "For each control network $\\alpha$ , we used a transformer network architecture inspired by PerceiverIO [36] from public repository6 to model functional representation. This transformer architecture was chosen for its efficiency in evaluating field representations over a large number of grid points and its ability to map arbitrary input arrays to arbitrary output arrays in a domain-agnostic way. Additionally, we adopted the attention mechanism proposed in [49]. The decoding cross-attention mechanism was also modified, inspired by [39], with the output query set to the target grid points, which were transformed as Gaussian Fourier features [67]. ", "page_idx": 22}, {"type": "table", "img_path": "WyQW4G57Zd/tmp/1f80ee1697bb585c3ea2711752f9ae3ae09a7fe29149a4d2467521f84aefe6d7.jpg", "table_caption": ["Table A.1: Network Hyper-parameters "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "In practice, we start by passing the evaluations $\\mathbf{X}[\\mathbf{p}]$ through a single MLP layer and combine it with the Fourier feature embeddings of the grid points p. This combined representation is then input into the encoder blocks as keys and values, where QKV attention is first applied with the latent array as the query, followed by self-attention for each block. We implement the time-modulated attention block proposed by [49], embedding the time $t$ into latent space. Next, the target grid points, represented as Fourier feature embeddings of the grid points p, are fed into the decoder blocks as queries. Here, QKV attention is applied with the encoded latent array serving as keys and values, followed by self-attention in each decoder block. Finally, the decoded array is mapped to grey scale or RBF channels using a single linear layer. Conceptual illustrations of the proposed network are presented in Figure A.1, and detailed network hyperparameters are listed in Table A.1. ", "page_idx": 23}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/9336abdd2b565b2b185de7ea98739024a47901abb5ab28444fa551404aaf9786.jpg", "img_caption": ["Figure A.2: Results on Unpaired image transfer task. (Up) MNIST $\\rightarrow$ EMNIST (Down) AFHQ-64 Cat $\\rightarrow$ Wild. (Left) Real data and (Right) generated samples from our model. For generation at unseen resolutions, the images within the red and blue boxed initial conditions were upsampled (using bi-linear transformation) from the observed resolution $(32^{2})$ for EMNIST and $(64^{2})$ for AFHQ-64 Cat, respectively. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "WyQW4G57Zd/tmp/bc59f9c43d7c841f8a27ac81d19bf2aeade2a8d48b00094f252ca4db8e2ff721.jpg", "img_caption": ["Figure A.3: Results on Unpaired image transfer task. AFHQ-64 Cat $\\rightarrow\\mathrm{Dog}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.8.2 Experiment on 1D-Domain ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the experiments on 1D-domain, we consistently set $\\begin{array}{r}{{\\mathcal A}:=-\\frac12}\\end{array}$ and $Q:=\\exp(-\\left\\|\\mathbf{p}-\\mathbf{p}^{\\prime}\\right\\|^{2}/\\gamma)$ and set $\\gamma\\,=\\,0.2$ and $\\gamma\\,=\\,0.02$ for GP regression and imputaion, respectively. The choice of $\\gamma$ is hyper-parameter, we search over the set $[0,01,0.1,0.2,0.5,1.0]$ and find optimal value for GP regression. For imputation, we set $\\gamma=0.02$ by following [6]. ", "page_idx": 24}, {"type": "text", "text": "Terminal Cost Computation For all experiments conducted in the 1D domain, we implemented a parameterized initial condition which takes as input the observed sequences $\\mathbf{X}_{0}^{\\theta}=\\mathbf{x}^{\\theta}\\dot{(}\\mathbf{Y}[\\mathbf{O}])$ . We employed the energy functional $\\boldsymbol{\\mathcal{U}}$ as the Gaussian negative log-likelihood (NLL). For each evaluation point on $\\mathbf{T},\\mathcal{U}$ can be computed as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{U}(\\mathbf{X}_{T}[\\mathbf{T}])=-\\log\\mathcal{N}(\\mathbf{X}_{T}[T]|\\mathbf{Y},\\sigma_{\\theta})=\\sum_{i=1}^{|\\mathbf{T}|}\\frac{\\left\\|\\mathbf{X}_{T}[\\mathbf{p}_{i}]-\\mathbf{Y}[\\mathbf{p}_{i}]\\right\\|^{2}}{2\\sigma_{\\theta}^{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\sigma_{\\theta}^{2}$ is set as an output from the neural network in accordance with [38] for GP regression, and fixed as $\\sigma_{\\theta}^{2}=0.5$ for imputation, to establish a loss function analogous to [6]. Additionally, we specified a learnable prior distribution $\\mu_{\\mathrm{prior}}=\\mathcal{N}(e^{-\\frac{T}{2}{\\bf x}^{\\theta}},Q_{T})$ . Consequently, the terminal cost retains only the NLL term, simplifying the computation. ", "page_idx": 24}, {"type": "text", "text": "GP regression For the GP regression, we borrow the experiment setting from [38]. The model trained with curves generated from GP with RBF kernel and tested in various settings such as data generated from GP with other type of kernel (Mate\u00b4rn 5/2, Periodic). We generated $\\mathbf{p}$ uniformly on the interval $[-2,2]$ and generated $\\mathbf{Y}[\\mathbf{p}]$ from using RBF kernel $\\kappa(\\mathbf{p}_{i},\\mathbf{p}_{j})=l_{1}^{2}\\exp(-\\left\\|\\mathbf{p}_{i}-\\mathbf{p}_{j}\\right\\|^{2}/l_{2}^{2})$ with $l_{1}\\sim\\mathrm{Unif}(0.1,1.0)$ and $l_{2}\\sim\\mathrm{Unif}(0.1,0.6)$ and the white noise $\\dot{\\xi}\\sim\\mathcal{N}(0,1e-2)$ is added. We set $|\\mathbf{O}|$ randomly fro\u221am $\\mathrm{Unif(3,37)}$ and $|\\mathbf{T}|$ from \u221aU $\\mathrm{nif}(3,50-|\\mathbf{O}|)$ . For the other test data, we define $\\kappa(\\mathbf{p}_{i},\\mathbf{p}_{j})=l_{1}^{2}(1+\\sqrt{5}d/l_{2}+5d^{2}/(3l_{2}^{2}))e x p(-\\sqrt{5}d/l_{2})$ with $d=\\left(\\|\\mathbf{p}_{i}-\\mathbf{p}_{j}\\|\\right)$ , $l_{1}\\sim\\mathrm{Unif}(0.1,1.0)$ and $l_{2}\\sim\\mathrm{Unif}(0.1,0.6)$ for Mat\u00b4ern kernel and $\\kappa(\\mathbf{p}_{i},\\mathbf{p}_{j})\\,=\\,l_{1}^{2}\\exp(-2\\sin^{2}(\\pi\\left\\|\\mathbf{p}_{i}-\\mathbf{p}_{j}\\right\\|^{2}/p)/l_{2})$ with $l_{1}\\sim\\mathrm{Unif}(0.1,1.0)$ , $l_{2}\\sim\\mathrm{Unif}(0.1,0.6)$ and $p\\sim\\mathrm{Unif}(0.1,0.5)$ for periodic kernel. ", "page_idx": 24}, {"type": "text", "text": "We set batch size of 100 and trained for $100,000$ iterations. The Adam optimizer is used, the initial learning rate 5e-4 decayed with cosine annealing scheme. For testing, we evaluated the trained models using 3,000 batches, each consisting of 16 samples. We report the mean and standard deviation for five runs. We a single A6000 GPU for this experiment. ", "page_idx": 24}, {"type": "text", "text": "The architectures for NP [28] and CNP [27], we use the same setting as described in $[38]^{7}$ . In our approach, we adapted the CNP architecture to incorporate a parameterized initial condition $\\mathbf{x}^{\\theta}$ (add one linear layer to output $\\mathbf{x}^{\\theta}$ ). The total number of parameters is similar across all three models. ", "page_idx": 24}, {"type": "text", "text": "Physionet Imputation The Physionet [30] contains 4000 clinical time series with 35 variables for 48 hours from intensive care unit. Following [68], we preprocess this datasets to hourly timeseries which have 48 time steps. Since the dataset already contains around $80\\%$ of missingness, we randomly choose $10/50/90\\%$ of observed values as test data. ", "page_idx": 24}, {"type": "text", "text": "In the imputation experiments, we employed the same experimental setup as $[6]^{8}$ which is slight modification of original CSDI model. In this setup, the Gaussian noise of the DDPM model was replaced with GP noise employing an RBF kernel. Additionally, we adjusted the measurement approach to record the actual elapsed time rather than rounding to the nearest hour, better capturing the inherent timing characteristics of the Physionet dataset. We use a single A6000 GPU for this experiment. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, the main claims made in the abstract and introduction are consistent with the scope of the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The limitations are discussed in the conclusion section. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we believe our proof and assumptions are both sufficient and correct. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we believe we have provided all the necessary information to reproduce our results in Appendix A.8. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided the codebase used for our experiments, particularly for unpaired image transfer. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we have provided the experimental details. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, when available, we conducted the experiments five times and reported the mean and standard deviations. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we provided the required resources in the experimental details section. ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]   \nJustification: We support the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The broader impacts are discussed in the conclusion section. ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: We adhere to ethical standards for using our model in generative AI. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, the license and terms of use are noted. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: We do not involve crowdsourcing or research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: We do not involve crowdsourcing or research with human subjects. ", "page_idx": 26}]