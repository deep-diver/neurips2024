[{"type": "text", "text": "Advancing Cross-domain Discriminability in Continual Learning of Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yicheng $\\mathbf{Xu^{1*}}$ Yuxin Chen2\u2217 Jiahao Nie3 Yusong Wang1 Huiping Zhuang4,5\u2020 Manabu Okumura1 ", "page_idx": 0}, {"type": "text", "text": "1Institute of Science Tokyo 2University of California Berkeley 3Nanyang Technological University 4South China University of Technology 5Greater Bay Area Institute for Innovation, Hunan University, China ", "page_idx": 0}, {"type": "text", "text": "yxu040@e.ntu.edu.sg, yuxinc@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs. However, existing methods require additional reference datasets to maintain such zero-shot ability and rely on domain-identity hints to classify images across different domains. In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space. Cooperating with a training-free fusion module, RAIL absolutely preserves the VLM\u2019s zero-shot ability on unseen domains without any reference data. Additionally, we introduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In this setting, a CL learner is required to incrementally learn from multiple domains and classify test images from both seen and unseen domains without any domain-identity hint. We theoretically prove RAIL\u2019s absolute memorization on incrementally learned domains. Experiment results affirm RAIL\u2019s state-ofthe-art performance in both X-TAIL and existing Multi-domain Task-Incremental Learning settings. The code is released at https://github.com/linghan1997/ Regression-based-Analytic-Incremental-Learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning (CL) [1, 2, 3] is a crucial area in machine learning, which requires a learner to incrementally learn new data instead of training from scratch. The main challenge in CL is known as catastrophic forgetting [4], where learning new knowledge results in the forgetting of the old one. To this end, various CL approaches [5, 6, 7, 8, 3, 9] have been proposed to solve the forgetting issue. As a typical CL setting, Class-Incremental Learning (CIL) (Fig. 1 (a)) aims to achieve robust discriminability on all seen classes. Despite the advancements, existing approaches mainly focus on classifying images only from seen classes, thereby limiting the model\u2019s generalizability. ", "page_idx": 0}, {"type": "text", "text": "Consequently, Zheng et al. [10] proposed Multi-domain Task-Incremental Learning (MTIL), which cooperates CL with the zero-shot ability of Vision-Language Models (VLMs) [11, 12, 13, 14] such as CLIP [13]. This integration equips models with the ability to classify domains they have not yet encountered, enhancing their generalizability across multiple domains (Fig. 1 (b)). Several methods [10, 15] have been specifically designed for MTIL, in which the model is required to retain both the incrementally learned knowledge during CL and the zero-shot ability of VLMs. However, these methods require a domain-identity hint to indicate the specific domain of the test image, which is often not applicable in real-world scenarios [7]. Additionally, the use of reference datasets during training is necessary to maintain pre-trained VLMs\u2019 zero-shot performance. ", "page_idx": 0}, {"type": "image", "img_path": "boGxvYWZEq/tmp/ef7c5a907e6b119cf079db5003eea35bd07fdc98dac8f5f3ba645b148efba5c5.jpg", "img_caption": ["Figure 1: Comparison of different CL settings. (a) In CIL, models classify images within all previously encountered classes. (b) In MTIL, models classify images from both seen and unseen domains based on the given domain-identities. (c) In X-TAIL, models classify images from both seen and unseen domains without any domain-identity hint. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned limitations, we introduce Regression-based Analytic Incremental Learning (RAIL), a novel approach that incrementally learns new knowledge and performs effectively on both seen and unseen domains. Specifically, we leverage non-linear projection functions from both primal and dual perspectives to enhance the expressiveness of features extracted by the pre-trained CLIP. It endows the learner with the ability to classify images in a cross-domain label set without any domain-identity hint. In the incremental learning process, RAIL utilizes a ridge regression-based adapter that updates the parameters recursively. This is identical to learning on all encountered domains at once, achieving absolute memorization on learned domains. Additionally, we freeze the pre-trained CLIP and design a training-free fusion module to determine whether the test data belongs to seen or unseen domains. This strategy absolutely preserves CLIP\u2019s zero-shot ability on unseen domains, meeting practical requirements for models deployed in dynamic environments. ", "page_idx": 1}, {"type": "text", "text": "To demonstrate the effectiveness of our method, we propose Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting as illustrated in Fig. 1 (c). Particularly, X-TAIL requires CL methods to incrementally transfer a pre-trained VLM to multiple domains while evaluating the model\u2019s performance on both seen and unseen domains. Moreover, domain hints are forbidden in X-TAIL, making it more realistic and challenging [3]. As a result, effective CL methods must classify a test image into the correct domain and class simultaneously. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a new CL method RAIL to incrementally adapt the pre-trained VLM to multiple domains without forgetting both pre-trained and incrementally learned knowledge.   \n\u2022 To meet the practical scenario where CL methods need to sequentially learn data from new domains and classify images across these domains, we propose a new setting X-TAIL to evaluate the preservation of VLM\u2019s zero-shot ability and the adaptability to new domains.   \n\u2022 We theoretically prove the RAIL\u2019s absolute memorization on incrementally learned domains and demonstrate that the zero-shot ability of the pre-trained VLM on unseen domains is absolutely preserved.   \n\u2022 We empirically show that the proposed method achieves state-of-the-art performances on both existing MTIL and the novel X-TAIL settings. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Early CL methods focused on Task-Incremental Learning (TIL) [16], where a task-id is given during testing. Subsequently, a more practical and challenging setting of Class-Incremental Learning (CIL) [7] was proposed, where the access to the task-id is forbidden at inference time. Methods for CIL must therefore distinguish between all classes encountered in learned tasks. More recently, Zheng et al. [10] proposed Multi-Domain Task-Incremental Learning (MTIL), which is especially designed to evaluate CL methods with pre-trained VLMs. In MTIL, a pre-trained VLM continually adapts to multi-domain tasks. The performance on both seen and unseen tasks measure the retention of both incrementally acquired and pre-trained knowledge. However, it still requires the task-id to create specific domain label space at inference time. Apart from them, X-TAIL combines the challenges of both CIL and MTIL, in which the model learns new classes from various incoming domains and distinguishes between both seen and unseen classes without any domain-identity. ", "page_idx": 2}, {"type": "text", "text": "Prevailing continual learning methods include replay-based, distillation-based, regularization-based, and architecture-based approaches [3]. Replay-based methods such as iCaRL [7] typically store a small portion of the previous task data as exemplars. The model is then trained jointly on new task data and the saved exemplars to preserve the previous knowledge. Distillation-based methods such as LwF [6] use either weight or function regularization to transfer knowledge from the previous model to the current model for knowledge distillation. Regularization-based methods such as ZSCL [10] penalize the shift of either model parameter or feature space by adding a regularization term to the cross-entropy loss function. To preserve the robustness of the strong pre-trained model without access to the pre-trained dataset, ZSCL utilizes large-scale reference datasets to regularize the parameter space. Architecture-based methods [17, 18, 19] expand the model by constructing task-specific parameters to avoid inter-task interference. For example, MoE-Adapters [15] cooperates the pretrained CLIP with mixture of experts (MoE) [20] to learn from different domains. By leveraging a reference dataset to initialize a task-id indicator, it enables the model to distinguish unseen tasks from seen ones. ", "page_idx": 2}, {"type": "text", "text": "The aforementioned methods either neglect the forgetting issue of pre-trained knowledge or require multiple iterations and large-scale reference datasets for training, making it challenging to efficiently adapt to new data in continual learning scenarios. By contrast, RAIL employs an analytical solution that achieves the optimum in a single epoch without additional reference data, ensuring its efficiency. ", "page_idx": 2}, {"type": "text", "text": "3 Cross-domain task-agnostic incremental learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define Cross-domain Task-Agnostic Incremental Learning (X-TAIL) as follows. Given a pre-trained VLM, the learner is required to incrementally transfer it to $N$ different domains $\\{D^{(1)},D^{(2)},...,D^{(N)}\\}$ . Each domain $D^{(n)}\\;=\\;\\{({\\bf x}_{j}^{(n)},y_{j}^{(n)})\\}_{j=1}^{|D^{(n)}|}$ is available only during the $n$ -th learning step. The class labels $y_{j}^{(n)}\\in C_{\\mathrm{label}}^{(n)}$ from the incrementally learned domain $D^{(n)}$ are added to the set of seen class labels. During inference at all steps, the learner attempts to classify input images from any domain without the domain-identity hint. In other words, the ground-truth lcalabsesl  loafb tehlse  ftreostm i amlla gpree bvieolouns gles atron $C_{N}=C_{L}\\cup C_{U}$ ,i sw thheer se $\\begin{array}{r}{C_{L}=\\bigcup_{i=0}^{n}C_{\\mathrm{label}}^{(i)}}\\end{array}$ aibse tlsh.e union of seen $C_{U}$ ", "page_idx": 2}, {"type": "text", "text": "Similar to the task-id in TIL, the domain-identity hint allows the learner to classify input data within the label space of a specific domain during evaluation. Essentially, the learner knows the domain of test images, which is far from real-world application scenarios. For instance, the learner is supposed to predict an image as the class of husky from all possible classes $C_{N}=\\!\\langle c a r,$ , bus, ..., churros, donuts, ..., husky, beagle, bulldog, ...}. However, if the domain-identity hint is given, the learner only needs to predict from a limited subset $C_{\\mathrm{dog}}=\\!\\!\\{h u s k y\\!\\!\\}$ , beagle, bulldog, ...}, which is simpler but less realistic compared to practical applications. Therefore, we extend our setting to a task-agnostic scenario. Specifically, the learner predicts images from $C_{N}$ , the union of any potential class labels, without any domain hint. ", "page_idx": 2}, {"type": "text", "text": "3.2 Datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In X-TAIL\u2019s cross-domain setting, the learner should encompass as extensive data distributions as possible. Following previous work [10, 15, 21, 13, 22, 23], we select 10 different imageclassification datasets from different domains for our setting: Aircraft [24], Caltech101 [25], DTD [26], EuroSAT [27], Flowers [28], Food [29], MNIST [30], OxfordPet [31], StanfordCars [32], and SUN397 [33]. Specifically, to prevent the redundancy of learning overlapping classes and to maintain the integrity of the setting, CIFAR-100 [34] was excluded because it includes many classes that overlap with those in other domains. As a result, CL methods under X-TAIL should discriminate images from a total of 1, 100 classes across all domains. ", "page_idx": 3}, {"type": "text", "text": "3.3 Evaluation metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We adopt the evaluation metrics from [10] for our setting. As illustrated in Fig. 2, each column represents the performance on a specific domain after each learning step, while the rows correspond to the learning sequence. In traditional CL settings, only the results in the lower diagonal where the learner has been exposed to the exemplars from the test domains are measured. Nevertheless, X-TAIL extends this evaluation to cover the entire matrix, recording performances across both learned and unlearned domains. The \u201cAverage\u201d metric, averaged on the orange blocks, indicates the average accuracy of all learning steps across all domains. The gray and green blocks under the diagonal show the classification performance on these domains after the model has learned these domains. Specifically, the green blocks represent the model\u2019s last performance on these domains after learning all domains. The \u201cLast\u201d metric, which is the average of the green blocks, reflects the learner\u2019s adaptability to new domains. Additionally, the blue blocks in the upper-right matrix indicate the model\u2019s zero-shot performance on these domains before learning these domains. The average of these blocks, referred to as the \u201cTransfer\u201d metric, measures the extent to which the zero-shot ability is preserved throughout incremental learning. ", "page_idx": 3}, {"type": "image", "img_path": "boGxvYWZEq/tmp/1b75a3dd2efd95b57a58a3b57d9a7b8f7799c36550dc51f8e8fde815873c983c.jpg", "img_caption": ["Figure 2: Metrics for X-TAIL setting. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Motivation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While CLIP demonstrates the generalizability of zero-shot classification, it still struggles in certain unfamiliar domains [21]. Leveraging CLIP\u2019s robust feature extraction capabilities, linear probe offers a straightforward approach to transfer the CLIP to these domains [13]. Among various linear solutions, Ridge Regression (RR) provides an effective classification strategy by mapping the image features onto one-hot-label targets [35]. Given a pre-trained CLIP image encoder $f_{I}$ and a dataset $D\\,=\\,\\{({\\bf X},{\\bf Y})\\}$ , where $\\mathbf{X}$ is the tensor of training images and $\\mathbf{Y}$ is the matrix of corresponding one-hot labels, the predicted logits and the optimization problem are defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{y}}=\\mathbf{X}_{e}\\mathbf{W},\\quad\\underset{\\mathbf{W}}{\\arg\\operatorname*{min}}\\left\\|\\mathbf{Y}-\\mathbf{X}_{e}\\mathbf{W}\\right\\|_{F}^{2}+\\lambda\\left\\|\\mathbf{W}\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{X}_{e}=f_{I}\\left(\\mathbf{X}\\right)$ denotes the CLIP extracted features, $\\mathbf{W}$ is the classifier parameter, and $\\lambda$ is the regularization parameter. ", "page_idx": 3}, {"type": "text", "text": "In the context of X-TAIL, the classifier needs to distinguish a wide range of classes from different domains. However, the extracted CLIP features of images from different domains suffer from certain cross-domain correlations, leading to limited domain discriminability. Based on Cover\u2019s theorem [36], one promising approach [37, 38, 39] to enhance the linear separability of features is to project the features into a higher-dimensional space via some non-linear projection function. We explore this non-linear projection function from two perspectives: ", "page_idx": 3}, {"type": "text", "text": "Primal form ridge regression. Following [38], we use a Randomly-initialized Hidden Layer (RHL) to project raw features to a higher dimensional space. By explicitly defining the projection function as $\\phi(\\cdot)$ , the classifier parameter is determined as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\bf W}=\\left({\\Phi}^{\\top}{\\Phi}+\\lambda{\\bf I}\\right)^{-1}{\\Phi}^{\\top}{\\bf Y},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Phi=\\phi\\left(\\mathbf{X}_{e}\\right)$ . In this way, $\\phi(\\cdot)$ is fixed throughout the training process. ", "page_idx": 4}, {"type": "text", "text": "Dual form ridge regression [40]. Instead of manually designing the projection function, we utilize the Kernel method [41] to implicitly define $\\phi(\\cdot)$ based on the inner-product nature of dual form ridge regression. Depending on the choice of kernel function, this approach allows for an infinite projection dimension, which is unachievable through any explicit definition. The dual form solution is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\alpha}=(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\,\\mathbf{Y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{K}=K\\left(\\mathbf{X},\\mathbf{X}\\right)$ denotes the covariance kernel matrix, and $\\kappa\\left(\\cdot,\\cdot\\right)$ can be any positive-definite kernel function. The classification logits are derived as $\\hat{\\mathbf{y}}=\\mathcal{K}\\left(f_{I}\\left(\\mathbf{x}_{\\mathrm{test}}\\right),\\mathbf{X}\\right)\\mathbf{\\alpha}$ . Throughout the paper, we use the Radial Basis Function (RBF) kernel [42] by default. The choice between primal and dual form ridge regression depends on whether the system is over-determined (more equations than unknowns) or under-determined (more unknowns than equations) [43]. Details on the relationships between primal and dual ridge regression can be found in Appendix B. ", "page_idx": 4}, {"type": "image", "img_path": "boGxvYWZEq/tmp/7b182a2dced914bfb6fe871941879d87e15082725905821997418baaca8b43b0.jpg", "img_caption": ["Figure 3: Pearson correlation coefficients (CCs) fo 10 pairs of domain-prototypes. ", "Figure 4: Comparison of in-domain accuracy $(\\%)$ on each domain with three classifiers. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "To empirically verify whether these non-linear projections enhances the separability of CLIP features of images from different domains, we trained three types of classifiers (denoted as Linear, Primal and Dual, respectively) on 10 domains introduced in Sec. 3.2 jointly. To compare the standard linear regression form with aforementioned two approaches, we take the averaged weight vectors as the domain-prototypes and then calculate the inter-domain Pearson correlation coefficients (CCs) between 10 pairs of domain-prototypes. As shown in Fig. 3, the linear regression classifier exhibits high cross-domain correlations. By contrast, the RHL in the primal form significantly reduces these correlations. The implicit projection provided by the kernel trick in the dual form enables better disentangling of different domains. We further evaluate the in-domain accuracy, which represents the rate of correctly classifying images into the appropriate domains. Fig. 4 shows that the indomain accuracy is negatively correlated to cross-domain correlations. Both primal and dual forms demonstrate certain improvements through the projection designs, allowing for accurate classification of images into their respective domains without domain identity hint. ", "page_idx": 4}, {"type": "text", "text": "4.2 Regression-based analytic incremental learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on the projection approaches introduced above, we propose the Regression-based Analytic Incremental Learning (RAIL) method, which incorporates a ridge regression-based adapter and a training-free fusion module. The adapter progressively adapts the pre-trained CLIP to new domains, while the training-free fusion module preserves CLIP\u2019s zero-shot ability on unseen domains. An overview of RAIL is illustrated in Fig. 5. The pseudo-codes of both training and testing algorithms are provided in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "4.2.1 RAIL-Adapter ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the context of CL, in which data arrives progressively, we extend both primal and dual ridge regression solutions to an incremental learning manner. Our solutions are identical to that obtained by joint training, which achieves absolute non-forgetting of learned knowledge. Let $D^{(n)}=\\{{\\bf X}^{(n)},\\dot{\\bf Y}^{(n)}\\}$ represent the $n$ -th training set and $D^{(1:n)}=\\{\\mathbf{X}^{(1:n)},\\mathbf{Y}^{(1:n)}\\}$ represent the union of the training sets from the first $n$ domains. At the $n$ -th learning step, the optimization target for the joint training is expressed as ", "page_idx": 4}, {"type": "image", "img_path": "boGxvYWZEq/tmp/1ba016f6312cf22d6cca906eadd2ee7cbd1f65f2885b0307092bd68d77620909.jpg", "img_caption": ["Figure 5: RAIL Overview. (a) During inference, the fusion module utilizes the Zero-shot logits to identify whether a test image is aligned with seen or unseen classes. If classified as a seen class, the Fusion logits combine the RAIL-Adapter logits and the Zero-shot logits; otherwise solely rely on the Zero-shot logits. (b) Primal: at the $n$ -th learning step, features $\\mathbf{X}_{e}$ extracted by CLIP\u2019s image encoder are projected to higher dimensional $\\Phi$ via RHL and then update the parameter W and memory $\\mathbf{M}_{p}$ by Theorem 1. (c) Dual: features extracted by CLIP\u2019s image encoder update the kernel $\\mathbf{K}$ , parameter $_{\\alpha}$ , and memory $\\mathbf{M}_{d}$ by Theorem 2. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\mathbf{W}^{(n)}}\\left\\|\\mathbf{Y}^{(1:n)}-\\Phi^{(1:n)}\\mathbf{W}^{(n)}\\right\\|_{F}^{2}+\\lambda\\left\\|\\mathbf{W}^{(n)}\\right\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Phi^{(1:n)}\\,=\\,\\phi\\left(f_{I}\\left(\\mathbf{X}^{(1:n)}\\right)\\right)$ . The objective is to obtain $\\mathbf{W}^{(n)}$ that satisfies Eqn. 4 without accessing data from the previous $n\\,-\\,1$ domains. For primal form, we propose to solve $\\mathbf{W}^{(n)}$ recursively using $\\mathbf{W}^{(n-1)}$ and a memory matrix $\\mathbf{M}_{p}^{(n)}$ . The solution is summarized as in Theorem 1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 The parameter calculated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(n)}=\\left[\\mathbf{W}^{(n-1)}-\\mathbf{M}_{p}^{(n)}\\Phi^{(n)\\top}\\Phi^{(n)}\\mathbf{W}^{(n-1)}\\quad\\mathbf{M}_{p}^{(n)}\\Phi^{(n)\\top}\\mathbf{Y}^{(n)}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is an optimal solution to the optimization problem of joint training on all n domains in Eqn. 4, where $\\mathbf{M}_{p}^{(n)}$ is obtained by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{M}_{p}^{(n)}=\\mathbf{M}_{p}^{(n-1)}-\\mathbf{M}_{p}^{(n-1)}\\Phi^{(n)\\top}\\left(\\mathbf{I}+\\Phi^{(n)}\\mathbf{M}_{p}^{(n-1)}\\Phi^{(n)\\top}\\right)^{-1}\\Phi^{(n)}\\mathbf{M}_{p}^{(n-1)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, the dual parameter $\\alpha^{(n)}$ satisfying Eqn. 4 can be obtained based on $\\alpha^{(n-1)}$ , an updating kernel $\\mathbf{K}^{(n)}$ , and a memory matrix $\\mathbf{M}_{d}^{(n)}$ . We denote the matrix $\\mathbf{C}^{\\left(n\\right)}$ as the concatenated one-hot label matrices of all $n$ domains. The solution is defined in Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 The parameter calculated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\alpha}^{(n)}=\\left(\\mathbf{K}^{(n)}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{C}^{(n)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is an optimal solution to the optimization problem of joint training on all n domains in Eqn. 4, where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{K}^{(n)}=\\left[\\boldsymbol{\\kappa}\\left(\\mathbf{X}_{e}^{(n-1)},\\mathbf{M}_{d}^{(n-1)}\\right)\\begin{array}{c c}{\\boldsymbol{K}\\left(\\mathbf{X}_{e}^{(n)},\\mathbf{M}_{d}^{(n-1)}\\right)^{\\top}}\\\\ {\\boldsymbol{K}\\left(\\mathbf{X}_{e}^{(n)},\\mathbf{M}_{d}^{(n-1)}\\right)}&{\\boldsymbol{K}\\left(\\mathbf{X}_{e}^{(n)},\\mathbf{X}_{e}^{(n)}\\right)}\\end{array}\\right],\\quad\\mathbf{C}^{(n)}=\\left[\\mathbf{C}^{(n-1)}\\begin{array}{c c}{\\mathbf{\\Sigma}\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{Y}^{(n)}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the memory matrix is given by M(dn) $\\mathbf{M}_{d}^{(n)}=\\bigg[\\mathbf{M}_{d}^{(n-1)\\top}\\quad\\mathbf{X}_{e}^{(n)\\top}\\bigg]^{\\top}.$ ", "page_idx": 6}, {"type": "text", "text": "At each incremental learning step, the kernel matrix $\\mathbf{K}$ updates recursively along the main diagonal, preserving the correlations among class-prototypes from all domains. During testing, the kernel covariance between the feature of test image extracted by CLIP and memory matrix $\\mathbf{M}_{d}$ is calculated to obtain the classification logits $\\hat{\\mathbf{y}}=\\mathcal{K}\\left(f_{I}\\left(\\mathbf{x}_{\\mathrm{test}}\\right),\\mathbf{M}_{d}\\right)\\boldsymbol{\\alpha}$ . Specifically, $\\mathbf{M}_{d}$ dynamically updates according to the data stream via concatenated class-prototypes. These class-prototypes can be the feature embeddings $\\mathbf{X}_{e}$ extracted by CLIP, K-means centroids, or Gaussian Mixture Model means. We use raw feature embeddings $\\mathbf{X}_{e}$ by default, which is sufficient to validate our method. The complete proofs for both theorems are provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 RAIL-Fusion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we introduce the fusion strategy, which leverages the refined knowledge on seen domains from the RAIL-Adapter while preserving CLIP\u2019s pre-trained knowledge on unseen domains. To distinguish data from different domains without any domain-identity hint, a common approach [18] is to compute domain centers from class-prototypes. The test image is first assigned to a specific domain based on the distances to these domain centers and then classified by a domain-specific classifier. However, this method fails when statistics for unseen domains are unavailable. ", "page_idx": 6}, {"type": "text", "text": "An alternative solution is to leverage CLIP\u2019s zero-shot ability to indicate the domain of the test image. Despite CLIP\u2019s strong generalization ability across domains, certain cross-domain errors (i.e., misclassification into an incorrect domain) persist and do not diminish during incremental learning process. Therefore, the task of classifying images across seen domains is delegated to the RAILAdapter, which significantly reduces these errors, as discussed in Sec. 4.1. Consequently, CLIP\u2019s zero-shot ability is only leveraged to distinguish classes in unseen domains (i.e., Out-Of-Distribution or OOD) from those in seen ones (i.e., In-Distribution or ID) to maintain its performance on unseen domains. We summarize this approach as RAIL-Fusion, which combines both CLIP\u2019s zero-shot logits and RAIL-Adapter logits for prediction, regardless of whether the domain of the test image is seen or unseen. ", "page_idx": 6}, {"type": "text", "text": "Specifically, CLIP first makes a rough prediction based on its zero-shot logits, i.e., the similarity scores between image embeddings and language embeddings from the cross-domain label set $C_{N}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{y}}_{\\mathrm{zs}}=\\operatorname{Softmax}\\left(f_{I}\\left(\\mathbf{x}_{\\mathrm{test}}\\right)f_{T}\\left(\\operatorname{Tokenizer}\\left([P,C_{N}]\\right)\\right)^{\\top}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mathbf{y}}_{\\mathrm{zs}}$ represents the zero-shot logits, $P$ denotes the pre-defined prompt template, and $f_{T}$ and $f_{I}$ are the CLIP text encoder and image encoder, respectively. The result determines whether the test image aligns with the seen classes (ID) that have been encountered during the incremental learning or with the unseen classes (OOD). If classified as ID, the RAIL-adapter refines the rough prediction using its incrementally learned knowledge. If classified as OOD, the rough prediction is taken as the final prediction, fully relying on CLIP\u2019s zero-shot ability. Notably, our fusion strategy guarantees that OOD images correctly classified by CLIP\u2019s zero-shot prediction will never be misclassified as ID, thereby absolutely preserving CLIP\u2019s zero-shot ability on unseen domains. In addition, to prevent the forgetting of the pre-trained knowledge of CLIP on domains with good zero-shot performance, we combine the zero-shot logits and RAIL-Adapter logits with a weighted sum: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{y}}_{\\mathrm{fs}}=\\left(1-\\beta\\right)\\hat{\\mathbf{y}}_{\\mathrm{ad}}+\\beta\\hat{\\mathbf{y}}_{\\mathrm{zs}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mathbf{y}}_{\\mathrm{ad}}$ denotes the RAIL-Adapter logits and $\\beta$ is the fusion ratio that adjusts the influence of zero-shot prediction on seen domains. The ablation study on $\\beta$ is presented in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate RAIL method under both X-TAIL and MTIL settings, as mentioned in Sec. 3.2. The learning order is set alphabetically: Aircraft, Caltech101, DTD, EuroSAT, Flowers, Food, MNIST, OxfordPet, StanfordCars, and SUN397. Additional experiments with a random order are provided in Tab. 3 in Appendix G. To ensure compatibility with different domains, we follow the common practice of sampling a 16-shot training set for each domain, while using the original test set for evaluation [21, 44, 45, 22, 46]. The implementation details are provided in Appendix D. ", "page_idx": 6}, {"type": "image", "img_path": "boGxvYWZEq/tmp/4a929bbd17d27f577185557df346bcaf8dfec4cc6be89c0054ff538b7770468f.jpg", "img_caption": ["Figure 6: Accuracy $(\\%)$ on five domains changes over all learning steps. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Comparison results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Cross-domain task-agnostic incremental learning. The performances averaged on 10 domains of RAIL and other baseline methods in the X-TAIL setting are presented in the Average column of Tab. 1. Specific performances on each domain are provided in the columns named after each respective domain. We evaluate RAIL in both primal and dual forms. Zero-shot indicates the zeroshot performance of the pre-trained CLIP model on each domain. Fine-tune denotes the performance of fine-tuning both CLIP image and text encoders with a joint dataset of all 10 domains, serving as a strong baseline for comparison. ", "page_idx": 7}, {"type": "text", "text": "Specifically, the primal-RAIL outperforms the previous best one with a $6.4\\%$ improvement in \u201cTransfer\u201d accuracy, achieves an additional $7.7\\%$ in \u201cAverage\u201d accuracy, and gains an $8.6\\%$ improvement in \u201cLast\u201d accuracy. The dual-RAIL further surpasses the primal one by $1.2\\%$ in \u201cAverage\u201d accuracy and $3.3\\%$ in \u201cLast\u201d accuracy, while maintaining consistent \u201cTransfer\u201d accuracy due to the same fusion strategy. These results indicate that RAIL has more stable transfer performance and is more robust to catastrophic forgetting, effectively preserving both knowledge from new domains and pre-trained knowledge. We repeat the experiments with a random order and present the results in Tab. 3. RAIL consistently outperforms the baselines, reaffirming the previous conclusions. ", "page_idx": 7}, {"type": "text", "text": "We illustrate how accuracy changes on several example domains in Fig. 6. We observe that the accuracy of RAIL remains consistent with the zero-shot results before learning the corresponding domain. Furthermore, RAIL exhibits strong cross-domain discriminative capabilities. For example, once DTD is learned, learning further new domains does not affect the accuracy on DTD. Accuracy on certain domains, like Caltech101, even improves due to the fusion module\u2019s ability to reduce OOD errors by learning from more domains. ", "page_idx": 7}, {"type": "text", "text": "Multi-domain task-incremental learning. We follow the setting in [15] to evaluate our methods on the few-shot MTIL, comparing it against the performance of baselines reported in [15]. In this context, RAIL is reduced to the structure of multiple domain-specific classifiers trained on each domain separately. The domain-identity guides the test image to the corresponding classifier for within-domain prediction. The comparison results are shown in Tab. 2. RAIL consistently outperforms previous state-of-the-art approaches on all three metrics. These results demonstrate that the proposed non-linear projections significantly improve the separability of features extracted by CLIP. Consequently, the ridge regression-based classifier can effectively adapt the pre-trained model to new domains. ", "page_idx": 7}, {"type": "text", "text": "5.2 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Regression targets. For VLMs, aside from using one-hot labels as the regression targets $\\mathbf{Y}$ in Eqn. 4, the text embeddings generated from class labels is also a viable option [39]. We compare the \u201cLast\u201d accuracy of 10 domains using the dual RAIL-Adapter with these two different regression targets as shown in Fig. 7a. The results indicate that training with one-hot labels surpasses its counterpart with text embeddings by an average of $3.8\\%$ . We emphasize that using text embeddings as targets is suboptimal compared to uniformly distributed one-hot labels. This effect is particularly notable in domains such as Aircraft, where the \u201cLast\u201d accuracy with one-hot label targets outperforms that with text embedding targets by $7.5\\%$ . We argue that insufficiently semantic class names (e.g., \u201c707-320\u201d) result in text embeddings that are not well-dispersed in the feature space, thus compromising the classification performance. ", "page_idx": 7}, {"type": "table", "img_path": "boGxvYWZEq/tmp/745bad2bcba76bc9e9ad8c901f3fb46466b31627d2596850cf0c912971c1db5c.jpg", "table_caption": ["Table 1: Comparison of different CL methods on X-TAIL for each domain in terms of \"Transfer\", \"Average\", and \"Last\" scores $(\\%)$ . The best results are highlighted with bold style. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["(a) Comparison of different regression targets. ", "Figure 7: Each bar indicates the \u201cLast\u201d accuracy $(\\%)$ on each domain after the last learning step. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["(b) Comparison of different fusion strategies. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Fusion strategies. The CL setting associated with multiple domains is typically decomposed into two stages: domain-identity inference and in-domain prediction [19]. As discussed in Sec. 4.2.2, an intuitive strategy for distinguishing classes from different domains in X-TAIL is to utilize CLIP\u2019s zero-shot prediction as a domain indicator, which then cooperates with multiple domain-specific classifiers to perform classification within each distinct domain. We evaluate the effectiveness of the RAIL-Fusion against this strategy by comparing the \u201cLast\u201d accuracy across 10 domains using the dual RAIL-Adapter. The \u201cTransfer\u201d accuracy remains consistent between these two strategies. ", "page_idx": 8}, {"type": "text", "text": "As shown in Fig. 7b, the RAIL-Fusion strategy outperforms the multi-classifier approach in most domains by an average of $7.5\\%$ . This improvement is due to RAIL-Fusion\u2019s design, which focuses on distinguishing unseen classes (OOD domain) from seen classes (ID domain), rather than identifying specific domains (Sec. 4.2.2). This OOD detection design incrementally reduces errors as the number of ID domains grows, making it particularly effective in dynamically adapting to new domains. By contrast, using a domain indicator maintains a consistent level of cross-domain errors associated for each domain, leading to error propagation in the final prediction. This issue is especially critical for domains with low in-domain accuracy, where any misalignment between the domain indicator and the correct domain can significantly impact performance. These cross-domain errors are mitigated in the RAIL-Adapter thanks to its non-linear projection design. ", "page_idx": 8}, {"type": "table", "img_path": "boGxvYWZEq/tmp/41e44068727e79580661434699f4c8d05cbec35970ef1426759bfc5025524c07.jpg", "table_caption": ["Table 2: Comparison with state-of-the-art methods on 5-shot MTIL setting in terms of \"Transfer\", \"Average\", and \"Last\" scores $(\\%)$ . The best results are highlighted with bold style. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce the Cross-domain Task-Agnostic Incremental Learning (X-TAIL) to evaluate the preservation of pre-trained knowledge and cross-domain discriminative ability in a continual learning context. We introduce a novel CL approach, Regression-based Analytic Incremental Learning (RAIL), to improve the performance of pre-trained vision-language models on progressively incoming domains, while maintaining its zero-shot ability on unseen domains. We theoretically prove the absolute memorization on learned knowledge and show that the fusion module inherently avoids the forgetting of VLM\u2019s zero-shot ability. Comprehensive experiments on both existing and proposed settings empirically demonstrate the superiority of our method. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the National Natural Science Foundation of China (62306117) and the Guangzhou Basic and Applied Basic Research Foundation (2024A04J3681). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in Cognitive Sciences, 24(12):1028\u20131040, 2020.   \n[2] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366\u20133385, 2021.   \n[3] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[4] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation, volume 24, pages 109\u2013165. Elsevier, 1989.   \n[5] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017. [6] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935\u20132947, 2017.   \n[7] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2001\u20132010, 2017. [8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in Neural Information Processing Systems, 33:15920\u201315930, 2020.   \n[9] Zichen Liu, Chao Du, Wee Sun Lee, and Min Lin. Locality sensitive sparse encoding for learning world models online. arXiv preprint arXiv:2401.13034, 2024.   \n[10] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing zero-shot transfer degradation in continual learning of vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 19125\u201319136, October 2023.   \n[11] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34:9694\u20139705, 2021.   \n[12] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.   \n[13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[15] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Dong Wang, Huchuan Lu, and You He. Boosting continual learning of vision-language models via mixture-of-experts adapters. arXiv preprint arXiv:2403.11549, 2024.   \n[16] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018.   \n[17] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67\u201382, 2018.   \n[18] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts learning with pre-trained transformers: An occam\u2019s razor for domain incremental learning. Advances in Neural Information Processing Systems, 35:5682\u20135695, 2022.   \n[19] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991.   \n[21] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581\u2013595, 2024.   \n[22] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, July 2022.   \n[23] Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, and Limin Wang. Awt: Transferring vision-language models via augmentation, weighting, and transportation. arXiv preprint arXiv:2407.04603, 2024.   \n[24] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft, 2013.   \n[25] Xinxing Xu, Joey Tianyi Zhou, IvorW. Tsang, Zheng Qin, Rick Siow Mong Goh, and Yong Liu. Simple and efficient learning using privileged information, 2016.   \n[26] Ahmed Ben Saad, Youssef Tamaazousti, Josselin Kherroubi, and Alexis He. Where is the fake? patch-wise supervised gans for texture inpainting. In 2020 IEEE International Conference on Image Processing (ICIP). IEEE, October 2020.   \n[27] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.   \n[28] Alexander Mamaev. Flowers dataset. https://universe.roboflow.com/ joseph-nelson/flowers, apr 2020. visited on 2024-05-02.   \n[29] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components with random forests. In European Conference on Computer Vision, 2014.   \n[30] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[31] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3498\u20133505. IEEE, 2012.   \n[32] Afshin Dehghan, Syed Zain Masood, Guang Shu, and Enrique. G. Ortiz. View independent vehicle make, model and color recognition using convolutional neural network, 2017.   \n[33] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 3485\u20133492, 2010.   \n[34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[35] Kar-Ann Toh and How-Lung Eng. Between classification-error approximation and weighted least-squares learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(4):658\u2013669, 2008.   \n[36] Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Transactions on Electronic Computers, pages 326\u2013334, 1965.   \n[37] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: theory and applications. Neurocomputing, 70(1-3):489\u2013501, 2006.   \n[38] Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi Xie, Kar-Ann Toh, and Zhiping Lin. Acil: Analytic class-incremental learning with absolute memorization and privacy protection. Advances in Neural Information Processing Systems, 35:11602\u201311614, 2022.   \n[39] Mark D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van den Hengel. Ranpac: Random projections and pre-trained models for continual learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[40] Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression learning algorithm in dual variables. In Proceedings of the Fifteenth International Conference on Machine Learning, page 515\u2013521, San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc.   \n[41] Thomas Hofmann, Bernhard Sch\u00f6lkopf, and Alexander J. Smola. Kernel methods in machine learning. The Annals of Statistics, 36(3):1171 \u2013 1220, 2008.   \n[42] Shunjie Han, Cao Qubo, and Han Meng. Parameter selection in svm with rbf kernel function. In World Automation Congress 2012, pages 1\u20134. IEEE, 2012.   \n[43] James E Gentle. Numerical linear algebra for applications in statistics. Springer Science & Business Media, 2012.   \n[44] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021.   \n[45] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816\u201316825, 2022.   \n[46] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19113\u201319122, 2023.   \n[47] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[48] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Algorithm details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we summarize the training and testing procedures of RAIL in Algorithm 1 and 2, respectively. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 RAIL training   \nRequire: $N$ domains $\\{D^{(1)},...,D^{(N)}\\}$ , pre-trained CLIP model $\\{f_{I},f_{T}\\}$   \nInitialize: Ridge regression parameter $\\lambda$ , random projection function $\\phi\\left(\\cdot\\right)$ , kernel function $\\kappa\\left(\\cdot,\\cdot\\right)$ Dataset 1 learning for training batches in $D^{(1)}$ do Extract features $\\mathbf{X}_{e}^{(1)}=f_{I}\\left(\\mathbf{X}^{(1)}\\right)$ if Primal: then Initialize the memory matrix $\\mathbf{M}_{p}^{(1)}$ using Eqn. 15 Obtain the primal parameter $\\mathbf{W}^{(1)}$ using Eqn. 2 end if if Dual: then Initialize the memory matrix $\\mathbf{M}_{d}^{(1)}$ , label matrix $\\mathbf{C}^{(1)}$ & kernel $\\mathbf{K}^{(1)}$ by Theorem 2 Obtain the dual parameter $\\pmb{\\alpha}^{(1)}$ using Eqn. 7 end if end for Incremental learning for $D^{(n)}$ in $\\{D^{(2)},...,^{\\smile}\\!D^{(N)}\\}$ do for training batches in $D^{(1)}$ do Extract features $\\mathbf{X}_{e}^{\\left(n\\right)}=f_{I}\\left(\\mathbf{X}^{\\left(n\\right)}\\right)$ if Primal: then Update the memory matrix $\\mathbf{M}_{p}^{(n)}$ using Eqn.6 Update the primal parameter $\\mathbf{W}^{(n)}$ using Eqn. 5 end if if Dual: then Update the memory matrix $\\mathbf{M}_{d}^{(n)}$ , label matrix $\\mathbf{C}^{\\left(n\\right)}$ & kernel $\\mathbf{K}^{(n)}$ by Theorem 2 Update the dual parameter $\\alpha^{(n)}$ using Eqn. 7 end if end for end for ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2 RAIL testing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Require: Test dataset $D_{\\mathrm{test}}$ , CLIP model $\\{f_{I},f_{T}\\}$ , trained RAIL-Adapter $R_{a d}\\left(\\cdot\\right)$ , cross-domain label set $C_{N}$ for $\\mathbf{x}_{\\mathrm{test}}\\in D_{\\mathrm{test}}$ do Obtain the rough prediction $y_{\\mathrm{zs}}^{*}=\\operatorname{argmax}\\hat{\\mathbf{y}}_{z s}$ by Eqn. 9 if $y_{\\mathrm{zs}}^{*}\\in C_{L}$ then Obtain RAIL-adapter logits $\\hat{\\mathbf{y}}_{a d}=R_{a d}\\left(f_{I}\\left(\\mathbf{x}_{\\mathrm{test}}\\right)\\right)$ Obtain predicted class based on fusion logits $y^{\\ast}=\\operatorname{argmax}\\hat{\\mathbf{y}}_{f s}$ by Eqn. 10 else Regard $y_{\\mathrm{zs}}^{*}$ as the final prediction end if end for ", "page_idx": 13}, {"type": "text", "text": "B Connection between primal & dual ridge regression ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we introduce the connection between primal and dual forms of ridge regression. Based on the identity $(\\mathbf{P}^{-1}\\!+\\!\\mathbf{B}^{\\top}\\mathbf{R}^{-1}\\mathbf{B})^{-1}\\mathbf{B}^{\\top}\\mathbf{R}^{-1}=\\mathbf{P}\\mathbf{B}^{\\top}(\\mathbf{B}\\mathbf{P}\\mathbf{B}^{\\top}+\\mathbf{R})^{-1}$ , the solution of ridge regression is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\bf W}=({\\Phi}^{\\top}{\\Phi}+\\lambda{\\bf I}_{d})^{-1}{\\Phi}^{\\top}{\\bf Y}={\\Phi}^{\\top}({\\Phi}{\\Phi}^{\\top}+\\lambda{\\bf I}_{n})^{-1}{\\bf Y},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the former solution is based on the outer-product of data and the latter one is based on the innerproduct of data. Parameter W can thus be rewritten as: $\\mathbf{W}=\\Phi^{\\top}\\alpha$ with $\\alpha=(\\Phi\\Phi^{\\top}+\\lambda\\mathbf I_{n})^{-1}\\mathbf Y$ . In this way, the solution $\\mathbf{W}$ is interpreted to lie in the span of the sample-cases, even if the dimensionality of the projected features $\\Phi$ is larger than the number of samples. ", "page_idx": 14}, {"type": "text", "text": "Utilizing the kernel method, we never actually require access to the explicit features $\\Phi$ , which could be of indefinite dimensions. We obtain the prediction with given data $\\mathbf{x}$ by projecting it onto the solution $\\mathbf{W}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\hat{\\mathbf{y}}}=\\phi\\left(\\mathbf{x}\\right)\\mathbf{W}=\\phi\\left(\\mathbf{x}\\right){\\boldsymbol{\\Phi}}^{\\top}({\\boldsymbol{\\Phi}}{\\boldsymbol{\\Phi}}^{\\top}+\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{Y}={\\boldsymbol{\\mathcal{K}}}\\left(\\mathbf{x},\\mathbf{X}\\right){\\boldsymbol{\\alpha}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $K\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)=\\phi\\left(\\mathbf{x}_{i}\\right)^{\\top}\\phi\\left(\\mathbf{x}_{j}\\right)$ . What we require here is the choice of kernel function $\\kappa\\left(\\cdot,\\cdot\\right)$ instead of explicitly defining the projection function. ", "page_idx": 14}, {"type": "text", "text": "C Proof of Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide two mathematical proofs for both Theorem 1 and Theorem 2. ", "page_idx": 14}, {"type": "text", "text": "First, we prove the Theorem 1 from the solution of joint training on $n$ datasets with primal ridge regression: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(n)}=\\left(\\Phi^{(1:n)\\top}\\Phi^{(1:n)}+\\lambda\\mathbf{I}\\right)^{-1}\\Phi^{(1:n)\\top}\\mathbf{Y}^{(1:n)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By decoupling the $n$ -th data from previous datasets, the $\\mathbf{W}^{(n)}$ can be written as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\cal N}^{(n)}=\\left(\\left[\\Phi^{(1:n-1)\\top}\\quad\\Phi^{(n)\\top}\\right]\\left[\\binom{\\Phi^{(1:n-1)}}{\\Phi^{(n)}}\\right]+\\lambda{\\bf I}\\right)^{-1}\\left[\\Phi^{(1:n-1)\\top}\\quad\\Phi^{(n)\\top}\\right]\\left[\\begin{array}{c c}{{{\\bf N}^{(1:n-1)}}}&{{{\\bf0}}}\\\\ {{{\\bf0}}}&{{{\\bf Y}^{(n)}}}\\end{array}\\right]}}\\\\ {{=\\left(\\Phi^{(1:n-1)\\top}\\Phi^{(1:n-1)}+\\lambda{\\bf I}+\\Phi^{(n)\\top}\\Phi^{(n)}\\right)^{-1}\\left[\\Phi^{(1:n-1)\\top}{\\bf Y}^{(1:n-1)}}}&{{\\Phi^{(n)\\top}{\\bf Y}^{(n)}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We introduce the memory matrix as in the following definition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{M}_{p}^{(n)}=\\left(\\Phi^{(1:n)\\top}\\Phi^{(1:n)}+\\lambda\\mathbf{I}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is the matrix inversion term of Eqn 13. ", "page_idx": 14}, {"type": "text", "text": "Noticing that $\\mathbf{M}_{p}^{(n-1)}\\ =\\ \\left(\\Phi^{(1:n-1)\\top}\\Phi^{(1:n-1)}+\\lambda\\mathbf{I}\\right)^{-1}$ , by Woodbury matrix identity where $\\mathbf{\\left(A+UCV\\right)}^{-1}=\\mathbf{A}^{-1}-\\mathbf{A}^{-1}\\mathbf{U}\\left(\\mathbf{C}^{-1}+\\mathbf{VA}^{-1}\\mathbf{U}\\right)^{-1}\\mathbf{V}\\mathbf{A}^{-1}$ and treating $\\mathbf{M}_{p}^{(n-1)}$ as ${\\bf A}^{-1}$ , the memory at $n$ -th step can be further defined as a recursive solution: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{M}_{p}^{(n)}=\\mathbf{M}_{p}^{(n-1)}-\\mathbf{M}_{p}^{(n-1)}\\Phi^{(n)\\top}\\left(\\mathbf{I}+\\Phi^{(n)}\\mathbf{M}_{p}^{(n-1)}\\Phi^{(n)\\top}\\right)^{-1}\\Phi^{(n)}\\mathbf{M}_{p}^{(n-1)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the parameter $\\mathbf{W}^{(n)}$ is derived as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(n)}=\\left[\\mathbf{M}_{p}^{(n)}\\Phi^{(1:n-1)\\top}\\mathbf{Y}^{(1:n-1)}\\quad\\mathbf{M}_{p}^{(n)}\\Phi^{(n)\\top}\\mathbf{Y}^{(n)}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Denotes the left submatrix $\\mathbf{M}_{p}^{(n)}\\Phi^{(1:n-1)\\top}\\mathbf{Y}^{(1:n-1)}$ as $\\mathbf{H}$ . By substituting Eqn 16 into 17, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{W}^{(n-1)}-\\mathbf{M}_{p}^{(n-1)}\\Phi^{(n)\\top}\\left(\\mathbf{I}+\\Phi^{(n)}\\mathbf{M}_{p}^{(n-1)}\\Phi^{(n)\\top}\\right)^{-1}\\Phi^{(n)}\\mathbf{W}^{(n-1)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Based on the identity of $\\left(\\mathbf{I}+\\mathbf{P}\\right)^{-1}=\\mathbf{I}-\\left(\\mathbf{I}+\\mathbf{P}\\right)^{-1}\\mathbf{P}$ , it is further derived as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{W}^{(n-1)}-\\mathbf{M}_{p}^{(n)}\\Phi^{(n)\\top}\\Phi^{(n)}\\mathbf{W}^{(n-1)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(n)}=\\left[\\mathbf{W}^{(n-1)}-\\mathbf{M}_{p}^{(n)}\\Phi^{(n)\\top}\\Phi^{(n)}\\mathbf{W}^{(n-1)}\\quad\\mathbf{M}_{p}^{(n)}\\Phi^{(n)\\top}\\mathbf{Y}^{(n)}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The Theorem 1 is proved. ", "page_idx": 15}, {"type": "text", "text": "Next, we prove the Theorem 2 as follows. We use the few-shot features $\\mathbf{X}_{e}$ as the class-prototypes as default. The solution of joint training on $n$ datasets with dual form ridge regression is shown as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{\\alpha}^{(n)}=\\left(\\mathcal{K}\\left(\\mathbf{X}_{e}^{(1:n)},\\mathbf{X}_{e}^{(1:n)}\\right)+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{Y}^{(1:n)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define the memory matrix $\\mathbf{M}_{d}$ as the concatenation of class-prototypes from learned domains, the memory matrix at $n$ -th step is obtained by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{M}_{d}^{\\left(n\\right)}=\\left[\\mathbf{M}_{d}^{\\left(n-1\\right)}\\quad\\mathbf{X}_{e}^{\\left(n\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The kernel matrix at $n$ -th step for $\\alpha^{(n)}$ can be partitioned as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{K}^{\\left(n\\right)}=K\\left(\\mathbf{X}_{e}^{\\left(1:n\\right)},\\mathbf{X}_{e}^{\\left(1:n\\right)}\\right)}\\\\ &{\\phantom{\\left(\\begin{array}{l}{\\mathbf{K}^{\\left(n\\right)}=\\left(\\begin{array}{l l}{\\mathbf{K}\\left(\\mathbf{X}_{e}^{\\left(1:n-1\\right)},\\mathbf{X}_{e}^{\\left(1:n-1\\right)}\\right)}&{K\\left(\\mathbf{X}_{e}^{\\left(n\\right)},\\mathbf{M}_{d}^{\\left(n-1\\right)}\\right)^{\\top}}\\\\ {K\\left(\\mathbf{X}_{e}^{\\left(n\\right)},\\mathbf{M}_{d}^{\\left(n-1\\right)}\\right)}&{K\\left(\\mathbf{X}_{e}^{\\left(n\\right)},\\mathbf{X}_{e}^{\\left(n\\right)}\\right)}\\end{array}\\right)^{\\top}}}\\\\ &{\\phantom{\\left(\\begin{array}{l}{\\mathbf{K}^{\\left(n\\right)}=\\left(\\begin{array}{l l}{\\mathbf{K}^{\\left(n-1\\right)}}&{K\\left(\\mathbf{X}_{e}^{\\left(n\\right)},\\mathbf{M}_{d}^{\\left(n-1\\right)}\\right)^{\\top}}\\\\ {K\\left(\\mathbf{X}_{e}^{\\left(n\\right)},\\mathbf{M}_{d}^{\\left(n-1\\right)}\\right)}&{K\\left(\\mathbf{X}_{e}^{\\left(n\\right)},\\mathbf{X}_{e}^{\\left(n\\right)}\\right)^{\\top}}\\end{array}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It indicates that the kernel matrix updates recursively along the diagonal by kernel matrices of the intra-domain covariance within ${\\bf X}_{e}^{(n)}$ and the inter-domain covariance between ${\\bf X}_{e}^{(n)}$ and the memory $\\mathbf{M}_{d}^{(n-1)}$ . The kernel matrix $\\mathbf{K}$ therefore memorizes all the covariance information of learned domains. We further denote $\\mathbf{Y}^{(1:n)}$ as $\\mathbf{C}^{\\left(n\\right)}$ , which is updated by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{C}^{(n)}=\\left[\\mathbf{C}_{\\mathbf{\\Lambda}\\mathbf{0}}^{(n-1)}\\mathbf{\\Lambda}\\mathbf{\\Lambda}\\mathbf{\\Lambda}\\mathbf{0}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the matrix $\\mathbf{Y}^{(n)}$ consists of one-hot labels that are disjoint with those in previous $n-1$ domains. In this way, the parameter $\\alpha^{(n)}$ is solved by updating $\\mathbf{K}^{(n)}$ and $\\mathbf{C}^{\\left(n\\right)}$ , resulting in an identical solution to the one of joint learning in Eqn. 21. Thus, the Theorem 2 is proved. ", "page_idx": 15}, {"type": "text", "text": "D Implementation details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we introduce the implementation details, including model configuration, hardware setup, and hyperparamter selection. We use the pre-trained CLIP [13] model of the ViT-B/16 image encoder [48]. All the results are conducted on Ubuntu 20.04 with Intel Core i9-13900K CPU with a single RTX 4090Ti GPU by the average of 3 runs. We conduct a grid search for the regularization parameter $\\lambda$ over the range $10^{-6}$ , $10^{-5},...,1$ and the RBF kernel bandwidth over the range $10^{-6}$ , $10^{5-5},...,10$ . The optimal values are determined by minimizing the regression error on the validation set of the first domain, without access to future domains. These parameters are then fixed for all subsequent learning steps. We use the simplest prompt template \u201cA photo of a {}.\u201d for generalization across different domains. ", "page_idx": 15}, {"type": "image", "img_path": "boGxvYWZEq/tmp/4a08db11c7e9d066c4b62bd1655bb364d53be0ff05368d6662d4d0aa93f9f4ad.jpg", "img_caption": ["Figure 8: RHL dimension vs. \u201cLast\u201d accuracy $(\\%)$ averaged on 10 domains. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "boGxvYWZEq/tmp/389c3a7d6adf29dfcefee52dec43e7456fc3f306cc9683edf97ca43f706df3af.jpg", "img_caption": ["Figure 9: Fusion ratio vs. \u201cAverage\u201d and \u201cLast\u201d accuracy $(\\%)$ averaged on 10 domains. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Ablation studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we conduct two ablation studies to observe the average performance on 10 domains w.r.t. the hidden dimension of RHL and the fusion ratio, respectively. ", "page_idx": 16}, {"type": "text", "text": "E.1 RHL dimension ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first ablate the hidden dimension of RHL in primal RAIL over the \u201cLast\u201d accuracy in Fig. 8. It is evident that an increase in the RHL dimension correlates with improved adapter\u2019s accuracy. Specifically, increasing the dimension from 1k to 10k leads to notable improvements (from $73.6\\%$ to $79.0\\%$ ). However, beyond the 10k threshold, the gain in accuracy becomes saturated. The dimensions of $15\\mathbf{k}$ and $20\\mathbf{k}$ result the same performance of $79.1\\%$ . Considering the computational cost associated with higher dimensions, we set the RHL dimension to $15\\mathrm{k}$ as default in the experiments. ", "page_idx": 16}, {"type": "text", "text": "E.2 Fusion ratio ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Additionally, we conduct an ablation study of fusion ratio $\\beta$ in terms of \u201cAverage\u201d and \u201cLast\u201d accuracy. The \u201cTransfer\u201d accuracy is not considered here since $\\beta$ does not influence it. From Fig. 9, we observe that the best ratio is 0.8 for both \u201cAverage\u201d and \u201cLast\u201d scores. Note that when $\\beta$ equals to 1, the performance on seen domains fully relies on the RAIL-Adapter. The \u201cLast\u201d accuracy with $\\beta=0.8$ surpasses the one of pure RAIL-Adapter performance ( $\\left.\\rho=1\\right.$ ) by $1.9\\%$ and $1.3\\%$ in the primal and dual RAIL, respectively. This result verifies our claim that the cooperation with CLIP\u2019s generalization ability can preserve the performance on its confident domains (already having good zero-shot performance) and avoid overfitting on limited domain exemplars. ", "page_idx": 16}, {"type": "image", "img_path": "boGxvYWZEq/tmp/c2c982c525aec8871f56ae24ad266a1d0630248400445833c688f7b2ccdb7f84.jpg", "img_caption": ["Figure 10: Accuracy $(\\%)$ of Primal-RAIL and Dual-RAIL in the X-TAIL setting with order-I. Each row denotes the performance on every domain after learning on one domain. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "F Detailed performance of RAIL with order-I ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we visualize the performance on each domain after every learning step in Fig. 10. The upper-diagonal represents the performance before learning on the corresponding domain, which remains consistent with the zero-shot performance thanks to RAIL\u2019s absolute memorization of the zero-shot ability on unseen domains. On the other hand, the performance after learning a specific domain (lower-diagonal) does not degrades. Performance on some domains even improves with the learning step (e.g., Caltech-101), benefiting from our RAIL fusion module design, which reduces OOD errors as more domains are learned, thereby enhancing overall accuracy. ", "page_idx": 17}, {"type": "text", "text": "G Comparison of different methods on X-TAIL with order II. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we compare different methods in X-TAIL setting with a random order: StanfordCars, Aircraft, OxfordPet, Food, SUN397, MNIST, Flowers, DTD, Caltech101, EuroSAT. As shown in Tab. 3, our method again outperforms previous methods on all metrics, reinforcing the conclusions presented in Sec. 5.1. ", "page_idx": 17}, {"type": "text", "text": "H Limitation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "RAIL exhibits its superior performance and efficiency for transferring pre-trained VLMs to various domains. A limitation in this approach is that the pre-trained VLM remains fixed, preventing any improvement in its feature extraction capability throughout the incremental learning process. A promising direction for future work is to adjust the pre-trained encoder according to new data with low computational cost, thus further boosting RAIL\u2019s performance while maintain its efficiency. Beside, extending RAIL to encompass additional downstream tasks of VLMs, such as image segmentation, can broaden its applicability and enhance its utility in more complex visual understanding scenarios. ", "page_idx": 17}, {"type": "table", "img_path": "boGxvYWZEq/tmp/31b907cbd217c3681861af22bd7487688aa7e1905e73452dce591378084e9bb9.jpg", "table_caption": ["Table 3: Comparison of different CL methods on X-TAIL for each domain with order II in terms of \"Transfer\", \"Average\", and \"Last\" scores $(\\%)$ . The best results are highlighted with bold style. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We summarize our contributions in both the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitations in Sec. H. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide a complete proof of our theorem in Sec. C. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Our method is mainly based on a pre-trained CLIP and close-form solutions.   \nAll required equations have been provided in Sec. 4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We release the code at https://github.com/linghan1997/ Regression-based-Analytic-Incremental-Learning. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We describe our implementation details in Sec. D. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Our method is based on analytic learning, whose closed-form solutions are not influenced by random seeds. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We describe the computer resources in Sec. D. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We preserve the anonymity with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not bring any societal impacts. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper is based on open-source datasets and pre-trained models. There is no potential risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code package or dataset. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]