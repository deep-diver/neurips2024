[{"figure_path": "5AeLrXb9sQ/figures/figures_2_1.jpg", "caption": "Figure 1: Brief illustration of temporal modeling paradigms.", "description": "This figure illustrates three different temporal modeling paradigms: (a) Causal temporal relation modeling, which includes Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs); (b) Parallelized sequence representation modeling, which includes Convolutional Neural Networks (CNNs) and Transformers; and (c) the proposed Target-History Relation Modeling (TRAM) paradigm.  HMMs model temporal dependencies probabilistically using hidden states. RNNs use learnable vectorized dependencies between hidden states. Convolution uses local contextual learning, while self-attention models dense pair-wise relations.  The TRAM paradigm focuses on data-driven aggregation of temporal information with learned target-history relations, addressing limitations of previous methods.", "section": "Discussion of Temporal Modeling Paradigms for RSS"}, {"figure_path": "5AeLrXb9sQ/figures/figures_4_1.jpg", "caption": "Figure 2: The illustration of multi-view TARSS-Net. The segmentation results for the radar data in RD and RA views, as well as referenced detection results in the camera image are presented for intuitive illustration.", "description": "This figure shows the architecture of the proposed multi-view TARSS-Net.  It illustrates the flow of data through the network, starting with multi-view radar input sequences (RD, AD, RA). Each view is processed by an encoder, a temporal relation-aware module (TRAM), and a temporal compression (TC) module.  The resulting single-view temporal-compressed embeddings are then fused in a latent space encoder (LSE) before being passed to the decoders that generate segmentation results for the RD and RA views. The figure also includes example segmentation outputs for the radar data alongside corresponding camera images for visual comparison and intuitive understanding.", "section": "3 Temporal Relation Attentive Model (TRAM)"}, {"figure_path": "5AeLrXb9sQ/figures/figures_4_2.jpg", "caption": "Figure 3: The illustration of TH-TRE.", "description": "This figure shows the architecture of the Target-History Temporal Relation Encoding (TH-TRE) block, a key component of the Temporal Relation-Aware Module (TRAM).  The TH-TRE block takes as input a sequence of feature maps representing the current frame and its adjacent historical frames.  It uses a Temporal Relation Inception Convolution (TRIC) block to capture the relationships between these frames.  The TRIC block employs two convolutional layers (Conv_1 and Conv_2), which are shared across time to ensure efficiency.  The output of the TH-TRE block is a set of relation embeddings that capture the temporal relationships between the current frame and its history.", "section": "3 Temporal Relation Attentive Model (TRAM)"}, {"figure_path": "5AeLrXb9sQ/figures/figures_8_1.jpg", "caption": "Figure 1: Brief illustration of temporal modeling paradigms.", "description": "This figure provides a visual comparison of different temporal modeling paradigms used in time series analysis. It showcases three main approaches: (a1) Hidden Markov Models (HMMs), which utilize hidden states and probabilistic transitions to model temporal dependencies; (a2) Recurrent Neural Networks (RNNs), which use a continuous vector representation of hidden states and learnable connections to model dependencies; (b1) Convolutional methods, which focus on local contextual learning; (b2) Self-attention mechanisms, which create dense pair-wise relations between sequence elements; and (c) the proposed Target-History Relation-Aware Module (TRAM), which captures target-history temporal relations and utilizes data-driven temporal information aggregation with learned relations.  The figure visually compares these methods using graphs to represent the flow of information and dependencies.", "section": "Discussion of Temporal Modeling Paradigms for RSS"}, {"figure_path": "5AeLrXb9sQ/figures/figures_14_1.jpg", "caption": "Figure S1: The basic learning paradigm (left) and implementation of proposed TRAM (right). TRE in the yellow boxes denote TH-TRE; Temporal Relation Importance Measurement relates to the attention weights calculation in TRAP block, and Relation Aggregation relates to weighted summation and skip connection in TRAP block.", "description": "This figure illustrates the core idea and implementation details of the Temporal Relation Aware Module (TRAM). The left part shows the basic learning paradigm, which involves capturing target-history relations and aggregating the whole sequence using weighted relations to enhance the target frame representation and prediction.  The right side details the TRAM implementation, highlighting the Target-History Temporal Relation Encoder (TH-TRE) blocks, the Temporal Relation Importance Measurement (attention mechanism), and the Relation Aggregation (weighted summation and skip connection).", "section": "C Additional Descriptions of TARSS-Net"}, {"figure_path": "5AeLrXb9sQ/figures/figures_15_1.jpg", "caption": "Figure 2: The illustration of multi-view TARSS-Net. The segmentation results for the radar data in RD and RA views, as well as referenced detection results in the camera image are presented for intuitive illustration.", "description": "This figure illustrates the architecture of the multi-view TARSS-Net, a deep learning model for radar semantic segmentation.  It shows how the model processes radar data from multiple views (Range-Doppler (RD), Angle-Doppler (AD), and Range-Angle (RA)) to generate segmentation maps. The figure highlights the key components of the model:  basic encoders for each view, the temporal relation-aware module (TRAM) for incorporating temporal information, the latent space encoder (LSE) for fusing information across views, and decoders for generating the final segmentation predictions in RD and RA views.  The inclusion of camera images helps to contextualize and visually validate the radar-based segmentation results.", "section": "3 Temporal Relation Attentive Model (TRAM)"}, {"figure_path": "5AeLrXb9sQ/figures/figures_16_1.jpg", "caption": "Figure S3: The overall framework of TARSS-Net: details of the encoding branch for RD-view is given as an example.", "description": "This figure presents a detailed illustration of the TARSS-Net architecture, focusing on the RD-view encoding branch as an example.  It shows the flow of data through the different components:  Three separate encoding branches (RD, AD, and RA) process input radar sequences.  Each branch utilizes an encoder, a Temporal Relation Aware Module (TRAM), and an atrous spatial pyramid pooling (ASPP) module. The outputs of these branches are then combined using a Latent Space Encoder (LSE). Finally, the combined features are passed to RD and RA decoders to generate the output segmentation predictions.", "section": "C Additional Descriptions of TARSS-Net"}, {"figure_path": "5AeLrXb9sQ/figures/figures_17_1.jpg", "caption": "Figure S4: A more parallel implementation of TRIC*. ", "description": "This figure shows an improved implementation of the TRIC (temporal-relation-inception convolution) block, a key component of the TH-TRE (Target-History Temporal Relation Encoding) module within the TARSS-Net architecture.  The original TRIC implementation used a sequential approach, processing target-history feature pairs one at a time. This modified version leverages temporal cross-reorganization of target-history feature pairs and employs a 3D convolution with 2x3x3 kernels and a temporal stride of 2. This parallel design speeds up processing and improves efficiency compared to the original implementation while maintaining the same parameter scale.", "section": "C.2 More in-depth discussion of TH-TRE block"}, {"figure_path": "5AeLrXb9sQ/figures/figures_19_1.jpg", "caption": "Figure 5: Comparison of class-wise performances.", "description": "This figure compares the class-wise performances (IoU and Dice scores) of different methods including TARSS-Net_D, TARSS-Net_S, TMVA-Net, RAMP-CNN, and RSS-Net for both RD and RA views. The results show the performance differences between the models for different object classes (global, background, vehicle, cyclist, pedestrian).", "section": "4.2 Comparisons with State-of-The-Art Methods"}, {"figure_path": "5AeLrXb9sQ/figures/figures_19_2.jpg", "caption": "Figure S6: Feature Visualization. (a) Input RD-view frame. (b) The activation response heatmaps of TRAM outputs. (c) TARSS-Net outputs before Softmax. (d) Ground Truth Mask.", "description": "This figure visualizes the feature maps at different stages of the TARSS-Net model for a single RD-view frame. (a) shows the input RD-view frame. (b) shows the activation response heatmaps of the TRAM outputs, highlighting which parts of the input the model is focusing on. (c) shows the TARSS-Net outputs before the softmax activation, representing the model's raw predictions. (d) shows the ground truth mask for comparison.", "section": "E.3 Features visualization"}, {"figure_path": "5AeLrXb9sQ/figures/figures_20_1.jpg", "caption": "Figure S7: Visualization of some examples.", "description": "This figure visualizes some examples of the semantic segmentation results obtained by the proposed TARSS-Net. For each scenario (pedestrian & vehicle, pedestrian only, and bicycle), the figure shows (a) synchronized camera images, (b) annotated radar RD representation, and (c) RD predictions by TARSS-Net.  The visualization helps to illustrate the model's performance on different objects and weather conditions.", "section": "E.4 Visualization of some examples"}]