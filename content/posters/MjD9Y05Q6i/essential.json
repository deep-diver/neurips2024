{"importance": "This paper is crucial because it significantly advances the interpretability of black-box models.  It introduces a novel method, **LG-CAV**, to train concept activation vectors (CAVs) without requiring large labeled datasets. This addresses a major limitation of existing CAV methods, opening doors for broader application in explainable AI and related fields.  The proposed model correction technique further enhances the practical value of LG-CAV, offering a potential paradigm shift in how we understand and improve model performance.", "summary": "LG-CAV: Train any Concept Activation Vector with Language Guidance, leverages vision-language models to train CAVs without labeled data, achieving superior accuracy and enabling state-of-the-art model correction.", "takeaways": ["LG-CAV enables training of concept activation vectors (CAVs) without labeled images, using language guidance from vision-language models.", "LG-CAV significantly outperforms existing methods in CAV quality (concept and concept-to-class accuracy).", "A novel activation sample reweighting method improves target model performance using LG-CAVs."], "tldr": "Training Concept Activation Vectors (CAVs) usually needs many high-quality images, limiting their use.  Existing methods struggle with this data scarcity problem, hindering their ability to provide insightful interpretations of model predictions.  This makes it difficult to analyze and improve complex models effectively.\nThe proposed Language-Guided CAV (LG-CAV) method uses vision-language models to overcome this challenge. **LG-CAV trains CAVs guided by concept descriptions**, eliminating the need for extensive labeled data.  It incorporates additional modules for improved accuracy.  Furthermore, **LG-CAV enables a novel model correction technique, boosting performance**. Experiments show significant improvement over existing methods, offering a superior approach to understanding and improving models.", "affiliation": "Zhejiang University", "categories": {"main_category": "Computer Vision", "sub_category": "Vision-Language Models"}, "podcast_path": "MjD9Y05Q6i/podcast.wav"}