[{"figure_path": "qf1ncViBr5/tables/tables_8_1.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the accuracy results achieved by employing various search strategies (random sampling, random search, and regularized evolution) within the proposed einspace search space.  It compares the performance of einspace against several established NAS methods (PC-DARTS, DrNAS, Bonsai-Net) on eight diverse tasks from the Unseen NAS benchmark. The table highlights the impact of the expressive nature of einspace, demonstrating competitive results even with relatively simple search strategies.  It also shows that einspace outperforms existing CFG-based spaces such as hNASBench201 when evolutionary search is used.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_15_1.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the accuracy results obtained by applying three different search strategies (random sampling, random search, and regularized evolution) within the einspace search space on eight diverse datasets from the Unseen NAS benchmark.  It compares einspace's performance to several baselines from the literature (PC-DARTS, DrNAS, Bonsai-Net, and a ResNet18) using the same datasets and metrics.  The table highlights the competitive performance achieved by einspace, particularly with regularized evolution, even when using simple search algorithms, demonstrating its effectiveness in discovering high-performing architectures. It also shows superior performance compared to the hNASBench201 search space when using evolutionary search.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_16_1.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the results of three different search strategies (random sampling, random search, and regularized evolution) applied to the einspace search space on several datasets from the Unseen NAS benchmark.  It compares the performance of einspace against several baselines from the literature, highlighting its competitive performance, especially when using the regularized evolution strategy.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_19_1.jpg", "caption": "Table 4: Comparison of einspace with existing search spaces. RS: random search, RE: regularised evolution, RL: reinforcement learning, BO: Bayesian optimisation and GB: gradient-based. \u2021 The size of einspace is infinite, but we can bound the number of possible architectures strings of a certain length, as discussed in Section A.6. \u2020 Gradient-based search is difficult in these spaces due to their size, but other weight-sharing methods may be available. * The paper introducing hNASBench201 [46] also considers versions of the search space for Transformer language models.", "description": "This table compares the properties of the proposed einspace search space with four other existing search spaces in the literature. The comparison is made based on the type of grammar used (PCFG, CFG, Cell), size of the space (finite or infinite), the focus of the space (ConvNets, Transformers, MLP, etc.), and the available search strategies (RS, RE, RL, BO, GB). It also includes notes regarding specific characteristics of the spaces. ", "section": "A.7 Comparison to Other Search Spaces"}, {"figure_path": "qf1ncViBr5/tables/tables_20_1.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the results of using three different search strategies (random sampling, random search, and regularized evolution) within the proposed einspace search space, compared to various baselines from the literature.  The results are shown across eight diverse datasets from the Unseen NAS benchmark [16], covering various modalities (vision, language, audio, chess). The table highlights the performance gains achieved by using einspace, especially when employing regularized evolution, and demonstrates its competitiveness against more complex search strategies and other state-of-the-art search spaces like hNASBench201.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_23_1.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the results of three different search strategies (random sampling, random search, and regularized evolution) applied to the einspace search space on eight diverse datasets from the Unseen NAS benchmark.  It compares the performance of these strategies against several baselines (ResNet18, PC-DARTS, DrNAS, Bonsai-Net) from previous work.  The table shows that even simple search strategies in the expressive einspace can achieve competitive results, and that the performance of the regularized evolution approach surpasses other baseline methods in multiple datasets. The table also provides a comparison to the results of using another CFG-based search space (hNASBench201) demonstrating the superior performance of the proposed method. ", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_23_2.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the results of three different search strategies (random sampling, random search, and regularized evolution) applied to the einspace search space on eight diverse datasets from the Unseen NAS benchmark [16].  It compares the performance of these simple search methods on einspace to several baselines from previous work, including results from DARTS [30] and Bonsai [15] search spaces.  The table highlights that einspace's expressiveness allows it to achieve competitive results even with simple search strategies, and in some cases outperforms a previously reported CFG-based search space (hNASBench201 [46]).  The table also indicates the best and second-best performing architecture for each dataset.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_24_1.jpg", "caption": "Table 8: Lower is better. Performance on CIFAR10 as measured by the 0-1 error.", "description": "This table presents the 0-1 error results for CIFAR-10 using three different approaches: a baseline ResNet18 model (RN18), regularized evolution initialized with ResNet18 (RE(RN18)), and regularized evolution initialized with a mix of architectures (RE(Mix)).  The lower the 0-1 error, the better the performance.  The table highlights the performance improvement achieved by the evolutionary search methods, especially RE(RN18), over the baseline ResNet18 model.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_24_2.jpg", "caption": "Table 9: Runtime of search algorithms, as well as the number of model parameters for the found architectures. Numbers for PC-DARTS on two datasets are missing due to missing logs from the authors of [16]. RE on einspace is the RE(Mix) variant, while RE on hNASBench201 searches from scratch.", "description": "This table compares the runtime and number of parameters of the architectures found by different neural architecture search (NAS) methods on eight datasets from the Unseen NAS benchmark [16].  The methods compared are DrNAS, PC-DARTS, regularized evolution (RE) on hNASBench201, and RE on einspace (using a mix of existing architectures as initialization). The table highlights the trade-off between search efficiency (runtime) and model complexity (number of parameters) across different NAS approaches and datasets.  Note that some PC-DARTS runtimes are missing because the authors of [16] did not make the logs available.", "section": "4 Experiments"}, {"figure_path": "qf1ncViBr5/tables/tables_25_1.jpg", "caption": "Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS [16]. Results transcribed from [16] are denoted *, where DARTS [30] and Bonsai [15] search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201 [46] when using evolutionary search in both spaces. Best and second best performance per dataset.", "description": "This table presents the results of three different search strategies (random sampling, random search, and regularized evolution) applied to the einspace search space on various datasets from the Unseen NAS benchmark.  It compares the performance of einspace to several baselines, highlighting the impact of the search strategy and the expressiveness of the search space.  The table also includes comparative results from other neural architecture search spaces (DARTS and Bonsai) for context.", "section": "4 Experiments"}]