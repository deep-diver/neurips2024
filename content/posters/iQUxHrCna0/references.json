{"references": [{"fullname_first_author": "Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduced the Vision Transformer (ViT) architecture, a foundational model used as the backbone for the proposed UDON method."}, {"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-06-10", "reason": "CLIP, introduced in this paper, is a large visual foundational model that was used as a pre-training model and baseline for comparison."}, {"fullname_first_author": "Hinton", "paper_title": "Distilling the Knowledge in a Neural Network", "publication_date": "2015-12-01", "reason": "This paper introduced the concept of knowledge distillation, a core technique used in the proposed UDON method to improve the universal embedding."}, {"fullname_first_author": "Ypsilantis", "paper_title": "Towards universal image embeddings: A large-scale dataset and challenge for generic image representations", "publication_date": "2023-10-26", "reason": "This paper introduced the UnED dataset, which is the dataset used to evaluate the proposed method."}, {"fullname_first_author": "Guo", "paper_title": "Online knowledge distillation via collaborative learning", "publication_date": "2020-06-01", "reason": "This paper introduced online knowledge distillation, a technique that was adapted for the proposed UDON method to train the universal embedding efficiently."}]}