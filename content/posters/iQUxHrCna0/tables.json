[{"figure_path": "iQUxHrCna0/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison of the universal embedding for the proposed UDON method against the previous state-of-the-art and the proposed baselines, on the test set of UnED dataset. Off-the-shelf models are shown for reference, as they employ much higher dimensional descriptors (768-D vs. 64-D) than the rest of the methods. For each type of pre-training, the best method is highlighted in bold. The Specialist+Oracle model constitutes a non-realistic method that is presented in order to get an estimate of the maximum performance that can be achieved in each domain. All of the methods use the ViT-Base/16 backbone.", "description": "This table presents a comparison of different universal embedding methods on the UnED dataset.  It shows the performance (P@5 and R@1) of various methods, including off-the-shelf models and the proposed UDON method, across eight image domains. The table also highlights the best performing method for each pre-training type (ImageNet21k and CLIP) and includes a non-realistic \"Specialist+Oracle\" model to show the upper bound of performance.  All methods use the same ViT-Base/16 backbone architecture.", "section": "4 Experiments"}, {"figure_path": "iQUxHrCna0/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison of the universal embedding for the proposed UDON method against the previous state-of-the-art and the proposed baselines, on the test set of UnED dataset. Off-the-shelf models are shown for reference, as they employ much higher dimensional descriptors (768-D vs. 64-D) than the rest of the methods. For each type of pre-training, the best method is highlighted in bold. The Specialist+Oracle model constitutes a non-realistic method that is presented in order to get an estimate of the maximum performance that can be achieved in each domain. All of the methods use the ViT-Base/16 backbone.", "description": "This table compares the performance of the proposed UDON model against several baseline methods and off-the-shelf models on the UnED dataset.  It shows Recall@1 and Mean Precision@5 for each of eight domains, as well as the mean across all domains. The table highlights the best performing method for each pre-training type (ImageNet21k and CLIP). A non-realistic 'Specialist+Oracle' model is included to show the upper bound of performance achievable.", "section": "4 Experiments"}, {"figure_path": "iQUxHrCna0/tables/tables_9_1.jpg", "caption": "Table 3: Performance comparison of the universal embedding using different distillation approaches, on the test set of UnED. \u201c8 separate teachers\u201d indicates the setting of 8 independent specialist models distilling to a universal model, while \u201c1 separate teacher\u201d indicates the setting of 1 independent model with 8 separate domain heads distilling to a universal model.", "description": "This table compares the performance of three different multi-teacher distillation methods on the UnED dataset. The methods are: 8 separate teachers (where each teacher is a separate model trained on a single domain), 1 separate teacher (where a single model with multiple heads is used, one for each domain), and UDON (the proposed method). The table shows that UDON outperforms both other methods in terms of mean P@5 and R@1.", "section": "4.2 Main results"}, {"figure_path": "iQUxHrCna0/tables/tables_9_2.jpg", "caption": "Table 4: Performance comparison of the teacher embeddings (256D) that are used by the different distillation approaches, on the test set of UnED, but on the separate index setting, i.e. each query is only compared against the index of its own domain.", "description": "This table compares the performance of different teacher embedding methods (8 separate teachers, 1 separate teacher, and UDON teachers) when evaluating on a per-domain basis.  Instead of comparing against the entire UnED dataset, each query is only compared to images from its own domain (as indicated by the parenthetical '(Oracle)'). This setup provides a more controlled evaluation of the teacher embeddings' ability to capture domain-specific knowledge.", "section": "4.2 Main results"}, {"figure_path": "iQUxHrCna0/tables/tables_14_1.jpg", "caption": "Table 8: Performance comparison of the proposed UDON method with the UJCRR method of [45] on the test set of UnED, for the smaller backbone size of ViT-Small.", "description": "This table compares the performance of UDON with the UJCRR baseline method from the paper [45] using a smaller backbone size (ViT-Small). The table shows that UDON achieves better performance than the baseline in terms of both P@5 and R@1 metrics on the UnED dataset.", "section": "4.2 Main results"}, {"figure_path": "iQUxHrCna0/tables/tables_14_2.jpg", "caption": "Table 9: Performance comparison of the proposed UDON method and USCRR method of [45] on the test set of UnED, for two different backbone sizes, namely the ViT-Base and the larger backbone size of ViT-Large.", "description": "This table compares the performance of the proposed UDON model and the USCRR baseline model from a previous work ([45]).  The comparison is done using two different backbone sizes: ViT-Base and ViT-Large. The results show the mean P@5 and R@1 scores across multiple domains for each model and backbone size, highlighting the impact of the model and backbone size on retrieval performance.", "section": "4.2 Main results"}, {"figure_path": "iQUxHrCna0/tables/tables_14_3.jpg", "caption": "Table 1: Performance comparison of the universal embedding for the proposed UDON method against the previous state-of-the-art and the proposed baselines, on the test set of UnED dataset. Off-the-shelf models are shown for reference, as they employ much higher dimensional descriptors (768-D vs. 64-D) than the rest of the methods. For each type of pre-training, the best method is highlighted in bold. The Specialist+Oracle model constitutes a non-realistic method that is presented in order to get an estimate of the maximum performance that can be achieved in each domain. All of the methods use the ViT-Base/16 backbone.", "description": "This table compares the performance of the proposed UDON model against other state-of-the-art universal image embedding methods on the UnED dataset.  It shows Recall@1 and Mean Precision@5 for each of eight image domains, along with overall mean performance.  The table also includes results for off-the-shelf models (with much higher dimensional embeddings) and a non-realistic 'Specialist+Oracle' model to show the potential upper bound of performance.", "section": "4 Experiments"}, {"figure_path": "iQUxHrCna0/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparison of the universal embedding for the proposed UDON method against the previous state-of-the-art and the proposed baselines, on the test set of UnED dataset. Off-the-shelf models are shown for reference, as they employ much higher dimensional descriptors (768-D vs. 64-D) than the rest of the methods. For each type of pre-training, the best method is highlighted in bold. The Specialist+Oracle model constitutes a non-realistic method that is presented in order to get an estimate of the maximum performance that can be achieved in each domain. All of the methods use the ViT-Base/16 backbone.", "description": "This table compares the performance of the proposed UDON model against several baselines and off-the-shelf models on the UnED dataset.  It shows Recall@1 and Mean Precision@5 for each of eight domains and the average across all domains.  The table highlights the best performing model for different pre-training methods (ImageNet-21k and CLIP).  A \"Specialist+Oracle\" model provides an upper bound of performance.", "section": "4 Experiments"}, {"figure_path": "iQUxHrCna0/tables/tables_15_2.jpg", "caption": "Table 12: Performance comparison of the proposed UDON method to a version of it where the embedding projections are changed from linear layers to MLPs (UDON - MLP projectors), on the test set of UnED.", "description": "This table compares the performance of the original UDON model against a modified version.  The modification replaces the linear projection layers with multi-layer perceptrons (MLPs) for both the universal embedding and domain-specific teacher embeddings. The results show that the MLP-based projectors lead to a slight decrease in performance on the UnED benchmark.", "section": "4.3 Ablations"}]