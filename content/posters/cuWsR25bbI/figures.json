[{"figure_path": "cuWsR25bbI/figures/figures_1_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of five skills in a two-layer multi-layer perceptron (MLP) neural network trained on a multitask sparse parity problem.  The x-axis of each subplot represents a different training parameter: time (T), dataset size (D), and number of parameters (N). The y-axis represents the skill strength (Rk/S), which is a normalized measure of how well a particular skill has been learned. Dashed lines show the empirically observed skill emergence in the MLP; solid lines show the predictions of an analytically solvable multilinear model. The model, calibrated using only the emergence of the first skill, accurately predicts the emergence of the remaining skills.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_5_1.jpg", "caption": "Figure 2: Scaling laws. The learning curve (L is the MSE loss) of the multilinear model (solid) and the theoretical power-law (dotted) for (a) time T, (b) data D, and (c) parameters N. Lower left legends show the condition (top) and the scaling law (bottom) where a + 1 is the exponent of the power-law input data (Eq. (1)). See the appendices for 1) rigorous derivations of the theoretical scaling laws including the exponents, prefactors (e.g., A\u00d1 for L = A\u00d1\u00d1\u00af\u00ba), and conditions (Appendix J); 2) simplified derivations of the exponent only (Appendix E); 3) details of the experiment (Appendix K.4).", "description": "This figure compares simulation results of the multilinear model with the theoretical scaling laws for training time, dataset size, and the number of parameters.  The plots show that the multilinear model captures the power-law scaling behavior observed empirically in deep neural networks, but with added prefactor constants.  The appendices provide full mathematical derivations and details of the experimental setup.", "section": "4 Scaling laws"}, {"figure_path": "cuWsR25bbI/figures/figures_5_2.jpg", "caption": "Figure 3: Scaling law for optimal compute. The solid lines are the learning curves of the multilinear model as a function of compute C = T \u00d7 N with varying parameters N from 10<sup>1</sup> (top plateau) to 10<sup>4</sup> (bottom plateau). The dotted lines are optimal compute scaling laws with exponent -\u03b1/(\u03b1+2) (Appendix E.4) and calculated prefactor constants (Appendix J). See Appendix K.4 for details of the experiment. For a given C, we achieve the optimal tradeoff when T is large enough to fit all N skills (i.e. when the solid lines plateau). For the case \u03b1 = 0.3, the optimal C for the model decays faster than the power-law, see Appendix E.1.", "description": "This figure shows the scaling law for optimal compute in the multilinear model.  It plots the learning curves (loss vs compute) for different numbers of parameters (N) and compares them against the predicted optimal compute scaling laws. The optimal tradeoff is achieved when enough time (T) is used to fit all skills (parameters). For \u03b1 = 0.3, the optimal compute decreases faster than the power law.", "section": "4 Scaling laws"}, {"figure_path": "cuWsR25bbI/figures/figures_8_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of five skills in a two-layer multilayer perceptron (MLP) trained on the multitask sparse parity problem.  The skill strength (Rk) is plotted against training time (T), dataset size (D), and number of parameters (N).  The dashed lines represent the empirically observed skill emergence, showing a step-like increase in skill strength as the resources increase. The solid lines are predictions from an analytically tractable multilinear model, which accurately captures the emergence of skills using only a single fit parameter calibrated to the emergence of the first skill.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_16_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of 5 skills in a 2-layer MLP trained on the multitask sparse parity problem, as measured by the skill strength Rk. The dashed lines represent empirical results showing an abrupt increase in skill strength with respect to training time (T), dataset size (D), and number of parameters (N). The solid lines are predictions from a multilinear model calibrated using only the first skill, demonstrating the model's ability to predict the emergence of subsequent skills.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_19_1.jpg", "caption": "Figure 6: Stage-like training. The multilinear model is trained on the multitask sparse parity problem with a = 0.6 and S = 5. (a): Skill strength of the model as a function of time. The emergent time Te (\u20ac) is the time required for the kth skill to reach Rk/S = \u20ac. The saturation time T(s)(\u20ac) is the time required for Rk/S to saturate from \u03b5 to 1 \u2013 \u03b5. The model shows stage-like training if the emergent time interval T(e)1(\u20ac) \u2013 T(e)k(\u03b5) is larger than the saturation time T(s)k(\u03b5) for sufficiently small \u03b5 (0.05 in the figure). (b): The loss as a function of time for the same system as (a). For stage-like training, the change in the loss for the kth emergence is Ps(k)Lk + O(\u03b5) and the interval for the next emergence is \u0394T(e)k(\u03b5) = T(e)k+1(\u03b5) \u2013 T(e)k(\u03b5).", "description": "This figure illustrates the concept of stage-like training in the context of the multilinear model applied to the multitask sparse parity problem.  Panel (a) shows the skill strength (Rk/S) as a function of time (T) for two skills (k=1 and k=2).  The sigmoidal curves illustrate how each skill reaches near-saturation before the next skill begins to emerge. The emergent time (T(e)) and saturation time (T(s)) are graphically defined for each skill. Panel (b) shows the corresponding loss changes over time, highlighting the distinct stages of skill acquisition.", "section": "D Stage-like training: intuitive derivation of the scaling laws"}, {"figure_path": "cuWsR25bbI/figures/figures_19_2.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of 5 skills in a 2-layer multilayer perceptron (MLP) trained on the multitask sparse parity problem. The skill strength, which represents how well a particular skill is learned, is plotted against training time (T), dataset size (D), and the number of parameters (N) in the model. The dashed lines show the empirical results, while the solid lines are the predictions of a simpler multilinear model. The shaded areas represent the 1-standard deviation over at least 10 runs of the experiment. The figure demonstrates that the multilinear model, even when only calibrated on the first skill, can accurately predict the emergence of subsequent skills as training time, data size, or model size increase. This suggests the multilinear model captures the essential dynamics of skill emergence in the MLP.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_22_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of five skills in a two-layer Multilayer Perceptron (MLP) neural network trained on the multitask sparse parity problem.  The skill strength (Rk) is plotted against training time (T), dataset size (D), and number of parameters (N).  The dashed lines represent the empirical results showing an abrupt increase in skill strength (emergence), while the solid lines show predictions from a multilinear model. The multilinear model, calibrated using only the emergence of the first skill, accurately predicts the emergence of subsequent skills.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_29_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure compares the emergence of skills in a two-layer Multilayer Perceptron (MLP) neural network trained on a multitask sparse parity problem with the predictions of a simpler, analytically solvable multilinear model.  The plots show the strength of five different skills (Rk/S) as a function of training time (T), dataset size (D), and the number of model parameters (N).  The dashed lines represent the empirical results from the MLP, showing a step-like emergence of skills.  The solid lines represent the predictions of the multilinear model, demonstrating that the model can accurately predict the emergence of multiple skills (after calibration to the first skill).", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_31_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of skills in a 2-layer MLP trained on the multitask sparse parity problem.  The skill strength (Rk) is plotted against training time (T), dataset size (D), and the number of parameters (N). Dashed lines represent the empirically observed skill emergence, showing a sharp increase in skill strength. Solid lines are predictions from the authors' analytically tractable multilinear model, which is only calibrated on the first skill's emergence. The model successfully predicts the emergence of subsequent skills, demonstrating its ability to capture the key dynamics of skill acquisition in this problem.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_32_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of skills in a 2-layer Multilayer Perceptron (MLP) neural network trained on the multitask sparse parity problem.  The skill strength (Rk/S) is plotted against training time (T), dataset size (D), and number of parameters (N). The dashed lines represent the empirical results showing an abrupt increase in skill strength (emergence), while the solid lines are predictions from a simpler, analytically tractable multilinear model. This model, calibrated only on the first skill, accurately predicts the emergence of the other skills, demonstrating its effectiveness in capturing the essential dynamics of skill acquisition in the MLP.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_33_1.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of five skills in a two-layer multi-layer perceptron (MLP) neural network trained on a multitask sparse parity problem.  The skill strength (Rk) is plotted against training time (T), dataset size (D), and number of parameters (N).  The dashed lines represent the observed skill emergence, while the solid lines show the predictions of a simpler multilinear model. The multilinear model, despite its simplicity, accurately predicts the emergence of multiple skills, demonstrating its effectiveness in capturing the key dynamics of skill acquisition in the MLP.", "section": "Predicting emergence"}, {"figure_path": "cuWsR25bbI/figures/figures_33_2.jpg", "caption": "Figure 1: Predicting emergence. The skill strength Rk, defined as the kth coefficient if a model is expanded in the basis of the skill functions (gk), measures how well the kth skill is learned, and is plotted against (a) time T, (b) data set size D, and (c) number of parameters N (width of the hidden layer). Rk is normalized by the target scale S such that Rk/S = 1 means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent a = 0.6 (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.", "description": "This figure shows the emergence of five skills in a two-layer multi-layer perceptron (MLP) trained on a multitask sparse parity problem. The skill strength (Rk) is plotted against training time (T), dataset size (D), and the number of parameters (N). The dashed lines represent the empirically observed skill emergence, while the solid lines show the predictions from a multilinear model. The figure demonstrates that the multilinear model, calibrated using only the emergence of the first skill, accurately predicts the emergence of subsequent skills as a function of T, D, and N. The shaded areas indicate the standard deviation over multiple runs of the experiment.", "section": "Predicting emergence"}]