{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the concept of emergence in large language models and directly inspired the study of emergence in the multitask sparse parity problem."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper established empirical scaling laws for neural language models that the current work seeks to explain theoretically."}, {"fullname_first_author": "Eric Michaud", "paper_title": "The quantization model of neural scaling", "publication_date": "2023-12-01", "reason": "This paper introduced the multiple unique sparse parity problem as a benchmark for studying emergence and neural scaling laws, which is the dataset this work uses."}, {"fullname_first_author": "Andrew M Saxe", "paper_title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "publication_date": "2014-12-01", "reason": "This paper provides an analytical solution for the dynamics of a deep linear network that informs the construction and analysis of the current work's multilinear model."}, {"fullname_first_author": "Marcus Hutter", "paper_title": "Learning curve theory", "publication_date": "2021-02-01", "reason": "This paper provides a theoretical framework for analyzing learning curves that is used to interpret the scaling laws derived in the current work."}]}