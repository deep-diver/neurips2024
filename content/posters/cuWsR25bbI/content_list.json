[{"type": "text", "text": "An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yoonsoo Nam\\*a, Nayara Fonseca\\*a, Seok Hyeong Leeb, Chris Mingarda c, and Ard A. Louisa ", "page_idx": 0}, {"type": "text", "text": "aRudolf Peierls Centre for Theoretical Physics, University of Oxford bCenter for Quantum Structures in Modules and Spaces, Seoul National University cPhysical and Theoretical Chemistry Laboratory, University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time, training data, or model size increases, a phenomenon known as emergence. In this paper, we present a framework where each new ability (a skill) is represented as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute. We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law. Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Emergence in large language models (LLMs) has attracted a lot of recent attention [1\u20134]. It motivates the costly drive to train ever larger models on ever larger datasets, in the hope that new skills will emerge. While the concept of emergence has been critiqued on the grounds that the sharpness of the transition to acquiring a new skill may be sensitive to the measure being used [5], the observation that important new skills are learned for larger models raises many challenging questions: when the skills emerge and what drives the emergence. These questions are complicated by difficulties in formally defining skills or capabilities [6], and by our general limited understanding of the internal representations of deep neural networks [7]. ", "page_idx": 0}, {"type": "text", "text": "Another widely observed property of deep learning models is that the loss improves predictably as a power-law in the number of data points or the number of model parameters or simply in the amount of compute thrown at a problem. These neural scaling laws [8, 9] have been widely observed across different architectures and datasets [10\u201316]. While the scaling exponents can depend on these factors, the general phenomena of scaling appear to be remarkably robust. This raises many interesting questions such as: What causes the near-universal scaling behavior? How does the continuous scaling of the loss relate to the discontinuous emergence of new skills? ", "page_idx": 0}, {"type": "text", "text": "A challenge in answering the questions raised by the phenomena of emergence and scaling laws arises from the enormous scale and expense of training cutting-edge modern LLMs, which are optimized for commercial applications, and not for answering scientific questions about how they work. One way that progress can be made is to study simpler dataset/architecture combinations that are more tractable. The current paper is inspired in part by recent work in this direction that proposed studying emergence in learning the sparse parity problem [17, 18], which is easy to define, but known to be computationally hard. In particular, Michaud et al. [18] introduce the multiple unique sparse parity problem \u2013 where tasks are distributed in the data through a power-law distribution of frequencies \u2013 as a proxy for studying emergence and neural scaling in LLMs. For this data set, the authors empirically measure the scaling laws of a 2-layer multilayer perceptron (MLP) as a function of training steps $(T)$ , parameters $(N)$ , and training samples $(D)$ . Based on their quanta model of abrupt skill acquisition, they schematically derive neural scaling laws as a sum of emergences of new skills. However, no link was established between the neural network dynamics and the quanta model. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce an analytically tractable model by defining a basis of orthogonal functions for the multitask sparse parity problem. Each basis function corresponds to a skill that can be learned, and their respective frequencies are distributed following a power-law with exponent $\\alpha+1$ . We then propose a simple multilinear expansion in these orthogonal functions that introduces a layered structure reminiscent of neural networks (NNs) and gives rise to the stage-like training dynamics [19]. With our simple model, we can analytically calculate full scaling laws, including pre-factors, as a function of data exponents $\\alpha$ $x,T,D,N$ , and optimal compute $C$ . Our simple model can, with just one parameter calibrated to the emergence of the first skill, predict the ordered emergence of multiple skills in a 2-layer MLP. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "1. Skills as basis functions. We establish a framework for investigating emergence by representing skills as orthogonal functions that form a basis in function space (Section 2). We apply our methods to controlled experiments on the multitask sparse parity dataset.   \n2. Multilinear model. We propose an analytically tractable model that is expanded in the basis of skill functions, and is multilinear with respect to its parameters so that it possesses a layerwise structure (Section 3). The multilinear nature of the model produces non-linear dynamics, and the orthogonal basis decouples the dynamics of each skill.   \n3. Scaling laws. We derive scaling laws for our multilinear model, including the prefactor constants, which relate the model\u2019s performance to training time $(T)$ , dataset size $(D)$ , number of parameters $(N)$ , and optimal compute $\\left(C=N\\times T\\right)$ , see Section 4. We show that the scaling exponents for these factors are $-\\alpha/(\\alpha+1),-\\alpha/(\\alpha+1),-\\alpha,-\\alpha/(\\alpha+2)$ , respectively, where $\\alpha+1$ is the exponent of the power-law input data.   \n4. Predicting emergence. We demonstrate that our multilinear model captures the skill emergence of an MLP with 2 layers for varying training time, dataset size, and number of trainable parameters. Our results show that the multilinear model, calibrated only on the first skill, can predict the emergence of subsequent skills in the 2-layer MLP, see Fig. 1 and Section 5. We obtain an equivalent result on the time emergence for a transformer architecture (Fig. 4). ", "page_idx": 1}, {"type": "image", "img_path": "cuWsR25bbI/tmp/268715025db7c7c05d6a862e40d5cd6e8d6796c74ab7fbda78bc346d6318b2cc.jpg", "img_caption": ["Figure 1: Predicting emergence. The skill strength $\\mathcal{R}_{k}$ , defined as the $k^{t h}$ coefficient if a model is expanded in the basis of the skill functions $(g_{k})$ , measures how well the $k^{\\mathrm{th}}$ skill is learned, and is plotted against (a) time $T$ , (b) data set size $D$ , and (c) number of parameters $N$ (width of the hidden layer). $\\mathcal{R}_{k}$ is normalized by the target scale $S$ such that $\\mathcal{R}_{k}/S=\\bar{1}$ means zero skill loss. The dashed lines show the abrupt growth \u2013 emergence \u2013 of 5 skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent $\\alpha=0.6$ (shaded area indicate 1-standard deviation over at least 10 runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only. "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "cuWsR25bbI/tmp/4728eab4416dc03f2274ab3f8377065b51a9e22a45eb70648725f6e588d0f73d.jpg", "table_caption": ["Table 1: Multitask sparse parity dataset and skill basis functions. The control bits are $n_{s}$ - dimensional one-hot vectors encoding specific parity tasks, indexed in the first column. The frequency of the distinct parity tasks follows a rank-frequency distribution with an inverse power law relation (Eq. (1)). The skill bits are binary strings with $m=3$ relevant sparse bits (highlighted in colors) with their locations varying by skill. The $y$ column shows the target scale $S$ multiplied by the parity computed from the relevant bit set $M(i,x)$ . The last columns show the values of the skill basis functions $g_{k}(i,x)$ , defined in Eq. (2). "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we define the multitask sparse parity problem under the mean-squared error (MSE) loss. We represent skills as orthogonal functions and measure their strength in a model by calculating the linear correlation between the model output and the skill basis functions. For a comprehensive list of notations, refer to the glossary in Appendix A. Our code is also available online.1 ", "page_idx": 2}, {"type": "text", "text": "Multitask sparse parity problem. In the sparse parity problem, $n_{b}$ skill bits are presented to the model. The target function is a parity function applied to a fixed subset of the input bits. The model must detect the relevant $m<n_{b}$ sparse bits and return the parity function on this subset $(M(i,x)$ , see Table 1). Michaud et al. [18] introduced the multitask sparse parity problem by introducing $n_{s}$ unique sparse parity variants \u2013 or skills \u2013 with different sparse bits (for a representation, see Table 1). Each skill is represented in the $n_{s}$ control bits as a one-hot string, and the model must solve the specific sparse parity task indicated by the control bits (for more details, see Appendix B.1). ", "page_idx": 2}, {"type": "text", "text": "The $n_{s}$ skills (random variable $I\\in\\{1,2,\\ldots,n_{s}\\})$ follow a power law distribution $\\mathcal{P}_{s}$ , and the skill bits (random variable $X\\in\\{0,1\\}^{n_{b}})$ are uniformly distributed. Because $\\mathcal{P}_{s}$ and $\\mathcal{P}_{b}$ are independent, the input distribution $\\mathcal{P}(I,X)$ follows a product of two distributions: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{P}_{s}(I=i):=\\frac{i^{-(\\alpha+1)}}{\\sum_{j}^{n_{s}}j^{-(\\alpha+1)}},\\qquad\\mathcal{P}_{b}(X=x):=2^{-n_{b}},\\qquad\\mathcal{P}(I,X):=\\mathcal{P}_{s}(I)\\mathcal{P}_{b}(X).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We denote $\\begin{array}{r}{A=\\left(\\sum_{j=1}^{n_{s}}j^{-(\\alpha+1)}\\right)^{-1}}\\end{array}$ so that Ps(i) := Ai\u2212(\u03b1+1). ", "page_idx": 2}, {"type": "text", "text": "Skill basis functions. We represent the $k^{t h}$ skill as a function $g_{k}:\\{0,1\\}^{n_{s}+n_{b}}\\rightarrow\\{-1,0,1\\}$ that returns the parity $(\\{-1,1\\})$ on the $k^{t h}$ skill\u2019s sparse bits if $i=k$ , but returns 0 if the control bit mismatches that of the $k^{t h}$ skill $(i\\neq k)$ ): ", "page_idx": 2}, {"type": "equation", "text": "$$\ng_{k}(i,x):={\\biggl\\{\\!\\!\\!\\begin{array}{c c}{{(-1)^{\\sum_{j}M_{j}(i,x)}}}&{{{\\mathrm{~if~}}i=k}}\\\\ {{0}}&{{{\\mathrm{otherwise}}}}\\end{array}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $M:\\{0,1\\}^{n_{s}+n_{b}}\\ \\to\\ \\{0,1\\}^{m}$ is the map that selects the relevant sparse bits for the $i^{t h}$ skill (Table 1) and $M_{j}(i,x)$ is the $j^{\\mathrm{th}}$ entry of $M(i,x)$ . Note that different skill functions have 0 correlation as the supports of skills functions are mutually exclusive: ", "page_idx": 2}, {"type": "equation", "text": "$$\ng_{k}(i,x)g_{k^{\\prime}}(i,x)=\\delta_{i,k}\\delta_{k,k^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The target function. The target function is a sum over $n_{s}$ skill functions multiplied by a target scale $S$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nf^{*}(i,x):=S\\sum_{k=1}^{n_{s}}g_{k}(i,x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The target scale $S$ is the norm of the target function $({\\bf E}_{I,X}\\left[f^{*}(I,X)f^{*}(I,X)\\right]=S^{2})$ . Note that the skill functions serve as \u2018features\u2019 or countable basis for describing the target function as in Hutter [20]. ", "page_idx": 3}, {"type": "text", "text": "MSE loss. We use MSE loss for analytic tractability: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}:=\\frac{1}{2}\\mathbf{E}_{X,I}\\left[\\left(f^{*}(I,X)-f(I,X)\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f$ is the function expressed by a given model. We define the skill loss $\\mathcal{L}_{k}$ as the loss when only the $k^{t h}$ skill is given, which can be weighted by their skill frequencies to express the total loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}:=\\frac{1}{2}\\mathbf{E}_{X}\\left[\\left(f^{*}(I=k,X)-f(I=k,X)\\right)^{2}\\right],\\qquad\\mathcal{L}=\\sum_{k=1}^{n_{s}}\\mathcal{P}_{s}(I=k)\\mathcal{L}_{k}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Skill strength. The skill strength or the linear correlation between the $k^{t h}$ skill $(g_{k})$ and a function expressed by the model at time $T\\,(f_{T})$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{k}(T):=\\mathbf{E}_{X}\\left[g_{k}(I=k,X)f_{T}(I=k,X)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The skill strength $\\mathcal{R}_{k}$ is the $k^{t h}$ coefficient if a model is expanded in the basis of the skill functions $(g_{k})$ . The skill strength, like the test loss, can be accurately approximated by a sum (see Appendix K.3). The skill loss $\\mathcal{L}_{k}$ (Eq. (6)) can be expressed by the skill strength and the norm of the learned function for $I=k$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(T)=\\frac{1}{2}\\left(S^{2}+\\mathbf{E}_{X}\\left[f_{T}(I=k,X)^{2}\\right]-2S\\mathcal{R}_{k}(f_{T})\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The skill loss becomes 0 if and only if $f_{T}(I=k,X)=S g_{k}(I=k,X)$ . ", "page_idx": 3}, {"type": "text", "text": "Experimental setting. We use a 2-layer MLP that receives the $n_{s}+n_{b}$ bits as inputs and outputs a scalar $(\\{0,1\\}^{n_{s}+n_{b}}\\ {\\overset{.}{\\rightarrow}}\\ \\mathbb{R})$ . In most of the experiments, the NN is trained with stochastic gradient descent (SGD) with width 1000, using $n_{s}=5$ , $m=3$ , and $n_{b}\\,=32$ , unless otherwise stated. A decoder transformer is also used for the time emergent experiments. See Appendix $\\mathbf{K}$ for details. ", "page_idx": 3}, {"type": "text", "text": "3 Multilinear model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose a simple multilinear model \u2013 multilinear with respect to the parameters \u2013 with the first $N$ most frequent skill functions $g_{k}(i,x)$ as the basis functions (features): ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{T}(i,x;a,b)=\\sum_{k=1}^{N}a_{k}(T)b_{k}(T)g_{k}(i,x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $a,b\\in\\mathbb{R}^{N}$ are the parameters. The model has built-in skill functions $g_{k}$ \u2013 which transform control bits and skill bits into the parity outputs of each skill \u2013 so the model only needs to scale the parameters to $a_{k}b_{k}=S$ . ", "page_idx": 3}, {"type": "text", "text": "The multilinear structure (product of $a_{k},b_{k})$ is analogous to the layered structure of NNs and results in emergent dynamics (Fig. 1(a)) different from a linear model with the same basis functions (Appendix H). A similar model has been studied by Saxe et al. [19] in the context of linear neural networks (Appendix B.2). ", "page_idx": 3}, {"type": "text", "text": "For the multilinear model, note that $a_{k}(T)b_{k}(T)$ is the skill strength $\\mathcal{R}_{k}$ (Eq. (7)) and the skill loss (Eq. (6)) is a function of $S$ and $\\mathcal{R}_{k}$ only: ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{k}(T)b_{k}(T)=\\mathcal{R}_{k}(T),\\qquad\\mathcal{L}_{k}(T)=\\frac{1}{2}(S-\\mathcal{R}_{k}(T))^{2}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assuming that we are training the model on $D$ samples from $\\mathcal{P}(I,X)$ , the empirical loss decomposes into a sum of empirical skill losses because $g_{k}$ \u2019s supports are mutually exclusive. This decouples the dynamics of each skill $(\\mathcal{R}_{k}(T))$ , which is analytically solvable under gradient flow (Appendix C.1). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boxed{\\mathcal{L}^{(D)}(T)=\\frac{1}{2D}\\sum_{k=1}^{n_{s}}d_{k}(S-\\mathcal{R}_{k}(T))^{2},\\qquad\\frac{\\mathcal{R}_{k}(T)}{S}=\\frac{1}{1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)e^{-2\\eta\\frac{d_{k}}{D}S T}},}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d_{k}$ is the number of samples of the $k^{t h}$ skill (i.e., number of samples $(i,x)$ with $g_{k}(i,x)\\neq0,$ ), $\\eta$ is the learning rate, and $0<\\bar{\\mathcal{R}}_{k}(0)<S$ is the skill strength at initialization. ", "page_idx": 4}, {"type": "text", "text": "4 Scaling laws ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recent literature has extensively explored scaling laws; see Section 7 for an overview. In this section, we derive the scaling laws of our multilinear model (Section 3) for time $(T)$ , data $(D)$ , parameters $(N)$ and optimal compute $(C)$ . We define compute as $C:=T\\times N$ [21]. ", "page_idx": 4}, {"type": "text", "text": "Table 2 shows our analytical scaling laws including their prefactor constants (Appendix J) and Fig. 2 compares the simulation of our model with our scaling law predictions. For the scaling law exponents, we achieve the same exponent as in Hutter [20] for $D$ and in Michaud et al. [18] for $T,D$ , and $N$ . Assuming $0<\\alpha<1$ , the exponents are consistent with the small power-law exponents reported in large-scale experiments, see, e.g., [9, 14, 22]. ", "page_idx": 4}, {"type": "text", "text": "Using Eqs. (6), (10) and (11), we derive the loss as a function of time $(T)$ , data $(D)$ , parameters $(N)$ , and the number of observations for each skill $\\left[d_{1},\\cdot\\cdot\\cdot,d_{n_{s}}\\right]$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{S^{2}}{2}\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{1}{\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta\\frac{d_{k}}{D}S T}\\right)^{2}}+\\frac{S^{2}}{2}\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Under suitable assumptions (e.g., for the $T$ scaling law, we take $D,N\\to\\infty$ and $d_{k}/D\\rightarrow\\mathcal{P}_{s}(k))$ , we can use Eq. (12) to derive the scaling laws. For $T,D$ , and $N$ , we used Eq. (11) \u2013 decoupled dynamics induced the basis functions $g_{k}$ \u2013 to decouple the evolution of each skill loss: ", "page_idx": 4}, {"type": "text", "text": "1. For the time scaling law, each $\\mathcal{L}_{k}$ shares the same dynamics with $T$ scaled by $\\mathcal{P}_{s}(k)$ . 2. For the data scaling law, each $\\mathcal{L}_{k}$ depends only on the observation the $k^{t h}$ skill $(d_{k}>0)$ ). 3. For the parameter scaling law, each $\\mathcal{L}_{k}$ depends on whether the model has $g_{k}$ as a basis function. ", "page_idx": 4}, {"type": "text", "text": "For the optimal compute scaling law, we show in Corollary 4 (Appendix J) that the optimal tradeoff between $T$ and $N$ for given $C$ is when $T$ is large enough to fti the $\\bar{N}^{t h}$ skill (Fig. 3). In Appendix J, we show rigorous derivations of all scaling laws, including the prefactors, error bounds, and conditions (e.g., how large $N$ must be compared to $T$ to be treated as infinity). For simplified derivations for the exponents only, see Appendix E. For an intuitive derivation (stage-like training) and connection to Michaud et al. [18], see Appendix D. ", "page_idx": 4}, {"type": "text", "text": "5 Predicting emergence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The literature on emergence has rapidly expanded lately; for a review of these developments, see Section 7. In this section, we analyze the emergence of a 2-layer NN (Section 2) and discuss to what degree the emergence in NNs can be described with our model. At initialization, NNs lack the information about the data and must \u2018discover\u2019 each $g_{k}$ . To take this effect into account in our model, we add an extra parameter which we calibrate (fit) on an NN trained on one skill $\\mathit{n}_{s}=1$ ) system and use it to predict the emergence of subsequent skills for the $n_{s}=5$ setup (Fig. 1). ", "page_idx": 4}, {"type": "text", "text": "5.1 Time emergence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In our multilinear model, the layerwise structure \u2013 the product of parameters $a_{k}b_{k}$ \u2013 leads to a sigmoidal saturation where an update of one layer hastens the update of the other layer. Feature ", "page_idx": 4}, {"type": "image", "img_path": "cuWsR25bbI/tmp/d0942878b608519f27ee09a3cb600a510370e1872d313e706c803c290d5bcb7b.jpg", "img_caption": ["Figure 2: Scaling laws. The learning curve ( $\\mathcal{L}$ is the MSE loss) of the multilinear model (solid) and the theoretical power-law (dotted) for (a) time $T$ , (b) data $D$ , and (c) parameters $N$ . Lower left legends show the condition (top) and the scaling law (bottom) where $\\alpha+1$ is the exponent of the power-law input data (Eq. (1)). See the appendices for 1) rigorous derivations of the theoretical scaling laws including the exponents, prefactors (e.g., $\\boldsymbol{A}_{N}$ for $\\mathcal{L}=\\mathcal{A}_{N}N^{-\\alpha})$ , and conditions (Appendix J); 2) simplified derivations of the exponent only (Appendix E); 3) details of the experiment (Appendix K.4). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 2: Summary of the scaling laws for the multilinear model. The leftmost column indicates the bottleneck resource while the next two columns are the conditions for the \u2018large resources\u2019 \u2013 large enough to be treated as infinity. The fourth column is the bottleneck resource\u2019s scaling law exponent for the loss. The last two columns show the statement for the prefactor constant and the scaling law (with the assumptions and explicit error terms) in Appendix J. ", "page_idx": 5}, {"type": "table", "img_path": "cuWsR25bbI/tmp/be0bed1f810c9bf6dbb9ce6dda17b83e0e356757ec2ea5373718d166440ffdd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "cuWsR25bbI/tmp/98e61616e92786a8829afa2ca74b6854564afa99436f245cd801e2a51ff82b20.jpg", "img_caption": ["Figure 3: Scaling law for optimal compute. The solid lines are the learning curves of the multilinear model as a function of compute $C=T\\times N$ with varying parameters $N$ from $10^{1}$ (top plateau) to $10^{4}$ (bottom plateau). The dotted lines are optimal compute scaling laws with exponent $-\\alpha/(\\alpha+2)$ (Appendix E.4) and calculated prefactor constants (Appendix J). See Appendix K.4 for details of the experiment. For a given $C$ , we achieve the optimal tradeoff when $T$ is large enough to fti all $N$ skills (i.e. when the solid lines plateau). For the case $\\alpha=0.3$ , the optimal $C$ for the model decays faster than the power-law, see Appendix E.1. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "learning dynamics in a 2-layer MLP shares the positive feedback between the layers but require a non-trivial update of parameters to express $g_{k}$ . ", "page_idx": 6}, {"type": "text", "text": "Extended model. Given that feature learning, though nonlinear, involves parameter updates, we compensate for the additional delay in feature-learning by multiplying $g_{k}$ by a calibration constant $0<B<1$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{T}(i,x;a,b)=\\sum_{k=1}^{N}a_{k}(T)b_{k}(T)\\mathcal{B}g_{k}(i,x),\\qquad0<B<1.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The calibration constant $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ rescales the dynamics in $T$ (Eq. (11)): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{R}_{k}(T)}{S}=\\frac{1}{1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)e^{-2\\eta\\mathcal{P}_{s}(k)B^{2}S T}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $d_{k}/D\\to\\mathcal{P}_{s}(k)$ because we assume $D\\rightarrow\\infty$ . We observe that $B^{2}=1/22$ ftis the NN trained on one skill (see Fig. 11 in Appendix I), and the calibrated model predicts emergence in the $n_{s}=5$ system (Fig. 1(a)): suggesting that the dynamics of feature-learning $g_{k}$ in 2-layers NNs is similar to that of parameter learning $(a_{k}b_{k})$ in a simple multilinear model. For further intuition of the extended model, see an example of time emergence in an NN in Appendix G. ", "page_idx": 6}, {"type": "text", "text": "5.2 Data point emergence ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our multilinear model can learn the $k^{t h}$ skill with a single observation of the skill because the skill functions $g_{k}$ are built in (see Corollary 1 in Appendix C.2). NNs, without the fixed basis functions, must \u2018discover\u2019 each $g_{k}$ , which requires multiple samples from the $k^{t h}$ skill. ", "page_idx": 6}, {"type": "text", "text": "Extended model. To make our model a $D_{c}$ -shot learner, we extend it by replacing $g_{k}$ with the $e_{k,l}$ basis: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{T}(i,x;a,B)=\\sum_{k=1}^{N}a_{k}(T)\\sum_{l=1}^{D_{c}}B_{k,l}(T)e_{k,l}(i,x),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the matrix $B\\,\\in\\,\\mathbb{R}^{N\\times D_{c}}$ is an extension of $b\\,\\in\\,\\mathbb{R}^{N}$ in Eq. (9), $D_{c}$ is a fixed scalar, and $e_{k,l}(i,x):\\{0,1\\}^{n_{s}+n_{b}}\\rightarrow\\mathbb{R}$ are functions with the following properties: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{E}_{X|I=k}\\left[e_{k,l}e_{k,l^{\\prime}}\\right]=\\delta_{l l^{\\prime}},\\ \\ \\ \\ e_{k,l}(I\\neq k,x)=0,\\ \\ \\ \\sum_{l=1}^{D_{c}}\\frac{1}{\\sqrt{D_{c}}}e_{k,l}=g_{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The first property states that $e_{k}$ \u2019s, when $I=k$ , are orthonormal in $X$ . The second property asserts that, similar to $g_{k}$ (Eq. (2)), $e_{k,l}$ is non-zero only when $I=k$ , and ftiting of the $k^{t h}$ skill only occurs among $e_{k,l}$ \u2019s, keeping the skills decoupled. The third property states that $g_{k}$ can be expressed using $e_{k,l}$ . ", "page_idx": 6}, {"type": "text", "text": "For the $k^{t h}$ skill, the extended model overfits $g_{k}$ when there are fewer observations $(d_{k})$ than the dimension of the $e_{k,l}$ basis $(D_{c})$ , and fits $g_{k}$ when $d_{k}\\geq D_{c}$ , making our model a $D_{c}$ shot learner. ", "page_idx": 6}, {"type": "text", "text": "$D_{c}$ shot learner. If we initialize the extended model in Eq. (15) with sufficiently small initialization and if the conditions in Eq. (16) are satisfied, then the skill strength after training $T\\rightarrow\\infty,$ ) on $D$ datapoints is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(\\infty)=\\left\\{S\\left(1-\\sqrt{1-d_{k}/D_{c}}\\right)\\quad:d_{k}<D_{c}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The number $d_{k}$ is the number of samples in the training set for the $k^{t h}$ skill (i.e., datapoints with $g_{k}(i,x)\\neq0,$ ). ", "page_idx": 6}, {"type": "text", "text": "Proof See Appendix F.3. ", "page_idx": 6}, {"type": "text", "text": "Using Eq. (17), we can calculate the emergence of $\\mathcal{R}_{k}/S$ as a function of $D$ . Note that Eq. (17) is similar to the model in Michaud et al. [18] in that, to learn a skill, the model requires a certain number of samples from the skill. ", "page_idx": 6}, {"type": "text", "text": "The derivation of Eq. (17) follows trivially from the dynamics of the extended model (Eq. (15)) and well-known results in linear/kernel regression [23\u201327]. To be more specific, the model finds the minimum norm solution as if we performed ridgeless regression on $g_{k}$ with basis functions $\\left[e_{k,1},\\cdot\\cdot\\cdot e_{k,D_{c}}\\right]$ . See Appendix F.3 for details. ", "page_idx": 7}, {"type": "text", "text": "We observe that $D_{c}=800$ approximates the data emergence for the $n_{s}=1$ system (see Fig. 11 in Appendix I) and also the emergence for $n_{s}=5$ system (Fig. 1(b)), suggesting that the NN discovers $g_{k}$ when it observes $D_{c}$ samples from the $k^{t h}$ skill. ", "page_idx": 7}, {"type": "text", "text": "5.3 Parameter emergence ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since our multilinear model has $g_{k}$ \u2019s as the basis functions, it requires only one basis function (2 parameters) to express a skill (see Corollary 2 in Appendix C.3). A 2-layer NN cannot express a skill with a single hidden node (i.e., a hidden layer with width 1); it requires multiple hidden nodes to express a single skill. ", "page_idx": 7}, {"type": "text", "text": "Extended model. To compensate for the need for multiple hidden nodes in expressing one skill, we extend our model similarly to Eq. (15). Because the number of parameters is now a bottleneck, we ensure the model has $N$ basis functions $(e_{k,l}\\,^{,}\\mathrm{s})$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nf_{T}(i,x;a,B)=\\sum_{k=1}^{q-1}\\sum_{l=1}^{N_{c}}a_{k}(T)B_{k,l}(T)e_{k,l}(i,x)+\\sum_{l^{\\prime}=1}^{r}a_{q}(T)B_{q,l^{\\prime}}(T)e_{q,l^{\\prime}}(i,x),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $N_{c}$ is the number of basis functions needed to express a skill, quotient $q$ is $[(N-1)/N_{c}]+1$ and remainder $r$ is such that $(q-1)N_{c}+r=N$ . In short, the $N$ basis functions are ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\big[e_{1,1},\\cdot\\cdot\\cdot\\;,e_{1,N_{c}},\\quad e_{2,1},\\cdot\\cdot\\cdot\\;,e_{2,N_{c}}\\quad\\cdot\\cdot\\cdot\\quad e_{q,1},\\cdot\\cdot\\cdot\\;,e_{q,r}\\big].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similar to Eq. (16), the basis functions satisfy the following properties ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{E}_{X|I=k}\\left[e_{k,l}e_{k,l^{\\prime}}\\right]=\\delta_{l l^{\\prime}},\\ \\ \\ \\ e_{k,l}(I\\neq k,x)=0,\\ \\ \\ \\sum_{l=1}^{N_{c}}\\frac{1}{\\sqrt{N_{c}}}e_{k,l}=g_{k}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$N_{c}$ basis functions for a skill. For the extended model in Eq. (18), the skill strength at $T,D\\to\\infty$ for a given $N$ becomes ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(\\infty)=\\left\\{\\begin{array}{l l}{0}&{:k>q}\\\\ {S\\frac{r}{N_{c}}}&{:k=q}\\\\ {S}&{:k<q\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof See Appendix F.4. ", "page_idx": 7}, {"type": "text", "text": "The model can express the $k^{\\mathrm{th}}$ skill based on the number of available basis functions for the given skill (Eq. (21)). For example, skills with $k\\,<\\,q$ have all $N_{c}$ basis functions $\\left[e_{k,1},\\cdot\\cdot\\cdot\\,,e_{k,N_{c}}\\right]$ to express the $k^{\\mathrm{th}}$ skill (Eq. (20)), while for $k=q$ , only $r$ of the $N_{c}$ basis functions are available. ", "page_idx": 7}, {"type": "text", "text": "We observe that $N_{c}\\,=\\,4$ fits the parameter emergence for the $n_{s}~=~1$ system (see Fig. 11 in Appendix I) and also the emergence for the $n_{s}\\,=\\,5$ system (Fig. 1(c)), suggesting that the NN requires 4 nodes in expressing $g_{k}$ . The results also suggest that an NN, while lacking the ordering of basis functions (Eq. (19)), prefers to use the hidden neuron in fitting more frequent skills. The \u2018preference\u2019 toward frequent skills agrees with Fig. 1(a) where the NN learns more frequent skills first. Note that for the parameter emergence experiment, Adam [28] was used, instead of SGD, to increase the chance of escaping the near-flat saddle points induced by an insufficient number of parameters. ", "page_idx": 7}, {"type": "text", "text": "5.4 Time emergence in a transformer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To test whether our conceptual framework extends to other architectures, we perform a time emergence experiment with a transformer (Fig. 4). Note that the emergent time \u03c4emerge \u2013 when the skill strength is sufficiently larger than 0 \u2013 follows the same power-law relationship as Eq. (11): $\\tau_{e m e r g e}(k)\\propto k^{\\alpha+1}$ (see Fig. 6 in Appendix D for a discussion on emergent time). This suggests that, in the multitask sparse parity setup, other architectures may follow similar decoupled dynamics (Eq. (11)) and the consequent scaling laws (Section 4) and emergence (Section 5). An in-depth study of these findings across different architectures is left for future work. ", "page_idx": 7}, {"type": "image", "img_path": "cuWsR25bbI/tmp/10ebdebed77b9af9990fec8e6318bdb0f627fff6221940aad7303210cb73a922.jpg", "img_caption": ["Figure 4: Transformer on multitask sparse parity task. We trained a transformer on the multitask sparse parity task with $\\alpha=0.9$ ; see Appendix K for details. Left: An example of the time emergence (measued in steps) for the transformer in the $n_{s}=5$ setup. See Appendix I for enlarged plots showing the saturation of each skill in linear scale. Right: The $k^{t h}$ skill\u2019s emergent time $\\tau_{e m e r g e}(k)$ (i.e. $\\mathcal{R}_{k}(\\tau_{e m e r g e}(k))/S=0.05)$ as a function of $k$ (error bars indicate 1-standard deviation over 5 runs). The emergent times follow a power law of $k^{\\alpha+1}$ , following the same relationship in the multilinear model (Eq. (11)). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.5 Limitations of the multilinear model ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The strength of our extended multilinear model comes from the decoupled dynamics for each skill: leading to the prediction of the time, data, and parameter emergence with a single calibration. The weakness of our model is that it simplifies the more complex dynamics of NNs. ", "page_idx": 8}, {"type": "text", "text": "Time emergence. We note that the NN and the multilinear model emerge at similar instances, but the NN takes longer to saturate fully. This is because, for a given skill, the dynamics of the NN is not one sigmoidal saturation but a sum of multiple sigmoidal dynamics with different saturation times. To express the parity function, the NN must use multiple hidden neurons, and the skill strength can be divided into the skill strength from each neuron whose dynamics follow a sigmoidal saturation. Because of the non-linearity and the function it expresses, each neuron is updated at different rates, and the slowly saturating neurons result in a longer tail compared to our multilinear model. For an example, see Fig. 8 in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Data point emergence. Our extended model (Eq. (17)) deviates from NNs when $d_{k}\\ll D_{c}$ and NNs show a more abrupt change in $\\mathcal{R}_{k}$ as a function of $D$ . This is because our model asserts strict decoupling among the skills: even a few $d_{k}$ will contribute to learning $g_{k}$ from $e_{k,l}$ . This differs from the NN, which lacks strict decoupling among the samples from different skills. We speculate that because NNs can perform benign [29] or tempered [30] overfitting, they treat a few data points from less frequent skills as \u2018noise\u2019 from more frequent skills: requiring more samples to learn the infrequent skills. ", "page_idx": 8}, {"type": "text", "text": "Parameter emergence. Note that Fig. 1(c) has high variance compared to other emergence plots in Fig. 1; this is because the NN sparsely, over many repeated trials, uses the hidden neurons to learn less frequent skills over more frequent ones (see Table 5 in Appendix I for an example of such outliers). Because NNs are less strictly biased toward frequent skills than our model, we speculate that initial conditions favoring less frequent skills may contribute to the outliers. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work demonstrated scaling laws and predicted emergence in a 2-layer MLP using a tractable multilinear model. We found that representing skills as mutually exclusive functions leads to the decoupled dynamics, resulting in the scaling laws observed in a 2-layer MLP. The layerwise structure leads to emergent (sigmoidal) saturation of the skill strength, similar to what is observed in 2-layer MLPs. ", "page_idx": 8}, {"type": "text", "text": "Despite lacking explicit skill functions, NNs exhibit similar emergence patterns. We speculate that the model\u2019s layerwise structure and power-law frequencies of the skills induce stage-like dynamics (Appendix D) in NNs. The parameters relevant for expressing more frequent skills are updated significantly faster than those for less frequent skills. When skill \u2018discovery\u2019 operates on different time scales with minimal interaction, the skill dynamics effectively become decoupled, justifying our model setup. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Our results suggest a link between feature learning and emergence [6] driven by decoupled, stage-like dynamics. The layerwise dynamics leading to sigmoidal saturation may also disentangle the problem into skills (features) of varying importance (frequencies). Then feature learning, or discovering the basis functions that describe the target function [31, 32] (for recent studies, see [33\u201338]), likely occurs in stages. Investigating this connection through layerwise dynamics is left for future work. ", "page_idx": 9}, {"type": "text", "text": "Similar to many prior works (see, e.g., [20, 18]), we studied a simple model on an idealized powerlaw distributed dataset. Also, our model cannot capture the complex non-linear interactions among multiple skills but can express any linear superposition of skills. In future work, we will explore \u2018complex skills\u2019 in language as a superposition of linearly independent skills. By validating our findings in language tasks, we aim to contribute to a broader understanding of how neural networks acquire and exhibit complex behaviors. ", "page_idx": 9}, {"type": "text", "text": "7 Related works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we review the literature on scaling laws and emergence in NNs. Focusing on data scaling, Hutter [20] develops a model with a discrete set of features. Under the assumption of a power-law distribution of features, this model demonstrates that the error decreases as a power law with increasing data size. In a related vein, Michaud et al. [18] propose a model of neural scaling laws in which the loss is decomposed into a sum over \u2018quanta\u2019. Their model aims to reconcile the apparent discrepancy between loss metrics\u2019 regular power-law scaling and the abrupt development of novel capabilities in large-scale models. Various other models for neural scaling laws have been proposed in recent research, including connecting neural scaling exponents to the data manifold\u2019s dimension [39] and their relation with kernels [40], proposing solvable random-feature models [41, 21], and developing data scaling models using kernel methods [42, 43, 25]. ", "page_idx": 9}, {"type": "text", "text": "Closely related to the study of neural scaling laws is the understanding of emergent abilities in large language models. Several studies [1\u20134] document examples of such emergent abilities. Arora and Goyal [44] propose a framework for the emergence of tuples of skills in language models, in which the task of predicting text requires combining different skills from an underlying set of language abilities. Okawa et al. [45] demonstrate that a capability composed of smoothly scaling skills will exhibit emergent scaling due to the multiplicative effect of the underlying skills\u2019 performance. Other works related to the skill acquisition include Yu et al. [46], who introduce a new evaluation to measure the ability to combine skills and develop a methodology for grading such evaluations, and Chen et al. [47], who formalize the notion of skills and their natural acquisition order in language models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "NF acknowledges the UKRI support through the Horizon Europe guarantee Marie Sk\u0142odowska-Curie grant (EP/X036820/1). SL was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No.2020R1A5A1016126). We thank Charles London, Eric Michaud, Zohar Ringel, and Shuofeng Zhang for their helpful comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[2] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1747\u20131764, 2022.   \n[3] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint:2206.04615, 2022.   \n[4] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint: 2206.07682, 2022.   \n[5] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? Advances in Neural Information Processing Systems, 36, 2023.   \n[6] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario G\u00fcnther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se\u00e1n \u00d3 h\u00c9igeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint: 2404.09932, 2024.   \n[7] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html.   \n[8] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint:1712.00409, 2017.   \n[9] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint:2001.08361, 2020.   \n[10] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. arXiv preprint: 1909.12673, 2019.   \n[11] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint:2010.14701, 2020.   \n[12] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machine translation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5915\u20135922, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[13] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113, 2022.   \n[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint:2203.15556, 2022.   \n[15] Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti. Scaling laws for associative memories. arXiv preprint arXiv:2310.02984, 2023.   \n[16] Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hofmann. Scaling mlps: A tale of inductive bias. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems, 35:21750\u201321764, 2022.   \n[18] Eric Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural scaling. Advances in Neural Information Processing Systems, 36, 2023.   \n[19] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. Proceedings of the International Conference on Learning Representations 2014, 2014. arXiv:1312.6120.   \n[20] Marcus Hutter. Learning curve theory. arXiv preprint:2102.04074, 2021.   \n[21] Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling laws. arXiv preprint:2402.01092, 2024.   \n[22] Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication attempt. arXiv preprint:2404.10102, 2024.   \n[23] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature communications, 12(1):2914, 2021.   \n[24] Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cl\u00e9ment Hongler, and Franck Gabriel. Kernel alignment risk estimator: Risk prediction from training data. Advances in Neural Information Processing Systems, 33:15568\u201315578, 2020.   \n[25] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural Information Processing Systems, 34:10131\u201310143, 2021.   \n[26] Ouns El Harzli, Bernardo Cuenca Grau, Guillermo Valle-P\u00e9rez, and Ard A Louis. Doubledescent curves in neural networks: a new perspective using gaussian processes. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11856\u201311864, 2024.   \n[27] James B Simon, Madeline Dickens, Dhruva Karkada, and Michael R DeWeese. The eigenlearning framework: A conservation law perspective on kernel regression and wide neural networks. Transactions on Machine Learning Research, 2023. arXiv:2110.03922.   \n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint:1412.6980, 2014.   \n[29] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020.   \n[30] Neil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. Advances in Neural Information Processing Systems, 35:1182\u20131195, 2022.   \n[31] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u2013 1828, 2013.   \n[32] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.   \n[33] Greg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In International Conference on Machine Learning, pages 11727\u201311737. PMLR, 2021.   \n[34] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent alignment effect. arXiv preprint arXiv:2111.00034, 2021.   \n[35] Arthur Jacot, Eugene Golikov, Cl\u00e9ment Hongler, and Franck Gabriel. Feature learning in $l\\_2.$ - regularized dnns: Attraction/repulsion and sparsity. Advances in Neural Information Processing Systems, 35:6763\u20136774, 2022.   \n[36] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. Advances in Neural Information Processing Systems, 35:32240\u201332256, 2022.   \n[37] Inbar Seroussi, Gadi Naveh, and Zohar Ringel. Separation of scales and a thermodynamic description of feature learning in some cnns. Nature Communications, 14(1):908, 2023.   \n[38] Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborov\u00e1, and Bruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step. arXiv preprint arXiv:2402.04980, 2024.   \n[39] Utkarsh Sharma and Jared Kaplan. Scaling laws from the data manifold dimension. Journal of Machine Learning Research, 23(9):1\u201334, 2022. arXiv:2004.10802.   \n[40] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. arXiv preprint:2102.06701, 2021.   \n[41] Alexander Maloney, Daniel A Roberts, and James Sully. A solvable model of neural scaling laws. arXiv preprint:2210.16859, 2022.   \n[42] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher\u2013student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):124001, 2020.   \n[43] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning, pages 1024\u20131034. PMLR, 2020.   \n[44] Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language models. arXiv preprint:2307.15936, 2023.   \n[45] Maya Okawa, Ekdeep S Lubana, Robert Dick, and Hidenori Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix: A flexible and expandable family of evaluations for ai models. arXiv preprint:2310.17567, 2023.   \n[47] Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher R\u00e9. Skill-it! a data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36, 2023.   \n[48] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv:2201.02177, 2022.   \n[49] Irina Gennad\u2019evna Shevtsova. Sharpening of the upper bound of the absolute constant in the berry\u2013esseen inequality. Theory of Probability and Its Applications, 51(3):549\u2013553, 2007.   \n[50] Hugh L Montgomery and Robert C Vaughan. Multiplicative Number Theory I: Classical Theory. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2007. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Glossary ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$A$ Normalization constant for $\\mathcal{P}_{s}$ such that $\\mathcal{P}_{s}(k)=A k^{-(\\alpha+1)}$   \n$T$ Time or step   \n$D$ Number of data points   \n$N$ Number of parameters (skill basis functions in the model for the multilinear model; the width of hidden layer for MLP)   \n$C$ The computation cost $T\\times N$   \n$n_{s}$ The number of skills in the multitask sparse parity problem   \n$I$ Random variable of the control bits   \n$X$ Random variable of the skill bits   \n$\\mathcal{P}_{s}$ Probability of skills (control bits)   \n$\\mathcal{P}_{b}$ Probability of skill bits   \n$S$ The target scale or the norm of the target function   \n$\\mathcal{R}_{k}$ Skill strength of the $k^{t h}$ skill (Eq. (7))   \n$\\mathcal{L}$ Total (generalization) loss   \nL(D) Empirical loss for $D$ samples   \n$\\mathcal{L}_{k}$ Skill loss of the $k^{t h}$ skill (Eq. (6))   \n$d_{k}$ Number of observation of the $k^{t h}$ skill (i.e. number of training points $(i,x)$ with $g_{k}(i,x)\\neq0)$   \n$f^{*}$ Target function $f^{*}:\\{0,1\\}^{n_{s}+n_{b}}\\rightarrow\\{-S,S\\}$ (Eq. (4))   \n$g_{k}$ The $k^{t h}$ skill basis function $g_{k}:\\{0,1\\}^{n_{s}+n_{b}}\\rightarrow\\{-1,0,1\\}$ (Eq. (2)) ", "page_idx": 14}, {"type": "text", "text": "Table 3: Representation of the multitask sparse parity as presented in [18]. The control bits are one-hot vectors encoding a specific parity task. The frequency of the different tasks follows a powerlaw distribution. In this example, there are $n_{s}=10$ tasks, and skill bits are length $n_{b}=15$ . The $y$ column is the resulting parity computed from $m=3$ bits (highlighted in colors). The multitask dataset provides a controlled experimental setting designed to investigate skills. ", "page_idx": 15}, {"type": "table", "img_path": "cuWsR25bbI/tmp/f47f6d789950f34454604004d095c16a3723f8122209ea935a8f9f92e29d3ca7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Background ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we review the multitask sparse parity dataset, as described by Michaud et al. [18] and discuss the nonlinear dynamics of two-layer linear networks, following the work of Saxe et al. [19]. ", "page_idx": 15}, {"type": "text", "text": "B.1 Multitask sparse parity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The sparse parity task can be stated as follows: for a bit string of length $n_{b}$ , the goal is to determine the parity (sum mod 2) of a predetermined subset of $m$ bits within that string. The multitask sparse parity [18] extends this problem by introducing $n_{s}$ unique sparse parity variants in the dataset. The input bit strings have a length of $n_{s}+n_{b}$ . The first $n_{s}$ bits function as indicators by assigning a specific task. The frequency of the distinct parity tasks follows a rank-frequency distribution with an inverse power law relation (power-law distribution). The last $n_{b}$ bits are uniformly distributed. This sets a binary classification problem $\\{0,1\\}^{n_{s}+n_{b}}\\rightarrow\\{0,1\\}$ where only a single bit of the initial $n_{s}$ bits is nonzero. In Table 3, the many distinct parity tasks represent different skills. 2 ", "page_idx": 15}, {"type": "text", "text": "The proposal in [18] aims to reconcile the regularity of scaling laws with the emergence of abilities with scale using three key hypotheses: (i) skills, represented as a finite set of computations, are distinct and separate; (ii) these skills differ in their effectiveness, leading to a ranking based on their utility to reduce the loss; and (iii) the pattern of how frequently these skills are used in prediction follows a power-law distribution. Interestingly, the multitask problem has a consistent pattern across scaling curves: each parity displays a distinct transition, characterized by a sharp decrease in loss at a specific scale of parameters, data, or training step. Such a sudden shift occurs after an initial phase of no noticeable improvement, leading to reverse sigmoid-shaped learning curves. Michaud et al. [18] empirically show that for a one-hidden-layer neural network with ReLU activation, trained using cross-entropy loss and the Adam optimizer, these transitions happen at different scales for distinct tasks. This results in a smooth decrease in the overall loss as the number of skill levels increases. ", "page_idx": 15}, {"type": "text", "text": "B.2 Nonlinear dynamics of linear neural network ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Saxe et al. [19] have solved the exact dynamics for two-layer linear neural networks with gradient descent under MSE loss (Fig. 5(a)).3The dynamics decompose into independent modes that show sigmoidal growth at different timescales (Fig. 5(c)). The setup assumes orthogonal input features $\\bar{X_{\\mathrm{~\\}}}\\in\\mathbb{R}^{d_{1}}$ and input-output correlation matrix $\\sum\\in\\dot{\\mathbb{R}}^{d_{1}\\times d_{3}}$ for target output $f^{*}(\\bar{X})\\in\\mathbb{R}^{\\bar{d}_{3}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf E}_{X}\\left[X_{i}X_{j}\\right]=\\delta_{i j},\\qquad\\Sigma={\\bf E}_{X}\\left[X f^{*T}(X)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "cuWsR25bbI/tmp/eb4d7a9bf6ba41c8706b3151f7961b756668fa829b85f930c5e32cf4b707e2b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Nonlinear dynamics of linear neural networks. (a): A two-layer undercomplete linear neural network, which is a multiplication of two matrices, where $d_{2}<d_{1}$ and $d_{2}<d_{3}$ . (b): The $d_{2}$ independent modes of dynamics for linear neural network (Eq. (24)). The product of parameters $a_{k}b_{k}$ are learnable parameters and vectors $u_{k},v_{k}$ are obtained from SVD of the input-output correlation matrix $\\Sigma$ (Eq. (22)). (c): The temporal evolution of $a_{k}b_{k}$ under gradient descent, which follows a sigmoidal growth (Eq. (25)). Note that smaller $\\lambda_{k}$ \u2013 the singular value of $\\Sigma$ \u2013 results in a more delayed saturation of $a_{k}b_{k}$ . ", "page_idx": 16}, {"type": "text", "text": "By performing SVD (singular value decomposition) on input-output correlation matrix $\\Sigma=U\\Lambda V$ , the target function $f^{*}:\\mathbb{R}^{d_{1}}\\rightarrow\\mathbb{R}^{d_{3}}$ becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{*}(x)=\\sum_{k=1}^{d_{2}}v_{k}\\lambda_{k}u_{k}^{T}x,\\qquad U^{T}\\Lambda V={\\bf E}_{X}\\left[X f^{*}(X)^{T}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $u_{k}\\in\\mathbb{R}^{d_{1}},v_{k}\\in\\mathbb{R}^{d_{3}}$ are the row vectors of $U,V$ and $\\lambda_{k}\\in\\mathbb{R}$ are the singular values of $\\Lambda$ . ", "page_idx": 16}, {"type": "text", "text": "Saxe et al. [19] have shown that the dynamics of a two-layer (one-hidden-layer) undercomplete (the width of the hidden layer is smaller than the width of the input and output) linear neural network decomposes into that of the following \u2018modes\u2019: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{k}^{T}f(x;a,b)=a_{k}b_{k}u_{k}^{T}x\\qquad k\\in\\{1,2,\\cdot\\cdot\\cdot,d_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $a_{k},b_{k}\\in\\mathbb{R}$ are the parameters. Note that Eq. (24) are $d_{2}$ decoupled functions $v_{k}^{T}f(x):\\mathbb{R}^{d_{1}}\\rightarrow$ $\\mathbb{R}$ (Fig. 5(b)). Assuming small and positive initialization $(0\\,<\\,a_{k}\\bar{(}0)b_{k}(0)\\,\\ll\\,\\lambda_{k})$ , the dynamics of Eq. (24) under gradient descent with learning rate $\\eta$ can be solved analytically; the product of parameters $a_{k}b_{k}$ grows sigmoidally with saturation time proportional to $\\lambda_{k}^{-\\mathrm i}$ (Fig. 5(c)): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{a_{k}(T)b_{k}(T)}{\\lambda_{k}}=\\frac{1}{1+\\left(\\frac{\\lambda_{k}}{a_{i}(0)b_{i}(0)}-1\\right)e^{-2\\eta\\lambda_{k}t}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the analytic equation of the multilinear model, Saxe et al. [19] have empirically demonstrated that the dynamics of both linear and nonlinear neural networks closely resemble that of the multilinear model (Eq. (25)). ", "page_idx": 16}, {"type": "text", "text": "C Derivation of the multilinear model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide derivations of how the skill loss of our multilinear model evolves with a given resource: time (Lemma 1), data (Corollary 1), and parameters (Corollary 2). Note that two corollaries for data and parameters (Corollaries 1 and 2) follow from the decoupled dynamics (Lemma 1). ", "page_idx": 17}, {"type": "text", "text": "C.1 Decoupled dynamics of the multilinear model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 1. Let the multilinear model Eq. (9) be trained with gradient flow on $D$ i.i.d samples for the setup in Section 2 (input distribution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5)). Let $k\\leq N$ be a skill index in the multilinear model and the input distribution $(k\\leq n_{s})$ . Then assuming the following initialization $a_{k}(0)=b_{k}(0)$ and $0<a_{k}(0)b_{k}(0)<S$ , the dynamics of the $k^{t h}$ skill strength $(\\mathcal{R}_{k})$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(T)=\\frac{S}{1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)e^{-2\\eta S\\frac{d_{k}}{D}T}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the skill loss is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(T)=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta S\\frac{d_{k}}{D}T}\\right)^{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where \u03b7 is the learning rate and $d_{k}$ is the number of observations with $g_{k}(I=k,x^{(j_{k})})\\neq0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof For $j\\,=\\,1,\\cdot\\cdot\\cdot\\,,D$ , denote $(i^{(j)},x^{(j)})$ be the $j^{t h}$ data point in the training set. Then the empirical loss for $D$ datapoints is given as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}^{(D)}=\\frac{1}{2D}\\sum_{j=1}^{D}\\left(f^{*}(i^{(j)},x^{(j)})-f(i^{(j)},x^{(j)})\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Big(f^{*}(i^{(j)},x^{(j)})-f(i^{(j)},x^{(j)})\\Big)^{2}=\\left(\\displaystyle\\sum_{k=1}^{n_{s}}(S-a_{k}b_{k})g_{k}(i^{(j)},x^{(j)})\\right)^{2}}\\\\ &{}&{=(S-a_{i^{(j)}}b_{i^{(j)}})^{2}g_{i^{(j)}}(i^{(j)},x^{(j)})^{2}}\\\\ &{}&{=(S-a_{i^{(j)}}b_{i^{(j)}})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as $g_{i}(i,j)\\in\\{1,-1\\}$ and $g_{k}(i,j)=0$ for $i\\neq k$ . So if we denote $d_{k}$ the number of data points with $i^{(j)}=k$ , then we can conclude ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}^{(D)}=\\frac{1}{2D}\\sum_{j=1}^{D}(S-a_{i^{(j)}}b_{i^{(j)}})^{2}=\\frac{1}{2D}\\sum_{k=1}^{n_{s}}d_{k}(S-a_{k}b_{k})^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the decoupled loss in the main text (Eq. (11)). Using the gradient descent equation and Eq. (29), we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d a_{k}}{d t}=-\\eta\\frac{d\\mathcal{L}_{D}}{d a_{k}}}\\\\ {\\displaystyle=-\\eta\\frac{d_{k}}{D}b_{k}(a_{k}b_{k}-S).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Likewise, we can obtain the equation for $b_{k}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d b_{k}}{d t}=-\\eta\\frac{d_{k}}{D}a_{k}(a_{k}b_{k}-S).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Because of symmetry between $a$ and $b$ (See Appendix B.2 or [19]), assuming $a_{k}(0)=b_{k}(0)$ , and $a_{k}(0)b_{k}(0)\\overset{.}{>}0$ results in $a_{k}(T)=b_{k}(T)$ for all $T$ . The equation for $\\mathcal{R}_{k}=a_{k}b_{k}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\frac{d{\\mathcal{R}}_{k}}{d t}=-\\eta\\frac{d a_{k}}{d t}b_{k}+a_{k}\\frac{d b_{k}}{d t}=-\\eta\\frac{d_{k}}{D}(b_{k}^{2}+a_{k}^{2})(a_{k}b_{k}-S)}}\\\\ {=-2\\eta\\frac{d_{k}}{D}{\\mathcal{R}}_{k}({\\mathcal{R}}_{k}-S).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assuming $a_{k}(0)b_{k}(0)<S$ , we can solve the differential equation to obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(T)=\\frac{S}{1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)e^{-2\\eta S\\frac{d_{k}}{D}T}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The equation for $\\mathcal{L}_{k}$ follows from Eq. (10). ", "page_idx": 18}, {"type": "text", "text": "C.2 One-shot learner ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Corollary 1. For the setup in Lemma $^{\\,l}$ , the $k^{t h}$ skill loss $(\\mathcal{L}_{k})$ at $T,N\\to\\infty$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\infty)=\\left\\{(S-\\mathcal{R}_{k}(0))^{2}/2\\approx S^{2}/2\\quad:d_{k}>0\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $d_{k}$ is the number of $k^{t h}$ skill\u2019s observations. ", "page_idx": 18}, {"type": "text", "text": "Proof The corollary follows directly from Lemma 1. By taking $T,N\\to\\infty$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(\\infty)=\\left\\{\\mathcal{S}\\begin{array}{l l}{\\displaystyle{S}}&{\\displaystyle:d_{k}>0}\\\\ {\\displaystyle{\\mathcal{R}_{k}(0)}}&{\\displaystyle:d_{k}=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We obtain the result by using the relationship between $\\mathcal{R}_{k}$ and $\\mathcal{L}_{k}$ in Eq. (10). ", "page_idx": 18}, {"type": "text", "text": "C.3 Equivalence between a basis function and a skill ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Corollary 2. Let the multilinear model Eq. (9) be trained with gradient flow on $D$ i.i.d samples for the setup in Section $^3$ (input distribution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5)). Assume $a_{k}(0)=b_{k}(0)$ , $\\bar{0}<a_{k}(0)b_{k}(0)<\\bar{S},$ , and that the model has the $N$ most frequent skills as basis functions. Then $\\mathcal{R}_{k}$ for the $k^{t h}\\leq n_{s}$ skill at $T,D\\to\\infty$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{k}(\\infty)=\\left\\{{\\underset{S^{2}/2}{0}}\\quad:k\\leq N\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof The corollary follows directly from Lemma 1. By taking $T,D\\to\\infty$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(\\infty)=\\left\\{\\mathcal{S}\\begin{array}{c l}{S}&{:k\\leq N}\\\\ {\\vdots\\,k>N}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We obtain the result by using the relationship between $\\mathcal{R}_{k}$ and $\\mathcal{L}_{k}$ in Eq. (10) and $\\mathcal{R}_{k}(0)\\ll S$ . ", "page_idx": 18}, {"type": "text", "text": "D Stage-like training: intuitive derivation of the scaling laws ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Even though we provide more detailed (Appendix E) and rigorous (Appendix J) derivation of the scaling laws, a less general yet more intuitive solution aids in understanding the scaling laws of our model and NNs. In this section, we define stage-like training \u2013 one skill is completely learned before the next skill initiates learning (Fig. 6(a)) \u2013 and state the conditions for it to occur. We provide an example of how stage-like training results in the time scaling law and explain how the model in Michaud et al. [18] may arise from the NN dynamics. Finally, we discuss the stage-like training\u2019s role in emergence in NNs. ", "page_idx": 19}, {"type": "image", "img_path": "cuWsR25bbI/tmp/856c4df4751e796ab4477ac25268653592bfd6a3ee0d52306f0641b594952d07.jpg", "img_caption": ["(a) Emergent and saturation time "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cuWsR25bbI/tmp/f0088831e0d062043418411b25210cd077039de461d52c5548ac30e57320cb63.jpg", "img_caption": ["(b) Loss change between emergences "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Stage-like training. The multilinear model is trained on the multitask sparse parity problem with $\\alpha\\,=\\,0.6$ and $S\\,=\\,5$ . (a): Skill strength of the model as a function of time. The emergent time $\\tau_{k}^{(e)}(\\epsilon)$ is the time required for the $k^{t h}$ sk to $\\mathcal{R}_{k}/S=\\epsilon$ . The saturation time $\\tau_{k}^{(s)}(\\epsilon)$ is the time required for to saturate from $\\epsilon$ $1-\\epsilon$ . The model shows stage-like training if the emergent time interval $\\tau_{k+1}^{(e)}(\\epsilon)-\\tau_{k}^{(e)}(\\epsilon)$ is larger than the saturation time $\\tau_{k}^{(s)}(\\epsilon)$ for sufficiently small (0.05 in the figure). (b): The loss as a function of time for the same system as (a). For stage-like training, the change in the loss for the $k^{t h}$ emergence is $\\mathcal{P}_{s}(k)\\mathcal{L}_{k}+\\mathcal{O}(\\epsilon)$ and the interval for the next emergence is $\\Delta\\tau^{(e)}(\\epsilon)=\\tau_{k+1}^{(e)}(\\epsilon)-\\tau_{k}^{(e)}(\\epsilon)$ . ", "page_idx": 19}, {"type": "text", "text": "D.1 Stage-like training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "When a model exhibits an emergence behavior \u2013 when saturation of skill occurs abruptly after a delay \u2013 and the intervals between each emergence are sufficiently large, the model admits stage-like training. The multilinear model (sigmoidal saturation of skills strength, Eq. (11)) in the multitask sparse parity dataset (power-law decay of skill frequencies, Eq. (1)) can satisfy such conditions: In Fig. 6(a), we observe the stage-like training in time in which one skill saturates (reaches $\\mathcal{R}_{k}/S\\approx1\\rangle$ before the next skill initiates its emergence. To quantify this behavior, we define two intervals for each skill (see Fig. 6(a)): ", "page_idx": 19}, {"type": "text", "text": "\u2022 The emergent time $\\tau_{k}^{(e)}(\\epsilon)$ : the time for $\\mathcal{R}_{k}/S$ to reach $\\epsilon$ ;   \n\u2022 The saturation time $\\tau_{k}^{(s)}(\\epsilon)$ : the time for $\\mathcal{R}_{k}/S$ to saturate from $\\epsilon$ to $1-\\epsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Using the dynamics equation (Eq. (11)) and that $d_{k}/D\\to\\mathcal{P}_{s}(k)$ , the emergent time and saturation time of the $k^{t h}$ skill becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau_{k}^{(e)}(\\epsilon)=\\frac{1}{2\\eta\\mathcal{P}_{s}(k)S}\\ln\\left(\\frac{\\frac{S}{\\mathcal{R}_{k}(0)}-1}{\\frac{1}{\\epsilon}-1}\\right)\\propto k^{\\alpha+1},\\qquad\\tau_{k}^{(s)}(\\epsilon)=\\frac{1}{\\eta\\mathcal{P}_{s}(k)S}\\ln\\left(\\frac{1}{\\epsilon}-1\\right)\\propto k^{\\alpha+1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For sufficiently small initialization $(\\mathcal{R}_{k}(0)\\ll S)$ , we get a stage-like training: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau_{k}^{(s)}(\\epsilon)<\\tau_{k+1}^{(e)}(\\epsilon)-\\tau_{k}^{(e)}(\\epsilon),\\qquad\\epsilon\\ll1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the model finishes learning (saturating) the $k^{t h}$ skill before starting to learn (emerging) the next skill. ", "page_idx": 19}, {"type": "text", "text": "D.2 Time scaling law from stage-like training ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Assuming our model satisfies the stage-like training for all $k$ of interest, we can derive the time scaling law from the stage-like training. ", "page_idx": 20}, {"type": "text", "text": "At $\\tau_{k}^{(e)}(\\epsilon)$ , because of stage-like training, all skills with index up to but not including $k$ have saturated $(\\mathcal{R}_{i<k}\\,\\approx\\,S)$ , or equivalently $\\mathcal{L}_{i<k}\\,\\approx\\,0$ (Eq. (10)). The total loss, the sum of ${\\mathcal{L}}_{j}$ weighted by $\\mathcal{P}_{s}(j)\\propto j^{-(\\alpha+1)}$ (Eq. (6)), becomes $\\begin{array}{r}{\\sum_{j=k}^{\\infty}\\mathcal{P}_{s}(I=j)S^{2}/2}\\end{array}$ (Fig. 6(b)). The saturation of the $k^{t h}$ skill results in a loss difference of $\\mathcal{P}_{s}(I=k)S^{2}/2$ . Thus, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\Delta\\mathcal{L}}{\\mathcal{L}}}\\approx{\\frac{\\mathcal{P}_{s}(I=k)}{\\sum_{j=k}^{\\infty}\\mathcal{P}_{s}(I=j)}}=-{\\frac{k^{-(\\alpha+1)}}{\\sum_{j=k}^{\\infty}j^{-(\\alpha+1)}}}\\approx-{\\frac{k^{-(\\alpha+1)}}{\\int_{k}^{\\infty}j^{-(\\alpha+1)}d j}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Accordingly, the emergent interval between the $k$ and $k+1$ skills relative to the $\\tau_{k}^{(e)}(\\epsilon)$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\Delta T}{T}=\\frac{\\tau_{k+1}^{(e)}(\\epsilon)-\\tau_{k}^{(e)}(\\epsilon)}{\\tau_{k}^{(e)}(\\epsilon)}=\\frac{(k+1)^{\\alpha+1}-k^{\\alpha+1}}{k^{\\alpha+1}}}\\\\ {\\displaystyle=(\\alpha+1)k^{-1}+\\mathcal{O}(k^{-2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assuming $k\\gg1$ and combining Eq. (43) and Eq. (45) to the largest order, we have the equation for the power-law with exponent $-\\alpha/(\\alpha+1)$ in Fig. 2(a): ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\Delta{\\mathcal{L}}}{\\mathcal{L}}}=-{\\frac{\\alpha}{\\alpha+1}}{\\frac{\\Delta T}{T}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If the stage-like training holds for any resource (e.g., time, data, or parameters), the scaling law can be derived using the ratio of change in loss per skill (Eq. (43)) and the ratio of change with respect to the resource (given by the emergent time in Eq. (45)). The quanta model in Michaud et al. [18] is an example where the stage-like training holds for all resources. ", "page_idx": 20}, {"type": "text", "text": "D.3 Discussion on the effective decoupling of skills in neural networks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 5, we have empirically demonstrated that the multilinear model predicts the emergence of a 2-layer NN (Fig. 1). In Section 6, we briefly discussed why NNs, despite their lack of the decoupling among the skills, behave similarly to the decoupled model with $g_{k}\\mathbf{s}$ as fixed basis functions: the stage-like training in NNs \u2013 induced by the model\u2019s layerwise structure and power-law frequencies of the skills \u2013 effectively decouples the skills. In this subsection, we extend the discussion in more detail. ", "page_idx": 20}, {"type": "text", "text": "In NNs, even though $g_{k}\\mathbf{s}$ are \u2018discovered\u2019 (feature learned) by non-tractable dynamics, we speculate that similar stage-like dynamics also hold in \u2018discovering\u2019 (feature learning) $g_{k}\\mathbf{s}$ : parameters \u2018useful\u2019 for expressing more frequent skills will be updated significantly faster than parameters useful for expressing less frequent skills. ", "page_idx": 20}, {"type": "text", "text": "If skill discovery and saturation dynamics operate at different time scales (stages), with negligible interaction among the skills, the skill dynamics become effectively decoupled. Because the dynamics are decoupled in stages, NNs repeat the feature learning process \u2013 using the limited resource (time, data, parameters) to express the skill \u2013 for all skills with each iteration varying only in the scale of the resource (e.g. training time, number of observations, and number of hidden layer neurons): resulting in a similar emergence to our multilinear model. ", "page_idx": 20}, {"type": "text", "text": "A more concrete understanding of our speculation that feature learning also occurs in stages due to a layerwise structure is left for future work. ", "page_idx": 20}, {"type": "text", "text": "E Derivation of the scaling law exponents ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section provides a detailed derivation of the scaling laws up to a rigor common in physics and engineering. For example, we approximate the Riemann sum as integral or treat $k$ , the number of ", "page_idx": 20}, {"type": "text", "text": "skills, as a differentiable parameter. For more general and rigorous derivations including the prefactor constants, see Appendix J. Instead, for more intuition and the relationship to the quanta model in Michaud et al. [18], see Appendix D. ", "page_idx": 21}, {"type": "table", "img_path": "cuWsR25bbI/tmp/9896de91a1b3b824acca37adfff168a1b8f632522fa6c24a094434b90287b0c8.jpg", "table_caption": ["Table 4: Summary of the scaling laws. The leftmost column shows the bottleneck of the scaling law. The middle three columns show the resource values in terms of the bottleneck (either taken to infinity or proportional to the bottleneck). The last column shows the scaling exponent for the loss as power-law of the bottleneck where $\\alpha+1$ is the exponent of the Zipfian input data (Eq. (1)). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.1 Time scaling law exponent ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To derive the time scaling law exponent, we assume the time as the bottleneck and take $N,D\\to\\infty$ . By using the decoupled dynamics of each skill loss (Lemma 1), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta\\frac{d_{k}}{D}S T}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noting that $d_{k}/D\\to\\mathcal{P}_{s}(k)$ as $D\\rightarrow\\infty$ , where $\\mathcal{P}_{s}(k)=A k^{-(\\alpha+1)}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta A k^{-(\\alpha+1)}S T}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This is a function of $k^{-(\\alpha+1)}T$ only, suggesting the decoupling dynamics for each skill. Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{d{\\mathcal{L}}_{k}}{d T}}=-{\\frac{k}{(\\alpha+1)T}}{\\frac{d{\\mathcal{L}}_{k}}{d k}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using Eq. (6) and taking $N,n_{s}\\to\\infty$ at the same rate,4 we can approximate the loss as an integral instead of a sum over $k$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}\\approx\\operatorname*{lim}_{N\\to\\infty}\\int_{1}^{N}A k^{-(\\alpha+1)}\\mathcal{L}_{k}d k,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $A$ is the normalization constant for $\\mathcal{P}_{s}$ . We can differentiate the loss and use Eq. (49) to express the equation in terms of $k$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{d{\\mathcal{L}}}{d T}}=\\operatorname*{lim}_{N\\to\\infty}\\int_{1}^{N}A k^{-(\\alpha+1)}{\\frac{d{\\mathcal{L}}_{k}}{d T}}d k=-\\operatorname*{lim}_{N\\to\\infty}{\\frac{1}{(\\alpha+1)T}}\\int_{1}^{N}A k^{-\\alpha}{\\frac{d{\\mathcal{L}}_{k}}{d k}}d k.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Integrating by parts, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\mathcal{L}}{d T}=-\\operatorname*{lim}_{N\\to\\infty}\\displaystyle\\frac{1}{(\\alpha+1)T}\\left[A k^{-\\alpha}\\mathcal{L}_{k}\\right]_{1}^{N}-\\operatorname*{lim}_{N\\to\\infty}\\displaystyle\\frac{\\alpha}{(\\alpha+1)T}\\int_{1}^{N}A k^{-(\\alpha+1)}\\mathcal{L}_{k}d k}\\\\ {\\displaystyle\\qquad=-\\operatorname*{lim}_{N\\to\\infty}\\mathcal{O}\\left(N^{-\\alpha}\\displaystyle\\frac{1}{T}\\right)+\\mathcal{O}\\left(\\displaystyle\\frac{1}{T e^{T}}\\right)-\\displaystyle\\frac{\\alpha}{(\\alpha+1)T}\\mathcal{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first term goes to 0 as $N\\rightarrow\\infty$ and the second term goes to 0 exponentially faster compared to the last term for $T\\gg1$ , which leads to the scaling law with exponent $-\\alpha/(\\alpha+1)$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{d{\\mathcal{L}}(T)}{{\\mathcal{L}}(T)}}=-{\\frac{\\alpha}{\\alpha+1}}{\\frac{d T}{T}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finite $_{N}$ correction for small $_{\\alpha}$ . In Fig. 7, we observe that our model with $\\alpha=0.1$ deviates from the expected power-law with exponent $-\\alpha/(\\alpha+1)$ . The deviation can be explained by the antiderivative term in Eq. (52): ", "page_idx": 22}, {"type": "image", "img_path": "cuWsR25bbI/tmp/f3891a85a0b7c9ec54df83f26541bd8bd04d7bb091ef12915c331da4370b2b4c.jpg", "img_caption": ["Figure 7: Scaling law and corrected predictions. A simulation of our multilinear model with $N\\,=\\,50,000$ (solid), a scaling law with exponent $-\\alpha/(\\alpha+1)$ (dotted), and a corrected scaling law considering finite $N$ (dashed, Eq. (56)). The finite $N$ corrected scaling law better predicts the dynamics, especially for smaller $\\alpha$ . "], "img_footnote": [], "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\ell\\to\\infty}\\left[{\\frac{1}{2(\\alpha+1)}}{\\frac{S^{2}A}{\\left(1+{\\frac{1}{S/R_{k}(0)-1}}e^{2\\eta S A k^{-(\\alpha+1)}T}\\right)^{2}}}{\\frac{k^{-\\alpha}}{T}}\\right]_{1}^{N}=\\operatorname*{lim}_{N\\to\\infty}\\left({\\mathcal{O}}\\left(N^{-\\alpha}{\\frac{1}{T}}\\right)-{\\mathcal{O}}\\left({\\frac{1}{T e^{T}}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second term ( $\\;k=1\\;\\;$ ) goes to 0 faster than ${\\mathcal{O}}(T^{-1})$ for sufficiently larger $T$ but the first term $\\mathit{k}=N)$ may not decay fast enough for finite $N$ and sufficiently small $\\alpha$ . For example, $N=50,000$ and $\\alpha=0.1$ leads to $\\dot{N}^{-\\alpha}\\approx0.3$ , which is not negligibly small. ", "page_idx": 22}, {"type": "text", "text": "Assuming finite $N$ and small $\\alpha$ such that the first term in Eq. (55) is non-negligible, we can rewrite Eq. (52) as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{L}}{d T}\\approx-\\frac{\\alpha}{(\\alpha+1)}\\frac{\\mathcal{L}+\\mathcal{L}_{C}}{T},\\ \\ \\ \\ \\ \\ \\mathcal{L}_{C}\\approx S^{2}A N^{-\\alpha}/2\\alpha,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we assumed a small initialization $S/\\mathcal{R}_{k}(0)\\gg1$ and sufficiently large number of parameters $N^{\\alpha+1}\\gg T$ to approximate $\\mathcal{L}_{C}$ . Because the total loss at initialization is ${\\mathcal{L}}(0)\\,=\\,S^{\\dot{2}}/2$ , $\\mathcal{L}_{C}$ is non-negligible compared to the loss for sufficiently small $\\alpha$ . Thus considering $\\mathcal{L}_{C}$ , we obtain the corrected power-law which better approximates the time scaling law (dashed lines in Fig. 7). For a rigorous and comprehensive analysis of the time scaling law, see Theorem 2 and Theorem 3 in Appendix J. ", "page_idx": 22}, {"type": "text", "text": "E.2 Data scaling law exponent ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we derive the data scaling law exponent. The data scaling law assumes $T\\rightarrow\\infty$ and $N\\rightarrow\\infty$ with data as the bottleneck. From the decoupled dynamics of the multilinear model (Lemma 1), we can show that our model is a one-shot learner (Corollary 1): ", "page_idx": 22}, {"type": "text", "text": "One shot learner. Given that $N>k$ , $T\\rightarrow\\infty$ , and $d_{k}$ is the number of samples from the training set with $g_{k}(i,x)\\neq0$ , the $k^{t h}$ skill loss after training is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\infty)=\\left\\{\\begin{array}{c c}{0}&{:d_{k}>0}\\\\ {(S-\\mathcal{R}_{k}(0))^{2}/2\\approx S^{2}/2}&{:d_{k}=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof See Appendix C.2. ", "page_idx": 22}, {"type": "text", "text": "Our model requires only one sample from the $k^{t h}$ skill to learn such a skill, similar to how language models are few-shot learners at inference.5 The model can one-shot learn a skill since it has $g_{k}$ as the basis functions, and the dynamics among different skills are decoupled. A similar one-shot learner has been studied in Hutter [20] where the error depends on a single \u2018observation\u2019 of a feature. ", "page_idx": 23}, {"type": "text", "text": "Because the $k^{t h}$ skill loss only depends on $d_{k}$ (number of observations for the $k^{t h}\\mathrm{~skill}$ ), we can calculate the expectation of the skill loss for $D$ data points from $P_{o b s e r v e d}(k|D)$ or the probability that $d_{k}>0$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{o b s e r v e d}(k|D)=1-\\left(1-\\mathcal{P}_{s}(k)\\right)^{D}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the one-shot learning property (Eq. (57)), the probability of observing the $k^{t h}$ skill (Eq. (58)), and the decomposition of the loss into skill losses (Eq. (6)), the expected loss for $D$ datapoints is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf E}_{D}\\left[\\mathcal{L}\\right]=\\frac{1}{2}\\sum_{k=1}^{\\infty}S^{2}\\mathcal{P}_{s}(k)(1-P_{o b s e r v e d}(k))}}\\\\ {~~}\\\\ {{\\displaystyle~~~~~~=\\frac{1}{2}S^{2}A\\sum_{k=1}^{\\infty}k^{-(\\alpha+1)}\\left(1-\\mathcal{P}_{s}(k)\\right)^{D}}}\\\\ {~~}\\\\ {{\\displaystyle~~~~~~\\approx\\frac{1}{2}S^{2}A\\int_{1}^{\\infty}k^{-(\\alpha+1)}\\left(1-A k^{-(\\alpha+1)}\\right)^{D}d k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the expectation $\\mathbf{E}_{D}$ is over all possible training sets of size $D$ , and $A$ is the normalization constant such that $\\mathcal{P}(k)=A k^{-(\\alpha+1)}$ . The difference in the loss $\\Delta\\mathcal{L}=\\mathbf{E}_{D+1}\\left[\\mathcal{L}\\right]-\\mathbf{E}_{D}\\left[\\mathcal{L}\\right]$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Delta\\mathcal{L}=\\frac{1}{2}S^{2}A\\int_{1}^{\\infty}k^{-(\\alpha+1)}\\left(1-A k^{-(\\alpha+1)}\\right)^{D}\\left(\\left(1-A k^{-(\\alpha+1)}\\right)-1\\right)d k}}\\\\ {{\\mathrm{}=-\\frac{1}{2}S^{2}A^{2}\\int_{1}^{\\infty}k^{-2(\\alpha+1)}\\left(1-A k^{-(\\alpha+1)}\\right)^{D}d k.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can integrate $\\Delta\\mathcal{L}$ by parts. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\lambda\\mathcal{L}=\\frac{1}{2}\\left[-\\frac{S^{2}A k^{-\\alpha}}{(\\alpha+1)(D+1)}\\left(1-A k^{-(\\alpha+1)}\\right)^{D+1}\\right]_{1}^{\\infty}}\\\\ &{\\qquad\\qquad-\\,\\frac{S^{2}A\\alpha}{2(\\alpha+1)(D+1)}\\int_{1}^{\\infty}k^{-(\\alpha+1)}\\left(1-A k^{-(\\alpha+1)}\\right)^{D+1}d k}\\\\ &{\\approx\\mathcal{O}\\left((1-\\mathcal{P}_{s}(1))^{D+1}\\right)-\\frac{S^{2}A\\alpha}{2(\\alpha+1)(D+1)}\\int_{1}^{\\infty}k^{-(\\alpha+1)}\\left(1-A k^{-(\\alpha+1)}\\right)^{D}\\left(1-A k^{-(\\alpha+1)}\\right)}\\\\ &{\\approx-\\displaystyle\\frac{\\alpha}{(\\alpha+1)(D+1)}\\mathbf{E}_{D}\\left[\\mathcal{L}\\right]+\\frac{\\alpha}{(\\alpha+1)(D+1)}\\Delta\\mathcal{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the second line, the first term goes to 0 for $D\\gg1$ . In the last line, we used the expression for $\\Delta\\mathcal{L}$ (Eq. (62)) and $\\mathbf{E}_{D}\\left[\\mathcal{L}\\right]$ (Eq. (59)). Rearranging the equation above and using that $D\\gg1$ , we obtain the scaling law with exponent $-\\alpha/(\\alpha+1)$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Delta\\mathcal{L}}{\\mathbf{E}_{D}\\left[\\mathcal{L}\\right]}=-\\frac{\\alpha}{1+\\left(\\alpha+1\\right)D}\\approx-\\frac{\\alpha}{\\left(\\alpha+1\\right)}\\frac{1}{D}}\\\\ &{\\qquad\\quad=-\\frac{\\alpha}{\\left(\\alpha+1\\right)}\\frac{\\Delta D}{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the last line, $\\Delta D/D=1/D$ as the change in the number of data points relative to $D$ is one. ", "page_idx": 23}, {"type": "text", "text": "E.3 Parameter scaling law exponent ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The parameter scaling law assumes $T\\to\\infty$ and $D\\to\\infty$ , with the parameters $N<n_{s}$ as the bottleneck. Because our model is a one-shot learner (Eq. (57)), learning of the $k^{t h}$ skill only depends on the existence of $g_{k}$ in the model; the model with $\\left[g_{1},\\cdot\\cdot\\cdot\\,,g_{N}\\right]$ will learn all $k\\leq N$ skills with $\\mathcal{L}_{k}=0$ . ", "page_idx": 23}, {"type": "text", "text": "The $\\mathcal{L}_{k}$ dependence on $g_{k}$ is formalized in Corollary 2, which we repeat here. ", "page_idx": 23}, {"type": "text", "text": "Equivalence between a basis function and a skill. Given $T,D\\to\\infty$ and if the multilinear model has the $N$ most frequent skill functions as a basis, ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\infty)=\\left\\{\\begin{array}{c l}{0}&{:k\\leq N}\\\\ {S^{2}/2}&{:k>N.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof See Appendix C.3. ", "page_idx": 24}, {"type": "text", "text": "Using Eq. (66) and Eq. (6), we can express the total loss as function of $N$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{L}}\\approx{\\frac{S^{2}}{2}}\\int_{N+1}^{\\infty}A k^{-(\\alpha+1)}d k\\propto(N+1)^{-\\alpha}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By approximating $N\\approx N+1$ for $N\\gg1$ , we obtain the power-law with exponent $-\\alpha$ . ", "page_idx": 24}, {"type": "text", "text": "E.4 Optimal compute scaling law ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For analytical tractability, we define compute as $C:=T\\times N$ . We start from Eq. (12) with $D\\rightarrow\\infty$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathscr{L}\\approx\\int_{1}^{N}A k^{-(\\alpha+1)}\\mathscr{L}_{k}d k+\\operatorname*{lim}_{n_{s}\\rightarrow\\infty}\\frac{S^{2}}{2}\\int_{N}^{n_{s}}A k^{-(\\alpha+1)}d k.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can use Eq. (56) to calculate the first term and integrate the last term to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal{L}}\\approx({\\mathcal{L}}(0)+{\\mathcal{L}}_{C})T^{-\\alpha/(\\alpha+1)}-{\\mathcal{L}}_{c}+\\frac{S^{2}A}{2\\alpha}N^{-\\alpha}}}\\\\ {{\\displaystyle\\approx{\\mathcal{O}}(T^{-\\alpha/(\\alpha+1)})+{\\mathcal{O}}(N^{-\\alpha}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used that $\\mathcal{L}(0)\\gg\\mathcal{L}_{C}$ and $S^{2}A/(2\\alpha)-{\\mathcal{L}}_{C}>0$ . Intuitively, the approximation shows the tradeoff between $T-$ when increased, decreases the loss of the first $N$ skills \u2013 and $N$ \u2013 when increased, decreases the loss at sufficiently large $T-$ for fixed compute $C$ . For a comprehensive analysis of the approximation above, see Appendix $\\mathrm{\\bfJ}$ . ", "page_idx": 24}, {"type": "text", "text": "Removing the irrelevant constant terms, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}=T^{-\\alpha/(\\alpha+1)}+N^{-\\alpha}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can use the method of Lagrangian multiplier to obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\frac{\\alpha}{\\alpha+1}T^{-\\alpha/(\\alpha+1)-1}+\\lambda N=0,}\\\\ {-\\alpha N^{-(\\alpha+1)}+\\lambda T=0,}\\\\ {N T-C=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\lambda$ is the Lagrange multiplier and $C$ is compute. We can solve the above set of equations to obtain $T^{\\alpha+1}\\propto\\bar{N}$ or equivalently ", "page_idx": 24}, {"type": "equation", "text": "$$\nT\\propto C^{(\\alpha+1)/(\\alpha+2)},\\quad N\\propto C^{1/(\\alpha+2)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can plug it in Eq. (71) to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{L}}\\propto C^{-\\alpha/(\\alpha+2)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This derivation is similar to that of Bordelon et al. [21] (see Appendix N: Compute Optimal Scaling from Sum of Power-Laws in [21]). For a rigorous derivation of the optimal compute scaling law, see Corollary 4 and Appendix J. ", "page_idx": 24}, {"type": "text", "text": "F Derivation of the extended multilinear model ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we show the derivation for the extended multilinear model. ", "page_idx": 25}, {"type": "text", "text": "F.1 Gradient flow in the extended multilinear model ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 2. Let the extended multilinear model Eq. (15) be trained with gradient flow on D i.i.d samples for the setup in Section 2 (input distribution: $E q$ . (1), target function: Eq. (4), and MSE loss: Eq. (5)). For the skill index $k\\leq N$ be a skill index in the multilinear model, let the feature matrix $\\bar{\\Phi^{\\prime}}\\in\\mathbb{R}^{D_{c}\\times d_{k}}$ for the $k^{t h}$ skill be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Phi_{l j}=e_{k,l}\\bigl(i^{(j)}=k,x^{(j)}\\bigr),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and SVD on $\\Phi=U S V$ . Assuming that the system is overparametrized $(d_{k}<D_{c})$ , the gradient on $\\vec{B}_{k}\\,\\in\\,\\mathbb{R}^{D_{c}}\\,([B_{k,1},\\cdot\\cdot\\cdot\\cdot,B_{k,D_{c}}])$ is contained in the column space of semi-orthogonal matrix $U\\in\\mathbb{R}^{D_{c}\\times d_{k}}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nU U^{T}\\frac{d\\vec{B_{k}}}{d t}=\\frac{d\\vec{B_{k}}}{d t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof Similar to Lemma 1, the total loss can be decomposed into each skill such that the dynamics of $B_{k,l}$ relies only on $d_{k}$ observations of the $k^{t h}$ skill: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{D}=\\displaystyle\\frac{1}{2D}\\displaystyle\\sum_{k=1}^{n_{s}}\\sum_{j=1}^{D}\\left(f^{*}(i^{(j)},x^{(j)})-f(i^{(j)},x^{(j)})\\right)^{2}}\\\\ &{\\quad=\\displaystyle\\frac{1}{2D}\\displaystyle\\sum_{k=1}^{n_{s}}\\sum_{j_{k}=1}^{d_{k}}\\left(S g_{k}(k,x^{(j_{k})})-\\displaystyle\\sum_{l=1}^{D_{c}}a_{k}B_{k,l}e_{k,l}(k,x^{(j_{k})})\\right)^{2}}\\\\ &{\\quad=\\displaystyle\\frac{1}{2D}\\displaystyle\\sum_{k=1}^{n_{s}}\\sum_{j_{k}=1}^{d_{k}}\\left(\\sum_{l=1}^{D_{c}}(\\frac{S}{\\sqrt{D_{c}}}-a_{k}B_{k,l})e_{k,l}(k,x^{(j_{k})})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the second line, we used Eq. (16) that $e_{k,l}(I\\neq k,x)=0$ and the orthogonality of $g_{k}$ (Eq. (3)). In the last line, we used Eq. (16) that $g_{k}=D_{c}{}^{-1/2}\\sum_{l}e_{k,l}$ . We can find the gradient descent equation of $B_{k,l}$ from Eq. (81): ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{d B_{k,l}}{d t}=-\\eta\\sum_{j=1}^{d_{k}}\\frac{1}{D}\\left[a_{k}e_{k,l}(k,x^{(j)})\\sum_{l^{\\prime}=1}^{D_{c}}(a_{k}B_{k,l^{\\prime}}-\\frac{S}{\\sqrt{D_{c}}})e_{k,l^{\\prime}}(k,x^{(j)})\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which in the matrix form is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{d\\vec{B}_{k}}{d t}=-\\frac{\\eta a_{k}}{D}\\Phi\\Phi^{T}\\left(B_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $D_{c}$ dimensional vectors $\\vec{B_{k}}$ and $\\vec{S}$ are $\\left[B_{k,1},\\cdot\\cdot\\cdot,B_{k,D_{c}}\\right]$ and $[S,\\cdot\\cdot\\cdot\\,,S]$ respectively. It illustrates that $\\frac{d B_{k}}{d t}$ is contained in $\\mathrm{im}(\\Phi)$ , which is contained in $\\mathrm{im}(U)$ (immediate from $\\Phi=U S V$ As $U U^{T}(U z)=U(U^{T}U)z=U z,$ , $U U^{T}$ acts as identity on image of $U$ , showing that $U U^{T}\\frac{d\\vec{B}_{k}}{d t}=$ $\\frac{d\\vec{B}_{k}}{d t}$ ", "page_idx": 25}, {"type": "text", "text": "F.2 Conserved quantity of extended multilinear model ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 3. In the setup of Lemma 2, $a_{k}^{2}-|\\vec{B}_{k}|^{2}$ is conserved over time. ", "page_idx": 25}, {"type": "text", "text": "Proof We can use Eq. (81) to find the equation for $a_{k}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{d a_{k}}{d t}=-\\eta\\sum_{j=1}^{d_{k}}\\frac{1}{D}\\left[\\sum_{l=1}^{}B_{k,l}e_{k,l}(k,x^{(j)})\\sum_{l^{\\prime}=1}^{D_{c}}(a_{k}B_{k,l^{\\prime}}-\\frac{S}{\\sqrt{D_{c}}})e_{k,l^{\\prime}}(k,x^{(j)})\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which in the matrix form is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{d a_{k}}{d t}=-\\frac{\\eta}{D}\\vec{B}_{k}^{T}\\Phi\\Phi^{T}\\left(\\vec{B}_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{a_{k}\\displaystyle\\frac{d a_{k}}{d t}=-\\displaystyle\\frac{\\eta a_{k}}{D}\\vec{B}_{k}^{T}\\Phi\\Phi^{T}\\left(\\vec{B}_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right)}}\\\\ {{=\\displaystyle\\vec{B}_{k}^{T}\\frac{d\\vec{B}_{k}}{d t},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used Eq. (83) in the last line. Thus, $a_{k}^{2}-|\\vec{B_{k}}|^{2}$ is conserved during the dynamics. ", "page_idx": 26}, {"type": "text", "text": "F.3 $D_{c}$ shot learner ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition 1. Let the setup be as that in Lemma 2. Suppose that $a_{k}(T)$ is eventually bounded away from zero, i.e. there exists $\\delta>0$ and $M>0$ such that $T>M\\Rightarrow|a_{k}(T)|\\geq\\delta.$ . Also assume that $U^{\\perp}$ -component of $\\vec{B}_{k}(0)a_{k}(0)$ and $\\vec{B}_{k}(0)S$ is negligible. Then the skill strength $\\mathcal{R}_{k}$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(\\infty)=\\left\\{d_{k}<D_{c}:\\begin{array}{c c}{S\\left(1-\\sqrt{1-d_{k}/D_{c}}\\right)}\\\\ {S}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof First, we show that $\\begin{array}{r}{\\frac{d\\mathcal{L}_{k}}{d t}\\leq0}\\end{array}$ with equality only holding when the gradient is 0. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\mathcal{L}_{k}}{d t}=\\frac{d\\mathcal{L}_{k}}{d a_{k}}\\frac{d a_{k}}{d t}+\\sum_{i}^{D_{c}}\\frac{d\\mathcal{L}_{k}}{d B_{k,i}}\\frac{d B_{k,i}}{d t}}\\\\ {\\displaystyle=-\\eta\\frac{d_{k}}{D}\\left(\\frac{d\\mathcal{L}_{k}}{d a_{k}}\\frac{d\\mathcal{L}_{k}}{d a_{k}}+\\sum_{i}^{D_{c}}\\frac{d\\mathcal{L}_{k}}{d B_{k,i}}\\frac{d\\mathcal{L}_{k}}{d B_{k,i}}\\right)\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The equality holds only when ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{L}_{k}}{d a_{k}}=\\frac{d a_{k}}{d t}=0\\quad\\mathrm{and}\\quad\\frac{d\\mathcal{L}_{k}}{d B_{k,i}}=\\frac{d B_{k,i}}{d t}=0\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We show that both $a_{k}$ and $\\vec{B}_{k}$ are bounded throughout whole dynamics. As ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}=\\left|\\Phi\\left(\\vec{B}_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right)\\right|^{2}\\geq\\sigma^{2}\\left|U U^{T}\\left(\\vec{B}_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right)\\right|^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\sigma^{2}$ the smallest nonzero eigenvalue of $\\Phi\\Phi^{T}$ , where $\\Phi=U S V$ . This shows that ", "page_idx": 26}, {"type": "equation", "text": "$$\nU U^{T}\\left(\\vec{B}_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is bounded, so $U U^{T}\\vec{B}_{k}a_{k}$ is bounded. Meanwhile, in Lemma 2, we showed that $\\begin{array}{r}{(1\\!-\\!U U^{T})\\frac{d\\vec{B}_{k}}{d t}=0}\\end{array}$ , so $(1-U U^{T})\\vec{B}_{k}a_{k}$ is bounded. This shows that $\\vec{B}_{k}a_{k}$ is bounded. As $a_{k}^{2}\\mathrm{~-~}|\\vec{B}_{k}|^{2}$ is constant (Lemma 3) and $|\\vec{B}_{k}a_{k}|=|a_{k}||\\vec{B}_{k}|$ is bounded, this shows that both $a_{k}$ and $|\\vec{B}_{k}|$ are bounded. ", "page_idx": 26}, {"type": "text", "text": "The dynamics moving in some bounded region always has at least one accumulation point, which we denote as $p$ . We will show that $\\begin{array}{r}{\\frac{d\\mathcal{L}_{k}}{d t}=0}\\end{array}$ at $p$ . The function $\\mathcal{L}_{k}(t)$ in $t$ is a decreasing differential function which is positive. We also note that d2dLtk2(t)is globally bounded, as it can be expressed in polynomial expression in $(a_{k},\\vec{B}_{k})$ and we showed that $(a_{k}(t),\\vec{B}_{k}(t))$ is bounded. From Taylor\u2019s theorem, one can obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}{\\mathcal{L}}_{k}(t)\\leq{\\mathcal{L}}_{k}(t_{1}+t_{2})\\leq{\\mathcal{L}}_{k}(t_{1})+t_{2}{\\frac{d{\\mathcal{L}}_{k}}{d t}}(t_{1})+{\\frac{t_{2}^{2}}{2}}M\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\begin{array}{r}{M=\\operatorname*{sup}\\vert\\frac{d^{2}\\mathcal{L}_{k}(t)}{d t^{2}}\\vert}\\end{array}$ . Choosing $\\begin{array}{r}{t_{2}=-\\frac{d\\mathcal{L}_{k}}{d t}(t_{1})M^{-1}}\\end{array}$ shows that ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{k}(t_{1})-{\\frac{1}{2M}}\\left({\\frac{d{\\mathcal{L}}_{k}}{d t}}(t_{1})\\right)^{2}\\geq\\operatorname*{inf}{\\mathcal{L}}_{k}(t)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and letting $t_{1}\\rightarrow\\infty$ here gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t_{1}\\to\\infty}{\\frac{1}{2M}}\\left({\\frac{d{\\mathcal{L}}_{k}}{d t}}(t_{1})\\right)^{2}\\leq\\operatorname*{lim}_{t_{1}\\to\\infty}({\\mathcal{L}}_{k}(t_{1})-\\operatorname*{inf}{\\mathcal{L}}_{k}(t))=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so $\\begin{array}{r}{\\frac{d\\mathcal{L}_{k}}{d t}\\rightarrow0}\\end{array}$ as $t\\to\\infty$ . Meanwhile, as $p$ is accumulation point of $(a_{k},B_{k})$ , $\\begin{array}{r}{\\frac{d\\mathcal{L}_{k}}{d t}(p)}\\end{array}$ is accumulation point of $\\begin{array}{r l}{\\frac{d\\mathcal L_{k}}{d t}(a_{k}(t),\\vec{B}_{k}(t))}&{{}}\\end{array}$ . As $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{d\\mathcal L_{k}}{d t}(t)=0}\\end{array}$ , the only accumulation point of $\\begin{array}{r}{\\frac{d\\mathcal{L}_{k}}{d t}(t)}\\end{array}$ is zero, which shows that $\\begin{array}{r}{\\frac{d\\mathcal{L}_{k}}{d t}(p)=0}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "We have seen that $a_{k}^{2}-|\\vec{B}_{k}|^{2}$ and $(I\\!-\\!U U^{T})\\vec{B}_{k}$ are conserved in our dynamics. A quantity conserved in dynamics should also be conserved at $p$ , so $p=(a,{\\vec{B}})$ should satisfy the following conditions: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a^{2}-|\\vec{B}|^{2}=a_{k}(0)^{2}-|\\vec{B}_{k}(0)|^{2}\\,(\\mathrm{Lemma}\\,3);}\\\\ &{(I-U U^{T})\\vec{B}=(I-U U^{T})\\vec{B}_{k}(0)\\,(\\mathrm{Lemma}\\,2);}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will solve for $p$ satisfying those three conditions. The third condition is equivalent to that ", "page_idx": 27}, {"type": "equation", "text": "$$\na U U^{T}\\left(\\vec{B a}-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As $a_{k}(T)$ is eventually bounded away from zero, we have $a\\ne0$ , so ", "page_idx": 27}, {"type": "equation", "text": "$$\nU U^{T}\\left(\\vec{B}a-\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It follows that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\vec{B}=U U^{T}\\vec{B}+(I-U U^{T})\\vec{B}=U U^{T}\\frac{\\vec{S}}{\\sqrt{D_{c}}}a^{-1}+(I-U U^{T})\\vec{B}_{k}(0)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and substituting to first condition gives ", "page_idx": 27}, {"type": "equation", "text": "$$\na^{2}-\\frac{1}{a^{2}}\\left|U U^{T}\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right|^{2}-\\left|(I-U U^{T})\\vec{B}_{k}(0)\\right|^{2}=a_{k}(0)^{2}-|\\vec{B}_{k}(0)|^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This is equivalent to a quadratic equation in $a^{2}$ , and has a following solution of ", "page_idx": 27}, {"type": "equation", "text": "$$\na^{2}=\\sqrt{\\left|U U^{T}\\frac{\\vec{S}}{\\sqrt{D_{c}}}\\right|^{2}+\\frac{(a_{k}(0)^{2}-|U U^{T}\\vec{B}_{k}(0)|^{2})^{2}}{4}}+\\frac{a_{k}(0)^{2}-|U U^{T}\\vec{B}_{k}(0)|^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This shows that there are two candidates for $p$ , with $a$ given as two square roots of Eq. (101) and $B$ determined from $a$ by Eq. (99). It is impossible for $\\mathcal{L}_{k}(t)$ to have accumulation points both in regions $a>0$ and $a<0$ , as it would imply $a_{k}(t)=0$ happens infinitely many often, contradicting that $a_{k}$ is eventually bounded away from zero. Thus it follows that $\\mathcal{L}_{k}(t)$ can only have one accumulation point. As dynamics having unique accumulation point should converge, it follows that ", "page_idx": 27}, {"type": "equation", "text": "$$\n(a,\\vec{B})=(a_{k}(\\infty),\\vec{B}_{k}(\\infty)).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "One can check that the $U^{\\perp}$ -component of $\\vec{B}_{k}(\\infty)a_{k}(\\infty)$ is given as ", "page_idx": 27}, {"type": "equation", "text": "$$\n(I-U U^{T})\\vec{B}_{k}(\\infty)a_{k}(\\infty)=(I-U U^{T})\\vec{B}_{k}(0)a_{k}(0)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and this is bounded by $|(1-U U^{T})B_{k}(0)|(S+a_{k}(0))$ , so by our assumption this is negligible. Thus, we find that $\\vec{B}_{k}(\\infty)a_{k}(\\infty)$ is the pseudo-inverse solution, which is also found by the linear model with $e_{k,l}$ as basis functions. We can calculate $\\mathcal{L}_{k}(\\infty)$ using the result from kernel (linear) regression [23\u201327] (for a summary, see tables 1 and 2 in appendix A of [27]). Using the terminology in table 1 of [27], the sample size is $d_{k}$ ; the number of parameters is $D_{c}$ ; ridge and noise are absent; the eigenfunctions are $\\left[e_{k,1},\\cdot\\cdot\\cdot\\,,e_{k,D_{c}}\\right]$ ; the eigen coefficients are ${\\bf E}_{X}[e_{k,i}(x)S g_{k}(x)]=S D_{c}^{-1/2}$ (Eq. (16)); eigenvalues are uniform; the learnability is $d_{k}/D_{c}$ for all $i$ ; and the overftiting coefficient is $(1-d_{k}/\\bar{D_{c}})^{-1}$ . Taking into account that we have halved the MSE loss (Eq. (5)), the test loss is ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\infty)=\\frac{S^{2}}{2}\\left(1-\\frac{d_{k}}{D_{c}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We obtain the result by using Eq. (10). ", "page_idx": 28}, {"type": "text", "text": "F.4 $N_{c}$ basis functions for a skill ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition 2. Let the extended multilinear model Eq. (18) be trained with gradient flow on $D\\rightarrow\\infty$ i.i.d samples for the setup in Section $^3$ with $n_{s}\\to\\infty$ (input distribution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5), initialization: that of Proposition $^{\\,l}$ ). For a model with the following finite $N$ basis functions ", "page_idx": 28}, {"type": "equation", "text": "$$\n[e_{1,1},~\\cdot\\cdot\\cdot\\ ,~e_{1,N_{c}},~e_{2,1},~\\cdot\\cdot\\cdot\\ ,~e_{q,r}],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where quotient $q=\\lfloor(N-1)/N_{c}\\rfloor+1$ and remainder $r$ is such that $(q-1)N_{c}+r=N$ . The skill strength at $T\\rightarrow\\infty$ becomes ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}(\\infty)=\\left\\{k>q:\\begin{array}{c c}{0}\\\\ {S\\frac{r}{N_{c}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof Because we have $D\\rightarrow\\infty$ and because $\\left[e_{k,1},\\cdot\\cdot\\cdot e_{k,N_{c}}\\right]$ can express $g_{k}$ (Eq. (20)), it is trivial to show that $\\mathcal{R}_{k}(\\infty)=S$ for $k<q$ . For $k=q$ , the gradient descent dynamics (Eq. (83)) leads to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{d\\vec{B}_{k}}{d t}=-\\frac{\\eta a_{k}}{D}\\Phi\\Phi^{T}\\left(\\vec{B}_{k}a_{k}-\\frac{\\vec{S}}{\\sqrt{N_{c}}}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the matrix $\\Phi\\in\\mathbb{R}^{r\\times d_{k}}$ and vector $\\vec{B}_{k}\\in\\mathbb{R}^{r}$ are the feature matrix(Eq. (77)) and parameters for the $k^{t h}$ skill respectively. As $D\\rightarrow\\infty$ , the matrix $\\Phi\\Phi^{T}$ becomes a rank $r$ identity matrix scaled by the frequency of the skill: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{D\\rightarrow\\infty}\\frac{1}{D}(\\Phi\\Phi^{T})_{l l^{\\prime}}=\\mathbf{E}_{I,X}\\left[e_{k,l}(k,X)e_{k,l^{\\prime}}(k,X)\\right]=\\mathcal{P}(k)\\delta_{l,l^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Plugging in $\\Phi\\Phi^{T}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{d B_{k,l}}{d t}=-\\eta\\mathcal{P}(k)a_{k}\\left(B_{k,l}a_{k}-\\frac{S}{\\sqrt{N_{c}}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assuming the initialization in Proposition 1, we can show that $a_{k}(\\infty)B_{k,l}(\\infty)=S/\\sqrt{N_{c}}$ for $l\\leq r$ . From Eq. (7), the skill strength $\\mathcal{R}_{k}(\\infty)$ is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{k}(\\infty)=\\displaystyle\\sum_{l=1}^{r}\\frac{S}{\\sqrt{N_{c}}}\\mathbf{E}_{X}\\left[e_{k,l}(k,X)g_{k}(k,X)\\right]}\\\\ &{\\qquad\\quad=S\\frac{r}{N_{c}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used Eq. (20) for the linear correlation between $e_{k,l}$ and $g_{k}$ . ", "page_idx": 28}, {"type": "text", "text": "G Time emergence example in NN ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we discuss an example for the time emergence case (Fig. 1(a)) in which the saturation of skill in an NN consists of multiple saturating \u2018modes\u2019 as in Fig. 8. ", "page_idx": 29}, {"type": "image", "img_path": "cuWsR25bbI/tmp/b5f42b99786c3f7f5d56106a61b46d1f62ee7df4614ea351119e60bdd977f58f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 8: Modes in NN. A 2-layer MLP with ReLU activations with a width of 3 and weight sharing (Eq. (114)) is trained to fit the parity function. (a): The skill strength $\\mathcal{R}$ , because of the last layer\u2019s linearity, can be decomposed into skill strength from each hidden neuron or each \u2018mode\u2019 (shown in different colors, Eq. (119)). (b): The skill strength for each mode follows a near-sigmoidal curve with different emergent/saturation times (colors) whose sum results in the total skill strength (solid black). Note that different saturation times of each mode result in a deviation from the prediction of the multilinear model with $B^{2}=1/3$ (dashed black). ", "page_idx": 29}, {"type": "text", "text": "Task. We assume an input $X\\in\\mathbb{R}^{3\\times8}$ (note that we are not using $X$ as a random variable) that is all 8 possible inputs for bits with dimension 3. The target $Y$ is the parity function scaled by $S$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X=\\binom{0\\mathrm{~0~0~0~1~1~1~1~1~}}{0\\mathrm{~1~0~1~0~0~1~0~1~1~}},\\qquad Y=\\bigl(\\,S\\mathrm{~-}S\\mathrm{~-}S\\mathrm{~-}S\\mathrm{~}S\\mathrm{~-}S\\mathrm{~}\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "NN. We assume a 2-layer width $3\\;\\mathrm{NN}$ with ReLU activation with the input dimension 3 (Fig. 8(a)). The NN has 16 parameters, but to simplify the argument, we use weight sharing so NN has only 4 parameters: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(x;\\alpha,\\beta,\\gamma,c)=w^{T}\\sigma(W x+b)+c\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\sigma$ is the ReLU activation and $W,b,w$ are ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W=\\bigl(\\,\\frac{-\\alpha}{\\hbar}\\,\\underset{-\\gamma}{\\overset{\\alpha}{\\beta}}\\,\\underset{\\gamma}{-\\alpha}\\,\\bigr),\\qquad b=\\bigl(\\,\\underset{-\\gamma}{\\overset{0}{\\beta}}\\,\\bigr),\\qquad w=\\bigl(\\,\\underset{\\gamma}{\\overset{-2\\alpha}{\\beta}}\\,\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Modes. It is easy to see that $\\alpha=\\beta=\\gamma=\\sqrt{2S}$ and $c=-S$ leads to the target parity function. We note that one parameter except $c$ (i.e. $\\alpha,\\beta,\\gamma)$ maps to one neuron or a mode (colors in Fig. 8(a)). We define the first mode $f^{(1)}$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f^{(1)}(x)=w_{1}\\sigma(W_{1}^{T}x+b_{1})=-2\\alpha^{2}\\sigma(x_{2}-x_{1}-x_{3})}}\\\\ {{=-2\\alpha^{2}h_{1}(x),\\qquad h_{1}(x):=\\sigma(x_{2}-x_{1}-x_{3}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $w_{1},b_{1}$ are the first entry of $w,b$ respectively and $W_{1}$ is the first row of $W$ . Note that $f^{(1)}(x)$ takes a form similar to the multilinear model (Eq. (9)) but with $h_{1}$ as the respective basis. We define $f^{(2)},f^{(3)}$ similarly, and the sum of modes becomes the NN: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(x)=\\sum_{q=1}^{3}f^{(i)}(x)+c,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which resembles the multilinear model with different skills. ", "page_idx": 29}, {"type": "text", "text": "Mode strength. Analogous to the skill strength in Eq. (7), we define mode $q$ \u2019s strength $\\mathcal{R}^{\\left(q\\right)}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{R}^{(q)}=\\frac{1}{8S^{2}}Y^{T}f^{(q)}(X),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $f^{(q)}(X)=[f^{(q)}(X_{1}),\\cdot\\cdot\\cdot\\cdot,f^{(q)}(X_{8})]$ and $X_{j}$ are the $j^{t h}$ column of $X$ . By the linearity of the expectation, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\sum_{q=1}^{3}\\mathcal{R}^{(q)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that constant $c$ always has zero correlation (inner product) to the target $(Y)$ . ", "page_idx": 30}, {"type": "text", "text": "Analysis. The dynamics of each mode $\\mathcal{R}^{(q)}(x)$ differs from that of the multilinear model (Eq. (11)) because $h_{q}(x)$ often depends on the parameter, and the dynamics are no longer decoupled among each mode. Nevertheless, each mode follows a sigmoid-like growth (Fig. 8(b)). We note that each mode has a different saturation time scale or is updated at different frequencies. A mode with a longer time scale leads to a longer \u2018tail\u2019 of saturation as discussed in the main text. ", "page_idx": 30}, {"type": "text", "text": "Update frequency. Because of the non-linearity, each mode differs in the gradients it receives. We can explicitly calculate the gradient for each parameter as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\alpha^{2}}{d t}=2\\eta\\alpha^{2}(-S-(-2\\alpha^{2}+2\\beta^{2}+c))}\\\\ {\\displaystyle\\frac{d\\beta^{2}}{d t}=-\\eta\\beta^{2}(S-(-2\\alpha^{2}+5\\beta^{2}+5c))}\\\\ {\\displaystyle\\frac{d\\gamma^{2}}{d t}=-\\eta\\gamma^{2}(S-(\\gamma^{2}+c))}\\\\ {\\displaystyle\\frac{d c}{d t}=-\\eta(2\\alpha^{2}-5\\beta^{2}-\\gamma^{2}-8c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We immediately notice that $c$ will grow the fastest for small initialization $(\\alpha,\\beta,\\gamma,c\\ll1)$ because it saturates exponentially while other parameters saturate sigmoidally. Considering that $S$ is always the largest term and $c$ saturate to $S$ quickly, we notice that the saturation is in the order of $\\dot{\\alpha}^{2}$ $(\\approx2S+2c\\approx4S)$ , $\\beta^{2}$ $\\approx-S+5c\\approx4S)$ , and $\\gamma^{2}(\\approx2S)$ . We observe that our crude approximation holds in Fig. 8(b): the first $(\\alpha)$ and the second $(\\beta)$ modes saturate at similar timescale, while the third mode $(\\gamma)$ requires approximately twice the time for saturation. ", "page_idx": 30}, {"type": "text", "text": "H Details of the multilinear model ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The multilinear model (Fig. 9(a)) has two identifying properties: 1) the layerwise structure and 2) $g_{k}$ as the basis functions. In this section, we discuss the role of each property in more detail. ", "page_idx": 31}, {"type": "image", "img_path": "cuWsR25bbI/tmp/d531032fc6bc2f3d2f227efc21682d76cecf1ae7112afaac21929f46342dc148.jpg", "img_caption": ["Figure 9: Multilinear model. (a): An illustration of the multilinear model which is multilinear in terms of parameters, generating a layerwise structure. The model also has the skill functions $g_{k}\\mathbf{s}$ as basis functions. (b): The dynamics of the multilinear model are decoupled and each skill strength $(\\mathcal{R}_{k})$ shows a sigmoidal growth in time. Note that less frequent skills have a more delayed growth. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Multilinearity. The product of two parameters $(a_{k}b_{k})$ creates the layerwise structure (Fig. 9(a)) that gives rise to the emerging dynamics (sudden saturation or sigmoidal growth) in Fig. 9(b). The time emergence of NN is well-described by the sigmoidal dynamics (Fig. 1(a)); a non-sigmoidal saturation dynamics, for example, that of linear models (Fig. 10(a)), would inadequately describe the time emergence. Such dynamics have first been studied by Saxe et al. [19] (See Appendix B.2 for an overview). ", "page_idx": 31}, {"type": "text", "text": "Assuming a sufficiently fast decay of $d_{k}$ for the skills, the sigmoidal growth results in a stage-like training (Appendix D) where one skill fully saturates before the next skill emerges. In Appendix D, we discuss how the stage-like training can describe the quanta model [18] and how NNs, without explicit $g_{k}\\mathbf{s}$ , decouple each skill. ", "page_idx": 31}, {"type": "text", "text": "Finally, note that even though sigmoidal saturation has a resemblance to the test accuracy in grokking [48], our model is irrelevant to grokking because $\\mathcal{R}_{k}$ \u2013 which is defined over the expectation over the $\\bar{k}^{t h}$ skill (Eq. (7)) \u2013 appears both in the empirical loss (Eq. (11)) and the test loss: failing to describe the discrepancy between train and test accuracy in grokking. ", "page_idx": 31}, {"type": "text", "text": "Connection to linear models. In Section 4 and Appendix E, we have shown how the scaling laws follow from the basis functions $g_{k}$ that decouples the loss. To analyze the role of $g_{k}$ , we can ask whether a simpler linear model with $g_{k}$ as basis functions (Eq. (124)) also recovers the scaling laws. The answer is yes and we outline how a linear model can recover all scaling laws. In addition, we also outline how extended linear models \u2013 extended similar to Section 5 such that skills are decoupled \u2013 can recover all emergence behaviors shown in Appendix $\\boldsymbol{\\mathrm{F}}$ except the time emergence. ", "page_idx": 31}, {"type": "text", "text": "By replacing $a_{k}b_{k}$ with $w_{k}$ , we obtain the linear model with skill basis functions: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{T}(i,x;w)=\\sum_{k=1}^{N}w_{k}(T)g_{k}(i,x).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The dynamics of the linear model under gradient flow is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{k}(T)=w_{k}(T)=S(1-e^{-\\eta\\frac{d_{k}}{D}T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we assumed $w_{k}(0)\\,=\\,0$ . The linear model follows an exponential saturation of the skill strength in contrast to the sigmoidal saturation of the multilinear model (Fig. 10). ", "page_idx": 31}, {"type": "image", "img_path": "cuWsR25bbI/tmp/5d82b2eae813c9096d01218211db051cd2f0fa210e1fa15a75b9dea689c31046.jpg", "img_caption": ["Figure 10: Dynamics of linear and multilinear model. (a): Skill strength dynamics of the linear model (Eq. (125)) (b): Skill strength dynamics of the multilinear model (Eq. (11)). For the linear model, $\\mathcal{R}_{k}$ emerges from $T\\,=\\,0$ for all $d_{k}/D\\,>\\,0$ : obstructing the stage-like training. For the multilinear model, $\\mathcal{R}_{k}$ shows a delayed emergence depending on $d_{k}/D$ : allowing the stage-like training and describing the sigmoidal time emergence in Fig. 1(a). "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Nevertheless, the linear model Eq. (125) results in the same scaling laws in Section 4. For the time scaling law, we recover the relationship between $d\\mathcal{L}_{k}/d T$ and $d{\\mathcal{L}}_{k}/d k$ in Appendix E.1 because $\\mathcal{R}_{k}(T)$ is a function of $\\scriptstyle{\\frac{d_{k}}{D}}T$ only (where $d_{k}/D=\\mathcal{P}_{s}(k)$ for $D\\to\\infty$ ). For the data scaling law, we recover Corollary 1 because each $w_{k}$ (i.e. $\\mathcal{R}_{k}$ ) is decoupled. For the parameter scaling law, we recover Corollary 2 trivially as the linear model shares the same basis functions. ", "page_idx": 32}, {"type": "text", "text": "The data and parameter emergence in Section 5 can be obtained from the linear model in Eq. (124) if we extend the model analogous to Eqs. (15) and (18). For example, we can extend the model for data emergence as ", "page_idx": 32}, {"type": "equation", "text": "$$\nf_{T}(i,x;W)=\\sum_{k=1}^{N}\\sum_{l=1}^{D_{c}}W_{k,l}(T)e_{k,l}(i,x),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the matrix $W\\in\\mathbb{R}^{N\\times D_{c}}$ is an extension of $w\\in\\mathbb{R}^{N}$ in Eq. (124), $D_{c}$ is a fixed scalar, and $e_{k,l}(i,x):\\{0,1\\}^{n_{s}+n_{b}}\\rightarrow\\mathbb{R}$ are functions with the following properties: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{E}_{X|I=k}\\left[e_{k,l}e_{k,l^{\\prime}}\\right]=\\delta_{l l^{\\prime}},\\ \\ \\ \\ e_{k,l}(I\\neq k,x)=0,\\ \\ \\ \\sum_{l=1}^{D_{c}}\\frac{1}{\\sqrt{D_{c}}}e_{k,l}=g_{k}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The equivalence can be shown by Lemma 2 which states that the multilinear model finds the minimum norm solution: the solution that the linear model finds in a ridgeless regression setup. ", "page_idx": 32}, {"type": "text", "text": "Thus, for our setup, the basis functions play a critical role in the scaling laws and data/parameter emergences. The choice of basis functions, also known as the task-model alignment (see [23, 27]), determines the linear model\u2019s scaling laws and emergence behaviors. See Bordelon et al. [21] for a study of the scaling laws in linear models. ", "page_idx": 32}, {"type": "image", "img_path": "cuWsR25bbI/tmp/8edba9319f276db3371bf53628d2b89a800166dcc69805c5530f4b1faafe5be4.jpg", "img_caption": ["Figure 11: Calibration and prediction on emergence. The calibration of the extended multilinear model (solid) on the 2-layer NN (dashed) for $n_{s}=1$ system. For the calibrated parameters, we have $B^{2}=1/22$ for time (Eq. (14)), $D_{c}=800$ for data (Eq. (17)), and $N_{c}=4$ for hidden layer width (Eq. (21)). "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 5: Samples of skill strength $\\mathcal{R}_{k}/S$ . The table shows the skill strength at $N=10$ for 10 different runs of the parameter emergence experiment (Fig. 1(c)). Note that the variance of $\\mathcal{R}_{k}/S$ is amplified by the outliers \u2013 shaded columns \u2013 that learn a less frequent skill at the cost of a more frequent skill (second column) or fail to learn a skill (seventh column). ", "page_idx": 33}, {"type": "image", "img_path": "cuWsR25bbI/tmp/935055b0b576e44a77fc4a80a835e78034a67e79dbc71c1a6e31c713af0dbbed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 12: Enlarged emergence. Enlarged view of skill emergence from Fig. 4, showing that saturations also follow a sigmoidal pattern. The $x$ -axis is measured in steps. ", "page_idx": 33}, {"type": "text", "text": "J Rigorous derivation of the scaling laws ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In Appendix E, we discussed the scaling laws in simplified settings, favoring intuition over mathematical rigor. Building upon the intuitive understanding developed in Appendix E, we now turn our attention to a rigorous analysis of the scaling laws. In this section, we will derive general scaling laws by considering a comprehensive set of parameters and variables. Our goal is to establish the conditions under which these scaling laws hold and to quantify the associated error terms. By explicitly analyzing the error terms, this section aims to provide a rigorous assessment of the validity and limitations of our scaling law estimates. ", "page_idx": 34}, {"type": "text", "text": "Table 6: Scaling laws and their conditions. The leftmost column indicates the condition for the \u2018large resource\u2019 \u2013 large enough to be treated as infinity, while the second column is the condition between the other two resources for the scaling law (third column). The last two columns show where the statement for the prefactor constant (e.g. $\\boldsymbol{A}_{N}$ for scaling law $\\mathcal{L}=\\mathcal{A}_{N}N^{-\\alpha})$ and the scaling law (with the assumptions and explicit error terms) are given. Note that whenever $T$ appears in theorems and corollaries, $\\eta S$ is multiplied to make it dimensionless. ", "page_idx": 34}, {"type": "table", "img_path": "cuWsR25bbI/tmp/aa7c6dce65308855edad1c3ebe03f404702c666b46ffde0b42ca25fa7b2ba3d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "J.1 General set up, repeated ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We go back to the most general settings possible. Our starting point is Eq. (27), which describes the dynamics of $\\mathcal{R}_{k}$ and $\\mathcal{L}_{k}$ valid for $k\\leq N$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta\\frac{d_{k}}{D}S T}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We do not use skills for indices $k>N$ in our model, but we can still denote ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}=0\\quad\\mathrm{and}\\quad\\mathcal{L}_{k}={\\frac{S^{2}}{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $\\mathcal{P}_{s}(k)=A k^{-\\alpha-1}$ , the total loss is given as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\sum_{k=1}^{n_{s}}\\mathcal{P}_{s}(k)\\mathcal{L}_{k}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\mathcal{L}_{k}+\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "When $n_{s},N,T$ are all set, their dependency with the data is only determined by the statistics $d_{k}$ , the number of data with $i^{(j)}=k$ . We assumed that $(i,x)\\in I\\times\\{0,1\\}^{n_{d}}$ was collected as random samples with $i$ following the Zipfian distribution of size $n_{s}$ and exponent $\\alpha+1$ , or equivalently $P(i=k)=\\mathcal{P}_{s}(k)=A\\bar{k}^{-\\alpha-1}$ for $1\\le k\\le n_{s}$ . Then $(d_{1},\\cdot\\cdot\\cdot,d_{n_{s}})$ is a vector denoting the number of occurrences in $D$ independent sampling from that distribution. It follows that $d_{i}$ follows binomial distribution $B(D,\\mathcal{P}_{s}(k))$ . ", "page_idx": 34}, {"type": "text", "text": "In this complete perspective, our loss is dependent on all of those parameters and variables ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}(n_{S},D,\\mathcal{R}_{i n i t},N,T)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\mathcal{R}_{i n i t}=(\\mathcal{R}_{1}(0),\\cdot\\cdot\\cdot,\\mathcal{R}_{N}(0))$ denotes the vector representing initial condition. We will also simply denote $r_{k}=\\mathcal{R}_{k}(0)$ . We will not assume much on $r_{k}$ , but we absolutely need $0<r_{k}<S$ for dynamics to hold, and we also should have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n_{s}}\\mathcal{P}_{s}(k)r_{k}^{2}=\\mathbf{E}[f(0)^{2}]\\ll S^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We will not impose any particular distribution on $\\mathcal{R}_{i n i t}$ . Instead, we will try to identify sufficient conditions on $r_{k}$ for our desired result to hold, and those conditions will differ by the situation we are considering. For example, in Theorems 2 and 3 where we prove time scaling law $\\mathcal{L}=\\Theta(T^{-\\alpha/(\\alpha+1)})$ for large enough $D$ and bottleneck $T$ , we only require $\\epsilon<r_{k}<S/2$ for some $\\epsilon>0$ . However, the exact constant depends on the distribution of $r_{k}$ , and figuring out the explicit constant seems to be only feasible when we fix $r_{k}=r$ as in Theorem 4. ", "page_idx": 35}, {"type": "text", "text": "J.2 Estimates for large $D$ ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We will first consider the situation where $D$ becomes the \u2018large resource\u2019 so that its effect on the loss function is negligible. The number of data $d_{k}$ follows binomial distribution $B(D,\\mathcal{P}_{s}(k))$ , so $d_{k}/D$ converges to $\\mathcal{P}_{s}(k)$ for large enough $D$ . So taking the limit of $\\mathcal{L}$ when we let $D\\rightarrow\\infty$ has the effect of replacing $d_{k}/D$ by $\\mathcal{P}_{s}(k)$ in the expression of $\\mathcal{L}$ . We will establish an explicit inequality comparing the difference between $\\mathcal{L}$ and this limit. ", "page_idx": 35}, {"type": "text", "text": "Lemma 4. For a function $F:\\mathbb{R}\\rightarrow\\mathbb{R}$ with its total variation $V(F)$ bounded, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\mathbf{E}_{\\mathcal{D}}\\left[F(\\frac{d_{k}}{\\boldsymbol{D}})\\right]-\\mathbf{E}_{z\\sim\\mathcal{N}(\\mathcal{P}_{s}(k),\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))/\\boldsymbol{D})}\\left[F(z)\\right]\\right|<\\frac{V(F)}{\\sqrt{\\boldsymbol{D}}\\sqrt{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\mathcal{N}(\\mu,\\sigma^{2})$ denotes normal distribution of mean $\\mu$ and variance $\\sigma^{2}$ . ", "page_idx": 35}, {"type": "text", "text": "Proof This is just an application of the Berry-Esseen inequality (with constant 1, see [49] for modern treatment) applied to $d_{k}$ following binomial distribution $\\mathbf{\\bar{\\boldsymbol{B}}}(\\bar{\\boldsymbol{D}_{}},\\mathcal{P}_{s}(\\boldsymbol{k}))$ . \u25a0 ", "page_idx": 35}, {"type": "text", "text": "Lemma 5. Let $F:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a $C^{2}$ function such that $F^{\\prime\\prime}$ is bounded. Then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\big|\\mathbf{E}_{z\\sim\\mathcal{N}(\\mathcal{P}_{s}(k),\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))/D)}\\left[F(z)\\right]-F(\\mathcal{P}_{s}(k))\\big|\\,\\leq\\frac{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}{2D}\\mathrm{sup}|F^{\\prime\\prime}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof First, we apply Taylor\u2019s theorem to show that ", "page_idx": 35}, {"type": "equation", "text": "$$\n|F(z)-F(\\mathcal{P}_{s}(k))-F^{\\prime}(\\mathcal{P}_{s}(k))(z-\\mathcal{P}_{s}(k))|\\le\\frac{(z-\\mathcal{P}_{s}(k))^{2}}{2}\\operatorname*{sup}|F^{\\prime\\prime}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Taking expectation when $z$ follows normal distribution $\\begin{array}{r}{\\mathcal{N}(\\mathcal{P}_{s}(k),\\frac{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}{D})}\\end{array}$ gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|{\\bf E}_{z}\\left[F(z)-F(\\mathcal{P}_{s}(k))\\right]\\right|=\\left|{\\bf E}_{z}\\left[F(z)-F(\\mathcal{P}_{s}(k))-F^{\\prime}(\\mathcal{P}_{s}(k))(z-\\mathcal{P}_{s}(k))\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq{\\bf E}_{z}\\left[|F(z)-F(\\mathcal{P}_{s}(k))-F^{\\prime}(\\mathcal{P}_{s}(k))(z-\\mathcal{P}_{s}(k))|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq{\\bf E}_{z}\\left[\\frac{\\left(z-\\mathcal{P}_{s}(k)\\right)^{2}}{2}\\operatorname*{sup}|F^{\\prime\\prime}|\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\!\\!\\frac{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}{2D}\\!\\operatorname*{sup}|F^{\\prime\\prime}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proposition 3. We have ", "text_level": 1, "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbf{E}_{\\mathcal{D}}\\left[\\mathcal{L}_{k}\\right]-\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}\\right|<\\frac{2^{\\alpha}S^{2}}{\\sqrt{D\\mathcal{P}_{s}(k)}}+\\frac{4S^{4}\\eta^{2}T^{2}\\mathcal{P}_{s}(k)}{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof Consider the function $F:\\mathbb{R}\\rightarrow\\mathbb{R}$ given as ", "page_idx": 35}, {"type": "equation", "text": "$$\nF(z)=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta S T z}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This function is monotone decreasing and $C^{2}$ on the whole domain, and its supremum and infimum are given as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{sup}F=\\operatorname*{lim}_{z\\to-\\infty}F(z)={\\frac{S^{2}}{2}}\\quad{\\mathrm{and}}\\quad{\\mathrm{inf~}}F=\\operatorname*{lim}_{z\\to\\infty}F(z)=0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This implies that ", "page_idx": 36}, {"type": "equation", "text": "$$\nV(F)=\\operatorname*{sup}F-\\operatorname*{inf}F=\\frac{S^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also, we will show that $F^{\\prime\\prime}$ is globally bounded. We first calculate ", "page_idx": 36}, {"type": "equation", "text": "$$\nF^{\\prime\\prime}(z)=-4S^{3}r_{k}(1-\\frac{r_{k}}{S})^{2}\\eta^{2}T^{2}\\frac{e^{2\\eta S T z}(1-\\frac{r_{k}}{S}-\\frac{2r_{k}}{S}e^{2\\eta S T z})}{\\left(1-\\frac{r_{k}}{S}+\\frac{r_{k}}{S}e^{2\\eta S T z}\\right)^{4}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We consider the following inequalities ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{e^{2\\eta S T z}\\leq\\displaystyle\\frac{S}{r_{k}}\\left(1-\\frac{r_{k}}{S}+\\frac{r_{k}}{S}e^{2\\eta S T z}\\right)}}\\\\ {{\\left|1-\\frac{r_{k}}{S}-\\frac{2r_{k}}{S}e^{2\\eta S T z}\\right|\\leq\\left|1-\\frac{r_{k}}{S}\\right|+\\frac{2r_{k}}{S}e^{2\\eta S T z}<2\\left(1+\\frac{r_{k}}{S}(e^{2\\eta S T z}-1)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "to show that ", "page_idx": 36}, {"type": "equation", "text": "$$\n|F^{\\prime\\prime}(z)|<4S^{3}r_{k}(1-\\frac{r_{k}}{S})^{2}\\eta^{2}T^{2}\\frac{\\frac{2S}{r_{k}}\\left(1-\\frac{r_{k}}{S}+\\frac{r_{k}}{S}e^{2\\eta S T z}\\right)^{2}}{\\left(1-\\frac{r_{k}}{S}+\\frac{r_{k}}{S}e^{2\\eta S T z}\\right)^{4}}<8S^{4}\\eta^{2}T^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all $z$ . Thus we can apply both Lemma 4 and Lemma 5 to this function $F$ and we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbf{E}_{\\mathcal{D}}\\left[F(\\frac{d_{k}}{D})\\right]-F(\\mathcal{P}_{s}(k))\\right|<\\!\\frac{V(F)}{\\sqrt{D}\\sqrt{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}}+\\frac{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}{2D}\\!\\operatorname*{sup}|F^{\\prime\\prime}|}\\\\ &{\\qquad\\qquad\\qquad\\quad<\\!\\frac{S^{2}}{2\\sqrt{D}\\sqrt{\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))}}+\\frac{4\\mathcal{P}_{s}(k)S^{4}\\eta^{2}T^{2}}{D}}\\\\ &{\\qquad\\qquad\\qquad\\quad<\\!\\frac{2^{\\alpha}S^{2}}{\\sqrt{D\\mathcal{P}_{s}(k)}}+\\frac{4\\mathcal{P}_{s}(k)S^{4}\\eta^{2}T^{2}}{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last line follows from that we always have ", "page_idx": 36}, {"type": "equation", "text": "$$\n1-\\mathcal{P}_{s}(k)\\geq1-\\mathcal{P}_{s}(1)=\\frac{2^{-(\\alpha+1)}+\\cdot\\cdot\\cdot+n_{s}^{-(\\alpha+1)}}{1+2^{-(\\alpha+1)}+\\cdot\\cdot\\cdot+n_{s}^{-(\\alpha+1)}}>\\frac{2^{-(\\alpha+1)}}{1+2^{-(\\alpha+1)}}>\\frac{1}{2^{2(\\alpha+1)}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma 6. For any integer $N$ and $\\sigma\\geq1/2$ and $\\sigma\\ne1$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{N}k^{-\\sigma}=\\zeta(\\sigma)+\\frac{N^{1-\\sigma}}{1-\\sigma}+O(N^{-\\sigma})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\zeta$ is the Riemann zeta function (defined over the whole complex plane except 1 via analytic continuation). In addition, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{N}k^{-1}=\\log N+\\gamma+O(N^{-1})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where \u03b3 = 0.5772156649... is Euler\u2019s constant. ", "page_idx": 36}, {"type": "text", "text": "Proof See Corollary 1.15 of [50], or other analytic number theory textbooks. Proposition 4. (Large $D$ approximation) We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]-\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}-\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}}\\\\ {\\displaystyle=O\\left(S^{2}D^{-1/2}f_{\\alpha}(N)+S^{4}\\eta^{2}T^{2}D^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\nf_{\\alpha}(N)={\\left\\{\\begin{array}{l l}{1}&{i f\\alpha>1}\\\\ {\\log N}&{i f\\alpha=1}\\\\ {N^{(1-\\alpha)/2}}&{i f\\alpha<1.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The constant on the $O$ term only depends on $\\alpha$ . ", "page_idx": 36}, {"type": "text", "text": "Proof From the description of $\\mathcal{L}$ in Eq. (129), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]-\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}-\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}}\\\\ {\\displaystyle=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\left(\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}_{k}]-\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We apply Proposition 3 to give ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{=1}^{N}\\mathcal{P}_{s}(k)\\left(\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}_{k}]-\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}\\right)<\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\left(\\frac{2^{\\alpha}S^{2}}{\\sqrt{D\\mathcal{P}_{s}(k)}}+\\frac{4S^{4}\\eta^{2}T^{2}}{D}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Each of these sum involving $\\mathcal{P}_{s}(k)$ is bounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)^{2}<\\left(\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\right)^{2}<1\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{N}\\sqrt{\\mathcal{P}_{s}(k)}<\\sum_{k=1}^{N}k^{-(\\alpha+1)/2}=O(f_{\\alpha}(N))\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which follows from Lemma 6. Combining those two gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\left(\\frac{2^{\\alpha}S^{2}}{\\sqrt{D\\mathcal{P}_{s}(k)}}+\\frac{S^{4}\\eta^{2}T^{2}\\mathcal{P}_{s}(k)}{D}\\right)=O\\left(S^{2}D^{-1/2}f_{\\alpha}(N)+S^{4}\\eta^{2}T^{2}D^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "While Proposition 4 holds for any $D$ , it becomes only meaningful if the resulting error terms are less than the main term we desire. We will revisit this when the exact main term is found, and determine the sufficient size of $D$ for error terms to become small enough. ", "page_idx": 37}, {"type": "text", "text": "J.3 Estimates for not too small $n_{s}$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We next discuss the effect of $n_{s}$ . When $n_{s}\\to\\infty$ heuristically, then intuitively we have $\\mathcal{P}_{s}(k)\\to$ $k^{-(\\alpha+1)}/\\zeta(\\alpha+1)$ . We will discuss the difference between when we regard $n_{s}$ as $\\infty$ and when we do not. ", "page_idx": 37}, {"type": "text", "text": "Proposition 5. The following equations hold: ", "page_idx": 37}, {"type": "equation", "text": "$$\nA^{-1}=\\sum_{k=1}^{n_{s}}k^{-(\\alpha+1)}=\\zeta(\\alpha+1)-\\frac{n_{s}^{-\\alpha}}{\\alpha}+O(n_{s}^{-\\alpha-1})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{P}_{s}(k)=\\frac{k^{-\\alpha-1}}{\\zeta(\\alpha+1)}\\left(1+\\frac{n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}O(n_{s}^{-\\alpha-1})\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)=\\frac{N^{-\\alpha}-n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}+O(N^{-\\operatorname*{min}(\\alpha+1,2\\alpha)})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "All implied constants on $O$ only depend on $\\alpha$ . ", "page_idx": 37}, {"type": "text", "text": "Proof The first statement Eq. (160) follows from substituting $\\sigma=\\alpha+1$ in Lemma 6. As $\\mathcal{P}_{s}(k)=$ $A k^{-(\\alpha+1)}$ , the second statement Eq. (161) immediately follows. If we substitute $n_{s}\\;=\\;N$ into Eq. (160) and calculate differences between them, we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{k=N+1}^{n_{s}}k^{-\\alpha-1}=\\frac{N^{-\\alpha}-n_{s}^{-\\alpha}}{\\alpha}+O(N^{-\\alpha-1}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)=A\\sum_{k=N+1}^{n_{s}}k^{-(\\alpha+1)}=\\frac{N^{-\\alpha}-n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}+O\\left(N^{-\\alpha-1}+(N^{-\\alpha}-n_{s}^{-\\alpha})n_{s}^{-\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Regardless of the size of $n_{s}$ , We always have ", "page_idx": 38}, {"type": "equation", "text": "$$\n(N^{-\\alpha}-n_{s}^{-\\alpha})n_{s}^{-\\alpha}\\leq\\left(\\frac{N^{-\\alpha}}{2}\\right)^{2}=\\frac{N^{-2\\alpha}}{4}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "so the third statement Eq. (162) follows. ", "page_idx": 38}, {"type": "text", "text": "We go back to the description of total loss given in Eq. (129) as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\mathcal{L}_{k}+\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and we take its expectation in $\\mathcal{D}$ . Proposition 4 suggests that its limit when $D\\rightarrow\\infty$ is given as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{D\\rightarrow\\infty}{\\bf E}_{\\mathcal{D}}[\\mathcal{L}]=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}+\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Denote ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{1}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}}\\\\ {\\displaystyle\\mathcal{L}_{2}=\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We discuss the effect of $n_{s}$ in $\\mathcal{L}_{1}$ and $\\mathcal{L}_{2}$ , by comparing limit of $\\mathcal{L}_{1}$ and $\\mathcal{L}_{2}$ when $n_{s}\\to\\infty$ and their original values. ", "page_idx": 38}, {"type": "text", "text": "\u2022 For the term $\\mathcal{L}_{1}$ , the change of letting $n_{s}$ as finite value from $n_{s}~\\rightarrow~\\infty$ has effect of multiplying $T$ by $1\\!+\\!n_{s}^{-\\alpha}/(\\alpha\\zeta(\\alpha\\!+\\!1))$ , and multiplying whole $\\mathcal{L}_{1}$ by $1\\!+\\!n_{s}^{-\\alpha}/(\\alpha\\zeta(\\alpha\\!+\\!1))$ . It can be equivalently put as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}(n_{s},N,T)=\\left(1+\\frac{n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}+O(n_{s}^{-\\alpha-1})\\right)\\mathcal{L}_{1}\\left(\\infty,N,T\\left(1+\\frac{n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}+O(n_{s}^{-\\alpha-1})\\right)\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We always have $n_{s}\\,>\\,N$ and $N\\to\\infty$ eventually, so if dependency of $\\mathcal{L}_{1}$ with respect to $T$ is at most polynomial order, then change of main term of $\\mathcal{L}_{1}$ is negligible. We can\u2019t establish exact statements yet without the descriptions of size of ${\\mathcal{L}}_{1}$ . ", "page_idx": 38}, {"type": "text", "text": "\u2022 The term $\\mathcal{L}_{2}$ only depends on $N$ and $n_{s}$ , not on $T$ . Applying Proposition 5 (especially Eq. (162)) gives ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}(n_{s},N,T)=\\frac{N^{-\\alpha}-n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}\\frac{S^{2}}{2}+O(N^{-\\operatorname*{min}(\\alpha+1,2\\alpha)}S^{2})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "When $n_{s}$ grows faster than $N$ then $n_{s}^{-\\alpha}$ part is totally negligible, and when $n_{s}$ has same order as $N$ then $n_{s}^{-\\alpha}$ affects the constant for main term of $\\mathcal{L}_{2}$ . Things might get little complicated when $\\bar{n}_{s}=N+o(N)$ , where $N^{-\\alpha}-n_{s}^{-\\alpha}=o(N^{-\\alpha})$ can happen then. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Comparing size of $\\mathcal{L}_{1}$ and $\\mathcal{L}_{2}$ mainly depends on time. The term $\\mathcal{L}_{2}$ is fixed, and $\\mathcal{L}_{1}$ decreases as $T$ increases. For $T=\\infty$ we have $\\mathcal{L}_{1}=0$ , so $\\mathcal{L}_{2}$ having order $N^{-\\alpha}$ dominates (this proves scaling law for $N$ of exponent $\\alpha$ ), so restriction on $n_{s}$ becomes quite substantial. For small $T$ and large $N$ where the size of $\\mathcal{L}_{2}$ is small, we can expect the restriction on $n_{s}$ to be less substantial. For example, in the extreme case $N=\\infty$ , we have $\\mathcal{L}_{2}=0$ , and $n_{s}$ does not matter at all (except that, of course, it should satisfy $n_{s}\\geq N$ ). ", "page_idx": 39}, {"type": "text", "text": "For such reasons, it is hard to quantify exact conditions for $n_{s}$ such that error terms are controlled, unless we specify relative growth of $(N,T)$ . However, $n_{s}=\\omega(N)$ suffices to assure that setting $n_{s}=\\infty$ has zero effect on the main term. We will not worry about $n_{s}$ in this setting anymore too, and come back to this at the very end to determine enough $n_{s}$ . ", "page_idx": 39}, {"type": "text", "text": "J.4 Estimating main terms ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We assume $~D~=~\\infty$ and $n_{s}~=~\\infty~-$ virtually implying that $d_{k}/D~=~\\mathcal{P}_{s}(k)$ and $\\mathcal{P}_{s}(k)~=$ $k^{-\\alpha-1}/\\zeta(\\alpha+1)$ (calculated by rule of $n_{s}=\\infty$ ). We decomposed our main term into ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n_{s}\\to\\infty}\\operatorname*{lim}_{D\\to\\infty}\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\mathcal{L}_{1}+\\mathcal{L}_{2}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}=\\sum_{k=N+1}^{\\infty}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Proposition 5, $\\mathcal{L}_{2}$ is determined almost completely as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}=\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}+O(N^{-\\alpha-1}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now focus on $\\mathcal{L}_{1}$ . For ", "page_idx": 39}, {"type": "equation", "text": "$$\nF(z)={\\frac{S^{2}}{2\\left(1+\\left({\\frac{S}{r_{k}}}-1\\right)^{-1}e^{2\\eta S T z}\\right)^{2}}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "(note: it really depends on $r_{k}$ so it is correct to write $F_{k}$ , but for convenience we will keep using $F$ .) one can express $\\mathcal{L}_{1}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)F(\\mathcal{P}_{s}(k)).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma 7. Let $F(z)$ be defined as Eq. (175) ", "page_idx": 39}, {"type": "text", "text": "1. (Estimate for large $z$ ) We have ", "page_idx": 39}, {"type": "equation", "text": "$$\n0\\leq{\\cal F}(z)\\leq\\frac{(S-r_{k})^{2}}{2}\\mathrm{min}\\left(1,\\frac{S^{2}}{r_{k}^{2}}e^{-4\\eta S T z}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "2. (Estimate for small $z$ ) For $z\\geq0$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\frac{(S-r_{k})^{2}}{2}}-{\\frac{8\\eta S^{3}T}{27}}z\\leq F(z)\\leq{\\frac{(S-r_{k})^{2}}{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. The left side is obvious. For the right side, $F(z)\\leq(S-r_{k})^{2}/2$ follows from noting that F(0) = (S\u2212rk)2 and proving F \u2032(z) \u22640, and F(z) \u2264 (S\u22122rk)2rS22 e \u22124\u03b7ST z follows from just replacing $\\begin{array}{r}{1+\\left(\\frac{S}{r_{k}}-1\\right)^{-1}e^{2\\eta S T z}}\\end{array}$ in the denominator of $F$ by $\\left(\\frac{S}{r_{k}}\\,-1\\right)^{-1}e^{2\\eta S T z}$ ", "page_idx": 39}, {"type": "text", "text": "2. For the left side, it suffices to show $\\begin{array}{r}{-F^{\\prime}(z)\\leq\\frac{8\\eta S^{3}T}{27}}\\end{array}$ . One can calculate ", "page_idx": 40}, {"type": "equation", "text": "$$\nF^{\\prime}(z)=-2S^{2}r_{k}(1-\\frac{r_{k}}{S})^{2}\\eta T\\frac{e^{2\\eta S T z}}{\\left(1+\\frac{r_{k}}{S}(e^{2\\eta S T z}-1)\\right)^{3}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\nF^{\\prime\\prime}(z)=-4S^{3}r_{k}(1-\\frac{r_{k}}{S})^{2}\\eta^{2}T^{2}\\frac{e^{2\\eta S T z}(1-\\frac{r_{k}}{S}-\\frac{2r_{k}}{S}e^{2\\eta S T z})}{\\left(1+\\frac{r_{k}}{S}(e^{2\\eta S T z}-1)\\right)^{4}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "so $F$ has unique inflection point at ", "page_idx": 40}, {"type": "equation", "text": "$$\n1-\\frac{r_{k}}{S}-\\frac{2r_{k}}{S}e^{2\\eta S T z}=0\\quad\\Rightarrow\\quad e^{2\\eta S T Z}=\\frac{1}{2}\\left(\\frac{S}{r_{k}}-1\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and this point is where $-F^{\\prime}(z)$ obtains maximum. Substituting this to the expression of F \u2032(z) gives \u2212F \u2032(z) = 8\u03b72S73T. ", "page_idx": 40}, {"type": "text", "text": "Our threshold for distinguishing two approximation methods will be set as $z\\;=\\;z_{0}\\;=\\;(\\zeta(\\alpha+$ $1)\\eta S T)^{-1}$ , where both two error terms are bounded by $O(S^{2})$ . The constant $\\zeta(\\alpha+1)$ is set to make later calculations much easier. Applying Lemma 7 gives ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{L}_{1}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)F(\\mathcal{P}_{s}(k))}}\\\\ &{=}&{\\sum_{1\\leq k\\leq N,\\mathcal{P}_{s}(k)<z_{0}}\\frac{(S-r_{k})^{2}}{2}\\mathcal{P}_{s}(k)}\\\\ &{}&{+\\ O\\left(\\eta S^{3}T\\sum_{1\\leq k\\leq N,\\mathcal{P}_{s}(k)<z_{0}}\\mathcal{P}_{s}(k)^{2}+S^{2}\\sum_{1\\leq k\\leq N,\\mathcal{P}_{s}(k)>z_{0}}\\mathcal{P}_{s}(k)\\mathrm{min}\\left(1,\\frac{S^{2}}{r_{k}^{2}}e^{-4\\eta S T\\mathcal{P}_{s}(k)}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Denote ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal M=\\displaystyle\\sum_{1\\leq k\\leq N,\\mathcal P_{s}(k)<z_{0}}\\frac{(S-r_{k})^{2}}{2}\\mathcal P_{s}(k)}\\\\ &{\\mathcal S_{1}=\\eta S^{3}T\\mathop{\\sum_{1\\leq k\\leq N,\\mathcal P_{s}(k)<z_{0}}^{\\infty}}_{1\\leq k\\leq N,\\mathcal P_{s}(k)<z_{0}}\\mathcal P_{s}(k)^{2}}\\\\ &{\\mathcal S_{2}=S^{2}\\displaystyle\\sum_{1\\leq k\\leq N,\\mathcal P_{s}(k)\\geq z_{0}}\\mathcal P_{s}(k)\\mathrm{min}\\left(1,\\frac{S^{2}}{r_{k}^{2}}e^{-4\\eta S T\\mathcal P_{s}(k)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proposition 6. Suppose that there exists $0<r<\\sqrt{S}$ such that $r\\,\\le\\,r_{k}\\,<\\,S/2$ for all $k$ . In the decomposition of ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n_{s}\\to\\infty}\\operatorname*{lim}_{D\\to\\infty}\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\mathcal{M}+\\mathcal{L}_{2}+O(\\mathcal{\\ E}_{1}+\\mathcal{E}_{2})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "given as above, we have the following bound. ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{L}_{2}=\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}+O(S^{2}N^{-\\alpha-1})}}\\\\ {{\\displaystyle\\mathcal{M}=\\mathcal{E}_{1}=0}}\\\\ {{\\displaystyle\\mathcal{E}_{2}=O\\left(S^{2}(\\log(S/r))^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "2. If $(\\eta S T)^{1/(\\alpha+1)}<N,$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{2}+\\mathcal{M}=\\Theta\\left(S^{2}\\sum_{k>(\\eta S T)^{1/(\\alpha+1)}}\\mathcal{P}_{s}(k)\\right)=\\Theta(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)})}\\\\ &{\\qquad\\quad\\mathcal{C}_{1}=O\\left(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)}\\\\ &{\\qquad\\quad\\mathcal{C}_{2}=O\\left(S^{2}(\\log(S/r))^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Here all constants in $O$ and $\\Theta$ terms are absolute with respect to $\\eta,S,T,N$ . (They may depend on $\\alpha$ .) ", "page_idx": 41}, {"type": "text", "text": "Proof We first note that the condition $\\mathcal{P}_{s}(k)<z_{0}=(\\zeta(\\alpha+1)\\eta S T)^{-1}$ is equivalent to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{P}_{s}(k)<z_{0}=(\\zeta(\\alpha+1)\\eta S T)^{-1}\\,\\Leftrightarrow\\,k^{-\\alpha-1}<\\frac{1}{\\eta S T}\\,\\Leftrightarrow\\,k>(\\eta S T)^{1/(\\alpha+1)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus we can rephrase the descriptions of terms as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}=\\displaystyle\\sum_{(\\eta S T)^{1/(\\alpha+1)}<k\\leq N}\\frac{(S-r_{k})^{2}}{2}\\mathcal{P}_{s}(k)}\\\\ &{\\overset{\\mathcal{C}}{\\mathcal{C}_{1}}=\\eta S^{3}T\\sum_{(\\eta S T)^{1/(\\alpha+1)}<k\\leq N}\\mathcal{P}_{s}(k)^{2}}\\\\ &{\\overset{\\mathcal{C}}{\\mathcal{C}_{2}}=S^{2}\\sum_{k\\leq\\operatorname*{min}((\\eta S T)^{1/(\\alpha+1)},N)}\\mathcal{P}_{s}(k)\\mathrm{min}\\left(1,\\frac{S^{2}}{r_{k}^{2}}e^{-4\\eta S T\\mathcal{P}_{s}(k)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Applying Proposition 5 easily shows that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}=\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}+O(S^{2}N^{-\\alpha-1}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For $\\mathcal{M}$ and $\\mathcal{E}_{1}$ , we will consider them by dividing two cases depending on whether $(\\eta S T)^{1/(\\alpha+1)}>$ $N$ or $(\\eta S T)^{1/(\\alpha+1)}\\,<\\,N$ . If $(\\eta S T)^{1/(\\alpha+1)}>\\dot{N}$ , then the condition $(\\eta S T)^{1/(\\alpha+1)}\\,<\\,k\\,\\le\\,N$ is never satisfied, so $\\mathcal{M}=\\mathcal{E}_{1}=0$ . Now suppose $(\\eta S T)^{1/(\\alpha+1)}<N$ . We first note that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}+\\mathcal{M}=\\sum_{(\\eta S T)^{1/(\\alpha+1)}<k\\leq N}\\frac{(S-r_{k})^{2}}{2}\\mathcal{P}_{s}(k)+\\sum_{k>N}\\frac{S^{2}}{2}\\mathcal{P}_{s}(k).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "As $(S-r_{k})^{2}=\\Theta(S^{2})$ , we can let ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}+\\mathcal{M}=\\Theta\\left(S^{2}\\sum_{k>(\\eta S T)^{1/(\\alpha+1)}}\\mathcal{P}_{s}(k)\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and using Proposition 5 gives the desired estimate $\\mathcal{L}_{2}+\\mathcal{M}=\\Theta(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)})$ . For $\\mathcal{E}_{1}$ , estimating sum of $\\mathcal{P}_{s}(k)^{2}$ using Lemma 6 gives ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{E}_{1}=O\\left(\\eta S^{3}T\\sum_{k>(\\eta S T)^{1/(\\alpha+1)}}k^{-2(\\alpha+1)}\\right)=O\\left(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For ${\\mathcal{E}}_{2}$ we always have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\ell_{2}^{\\circ}\\le S^{2}\\sum_{k\\le(\\eta S T)^{1/(\\alpha+1)}}\\mathcal{P}_{s}(k)\\mathrm{min}\\left(1,\\frac{S^{2}}{r^{2}}e^{-4\\eta S T\\mathcal{P}_{s}(k)}\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "regardless of the size of $N$ , so it suffices to bound this sum. If we denote $l=(\\eta S T)^{1/(\\alpha+1)}$ and define ", "page_idx": 41}, {"type": "equation", "text": "$$\nF_{2}(z)=\\operatorname*{min}\\left(1,{\\frac{S^{2}}{r^{2}}}e^{-4\\eta S T z}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "it suffices to show the bound ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{k\\le l}\\mathcal{P}_{s}(k)F_{2}(\\mathcal{P}_{s}(k))=O\\left((\\log(S/r))^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We will approximate this sum as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k\\leq l}\\mathcal{P}_{s}(k)F_{2}(\\mathcal{P}_{s}(k))=\\displaystyle\\sum_{k\\leq l}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\frac{\\mathcal{P}_{s}(k)}{\\mathcal{P}_{s}(k+1)-\\mathcal{P}_{s}(k)}F_{2}(\\mathcal{P}_{s}(k))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{k\\leq l}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\frac{k^{-\\alpha-1}}{(\\alpha+1)k^{-\\alpha-2}(1+O(k^{-1}))}F_{2}(\\mathcal{P}_{s}(k))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=O\\left(\\displaystyle\\sum_{k\\leq l}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k)^{-1/(\\alpha+1)}F_{2}(\\mathcal{P}_{s}(k))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "to obtain the form of Riemann sum approximation for the integral of ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\int_{z=\\mathcal{P}_{s}(l)}^{\\infty}z^{-1/(\\alpha+1)}F_{2}(z)d z\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "at $\\mathcal{P}_{s}(l)<\\mathcal{P}_{s}(l-1)<\\cdot\\cdot<\\mathcal{P}_{s}(1)$ . As $F_{2}(z)$ is decreasing function, this Riemann sum is always less than the integral, so we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{k\\leq l}\\mathcal{P}_{s}(k)F_{2}(\\mathcal{P}_{s}(k))=O\\left(\\int_{z=\\mathcal{P}_{s}(l)}^{\\infty}z^{-1/(\\alpha+1)}F_{2}(z)d z\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We note that $\\mathcal{P}_{s}(l)=(\\zeta(\\alpha+1)\\eta S T)^{-1}$ . The threshold for $F_{2}(z)$ to become 1 is given at ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{S^{2}}{r^{2}}e^{-4\\eta S T z}=1\\quad\\Leftrightarrow\\quad z=\\frac{1}{2\\eta S T}\\log\\frac{S}{r}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "As $r<{\\sqrt{S}}$ , this value is always greater than $\\mathcal{P}_{s}(l)$ . Thus we can divide our integral as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{(\\zeta(\\alpha+1)\\eta S T)^{-1}}^{\\infty}z^{-1/(\\alpha+1)}F_{2}(z)d z}\\\\ &{=\\displaystyle\\int_{(\\zeta(\\alpha+1)\\eta S T)^{-1}}^{(2\\eta S T)^{-1}\\log(S/r)}z^{-1/(\\alpha+1)}d z+\\displaystyle\\int_{(2\\eta S T)^{-1}\\log(S/r)}^{\\infty}z^{-1/(\\alpha+1)}\\displaystyle\\frac{S^{2}}{r^{2}}e^{-4\\eta S T z}d z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The first part is bounded by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\int_{(\\zeta(\\alpha+1)\\eta S T)^{-1}}^{(2\\eta S T)^{-1}\\log(S/r)}z^{-1/(\\alpha+1)}d z=O\\left(\\left((2\\eta S T)^{-1}\\log(S/r)\\right)^{\\alpha/(\\alpha+1)}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which can be shown to be $O\\left((\\log(S/r))^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)$ . For the second part, we apply substitution of $w=4\\eta S T z$ to show ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{(2\\eta S T)^{-1}\\log(S/r)}^{\\infty}z^{-1/(\\alpha+1)}\\frac{S^{2}}{r^{2}}e^{-4\\eta S T z}d z=\\frac{S^{2}}{r^{2}}(4\\eta S T)^{-\\alpha/(\\alpha+1)}\\int_{2\\log(S/r)}^{\\infty}w^{-1/(\\alpha+1)}e^{-w}d w}}\\\\ &{}&{(214)}\\\\ &{}&{=\\frac{S^{2}}{r^{2}}(4\\eta S T)^{-\\alpha/(\\alpha+1)}\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},2\\log\\frac{S}{r}\\right)\\qquad(215)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and applying the asymptotic $\\Gamma(s,x)=O(x^{s-1}e^{-x})$ suggests that this is bounded by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\ll\\frac{S^{2}}{r^{2}}(4\\eta S T)^{-\\alpha/(\\alpha+1)}\\left(\\log\\frac{S}{r}\\right)^{-1/(\\alpha+1)}e^{-2\\log(S/r)}=O\\left((\\eta S T)^{-\\alpha/(\\alpha+1)}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Theorem 1. (Parameter scaling law) Assume the foll\u221aowing conditions: $n_{s}>N$ with $\\operatorname*{lim}(N/n_{s})=$ $\\gamma<1$ $\\gamma$ can be zero), and there exists $0\\,<\\,r\\,<\\,\\sqrt{S}$ such that $r\\,<\\,\\mathcal{R}_{k}(0)\\,<\\,S/2$ for all $k$ . If $N,T\\to\\infty$ while satisfying $N^{\\alpha+1}=o(T)$ , the expected loss $\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]$ for all datasets $\\mathcal{D}$ of size $D$ satisfies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\bf{E}}_{\\mathcal{D}}[{\\mathcal{L}}]=\\displaystyle\\frac{S^{2}(1-\\gamma^{\\alpha})}{2\\alpha\\zeta(\\alpha+1)}N^{-\\alpha}}}\\\\ {{\\quad\\quad\\quad+{\\cal O}\\left(S^{2}N^{-\\operatorname*{min}(\\alpha+1,2\\alpha)}+S^{2}\\left(\\log(S/r)\\right)^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)}}\\\\ {{\\quad\\quad\\quad+{\\cal O}\\left(S^{2}D^{-1/2}f_{\\alpha}(N)+S^{4}\\eta^{2}T^{2}D^{-1}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\nf_{\\alpha}(N)={\\left\\{\\begin{array}{l l}{1}&{i f\\alpha>1}\\\\ {\\log N}&{i f\\alpha=1}\\\\ {N^{(1-\\alpha)/2}}&{i f\\alpha<1.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The constant on the $O$ term only depends on $\\alpha$ . When $D\\gg T^{3}$ , then all the error terms involving $D$ are negligible. ", "page_idx": 43}, {"type": "text", "text": "Proof In the situation $n_{s}=\\infty$ and $D=\\infty$ , Proposition 6 shows that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\frac{S^{2}}{2\\alpha\\zeta(\\alpha+1)}N^{-\\alpha}+O\\left(S^{2}N^{-(\\alpha+1)}+S^{2}\\left(\\log(S/r)\\right)^{\\alpha/(\\alpha+1)}\\left(\\eta S T\\right)^{-\\alpha/(\\alpha+1)}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We consider the effect of $n_{s}$ first. As $\\mathcal{L}_{1}$ becomes an error term in this estimation, letting $n_{s}$ as a finite value has no effect on overall estimation. The term $\\mathcal{L}_{2}$ accounts for the main term, and letting $n_{s}$ as finite value changes it to ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{N^{-\\alpha}-n_{s}^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}\\frac{S^{2}}{2}+O(N^{-\\operatorname*{min}(\\alpha+1,2\\alpha)}S^{2}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This accounts for the factor $(1-\\gamma^{\\alpha})$ on the main term and $O(N^{-\\operatorname*{min}(\\alpha+1,2\\alpha)}S^{2})$ added to the error term. The effect of $D$ is exactly described in Proposition 4, contributing the error term of $O\\left(S^{2}D^{-1/2}f_{\\alpha}(N)+S^{4}\\eta^{2}T^{2}D^{-1}\\right)$ . Regarding the sufficient condition for $D$ , if $D\\gg T^{3}$ then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\nS^{4}\\eta T^{2}D^{-1}\\ll T^{-\\alpha/(\\alpha+1)},\\quad S^{2}D^{-1/2}f_{\\alpha}(N)\\ll T^{-3/2}N^{1/2}\\ll T^{-1}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "so all error terms involving $D$ are less than $O(T^{-\\alpha/(\\alpha+1)})$ . ", "page_idx": 43}, {"type": "text", "text": "For the situation $T=O(N^{\\alpha+1})$ however, the error terms $\\mathcal{E}_{1}$ and $\\mathcal{E}_{2}$ are of same size, so we can only say that the main term is of $O(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)})$ . ", "page_idx": 43}, {"type": "text", "text": "Theorem 2. (Upper bound for the t\u221aime scaling law) Assume the following conditions: $n_{s}>N$ , and there exists there exists $0<r<\\sqrt{S}$ such that $r<\\mathcal{R}_{k}(0)<S/2$ for all $k$ . If $N,T\\to\\infty$ while satisfying $\\eta S T=O(N^{\\alpha+1}).$ , the expected loss $\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]$ is ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=O\\left(S^{2}\\left(\\log(S/r)\\right)^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}+S^{2}D^{-1/2}f_{\\alpha}(N)+S^{4}\\eta^{2}T^{2}D^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with constant on $O$ only depending on $\\alpha$ and $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}((\\eta S T)^{1/(\\alpha+1)}/N).}\\end{array}$ , with $f_{\\alpha}$ defined as in Theorem $^{\\,l}$ . If $D\\gg N T^{\\dot{2}}$ and $D\\gg T^{3}$ , then all the error terms involving $D$ are negligible. ", "page_idx": 43}, {"type": "text", "text": "Proof The error term regarding $D$ can be obtained in the same way as Theorem 1, so we will let $D=\\infty$ for the rest of the proof. Also, we can let $n_{s}=\\infty$ , as we observed that it contributes at most to the constant factor of the upper bound and does not change the scaling. ", "page_idx": 43}, {"type": "text", "text": "In the decomposition of Proposition 6, we always have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathcal{\\ O}_{2}=O\\left(S^{2}\\left(\\log(S/r)\\right)^{\\alpha/(\\alpha+1)}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathcal{\\ell}_{1}=O\\left(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "holding regardless of $N$ , so it only remains to consider $\\mathcal{L}_{2}+\\mathcal{M}$ . If $(\\eta S T)^{1/(\\alpha+1)}\\,<\\,N$ , then $\\mathcal{L}_{2}+\\mathcal{M}$ is of size $O\\left(S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)}\\right)$ . If $(\\eta S T)^{1/(\\alpha+1)}\\geq N$ , then $N$ and $(\\eta S T)^{1/(\\alpha+1)}$ has same order, so $\\mathcal{L}_{2}+\\mathcal{M}=\\mathcal{L}_{2}=\\Theta(S^{2}N^{-\\alpha})$ is $O\\left(S^{2}(\\eta S T)^{-\\alpha/\\left(\\alpha+1\\right)}\\right)$ . Thus in either cases we have the desired bound. ", "page_idx": 43}, {"type": "text", "text": "Theorem 3. (Lower bound for the time scaling law) Assume the following conditions: $n_{s}>N$ and $0<\\mathcal{R}_{k}(0)<S/2.$ . If $N,T\\to\\infty$ while satisfying $(8\\zeta(\\alpha+1)^{-1}\\eta S T)^{1/(\\alpha+1)}<N_{\\!\\!\\!/}$ , the expected loss $\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]$ is ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]\\geq\\kappa S^{2}(\\eta S T)^{-\\alpha/(\\alpha+1)}+O\\left(\\eta^{-1}S T^{-1}+S^{2}D^{-1/2}f_{\\alpha}(N)+S^{4}\\eta^{2}T^{2}D^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for \u03ba and constant on $O$ only depending on $\\alpha$ , with $f_{\\alpha}$ defined as in Theorem $^{\\,l}$ . If $D\\gg N T^{2}$ and $D\\gg T^{3}$ , then all the error terms involving $D$ are negligible. ", "page_idx": 44}, {"type": "text", "text": "Proof The error term regarding $D$ can be obtained in the same way as Theorem 1, so we will let $D=\\infty$ for the rest of the proof. We only show the lower bound for $\\mathcal{L}_{1}$ , holding regardless of $N$ and $n_{s}$ . In Lemma 7 (Eq. (178)) we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nF(z)\\geq{\\frac{(S-r_{k})^{2}}{2}}-{\\frac{8\\eta S^{3}T}{27}}z\\geq{\\frac{S^{2}}{8}}-{\\frac{8\\eta S^{3}T}{27}}z\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for $z\\ \\geq\\ 0$ , so if $z\\ \\le\\ (4\\eta S T)^{-1}$ then $F(z)~\\ge~S^{2}/8\\:-\\:2S^{2}/27~>~S^{2}/20.$ . The condition $\\mathcal{P}_{s}(k)\\ \\leq\\ (4\\eta S T)^{-1}$ is equivalent to that $\\begin{array}{r}{k\\ \\geq\\ (4\\zeta(\\alpha+1)^{-1}\\eta S T)^{1/(\\alpha+1)}}\\end{array}$ . In evaluating $\\begin{array}{r}{\\mathcal{L}_{1}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)F(\\mathcal{P}_{s}(k))}\\end{array}$ , we will only add over $k$ in range of ", "page_idx": 44}, {"type": "equation", "text": "$$\n(4\\zeta(\\alpha+1)^{-1}\\eta S T)^{1/(\\alpha+1)}<k<(8\\zeta(\\alpha+1)^{-1}\\eta S T)^{1/(\\alpha+1)}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "From the assumption, this interval sits inside $1\\,<\\,k\\,<\\,N$ . For such $k$ we use upper bound of $F(\\mathcal{P}_{s}(k))>S^{2}\\bar{/}20$ . Then by using Proposition 5 we can obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{1}\\geq\\displaystyle\\frac{S^{2}}{20}\\sum_{\\left(4\\zeta(\\alpha+1)^{-1}\\eta S T\\right)^{1/(\\alpha+1)}<k<\\left(8\\zeta(\\alpha+1)^{-1}\\eta S T\\right)^{1/(\\alpha+1)}}\\mathcal{P}_{s}(k)}\\\\ &{\\quad=\\displaystyle\\frac{S^{2}}{20}\\left(\\frac{\\left(\\zeta(\\alpha+1)^{-1}\\eta S T\\right)^{-\\alpha/(\\alpha+1)}}{\\alpha\\zeta(\\alpha+1)}(4^{-\\alpha/(\\alpha+1)}-8^{-\\alpha/(\\alpha+1)})+O\\left((\\eta S T)^{-1}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The possible effect of $n_{s}$ on the main term is to multiply both the main term by and $T$ by $(1+n_{s}^{-\\alpha})$ , so it increases the bound. ", "page_idx": 44}, {"type": "text", "text": "The condition $(8\\zeta(\\alpha+1)^{-1}\\eta S T)^{1/(\\alpha+1)}<N$ is not absolutely necessary for lower bound. The condition $(\\eta S T)^{1/(\\alpha+1)}\\,=\\,\\Theta(N)$ and $n_{s}\\;\\geq\\;2N$ would suffice and one can formulate a similar theorem, although the constant of lower bound might be much smaller if $(\\eta S T)^{1/(\\alpha+1)}/N$ is small. ", "page_idx": 44}, {"type": "text", "text": "Lastly, we provide a simpler version of those results combined and discuss the special case where the optimal compute $C=N T$ , or the given engineering budget, is specified. ", "page_idx": 44}, {"type": "text", "text": "Corollary 3. (Summary of the large data estimation) Assuming $D\\gg N T^{2},T^{3}$ and $n_{s}\\gg N^{1+\\epsilon}$ such that effects of $n_{s}$ and $D$ are negligible, then for $N,T\\to\\infty$ we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta_{\\eta,S,r}\\left(\\operatorname*{max}(N^{-\\alpha},T^{-\\alpha/(\\alpha+1)})\\right),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $\\Theta_{\\eta,S,r}$ denotes that the implied constant depends on $\\eta,S,\\alpha$ and $r\\,=\\,\\mathrm{min}\\,\\mathcal{R}_{k}(0)\\,>\\,0$ . In particular, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nN^{\\alpha+1}=O(T)\\quad\\Rightarrow\\quad\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta_{\\eta,S,r}(N^{-\\alpha})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\nT=O(N^{\\alpha+1})\\quad\\Rightarrow\\quad\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta_{\\eta,S,r}(T^{-\\alpha/(\\alpha+1)}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof Apply Theorem 1 if $N^{\\alpha+1}=o(T)$ and Theorem 2 and Theorem 3 if $N^{\\alpha+1}\\gg T$ ", "page_idx": 44}, {"type": "text", "text": "Corollary 4. (The \u2018computationally optimal\u2019 case) Denote $C=N T$ and assume the conditions in Corollary 3. Then we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]\\gg C^{-\\alpha/(\\alpha+2)}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "When $N=\\Theta(C^{1/(\\alpha+2)})$ and $T=\\Theta(C^{(\\alpha+1)/(\\alpha+2)})$ , we achieve $\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta(C^{-\\alpha/(\\alpha+2)})$ . (Its implied constant may depend on implied constant for growth of $N$ and $T$ .) ", "page_idx": 44}, {"type": "text", "text": "Proof The first part follows from ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]\\gg\\operatorname*{max}(N^{-\\alpha},T^{-\\alpha/(\\alpha+1)})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{max}(N^{-\\alpha},T^{-\\alpha/(\\alpha+1)})\\ge(N^{-\\alpha})^{1/(\\alpha+2)}(T^{-\\alpha/(\\alpha+1)})^{(\\alpha+1)/(\\alpha+2)}=(N T)^{-\\alpha/(\\alpha+2)}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The second part can be checked by substituting $(N,T)\\;=\\;(C^{1/(\\alpha+2)},C^{(\\alpha+1)/(\\alpha+2)})$ (or their constant multiples) to Corollary 3. \u25a0 ", "page_idx": 44}, {"type": "text", "text": "J.5 Computing the constant for time scaling law ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "While we have found the time scaling law $\\mathbf{E}[{\\mathcal{L}}]\\,=\\,O(T^{-\\alpha/(\\alpha+1)})$ holding for $T\\,=\\,O(N^{\\alpha+1})$ , bounds in Theorem 2 and Theorem 3 were chosen rather lazily and do not depict the correct picture. We will find the constant using a more refined estimation, but we require additional assumptions on parameters. We will focus on the setting where $D$ and $n_{s}$ are large enough to be negligible, $\\mathcal{R}_{k}\\bar{(}0)=r$ is fixed, and $T=O(N^{\\alpha+1})$ with fixed constant such that time scaling law holds. ", "page_idx": 45}, {"type": "text", "text": "Theorem 4. (Constant for time scaling law) Denote ${\\mathcal{L}}^{\\infty}$ as the loss when $D,n_{s}\\to\\infty$ so that their effect is negligible: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\infty}=\\mathcal{L}^{\\infty}(T,N)=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}+\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "When $T,N\\to\\infty$ and $\\mathrm{lim}\\,N/(\\eta S T)^{1/(\\alpha+1)}=\\lambda$ for $a$ fixed constant $\\lambda\\in(0,\\infty]$ , the following limit exists: ", "page_idx": 45}, {"type": "equation", "text": "$$\nA(\\lambda)=\\operatorname*{lim}_{T,N\\to\\infty}(\\eta S T)^{\\alpha/(\\alpha+1)}\\mathcal{L}^{\\infty}(T,N).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The prefactor constant $\\boldsymbol{\\mathcal{A}}$ as the a function of $\\lambda$ (when $\\lambda=\\infty$ then let $\\lambda^{-\\alpha}=\\lambda^{-(\\alpha+1)}=0;$ is ", "page_idx": 45}, {"type": "equation", "text": "$$\nA(\\lambda)=\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}\\int_{\\lambda^{-(\\alpha+1)}/\\zeta(\\alpha+1)}^{\\infty}u^{-1/(\\alpha+1)}\\Phi_{S,r}(u)d u+\\frac{S^{2}}{2\\alpha\\zeta(\\alpha+1)}\\lambda^{-\\alpha},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\Phi_{S,r}(u)=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r}-1\\right)^{-1}e^{2u}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof We first observe ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\infty}=\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\Phi_{S,r}(\\eta S T\\mathcal{P}_{s}(k))+\\frac{S^{2}N^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We will seek to convert it into Riemann sum form of certain integral. We start by noting that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{P}_{s}({k})=(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\frac{k}{\\alpha+1}(1+O(k^{-1}))}\\\\ {\\displaystyle=\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k)^{-1/(\\alpha+1)}(1+O(k^{-1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Denote $u_{k}=\\eta S T\\mathcal{P}_{s}(k)$ , then the sum can be approximated to ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k}\\mathcal{P}_{s}(k)\\Phi_{S,r}(\\eta S T\\mathcal{P}_{s}(k))}\\\\ &{\\approx\\displaystyle\\sum_{k}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k)^{-1/(\\alpha+1)}\\Phi_{S,r}(\\eta S T\\mathcal{P}_{s}(k))}\\\\ &{=(\\eta S T)^{-\\alpha/(\\alpha+1)}\\sum_{k}(u_{k}-u_{k+1})u_{k}^{-1/(\\alpha+1)}\\Phi_{S,r}(u_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "if we ignore small $k$ . As $\\Phi_{S,r}$ is decreasing, this corresponds to Riemann sum taking minimum in the interval $[u_{k+1},u_{k}]$ . So integral provides an upper bound for this sum. Similarly, we can approximate it with Riemann sum taking maximum in $[u_{k},u_{k-1}]$ if we use ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{P}_{s}(k)=\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}(\\mathcal{P}_{s}(k-1)-\\mathcal{P}_{s}(k))\\mathcal{P}_{s}(k-1)^{-1/(\\alpha+1)}(1+O(k^{-1}))\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "instead. As $\\Phi_{S,r}$ shows exponential decay, we can ignore values at small $k$ , so this shows ", "page_idx": 45}, {"type": "equation", "text": "$$\n(\\eta S T)^{-\\alpha/(\\alpha+1)}\\sum_{k}(u_{k}-u_{k+1})u_{k}^{-1/(\\alpha+1)}\\Phi_{S,r}(u_{k})\\approx\\int_{u_{N}}^{\\infty}u^{-1/(\\alpha+1)}\\Phi_{S,r}(u)d u\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and from that ", "page_idx": 46}, {"type": "equation", "text": "$$\nu_{N}=\\eta S T N^{-(\\alpha+1)}\\zeta(\\alpha+1)^{-1}=\\lambda^{-(\\alpha+1)}\\zeta(\\alpha+1)^{-1}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "we obtain our desired result. ", "page_idx": 46}, {"type": "text", "text": "Theorem 4 basically tells that for $N=\\lambda(\\eta S T)^{1/(\\alpha+1)}$ and $D,n_{s}$ large enough, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal{L}\\sim\\mathcal{A}(\\lambda)(\\eta S T)^{-\\alpha/(\\alpha+1)}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with $\\boldsymbol{A}(\\lambda)$ given as Eq. (238), thus specifying the constant for time scaling law. For finite $\\lambda$ , this theorem covers the computationally optimal case of $(N,T)=(\\lambda_{1}C^{1/(\\alpha+2)},\\lambda_{2}C^{(\\alpha+1)/(\\alpha+2)})$ for some nonzero constant $\\lambda_{1},\\lambda_{2}$ . For $\\lambda=\\infty$ , it describes the case $\\dot{T}=o(N^{\\alpha+1})$ where effect of $N$ is negligible. ", "page_idx": 46}, {"type": "text", "text": "Corollary 5. Denote ${\\mathcal{L}}^{\\infty}$ as ${\\mathcal{L}}^{\\infty}$ as the loss when $D,n_{s}\\to\\infty$ same as Eq. (236). Denote $C=N T$ and suppose that ", "page_idx": 46}, {"type": "equation", "text": "$$\n(N,\\eta S T)=(\\lambda(\\eta S C)^{1/(\\alpha+2)},\\lambda^{-1}(\\eta S C)^{(\\alpha+1)/(\\alpha+2)})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for a fixed constant $0<\\lambda<\\infty$ . Then as $C\\to\\infty$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\infty}=A\\left(\\lambda^{(\\alpha+2)/(\\alpha+1)}\\right)\\lambda^{\\alpha/(\\alpha+1)}\\left(\\eta S C\\right)^{-\\alpha/(\\alpha+2)}\\left(1+o(1)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{A}}$ is given as Eq. (238) of Theorem $^{4}$ . ", "page_idx": 46}, {"type": "text", "text": "Proof As $\\mathrm{lim}\\,N/(\\eta S T)^{1/(\\alpha+1)}=\\lambda^{(\\alpha+2)/(\\alpha+1)}$ under above conditions, we can apply Theorem 4 and substituting Eq. (250) into Eq. (249) gives the desired result. ", "page_idx": 46}, {"type": "text", "text": "Technically we can optimize ${\\mathcal{L}}^{\\infty}$ for a given fixed value of $C=N T$ by letting $\\lambda$ as argument of minimum of $\\mathcal{A}\\left(\\lambda^{\\left(\\alpha+2\\right)/\\left(\\alpha+1\\right)}\\right)\\lambda^{-\\alpha/\\left(\\alpha+1\\right)}$ , although it seems almost impossible to obtain any form of formula for such $\\lambda$ . ", "page_idx": 46}, {"type": "text", "text": "Lastly, we provide the following estimate for the time scale constant $(\\mathcal{A}(\\lambda))$ when $r$ is small, especially the first term in Eq. (238). ", "page_idx": 46}, {"type": "text", "text": "Proposition 7. As $r\\rightarrow0$ , we have $\\Lambda>0$ fixed) ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\int_{\\Lambda}^{\\infty}u^{-1/(\\alpha+1)}\\Phi_{S,r}(u)d u\\approx\\left(\\log\\frac{S-r}{r}\\right)^{\\alpha/(\\alpha+1)}\\frac{2^{1/(\\alpha+1)}S^{2}(\\alpha+1)}{4\\alpha}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof Denote $\\begin{array}{r}{M=\\left(\\frac{S}{r}-1\\right)}\\end{array}$ , and replace $u$ by $(\\log M)v$ . Then we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\Lambda}^{\\infty}u^{-1/(\\alpha+1)}\\Phi_{S,r}(u)d u=(\\log M)^{\\alpha/(\\alpha+1)}\\frac{S^{2}}{2}\\int_{\\Lambda/\\log M}^{\\infty}\\frac{v^{-1/(\\alpha+1)}d v}{\\left(1+M^{2v-1}\\right)^{2}}}}\\\\ &{}&{=(\\log M)^{\\alpha/(\\alpha+1)}\\frac{S^{2}}{2}\\int_{0}^{\\infty}1_{v\\geq\\Lambda/\\log M}\\frac{v^{-1/(\\alpha+1)}d v}{\\left(1+M^{2v-1}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "As $M\\rightarrow\\infty$ , the integrand converges to ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{M\\rightarrow\\infty}1_{v\\geq\\Lambda/\\log M}{\\frac{v^{-1/(\\alpha+1)}d v}{\\left(1+M^{2v-1}\\right)^{2}}}={\\binom{v^{-1/(\\alpha+1)}}{0}}\\quad{\\mathrm{if~}}v\\geq1/2.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The integrand is bounded by $v^{-1/(\\alpha+1)}$ if $v\\leq1/2$ and $v^{-1/(\\alpha+1)}e^{-2(2v-1)}$ if $v>1/2$ , those of which are all integrable. So we can apply Lebesgue\u2019s dominated convergence theorem to show ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{M\\to\\infty}\\int_{\\Lambda/\\log M}^{\\infty}\\frac{v^{-1/(\\alpha+1)}d v}{\\left(1+M^{2v-1}\\right)^{2}}=\\int_{0}^{\\infty}\\left(\\operatorname*{lim}_{M\\to\\infty}^{\\infty}1_{v\\ge\\Lambda/\\log M}\\frac{v^{-1/(\\alpha+1)}d v}{\\left(1+M^{2v-1}\\right)^{2}}\\right)}}\\\\ &{}&{=\\int_{0}^{1/2}v^{-1/(\\alpha+1)}d v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Thus we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\operatorname*{lim}_{r\\to0}\\left(\\log\\displaystyle\\frac{S-r}{r}\\right)^{-\\alpha/(\\alpha+1)}\\int_{\\Lambda}^{\\infty}u^{-1/(\\alpha+1)}\\Phi_{S,r}(u)d u=\\displaystyle\\frac{S^{2}}{2}\\int_{0}^{1/2}v^{-1/(\\alpha+1)}d v}}\\\\ {{\\displaystyle=\\frac{2^{1/(\\alpha+1)}S^{2}(\\alpha+1)}{4\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "which can be observed to be equivalent to the desired expression of Eq. (252). ", "page_idx": 46}, {"type": "text", "text": "J.6 Estimates for large $T$ and threshold between data/parameter scaling ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "The estimates for small $D$ require different techniques from estimates for large $D$ . We will consider the situation $T$ grows much faster than $D$ and $N$ , and discuss when data scaling law of ${\\mathcal{L}}\\,=$ $\\Theta(D^{-\\alpha/(\\alpha+1)})$ happens. We will consider a simpler setting of $\\left.n_{s}=\\infty\\right.$ \u2019 or equivalently that effects of $n_{s}$ are negligible $(n_{s}=\\omega(N)$ seems to suffice) and $\\mathcal{R}_{k}(0)=r<S$ is fixed, although it won\u2019t be impossible to discuss their subtle effects. ", "page_idx": 47}, {"type": "text", "text": "First we single out effect of $T$ by comparing $\\mathcal{L}(T)$ and $\\mathcal{L}(\\infty)$ . We remind ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(T)=\\frac{S^{2}}{2\\left(1+\\left(\\frac{S}{r}-1\\right)^{-1}e^{2\\eta d_{k}S T/D}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and its limit when $T\\to\\infty$ is given as ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{k}(\\infty)=\\operatorname*{lim}_{T\\rightarrow\\infty}{\\mathcal{L}}_{k}(T)={\\left\\{\\frac{(S-r)^{2}}{2}\\quad{\\mathrm{if~}}d_{k}=0\\right.}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proposition 8. Suppose that $\\mathcal{R}_{k}(0)=r<S$ is fixed. For large $T$ , we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(T)]-\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(\\infty)]=O\\left(S^{4}r^{-2}D e^{-4\\eta S T/D}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof As $\\mathcal{L}_{k}(T)$ is decreasing in $T$ , we always have $\\mathcal{L}_{k}(T)\\geq\\mathcal{L}_{k}(\\infty)$ so therefore ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(T)]-\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(\\infty)]\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "So we only need to establish an upper bound for $\\mathscr{L}_{k}(T)-\\mathscr{L}_{k}(\\infty)$ . We note that $\\mathscr{L}_{k}(T)-\\mathscr{L}_{k}(\\infty)$ when $d_{k}=0$ , so one can write ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(T)-\\mathcal{L}_{k}(\\infty)=1_{d_{k}>0}\\mathcal{L}_{k}(T)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $1_{d_{k}>0}$ denotes the characteristic function ", "page_idx": 47}, {"type": "equation", "text": "$$\n1_{d_{k}>0}=\\left\\{1\\quad\\mathrm{if}\\;d_{k}>0\\right.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We use simple bound of ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(T)<\\frac{S^{2}}{2\\left(\\left(\\frac{S}{r}-1\\right)^{-1}e^{2\\eta d_{k}S T/D}\\right)^{2}}<\\frac{S^{4}}{2}r^{-2}e^{-4\\eta d_{k}S T/D}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "As $d_{k}$ follows binomial distribution $B(D,\\mathcal{P}_{s}(k))$ , considering its moment generating function gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{E}_{d_{k}}[e^{-4\\eta d_{k}S T/D}]=\\left(1-\\mathcal{P}_{s}(k)+\\mathcal{P}_{s}(k)e^{-4\\eta S T/D}\\right)^{D}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "so thus ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{E}_{d_{k}}[1_{d_{k}>0}e^{-4\\eta d_{k}S T/D}]=\\left(1-\\mathcal{P}_{s}(k)+\\mathcal{P}_{s}(k)e^{-4\\eta S T/D}\\right)^{D}-(1-\\mathcal{P}_{s}(k))^{D}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Meanwhile, for $0\\leq u,v\\leq1$ real numbers, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n|u^{D}-v^{D}|=|u-v||u^{D-1}+u^{D-2}v+\\cdot\\cdot+v^{D-1}|\\leq D|u-v|\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "so, applying this inequality to above gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{E}_{d_{k}}[1_{d_{k}>0}e^{-4\\eta d_{k}S T/D}]\\le D\\mathcal{P}_{s}(k)e^{-4\\eta S T/D}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, we can deduce ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phantom{\\leq}\\mathbf{E}_{d_{k}}[\\mathcal{L}_{k}(T)]-\\mathbf{E}_{d_{k}}[\\mathcal{L}_{k}(\\infty)]=\\mathbf{E}_{d_{k}}[\\boldsymbol{1}_{d_{k}>0}\\mathcal{L}_{k}(T)]}&{}\\\\ &{\\phantom{\\leq\\overset{,}{\\leq}\\frac{S^{4}r^{-2}}{2}}\\mathbf{E}_{d_{k}}[\\boldsymbol{1}_{d_{k}>0}e^{-4\\eta d_{k}S T/D}]}\\\\ &{\\phantom{\\leq\\frac{S^{4}r^{-2}}{2}}\\leq\\frac{S^{4}r^{-2}}{2}D e^{-4\\eta S T/D}\\mathcal{P}_{s}(k)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and thus ", "page_idx": 48}, {"type": "equation", "text": "$$\n0\\leq\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(T)]-\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(\\infty)]<\\frac{S^{4}r^{-2}}{2}D e^{-4\\eta S T/D}\\sum_{k=1}^{\\infty}\\mathcal{P}_{s}(k)^{2}=O\\left(S^{4}r^{-2}D e^{-4\\eta S T/D}\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "This provides an almost complete account for the effect of very large $T$ . We will let $T=\\infty$ from this point. We have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(\\infty)]=\\frac{(S-r)^{2}}{2}\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}+\\frac{S^{2}}{2}\\sum_{k=N+1}^{\\infty}\\mathcal{P}_{s}(k).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Applying Lemma 6 gives ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{k=N+1}^{\\infty}\\mathcal{P}_{s}(k)=\\frac{N^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}+O(N^{-\\alpha-1})\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "so it suffices to focus on the first sum. We will divide the range of $k$ into two $1\\leq k\\leq M$ and $M<k\\le N$ . For the sum over $1\\leq k\\leq M$ , we will apply the following simple bound (in the last part, we used $1-x\\leq e^{-x}$ ) ", "page_idx": 48}, {"type": "equation", "text": "$$\n0\\le\\sum_{k=1}^{M}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}\\le(1-\\mathcal{P}_{s}(M))^{D}\\le e^{-\\mathcal{P}_{s}(M)D}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For the sum over $M<k\\le N$ , we will approximate the sum into some integral, which happens to be incomplete gamma function. ", "page_idx": 48}, {"type": "text", "text": "Proposition 9. For $2<M<N$ integers, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=M+1}^{N}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}}\\\\ &{=\\!D^{-\\alpha/(\\alpha+1)}\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}\\left(\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(N)\\right)-\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(M)\\right)\\right)}\\\\ &{\\qquad+\\,O\\left(D^{-(2\\alpha+1)/(\\alpha+1)}+D^{-\\alpha/(\\alpha+1)}M^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Here $\\Gamma$ denotes the incomplete gamma function ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\Gamma\\left(s,x\\right)=\\int_{x}^{\\infty}y^{s-1}e^{-y}d y.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof Consider the interval $[\\mathcal{P}_{s}(N),\\mathcal{P}_{s}(M)]$ and its partition $\\mathcal{P}=\\{\\mathcal{P}_{s}(N)<\\mathcal{P}_{s}(N-1)<\\cdot\\cdot<$ $\\mathcal{P}_{s}(M)\\}$ . For a function $f(x)=x^{-1/(\\alpha+1)}(1-x)^{D}$ , we will consider its upper and lower Darboux sums with respect to $\\mathcal{P}$ . As $f$ is decreasing in $(0,1]$ , its upper and lower Darboux sums are given respectively as ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{U(f,\\mathcal{P})=\\displaystyle\\sum_{k=M}^{N-1}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k+1)^{-1/(\\alpha+1)}(1-\\mathcal{P}_{s}(k+1))^{D}}}\\\\ {{L(f,\\mathcal{P})=\\displaystyle\\sum_{k=M}^{N-1}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k)^{-1/(\\alpha+1)}(1-\\mathcal{P}_{s}(k))^{D}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and those give bound of the integral of $f$ as ", "page_idx": 48}, {"type": "equation", "text": "$$\nL(f,\\mathcal{P})\\leq\\int_{\\mathcal{P}_{s}(N)}^{\\mathcal{P}_{s}(M)}f(x)d x\\leq U(f,\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Meanwhile, by noting that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathcal{P}_{s}(k)=\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k)^{-1/(\\alpha+1)}(1+O(k^{-1}))\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "one can show ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=M}^{N}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}}}\\\\ &{=}&{\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}\\left(\\displaystyle\\sum_{k=M}^{N-1}(\\mathcal{P}_{s}(k)-\\mathcal{P}_{s}(k+1))\\mathcal{P}_{s}(k)^{-1/(\\alpha+1)}(1-\\mathcal{P}_{s}(k))^{D}\\right)(1+O(M^{-1}))}\\\\ &{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n=\\!\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}L(f,\\mathcal{P})(1+O(M^{-1})).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Applying a similar argument for upper Darboux sum gives ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{k=M}^{N}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}=\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}U(f,\\mathcal{P})(1+O(M^{-1}))\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and from Eq. (283) it follows ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{k=M}^{N}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}=\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}\\left(\\int_{\\mathcal{P}_{s}(N)}^{\\mathcal{P}_{s}(M)}x^{-1/(\\alpha+1)}(1-x)^{D}d x\\right)(1+O(M^{-1}))\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "From now we will estimate the integral ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{P}_{s}(N)}^{\\mathcal{P}_{s}(M)}x^{-1/(\\alpha+1)}(1-x)^{D}d x.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We replace $x=y/D$ in the integral inside, then it becomes ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{P}_{s}(N)}^{\\mathcal{P}_{s}(M)}x^{-1/(\\alpha+1)}(1-x)^{D}d x=D^{-\\alpha/(\\alpha+1)}\\int_{D\\mathcal{P}_{s}(N)}^{D\\mathcal{P}_{s}(M)}y^{-1/(\\alpha+1)}\\left(1-\\frac{y}{D}\\right)^{D}d y.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We want to approximate $\\begin{array}{r}{\\left(1-\\frac{y}{D}\\right)^{D}}\\end{array}$ by $e^{-y}$ , so we will estimate difference between them. We have ", "page_idx": 49}, {"type": "equation", "text": "$$\nD\\log(1-y/D)=-y-\\sum_{k=2}^{\\infty}{\\frac{y^{k}}{k D^{k-1}}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "so if $D>2y$ then ", "page_idx": 49}, {"type": "equation", "text": "$$\n-y>D\\log(1-y/D)=-y-{\\frac{1}{D}}\\sum_{k=2}^{\\infty}{\\frac{y^{k}}{k D^{k-2}}}>-y-{\\frac{1}{D}}\\sum_{k=2}^{\\infty}{\\frac{y^{k}}{2(2y)^{k-2}}}=-y-{\\frac{y^{2}}{D}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "so ", "page_idx": 49}, {"type": "equation", "text": "$$\ne^{-y}\\left(1-\\frac{y^{2}}{D}\\right)<e^{-y}e^{-y^{2}/D}<\\left(1-\\frac{y}{D}\\right)^{D}<e^{-y},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where we used the inequality $1-x\\leq e^{-x}$ . As $\\mathcal{P}_{s}(M)<1/2$ if $M>2$ (obvious from $\\mathcal{P}_{s}(M)<$ $(\\mathcal{P}_{s}(1)+\\mathcal{P}_{s}(2))/2<\\bar{1}/2)$ , any $y$ in the interval $[D\\mathcal{P}_{s}(N),\\dot{D}\\mathcal{P}_{s}(M)]$ satisfies $D>2y$ . So, we can apply this approximation in every $y$ . It follows that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{{=}}\\int_{D P_{s}(N)}^{D P_{s}(M)}y^{-1/(\\alpha+1)}\\left(1-\\frac{y}{D}\\right)^{D}d y}\\\\ &{{=}\\int_{D P_{s}(N)}^{D P_{s}(M)}y^{-1/(\\alpha+1)}e^{-y}d y+O\\left(\\int_{D P_{s}(N)}^{D P_{s}(M)}y^{-1/(\\alpha+1)}e^{-y}\\frac{y^{2}}{D}d y\\right)}\\\\ &{{=}\\int_{D P_{s}(N)}^{D P_{s}(M)}y^{-1/(\\alpha+1)}e^{-y}d y+O\\left(D^{-1}\\int_{0}^{\\infty}y^{-1/(\\alpha+1)}e^{-y}y^{2}d y\\right)}\\\\ &{{=}\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(N)\\right)-\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(M)\\right)+O(D^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Combining this with Eq. (289) and Eq. (291) gives the desired result. ", "page_idx": 49}, {"type": "text", "text": "We combine Proposition 8 and Proposition 9 together to obtain this final estimation result. ", "page_idx": 49}, {"type": "text", "text": "Theorem 5. (Scaling laws for large time estimation) Suppose that $N,D\\to\\infty$ and $n_{s}\\gg N^{1+\\epsilon}$ for some $\\epsilon>0$ so that effect of $n_{s}$ is negligible. Suppose that $\\mathcal{R}_{k}(0)=r$ for all $1\\leq k\\leq N$ . ", "page_idx": 50}, {"type": "text", "text": "1. (Parameter scaling law) If $N=o(D^{1/(\\alpha+1)})$ , then we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\frac{S^{2}}{2\\alpha\\zeta(\\alpha+1)}N^{-\\alpha}+O\\left(S^{2}D^{-\\alpha/(\\alpha+1)}+S^{2}N^{-\\alpha-1}+S^{4}r^{-2}D e^{-4\\eta S T/D}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "2. (Data scaling law) If $D=O(N^{\\alpha+1})$ and $\\mu=\\mathrm{lim}(D/N^{\\alpha+1})$ exists (it can be zero), then ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=D^{-\\alpha/(\\alpha+1)}\\left(\\frac{(S-r)^{2}\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{2(\\alpha+1)}\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},\\frac{D}{N^{\\alpha+1}\\zeta(\\alpha+1)}\\right)+\\frac{S^{2}(D/N^{\\alpha+1})^{\\alpha+1}}{2\\alpha\\zeta(\\alpha+1)}\\Gamma^{\\alpha+1}\\right)^{-1/(\\alpha+1)}}\\\\ &{\\phantom{\\quad\\quad}+\\mathcal{O}\\left(S^{2}D^{-(2\\alpha+1)/(2\\alpha+2)}+S^{4}r^{-2}D e^{-4\\eta S T/D}\\right)\\phantom{\\frac{(D-1)^{\\alpha+1}}{(\\alpha+1)^{-1/(\\alpha+1)}}\\Gamma^{\\alpha+1}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Here $\\Gamma$ denotes the incomplete gamma function ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\Gamma\\left(s,x\\right)=\\int_{x}^{\\infty}y^{s-1}e^{-y}d y.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In particular, if $D=o(N^{\\alpha+1})$ such that $\\mu=0$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=D^{-\\alpha/(\\alpha+1)}\\frac{(S-r)^{2}\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{2(\\alpha+1)}\\Gamma\\left(\\frac{\\alpha}{\\alpha+1}\\right)(1+o(1))}\\\\ &{\\qquad\\qquad\\qquad+\\ O\\left(S^{4}r^{-2}D e^{-4\\eta S T/D}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In either case, $T\\gg D(\\log D)^{1+\\epsilon}$ for some $\\epsilon>0$ implies that error terms involving $T$ are negligible. ", "page_idx": 50}, {"type": "text", "text": "Proof Proposition 8 states ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(T)]-\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(\\infty)]=O\\left(S^{4}r^{-2}D e^{-4\\eta S T/D}\\right)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and we showed ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}(\\infty)]=\\frac{(S-r)^{2}}{2}\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}+\\frac{S^{2}}{2}\\sum_{k=N+1}^{\\infty}\\mathcal{P}_{s}(k)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{k=N+1}^{\\infty}\\mathcal{P}_{s}(k)=\\frac{N^{-\\alpha}}{\\alpha\\zeta(\\alpha+1)}+O(N^{-\\alpha-1}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For the sum of $\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}$ over $1\\leq k\\leq N$ , we use the estimate (see Eq. (276)) of ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{M}\\mathcal{P}_{s}(k)(1-\\mathcal{P}_{s}(k))^{D}=O\\left(e^{-\\mathcal{P}_{s}(M)D}\\right)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and the estimate of Proposition 9. Combining all those gives ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\bf~E}_{D}[\\mathcal{L}]}\\\\ &{=\\!\\!\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}\\!\\!\\!\\!\\!}\\\\ &{+{\\cal D}^{-\\alpha/(\\alpha+1)}\\!\\!\\!\\frac{(S-r)^{2}\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{2(\\alpha+1)}\\left(\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(N)\\right)-\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(M)\\right)\\right)}\\\\ &{+O\\left(S^{2}(D^{-(2\\alpha+1)/(\\alpha+1)}+D^{-\\alpha/(\\alpha+1)}M^{-1}+N^{-\\alpha-1}+e^{-\\mathcal{P}_{s}(M)D})+S^{4}r^{-2}e^{-4\\eta S T/D}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We will prove our main statement by choosing appropriate $M$ depending on size comparison between $D$ and $N$ . ", "page_idx": 50}, {"type": "text", "text": "1. If $N=o(D^{1/(\\alpha+1)})$ , then we let $M=3$ , and also regard all incomplete gamma function values as $O(1)$ . Then it follows ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}+O\\left(S^{2}D^{-\\alpha/(\\alpha+1)}+S^{2}N^{-\\alpha-1}+S^{4}r^{-2}e^{-4\\eta S T/D}\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and thus obtaining the parameter scaling law. ", "page_idx": 51}, {"type": "text", "text": "2. Suppose $D=O(N^{\\alpha+1})$ and $\\mu=\\mathrm{lim}(D/N^{\\alpha+1})$ exists. We want ", "page_idx": 51}, {"type": "equation", "text": "$$\nD^{-\\alpha/(\\alpha+1)}{\\frac{(S-r)^{2}\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{2(\\alpha+1)}}\\Gamma\\left({\\frac{\\alpha}{\\alpha+1}},D\\mathcal P_{s}(N)\\right)+{\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "to be our main term, and set $M<N$ such that the term ", "page_idx": 51}, {"type": "equation", "text": "$$\nS^{2}D^{-\\alpha/(\\alpha+1)}\\Gamma\\left({\\frac{\\alpha}{\\alpha+1}},D\\mathcal{P}_{s}(M)\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and error terms not depending on $T$ given as ", "page_idx": 51}, {"type": "equation", "text": "$$\nO\\left(S^{2}(D^{-(2\\alpha+1)/(\\alpha+1)}+D^{-\\alpha/(\\alpha+1)}M^{-1}+N^{-\\alpha-1}+e^{-\\mathcal{P}_{s}(M)D})\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "are all bounded by $O(D^{-(2\\alpha+1)/(2\\alpha+2)})$ . Set $M\\ =\\ D^{1/(2\\alpha+2)}$ . Then ${\\mathcal{P}}_{s}(M)\\;=\\;$ $D^{-1/2}/\\zeta(\\alpha+1)$ , so applying the asymptotic $\\Gamma(s,x)=O(x^{s-1}e^{-x})$ gives ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},D\\mathcal{P}_{s}(M)\\right)=O\\left(D^{-1/2(\\alpha+1)}e^{-\\sqrt{D}/\\zeta(\\alpha+1)}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "This term and $\\begin{array}{r l r}{e^{-\\mathcal{P}_{s}(M)D}}&{{}=}&{e^{-\\sqrt{D}/\\zeta(\\alpha+1)}}\\end{array}$ are less than $\\begin{array}{r l}{D^{-\\alpha/(\\alpha+1)}M^{-1}}&{{}=}\\end{array}$ $O(D^{-(2\\alpha+1)/(2\\alpha+2)})$ , and obviously $D^{-(2\\alpha+1)/(\\alpha+1)}$ is less than $D^{-(2\\alpha+1)/(2\\alpha+2)}$ . Thus it follows that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf E}_{\\mathcal{D}}[\\mathcal{L}]=D^{-\\alpha/(\\alpha+1)}\\frac{(S-r)^{2}\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{2(\\alpha+1)}\\Gamma\\left(\\frac{\\alpha}{\\alpha+1},\\frac{D}{N^{\\alpha+1}\\zeta(\\alpha+1)}\\right)+\\frac{S^{2}N^{-\\alpha}}{2\\alpha\\zeta(\\alpha+1)}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~+O\\left(S^{2}D^{-(2\\alpha+1)/(2\\alpha+2)}+S^{4}r^{-2}D e^{-4\\eta S T/D}\\right).~~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Regarding the final statement regarding sufficient condition for large $T,\\,T\\gg D(\\log D)^{1+\\epsilon}$ implies ", "page_idx": 51}, {"type": "equation", "text": "$$\nD e^{-4\\eta S T/D}<D e^{-4\\eta S(\\log D)^{1+\\epsilon}}<D\\cdot D^{-4\\eta S(\\log D)^{\\epsilon}}\\ll D^{-K}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for any $K>0$ , showing that the error term $O\\left(S^{4}r^{-2}D e^{-4\\eta S T/D}\\right)$ is negligible compared to all other error terms of Eq. (299) and Eq. (300). \u25a0 ", "page_idx": 51}, {"type": "text", "text": "We also provide a summary of all large time estimation results. ", "page_idx": 51}, {"type": "text", "text": "Corollary 6. (Summary of large time estimation) Assuming $T\\gg D(\\log D)^{1+\\epsilon}$ and $n_{s}\\gg N^{1+\\epsilon}$ such that effects of $n_{s}$ and $T$ are negligible, and $\\mathcal{R}_{k}(0)=r$ for all $1\\leq k\\leq N$ . Then for $D,N\\to\\infty$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta_{\\eta,S,r}\\left(\\operatorname*{max}(N^{-\\alpha},D^{-\\alpha/(\\alpha+1)})\\right),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\Theta_{\\eta,S,r}$ denotes that the implied constant depends on $\\eta,S,r$ and $\\alpha$ . In particular, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\nN^{\\alpha+1}=O(D)\\quad\\Rightarrow\\quad\\mathbf{E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta_{\\eta,S,r}(N^{-\\alpha})\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\nD={\\cal O}(N^{\\alpha+1})\\quad\\Rightarrow\\quad{\\bf E}_{\\mathcal{D}}[\\mathcal{L}]=\\Theta_{\\eta,S,r}\\bigl(D^{-\\alpha/(\\alpha+1)}\\bigr).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof Just summarize the results of Theorem 5. ", "page_idx": 51}, {"type": "text", "text": "K Methods ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "In this section, we present the methods used in our experiments. ", "page_idx": 52}, {"type": "text", "text": "K.1 2-layer MLP ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "We trained a 2-layer fully connected neural network (MLP) with ReLU activations. All parameters of the MLP were initialized with a Gaussian distribution with a standard deviation of 0.001. The input dimension of the model was $n_{s}+n_{b}=5+32$ where $n_{s}$ is the length of control bits (number of skills) and $n_{b}$ is the length of the skill bits. Each skill has $m=3$ mutually exclusive sparse bits that are used to express the skill function. The target scale was $S=5$ . The model was trained with SGD without momentum and no weight decay (the exception is the parameter emergence experiment where Adam with learning rate 0.001 and weight decay of $5\\times10^{-5}$ was used to escape the local minima).6 For the data emergence experiment, the learning rate was halved every $50,000$ step. ", "page_idx": 52}, {"type": "text", "text": "The skill strength $\\mathcal{R}_{k}(T)$ (Eq. (7)) was measured using 20, 000 i.i.d samples from the $k^{t h}$ skill.7 For the time emergence, the skill strengths were measured every 50 steps, while for other experiments, they were measured after training. To mimic the infinite parameter $N\\rightarrow\\infty$ , we used the model of width 1000 (for the hidden layer). To mimic the infinite time $T\\to\\infty$ , we trained for $5\\times10^{5}$ steps $(3\\times10^{4}$ steps for time emergence) where each step had the batch size of 4000 (2000 for the data emergence experiment). To mimic $D\\rightarrow\\infty$ , we sampled new data points for every batch. The details are given in the following table. ", "page_idx": 52}, {"type": "table", "img_path": "cuWsR25bbI/tmp/ce163a63f63dc0d6be9443170e65792e124d5e6a47e7654b3d91c6278daff288.jpg", "table_caption": [], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "K.2 Transformer ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "This section outlines the transformer architecture used in Fig. 4. Data is encoded as for the 2-layer MLP, but with one-hot positional encoding appended to the data. We use a basic decoder transformer with 1 block, an initial embedding layer with output dimension 512, and a final linear layer. For the attention mechanism, we used 4 attention heads. For non-linearity, we used ReLU. A batch size of 5000 was used with a target scale $S=1$ and default Pytorch initialization. The model was trained with SGD with a learning rate of $5\\times10^{-5}$ , weight decay of $10^{-5}$ , and momentum with $\\beta=0.9$ . At every 100 steps, the skill strength $\\mathcal{R}_{k}(T)$ (Eq. (7)) was measured using 20, 000 i.i.d samples from the $k^{t h}$ skill. ", "page_idx": 52}, {"type": "text", "text": "K.3 Measurement of skill strength ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "The skill strength $\\mathcal{R}_{k}$ is a simple linear correlation between the learned function $f$ \u2013 function expressed by NN \u2013 and $g_{k}$ for $\\mathcal{P}_{b}$ given $I=k$ . We approximate the expectation over $X$ by taking the mean over $20,000$ i.i.d samples from $\\mathcal{P}_{b}$ for the $k^{t h}$ skill: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathcal{R}_{k}=\\mathbf{E}_{X}[f(k,X)g_{k}(k,X)]\\approx\\frac{1}{20000}\\sum_{j=1}^{20000}f(k,x^{(j)})g_{k}(k,x^{(j)}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "7Note that except the data scaling law experiment, the training set size is infinite. ", "page_idx": 52}, {"type": "text", "text": "K.4 Details of the scaling law experiment ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "For the loss of the model (solid lines) in Fig. 2, we used the analytic equation for the model (Eq. (12)) under suitable assumptions such as sufficiently large $n_{s}$ (Table 2). For the scaling laws (dotted lines) in Fig. 2, we used the exponents from Appendix $\\boldsymbol{\\mathrm E}$ or Appendix J and the prefactor constants from Theorem 4 (time scaling law), Theorem 5 (data scaling law), and Theorem 1 (parameter scaling law). For the hyperparameters of the simulation, we used $\\bar{n_{s}}=10^{5}$ such that $n_{s}$ is large compared to other resources; $S=1$ and $\\mathcal{R}_{k}(0)=0.01$ such that $S-\\mathcal{R}_{k}(0)\\approx S$ ; and $\\eta=1$ . ", "page_idx": 53}, {"type": "text", "text": "Time scaling law. The total loss as a function of $T$ for $D,N\\to\\infty$ (Fig. 2(a), solid) is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{S^{2}}{2}\\sum_{k=1}^{n_{s}}\\mathcal{P}_{s}(k)\\frac{1}{\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which follows by taking $D\\rightarrow\\infty$ and $N=n_{s}$ on Eq. (12). The scaling law (Fig. 2(a), dotted) is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathscr{L}=\\mathscr{A}_{T}T^{-\\alpha/(\\alpha+1)},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the exponent is derived in Appendix E.1 or Theorems 2 and 3. The prefactor constant is ", "page_idx": 53}, {"type": "equation", "text": "$$\nA_{t}=\\frac{S^{2}}{2}\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{(\\alpha+1)(\\eta S)^{\\alpha/(\\alpha+1)}}\\int_{0}^{\\infty}\\frac{u^{-1/(\\alpha+1)}}{\\left(1+\\left(\\frac{S}{r}-1\\right)^{-1}e^{2u}\\right)^{2}}d u,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which we obtained by taking $D\\rightarrow\\infty$ on Eq. (238). ", "page_idx": 53}, {"type": "text", "text": "Data scaling law. The total loss as a function of $D$ when $N,T\\to\\infty$ (Fig. 2(b), solid) is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbf{E}_{D}\\left[\\mathcal{L}\\right]=\\frac{S^{2}}{2}\\sum_{k=1}^{n_{s}}\\left(1-\\mathcal{P}_{s}(k)\\right)^{D}\\mathcal{P}_{s}(k),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which follows from Eq. (58). The scaling law (Fig. 2(b), dotted) is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{A}_{D}D^{-\\alpha/(\\alpha+1)},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the exponent follows from Appendix E.2 or Theorem 5. The prefactor constant is ", "page_idx": 53}, {"type": "equation", "text": "$$\nA_{D}={\\frac{S^{2}}{2}}{\\frac{\\zeta(\\alpha+1)^{-1/(\\alpha+1)}}{\\alpha+1}}\\Gamma\\left({\\frac{\\alpha}{\\alpha+1}}\\right)\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which we obtained by taking $N\\rightarrow\\infty$ in Eq. (302). ", "page_idx": 53}, {"type": "text", "text": "Parameter scaling law. The total loss as a function of $N$ when $T,D\\to\\infty$ (Fig. 2(c), solid) is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{S^{2}}{2}\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which follows from taking $T,D\\to\\infty$ on Eq. (12). The scaling law (Fig. 2(c), dotted) is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{A}_{N}N^{-\\alpha},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the exponent follows from Theorem 1. The prefactor constant is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{A_{N}=\\frac{S^{2}}{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which we obtained by taking $D,T\\to\\infty$ , $N/n_{s}\\rightarrow0$ , and $\\zeta(\\alpha+1)\\approx\\alpha^{-1}$ in Eq. (217). ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{S^{2}}{2}\\sum_{k=1}^{N}\\mathcal{P}_{s}(k)\\frac{1}{\\left(1+\\left(\\frac{S}{\\mathcal{R}_{k}(0)}-1\\right)^{-1}e^{2\\eta\\mathcal{P}_{s}(k)S T}\\right)^{2}}+\\sum_{k=N+1}^{n_{s}}\\mathcal{P}_{s}(k),\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "which follows by taking $\\begin{array}{r l}{D}&{{}\\to}&{\\infty}\\end{array}$ in Eq. (12). In Fig. 3, we plotted for $N\\quad\\in$ $\\{10,20,50,70,100,200,500,700,1000,2000,5000,10000\\}$ and $T\\in[1,1000]$ as examples of different tradeoff between $T$ and $N$ for fixed $C$ . ", "page_idx": 54}, {"type": "text", "text": "The scaling law (Fig. 3, dotted) is ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{A}_{c}C^{-\\alpha/(\\alpha+2)},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the exponent is derived in Appendix E.4 or Corollary 4. Using Corollary 5, the prefactor constant is ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathcal{A}_{c}=\\mathcal{A}\\left(\\lambda^{\\left(\\alpha+2\\right)/\\left(\\alpha+1\\right)}\\right)\\lambda^{\\alpha/\\left(\\alpha+1\\right)}\\left(\\eta S\\right)^{-\\alpha/\\left(\\alpha+2\\right)}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\mathcal{A}:\\mathbb{R}\\rightarrow\\mathbb{R}$ is defined in Eq. (238). We used the minimum value of $\\mathcal{A}_{c}$ for $\\lambda\\in(0,\\infty]$ . ", "page_idx": 54}, {"type": "text", "text": "K.5 Estimates of the compute use ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "On CPU, our emergence experiments on the 2-layer MLP (Fig. 1) take $2\\sim5$ hours for a single run of time emergence experiments and $20\\sim40$ hours for a single run of other experiments depending on the CPU. All experiments were repeated 10 times (except for parameter emergence where we repeated the experiment 50 times). Each experiment requires memory of at most 5GB. The CPU cluster in which we experimented contained the following CPUs: Intel(R) Core(TM) i5-7500, i7- 9700K, i7-8700; and Intel(R) Xeon(R) Silver 4214R, Gold 5220R, Silver 4310, Gold 6226R, E5-2650 v2, E5-2660 v3, E5-2640 v4, Gold 5120, Gold 6132. The transformer experiment (Fig. 4) takes $48\\sim72$ hours for each run; we used an RTX4090 with 24GB RAM, with 1 CPU from the list above. ", "page_idx": 54}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The claims in the abstract and introduction accurately reflect the paper\u2019s contributions, as evidenced by the contribution list in the introduction section, which references the sections presenting each result. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 55}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: The paper discusses the limitations of the multilinear model in Section 5.5 and the general limitations regarding the assumptions about the framework in Section 6 (Discussion and Conclusion). These sections address the robustness of the results and the scope of the claims made. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 55}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: All theoretical results are accompanied by intuitive explanations in the main text, with detailed derivations (Appendices C and E) and rigorous proofs (Appendix J) in the supplemental material. In addition, an alternative derivation of the scaling laws via stage-like training is given in Appendix D. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 56}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We have provided details \u2013 architecture, initialization, learning setup, and measurements \u2013 of our experiments in Appendix K including the details of NN for emergence experiment (Fig. 1) in Appendix K.1, the details of scaling law experiment (Figs. 2 and 3) in Appendix K.4, the details of the transformer experiment (Fig. 4) in Appendix K.2. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 56}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 57}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We provide a link to our source code in the main text. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 57}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We specify the details in Appendix K.1 (2-layer NN) and Fig. 4 (transformer). Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 57}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: Yes, all our figures in the main text, which are not simulations (Figs. 1 and 4), have 1-standard deviation error bars. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 57}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 58}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: We provide the compute resource details in Appendix K.5. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 58}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The research conducted in this paper adheres to the principles outlined in the NeurIPS Code of Ethics. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 58}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: This research aims to deepen the fundamental understanding of emergence phenomena and scaling laws in deep learning. As our theoretical and empirical investigations are conducted in a carefully controlled, idealized environment, we do not anticipate any immediate societal consequences arising directly from the findings of this particular study. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 59}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: The research presented in this paper does not involve the release of data or models that pose a risk for misuse. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 59}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: This paper does not use any existing assets from other sources. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 59}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 60}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 60}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcing experiments or research with human subjects. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 60}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] Justification: No IRB approvals or equivalent reviews were required. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 61}]