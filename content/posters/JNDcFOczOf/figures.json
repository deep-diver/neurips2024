[{"figure_path": "JNDcFOczOf/figures/figures_8_1.jpg", "caption": "Figure 1: Cumulative regret for static CVaR over different \u03b1", "description": "This figure shows the cumulative regret for the static CVaR objective across different values of \u03b1 (risk aversion levels). The x-axis represents the number of episodes, and the y-axis represents the cumulative regret.  Four subplots are presented, each corresponding to a different \u03b1 value (0.05, 0.10, 0.20, 0.40).  Each subplot displays the performance of three algorithms: RA-PbRL (the proposed algorithm), PbOP (a risk-neutral baseline), and ICVaR-RLHF (a risk-aware baseline). The shaded areas represent the confidence intervals. The figure demonstrates that RA-PbRL generally exhibits lower regret than the baseline methods across various levels of risk aversion.", "section": "5 Experiment Results"}, {"figure_path": "JNDcFOczOf/figures/figures_8_2.jpg", "caption": "Figure 1: Cumulative regret for static CVaR over different \u03b1", "description": "The figure shows the cumulative regret for static CVaR for four different values of \u03b1 (0.05, 0.10, 0.20, 0.40).  Each subplot represents a different \u03b1 value and shows the performance of three algorithms: RA-PbRL, PbOP, and ICVaR-RLHF. The y-axis represents the cumulative regret, and the x-axis represents the number of episodes.  The shaded regions around the lines represent confidence intervals. The figure demonstrates the performance of the RA-PbRL algorithm compared to existing methods across different levels of risk aversion (represented by \u03b1).", "section": "5 Experiment Results"}, {"figure_path": "JNDcFOczOf/figures/figures_9_1.jpg", "caption": "Figure 3: Cumulative regret for static CVaR in the MuJoCo setting over different \u03b1.", "description": "This figure displays the cumulative regret for static CVaR across four different risk aversion levels (\u03b1 = 0.05, 0.10, 0.20, 0.40) in the MuJoCo Half-Cheetah environment.  It compares the performance of the proposed RA-PbRL algorithm against two baseline algorithms: PbOP and ICVaR-RLHF.  The x-axis represents the timestep, while the y-axis shows the cumulative regret. The plot illustrates how the cumulative regret changes over time for each algorithm under various risk aversion settings.", "section": "5 Experiment Results"}, {"figure_path": "JNDcFOczOf/figures/figures_14_1.jpg", "caption": "Figure 4: Cumulative regret for the different \u03b1", "description": "This figure shows a comparison of cumulative regret between two policies (policy A and policy B) under different risk levels (\u03b1).  The MDP instance is designed to have identical reward distributions for both policies, leading to similar preferences but differing risk profiles.  The policies share the same actions in the first two steps but differ in the third step.  This figure demonstrates the impact of the choice of risk measure (nested vs. static CVaR) on the overall cumulative regret.  The nested CVaR and static CVaR methods are shown for different \u03b1 values (risk aversion levels).", "section": "Difference between nested and static risk measure"}, {"figure_path": "JNDcFOczOf/figures/figures_24_1.jpg", "caption": "Figure 5: Hard to learn case 1", "description": "This figure presents two scenarios, (a) optimal policy and (b) suboptimal policy, to illustrate a hard-to-learn instance for the nested CVaR RA-PbRL algorithm.  The instance features a state space with absorbing states X1, X2, and X3 and a set of intermediate states S1...Sn.  The transition probabilities and rewards are designed such that the optimal policy leads to higher cumulative rewards, while the suboptimal policy yields lower rewards.  The difference highlights the challenge in achieving optimal risk-averse policies within the one-episode reward setting of PbRL.", "section": "E.1 Regret Lower Bound of Nested Reward"}, {"figure_path": "JNDcFOczOf/figures/figures_26_1.jpg", "caption": "Figure 6: Hard to learn case 2", "description": "This figure presents a hard-to-learn instance for the nested CVaR objective. In this instance, two policies exhibit almost identical reward distributions and, consequently, similar preferences. However, the nested CVaR metric assigns significantly different values to these policies, demonstrating that nested CVaR risk is sensitive to the order of states and actions.", "section": "E.1 Regret Lower Bound of Nested Reward"}]