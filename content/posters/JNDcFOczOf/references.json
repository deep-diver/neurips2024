{"references": [{"fullname_first_author": "Ayoub", "paper_title": "Model-based reinforcement learning with value-targeted regression", "publication_date": "2020-00-00", "reason": "This paper provides theoretical foundations for the use of model-based methods in reinforcement learning, which are relevant to the risk-aware preference-based reinforcement learning problem discussed in the target paper."}, {"fullname_first_author": "Bastani", "paper_title": "Regret bounds for risk-sensitive reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper provides theoretical analysis of risk-sensitive reinforcement learning, which directly addresses the risk-aware objectives explored in the target paper."}, {"fullname_first_author": "Bellemare", "paper_title": "A distributional perspective on reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper introduces a distributional approach to reinforcement learning, which provides insights for measuring and managing risk in reinforcement learning problems, relevant to the risk-aware preference-based setting of the target paper."}, {"fullname_first_author": "Chen", "paper_title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation", "publication_date": "2022-00-00", "reason": "This paper presents a provably efficient algorithm for preference-based reinforcement learning, serving as a foundation for the risk-aware algorithm proposed in the target paper."}, {"fullname_first_author": "Du", "paper_title": "Risk-sensitive reinforcement learning: Iterated cvar and the worst path", "publication_date": "2022-00-00", "reason": "This paper proposes a novel nested risk-aware reinforcement learning algorithm with theoretical guarantees, offering valuable insights and techniques for designing the RA-PbRL algorithm in the target paper."}]}