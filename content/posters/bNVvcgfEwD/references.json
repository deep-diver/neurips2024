{"references": [{"fullname_first_author": "L\u00e9on Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018-00-00", "reason": "This paper provides a comprehensive overview of optimization methods for large-scale machine learning, which is the foundation of the current research on Adafactor."}, {"fullname_first_author": "John Duchi", "paper_title": "Adaptive subgradient methods for online learning and stochastic optimization", "publication_date": "2011-00-00", "reason": "This paper introduced the AdaGrad algorithm, which is a crucial precursor to Adafactor and serves as a foundational work in adaptive gradient methods."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: a method for stochastic optimization", "publication_date": "2015-00-00", "reason": "The Adam optimizer, introduced in this paper, is a widely used algorithm in deep learning and shares similarities with Adafactor, particularly in its adaptive learning rate mechanism."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost", "publication_date": "2018-00-00", "reason": "This is the paper that introduced Adafactor, the main subject of the current work, and therefore its importance is self-evident."}, {"fullname_first_author": "Rachel Ward", "paper_title": "Adagrad stepsizes: sharp convergence over nonconvex landscapes", "publication_date": "2020-00-00", "reason": "This paper provides important theoretical convergence results for Adagrad in nonconvex settings, which are directly relevant to the analysis of Adafactor in the current work."}]}