[{"type": "text", "text": "CoFie: Learning Compact Neural Surface Representations with Coordinate Fields ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanwen Jiang Haitao Yang Georgios Pavlakos Qixing Huang Department of Computer Science, The University of Texas at Austin {hwjiang,yanght,pavlakos,huangqx}@cs.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces CoFie, a novel local geometry-aware neural surface representation. CoFie is motivated by the theoretical analysis of local SDFs with quadratic approximation. We find that local shapes are highly compressive in an aligned coordinate frame defined by the normal and tangent directions of local shapes. Accordingly, we introduce Coordinate Field, which is a composition of coordinate frames of all local shapes. The Coordinate Field is optimizable and is used to transform the local shapes from the world coordinate frame to the aligned shape coordinate frame. It largely reduces the complexity of local shapes and benefits the learning of MLP-based implicit representations. Moreover, we introduce quadratic layers into the MLP to enhance expressiveness concerning local shape geometry. CoFie is a generalizable surface representation. It is trained on a curated set of 3D shapes and works on novel shape instances during testing. When using the same amount of parameters with prior works, CoFie reduces the shape error by $48\\%$ and $56\\%$ on novel instances of both training and unseen shape categories. Moreover, CoFie demonstrates comparable performance to prior works when using even $70\\%$ fewer parameters. Code and model can be found here: https://hwjiang1510.github.io/CoFie/ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the realm of geometry modeling, neural implicit shape representations have become a powerful tool [32, 7, 4, 13, 39, 37, 2]. These representations typically use latent codes to represent shapes and employ multilayer perceptions (MLPs) to decode their Signed Distance Functions (SDFs). Early works in this field use a single latent code to represent an entire shape [32]. Nevertheless, the decoded SDFs usually lack geometry details. To improve the shape modeling quality, recent approaches have introduced local-based designs [4, 25, 40]. By decomposing an entire shape into many local surfaces, the shape modeling task becomes effortless \u2013 local surfaces are in simpler geometry which are easier to represent. Despite the progress, the local-aware design significantly increases the number of parameters, as each local surface is represented by one or even multiple latent codes. Thus, proposing a neural surface representation that is both accurate and compact is necessary. ", "page_idx": 0}, {"type": "text", "text": "To achieve this goal, we argue it is important to understand the properties of local surfaces. Following prior works [30, 47, 10, 42], we approximate the local geometry with quadratic patches [9] and perform analysis. Results show the feasibility of fitting the geometry of a specific category of quadratic patches. In detail, the quadratic patches are aligned with the coordinate system defined by the normal, principal directions, and principal curvatures of quadratic patch [9, 30]. However, when the quadratic patches are not aligned \u2013 they are freely transformed with random rotations and translations in 3D, mimicking real local surfaces \u2013 the optimization will be easily trapped into local minima. This analysis reveals the difficulty of jointly recovering transformation information and geometry of local patches. ", "page_idx": 0}, {"type": "image", "img_path": "0KseSacluJ/tmp/31bdd0fb8e9e85e2b598fbb1f642c0e8530bfee5c475b14a33eea41ac558543f.jpg", "img_caption": ["Figure 1: CoFie is a local geometry-aware shape representation. (Left) CoFie divides a shape into nonoverlapping local patches, where each local patch is represented by an MLP-based Signed Distance Function. (Right) CoFie introduces Coordinate Field, which attaches a coordinate frame to each local patch. It transforms local patches from the world coordinate system to an aligned coordinate system, reducing shape complexity. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Based on the analysis, we propose CoFie, a novel local geometry-aware neural surface representation. The key insight of CoFie is decomposing the transformation information of local shapes from its geometry. As shown in Fig. 1, we associate each local surface with a learnable coordinate frame, which forms a Coordinate Field. We use the Coordinate Field to transform all local surfaces into an aligned coordinate system, reducing their spatial complexity. Thus, the geometry space of local surfaces becomes more compact, where the MLP-based neural SDFs are easier to learn. ", "page_idx": 1}, {"type": "text", "text": "An important design aspect is how to represent the Coordinate Field. Departing from the implicitbased representations, we use an explicit representation. Specifically, the coordinate frame of each local surface is parameterized by a rotation and a translation, forming a 6 Degree-of-Freedom pose. Moreover, we initialize the rotation using the estimated normal, principal direction, and principal curvature of a local surface. This design makes CoFie local geometry-aware and facilitates the learning of Coordinate Fields. ", "page_idx": 1}, {"type": "text", "text": "To better represent local surfaces\u2019 geometry, we introduce quadratic layers to the MLP. Prior works typically employ ReLU-based MLP with shallow layers and limited hidden size [32, 4]. Thus, the MLP is piece-wise linear [24] and cannot represent the distribution of local surfaces well. We demonstrate a simple quadratic layer improves the geometry modeling capability. ", "page_idx": 1}, {"type": "text", "text": "CoFie is a generalizable shape representation. After training on a curated dataset, it can represent arbitrary shapes that belong to any novel category. We evaluate CoFie on novel shape instances from both seen (training) and unseen categories, encompassing both synthetic and real shapes. Results show that CoFie outperforms prior arts, reducing the chamfer distance by $50\\%$ on instances from both seen and unseen categories. Moreover, CoFie achieves comparable results with prior work using $70\\%$ less parameters. In addition, we demonstrate that CoFie, which uses a single shared MLP for all shapes, achieves comparable results with methods that overfit a specific model for each testing shape. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Implicit Shape Representations. Implicit shape representations are state-of-the-art in encoding shape geometric details [32, 39, 37, 3, 35, 50, 46, 49, 43, 27, 15]. To improve the shape modeling capability, researchers inject local-aware designs. For example, DeepLS [4] integrates voxel grids and local MLPs to decode geometric shapes. Another line of work explores hierarchical representations where the local surfaces are divided unevenly [29, 45, 25, 40, 44, 38], leveraging Octree. For example, Multilevel Partition of Pnity [29] (MPU) blends parametric implicit surface patches into a global implicit surface. DOGNet uses dual-octree designs for neural MPU. The contribution of CoFie is perpendicular to these methods. CoFie still works on evenly divided voxels. However, instead of resolving high-frequency details of local shapes by using higher local resolution, CoFie proposes the Coordinate Field to reduce the spatial complexity. This is motivated by the analysis result that local geometric shapes are highly compressive under suitable coordinate frames. ", "page_idx": 1}, {"type": "text", "text": "The idea of a coordinate field is related to several existing approaches. For example, MVP [26] introduced oriented boxes for 3D face synthesis. However, in our setting, the variations in geometry and topology are much more significant than those of 3D human faces. Another relevant work is LDIF [17], which transforms a 3D point in the local coordinate system of each primitive to decode the iso-value of each shape. However, LDIF uses a fixed coordinate frame for each primitive. In contrast, the coordinate field varies spatially in CoFie and can be optimized, allowing us to capture detailed variations of the parts flexibly. Moreover, CoFie is based on a rigorous analysis of the expressivity of SDF and SDF learning. A follow-up approach [52] uses a warping field to transform a 3D point into a canonical space of specific categories. In contrast, CoFie is category-agnostic, beneftiing from the use of local shapes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "On the learning side, many approaches show that MLPs are expressive and that their performance depends on the loss of training. For example, SAL [1] and SALD [2] show the importance of integrating normal losses to capture geometric features. SIREN [37] introduced other regularization losses to improve the quality of implicit representations learned. Although these approaches focus on local shape details, CoFie focuses on network design using coordinate frames. The CoFie approach is orthogonal to the encoding schemes. ", "page_idx": 2}, {"type": "text", "text": "Hybrid 3D Representations. Each 3D representation has fundamental advantages and limitations from the machine learning and representation perspective. For example, implicit representations allow flexible topologies, whereas explicit representations are easier to edit. Therefore, hybrid 3D representations, which aim to add the strength of different 3D representations for representation learning, have received a lot of attention. The main stream in hybrid 3D representations sequentially applies hybrid 3D models [23, 48, 12, 11, 36, 28, 8]. For example, GRASS [23] combines a partbased representation to capture geometric structures of 3D shapes and a volumetric representation per part to capture geometric details of the parts. DSG-Net [48] employs part-based deformations to capture geometric details of the part. Other examples [12, 11, 36, 4, 14, 5, 18, 19] combine explicit graph, mesh, voxel and triplane representations with implicit volumetric representations to encode geometry details. CoFie is relevant to this series of approaches, where it combines voxel grids to encode global shapes and an implicit representation to decode local geometric details. The novelty of CoFie is that the local module employs a coordinate frame representation and enforces the prior knowledge that the local shape is roughly a low-complexity polynomial surface in the coordinate system defined by normal and principal directions. ", "page_idx": 2}, {"type": "text", "text": "Coordinate Field Optimization. The task of computing the proposed cell-based coordinate field is related to the problem of vector-field and frame-field design on meshes, where we want to ensure that the coordinate field is smooth and consistent among adjacent cells, and where we want the normal and tangent directions of each coordinate frame to align with the local fitting results if the fitting results are highly confident. This problem was studied in [33], which introduced a global optimization framework to compute a global vector field on a triangular mesh. Several more recent approaches have developed improved formulations for vector field optimization [16, 21] and extensions to frame field optimization [31, 34]. We refer to [41] for surveys on this topic. Rather than solving a global optimization problem to compute the coordinate field, the learning of the coordinate field in CoFie is driven by learning a compressive MLP. ", "page_idx": 2}, {"type": "text", "text": "3 Analysis of Fitting SDFs of Local Patches ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we provide an analysis of ftiting local surfaces. Following prior works [30, 47, 10, 42], we simplify local surfaces as quadratic patches. Additionally, we note that some works approximate local surfaces with linear patches [22, 45]. However, to handle the geometry details, it usually requires extremely high [45] or infinite resolution [22] during local surface partition. Approximating local surfaces with quadratic patch is more practical. ", "page_idx": 2}, {"type": "text", "text": "3.1 Importance of Non-linearity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A quadratic surface patch can be represented by $\\begin{array}{r}{\\pmb{f}(u,v)\\,=\\,(u,v,\\frac{1}{2}(a u^{2}+c v^{2}+2b u v))}\\end{array}$ , where $u^{2}+v^{2}\\leq r^{2}$ for locality, and $a,b,c$ are parameters for controlling the shape of the quadratic patch. The following proposition characterizes the SDF of a point $\\textbf{\\emph{p}}$ to $\\boldsymbol{\\textbf{\\textit{f}}}$ . ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 For each point $\\pmb{p}=(x,y,z)^{T}$ in the neighborhood of the origin o, the signed distance function from $\\textbf{\\emph{p}}$ to ${\\pmb f}(u,v)$ can be approximated as ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(p,f(u,v))\\approx z-\\frac{1}{2}(a x^{2}+c y^{2}+2b x y).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the approximation omits third-and-higher order terms in $x$ , $y$ , and $z$ . ", "page_idx": 3}, {"type": "text", "text": "Proof: See Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Prop. 1 suggests that the SDF is non-linear. However, a shallow MLP using ReLU activation is piecewise linear, where the ReLU activation functions essentially decompose the input space into subspaces and the function in each subspace is still linear. This motivates the use of quadratic layers instead of linear layers (Sec. 4.2). ", "page_idx": 3}, {"type": "text", "text": "To hold generality, in Appendix B, we also analyze the local surface that can not be simplified as a single quadratic patch, i.e. sharp edges as the intersection of two quadratic patches. ", "page_idx": 3}, {"type": "text", "text": "3.2 Difficulty of Fitting Transformation Information ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We demonstrate the difficulty of recovering the transformation information of quadratic patches during geometry fitting. ", "page_idx": 3}, {"type": "text", "text": "Aligned Quadratic Patches. Same as the previous section, we define the SDF of a quadratic local patch as $z\\textrm{-}\\frac{1}{2}(a x^{2}+c y^{2}+2b x y)$ , where the quadratic patch is axis-aligned. Consider a set of samples $\\{((x_{i},\\bar{y_{i}},z_{i}),d_{i}),1\\leq i\\leq n\\}$ from this quadratic patch, where $\\left(x_{i},y_{i},z_{i}\\right)$ is the location of the point $\\pmb{p}_{i}$ , $d_{i}$ is the SDF value, and $n$ is the number of samples. To fti the surface from the samples, we solve the optimization problem as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathop{\\arg\\operatorname*{min}}_{a,b,c}\\sum_{i=1}^{n}\\left(z_{i}-\\frac{1}{2}(a x_{i}^{2}+c y_{i}^{2}+2b x_{i}y_{i})-d_{i}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is a convex problem that has a unique global optimal. ", "page_idx": 3}, {"type": "text", "text": "Unaligned Quadratic Patches. Consider transforming the quadratic patch with a random rigid transformation $(R,t)$ . This quadratic patch is not axis-aligned. In this case, the SDF function is given by $z^{\\prime}-\\textstyle{\\frac{1}{2}}(a x^{\\prime^{2}}+c y^{\\prime^{2}}+2b x^{\\prime}y^{\\prime})$ where $(x^{\\prime},y^{\\prime},z^{\\prime})=R(x,y,z)+t$ . To fit the surface from the samples, we solve the optimization problem as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{a,b,c,R,t}\\sum_{i=1}^{n}\\left(z_{i}^{\\prime}-\\frac{1}{2}({a x_{i}^{\\prime}}^{2}+{c y_{i}^{\\prime}}^{2}+2b x_{i}^{\\prime}y_{i}^{\\prime})-d_{i}\\right)^{2},\\quad\\left(\\begin{array}{l}{x_{i}^{\\prime}}\\\\ {y_{i}^{\\prime}}\\\\ {z_{i}^{\\prime}}\\end{array}\\right)=R\\left(\\begin{array}{l}{x_{i}}\\\\ {y_{i}}\\\\ {z_{i}}\\end{array}\\right)+t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this case, (3) becomes non-convex and has local minima. We defer a detailed characterization of the local minima of (3) to Appendix C. ", "page_idx": 3}, {"type": "text", "text": "In general, this non-convex problem makes geometry ftiting non-trivial. It motivates the use of the Coordinate Field to explicitly model the transformation information and disentangle the transformation information of local patches from its geometry (Sec. 4.1). ", "page_idx": 3}, {"type": "text", "text": "4 CoFie ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce details of CoFie, including its representation (Sec. 4.1), MLP architecture (Sec. 4.2), and its learning scheme (Sec. 4.3). ", "page_idx": 3}, {"type": "text", "text": "4.1 CoFie Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Fig. 2, CoFie is based on a hierarchical representation, with coarse and fine-grained geometry. At the coarse level, it represents a shape with voxels. In detail, for a shape S, it divides the space that contains the shape into $V\\times V\\times V$ non-overlapping voxel grids, where $V$ is the resolution of the voxel grids. A subset of voxels that intersect with the shape surface will be valid and CoFie only consider the valid sparse voxels to ensure its efficiency. ", "page_idx": 3}, {"type": "table", "img_path": "0KseSacluJ/tmp/fff33c0994f45e7e309f5615a20c63cd4fd4840cd01b216f8c6048c774c4309b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Overview of CoFie. CoFie represents a shape using a hybrid representation of voxels/cells and local implicit functions. (Left) For preparing the data for training the MLP-based local implicit functions, we split the training shapes into local shapes and initialize their coordinate frames using PCA. (Right) During training, a point will be transformed to the aligned coordinate of all local shapes using the coordinate frame. The MLP takes the transformed point and the latent code of the local shape to predict its SDF value. During testing, we fix the MLP, optimizing the latent codes and coordinate fields of valid cells. ", "page_idx": 4}, {"type": "text", "text": "At the fine-grained level, for each valid voxel $v$ , we use an implicit representation to encode the geometry details for the local surface inside the voxel. Specifically, we use MLP-based neural SDFs. Each voxel $v$ has a latent code $z_{v}$ representing the local geometry and we use the MLP $g^{\\theta}$ to decode the SDF values. For a point $\\textbf{\\em x}$ , its SDF value contributed by the voxel $v$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\pmb{x},v)=g^{\\theta}(\\pmb{x}_{v},\\boldsymbol{z}_{v}),\\qquad\\pmb{x}_{v}=(\\pmb{n}_{v},\\pmb{t}_{v},\\pmb{n}_{v}\\times\\pmb{t}_{v})^{T}(\\pmb{x}-\\pmb{o}_{v}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(o_{v},n_{v},t_{v})$ parameterize the coordinate frame of voxel $v$ . Ideally, $\\mathbf{\\Delta}o_{v},\\,n_{v}$ and $\\scriptstyle t_{v}$ are the origin, normal direction, and tangent direction of the local surface, respectively. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, for decoding the SDF value, we transform the point from the world coordinate system to the shared coordinate system for all local surfaces. $\\scriptstyle o_{v}$ forms the translation between the two coordinate system, and $(n_{v},t_{v})$ form the rotation. ", "page_idx": 4}, {"type": "text", "text": "The final SDF value at $\\textbf{\\em x}$ is then given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\pmb{x})=\\frac{\\sum_{v\\in\\mathcal{V}}w(\\pmb{x},v)f(\\pmb{x},v)}{\\sum_{v\\in\\mathcal{V}}w(\\pmb{x},v)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nu$ is the set of all valid voxels, $w(\\pmb{x},v)$ is the weight assigned for the voxel $v$ with regard to point $\\textbf{\\em x}$ . In practice, we use $w(\\pmb{x},v)=1$ if $x\\in v$ , and $w(\\pmb{x},v)=0$ otherwise. Finally, the surface of a 3D shape is defined as the union set of local surfaces in its valid voxels $\\mathcal{V}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 CoFie MLP Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the common practice of MLP, we define ", "page_idx": 4}, {"type": "equation", "text": "$$\ng^{\\theta}({\\pmb x},z)=g_{L}^{\\theta_{L}}\\circ\\phi\\circ g_{L-1}^{\\theta_{L-1}}\\circ\\phi\\cdot\\cdot\\cdot\\circ\\phi\\circ g_{1}^{\\theta_{1}}({\\pmb x},z)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{g}_{l}^{\\theta_{l}}:\\mathbb{R}^{m_{l-1}}\\rightarrow\\mathbb{R}^{m_{l}}$ is a layer with trainable parameters $\\theta_{l}$ , and where $\\phi$ is an activation function. Denote $z_{l}$ as the output in layer $l$ , i.e., $z_{0}=(x;z)$ . A common strategy is to set each $\\pmb{g}_{l}^{\\theta_{l}}$ as a linear function, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{g}_{l}^{\\theta_{l}}(\\pmb{z}_{l-1})=A_{l}\\pmb{z}_{l-1}+\\pmb{b}_{l},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{l}=\\left(A_{l},b_{l}\\right)$ . Furthermore, $\\phi$ is chosen as the ReLU layer, i.e., $\\phi(z_{l})=\\mathrm{max}(z_{l},\\mathbf{0})$ where the max operator is applied element-wise. This strategy is widely used in prior works [32, 7]. ", "page_idx": 4}, {"type": "text", "text": "However, in Sec. 3.1, we demonstrate the SDF function has non-negligible quadratic components locally and its incompatibility with MLPs with linear layers and ReLU activation. Therefore, instead, we model the quadratic components with quadratic layers. We let the top $k$ layers of $g^{\\theta}$ to be quadratic functions, where $k\\geq1$ . The quadratic layer can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{g}_{l}^{\\theta_{l}}(\\pmb{z}_{l-1})=\\pmb{z}_{l-1}^{T}T_{l}\\pmb{z}_{l-1}+A_{l}\\pmb{z}_{l-1}+\\pmb{b}_{l}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $T_{l}\\in\\mathbb{R}^{m_{l-1}\\times m_{l}\\times m_{l-1}}$ is a tensor, and $\\theta_{l}=(T_{l},A_{l},\\pmb{b}_{l})$ . ", "page_idx": 4}, {"type": "text", "text": "We can understand the trade-offs between the use of linear layers (Eq. 6) and the quadratic layers (Eq. 7) as follows. With the same latent dimensions $m_{l}$ , the quadratic layers have many more parameters than the linear layers. Therefore, with the same network size, we have to use fewer layers or smaller latent dimensions for quadratic layers. This will limit the capability of the network instead. In practice, setting $k=1$ leads to the best performance. ", "page_idx": 5}, {"type": "text", "text": "4.3 CoFie Learning Scheme ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Problem Setup. Following DeepSDF-series, we perform shape auto-decoding [32, 4]. The task assesses the capability of models to fit/represent given shapes. During both training and inference, the input is points sampled freely in space with their ground-truth SDF values. The output is the neural SDF. Additionally, we notify that the task is different from shape reconstruction from point cloud inputs, or so-called shape auto-encoding, which is studied in [51, 8, 28]. ", "page_idx": 5}, {"type": "text", "text": "Moreover, CoFie is a generalizable shape representation. It is trained on a curated dataset with multiple shapes. Once trained, the MLP can be used to represent or decode the SDF of any incoming shapes. We note the setting of generalizable shape representation is different from overfitting a shape, where an MLP is specialized for each shape. ", "page_idx": 5}, {"type": "text", "text": "Training and Inference. We follow the protocol of the shape auto-decoding task [32, 4]. We train CoFie with a set of shapes denoted as $S=\\{S_{i},1\\leq i\\leq n\\}$ . For each shape, we perform voxelization (Sec. 4.1) and train CoFie with valid local shapes. We denote the set of valid local shapes of shape $S_{i}$ as $\\nu_{i}$ . Following [32, 37, 4], we collect a set of point samples $\\mathcal{P}_{v}=(p^{j},d^{j})$ in the neighborhood of each voxel $v\\in\\mathcal{V}_{i}$ , where $p^{j}$ and $d^{j}$ denote the position of the sample and the SDF value of $p^{j}$ . The point samples are sampled in free space and are not necessary to be on-surface points. For each local shape in voxel $v$ , we associate it with a latent code $\\scriptstyle z_{v}$ and the coordinate frame $(o_{v},n_{v},t_{v})$ . Then the training objective can be formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{\\theta,\\{o_{v},n_{v},t_{v},z_{v}\\}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{n}\\sum_{v\\in\\mathcal{V}_{i}}\\sum_{(p^{j},d^{j})\\in\\mathcal{P}_{v}}||g^{\\theta}(p_{v}^{j},z_{v})-d^{j}||_{1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{p}_{v}^{j}=(\\pmb{n}_{v},t_{v},\\pmb{n}_{v}\\times\\pmb{t}_{v})^{T}(\\pmb{p}^{j}-\\pmb{o}_{v})$ . In this step, we jointly optimize the MLP, the latent codes, and the coordinate field for all training shapes. Intuitively, it trains the MLP to represent training shapes and optimize the compatibility between the MLP, latent codes and the coordinate fields. ", "page_idx": 5}, {"type": "text", "text": "During inference, we freeze the MLP $g^{\\theta}$ . We optimize the latent code and the coordinate field for a single target shape at one time. It is formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{\\{z_{v},o_{v},n_{v},t_{v}|v\\in\\mathcal{V}\\}}\\sum_{v\\in\\mathcal{V}}\\sum_{(p^{j},n^{j})\\in\\mathcal{P}_{v}}||g^{\\theta}(p_{v}^{j},z_{v})-d^{j}||_{1}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Besides, we use the regularization term over the inferred latent codes following [32, 4]. ", "page_idx": 5}, {"type": "text", "text": "Shape Consistency at Boundary of Voxels. If we sample the points $\\mathcal{P}_{v}$ within each voxel $v$ , Eq. 8 and Eq. 9 optimize the local geometry within each voxel independently. This may lead the nonsmooth and inconsistency surface at the boundary of voxels. To solve this, we follow [4] to expand receptive field of each voxel by sampling points from their neighbouring voxels. ", "page_idx": 5}, {"type": "text", "text": "Coordinate Field Initialization. Eq. 8 has many unwanted local minima, especially for optimizing the coordinate field. Thus, a good initialization of the coordinate fields ensures the compactness of local shape at early stage of training, and facilitates the learning of MLP. Motivated by the analysis in Sec. 3.2, we use estimated normal and tangent directions to initialize the coordinate fields. In detail, we compute the derivatives of SDF values at these point samples and perform PCA to get them. Besides, $\\pmb{o}_{c}$ is initialized as the center of the cell. We find that this initialization is important to reduce errors (Sec. 5.2). ", "page_idx": 5}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents an experimental evaluation of CoFie. We begin with the experimental setup and then present the results and ablations. ", "page_idx": 5}, {"type": "image", "img_path": "0KseSacluJ/tmp/4d0d538d5be05e8e006fef0d4b4d2c541d5b419c53b8e34c2c16d15597385fe8.jpg", "img_caption": ["Figure 3: Diveristy and quality of meshes that CoFie can represent. The results include both novel instances from ShapeNet training categories (top left), instances from ShapeNet unseen categories (bottom left), and real shapes from the Thingi dataset (right). We visualize the shapes with surface normal to better show their geometry. Please see the appendix for comparisons with ground-truth. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Implementation Deatils. We use latent code of size 125 for all cells. The MLP is composed of 5 layers where the first 4 layers are linear layers and the last layer is quadratic. The hidden channel size is 128. We use the voxel grid size of $32\\times32\\times32$ . During training, we use 12 shapes for each batch. For each shape, we sample 3000 voxels that intersect with the surface of the shape (with return). We sample 24 points for each cell for training, and each point is sampled within 1.5 times the radius of the voxel to ensure boundary consistency between cells. We use the Adam optimizer [20] with learning rates $5e-4$ , $1e-3$ , and $1e-3$ for the MLP, coordinate fields, and latent codes. We train with 150000 iterations and reduce the learning rates by half for every 20000 iteration. During inference, we use a learning rate of $5e-4$ for 800 iterations. Reconstructed meshes are obtained by performing Marching Cubes with a 128 resolution by default. We use the quaternion representation for the rotation matrix of the coordinate frames. We train on 4 GPUs with 24GB memory for 1 day. ", "page_idx": 6}, {"type": "text", "text": "Training and testing data. We train CoFie on 1000 shape instances sample from ShapeNet [6] of chairs, planes, tables, lamps, and sofas (200 instances for each category). We test CoFie with three test sets for comprehensive analysis of CoFie: i) 250 novel instances from the 5 training ShapeNet categories; ii) 250 novel instances from 10 unseen ShapeNet categories; iii) 24 meshes from the Thingi dataset [53], which captures real scenes. The test set i) checks how CoFie fits the training distribution. Test sets ii) and iii) are used to test the generalization capability of CoFie on novel shapes that observe different structures with training shapes. ", "page_idx": 6}, {"type": "text", "text": "Baseline Approaches We compare our CoFie with three types of methods: generalizable methods, which use a single MLP to represent multiple shapes; shape-specific methods, which train an MLP for each testing shape. Generally, the latter genre demonstrates a better performance as the MLP model can be trained to overfti a single testing shape. Both the two types of methods performs shape auto-decoding. Besides, we also report results for a state-of-the-art shape auto-encoding method. We note that it is a reference method while the result is not directly comparable. ", "page_idx": 6}, {"type": "text", "text": "Note that CoFie is a generalizable method for shape auto-decoding. We include more details for baselines as follows. ", "page_idx": 6}, {"type": "text", "text": "\u2022 DeepSDF [32] is a generalizable shape auto-decoding method using a global latent code to represent one shape.   \n\u2022 DeepLS [4] is a generalizable shape auto-decoding method using local-based representations. DeepLS is a direct comparable baseline.   \n\u2022 NGLOD [38] is a shape-specific method for shape auto-decoding, achieving state-of-the-art performance. For a fair comparison with CoFie, we use the level of detail as 3, keeping the number of parameters of the latent codes in the same magnitude as our CoFie.   \n\u2022 3DS2VS [51] is a generalizable shape auto-encoding method. It employs transformers to predict the shape latent code, rather than getting it by optimization (shape auto-decoding). The input is on-surface point clouds. ", "page_idx": 6}, {"type": "table", "img_path": "0KseSacluJ/tmp/f3675a3191987ab4e6bd892c434c21fb25f082ff85d5ab9726d9d98e1e984eb2.jpg", "table_caption": ["Table 1: Shape errors on novel instances of the ShapeNet training categories. We report chamfer distance $(10^{-\\hat{4}})$ and highlight the best. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "0KseSacluJ/tmp/4a96e9777aeb3dffc8a6a4c31c62c1fa610800a6605217691128abfd7114b8dd.jpg", "img_caption": ["Figure 4: Trade-off between accuracy and model size ( notified by the radius of circles). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "0KseSacluJ/tmp/5704132e61b6eb24f07653b7fb9ddefe5dc0d4667d99d7777b7b5e3e6dfa1b4d.jpg", "img_caption": ["Figure 5: Comparison with prior works. (Left) Results of generalizable methods, where our CoFie demonstrates better capability for modeling geometry details. (Right) Compare with the per-shape-based method NGLOD. We note that NGLOD is a shape-specific method that overfits one MLP on one testing shape. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Besides, we also compare with state-of-the-art shape auto-encoding (point cloud reconstruction) methods. We note these methods are used as reference for understanding the model performance. They are not directly comparable. ", "page_idx": 7}, {"type": "text", "text": "We train DeepSDF, DeepLS, and CoFie using the same dataset for fair comparisons. NGLOD is trained on each test shape. All methods receive the same inputs during inference. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics We report the mesh reconstruction error as the chamfer\u2212 $L_{2}$ distance between the reconstructed and ground-truth meshes. We sample 30000 points to compute the chamfer distances. The meshes are normalized into a unit scale. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Qualitative Results. As shown in Fig. 3, CoFie demonstrates strong surface representation capability. The details of geometry are maintained well. The results on out-of-distribution shapes from unseen categories are comparable to the training categories. ", "page_idx": 7}, {"type": "text", "text": "Performance on Training Categories. As shown in Table 1, CoFie outperforms baselines by a large margin. In detail, the average chamfer distance of CoFie is 1.86 $48\\%$ relatively) smaller than the best baseline DeepLS. Moreover, we provide a more detailed comparison with DeepLS, as shown in Fig. 4. We observe that CoFie is consistently better than DeepLS with different latent code and MLP size. Specifically, CoFie with latent code size 48 achieves slightly better performance compared with DeepLS with latent code size 128. Note that the number of MLP parameters for the former is about $15\\%$ for the latter. ", "page_idx": 7}, {"type": "table", "img_path": "0KseSacluJ/tmp/612f56bc74d9a88244f0d6d830991f2569d679cc21cd6417faf8d5a40e04a09e.jpg", "table_caption": ["Table 2: Shape errors on instances of the ShapeNet novel categories. We evaluate the chamfer distance $(10^{-4})$ ). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "0KseSacluJ/tmp/1fa9f393f9f3aaeb9f344302685355d1939be8bed6c2d74e5232f87a8c601f4c.jpg", "table_caption": ["Table 3: Results on Thingi meshes. We evaluate the chamfer distance $(10^{-\\tilde{4}})$ with a marching cube resolution of 256. Note that NGLOD is trained on each test shape, while CoFie uses a shared MLP for all shapes as a generalizable method. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "0KseSacluJ/tmp/b9c037daf86f3bcc2e86895935a64fff35780c957d6ef29b50ee00d4b605f2f4.jpg", "table_caption": ["Table 4: Ablation study of (0) Base performance; (1) coordinate field and its initialization methods; (2) using quadratic MLP; (3) full performance. We use resolution 128 to get reconstructed meshes. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Performance on Unseen Categories. We compare CoFie with previous generalizable methods on ShapeNet unseen categories and the state-of-the-art per-shape-based method on the challenging real scans. We provide visualization results in Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "\u2022 ShapeNet Unseen Categories. As shown in Table 2, CoFie achieves better generalization on 9 out of 10 novel shape categories. We also observe that the performance gap between CoFie and prior works is larger in the unseen categories, showing the strong generalization capability of CoFie. \u2022 Thingi Real Shapes. As shown in Table 3, CoFie achieves comparable results with NGLOD. We note that NGLOD is a per-shape-based method, which trains a model for each shape and performs better naturally. In contrast, CoFie is trained on ShapeNet shapes. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Table 4, we experiment with CoFie variants to validate the effectiveness of our coordinate field and MLP designs. ", "page_idx": 8}, {"type": "text", "text": "Coordinate Field and Initialization. As shown in Table 4 (1), using coordinate fields with different initialization strategies can both reduce the shape error. In detail, when using axis-aligned coordinate field initialization, where all coordinate frames are initialized as the world frame, the shape error reduced slightly from 3.91 to 3.45. The result demonstrates the difficulty of optimizing coordinate frames. In contrast, when using geometry-aware initialization, i.e., initializing local frames with estimated normal and tangent directions of local shapes, the shape error is reduced to 2.33, observing a $40\\%$ improvement. ", "page_idx": 8}, {"type": "text", "text": "MLP Design. As shown in Table 4 (2), using a quadratic layer as the last layer of the MLP observes a 0.9 $23\\%$ relatively) reduction of shape error. As the use of the quadratic layer introduces additional parameters, we compare it with a variant for a fair comparison. In detail, we compare it with a linear MLP with an additional layer (6 layers in total), where the two MLPs have the same amount of parameters because the output channel size of the last layer is 1. The result shows that increasing the number of linear layers can only reduce the shape error slightly. ", "page_idx": 8}, {"type": "text", "text": "Moreover, Table 4 (3) demonstrates the combination of the two introduced techniques can jointly reduce the shape error. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper has introduced CoFie, a novel neural surface representation. It is based on the theoretical results of using a ReLU-based MLP to encode geometric shapes. The results strongly motivate the use of local coordinate frames, which encompass the coordinate fields, to transform a point before decoding its SDF value using an MLP. This leads to a hybrid representation combined with coordinate frames associated with local voxels. The experimental results show a strong generalization behavior of CoFie in new instances for shape reconstruction, which significantly outperforms previous generalizable methods and achieves comparable results to shape-specific methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations. One limitation of CoFie is that it is based on local shapes and cannot be used for the shape completion task. Different from DeepSDF, which learns global shape priors and can fill the large missing components in the input, CoFie is restricted to observable parts. We plan to incorporate more global priors into CoFie. Besides, with a fixed cell resolution, the local shape analysis is broken when a local cell intersects with thin structures. We plan to extend it with adaptive local cell resolutions. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. CoFie is a neural surface representation, which have the potential to be used for 3D reconstruction and generation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Matan Atzmon and Yaron Lipman. SAL: sign agnostic learning of shapes from raw data. In CVPR, 2020.   \n[2] Matan Atzmon and Yaron Lipman. SALD: sign agnostic learning with derivatives. In ICLR, 2021.   \n[3] Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven Dickinson, and Allan D Jepson. Representing 3d shapes with probabilistic directed distance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19343\u201319354, 2022.   \n[4] Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and Richard A. Newcombe. Deep local shapes: Learning local SDF priors for detailed 3d reconstruction. In ECCV, 2020.   \n[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16123\u201316133, 2022.   \n[6] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository, 2015.   \n[7] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, 2019.   \n[8] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4456\u20134465, 2023.   \n[9] Manfredo P. do Carmo. Differential geometry of curves and surfaces. 1976.   \n[10] Roderik GF Erens, Astrid ML Kappers, and Jan J Koenderink. Perception of local shape from shading. Perception & psychophysics, 54:145\u2013156, 1993.   \n[11] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao Zhang. Sdm-net: Deep generative network for structured deformable mesh. ACM Trans. Graph., 2019.   \n[12] Georgia Gkioxari, Justin Johnson, and Jitendra Malik. Mesh R-CNN. In ICCV, 2019.   \n[13] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In ICML, 2020.   \n[14] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023.   \n[15] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4369\u20134379, 2023.   \n[16] Wenzel Jakob, Marco Tarini, Daniele Panozzo, and Olga Sorkine-Hornung. Instant field-aligned meshes. ACM Trans. Graph., 2015.   \n[17] Chiyu \"Max\" Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, and Thomas A. Funkhouser. Local implicit grid representations for 3d scenes. In CVPR, 2020.   \n[18] Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke Zhu. Few-view object reconstruction with unknown categories and camera poses. arXiv preprint arXiv:2212.04492, 2022.   \n[19] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang. Leap: Liberate sparse-view 3d modeling from camera poses. arXiv preprint arXiv:2310.01410, 2023.   \n[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.   \n[21] Felix Kn\u00f6ppel, Keenan Crane, Ulrich Pinkall, and Peter Schr\u00f6der. Globally optimal direction fields. ACM Trans. Graph., 2013.   \n[22] Ravikrishna Kolluri. Provably good moving least squares. ACM Transactions on Algorithms (TALG), 4(2):1\u201325, 2008.   \n[23] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, and Leonidas Guibas. Grass: Generative recursive autoencoders for shape structures. ACM Trans. Graph., 2017.   \n[24] Ruiyuan Lin, Suya You, Raghuveer Rao, and C-C Jay Kuo. Constructing multilayer perceptrons as piecewise low-order polynomial approximators: a signal processing approach. arXiv preprint arXiv:2010.07871, 2020.   \n[25] Qinqing Liu, Peng-Shuai Wang, Chunjiang Zhu, Blake Blumenfeld Gaines, Tan Zhu, Jinbo Bi, and Minghu Song. Octsurf: Efficient hierarchical voxel-based molecular surface representation for protein-ligand affinity prediction. Journal of Molecular Graphics and Modelling, 105:107865, 2021.   \n[26] Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of volumetric primitives for efficient neural rendering. ACM Trans. Graph., 2021.   \n[27] Yujie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, and Lin Gao. Unsigned orthogonal distance fields: An accurate neural implicit representation for diverse 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20551\u201320560, 2024.   \n[28] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d completion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 306\u2013315, 2022.   \n[29] Yutaka Ohtake, Alexander Belyaev, Marc Alexa, Greg Turk, and Hans-Peter Seidel. Multi-level partition of unity implicits. ACM Trans. Graph., 2003.   \n[30] Yutaka Ohtake, Alexander G. Belyaev, Marc Alexa, Greg Turk, and Hans-Peter Seidel. Multi-level partition of unity implicits. ACM SIGGRAPH 2003 Papers, 2003.   \n[31] Daniele Panozzo, Enrico Puppo, Marco Tarini, and Olga Sorkine-Hornung. Frame fields: Anisotropic and non-orthogonal cross fields. ACM Trans. Graph., 2014.   \n[32] Jeong Joon Park, Peter R. Florence, Julian Straub, Richard A. Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, 2019.   \n[33] Nicolas Ray, Wan Chiu Li, Bruno L\u00e9vy, Alla Sheffer, and Pierre Alliez. Periodic global parameterization. ACM Trans. Graph., 2006.   \n[34] Nicolas Ray, Dmitry Sokolov, and Bruno L\u00e9vy. Practical 3d frame field generation. ACM Trans. Graph., 2016.   \n[35] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:6087\u20136101, 2021.   \n[36] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. In NeurIPS, 2021.   \n[37] Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In NeurIPS, 2020.   \n[38] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11358\u201311367, Washington, DC, USA, 2021. IEEE.   \n[39] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS, 2020.   \n[40] Jia-Heng Tang, Weikai Chen, Jie Yang, Bo Wang, Songrun Liu, Bo Yang, and Lin Gao. Octfield: Hierarchical implicit functions for 3d modeling. arXiv preprint arXiv:2111.01067, 2021.   \n[41] Amir Vaxman, Marcel Campen, Olga Diamanti, David Bommes, Klaus Hildebrandt, Mirela Ben-Chen Technion, and Daniele Panozzo. Directional field synthesis, design, and processing. In ACM SIGGRAPH 2017 Courses, 2017.   \n[42] Timothy P Wallace, Owen Robert Mitchell, and Keinosuke Fukunaga. Three-dimensional shape analysis using local shape descriptors. IEEE Transactions on Pattern Analysis and Machine Intelligence, (3):310\u2013 323, 1981.   \n[43] Li Wang, Weikai Chen, Xiaoxu Meng, Bo Yang, Jintao Li, Lin Gao, et al. Hsdf: Hybrid sign and distance field for modeling surfaces with arbitrary topologies. Advances in Neural Information Processing Systems, 35:32172\u201332185, 2022.   \n[44] Peng-Shuai Wang, Yang Liu, and Xin Tong. Dual octree graph networks for learning adaptive volumetric shape representations. 2022.   \n[45] Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, and Xin Tong. Adaptive o-cnn: A patch-based deep representation of 3d shapes. ACM Transactions on Graphics (TOG), 37(6):1\u201311, 2018.   \n[46] Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, Peng-Shuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, and Changhe Tu. Neural-singular-hessian: Implicit neural representation of unoriented point clouds by enforcing singular hessian. ACM Transactions on Graphics (TOG), 42(6):1\u201314, 2023.   \n[47] Ying Xiong, Ayan Chakrabarti, Ronen Basri, Steven J Gortler, David W Jacobs, and Todd Zickler. From shading to local shape. IEEE transactions on pattern analysis and machine intelligence, 37(1):67\u201379, 2014.   \n[48] Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas J. Guibas, and Lin Gao. Dsg-net: Learning disentangled structure and geometry for 3d shape generation. ACM Trans. Graph., 2022.   \n[49] Jianglong Ye, Yuntao Chen, Naiyan Wang, and Xiaolong Wang. Gifs: Neural implicit function for general shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12829\u201312839, 2022.   \n[50] Tarun Yenamandra, Ayush Tewari, Nan Yang, Florian Bernard, Christian Theobalt, and Daniel Cremers. Fire: Fast inverse rendering using directional and signed distance functions. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3077\u20133087, 2024.   \n[51] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):1\u201316, 2023.   \n[52] Zerong Zheng, Tao Yu, Qionghai Dai, and Yebin Liu. Deep implicit templates for 3d shape representation. In CVPR, 2021.   \n[53] Qingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3d-printing models, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Proof of Prop. 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $(u,v)$ be the parameters of the closest point of $\\pmb{p}=(x,y,z)^{T}$ on $\\pmb{f}(u,v)$ . We have the following constraints on $(u,v)$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\pmb{p}-\\pmb{f}(u,v))^{T}\\pmb{f}_{u}(u,v)=0}\\\\ {(\\pmb{p}-\\pmb{f}(u,v))^{T}\\pmb{f}_{v}(u,v)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\pmb{f}_{u}(u,v)=(1,0,a u+b v)^{T}}\\\\ {\\pmb{f}_{v}(u,v)=(0,1,b u+c v)^{T}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Ignoring quadratic-and-higher order terms in $u,v,x,y$ , and $z$ in (10) and (11), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(x-u)+z(a u+b v)\\approx0}\\\\ {(y-v)+z(b u+c v)\\approx0}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This leads to ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\begin{array}{l}{u}\\\\ {v}\\end{array}\\right)=\\left(\\begin{array}{c c}{1-a z}&{-b z}\\\\ {-b z}&{1-c z}\\end{array}\\right)^{-1}\\left(\\begin{array}{l}{x}\\\\ {y}\\end{array}\\right)}\\\\ {\\approx\\left(\\begin{array}{l}{x}\\\\ {y}\\end{array}\\right)+\\left(\\begin{array}{l l}{a}&{b}\\\\ {b}&{c}\\end{array}\\right)\\left(\\begin{array}{l}{x}\\\\ {y}\\end{array}\\right)z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The normal direction at $(u,v)$ is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{n}(u,v)=\\frac{\\pmb{f}_{u}(u,v)\\times\\pmb{f}_{v}(u,v)}{\\lVert\\pmb{f}_{u}(u,v)\\times\\pmb{f}_{v}(u,v)\\rVert}}\\\\ &{\\quad\\quad\\quad=\\frac{(-(a u+b v),\\,-(b u+c v),1)^{T}}{\\sqrt{1+(a u+b v)^{2}+(b u+c v)^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The signed-distance function of $\\textbf{\\emph{p}}$ to ${\\pmb f}(u,v)$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d(\\pmb{p},\\pmb{f}(u,v))=(\\pmb{p}-\\pmb{f}(u,v))^{T}\\pmb{n}(u,v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Substituting (15), (14) into (16) and ignoring third-and-higher terms in $u,v,x,y,z$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\displaystyle d(\\pmb{p},\\pmb{f}(u,v))\\approx-(x-u)(a u+b v)-(y-v)(b u+c v)}\\\\ {\\displaystyle+\\left(z-\\frac{1}{2}(a u^{2}+2b u v+c v^{2})\\right)}\\\\ {\\displaystyle\\approx z-\\frac{1}{2}(a u^{2}+2b u v+c v^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B Representing Sharp Edges as Quadratic Patches ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We consider the intersection of two quadratic patches where the intersection is along the $y$ -axis. In this case, we can define the surface patch as $\\pmb{\\bar{f}}(u,v)=(u,v,f(u,v))^{T}$ where ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(u,v)=\\left\\{\\begin{array}{l l}{\\frac{1}{2}(a_{1}u^{2}+c_{1}v^{2}+2b_{1}u v)+e_{1}u}&{\\quad u\\leq0}\\\\ {\\frac{1}{2}(a_{2}u^{2}+c_{1}v^{2}+2b_{2}u v)+e_{2}u}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In (17), we do not have any linear term in $v$ , so that the normals to these two patches at $(0,0,0)^{T}$ are in the $x z$ plane. In addition, the coefficients in front of $v^{2}$ are identical, so these two patches stitch along $u=0$ . ", "page_idx": 12}, {"type": "text", "text": "The following proposition provides an approximation to the SDF function of $\\pmb{f}(u,v)$ . ", "page_idx": 12}, {"type": "text", "text": "Proposition 2 For each point $\\pmb{p}=(x,y,z)^{T}$ in the neighborhood of the origin o, the signed distance function from $\\textbf{\\emph{p}}$ to ${\\pmb f}(u,v)$ can be approximated as ", "page_idx": 13}, {"type": "equation", "text": "$$\nd(p,f(u,v))\\approx\\left\\{\\begin{array}{l l}{z-\\frac{1}{2}\\big(a_{1}x^{2}+c_{1}y^{2}+2b_{1}x y\\big)-e_{1}x}&{\\quad x\\leq0}\\\\ {z-\\frac{1}{2}\\big(a_{2}x^{2}+c_{1}y^{2}+2b_{2}x y\\big)-e_{2}x}&{\\mathrm{otherwise}}\\end{array}\\right.\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof is very similar to that of Prop. 1. When $x\\geq0$ , the parameters $(u,v)$ of the closest point satisfy $u\\geq0$ , and vice versa. Therefore, the proof applies the description in Section A. \u25a1 ", "page_idx": 13}, {"type": "text", "text": "C Local Minima of (3) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We will show that there are nontrivial local minima due to symmetries induced by the rotation group. However, those local minima do not recover the underlying ground-truth shape. As a result, they force the network to learn the wrong patterns from the data. For simplicity, we focus on the 2D setting. The extension to 3D is straightforward. ", "page_idx": 13}, {"type": "text", "text": "In 2D, we assume that the underlying curve is $(x,k_{0}x^{2})$ . SDF samples are given by $(x,k_{0}x^{2}+y,y)$ where $x\\sim p,y\\sim q$ . Consider the 2D rigid pose parameters\u03b8, $t_{x},t_{y}$ . Let $k$ be the curve parameter. Our goal is to optimize parameters $\\theta,t_{x},t_{y},k$ to minimize the following $L^{2}$ reconstruction loss: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(k,t_{x},t_{y},\\theta)=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}\\Big(\\sin(\\theta)x+\\cos(\\theta)(k_{0}x^{2}+y)+t_{y}-}\\\\ &{\\qquad\\qquad\\qquad\\quad k\\big(\\cos(\\theta)x-\\sin(\\theta)(k_{0}x^{2}+y)+t_{x}\\big)^{2}-y\\Big)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Clearly, $(k_{0},0,0,0)$ is a global minimum of $r$ . The following proposition shows that there is another local minimum of $r$ . ", "page_idx": 13}, {"type": "text", "text": "Proposition 3 Suppose p and $q$ are independent, and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{x\\sim p}x=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then $(-k_{0},0,2c,\\pi)$ is a critical point of $r$ , where $c=\\underset{y\\sim q}{E}y.$ . In addition, it is a local minimum of r if we assume ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\times\\sim p}x^{3}=\\mathbb{E}_{\\times\\sim p}x^{5}=0,\\quad|y|\\ll|x|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We defer the proof of Prop. 3 to Appendix C.1. Prop. 3 shows that there is a non-trivial critical point whose parameters depend on the sampling pattern. As neural network training mostly uses firstorder methods that can be trapped into critical points, this means that without careful initialization, the network will memorize non-shape-related patterns from data, and significantly impairs the generalization ability of the resulting network. ", "page_idx": 13}, {"type": "text", "text": "C.1 Proof of Prop. 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{l(x,y,k,t_{x},t_{y},\\theta)=\\sin(\\theta)x+\\cos(\\theta)(k_{0}x^{2}+y)+t_{y}-}\\\\ {k\\big(\\cos(\\theta)x-\\sin(\\theta)(k_{0}x^{2}+y)+t_{x}\\big)^{2}-y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is easy to check that ", "page_idx": 13}, {"type": "equation", "text": "$$\nl(x,y,-k_{0},0,2c,\\pi)=2c-2y.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The first-order gradients of $l$ with respect to $k,t_{x},t_{y},\\theta$ are given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\frac{\\partial{\\cal l}}{\\partial k}(x,y,-k_{0},0,2c,\\pi)=-x^{2}},}}\\\\ {{{\\displaystyle\\frac{\\partial{\\cal l}}{\\partial t_{x}}(x,y,-k_{0},0,2c,\\pi)=-k_{0}x},}}\\\\ {{{\\displaystyle\\frac{\\partial{\\cal l}}{\\partial t_{y}}(x,y,-k_{0},0,2c,\\pi)=1,}}}\\\\ {{{\\displaystyle\\frac{\\partial{\\cal l}}{\\partial\\theta}(x,y,-k_{0},0,2c,\\pi)=-x-k_{0}^{2}x^{3}-k_{0}x y.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial r}{\\partial k}(-k_{0},0,2c,\\pi)=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}\\frac{\\partial l}{\\partial k}(x,y,-k_{0},0,2c,\\pi)l(x,y,-k_{0},0,2c,\\pi)}\\\\ {=-\\underset{x\\sim p y\\sim q}{\\mathbb{E}}(2c-2y)x^{2}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial r}{\\partial t_{x}}(-k_{0},0,2c,\\pi)=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}\\frac{\\partial l}{\\partial t_{x}}(x,y,-k_{0},0,2c,\\pi)l(x,y,-k_{0},0,2c,\\pi)}\\\\ {=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}(2c-2y)k x=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial r}{\\partial t_{y}}(-k_{0},0,2c,\\pi)=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}\\frac{\\partial l}{\\partial t_{y}}(x,y,-k_{0},0,2c,\\pi)l(x,y,-k_{0},0,2c,\\pi)}\\\\ {=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}(2c-2y)=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial r}{\\partial t_{x}}(-k_{0},0,2c,\\pi)=\\underset{x\\sim p y\\sim q}{\\mathbb{E}}\\frac{\\partial l}{\\partial t_{x}}(x,y,-k_{0},0,2c,\\pi)l(x,y,-k_{0},0,2c,\\pi)}\\\\ {=-\\underset{x\\sim p y\\sim q}{\\mathbb{E}}(2c-2y)(x+k_{0}^{2}x^{3}+k_{0}x y)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This means that $(-k_{0},0,2c,\\pi)$ is a critical point of $r$ . To show that it is indeed a local minimum, we study the second-order derivatives of $r$ . We begin with the second-order derivatives of $l$ . They are ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial^{2}l}{\\partial^{2}\\theta}(x,y,-k_{0},0,2c,\\pi)=2k_{0}(k_{0}x^{2}+y)^{2}+y-k_{0}x^{2}}\\\\ {\\displaystyle\\frac{\\partial^{2}l}{\\partial\\theta\\partial t_{x}}(x,y,-k_{0},0,2c,\\pi)=2k_{0}(k_{0}x^{2}+y)}\\\\ {\\displaystyle\\frac{\\partial^{2}l}{\\partial^{2}t_{x}}(x,y,-k_{0},0,2c,\\pi)=2k_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial^{2}l}{\\partial^{2}k}(x,y,-k_{0},0,2c,\\pi)=0}}\\\\ {{\\displaystyle\\frac{\\partial^{2}l}{\\partial k\\partial t_{x}}(x,y,-k_{0},0,2c,\\pi)=2x}}\\\\ {{\\displaystyle\\frac{\\partial^{2}l}{\\partial k\\partial\\theta}(x,y,-k_{0},0,2c,\\pi)=2x(k_{0}x^{2}+y),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle{\\frac{\\partial^{2}l}{\\partial^{2}t_{y}}(x,y,-k_{0},0,2c,\\pi)}=0,}&{{}\\quad\\displaystyle{\\frac{\\partial^{2}l}{\\partial t_{y}\\partial k}(x,y,-k_{0},0,2c,\\pi)}=0}\\\\ {\\displaystyle{\\frac{\\partial^{2}l}{\\partial t_{y}\\partial t_{x}}(x,y,-k_{0},0,2c,\\pi)}=0,}&{{}\\quad\\displaystyle{\\frac{\\partial^{2}l}{\\partial t_{y}\\partial\\theta}(x,y,-k_{0},0,2c,\\pi)}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\forall\\alpha,\\beta\\in\\{k,t_{x},t_{y},\\theta\\}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\displaystyle\\frac{\\partial^{2}r}{\\partial\\alpha\\partial\\beta}(-k_{0},0,2c,\\pi)}\\\\ &{=\\displaystyle\\mathbb{E}_{x\\sim p y\\sim q}^{\\mathrm{~\\tiny~E~}}\\Big(l(x,y,-k_{0},0,2c,\\pi)\\frac{\\partial l^{2}}{\\partial\\alpha\\partial\\beta}(x,y,-k_{0},0,2c,\\pi)}\\\\ &{\\qquad\\qquad\\quad\\,+\\displaystyle\\frac{\\partial l}{\\partial\\alpha}(x,y,-k_{0},0,2c,\\pi)\\frac{\\partial l}{\\partial\\beta}(x,y,-k_{0},0,2c,\\pi)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denote ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{x}^{i}=\\underset{x\\sim p}{\\mathbb{E}}x^{i},\\qquad V_{y}^{i}=\\underset{y\\sim q}{\\mathbb{E}}y^{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial^{2}r}{\\partial^{2}\\theta}(-k_{0},0,2c,\\pi)=V_{x}^{2}+2k_{0}c(V_{x}^{2}+2V_{y}^{2})+(2+4V_{x}^{2}k_{0}^{2})(c^{2}-V_{y}^{2})}}\\\\ {{\\displaystyle-4k_{0}V_{y}^{3}+k_{0}^{2}(2V_{x}^{4}+V_{x}^{2}V_{y}^{2})+2k_{0}^{3}c V_{x}^{4}+k_{0}^{4}V_{x}^{6}}}\\\\ {{\\displaystyle\\frac{\\partial^{2}r}{\\partial\\theta\\partial t_{x}}(-k_{0},0,2c,\\pi)=k_{0}\\big(V_{x}^{2}+k_{0}^{2}V_{x}^{4}+k_{0}c V_{x}^{2}+4(c^{2}-V_{y}^{2})\\big)}}\\\\ {{\\displaystyle\\frac{\\partial^{2}r}{\\partial^{2}t_{x}}(-k_{0},0,2c,\\pi)=k_{0}^{2}V_{x}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial^{2}r}{\\partial^{2}k}(-k_{0},0,2c,\\pi)=V_{x}^{4}}\\\\ {\\displaystyle\\frac{\\partial^{2}r}{\\partial k\\partial t_{x}}(-k_{0},0,2c,\\pi)=k_{0}V_{x}^{3}=0}\\\\ {\\displaystyle\\frac{\\partial^{2}r}{\\partial k\\partial\\theta}(-k_{0},0,2c,\\pi)=V_{x}^{3}(1+k_{0}c)+k_{0}^{2}V_{x}^{5}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial^{2}r}{\\partial^{2}t_{y}}(-k_{0},0,2c,\\pi)=1,}}\\\\ {{\\displaystyle\\frac{\\partial^{2}r}{\\partial t_{y}\\partial k}(-k_{0},0,2c,\\pi)=-V_{x}^{2}}}\\\\ {{\\displaystyle\\frac{\\partial^{2}r}{\\partial t_{y}\\partial t_{x}}(-k_{0},0,2c,\\pi)=0,}}\\\\ {{\\displaystyle\\frac{\\partial^{2}r}{\\partial t_{y}\\partial\\theta}(-k_{0},0,2c,\\pi)=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It remains to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}r}{\\partial^{2}\\theta}(-k_{0},0,2c,\\pi)\\frac{\\partial^{2}r}{\\partial^{2}t_{x}}(-k_{0},0,2c,\\pi)>\\big(\\frac{\\partial^{2}r}{\\partial\\theta\\partial t_{x}}(-k_{0},0,2c,\\pi)\\big)^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}r}{\\partial^{2}k}(-k_{0},0,2c,\\pi)\\frac{\\partial^{2}r}{\\partial^{2}t_{y}}(-k_{0},0,2c,\\pi)>\\big(\\frac{\\partial^{2}r}{\\partial k\\partial t_{y}}(-k_{0},0,2c,\\pi)\\big)^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The difference between the left and right-hand sides of (24) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{0}^{2}\\Big(V_{x}^{2}\\big(V_{x}^{2}+2k_{0}c(V_{x}^{2}+2V_{y}^{2})+(2+4V_{x}^{2}k_{0}^{2})(c^{2}-V_{y}^{2})-4k_{0}V_{y}^{3}}\\\\ &{\\big.+k_{0}^{2}(2V_{x}^{4}+V_{x}^{2}V_{y}^{2})+2k_{0}^{3}c V_{x}^{4}+k_{0}^{4}V_{x}^{6}\\big)-\\big(V_{x}^{2}+k_{0}^{2}V_{x}^{4}\\big)^{2}}\\\\ &{\\big.-\\big(k_{0}c V_{x}^{2}+4(c^{2}-V_{y}^{2})\\big)^{2}-2\\big(V_{x}^{2}+k_{0}^{2}V_{x}^{4}\\big)\\big(k_{0}c V_{x}^{2}+4(c^{2}-V_{y}^{2})\\big)\\Big)}\\\\ &{\\!=\\!k_{0}^{2}\\Big(k_{0}^{4}\\big(V_{x}^{2}V_{x}^{6}-V_{x}^{4}\\big)\\Big)+\\Big(V_{x}^{2}\\big(4k_{0}c(3V_{y}^{2}-2c^{2})}\\\\ &{\\big.\\big.+(6+5V_{x}^{2}k_{0}^{2})(V_{y}^{2}-c^{2})-4k_{0}V_{y}^{3}}\\\\ &{\\big.\\big.+2k_{0}^{2}V_{x}^{4}\\big)-16(c^{2}-V_{y}^{2})^{2}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $y\\ll x$ , the above quantity is above zero if ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{x}^{2}V_{x}^{6}>V_{x}^{4}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which can be derived from Cauchy inequality. (25) is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{x}^{4}>{V_{x}}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which can be derived from the Cauchy inequality. ", "page_idx": 16}, {"type": "text", "text": "D More Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Visualization . We include more visualization comparisons. We show the comparison with generalizable methods and scene-specific methods in Fig. 6 and Fig. 7, respectively. We also include a failure case of CoFie in Fig. 8. ", "page_idx": 16}, {"type": "text", "text": "Quantitative Results. We include a more comprehensive comparison with generalizable shape auto-encoding (GAE) and shape-specific auto-decoding (SSAD) methods for understanding the performance of our model. Again, we note CoFie performs generalizable shape auto-decoding (GAD) and is not directly comparable to these models. ", "page_idx": 16}, {"type": "text", "text": "Table 5: Performance on ShapeNet 10 novel categories. Specifically, the reported 3DS2VS [51] and NKSR [15] are trained on the full set of the training categories. In contrast, the reported numebrs in the main paper use a subset of 1000 instances for training. ", "page_idx": 16}, {"type": "text", "text": "Table 6: Performance on Thingi shapes. Note that SSAD methods take a long time for inference, e.g. NGLOD and UODFs take 105 and 300 minutes, respectively. In contrast, CoFie takes 10 minutes. ", "page_idx": 16}, {"type": "table", "img_path": "0KseSacluJ/tmp/da0cbf01765eefa1c8c8657bf1905801bdd85857d3c190436982d78e1c30997a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "0KseSacluJ/tmp/211d7c2c24fc5deef9096bf9c3bfb943b9b016b72b4118bd886b1795f405f1a4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "0KseSacluJ/tmp/f70eaa3bc9a5c44aa0b66c436e02245639bdc1d4a1e243bee827aeaea9c138fc.jpg", "img_caption": [], "img_footnote": ["Figure 6: Compare with the generalizable methods DeepSDF and DeepLS on ShapeNet shapes. We show two images for each method, one for the overall shape quality, and a zoom-in detail check. "], "page_idx": 17}, {"type": "image", "img_path": "0KseSacluJ/tmp/f4bc1ec5b5913dfdaf247019a69d5af38d3003573f1cd891d834b29ec621ced9.jpg", "img_caption": ["Figure 7: Compare with the shape-specific method NGLOD on Thingi shapes. We show two images for each method, one for the overall shape quality, and a zoom-in detail check. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "0KseSacluJ/tmp/3c301bbd450d7ed74e2b94614fee01fd07b98a258cf34f3dfcda238ff05307ac.jpg", "img_caption": ["Figure 8: Analysis of the failure case. CoFie still struggles to represent extremely detailed geometry parts. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We include the experiments to validate every claim. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please see the last page of the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have included the theoretical analysis in the supplementary. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have included all experiment details. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We committed to releasing code upon acceptance. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have included all experiment details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our method outperforms the baselines with significant margins. Visualization also verifies the contributions. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have included the details in the experiment details section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not contain any harmful results. It is neutral research. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed the broader impacts on the last page of the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper on shape representation does not have the risk of being misused. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "\u2022 ShapeNet: Custom (non-commercial)   \n\u2022 Thingi: CC BY-NC 2.0 license   \n\u2022 DeepSDF: MIT License   \n\u2022 3DLatent2VecSet: Custom (non-commercial)   \n\u2022 DeepLS: MIT License   \n\u2022 NGLOD: MIT License ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We will release the code with documentation. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]