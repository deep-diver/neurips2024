[{"type": "text", "text": "KFNN: K-Free Nearest Neighbor For Crowdsourcing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenjun Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liangxiao Jiang\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Computer Science China University of Geosciences Wuhan 430074, China wjzhang@cug.edu.cn ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science China University of Geosciences Wuhan 430074, China ljiang@cug.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Chaoqun Li   \nSchool of Mathematics and Physics   \nChina University of Geosciences Wuhan 430074, China chqli@cug.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To reduce annotation costs, it is common in crowdsourcing to collect only a few noisy labels from different crowd workers for each instance. However, the limited noisy labels restrict the performance of label integration algorithms in inferring the unknown true label for the instance. Recent works have shown that leveraging neighbor instances can help alleviate this problem. Yet, these works all assume that each instance has the same neighborhood size, which defies common sense. To address this gap, we propose a novel label integration algorithm called K-free nearest neighbor (KFNN). In KFNN, the neighborhood size of each instance is automatically determined based on its attributes and noisy labels. Specifically, KFNN initially estimates a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution is then utilized to enhance the multiple noisy label distribution of each instance. Subsequently, a Kalman filter is designed to mitigate the impact of noise incurred by neighbor instances. Finally, KFNN determines the optimal neighborhood size by the max-margin learning. Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios. Our codes and datasets are available at https://github.com/jiangliangxiao/KFNN. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Crowdsourcing provides a more cost-effective way to obtain annotated instances than traditional expert annotation [1]. Through crowdsourcing platforms such as Figure Eight and Clickworker, instances can be annotated by crowd workers at a low cost [2, 3]. While more affordable, these workers possess less expertise than domain experts and are more prone to assigning noisy labels to instances [4]. To address this issue, the concept of repeated annotation is introduced and becomes popular in crowdsourcing [5]. With repeated annotation, each instance is annotated by several workers, thereby obtaining multiple noisy labels. To train supervised models using multiple noisy labels, two main categories of methods have been developed: one-stage methods and two-stage methods. One-stage methods [6, 7, 8] train models directly using multiple noisy labels. Two-stage methods [1, 9] first infer the unknown true label for each instance from its multiple noisy labels via label integration (also known as answer aggregation or ground truth inference) [10] and then train models on integrated labels. One-stage methods, although end-to-end, can only be used to train specifically designed models. As a result, label integration, which is required for the more common two-stage methods, has received a great deal of attention from researchers. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "It has been theoretically demonstrated that, when worker annotation is more accurate than random annotation, the more noisy labels an instance receives, the easier it becomes to infer its unknown true label [11]. However, to reduce annotation costs, only a few noisy labels can be collected for each instance in crowdsourcing. The limited noisy labels restrict the performance of label integration algorithms in inferring the unknown true label for the instance. Furthermore, some common strategies in crowdsourcing, such as worker modelling, worker elimination and task assignment [12], fail to mitigate the effects of limited labels in label integration. To alleviate this problem, recent works have begun to focus on leveraging neighbor instances [13, 14, 1]. These works successfully improve the performance of label integration by leveraging the information from neighbor instances obtained by the K-nearest neighbor (KNN) algorithm. However, due to the use of KNN, these algorithms all assume that each instance has the same neighborhood size. This assumption is difficult to hold because it defies common sense, e.g. instances close to the center of classes should have more neighbors than instances close to the boundary of classes. ", "page_idx": 1}, {"type": "text", "text": "To address this gap, we propose a novel label integration algorithm called K-free nearest neighbor (KFNN). In KFNN, the optimal neighborhood size of each instance is automatically determined based on its attributes and noisy labels. Notably, KFNN is different from some supervised works [15, 16] that determine the optimal K-value for KNN. Unlike in supervised learning, the true label of each instance in crowdsourcing is unknown and only its multiple noisy labels can be used, which makes it difficult to model the relationship between the instance and all classes. To do this, KFNN initially estimates a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution is then utilized to enhance the label distribution for each instance. Subsequently, a Kalman filter is designed to mitigate the impact of noise incurred by neighbor instances. Finally, KFNN determines the optimal neighborhood size by the max-margin learning. In general, the contributions of this paper can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We reveal the limitations caused by fixing the neighborhood size in existing label integration algorithms and propose a novel algorithm called KFNN. In KFNN, the neighborhood size of each instance is automatically determined based on its attributes and noisy labels.   \n\u2022 We estimate a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution enhances the multiple noisy label distribution of each instance.   \n\u2022 We design a Kalman fliter to mitigate the impact of noise incurred by neighbor instances and then determine the optimal neighborhood size by the max-margin learning, which provides strong theoretical support for our algorithm.   \n\u2022 Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art label integration algorithms and exhibits greater robustness than existing algorithms in various crowdsourcing scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Depending on whether neighbor instances are leveraged or not, existing label integration algorithms can be divided into two categories. The first category of algorithms does not leverage neighbor instances, which considers only the information of the instance itself or the information of all instances globally in label integration. For example, [17] models the ability of each worker with a confusion matrix. In this matrix, each element reflects the probability that this worker annotates an instance with the class corresponding to the row as the class corresponding to the column. [18, 19] are Bayesian versions of [17], which can be used for binary tasks and multi-class tasks, respectively. Further, [20, 21] improve [19] by introducing the correlation between workers. [11, 22, 23] are classical algorithms based on majority voting and they tend to use the label with the highest number of votes as the integrated label. [24, 25, 26] synchronously model the ability of workers and the difficulty of tasks from different perspectives. [27, 28] use clustering algorithms to divide instances into different clusters from different views, and then map these clusters to different classes. Recently, [29] augments the multiple noisy label distributions of instances as new attributes to the original attribute space and then learns a classifier on the augmented attribute space to predict the integrated labels of instances. [9] constructs graphs for workers and uses a graph neural network to aggregate multi-order information in label integration. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The second category of algorithms performs label integration by leveraging the information from neighbor instances obtained by the KNN algorithm. For example, [13] proposes to use the labels assigned to the neighbor instances of an instance to augment this instance\u2019s multiple noisy labels and use the augmented multiple noisy labels to infer the integrated label of this instance. [14] considers both nearest and farthest neighbors in weighted voting to address class-imbalanced tasks. Further, inspired by label distribution learning [30, 31], given an instance, [1] iteratively absorbs the label distributions of its neighbor instances into its label distribution through label distribution propagation. ", "page_idx": 2}, {"type": "text", "text": "While simpler and more efficient, the first category of algorithms are limited in effectiveness because each instance can only obtain few noisy labels. Both experimental results and theoretical analysis demonstrate the effectiveness of the second category of algorithms in leveraging the information from neighbor instances. However, these algorithms all assume a fixed neighborhood size for each instance, which is often unrealistic and thus limits their performance. To further ensure that each instance has a free neighborhood size, this paper proposes a novel label integration algorithm called KFNN. KFNN automatically determines the optimal neighborhood size for each instance based on its attributes and noisy labels, which improves the performance and robustness of label integration. ", "page_idx": 2}, {"type": "text", "text": "3 Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we respond to how to automatically determine the optimal neighborhood size for each instance. First, we present some basic notations in crowdsourcing and then define the problem settings. Subsequently, we introduce our KFNN for label integration. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $D=\\{(\\pmb{x}_{i},\\pmb{L}_{i})\\}_{i=1}^{N}$ denote a crowdsourced dataset, where $N$ is the number of instances, and $\\pmb{x}_{i}$ denotes the $i$ -th instance in $D$ . $\\pmb{x}_{i}$ can be represented as $\\{x_{i m}\\}_{m=1}^{M}$ . Here, $M$ is the dimension of attributes, and $x_{i m}$ denotes the attribute value of $\\pmb{x}_{i}$ on the $m$ -th attribute $A_{m}$ . $\\pmb{L}_{i}$ denotes multiple noisy labels of $\\pmb{x}_{i}$ , which can be expressed as $\\{l_{i r}\\}_{r=1}^{R}$ . $R$ is the number of workers and $l_{i r}$ denotes the label of $\\pmb{x}_{i}$ annotated by the $r$ -th worker $u_{r}$ . $l_{i r}$ takes a value from a fixed set $\\{-1,c_{1},\\ldots,c_{q},\\ldots,c_{Q}\\}$ , where $Q$ is the number of classes, $c_{q}$ denotes the $q_{\\mathrm{r}}$ -th class and $-1$ indicates that $u_{r}$ has not annotated $\\pmb{x}_{i}$ . Label integration aims to infer an integrated label $\\hat{y}_{i}$ for $\\pmb{x}_{i}$ and minimize the error between $\\hat{y}_{i}$ and the unknown true label $y_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Recent works [1, 13] have shown that leveraging neighbor instances $\\pmb{N}_{i}\\,=\\,\\{\\pmb{x}_{i}^{k}\\}_{k=1}^{K}$ of $\\pmb{x}_{i}$ can mitigate the restriction of limited noisy labels on the performance of label integration. Here, $\\pmb{x}_{i}^{k}$ denotes the $k$ -th nearest neighbor of $\\pmb{x}_{i}$ and $K$ is the neighborhood size. However, in these works, the value of $K$ is fixed for each instance within the same dataset, which does not make sense. On the one hand, instances closer to the center of a class benefit from a larger $K$ , as it enables them to collect more labels from similar instances. Conversely, for instances close to the boundary of classes, a larger $K$ plays a negative role in label integration. On the other hand, using a fixed $K$ can bias algorithms towards the majority class in class-imbalanced datasets, as instances from the majority class are more likely to dominate the neighborhood of instances from minority classes. Therefore, we define the Problem 1 to be addressed in this paper as follows: ", "page_idx": 2}, {"type": "text", "text": "Problem 1. Given a crowdsourced dataset $D$ , how to automatically determine the optimal neighborhood size $K_{i}^{*}$ for each instance $\\pmb{x}_{i}$ with $\\{x_{i m}\\}_{m=1}^{M}$ and $\\{l_{i r}\\}_{r=1}^{R}$ but without $y_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Problem 1 cannot be treated simply as learning an optimal neighborhood size for the KNN algorithm in supervised learning [15, 16]. This is because the true labels of instances in crowdsourcing are unknown. As a result, $K$ can not be evaluated accurately by supervised metrics such as classification accuracy. Moreover, label integration does not divide the crowdsourced dataset into training, validation and test sets, which means that KFNN has to determine $K_{i}^{*}$ immediately when inferring $\\hat{y}_{i}$ , rather than with a validation phase. ", "page_idx": 2}, {"type": "text", "text": "3.2 K-free nearest neighbor algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we propose our KFNN to address Problem 1. We argue that $K_{i}^{*}$ should be related to the information from both the attribute space and the multiple noisy label space. Based on this, KFNN divides Problem 1 into two parts: 1) How to fuse the information from the attribute space and the multiple noisy label space? 2) How to determine an optimal $K_{i}^{*}$ for $\\pmb{x}_{i}$ ? Correspondingly, KFNN consists of two components, namely label distribution enhancement and K-free optimization, which are used to address the two parts of Problem 1. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Label distribution enhancement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For each instance $\\pmb{x}_{i}$ , $\\{x_{i m}\\}_{m=1}^{M}$ reflects all the information of it in the attribute space and $\\{l_{i r}\\}_{r=1}^{R}$ reflects all the information of it in the multiple noisy label space. Inspired by label enhancement (LE) [32, 33], we design a label distribution enhancement (LDE) component for KFNN. LDE recovers a potential label distribution using $\\{x_{i m}\\}_{m=1}^{M}$ , and then enhances the multiple noisy label distribution calculated from $\\{l_{i r}\\}_{r=1}^{R}$ by this potential label distribution. Specifically, KFNN first uses majority voting to initialize the integrated label $\\hat{y}_{i}$ for $\\pmb{x}_{i}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{y}_{i}=\\underset{c\\in\\{c_{1},c_{2},\\ldots,c_{Q}\\}}{\\arg\\operatorname*{max}}p(c_{q}|L_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p(c_{q}|L_{i})$ can be calculated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(c_{q}|L_{i})=\\frac{\\sum_{r=1}^{R}\\delta(l_{i r},c_{q})}{\\sum_{q=1}^{Q}\\sum_{r=1}^{R}\\delta(l_{i r},c_{q})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $p(c_{q}|L_{i})$ reflects the proportion of labels in $\\pmb{L}_{i}$ that take the value $c_{q}$ . The function $\\delta(\\cdot)$ outputs 1 if its two parameters are identical, and 0 otherwise. Subsequently, according to $\\hat{y}_{i}$ , the crowdsourced dataset $D$ can be divided into $Q$ subsets $\\{D_{q}\\}_{q=1}^{Q}$ . The subset $D_{q}$ contains all instances with initial integrated labels of $c_{q}$ , i.e., $D_{q}=\\{\\pmb{x}_{i}|\\hat{y}_{i}=c_{q}\\}_{i=1}^{N}$ . Then, KFNN calculates a Mahalanobis distance distribution $\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(\\pmb{x}_{i},D_{q})=\\sqrt{(\\pmb{x}_{i}-\\pmb{\\mu}_{q})^{T}\\pmb{\\mathcal{C}}_{q}^{-1}(\\pmb{x}_{i}-\\pmb{\\mu}_{q})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\mu}_{q}$ denotes the centroid of $D_{q}$ and $\\ensuremath{\\boldsymbol{\\mathcal{C}}}_{q}^{-1}$ denotes the inverse matrix of the covariance matrix of $D_{q}$ $\\mathbf{\\nabla}.~d(\\pmb{x}_{i},D_{q})$ is the Mahalanobis distance from $\\pmb{x}_{i}$ to $D_{q}$ calculated in the attribute space. A larger $d(\\pmb{x}_{i},D_{q})$ means that $\\pmb{x}_{i}$ is less likely to belong to $c_{q}$ , conversely a smaller $d(\\pmb{x}_{i},D_{q})$ means that $\\pmb{x}_{i}$ tends to belong to $c_{q}$ . Therefore, $\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q}$ can be used to model the relationship between each instance and all classes. Based on this, $\\{d(\\bar{\\pmb{x}_{i}},D_{q})\\}_{q=1}^{Q}$ can be transformed into a potential label distribution $\\{p(c_{q}|\\mathbf{x}_{i},D_{q})\\}_{q=1}^{Q}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(c_{q}|\\pmb{x}_{i},D_{q})=\\frac{m a x(\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q})-d(\\pmb{x}_{i},D_{q})}{m a x(\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q})-m i n(\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $m a x(\\cdot)$ and $m i n(\\cdot)$ denote the maximum and minimum values of the set, respectively. ", "page_idx": 3}, {"type": "text", "text": "In addition to the potential label distribution, a multiple noisy label distribution $\\{p(c_{q}|L_{i})\\}_{q=1}^{Q}$ can also be directly transformed from $\\pmb{L}_{i}$ . Different from $\\{p(c_{q}|\\mathbf{x}_{i},D_{q})\\}_{q=1}^{Q}$ , which learns the potential relationship between instances and classes from the attribute space, $\\bar{\\{p(c_{q}|L_{i})\\}}_{q=1}^{Q}$ learns the label distribution reflected by noisy labels from the multiple noisy label space. Finally, KFNN fuses them into an enhanced label distribution $P_{i}=\\{p_{i q}\\}_{q=1}^{Q}$ by averaging as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{i q}=\\frac{p(c_{q}|\\pmb{x}_{i},D_{q})+p(c_{q}|\\pmb{L}_{i})}{\\sum_{q=1}^{Q}[p(c_{q}|\\pmb{x}_{i},D_{q})+p(c_{q}|\\pmb{L}_{i})]}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this way, the enhanced label distribution $P_{i}$ can fuse the information from the attribute space and the multiple noisy label space. Therefore, the first part of Problem 1 has been addressed. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 K-free optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After obtaining $P_{i}$ by label distribution enhancement, KFNN proceeds to determine the optimal neighborhood size $K_{i}^{*}$ for $\\pmb{x}_{i}$ . First, KFNN calculates the distance between each pair of instances $x_{1}$ and $\\pmb{x}_{2}$ by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(\\pmb{x}_{1},\\pmb{x}_{2})=\\sum_{q=1}^{Q}d(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q})$ can be calculated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q})=\\sqrt{(\\pmb{x}_{1}-\\pmb{x}_{2})^{T}\\pmb{\\mathcal{C}}_{q}^{-1}(\\pmb{x}_{1}-\\pmb{x}_{2})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compared to the Euclidean distance, Eq. (6) introduces the label information by calculating the distance between $x_{1}$ and $\\pmb{x}_{2}$ on each subset $D_{q}$ . According to Eq. (6), we can calculate distances between $\\pmb{x}_{i}$ and all instances in $D$ . By sorting these distances we can obtain a neighbor sequence $<\\pmb{x}_{i}^{1},\\ldots,\\pmb{x}_{i}^{k},\\ldots,\\pmb{x}_{i}^{N}>$ for $\\pmb{x}_{i}$ . Here, $\\pmb{x}_{i}^{k}$ is the $k$ -th neighbor instance of $\\pmb{x}_{i}$ satisfying $d(\\pmb{x}_{i},\\pmb{\\dot{x}}_{i}^{k})\\geq$ $d(\\pmb{x}_{i},\\pmb{x}_{i}^{k-1})$ when $k$ greater than 1. Then, we calculate the weight $w_{i k}$ for $\\pmb{x}_{i}^{k}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i k}=\\frac{\\sum_{r=1}^{R}\\delta(l_{i r},l_{i k r})}{\\sum_{r=1}^{R}[1-\\delta(l_{i r},-1)]*[1-\\delta(l_{i k r},-1)]},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $l_{i k r}$ denotes the label of $\\pmb{x}_{i}^{k}$ annotated by the $r$ -th worker $u_{r}$ . $w_{i k}$ reflects the proportion of workers assigned the same label for $\\pmb{x}_{i}$ and $\\pmb{x}_{i}^{k}$ . Subsequently, $\\pmb{x}_{i}$ is allowed to absorb the enhanced label distributions of neighbor instances in the neighbor sequence one by one. Let $P_{i}^{k}=\\{p_{i q}^{k}\\}_{q=1}^{Q}$ denote the label distribution of $\\pmb{x}_{i}$ after absorbing $\\pmb{x}_{i}^{k}$ , which can be updated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{i q}^{k}=\\frac{p_{i q}^{k-1}+w_{i k}*p_{i k q}}{\\sum_{q=1}^{Q}[p_{i q}^{k-1}+w_{i k}*p_{i k q}]},\\quad k\\geq2,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{i k q}$ denotes the probability value corresponding to $c_{q}$ in the enhanced label distribution of $\\pmb{x}_{i}^{k}$ .   \nSince the first neighbor instance of $\\pmb{x}_{i}$ is itself, $P_{i}^{k}=P_{i}$ when $k$ is equal to 1. ", "page_idx": 4}, {"type": "text", "text": "According to $P_{i}^{k}$ , KFNN calculates a class margin as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{M}}_{k}=m a x(\\mathbf{P}_{i}^{k})-s e c(\\mathbf{P}_{i}^{k}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $s e c(\\cdot)$ denotes the second-largest value of the set. Since the true labels are unknown, Eqs. (5) (7) (8) are all designed based on multiple noisy labels, which lead to that $\\widetilde{\\mathcal{M}}_{k}$ contains a degree of noise incurred by neighbor instances. Therefore, KFNN designs a Kalman filter to mitigate the impact of noise in $\\widetilde{\\mathcal{M}}_{k}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\hat{\\mathcal{M}}_{k}^{-}=\\hat{\\mathcal{M}}_{k-1}}\\\\ {\\mathcal{P}_{k}^{-}=\\mathcal{P}_{k-1}+\\alpha}\\\\ {\\mathcal{K}_{k}=\\frac{\\mathcal{P}_{k}^{-}}{\\mathcal{P}_{k}^{-}+\\beta}}\\\\ {\\hat{\\mathcal{M}}_{k}=\\hat{\\mathcal{M}}_{k}^{-}+\\mathcal{K}_{k}\\ast(\\widetilde{\\mathcal{M}}_{k}-\\hat{\\mathcal{M}}_{k}^{-})}\\\\ {\\mathcal{P}_{k}=(1-\\mathcal{K}_{k})\\ast\\mathcal{P}_{k}^{-}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\mathcal{M}}_{k}$ denotes the filtered margin, determined by both the estimated margin $\\hat{\\mathcal{M}}_{k}^{-}$ and the calculated margin $\\widetilde{\\mathcal{M}}_{k}$ . The designed Kalman filter can be divided into an estimation phase and an update phase. In the estimation phase, the fliter estimates $\\hat{\\mathcal{M}}_{k}^{-}$ and the estimated error $\\mathcal{P}_{k}^{-}$ based on the filtered margin $\\hat{\\mathcal{M}}_{k-1}$ and error $\\mathcal{P}_{k-1}$ of the previous time index. In the update phase, the filter first updates the Kalman gain $\\kappa_{k}$ of the $k$ -th time index and then updates $\\hat{\\mathcal{M}}_{k}$ and error $\\mathcal{P}_{k}$ of the $k$ -th time index according to $\\kappa_{k}$ . $\\alpha$ and $\\beta$ are the process error and the measurement error in the Kalman filter. When $k$ is equal to $0$ , $\\hat{\\mathcal{M}}_{k}$ takes the value of 0 and $\\mathcal{P}_{k}$ takes the value of 1. ", "page_idx": 4}, {"type": "text", "text": "To address the second part of Problem 1, KFNN determines the optimal neighborhood size $K_{i}^{*}$ for $\\pmb{x}_{i}$ by the max-margin learning as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nK_{i}^{*}=\\underset{k\\in\\{1,2,\\cdots,N\\}}{\\arg\\operatorname*{max}}\\,\\hat{\\mathcal{M}}_{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Ultimately, according to $K_{i}^{*}$ , KFNN updates the integrated label $\\hat{y}_{i}$ for $\\pmb{x}_{i}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{y}_{i}=\\underset{c\\in\\{c_{1},c_{2},\\cdots,c_{Q}\\}}{\\arg\\operatorname*{max}}P_{i}^{K_{i}^{*}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The whole learning process of KFNN is shown in Algorithm 1. In Algorithm 1, lines 1-3 initialize the integrated label and multiple noisy label distribution for each instance and their time complexity is $O(N Q R)$ . Line 4 divides the crowdsourced dataset $D$ into $Q$ subsets and its time complexity is $O(N Q)$ . Lines 5-9 perform label distribution enhancement and their time complexity is $\\bar{O(N M^{2}Q)}$ . Line 11 calculates the distances from $\\pmb{x}_{i}$ to other instances and sorts these distances, its time complexity is $O(N M^{2}Q+N\\log(N))$ . Lines 12-16 calculate the margins $\\{\\widetilde{\\mathcal{M}}_{k}\\}_{k=1}^{N}$ and their time complexity is $O(N R+N Q)$ . Line 17 fliters the margins $\\{\\widetilde{\\mathcal{M}}_{k}\\}_{k=1}^{N}$ and its time complexity is $O(N)$ . Line 18 determines the optimal neighborhood size and its time complexity is $O(N)$ . Line 19 infers the integrated label for each instance and its time complexity is $O(Q)$ . Therefore, the time complexity of lines 10-20 is $O(N^{2}(M^{2}Q+\\log(N)+R))$ . If only the highest order terms are taken, the time complexity of KFNN is $O(N(N M^{2}Q+N\\log(N)+N R+Q R))$ ). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 The learning process of KFNN ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Require: $D=\\{(\\pmb{x}_{i},\\pmb{L}_{i})\\}_{i=1}^{N}$ - a crowdsourced dataset; $\\alpha,\\beta$ - the predefined parameters   \nEnsure: $\\{\\hat{y}_{i}\\}_{i=1}^{N}$ - the integrated labels   \n1: for $i=1$ to $N$ do   \n2: Initialize $\\hat{y}_{i}$ and $\\{p(c_{q}|L_{i})\\}_{q=1}^{Q}$ for $\\pmb{x}_{i}$ by Eqs. (1) (2)   \n3: end for   \n4: Divide $D$ into $\\{D_{q}\\}_{q=1}^{Q}$ based on $\\hat{y}_{i}$   \n5: for $i=1$ to $N$ do   \n6: Calculate $\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q}$ for $\\pmb{x}_{i}$ by Eq. (3)   \n7: Transform $\\{d(\\pmb{x}_{i},D_{q})\\}_{q=1}^{Q}$ into $\\{p(c_{q}|\\mathbf{x}_{i},D_{q})\\}_{q=1}^{Q}$ by Eq. (4)   \n8: Fuse $\\{p(c_{q}|\\mathbf{x}_{i},D_{q})\\}_{q=1}^{Q}$ and $\\{p(c_{q}|L_{i})\\}_{q=1}^{Q}$ into $P_{i}=\\{p_{i q}\\}_{q=1}^{Q}$ by Eq. (5)   \n9: end for   \n10: for $i=1$ to $N$ do   \n11: Calculate $<x_{i}^{1},\\ldots,x_{i}^{k},\\ldots,x_{i}^{N}>$ for $\\pmb{x}_{i}$ by Eqs. (6) (7)   \n12: for $k=1$ to $N$ do   \n13: Calculate the weight $w_{i k}$ for $\\pmb{x}_{i}^{k}$ by Eq. (8)   \n14: Update the label distribution $P_{i}^{k}$ by Eq. (9)   \n15: Calculate the $\\widetilde{\\mathcal{M}}_{k}$ by Eq. (10)   \n16: end for   \n17: Filter $\\{\\widetilde{\\mathcal{M}}_{k}\\}_{k=1}^{N}$ using the designed Kalman filter by Eq. (11)   \n18: Determ ine the optimal neighborhood size $K_{i}^{*}$ for $\\pmb{x}_{i}$ by Eq. (12)   \n19: Infer the integrated label $\\hat{y}_{i}$ for $\\pmb{x}_{i}$ by Eq. (13)   \n2201::  erentdu fronr $\\{\\hat{y}_{i}\\}_{i=1}^{N}$ ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide some detailed theoretical analysis for KFNN. First, in Eq. (6), KFNN defines the distance $d(x_{1},x_{2})$ between $x_{1}$ and $\\pmb{x}_{2}$ based on the Mahalanobis distance $d(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q})$ rather than the traditional Euclidean distance $d_{E}({\\pmb x}_{1},{\\pmb x}_{2})$ . According to Eqs. (3) (7), the Mahalanobis distance works based on a basic assumption, which can be described as follows: ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. Given the subset $D_{q}.$ , its covariance matrix $\\ensuremath{{\\mathcal{C}}}_{q}$ is a nonsingular matrix. ", "page_idx": 5}, {"type": "text", "text": "The Assumption 1 holds based on the condition that $\\vert c_{q}\\vert$ is non-zero, which is usually satisfied. Even if this condition is not satisfied, we can ensure that the Assumption 1 holds by adding a small value to each element of the principal diagonal on $\\ensuremath{{\\mathcal{C}}}_{q}$ until $\\vert\\mathcal{C}_{q}\\vert$ is non-zero. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. If Assumption 1 holds, there will be an orthogonal matrix $\\mathcal{P}$ satisfying that ${\\mathcal{P}}^{-1}c_{q}{\\mathcal{P}}=$ $\\pmb{\\mathcal{P}}^{T}\\pmb{\\mathcal{C}}_{q}\\pmb{\\mathcal{P}}=\\pmb{\\Lambda}$ , where $\\Lambda$ is a diagonal matrix with all $M$ eigenvalues of $\\ensuremath{{\\mathcal{C}}}_{q}$ as its elements of the principal diagonal. ", "page_idx": 6}, {"type": "text", "text": "Due to the limited pages, the proof of Theorem 1 is provided in Appendix A. Based on Theorem 1, we can obtain some interesting corollaries about Eqs. (3) (6) (7). ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Compared to $d_{E}({\\pmb x}_{1},{\\pmb x}_{2})$ , $d(x_{1},x_{2})$ in Eq. (6) does not suffer from the correlation and magnitude of attributes. ", "page_idx": 6}, {"type": "text", "text": "Proof. According to Theorem 1, $d(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q})$ can be transformed as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q})=\\sqrt{(\\pmb{x}_{1}-\\pmb{x}_{2})^{T}\\mathcal{C}_{q}^{-1}(\\pmb{x}_{1}-\\pmb{x}_{2})}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{(\\pmb{x}_{1}-\\pmb{x}_{2})^{T}((\\pmb{\\mathscr{P}}^{T})^{-1}\\pmb{\\Lambda}\\pmb{\\mathscr{P}}^{-1})^{-1}(\\pmb{x}_{1}-\\pmb{x}_{2})}\\cdot}\\\\ &{\\qquad\\qquad\\quad=\\sqrt{(\\pmb{\\mathscr{P}}^{T}(\\pmb{x}_{1}-\\pmb{x}_{2}))^{T}\\pmb{\\Lambda}^{-1}(\\pmb{\\mathscr{P}}^{T}(\\pmb{x}_{1}-\\pmb{x}_{2}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When $\\Lambda^{-1}$ is not considered, the derivation of Eq. (14) implies that $d(\\pmb{x}_{1},\\pmb{x}_{2}|D_{q})$ is the Euclidean distance of instances after orthogonal transformation using $\\mathcal{P}^{T}$ . After orthogonal transformation, the attributes are independent of each other, so $d(x_{1},x_{2})$ does not suffer from the correlation of attributes. $\\Lambda^{-1}$ is equivalent to $\\begin{array}{r}{\\mathrm{diag}(\\frac{1}{\\lambda_{1}},\\frac{1}{\\lambda_{2}},\\dots,\\frac{1}{\\lambda_{M}})}\\end{array}$ , where $\\lambda_{M}$ is the $M$ -th eigenvalue of $\\ensuremath{{\\mathcal{C}}}_{q}$ and is equal to the variance on the direction of the corresponding eigenvectors. Briefly, ensures that the calculated result on each dimension is normalized by the corresponding variance when calculating the distance by Eq. (7). Therefore, $d(\\pmb{x}_{1},\\pmb{x}_{2})$ does not also suffer from the magnitude of attributes. ", "page_idx": 6}, {"type": "text", "text": "Corollary 2. Compared to $d_{E}({\\pmb x}_{1},{\\pmb x}_{2})$ , $d(x_{1},x_{2})$ in Eq. (6) provides a smaller distance for $x_{1}$ and $\\pmb{x}_{2}$ coming from the same class. ", "page_idx": 6}, {"type": "text", "text": "Proof. $\\mathcal{P}^{T}$ causes the original attribute space to be rotated according to the direction of the eigenvectors of $\\mathcal{C}_{q}$ , and $\\Lambda^{-1}$ causes the rotated attribute space to be scaled according to the eigenvalues $\\ensuremath{{\\mathcal{C}}}_{q}$ . Referring to the principle of principal component analysis [34], the eigenvectors of $\\ensuremath{{\\mathcal{C}}}_{q}$ reflect the principal component directions of $D_{q}$ . This means that $d(x_{1},x_{2})$ will provide a smaller distance for instances coming from the same class compared to $d_{E}(\\pmb{x}_{1},\\pmb{x}_{2})$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Assumption 2. When we estimate $\\hat{\\mathcal{M}}_{k}^{-}$ based on $\\hat{\\mathcal{M}}_{k-1}$ , the estimated error satisfies $N(O,\\,\\mathcal{P}_{k}^{-})$ .   \nWhen we measure $\\widetilde{\\mathcal{M}}_{k}$ by $E q.$ . (10), the measurement error satisfies $N\\!(\\theta,\\,\\beta)$ ). ", "page_idx": 6}, {"type": "text", "text": "The Kalman filter we designed as Eq. (11) works based on Assumption 2, which usually holds because the noise in practice usually satisfies a normal distribution. Since $\\hat{\\mathcal{M}}_{k-1}$ changes in each time index, the variance of the estimated error $\\mathcal{P}_{k}^{-}$ changes with the time index. Since Eq. (10) remains constant, the variance of the measurement error $\\beta$ is constant. According to Assumption 2, the following theorem can be proved: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. When the Kalman gain $\\kappa_{k}$ takes the value PP\u2212k+\u03b2 , the error between the flitered margin $\\hat{\\mathcal{M}}_{k}$ and the true margin $\\mathcal{M}_{k}$ is minimized. ", "page_idx": 6}, {"type": "text", "text": "Proof. When Assumption 2 holds, due to $\\hat{\\mathcal{M}}_{k}=\\hat{\\mathcal{M}}_{k}^{-}+\\mathcal{K}_{k}\\ast\\left(\\widetilde{\\mathcal{M}}_{k}-\\hat{\\mathcal{M}}_{k}^{-}\\right)$ , it can be proved that minimizing the error between $\\hat{\\mathcal{M}}_{k}$ and $\\mathcal{M}_{k}$ is equivalent to minimizing the variance of $\\hat{\\mathcal{M}}_{k}$ . Since $\\hat{\\mathcal{M}}_{k}^{-}$ and $\\widetilde{\\mathcal{M}}_{k}$ are independent of each other, the following equation can be derived: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V a r(\\hat{\\mathcal{M}}_{k})=V a r(\\hat{\\mathcal{M}}_{k}^{-}+{K}_{k}*(\\widetilde{\\mathcal{M}}_{k}-\\hat{\\mathcal{M}}_{k}^{-}))}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =(1-{K}_{k})^{2}*V a r(\\hat{\\mathcal{M}}_{k}^{-})+{K}_{k}^{2}*V a r(\\widetilde{\\mathcal{M}}_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $V a r(\\cdot)$ denotes the variance of the variable. According to Assumption 2, $V a r(\\hat{\\mathscr{M}}_{k}^{-})$ equals to $\\mathcal{P}_{k}^{-}$ and $V a r(\\widetilde{\\mathcal{M}}_{k})$ equals to $\\beta$ . To minimize the error between the flitered margin $\\hat{\\mathcal{M}}_{k}$ and the true ", "page_idx": 6}, {"type": "text", "text": "margin $\\mathcal{M}_{k}$ , we can calculate the partial derivative $\\frac{\\partial V a r(\\hat{\\mathscr{M}}_{k})}{\\partial K_{k}}$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial V a r(\\hat{\\mathcal{M}}_{k})}{\\partial K_{k}}=-2*(1-K_{k})*\\mathcal{P}_{k}^{-}+2*K_{k}*\\beta.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Ultimately, it can be proved that $\\kappa_{k}$ is equal to $\\frac{\\mathcal{P}_{k}^{-}}{\\mathcal{P}_{k}^{-}+\\beta}$ by setting \u2202V ar( M\u02c6k)to 0. \u2202Kk ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 ensures the effectiveness of KFNN in determining the optimal neighborhood size by the max-margin learning, and its proof is provided in Appendix B due to the limited pages. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate the effectiveness of KFNN, we construct extensive experiments on the whole 34 simulated and two real-world crowdsourced datasets published on the Crowd Environment and its Knowledge Analysis (CEKA) [35] platform. For simulated datasets, we first use the unsupervised attribute filter ReplaceMissingValues in the Waikato Environment and Knowledge Analysis (WEKA) [36] platform to replace all missing values. Subsequently, with the CEKA platform, we hide true labels of simulated datasets and simulate five workers whose label qualities are randomly generated from a normal distribution with $\\mathrm{N}(0.65,0.05^{2})$ to annotate these datasets. The real-world datasets, Income and Leaves, which were both collected from the online platform Amazon Mechanical Turk (AMT), can be used directly without any processing since they do not contain missing values. ", "page_idx": 7}, {"type": "text", "text": "We compare our KFNN with six state-of-the-art label integration algorithms. Among them, MV (majority voting) [11] is the simplest label integration algorithm and is used as a baseline for all algorithms. IWMV (iterative weighted majority voting) [22], AALI (attribute augmentation-based label integration) [29], and LAGNN (label aggregation with graph neural networks) [9] are three state-of-the-art label integration algorithms that do not leverage neighbor instances. LAWMV (label augmented and weighted majority voting) [13] and MNLDP (multiple noisy label distribution propagation) [1] are two state-of-the-art label integration algorithms that leverage neighbor instances. For MV, we use the existing implementation of the CEKA platform. For IWMV, AALI, LAGNN, LAWMV, and MNLDP, we use the implementations provided by their authors. All parameters of the comparison algorithms are set to the recommended values in the corresponding published papers. In addition, since true labels are unknown in our experiments, we use the lazy version of LAGNN. In our KFNN, $\\alpha$ and $\\beta$ are set to 0.1 and 1 by default. ", "page_idx": 7}, {"type": "text", "text": "The performance of each algorithm is evaluated using the Macro-F1 score, which highlights the performance of algorithms on different classes and better reveals algorithmic limitations compared to traditional integration accuracy. Due to the limited pages, more detailed descriptions of the experimental datasets and metrics are provided in Appendix C. All experiments are independently repeated ten times on a Windows 10 machine with an AMD Athlon(tm) X4 860K Quad Core Processor $\\ @\\ 3.70\\,\\mathrm{GHz}$ and 16 GB of RAM, and we report the average results of ten experiments. ", "page_idx": 7}, {"type": "text", "text": "5.2 Results and discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Simulation experiment results. Table 1 shows the detailed Macro-F1 score $(\\%)$ comparisons of each label integration algorithm on each simulated dataset, respectively. Based on these results, we perform the Wilcoxon signed-rank test [37] to further compare each pair of algorithms. Table 3 summarizes the Wilcoxon test results. In Table 3, the symbol $\\bullet$ indicates that the algorithm in the row significantly outperforms the algorithm in the corresponding column, the symbol $\\circ$ indicates the exact opposite of that indicated by the symbol $\\bullet$ , and the missing item indicates no significant difference between the algorithm in the row and the algorithm in the column. The significance levels of the lower and upper diagonals are $\\alpha=0.05$ and $\\alpha=0.1$ , respectively. Based on these experimental results, we can summarize the following highlights: 1) The average Macro-F1 score of KFNN on all datasets is $79.64\\%$ , which is much higher than those of MV $(72.46\\%)$ , IWMV $(72.71\\%)$ , AALI $(72.95\\%)$ , LAGNN $(73.71\\%)$ , LAWMV $(73.44\\%)$ and MNLDP $(76.68\\%)$ . KFNN achieves the highest Macro-F1 score, which indicates that KFNN is more effective and robust than these comparison algorithms in various crowdsourcing scenarios. 2) Among all comparison algorithms of KFNN, MNLDP performs better than IWMV, AALI and LAGNN, which demonstrates the advantages of leveraging neighbor instances. 3) Based on the Wilcoxon test results, KFNN significantly outperforms all comparison algorithms, which strongly validates the effectiveness and robustness of KFNN. Besides, we also observe the experimental results in terms of the integration accuracy, which are shown in Tables 2 and 4. According to Tables 2 and 4, we can see that KFNN can also achieve better or comparable integration accuracy compared with these state-of-the-art label integration algorithms. These results again validate the effectiveness and robustness of KFNN. ", "page_idx": 7}, {"type": "table", "img_path": "wnPlJNiqfA/tmp/3ecbc0cac63d72df8578aca26b9d11628657e98438b4d90408121c74d64465b8.jpg", "table_caption": ["Table 1: The Macro-F1 score $(\\%)$ comparisons Table 2: The integration accuracy $(\\%)$ comparfor KFNN versus its comparison algorithms on isons for KFNN versus its comparison algorithms 34 simulated datasets. on 34 simulated datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wnPlJNiqfA/tmp/2e054f1f066520c7076d6fd9651eac6a55557f76fe989489d4e583bfcc32c16e.jpg", "table_caption": ["Table 3: The Macro-F1 score $(\\%)$ comparisons Table 4: The integration accuracy $(\\%)$ comparusing Wilcoxon tests for KFNN versus its com- isons using Wilcoxon tests for KFNN versus its parison algorithms. comparison algorithms. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Real-world experiment results. Compared to simulated crowdsourced datasets, real-world crowdsourced datasets may include some special factors that influence label integration to work effectively, such as sparsity and bias. Therefore, we further observe the performance of KFNN and its comparison algorithms on two real-world datasets, Income and Leaves. Figure 1 shows the detailed Macro-F1 score $(\\%)$ and integration accuracy $(\\%)$ comparisons of each label integration algorithm on Income and Leaves, respectively. As can be seen from Figure 1, compared to these state-of-the-art label integration algorithms, our KFNN achieves the highest integration accuracies and Macro-F1 scores on both Income and Leaves. These results strongly support the effectiveness of our KFNN. ", "page_idx": 8}, {"type": "text", "text": "Parameter sensitivity analysis. There are two parameters $\\alpha$ and $\\beta$ that can be adjusted in the Kalman fliter designed by KFNN. To observe the effect of these two parameters on the performance of KFNN, we perform the parameter sensitivity analysis for KFNN on Income and Leaves. We change both $\\alpha$ and $\\beta$ from 0.1 to 1 and then observe the Macro-F1 score of KFNN on two datasets. ", "page_idx": 8}, {"type": "image", "img_path": "wnPlJNiqfA/tmp/c5df7e778453892bacd1b14e95ca0da20f55d040f360d0025d2c625d7cf30489.jpg", "img_caption": ["Figure 1: The Macro-F1 scores $(\\%)$ and integration accuracies $(\\%)$ of KFNN and its comparison algorithms on the Income and Leaves datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2a and Figure 2b show the Macro-F1 score of KFNN on Income and Leaves when $\\alpha$ and $\\beta$ vary. Based on these results, we can find that KFNN is more sensitive to $\\beta$ compared to $\\alpha$ . As $\\beta$ tends to 1, KFNN tends to achieve optimal performance. Therefore, the default value of $\\beta$ in this paper is set to 1. $\\alpha$ hardly affects the performance of KFNN, which is set to 0.1 by default. ", "page_idx": 9}, {"type": "image", "img_path": "wnPlJNiqfA/tmp/012b34fc9e74918171a27d9222e3cd8576e3c9fa629ac8b5307f7d3de8817a83.jpg", "img_caption": ["Figure 2: The Macro-F1 score $(\\%)$ of KFNN on Figure 3: The Macro-F1 score $(\\%)$ and class the Income and Leaves datasets when $\\alpha$ and $\\beta$ margin $(\\%)$ of KFNN or its components on the vary from 0.1 to 1. Income dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "wnPlJNiqfA/tmp/d48a8317f33e820ee3f24489baa0b9ba53ec6a6c829b3b0864b1c92fd3f7c416.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Ablation experiment. There are two components in KFNN, namely label distribution enhancement (LDE) and K-free optimization (KF). To validate their effectiveness, we observe the Macro-F1 score of KFNN after taking away each component on the Income dataset. For simplicity, we use \"KFNNKF\" to denote the variant of KFNN after taking away the component KF. Similarly, we create its another two variants \"KFNN-LDE\" and \"KFNN-KF-LDE\". Based on the results shown in Figure 3a, it can be seen that the performance becomes worse when any component is taken away. These results validate the effectiveness of LDE and KF. Figure 3b shows the change of the class margin before and after using our designed Kalman fliter (observed on the first instance of Income). As can be seen from Figure 3b, compared to the margin before filter $(\\widetilde{\\mathcal{M}}_{k})$ , the filtered margin $(\\hat{\\mathcal{M}}_{k})$ changes smoother. These results validate the effectiveness of our des igned Kalman filter, which successfully mitigates the impact of noise incurred by neighbor instances. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To ensure that each instance in crowdsourced datasets has a free neighborhood size, we propose a novel algorithm called KFNN. KFNN consists of two key components, namely label distribution enhancement and K-free optimization. Label distribution enhancement fuses the information from the attribute space and the multiple noisy label space. K-free optimization automatically determines the optimal neighborhood size for each instance by the max-margin learning. Both theoretical analysis and experimental results validate the effectiveness and robustness of KFNN. ", "page_idx": 9}, {"type": "text", "text": "Nevertheless, there are still some limitations in KFNN that can be improved in the future. For example, the parameters $\\alpha$ and $\\beta$ in the Kalman filter designed by KFNN can not automatically adapt to the dataset, which restricts the robustness of KFNN. In addition, in Eq. (4), transforming the distance distribution into the potential label distribution using max-min normalization is rough. Considering that the distance metric is not effective across all datasets (e.g., autos and breast-cancer in Table 1), this transformation may also lead to KFNN performing poorly. In the future, we will design more sophisticated parameters and transformations to improve KFNN. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work was partially supported by National Natural Science Foundation of China (62276241), Foundation of Key Laboratory of Artificial Intelligence, Ministry of Education, P.R. China (AI2022004), and Science and Technology Project of Hubei Province-Unveiling System (2021BEC007). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. Jiang, H. Zhang, F. Tao, and C. Li, \u201cLearning from crowds with multiple noisy label distribution propagation,\u201d IEEE Trans. Neural Networks Learn. Syst., vol. 33, no. 11, pp. 6558\u20136568, 2022.   \n[2] P. Chen, Y. Yang, D. Yang, H. Sun, Z. Chen, and P. Lin, \u201cBlack-box data poisoning attacks on crowdsourcing,\u201d in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China. ijcai.org, 2023, pp. 2975\u20132983.   \n[3] Z. Chen, H. Sun, H. He, and P. Chen, \u201cLearning from noisy crowd labels with logics,\u201d in 39th IEEE International Conference on Data Engineering, ICDE 2023, Anaheim, CA, USA, April 3-7, 2023. IEEE, 2023, pp. 41\u201352.   \n[4] J. Li, Y. Kawase, Y. Baba, and H. Kashima, \u201cPerformance as a constraint: An improved wisdom of crowds using performance regularization,\u201d in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, C. Bessiere, Ed. ijcai.org, 2020, pp. 1534\u20131541.   \n[5] V. S. Sheng, J. Zhang, B. Gu, and X. Wu, \u201cMajority voting and pairing with multiple noisy labeling,\u201d IEEE Trans. Knowl. Data Eng., vol. 31, no. 7, pp. 1355\u20131368, 2019.   \n[6] F. Rodrigues and F. C. Pereira, \u201cDeep learning from crowds,\u201d in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, S. A. McIlraith and K. Q. Weinberger, Eds. AAAI Press, 2018, pp. 1611\u20131618.   \n[7] Z. Chen, H. Wang, H. Sun, P. Chen, T. Han, X. Liu, and J. Yang, \u201cStructured probabilistic end-to-end learning from crowds,\u201d in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, C. Bessiere, Ed. ijcai.org, 2020, pp. 1512\u20131518.   \n[8] J. Li, H. Sun, and J. Li, \u201cBeyond confusion matrix: learning from multiple annotators with awareness of instance features,\u201d Mach. Learn., vol. 112, no. 3, pp. 1053\u20131075, 2023.   \n[9] Z. Ying, J. Zhang, Q. Li, M. Wu, and V. S. Sheng, \u201cA little truth injection but a big reward: Label aggregation with graph neural networks,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 5, pp. 3169\u20133182, 2024.   \n[10] J. Zhang, X. Wu, and V. S. Sheng, \u201cLearning from crowdsourced labeled data: a survey,\u201d Artif. Intell. Rev., vol. 46, no. 4, pp. 543\u2013576, 2016.   \n[11] V. S. Sheng, F. J. Provost, and P. G. Ipeirotis, \u201cGet another label? improving data quality and data mining using multiple, noisy labelers,\u201d in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008, Y. Li, B. Liu, and S. Sarawagi, Eds. ACM, 2008, pp. 614\u2013622.   \n[12] G. Li, J. Wang, Y. Zheng, and M. J. Franklin, \u201cCrowdsourced data management: A survey,\u201d IEEE Trans. Knowl. Data Eng., vol. 28, no. 9, pp. 2296\u20132319, 2016.   \n[13] Z. Chen, L. Jiang, and C. Li, \u201cLabel augmented and weighted majority voting for crowdsourcing,\u201d Inf. Sci., vol. 606, pp. 397\u2013409, 2022.   \n[14] W. Zhang, L. Jiang, Z. Chen, and C. Li, \u201cFnnwv: Farthest-nearest neighbor-based weighted voting for class-imbalanced crowdsourcing,\u201d Sci. China Inf. Sci., pp. 10.1007/s11 432\u2013023\u20133854\u20137, 2023.   \n[15] J. S. Olsson, \u201cAn analysis of the coupling between training set and neighborhood sizes for the knn classifier,\u201d in SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006, E. N. Efthimiadis, S. T. Dumais, D. Hawking, and K. J\u00e4rvelin, Eds. ACM, 2006, pp. 685\u2013686.   \n[16] S. Zhang, X. Li, M. Zong, X. Zhu, and D. Cheng, \u201cLearning $k$ for knn classification,\u201d ACM Trans. Intell. Syst. Technol., vol. 8, no. 3, pp. 43:1\u201343:19, 2017.   \n[17] A. P. Dawid and A. M. Skene, \u201cMaximum likelihood estimation of observer error-rates using the em algorithm,\u201d Journal of the Royal Statistical Society: Series C (Applied Statistics), vol. 28, no. 1, pp. 20\u201328, 1979.   \n[18] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy, \u201cLearning from crowds,\u201d J. Mach. Learn. Res., vol. 11, pp. 1297\u20131322, 2010.   \n[19] H. Kim and Z. Ghahramani, \u201cBayesian classifier combination,\u201d in Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2012, La Palma, Canary Islands, Spain, April 21-23, 2012, ser. JMLR Proceedings, N. D. Lawrence and M. A. Girolami, Eds., vol. 22. JMLR.org, 2012, pp. 619\u2013627.   \n[20] M. Venanzi, J. Guiver, G. Kazai, P. Kohli, and M. Shokouhi, \u201cCommunity-based bayesian aggregation models for crowdsourcing,\u201d in 23rd International World Wide Web Conference, WWW \u201914, Seoul, Republic of Korea, April 7-11, 2014, C. Chung, A. Z. Broder, K. Shim, and T. Suel, Eds. ACM, 2014, pp. 155\u2013164.   \n[21] Y. Li, B. I. P. Rubinstein, and T. Cohn, \u201cExploiting worker correlation for label aggregation in crowdsourcing,\u201d in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 3886\u20133895.   \n[22] H. Li and B. Yu, \u201cError rate bounds and iterative weighted majority voting for crowdsourcing,\u201d CoRR, vol. abs/1411.4086, 2014.   \n[23] T. Tian, J. Zhu, and Y. Qiaoben, \u201cMax-margin majority voting for learning from crowds,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 10, pp. 2480\u20132494, 2019.   \n[24] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. R. Movellan, \u201cWhose vote should count more: Optimal integration of labels from labelers of unknown expertise,\u201d in Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada, Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, Eds. Curran Associates, Inc., 2009, pp. 2035\u20132043.   \n[25] P. Welinder, S. Branson, S. J. Belongie, and P. Perona, \u201cThe multidimensional wisdom of crowds,\u201d in Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, Eds. Curran Associates, Inc., 2010, pp. 2424\u20132432.   \n[26] A. Kurve, D. J. Miller, and G. Kesidis, \u201cMulticategory crowdsourcing accounting for variable task difficulty, worker skill, and worker intention,\u201d IEEE Trans. Knowl. Data Eng., vol. 27, no. 3, pp. 794\u2013809, 2015.   \n[27] J. Zhang, V. S. Sheng, J. Wu, and X. Wu, \u201cMulti-class ground truth inference in crowdsourcing with clustering,\u201d IEEE Trans. Knowl. Data Eng., vol. 28, no. 4, pp. 1080\u20131085, 2016.   \n[28] G. Wu, L. Zhou, J. Xia, L. Li, X. Bao, and X. Wu, \u201cCrowdsourcing truth inference based on label confidence clustering,\u201d ACM Trans. Knowl. Discov. Data, vol. 17, no. 4, pp. 46:1\u201346:20, 2023.   \n[29] Y. Zhang, L. Jiang, and C. Li, \u201cAttribute augmentation-based label integration for crowdsourcing,\u201d Frontiers Comput. Sci., vol. 17, no. 5, p. 175331, 2023.   \n[30] C. Xu and X. Geng, \u201cHierarchical classification based on label distribution learning,\u201d in The ThirtyThird AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019. AAAI Press, 2019, pp. 5533\u20135540.   \n[31] Y. Lu and X. Jia, \u201cPredicting label distribution from multi-label ranking,\u201d in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022.   \n[32] N. Xu, A. Tao, and X. Geng, \u201cLabel enhancement for label distribution learning,\u201d in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J. Lang, Ed. ijcai.org, 2018, pp. 2926\u20132932.   \n[33] K. Wang, N. Xu, M. Ling, and X. Geng, \u201cFast label enhancement for label distribution learning,\u201d IEEE Trans. Knowl. Data Eng., vol. 35, no. 2, pp. 1502\u20131514, 2023.   \n[34] Y. Gao, T. Lin, Y. Zhang, S. Luo, and F. Nie, \u201cRobust principal component analysis based on discriminant information,\u201d IEEE Trans. Knowl. Data Eng., vol. 35, no. 2, pp. 1991\u20132003, 2023.   \n[35] J. Zhang, V. S. Sheng, B. Nicholson, and X. Wu, \u201cCEKA: a tool for mining the wisdom of crowds,\u201d J. Mach. Learn. Res., vol. 16, pp. 2853\u20132858, 2015.   \n[36] I. H. Witten, E. Frank, and M. A. Hall, Data mining: practical machine learning tools and techniques, 3rd Edition. Morgan Kaufmann, Elsevier, 2011.   \n[37] J. Demsar, \u201cStatistical comparisons of classifiers over multiple data sets,\u201d J. Mach. Learn. Res., vol. 7, pp. 1\u201330, 2006. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix A The proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. The covariance matrix $\\mathcal{C}_{q}$ is a symmetric matrix. Therefore, if Assumption 1 holds, i.e., the covariance matrix $\\mathcal{C}_{q}$ is a nonsingular matrix, $\\mathcal{C}_{q}$ must be also a $M$ -order symmetric matrix. This means that we can obtain $M$ different eigenvalues and $M$ mutually orthogonal normed eigenvectors when $\\ensuremath{{\\mathcal{C}}}_{q}$ is given. These orthogonal normed eigenvectors can form an orthogonal matrix $\\mathcal{P}$ , and thus $\\mathcal{P}$ satisfies $\\mathcal{P}^{-1}\\pmb{\\mathscr{C}}_{q}\\pmb{\\mathscr{P}}=\\pmb{\\mathscr{P}}^{T}\\pmb{\\mathscr{C}}_{q}\\pmb{\\mathscr{P}}=\\pmb{\\Lambda}$ . Here, $\\Lambda$ is a diagonal matrix with all $M$ eigenvalues of $\\ensuremath{{\\mathcal{C}}}_{q}$ as its elements of the principal diagonal. Moreover, the order of eigenvalues in $\\Lambda$ should correspond to the order of eigenvectors in $\\mathcal{P}$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Appendix B The proof of Theorem 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. Theorem 3 holds when the distance metric can work effectively given a crowdsourced dataset. The distance metric works effectively, which means that the smaller the $d(x_{1},x_{2})$ , the more similar $x_{1}$ and $\\pmb{x}_{2}$ are to each other and the more likely they are to belong to the same class. Therefore, in the neighbor sequence $<\\pmb{x}_{i}^{1},\\ldots,\\pmb{x}_{i}^{k},\\ldots,\\pmb{x}_{i}^{N}>$ of $\\pmb{x}_{i}$ , $\\pmb{x}_{i}^{k}$ and $\\pmb{x}_{i}$ are more likely to belong to the same class when $k$ is small. At this point, when $P_{i}^{k}$ is updated, the probability corresponding to the unknown true label $y_{i}$ of $\\pmb{x}_{i}$ will increase. When $k$ gradually increases and exceeds a certain threshold, $\\pmb{x}_{i}$ and $\\pmb{x}_{i}^{k}$ begin to belong to different classes, at which point the probability corresponding to $y_{i}$ will decrease. In other words, as $k$ increases from 0, the probability corresponding to $y_{i}$ increases first. As $k$ exceeds a certain threshold (the optimal neighborhood size), the probability corresponding to $y_{i}$ begins to decrease. Therefore, $m a x(P_{i}^{k})$ tends to be the probability corresponding to $y_{i}$ when $k$ increases from 0, and $k$ tends to be $K_{i}^{*}$ when $\\hat{\\mathcal{M}}_{k}$ achieves the highest value. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Appendix C More descriptions of the experimental datasets and metrics ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Simulated datasets. The descriptions of the whole 34 simulated datasets are listed in Table 5. Here, \u201c#Instances\u201d denotes the number of instances, \u201c#Attributes\u201d denotes the number of attributes, \u201c#Classes\u201d denotes the number of classes, \u201cMissing\u201d denotes whether the dataset contains missing values and \u201cAttribute type\u201d denotes the type of attributes the dataset contains. These datasets are collected from different application scenarios and represent different crowdsourcing requirements. ", "page_idx": 12}, {"type": "text", "text": "Real-world datasets. The Income dataset is annotated by 67 workers through the online platform Amazon Mechanical Turk (AMT), and each instance is annotated by 10 different workers. The Income dataset is a binary crowdsourced dataset, which contains 600 instances, 6000 labels, 10 attributes (nominal attributes) and 0 missing values. The Leaves dataset is annotated by 83 workers through AMT, and each instance is annotated by 10 different workers. The Leaves dataset is a multi-class crowdsourced dataset, which contains 384 instances, 3840 labels, 64 attributes (numeric attributes) and 0 missing values. ", "page_idx": 12}, {"type": "text", "text": "Experimental metrics. The integration accuracy is calculated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nA c c u r a c y=\\frac{\\sum_{i=1}^{N}\\delta(\\hat{y}_{i},y_{i})}{N}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The Macro-F1 score is calculated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nF1=\\frac{\\sum_{q=1}^{Q}\\frac{2*P r e c i s i o n_{q}*R e c a l l_{q}}{P r e c i s i o n_{q}+R e c a l l_{q}}}{Q},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where Precisionq and $R e c a l l_{q}$ can be calculated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nP r e c i s i o n_{q}=\\frac{\\sum_{i=1}^{N}\\delta(\\hat{y}_{i},c_{q})*\\delta(y_{i},c_{q})}{\\delta(\\hat{y}_{i},c_{q})}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nR e c a l l_{q}=\\frac{\\sum_{i=1}^{N}\\delta(\\hat{y}_{i},c_{q})*\\delta(y_{i},c_{q})}{\\delta(y_{i},c_{q})}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "table", "img_path": "wnPlJNiqfA/tmp/e6696ba45de95c67ca242b303f5750ac8cfb37a12ffd74336a542446753a3b89.jpg", "table_caption": ["Table 5: The descriptions of 34 simulated datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We clearly claim in the abstract and introduction section that the KFNN proposed in our paper is mainly used to improve the effectiveness and robustness of label integration in crowdsourcing. The contributions of our paper are also highlighted in the introduction. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We mentioned the limitations of our KFNN in the Conclusion and future work section. KFNN has two empirical parameters and its distribution transformation process is not refined enough. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We give proofs to theorems that appear in the paper. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: This paper fully discloses all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 14}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: This paper provides open access to the data and code, and provides a document to guide readers in reproducing all experimental results in supplemental material. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 15}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper specifies all the training and test details necessary to understand the results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We explain in detail the experimental metrics in the paper and provide the results of the significance tests. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We describe in detail the experimental setting on the computer resources in this paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The research conducted in this paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The new algorithm proposed in this paper helps to improve the effectiveness and robustness of label integration. There are no negative societal impacts. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We used the datasets and algorithmic implementations published by the CEKA platform, which is described in detail in the paper. The license and terms of use explicitly are properly respected in this paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper provides open access to the data and code, and provides a document to guide readers in reproducing all experimental results in supplemental material. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper did not select new crowdsourced datasets. The datasets used for experiments are publicly available and their information is described in detail in the paper. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not involve the research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]