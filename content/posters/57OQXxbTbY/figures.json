[{"figure_path": "57OQXxbTbY/figures/figures_1_1.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "The figure shows the training dynamics of DPO and Cal-DPO algorithms on the UltraFeedback dataset.  It illustrates how the implicit rewards for chosen and rejected responses change during training.  DPO shows a decrease in rewards for chosen responses, even while maintaining a margin over rejected responses.  In contrast, Cal-DPO maintains positive rewards for chosen responses while still increasing the margin. The bar chart shows that Cal-DPO significantly outperforms DPO on several reasoning benchmarks.", "section": "1 Introduction"}, {"figure_path": "57OQXxbTbY/figures/figures_8_1.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "This figure shows the training dynamics of DPO and Cal-DPO on the UltraFeedback dataset using the Zephyr-7b-sft model.  It illustrates that DPO's rewards for chosen responses decrease below zero, whereas Cal-DPO's chosen rewards remain positive and increase.  The margins (difference between chosen and rejected rewards) also increase substantially in Cal-DPO.  The results highlight Cal-DPO's improved performance on various reasoning benchmarks over DPO.", "section": "1 Introduction"}, {"figure_path": "57OQXxbTbY/figures/figures_8_2.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "This figure shows a comparison of the training dynamics of DPO and Cal-DPO on the UltraFeedback dataset using the Zephyr-7b-sft model.  It highlights how Cal-DPO maintains positive rewards for chosen responses, unlike DPO which shows decreasing rewards, even falling below zero.  This difference demonstrates Cal-DPO's improved performance, particularly in reasoning tasks, which is further elaborated in section 5.", "section": "1 Introduction"}, {"figure_path": "57OQXxbTbY/figures/figures_17_1.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "This figure shows the training dynamics of both DPO and Cal-DPO on the UltraFeedback dataset using the Zephyr-7b-sft base model. It demonstrates that while DPO's rewards for chosen responses decrease below zero, Cal-DPO's rewards remain positive and continue to increase. This difference in reward dynamics contributes to Cal-DPO's superior performance across reasoning benchmarks.", "section": "Introduction"}, {"figure_path": "57OQXxbTbY/figures/figures_18_1.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "This figure displays the training dynamics of DPO and Cal-DPO on the UltraFeedback dataset using the Zephyr-7b-sft model.  It shows that while both methods increase the margin between chosen and rejected responses, DPO's rewards for chosen responses fall below zero, whereas Cal-DPO keeps them positive.  Cal-DPO demonstrates superior performance across various reasoning benchmarks, highlighting the effectiveness of reward calibration.", "section": "1 Introduction"}, {"figure_path": "57OQXxbTbY/figures/figures_19_1.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "The figure shows the implicit reward dynamics during training of DPO and Cal-DPO.  It reveals that DPO's rewards for chosen responses decrease below zero while Cal-DPO's rewards remain positive and increase. Cal-DPO significantly outperforms DPO across reasoning benchmarks, demonstrating the effectiveness of reward calibration.", "section": "1 Introduction"}, {"figure_path": "57OQXxbTbY/figures/figures_20_1.jpg", "caption": "Figure 1: The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with the base model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decrease below zero, whereas in our Cal-DPO, they keep increasing and remain positive. Our Cal-DPO significantly outperforms DPO across reasoning benchmarks. More results on other datasets are provided in Section 5.", "description": "This figure shows the reward dynamics during the training process of DPO and Cal-DPO algorithms.  The results show that while DPO's rewards for chosen responses decrease below zero, Cal-DPO keeps increasing and remains positive. This difference highlights the effectiveness of Cal-DPO in maintaining positive rewards for correct responses which ultimately improves its performance. The figure also demonstrates Cal-DPO's superior performance across reasoning benchmarks compared to DPO.", "section": "Introduction"}]