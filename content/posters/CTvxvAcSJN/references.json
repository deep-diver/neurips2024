{"references": [{"fullname_first_author": "B. Mildenhall", "paper_title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "publication_date": "2020-00-00", "reason": "This paper introduces NeRF, a foundational method for novel view synthesis that is heavily used in the current paper's approach."}, {"fullname_first_author": "B. Poole", "paper_title": "DreamFusion: Text-to-3D using 2D diffusion", "publication_date": "2022-00-00", "reason": "This paper introduces the method of using 2D diffusion models for generating 3D scenes, which is the core approach adopted and improved upon in the current work."}, {"fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper details the Stable Diffusion model, which is used as a base for the SceneCraft2D model, demonstrating the power of diffusion models that is leveraged in this paper's approach."}, {"fullname_first_author": "D. Cohen-Bar", "paper_title": "Set-the-scene: Global-local training for generating controllable NeRF scenes", "publication_date": "2023-00-00", "reason": "This paper is among the most important due to its focus on generating 3D scenes guided by semantic layouts which is a closely related approach compared to the current paper."}, {"fullname_first_author": "L. H\u00f6llein", "paper_title": "Text2Room: Extracting textured 3D meshes from 2D text-to-image models", "publication_date": "2023-00-00", "reason": "This paper is highly relevant due to its method of generating 3D scenes from 2D images guided by text prompts which shares a similar goal and approach to the work in the current paper."}]}