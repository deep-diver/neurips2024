[{"figure_path": "0G0VpMjKyV/figures/figures_8_1.jpg", "caption": "Figure 1: Communication Efficiency. Count-Sketch algorithm in Algorithm 1 against local Top-r [8] and FetchSGD [2], with and without error feedback. 100 clients run 5 local iterations with full-batch at every round.", "description": "The figure compares the communication efficiency of the Count-Sketch algorithm proposed in the paper against two other common approaches, Local Top-r and FetchSGD, both with and without error feedback mechanisms.  The experiment used 100 clients, each performing 5 local iterations using full batches of data in every round. The x-axis represents the degree of dimension reduction, and the y-axis shows the accuracy achieved. The results demonstrate that Count-Sketch achieves competitive accuracy even without error feedback.", "section": "Experimental Results"}, {"figure_path": "0G0VpMjKyV/figures/figures_24_1.jpg", "caption": "Figure 24: Estimate of \u03ba = \u03a3i=1 |\u03bbi|/\u03bbmax over training iterations.", "description": "This figure shows the value of \u03ba (kappa) across training epochs for two different loss functions: Binary Cross Entropy (BCE) and Mean Squared Error (MSE). Kappa is defined as the sum of absolute eigenvalues divided by the maximum eigenvalue of the predictor Hessian.  The plot shows how this value changes as the model trains, indicating the evolution of the second-order properties of the loss function over time. This is relevant to the paper's analysis because it helps demonstrate that the restricted strong smoothness (RSS) property, which is crucial to their theoretical findings, holds in practice.  The relatively small values of \u03ba compared to the model dimension support their core argument that the ambient dimension does not hinder the performance of sketching-based distributed learning.", "section": "Experimental Results"}, {"figure_path": "0G0VpMjKyV/figures/figures_25_1.jpg", "caption": "Figure 3: Spectral Density of Predictor Hessian :H(\u03b8, x\u2081) = l'i\u2207\u00b2f(\u03b8; x\u2081) for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian for a ResNet-18 model trained on 1000 samples from the CIFAR-10 dataset, at different training epochs (500, 1000, and 1500).  The spectral density is calculated using the Binary Cross Entropy loss function. The plots visualize the distribution of eigenvalues of the Hessian, showing a concentration of eigenvalues around zero and a rapid decay in density as eigenvalues move away from zero. This demonstrates the restricted strong convexity of the loss function, a key property used in the paper's analysis.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_1.jpg", "caption": "Figure 2: Estimate of k = \u03a3=1 |\u03bbi|/\u03bbmax of H\u2081 for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 107 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "The figure shows the estimate of kappa (\u03ba) which is the sum of absolute eigenvalues of the predictor Hessian (H) divided by the maximum eigenvalue (\u03bbmax) for a fixed training input across training epochs.  The data is from 1000 samples of the CIFAR-10 dataset, using a ResNet-18 model with approximately 11 million parameters and the Binary Cross Entropy loss function.", "section": "Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_2.jpg", "caption": "Figure 5: Spectral Density of Predictor Hessian :H(\u03b8, x\u2081) = l'i\u2207\u00b2f(\u03b8; x\u2081) for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian for a fixed training input at different training epochs.  The dataset used is CIFAR-10, the model is ResNet-18, and the loss function is Binary Cross Entropy. The plot visualizes the distribution of eigenvalues of the Hessian matrix, which provides insights into the model's curvature and how it changes during training. The density is shown on a log scale.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_3.jpg", "caption": "Figure 6: Spectral Density of Predictor Hessian :H(\u03b8, x\u2081) = l'i\u2207\u00b2f(\u03b8; x\u2081) for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian for a fixed training input across multiple training epochs.  The data is from the CIFAR-10 dataset, and a ResNet-18 model (with approximately 11 million parameters) and binary cross-entropy loss function were used. The plot shows the distribution of eigenvalues of the Hessian, illustrating the concentration of eigenvalues around zero and the rapid decay of the density for larger eigenvalues.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_4.jpg", "caption": "Figure 7: Epoch: 600", "description": "This figure shows the spectral density of the predictor Hessian H(\u03b8,x) = l'i\u2207\u00b2f(\u03b8;x) for a fixed training input at epoch 600. The dataset used is CIFAR-10, and the model is ResNet-18 with 1.1 \u00d7 10\u2077 parameters. The loss function is Binary Cross Entropy (BCE) Loss. The figure illustrates the distribution of eigenvalues of the Hessian matrix, highlighting the concentration of eigenvalues around zero, which is consistent with the RSS property discussed in the paper.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_5.jpg", "caption": "Figure 8: Spectral Density of Predictor Hessian :H(0, x\u2081) = l'\u00bf\u2207\u00b2f(0; x\u2081) for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian for a fixed training input at different training epochs (epoch 800 is shown here).  It uses the ResNet-18 model trained on 1000 samples from the CIFAR-10 dataset with binary cross-entropy loss. The x-axis represents the eigenvalues, and the y-axis represents the density (log scale). This visualization helps to understand the distribution of eigenvalues of the Hessian, which is relevant to the paper's analysis of the restricted strong smoothness property of deep learning losses.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_6.jpg", "caption": "Figure 3: Spectral Density of Predictor Hessian :H(0, x\u2081) = l'\u00bf\u2207\u00b2f(0; x\u2081) for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian for a ResNet-18 model trained on 1000 samples from the CIFAR-10 dataset, using the binary cross-entropy loss function. The spectral density is computed for a fixed training input across multiple training epochs (500, 1000, and 1500). The figure helps to visualize how the eigenvalues of the Hessian change during the training process, providing insights into the optimization landscape and the model's learning dynamics.", "section": "Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_7.jpg", "caption": "Figure 10: Epoch: 1200", "description": "This figure is one of a set of figures (Figure 10-Figure 12 and Figure 14-Figure 22) showing the spectral density of the predictor Hessian H(\u03b8, x\u2081) = l'i\u2207\u00b2f(\u03b8; x\u2081) for a fixed training input across multiple training epochs.  The model used is ResNet-18 with 1.1 \u00d7 10\u2077 parameters. Two loss functions are used: Binary Cross Entropy (BCE) and Mean Squared Error (MSE).  Each figure shows the density distribution of eigenvalues. The data used is 1000 samples from the CIFAR-10 dataset.  These figures support the authors' analysis and observations regarding the properties of the loss function in deep learning models.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_8.jpg", "caption": "Figure 11: Epoch: 1400", "description": "This figure shows the spectral density of the predictor Hessian for a fixed training input at epoch 1400.  The dataset used is 1000 samples from CIFAR-10, the model is ResNet-18 (with 11 million parameters), and the loss function is Binary Cross Entropy (BCE) Loss. The graph displays the density (in log scale) on the y-axis and the eigenvalue on the x-axis.  It shows a distribution of eigenvalues, concentrating near zero, suggesting that the Hessian's spectrum displays a bulk and outliers structure, with most of the eigenvalues having very small values.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_26_9.jpg", "caption": "Figure 23: Spectral Density of Predictor Hessian :H(0, x\u2081) = l'\u00bf\u2207\u00b2f(0; x\u2081) for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Mean Squared Error (MSE).", "description": "This figure shows the spectral density of the predictor Hessian for a ResNet-18 model trained on 1000 samples from the CIFAR-10 dataset using the mean squared error loss function. The spectral density is calculated for a fixed training input across multiple training epochs. The figure is intended to visually support the claim that the sum of absolute eigenvalues of the predictor Hessian is bounded (Assumption 4.2).", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_1.jpg", "caption": "Figure 2: Estimate of k = \u03a3i=1 |\u03bbi|/\u03bbmax of H\u2081 for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 107 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the estimate of the constant kappa (\u03ba) across training epochs. Kappa is calculated as the sum of absolute eigenvalues divided by the maximum eigenvalue of the Hessian matrix of the predictor.  The experiment used 1000 samples from the CIFAR-10 dataset, a ResNet-18 model, and binary cross-entropy loss. The results demonstrate that \u03ba remains significantly smaller than the model dimension (p) throughout training, supporting Assumption 4.2 of the paper.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_2.jpg", "caption": "Figure 3: Spectral Density of Predictor Hessian :H(0, x\u2081) = l'i\u2207\u00b2f(0; x\u2081) for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian across training epochs for a fixed training input using the BCE loss function.  The dataset is CIFAR-10, and the model is ResNet-18. The plot shows that most eigenvalues of the Hessian are very close to zero with a quick decay away from zero.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_3.jpg", "caption": "Figure 1: Communication Efficiency. Count-Sketch algorithm in Algorithm 1 against local Top-r [8] and FetchSGD [2], with and without error feedback. 100 clients run 5 local iterations with full-batch at every round.", "description": "The figure compares the communication efficiency of three different distributed learning algorithms: Count-Sketch (Algorithm 1 from the paper), Local Top-r, and FetchSGD.  It shows the accuracy achieved by each algorithm with varying degrees of dimension reduction.  The experiment uses 100 clients performing 5 local full-batch gradient descent iterations per round. Notably, the comparison includes versions of Local Top-r and FetchSGD both with and without error feedback mechanisms, highlighting the impact of unbiased sketching in Count-Sketch.", "section": "Experimental Results"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_4.jpg", "caption": "Figure 7: Epoch: 600", "description": "This figure shows the spectral density of the predictor Hessian for a fixed training input at epoch 600.  The dataset used is CIFAR-10, the model is ResNet-18, and the loss function is Binary Cross Entropy (BCE). The x-axis represents the eigenvalues, and the y-axis shows the density (on a log scale). The plot visually demonstrates the distribution of eigenvalues, highlighting the concentration around zero and a rapid decay in density as the eigenvalues move away from zero. This observation is crucial to the paper's argument about the restricted strong smoothness property of deep learning losses.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_5.jpg", "caption": "Figure 8: Epoch: 800", "description": "This figure is one of a series of figures (Figure 7-Figure 9) showing the spectral density of the predictor Hessian across various training epochs.  The x-axis represents the eigenvalues, and the y-axis represents the density (on a log scale). The data is from the CIFAR-10 dataset, using a ResNet-18 model with Binary Cross-Entropy loss. Each figure in the sequence shows the distribution of eigenvalues at a different training epoch (Epoch: 800 in this specific figure). This visualization is used to illustrate the distribution of eigenvalues of the predictor Hessian, which is relevant to the paper's analysis of the restricted strong smoothness (RSS) property of deep learning models.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_6.jpg", "caption": "Figure 3: Spectral Density of Predictor Hessian :H(\u03b8, x\u2081) = l'i\u2207\u00b2f(\u03b8; x\u2081) for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Binary Cross Entropy(BCE) Loss.", "description": "This figure shows the spectral density of the predictor Hessian for a ResNet-18 model trained on 1000 samples from the CIFAR-10 dataset. The spectral density is computed using stochastic Lanczos quadrature for three different training epochs (500, 1000, and 1500).  The figure illustrates that most of the eigenvalues of the Hessian are near zero, with a rapid decay in density as the magnitude of the eigenvalue increases. This observation supports the assumption of restricted strong smoothness, crucial for the paper's analysis.", "section": "Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_7.jpg", "caption": "Figure 10: Epoch: 1200", "description": "This figure shows the spectral density of the predictor Hessian for a fixed training input at epoch 1200. The dataset used is CIFAR-10 with 1000 samples, the model is ResNet-18, and the loss function is Binary Cross Entropy (BCE). The x-axis shows the eigenvalue, and the y-axis shows the density on a logarithmic scale.  The plot illustrates the distribution of eigenvalues of the Hessian matrix, highlighting the concentration of eigenvalues around zero, which is a key observation supporting the paper's theoretical analysis about the restricted strong smoothness property of deep learning losses.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_8.jpg", "caption": "Figure 11: Epoch: 1400", "description": "This figure shows the spectral density of the predictor Hessian H(\u03b8,x) = l'i\u2207\u00b2f(\u03b8;x) for a fixed training input across training epochs. The dataset used is 1000 samples from the CIFAR-10 dataset. The model used is ResNet-18, which has 1.1 x 10\u2077 parameters.  The loss function is Mean Squared Error (MSE).  The figure visually represents the distribution of eigenvalues of the Hessian matrix at epoch 1400 of training.  It illustrates the concentration of eigenvalues around zero and the sparsity of the Hessian in this model.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_9.jpg", "caption": "Figure 23: Spectral Density of Predictor Hessian :H(0, x\u2081) = l'i\u2207\u00b2f(0; x\u2081) for a fixed training input across training epochs. Dataset: 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has 1.1 \u00d7 10\u2077 parameters. Loss function: Mean Squared Error (MSE).", "description": "The figure shows the spectral density of the predictor Hessian for a fixed training input across different training epochs. The dataset used is CIFAR-10, with a ResNet-18 model having 11 million parameters. The loss function employed is the Mean Squared Error (MSE).  It illustrates the distribution of eigenvalues of the Hessian matrix, providing insights into the model's optimization landscape during training.", "section": "G Additional Results on Spectral Density of Predictor Hessian"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_10.jpg", "caption": "Figure 24: Estimate of k = \u2211i=1 |\u03bbi|/\u03bbmax over training iterations.", "description": "This figure shows the estimate of kappa (\u03ba) which is the sum of absolute eigenvalues divided by the maximum eigenvalue of the Hessian across training iterations.  The plot shows how this value changes over the training process for both Binary Cross Entropy (BCE) and Mean Squared Error (MSE) loss functions. The value of kappa is related to the restricted strong smoothness property of the loss function which is critical to the paper's theoretical analysis.", "section": "Experimental Results"}, {"figure_path": "0G0VpMjKyV/figures/figures_27_11.jpg", "caption": "Figure 24: Estimate of \u03ba = \u03a3<sup>p</sup><sub>i=1</sub> |\u03bb<sub>i</sub>|/\u03bb<sub>max</sub> over training iterations.", "description": "This figure shows the estimate of kappa (\u03ba) across training iterations. Kappa is calculated as the sum of the absolute values of eigenvalues (\u03bb<sub>i</sub>) divided by the maximum eigenvalue (\u03bb<sub>max</sub>) of the predictor Hessian.  The plot illustrates how this value changes over time during the training process for two different loss functions: Binary Cross Entropy (BCE) and Mean Squared Error (MSE).  The x-axis represents the training epoch, and the y-axis represents the value of \u03ba.", "section": "Experimental Results"}]