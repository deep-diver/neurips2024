[{"type": "text", "text": "Sketching for Distributed Deep Learning: A Sharper Analysis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mayank Shrivastava University of Illinois Urbana-Champaign mayanks4@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Berivan Isik\u2217 Google berivan@google.com ", "page_idx": 0}, {"type": "text", "text": "Qiaobo Li University of Illinois Urbana-Champaign qiaobol2@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Sanmi Koyejo Stanford University sanmi@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Arindam Banerjee University of Illinois Urbana-Champaign arindamb@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The high communication cost between the server and the clients is a significant bottleneck in scaling distributed learning for overparametrized deep models. One popular approach for reducing this communication overhead is randomized sketching. However, existing theoretical analyses for sketching-based distributed learning (sketch-DL) either incur a prohibitive dependence on the ambient dimension [1] or need additional restrictive assumptions such as heavy-hitters [2]. Nevertheless, despite existing pessimistic analyses, empirical evidence suggests that sketch-DL is competitive with its uncompressed counterpart \u2013 thus motivating a sharper analysis. In this work, we introduce a sharper ambient dimension-independent convergence analysis for sketch-DL using the second-order geometry specified by the loss Hessian. Our results imply ambient dimension-independent communication complexity for sketch-DL. We present empirical results both on the loss Hessian and overall accuracy of sketch-DL supporting our theoretical results. Taken together, our results provide theoretical justification for the observed empirical success of sketch-DL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributed learning is a popular framework for training machine learning models, often deployed to support large-scale deployments and to support privacy, among other systems goals [3, 4]. To this end, distributed learning generally employs both server and client devices. Most standard implementations are set up in rounds \u2013 in each round, clients participate by performing (multiple) local model updates using their local data, then share the model updates with a server. The server aggregates the collected local updates and broadcasts either the aggregate or the updated global model to the clients for the next round. As a result of the system setup, distributed learning generally requires a high communication overhead due to the frequent communication between the server and the clients [5]. Federated learning \u2013 a popular distributed learning setup, further extends the generic setting to where clients\u2019 data distribution may be non-i.i.d. and the clients may change over time \u2013 thus suffering from the same costly communication. This communication overhead has motivated research in compressing the model updates [6, 7] through sparsification [8, 9, 10, 11, 12], quantization [13, 14, 15], lowrank projection [16, 17, 18, 19], and sketching [2, 1, 20, 21, 22]. Among these, linear sketching mechanisms have attracted significant attention due to their efficient and simple integration with existing distributed and federated learning frameworks. For instance, unlike many sparsification approaches [8, 11], sketching is an unbiased operation that does not require bias correction with error feedback mechanisms [23] \u2013 especially as error feedback typically increases the memory cost and the complexity of integrating differential privacy [24]. Moreover, the linearity of the sketching operation makes it compatible with secure aggregation [25, 26] as opposed to the popular Top- $\\cdot\\mathbf{\\bar{r}}^{\\overline{{2}}}$ sparsification [8] or quantization methods [13] that break the linearity in aggregation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the rising interest in sketching for distributed and federated learning, the existing bounds on the convergence error of sketching scale with the ambient dimension of the model [1] (i.e., in contrast to the lower sketching dimension) which may be extremely large for modern overparameterized deep models [27, 3, 28]. This dimension dependence has been a limitation of the sketching mechanisms at scale, making them less attractive for modern deep models. We identify and demonstrate the root cause of the dimension dependence of the standard optimization convergence analysis as associated with the assumption of smoothness of the loss, commonly made in such analyses [1, 2]. Several modifications have been proposed to get rid of the dimension dependence. For instance, Rothchild et al. [2] assume that the model updates at the clients have heavy hitters and, further, apply a Top-r sparsification to break the dimension dependence. However, their heavy hitter assumption may not hold in general, and the Top-r operation introduces a bias that is, in turn, eliminated via error feedback. While their analysis requires these restrictive assumptions and modifications to eliminate the dimension dependence, they also note that they actually do not observe a dimension dependence empirically even without these restrictions \u2013 indicating that the theoretical analysis of their sketching algorithm lacks behind its empirical success. Quoting from Appendix B.3 of Rothchild et al. [2], \u201cHowever, this dimensionality dependence does not reflect our observation that the algorithm performs competitively with uncompressed SGD in practice, motivating our assumptions and analysis\u201d. Motivated by this mismatch between the existing analyses of sketching for distributed learning (which impose dimension dependence) [2, 1] and its empirical success in practice, we provide a substantially tighter analysis than the prior work and eliminate the dimension dependence in sketching without imposing any unrealistic restrictive assumptions in the setup. ", "page_idx": 1}, {"type": "text", "text": "Our sharper analysis avoids the global smoothness assumption and utilizes the approximate restricted strong smoothness (RSS) property of overparameterized deep models [29, 30, 31, 32, 33, 34], and our results are in terms of the second order geometry of the loss. We present optimization results for single-local step as well as $K$ -local step distributed learning, and present bounds on communication complexity based on the optimization results. We refer the reader to Table 1 for an overview of our results. We do not restrict our analysis to any specific sketching matrix, and our results hold for any sub-Gaussian sketching matrix. For instance, for computational beneftis, one can use the popular Count-Sketch [35] or Hadamard sketch [36] approaches, which are both examples of sub-Gaussian sketching. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We identify and demonstrate the widely used global smoothness assumption of loss functions as the root cause of the dimension dependence of sketching methods.   \n2. We provide a novel analysis for sketching in distributed and federated learning that eliminates the ambient dimension dependence in the convergence error.   \n3. We are the first to break this dimension dependence without making restrictive assumptions such as the heavy hitter assumption by Rothchild et al. [2].   \n4. We are again the first to do this without a Top-r sparsification step in the framework which would have required additional measures to eliminate the bias.   \n5. We are the first to use more precise second-order properties of the loss of deep models, e.g., approximate restricted strong smoothness, stable rank of predictor Hessian, to analyze distributed and federated learning frameworks. ", "page_idx": 1}, {"type": "table", "img_path": "0G0VpMjKyV/tmp/425efa81ff8a559f5138cd44371c8c14adc4f5640f2d29fa9ba58464f38724e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "0G0VpMjKyV/tmp/bae5388de58f2e8163666225e6a47fd5d2c765108226472f20be8afdeeef52d4.jpg", "table_caption": ["(a) $\\scriptstyle\\mathrm{K}=1$ local steps ", "(b) $\\mathrm{K}{>}1$ local steps "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Table 1: Comparison of iteration complexities and assumptions with prior work. $c_{s},c_{l}$ are constants defined in Assumption 3.2 and $\\varrho,c_{H}$ are defined in Lemma C.1,Lemma C.2 respectively. Note that $\\varrho$ and $c_{H}$ are $\\mathcal{O}(\\mathfrak{p o l y}(L))$ and can be seen as independent of ambient dimension for wide networks. Under our assumptions and setup (Section 3). According to Lemma C.4 the loss function $\\mathcal{L}$ can be shown to be $\\beta.$ -smooth where $\\begin{array}{r}{\\vec{\\beta^{}}=\\mathcal{O}(\\varrho^{2}+\\frac{c_{H}}{\\sqrt{m}})}\\end{array}$ . SC refers to strong-convexity and PL refers to PL-condition. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Communication-Efficient Distributed Learning. The high cost of communicating model updates between the clients and the server has motivated a recent interest in improving communication efficiency in distributed and federated learning. One common strategy called FedAvg [20] enables less frequent communication by letting the client perform multiple local iterations at every round. Another common approach is to compress the model updates before the communication to reduce the cost of each round. These efforts can broadly be categorized into sparsification [37, 9, 8], quantization [38, 13, 14], low-rank factorization [17, 39, 16], sketching [2, 1, 40], and sparse subnetwork training techniques [12, 41, 42, 43, 44]. While some of these compression methods are already unbiased [13], many are biased and have to be combined with other mechanisms to reduce the bias for better convergence [8, 2]. Linearity is another desired feature, as it simplifies the implementation of distributed learning with security-promoting techniques like secure aggregation (in the compressed dimension). Among the general compression approaches mentioned, sketching stands out as a simple linear and unbiased operation, allowing for computations in the reduced dimension before desketching. We do not propose a new compression method in this work but instead provide a substantially improved convergence analysis for sketching-based frameworks that breaks the dimension dependence\u2013which explains why sketching would not explode the converge error in large models. ", "page_idx": 2}, {"type": "text", "text": "Sketching. Over many years, sketching has been a fundamental tool for many applications, even before the surge of deep learning in 2010s [45, 46, 47] for low-rank approximation [48], graph sparsification [49], and least squares regression [50]. More recently, sketching has also found use in distributed and federated learning frameworks to reduce the dimension of the model updates for communication efficiency [40, 21, 2, 1, 22]. The linearity of these sketching-based frameworks has led to their successful integration with secure aggregation and differential privacy as well [51, 52, 1]. Despite the empirical success of these sketching-based applications in distributed and federated learning, the existing upper bounds on the convergence errors have a dependence on the ambient dimension of the model\u2013which limits their scalability to larger models. In this work, we provide a tighter convergence analysis and get rid of the dimension dependence, suggesting the promise of sketching at scale. The closest to our work is by Rothchild et al. [2] who also provide a dimension-independent convergence bound for their sketching algorithm, called FetchSGD (which is a combination of Count-Sketch projection, Top-r sparsification, and bias reduction with error feedback), under the assumption that the model updates have heavy-hitters. We note that both the heavy-hitter assumption and the Top-r step (and the error feedback coming along to minimize the bias) are necessary to get rid of the dimension dependence in their convergence analysis. We avoid both of these restrictions. Our key contributions over FetchSGD are: (1) We do not make the heavy-hitter assumption since it does not hold in general. (2) We do not require Top-r sparsification to break the dimension dependence \u2013 this way, we have an unbiased sketching mechanism without the need for error feedback, which would increase the memory cost and make the integration with differential privacy mechanisms complicated. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Notation. For a quantity $x$ , we use the notation $x_{t}$ to refer to the global variable at round $t$ shared by all the clients and the server, and $x_{c,t}$ to refer to the local variable for client $c$ at round $t$ . For a positive integer $n$ , $[n]=\\{1,\\cdot\\cdot\\cdot,n\\}$ . We use $\\mathbb{I}^{p\\times p}$ as the $p\\times p$ identity matrix. We use $\\mathbb{E}[\\cdot]$ for expectation. For a vector $\\mathbf{x}$ , we use $\\|\\mathbf{x}\\|$ or $\\lVert\\mathbf{x}\\rVert_{2}$ to denote its $\\mathbb{L}_{2}$ norm. For a matrix $A$ , we use $\\left\\|A\\right\\|$ or $\\left\\Vert A\\right\\Vert_{2}$ for the spectral norm of $A$ . We use $\\begin{array}{r}{\\nabla_{\\theta}\\mathcal{L}(\\theta^{'})=\\frac{\\partial\\mathcal{L}}{\\partial\\theta}|_{\\theta=\\theta^{\\prime}}}\\end{array}$ . For a random vector $\\mathbf{y}$ , $\\|\\mathbf{y}\\|_{\\psi_{2}}$ denotes its sub-Gaussian norm [53]. The notations $\\tilde{\\mathcal{O}}(t),\\tilde{\\Omega}(t),\\tilde{\\Theta}(t)$ are the same as the common $O(t),\\Omega(t),\\Theta(t)$ but they hide the dependence on logarithmic terms. polylog $(n)$ denotes ${\\mathcal{O}}(\\log^{k}(n))$ for some $k$ . ", "page_idx": 3}, {"type": "text", "text": "3 Sketching for Distributed Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a distributed learning framework with $C$ clients, each client $c=1,\\ldots,C$ having a local dataset $\\mathcal{D}_{c}\\,=\\,\\{x_{i,c},y_{i,c}\\}_{i=1}^{n_{c}}$ of size $n_{c}$ and a local loss $\\mathcal{L}_{c}:\\mathbb{R}^{p}\\,\\rightarrow\\,\\mathbb{R}$ defined as $\\mathcal{L}_{c}(\\theta)=$ $\\begin{array}{r}{\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell(y_{i,c},\\hat{y}_{i,c})}\\end{array}$ , where $\\theta\\in\\mathbb{R}^{p}$ is the parameter vector, $\\hat{y}_{i,c}:=f(\\boldsymbol{\\theta};\\mathbf{x}_{i,c})$ is the prediction of the model for input ${\\bf x}_{i,c}$ , and $\\ell:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ is a loss function that measures the error between $\\hat{y}_{i,c}$ and the groundtruth $y_{i,c}$ . Our goal is to minimize the empirical loss ${\\mathcal{L}}(\\theta)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\frac{1}{C}\\sum_{c=1}^{C}\\mathcal{L}_{c}(\\theta).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Unlike much of the existing literature [2, 21, 1], our analysis does not ignore the fact that the predictor $f$ is a deep learning model \u2013 i.e., the loss $\\mathcal{L}_{c}$ is not just an arbitrary smooth loss. The motivation behind this focus is that many of the modern models being used in distributed learning are indeed deep learning models. Interestingly, the losses associated with the deep learning models have a certain second-order structure beyond basic smoothness which will be the key to our sharper analysis. We also demonstrate that just assuming that the loss is smooth, as is typically done in most of the existing literature [2, 21, 1], does not avail one of the sharper analysis we introduce \u2013 making the dimension dependence unavoidable. ", "page_idx": 3}, {"type": "text", "text": "Following standard literature [31, 54, 55, 29], we consider a fully-connected feed-forward neural network $f$ of depth $L$ , with widths $m$ and activations $\\alpha^{(l)}$ for each layer $l\\,\\in\\,[L]\\,:=\\,\\{1,\\dots,L\\}$ , described as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\alpha}^{(0)}(\\mathbf{x})=\\mathbf{x},}\\\\ &{\\boldsymbol{\\alpha}^{(l)}(\\mathbf{x})=\\phi\\left(\\frac{1}{\\sqrt{m_{l-1}}}W_{t}^{(l)}\\boldsymbol{\\alpha}^{(l-1)}(\\mathbf{x})\\right),\\ \\ \\forall l\\in[L]}\\\\ &{\\boldsymbol{f}(\\theta;\\mathbf{x})=\\boldsymbol{\\alpha}^{(L+1)}(\\mathbf{x})=\\frac{1}{\\sqrt{m_{L}}}\\mathbf{v}_{t}^{\\top}\\boldsymbol{\\alpha}^{(L)}(\\mathbf{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{t}^{(l)}\\in\\mathbb{R}^{m_{l}\\times m_{l-1}}$ is the layer-wise weight matrix for layer $l\\in[L]$ and $\\mathbf{v}_{t}\\in\\mathbb{R}^{m_{L}}$ is the last layer vector at iteration $t$ , $\\phi(\\cdot)$ is the smooth (pointwise) activation function, and $m_{0}=\\dim(\\mathbf{x})=d$ We denote the total set of parameters as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t}:=((\\vec{W}_{t}^{(1)})^{\\top},\\dots,(\\vec{W_{t}}^{(L)})^{\\top},\\mathbf{v}_{t}^{\\top})^{\\top}\\in\\mathbb{R}^{p}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For simplicity, we will assume that the width of all the layers is the same, i.e., $m_{l}\\,=\\,m$ for all $l\\in[L]$ , and thus $p=(L-1)m^{2}+m d+m$ . We consider deep models with only one output, i.e., $f(\\theta;\\mathbf{x})\\in\\mathbb{R}$ , but our results can be extended to multi-dimensional outputs. ", "page_idx": 4}, {"type": "text", "text": "As prevalent in literature [31, 29], we make the following assumptions regarding the activation function $\\phi$ , loss function $\\ell$ , and the initialization parameter $\\theta_{0}$ which hold true for the commonly used activation functions, loss functions, and initializations used in practice. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 (Activation function). The activation $\\phi$ is $^{\\,l}$ -Lipschitz, i.e., $|\\phi^{\\prime}|\\leq1$ , and $\\beta_{\\phi}$ -smooth, i.e., $|\\phi_{l}^{\\prime\\prime}|\\le\\beta_{\\phi}$ . ", "page_idx": 4}, {"type": "text", "text": "Assum\u2032ption 3.2 (Loss f\u2032u\u2032nction). The \u2032l\u2032oss $\\ell_{i}=\\ell(y_{i},\\hat{y}_{i})$ with $\\begin{array}{r}{\\ell_{i}^{\\prime}=\\frac{d\\ell_{i}}{d\\hat{y}_{i}},\\ell_{i}^{\\prime\\prime}=\\frac{d^{2}\\ell_{i}}{d\\hat{y}_{i}}}\\end{array}$ is $(i)$ Lipschitz, i.e., , and (ii) and smooth $\\ell_{i}^{\\prime\\prime}\\leq c_{s}$ for some $c_{l},c_{s}>0$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3 (Initialization). We initialize $\\theta_{0}$ with $w_{0,i j}^{(l)}\\sim\\mathcal{N}(0,\\sigma_{0}^{2})\\,f o r\\,l\\in[L]$ where $\\sigma_{0}=$ $\\begin{array}{r}{\\frac{\\sigma_{1}}{2\\left(1+\\frac{\\sqrt{\\log{m}}}{\\sqrt{2m}}\\right)},\\sigma_{1}>0,}\\end{array}$ , and $\\mathbf{v}_{\\mathrm{0}}$ is a random unit vector with $\\|\\mathbf{v}_{0}\\|_{2}=1$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Sketching-Based Distributed Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Random sketching [56, 57] is a compression technique that uses random projections to reduce the dimensionality and helps speed up computations. These random linear mappings can be represented by sketching matrices $\\dot{\\boldsymbol{R}}\\in\\mathbb{R}^{b\\times\\dot{\\boldsymbol{p}}}$ where typically $b\\ll p$ . Examples of sketching matrices include Count-Sketch [35], Subsampled Randomized Hadamard Transforms (SRHT) [58], and sparse Johnson\u2013Lindenstrauss (JL) transforms [59]. In this work, we use sketching matrices to compress local updates before sending them to the server and refer to the operation of recovering true gradient vectors from the sketched updates as \u201cdesketching\u201d. ", "page_idx": 4}, {"type": "text", "text": "We outline the sketching-based distributed learning framework in Algorithm 1. Each client receives a random seed from the server to initialize the local parameters $\\theta_{c,1}$ , and generate a sketching matrix $R$ At each local step $k\\in[1,\\cdots\\,,K]$ , each client performs local gradient descent (GD) over their local dataset $D_{c}$ . At each communication round, the client accumulates the changes over $K$ -local steps, sketches the local updates, and sends the sketched update to the server. The server then aggregates the sketched changes and sends the aggregated sketched updates back to the clients. To update the local parameters, each client needs to recover an unbiased estimate of the true vector from the aggregated sketched update. We call this the desk (desketch) operation (Line 9), for which we use the transpose of the sketching matrix $R$ . Each client then desketches the received aggregated sketched updates by applying desk and updates their local parameters. We refer to the sketching and desketching operations using the sk and desk operators defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\quad\\mathrm{sk}:=R\\in\\mathbb R^{b\\times p}\\quad\\mathrm{(Sketching)}\\;,\\qquad}\\\\ {\\mathrm{desk}:=R^{\\top}\\in\\mathbb R^{p\\times b}\\quad\\mathrm{(Desketching)}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "While we use the same sketching matrix across communication rounds $t=1,\\ldots,T$ , in general, using different matrices for each round does not affect the analysis. ", "page_idx": 4}, {"type": "text", "text": "Choice of sketching matrix: We use a $(1/\\sqrt{b})$ -sub-Gaussian matrix as the choice of sketching matrix. We say $R\\in\\mathbb{R}^{b\\times p}$ is a $(1/{\\sqrt{b}})$ -sub-Gaussian matrix [60] if each ro\u221aw $R_{i}$ is an independent mean-zero, sub-Gaussian isotropic random-vector such that $\\|R_{i}\\|_{\\psi_{2}}\\,\\le\\,1/\\sqrt{b}$ . We assume $\\mathbb{E}[R^{\\top}R]\\,=\\,\\mathbb{I}_{p\\times p}$ . From the above definition, we can see that for $g_{1},g_{2}\\in\\mathbb{R}^{p}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{R(g_{1}+g_{2})=R g_{1}+R g_{2}\\quad}&{\\mathrm{(Linearity)},}\\\\ {\\mathbb{E}_{\\mathbb{N}}[R^{\\top}R g]=g\\quad}&{\\mathrm{(Unbiasedness)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2 Limitations of the Existing Analyses ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When analyzing the convergence rates of the sketching-based distributed learning frameworks, previous works [1, 2] assume that the loss function $\\mathcal{L}$ is $\\beta$ -smooth, i.e, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta^{\\prime})\\leq\\mathcal{L}(\\theta)+\\langle\\nabla\\mathcal{L}(\\theta),\\theta^{\\prime}-\\theta\\rangle+\\frac{\\beta}{2}\\|\\theta^{\\prime}-\\theta\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Sketching-Based Distributed Learning. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Hyperparameters: server learning rate $\\eta_{\\mathrm{global}}$ , local learning rate $\\eta_{\\mathrm{local}}$ .   \nInputs: local datasets $\\mathcal{D}_{c}$ of size $n_{c}$ for clients $c=1,\\ldots,C$ , number of communication rounds $T$ .   \nOutput: final model $\\theta_{T}$ .   \n1: Broadcast a random SEED to the clients.   \n2: for $t=1,\\dots,T$ do   \n3: On Client Nodes:   \n4: for $c=1,\\ldots,C$ do   \n5: if $t=1$ then   \n6: Receive the random SEED from the server. Initialize the local model $\\theta_{c,1}\\,\\in\\,\\ensuremath{\\mathbb{R}}^{p}$ and   \ngenerate the sketching matrix $R\\in\\mathbb{R}^{b\\times p}$ (hence sk, desk) using the random SEED.   \n7: else   \n8: Receive $\\mathrm{sk}(\\bar{\\Delta}_{t-1})$ from the server.   \n9: Desketch and update the model parameters $\\theta_{t}\\leftarrow\\theta_{t-1}+\\mathrm{desk}(\\mathrm{sk}(\\bar{\\Delta}_{t-1}))$ .   \n10: Assign the local model\u2019s parameters $\\theta_{c,t}\\gets\\theta_{t}$ to be updated locally.   \n11: end if   \n12: for $k=1,\\ldots,K$ do   \n13: $\\theta_{c,t}\\leftarrow\\theta_{c,t}-\\eta_{\\mathrm{local}}\\cdot\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t})$   \n14: end for   \n15: $\\Delta_{c,t}\\gets\\theta_{c,t}-\\theta_{t}$   \n16: Send sketched updates $\\mathrm{sk}(\\Delta_{c,t})$ to the server.   \n17: end for   \n18:   \n19: On the Server Node:   \n20: Receive sketched updates $\\mathrm{sk}(\\Delta_{c,t})$ from clients $c=1,\\ldots,C$ .   \n21: Aggregate: $\\begin{array}{r}{\\mathrm{sk}(\\bar{\\Delta}_{t})\\leftarrow\\eta_{\\mathrm{global}}\\cdot\\frac{1}{C}\\sum_{c=1}^{C}\\mathrm{sk}(\\Delta_{c,t})}\\end{array}$   \n22: Broadcast $\\operatorname{sk}(\\bar{\\Delta}_{t})$ to the clients.   \n23: end for ", "page_idx": 5}, {"type": "text", "text": "In the model updates based on sketching followed by desketching, the term $\\|\\theta^{\\prime}\\!-\\!\\theta\\|^{2}$ effectively yields a term of the form $\\|\\boldsymbol{R}^{\\top}\\boldsymbol{R}\\boldsymbol{g}\\|_{2}^{2}$ , where $g\\in\\mathbb{R}^{p}$ stands for a suitable gradient on the full model. While $\\mathbb{E}[R^{\\top}R g]=g$ , i.e., the desk-sk operation is unbiased, we unfortunately have $\\begin{array}{r}{\\|R^{\\top}R g\\|_{2}^{2}=\\Theta(\\frac{p}{b}\\|g\\|_{2}^{2})}\\end{array}$ with high probability (see Lemma B.1 in Appendix ). Thus, such an analysis picks up a dimension dependence $\\Theta(p)$ which can be neutralized only if the sketching dimension is $b=\\Omega(p)$ . However, such a high-dimensional projection will be ill-conceived as we will not get the benefits of the sketching projection. We note that prominent recent work has all hit this dimension dependence. For instance, Song et al. [1] have the dimension dependence in all their results including communication complexity, and Rothchild et al. [2] discuss the dimension dependence in Appendix $\\mathbf{B.3-}$ and get around the dependence by using Top-r components of the gradient vector, with an analysis having to rely on heavy-hitter assumptions. Interestingly, Rothchild et al. [2] noted that the sketching-based distributed deep learning approach seemed to work fine empirically without getting the adverse effect of dimension dependence despite what their theoretical results (based on smoothness) suggest. Our work sheds light on this discrepancy and proves (see Section 4) why the sketching-based distributed learning approach in its simplest form (see Algorithm 1) does not pick up the dimension dependence. ", "page_idx": 5}, {"type": "text", "text": "3.3 Restricted Strong Smoothness (RSS) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we describe the RSS property, an interesting property of deep learning losses that we use to derive dimension-independent convergence guarantees for sketch-DL. Using Taylor\u2019s expansion, the loss at ${\\boldsymbol{\\theta}}={\\boldsymbol{\\theta}}^{\\prime}$ can be written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{\\theta}^{\\prime})=\\mathcal{L}(\\boldsymbol{\\theta}_{t})+\\langle\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}_{t},\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta}_{t})\\rangle+\\frac{1}{2}(\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}_{t})^{\\top}\\nabla_{\\boldsymbol{\\theta}}^{2}\\mathcal{L}(\\widetilde{\\boldsymbol{\\theta}})(\\boldsymbol{\\theta}^{'}-\\boldsymbol{\\theta}_{t}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\theta}=\\epsilon\\theta^{'}+(1-\\epsilon)\\theta_{t},\\epsilon\\in[0,1]$ and $\\nabla_{{\\boldsymbol{\\theta}}}^{2}{\\mathcal{L}}({\\boldsymbol{\\tilde{\\theta}}})$ is the Hessian of the loss. Several prior works [61, 62, 63] have studied the loss Hessian by decomposing the Hessian into the Gauss-Newton matrix $(\\mathbf G)$ and averaged Hessian of the predictors as $(\\mathbf{H})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta):=\\mathbf{G}+\\mathbf{H},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{G}=\\frac{1}{C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{\\prime\\prime}\\nabla f_{i,c}\\nabla f_{i,c}^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{H}={\\frac{1}{C}}\\sum_{c=1}^{C}\\left({\\frac{1}{n_{c}}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{'}\\nabla^{2}f_{i,c}\\right)={\\frac{1}{C}}\\left({\\frac{1}{n_{c}}}\\sum_{i=1}^{n_{c}}H_{i,c}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\nabla f_{i,c}=\\nabla_{\\theta}f(\\theta;\\mathbf{x}_{i,c})$ and $H_{i,c}=\\ell_{i,c}^{'}\\nabla^{2}f_{i,c}=\\ell_{i,c}^{'}\\nabla_{\\theta}^{2}f(\\theta;{\\bf x}_{i,c})$ respectively. Recent works [55, 29, 31] have derive\u221ad sharp upper bounds on the spectral norm of the predictor Hessian: $\\|\\nabla^{2}f\\|=$ $\\Lambda_{m a x}(\\nabla^{2}f)\\,=\\,\\mathcal{O}(1/\\sqrt{m})$ . This leads to the following restrcited smoothness property of deep learning losses: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{\\theta}^{\\prime})\\leq\\mathcal{L}(\\boldsymbol{\\theta}_{t})+\\langle\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}_{t},\\nabla_{\\theta}\\mathcal{L}(\\boldsymbol{\\theta}_{t})\\rangle+\\frac{1}{2C}\\sum_{c=1}^{C}\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{\\prime\\prime}\\langle\\nabla f_{i,c},\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}_{t}\\rangle^{2}+\\frac{c_{0}}{\\sqrt{m}}\\|\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In contrast to the $\\beta$ -smoothness assumption which is common in optimization literature, this new property shows that deep learning losses exhibit strong smoothness in a restricted set of directions. The effect of strong smoothness suitably manifests for $\\theta^{'}$ such that $|\\cos(\\theta^{'}-\\theta_{t},\\nabla f_{i,c})|\\geq\\kappa>0$ for a restricted set of directions. In other directions, the strong smoothness constant is $\\mathcal{O}(1/\\sqrt{m})$ , i.e., a tiny value. For our analysis, we use an even sharper perspective on RSS based on the eigenvalues of predictor\u221a Hessian, $H_{i,c}=\\ell_{i}^{'}\\nabla^{2}f_{i,c}$ . While the smoothness perspective based on the spectral norm of $\\mathcal{O}(1/\\sqrt{m})$ is promising, it implicitly assumes all directions have this level of smoothness which would impact the analysis since there are $p=O(L m^{2})$ directio\u221ans. Based on empirical evidence [64, 65], most directions have smoothness much smaller than $1/\\sqrt{m}$ , and an analysis based on the eigenvalues of the $H_{i,c}$ captures this sharper perspective as opposed to picking up a dependence on the ambient dimension $p$ . ", "page_idx": 6}, {"type": "text", "text": "4 RSS-based analysis for Sketching-based Distributed Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we analyze the convergence of the sketching-based distributed learning approach, summarized in Algorithm 1, using the RSS property of deep learning losses and provide novel dimension-independent convergence results. First, we state our key assumptions in Section 4.1 and explain why they are well-supported by recent work. Then, we provide our novel analysis that eliminates the dimension dependence for the single-local step ( $K=1$ ) case in Section 4.2. Next in Section 4.3, we extend our analysis to the multiple-local step $(K>1)$ ) case. We conclude in Section 4.4 by deriving the communication cost of Algorithm 1 under our novel convergence analysis. ", "page_idx": 6}, {"type": "text", "text": "4.1 Assumptions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Before we introduce the assumptions, we would like to recall a key property. ", "page_idx": 6}, {"type": "text", "text": "Definition 1. $P L$ condition) Consider a loss function $\\mathcal{L}:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}$ and the solution set $\\chi^{*}=\\left\\{\\boldsymbol{\\theta}^{'}:\\right.$ $\\theta^{'}\\in\\operatorname*{argmin}_{\\theta}\\mathcal{L}(\\theta)\\}$ and we use $\\mathcal{L}^{\\ast}$ to denote the corresponding minimum value. Then, $\\mathcal{L}$ is said to satisfy the Polyak-\u0141ojasiewicz $(P L)$ condition with constant $\\mu$ if ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{2\\mu}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta)\\|^{2}\\geq\\mathcal{L}(\\theta)-\\mathcal{L}^{*}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 4.1. PL condition [66] can be used to establish linear convergence of gradient descent while still being weaker than strong convexity. One can show that if a function $f$ is $\\mu$ -strongly convex then it also satisfies PL condition with the same constant $\\mu$ . Recent literature has shown that wide neural networks can be shown to satisfy some variant of PL condition [67, 68]. ", "page_idx": 6}, {"type": "text", "text": "Relying on this recent evidence, we make the following assumption in our analysis. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. Loss function $\\mathcal{L}(\\cdot)$ satisfies the PL condition with constant $\\mu$ . ", "page_idx": 6}, {"type": "text", "text": "We further assume the following upper bound on the sum of eigenvalues of the $H_{i,c}$ as follows: ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. Let $\\Lambda_{1},,\\Lambda_{2},\\cdots\\,,\\Lambda_{p}$ be the eigenvalues of the predictor Hessian $H_{i,c,t}\\;\\;=\\;$ $\\ell_{i,c}^{'}\\nabla^{2}f(\\theta_{t};{\\bf x}_{i,c})$ for $t~\\in~[T]$ and $\\Lambda_{m a x}~=~\\operatorname*{max}_{i}|\\Lambda_{i}|$ then there exists $\\kappa\\ =\\ O(1)$ , such that $\\begin{array}{r}{\\sum_{j=1}^{p}|\\Lambda_{j}|\\leq\\kappa\\Lambda_{m a x}.}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4.2. Several works [69, 70, 71] have shown that the spectrum of loss Hessian follows a bulk and outliers structure where the bulk can be attributed to $\\mathbf{H}$ and follows power law trend [Figure 3(b), Figure 4 in Papyan [72]], motivating our assumption. Further empirical evidence is presented in Appendix $\\mathrm{G}$ (refer to Figure 2), showing that for common networks, $\\kappa$ is much smaller than $p$ . ", "page_idx": 7}, {"type": "text", "text": "4.2 Single-Local Step $(K=1)$ ) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now analyze the simpler setting where clients communicate their local updates after running a single local update step $\\left[K\\,=\\,1\\right]$ ). In this case, there is no local drift, i.e., all clients share the same local parameter vector. As a result, we can show that local update (Line 9 in Algorithm 1) can be written as $\\theta_{t}\\gets\\theta_{t-1}+\\operatorname{desk}\\left(\\mathrm{sk}(\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}))\\right)$ . We follow a similar analysis to Song et al. [1], but we exploit the second order structure of the deep learning losses, which helps avoid picking up dimension dependence due to the $\\mathrm{desk}(\\mathrm{sk}(\\cdot))$ operator. We state the main theorem below with the full proof in Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1. (Informal version of Theorem $D.I$ ) Set $K{=}l$ in Algorithm 1. Under Assumptions 4.1, and 4.2, for suitable constants $\\varepsilon,\\delta\\,<\\,1$ , learning rate $\\eta\\,=\\,\\eta_{g l o b a l}\\,\\cdot\\,\\eta_{l o c a l}$ independent of $p$ , and $\\begin{array}{r}{b=\\Omega\\left(\\frac{1}{\\varepsilon^{2}}\\mathrm{polylog}(\\frac{T p^{2}}{\\delta})\\right)}\\end{array}$ , with probability at least $1-\\delta$ , we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{T})-\\mathcal{L}(\\theta^{*})\\leq\\left(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*})\\right)e^{-\\mu(1-\\varepsilon)\\eta T},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\theta^{*}$ is a minimizer of $(I)$ . ", "page_idx": 7}, {"type": "text", "text": "In Theorem 1 (which also implies linear convergence), the sketching dimension, $b$ , depends on $p$ only polylogarithmically, which explains the competitive performance of the sketching-based distributed learning frameworks with uncompressed GD without requiring any additional costly steps such as Top-r sparsification (as done by Rothchild et al. [2]), which also introduces a bias that needs to be suitably corrected. ", "page_idx": 7}, {"type": "text", "text": "4.3 Multiple-Local Step $(K>1)$ ) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now, we provide the convergence result for Algorithm 1, for the more general case of multiple local steps $(K>1)$ ) at each round. Unlike the single-local step, here, the local clients\u2019 parameters drift during $K$ local steps and thus, we need to have additional assumptions to guarantee convergence. For the purpose of our analysis, we assume that the gradient norms are bounded, i.e., $\\|\\nabla_{\\theta}\\ell(\\theta)\\|\\leq G$ We refer the reader to Lemma C.2 in the Appendix for a more formal statement. We now present an informal version of our theorem for the convergence of the multiple-local step case: ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. (Informal version of Theorem $D.2$ ) Let $\\|\\nabla_{\\theta}\\ell(\\theta)\\|\\,\\leq\\,G$ . Under Assumptions 4.1 and 4.2, for a suitable constants $\\varrho,c_{H}$ and $\\varepsilon<1$ , learning rate \u03b7 = \u03b7global\u03b7local 2\u00b5K(1\u2212\u03b5), and $\\begin{array}{r}{b=\\Omega\\left(\\frac{1}{\\varepsilon^{2}}\\mathrm{polylog}(\\frac{T N p^{2}}{\\delta})\\right)}\\end{array}$ , with probability at least $1-\\delta$ , we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{T})-\\mathcal{L}(\\theta^{*})\\leq(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))e^{-2(1-\\varepsilon)\\mu\\eta K T}+\\frac{\\eta C_{2}(\\varepsilon,m,\\kappa)K G^{2}}{2\\mu},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\theta^{*}$ is a minimizer of the problem $^{\\,I}$ , $N$ is the number of training samples, and ", "page_idx": 7}, {"type": "equation", "text": "$$\nC_{2}(\\varepsilon,m,\\kappa)=\\mathcal{O}\\left(\\varrho^{2}+\\frac{c_{H}}{\\sqrt{m}}\\right)+\\mathcal{O}\\left(\\frac{\\varepsilon\\kappa c_{H}}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We refer the reader to Appendix D.2 for the full proof. ", "page_idx": 7}, {"type": "text", "text": "4.4 Communication Efficiency ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As a direct consequence of our analysis, we now provide an improved communication complexity for Algorithm 1 compared to prior works [1] that build their analysis solely based on the smoothness assumption without taking advantage of the RSS property of the deep learning losses. With this, we manage to eliminate the linear dependence of the communication complexity on the ambient dimension $p$ \u2013 providing a substantial improvement over the analysis of Song et al. [1]. Unlike Rothchild et al. [2], we break this dependence without requiring a Top- $^{,\\pm}$ sparsification step or a heavy-hitter assumption. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In the theorem below, we state the required number of communication bits to achieve an $\\epsilon$ -optimal solution based on our sharper convergence analysis in the previous sections. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. (Informal version of Theorem E.1) Under Assumptions 4.1 and 4.2, Algorithm 1 obtains an optimal solution satisfying the error ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\theta}_{T})-\\mathcal{L}(\\boldsymbol{\\theta}^{*})\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We provide the full proof in the Appendix. We state the result by Song et al. [1] for comparison: ", "page_idx": 8}, {"type": "equation", "text": "$$\nO\\left(\\frac{\\beta C}{\\mu}\\operatorname*{max}\\left\\{p,\\sqrt{\\frac{G^{2}}{\\mu\\epsilon}}\\right\\}\\log\\left(\\frac{\\beta\\mathbb{E}[\\|\\theta_{0}-\\theta^{*}\\|_{2}^{2}]}{\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Song et al. [1] assumes that the loss function $\\mathcal{L}$ is $\\beta$ -smooth and the expectation is over the randomness of sketching. Note that our bound has only polylogarithmic dependence on dimension $p$ . This shows the efficacy of our approach in improving the overall communication complexity over prior works. ", "page_idx": 8}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide a comparison of the sketching approach in Algorithm 1 with other common approaches such as local Top- $^{\\cdot\\Sigma}$ [8] and FetchSGD [2]. We note that both Local Top- $^{\\cdot\\Sigma}$ (as outlined in Algorithm 2 in Appendix F) and FetchSGD are biased algorithms and they are typically used with error feedback mechanisms to correct the bias. As the sub-Gaussian sketching matrix in Algorithm 1, we use Count-Sketch. This means the only difference between FetchSGD without error feedback and Count-Sketch is the extra global Top-r step at the server. We conducted our experiments on NVIDIA Titan X GPUs on an internal cluster server, using 1 GPU per one run. ", "page_idx": 8}, {"type": "text", "text": "We train ResNet-18 [73] on CIFAR-10 dataset [74] that is i.i.d. distributed to 100 clients. Each client performs 5 local gradient descent iterations (i.e., using full-batch of size 500) at every round. Figure 1 shows that Count-Sketch-based distributed learning approach in Algorithm 1 performs competitively with FetchSGD. This result highlights the potential of sketching alone, without additional modifications as in FetchSGD, to maintain competitive accuracy. Additionally, the error-feedback free approach enables compatibility with Differential Privacy(DP) techniques which we leave as future work. ", "page_idx": 8}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/affcd7da25df45a7fbaf28b6d7f0897bdf290c7ec865caf478b1f02c0e9209e3.jpg", "img_caption": ["Figure 1: Communication Efficiency. Count-Sketch algorithm in Algorithm 1 against local Top- $^{\\cdot\\Sigma}$ [8] and FetchSGD [2], with and without error feedback. 100 clients run 5 local iterations with full-batch at every round. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We note that we do not claim novelty of any of the methods discussed in this section as they have been introduced [8, 40], improved [2], and analyzed [1] extensively in prior work. We present these empirical comparisons for completeness and to provide support for why a tight analysis of sketching is important, given the mismatch between its empirical success and the dimension dependence in existing convergence analysis as highlighted in Appendix B.3 of Rothchild et al. [2]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provide a significantly improved convergence analysis for sketching-based distributed learning frameworks by exploiting the properties of the deep learning losses, such as restricted strong smoothness. This allows us to break the dimension dependence in the convergence error, and consequently, the communication cost\u2013a milestone prior work could not achieve due to relying only on the smoothness assumption of the losses, i.e., ignoring the more special properties of \u201cdeep models.\u201d By breaking this dimension dependency in the convergence analysis and communication cost, we hope to motivate the use of sketching for larger models. One of the many exciting future extensions is to revisit the privacy analysis of private sketching mechanisms using our findings. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We provide a sharper analysis for sketching in distributed learning. In future work, we plan to extend our analysis to federated learning by allowing client dropout. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. This paper provides a tighter analysis for sketching-based distributed learning and federated learning frameworks. We expect this work to be helpful for the community as it contributes to the efforts in making machine learning models more decentralized, accessible, and trustworthy. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers of NeurIPS for their valuable feedback and suggestions. BI was supported in part by a Google PhD Fellowship. SK acknowledges support by NSF III 2046795, IIS 1909577, CCF 1934986, NIH 1R01MH116226-01A, NIFA award 2020-67021- 32799, the Alfred P. Sloan Foundation, and Google Inc. This work was also supported in part by the National Science Foundation (NSF) through awards IIS 21-31335, OAC 21-30835, DBI 20-21898, as well as a C3.ai research award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang. Sketching for first order method: Efficient algorithm for low-bandwidth channel and vulnerability. In International Conference on Machine Learning, pages 32365\u201332417. PMLR, 2023.   \n[2] Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph Gonzalez, and Raman Arora. Fetchsgd: Communication-efficient federated learning with sketching. In International Conference on Machine Learning, pages 8253\u20138265. PMLR, 2020.   \n[3] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\u2019aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in neural information processing systems, 25, 2012.   \n[4] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of deep neural networks with natural gradient and parameter averaging. In ICLR (Workshop), 2015.   \n[5] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on operating systems design and implementation (OSDI 14), pages 583\u2013598, 2014.   \n[6] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021. [7] Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed machine learning with the parameter server. Advances in Neural Information Processing Systems, 27, 2014.   \n[8] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on Learning Representations, 2018. [9] Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. Atomo: Communication-efficient learning via atomic sparsification. Advances in neural information processing systems, 31, 2018.   \n[10] Leighton Pate Barnes, Huseyin A Inan, Berivan Isik, and Ayfer \u00d6zg\u00fcr. rtop-k: A statistical estimation approach to distributed sgd. IEEE Journal on Selected Areas in Information Theory, 1(3):897\u2013907, 2020.   \n[11] Berivan Isik, Tsachy Weissman, and Albert No. An information-theoretic justification for model pruning. In International Conference on Artificial Intelligence and Statistics, pages 3821\u20133846. PMLR, 2022.   \n[12] Berivan Isik, Francesco Pase, Deniz Gunduz, Tsachy Weissman, and Zorzi Michele. Sparse random networks for communication-efficient federated learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=k1FHgri5y3-.   \n[13] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. Advances in neural information processing systems, 30, 2017.   \n[14] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. Advances in neural information processing systems, 30, 2017.   \n[15] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pages 560\u2013569. PMLR, 2018.   \n[16] Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient compression for distributed optimization. Advances in Neural Information Processing Systems, 32, 2019.   \n[17] Shay Vargaftik, Ran Ben-Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak, and Michael Mitzenmacher. Drive: One-bit distributed mean estimation. Advances in Neural Information Processing Systems, 34:362\u2013377, 2021.   \n[18] Shay Vargaftik, Ran Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben Itzhak, and Michael Mitzenmacher. Eden: Communication-efficient and robust distributed mean estimation for federated learning. In International Conference on Machine Learning, pages 21984\u201322014. PMLR, 2022.   \n[19] Ran Ben Basat, Shay Vargaftik, Amit Portnoy, Gil Einziger, Yaniv Ben-Itzhak, and Michael Mitzenmacher. QUICK-FL: Quick unbiased compression for federated learning. arXiv preprint arXiv:2205.13341, 2022.   \n[20] H Brendan McMahan, FX Yu, P Richtarik, AT Suresh, and D Bacon. Federated learning: Strategies for improving communication efficiency. In Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS), Barcelona, Spain, pages 5\u201310, 2016.   \n[21] Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Ion Stoica, Raman Arora, et al. Communicationefficient distributed sgd with sketching. Advances in Neural Information Processing Systems, 32, 2019.   \n[22] Farzin Haddadpour, Belhal Karimi, Ping Li, and Xiaoyun Li. Fedsketch: Communicationefficient and private federated learning via sketching. arXiv preprint arXiv:2008.04975, 2020.   \n[23] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. Advances in Neural Information Processing Systems, 31, 2018.   \n[24] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n[25] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482, 2016.   \n[26] James Henry Bell, Kallista A Bonawitz, Adri\u00e0 Gasc\u00f3n, Tancr\u00e8de Lepoint, and Mariana Raykova. Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 1253\u20131269, 2020.   \n[27] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n[28] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[29] Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Misha Belkin. Restricted strong convexity of deep learning models with smooth activations. In The Eleventh International Conference on Learning Representations, 2023.   \n[30] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[31] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675\u20131685. PMLR, 2019.   \n[32] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep Ravikumar. A unified framework for high-dimensional analysis of $m$ -estimators with decomposable regularizers. Advances in neural information processing systems, 22, 2009.   \n[33] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1): 1665\u20131697, 2012.   \n[34] Sheng Chen and Arindam Banerjee. Structured estimation with atomic norms: General bounds and applications. Advances in Neural Information Processing Systems, 28, 2015.   \n[35] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pages 693\u2013703. Springer, 2002.   \n[36] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnsonlindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 557\u2013563, 2006.   \n[37] Alham Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In EMNLP 2017: Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics (ACL), 2017.   \n[38] Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International Conference on Machine Learning, pages 3329\u20133337. PMLR, 2017.   \n[39] Amirkeivan Mohtashami, Martin Jaggi, and Sebastian Stich. Masked training of neural networks with partial gradients. In International Conference on Artificial Intelligence and Statistics, pages 5876\u20135890. PMLR, 2022.   \n[40] Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui. Sketchml: Accelerating distributed machine learning with data sketches. In Proceedings of the 2018 International Conference on Management of Data, pages 1269\u20131284, 2018.   \n[41] Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryf:l Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets. arXiv preprint arXiv:2008.03371, 2020.   \n[42] Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, pages 42\u201355, 2021.   \n[43] Yang Liu, Yi Zhao, Guangmeng Zhou, and Ke Xu. Fedprune: Personalized and communicationefficient federated learning on non-iid data. In International Conference on Neural Information Processing, pages 430\u2013437. Springer, 2021.   \n[44] Berivan Isik, Francesco Pase, Deniz Gunduz, Sanmi Koyejo, Tsachy Weissman, and Michele Zorzi. Adaptive compression in federated learning via side information. In International Conference on Artificial Intelligence and Statistics, pages 487\u2013495. PMLR, 2024.   \n[45] Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min sketch and its applications. Journal of Algorithms, 55(1):58\u201375, 2005.   \n[46] Michael Greenwald and Sanjeev Khanna. Space-efficient online computation of quantile summaries. ACM SIGMOD Record, 30(2):58\u201366, 2001.   \n[47] Daniel M Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms. Journal of the ACM (JACM), 61(1):1\u201323, 2014.   \n[48] Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. Practical sketching algorithms for low-rank matrix approximation. SIAM Journal on Matrix Analysis and Applications, 38(4):1454\u20131485, 2017.   \n[49] Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Graph sketches: sparsification, spanners, and subgraphs. In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems, pages 5\u201314, 2012.   \n[50] Edgar Dobriban and Sifan Liu. Asymptotics for sketching in least squares regression. Advances in Neural Information Processing Systems, 32, 2019.   \n[51] Wei-Ning Chen, Christopher A Choquette Choo, Peter Kairouz, and Ananda Theertha Suresh. The fundamental price of secure aggregation in differentially private federated learning. In International Conference on Machine Learning, pages 3056\u20133089. PMLR, 2022.   \n[52] Enayat Ullah, Christopher A Choquette-Choo, Peter Kairouz, and Sewoong Oh. Private federated learning with autotuned compression. In International Conference on Machine Learning, pages 34668\u201334708. PMLR, 2023.   \n[53] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.   \n[54] Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with linear widths, 2021.   \n[55] Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. In Advances in Neural Information Processing Systems, 2020.   \n[56] David P. Woodruff. Computational advertising: Techniques for targeting relevant ads. Foundations and Trends\u00ae in Theoretical Computer Science, 10(1\u20132):1\u2013157, 2014. ISSN 1551-3068. doi: 10.1561/0400000060. URL http://dx.doi.org/10.1561/0400000060.   \n[57] Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends\u00ae in Machine Learning, 3(2):123\u2013224, 2011. ISSN 1935-8237. doi: 10.1561/2200000035. URL http://dx.doi.org/10.1561/2200000035.   \n[58] Christos Boutsidis and Alex Gittens. Improved matrix algorithms via the subsampled randomized hadamard transform. SIAM Journal on Matrix Analysis and Applications, 34(3):1301\u20131340, 2013. doi: 10.1137/120874540. URL https://doi.org/10.1137/120874540.   \n[59] Anirban Dasgupta, Ravi Kumar, and Tam\u00e1s Sarlos. A sparse johnson: Lindenstrauss transform. In Proceedings of the Forty-Second ACM Symposium on Theory of Computing, STOC \u201910, page 341\u2013350, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450300506. doi: 10.1145/1806689.1806737. URL https://doi.org/10.1145/ 1806689.1806737.   \n[60] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, page 210\u2013268. Cambridge University Press, 2012.   \n[61] Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint arXiv:1802.08770, 2018.   \n[62] Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks, 2018.   \n[63] Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016.   \n[64] Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians, 2019.   \n[65] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace, 2018.   \n[66] B.T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864\u2013878, 1963. ISSN 0041-5553. doi: https://doi. org/10.1016/0041-5553(63)90382-3. URL https://www.sciencedirect.com/science/ article/pii/0041555363903823.   \n[67] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0141ojasiewicz condition, 2020.   \n[68] Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33:15954\u201315964, 2020.   \n[69] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density. In International Conference on Machine Learning, pages 2232\u20132241. PMLR, 2019.   \n[70] Michael W Mahoney et al. Randomized algorithms for matrices and data. Foundations and Trends\u00ae in Machine Learning, 3(2):123\u2013224, 2011.   \n[71] Zeke Xie, Qian-Yuan Tang, Yunfeng Cai, Mingming Sun, and Ping Li. On the power-law hessian spectrums in deep learning, 2022.   \n[72] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size, 2019.   \n[73] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[74] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.(2009), 2009.   \n[75] Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation with norm regularization. Advances in neural information processing systems, 27, 2014.   \n[76] Xinwei Zhang, Zhiqi Bu, Zhiwei Steven Wu, and Mingyi Hong. Differentially private sgd without clipping bias: An error-feedback approach, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Sketching Guarantee ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma A.1. For a randomised matrix $R\\in\\mathbb{R}^{b\\times p}$ with i.i.d. subgaussian rows $R_{i}$ , i.e. $\\|R_{i}\\|_{\\psi_{2}}\\leq\\kappa$ and $\\mathbb{E}[R_{i}R_{i}^{T}]=\\mathbb{I}^{p\\times p}$ and for $U,V\\subset\\mathbb{R}^{p}$ , such that $\\begin{array}{r}{b=\\Omega(\\frac{\\log^{3}(p|U||V|/\\delta)}{\\varepsilon^{2}})}\\end{array}$ with probability atleast $1-\\delta$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{u\\in U,v\\in V}\\left|\\langle R u,R v\\rangle-\\langle u,v\\rangle\\right|\\leq\\varepsilon\\|u\\|\\|v\\|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof, We use the following lemma from [1]: ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1 (Lemma D.24 from [1]). Let $R\\in\\mathbb{R}^{b\\times p}$ denote a random Gaussian matrix . Then for any fixed vector $h\\in\\mathbb{R}^{p}$ and any fixed vector $g\\in\\mathbb{R}^{p}$ , the following properties hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{R\\sim\\Pi}\\left[|(g^{\\top}R^{\\top}R h)-(g^{\\top}h)|>\\frac{\\log^{1.5}(p/\\delta)}{\\sqrt{b}}\\|g\\|_{2}\\|h\\|_{2}\\right]\\le\\Theta(\\delta).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Following a similar proof in [1], we can get a sub-Gaussian version of this lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. Let $R\\in\\mathbb{R}^{b\\times p}$ be a random sub-Gaussian matrix, with $\\psi_{2}$ norm of each entry bounded by $\\textstyle{\\frac{1}{\\sqrt{b}}}$ . Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nP r\\left[\\operatorname*{max}_{i\\neq j}|\\langle R_{*,i},R_{*,j}\\rangle|\\geq\\frac{c\\sqrt{\\log\\left(n/\\delta\\right)}}{\\sqrt{b}}\\right]\\leq\\Theta\\left(\\delta\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Note for $i\\neq j,R_{*,i},R_{*,j}$ are two independent sub-Gaussian vectors. Let $z_{k}=R_{k,i}R_{k,j}$ and $z\\,=\\,\\langle R_{*,i},R_{*,j}\\rangle$ . Then according to the definition of sub-Gaussian random variables, $z_{k}~\\in$ $\\begin{array}{r}{\\mathrm{SE}\\left(\\frac{c_{1}^{2}}{b^{2}},\\frac{c_{1}}{b}\\right)}\\end{array}$ is a sub-exponential random variable with an absolute constant $c_{1}$ . Thus, we have $\\begin{array}{r}{z=\\overset{\\cdot}{\\sum_{k=1}^{b}z_{k}}\\in\\mathrm{SE}\\left(\\frac{c_{1}^{2}}{b},\\frac{c_{1}}{b}\\right)}\\end{array}$ , by sub-exponential concentration Lemma B.7 in [1] we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\vert z\\vert\\ge t\\right]\\le2\\exp(-c_{2}b\\operatorname*{min}\\left\\{t^{2},t\\right\\})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Picking $t=c_{3}\\sqrt{\\log\\left(p^{2}/\\delta\\right)/b}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[|\\langle R_{*,i},R_{*,j}\\rangle|\\geq\\frac{c\\sqrt{\\log{(p/\\delta)}}}{\\sqrt{b}}\\right]\\leq\\delta/p^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking the union bound over all $(i,j)\\in[p]\\times[p]$ and $i\\neq j$ , we complete the proof. ", "page_idx": 15}, {"type": "text", "text": "Then following the same proof with the only difference to apply Lemma A.2 instead of Lemma D.18 in [1], we can get the following sub-Gaussian version of Lemma D.24 in [1]. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. Let $R\\in\\mathbb{R}^{b\\times p}$ denote a random sub-Gaussian matrix, with $\\psi_{2}$ norm of each entry bounded by $\\frac{1}{b}$ . Then for any fixed vector $h\\in\\mathbb{R}^{p}$ and any fixed vector $g\\in\\mathbb{R}^{p}$ , the following properties hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{R\\sim\\Pi}\\left[|(g^{\\top}R^{\\top}R h)-(g^{\\top}h)|>\\frac{c\\log^{1.5}(p/\\delta)}{\\sqrt{b}}\\|g\\|_{2}\\|h\\|_{2}\\right]\\le\\Theta(\\delta).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on Lemma A.3, we have: ", "page_idx": 15}, {"type": "text", "text": "Theorem A.1. Let $R\\in\\mathbb{R}^{b\\times p}$ denote a random sub-Gaussian matrix, with $\\psi_{2}$ norm of each entry bounded by $\\frac{1}{b}$ . Then for any $G,H\\subset\\mathbb{R}$ and $\\begin{array}{r}{b=\\frac{C\\log^{3}(p|G||H|/\\delta^{'})}{\\epsilon^{2}}}\\end{array}$ we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{R\\sim\\Pi}\\left[\\operatorname*{sup}_{g\\in G,h\\in H}|(g^{\\top}R^{\\top}R h)-(g^{\\top}h)|>\\epsilon\\|g\\|_{2}\\|h\\|_{2}\\right]\\le\\Theta(\\delta)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Taking union bound over $g\\in G,h\\in H$ in the bound of Lemma A.3, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\delta|G||H|\\leq\\delta^{'},\\frac{c\\log^{1.5}(p/\\delta)}{\\sqrt{b}}\\leq\\epsilon}\\\\ &{}&{\\implies b\\geq\\frac{C\\log^{3}(p|G||H|/\\delta^{'})}{\\epsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Dimension dependence ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem B.1. [Theorem 5 in Banerjee et al. [75]] Let $X$ be a design matrix with independent isotropic sub-Gaussian rows, i.e., $\\begin{array}{r}{\\|\\boldsymbol{X}_{i}\\|_{\\psi_{2}}\\leq\\kappa}\\end{array}$ and $\\bar{E}[X_{i}X_{i}^{T}]=\\mathbb{I}_{p\\times p}$ . Then, for absolute constants $\\eta,c>0$ , with probability at least $(1-2\\exp(-\\eta w^{2}(A)))$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{u\\in A}\\left|\\frac{1}{n}\\|X u\\|^{2}-1\\right|\\ =\\ \\operatorname*{sup}_{u\\in A}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i},u\\rangle^{2}-1\\right|\\ \\leq\\ c\\frac{w(A)}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. Fix a vector $g\\,\\in\\,\\mathbb{R}^{p}$ . For a sketching matrix $R$ defined earlier, we have for some constants $\\eta_{1},\\eta_{2}$ with probability at least $1-2\\exp(-\\eta_{1}\\epsilon^{2}b)-2\\exp(-\\eta_{2}\\epsilon^{2}p).$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{R}^{\\top}\\boldsymbol{R}\\boldsymbol{g}\\|_{2}^{2}\\geq(1-\\epsilon)^{2}\\frac{p}{b}\\|\\boldsymbol{g}\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We consider the set of all gradients $\\mathcal{G}=\\{g_{i}|i=1,2,\\cdot\\cdot\\cdot T\\}$ . ", "page_idx": 16}, {"type": "text", "text": "We consider a finite set A = {ui : ui = \u2225ggii\u22252 $\\begin{array}{r}{\\mathcal{A}\\;=\\;\\{u_{i}\\;:\\;u_{i}\\;=\\;\\frac{g_{i}}{\\|g_{i}\\|_{2}}\\forall i\\;=\\;1,2,\\cdots T\\}}\\end{array}$ , Gaussian width $w(A)\\;=\\;$ $O({\\sqrt{\\log|{\\mathcal{G}}|}})$ . ", "page_idx": 16}, {"type": "text", "text": "Using Theorem 5 [75], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{g\\in G}\\big|\\|R g\\|_{2}^{2}-\\|g\\|^{2}\\big|\\geq c_{1}\\frac{w(A)}{\\sqrt{b}}\\|g\\|^{2}\\right]\\leq2\\exp(-\\eta w^{2}(A))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we can define $\\mathcal{B}\\ =\\ \\{h_{i}\\ :\\ h_{i}\\ =\\ R g_{i}\\forall i\\ =\\ 1,2,\\cdot\\cdot\\cdot T\\}$ , Gaussian width $w(B)\\;=\\;$ $O(\\|R\\|\\sqrt{\\log{|\\mathcal{G}|}})$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{g\\in G}\\left|\\frac{b}{p}\\|R^{\\top}R g\\|_{2}^{2}-\\|R g\\|^{2}\\right|\\geq c_{2}\\frac{w(\\beta)}{\\sqrt{p}}\\|R g\\|^{2}\\right]\\leq2\\exp(-\\eta w^{2}(\\beta))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking $\\begin{array}{r}{\\epsilon=\\operatorname*{max}\\left\\{c_{1}\\frac{w(A)}{\\sqrt{b}},c_{2}\\frac{w(B)}{\\sqrt{p}}\\right\\}}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{g\\in G}\\left|\\|R g\\|_{2}^{2}-\\|g\\|_{2}^{2}\\right|\\geq\\epsilon\\|g\\|_{2}^{2}\\right]\\leq2\\exp(-\\eta_{1}\\epsilon^{2}b))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{g\\in G}\\left|\\frac{b}{p}\\|R^{\\top}R g\\|_{2}^{2}-\\|R g\\|^{2}\\right|\\geq\\epsilon\\|R g\\|^{2}\\right]\\leq2\\exp(-\\eta\\epsilon^{2}p)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Succintly, we can write $1-2\\exp(-\\eta_{1}\\epsilon^{2}b)-2\\exp(-\\eta_{2}\\epsilon^{2}p)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{R}^{\\top}\\boldsymbol{R}\\boldsymbol{g}\\|_{2}^{2}\\geq(1-\\epsilon)^{2}\\|\\boldsymbol{g}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C Background ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We state some useful results from [29]: ", "page_idx": 16}, {"type": "text", "text": "Lemma C.1 (Hessian Spectral Norm Bound, Theorem 4.1 and Lemma 4.1 in Banerjee et al. [29] ). Under Assumptions 3.2 and 3.3, for any $\\mathbf{x}\\in\\mathcal{X}$ , $\\theta\\in B_{\\rho,\\rho_{1}}^{\\mathrm{Spec}}(\\theta_{0})$ , with probability at least $\\textstyle{\\left(1-{\\frac{2(L+1)}{m}}\\right)}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}^{2}f(\\theta;\\mathbf{x})\\|_{2}\\leq\\frac{c_{H}}{\\sqrt{m}}\\quad a n d\\quad\\|\\nabla_{\\theta}f(\\theta;\\mathbf{x})\\|_{2}\\leq\\varrho\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{H}=L(L^{2}\\gamma^{2L}+L\\gamma^{L}+1)\\cdot(1+\\rho_{1})\\cdot\\psi_{H}\\cdot\\underset{l\\in[L]}{\\operatorname*{max}}\\gamma^{L-l}+L\\gamma^{L}\\underset{l\\in[L]}{\\operatorname*{max}}h(l)\\ ,}\\\\ &{\\qquad\\qquad\\quad\\gamma=\\sigma_{1}+\\frac{\\rho}{\\sqrt{m}},\\quad h(l)=\\gamma^{l-1}+|\\phi(0)|\\underset{i=1}{\\overset{l-1}{\\sum}}\\gamma^{i-1},}\\\\ &{\\psi_{H}=\\underset{1\\leq l_{1}<l_{2}\\leq L}{\\operatorname*{max}}\\left\\lbrace\\beta_{\\phi}h(l_{1})^{2}\\cdot\\,h(l_{1})\\left(\\frac{\\beta_{\\phi}}{2}(\\gamma^{2}+h(l_{2})^{2})+1\\right)\\;,\\;\\beta_{\\phi}\\gamma^{2}h(l_{1})h(l_{2})\\right\\rbrace\\;,}\\\\ &{\\qquad\\qquad\\quad\\varrho^{2}=(h(L+1))^{2}+\\frac{1}{m}(1+\\rho_{1})^{2}\\underset{l=1}{\\overset{L+1}{\\sum}}(h(l))^{2}\\gamma^{2(L-l)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.2 (Loss bounds, Lemma 4.2 of Banerjee et al. [29]). Under Assumptions 3.2 and 3.3, for \u03b3 = \u03c31 +\u221a\u03c1m, each of the following inequalities hold with probability at least 1 \u22122(Lm+1) : $\\ell(\\theta_{0})\\leq c_{0,\\sigma_{1}}$ and $\\ell(\\theta)\\leq c_{\\rho_{1},\\gamma}$ for $\\theta\\in B_{\\rho,\\rho_{1}}^{\\mathrm{Frob}}(\\theta_{0})$ , where $\\begin{array}{r}{c_{a,b}=2\\sum_{i=1}^{N}y_{i}^{2}+2(1+\\dot{a})^{2}|g(b)|^{2}}\\end{array}$ and $\\begin{array}{r}{g(a)=a^{L}+|\\phi(0)|\\sum_{i=1}^{L}a^{i}}\\end{array}$ for any $a,b\\in\\mathbb{R}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma C.3 (Loss gradient bound, Corollary 4.1 in Banerjee et al. [29]). Under Assumptions 3.2 and 3.3, for $\\theta~\\in~B_{\\rho,\\rho_{1}}^{\\mathrm{Frob}}(\\theta_{0})$ , with probability at least $\\textstyle\\left(1-{\\frac{2(L+1)}{m}}\\right)$ , we have $\\|\\nabla_{\\theta}\\ell(\\theta)\\|_{2}\\;\\leq$ $2\\sqrt{\\ell(\\theta)}\\varrho\\leq2\\sqrt{c_{\\rho_{1},\\gamma}}\\varrho,$ , with $\\varrho$ as in Lemma C.1 and $c_{\\rho_{1},\\gamma}$ as in Lemma C.2. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.4 (Local Smoothness, Theorem 5.2 in Banerjee et al. [29]). Under Assumptions 3.2 and 3.3, with probability at least $\\begin{array}{r}{(1-\\frac{2(L+1)}{m}),\\forall\\theta,\\theta^{\\prime}\\in B_{\\rho,\\rho_{1}}^{\\mathrm{Frob}}(\\theta_{0}),}\\end{array}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(\\theta^{\\prime})\\le\\ell(\\theta)+\\langle\\theta^{\\prime}-\\theta,\\nabla_{\\theta}\\hat{\\mathcal{L}}(\\theta)\\rangle+\\frac{\\beta}{2}\\|\\theta^{\\prime}-\\theta\\|_{2}^{2}\\,,\\quad w i t h\\quad\\beta=c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Analysis of Convergence ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\mathcal{L}}(\\theta)=\\frac{1}{C}\\sum_{c=1}^{C}\\mathcal{L}_{c}(\\theta)=\\frac{1}{C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell(y_{i,c},f(\\theta;\\mathbf{x}_{i,c}))\\right)}}\\\\ {{\\displaystyle{\\nabla_{\\theta}\\mathcal{L}}(\\theta)=\\frac{1}{C}\\sum_{c=1}^{C}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta)=\\frac{1}{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{'}\\nabla_{\\theta}f(\\theta;\\mathbf{x}_{i,c})\\right)}}\\\\ {{\\displaystyle{\\nabla_{\\theta}^{2}\\mathcal{L}}(\\theta)=\\frac{1}{C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\left(\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{'}\\nabla_{\\theta}f(\\theta;\\mathbf{x}_{i,c})\\nabla_{\\theta}f(\\theta;\\mathbf{x}_{i,c})^{\\top}+\\sum_{i=1}^{n_{c}}\\underbrace{\\ell_{i,c}^{'}\\nabla_{\\theta}^{2}f(\\theta;\\mathbf{x}_{i,c})}_{H_{i,c}(\\theta)}\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\ell_{i}^{'},\\ell_{i}^{'\\prime}$ are the first and second order derivatives of the point-wise losses. For an iterate $\\theta_{t-1}$ , by the second-order Taylor expansion of the empirical loss around $\\theta_{t-1}$ we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{t})=\\mathcal{L}(\\theta_{t-1})+\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\theta_{t}-\\theta_{t-1}\\rangle+\\frac{1}{2}(\\theta_{t}-\\theta_{t-1})^{\\top}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta_{t-1})(\\theta_{t}-\\theta_{t-1})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using 32, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}(\\theta_{t})=\\mathcal{L}(\\theta_{t-1})+\\underbrace{\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\theta_{t}-\\theta_{t-1}\\rangle}_{T_{1}}+\\underbrace{\\frac{1}{2C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{\\prime\\prime}\\langle\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{i,c}),\\theta_{t}-\\theta_{t-1}\\rangle^{2}\\right)}_{T_{2}}}\\\\ {\\displaystyle\\qquad+\\underbrace{\\frac{1}{2C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}(\\theta_{t}-\\theta_{t-1})^{T}H_{i,c}(\\theta_{t}-\\theta_{t-1})\\right)}_{T_{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where, $H_{i,c}=H_{i,c}(\\theta_{t-1})$ for brevity. ", "page_idx": 17}, {"type": "text", "text": "D.1 Single-step scheme $(\\mathbf{K}{=}\\mathbf{1})$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem D.1. Under assumptions 4.1 and 4.2, set $K=1$ in Algorithm 1. For a suitable constant $\\varepsilon<$ $\\eta\\,=\\,\\eta_{\\mathrm{global}}\\eta_{\\mathrm{local}}\\,=\\,\\frac{2(1-\\varepsilon)}{((1+\\varepsilon)^{2}c_{s}\\varrho^{2}+{^c_{H}c_{l}}/\\sqrt{m}(1+\\kappa(2\\varepsilon+\\varepsilon^{2})))}$ and $\\begin{array}{r}{b\\,=\\,\\Omega\\left(\\frac{1}{\\varepsilon^{2}}\\log^{3}(p^{2}N T/\\delta)\\right)}\\end{array}$ with probability at least $1-\\delta$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{T})-\\mathcal{L}(\\theta^{*})\\leq\\left(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*})\\right)e^{-\\mu(1-\\varepsilon)\\eta T}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\theta^{*}$ is the minimizer of the problem 1. ", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{t-1}=-\\eta_{\\mathrm{global}}\\operatorname*{desk}\\left(\\frac{1}{C}\\sum_{c=1}^{C}\\mathbf{s}\\mathbf{k}\\left(\\eta_{\\mathrm{local}}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{t-1})\\right)\\right)=-\\eta\\operatorname*{desk}\\mathbf{s}\\mathbf{k}\\,\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\eta=\\eta_{\\mathrm{global}}\\eta_{\\mathrm{local}}$ for notation simplicity. ", "page_idx": 18}, {"type": "text", "text": "D.1.1 Bounding $T_{1}$ ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{t}-\\theta_{t-1},\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\rangle=-\\eta\\langle\\mathrm{desk}\\,\\mathrm{sk}\\,\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\eta\\langle R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{a}{\\le}-\\eta\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}+\\eta\\varepsilon_{1}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where, $a$ follows from A.1 for some for suitable choice of $\\varepsilon_{1}$ ", "page_idx": 18}, {"type": "text", "text": "D.1.2 Bounding $T_{2}$ ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{m}\\ell_{t,x}^{\\prime}(\\theta_{t}-\\theta_{t-1},\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{t,u}))^{2}}\\\\ &{\\le\\eta^{2}\\displaystyle\\sum_{t=1}^{m}\\ell_{t,x}^{\\prime}\\Big[\\nabla_{\\theta}f(\\theta_{t-1}),\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{t,u}))^{2}+\\sigma_{2}^{2}\\theta^{2}\\|\\nabla_{\\theta}G(\\theta_{t-1})\\|_{2}^{2}+2(\\nabla_{\\theta}G(\\theta_{t-1}),\\nabla_{\\theta}f(\\theta_{t-1}),}\\\\ &{\\le\\eta^{2}\\displaystyle\\sum_{t=1}^{m}\\ell_{t,x}^{\\prime}\\Big[(\\nabla_{\\theta}G(\\theta_{t-1}),\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{t,u}))^{2}+\\sigma_{2}^{2}\\|\\nabla_{\\theta}G(\\theta_{t-1})\\|_{2}^{2}+2\\sigma_{2}^{2}\\sigma_{2}\\|\\nabla_{\\theta}G(\\theta_{t-1})}\\\\ &{\\le\\eta^{2}\\displaystyle\\sum_{t=1}^{m}\\ell_{t,x}^{\\prime}\\Big[(\\nabla_{\\theta}G(\\theta_{t-1}),\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{t,u}))^{2}+\\sigma^{2}\\left(\\ell_{t,x}^{2}+2\\sigma_{2}\\right)\\|\\nabla_{\\theta}G(\\theta_{t-1})\\|_{2}^{2}\\Big]}\\\\ &{\\le\\eta^{2}\\displaystyle\\sum_{t=1}^{m}\\ell_{t,x}^{\\prime}(\\nabla_{\\theta}G(\\theta_{t-1}),\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{t,u}))^{2}+\\eta^{2}\\left(\\ell_{t,x}^{2}+2\\sigma_{2}\\right)\\|\\nabla_{\\theta}G(\\theta_{t-1})\\|_{2}^{2}}\\\\ &{\\le\\eta^{2}\\displaystyle\\sum_{t=1}^{m}\\ell_{t,x}^{\\prime}(\\nabla_{\\theta}G(\\theta_{t-1}),\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{t,u}))^{2}+\\eta^{2}\\sigma_{2} \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{2C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{\\prime\\prime}\\left\\langle\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{i,c}),\\theta_{t}-\\theta_{t-1}\\right\\rangle^{2}\\right)\\leq\\frac{\\eta^{2}}{2}\\left(1+\\varepsilon_{2}\\right)^{2}c_{s}\\varrho^{2}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some for suitable choice of $\\varepsilon_{2}$ . ", "page_idx": 18}, {"type": "text", "text": "D.1.3 Bounding $T_{3}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Using eigen-decomposition of $H_{i,c}.$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\theta_{t}-\\theta_{t-1})^{\\top}H_{i,c}(\\theta_{t}-\\theta_{t-1})=\\sum_{j=1}^{p}\\Lambda_{j,i,c}\\langle\\theta_{t}-\\theta_{t-1},\\mathbf{v}_{j,i,c}\\rangle^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Lambda_{j,i,c},{\\bf v}_{j,i,c}$ for $j\\in[1,\\cdot\\cdot\\cdot\\,,p]$ are the eigen-values and eigen-vectors of $H_{i,c}$ respectively. Now, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{t}-\\theta_{t-1},\\mathbf{v}_{j,i,c}\\rangle=-\\eta\\langle\\mathrm{desk}\\,\\mathrm{sk}\\,\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\mathbf{v}_{j,i,c}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\eta\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\mathbf{v}_{j,i,c}\\rangle+\\eta\\varepsilon_{3}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\sum_{j=1}^{p}\\Lambda_{j,i,c}\\bigg\\{\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\mathbf{v}_{j,i,c}\\rangle^{2}+(\\varepsilon_{3}^{2}+2\\varepsilon_{3})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}\\bigg\\}}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{p}\\Lambda_{j,i,c}\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\mathbf{v}_{j,i,c}\\rangle^{2}+\\left(\\displaystyle\\sum_{j=1}^{p}|\\Lambda_{j,i,c}|\\right)(\\varepsilon_{3}^{2}+2\\varepsilon_{3})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}\\\\ &{\\le\\Lambda_{\\operatorname*{max},i,c}\\displaystyle\\sum_{j=1}^{p}\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\mathbf{v}_{j,i,c}\\rangle)^{2}+\\left(\\displaystyle\\sum_{j=1}^{p}|\\Lambda_{j,i,c}|\\right)(\\varepsilon_{3}^{2}+2\\varepsilon_{3})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}\\\\ &{\\le\\displaystyle\\frac{c_{l}c_{H}}{\\sqrt{m}}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|^{2}+\\frac{\\kappa c_{l}c_{H}}{\\sqrt{m}}(\\varepsilon_{3}^{2}+2\\varepsilon_{3})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}(\\theta_{t}-\\theta_{t-1})^{\\top}H_{i,c}(\\theta_{t}-\\theta_{t-1})\\right)\\leq\\frac{\\eta^{2}c_{H}c_{l}}{2\\sqrt{m}}(1+\\kappa(2\\varepsilon_{3}+\\varepsilon_{3}^{2}))\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some for suitable choice of $\\varepsilon_{3}$ . ", "page_idx": 19}, {"type": "text", "text": "D.1.4 Combining $T_{1},T_{2},T_{3}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Combining $T_{1},T_{2},T_{3}$ , we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{L}(\\theta_{t})\\leq\\bar{\\mathcal{L}}(\\theta_{t-1})-\\eta\\|\\nabla_{\\theta}^{\\circ}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}+\\eta\\varepsilon_{1}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}}\\\\ &{}&{+\\,\\frac{\\eta^{2}}{2}\\left(1+\\varepsilon_{2}\\right)^{2}c_{s}\\varrho^{2}\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}+\\frac{\\eta^{2}c_{H}c_{l}}{2\\sqrt{m}}(1+\\kappa(2\\varepsilon_{3}+\\varepsilon_{3}^{2})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}\\\\ &{}&{\\leq\\mathcal{L}(\\theta_{t-1})-\\left[\\eta-\\eta\\varepsilon_{1}-\\frac{\\eta^{2}}{2}\\left(1+\\varepsilon_{2}\\right)^{2}c_{s}\\varrho^{2}-\\frac{\\eta^{2}c_{H}c_{l}}{2\\sqrt{m}}(1+\\kappa(2\\varepsilon_{3}+\\varepsilon_{3}^{2}))\\right]\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}\\\\ &{}&{\\quad\\times2\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using $\\mathrm{PL}$ condition, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathcal{L}(\\theta_{t})\\leq\\mathcal{L}(\\theta_{t-1})\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(60)}\\\\ {\\displaystyle-\\left[\\eta-\\eta\\varepsilon_{1}-\\frac{\\eta^{2}}{2}\\left(1+\\varepsilon_{2}\\right)^{2}c_{s}\\varrho^{2}-\\frac{\\eta^{2}c_{H}c_{l}}{2\\sqrt{m}}(1+\\kappa(2\\varepsilon_{3}+\\varepsilon_{3}^{2}))\\right]2\\mu\\left(\\mathcal{L}(\\theta_{t-1})-\\mathcal{L}(\\theta^{*})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{t})-\\mathcal{L}(\\theta^{*})\\leq(1-\\beta)\\left(\\mathcal{L}(\\theta_{t-1})-\\mathcal{L}(\\theta^{*})\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\beta=2\\mu\\eta\\left(1-\\varepsilon_{1}-\\frac{\\eta}{2}\\left(\\left(1+\\varepsilon_{2}\\right)^{2}c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\big(1+\\kappa(2\\varepsilon_{3}+\\varepsilon_{3}^{2})\\big)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Setting \u03b7 < (1 + \u03b52)2cs\u03f12 + c\u221aHmcl (11 + \u03ba(2\u03b53 + \u03b523)) gives a contraction map. If we iterate this recursion, we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{t})-\\mathcal{L}(\\theta^{*})\\leq(1-\\beta)^{t}\\left(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*})\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Set ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varepsilon_{1}=\\varepsilon_{2}=\\varepsilon_{3}=\\varepsilon\\;,\\;\\eta=\\frac{2(1-\\varepsilon)}{((1+\\varepsilon)^{2}c_{s}\\varrho^{2}+{c_{H}c_{l}}/{\\sqrt{m}(1+\\kappa(2\\varepsilon+\\varepsilon^{2}))})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{L}}(\\theta_{T})-{\\mathcal{L}}(\\theta^{*})\\leq(1-\\mu(1-\\varepsilon)\\eta)^{T}\\left({\\mathcal{L}}(\\theta_{0})-{\\mathcal{L}}(\\theta^{*})\\right)}\\\\ &{\\qquad\\qquad\\leq\\left({\\mathcal{L}}(\\theta_{0})-{\\mathcal{L}}(\\theta^{*})\\right)e^{-\\mu(1-\\varepsilon)\\eta T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.1.5 Choice of sketching dimension $b$ : ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Using A.1, for each step $t\\in[0,\\cdots T]$ , union bounding over $T_{1},T_{2}$ and $T_{3}$ and over all $\\mathrm{T}$ time-steps, Union bounding over all T time-steps, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nb=\\Omega\\left(\\frac{1}{\\varepsilon^{2}}\\log^{3}(p^{2}N T/\\delta)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{N=\\sum_{i=1}^{C}n_{c}}\\end{array}$ is the number of training samples. ", "page_idx": 20}, {"type": "text", "text": "D.2 Multi-step scheme ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem D.2. Let $\\|\\nabla_{\\theta}\\ell(\\theta)\\|\\,\\leq\\,G$ . Under Assumption 4.1, for a suitable constant $\\varepsilon<1$ , $\\eta=$ \u03b7global\u03b7local < 2\u00b5K(11\u2212\u03b5) and b = \u2126 \u03b512 log3(p2NT|C||K|/\u03b4) , with probability at least 1 \u2212\u03b4, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{T})-\\mathcal{L}(\\theta^{*})\\leq(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))e^{-2(1-\\varepsilon)\\mu\\eta K t}+\\frac{\\eta C_{2}(\\varepsilon,m,\\kappa)K G^{2}}{2\\mu(1-\\varepsilon)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\theta^{*}$ is the minimizer of the problem 1. And, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\upzeta_{2}(\\varepsilon,m,\\kappa)=\\frac{1}{1-\\varepsilon}\\left[\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)\\left(1+\\varepsilon\\right)+\\frac{1}{2}c_{s}\\left(1+\\varepsilon\\right)^{2}\\varrho^{2}+\\frac{c_{l}c_{H}}{2\\sqrt{m}}+\\frac{1}{2}\\left(2\\varepsilon+\\varepsilon^{2}\\right)\\frac{c_{l}\\kappa\\mathbf{c}_{H}}{\\sqrt{m}}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, $\\varrho$ and $c_{H}$ are defined in Lemma C.1. ", "page_idx": 20}, {"type": "text", "text": "Proof. We assume that the gradients at each time step t are bounded i.e $\\|\\nabla_{\\theta}\\mathcal{L}(\\theta)\\|_{2}\\leq G$ for some suitable constant $\\mathrm{G}$ . ", "page_idx": 20}, {"type": "text", "text": "First, we introduce some notation: In the K-step case since the parameters are shared only in the sync step, we define $\\theta_{c,t,k}\\,\\in\\,\\mathbb{R}^{p}$ as the local parameters for the client $c$ at time step $t$ and step $k\\in[1,2,\\cdots,K]$ . Based on the sync step in Alg: 1, we can see that $\\theta_{c,t,0}$ is the same for all clients ", "page_idx": 20}, {"type": "text", "text": "$c\\in C$ and thus, we denote it as $\\theta_{t}$ , i,e. $\\theta_{c,t,0}=\\theta_{t}\\forall c\\in C$ . Thus, we can write the update in the sync step as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t}-\\theta_{t-1}=-\\eta_{\\mathrm{global}}\\operatorname*{desk}\\left(\\cfrac{1}{C}\\csum_{c=1}^{C}\\mathrm{sk}\\left(\\eta_{\\mathrm{noal}}\\csum_{k=0}^{K-1}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k})\\right)\\right)}\\\\ &{\\qquad\\qquad=-\\eta_{\\mathrm{global}}\\eta_{\\mathrm{local}}\\left(\\cfrac{1}{C}\\csum_{c=1}^{C}\\left(\\csum_{k=0}^{K-1}\\operatorname*{desk}\\mathrm{sk}\\,\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k})\\right)\\right)}\\\\ &{\\qquad=-\\cfrac{\\eta}{C}\\left(\\displaystyle\\sum_{c=1}^{C}\\left(\\sum_{k=0}^{K-1}\\operatorname*{desk}\\mathrm{sk}\\,\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k})\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\eta=\\eta_{\\mathrm{global}}\\eta_{\\mathrm{local}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similar to D.1, we can bound $T_{1},T_{2}$ and $T_{3}$ in 34 as: ", "page_idx": 21}, {"type": "text", "text": "D.2.1 Bounding $T_{1}$ ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\theta_{t}-\\theta_{t-1}\\rangle}\\\\ &{=\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),-\\frac{\\eta}{C}\\displaystyle\\sum_{c=1}^{C}\\sum_{k=1}^{K}\\operatorname{desk}\\mathrm{sig}_{\\theta}\\mathcal{L}_{c}(\\theta_{t-1,c,k})\\rangle}\\\\ &{=-\\eta\\langle R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\frac{1}{C}\\displaystyle\\sum_{c=1}^{C}\\sum_{k=1}^{K}R\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{t-1,c,k})-K R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})+K R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\rangle}\\\\ &{=-\\eta K\\langle R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\rangle}\\\\ &{\\qquad\\quad-\\frac{\\eta}{C}\\displaystyle\\sum_{c=1}^{C}\\langle R\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1}),\\displaystyle\\sum_{k=1}^{K}[R(\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{t-1,k,c})-R\\nabla_{\\theta}\\mathcal{L}(\\theta_{c,t-1,0}))]\\rangle}\\\\ &{\\le-\\eta K(1-\\varepsilon_{1})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|^{2}+\\eta\\cdot\\eta_{\\mathrm{ded}}\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)(1+\\varepsilon_{2})K^{2}G^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1,k,c})-\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1,c,0})\\|\\le\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)\\|\\theta_{t-1,k,c}-\\theta_{t-1,c,0}\\|}&{}\\\\ {\\le\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)\\|\\displaystyle\\sum_{k=1}^{K}-\\eta_{\\mathrm{local}}\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1,k,c})\\|}&{}\\\\ {\\le\\eta_{\\mathrm{local}}\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)K G}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.2.2 Bounding $T_{2}$ ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf x_{i}),\\theta_{t}-\\theta_{t-1}\\right\\rangle^{2}=\\left\\langle\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf x_{i}),-\\displaystyle\\frac{\\eta}{C}\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\operatorname{desk}\\mathrm{sk}\\,\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k})\\right\\rangle^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{\\eta^{2}}{C^{2}}\\left(\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\left\\langle\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf x_{i}),\\operatorname{desk}\\mathrm{sk}\\,\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k})\\right\\rangle\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\eta^{2}(1+\\varepsilon_{3})^{2}\\varrho^{2}K^{2}G^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some suitable choice of $\\varepsilon_{2}$ and $b$ which we set later. Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{2C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\ell_{i,c}^{\\prime\\prime}\\left\\langle\\nabla_{\\theta}f(\\theta_{t-1};\\mathbf{x}_{i}),\\theta_{t}-\\theta_{t-1}\\right\\rangle^{2}\\right)\\leq\\frac{\\eta^{2}}{2}c_{s}\\left(1+\\varepsilon_{3}\\right)^{2}\\varrho^{2}K^{2}G^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.2.3 Bounding $T_{3}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Using eigen-decomposition, ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\theta_{t}-\\theta_{t-1})^{T}\\nabla_{\\theta}^{2}H_{i,c}(\\theta_{t}-\\theta_{t-1})=\\sum_{j=1}^{p}\\Lambda_{j,i,c}\\langle\\theta_{t}-\\theta_{t-1},\\mathbf{v}_{j,i,c}\\rangle^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\Lambda_{j,i,c},{\\bf v}_{j,i,c}$ for $j\\;\\;\\in\\;\\;[1,\\cdots\\,,p]$ are the eigen-values and eigen-vectors of $\\begin{array}{r l}{H_{i,c}}&{{}=}\\end{array}$ $\\ell_{i}^{'}\\nabla_{\\theta}^{2}f(\\theta_{t-1};\\mathbf{x}_{i})$ respectively. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\langle\\theta_{t}-\\theta_{t-1},{\\bf v}_{j,i,c}\\rangle^{2}=\\eta^{2}\\langle\\displaystyle\\frac{1}{C}\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\mathrm{desk\\,sk\\,}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k}),{\\bf v}_{j,i,c}\\rangle^{2}\\eqno(96)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\le\\langle\\frac{1}{C}\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k}),{\\bf v}_{j,i,c}\\rangle^{2}+\\varepsilon_{4}^{2}\\|\\displaystyle\\frac{1}{C}\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k})\\|^{2}\\le\\|\\theta_{t-1}\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\leq\\left\\langle\\frac{1}{C}\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k}),\\mathbf{v}_{j,i,c}\\right\\rangle^{2}+(\\varepsilon_{4}^{2}+2\\varepsilon_{4})K^{2}G^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for suitable choice of $\\varepsilon_{4}$ and $b$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{2C}\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\left(\\sum_{j=1}^{p}\\Lambda_{j,i,c}\\langle\\theta_{t}-\\theta_{t-1},\\mathbf{v}_{j,i,c}\\rangle^{2}\\right)\\right)}}\\\\ &{}&{\\le\\frac{1}{2C}\\displaystyle\\sum_{c=1}^{C}\\left(\\frac{1}{n_{c}}\\sum_{i=1}^{n_{c}}\\left(\\sum_{j=1}^{p}\\Lambda_{j,i,c}\\left\\{\\left\\langle\\frac{1}{C}\\displaystyle\\sum_{c=1}^{C}\\sum_{k=0}^{K-1}\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t-1,k}),\\ \\mathbf{v}_{j,i,c}\\right\\rangle^{2}+(\\varepsilon_{4}^{2}+2\\varepsilon_{4})K^{2}G^{2}\\right\\}\\right)\\right)}\\\\ &{}&{\\le\\frac{\\eta^{2}}{2}\\displaystyle\\frac{c_{l}c_{H}}{\\sqrt{m}}K^{2}G^{2}+\\frac{\\eta^{2}}{2}\\left(2\\varepsilon_{4}+\\varepsilon_{4}^{2}\\right)\\frac{c_{l}\\kappa c_{H}}{\\sqrt{m}}K^{2}G^{2}}&{\\scriptscriptstyle{(96)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.2.4 Combining $T_{1},T_{2},T_{3}$ : ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Combining $T_{1},T_{2}$ and $T_{3}$ , we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(\\theta_{t})\\leq\\mathcal{L}(\\theta_{t-1})-\\eta K(1-\\varepsilon_{1})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|^{2}+\\eta\\cdot\\eta_{\\mathrm{local}}\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)(1+\\varepsilon_{2})K^{2}G^{2}}\\\\ &{\\qquad+\\frac{\\eta^{2}}{2}c_{s}\\left(1+\\varepsilon_{3}\\right)^{2}\\varrho^{2}K^{2}G^{2}+\\frac{\\eta^{2}}{2}\\frac{c_{l}c_{H}}{\\sqrt{m}}K^{2}G^{2}+\\frac{\\eta^{2}}{2}\\left(2\\varepsilon_{4}+\\varepsilon_{4}^{2}\\right)\\frac{c_{l}\\kappa_{C H}}{\\sqrt{m}}K^{2}G^{2}}\\\\ &{\\qquad\\leq\\mathcal{L}(\\theta_{t-1})-\\eta K(1-\\varepsilon_{1})\\|\\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})\\|_{2}^{2}}\\\\ &{\\qquad+\\,\\eta\\left[\\eta_{\\mathrm{local}}\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)(1+\\varepsilon_{2})+\\frac{\\eta}{2}c_{s}\\left(1+\\varepsilon_{3}\\right)^{2}\\varrho^{2}+\\frac{\\eta}{2}\\frac{c_{l}c_{H}}{\\sqrt{m}}+\\frac{\\eta}{2}\\left(2\\varepsilon_{4}+\\varepsilon_{4}^{2}\\right)\\frac{c_{l}\\kappa c_{H}}{\\sqrt{m}}\\right]K^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using $\\mathrm{PL}$ condition and iterating over this recursion we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z(\\theta_{T})-\\mathcal{L}(\\theta^{*})\\leq(1-2\\mu\\eta K(1-\\varepsilon_{1}))^{T}(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))+\\eta^{2}C_{2}(\\varepsilon,m,\\kappa)\\frac{1-(1-2\\mu\\eta K(1-\\varepsilon_{1}))}{1-(1-2\\mu\\eta K(1-\\varepsilon_{1}))^{2}(\\lambda^{2}\\eta^{2})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))e^{-2(1-\\varepsilon)\\mu\\eta K T}+\\frac{\\eta C_{2}(\\varepsilon,m,\\kappa)K G^{2}}{2\\mu}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{\\eta\\leq\\frac{1}{2\\mu K(1-\\varepsilon)}}\\end{array}$ . For simplicity, let $\\eta_{\\mathrm{global}}=1$ i.e. $\\eta=\\eta_{\\mathrm{local}}$ and $\\varepsilon_{1}=\\varepsilon_{2}=\\varepsilon_{3}=\\varepsilon$ . And, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l l r}{\\displaystyle C_{2}(\\varepsilon,m,\\kappa)=\\frac{1}{1-\\varepsilon}\\left[\\left(c_{s}\\varrho^{2}+\\frac{c_{H}c_{l}}{\\sqrt{m}}\\right)\\left(1+\\varepsilon\\right)+\\frac{1}{2}c_{s}\\left(1+\\varepsilon\\right)^{2}\\varrho^{2}+\\frac{c_{l}c_{H}}{2\\sqrt{m}}+\\frac{1}{2}\\left(2\\varepsilon+\\varepsilon^{2}\\right)\\frac{c_{l}\\kappa\\mathbf{c}_{H}}{\\sqrt{m}}\\right]}\\\\ {\\displaystyle C_{2}(\\varepsilon,m,\\kappa)=\\mathcal{O}(\\varrho^{2}+\\frac{c_{H}}{\\sqrt{m}})+\\mathcal{O}(\\frac{\\varepsilon\\kappa\\mathbf{c}_{H}}{\\sqrt{m}})}&{(102\\varepsilon^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.2.5 Choice of sketching dimension $b$ : ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Using A.1, for each step $t\\in[0,\\cdot\\cdot\\cdot T]$ , union bounding over $T_{1},T_{2}$ and $T_{3}$ , Union bounding over all T time-steps, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\nb=\\Omega\\left(\\frac{1}{\\varepsilon^{2}}\\log^{3}(p^{2}N T|C||K|/\\delta)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{N=\\sum_{i=1}^{C}n_{c}}\\end{array}$ is the number of training samples. ", "page_idx": 23}, {"type": "text", "text": "E Communication Efficiency ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem E.1. If Theorem 2 holds, then with $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(C\\operatorname*{max}\\left\\{1,\\frac{C_{2}(\\varepsilon,m,\\kappa)G^{2}}{2\\mu^{2}(1-\\varepsilon)K\\epsilon}\\right\\}\\log\\left(\\frac{\\mathcal{L}\\left(\\theta_{0}\\right)-\\mathcal{L}\\left(\\theta^{*}\\right)}{\\epsilon}\\right)\\right)}\\end{array}$ bits of communication, with w.p at least Algorithm: 1 outputs an $\\epsilon$ -optimal solution $\\theta_{T}$ satisfying: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{T})-\\mathcal{L}(\\theta^{*})\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Setting $\\begin{array}{r}{\\eta=\\operatorname*{min}\\{\\frac{1}{2\\mu K(1-\\varepsilon)}}\\end{array}$ , $\\frac{\\epsilon\\mu}{C_{2}(\\varepsilon,m,\\kappa)K G^{2}}\\Bigr\\}$ and $\\begin{array}{r}{T=\\frac{1}{2\\mu\\eta K(1-\\varepsilon)}\\log\\left(\\frac{2(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))}{\\epsilon}\\right)}\\end{array}$ have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\exp(-2\\mu\\eta K(1-\\varepsilon)T)(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))\\leq\\frac{\\epsilon}{2}}\\\\ &{}&{\\frac{\\eta C_{2}(\\varepsilon,m,\\kappa)K G^{2}}{2\\mu}\\leq\\frac{\\epsilon}{2}}\\\\ &{\\implies(\\mathcal{L}(\\theta_{t})-\\mathcal{L}(\\theta^{*}))\\leq(1-2\\mu\\eta K(1-\\varepsilon))^{T}(\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*}))+\\frac{\\eta C_{2}(\\varepsilon,m,\\kappa)K G^{2}}{2\\mu}\\leq\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The total number of communication bits are given as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nC b T=\\tilde{\\mathcal{O}}\\left(C\\operatorname*{max}\\left\\{1,\\frac{C_{2}(\\varepsilon,m,\\kappa)G^{2}}{2\\mu^{2}(1-\\varepsilon)\\epsilon}\\right\\}\\log\\left(\\frac{\\mathcal{L}(\\theta_{0})-\\mathcal{L}(\\theta^{*})}{\\epsilon}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, O\u02dc hides poly-log dependence. ", "page_idx": 23}, {"type": "text", "text": "F Additional Experimental Details & Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide the overview of the local Top-r baseline in Algorithm 2. ", "page_idx": 23}, {"type": "text", "text": "Algorithm 2 Local Top-r-based Distributed Learning. ", "page_idx": 24}, {"type": "text", "text": "Hyperparameters: server learning rate $\\eta_{\\mathrm{global}}$ , local learning rate $\\eta_{\\mathrm{local}}$ .   \nInputs: local datasets $\\mathcal{D}_{c}$ of size $n_{c}$ for clients $c=1,\\ldots,C$ , number of communication rounds $T$ .   \nOutput: final model $\\theta_{T}$ .   \n1: Broadcast a random SEED to the clients.   \n2: for $t=1,\\dots,T$ do   \n3: On Client Nodes:   \n4: for $c=1,\\ldots,C$ do   \n5: if $t=1$ then   \n6: Receive the random SEED from the server. Initialize the local model $\\theta_{c,1}\\in\\ensuremath{\\mathbb{R}}^{p}$ using the   \nrandom SEED.   \n7: else   \n8: Receive $\\bar{\\hat{\\Delta}}_{t-1}$ from the server.   \n9: Update the model parameters $\\theta_{t}\\gets\\theta_{t-1}+\\bar{\\hat{\\Delta}}_{t-1}$ .   \n10: Assign the local model\u2019s parameters $\\theta_{c,t}\\gets\\theta_{t}$ to be updated locally.   \n11: end if   \n12: for $k=1,\\ldots,K$ do   \n13: $\\theta_{c,t}\\leftarrow\\theta_{c,t}-\\eta_{\\mathrm{local}}\\cdot\\nabla_{\\theta}\\mathcal{L}_{c}(\\theta_{c,t})$   \n14: end for   \n15: $\\Delta_{c,t}\\gets\\theta_{c,t}-\\theta_{t}$   \n16: Send Top- $\\mathbf{r}$ sparsified updates $\\hat{\\Delta}_{c,t}\\gets\\mathrm{Top}\\mathrm{-}\\mathrm{r}(\\Delta_{c,t})$ to the server.   \n17: end for   \n18:   \n19: On the Server Node:   \n20: Receive Top- $\\mathbf{r}$ sparsified updates $\\hat{\\Delta}_{c,t}$ from clients $c=1,\\ldots,C$ .   \n21: Aggregate them $\\begin{array}{r}{\\bar{\\hat{\\Delta}}_{t}\\gets\\eta_{\\mathrm{global}}\\cdot\\frac{1}{C}\\sum_{c=1}^{C}\\hat{\\Delta}_{c,t}}\\end{array}$   \n22: Broadcast $\\bar{\\hat{\\Delta}}_{t}$ to the clients.   \n23: end for ", "page_idx": 24}, {"type": "text", "text": "G Additional Results on Spectral Density of Predictor Hessian ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/f4cfbc12a52c4a4698ff9730183a3bfe02bca792e996d9926c0e4fe592c5a31e.jpg", "img_caption": ["Figure 2: Estimate $\\hat{\\kappa}$ of $\\begin{array}{r}{\\kappa=\\left.\\sum_{i=1}^{p}\\vert\\Lambda_{i}\\vert\\right/\\Lambda_{\\mathrm{max}}}\\end{array}$ of $H_{i}$ for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has $\\bar{1.1}\\stackrel{.}{\\times}10^{7}$ parameters. Loss function: Binary Cross Entropy(BCE) Loss. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "This section presents empirical results verifying our assumption on the sum of absolute eigenvalues of the predictor Hessian (Assumption 4.2). Since it is infeasible to compute all eigenvalues for deep models like ResNet-18, we rely on numerical approximations introduced in several prior works [69, 76]. SLQ is a technique in numerical linear algebra to estimate the ESD of large matrices. The complete algorithm can be found in Algorithm 3. We use PyHessian ( Ghorbani et al. [69]), an open-source implementation of SLQ for our experiments. We keep the default parameters $n_{v}=10,m=100$ for the plots as they have been shown to be of high accuracy. We train a ResNet-18 from scratch model to classify 1000 samples from two classes of CIFAR-10. We use a learning rate of $1e-3$ , SGD as the optimizer and and perform GD. We use a fix training sample to plot the ESD of predictor Hessian $H(\\theta;\\mathbf{x}_{i})=\\ell_{i}^{'}\\nabla_{\\theta}^{2}f(\\theta;\\mathbf{x}_{i})$ for two choices of loss functions $:$ BCE (Binary Cross ", "page_idx": 24}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/0422ec2881a3bf74b20c1fd3ab8b27528dc5eea083730fa789beae47ee0848d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 3: Spectral Density of Predictor Hessian : $:H(\\theta,{\\bf x}_{i})=\\ell_{i}^{'}\\nabla_{\\theta}^{2}f(\\theta;{\\bf x}_{i})$ for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has $1.1\\times10^{7}$ parameters. Loss function: Binary Cross Entropy(BCE) Loss. ", "page_idx": 25}, {"type": "text", "text": "Algorithm 3 Stochastic Lanczos Quadrature for ESD Computation (Ghorbani et al. [69]) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1: Input: Parameter: $\\theta$ , degree $m$ , and $n_{v}$ .   \n2: Compute the gradient of $\\theta$ by backpropagation, i.e., compute $\\begin{array}{r}{g_{\\theta}=\\frac{d f(\\theta;x_{i})}{d\\theta}}\\end{array}$ .   \n3: for $i=1,2,\\dots,n_{v}$ do   \n4: {Different Seeds}   \n5: Draw a random vector $v$ from $\\mathcal{N}(0,1)$ and normalize it (same dimension as $\\theta$ ).   \n6: Get the tridiagonal matrix $T$ through Lanczos algorithm.   \n7: Compute \u03c4 k $\\tau_{k}^{\\left(i\\right)}$ and $\\tilde{\\lambda}_{k}^{(i)}$ from $T$ .   \n8: $\\begin{array}{r}{\\phi_{\\sigma}^{z_{i}}=\\sum_{k=1}^{q}{\\tau_{k}f(\\tilde{\\lambda}_{k};t,\\sigma)}}\\end{array}$   \n9: end for   \n10: Return $\\begin{array}{r}{\\phi(t)=\\frac{1}{n_{v}}\\sum_{l=1}^{n_{v}}\\left(\\sum_{i=1}^{q}\\tau_{i}^{(l)}f(\\tilde{\\lambda}_{i}^{(l)};t,\\sigma)\\right)}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Entropy) and MSE (Mean Squared Loss). We compute the spectrum by backpropagating through the output layer instead of the loss. From the results in Figure 3, we note that a bulk of eigenvalues are close to zero, and the spectral density decays quickly far from zero, i.e., most of the eigenvalues are tiny. To verify our assumption numerically, we estimate the sum of absolute eigenvalues from the spectrum in Figure 2. Our results for the estimate $\\hat{\\kappa}$ of $\\kappa\\,=\\,\\Sigma_{i=1}^{p}\\big/\\Lambda_{\\mathrm{max}}$ for a fixed training sample are shown in Figure 2. As we can see from the Figure 2, for a ResNet-18 model, the constant $\\hat{\\kappa}\\approx4000<<1e7$ (model size), ", "page_idx": 25}, {"type": "text", "text": "Given the approximate density as a normalized histogram, we compute the empirical average and estimate $\\hat{\\kappa}$ an approximation to $\\begin{array}{r}{\\kappa=\\sum|\\Lambda_{i}|/\\Lambda_{\\operatorname*{max}}}\\end{array}$ as $p\\times$ the empirical average. Our results for $\\hat{\\kappa}$ are presented in Figure 24. ", "page_idx": 25}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/e4661ee4d6acda87be13c77a50f1f4d98be2172be90894b5285529ff845854d0.jpg", "img_caption": ["Figure 4: Epoch: 0 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/a486874f542c5c14d20bc15358855bf080c2b39d42c79d09ac454b00572b486d.jpg", "img_caption": ["Figure 5: Epoch: 200 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/f492a3a29e840a78c091f5e6d4995d299fe94747e6c4f9066b04f1f1dcdfe270.jpg", "img_caption": ["Figure 6: Epoch: 400 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/90fa496d092c3c67098042d23617b9348889e0f757bf716323b558eecd2e5bfe.jpg", "img_caption": ["Figure 7: Epoch: 600 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/f623588db4c5251181954d2ec6ca04cecf215277582511dde79eccff2c7d37ed.jpg", "img_caption": ["Figure 8: Epoch: 800 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/150916c1f487dd57c560e6eb1bea1a7c80f50e1887bd9c897ed8c963c5d98676.jpg", "img_caption": ["Figure 9: Epoch: 1000 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/7815b7e1613af43bd073d0aca23f0d48343bbaa87ecd7b279b1ce0f22f371b79.jpg", "img_caption": ["Figure 10: Epoch: 1200 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/3493c196e8ea8f03b0c68d36f99725ddbd99d7e9f20a761042fda3dd1247982a.jpg", "img_caption": ["Figure 11: Epoch: 1400 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/9eefeb0e4519a408c77b2b061bd73acdee61d59bbe3c0d3728d555dd24703dc5.jpg", "img_caption": ["Figure 12: Epoch: 1600 "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: Spectral Density of Predictor Hessian : $\\begin{array}{r}{\\mathcal{H}(\\theta,\\mathbf{x}_{i})=\\ell_{i}^{'}\\nabla_{\\theta}^{2}f(\\theta;\\mathbf{x}_{i})}\\end{array}$ for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has $1.1\\times10^{7}$ parameters. Loss function: Binary Cross Entropy(BCE) Loss. ", "page_idx": 26}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/7e281bd724e91e3f3e94af62c5ac91a1d993a71fa3cb92c73438f7edd3ee6db1.jpg", "img_caption": ["Figure 14: Epoch: 0 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/f2f11b6f6a8b358c44e580a5f56b311e3c5fe3e9f4d81bdc8903badde99a1ffd.jpg", "img_caption": ["Figure 15: Epoch: 200 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/6cc93fddf4e61ce98182bfabdc2d0c9bf84a6e66ee676165e19a421d58161621.jpg", "img_caption": ["Figure 16: Epoch: 400 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/0d169fc6690c304fff1f47d4b3ca9c28ceae6bde3b7f5516b8806e57cde1d6da.jpg", "img_caption": ["Figure 17: Epoch: 600 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/dc97e2d87e95a3a53538eebb9347cde9553b090b75e4a8532d1538501fe263bc.jpg", "img_caption": ["Figure 18: Epoch: 800 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/bc1ae169b1b32780deab73ec330ee7f7397616480b74f2cf2ce13681d4ab6865.jpg", "img_caption": ["Figure 19: Epoch: 1000 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/5d1e82640a653052e8add8f7ef15a0c5b4bd2c89f9eb6158eb9615123a314208.jpg", "img_caption": ["Figure 20: Epoch: 1200 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/58147dd8516f8385c9d8150d0bdc9a6d1c827d4898dec34ed692acde34092a02.jpg", "img_caption": ["Figure 21: Epoch: 1400 "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/46e6c5661b57d59540215cd00cdc9534f12db671879dec9d59186c67654cabcc.jpg", "img_caption": ["Figure 22: Epoch: 1600 "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 23: Spectral Density of Predictor Hessian : $:H(\\theta,{\\bf x}_{i})=\\ell_{i}^{'}\\nabla_{\\theta}^{2}f(\\theta;{\\bf x}_{i})$ for a fixed training input across training epochs. Dataset : 1000 samples from CIFAR-10 dataset. Model: ResNet-18. The model has $1.1\\times10^{7}$ parameters. Loss function: Mean Squared Error (MSE). ", "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/6f4c16b1d96ca7cd6db86cb1caab754c146975f36f8f7b246407944e41b070df.jpg", "img_caption": ["(a) $\\hat{\\kappa}$ for BCE Loss "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "0G0VpMjKyV/tmp/55691ef8b777400bdedf892ee53a161fb6d5eb06c5d415e665c65692b36587be.jpg", "img_caption": ["Figure 24: Estimate $\\hat{\\kappa}$ of $\\begin{array}{r}{\\kappa=\\sum_{i=1}^{p}|\\Lambda_{i}|\\big/\\Lambda_{\\operatorname*{max}}}\\end{array}$ over training iterations. ", "(b) $\\hat{\\kappa}$ for MSE Loss "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All of our claims are supported by our theoretical and empirical results throughout the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss limitations throughout the paper and also in Section 6 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Proof sketched are provided right after theorem statements and the full proofs are given in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide experimental details in Section 5 and Appendix F. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Due to confidentiality constraints, we are unable to share the code at this time, but we provide sufficient details to reproduce the results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide implementation details in Section 5 and Appendix F. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Computing spectral density for predictor Hessian is computationally demanding. Therefore, we repeated the experiments only once with a randomly selected homogeneous subset of the dataset. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide details in Appendix F. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We followed the Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide a discussion on broader impact in Section 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We properly cited the models and datasets in Sections 1 and 5 and in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]