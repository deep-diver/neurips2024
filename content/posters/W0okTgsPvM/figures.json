[{"figure_path": "W0okTgsPvM/figures/figures_1_1.jpg", "caption": "Figure 1: Multimodal Task Vectors (MTV) Overview. We overcome an LMM's context length limitation by encoding many shots of multimodal examples as activations in the LMM's latent space. We then directly replace this encoding into the LMM's activation space during downstream inference.", "description": "This figure illustrates the core idea of Multimodal Task Vectors (MTV).  Instead of directly feeding many multimodal examples (images and text) into the Large Multimodal Model (LMM), which is limited by context length, the MTV method compresses these examples into a compact vector representation in the model's activation space. During inference, this compact MTV vector is directly injected into the LMM's activation space, bypassing the context length limitation.  The figure visually depicts how multiple multimodal examples are compressed into MTV and how this MTV vector is used for downstream inference, resulting in predictions for new inputs.", "section": "1 Introduction"}, {"figure_path": "W0okTgsPvM/figures/figures_3_1.jpg", "caption": "Figure 2: Multimodal Task Vectors (MTV). In the standard multimodal in-context learning (ICL) paradigm, the number of shots is limited by an LMM's context length. We solve this issue by first finding the mean activations corresponding to the last token of the examples' input (Step 1), and then calculating a set of attention head locations (Step 2) that best align with the downstream task. These mean activations are then replaced directly in these attention head locations (Step 3), enabling many-shot multimodal ICL.", "description": "This figure illustrates the three steps of the proposed Multimodal Task Vectors (MTV) method.  First, it shows how mean activations are calculated from the last token of multiple inference iterations on a set of many-shot multimodal examples.  Second, it demonstrates how attention head locations are selected to best align with the downstream task using an adapted REINFORCE algorithm. Finally, it depicts how these mean activations are directly replaced into the selected attention head locations during downstream inference, enabling many-shot in-context learning without being limited by context length.", "section": "3 Multimodal Task Vectors"}, {"figure_path": "W0okTgsPvM/figures/figures_8_1.jpg", "caption": "Figure 3: Scaling of Qwen-MTV on VizWiz: (Left) We show the effect of varying the number of shots per iteration for a fixed 100 iterations. (Right) We also show the effect of varying numbers of iterations fixing 4 shots per iteration.", "description": "This figure shows the impact of the number of shots per iteration and the number of iterations on the performance of the Qwen-MTV model on the VizWiz dataset. The left panel shows that increasing the number of shots per iteration up to 16 improves performance, after which performance plateaus. The right panel shows that increasing the number of iterations up to 100 also improves performance, after which performance again plateaus. This demonstrates that MTV can effectively scale with more examples.", "section": "5.1 MTV scales with more examples"}, {"figure_path": "W0okTgsPvM/figures/figures_17_1.jpg", "caption": "Figure 3: Scaling of Qwen-MTV on VizWiz: (Left) We show the effect of varying the number of shots per iteration for a fixed 100 iterations. (Right) We also show the effect of varying numbers of iterations fixing 4 shots per iteration.", "description": "This figure shows the impact of two hyperparameters on the performance of the proposed MTV method.  The left panel shows how the number of shots per iteration affects accuracy, while keeping the total number of iterations constant at 100. The right panel illustrates the effect of varying the number of iterations while keeping the shots per iteration fixed at 4. The results demonstrate that MTV's performance scales with both the number of shots and iterations.", "section": "5.1 MTV scales with more examples"}, {"figure_path": "W0okTgsPvM/figures/figures_23_1.jpg", "caption": "Figure 2: Multimodal Task Vectors (MTV). In the standard multimodal in-context learning (ICL) paradigm, the number of shots is limited by an LMM's context length. We solve this issue by first finding the mean activations corresponding to the last token of the examples' input (Step 1), and then calculating a set of attention head locations (Step 2) that best align with the downstream task. These mean activations are then replaced directly in these attention head locations (Step 3), enabling many-shot multimodal ICL.", "description": "This figure illustrates the three steps of the proposed Multimodal Task Vectors (MTV) method.  It addresses the context length limitation in large multimodal models (LMMs) by encoding many shots into fewer tokens. Step 1 calculates the mean activations of the attention heads for the last token of the input examples across multiple inference iterations. Step 2 identifies optimal attention head locations within the model for storing the mean activations, which form the MTV.  Finally, Step 3 replaces the original activations in the identified locations with the MTV for downstream inference. This allows the model to effectively utilize many shots without exceeding its context length.", "section": "3 Multimodal Task Vectors"}]