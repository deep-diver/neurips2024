[{"figure_path": "3hcn0UxP72/figures/figures_3_1.jpg", "caption": "Figure 1: a. Depiction of the two group actions acting on the space of the network's parameters: the neuron rescaling of Equation (2) (top) and the neuron permutation of Equation (4) (bottom). b. Depiction of the geometry of the parameter space induced by the rescaling invariance of ReLU networks. The dotted lines denote the orbits T(\u03b8) while the solid lines represent the invariant sets H(c) associated with \u03b8 and the one associated with its rescaled version \u03b8'. Notice how the gradient of the loss g(\u03b8) is tangent to H(c) and orthogonal to T(\u03b8).", "description": "Figure 1 illustrates two group actions on the network's parameters: neuron rescaling and neuron permutation.  Panel (a) visually depicts these actions, showing how rescaling multiplies input and divides output weights by the same factor \u03b1, and permutation reorders the neurons. Panel (b) shows the parameter space geometry resulting from ReLU's rescaling invariance.  Orbits (T(\u03b8)) are depicted as dotted lines, and invariant sets (H(c)) are solid lines, demonstrating the gradient's tangency to H(c) and orthogonality to T(\u03b8).", "section": "Symmetries and observationally equivalent networks"}, {"figure_path": "3hcn0UxP72/figures/figures_6_1.jpg", "caption": "Figure 2: a. The invariant hyperquadric Q(ck) of a neuron with two inputs (d = 2) and one output (e = 1) in the cases where ck < 0 (left) and ck > 0 (right). b. Depiction of the invariant set H(c) in the case where l_ = 2 so that there are 2\u00b9- = 4 connected components. C++ denotes the connected component such that s = (\u00b11,+1). The blue lines separate the different effective components of H(c).", "description": "This figure visualizes the geometry of the invariant sets in the parameter space of a two-layer ReLU neural network. Panel (a) shows the shapes of the invariant hyperquadrics Q(ck) for a single neuron with two inputs and one output, for both positive and negative values of ck.  Panel (b) illustrates the invariant set H(c) for the entire network when there are two neurons with ck < 0, resulting in four connected components. These components are grouped into three effective components based on the symmetries discussed in the paper. The figure highlights how the geometry of the invariant set can lead to topological obstructions during training, depending on the initialization.", "section": "Topology of the invariant set"}, {"figure_path": "3hcn0UxP72/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of the experimental setup described in Section 6. a. The small 2-layer neural network architecture considered. b. The hidden neurons\u2019 parameter spaces, together with the invariant hyperquadrics associated with hidden neurons 1 (left) and 2 (right), for an initialization with topological obstruction (top) and without it (bottom). The colored curves represent the gradient descent trajectories from initialization \u03b8k(0) up to t* = 500 optimization steps. c. The loss curves for the bad (obstructed) and good initializations.", "description": "This figure visualizes an experiment's setup to demonstrate topological obstruction in training shallow ReLU neural networks.  Panel (a) shows the simple two-layer neural network architecture. Panel (b) illustrates the parameter space of two hidden neurons, showing how the gradient descent trajectory is constrained by the invariant hyperquadrics. It highlights two scenarios: one with topological obstruction (the trajectory gets stuck), and another without (the trajectory reaches the optimum). Panel (c) presents the loss curves for both scenarios.", "section": "6 Empirical Validation"}, {"figure_path": "3hcn0UxP72/figures/figures_9_1.jpg", "caption": "Figure 4: Left. Average test BCE loss of a two-layer ReLU neural network trained on the breast cancer dataset over 100 different initializations for each pair (l,l+), l = 2,...,9 and l+ \u2264 l, of numbers of hidden neurons and non-pathological neurons. Right. the y-axis displays the percentage of non-pathological neurons.", "description": "This figure displays the average test BCE loss of a two-layer ReLU neural network trained on the breast cancer dataset.  The left panel shows the average loss for different numbers of total neurons (l) and non-pathological neurons (l+), revealing a performance gradient where increasing non-pathological neurons improves performance.  The right panel visualizes the same data, showing the average test loss depending on the percentage of non-pathological neurons. This illustrates that the topological obstruction's impact depends on the absolute number of non-pathological neurons rather than the proportion.", "section": "6 Empirical Validation"}, {"figure_path": "3hcn0UxP72/figures/figures_22_1.jpg", "caption": "Figure 5: Probability of the topological obstruction as a function of the number of input d and hidden l neurons, when the initial weights are sampled with Xavier normal (left) and Kaiming normal (right) initialization schemes.", "description": "This figure shows the probability of encountering a topological obstruction during the training of a neural network, as a function of the number of input neurons (d) and hidden neurons (l).  The obstruction arises from the loss landscape having multiple disconnected components. Two common weight initialization schemes are compared: Xavier and Kaiming normal initialization. The heatmaps illustrate that the probability of obstruction is significantly higher for smaller input dimensions (d) and is generally lower for the Kaiming scheme.  For larger d values, the probability decreases rapidly for both initialization methods. This suggests that using appropriate initialization and sufficient input dimension can mitigate the risk of this topological obstruction.", "section": "F Probability of obstruction"}]