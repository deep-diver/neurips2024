[{"type": "text", "text": "Topological obstruction to the training of shallow ReLU neural networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Marco Nurisso Politecnico di Torino & CENTAI Institute Torino, 10100 - ITALY marco.nurisso@polito.it ", "page_idx": 0}, {"type": "text", "text": "Pierrick Leroy Politecnico di Torino Torino, 10100 - ITALY pierrick.leroy@polito.it ", "page_idx": 0}, {"type": "text", "text": "Francesco Vaccarino Politecnico di Torino Torino, 10100 - ITALY francesco.vaccarino@polito.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Studying the interplay between the geometry of the loss landscape and the optimization trajectories of simple neural networks is a fundamental step for understanding their behavior in more complex settings. This paper reveals the presence of topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow. We discuss how the homogeneous nature of the ReLU activation function constrains the training trajectories to lie on a product of quadric hypersurfaces whose shape depends on the particular initialization of the network\u2019s parameters. When the neural network\u2019s output is a single scalar, we prove that these quadrics can have multiple connected components, limiting the set of reachable parameters during training. We analytically compute the number of these components and discuss the possibility of mapping one to the other through neuron rescaling and permutation. In this simple setting, we find that the non-connectedness results in a topological obstruction, which, depending on the initialization, can make the global optimum unreachable. We validate this result with numerical experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training a neural network consists of navigating the complex geometry of the loss landscape to reach one of its deepest valleys. Gradient descent and its variants are, by far, the most commonly used algorithms to perform this task. While technically correct, the standard picture of the parameter space as Euclidean space with the trajectory rolling down the loss\u2019s surface in the steepest direction towards a minimum is slightly misleading because different choices of parameters can be observationally equivalent i.e. encode the same function [10]. The observational equivalence of parameters shape the loss landscape by imposing specific geometric structures on the parameter space. Minima are not isolated points but high-dimensional manifolds with complex geometry [17, 9, 42] and the loss function\u2019s gradients and Hessian are constrained to obey some specific laws [46, 27]. Gradient-based optimization methods, where the parameters are updated by performing discrete steps in the gradient\u2019s direction, are thus very much dependent on the symmetry-induced geometry [12, 29]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we provide a topological perspective on the constraints induced by some groups of network symmetries on the optimization trajectories. Topology is a field of mathematics that studies the properties of a space that are preserved under continuous deformations. Our main goal is to find and quantify in topological terms the impossibility of the training trajectories to freely explore the parameter space and get from any initialization to an optimal parameter. This idea is formalized in the topological notion of connectedness and, in particular, with the 0-th Betti number, which counts the number of connected components the space is composed of. The presence, or the absence, of topological obstructions in the parameter space does not depend on the particular loss function or the training data but is intrinsic to the interplay between the geometry and the topology of the parameter space under the action of groups of symmetries inducing observationally equivalent networks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Main contributions. Our main contributions are the following. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We find that, for two-layer neural networks, the gradient flow trajectories lie on an invariant set, which can be factored as the product of quadric hypersurfaces.   \n2. We analytically compute its Betti numbers, i.e., the number of connected components, holes, and higher-dimensional cavities.   \n3. We find that the invariant set can be disconnected when the network\u2019s output dimension is 1, leading to a clear topological obstruction.   \n4. We find that the obstruction is caused by \u201cpathological\u201d neurons that cannot change the sign of their output weights when trained with gradient flow.   \n5. We discuss the relation between the invariant set and the network\u2019s symmetries, finding that if we consider permutations, the number of effective connected components scales linearly in the number of pathological neurons.   \n6. We perform numerical validations on controlled toy scenarios, displaying the effect of obstruction in practice. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A large body of work studies gradient flow and gradient descent optimization of one hidden layer networks with homogeneous activations. Convergence properties have been found for wide networks [37, 43] with bounded density at initialization [31]. The implicit regularization provided is studied under various assumptions on: orthogonal input data [4], initialization scale [30, 4], wide (overparameterized regime) and infinitely wide [6], linearly separable data [30, 45]. Deeper linear networks [24] have also been studied. ", "page_idx": 1}, {"type": "text", "text": "These works focus on proving convergence and understanding which (optimal) solution is found, whereas our work investigates the shape of the optimization space and focuses on cases where the optimum might not be reachable from a given initialization. ", "page_idx": 1}, {"type": "text", "text": "Closer to our work, Safran et al. [39] studies two-layer ReLU binary classifiers with single input and output, counting the number of their piecewise-linear components after training. Eberle et al. [13] focuses on the differential challenge posed by the ReLU activation function and studies properties like the uniqueness of the solution of a gradient flow differential equation for a given initialization. ", "page_idx": 1}, {"type": "text", "text": "ReLU activation is a nonnegative homogeneous function, meaning that particular weight rescalings do not change the neural network\u2019s function. This is at the heart of the counterargument to flatness measures made by Dinh et al. [10], which shows that the Hessian eigenvalues can be made arbitrarily large in this way. Neyshabur et al. [34] explores the effect such rescalings can have on the gradient, proposing a rescaling-invariant regularization, and Pittorino et al. [36] employs them to define invariant flatness measures. Generally speaking, neural networks possess symmetries [20], and symmetries influence the geometry of training. Du et al. [12] studies how symmetry leads ReLU networks to automatically balance the neurons\u2019 weights. Kunin et al. [27], Zhao et al. [51] study how it constrains the gradient and Hessian matrix, leading to conservation laws w.r.t. gradient flow and Tanaka et al. [46] leverages it to propose a network pruning scheme. Ziyin [52] studies general mirror-reflect symmetries of the loss function and their effect on the weights of the trained network. Other conserved quantities stem from batch normalization\u2019s scale invariance [23, 47]. The transition from gradient flow to finite step size gradient descent breaks the conservation laws, resulting in altered trajectories [14, 2, 27, 44]. ", "page_idx": 1}, {"type": "text", "text": "Numerous works have explored the geometry and topology of the loss landscape to obtain insight into a neural network\u2019s training behavior. Motivated by the striking experimental observation that low loss points can be connected by simple curves [11, 18] or line segments [40, 16, 15], a large body of literature tries to understand this phenomenon of mode connectivity under the topological lens of the connectedness of the loss function\u2019s sublevel sets [17, 35, 26], especially for overparameterized neural networks [9, 8, 42]. Another line of work approaches the connectivity of minima from another point of view, studying the presence [50, 38, 48] or absence [28] of spurious minima, i.e. minima which are not global. Bucarelli et al. [5] analytically derives bounds on the sum of the Betti numbers of the loss landscape\u2019s sublevel set. Topological data analysis methods have also been exploited to numerically study the shape of the loss landscape [1, 22]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Setup and preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 One-hidden layer neural network ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unless otherwise stated, all vectors are column vectors, that is, $x=(x_{1},\\ldots,x_{d})^{\\intercal}\\in\\mathbb{R}^{d}\\cong\\mathbb{R}^{d\\times1}$ . Let us consider a two-layer neural network $f(\\cdot,\\theta):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{e}$ specified by the function ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x;\\theta)=W^{(2)}\\sigma(W^{(1)}x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is the input, $\\theta=\\big(W^{(1)},W^{(2)}\\big)$ with $W^{(1)}\\in\\mathbb{R}^{l\\times d}$ and $W^{(2)}\\in\\mathbb{R}^{e\\times l}$ are the parameters, $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the component-wise activation function and $l$ is the number of neurons in the hidden layer. Notice that we consider a network with no biases, as it allows us a discussion with lighter notation. The case with biases is discussed in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 2}, {"type": "text", "text": "In this work, following [12], we focus on the case where $\\sigma$ is homogeneous, namely $\\sigma(x)=\\sigma^{\\prime}(x)\\cdot x$ for every $x$ and for every element of the sub-differential $\\sigma^{\\prime}(x)$ if $\\sigma$ is non-differentiable at $x$ . The commonly used ReLU $\\dot{(\\sigma(z)=\\operatorname*{max}\\left\\{z,0\\right\\})}$ and Leaky ReLU $\\left(\\sigma(z)=\\operatorname*{max}\\left\\lbrace z,\\gamma\\right\\rbrace$ with $0\\leq\\gamma\\leq1$ ) activation functions satisfy this property. ", "page_idx": 2}, {"type": "text", "text": "We call parameter space the vector space $\\Theta=\\big\\{\\theta=(W^{(1)},W^{(2)})\\;\\vert\\;W^{(1)}\\in\\mathbb{R}^{l\\times d},W^{(2)}\\in\\mathbb{R}^{e\\times l}\\big\\}.$ ", "page_idx": 2}, {"type": "text", "text": "It will also be convenient to examine the single hidden neurons and their associated parameters for the following discussions. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1. For the two-layer neural network defined in Equation (1). Let $\\begin{array}{r l r}{k}&{{}=}&{1,\\ldots,l,}\\end{array}$ , let $(e_{11},e_{12},\\dots,e_{l l})$ be the canonical basis of $\\mathbb{R}^{l\\times l}$ and $\\Theta_{k}\\quad=$ $\\left\\{\\theta_{k}=\\left(e_{k k}W^{(1)},W^{(2)}e_{k k}\\right)\\mid(W^{(1)},W^{(2)})\\in\\Theta\\right\\}\\subset\\Theta$ , then $\\Theta=\\Theta_{1}\\oplus\\cdots\\oplus\\Theta_{l}$ . ", "page_idx": 2}, {"type": "text", "text": "Details of the proof are provided in Appendix A. Fixing $k\\,\\in\\,\\{1,\\ldots,l\\}$ , we can consider $\\Theta_{k}$ as the parameter space of the $k$ -th hidden neuron, which consists of the inputs and output weights of neuron $k$ , namely the rows and columns of $W^{(1)}$ and $W^{(2)}$ , respectively. For simplicity, when we work in \u0398k, we write W k(1)\u2236 $W_{k}^{(1)}:=e_{k k}W^{(1)}$ and $W_{k}^{(2)}:=W^{(2)}e_{k k}$ . Interestingly, the decomposition of Proposition 1 only holds for two-layer neural networks and will be crucial to the formulations of this paper\u2019s results. ", "page_idx": 2}, {"type": "text", "text": "3.2 Symmetries and observationally equivalent networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "It is well known that the properties of the activation function heavily influence the geometry of the parameter space $\\Theta$ . The activation function\u2019s commutativity with some classes of transformations can result in the latter having no effect on the function implemented by the neural network. This means that, in general, the mapping from the parameter space to the hypothesis class of functions is not injective. Following the terminology in Dinh et al. [10], we say that two parameters $\\theta_{1},\\theta_{2}\\in\\Theta$ are observationally equivalent, if they encode the same function $f(\\cdot;\\theta_{1})=f(\\cdot,\\mathbf{\\bar{\\theta}}_{2})$ and write $\\theta_{1}\\sim\\theta_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "In the case of homogeneous activations (ReLU or Leaky ReLU), we describe two kinds of transformations that send a parameter $\\theta$ into an observationally equivalent one. ", "page_idx": 2}, {"type": "text", "text": "Neuron rescaling. The input weights of a hidden neuron can be rescaled by a positive scalar $\\alpha>0$ provided that its output weights are rescaled by the inverse \u03b1\u2212 (top panel of Figure 1a). We formalize this as the action of the group $\\mathbb{R}_{+}$ of positive real numbers on $\\Theta_{k}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\colon\\mathbb{R}_{+}\\times\\Theta_{k}\\to\\Theta_{k}}\\\\ &{\\quad\\quad\\left(\\alpha,\\theta_{k}\\right)\\mapsto T_{\\alpha}(\\theta_{k})=\\left(\\alpha\\cdot W_{k}^{(1)},\\frac{1}{\\alpha}\\cdot W_{k}^{(2)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This action can be naturally extended to the space of all parameters by considering the possibility of rescaling all hidden neurons simultaneously by different factors. If $\\dot{\\alpha^{}}=\\left(\\alpha_{1},\\dots,\\bar{\\alpha}_{l}\\right)\\in\\bar{\\mathbb{R}}_{+}^{l}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nT_{\\alpha}(\\theta)=(\\mathrm{diag}(\\alpha)W^{(1)},W^{(2)}\\mathrm{diag}(\\alpha)^{-1}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "3hcn0UxP72/tmp/3f3d1b27e294400047deec4361a864b29e3cddd388ae39b2d22b65f6b5ae7b60.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: a. Depiction of the two group actions acting on the space of the network\u2019s parameters: the neuron rescaling of Equation (2) (top) and the neuron permutation of Equation (4) (bottom). b. Depiction of the geometry of the parameter space induced by the rescaling invariance of ReLU networks. The dotted lines denote the orbits $T(\\theta)$ while the solid lines represent the invariant sets $\\mathcal{H}(c)$ associated with $\\theta$ and the one associated with its rescaled version $\\theta^{\\prime}$ . Notice how the gradient of the loss $g(\\theta)$ is tangent to $\\mathcal{H}(c)$ and orthogonal to $T(\\theta)$ . ", "page_idx": 3}, {"type": "text", "text": "Given that $\\sigma(a z)=a\\sigma(z)$ when $a\\in\\mathbb{R}_{+}$ , we see how $\\theta\\sim T_{\\alpha}(\\theta)$ . ", "page_idx": 3}, {"type": "text", "text": "We write $T(\\theta)$ to denote the orbit of a parameter $\\theta$ under the action of $T$ , i.e. the set of all parameters obtained from $\\theta$ by arbitrarily rescaling the neurons $T(\\theta)=\\left\\{T_{\\alpha}(\\theta):\\,\\alpha\\in\\mathbb{R}_{+}^{l}\\right\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Permutations of the neurons. Besides rescaling, we can obtain an observationally equivalent network by permuting the hidden neurons in such a way as to preserve their input and output weights (bottom panel of Figure 1a). ", "page_idx": 3}, {"type": "text", "text": "Given the symmetric group on $l$ elements ${\\mathfrak{S}}_{l}$ of the permutations of $\\{1,\\ldots,l\\}$ , we write the action ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\colon\\mathfrak{S}_{l}\\times\\Theta\\to\\Theta}\\\\ &{\\qquad(\\pi,\\theta)\\mapsto P_{\\pi}(\\theta)=(R_{\\pi}W^{(1)},W^{(2)}R_{\\pi}^{\\intercal}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $R_{\\pi}$ is the $l\\times l$ row-permutation matrix associated to the permutation $\\pi$ . ", "page_idx": 3}, {"type": "text", "text": "Given that the activation function $\\sigma$ is applied component-wise, we have that it commutes with $R_{\\pi}$ , namely ", "page_idx": 3}, {"type": "equation", "text": "$$\nf\\bigl(x;P_{\\pi}(\\theta)\\bigr)=W^{(2)}R_{\\pi}^{\\top}\\sigma\\bigl(R_{\\pi}W^{(1)}x\\bigr)=W^{(2)}R_{\\pi}^{\\top}R_{\\pi}\\sigma\\bigl(W^{(1)}x\\bigr)=W^{(2)}\\sigma(W^{(1)}x)=f(x;\\theta)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and thus $P_{\\pi}(\\theta)\\sim\\theta$ because $R_{\\pi}^{\\top}=(R_{\\pi})^{-1}$ . ", "page_idx": 3}, {"type": "text", "text": "Having defined these two actions, we say that $\\theta$ and $\\theta^{\\prime}$ are observationally equivalent by rescalings and permutations if $\\theta^{\\prime}$ can be obtained from $\\theta$ by a finite sequence of actions of $T$ and $P$ or, equivalently thanks to Lemma 3 in the Appendix, if there exists a rescaling $\\alpha$ and a permutation $\\pi$ such that $\\theta^{\\prime}=P_{\\pi}\\circ T_{\\alpha}(\\theta)$ . In this case, we write $\\theta\\stackrel{\\mathrm{rp}}{\\sim}\\theta^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Conserved quantities and the invariant hyperquadrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The presence of symmetries in the neural network\u2019s parameter-function map results in a specific geometric structure in the loss landscape. Let indeed D = {(xi,yi) \u2208Rd \u00d7 Re}i be a training set of $N$ input-output pairs and fix a loss function $L:\\Theta\\to\\mathbb{R}$ which depends on th=e parameters only through the output of the neural network (1), that is ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}\\ell(f(x_{i};\\theta),y_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell:\\mathbb R^{e}\\times\\mathbb R^{e}\\to\\mathbb R$ is differentiable. In this work, as empirical risk minimization, we consider the continuous time version of the gradient descent (GD) algorithm (with learning rate $h>0$ ) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-h\\nabla_{\\theta}L(\\theta_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "named gradient flow (GF), and defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta(t)\\in-\\nabla_{\\theta}L(\\theta(t)):=-g\\big(\\theta(t)\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla_{\\theta}L(\\theta(t))$ is the Clarke sub-differential [7] which takes into account the parameters $\\theta$ where $L(\\theta)$ is non-differentiable. Given that the loss function $L$ depends on the parameters only through $f$ , its value at $\\theta$ must be constant over the orbit $T(\\theta)$ . This, together with the fact that the gradient of a differentiable function at a point is orthogonal to the level set at that point, means that ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(\\theta)\\perp T(\\theta)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "at any parameter $\\theta$ where $L(\\theta)$ is differentiable, as represented in Figure 1b. This orthogonality condition constrains the possible values of the gradient and, by extension, the possible gradient flow trajectories. In particular, as proven in Liang et al. [29], Tanaka et al. [46], Equation (8) is equivalent to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}W_{k i}^{(1)}g_{k i}^{(1)}-\\sum_{j=1}^{e}W_{j k}^{(2)}g_{j k}^{(2)}=0\\ \\ \\forall k=1,\\ldots,l.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For convenience of notation, we define, for $k=1,\\hdots,l$ , the following bilinear forms on $\\Theta$ , which help us describe the geometry induced by the rescaling symmetry. If $\\theta\\,=\\,\\big(W^{(1)},W^{(2)}\\big)$ and $\\eta=$ $(V^{\\bar{(1)}},V^{(2)})$ , we define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle\\!\\langle\\theta,\\eta\\rangle\\!\\rangle_{k}=\\sum_{i=1}^{d}W_{k i}^{(1)}V_{k i}^{(1)}-\\sum_{j=1}^{e}W_{j k}^{(2)}V_{j k}^{(2)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which, notice, only depends on the $k$ -th row of $W^{(1)}$ and $k$ -th column of $W^{(2)}$ meaning that we can equivalently see it as a bilinear form on $\\Theta_{k}.\\,\\Theta_{k}$ , together with $\\langle\\!\\langle\\cdot,\\cdot\\rangle\\!\\rangle_{k}$ is a pseudo-Euclidean space. ", "page_idx": 4}, {"type": "text", "text": "With the notation given by Equation (10), we see that Equation (9) can be simply rewritten as $\\langle\\!\\langle\\theta,g(\\theta)\\rangle\\!\\rangle_{k}=0$ for every neuron $k$ . This condition, akin to orthogonality w.r.t. the bilinear form of Equation (10), implies that, under gradient flow optimization, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}=2\\langle\\!\\langle\\dot{\\theta},\\theta\\rangle\\!\\rangle_{k}=-2\\langle\\!\\langle g(\\theta),\\theta\\rangle\\!\\rangle_{k}=0\\ \\,\\,\\forall k=1,\\dots,l.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This result, first obtained in Saxe et al. [41] for linear networks and discussed in Du et al. [12], Liang et al. [29], Kunin et al. [27], tells us that the rescaling symmetry results in the quantities $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}$ being conserved. This means that the difference between the Euclidean norm of the inputs and the outputs is constant for each neuron throughout the GF training trajectory. Moreover, under the condition of homogeneity of the activation function, Du et al. [12] proves that Equation (11) holds even at non-differentiable points of $L$ and in the case of multiple layers. ", "page_idx": 4}, {"type": "text", "text": "Invariant sets. Assume that at the initialization $\\theta_{0}$ we have $\\langle\\!\\langle\\theta_{0},\\theta_{0}\\rangle\\!\\rangle_{k}=c_{k}$ , for all $k$ , then Equation (11) implies that the GF trajectory will lie on the set characterized by the system of equations $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}=c_{k}$ for $k=1,\\hdots,l$ . This subset is mapped to itself under the GF dynamics by Equation (11) (see Figure 1b) and constitutes the main object of our study. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Invariant set). Given $c=\\left(c_{1},\\ldots,c_{l}\\right)$ , we call invariant set the subset $\\mathcal{H}(c)\\subseteq\\Theta$ given by the equations $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}=c_{k}\\ \\forall k=1,\\ldots,l$ . ", "page_idx": 4}, {"type": "text", "text": "If we look at each single equation (i.e. to each hidden neuron), we see that Equation (11) can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}\\left(W_{k i}^{(1)}\\right)^{2}-\\sum_{j=1}^{e}\\left(W_{j k}^{(2)}\\right)^{2}=c_{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which corresponds to a hyperquadric (or quadric hypersurface) in $\\Theta_{k}$ . We denote with $\\mathcal{Q}(c_{k})\\subseteq\\Theta_{k}$ this hypersurface and call it the invariant hyperquadric associated to the $k$ -th hidden neuron. ", "page_idx": 4}, {"type": "text", "text": "Here $c_{k}\\in\\mathbb{R}$ takes the role of a label associated with the $k$ -th hidden neuron, which, we see in the next section, plays a key role in specifying the shape of $\\mathcal{Q}(c_{k})$ . Figure 2a shows how, for $d=2$ and $e=1$ , $\\mathcal{Q}(c_{k})$ is an hyperboloid with 1 sheet (connected) if $c_{k}>0$ and 2 sheets if $c_{k}<0$ . ", "page_idx": 4}, {"type": "text", "text": "4 Topology of the invariant set ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we discussed above, Equation (11) tells us that gradient flow trajectories can\u2019t explore the whole space $\\Theta$ but are constrained to lie on the invariant set $\\mathcal{H}(c)$ . The values of $c$ , in turn, depend on the initialization and, we see from Equation (12), quantify the balance between the norms of input and output weights in every hidden neuron. ", "page_idx": 5}, {"type": "text", "text": "The goal of this section is to provide a topological characterization of $\\mathcal{H}(c)$ that can tell us something about the presence or absence of fundamental obstructions to the network\u2019s training process. With obstruction, we mean the impossibility of a GF trajectory to travel freely from one point to the other in $\\mathcal{H}(c)$ . We refer the reader to Appendix B for an essential overview of some of the topological concepts that we rely on in the next paragraphs. ", "page_idx": 5}, {"type": "text", "text": "Counting high-dimensional holes. Our topological characterization will be framed using Betti numbers. Betti numbers are well-known topological invariants given by a sequence of natural numbers that intuitively encode the number of higher-dimensional holes and cavities present in space. In particular, the 0-th Betti number of a space $X$ , $\\beta_{0}(X)$ corresponds to the number of connected components of $X$ and thus will be fundamental for our goal of identifying obstructions. ", "page_idx": 5}, {"type": "text", "text": "The invariant set $\\mathcal{H}(c)$ is given as the set of solutions of $l$ polynomial equations of degree 2 sharing no variables. Furthermore, in the setting of two-layer neural networks, we can leverage the fact that the parameter space can be decomposed into the parameter spaces of the hidden neurons. This, in turn, allows us to decompose the invariant set as the product of the neurons\u2019 invariant hyperquadrics, greatly simplifying our study. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. In a two-layer ReLU neural network, the invariant set $\\mathcal{H}(c)$ is homeomorphic to the Cartesian product of the hidden neurons\u2019 invariant hyperquadrics, that is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}(c)\\cong\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 1 tells us that we can understand the topology of $\\mathcal{H}(c)$ by studying independently its factors. Moreover, the hyperquadrics we encounter here are well-studied objects for which the next proposition (proven in Appendix D.2) gives a topological characterization. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. If $c_{k}>0$ , $\\mathcal{Q}(c_{k})$ is a topological manifold homeomorphic to $\\mathbb{R}^{e}\\times S^{d-1}$ . If $c_{k}<0$ , $\\mathcal{Q}(c_{k})$ is a topological manifold homeomorphic to $\\mathbb{R}^{d}\\times S^{e-1}$ . $\\begin{array}{r}{I f{\\boldsymbol{c}}_{k}=0}\\end{array}$ , $\\mathcal{Q}(0)$ is a contractible space. ", "page_idx": 5}, {"type": "text", "text": "Leveraging the decomposition of Lemma 1 and the characterization of the factors given by Proposition 2, we can explicitly compute all the Betti numbers of the invariant set. We give the next result in terms of the Poincar\u00e9 polynomial of $\\mathcal{H}(c)$ , namely the polynomial whose coefficients are the Betti numbers (see Appendix B). ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $l_{+},l_{-},l_{0}$ be the number of positive, negative, and zero components of c, respectively. The Poincar\u00e9 poly+no\u2212mial of $\\mathcal{H}(c)$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\mathcal{H}(c)}(x)=(1+x^{d-1})^{l_{+}}(1+x^{e-1})^{l_{-}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result, which is proven in Appendix D.3, contains a wealth of topological information as it gives us the exact number of holes and cavities of any order, depending on the network\u2019s hyperparameters $(d,e)$ and initialization $(l_{+},l_{-})$ . In the rest of this work, we focus only on the 0-th Betti number as the non-connectedness of $\\mathcal{H}(c)$ \u2212 provides a clear obstruction to the GF trajectories. ", "page_idx": 5}, {"type": "text", "text": "Connectedness of the invariant set. With regard to the connectedness of $\\mathcal{H}(c)$ , we can leverage Theorem 1 to obtain the exact number of connected components. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. The 0-th Betti number $\\beta_{0}$ of $\\mathcal{H}(c)$ , corresponding to the number of its connected components, is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{0}=\\left\\{\\!\\!\\begin{array}{l l}{1}&{i f d,e>1}\\\\ {2^{l_{+}}}&{i f d=1,e>1}\\\\ {2^{l_{-}}}&{i f d>1,e=1}\\\\ {2^{l_{+}+l_{-}}}&{i f d=1,e=1}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. This can be directly obtained from the coefficient of degree 0 of the Poincar\u00e9 polynomial obtained through Theorem 1. \u53e3 ", "page_idx": 5}, {"type": "image", "img_path": "3hcn0UxP72/tmp/6a75b25f1e575e35514121067fdbffe130dd5adb81f329d63e618a5272bd4f20.jpg", "img_caption": ["Figure 2: a. The invariant hyperquadric $\\mathcal{Q}(c_{k})$ of a neuron with two inputs $(d=2)$ and one output $\\left.e=1\\right.$ ) in the cases where $c_{k}<0$ (left) and $c_{k}>0$ (right). b. Depiction of the invariant set $\\mathcal{H}(c)$ in the case where $l_{-}=2$ so that there are $2^{l_{-}}=4$ connected components. $C_{\\pm\\mp}$ denotes the connected component such\u2212 that $s\\,=\\,(\\pm1,\\mp1)$ . The blue lines separate the different\u00b1 e\u2213ffective components of $\\mathcal{H}(c)$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "What we see in Equation (15) is that in most cases, the invariant set is connected, and gradient flow has no topological limitations in exploring the whole of $\\mathcal{H}(c)$ . Instead, when the hidden neurons have only one input or only one output, the space is fragmented into several components whose number scales exponentially in $l_{+}$ or $l_{-}$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Let us focus on the more interesting case where $d>1$ and $e=1$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 2. If the output of a two-layer ReLU neural network is a single scalar $e=1$ , its input has dimension $d>1$ , and the initial parameter $\\theta_{0}$ is such that $\\langle\\!\\langle\\theta_{0},\\theta_{0}\\rangle\\!\\rangle_{k}<0$ for $l_{-}>0$ hidden neurons, then the set $\\mathcal{H}(c)$ is disconnected and has $2^{l_{-}}$ connected components. ", "page_idx": 6}, {"type": "text", "text": "This means that neurons initialized with the norm of their outgoing weight strictly greater than their incoming weights\u2019 norm are responsible for disconnecting the space. We now precisely identify which connected component a parameter $\\theta$ belongs to and clarify the meaning of the obstruction. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3. Let $e=1,d>1$ , and $\\theta\\in\\mathcal{H}(c)$ with c such that $c_{k_{1}},\\ldots,c_{k_{l_{-}}}<0$ while $c_{k}\\geq0$ for all other k. Let W (2)\u2236= (W k(2 ),. $W_{-}^{(2)}:=(W_{k_{1}}^{(2)},\\dots,W_{k_{l_{-}}}^{(2)})\\in\\mathbb{R}^{1\\times l_{-}}$ be the row vector whose components are the components of $W^{(2)}\\in\\mathbb{R}^{1\\times l}$ associated to $c_{k}<0$ . Then the vector $s(\\theta)=\\big(\\mathrm{sign}(W_{k_{1}}^{(2)}),\\dots,\\mathrm{sign}(W_{k_{l_{-}}}^{(2)})\\big)$ identifies uniquely the component $\\theta$ belongs to, namely: $\\theta$ and $\\theta^{\\prime}$ belong to the same connected component of $\\mathcal{H}(c)$ if and only i $f s(\\theta)=s(\\theta^{\\prime})$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 3, proven in Appendix D.4, implies that $s(\\theta)$ does not change when we move in $C$ on a continuous curve such as the one given by gradient flow. This gives us an interesting interpretation of the topological obstruction: gradient flow cannot change the signs of the outgoing weights of the hidden neurons $k$ such that $c_{k}<0$ (see Appendix $\\mathrm{G}$ for an intuitive explanation of the phenomenon). This same observation is also mentioned in Boursier and Flammarion [3]. Proposition 3 extends one of the results of Boursier et al. [4] which proves that the same also holds when $c_{k}=0$ (balanced initialization). ", "page_idx": 6}, {"type": "text", "text": "By also considering Corollary 1, one obtains that a clever initialization of the parameters given by $\\langle\\!\\langle\\bar{\\theta}_{0},\\theta_{0}\\rangle\\!\\rangle_{k}=c_{k}>0\\mathrm{~}\\bar{\\forall}k=1,\\dots,l$ can prevent the issue by ensuring the connectedness of the invariant set. We also find that under common initialization schemes such as Xavier [19] and Kaiming [21] the probability of having pathological neurons is negligible when the input dimension and number of hidden neurons is high (see Appendix F). ", "page_idx": 6}, {"type": "text", "text": "5 Taking symmetries into account ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Corollary 2 states that neurons $k$ such that $c_{k}<0$ are \u201cpathological\u201d, in the sense that they are responsible for disconnecting the invariant set into several components, whose number scales exponentially in the number of those neurons. This result gives us a grim picture of the possibility of actually optimizing the neural network: if the initial parameter $\\theta_{0}$ is in a particular connected component and the global optimum $\\theta_{*}$ lies in another, then any gradient flow trajectory will not be able to reach $\\theta_{\\ast}$ because it will be constrained in its connected component. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "This result, however, provides us only with a partial picture of the parameter space\u2019s geometry. It is a priori possible that the training trajectory, moving in its connected component, reaches a parameter $\\zeta$ , which itself is optimal as it is observationally equivalent to $\\theta_{\\ast}$ $(\\zeta\\sim\\theta_{*})$ . In this case, the topological obstruction given by the non-connectedness would be only apparent. ", "page_idx": 7}, {"type": "text", "text": "To take this fact into account, we define the following notion. ", "page_idx": 7}, {"type": "text", "text": "Definition 2 (Effective component). Let $\\theta\\in\\mathcal{H}(c)$ and $C(\\theta)$ be its connected component therein. We define its effective component $\\operatorname{Eff}(\\theta)$ as the union of the connected component of all $\\theta^{\\prime}$ such that $\\theta^{\\prime}\\stackrel{\\mathrm{rp}}{\\sim}\\theta.$ . So that $\\begin{array}{r}{\\mathrm{Eff}(\\theta):=\\bigcup_{\\theta^{\\prime}\\sim\\theta}C(\\theta^{\\prime})}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Figure 2b gives a picture which clarifies the definition, showing a space with 4 connected components that has only 3 effective components. If the optimum $\\theta_{*}$ belongs to the same effective component as the initialization, then it is possible to reach a parameter that is observationally equivalent to it (through permutations and rescalings). ", "page_idx": 7}, {"type": "text", "text": "We present a useful result which tells us that the action of rescaling of Equation (3) can take any non-degenerate parameter $\\theta\\in\\mathcal{H}(c)$ to any other invariant set $\\mathcal{H}(c^{\\prime})$ for every $c^{\\prime}\\in\\mathbb{R}^{l}$ . This means that any invariant set can realize all the neural network\u2019s functions. ", "page_idx": 7}, {"type": "text", "text": "uPnrioqpuoes Fsourc he vtehrayt $c_{k}\\in\\mathbb{R}$ .e rIyf $\\theta_{k}\\in\\Theta_{k}$ uacnhd $W_{k}^{(1)},W_{k}^{(2)}\\neq0$ s, athmeer eh oelxdisst fs oar $\\alpha_{k}\\in\\mathbb{R}_{+}$ $T_{\\alpha_{k}}(\\theta_{k})\\,\\in\\,\\mathcal{Q}(c_{k})$ $W_{k}^{(1)}=0$ $W_{k}^{(2)}\\,\\ne\\,0,$   \nevery $c_{k}<0$ , w+hile, i ${}^{c}W_{k}^{(1)}\\neq0$ and $W_{k}^{(2)}=0,$ , it holds for every $c_{k}>0$ . ", "page_idx": 7}, {"type": "text", "text": "The proof can be found in Appendix D.5 with the formula of the specific $\\alpha$ which realizes the rescaling. ", "page_idx": 7}, {"type": "text", "text": "The following theorem leverages the power of Proposition 4 to give necessary and sufficient conditions for $\\theta$ and $\\theta^{\\prime}$ to belong to the same effective component. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Let $d>1$ and $e\\,=\\,1$ . Let $c\\in\\mathbb{R}^{l}$ and $l_{-}$ be the number of neurons such that $c_{k}<0$ .   \nAssume that $l_{-}\\geq1$ . Let $C,C^{\\prime}\\subseteq\\mathcal{H}(c)$ be two distin\u2212ct connected components of $\\mathcal{H}(c)$ such that   \n$s(\\theta)=s$ , $\\forall\\theta\\in C$ , and $s(\\theta^{\\prime})=s^{\\prime}$ , $\\forall\\theta^{\\prime}\\in C^{\\prime}$ . Then, the following statements are equivalent: $^{\\,I}$ . for every $\\theta\\in C$ there exists $\\theta^{\\prime}\\in C^{\\prime}$ such that $\\theta\\stackrel{\\mathrm{rp}}{\\sim}\\theta^{\\prime}$ ; $2.\\;\\sum_{i=1}^{l_{-}}s_{i}=\\sum_{i=1}^{l_{-}}s_{i}^{\\prime}$ ", "page_idx": 7}, {"type": "text", "text": "The theorem, proven in Appendix D.6, tells us that, while connected components are identified by $s$ , the effective components are identified only by the values of $\\textstyle\\sum_{i}s_{i}$ or, equivalently, by the distribution of $\\pm1$ in $s$ . Therefore, we find that the number of effective components scales much slower than the exponential growth of the number of connected components given by Corollary 1. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3. The number of effective components of $\\mathcal{H}(c)$ is given by $1+l_{-}$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. Theorem 2 tells us that two connected components $C,C^{\\prime}$ belong to the same effective component if and only if their associated sign vectors $s,s^{\\prime}\\in\\left\\{-1,1\\right\\}^{l_{-}}$ have the same sum. The number of effective components will thus equal the number of different values that the sum $\\textstyle\\sum_{i=1}^{l_{-}}s_{i}$ can have. If $s_{i}=1\\;\\forall i$ then $\\textstyle\\sum_{i=1}^{l_{-}}s=l_{-}$ . Each switch of a component to $-1$ decreases the sum\u2019s v=alue by 2 until it reaches the minim=um $-l_{-}$ . Therefore, the total number of values of the sum will be $1+l_{-}$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "6 Empirical Validation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Task, dataset, and model setup. We display here a toy example, showing how the initialization of the model can cause a topological obstruction, making the optimum unreachable. ", "page_idx": 7}, {"type": "text", "text": "We consider the function $F(x_{1},x_{2})=-(x_{1}+x_{2})$ , which will be our ground-truth. Next, we generate a dataset of 8000 points $(x_{i},F(x_{i}))$ by sampling $x_{i}\\sim U([0,1]^{2})$ . Our model, depicted in Figure 3a) ", "page_idx": 7}, {"type": "image", "img_path": "3hcn0UxP72/tmp/bd2d915704ab379964c10744b9606776ad851d9c3199209faabfbc890a1086cf.jpg", "img_caption": ["Figure 3: Visualization of the experimental setup described in Section 6. a. The small 2-layer neural network architecture considered. b. The hidden neurons\u2019 parameter spaces, together with the invariant hyperquadrics associated with hidden neurons 1 (left) and 2 (right), for an initialization with topological obstruction (top) and without it (bottom). The colored curves represent the gradient descent trajectories from initialization $\\theta_{k}(0)$ up to $t_{*}=500$ optimization steps. c. The loss curves for the bad (obstructed) and good initializations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "is a one hidden layer neural network with 2 hidden neurons, ReLU activations and no biases. All the weights are initialized by independently sampling from $U([-{\\sqrt{2}},{\\sqrt{2}}])$ . From the task and the network\u2019s architecture, it is clear that at least one of the output weights has to be negative to approximate $F$ correctly. ", "page_idx": 8}, {"type": "text", "text": "To standardize our results, we apply the rescaling of Proposition 4 and relocate the initial parameters to an observationally equivalent one in the invariant set $\\mathcal{H}(c)$ with $c_{k}\\in\\{-0.1,0.1\\}$ , controlling the sign of the weights on the last layer. We allow ourselves to do these two manipulations to control the experiments while only marginally modifying the network initialization, avoiding the introduction of massively unbalanced weights, which could change the dynamics, as shown in Neyshabur et al. [34]. Finally, we train the network using gradient descent on the MSE loss with a small learning rate of $h=0.01$ . This limits the variations of $c_{k}$ values to less than one percent along training, giving us a good approximation of gradient flow. ", "page_idx": 8}, {"type": "text", "text": "Results. We initialize different models and collect all states and losses. First, when we initialize the model with an \u201cunlucky\u201d configuration, namely $c=\\left(-0.1,-0.1\\right)$ (the space has 4 connected components) and $s(\\theta)=\\bigl(+1,+1\\bigr)$ , we find that the trajectories are confined to the positive region of their invariant hyperquadric, resulting in a poor approximation of $F$ , as we can see in Figure 3b (top) and in the loss of Figure 3c. Instead, with an initial configuration such that $c=(-0.1,\\bar{+}0.1)$ (2 connected components) and $s(\\theta)=\\bigl(+1,+1\\bigr)$ , the model can leverage the connectedness of $\\mathcal{Q}(c_{2})$ to learn $F$ by flipping the sign of the second neuron\u2019s output weight (Figure 3b bottom right). ", "page_idx": 8}, {"type": "text", "text": "A more realistic experiment. We present here a further experiment to show how the topological obstruction can be a hindrance in a more realistic setting. We consider a simple binary classification task on the well-known breast cancer dataset [49], which we try to solve by ftiting a one-layer ReLU neural network trained to minimize the BCE loss. We vary the number of hidden neurons $l$ and, for each $l$ , we change the number of non-pathological neurons $l_{+}$ (neurons with $c_{k}>0$ ) from 0 to $l$ . We repeat the experiment with 100 different random initializa+tions and show how the model\u2019s average performance changes when the degree of disconnectedness of its invariant set is varied. The result, on the left panel of Figure 4, clearly shows the presence of a \"gradient\" in performance, where increasing the number of non-pathological neurons decreases the average value of the test loss after training. The right panel of Figure 4, moreover, shows how the impact of the obstruction depends on the number of non-pathological neurons and not on their fraction over the total number of hidden neurons. ", "page_idx": 8}, {"type": "image", "img_path": "3hcn0UxP72/tmp/e8e99a60a3fd66cb4df5748b0124e2b99353eb88bc45450d096d774c8bf6b3d8.jpg", "img_caption": ["Figure 4: Left. Average test BCE loss of a two-layer ReLU neural network trained on the breast cancer dataset over 100 different initializations for each pair $(l,l_{+})$ , $l=2,\\dots,9$ and $l_{+}\\leq l$ , of numbers of hidden neurons and non-pathological neurons. Right. th+e y-axis displays th+e percentage of non-pathological neurons. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have given analytical results that clarify the nature of the constraints imposed by gradient flow on the parameter space of a two-layer neural network with homogeneous activations. In the case of a single scalar output, which appears in tasks such as binary classification and scalar regression, we identified initial conditions that lead to a topological obstruction in the form of the parameter space\u2019s fragmentation into multiple connected components. This is caused by pathological neurons whose output weights cannot change their sign during training. Moreover, if one also considers the network\u2019s symmetries under permutations of the hidden neurons, we find that most of the connected components are equivalent. The number of effective components of the resulting space scales linearly with the number of pathological neurons, contrasting with the exponential growth of the number of connected components obtained without considering the permutation symmetries. ", "page_idx": 9}, {"type": "text", "text": "As shown in the last numerical experiment, the lack of non-pathological neurons hinders learning, even when the network\u2019s width is scaled. Our probabilistic analysis outlined in Appendix F, however, shows that with common initialization schemes, the probability of creating a pathological neuron decreases rapidly with increased inner layer width. Therefore, the combination of specific initialization schemes and a large number of hidden neurons (beyond the minimum required to solve a task) appears to make this obstruction unlikely in practice. This work describes a simple safeguard to avoid obstructions, which can, for instance, discourage the usage of initialization schemes that result in the proliferation of pathological neurons. ", "page_idx": 9}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The main limitation of the work is the network\u2019s architecture, which is limited to only one hidden layer. Considering multiple layers, we can still define rescalings and permutations and find invariant hyperquadrics for each hidden neuron. The issue emerges in the fact that these hyperquadrics are not \u201cindependent\u201d anymore, and the invariant set cannot be factored into the product of the $\\mathcal{Q}(c_{k})$ . This intuitively results from the fact that in the multi-layer case, each weight in the hidden layers is shared by two neurons. ", "page_idx": 9}, {"type": "text", "text": "The second limitation is that our study focuses on gradient flow optimization. This idealized situation doesn\u2019t take into account the fact that moderate step size of gradient descent and stochastic gradient descent can break the conservation of $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}$ and make the parameters drift away from the invariant set [2]. Moreover, popular optimizers like ADAM [25] update the parameters employing the gradients at previous iterations so their trajectories will not be constrained to lie on $\\mathcal{H}(c)$ as we defined it. ", "page_idx": 9}, {"type": "text", "text": "The inclusion of regularization terms in the loss function, such as $\\ell_{p}$ regularizations, also breaks the invariance to rescalings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M.N. acknowledges the project PNRR-NGEU, which has received funding from the MUR \u2013 DM 352/2022. ", "page_idx": 10}, {"type": "text", "text": "F.V. would like to thank the Isaac Newton Institute for Mathematical Sciences for the support and hospitality during the programme Hypergraphs: Theory and Applications when work on this paper was undertaken. This work was supported by: EPSRC Grant Number EP/V521929/1 ", "page_idx": 10}, {"type": "text", "text": "This study was carried out within the FAIR - Future Artificial Intelligence Research and received funding from the European Union Next-GenerationEU (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) \u2013 MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.3 \u2013 D.D. 1555 11/10/2022, PE00000013). This manuscript reflects only the authors\u2019 views and opinions; neither the European Union nor the European Commission can be considered responsible for them. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Serguei Barannikov, Daria Voronkova, Ilya Trofimov, Alexander Korotin, Grigorii Sotnikov, and Evgeny Burnaev. Topological obstructions in neural networks learning. arXiv preprint arXiv:2012.15834, 2020. [2] David GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint arXiv:2009.11162, 2020. [3] Etienne Boursier and Nicolas Flammarion. Early alignment in two-layer networks training is a two-edged sword. arXiv preprint arXiv:2401.10791, 2024. [4] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs. Advances in Neural Information Processing Systems, 35:20105\u201320118, 2022. [5] Maria Sofia Bucarelli, Giuseppe Alessio D\u2019Inverno, Monica Bianchini, Franco Scarselli, and Fabrizio Silvestri. A topological description of loss surfaces based on betti numbers. arXiv preprint arXiv:2401.03824, 2024. [6] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on learning theory, pages 1305\u20131338. PMLR, 2020. [7] Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth analysis and control theory, volume 178. Springer Science & Business Media, 2008. [8] Y Cooper. The critical locus of overparameterized neural networks. arXiv preprint arXiv:2005.04210, 2020. [9] Yaim Cooper. The loss landscape of overparameterized neural networks. arXiv preprint arXiv:1804.10200, 2018.   \n[10] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019\u20131028. PMLR, 2017.   \n[11] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In International conference on machine learning, pages 1309\u20131318. PMLR, 2018.   \n[12] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in neural information processing systems, 31, 2018.   \n[13] Simon Eberle, Arnulf Jentzen, Adrian Riekert, and Georg S. Weiss. Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with relu activation. Electronic Research Archive, 2023.   \n[14] Yuanyuan Feng, Tingran Gao, Lei Li, Jian-Guo Liu, and Yulong Lu. Uniform-in-time weak error analysis for stochastic gradient descent algorithms via diffusion approximation. arXiv preprint arXiv:1902.00635, 2019.   \n[15] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. Advances in Neural Information Processing Systems, 33:5850\u20135861, 2020.   \n[16] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259\u20133269. PMLR, 2020.   \n[17] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016.   \n[18] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in neural information processing systems, 31, 2018.   \n[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[20] Grzegorz G\u0142uch and R\u00fcdiger Urbanke. Noether: The more things change, the more stay the same. arXiv preprint arXiv:2104.05508, 2021.   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.   \n[22] Stefan Horoi, Jessie Huang, Bastian Rieck, Guillaume Lajoie, Guy Wolf, and Smita Krishnaswamy. Exploring the geometry and topology of neural network loss landscapes. In International Symposium on Intelligent Data Analysis, pages 171\u2013184. Springer, 2022.   \n[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456. pmlr, 2015.   \n[24] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=HJflg30qKX.   \n[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID:6628106.   \n[26] Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. Advances in neural information processing systems, 32, 2019.   \n[27] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv preprint arXiv:2012.04728, 2020.   \n[28] Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface of neural networks for binary classification. In International Conference on Machine Learning, pages 2835\u20132843. PMLR, 2018.   \n[29] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. In The 22nd international conference on artificial intelligence and statistics, pages 888\u2013896. PMLR, 2019.   \n[30] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. Advances in Neural Information Processing Systems, 34:12978\u201312991, 2021.   \n[31] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33): E7665\u2013E7671, 2018.   \n[32] James R Munkres. Elements of algebraic topology. CRC press, 2018.   \n[33] J.R. Munkres. Topology. Featured Titles for Topology. Prentice Hall, Incorporated, 2000. ISBN 9780131816299.   \n[34] Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. Advances in neural information processing systems, 28, 2015.   \n[35] Quynh Nguyen. On connected sublevel sets in deep learning. In International conference on machine learning, pages 4790\u20134799. PMLR, 2019.   \n[36] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo Zecchina. Deep networks on toroids: removing symmetries reveals the structure of flat regions in the landscape geometry. In International Conference on Machine Learning, pages 17759\u201317781. PMLR, 2022.   \n[37] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. Communications on Pure and Applied Mathematics, 75(9):1889\u20131935, 2022.   \n[38] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In International conference on machine learning, pages 4433\u20134441. PMLR, 2018.   \n[39] Itay Safran, Gal Vardi, and Jason D Lee. On the effective number of linear regions in shallow univariate relu networks: Convergence guarantees and implicit bias. Advances in Neural Information Processing Systems, 35:32667\u201332679, 2022.   \n[40] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.   \n[41] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.   \n[42] Berfin Simsek, Fran\u00e7ois Ged, Arthur Jacot, Francesco Spadaro, Cl\u00e9ment Hongler, Wulfram Gerstner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In International Conference on Machine Learning, pages 9722\u20139732. PMLR, 2021.   \n[43] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. SIAM Journal on Applied Mathematics, 80(2):725\u2013752, 2020.   \n[44] Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.   \n[45] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19 (70):1\u201357, 2018.   \n[46] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in neural information processing systems, 33:6377\u20136389, 2020.   \n[47] Twan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.   \n[48] Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural network optimization landscapes. Journal of Machine Learning Research, 20(133):1\u201334, 2019.   \n[49] William Wolberg, Olvi Mangasarian, Nick Street, and W Street. Breast cancer wisconsin (diagnostic). UCI Machine Learning Repository, 10:C5DW2B, 1995.   \n[50] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad local minima in neural networks. arXiv preprint arXiv:1802.03487, 2018.   \n[51] Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmamy. Symmetries, flat minima, and the conserved quantities of gradient flow. arXiv preprint arXiv:2210.17216, 2022.   \n[52] Liu Ziyin. Symmetry induces structure and constraint of learning. In Forty-first International Conference on Machine Learning, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Parameter spaces ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $(e_{11},e_{12},\\dots,e_{l l})$ and $(e_{1},\\ldots,e_{l})$ be the canonical bases of the spaces $\\mathbb{R}^{l\\times l}$ and $\\mathbb{R}^{l}$ , respectively and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Theta_{k}=\\left\\{\\left(e_{k k}W^{(1)},W^{(2)}e_{k k}\\right)\\mid\\left(W^{(1)},W^{(2)}\\right)\\in\\Theta\\right\\}\\subset\\Theta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\Theta_{k}$ , notice, is the subspace of $\\Theta$ consisting of the weight matrices $W^{(1)}$ with null rows except for the $k$ -th one, and weight matrices $W^{(2)}$ with null columns except for the $k$ -th one. We can check that $\\Theta=\\Theta_{1}\\oplus\\cdots\\oplus\\Theta_{l}$ , because, if $I_{l}$ is the $l\\times l$ identity matrix, $\\sum e_{k k}=I_{l}$ so that ", "page_idx": 14}, {"type": "equation", "text": "$$\n(W^{(1)},W^{(2)})=\\sum_{k}(W_{k}^{(1)},W_{k}^{(2)})=\\sum_{k}(e_{k k}W^{(1)},W^{(2)}e_{k k}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, $\\Theta\\cong\\Theta_{1}\\times\\cdots\\times\\Theta_{l}$ via the linear isomorphism ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta=\\left(W^{(1)},W^{(2)}\\right)\\leftrightarrow\\left(\\theta_{k}\\right)_{k=1}^{l}=\\left(\\left(e_{k k}W^{(1)},W^{(2)}e_{k k}\\right)\\right)_{k=1}^{l}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This, we see, is equivalent to decomposing the neural network of Equation (1) into the computations of the single hidden neurons. Indeed, let $\\bar{f}\\big(x;\\theta_{k}\\big):=f\\big(x,\\big(e_{k k}W^{(\\bar{1})},W^{(2)}e_{k k}\\big)\\big)$ , then, considering that $e_{k k}e_{k k}=e_{k k}$ and that $\\sigma(e_{k k}v)=e_{k k}\\sigma(v)$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\nf\\bigl(x;\\theta_{k}\\bigr)=W^{(2)}e_{k k}\\sigma\\bigl(W^{(1)}x\\bigr)\\;\\forall k=1,\\ldots,l.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore $\\begin{array}{r}{\\sum_{k}f\\big(x;\\theta_{k}\\big)=f\\big(x;\\theta\\big)}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "B Primer on topology ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we recall some basic facts about the topology required to understand the paper\u2019s results. A self-consistent introduction is outside this work\u2019s scope, so we refer the interested reader to more complete expositions in Munkres [33, 32]. ", "page_idx": 14}, {"type": "text", "text": "Topological manifold. An $n$ -dimensional topological manifold is a topological space $X$ which locally looks like the Euclidean space $\\mathbb{R}^{n}$ . More formally, for each $p\\in X$ , there exists a neighbourhood $U$ of $p$ and a homeomorphism mapping $U$ to an open subset of $\\mathbb{R}^{n}$ . ", "page_idx": 14}, {"type": "text", "text": "Contractible space. A topological space $X$ is contractible if it can continuously deform to a point $p\\in X$ . This means that there exists a continuous map ", "page_idx": 14}, {"type": "equation", "text": "$$\nF:X\\times\\left[0,1\\right]\\rightarrow X\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "such that $F(x,0)=x$ and $F(x,1)=p$ for every $x\\in X$ . ", "page_idx": 14}, {"type": "text", "text": "Betti numbers. Betti numbers formalize the notion of the hole in a topological space and extend it to describe higher-dimensional cavities. The general idea is that one can associate a sequence of Abelian groups named homology groups to any space $X$ , which encodes rich information about the higher-dimensional cavities in $X$ . For what we are concerned here, the rank of the $k$ -th homology group is called the $k$ -th Betti number $\\beta_{k}(X)$ . $\\beta_{k}(X)$ counts the number of $k$ -dimensional holes in the space: $\\beta_{0}(X)$ count the number of connected components, $\\beta_{1}(X)$ the number of \u201ccircular\u201d holes and $\\beta_{2}(X)$ the number of voids or cavities. ", "page_idx": 14}, {"type": "text", "text": "A contractible space $X$ is connected and cannot have any holes, and thus its Betti numbers are $\\beta_{0}(X)=1$ and $\\beta_{i}(X)=0\\;\\forall i>0$ . ", "page_idx": 14}, {"type": "text", "text": "Betti numbers are topological invariants, meaning they are preserved when a space is transformed via a homeomorphism, namely a bijective, continuous map with continuous inverse. ", "page_idx": 14}, {"type": "text", "text": "Poincar\u00e9 polynomials. The Poincar\u00e9 polynomial of a topological space $X$ is the polynomial whose $k$ -th coefficient is given by the $k$ -th Betti number ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{X}(x)=\\beta_{0}(X)+\\beta_{1}(X)x+\\beta_{2}(X)x^{2}+\\ldots.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "K\u00fcnneth formula. K\u00fcnneth\u2019s theorem describes assembling the homology groups of a Cartesian product of spaces $X\\times Y$ from the homology groups of the factors $X,Y$ . One of its corollaries tells us that if we care only about Betti numbers, a simple relation holds between the Poincar\u00e9 polynomials of $X\\times Y$ and the ones of $X$ and $Y$ , namely, ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{X\\times Y}(x)=p_{X}(x)p_{Y}(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Types of connectedness. In topology, there are several kinds of connectedness. Two of them are particularly important for this work. ", "page_idx": 15}, {"type": "text", "text": "1.) A topological space $X$ is connected if it cannot be divided into two disjoint non-empty open sets. If it is not connected, the connected component of a point $x\\in X$ is given by the union of all connected subsets of $X$ which contain $x$ . ", "page_idx": 15}, {"type": "text", "text": "A topological space equal to the Cartesian product of two spaces $X=Y\\times Z$ is connected if and only if $Y$ and $Z$ are both connected. ", "page_idx": 15}, {"type": "text", "text": "2.) A topological space is path-connected if, for every pair of points $x,y\\,\\in\\,X$ , there exists a continuous curve $\\gamma:[0,1]\\to X$ such that $\\gamma(0)=x$ , $\\gamma(1)=y$ . The path-component of $x$ is the set of all $y\\in X$ such that a continuous curve exists connecting $x$ to $y$ . ", "page_idx": 15}, {"type": "text", "text": "This second notion is more relevant to our setting, where we care about the possible destinations of the optimization trajectories. ", "page_idx": 15}, {"type": "text", "text": "Path-connectedness implies connectedness, but not the opposite. There are situations, however, where these two notions are equivalent. For example, when $X$ is a topological manifold, $X$ is connected if and only if $X$ is path connected. ", "page_idx": 15}, {"type": "text", "text": "Notice that the 0-th Betti number $\\beta_{0}(X)$ counts the number of connected components but, in general, not the number of path components. With Lemma 2, we prove that these two notions are equivalent for our object of study. ", "page_idx": 15}, {"type": "text", "text": "C Extra propositions and lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 2. The invariant set $\\mathcal{H}(c)$ is connected if and only if it is path connected. ", "page_idx": 15}, {"type": "text", "text": "Proof. Lemma 1 tells us that $\\mathcal{H}(c)\\cong\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l})$ . ", "page_idx": 15}, {"type": "text", "text": "Let us focus on a particular $\\mathcal{Q}(c_{k})$ . ", "page_idx": 15}, {"type": "text", "text": "When $c_{k}\\neq0$ , Proposition 2 tells us that $\\mathcal{Q}(c_{k})$ is a topological manifold, and thus, it is connected if and only if it is path connected. ", "page_idx": 15}, {"type": "text", "text": "When $c_{k}=0,\\,\\mathcal{Q}(0)$ is not a topological manifold but contractible, implying that it is connected. Let us prove that it is also path-connected. ", "page_idx": 15}, {"type": "text", "text": "Let $\\theta_{k},\\theta_{k}^{\\prime}\\in\\mathcal{Q}(0)$ and define the curve $\\gamma:[0,1]\\to\\mathcal{Q}(0)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{k;\\theta}(t)=t\\cdot\\theta_{k}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "such that $\\gamma_{k;\\theta}(0)=\\theta,\\ \\gamma_{k;\\theta}(1)=0.\\ \\gamma_{k;\\theta}(t)\\in\\mathcal{Q}(0)$ for every $t\\in[0,1]$ because ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\!\\langle\\gamma_{k;\\theta}(t),\\gamma_{k;\\theta}(t)\\rangle\\!\\rangle_{k}=t\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the segment from $\\theta_{k}$ to 0 belongs to $\\mathcal{Q}(0)$ . ", "page_idx": 15}, {"type": "text", "text": "A continuous curve from $\\theta_{k}$ to $\\theta_{k}^{\\prime}$ can be thus obtained by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{k;\\theta^{\\prime}}\\gamma_{k;\\theta}(t):={\\binom{\\gamma_{k;\\theta}(2t)}{\\gamma_{k;\\theta^{\\prime}}(2-2t)}}\\quad{\\mathrm{~if~}}t\\in[0,{\\frac{1}{2}}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is continuous because $\\gamma_{k;\\theta}(1)=\\gamma_{k;\\theta^{\\prime}}(1)=0$ . Therefore $\\mathcal{Q}(0)$ is path connected. ", "page_idx": 15}, {"type": "text", "text": "Finally, if $\\mathcal{H}(c)$ is connected, then all of its factors $\\mathcal{Q}(c_{k})$ are connected, which, in turn, is true if and only if they are path-connected. A product of path-connected space is again path-connected, and therefore $\\mathcal{H}(c)$ is path-connected. The other implication is true because path-connectedness implies connectedness, thus concluding the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (Interchange of rescalings and permutations). Let $\\theta\\in\\Theta$ , $\\alpha\\in\\mathbb{R}_{+}^{l}$ and $\\pi\\in\\mathfrak{S}_{l}$ , then, if $\\tilde{\\alpha}=R_{\\pi^{-1}}(\\alpha)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{\\alpha}P_{\\pi}(\\theta)=P_{\\pi}T_{\\tilde{\\alpha}}(\\theta).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Given that ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{\\alpha}P_{\\pi}(\\theta)=(\\mathrm{diag}(\\alpha)R_{\\pi}W^{(1)},W^{(2)}R_{\\pi}^{\\intercal}\\mathrm{diag}(\\alpha)^{-1})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we need to prove that $\\mathrm{diag}(\\alpha)R_{\\pi}=R_{\\pi}\\mathrm{diag}(\\tilde{\\alpha})$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mathrm{diag}(\\alpha)R_{\\pi})_{i j}=\\sum_{k=1}^{l}\\mathrm{diag}(\\alpha)_{i k}(R_{\\pi})_{k j}=\\alpha_{i}(R_{\\pi})_{i j}=\\left\\{\\!\\!\\begin{array}{l l}{\\alpha_{i}}&{\\mathrm{~if~}j=\\pi(i)}\\\\ {0}&{\\mathrm{~otherwise}}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us pick a generic $\\tilde{\\alpha}\\in\\mathbb{R}_{+}^{l}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n(R_{\\pi}\\mathrm{diag}(\\tilde{\\alpha}))_{i j}=\\sum_{k=1}^{l}(R_{\\pi})_{i k}\\mathrm{diag}(\\tilde{\\alpha})_{k j}=(R_{\\pi})_{i j}\\tilde{\\alpha}_{j}=\\left\\{\\tilde{\\alpha}_{j}\\ \\ \\ \\ \\mathrm{if~}j=\\pi(i)\\atop0\\right.\\ \\ \\ \\ \\mathrm{otherwise}\\ \\ \\ \\ \\ \\ \\ }\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us consider the inverse permutation $\\pi^{-1}$ so that $\\pi^{-1}(j)=i$ if $\\pi(i)=j$ . Then, if $\\tilde{\\alpha}={\\cal R}_{\\pi^{-1}\\alpha}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}_{j}=\\alpha_{\\pi^{-1}(j)}=\\alpha_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and thus we get that $\\mathrm{diag}(\\alpha)R_{\\pi}=R_{\\pi}\\mathrm{diag}(\\tilde{\\alpha})$ . ", "page_idx": 16}, {"type": "text", "text": "D Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Proposition 1 tells us that the invariant set can be decomposed as the direct sum of the single hidden neurons\u2019 parameter spaces. This means that, for every $\\theta\\,\\in\\,\\Theta$ , there exist unique $\\theta_{1}\\in\\Theta_{1},\\ldots,\\theta_{l}\\in\\Theta_{l}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta=\\theta_{1}+\\theta_{2}+\\cdots+\\theta_{l}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we have a linear isomorphism $\\varphi:\\Theta_{1}\\times\\cdots\\times\\Theta_{k}\\to\\Theta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi:(\\theta_{1},\\ldots,\\theta_{l})\\mapsto\\theta_{1}+\\cdots+\\theta_{l}=\\theta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The invariant set is a subset of $\\Theta$ , which is given as the set of solutions of $l$ equations $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}\\,=$ $c_{k}\\ k\\,=\\,1,\\ldots,l$ . Notice that each of these equations involves a set of variables that appear only in that particular equation. These variables are exactly the ones which belong to $\\Theta_{k}$ . In fact $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}\\!=\\langle\\!\\langle\\theta_{k},\\theta_{k}\\rangle\\!\\rangle_{k}$ . ", "page_idx": 16}, {"type": "text", "text": "Therefore, given $\\theta_{1}\\in\\mathcal{Q}(c_{1}),\\ldots,\\theta_{l}\\in\\mathcal{Q}(c_{l})$ we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi(\\theta_{1},\\ldots,\\theta_{l})=\\sum_{k=1}^{l}\\theta_{k}\\in{\\mathcal{H}}(c).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the opposite, given $\\theta\\in\\mathcal{H}(c)$ we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi^{-1}(\\theta)=(\\theta_{1},\\ldots,\\theta_{l})\\in\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, $\\mathcal{H}(c)$ is in bijection with $\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l})$ through $\\varphi$ which, being a linear isomorphism, implies also that $\\mathcal{H}(c)$ and $\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l})$ are homeomorphic. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Let us consider the three cases separately. If $c_{k}>0$ , $\\mathcal{Q}(c_{k})$ is defined by the equation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}\\left(W_{k i}^{(1)}\\right)^{2}-\\sum_{j=1}^{e}\\left(W_{j k}^{(2)}\\right)^{2}=c_{k}\\iff\\left\\lVert W_{k}^{(1)}\\right\\rVert_{F}^{2}-\\left\\lVert W_{k}^{(2)}\\right\\rVert_{F}^{2}=c_{k},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\left\\|\\cdot\\right\\|_{F}$ is the Frobenius norm of a matrix, i.e., the square root of the sum of the squares of its elements. This can be rewritten as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|W_{k}^{(1)}\\right\\|_{F}=\\sqrt{c_{k}+\\left\\|W_{k}^{(2)}\\right\\|_{F}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\sqrt{c_{k}+\\left\\|W_{k}^{(2)}\\right\\|_{F}^{2}}>0$ because $c_{k}>0$ . We define the map $h:\\mathcal{Q}(c_{k})\\rightarrow S^{d-1}\\times\\mathbb{R}^{e}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nh\\left(W_{k}^{\\left(1\\right)},W_{k}^{\\left(2\\right)}\\right)=\\left(\\frac{W_{k}^{\\left(1\\right)}}{\\sqrt{c_{k}+\\left\\Vert W_{k}^{\\left(2\\right)}\\right\\Vert_{F}^{2}}},W_{k}^{\\left(2\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where, notice, the first component belongs to the sphere $S^{d-1}$ because of Equation (17) and $W_{k}^{(2)}\\in\\mathbb{R}^{e}$ This map is bijective, differentiable and has the following inverse $h^{-1}:S^{d-1}\\times\\mathbb{R}^{e}\\to\\mathcal{Q}(_{k})$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nh^{-1}(u,x)=\\big(\\sqrt{c_{k}+\\left\\|v\\right\\|_{F}^{2}}\\,u,x\\big)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is differentiable. Therefore, $h$ is a diffeomorphism from $\\mathcal{Q}(c_{k})$ to $S^{d-1}\\times\\mathbb{R}^{e}$ ", "page_idx": 17}, {"type": "text", "text": "If $c_{k}<0$ , we write the equation of $\\mathcal{Q}(c_{k})$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\Vert W_{k}^{(2)}\\right\\Vert_{2}=\\sqrt{-c_{k}+\\left\\Vert W_{k}^{(1)}\\right\\Vert_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $W_{k}^{(1)}$ and ${W}_{k}^{(2)}$ have switched their role to guarantee the term on the right to be positive. The diffeomorphism is now built analogously to Equation (18) as a map $h:\\mathcal{Q}(c_{k})\\rightarrow S^{e-1}\\times\\mathbb{R}^{d}$ . ", "page_idx": 17}, {"type": "text", "text": "If $c_{k}=0$ , we prove that $\\mathcal{Q}(0)$ is a contractible space. To do that, we exhibit a homotopy equivalence between $\\mathcal{Q}(0)$ and the point 0, i.e. a continuous map $p:[0,1]\\times\\mathcal{Q}(0)\\rightarrow\\mathcal{Q}(0)$ such that $\\dot{p(0,\\theta_{k})}=\\theta_{k}$ and $p(1,\\theta_{k})=0\\ \\forall\\theta_{k}\\in\\mathcal{Q}(0)$ . The map is defined in the following way: ", "page_idx": 17}, {"type": "equation", "text": "$$\np(\\lambda,\\theta_{k})=\\big(1-\\lambda\\big)\\theta_{k}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is continuous and well-defined because ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\langle p(\\lambda,\\theta_{k}),p(\\lambda,\\theta_{k})\\rangle\\rangle_{k}=\\langle\\!\\langle(1-\\lambda)\\theta_{k},(1-\\lambda)\\theta_{k}\\rangle\\!\\rangle_{k}=(1-\\lambda)^{2}\\langle\\langle\\theta_{k},\\theta_{k}\\rangle\\!\\rangle_{k}=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "meaning that $p(\\lambda,\\theta_{k})\\in\\mathcal{Q}(0)$ for every $\\theta_{k}\\in\\mathcal{Q}(0)$ and for every $\\lambda\\in[0,1]$ . ", "page_idx": 17}, {"type": "text", "text": "D.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. An implication of the K\u00fcnneth formula is that the Poincar\u00e9 polynomial of the Cartesian product of two spaces is equal to the product of their Poincar\u00e9 polynomials: ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{X\\times Y}(x)=p_{X}(x)p_{Y}(y).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Starting from Proposition 2, we can apply this result to $\\mathcal{Q}(c_{k})$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\mathcal{Q}(c_{k})}=\\left\\{p_{\\mathbb{R}^{d}}(x)p_{S^{d-1}}(x)\\quad\\mathrm{~if~}c_{k}>0\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "because a contractible space has 1 connected component and all of its other Betti numbers equal to zero. ", "page_idx": 17}, {"type": "text", "text": "Moreover, we know that $\\mathbb{R}^{n}$ is contractible for any $n$ and its Poincar\u00e9 polynomial is $p_{\\mathbb{R}^{n}}(x)=1$ . The Poincar\u00e9 polynomial of the sphere $S^{n}$ is given by $p_{S^{n}}(x)=1+x^{n}$ . ", "page_idx": 17}, {"type": "text", "text": "Equation (19) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\mathcal{Q}(c_{k})}=\\left\\{\\begin{array}{l l}{1+x^{d-1}}&{\\mathrm{~if~}c_{k}>0}\\\\ {1+x^{e-1}}&{\\mathrm{~if~}c_{k}<0\\;.}\\\\ {1}&{\\mathrm{~if~}c_{k}=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given that Lemma 1 tells us that $\\mathcal{H}(c)$ can be factored into the product of the $\\mathcal{Q}(c_{k})$ , we apply K\u00fcnneth formula and find that ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\mathcal{H}(c)}\\big(x\\big)=p_{\\mathcal{Q}(c_{1})}\\big(x\\big)\\cdots p_{\\mathcal{Q}(c_{l})}\\big(x\\big)=\\big(1+x^{d-1}\\big)^{l_{+}}\\big(1+x^{e-1}\\big)^{l_{-}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.4 Proof of Proposition 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. In the following, we exploit Lemma 2 and use the terminology connected and path-connected interchangeably. ", "page_idx": 18}, {"type": "text", "text": "Let us first prove that $s(\\theta)=s(\\theta^{\\prime})$ means that $\\theta$ and $\\theta^{\\prime}$ belong to the same connected component. We do this by explicitly building a continuous curve $\\delta:[0,1]\\to{\\bar{\\mathcal{H}}}(c)$ such that $\\delta(0)=\\theta$ and $\\delta(1)=\\theta^{\\prime}$ . Let us proceed by leveraging the homeomorphism from $\\mathcal{H}(c)$ and $\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l})$ and consider the different components of $\\delta$ in the invariant hyperquadrics associated to each neuron $\\delta=\\left(\\delta_{1},\\dots,\\delta_{l}\\right)$ . If $c_{k}\\geq0$ , we know that $\\mathcal{Q}(c_{k})$ is path-connected and therefore we fix $\\delta_{k}(t)$ to any continuous curve in $\\mathcal{Q}(c_{k})$ such that $\\delta_{k}(0)=\\theta_{k}$ and $\\delta_{k}(1)=\\theta_{k}^{\\prime}$ . If $c_{k}<0$ for $k\\in K:=\\{k_{1},\\ldots,k_{l_{-}}\\}\\subseteq\\{1,\\ldots,l\\}$ , we define the curve $\\gamma_{i;\\theta}:[0,1]\\rightarrow\\mathcal{Q}(c_{k_{i}})$ with $\\gamma_{i;\\theta}(t)=\\left((1-t)W_{k_{i}1}^{(1)},\\dots,(1-t)W_{k_{i}l}^{(1)},s(\\theta)_{i}\\sqrt{-c_{k_{i}}+(1-t)^{2}\\left\\|W_{k_{i}}^{(1)}\\right\\|_{F}^{2}}\\right)$ ", "page_idx": 18}, {"type": "text", "text": "for every $i=1,\\ldots,l_{-}$ .   \n$\\gamma_{i;\\theta}$ is a continuous curve which connects the point $\\gamma_{i;\\theta}(0)=\\theta_{k_{i}}$ with $\\gamma_{i;\\theta}(1)=(0,s(\\theta)_{i}\\sqrt{-c_{k_{i}}})$ . ", "page_idx": 18}, {"type": "text", "text": "If we define $\\bar{\\gamma}_{i;\\theta}(t):=\\,\\gamma_{i;\\theta}(1-t)$ , which is the same curve as $\\gamma_{i;\\theta}$ but traversed in the opposite direction, we can define the curve ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\delta_{k_{i}}(t)=\\bar{\\gamma}_{i;\\theta^{\\prime}}\\gamma_{i;\\theta}(t)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "i.e. the curve which travels on $\\gamma_{i;\\theta}$ for $t\\in\\left[0,\\frac{1}{2}\\right]$ and on $\\bar{\\gamma}_{i;\\theta^{\\prime}}$ for $t\\in[\\textstyle{\\frac{1}{2}},1]$ , for $i=1,\\ldots,l_{-}$ . ", "page_idx": 18}, {"type": "text", "text": "Notice now that $\\delta_{k_{i}}(0)=\\theta_{k_{i}}$ and $\\delta_{k_{i}}(1)=\\theta_{k_{i}}^{\\prime}$ . Moreover $\\delta_{k_{i}}$ is continuous because ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma_{i;\\theta}(1)=(0,s(\\theta)_{i}\\sqrt{-c_{k_{i}}})=\\big(0,s(\\theta^{\\prime})_{i}\\sqrt{-c_{k_{i}}}\\big)=\\bar{\\gamma}_{i;\\theta^{\\prime}}(0)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "under the hypothesis that $s(\\theta)=s(\\theta^{\\prime})$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, we found a continuous curve $\\delta\\;=\\;\\left(\\delta_{1},\\ldots,\\delta_{l}\\right)$ such that $\\delta(t)\\;\\in\\;\\mathcal{H}(c)\\;\\;\\forall t\\;\\in\\;[0,1]$ and $\\delta(0)=\\theta$ , $\\delta(1)=\\theta^{\\prime}$ . Therefore, $\\theta$ and $\\theta^{\\prime}$ belong to the same connected component. ", "page_idx": 18}, {"type": "text", "text": "Let us now prove that if $\\theta$ and $\\theta^{\\prime}$ belong to the same connected component, then $s(\\theta)=s(\\theta^{\\prime})$ . Let $\\gamma:[0,1]\\to\\mathcal{H}(c)$ be a continuous curve in $\\mathcal{H}(c)$ such that $\\gamma(0)=\\theta$ and $\\gamma(1)=\\theta^{\\prime}$ . For each $k\\in K=\\{k_{1},\\ldots,k_{l_{-}}\\}$ such that $c_{k}<0$ , we know that $\\gamma_{k}(t)\\in\\mathcal{Q}(c_{k})$ means that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma_{k}\\big(t\\big)=\\left(\\gamma_{k1}^{(1)}(t),\\dots,\\gamma_{k d}^{(1)}(t),\\underbrace{s_{k}(t)\\sqrt{-c_{k}+\\left\\|\\gamma_{k}^{(1)}\\right\\|_{F}^{2}}}_{\\gamma_{k}^{(2)}(t)}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some function $s_{k}(t)\\in\\{-1,1\\}$ such that $s_{k}(0)=s(\\theta)_{k}$ and $s_{k}(1)=s(\\theta^{\\prime})_{k}$ . ", "page_idx": 18}, {"type": "text", "text": "Assume, by contradiction, that $s(\\theta)_{k}=-s(\\theta^{\\prime})_{k}$ . Assume also that $s(\\theta)_{k}=+1$ and $s(\\theta^{\\prime})_{k}=-1$ . ", "page_idx": 18}, {"type": "text", "text": "Notice that $c_{k}<0$ implies that $p(t):=\\sqrt{-c_{k}+\\left\\|\\gamma_{k}^{(1)}\\right\\|_{F}^{2}}>0.$ ", "page_idx": 18}, {"type": "text", "text": "The function $\\gamma_{k}^{(2)}(t)=s_{k}(t)p(t)$ , then, is a continuous function such that $\\gamma_{k}^{(2)}(0)>0$ and $\\gamma_{k}^{(2)}(1)<$ 0 and thus, by the intermediate value theorem, there exists $t_{*}\\in(0,1)$ such that $\\gamma_{k}^{(2)}(t_{*})=0$ . But $\\gamma_{k}^{(2)}(t)\\neq0$ for every $t_{\\cdot}$ , as $s_{k}(t)\\in\\{-1,1\\}$ and $\\sqrt{-c_{k}+\\left\\|\\gamma_{k}^{(1)}\\right\\|_{F}^{2}}>0.$ ", "page_idx": 18}, {"type": "text", "text": "Repeating the argument for $s(\\theta)_{k}=-1$ we then prove by contradiction that $s(\\theta)_{k}=s(\\theta^{\\prime})_{k}$ for all $k\\in K$ , thus concluding the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D.5 Proof of Proposition 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We have by Equation (10) and Equation (2): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle\\!\\langle T_{\\alpha}(\\theta),T_{\\alpha}(\\theta)\\rangle\\!\\rangle_{k}-c_{k}=0\\iff\\alpha_{k}^{2}\\sum_{i=1}^{d}(W_{k i}^{(1)})^{2}-\\frac{1}{\\alpha_{k}^{2}}\\sum_{j=1}^{e}(W_{j k}^{(2)})^{2}-c_{k}=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By renaming $A=\\sum_{i=1}^{d}(W_{k i}^{(1)})^{2}=\\left\\|W_{k}^{(1)}\\right\\|_{F}^{2},C=\\sum_{j=1}^{e}(W_{j k}^{(2)})^{2}=\\left\\|W_{k}^{(2)}\\right\\|_{F}^{2}$ and multiplying by $\\alpha_{k}^{2}>0$ we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nA\\alpha_{k}^{4}-c_{k}\\alpha_{k}^{2}-C=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Solving for $\\alpha_{k}^{2}$ gives us: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta=c_{k}^{2}+4A C\\geq4A C>0}}\\\\ {{\\alpha_{k}^{2}=\\frac{c_{k}\\pm\\sqrt{\\Delta}}{2A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given tha\u221at we want $\\alpha_{k}>0$ , we discard the negative solution. The other is positive because $\\Delta>c_{k}^{2}$ and thus $\\sqrt{\\Delta}>|c_{k}|$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\pm\\sqrt{\\frac{c_{k}+\\sqrt{\\Delta}}{2A}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Of which we keep the positive solution only, with its full expression being: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\sqrt{\\frac{c_{k}+\\sqrt{c_{k}^{2}+4\\Big\\|W_{k}^{(1)}\\Big\\|_{F}^{2}\\Big\\|W_{k}^{(2)}\\Big\\|_{F}^{2}}}{2\\Big\\|W_{k}^{(1)}\\Big\\|_{F}^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, if $\\boldsymbol{\\alpha}=\\left(\\alpha_{1},\\dots,\\alpha_{l}\\right)$ with $\\alpha_{k}$ given by Equation (23), we get that $\\langle\\!\\langle T_{\\alpha}(\\theta),T_{\\alpha}(\\theta)\\rangle\\!\\rangle_{k}=c_{k}\\ \\forall k=$ $1,\\ldots,l$ . ", "page_idx": 19}, {"type": "text", "text": "Let us consider now the pathological cases $W_{k}^{(1)}=0$ or $W_{k}^{(2)}=0$ . ", "page_idx": 19}, {"type": "text", "text": "$W_{k}^{(1)}=0,W_{k}^{(2)}\\neq0$ then $A=0,C\\neq0$ . Therefore, we have that Equation (22) becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n-c_{k}\\alpha_{k}^{2}-C=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which has solutions if and only if $c_{k}<0$ . In that case $\\alpha_{k}\\:=\\:\\frac{\\left\\|W_{k}^{(2)}\\right\\|_{F}}{\\sqrt{-c_{k}}}$ \u2225W\u221a k2ck\u2225F . This means that a hidden neuron with zero input weights and nonzero output weights can \u2212be rescaled only to the invariant hyperquadrics with $c_{k}<0$ . ", "page_idx": 19}, {"type": "text", "text": "If $W_{k}^{(2)}=0,W_{k}^{(1)}\\neq0$ then $C=0,A\\neq0$ . Therefore, we have that Equation (22) becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{k}^{2}A-c_{k}=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which has solutions if and only if $c_{k}>0$ . In that case $\\begin{array}{r}{\\alpha_{k}\\,=\\,\\frac{\\sqrt{c_{k}}}{\\left\\|W_{k}^{(1)}\\right\\|_{F}}}\\end{array}$ . This means that a hidden neuron with zero output weights and nonzero input weights can be rescaled only to the invariant hyperquadrics with $c_{k}>0$ . ", "page_idx": 19}, {"type": "text", "text": "Ihfy $W_{k}^{(1)}\\,=\\,0$ c .and $W_{k}^{(2)}\\,=\\,0$ , then $\\theta_{k}\\,=\\,0\\,\\in\\,\\mathcal{Q}(0)$ and it cannot be rescaled to any other invariant ", "page_idx": 19}, {"type": "text", "text": "D.6 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let us first prove that, if for every $\\theta\\:\\in\\:C$ there exists a $\\theta^{\\prime}\\,\\in\\,C^{\\prime}$ such that $\\theta\\stackrel{\\mathrm{~rp~}}{\\sim}\\theta^{\\prime}$ then $\\sum_{i=1}^{l_{-}}s(\\theta)_{i}=\\sum_{i=1}^{l_{-}}s(\\theta^{\\prime})_{i}.$ . ", "page_idx": 19}, {"type": "text", "text": "First, Lemma 3 tells us that we can interchange rescaling and permutation if we permute the rescaling factors accordingly. This means we can reduce any composite action of rescalings and permutations to the action of a single rescaling and a single permutation. ", "page_idx": 19}, {"type": "text", "text": "Let $\\theta\\in C$ and $\\theta^{\\prime}\\in C^{\\prime}$ such that $\\theta\\stackrel{\\mathrm{rp}}{\\sim}\\theta^{\\prime}$ . Then there exist $\\alpha\\in\\mathbb{R}_{+}^{l}$ and $\\pi\\in\\mathfrak{S}_{l}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{\\alpha}P_{\\pi}(\\theta)=\\theta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This means that $T_{\\alpha}P_{\\pi}(\\theta)$ and $\\theta^{\\prime}$ belong to the same invariant set and, specifically, to the same connected component. Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(T_{\\alpha}P_{\\pi}(\\theta))=s(\\theta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(T_{\\alpha}P_{\\pi}(\\theta))=s(P_{\\pi}(\\theta))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "because $T_{\\alpha}$ does not change the sign of $W^{(2)}$ as it acts by scaling it by positive factors. Let us focus on ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(P_{\\pi}(\\boldsymbol{\\theta}))=\\mathrm{sign}\\big((\\boldsymbol{W}^{(2)}R_{\\pi}^{\\intercal})_{-}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If the neurons of $\\theta$ such that $c_{k}<0$ are indexed by $k_{1},k_{2},\\ldots,k_{l_{-}}$ , we will have that the neurons of $P_{\\pi}(\\theta)$ such that $c_{k}<0$ are indexed by $\\pi(k_{1}),\\pi(k_{2}),\\ldots,\\pi(k_{l_{-}})$ . Therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(P_{\\pi}(\\boldsymbol{\\theta}))_{i}=\\mathrm{sign}(W_{\\pi(k_{i})}^{(2)})=(s(\\boldsymbol{\\theta})R_{\\pi_{-}}^{\\intercal})_{i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some permutation $\\pi_{-}\\in\\mathfrak{S}_{l_{-}}$ . Therefore, Equation (24) implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(\\theta^{\\prime})=s(P_{\\pi}(\\theta))=s(\\theta)R_{\\pi_{-}}^{\\top}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The action of rescaling and permutation can only reshuffle the label $s$ of the connected component. This means that ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(\\theta)R_{\\pi_{-}}^{\\top}=s(\\theta^{\\prime})\\implies\\sum_{i=1}^{l_{-}}(s(\\theta)R_{\\pi_{-}}^{\\top})_{i}=\\sum_{i=1}^{l_{-}}s(\\theta^{\\prime})_{i}\\implies\\sum_{i=1}^{l_{-}}s(\\theta)_{i}=\\sum_{i=1}^{l_{-}}s(\\theta^{\\prime})_{i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us now prove the other implication. Let $s,s^{\\prime}\\in\\mathbb{R}^{l_{-}}$ such that $\\sum_{i=1}^{l_{-}}s_{i}=\\sum_{i=1}^{l_{-}}s_{i}^{\\prime}$ ", "page_idx": 20}, {"type": "text", "text": "Given that their sum is equal, $s$ and $s^{\\prime}$ have the same number of $+1$ and $-1$ and thus there exists a permutation $\\pi_{-}\\in\\mathfrak{S}_{l_{-}}$ such that $s^{\\prime}=s R_{\\pi_{-}}^{\\intercal}$ . ", "page_idx": 20}, {"type": "text", "text": "Let $\\pi\\in\\mathfrak{S}_{l}$ be the permutation which permutes the neurons such that $c_{k}<0$ according to $\\pi_{-}$ and leaves the others fixed. In this way $s(\\dot{P_{\\pi}}(\\theta))=s R_{\\pi_{-}}^{\\intercal}=s^{\\prime}$ . ", "page_idx": 20}, {"type": "text", "text": "$P_{\\pi}(\\theta)$ , however, doesn\u2019t belong to $\\mathcal{H}(c)$ but to another invariant set given by $\\mathcal{H}(R_{\\pi}c)$ . ", "page_idx": 20}, {"type": "text", "text": "Applying Proposition 4 we can find a rescaling $\\alpha=\\alpha(\\pi)\\in\\mathbb{R}_{+}^{l}$ such that $T_{\\alpha}P_{\\pi}(\\theta)\\in\\mathcal{H}(c)$ . ", "page_idx": 20}, {"type": "text", "text": "Since neurons such that $c_{k}\\geq0$ , are left unchanged by the permutation, we can apply the proposition ones whose weights satisf $\\alpha_{k}=1$ $\\left\\|W_{k}^{(1)}\\right\\|_{F}^{2}-\\left\\|W_{k}^{(2)}\\right\\|_{F}^{2}<0$ , meaning that $W_{k}^{(2)}\\neq0$ $c_{k}<0$ ", "page_idx": 20}, {"type": "text", "text": "As noted above, the action of the rescaling doesn\u2019t change the sign vector, and thus ", "page_idx": 20}, {"type": "equation", "text": "$$\ns(T_{\\alpha}P_{\\pi}(\\theta))=s(P_{\\pi}(\\theta))=s^{\\prime}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If we name $\\theta^{\\prime}:=T_{\\alpha}P_{\\pi}(\\theta)$ this result means that we found a $\\theta^{\\prime}\\stackrel{\\mathrm{rp}}{\\sim}\\theta$ such that $\\theta^{\\prime}\\in C^{\\prime}$ , thus concluding the proof. ", "page_idx": 20}, {"type": "text", "text": "E Including biases ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let us consider the case where we include biases. The resulting two-layer neural network can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(x;\\theta)=W^{(2)}\\sigma(W^{(1)}x+b^{(1)})+b^{(2)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $b^{(1)}\\in\\mathbb{R}^{l}$ and $b^{(2)}\\in\\mathbb{R}^{e}$ . ", "page_idx": 20}, {"type": "text", "text": "To work with this extended set of parameters, we re-define the space ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta=\\left\\{\\theta=\\big(W^{(1)},b^{(1)},W^{(2)},b^{(2)}\\big)\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the single hidden neuron spaces ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta_{k}=\\left\\{\\theta_{k}=\\big(e_{k k}W_{k}^{(1)},e_{k k}b_{k}^{(1)},W_{k}^{(2)}e_{k k}\\big)\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second bias term $b^{(2)}$ does not appear because it is not directly involved with the computations of the hidden neurons. This means that we can write ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta\\cong\\Theta_{1}\\times\\cdots\\times\\Theta_{l}\\times\\mathbb{R}^{e}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbb{R}^{e}$ is included to describe the parameters in $b^{(2)}$ . ", "page_idx": 21}, {"type": "text", "text": "The neuron rescaling action now acts on the biases $b^{(1)}$ as well as the weights: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\colon\\mathbb{R}_{+}\\times\\Theta_{k}\\rightarrow\\Theta_{k}}\\\\ &{\\quad\\quad\\big(\\alpha,\\theta_{k}\\big)\\mapsto T_{\\alpha}\\big(\\theta_{k}\\big)=\\big(\\alpha W_{k}^{(1)},\\alpha b_{k}^{(1)},\\frac1\\alpha W_{k}^{(2)}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and can be extended to the whole space of parameters ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T\\colon\\;\\mathbb{R}_{+}^{l}\\times\\Theta\\quad\\to\\quad\\Theta}\\\\ {(\\alpha,\\theta)\\quad\\mapsto\\quad T_{\\alpha}(\\theta)=(\\mathrm{diag}(\\alpha)W^{(1)},\\mathrm{diag}(\\alpha)b^{(1)},W^{(2)}\\mathrm{diag}(\\alpha)^{-1},b^{(2)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Once again, we find that $T_{\\alpha}\\theta\\sim\\theta$ . ", "page_idx": 21}, {"type": "text", "text": "In this more general case, we can rewrite the bilinear form to include the biases. If $\\theta\\ =$ $\\left(W^{(1)},b^{(1)},\\bar{W^{(2)}},b^{(2)}\\right)$ and $\\eta=\\big(V^{(1)},p^{(1)},V^{(2)},p^{(2)}\\big)$ , we define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\langle\\!\\langle\\theta,\\eta\\rangle\\!\\rangle_{k}=\\sum_{i=1}^{d}W_{k i}^{(1)}V_{k i}^{(1)}+b_{k}^{(1)}p_{k}^{(1)}-\\sum_{j=1}^{e}W_{j k}^{(2)}V_{j k}^{(2)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and see that, once gradient flow optimization, we have a conservation condition like the one of Equation (9) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\langle\\!\\langle\\theta(t),\\theta(t)\\rangle\\!\\rangle_{k}=c_{k}\\forall t>0\\ \\forall k=1,\\ldots,l.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Once again, we call $\\mathcal{Q}(c_{k})$ the hypersurface of $\\Theta_{k}$ which satisfies the equation $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}\\,=\\,c_{k}$ and $\\mathcal{H}(c_{k})$ the set in $\\Theta$ defined by $\\langle\\!\\langle\\theta,\\theta\\rangle\\!\\rangle_{k}=c_{k}\\ \\forall k=1,\\ldots,l$ . ", "page_idx": 21}, {"type": "text", "text": "With this in mind, it is not hard to extend the results of Proposition 2 and Theorem 1 which turn out to be slightly modified. ", "page_idx": 21}, {"type": "text", "text": "Proposition 5. If $\\ '_{c_{k}}>0,\\;\\mathcal{Q}(c_{k})$ is a topological manifold homeomorphic to $\\mathbb{R}^{e}\\times S^{d}$ . If $c_{k}<0,$ , $\\mathcal{Q}(c_{k})$ is a topological manifold homeomorphic to $\\mathbb{R}^{d}\\times S^{e-1}$ . $\\mathit{I f}\\,c_{k}=0,\\,Q(0)$ is contractible. ", "page_idx": 21}, {"type": "text", "text": "In this case, we can factor the space of parameters $\\mathcal{H}(c)$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}(c)\\cong\\mathcal{Q}(c_{1})\\times\\cdots\\times\\mathcal{Q}(c_{l})\\times\\mathbb{R}^{e},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last factor is due to the freedom in choosing the values of $b^{(2)}$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition 6. Let $c_{k}\\neq0\\;\\forall k=1,\\ldots,l.$ . Let $l_{+},l_{-},l_{0}$ be the number of positive, negative and zero elements of c, respectively. The Poincar\u00e9 polyn+om\u2212ial of $\\mathcal{H}(c)$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{\\mathcal{H}(c)}(x)=\\bigl(1+x^{d}\\bigr)^{l_{+}}\\bigl(1+x^{e-1}\\bigr)^{l_{-}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Corollary 4. The 0-th Betti number $\\beta_{0}(c)$ of $\\mathcal{H}(c)$ , corresponding to the number of its connected components, is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\beta_{0}(c)=\\left\\{1\\atop2^{l-}\\right.\\quad i f\\,e>1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The result we obtain is similar to Corollary 1 although slightly modified by the fact that having a single input neuron does not cause $\\mathcal{H}(c)$ to become disconnected anymore. In the case of $e=1$ , therefore, the picture presented in the main text is left unchanged. ", "page_idx": 21}, {"type": "image", "img_path": "3hcn0UxP72/tmp/2cc0dc5731612002d7270e6b802ff0da7532bcc34bac2e799bfce8a738216d6d.jpg", "img_caption": ["Figure 5: Probability of the topological obstruction as a function of the number of input $d$ and hidden $l$ neurons, when the initial weights are sampled with Xavier normal (left) and Kaiming normal (right) initialization schemes. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F Probability of obstruction ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let us consider the following question: what is the probability of having a disconnected invariant set given a realistic initialization? ", "page_idx": 22}, {"type": "text", "text": "Consider a one-layer ReLU neural network with $e=1$ and assume that the weights are sampled independently of one another from a normal distribution $W_{k i}^{(1)}\\sim\\mathcal{N}(0,\\sigma_{1}^{2})\\;\\forall k,i\\;,\\bar{W_{k}^{(2)}}\\sim\\mathcal{N}(0,\\bar{\\sigma_{2}^{2}})\\;\\forall k,j$ From Corollary 2 we know that the invariant set $\\mathcal{H}(c)$ will be disconnected if and only if there exists a hidden neuron satisfying $\\Sigma_{i=1}^{d}(W_{k i}^{(1)})^{2}\\,<\\,(W_{k}^{(2)})^{2}$ . Given independence of the initial weight sampling, this probability can b=e computed as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathsf{o b s t r u c t i o n}]=1-\\mathbb{P}[\\sum_{i=1}^{d}(W_{k i}^{(1)})^{2}>(W_{k}^{(2)})^{2}]^{l}=1-\\big(F_{1,d}\\big(d\\sigma_{1}^{2}/\\sigma_{2}^{2}\\big)\\big)^{l},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $F$ is the cumulative distribution function of the Fisher-Snedecor distribution. ", "page_idx": 22}, {"type": "text", "text": "Having obtained this general expression, we can specify it to two common initialization schemes. ", "page_idx": 22}, {"type": "text", "text": "\u2022 We obtain Kaiming initialization [21] with $\\sigma_{1}^{2}=2/d,\\sigma_{2}^{2}=2/l$ resulting in P[obstruction] = $1-F_{1,d}(l)^{l}$ .   \n\u2022 We obtain Xavier normal initialization [19] with $\\sigma_{1}^{2}=2/(d+l),\\sigma_{2}^{2}=2/(1+l)$ resulting in $\\begin{array}{r}{\\mathbb{P}[\\mathrm{obstruction}]=1-F_{1,d}(\\frac{d+l d}{d+l})^{l}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "We plot these two expressions in Figure 5. We can see how, for large values of $d$ , the probability of obstruction quickly falls to 0 for any number of hidden neurons. Instead, we see an opposite trend for small values of $d$ : the probability of disconnectedness grows with $l$ . Moreover, it is interesting to notice that the region of high obstruction probability is much larger for Xavier initialization than for Kaiming initialization, further showing why the latter is preferred when working with ReLU networks. ", "page_idx": 22}, {"type": "text", "text": "G Intuition on the occurrence of obstruction ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We can give some intuition on why there is no obstruction for multiple outputs. First, we consider a single hidden neuron $k$ , with $d$ incoming weights and a single output $e\\,=\\,1$ . If the neuron is pathological, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}\\left(W_{k i}^{(1)}\\right)^{2}<\\left(W_{1k}^{(2)}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the weights W i(ja )(t) are continuous curves in time, for W 1(k2) to change sign, its value needs to pass through 0 but, under the condition above, this cannot happen its square is always positive. ", "page_idx": 22}, {"type": "text", "text": "Consider now multiple outputs $e>1$ , resulting in the conservation condition being ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}\\left(W_{k i}^{(1)}\\right)^{2}<\\sum_{j=1}^{e}\\left(W_{j k}^{(2)}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, any component W j(k2) can change sign by passing through 0 because the other components can compensate for it by increasing their magnitude to keep the condition satisfied. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the claims in the abstract are supported by analytical results and numerical experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations of our analysis in Section 8. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the assumptions of the theoretical results are clearly specified and the complete proofs are written in Appendix D. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The \u201cTask, dataset and model setup.\u201d paragraph of Section 6 clearly explains our experimental setup. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Given the simplicity of our setup, we think that releasing the code would be unnecessary. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the experiment\u2019s details are explained in Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our simple experiment has the goal of displaying the meaning of the analytical results and, thus, we think it doesn\u2019t require error bars. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our experiment was performed on a simple laptop. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We reviewd the NeurIPS Code of Ethics and confirm that we conform to it in every repect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Being a purely theoretical work, we believe that it doesn\u2019t havesignificantt social impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We release no data or models that have a high risk for misuse. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We use no existing assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We release no new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]