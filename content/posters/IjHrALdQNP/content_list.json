[{"type": "text", "text": "GAMap: Zero-Shot Object Goal Navigation with Multi-Scale Geometric-Affordance Guidance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuaihang Yuan\u22171,2,4, Hao Huang\u22172,4, Yu Hao2,3,4, Congcong Wen2,4 ", "page_idx": 0}, {"type": "text", "text": "Anthony Tzes1,2, Yi Fang \u2020 1,2,3,4 ", "page_idx": 0}, {"type": "text", "text": "1NYUAD Center for Artificial Intelligence and Robotics (CAIR), Abu Dhabi, UAE. 2New York University Abu Dhabi, Electrical Engineering, Abu Dhabi 129188, UAE. 3New York University, Electrical & Computer Engineering Dept., Brooklyn, NY 11201, USA. 4Embodied AI and Robotics (AIR) Lab, NYU Abu Dhabi, UAE. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zero-Shot Object Goal Navigation (ZS-OGN) enables robots or agents to navigate toward objects of unseen categories without object-specific training. Traditional approaches often leverage categorical semantic information for navigation guidance, which struggles when only objects are partially observed or detailed and functional representations of the environment are lacking. To resolve the above two issues, we propose Geometric-part and Affordance Maps (GAMap), a novel method that integrates object parts and affordance attributes as navigation guidance. Our method includes a multi-scale scoring approach to capture geometric-part and affordance attributes of objects at different scales. Comprehensive experiments conducted on HM3D and Gibson benchmark datasets demonstrate improvements in Success Rate and Success weighted by Path Length, underscoring the efficacy of our geometricpart and affordance-guided navigation approach in enhancing robot autonomy and versatility, without any additional object-specific training or fine-tuning with the semantics of unseen objects and/or the locomotions of the robot. Our project is available at https://shalexyuan.github.io/GAMap/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zero-Shot Object Goal Navigation (ZS-OGN) is a pivotal research domain in embodied AI and robotics, enabling robots to navigate towards the objects of unseen categories without training or fine-tuning on these objects [1, 2, 3, 4, 5]. This capability is crucial for real-world robots, such as home service robots and blind guiding robots, allowing them to interact with diverse objects in real-world scenarios, thereby enhancing their autonomy and versatility. ", "page_idx": 0}, {"type": "text", "text": "Prior works on ZS-OGN either leverage deep neural networks to directly map RGB-D observations to actions learned from paired training data [6, 7, 8, 9, 10, 11] or utilize map-based navigation methods [12, 13, 14, 15, 16, 17, 3]. However, deep neural network approaches often struggle due to their dependence on extensive annotated data, resulting in poor generalization to unseen environments [18, 16], while map-based navigation methods instead offer an alternative. Map-based navigation methods track categorical semantics and topological information observed by the agent to select promising exploration locations [17]. With the advent of foundation models, the studies [3, 17, 19] have exploited the reasoning capabilities of Large Language Models (LLMs) to strategically select waypoints by analyzing commonsense, such as object co-occurrence relationships, to navigate robots towards the target. However, LLM-based approaches require converting visual and semantic information into categorical descriptions, which leads to a loss of spatial and visual information [9]. Vision Language Models (VLMs) enhance semantic reasoning capabilities, but still rely on maps that encompass only categorical information [16]. The primary limitation of exclusively relying on categorical information is that such maps treat objects as monolithic entities, disregarding local geometric features. This becomes particularly problematic when only the target object is partially observed, leading to incorrect categorical information and potential errors in waypoint selection. ", "page_idx": 0}, {"type": "image", "img_path": "IjHrALdQNP/tmp/f39c1c09744583896bf1f8ba7f75e16fd3aaef621dc447a445f2673fa95435ad.jpg", "img_caption": ["Figure 1: The leftmost RGB image shows the same observation for both methods. Our method (top row) effectively identifies the geometric part of the chair back, which is missed by the traditional method (bottom row). Consequently, GAMap successfully guides the agent to the target object, while the traditional method fails. The red circles highlight the areas where the chair is located, and the GA score is high, indicating the effectiveness of our approach in localizing relevant regions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We argue that using a categorical map for robot exploration is suboptimal, as it discards intricate geometric details and functional representations of the environment, as illustrated in Figure 1. Drawing inspiration from human cognitive processes \u2014 where distinctive geometric parts are often identified first when locating an object in an unfamiliar environment [20, 21] \u2014 we propose Geometric-part and Affordance Maps (GAMap), a zero-shot approach, for the geometric parts and affordance attribute driven semantic navigation to explore and find the target object in an unseen environment. Specifically, given a target object, our proposed method starts by using an LLM to infer the object\u2019s geometric parts and potential affordance attributes, aiming at providing a detailed understanding of both the object\u2019s physical structure and its functional properties. Given depth observations, GAMap maintains a 2D projection of obstacles and explored areas. Instead of relying on object detection and prompt engineering of LLMs to select the next area to explore, our approach employs a pre-trained CLIP [22] to score observations based on their similarity to the reasoned geometric parts and affordance attributes, guiding the exploration process. To construct the proposed GAMap, which requires obtaining scores for geometric parts at different scales, we further propose a Multi-scale Geometric and Affordance score (GA score). Such an integration addresses a notable limitation in existing approaches that compute similarity at a single scale, which usually results in the oversight of fine-grained details of an object, as such details are potentially essential for an accurate identification of geometric parts or affordance attributes of objects with different sizes. ", "page_idx": 1}, {"type": "text", "text": "Our proposed method is evaluated on HM3D [23] and Gibson [24] benchmarks and achieves significant improvements in Success Rate (SR) of $26.4\\%$ on HM3D [23] and $23.7\\%$ on Gibson [24] compared to previous approaches. Additionally, we achieve substantial gains in Success weighted by Path Length (SPL) of $37\\%$ on Gibson, highlighting the efficiency and effectiveness of our proposed geometric parts and affordance guided navigation. The contributions of our method are mainly summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a novel Geometric-part and Affordance Map (GAMap) for ZS-OGN using object part and affordance attributes as guidance. To the best of our knowledge, this is the first work to study the integration of these attributes in ZS-OGN. ", "page_idx": 1}, {"type": "text", "text": "2. Recognizing that geometric parts and affordance attributes often relate to multiple scales of an object, we propose a Multi-scale Geometric and Affordance score, which allows GAMap to be constructed in real-time, better capturing these attributes at different scales.   \n3. We achieve state-of-the-art performance on two navigation benchmark datasets without any training or fine-tuning with the semantics of unseen objects and/or the locomotions of the robot, which demonstrates the effectiveness of our method in unseen environments. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Semantic Mapping. In the context of object goal navigation, it is crucial to transform observations into structured information for effective decision-making. Frontier-based methods [3, 16, 19] utilize categorical semantic information near frontiers to select exploration areas. Additionally, graph-based mapping methods [17, 25, 18] predict waypoints from RGB-D images or simplified maps to create topological representations of the environment. Most of the aforementioned works rely on semantic segmentation or object detection to build semantic maps, which are constrained by the pre-defined semantic classes and thus fail to capture the full semantic richness of environments [18, 26]. To overcome these limitations, recent approaches like VLMaps [18] have introduced open-vocabulary semantic maps, enabling natural language indexing and expanding the scope of semantic mapping. In addition, previous works [27, 28] attempt to utilize attributes and long descriptions for object perception. While these methods have advanced the navigation field, they often overlook object parts, treating objects as monolithic entities and leading to errors when these objects are partially observed. Inspired by human cognitive processes [21, 20], where distinctive geometric parts are identified first in unseen environments, we propose Geometric-part and Affordance Maps (GAMap). Unlike previous methods focusing solely on categorical information, GAMap integrates geometric parts and affordance attributes, providing a richer and more functional representation of the environment. ", "page_idx": 2}, {"type": "text", "text": "Zero-shot Object Goal Navigation. In the context of object goal navigation, the aim is to efficiently explore a new environment while searching for a target object that is not directly visible. Previous research relies heavily on visual context via imitation [29, 6] or reinforcement learning [7, 8, 9] to guide robots. These approaches often require extensive data collection and annotation for training, which limits their practical application in real-world environments. Thus, the focus in object goal navigation has been shifting towards zero-shot object navigation, which aims to equip robots with the ability to adapt to unseen objects and environments without the need for training [12, 30, 13, 14]. Clip-Nav [31] utilizes CLIP [22] to execute vision-and-language navigation in a zero-shot scheme, whilst CoW [32] employs CLIP for object goal navigation. Recently, Frontier-based Exploration (FbE) [33] is widely adopted in navigation by moving the robot to the frontier between known and unknown spaces [32, 34, 35, 36, 37], leading to promising performance compared to learning-based exploration methods [38, 39]. More recently, ESC [3] leverages the reasoning ability of LLMs to select frontiers using pre-defined rules. Chen et al. [15] explore frontier selection by jointly considering the shortest path to frontiers and the relevance scoring between objects for exploration. To enable more robust and reliable exploration and waypoint selection, Wu et al. [17] propose a Voronoi-based scene graph for waypoint selection. Unlike the above methods that use the reasoning ability of LLMs to select frontiers, VLFM [16] introduces a value map to score frontiers based on the categorical similarity between the observation and the target object. In contrast to prior work, for the first time, we explore robot navigation using geometric parts and affordance attributes as guidance. This approach integrates detailed geometric parts and functional properties of objects, offering a more comprehensive strategy for navigation. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first formalize the ZS-OGN problem in Section 3.1. Then, we detail our method, as shown in Figure 2, from four phases: attribute generation in Section 3.2, multi-scale attribute scoring in Section 3.3, GAMap generation in Section 3.4, and exploration policy in Section 3.5. Initially, our method generates geometric parts and affordance attributes for the target object. During the exploration, the method computes a multi-scale attribute score from the RGB observations collected by the agent. These scores are then mapped onto a 2D geometric parts and affordance map, which is pivotal in guiding the exploration process. Subsequently, the agent selects the location with the highest score for further exploration. ", "page_idx": 2}, {"type": "image", "img_path": "IjHrALdQNP/tmp/b7be6c7af8336dbafb334dc656b4f7feb786f6457331abc3432ea64ed9255e3a.jpg", "img_caption": ["Figure 2: Pipeline of the GAMap generation. Geometric parts and affordance attributes are generated by an LLM. The RGB observation is partitioned into multiple scales, and a CLIP visual encoder generates multi-scale visual embeddings. GA scores are computed using cosine similarity between attribute text embeddings from a CLIP text encoder and the multi-scale visual embeddings. These scores are averaged and projected onto a 2D grid to form the GAMap. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In ZS-OGN, the robot must navigate to a target object $g_{i}$ that has never been encountered before in an unseen environment $s_{i}$ , without any training on $g_{i}$ and $s_{i}$ . A navigation episode can be defined as $\\mathcal{E}_{i}\\,=\\,\\{g_{i},s_{i},p_{0}\\}$ , where $p_{0}$ denotes the robot\u2019s initial location, the subscript $i$ refer to the $i^{t h}$ episode. The robot receives a color image $I_{t}$ , a depth image $D_{t}$ , and its pose, i.e., position $(x_{t},y_{t})$ and orientation $\\theta_{t}$ , at each exploration step $t$ . We denote these readings as an observation $\\mathcal{O}_{t}=\\left\\{I_{t},D_{t},x_{t},y_{t},\\theta_{t}\\right\\}$ . The agent accumulates pose readings over time to determine its relative position $p_{t}$ . Based on the readings at each step, the robot needs to select an action $a_{t}$ from the action space. A navigation episode is marked as successful if the robot executes the STOP action within a pre-defined distance to the target object. In this work, we approach the navigation task as a sequence of decisions made by the robot. The process starts at the initial time step $t=0$ and ends at the final time step $T$ . This final step is either when the robot executes the STOP action or when a pre-defined maximum number of exploration steps is reached. ", "page_idx": 3}, {"type": "text", "text": "3.2 Attribute Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We focus on two types of attributes essential for object recognition: Affordance and Geometric-part attributes. Affordance attributes refer to the potential actions that an object facilitates [40], which are crucial for understanding how an agent might interact with different objects within an environment. Geometric-part attributes, on the other hand, describe the shape and spatial characteristics of an object, aiding in its visual identification and differentiation from other objects. ", "page_idx": 3}, {"type": "text", "text": "To extract these attributes, we employ an LLM to reason about the target object\u2019s characteristics. Specifically, we utilize GPT-4V [41] for the attributes generation. We initiate this process by setting the system prompt as: \u201cI am a highly intelligent question-answering bot, and I answer questions from a human perspective.\u201d Subsequently, we employ two tailored prompts to extract the desired attributes. For affordance attributes, we design prompt as: \u201cFor the target object <target object $g_{i}\\!>_{;}$ , please provide ${<}N_{a}{>}$ affordance attributes that to the most reflect its characteristics.\u201d to query $N_{a}$ number of affordance attributes. For geometric parts, the prompt is: \u201cSummarize ${<}N_{g}{>}$ geometric part visual features of the <target object $g_{i}{>}$ which are typically used to identify why it is $a$ <target object $g_{i}\\!>$ .\u201d to get $N_{g}$ number of geometric attributes. Once the set of affordances attributes $\\{A_{n}\\}_{n=1}^{N_{a}}$ and gweitohm tehtrei ca gatetnritb\u2019su toesb $\\{G_{n}\\}_{n=1}^{N_{g}}$ hbayv ec obemepnu itidnegn tiGfeieod,m tehter ince-pxta rstt aagned i nAvfoflovredsa cnocue pslicnogr etsh e(sGe Aat tsricbourteess) through multi-scale visual features, as detailed in the following. ", "page_idx": 3}, {"type": "text", "text": "3.3 Multi-scale Attribute Scoring ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The key idea of Multi-scale Attribute Scoring is to quantify the relevance of observed areas\u2019 geometric and affordance characteristics in locating the target object. These scores determine the agent\u2019s subsequent exploration decisions, guiding it to the areas most likely to contain the target object. ", "page_idx": 4}, {"type": "text", "text": "To correlate the scoring with both the global frame and localized patches, ensuring that finer details and smaller components are effectively scored, we partition the observed image into patches of equal size across different scales. Specifically, given an RGB observation image $I_{t}$ , with height $H$ and width $W$ , the partitioning process is as follows. At level $k$ , the image is partitioned into $4^{(\\bar{k}-1)}$ equal parts with each patch at level $k$ of size 2(kH\u22121) \u00d72(kW\u22121) . The patches at level k can be represented as: $I_{t}^{k}=\\left\\{P_{h,w}\\;|\\;1\\leq h\\leq2^{(k-1)},1\\leq w\\leq2^{(k-1)}\\right\\}$ where $P_{h,w}$ denotes the patch located at the $h^{t h}$ row and $w^{t h}$ column of the partitioned image at level $k$ . All patches from all levels are then resized to the same dimensions for further processing. We utilize CLIP [32] to calculate the visual embeddings for all patches from all levels. For each patch $P_{h,w}^{k}$ at level $k$ , its visual embedding $v_{h,w}^{k}$ is computed using the image encoder of CLIP. Simultaneously, each attribute embedding $e$ is computed using the text encoder of CLIP, given the attribute descriptions, $\\{A_{n}\\}_{n=1}^{N_{a}}$ and $\\{G_{n}\\}_{n=1}^{N_{g}}$ , generated from an LLM. For each patch $P_{h,w}^{k}$ , we calculate the cosine similarity between its visual embedding $v_{h,w}^{k}$ and the attribute embedding $e$ to obtain a similarity score $S_{h,w}^{k,e}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{h,w}^{k,e}=\\frac{v_{h,w}^{k}\\cdot e}{\\Vert v_{h,w}^{k}\\Vert\\Vert e\\Vert}\\enspace.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Due to the hierarchical partitioning of the image, we accumulate the scores across all scales for each pixel location and then take the average to obtain the final score for the image. Specifically, let $L$ be the number of levels, and the accumulated score for the pixel at position $(p,q)$ in the image $I_{t}$ is calculated by summing the scores from all levels and then averaging: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS(p,q,e)=\\frac{1}{L}\\sum_{k=1}^{L}S_{h(p,q,k),w(p,q,k)}^{k,e}\\ ,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h(p,q,k)$ and $w(p,q,k)$ map the pixel position $(p,q)$ in the image to the corresponding patch indices at level $k$ . The scores are then used to generate a Geographic and Affordance Map (GAMap) for the explored area. The process for generating the GAMap is detailed in the next section. ", "page_idx": 4}, {"type": "text", "text": "3.4 Geographic and Affordance Map ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "At the core of our approach is the Geographic and Affordance Map. This map assigns a GA score to each pixel within the explored area to quantify the relevance of different regions in locating the target object, associating the areas with the highest values as the most promising for further exploration. ", "page_idx": 4}, {"type": "text", "text": "We define the GAMap at time step $t$ as $M_{t}\\in\\mathbb{R}^{\\hat{H}\\times\\hat{W}\\times C}$ , where $\\hat{H}$ and $\\hat{W}$ are the dimensions of the 2D projection grid map, and $C$ is the number of attributes, i.e., $C=N_{a}+N_{g}$ with $N_{a}$ and $N_{g}$ representing the number of attributes and parts, respectively. To construct the GAMap from the RGB-D observation $I_{t}$ and the depth data $D_{t}$ , we back-project every pixel from $D_{t}$ to reconstruct the point cloud following [18]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{X}=D_{t}(p,q)\\cdot\\mathbf{K}^{-1}\\cdot[p,q,1]^{T}\\enspace,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{K}$ is the intrinsic matrix of the depth camera, and $D_{t}(p,q)$ is the depth value of the pixel at the coordinate $(p,q)$ . To transform the point cloud into the world coordinate frame, we use $\\mathbf{X}_{\\mathbf{world}}=\\mathbf{T}_{\\mathbf{W}}\\cdot\\mathbf{X}$ , where $\\mathbf{Tw}$ is the transformation matrix from the camera coordinate to the world coordinate. The 3D points are then projected onto the ground plane to determine the corresponding positions on the grid map. We assume perfect calibration between the depth and RGB cameras, allowing us to project each image pixel\u2019s score to its corresponding grid cell in the map. Given that multiple 3D points may project to the same grid location, we retain the maximum value for each channel that falls into the same grid cell as the score of this cell: ", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{t}(\\hat{h},\\hat{w},e)=\\operatorname*{max}\\left\\{S(p,q,e)\\mid(p,q)\\in\\mathrm{cell}\\;M_{t}(\\hat{h},\\hat{w})\\right\\}\\;\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $M_{t}(\\hat{h},\\hat{w},e)$ represent the score of attribute $e$ in the 2D grid at position $(\\hat{h},\\hat{w})$ , $S(p,q,e)$ is the score for the pixel at $(p,q)$ for the attribute $e$ , and cell $M_{t}(\\hat{h},\\hat{w})$ denotes the grid cell on the GAMap. When the robot moves to a new position, resulting in overlapping observations with the previously explored regions, the GA scores for each pixel in the overlapping region are updated. The updated GAMap is computed by taking the maximum of the current attribute score and the previous attribute score for each cell: ", "page_idx": 5}, {"type": "equation", "text": "$$\nM(\\hat{h}_{o},\\hat{w}_{o},e)=\\operatorname*{max}\\left(M_{t}(\\hat{h}_{o},\\hat{w}_{o},e),M(\\hat{h}_{o},\\hat{w}_{o},e)\\right)\\,\\,\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M(\\hat{h}_{o},\\hat{w}_{o},e)$ is the GAMap constructed from the previous step, and the subscript $o$ represents the overlapped grid cell. ", "page_idx": 5}, {"type": "text", "text": "3.5 Exploration Policy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To determine the next area to explore, the robot selects the region with the highest GA score, calculated as the average of all attribute channels. To enable efficient exploration, only areas near the frontier with the highest scores are selected. Once the area with the highest GA score is identified, a heuristic search algorithm, i.e., the Fast Marching Method (FMM) [42], is employed to find the shortest path from the robot\u2019s current location to the selected area. The robot then generates the appropriate actions to navigate along this path. At each step, the GAMap is updated based on new observations. We repeat this process until the robot either identifies and reaches the target object or the episode ends. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. HM3D [23] is a dataset consisting of 3D data from real-world indoor spaces, along with semantic annotations, serving as a foundational resource for the Habitat 2022 ObjectNav Challenge [23]. This comprehensive dataset includes 142,646 object instance annotations, organized into 40 distinct classes across 216 environments, covering a total of 3,100 individual rooms. We follow the validation settings from [3, 32] to evaluate our proposed method. Gibson [24] was developed by Al-Halah et al. [43]. The dataset comprises 5 validation scenes across 6 object categories, and we adhere to the standard evaluation protocol [44, 19, 30, 2] to use all 5 validation scenes for evaluation. ", "page_idx": 5}, {"type": "text", "text": "Metrics. Success Rate (SR, $\\%$ ) [45] focuses on the agent\u2019s accuracy in reaching the designated target, where a higher value indicates better performance. SR is computed based on whether the robot successfully stops within $0.1\\mathrm{m}$ of the target object $g_{i}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nS R(\\pi)=\\frac{1}{K}\\sum_{i=1}^{K}{\\bf1}_{\\{d(g_{i},p_{T_{i}})\\leq0.1\\}}\\,\\,\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $K$ is the number of episodes, $d(g_{i},p_{T_{i}})$ is the distance between the target object $g_{i}$ and the robot\u2019s final position $p_{T_{i}}$ in the $i^{t h}$ episode, and $\\mathbf{1}_{\\{\\cdot\\}}$ is an indicator function. Success weighted by Path Length (SPL, $\\%$ ) [45] evaluates success relative to the shortest possible path, normalized by the actual path taken by the agent, measuring the efficiency of the agent\u2019s success in reaching a goal, defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS P L(\\pi)=\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{1}_{\\left\\{d(g_{i},p_{T_{i}})\\le0.1\\right\\}}\\cdot\\frac{L_{i}^{*}}{\\operatorname*{max}(L_{i},L_{i}^{*})}\\,\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{i}$ is the actual path length traveled by the robot in the $i^{t h}$ episode and $L_{i}^{*}$ is the shortest possible path length to the target in the same environment. ", "page_idx": 5}, {"type": "text", "text": "4.1 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We compare our method against several ZS-OGN approaches, including the state-of-the-art methods: Random Exploration: takes random actions to explore the environment. Nearest FbE [33]: explores the environment by selecting the nearest frontier. SemExp [2]: utilizes a category semantic map and trains a local navigation policy for exploration. PixNav [9]: trains models for navigation by selecting pixels as intermediate goals. PONI [44]: uses potential functions to select frontiers for exploration. ZSON [30]: employs categorical information to train a model for object-based navigation tasks. CoW [32]: uses CLIP for categorical information extraction and explores using the nearest frontier-based exploration. ESC [3]: utilizes a categorical semantic map and commonsense reasoning for target object exploration. L3MVN [19]: uses an LLM to reason about the next exploration area based on a trained detection head for semantic map construction. VLFM [16]: employs BLIP-2 [46] and categorical information of the target object to evaluate and select frontiers for exploration. VoroNav [17]: uses a Voronoi-based decomposition strategy for navigation. SemUtil [15]: considers the shortest path distance to frontiers and the relevance scoring between objects for exploration. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "IjHrALdQNP/tmp/0962ed50b83d2429dfeb303bed6427965e69ea99cf38d1709f36e7054c53c069.jpg", "table_caption": ["Table 1: Comparison of navigation performance between different methods on HM3D and Gibson datasets, measured by SR and SPL metrics. This table highlights the performance of our proposed method, demonstrating improvements over existing methods on both datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Results and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our method with existing approaches across four categories: those utilizing both locomotion and semantic training [2, 30], those employing only locomotion training [9, 16], those using only semantic training [19, 44], and those that do not incorporate any training [3, 32, 17, 15]. Locomotion training involves learning-based methods for navigation, while semantic training requires training or finetuning a perception module to construct a semantic map. The results and comparisons are shown in Table 1. ", "page_idx": 6}, {"type": "text", "text": "On the HM3D dataset, our method achieved a SR of $53.1\\%$ and a SPL of $26.0\\%$ . This represents a significant improvement over the best method [17] that does not use locomotion and semantic training, with a $26.4\\%$ increase in SR. Although the SPL $(26.0\\%)$ of our method is lower than that of VLFM $(30.4\\%)$ , this discrepancy can be attributed to the fact that VLFM\u2019s local policy planning is trained. Considering the different path planning methods adopted in our approach and VLFM, we construct a com", "page_idx": 6}, {"type": "image", "img_path": "IjHrALdQNP/tmp/f216d5d69e1bbec026727392d92f80d7403395d8d07ff0bae1c4627bb5213a6d.jpg", "img_caption": ["Figure 3: Heatmap showing the increase and decrease in the percentage of SR and time cost for varying the numbers of $N_{a}$ and $N_{g}$ . Darker colors indicate a greater decrease in SR, and red solid and dashed lines represent the associated time cost. ", ""], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "parative experiment with VLFM. Specifically, we kept the VLFM value map generation process unchanged and replaced its path planning method with FMM instead of the trained policy. The detailed results are shown in the second last row in Table 1. The comparison reveals that our model outperforms VLFM-based mapping method by $4.32\\%$ and $10.17\\%$ in SR and SPL on HM3D. On the Gibson dataset, GAMap attained an SR of $85.7\\%$ and an SPL of $55.5\\%$ , which marks a substantial improvement over methods that do not utilize locomotion and semantic training [17], with a $23.7\\%$ increase in SR and a $37.0\\%$ increase in SPL. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Ablative Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Effectiveness of Affordance and Geometric-part Attributes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We analyzed the effectiveness of the number of affordances and geometric parts in our proposed method. Ablation studies were conducted by varying $N_{a}$ and $N_{g}$ to assess performance gain versus time loss, as well as the contribution of each attribute type. We evaluated on the official mini-validation split of the HM3D dataset. ", "page_idx": 7}, {"type": "text", "text": "The heatmap, as shown in Figure 3, illustrates the SR with different combinations of $N_{a}$ and $N_{g}$ . The color changes from dark purple to yellow indicate the increased percentage in SR. Red sold and dashed lines with labels indicate the time cost associated with each combination of $N_{a}$ and $N_{g}$ . Increasing the number of geometric parts $(N_{g})$ from 0 to 3 results in a significant improvement in SR across all levels of $N_{a}$ . For example, when $N_{a}=0$ , increasing $N_{g}$ from 0 to 3 raises the SR more than increasing $N_{a}$ from 0 to 3 when $N_{g}=0$ . This demonstrates the substantial contribution of geometric parts to navigation performance. To more clearly demonstrate the specific impacts of $N_{a}$ and $N_{g}$ on various performance metrics, we have converted Figure 3 to Table 6 in the Appendix. Similarly, increasing the number of affordance attributes $(N_{a})$ also improves SR, though the effect is slightly less pronounced than that of geometric parts. The best performance is achieved when both $N_{a}$ and $N_{g}$ are maximized, with both set to 3. This suggests a synergistic effect, where the combination of both attributes leads to optimal navigation performance. However, it requires an $18.2\\%$ increase in time cost. ", "page_idx": 7}, {"type": "text", "text": "5.2 Effectiveness of Different Scaling Levels ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We analyzed the effectiveness of different scaling levels $(L)$ . Figure 4 presents changes in SR and SPL when using different numbers of scales and the time required for processing on the mini-validation split of HM3D. ", "page_idx": 7}, {"type": "text", "text": "Increasing the number of scaling levels from 1 to 4 leads to notable changes in both SR and SPL. At the highest scale level of 4, the SR improves by approximately $10\\%$ , and the SPL increases by about $20\\%$ than $L\\,=\\,1$ . However, there is a trade-off between performance improvement and time cost. The time required increases with the number of scales, as indicated by the red line in the figure. The time cost starts at 0.66 seconds for a single scale and rises progressively, reaching 0.78 seconds at the fourth scale level. This increase in time cost suggests that while higher scaling levels improve both SR and SPL, they also ", "page_idx": 7}, {"type": "image", "img_path": "IjHrALdQNP/tmp/13d80057023880de9fee597a63a5f7caefdb70f9cb8034baba987b98948feccf.jpg", "img_caption": ["Figure 4: Changes in SR, SPL, and processing time across different scaling levels on the mini-validation split of HM3D. Increasing scales improves SR and SPL but also increases processing time. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "demand more computational resources and time. This emphasizes the need to balance performance and computational efficiency when determining the optimal scaling level for practical applications. ", "page_idx": 7}, {"type": "text", "text": "5.3 Effectiveness of Different Methods for Calculating GA Scores ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We analyzed the proposed patch-based method by comparing it with the gradient-based method. The visualization of the GA score for different methods is shown in Figure 5, which illustrates the GA score of the armrest, backrest, and seat attributes of a target object chair. Observations indicate that the gradient-based method often attends to irrelevant areas. For example, the ceiling of the room has a higher GA score, which is incorrect. In contrast, the patch-based method more accurately focuses on relevant areas, such as the armrest, backrest, and seat of the chair, validating its effectiveness over the gradient-based method. Moreover, as observed from Table 2, the gradient-based method is also slower than the patch-based method. One reason for this is that the gradient-based method requires back-propagation of the gradient. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Based on different methods, we further explored the effectiveness of various pretrained encoders, as shown in Table 2. We compared SR and computation time required by three types of encoders: CLIP (ours), BLIP, and BLIP-2. Although using more powerful encoders such as BLIP and BLIP-2 leads to better performance, they require significantly more time than CLIP, while the performance gain is limited. This trade-off makes it less valuable for us to use a BLIP-based encoder. ", "page_idx": 8}, {"type": "text", "text": "Moreover, we analyzed different ways of aggregating GA scores using patch-based methods. Given the multiple levels of patches, we aimed to find the best method to aggregate the final GA score from multilevel GA scores. ", "page_idx": 8}, {"type": "image", "img_path": "IjHrALdQNP/tmp/8f1edf697343cba76353d5a16fa6cf5a9ca546002d57386730fa8f26b7816544.jpg", "img_caption": ["Figure 5: Comparison of GA score visualization between gradient-based and patch-based methods for the armrest, backrest, and seat attributes of a target chair. The gradient-based method (top row) often attends to irrelevant areas, such as the ceiling, while the patchbased method (bottom row) accurately focuses on the relevant areas. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We compared \u201cMax Value\u201d and \u201cAverage Value\u201d (ours). In the \u201cMax Value\u201d method, the maximum GA score across different scale levels is used. The \u201cAverage Value\u201d method, which we propose, calculates the average GA score across levels. Note that the gradient-based method directly gives the score, so it is not analyzed. As shown in Table 2, using the average to aggregate the score gives us the best performance. ", "page_idx": 8}, {"type": "table", "img_path": "IjHrALdQNP/tmp/611f5b5cc9a8b553e9557c50ff45beeac89e3152f5fbadf1ceecfc3d186c53b9.jpg", "table_caption": ["Table 2: Analysis of different GA scoring methods. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "IjHrALdQNP/tmp/6cb9ffeea0360c51ff8defa4f9382be1d9f3dd71264e8d0d70a653d801a189ba.jpg", "table_caption": ["Table 3: Impact of different GA score updating methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Effectiveness of Different Methods for Updating GA Scores ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Different methods of updating GA scores and their impact on navigation performance are shown in Table 3. In the \u201cReplacement\u201d method, the previous value is disregarded and overwritten with the new one. The \u201cAverage\u201d method calculates the new value as the average of the previous and current values. Our approach, \u201cMax\u201d, retains the maximum scores between the previous and new values, which memorizes the most salient score in a specific direction, as the agent could observe an object from different perspectives during exploration, thus potentially finding the optimal perspective. ", "page_idx": 8}, {"type": "text", "text": "Our findings indicate that the \u201cMax\u201d method consistently enhances performance compared to the other two methods across all three datasets. In Table 3, the \u201cMax\u201d method achieves SR of $50.25\\%$ and SPL of $25.3\\%$ . The \u201cAverage\u201d method results in a $3\\%$ decrease in SR and a $10\\%$ decrease in SPL, indicating a moderate impact on performance. The \u201cReplacement\u201d method shows the most significant performance drop, with a $4\\%$ decrease in SR and a $17\\%$ decrease in SPL. These results highlight that the \u201cMax\u201d method is the most effective in maintaining and enhancing navigation performance, as it better captures and retains the most relevant object attributes from different perspectives. ", "page_idx": 8}, {"type": "text", "text": "5.5 Effectiveness of Geometric and Affordance Guidance Navigation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluated the effectiveness of the proposed part and affordance guidance navigation by analyzing three types of errors: detection error, planning error, and exploration error. 1) Detection error happens when the agent either misses the goal or incorrectly believes it has detected the goal. 2) Planning error arises when the agent either recognizes the target but cannot reach it or gets stuck without spotting the goal, reflecting the path-planning ability of the system. 3) Exploration error occurs when the agent fails to see the goal object due to issues other than planning or detection, assessing its ability to approach the goal. Table 4 shows the comparison of these errors using our proposed method versus using categorical semantic information as the guidance for navigation. Note that our proposed method significantly decreases the errors in all three categories. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.6 Effectiveness of Multi-scale Approach ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Furthermore, we conduct an experiment to verify whether VLM [47] models can directly capture enough multi-scale information. We randomly selected a scene and compared the ability of GPT-4V and our ", "page_idx": 9}, {"type": "table", "img_path": "IjHrALdQNP/tmp/2c01ba493f996ce2670f3974445a27cf76835a802a3c917bf306941f8b1b1c12.jpg", "table_caption": ["Table 4: Comparison of errors using categorical semantic guidance versus geometric parts and affordance guidance. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "proposed CLIP with multi-scale scoring method to identify the target object. As shown in Figure 6, we input an image with a sofa located in a distant corner as the target object and compared the subsequent movement trajectories of the two algorithms. As illustrated in Figure 6, our method successfully captures the small sofa back in the far corner, leveraging geometric parts and affordance attributes to guide the exploration process. In contrast, GPT-4V failed to identify the object. ", "page_idx": 9}, {"type": "image", "img_path": "IjHrALdQNP/tmp/fd4f4f4572bf2d24a9d8df7089a4e9436ec1a8ac8e38724b1d7ac8ab30f2855b.jpg", "img_caption": ["Figure 6: The top row of images shows our proposed method, where the multi-scale approach effectively captures objects at all scales, such as the sofa back in the background. The bottom row of images shows the results of GPT-4V. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced the Geometric-part and Affordance Maps (GAMap) for zero-shot object goal navigation, leveraging geometric parts and affordance attributes to guide exploration in unseen environments. Our method employs LLMs for detailed attribute inference and VLMs for multi-scale scoring, capturing object intricacies at various scales. Comprehensive experiments on HM3D and Gibson datasets exhibit significant improvements in SR and SPL over previous methods. These results highlight the effectiveness of our approach in enhancing navigation efficiency without any task-specific training or fine-tuning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Authors appreciate the support provided by the NYUAD Center for Artificial Intelligence and Robotics (CAIR), funded by Tamkeen under the NYUAD Research Institute Award CG010. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. In International Conference on Learning Representations, 2020.   \n[2] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33:4247\u20134258, 2020.   \n[3] Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, and Xin Eric Wang. Esc: exploration with soft commonsense constraints for zero-shot object navigation. In Proceedings of the 40th International Conference on Machine Learning, pages 42829\u201342842, 2023. [4] Gengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 7641\u20137649, 2024. [5] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023.   \n[6] Haresh Karnan, Garrett Warnell, Xuesu Xiao, and Peter Stone. Voila: Visual-observation-only imitation learning for autonomous navigation. In International Conference on Robotics and Automation, pages 2497\u20132503. IEEE, 2022.   \n[7] Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, and Sergey Levine. Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation. In IEEE International Conference on Robotics and Automation, pages 5129\u20135136. IEEE, 2018. [8] Jan W\u00f6hlke, Felix Schmitt, and Herke van Hoof. Hierarchies of planning and reinforcement learning for robot navigation. In IEEE International Conference on Robotics and Automation, pages 10682\u201310688. IEEE, 2021.   \n[9] Wenzhe Cai, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, and Hao Dong. Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill. In IEEE International Conference on Robotics and Automation, pages 5228\u2013 5234. IEEE, 2024.   \n[10] Dhruv Shah, Michael Robert Equi, B\u0142a\u02d9zej Osi\u00b4nski, Fei Xia, Brian Ichter, and Sergey Levine. Navigation with large language models: Semantic guesswork as a heuristic for planning. In Conference on Robot Learning, pages 2683\u20132699. PMLR, 2023.   \n[11] Hao Huang, Shuaihang Yuan, Congcong Wen, Yu Hao, and Yi Fang. 3d-trans: 3d hierarchical transformer for shape correspondence learning. In International Conference on Automation, Robotics and Applications, pages 536\u2013540. IEEE, 2024.   \n[12] Qianfan Zhao, Lu Zhang, Bin He, Hong Qiao, and Zhiyong Liu. Zero-shot object goal visual navigation. In IEEE International Conference on Robotics and Automation, pages 2025\u20132031. IEEE, 2023.   \n[13] Vishnu Sashank Dorbala, James F Mullen Jr, and Dinesh Manocha. Can an embodied agent find your \u201ccat-shaped mug\u201d? llm-based zero-shot object navigation. IEEE Robotics and Automation Letters, 2023.   \n[14] Qianfan Zhao, Lu Zhang, Bin He, and Zhiyong Liu. Semantic policy network for zero-shot object goal visual navigation. IEEE Robotics and Automation Letters, 2023.   \n[15] Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, and Fisher Yu. How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers. In Robotics: Science and Systems, 2023.   \n[16] Naoki Harrison Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher. Vlfm: Vision-language frontier maps for zero-shot semantic navigation. In Workshop on Language and Robot Learning: Language as Grounding, 2023.   \n[17] Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, Shanghang Zhang, and Chang Liu. Voronav: Voronoi-based zero-shot object navigation with large language model. arXiv preprint arXiv:2401.02695, 2024.   \n[18] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In IEEE International Conference on Robotics and Automation, pages 10608\u201310615. IEEE, 2023.   \n[19] Bangguo Yu, Hamidreza Kasaei, and Ming Cao. L3mvn: Leveraging large language models for visual target navigation. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3554\u20133560. IEEE, 2023.   \n[20] Shimon Ullman. High-level vision: Object recognition and visual cognition. MIT press, 2000.   \n[21] M Gazzaniga, R Ivry, and G Mangun. Cognitive science. Springer, 2002.   \n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[23] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.   \n[24] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9068\u20139079, 2018.   \n[25] Obin Kwon, Jeongho Park, and Songhwai Oh. Renderable neural radiance map for visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9099\u20139108, 2023.   \n[26] Hao Huang, Shuaihang Yuan, CongCong Wen, Yu Hao, and Yi Fang. Noisy few-shot 3d point cloud scene segmentation. In IEEE International Conference on Robotics and Automation, pages 11070\u201311077. IEEE, 2024.   \n[27] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022.   \n[28] Songhao Han, Le Zhuo, Yue Liao, and Si Liu. Llms as visual explainers: Advancing image classification with evolving visual descriptions. arXiv preprint arXiv:2311.11904, 2023.   \n[29] David Silver, James Bagnell, and Anthony Stentz. High performance outdoor navigation from overhead data using imitation learning. Robotics: Science and Systems, 1, 2008.   \n[30] Arjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, and Dhruv Batra. Zson: Zero-shot object-goal navigation using multimodal goal embeddings. Advances in Neural Information Processing Systems, 35:32340\u201332352, 2022.   \n[31] Vishnu Sashank Dorbala, Gunnar A Sigurdsson, Jesse Thomason, Robinson Piramuthu, and Gaurav S Sukhatme. Clip-nav: Using clip for zero-shot vision-and-language navigation. In Workshop on Language and Robotics at CoRL, 2022.   \n[32] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song. Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23171\u201323181, 2023.   \n[33] Brian Yamauchi. A frontier-based approach for autonomous exploration. In Proceedings of IEEE International Symposium on Computational Intelligence in Robotics and Automation, pages 146\u2013151. IEEE, 1997.   \n[34] Shuaihang Yuan, Muhammad Shafique, Mohamed Riyadh Baghdadi, Farshad Khorrami, Anthony Tzes, and Yi Fang. Zero-shot object navigation with vision-language foundation models reasoning. In International Conference on Automation, Robotics and Applications, pages 501\u2013505. IEEE, 2024.   \n[35] Congcong Wen, Yisiyuan Huang, Hao Huang, Yanjia Huang, Shuaihang Yuan, Yu Hao, Hui Lin, Yu-Shen Liu, and Yi Fang. Zero-shot object navigation with vision-language models reasoning. arXiv preprint arXiv:2410.18570, 2024.   \n[36] Halil Utku Unlu, Shuaihang Yuan, Congcong Wen, Hao Huang, Anthony Tzes, and Yi Fang. Reliable semantic understanding for real world zero-shot object goal navigation. arXiv preprint arXiv:2410.21926, 2024.   \n[37] Shuaihang Yuan, Halil Utku Unlu, Hao Huang, Congcong Wen, Anthony Tzes, and Yi Fang. Exploring the reliability of foundation model-based frontier selection in zero-shot object goal navigation. arXiv preprint arXiv:2410.21037, 2024.   \n[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[39] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations, 2019.   \n[40] Austin Myers, Ching L Teo, Cornelia Ferm\u00fcller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In IEEE International Conference on Robotics and Automation, pages 1374\u20131381. IEEE, 2015.   \n[41] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[42] James A Sethian. A fast marching level set method for monotonically advancing fronts. proceedings of the National Academy of Sciences, 93(4):1591\u20131595, 1996.   \n[43] Ziad Al-Halah, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Zero experience required: Plug & play modular transfer learning for semantic visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17031\u201317041, 2022.   \n[44] Santhosh Kumar Ramakrishnan, Devendra Singh Chaplot, Ziad Al-Halah, Jitendra Malik, and Kristen Grauman. Poni: Potential functions for objectgoal navigation with interactionfree learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18890\u201318900, 2022.   \n[45] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018.   \n[46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[47] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. In Workshop on Vision-Language Models for Navigation and Manipulation at ICRA, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Difference with Existing Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As shown in Table 5, the key difference between our method and other methods is that we leverage geometric parts and affordance information to represent the environment, in addition to using objectlevel category information as previous methods do. Furthermore, we utilize multi-scale feature representation to capture local features, enhancing the overall accuracy and robustness. ", "page_idx": 13}, {"type": "table", "img_path": "IjHrALdQNP/tmp/61e6722a80f994e729fa8d0ddebed671fb524bf27f1e8b33bec84c804cf11c8c.jpg", "table_caption": ["Table 5: The differences between our work and existing methods. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Experiment Setup ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In our experiment, we adopt GPT-4V as the LLM to generate the geometric and affordance attributes. We set $N_{a}$ to 1 and $N_{g}$ to 3 for the experiments on the HM3D and Gibson datasets. For the partition process, we use three scaling levels in all our experiments: the first level is the original image, the second level has 4 equal-sized patches, and the third level has 16 equal-sized patches. We use CLIP as the pre-trained visual and text encoder. Following the standard evaluation protocol [19], we use 2000 episodes on the validation split of HM3D to report the results. Similarly, we follow this method [19]to produce the results on the Gibson dataset. We use a Titan XP GPU for the experiment evaluation, and the entire evaluation process takes around 44 hours. ", "page_idx": 13}, {"type": "text", "text": "C Effectiveness of Affordance and Geometric-part Attributes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide the quantitative result for Figure 3 in the main paper. Time is measured in seconds. ", "page_idx": 13}, {"type": "text", "text": "Table 6: Quantitative results for the effectiveness of affordance and geometric-part attributes. ", "page_idx": 13}, {"type": "table", "img_path": "IjHrALdQNP/tmp/38614cb181f66d445b5812f5f1ffb248c79adc8474e377fc71a1a2734311243c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "D Result Visualizations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we visualize the navigation paths on both the Gibson and HM3D datasets, as shown in Figures 7 and 8. Part of the visualization code is adapted from L3MVN [19]. ", "page_idx": 13}, {"type": "text", "text": "E Time Complexity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To validate the efficiency of our method, we compare the FPS of our method to SemExp [2], L3MVN [16], and VLFM [19] in navigation tasks on the HP3D dataset. The experimental results are shown in Table 7. It can be observed that SemExp has the highest FPS, indicating the fastest processing speed. This is because it uses a detection head and does not employ a foundation model. However, SemExp has the lowest SR and SPL, indicating that despite its fast processing speed, it performs poorly in navigation accuracy and path efficiency. In contrast, L3MVN has the second-highest FPS as it uses a lightweight foundation model. Although its processing speed is not as fast as SemExp, it shows improved navigation accuracy and path efficiency, achieving an SR of $76.1\\%$ and an SPL of $37.7\\%$ . On the other hand, VLFM has a lower FPS of only 2, but it significantly improves SR and SPL, reaching $84.0\\%$ and $52.2\\%$ , respectively. This indicates that although VLFM has a slower processing speed, it has considerable advantages in navigation accuracy and path efficiency. Our model has the same FPS as VLFM, both at 2, but further improves SR and SPL, reaching $85.7\\%$ and $55.5\\%$ , respectively. This demonstrates that our method maintains high navigation accuracy and path efficiency while providing comparable processing speed to VLFM. These experimental results verify that our proposed method achieves a good balance between time and accuracy. ", "page_idx": 14}, {"type": "table", "img_path": "IjHrALdQNP/tmp/4bd989c0ebb17950c80fb3372942836e244f687ed46ae3a3292aa1f10b9477a7.jpg", "table_caption": ["Table 7: Comparison of different method\u2019s FPS on the HM3D dataset. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "F Real-world Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our real-world experiment, we will evaluate the performance of four zero-shot object goal navigation algorithms, including L3MVN [19], COW [32], ESC [3], and VLFM [16], within a standard indoor apartment environment consisting of two bedrooms, two bathrooms, one kitchen, and one living room. The experiment adopts a four-wheeled robot. Specifically, we use a JetAuto-Pro from Hiwonder equipped with an Intel Realsense D435i camera as our robot agent to navigate the environment and locate specific target objects, including a bed, toilet, table, sofa, and chair, without any prior knowledge of their locations. To ensure a fair comparison, the starting position of the robots was kept consistent across all trials for each algorithm. The video demo can be found on our project page: https://shalexyuan.github.io/GAMap/. ", "page_idx": 14}, {"type": "image", "img_path": "IjHrALdQNP/tmp/c571abe35b2c61e66e8e4530dfb5664e9a8620d8e368978f6bfe27f5cf934ef9.jpg", "img_caption": ["Figure 7: Visualizations of the last observation frame, navigation path, and GAMap. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "IjHrALdQNP/tmp/4d3cc4dc297137155a7842af9f2a6f16a62932a669f9b3bffded48cc3c910012.jpg", "img_caption": ["Figure 8: Visualizations of last observation frame, navigation path, and GAMap. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "G Limitation and Future Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "While our proposed method, Geometric-Affordance Maps (GAMap), has demonstrated significant improvements in zero-shot object goal navigation, it is important to acknowledge its limitations. Our approach relies heavily on the visual processing power of vision-language models, i.e., CLIP. The effectiveness of GAMap also depends on the accuracy of geometric parts and affordance attributes inferred by LLMs. Although the multi-scale scoring method enhances attribute detection, it introduces additional complexity and computational overhead. ", "page_idx": 15}, {"type": "text", "text": "To address these limitations and further advance this research field, future work should optimize the integration of LLMs and VLMs to reduce computational overhead, potentially through techniques like model distillation. Enhancing the accuracy of geometric and affordance attribute inference is crucial, and more powerful foundational models could improve this accuracy. Additionally, exploring better methods for aggregating these attributes is also an interesting research direction. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 16}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 16}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 16}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 16}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 16}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We show the claim in the abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The limitation is included ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not have assumptions ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We include it in the appendix ", "page_idx": 17}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We will release our code upon the decision of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 18}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We follow the standard testing details which are stated in the paper. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The standard evaluation not include the error bar. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This is included in the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: It is discussed. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The paper has no such risk. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All assets are referenced. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]