[{"type": "text", "text": "Gradient-Variation Online Learning under Generalized Smoothness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yan-Feng Xie, Peng Zhao, Zhi-Hua Zhou ", "page_idx": 0}, {"type": "text", "text": "National Key Laboratory for Novel Software Technology, Nanjing University, China School of Artificial Intelligence, Nanjing University, China {xieyf, zhaop, zhouzh}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient-variation online learning aims to achieve regret guarantees that scale with variations in the gradients of online functions, which is crucial for attaining fast convergence in games and robustness in stochastic optimization, hence receiving increased attention. Existing results often require the smoothness condition by imposing a fixed bound on gradient Lipschitzness, which may be unrealistic in practice. Recent efforts in neural network optimization suggest a generalized smoothness condition, allowing smoothness to correlate with gradient norms. In this paper, we systematically study gradient-variation online learning under generalized smoothness. We extend the classic optimistic mirror descent algorithm to derive gradient-variation regret by analyzing stability over the optimization trajectory and exploiting smoothness locally. Then, we explore universal online learning, designing a single algorithm with the optimal gradient-variation regrets for convex and strongly convex functions simultaneously, without requiring prior knowledge of curvature. This algorithm adopts a two-layer structure with a meta-algorithm running over a group of base-learners. To ensure favorable guarantees, we design a new Lipschitz-adaptive meta-algorithm, capable of handling potentially unbounded gradients while ensuring a second-order bound to effectively ensemble the base-learners. Finally, we provide the applications for fast-rate convergence in games and stochastic extended adversarial optimization. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider online convex optimization (OCO) [Hazan, 2016; Orabona, 2019], a flexible framework that models the decision-making problem in an online fashion. At each round $t\\,\\in\\,[T]$ , an online learner is required to submit a decision $\\mathbf{x}_{t}$ from a convex compact set $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and the environments reveal a convex function $f_{t}:\\mathcal{X}\\mapsto\\mathbb{R}$ . Then the learner suffers a loss $f_{t}(\\mathbf{x}_{t})$ and updates her decision. The standard performance measure is the regret [Zinkevich, 2003] that benchmarks the cumulative loss of the learner against the best decision in hindsight, formally defined as ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname{REG}_{T}=\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\operatorname*{min}_{\\mathbf x\\in\\mathcal X}\\sum_{t=1}^{T}f_{t}(\\mathbf x).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Regret bounds of $\\mathcal{O}(\\sqrt{T})$ and $\\textstyle{\\mathcal{O}}({\\frac{1}{\\lambda}}\\log T)$ are established for convex and $\\lambda$ -strongly convex functions respectively [Zinkevich, 2003; Hazan et al., 2007]. While these results are known to be minimax optimal [Abernethy et al., 2008], in this paper we are more interested in obtaining gradientvariation regret guarantees, which replace the dependence of $T$ in the regret bounds by variations in the gradients of online functions [Chiang et al., 2012] defined as ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nV_{T}=\\sum_{t=2}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This quantity can be as small as a constant in stable environments where online functions remain fixed, and is at most ${\\mathcal{O}}(T)$ in the worst case under standard OCO assumptions, safeguarding minimax results. Besides this favorable adaptivity, recent studies have shown close relationships of gradient-variation online learning to various fields, including fast convergence in games [Rakhlin and Sridharan, 2013b; Syrgkanis et al., 2015; Zhang et al., 2022b] and robust stochastic optimization [Sachs et al., 2022; Chen et al., 2024], hence receiving increased attention [Zhao et al., 2020; Yan et al., 2023; Tsai et al., 2023; Ataee Tarzanagh et al., 2024; Zhao et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "In online learning, it is proved that the smoothness assumption is necessary for first-order algorithms to achieve gradient-variation regret bounds as discussed in Remark 1 of Yang et al. [2014], which is also restated in Proposition 2 in Appendix B. Previous works typically rely on the global $L$ -smoothness condition, imposing a fixed upper bound on the gradient Lipschitzness, i.e., requiring $\\|\\nabla^{2}f_{t}(\\mathbf{x})\\|_{2}\\leq L$ for all $t\\in[T]$ and $\\mathbf{x}\\in\\mathcal{X}$ . However, this global assumption restricts the applicability of theories to loss functions that are quadratically bounded from above. Furthermore, recent studies in neural network optimization have observed phenomena where the global smoothness condition fails to model optimization dynamics effectively, especially for important types of neural networks like LSTM [Zhang et al., 2020b] and Transformer [Crawshaw et al., 2022]. Therefore, modern optimization has devoted efforts to generalizing the smoothness condition. For example, Zhang et al. [2020b] introduce $\\left(L_{0},L_{1}\\right)$ -smoothness, which assumes $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\le L_{0}+L_{1}\\|\\bar{\\nabla}f(\\mathbf{x})\\|_{2}$ for an offilne objective function $f(\\cdot)$ . A notable generalization is the recent proposal of the $\\ell_{}$ -smoothness condition [Li et al., 2023], which assumes $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\,\\leq\\,\\ell(\\|\\nabla f(\\mathbf{x})\\mathbf{\\hat{\\|}}_{2})$ with a link function $\\ell(\\cdot)$ , significantly broadening previous assumptions through the flexibility of $\\ell(\\cdot)$ . Given this, it is natural to ask how to design online algorithms to exploit generalized smoothness and obtain favorable gradient-variation regret guarantees. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we provide a systematic study of gradient-variation online learning under generalized smoothness. We extend the classic optimistic online mirror descent (optimistic OMD) algorithm [Chiang et al., 2012; Rakhlin and Sridharan, 2013a] to derive gradient-variation regret bounds, achieving $O(\\sqrt{V_{T}})$ regret and ${\\mathcal{O}}(\\log V_{T})$ regret for convex and strongly convex functions under generalized smoothness, respectively. We emphasize the importance of stability analysis across the optimization trajectory, which allows generalized smoothness to be effectively exploited locally. Specifically, optimistic OMD maintains two sequences with submitted decisions $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ and intermediate decisions $\\{\\widehat{\\mathbf{x}}_{t}\\}_{t=1}^{T}$ . We need to control algorithmic stability by appropriate step size tuning and optimism design , ensuring that $\\mathbf{x}_{t}$ is sufficiently close to $\\widehat{\\mathbf{x}}_{t}$ to exploit local smoothness at $\\widehat{\\mathbf{x}}_{t}$ . ", "page_idx": 1}, {"type": "text", "text": "Based on this development, we investigate universal online l e arning [van Erven and Koolen,  2 016; Wang et al., 2019; Mhammedi et al., 2019; Zhang et al., 2022a; Yan et al., 2023; Yang et al., 2024], where the learner aims to design a single algorithm that simultaneously attains the optimal regret for both convex and strongly functions without the prior knowledge of curvature information. For this scenario, a common wisdom is to adopt an online ensemble consisting of a meta-base two-layer structure to handle the environmental uncertainty [Zhao et al., 2024], i.e., the unknown curvature of loss functions, where a meta-algorithm is running over a set of base-learners with different configurations. The base-learners are basically the instantiations of the developed variants of optimistic OMD, as mentioned earlier. However, designing the meta-algorithm is non-trivial with new challenges. The first challenge is from the potentially unbounded smoothness, which might lead to unbounded Lipschitz constants as well. This challenge requires the meta-algorithm to be Lipschitz-adaptive, adapting to Lipschitzness on the fly. Furthermore, we also expect it to provide a second-order regret, technically required when analyzing the ensemble errors, and to enable predictions with optimism, thereby producing the gradient variation. The second challenge is the complexity introduced by the combination procedure inherent in the ensemble method, which further complicates the smoothness estimation, making it difficult to properly tune the meta-algorithm and exploit smoothness. ", "page_idx": 1}, {"type": "text", "text": "To this end, we address both challenges with the function-variation-to-gradient-variation conversion and a newly-designed Lipschitz-adaptive meta-algorithm. The conversion technique, drawing inspiration from Bai et al. [2022], decouples the design between the meta and base levels and derives the gradient variation directly from function values, allowing us to avoid the cancellation-based analysis [Yan et al., 2023] for utilizing smoothness at the meta level. Nevertheless, this conversion requires the meta-algorithm to handle heterogeneous inputs due to certain technical considerations, and we are not aware of available algorithms satisfying all the requirements, motivating us to design a new algorithm. Based on optimistic Adapt-ML-Prod [Wei et al., 2016] and the clipping technique [Cutkosky, 2019], we present a new Lipschitz-adaptive meta-algorithm with a simpler algorithmic design, which can be of independent interest. With this algorithm, we can apply the function-variation-to-gradient-variation conversion to achieve the optimal results for both convex and strongly convex functions, up to doubly logarithmic factors of $T$ , without knowing curvature. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our findings for gradient-variation online learning are useful for several important applications, including fast-convergence online games [Rakhlin and Sridharan, 2013a; Syrgkanis et al., 2015] and stochastic extended adversarial online learning [Sachs et al., 2022], where we establish new results under the generalized smoothness condition. ", "page_idx": 2}, {"type": "text", "text": "The rest of paper is organized as follows. Section 2 provides preliminaries and key ideas for exploiting the generalized smoothness throughout the trajectory. In Section 3 we study universal online learning and present our key meta-algorithm. Section 4 discusses our applications. Related work is provided in Appendix A. All proofs can be found in the remaining appendices (Appendix B \u2013 D). ", "page_idx": 2}, {"type": "text", "text": "2 Gradient-Variation Online Learning under Generalized Smoothness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first introduce the problem setup, including the formal definition of generalized smoothness and other assumptions used in the paper. We then extend the optimistic online mirror descent framework to achieve gradient-variation regret bounds under generalized smoothness. ", "page_idx": 2}, {"type": "text", "text": "2.1 Problem Setup: Generalized Smoothness and Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent studies [Zhang et al., 2020b; Chen et al., 2023b] extend the global smoothness condition by allowing the smoothness to positively correlate with the gradient norm, where a particular function is required to model this relationship. Zhang et al. [2020b] introduce the $\\left(L_{0},L_{1}\\right)$ -smoothness condition, where the smoothness is upper bounded by a linear function of the gradient norm, i.e., $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\,\\le\\,L_{0}+L_{1}\\|\\nabla f(\\mathbf{x})\\|_{2}$ . Li et al. [2023] further generalize this by imposing a weaker assumption on the link function and propose the generalized smoothness defined as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 $\\ell_{\\mathrm{i}}$ -smoothness). A twice-differentiable function $f\\ :\\ \\mathcal{X}\\ \\mapsto\\ \\mathbb{R}$ is called $\\ell_{}$ -smooth for some non-decreasing continuous link function $\\ell~:~[0,+\\infty)~\\mapsto~(0,+\\infty)$ if it satisfies that $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\leq\\ell(\\|\\nabla f(\\mathbf{x})\\|_{2})$ for any $\\mathbf{x}\\in\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "The mild requirement on the link function $\\ell(\\cdot)$ allows for considerable generality. By selecting a linear link function, $\\ell\\cdot$ -smoothness immediately recovers $\\left(L_{0},L_{1}\\right)$ -smoothness [Zhang et al., 2020b]. Furthermore, it has been shown that $\\ell$ -smoothness can imply a wide class of functions including rational, logarithmic, and self-concordant functions [Li et al., 2023]. Based on this generalized smoothness notion, we now provide the formal assumption on the smoothness of online functions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (generalized smoothness). The online function $f_{t}:\\mathcal{X}\\mapsto\\mathbb{R}$ is $\\ell_{t}$ -smooth in an open set containing $\\bar{\\mathcal{X}}\\subseteq\\mathbb{R}^{d}$ for $t\\in[T]$ , and the learner can query $\\ell_{t}(\\mathbf{x})$ provided any point $\\mathbf{x}\\in\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "We also require a standard bounded domain assumption in the OCO literature [Hazan, 2016]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 (bounded domain). The feasible domain $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , which contains the origin 0, is non-empty and closed with the diameter bounded by $D$ , i.e., $\\|\\mathbf x-\\mathbf y\\|_{2}\\leq D$ for any $\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "We do not assume the prior knowledge of the Lipschitz constant of online functions. In fact, the unboundedness of smoothness may result in unbounded Lipschitz constants. If a Lipschitz upper bound were known, the generalized smoothness condition would be trivialized, as it would allow us to directly compute the upper bound of the smoothness constant. Furthermore, following the discussion in Jacobsen and Cutkosky [2023, Page 2, second paragraph on the right], we assume that there exist finite but unknown upper bounds $G$ and $L$ for Lipschitzness and smoothness to ensure the theoretical results are valid. Note that these quantities will only appear in the final regret bounds, and our algorithms does not use them as the inputs. Throughout the paper, we use the $\\bar{O(\\cdot)}$ -notation to hide the constants and use the $\\widetilde O(\\cdot)$ -notation to omit the poly-logarithmic factors in $T$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Algorithmic Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We choose optimistic online mirror descent (optimistic OMD) [Rakhlin and Sridharan, 2013a] as the algorithmic framework, which provides a unified view to design and analyze many online algorithms. Compared to classic OMD [Nemirovskij and Yudin, 1985; Beck and Teboulle, 2003], optimistic OMD predicts with side information, an optimistic vector $\\boldsymbol{M}_{t}\\,\\in\\,\\mathbb{R}^{d}$ . This optimistic vector, also known as optimism, serves as a prediction of the incoming function $f_{t+1}(\\cdot)$ , leading to tighter regret bounds when accurate. Optimistic OMD updates the decisions in two steps: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbf x}_{t}=\\underset{{\\mathbf x}\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\left\\{\\langle M_{t},{\\mathbf x}\\rangle+{\\mathcal B}_{\\psi_{t}}({\\mathbf x},\\widehat{{\\mathbf x}}_{t})\\right\\},\\quad\\widehat{{\\mathbf x}}_{t+1}=\\underset{{\\mathbf x}\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\left\\{\\langle\\nabla f_{t}({\\mathbf x}_{t}),{\\mathbf x}\\rangle+{\\mathcal B}_{\\psi_{t}}({\\mathbf x},\\widehat{{\\mathbf x}}_{t})\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{B}_{\\psi_{t}}(\\mathbf{x},\\mathbf{y})=\\psi_{t}(\\mathbf{x})-\\psi_{t}(\\mathbf{y})-\\langle\\nabla\\psi_{t}(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle$ is the Bregman divergence associated with the regularizer $\\psi_{t}:\\mathcal{X}\\mapsto\\mathbb{R}$ . Optimistic OMD maintains two sequences: the sequence of submitted decisions $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ , and the one of intermediate decisions $\\{\\widehat{\\mathbf{x}}_{t}\\}_{t=1}^{T}$ . Although a simplified optimistic OMD with one-step update per round exists [Joulani et al.,  2020], we will demonstrate later that tuning the step size based on intermediate decisions is crucial for adapting to generalized smoothness. ", "page_idx": 3}, {"type": "text", "text": "2.3 Gradient-Variation Regret for Convex and Strongly Convex Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When minimizing the convex or strongly convex functions, we set the regularizer as $\\psi_{t}(\\mathbf{x})\\;=\\;$ $\\begin{array}{r}{\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2}}\\end{array}$ and optimistic OMD updates with the following steps: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\Pi_{\\mathcal{X}}\\left[\\widehat{\\mathbf{x}}_{t}-\\eta_{t}M_{t}\\right],\\quad\\widehat{\\mathbf{x}}_{t+1}=\\Pi_{\\mathcal{X}}\\left[\\widehat{\\mathbf{x}}_{t}-\\eta_{t}\\nabla f_{t}(\\mathbf{x}_{t})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\Pi_{\\mathcal{X}}[\\mathbf{y}]=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}}\\end{array}$ denotes  the Euclidea n projection operator. Next, we briefly review approaches for obtaining the gradient-variation bound under global smoothness. This bound typically follows from the regret analysis for optimistic OMD: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{R E G}_{T}\\lesssim\\frac{1}{\\eta_{T}}+\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-M_{t}\\|_{2}^{2}-\\sum_{t=1}^{T}\\frac{1}{\\eta_{t}}(\\|\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2}+\\|\\widehat{\\mathbf{x}}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "On the right-hand side, the second term is known as the stability term, while the third one is the negative terms that can be further bounded by $\\begin{array}{r}{\\mathcal{O}(-\\sum_{t=1}^{T}\\frac{1}{\\eta_{t}}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2})}\\end{array}$ . Previous studies [Chiang et al., 2012; Zhao et al., 2024] for gradient-variation regret under global smoothness often set optimism as $M_{t}\\,=\\,\\nabla f_{t-1}\\bigl({\\mathbf{x}}_{t-1}\\bigr)$ , such that, the positive stability term can be upper bounded by $\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}+\\eta_{t}\\|\\nabla f_{t-1}(\\mathbf{x}_{t})-\\nabla$ ft\u22121(xt\u22121)\u222522, where the first part can be directly converted to the desired gradient variation and the second part will be at most $\\dot{\\eta}_{t}L^{2}\\lVert\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\rVert_{2}^{2}$ under global smoothness. Given the smoothness constant $L$ , tuning the step size as $\\eta_{t}\\,\\leq\\,1/(4L)$ ensures $\\begin{array}{r}{\\mathcal{O}(\\eta_{t}L^{2}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2}-\\frac{1}{\\eta_{t}}\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|_{2}^{2})\\leq0.}\\end{array}$ , thus obtaining the gradient-variation bound. ", "page_idx": 3}, {"type": "text", "text": "However, under generalized smoothness, we do not have a global parameter $L$ for setting the step sizes, and the smoothness constants are related to the decisions. To follow the previous approach, optimistic OMD would require the smoothness constant before generating $\\mathbf{x}_{t}$ to tune the step size, ensuring that the negative terms are large enough to cancel $\\eta_{t}\\|\\bar{\\nabla{f}}_{t-1}(\\mathbf x_{t})-\\nabla{f}_{t-1}(\\mathbf x_{t-1})\\|_{2}^{2}$ . Nevertheless, the smoothness constant between $\\mathbf{x}_{t}$ and $\\mathbf{x}_{t-1}$ can only be evaluated after updating to $\\mathbf{x}_{t}$ , resulting in a contradiction. Unlike offilne optimization, where the function is fixed and smoothness constants can be shown to decrease along the trajectory [Li et al., 2023], in online optimization, the online functions change at each round, preventing the reuse of previous smoothness estimations. ", "page_idx": 3}, {"type": "text", "text": "To address this challenge, our key idea is to perform a trajectory-wise analysis and configure the algorithm using estimated smoothness so far. The key technical lemma is the local smoothness property of $\\ell_{t}$ -smooth functions [Li et al., 2023], which allows the smoothness constant between two points to be estimated in advance, provided that the two points are close enough. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (local smoothness [Li et al., 2023, Lemma 3.3]). Suppose $f:\\mathcal{X}\\mapsto\\mathbb{R}$ is $\\ell$ -smooth. For $\\forall\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ such that $\\begin{array}{r}{\\|\\mathbf{x}-\\mathbf{y}\\|_{2}\\leq\\frac{\\|\\nabla f(\\mathbf{x})\\|_{2}}{\\ell_{t}(2\\|\\nabla f(\\mathbf{x})\\|_{2})}}\\end{array}$ , $\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|_{2}\\leq\\ell(2\\|\\nabla f(\\mathbf{x})\\|_{2})\\cdot\\|\\mathbf{x}-\\mathbf{y}\\|_{2}$ ", "page_idx": 3}, {"type": "text", "text": "Recall that in the update procedures (3) of optimistic OMD, the submitted decision $\\mathbf{x}_{t}$ is updated based on the intermediate decision ${\\widehat{\\bf x}}_{t}$ . Therefore, it is convenient to control their distance and then exploit the local smoothness at  p oint ${\\widehat{\\bf x}}_{t}$ . Specifically, we set optimism $M_{t}=\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})$ and the step size $\\eta_{t}\\leq1/(4\\widehat{L}_{t-1})$ , where $\\widehat{L}_{t-1}=\\ell_{t-1}(2\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2})$ denotes the locally est imated smoothness and is used to tune the step size. This configuration leads to $\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}^{2}$ for the second term in Eq. (5), which can be further upper bounded as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat\\mathbf{x}_{t})\\|_{2}^{2}\\leq2\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}+2\\|\\nabla f_{t-1}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat\\mathbf{x}_{t})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first part is basically the favorable gradient variation, so it suffices to handle the second part. Performing the stability analysis for OMD and noticing the step size setting, it can be verified that $\\|\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\|_{2}\\,\\leq\\eta_{t}\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}\\,\\leq\\,\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}/(4\\widehat{L}_{t-1})$ . This satisfies the criteria for applying Lemma  1 to the $\\ell_{t-1}$ -smo oth function $f_{t-1}(\\cdot)$ , allowi ng us to upper bound the second term in (6) by $O(\\widehat{L}_{t-1}^{2}\\cdot\\|\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2})$ . We have clipped $\\eta_{t}$ by $1/(4\\widehat{L}_{t-1})$ , thereby ensuring the negative term is sufficie nt to cancel o ut the positive term. Below, we sum marize the result for convex functions. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under Assumptions $I-2$ and assuming online functions are convex, we set the optimism as $M_{t}=\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})$ and $\\bar{f}_{0}(\\cdot)=0$ , with step sizes as $\\eta_{1}=D$ and, for $t\\geq2$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}=\\operatorname*{min}\\left\\{\\sqrt{\\frac{D^{2}}{1+\\sum_{s=1}^{t-1}\\lVert\\nabla f_{s}({\\mathbf x}_{s})-\\nabla f_{s-1}({\\mathbf x}_{s})\\rVert_{2}^{2}}},\\ \\operatorname*{min}_{s\\in[t]}\\frac{1}{4\\ell_{s-1}(2\\lVert\\nabla f_{s-1}(\\widehat{\\mathbf x}_{s})\\rVert_{2})}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "optimistic OMD in (4) ensures the following regret bound, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{REG}_{T}\\leq\\mathcal{O}\\left(D\\sqrt{V_{T}}+\\widehat{L}_{\\operatorname*{max}}\\cdot D^{2}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{V_{T}=\\sum_{t=2}^{T}\\operatorname*{sup}_{\\mathbf x\\in\\mathcal X}\\|\\nabla f_{t}(\\mathbf x)-\\nabla f_{t-1}(\\mathbf x)\\|_{2}^{2}}\\end{array}$ measures the gradient variations and $\\widehat{L}_{\\mathrm{max}}=$ $\\operatorname*{max}_{t\\in[T]}\\widehat{L}_{t}$ is the maximum smoothness constant over the optimization trajectory. ", "page_idx": 4}, {"type": "text", "text": "This result implies a tighter bound in scenarios where the environments change slowly, i.e., $V_{T}=$ $\\mathcal{O}(1)$ . Meanwhile, it safeguards the worst-case optimal result since $V_{T}\\leq\\mathcal{O}(\\bar{T})$ holds in all cases. When assuming $\\ell_{t}(\\cdot)\\ \\leq\\ \\bar{L}$ for $t\\ \\in\\ [T]$ , the $\\ell_{t}$ -smoothness condition degenerates to the classic global $L$ -smoothness condition, and our result implies an $\\mathcal{O}(\\sqrt{V_{T}}+L D^{2})$ bound, which matches the best-known gradient-variation regret bounds with the first-order oracle [Chiang et al., 2012; Yan et al., 2023; Zhao et al., 2024] even in terms of the dependence on $D$ and $L$ . Compared to offilne optimization, our result depends onLmax, the maximum smoothness constant along the trajectory. This dependence arises from the a d versarial nature of online learning, where the loss functions chosen in consecutive rounds may differ significantly, making it hopeless to leverage the previous estimates of smoothness to improve the dependence. ", "page_idx": 4}, {"type": "text", "text": "We further provide an improved gradient-variation regret bound for strongly convex functions, with step size tuning based on recent result under global smoothness [Chen et al., 2024, $\\S\\ 3.4]$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Under Assumptions $I\\ -\\ 2$ and assuming online functions are $\\lambda$ -strongly convex, we set the optimism as $M_{t}\\,=\\,\\nabla f_{t-1}({\\widehat{\\mathbf{x}}}_{t})$ , $f_{0}(\\cdot)\\;=\\;0,$ , and step sizes as $\\eta_{1}\\,=\\,2/\\lambda$ and, for $t\\,\\geq\\,2$ , $\\begin{array}{r l}&{\\eta_{t}=2/(\\lambda t+16\\operatorname*{max}_{s\\in[t]}\\ell_{s-1}(2\\|\\nabla f_{s-1}(\\widehat{\\mathbf{x}}_{s})\\|_{2}))}\\end{array}$ , optimistic OMD in (4) ensures the regret bound $\\begin{array}{r}{\\mathrm{REG}_{T}\\leq\\mathcal{O}\\big(\\frac{1}{\\lambda}\\log V_{T}+\\widehat{L}_{\\operatorname*{max}}\\cdot D^{2}\\big)}\\end{array}$ , where $\\widehat{L}_{\\mathrm{max}}=\\operatorname*{max}_{t\\in[T]}\\widehat{L}_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "The above theorem requires the knowledge of curvature information $\\lambda$ . In Section 3, we design a universal method to remove this requirement and achieve the optimal guarantees for convex and strongly convex functions simultaneously without knowing $\\lambda$ . In Appendix B.2, we discuss the challenge to obtain a gradient-variation bound for exp-concave functions under Assumption 1. ", "page_idx": 4}, {"type": "text", "text": "3 Universal Online Learning under Generalized Smoothness ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Classic online learning algorithms require the curvature information of online functions as algorithmic parameters to achieve favorable regret guarantees. However, obtaining these curvature parameters can be difficult in practice. This challenge motivates the recent study of universal online learning [van Erven and Koolen, 2016; Cutkosky and Boahen, 2017; Wang et al., 2019; Mhammedi et al., 2019; Zhang et al., 2022a; Yan et al., 2023; Yang et al., 2024], which aims to design a single algorithm that can achieve optimal regrets without knowing the curvature information. In this section, we study universal online learning with gradient-variation regret under generalized smoothness. ", "page_idx": 4}, {"type": "text", "text": "3.1 Reviewing Related Work and Techniques ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We review related work on gradient-variation universal online learning under global smoothness [Zhang et al., 2022a; Yan et al., 2023]. To handle the unknown curvature, universal online learning algorithms utilize a two-layer structure, consisting of a meta-algorithm that ensembles a group of base-learners. Each base-learner optimizes functions with a specific convex curvature, while the meta-algorithm is designed to ensure that ensemble errors do not ruin base-learners\u2019 guarantees. Denoted by $N$ the number of base-learners, the decision $\\begin{array}{r}{{\\bf x}_{t}=\\sum_{i\\in[N]}p_{t,i}{\\bf x}_{t,i}}\\end{array}$ submitted by a twolayer structure algorithm comprises two key components: $p_{t}\\,\\in\\,\\Delta_{N}$ , the weights provided by the meta-algorithm, and $\\mathbf{x}_{t,i}$ , the decision of the $i$ -th base-learner. The analysis of a universal algorithm begins by decomposing the regret into two parts against any base-learner. In particular, we choose the base-learner with the best performance (the index $i_{\\star}$ is unknown) as the benchmark: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{R E G}_{T}=\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t,i_{\\star}})+\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t,i_{\\star}})-\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first part is the meta-regret, evaluating the meta-algorithm\u2019s performance against the best base-learner, and the second part, known as the base-regret, measures the best learner\u2019s performance. ", "page_idx": 5}, {"type": "text", "text": "Zhang et al. [2022a] advocate for a simple approach by employing the meta-algorithms with secondorder regret guarantees, which facilitates the analysis at the meta level. In specific, they use Adapt-ML-Prod [Gaillard et al., 2014] as the meta-algorithm, showing that the meta-regret for strongly convex and exp-concave functions are constants by exploiting the negative terms from convexity. Consider $\\lambda$ -strongly convex functions as an example and assume the $i_{\\star}$ -th base-learner ensures the optimal $\\textstyle{\\mathcal{O}}({\\frac{1}{\\lambda}}\\log{\\bar{V}}_{T})$ base-regret. At the meta level, Zhang et al. [2022a] pass the linearized regret $r_{t,i}\\,=\\,\\langle\\dot{\\nabla}f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle$ to the meta-algorithm for each base-learner. By strong convexity and the guarantees of Adapt-ML-Prod, the meta-regret can be bounded by a constant: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\index{\\mathbf{META-REG}}\\le\\sum_{t=1}^{T}r_{t,i_{*}}-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\lVert\\mathbf{x}_{t}-\\mathbf{x}_{t,i_{*}}\\rVert_{2}^{2}\\lesssim\\sqrt{\\sum_{t=1}^{T}r_{t,i_{*}}^{2}}-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\lVert\\mathbf{x}_{t}-\\mathbf{x}_{t,i_{*}}\\rVert_{2}^{2}\\le\\mathcal{O}(1),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the last inequality follows from $\\begin{array}{r}{\\sqrt{\\sum_{t}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle^{2}}\\,\\leq\\,\\widehat{G}_{\\operatorname*{max}}\\sqrt{\\sum_{t}\\lVert\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rVert_{2}^{2}}}\\end{array}$ and is then canceled by the negative terms via the AM-GM inequality. By  leveraging the negative terms from strong convexity, the meta-regret can be well-bounded, allowing the overall regret to be dominated by the base-regret, which is then controlled by selecting appropriate base-algorithms. ", "page_idx": 5}, {"type": "text", "text": "However, this method is unsuitable for producing the gradient-variation bound for convex functions. To address it, Yan et al. [2023] propose to use a meta-algorithm which ensures an optimistic and second-order regret bound while provides additional negative terms $\\begin{array}{r}{-\\sum_{t}\\|p_{t}-p_{t-1}\\bar{\\|}_{1}^{2}}\\end{array}$ . Besides showing that the meta-regret is a constant for strongly convex and exp-concave functions following the previous approach, with newly designed optimism, Yan et al. [2023] prove that the meta-regret for convex functions can be roughly bounded by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\sqrt{V_{T}}+\\sum_{t=1}^{T}\\lVert\\mathbf{x}_{t,i_{*}}-\\mathbf{x}_{t-1,i_{*}}\\rVert_{2}^{2}+L^{2}\\sum_{t=1}^{T}\\lVert\\boldsymbol{p}_{t}-\\boldsymbol{p}_{t-1}\\rVert_{1}^{2}+L^{2}\\sum_{t=1}^{T}\\sum_{i=1}^{N}p_{t,i}\\lVert\\mathbf{x}_{t,i}-\\mathbf{x}_{t-1,i}\\rVert_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first term is the gradient variation, matching the optimal order of convex functions. Yan et al. [2023] demonstrate that the remaining stability terms can be canceled through the collaboration between the base and meta levels [Zhao et al., 2024] with the prior knowledge of the global smoothness constant $L$ , thus obtaining the near-optimal gradient-variation bounds for convex functions as well. Nevertheless, the employed meta-algorithm already has a two-layer structure, resulting in a three-layer structure for the overall algorithm, which is relatively complicated. ", "page_idx": 5}, {"type": "text", "text": "3.2 Key Challenges and Main Ideas ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We aim to design a universal algorithm with the optimal gradient-variation bounds under generalized smoothness, which exhibits two challenges. First, the Lipschitz condition of online functions is unknown to the meta-algorithm, which requires the meta-algorithm to be Lipschitz-adaptive, provide a second-order regret, and enable predictions with optimism. Second, the combination of the ensemble method further complicates the estimation of smoothness constants, making it challenging to tune algorithms properly and to cancel stability terms as Yan et al. [2023] did. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We tackle the second challenge by utilizing a function-variation-to-gradient-variation conversion to derive the gradient-variation bounds at the meta level, drawing inspiration from the development of dynamic regret minimization [Bai et al., 2022]. This conversion technique decouples the meta and base levels, allowing us to avoid cancellation-based analysis. To illustrate, suppose a meta-algorithm ensuring $\\mathcal{O}(\\sqrt{\\sum_{t}(\\ell_{t,i_{\\star}}-m_{t,i_{\\star}})^{2}})$ provided optimism $\\pmb{m}_{t}=(m_{t,1},\\ldots,m_{t,N})$ . By setting $\\ell_{t,i_{\\star}}=$ $f_{t}(\\mathbf{x}_{t,i_{\\star}})-f_{t}^{'}(\\overline{{\\mathbf{x}_{\\mathrm{ref}}}})$ and $m_{t,i_{\\star}}=f_{t-1}(\\mathbf x_{t,i_{\\star}})-f_{t-1}(\\mathbf x_{\\mathrm{ref}})$ , where ${\\bf x}_{\\mathrm{ref}}$ is a fixed reference point, the meta regret bound becomes $\\begin{array}{r}{\\mathcal{O}(\\sqrt{\\sum_{t}[\\left(f_{t}(\\mathbf{x}_{t,i})-f_{t-1}(\\mathbf{x}_{t,i})\\right)-\\left(f_{t}(\\mathbf{x}_{\\mathrm{ref}})-f_{t-1}(\\mathbf{x}_{\\mathrm{ref}})\\right)]^{2}})}\\end{array}$ . By the mean value theorem, [ $(f_{t}(\\mathbf{x}_{t,i})-f_{t-1}(\\mathbf{x}_{t,i}))-(f_{t}(\\mathbf{x}_{\\mathrm{ref}})-f_{t-1}(\\mathbf{x}_{\\mathrm{ref}}))]^{2}$ equals the first term below, which can be further upper bounded by the gradient variation: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\langle\\nabla f_{t}(\\xi_{t,i})-\\nabla f_{t-1}(\\xi_{t,i}),\\mathbf{x}_{t,i}-\\mathbf{x}_{\\mathrm{ref}}\\rangle]^{2}\\leq D^{2}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\lVert\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This technique brings hope for minimizing the convex functions. To develop a universal method, our first attempt is to utilize MsMwC-Master [Chen et al., 2021] as the meta-algorithm, which satisfies all the three requirements imposed by the first challenge. Nevertheless, the heterogeneous inputs at the meta level present another challenge to this approach. The heterogeneity arises as we pass $r_{t,i}\\,=\\,f_{t}(\\mathbf{x}_{t})\\,-\\,\\bar{f}_{t}(\\mathbf{x}_{t,i})$ to the meta-algorithm for base-learner responsible for convex functions, leveraging the conversion technique, while $r_{t,i}=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle$ for base-learners minimizing strongly convex functions. It remains unclear how to adapt MsMwC-Master [Chen et al., 2021] to our heterogeneous inputs, as MsMwC-Master requires an $\\ell_{t}$ as inputs, and the guarantee is for the regret in the form of ${\\bf\\Delta}{\\bf r}_{t}\\,=\\,\\langle{\\bf p}_{t},{\\boldsymbol{\\ell}}_{t}\\rangle\\,-{\\boldsymbol{\\ell}}_{t}$ . However, such an $\\ell_{t}$ cannot be retrieved from our above design. Fortunately, we observe that the Prod algorithms [Cesa-Bianchi et al., 2007; Gaillard et al., 2014; Wei et al., 2016] are friendly to heterogeneous inputs. Technically, the Prod algorithms provide the same guarantees as long as $\\begin{array}{r}{\\sum_{i\\in[N]}p_{t,i}r_{t,i}\\leq0}\\end{array}$ , thanks to the potential-based analysis. Therefore, aside from the requirements  of the first challenge, we expect that the meta-algorithm can be analyzed similarly to the Prod algorithms, motivating us to design a new meta-algorithm. As a byproduct, we present a universal algorithm with a two-layer structure under global smoothness with the developed techniques. It is more efficient than that by Yan et al. [2023] and attains the optimal gradient-variation guarantees for convex, strongly convex, and exp-concave functions, at a cost of additional function value queries. We defer algorithms and regret bounds to Appendix C.5. ", "page_idx": 6}, {"type": "text", "text": "3.3 A New Lipschitz-Adaptive Meta-Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Algorithm 1, we present our meta-algorithm, which builds on optimistic Adapt-ML-Prod [Wei et al., 2016] and incorporates the clipping technique introduced by Cutkosky [2019] and further refined by Chen et al. [2021]. This algorithm, described in the language of Prediction with Experts\u2019 Advice (PEA), may be of independent interest beyond adapting to the generalized smoothness. Apart for satisfying all expected requirements, this algorithm offers a simpler design, which does not need to restart as opposed to Squint $\\boldsymbol{\\cdot}+\\mathrm{L}$ [Mhammedi et al., 2019] and MsMwC-Master [Chen et al., 2021]. ", "page_idx": 6}, {"type": "text", "text": "This efficiency improvement is achieved through a novel self-confident learning rate in Line 6 of Algorithm 1, unlike previous Lipschitz-adaptive algorithms that use a fixed learning rate and thus require restarts. In essence, our approach incorporates the clipping mechanism by adding $B_{t}^{2}$ to the denominator and removing the threshold on learning rates commonly applied in prior Prod algorithms [Gaillard et al., 2014; Wei et al., 2016]. This term $B_{t}^{2}$ acts as a threshold, ensuring that $\\bar{\\eta}_{t,i}|\\bar{r}_{t,i}-m_{t,i}|\\,\\leq\\,1/2$ , a critical condition in the analysis (typically satisfied when the Lipschitz constant is provided for prior Prod algorithms). In contrast to previous Prod methods with an explicit clipping operation like $\\operatorname*{min}\\{\\eta_{t+1,i}^{\\mathrm{co}\\mathrm{inmon}},1/(2B_{t})\\}$ , where both the commonly applied learning rate \u03b7tco+m1,mio and the threshold $1/(2B_{t})$ are decreasing and thus unstable in our cases, our learning rate remains stable by merging $B_{t}^{2}$ to the denominator, allowing us to keep $\\eta_{t,i}/\\eta_{t+1,i}$ well-bounded. Theorem 3 summarizes the guarantee of Algorithm 1 and the proof is provided in Appendix C.3. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Setting $m_{t,i}\\,=\\,\\langle p_{t},\\ell_{t-1}\\rangle\\,-\\,\\ell_{t-1,i}$ in Algorithm $^{\\,l}$ ensures that, for any $\\smash{i_{\\star}\\in\\bigcup_{k=1}^{N}}$ , $\\begin{array}{r}{\\sum_{t=1}^{T}\\langle p_{t},\\ell_{t}\\rangle-\\sum_{t=1}^{T}\\ell_{t,i_{\\star}}}\\end{array}$ is bounded as follows, where $\\begin{array}{r}{B_{T}=\\operatorname*{max}\\{B_{0},\\operatorname*{max}_{t\\in[T]}\\lVert\\boldsymbol{r}_{t}-\\boldsymbol{m}_{t}\\rVert_{\\infty}\\}}\\end{array}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\sqrt{\\sum_{t=1}^{T}(r_{t,i_{*}}-m_{t,i_{*}})^{2}}\\cdot\\left(\\log(N)+\\log(B_{T}+\\log T)\\right)+B_{T}\\right)\\leq\\tilde{\\mathcal{O}}\\left(\\sqrt{\\sum_{t=1}^{T}\\lVert\\ell_{t}-\\ell_{t-1}\\rVert_{\\infty}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lipschitz Adaptive Optimistic Adapt-ML-Prod Input: prior information of the scale $B_{0}$ , the number of experts $N$ . 1: Initialization: set $w_{1,i}=1$ , $m_{1,i}=0$ and $\\eta_{1,i}=1/\\sqrt{1+4B_{0}^{2}}$ for all $i\\in[N]$ . 2: for $t=1$ to $T$ do 3: Update the weight for $i\\in[N]\\;\\widetilde{w}_{t,i}=w_{t,i}\\exp(\\eta_{t,i}m_{t,i});$ ; 4: Calculate decision pt \u2208\u2206N w i th pt,i =  j\u2208\u03b7[tN,i]  w\u03b7 tt,,ji w t,j and submit it; 5: Receive $\\pmb{r}_{t}$ , update $B_{t}=\\operatorname*{max}\\{B_{t-1},\\|r_{t}\\!-\\!m_{t}\\|_{\\infty}\\}$ , and build $\\begin{array}{r}{\\bar{r}_{t,i}=m_{t,i}\\!+\\!\\frac{B_{t-1}}{B_{t}}(r_{t,i}\\!-\\!m_{t,i})}\\end{array}$ ; 6: Update the learning rate for i \u2208[N]: \u03b7t+1,i = 1+ ts=1(r\u00afs,i1\u2212ms,i)2+4Bt2 ; \u03b7t+1,i 7: Update the weight for $i\\in[N]$ : wt+1,i = wt,i exp \u03b7t,ir\u00aft,i \u2212\u03b7t2,i(r\u00aft,i \u2212mt,i)2 \u03b7t,i . 8: end for ", "page_idx": 7}, {"type": "text", "text": "Input: curvature coefficient pool $\\mathcal{H}$ , number of base-learners $N$ , prior information of the scale $B_{0}$   \n1: Initialization: Send $N$ and $B_{0}$ to the meta-algorithm, for $\\lambda\\in\\mathcal H$ , initialize an algorithm in   \nTheorem 2 with it; initialize an algorithm in Theorem 1.   \n2: for $t=1$ to $T$ do   \n3: Obtain $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ from meta-algorithm, $\\mathbf{x}_{t,i}$ from each base-learner $i\\in[N]$ ;   \n4: Submit $\\begin{array}{r}{{\\bf x}_{t}=\\sum_{i\\in[N]}p_{t,i}{\\bf x}_{t,i}}\\end{array}$ ;   \n5: Receive $f_{t}(\\cdot)$ and send it to each base-learner for update;   \n6: For strongly convex functions learners: set $r_{t,i}=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle$ ;   \n7: For convex functions learner: set $r_{t,i}=f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,i})$ ;   \n8: Send $m_{t+1,i}=f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,i})$ to the meta-algorithm for $i\\in[N]$ .   \n9: end for ", "page_idx": 7}, {"type": "text", "text": "Our algorithm improves efficiency at the cost of an additional factor $\\mathcal{O}(\\sqrt{\\log N})$ . This factor is ignorable for universal online learning since we set $N\\,=\\,\\mathcal{O}(\\log T)$ , and the factor ${\\mathcal{O}}(\\log\\log T)$ is often treated as a constant [Gaillard et al., 2014; Luo and Schapire, 2015]. Considering other related Lipschitz-adaptive algorithms, Mhammedi et al. [2019] obtain a regret bound of $\\begin{array}{r}{\\mathcal{O}(\\sqrt{\\sum_{t}(r_{t,i_{\\star}})^{2}\\cdot(\\log(N)+\\log\\log(B_{T}T))}+B_{T}\\log(N))}\\end{array}$ , which offers better dependence on the dominant term $\\sqrt{\\Sigma_{t}(r_{t,i_{\\star}})^{2}}$ but it is unclear how to include optimism. Chen et al. [2021] achieve a bound of $\\begin{array}{r}{\\mathcal{O}(\\sqrt{\\sum_{t}(\\ell_{t,i_{\\star}}-m_{t,i_{\\star}})^{2}\\cdot\\log(N T)}+B_{T}\\log(N T))}\\end{array}$ with a two-layer algorithm; however, the ${\\mathcal{O}}({\\sqrt{\\log T}})$ term would ruin the desired ${\\mathcal{O}}(\\log V_{T})$ bound for strongly convex functions. We remark that the compared methods enjoy other strengths not discussed here, such as the ability to compete with an arbitrary competitor $\\pmb{x}\\in\\Delta_{N}$ and the versatility to handle various learning scenarios, while our method is sufficient for our purpose and the only option to tackle all the challenges as we mention in Section 3.2. Lastly, notice that the optimism $\\mathbf{\\nabla}m_{t}$ involves the decision $\\textstyle p_{t}$ , which might be improper since $\\textstyle p_{t}$ depends on $\\mathbf{\\nabla}m_{t}$ as well. We refer readers to Appendix C.1 for efficiently setting $\\mathbf{\\nabla}m_{t}$ through a univariate binary search. ", "page_idx": 7}, {"type": "text", "text": "We emphasize that optimism $\\mathbf{\\nabla}m_{t}$ in our algorithm is not chosen arbitrarily. In Line 5, we clip the regret with optimism to keep them on the same scale. The performance is then evaluated based on the clipped regret $\\bar{r}_{t,i}$ . For the analytical purpose, it is essential that $\\langle p_{t},\\bar{\\pmb{r}}_{t}\\rangle\\,\\leq\\,0$ , and a sufficient condition for this is ensuring $\\langle\\pmb{p}_{t},\\dot{m}_{t}\\rangle\\,\\leq\\,0$ , which imposes an additional requirement on $\\mathbf{\\nabla}m_{t}$ . In Appendix C.2, we discuss how this requirement introduces challenges for exp-concave functions minimization in universal online learning. ", "page_idx": 7}, {"type": "text", "text": "3.4 Overall Algorithm and Regret Guarantees ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The function-variation-to-gradient-variation technique decouples the design of universal methods into the base and meta levels, and we are ready to combine the proposed components together. We employ algorithms in Section 2.3 as the base-learners and use Algorithm 1 as the meta-learner, concluding in Algorithm 2. Theorem 4 presents its guarantee with the proof in Appendix C.4. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Under Assumptions $I\\ -\\ 2$ and assuming a global lower bound such that $\\underline{{f}}\\ \\leq\\ f_{t}(\\mathbf{x})$ for any $\\mathbf{x}\\,\\in\\,{\\boldsymbol{\\mathcal{X}}},t\\,\\in\\,[T]$ , setting $N\\,=\\,\\lceil\\log_{2}T\\rceil+1,$ , defining the curvature coefficient pool $\\mathcal{H}=$ $\\left\\{2^{i-1}/T:i\\in[N-\\dot{1}]\\right\\}$ , and specifying $B_{0}$ , Algorithm 2 simultaneously ensures: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{REG}_{T}\\leq\\left\\{\\begin{array}{l l}{\\mathcal{O}(\\sqrt{V_{T}}\\cdot\\log(B_{T}+\\log T)),}&{(c o n v e x),}\\\\ {\\mathcal{O}\\left(\\frac{1}{\\lambda}\\log V_{T}+\\widehat{G}_{\\operatorname*{max}}^{2}(\\log(B_{T}+\\log T))^{2}/\\lambda\\right),}&{(\\lambda{-}s t r o n g l y\\;c o n v e x),}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\lambda\\in[1/T,1]$ , $\\begin{array}{r}{B_{T}=\\mathcal{O}(\\operatorname*{max}\\{B_{0},D\\operatorname*{max}_{t\\in[T]}\\operatorname*{sup}_{\\mathbf{x}}\\lvert\\lvert\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\rvert\\rvert_{2}\\})}\\end{array}$ and $\\widehat{G}_{\\mathrm{max}}$ is the Lipschitz constant on the optimization trajectory. ", "page_idx": 8}, {"type": "text", "text": "Without loss of generality, we assume $\\lambda\\,\\in\\,[1/T,1]$ for strongly convex functions. If $\\lambda<1/T$ , the optimal ${\\mathcal O}((\\log V_{T})/\\dot{\\lambda})$ bound would imply linear regret, in which case we would treat them as general convex functions. If $\\lambda>1$ , our result is slightly worse than the optimal one by a negligible constant factor. This simplification is also employed by Zhang et al. [2022a]; Yan et al. [2023]. ", "page_idx": 8}, {"type": "text", "text": "Remark 1. This theorem additionally requires the lower bound of loss functions, which is used to perform the binary search when setting the optimism. We defer the details of the binary search to Appendix C.1. This assumption is also employed recently in parameter-free optimizations [Hazan and Kakade, 2019; Attia and Koren, 2024; Khaled and Jin, 2024], and we can simply choose $\\underline{{f}}=0$ in empirical risk minimization settings [Hazan and Kakade, 2019]. ", "page_idx": 8}, {"type": "text", "text": "4 Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate the importance of our results by providing two applications (SEA model and online games), where new results can be directly implied from our findings. ", "page_idx": 8}, {"type": "text", "text": "4.1 Stochastically Extended Adversarial (SEA) Model ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The stochastically extended adversarial (SEA) model [Sachs et al., 2022] interpolates adversarial and stochastic online optimization. It assumes that the environments select the loss function $f_{t}(\\cdot)$ from a distribution $\\mathcal{D}_{t}$ . The adversarial nature is characterized by shifts in distribution $\\mathcal{D}_{t}$ , and when $\\mathcal{D}_{t}$ remains constant, the model captures the environments\u2019 stochastic behavior. The following quantities are introduced to measure the levels of adversarial and stochastic behaviors in environments: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Sigma_{1:T}^{2}=\\mathbb{E}\\left[\\sum_{t=2}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\lVert\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\rVert_{2}^{2}\\right],\\sigma_{1:T}^{2}=\\sum_{t=1}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\mathbb{E}[\\lVert\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\rVert_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where we denote by $F_{t}(\\mathbf{x})=\\mathbb{E}_{f_{t}\\sim\\mathcal{D}_{t}}[f_{t}(\\mathbf{x})]$ . In above, $\\Sigma_{1:T}^{2}$ represents the adversarial shift of the distribution, and $\\sigma_{1:T}^{2}$ denotes the stochastic variance. ", "page_idx": 8}, {"type": "text", "text": "Sachs et al. [2022] prove an $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ regret for convex functions, and a refined regret bound of $\\mathcal{O}((\\sigma_{\\mathrm{max}}^{2}+\\Sigma_{\\mathrm{max}}^{2})\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ \u03a321:T )) for strongly convex functions is obtained by Chen et al. [2023a]; Sachs et al. [2023], where \u03c32max $\\begin{array}{r}{\\sigma_{\\mathrm{max}}^{2^{\\ast}\\,^{\\ast}}=\\mathrm{max}_{t\\in[T]}\\,\\mathrm{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\,\\mathbb{E}[\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}]}\\end{array}$ and $\\Sigma_{\\mathrm{max}}^{2}\\,=\\,\\mathrm{max}_{t=1}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\lVert\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\rVert_{2}^{2}$ . Yan et al. [2023] present a universal method which can obtain $\\widetilde{\\mathcal{O}}(\\sqrt{\\sigma_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})$ and $\\mathcal{O}((\\sigma_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2})\\log(\\sigma_{1:T}^{2}+\\Sigma_{1:T}^{2}))$ bounds for convex and strongly conv e x functions. However, these results require the global smoothness assumption. ", "page_idx": 8}, {"type": "text", "text": "Our result in Section 3 implies a new finding for the SEA model, relaxing the assumption from the global smoothness to the generalized smoothness, while adapting to unknown curvature, summarized in Corollary 1. The proof can be found in Appendix D.1. ", "page_idx": 8}, {"type": "text", "text": "Corollary 1. Under Assumptions $I\\,\\cdot\\,2$ and assuming a global lower bound for the loss functions such that $\\underline{{f}}\\ \\leq\\ f_{t}(\\mathbf{x})$ for any $\\mathbf{x}\\,\\in\\,\\mathcal{X},t\\,\\in\\,[T],$ , setting $\\bar{N}\\,=\\,\\lceil\\log_{2}T\\rceil+1,$ , defining the curvature coefficient \u00afpool $\\begin{array}{r}{\\mathcal{H}=\\{2^{i-1}/T:i\\in[N-1]\\}}\\end{array}$ , and specifying $B_{0}$ with a specific value, then, under the SEA model, Algorithm 2 simultaneously ensures: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{REG}_{T}]\\leq\\left\\{\\begin{array}{l l}{\\mathcal{O}\\big((\\sqrt{\\widetilde{\\sigma}_{1:T}^{2}}+\\sqrt{\\Sigma_{1:T}^{2}})\\cdot\\log(\\widehat{G}_{\\operatorname*{max}}D)\\big),}&{(c o n\\nu e x),}\\\\ {\\mathcal{O}\\left((\\widetilde{\\sigma}_{\\operatorname*{max}}^{2}+\\Sigma_{\\operatorname*{max}}^{2})\\log(\\widetilde{\\sigma}_{1:T}^{2}+\\Sigma_{1:T}^{2})\\right),}&{(s t r o n g l y\\;c o n\\nu e x),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widehat{G}_{\\mathrm{max}}$ is the maximum e mpirical Lipschitz  constant, $\\begin{array}{r}{\\widetilde{\\sigma}_{1:T}^{2}\\;=\\;\\sum_{t=1}^{T}\\mathbb{E}[\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla f_{t}(\\mathbf{x})-}\\end{array}$ $\\nabla F_{t}({\\bf x})\\|_{2}^{2}]$ , and $\\begin{array}{r}{\\widetilde{\\sigma}_{\\operatorname*{max}}^{2}=\\mathbb{E}[\\operatorname*{max}_{t\\in[T]}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}]}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Note that, in real-world streaming learning applications, this corollary can offer a more generalized depiction of data throughput with limited computing resources [Zhou, 2024; Wang et al., 2024], given the connection between these scenarios and the SEA model [Chen et al., 2024, $\\S\\ S.6]$ . Our result depends on $\\widetilde{\\sigma}_{1;T}^{2}$ , a larger quantity than $\\sigma_{1:T}^{2}$ but still can track the stochastic variance. This is because our algori t hm utilizes the information afterward $\\mathbf x_{t-1}$ to generate . We refer the interested readers for this subtle issue to the discussion by Chen et al. [2024]. This dependence currently is unknown how to improve even under the global smoothness condition since we need to leverage the function variation to produce gradient variation, inevitably involving the afterward information. ", "page_idx": 9}, {"type": "text", "text": "4.2 Fast Rates in Games ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our second application explores the min-max game, aiming to achieve an $\\varepsilon$ -approximate solution to the problem $\\mathrm{min}_{\\mathbf{x}\\in\\mathcal{X}}\\operatorname*{max}_{\\mathbf{y}\\in\\mathcal{Y}}f(\\mathbf{x},\\mathbf{y})$ within an $\\mathcal{O}(1/T)$ fast convergence rate. Here, we assume that $f(\\cdot,\\mathbf{y})$ is convex for any $\\textbf{y}\\in\\mathcal{V}$ , and correspondingly, $f(\\mathbf{x},\\cdot)$ is concave given any $\\textbf{x}\\in\\:\\mathcal{X}$ . Additionally, we assume that both $\\mathcal{X}\\,\\subset\\,\\mathbb{R}^{n}$ and $\\mathcal{V}\\subset\\mathbb{R}^{m}$ are bounded convex sets. Pioneering research [Syrgkanis et al., 2015] demonstrates that optimistic algorithms can reach a convergence rate of $\\mathcal{O}(1/T)$ by leveraging gradient variation. However, these results are limited to the global smoothness condition. In this part, we demonstrate that our findings in Section 2 directly imply a new algorithm that can exploit generalized smoothness. ", "page_idx": 9}, {"type": "text", "text": "Following the notations of Nemirovski [2004], we define $\\mathcal{Z}=\\mathcal{X}\\!\\times\\!\\mathcal{Y}$ , $\\mathbf{z}=(\\mathbf{x},\\mathbf{y})\\in{\\mathcal{Z}}$ and introduce an operator $F:\\mathcal{Z}\\rightarrow\\mathbb{R}^{n}\\times\\mathbb{R}^{m}$ with $F(\\mathbf{z})=(\\nabla_{\\mathbf{x}}f(\\mathbf{x},\\mathbf{y}),-\\nabla_{\\mathbf{y}}f(\\mathbf{x},\\mathbf{y}))$ ). We extend the concept of $\\ell_{}$ -smoothness to the min-max optimization setting as follows. ", "page_idx": 9}, {"type": "text", "text": "Definition 2 ( $\\ell$ -smoothness for min-max game). A differentiable convex-concave function $f:\\mathcal X\\times$ $\\mathcal{V}\\mapsto\\mathbb{R}$ is called $\\ell$ -smooth with a non-decreasing link function $\\ell:[0,+\\infty)\\mapsto(0,+\\infty)$ if it satisfies: for any $\\mathbf{z}_{1},\\mathbf{z}_{2}\\in\\mathcal{Z}$ , if $\\begin{array}{r}{\\mathbf{z}_{1},\\mathbf{z}_{2}\\in\\mathbb{B}\\big(\\mathbf{z},\\frac{\\|F(\\mathbf{z})\\|_{2}}{\\ell(2\\|\\nabla F(\\mathbf{z})\\|_{2})}\\big)}\\end{array}$ , then $\\|F(\\mathbf{z}_{1})-F(\\mathbf{z}_{2})\\|_{2}\\leq\\ell(2\\|F(\\mathbf{z})\\|)\\cdot\\|\\mathbf{z}_{1}-$ ${\\bf z}_{2}\\|_{2}$ , where ${\\mathbb{B}}(\\mathbf{z},r)$ denotes the Euclidean ball centered at point $\\mathbf{z}$ with radius $r$ . ", "page_idx": 9}, {"type": "text", "text": "This definition is a counterpart to Definition 1 in the min-max game, but weaker as it does not require the twice-differentiability requirement. The $\\varepsilon$ -approximate solution $(\\mathbf{x}_{\\star},\\mathbf{y}_{\\star})$ to the min-max game is formally defined by $f(\\mathbf{x}_{\\star},\\mathbf{\\bar{y}})-\\varepsilon\\leq f(\\mathbf{x}_{\\star},\\mathbf{y}_{\\star})\\leq f(\\mathbf{x},\\mathbf{y}_{\\star})+\\varepsilon$ for any $\\mathbf{x}\\in\\mathcal{X},\\mathbf{y}\\in\\mathcal{Y}$ . To achieve the fast convergence rate to the solution, indeed our result in Section 2 can be directly applied to the min-max game and obtains the following tailored algorithm for min-max optimization: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}=\\Pi_{\\mathcal{Z}}\\left[\\widehat{\\mathbf{z}}_{t}-\\eta_{t}F(\\widehat{\\mathbf{z}}_{t})\\right],\\quad\\widehat{\\mathbf{z}}_{t+1}=\\Pi_{\\mathcal{Z}}\\left[\\widehat{\\mathbf{z}}_{t}-\\eta_{t}F(\\mathbf{z}_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In above we set the optimism at the point ${\\widehat{\\mathbf{z}}}_{t}$ in order to exploit the smoothness locally. We conclude our result in Corollary 2, with the proof a v ailable in Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "Corollary 2. Assume that the convex-concave function $f(\\mathbf{x},\\mathbf{y})$ is $\\ell$ -smooth, and the domain $\\mathcal{Z}$ is bounded with diameter $D$ . By applying the tuning strategy described in Theorem 1, and defining the final approximated solution as $\\begin{array}{r}{\\bar{\\mathbf{z}}_{T}=\\frac{\\bar{1}}{T}\\sum_{t=1}^{T}\\mathbf{z}_{t}}\\end{array}$ , where $\\mathbf{z}_{t}$ is generated by Eq. (10), we achieve an $\\varepsilon$ -approximate solution with a convergence rate of $\\mathcal{O}(1/T)$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we provide a systematic study of gradient-variation online learning under the generalized smoothness condition. We exploit trajectory-wise smoothness to achieve the optimal regret bounds: $O(\\sqrt{V_{T}})$ for convex functions and ${\\mathcal{O}}(\\log V_{T})$ for strongly convex functions, respectively. We further consider more complicated online learning scenarios, motivating us to design a new Lipschitz-adaptive meta-algorithm, which can be of independent interest. Hinging on this algorithm with the function-variation-to-gradient-variation technique, we design a universal algorithm which guarantees the optimal results for convex functions and strongly convex functions simultaneously without knowing the curvature. In addition, our findings directly imply new results in stochastic extended adversarial online learning and fast-rate games under generalized smoothness. ", "page_idx": 9}, {"type": "text", "text": "An important future direction for future research is to explore whether our method can be further extended to accommodate the one-gradient feedback model, where the learner receives only the gradient information of the decision submitted in each round. Another interesting problem is to exploit the exp-concavity in gradient-variation online learning under generalized smoothness. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by National Key R&D Program of China (2022ZD0114800) and JiangsuSF (BK20220776). Peng Zhao was supported in part by the Xiaomi Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "J. D. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal stragies and minimax lower bounds for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages 415\u2013424, 2008.   \nD. Ataee Tarzanagh, P. Nazari, B. Hou, L. Shen, and L. Balzano. Online bilevel optimization: Regret analysis of online alternating gradient methods. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 2854\u20132862, 2024.   \nA. Attia and T. Koren. How free is parameter-free stochastic optimization? In Proceedings of the 41st International Conference on Machine Learning (ICML), pages 2009\u20132034, 2024.   \nP. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48\u201375, 2002.   \nY. Bai, Y.-J. Zhang, P. Zhao, M. Sugiyama, and Z.-H. Zhou. Adapting to online label shift with provable guarantees. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 29960\u201329974, 2022.   \nA. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operation Research Letter, 31(3):167\u2013175, 2003.   \nN. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.   \nN. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66(2-3):321\u2013352, 2007.   \nL. Chen, H. Luo, and C. Wei. Impossible tuning made possible: A new expert algorithm and its applications. In Proceedings of the 34th Conference on Learning Theory (COLT), pages 1216\u2013 1259, 2021.   \nS. Chen, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic and adversarial online convex optimization. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 5002\u20135035, 2023a.   \nS. Chen, Y.-J. Zhang, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic and adversarial online convex optimization. Journal of Machine Learning Research, 25 (178):1 \u2013 62, 2024.   \nZ. Chen, Y. Zhou, Y. Liang, and Z. Lu. Generalized-smooth nonconvex optimization is as efficient as smooth nonconvex optimization. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 5396\u20135427, 2023b.   \nC.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In Proceedings of the 25th Conference On Learning Theory (COLT), pages 6.1\u20136.20, 2012.   \nM. Crawshaw, M. Liu, F. Orabona, W. Zhang, and Z. Zhuang. Robustness to unbounded smoothness of generalized signsgd. In Advances in Neural Information Processing Systems 34 (NeurIPS), volume 35, pages 9955\u20139968, 2022.   \nA. Cutkosky. Artificial constraints and hints for unbounded online learning. In Proceedings of the 32nd Annual Conference on Computational Learning Theory (COLT), pages 874\u2013894, 2019.   \nA. Cutkosky and K. Boahen. Stochastic and adversarial online learning without hyperparameters. In Advances in Neural Information Processing Systems 30 (NIPS), pages 5059\u20135067, 2017.   \nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.   \nM. Faw, L. Rout, C. Caramanis, and S. Shakkottai. Beyond uniform smoothness: A stopped analysis of adaptive SGD. In Proceedings of the 36th Conference on Learning Theory (COLT), pages 89\u2013 160, 2023.   \nP. Gaillard, G. Stoltz, and T. van Erven. A second-order bound with excess losses. In Proceedings of the 27th Annual Conference on Learning Theory (COLT), pages 176\u2013196, 2014.   \nE. Hazan. Introduction to Online Convex Optimization. Foundations and Trends in Optimization, 2 (3-4):157\u2013325, 2016.   \nE. Hazan and S. Kakade. Revisiting the polyak step size. ArXiv preprint, arXiv:1905.00313, 2019.   \nE. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169\u2013192, 2007.   \nA. Jacobsen and A. Cutkosky. Parameter-free mirror descent. In Proceedings of the 35th Annual Conference on Learning Theory (COLT), pages 4160\u20134211, 2022.   \nA. Jacobsen and A. Cutkosky. Unconstrained online learning with unbounded losses. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 14590\u201314630, 2023.   \nP. Joulani, A. Gy\u00f6rgy, and C. Szepesv\u00e1ri. A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, variance reduction, and variational bounds. Theoretical Computer Science, 808:108\u2013138, 2020.   \nA. Khaled and C. Jin. Tuning-free stochastic optimization. In Proceedings of the 41st International Conference on Machine Learning (ICML), pages 23622\u201323661, 2024.   \nB. Kulis and P. L. Bartlett. Implicit online learning. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 575\u2013582, 2010.   \nH. Li, J. Qian, Y. Tian, A. Rakhlin, and A. Jadbabaie. Convex and non-convex optimization under generalized smoothness. In Advances in Neural Information Processing Systems 35 (NeurIPS), 2023.   \nH. Lu, R. M. Freund, and Y. Nesterov. Relatively smooth convex optimization by first-order methods, and applications. SIAM Journal on Optimization, 28(1):333\u2013354, 2018.   \nH. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge. In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT), pages 1286\u20131304, 2015.   \nZ. Mhammedi, W. M. Koolen, and T. van Erven. Lipschitz adaptivity with multiple learning rates in online learning. In Proceedings of the 32nd Annual Conference on Computational Learning Theory (COLT), pages 2490\u20132511, 2019.   \nA. Nemirovski. Prox-method with rate of convergence $O(1/t)$ for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.   \nA. Nemirovskij and D. B. Yudin. Problem complexity and method efficiency in optimization. SIAM Review, 27(2):264\u2013265, 1985.   \nY. Nesterov. Lectures on Convex Optimization, volume 137. Springer, 2018.   \nC. Nicol\u00f2 and O. Francesco. Temporal variability in implicit online learning. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 12377\u201312387, 2020.   \nF. Orabona. A Modern Introduction to Online Learning. ArXiv preprint, arXiv:1912.13213, 2019.   \nF. Orabona and D. P\u00e1l. Scale-free online learning. Theoretical Computer Science, 716:50\u201369, 2018.   \nA. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th Conference On Learning Theory (COLT), pages 993\u20131019, 2013a.   \nA. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences. In Advances in Neural Information Processing Systems 26 (NIPS), pages 3066\u20133074, 2013b.   \nA. Reisizadeh, H. Li, S. Das, and A. Jadbabaie. Variance-reduced clipping for non-convex optimization. ArXiv preprint, arXiv:2303.00883, 2023.   \nS. Sachs, H. Hadiji, T. van Erven, and C. Guzm\u00e1n. Between stochastic and adversarial online convex optimization: Improved regret bounds via smoothness. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 691\u2013702, 2022.   \nS. Sachs, H. Hadiji, T. van Erven, and C. Guzm\u00e1n. Accelerated rates between stochastic and adversarial online convex optimization. ArXiv preprint, arXiv:2303.03272, 2023.   \nV. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in games. In Advances in Neural Information Processing Systems 28 (NIPS), pages 2989\u20132997, 2015.   \nM. Telgarsky. Stochastic linear optimization never overftis with quadratically-bounded losses on general data. In Proceedings of the 35th Annual Conference on Learning Theory (COLT), pages 5453\u20135488, 2022.   \nC. Tsai, Y. Lin, and Y. Li. Data-dependent bounds for online portfolio selection without lipschitzness and smoothness. In Advances in Neural Information Processing Systems 36 (NeurIPS), pages 62764\u201362791, 2023.   \nT. van Erven and W. M. Koolen. MetaGrad: Multiple learning rates in online learning. In Advances in Neural Information Processing Systems 29 (NIPS), pages 3666\u20133674, 2016.   \nB. Wang, H. Zhang, Z. Ma, and W. Chen. Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions. In Proceedings of the 36th Annual Conference on Learning Theory (COLT), pages 161\u2013190, 2023.   \nG. Wang, S. Lu, and L. Zhang. Adaptivity and optimality: A universal algorithm for online convex optimization. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI), pages 659\u2013668, 2019.   \nJ. Wang, M. Yu, P. Zhao, and Z.-H. Zhou. Learning with adaptive resource allocation. In Proceedings of the 41st International Conference on Machine Learning (ICML), pages 52099\u201352116, 2024.   \nC.-Y. Wei, Y.-T. Hong, and C.-J. Lu. Tracking the best expert in non-stationary stochastic environments. In Advances in Neural Information Processing Systems 29 (NIPS), pages 3972\u20133980, 2016.   \nY.-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradient variations: A multilayer online ensemble approach. In Advances in Neural Information Processing Systems 36 (NeurIPS), pages 37682\u201337715, 2023.   \nT. Yang, M. Mahdavi, R. Jin, and S. Zhu. Regret bounded by gradual variation for online convex optimization. Machine Learning, 95(2):183\u2013223, 2014.   \nW. Yang, Y. Wang, P. Zhao, and L. Zhang. Universal online convex optimization with 1 projection per round. ArXiv preprint, arXiv:2405.19705, 2024.   \nB. Zhang, J. Jin, C. Fang, and L. Wang. Improved analysis of clipping algorithms for non-convex optimization. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 15511\u2013 15521, 2020a.   \nJ. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In Proceedings of the 8th International Conference on Learning Representations (ICLR), 2020b.   \nL. Zhang, G. Wang, J. Yi, and T. Yang. A simple yet universal strategy for online convex optimization. In Proceedings of the 39th International Conference on Machine Learning (ICML), pages 26605\u201326623, 2022a.   \nM. Zhang, P. Zhao, H. Luo, and Z.-H. Zhou. No-regret learning in time-varying zero-sum games. In Proceedings of the 39th International Conference on Machine Learning (ICML), pages 26772\u2013 26808, 2022b.   \nP. Zhao, Y.-J. Zhang, L. Zhang, and Z.-H. Zhou. Dynamic regret of convex and smooth functions. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 12510\u201312520, 2020.   \nP. Zhao, Y.-J. Zhang, L. Zhang, and Z.-H. Zhou. Adaptivity and non-stationarity: Problemdependent dynamic regret for online convex optimization. Journal of Machine Learning Research, 25(98):1 \u2013 52, 2024.   \nZ.-H. Zhou. Learnability with time-sharing computational resource concerns. National Science Review, 11(10):nwae204, 2024.   \nM. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML), pages 928\u2013936, 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we review the related work in gradient-variation online learning, the generalized smoothness conditions, and the Lipschitz-adaptive algorithms. ", "page_idx": 13}, {"type": "text", "text": "A.1 Gradient-Variation Online Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The gradient-variation quantity defined in Eq. (2) is firstly introduced by Chiang et al. [2012] for global smooth functions. Chiang et al. [2012] obtain $O(\\sqrt{V_{T}})$ and $\\bar{\\mathcal{O}}(\\frac{d}{\\alpha}\\log\\bar{V_{T}})$ regret bounds respectively for general convex and $\\alpha$ -exp-concave functions. Zhang et al. [2022a] later achieve $O(\\frac{1}{\\lambda}\\log V_{T})$ result for $\\lambda$ -strongly convex functions. Considering the non-stationary environments, Zhao et al. [2020] study gradient variation under the dynamic regret, a strengthened measure that requires the learner to compete with a sequence of time-varying comparators. Their work revives the study of gradient-variation online learning, especially revealing the importance of the stability analysis in the two-layer online ensemble. Their results are further improved in Zhao et al. [2024], where they only require one gradient per round with the same optimal guarantees through the collaboration between the meta and the base. For universal online learning [van Erven and Koolen, 2016], there are several recent researches trying to derive the gradient-variation bounds in this context [Zhang et al., 2022a; Sachs et al., 2023; Yan et al., 2023]. ", "page_idx": 13}, {"type": "text", "text": "Gradient variation demonstrates its importance due to its close connection with many online learning problems. Pioneering works [Rakhlin and Sridharan, 2013a; Syrgkanis et al., 2015; Zhang et al., 2022b] demonstrate the importance of gradient variation via a useful property (Regret bounded by Variation in Utilities, RVU) to achieve fast-rate convergences in multi-player games. Recently, Sachs et al. [2022] and Chen et al. [2024] reveal that the gradient variation is also essential in bridging stochastic and adversarial convex optimization. ", "page_idx": 13}, {"type": "text", "text": "However, all the mentioned works require the global smoothness assumption. As highlighted in Proposition 2 in Appendix B, the smoothness condition is necessary to obtain the gradient-variation bounds for algorithms with first-order oracle assumption [Nesterov, 2018]. Exceptions are that Jacobsen and Cutkosky [2022]; Bai et al. [2022] obtain the gradient-variation bounds without the smoothness assumption, but they require implicit updates [Kulis and Bartlett, 2010; Nicol\u00f2 and Francesco, 2020] per round, which may be inefficient under online settings. Our work considers generalized smoothness, and we develop first-order methods to achieve gradient-variation bounds. ", "page_idx": 13}, {"type": "text", "text": "A.2 Generalized Smoothness ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Generalized smoothness has received increasing attention in recent years since the analysis under the standard global smoothness is insufficient to depict the dynamics of neural network optimization. Based on the empirical observation for the relationship between the smoothness and the gradients of LSTMs, Zhang et al. [2020b] relax the global smoothness assumption by $\\left(L_{0},L_{1}\\right)$ -smoothness condition, which assumes $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\le\\bar{L_{0}}+\\bar{L_{1}}\\|\\nabla f(\\mathbf{x})\\|_{2}$ for offilne objective function $f:\\mathcal{X}\\mapsto\\mathbb{R}$ . With this new smoothness assumption, Zhang et al. [2020b] explain the importance of gradient clipping in neural network training. There are a variety of subsequent works developed for different methods and settings [Zhang et al., 2020a; Crawshaw et al., 2022; Reisizadeh et al., 2023; Faw et al., 2023; Wang et al., 2023]. There are studies further generalizing the $\\left(L_{0},L_{1}\\right)$ -smoothness condition. Chen et al. [2023b] introduce the $\\alpha$ -symmetric smoothness condition, which symmetrizes the $\\left(L_{0},L_{1}\\right)$ -smoothness condition and allows the smoothness to depend polynomially on the gradient. Notably, Li et al. [2023] propose the $\\ell_{}$ -smoothness, defined as $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}\\leq\\ell(\\|\\nabla f(\\mathbf{x})\\|_{2}^{-})$ . This condition does not specify a particular form for the function $\\ell:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ , other than some mild assumptions about its properties, thereby allowing great generality of this notion. ", "page_idx": 13}, {"type": "text", "text": "Telgarsky [2022] introduces the concept of $(G,L)$ -quadratically bounded functions, which aims to generalize the Lipschitz condition as $\\|\\nabla f(\\mathbf{x})\\|_{2}\\,\\leq\\,G+L\\|\\mathbf{x}\\,-\\,\\mathbf{x}_{0}\\|_{2}$ with a reference point $\\mathbf{x}_{0}\\,\\in\\,\\mathcal{X}$ . Though this notion covers the standard global smoothness condition, it lacks a detailed depiction of the relationship between the smoothness and the gradients, hindering further research into understanding the optimization dynamics such as the role of gradient clipping [Zhang et al., 2020b]. Jacobsen and Cutkosky [2023] investigate online convex optimization under this constraint such that the online functions may be unbounded. However, it is unclear how to obtain the gradientvariation regret in their context and using their methods. ", "page_idx": 13}, {"type": "text", "text": "We also mention another generalization of the standard smoothness called the relative smoothness [Lu et al., 2018]. A function $f\\ :\\ \\mathcal{X}\\ \\mapsto\\ \\mathbb{R}$ is $L$ -smooth relative function if $f(\\mathbf{x})\\;\\leq$ $f(\\mathbf{y})+\\langle\\nabla f(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle+L\\mathcal{D}_{h}(\\mathbf{x},\\mathbf{y})$ for any $\\mathbf{x},\\mathbf{y}\\,\\in\\,\\operatorname{int}\\mathcal{X}$ , where $h:\\,\\mathcal{X}\\,\\mapsto\\,\\mathbb{R}$ is a reference function with $\\mathcal{D}_{h}(\\mathbf{x},\\mathbf{y})$ denoting the Bregman divergence. However, the global constant $L$ is still required to tune the algorithm, and studying this notion is beyond the scope of our paper. ", "page_idx": 14}, {"type": "text", "text": "A.3 Lipschitz-Adaptive Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The upper bound of gradients $G$ and the diameter of the bounded feasible domain $D$ are often required to build up online algorithms. An algorithm that requires the diameter $D$ of the bounded feasible domain $D$ but not the upper bound of gradients is known to be Lipschitz-adaptive [Duchi et al., 2011; Orabona and P\u00e1l, 2018; Cutkosky, 2019; Mhammedi et al., 2019]. For the general OCO setting, to the best of our knowledge, there are no Lipschitz-adaptive algorithm that ensures a gradient-variation bound. When specialized to the Prediction with Experts\u2019 Advice (PEA) setting [Cesa-Bianchi and Lugosi, 2006], Chen et al. [2021] design a two-layer algorithm with a restarting mechanism that can obtain a gradient-variation bound in PEA setting. In this paper, we develop a new Lipschitz-adaptive algorithm with a lightweight design, which guarantees the gradient-variation bound as well and is the only option for our purposes, to the best of our knowledge. ", "page_idx": 14}, {"type": "text", "text": "B Omitted Details for Section 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we first provide a judgement of the necessity of smoothness for first-order online algorithms in Appendix B.1. Next, we will provide proofs for the theorems in Section 2.3. In Appendix B.2, we present the omitted discussion for the challenge in designing the algorithm for exp-concave functions. We introduce a key lemma in Appendix B.3, which abstracts the key idea of exploiting the generalized smoothness. Later, the proofs for Theorem 1 and Theorem 2 are provided in Appendix B.4 and Appendix B.5, respectively. ", "page_idx": 14}, {"type": "text", "text": "B.1 Necessity of Smoothness ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first provide a lower bound on the convergence rate for first-order methods with convex functions. ", "page_idx": 14}, {"type": "text", "text": "Proposition 1 (Theorem 3.2.1 of Nesterov [2018]). There exists a $G$ -Lipschitz and convex function $f:\\bar{\\mathbb{R}}^{d}\\mapsto\\mathbb{R}$ with $\\|\\mathbf{x}_{1}-\\mathbf{x}_{\\star}\\|\\leq D$ where $\\begin{array}{r}{\\mathbf{x}_{\\star}\\in\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}f(\\mathbf{x})}\\end{array}$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{t})-\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}f(\\mathbf{x})\\geq\\frac{G D}{2(2+\\sqrt{t})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any optimization scheme that generates a sequence $\\left\\{{\\bf x}_{t}\\right\\}$ satisfying that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t}\\in\\mathbf{x}_{1}+\\mathrm{Lin}\\left\\{\\nabla f(\\mathbf{x}_{1}),\\dots,\\nabla f(\\mathbf{x}_{t-1})\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\operatorname{Lin}\\{\\mathbf{a}_{1},\\ldots,\\mathbf{a}_{t}\\}$ denotes the linear span of vectors $\\mathbf{a}_{1},\\ldots...,\\mathbf{a}_{t}$ . ", "page_idx": 14}, {"type": "text", "text": "Yang et al. [2014] prove the necessity of the smoothness to obtain the gradient-variation bounds with convex functions using the first-order online algorithms. We formally present this idea in below. ", "page_idx": 14}, {"type": "text", "text": "Proposition 2 (Remark 1 of Yang et al. [2014]). The smoothness assumption for $G$ -Lipschitz and convex online functions $f_{t}:\\mathcal{X}\\mapsto\\mathbb{R}$ is necessary for any online algorithms, whose decisions are linear combinations of the queried gradients when no projections are made, and ensure: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\operatorname*{min}_{\\mathbf x\\in\\mathcal X}\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\le\\mathcal O(\\sqrt{V_{T}})+c o n s t a n t,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with only $c\\cdot T$ times gradient queries, with $c\\in\\mathbb{N}$ being a constant independent of $T$ and environmental parameters, such as the Lipschitz constant and the smoothness constant. ", "page_idx": 14}, {"type": "text", "text": "Proof. We prove this proposition by contradiction. Assume that $f_{t}$ is non-smooth. Then consider a special case where the algorithm projects the decisions onto $\\mathcal{X}=\\mathbb{R}^{d}$ , i.e., no projections are made, ", "page_idx": 14}, {"type": "text", "text": "and all the online functions are equal $f_{1}=\\cdot\\cdot\\cdot=f_{T}=f$ . Notice that, under such case, the gradient variation is zero and therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f(\\mathbf x_{t})-\\operatorname*{min}_{\\mathbf x\\in\\mathscr X}\\sum_{t=1}^{T}f(\\mathbf x)\\leq\\mathcal O\\left(1\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies $\\begin{array}{r}{\\bar{\\mathbf{x}}_{T}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_{t}}\\end{array}$ approaches an $\\mathcal{O}(1/T)$ convergence rate. Now it remains to check whether this convergence rate contradicts with Proposition 1. Denoted by $\\mathbf{x}_{1}^{\\prime},\\ldots,\\mathbf{x}_{c T}^{\\prime}$ the points that the algorithm queries gradients on, because the decisions are linear combinations of the gradients when no projections are made, then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}_{T}\\in\\mathbf{x}_{1}+\\mathrm{Lin}\\left\\{\\nabla f(\\mathbf{x}_{1}^{\\prime}),\\dots,\\nabla f(\\mathbf{x}_{c T}^{\\prime})\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which indicates that, there exists a method which promises $\\mathcal{O}(c/T)\\,=\\,\\mathcal{O}(1/T)$ convergence rate under the protocol considered in Proposition 1, contradicting the lower bound. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.2 Challenge for Exp-Concave Functions in Regret Minimization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 2, we design algorithms for convex and strongly convex functions respectively. However, the gradient-variation regret for exp-concave functions [Hazan et al., 2007] under generalized smoothness has not yet been addressed. For online learning with global smooth functions, it has been demonstrated that an $O(d\\log V_{T})$ regret is attainable for exp-concave functions [Chiang et al., 2012], which is realized by an optimistic variant of the online Newton step algorithm [Hazan et al., 2007] using the last-round gradient as optimism, i.e., $M_{t}=\\nabla f_{t-1}({\\mathbf{x}}_{t-1})$ . However, it remains unclear how to obtain a general optimistic bound of order ${\\mathcal{O}}(d\\log D_{T})$ with $\\begin{array}{r}{D_{T}=\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-M_{t}\\|_{2}^{2}}\\end{array}$ for arbitrary optimism $\\{M_{t}\\}_{t=1}^{T}$ . Our technique for achieving gradient-vari ation regret under generalized smoothness relies on a flexible setting for optimism (which may not be the last-round gradient) and step size tuning (which requires a trajectory-wise stability analysis), making it challenging for extension to exp-concave functions. We leave this as an open question for future research. ", "page_idx": 15}, {"type": "text", "text": "B.3 Lemma for Regret Minimization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Below, we present a lemma that leverages the local smoothness of the optimization trajectory to derive gradient variation within the OMD framework. This lemma can be applied in the analysis of both convex and strongly convex functions. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Under Assumptions 1 and 2, by selecting regularizer as $\\begin{array}{r}{\\psi_{t}(\\mathbf{x})=\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}\\|_{2}^{2}}\\end{array}$ , setting step sizes satisfying that $\\eta_{t+1}\\leq\\eta_{t}$ and $\\eta_{t}\\leq1/(4\\widehat{L}_{t-1})$ , where $\\widehat{L}_{t-1}=\\ell_{t-1}(2\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2})$ , and by choosing optimism as $M_{t}=\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})$ , the O MD in Eq. (4 )  ensures the regret bound : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle\\leq\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\left(\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2}-\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t+1}\\|_{2}^{2}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{4\\eta_{t}}\\|\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{D^{2}}{2\\eta_{T}}+2\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{4\\eta_{t}}\\|\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{x}_{\\star}\\in\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\end{array}$ denotes the best decision in hindsight. ", "page_idx": 15}, {"type": "text", "text": "Proof. Following the standard analysis of OMD (Lemma 3), OMD with the optimism chosen as $M_{t}=\\nabla f_{t-1}\\big(\\widehat{\\mathbf{x}}_{t}\\big)$ ensures the following regret bound: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle\\leq\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\left(\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2}-\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t+1}\\|_{2}^{2}\\right)+\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The analysis for TERM-A is straightforward under Assumption 2: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{TERM-A}\\leq\\frac{1}{2\\eta_{1}}\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{1}\\|_{2}^{2}+\\sum_{t=2}^{T}(\\frac{1}{2\\eta_{t}}-\\frac{1}{2\\eta_{t-1}})\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2}\\leq\\frac{D^{2}}{2\\eta_{1}}+\\sum_{t=2}^{T}(\\frac{D^{2}}{2\\eta_{t}}-\\frac{D^{2}}{2\\eta_{t-1}})=\\frac{D^{2}}{2\\eta_{T}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we analyze the TERM-B under the generalized smoothness condition. By the basic calculation, we can decompose the TERM-B into following two terms: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{TERM-B}\\leq2\\sum_{t=1}^{T}\\eta_{t}\\Vert\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\Vert_{2}^{2}+2\\sum_{t=2}^{T}\\eta_{t}\\Vert\\nabla f_{t-1}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat\\mathbf{x}_{t})\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first term is the desirable gradient variation and the second term should be further analyzed under the generalized smoothness. Given the optimism setting and the step size tuning, we demonstrate that $\\mathbf{x}_{t}$ and $\\widehat{\\mathbf{x}}_{t}$ are sufficiently close to apply local smoothness: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\|_{2}\\leq\\eta_{t}\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}\\leq\\frac{\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}}{4\\widehat{L}_{t-1}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In above, the first inequality is by the the Pythagorean theorem and the second inequality is by the step size tuning. Therefore, we can apply Lemma 1 to bound the gradient deviation in Eq. (12) by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=2}^{T}\\eta_{t}\\|\\nabla f_{t-1}(\\mathbf x_{t})-\\nabla f_{t-1}(\\widehat{\\mathbf x}_{t})\\|_{2}\\leq\\sum_{t=2}^{T}\\eta_{t}\\widehat L_{t-1}^{2}\\|\\mathbf x_{t}-\\widehat{\\mathbf x}_{t}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, combining TERM-A, TERM-B and TERM-C together, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{\\star}\\rangle\\leq\\displaystyle\\frac{D^{2}}{2\\eta_{T}}+2\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf x_{t})-\\nabla f_{t-1}(\\mathbf x_{t})\\|_{2}^{2}-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{4\\eta_{t}}\\|\\mathbf x_{t}-\\widehat{\\mathbf x}_{t}\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{t=1}^{T}(2\\eta_{t}\\widehat L_{t-1}^{2}-\\frac{1}{4\\eta_{t}})\\|\\mathbf x_{t}-\\widehat{\\mathbf x}_{t}\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{D^{2}}{2\\eta_{T}}+2\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf x_{t})-\\nabla f_{t-1}(\\mathbf x_{t})\\|_{2}^{2}-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{4\\eta_{t}}\\|\\mathbf x_{t}-\\widehat{\\mathbf x}_{t}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality is by setting $\\eta_{t}\\leq1/(4\\widehat{L}_{t-1})$ . Hence, we finish the proof. ", "page_idx": 16}, {"type": "text", "text": "B.4 Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The step size tuning in Eq. (7) in Theorem 1 satisfies the criterions for applying Lemma 2, specifically that $\\eta_{t+1}\\leq\\eta_{t}$ and $\\eta_{t}\\leq1/(4\\widehat{L}t-1)$ , where $\\widehat{L}_{t-1}=\\ell_{t-1}\\bigl(2\\|\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}\\bigr)$ . Therefore, by Lemma 2 and the convexity of loss fun ctions, we obta i n: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{\\star})\\leq\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle\\leq\\frac{D^{2}}{2\\eta_{T}}+2\\sum_{t=1}^{T}\\eta_{t}\\Vert\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{x}_{\\star}\\in\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\end{array}$ . The first term can be bounded as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{D^{2}}{2\\eta_{T}}\\leq2\\widehat{L}_{\\operatorname*{max}}D^{2}+\\frac{D}{2}\\sqrt{1+\\displaystyle\\sum_{t=2}^{T}\\!\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}}+\\frac{D}{2}\\cdot\\|\\nabla f_{1}(\\mathbf{x}_{1})\\|_{2}}\\\\ &{\\quad\\quad=\\mathcal{O}\\left(\\widehat{L}_{\\operatorname*{max}}D^{2}+D\\sqrt{V_{T}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we denote by $\\widehat{L}_{\\mathrm{max}}=\\operatorname*{max}_{t\\in[T]}\\widehat{L}_{t}$ . For the second term, we apply the self-confident tuning lemma (Lemma 5) b y  choosing $f(x)=1/\\sqrt{x}$ in the lemma statement: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}\\leq\\displaystyle\\sum_{t=1}^{T}\\frac{D\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}}{\\sqrt{1+\\sum_{s=1}^{t-1}\\|\\nabla f_{s}(\\mathbf{x}_{s})-\\nabla f_{s-1}(\\mathbf{x}_{s})\\|_{2}^{2}}}}\\\\ &{\\le2D\\sqrt{1+\\displaystyle\\sum_{s=1}^{T}\\|\\nabla f_{s}(\\mathbf{x}_{s})-\\nabla f_{s-1}(\\mathbf{x}_{s})\\|_{2}^{2}+D\\operatorname*{max}_{t\\in[T]}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}}}\\\\ &{=\\mathcal{O}\\left(D\\sqrt{V_{T}}+\\hat{G}_{\\operatorname*{max}}^{2}D\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use $\\widehat{G}_{\\mathrm{max}}$ to denote the empirically maximum Lipschitz constant. The additional term $\\mathcal{O}(\\widehat{G}_{\\mathrm{max}}^{2}D)$ re s ults from the lack of knowledge about $\\widehat{G}_{\\mathrm{max}}$ . However, we can improve this term to $\\mathcal{O}(\\widehat{G}_{\\mathrm{max}}D)$ by incorporating the clipping technique [ C utkosky, 2019; Chen et al., 2021] into OMD fra m ework. In the main text, we avoid introducing this method to prevent complicating the approach further. Our goal in the OMD introduction is to illustrate how to adapt to generalized smoothness at the base level. The details of this refined approach are provided in Appendix B.6. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.5 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. For $\\lambda$ -strongly convex functions, we tune the step size as $\\eta_{t}=2/(\\lambda t+16\\operatorname*{max}_{s\\in[t-1]}\\widehat{L}_{s})$ , where we denote by $\\widehat{L}_{t}\\,=\\,\\ell_{t}(2\\|\\nabla f_{t}(\\widehat{\\mathbf{x}}_{t+1})\\|)$ the locally estimated smoothness constant. By   the property of strongly  co nvex functions a nd Eq. (11) in Lemma 2, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{\\star})\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle-\\frac{\\lambda}{2}\\|\\mathbf{x}_{\\star}-\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\left(\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t}\\|_{2}^{2}-\\|\\mathbf{x}_{\\star}-\\widehat{\\mathbf{x}}_{t+1}\\|_{2}^{2}\\right)-\\frac{\\lambda}{2}\\|\\mathbf{x}_{\\star}-\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{TERM}\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Unlike in the case of convex functions, TERM-A involves negative terms derived from the strong convexity of the loss functions, which require a slightly different analysis: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathrm{TEM}}\\bot\\sum_{2\\eta}^{2}\\big\\|\\mathbf x_{\\star}-\\widehat\\mathbf\\ x_{1}\\big\\|_{2}^{2}+\\sum_{\\ell=2}^{\\mathcal{N}}\\frac1{2\\eta}\\big(\\frac12-\\frac1{2\\eta_{\\ell}}-\\frac1{2\\eta_{\\ell}-1}\\big)\\big\\|\\mathbf x_{\\star}-\\widehat\\mathbf x_{1}\\big\\|_{2}^{2}-\\frac{\\lambda}{2}\\big\\|\\mathbf x_{\\star}-\\mathbf x_{\\star}\\big\\|_{2}^{2}}\\\\ &{\\leq\\frac{{\\lambda}D^{2}}{4}+\\sum_{\\ell=2}^{D}\\langle\\frac\\lambda_{\\star}4+\\operatorname*{imax}_{\\ell}\\widehat L_{\\star}-4\\operatorname*{max}_{\\ell-1}\\widehat L_{\\star}\\rangle\\big\\|\\mathbf x_{\\star}-\\widehat\\mathbf x_{1}\\big\\|_{2}^{2}-\\frac{\\lambda}{2}\\big\\|\\mathbf x_{\\star}-\\mathbf x_{1}\\big\\|_{2}^{2}}\\\\ &{\\leq\\frac{{\\lambda}D^{2}}{2}+{4D}^{2}\\cdot\\operatorname*{sup}\\widehat L_{\\star}+\\sum_{\\ell=2}^{\\mathcal{N}}\\frac1{2}\\big\\|\\mathbf x_{\\star}-\\widehat z_{1}\\big\\|_{2}^{2}-\\frac{\\lambda}{2}\\big\\|\\mathbf x_{\\star}-\\mathbf x_{1}\\big\\|_{2}^{2}}\\\\ &{\\leq{\\frac{{\\lambda}D^{2}}{4}}+{4D}^{2}\\widehat L_{\\star}\\cos+\\sum_{\\ell=2}^{\\mathcal{N}}\\frac1{2}\\big\\|\\mathbf x_{\\star}-\\widehat z_{1}\\big\\|_{2}^{2}\\qquad\\quad(\\|\\mathbf x_{\\star}-\\widehat\\mathbf x_{1}\\|_{2}^{2}\\leq2\\|\\mathbf x_{\\star}-\\mathbf x_{1}\\|_{2}^{2}+2\\|\\mathbf x_{\\star}-\\widehat\\mathbf x_{1}\\|_{2}^{2})}\\\\ &{\\leq{\\frac{{\\lambda}D^{2}}{4}}+{4D}^{2}\\widehat L_{\\star}\\sin+\\sum_{\\ell=2}^{\\mathcal{N}}\\frac1{2}\\big\\|\\nabla f_{\\ell}(\\mathbf x_{\\star})-\\nabla f_{\\ell-1}(\\widehat\\mathbf x_{\\ell})\\big\\|_{2}^{2}}\\\\ &{\\leq{\\frac{{\\lambda}D^{2}}{4}}+{4D}^ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\leq\\frac{\\lambda D^{2}}{4}+4D^{2}\\widehat{L}_{\\operatorname*{max}}+\\sum_{t=1}^{T}2\\eta_{t}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}+\\sum_{t=1}^{T}2\\eta_{t}\\|\\nabla f_{t-1}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last term can be cancelled by TERM-C, and the third term is in the same order of TERM-B. Therefore, we only need to focus on TERM-B. For TERM-B, we follow the analysis of Chen et al. [2024], who applies a simpler analysis for the self-confident tuning. First, we define: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha=\\left\\lceil\\sum_{t=1}^{T}\\lVert\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\rVert_{2}^{2}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then by dividing the time horizon into $[1,\\alpha]$ and $[\\alpha+1,T]$ , we can upper bound TERM- $\\mathbf{B}$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\small\\textsc{T e t a}}\\cdot{\\textsc{B}}\\leq2\\frac{\\alpha}{r\\ln1}\\mu\\left\\vert\\nabla f_{i}({\\mathbf x}_{i})-\\nabla f_{i-1}({\\mathbf x}_{i})\\right\\vert\\!\\!2\\!+\\!2\\sum_{t=+1}^{r}\\frac{1}{r\\ln1}\\|\\nabla f_{i}({\\mathbf x}_{t})-\\nabla f_{i-1}({\\mathbf x}_{t})\\|_{2}^{2}}\\\\ &{\\leq4\\frac{\\alpha}{r\\ln1}\\frac{1}{\\lambda}\\left\\vert\\nabla f_{i}({\\mathbf x}_{i})-\\nabla f_{i-1}({\\mathbf x}_{i})\\right\\vert\\!\\!2+\\!4\\sum_{t=+1}^{r}\\frac{1}{\\lambda}\\left\\vert\\nabla f_{i}({\\mathbf x}_{t})-\\nabla f_{i-1}({\\mathbf x}_{t})\\right\\vert\\!\\!2}\\\\ &{\\leq4(\\frac{\\operatorname*{max}_{1}\\left\\vert\\nabla f_{i}({\\mathbf x}_{i})-\\nabla f_{i-1}({\\mathbf x}_{i})\\right\\vert\\!\\!2}{r\\ln1})\\displaystyle\\sum_{t=+1}^{r}\\frac{1}{\\lambda}+4\\sum_{t=+1}^{r}\\frac{1}{\\lambda}\\left\\vert\\nabla f_{i}({\\mathbf x}_{t})-\\nabla f_{i-1}({\\mathbf x}_{t})\\right\\vert\\!\\!2}\\\\ &{\\leq16\\tilde{\\alpha}_{t=\\ln2}^{2}\\displaystyle\\sum_{t=1}^{\\infty}\\frac{1}{\\lambda}+\\frac{4}{\\lambda(n+1)}\\displaystyle\\sum_{t=+1}^{r}\\|\\nabla f_{i}({\\mathbf x}_{t})-\\nabla f_{i-1}({\\mathbf x}_{t})\\|_{2}^{2}}\\\\ &{\\leq\\frac{16\\tilde{\\alpha}_{t=\\ln2}^{2}}{\\lambda}(1\\!+\\!\\ln\\!\\alpha)+\\frac{4}{\\lambda}}\\\\ &{\\leq\\frac{16\\tilde{\\alpha}_{t=\\ln2}^{2}}{\\lambda}\\ln\\left(\\displaystyle\\sum_{t=1}^{r}\\|\\nabla f_{i}({\\mathbf x}_{t})-\\nabla f_{i-1}({\\mathbf x}_{t})\\|_{2}^{2}+1\\right)+\\frac{16\\tilde{\\alpha}_{t=\\ln2}^{2}+4}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $\\widehat{G}_{\\mathrm{max}}$ to denote the maximum Lipschitz constant estimated empirically. Therefore: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x_{\\star})\\le\\mathcal{O}\\left(\\frac{\\widehat{G}_{\\operatorname*{max}}^{2}}{\\lambda}\\log V_{T}+\\widehat{L}_{\\operatorname*{max}}D^{2}+\\lambda D^{2}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 18}, {"type": "text", "text": "B.6 OMD Incorporating Clipping Technique ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Appendix B.4, an additional term $\\mathcal{O}(\\widehat{G}_{\\mathrm{max}}^{2}D)$ shows up in the final regret bound, which results from the lack of knowledge about $\\widehat{G}_{\\mathrm{max}}$ .  In this subsection, we improve the term $\\mathcal{O}(\\widehat{G}_{\\mathrm{max}}^{2}D)$ to $\\mathcal{O}(\\widehat{G}_{\\mathrm{max}}D)$ by incorporating the c li pping technique [Cutkosky, 2019; Chen et al., 20 2 1] into the OM  D framework. This modified OMD algorithm is defined as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t}=\\Pi_{\\mathcal{X}}\\left[\\widehat{\\mathbf{x}}_{t}-\\eta_{t}\\nabla f_{t-1}\\big(\\widehat{\\mathbf{x}}_{t}\\big)\\right],\\quad\\widehat{\\mathbf{x}}_{t+1}=\\Pi_{\\mathcal{X}}\\left[\\widehat{\\mathbf{x}}_{t}-\\eta_{t}\\widetilde{\\mathbf{g}}_{t}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\check{\\bf y}_{t}=\\nabla f_{t-1}(\\widehat{\\bf x}_{t})+\\frac{B_{t-1}}{B_{t}}(\\nabla f_{t}({\\bf x}_{t})-\\nabla f_{t-1}(\\widehat{\\bf x}_{t}))}\\end{array}$ is a clipped gradient with the maintained thresh o ld $\\begin{array}{r}{B_{t}=\\operatorname*{max}\\{B_{0},\\operatorname*{max}_{s\\in[t]}\\|\\nabla f_{s}(\\mathbf{x}_{s})-\\nabla f_{s-1}(\\widehat\\mathbf{x}_{s})\\|_{2}\\}}\\end{array}$ . Notice that, $\\mathbf{x}_{t}$ still updates from $\\widehat{\\mathbf{x}}_{t}$ in the same manner as illustrated in Theorem 1, there fore, we can apply the similar analysis to i t when exploiting the smoothness locally. Correspondingly, we provided the following step size tuning: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\eta_{t}=\\operatorname*{min}\\Bigg\\{\\sqrt{\\frac{D^{2}}{B_{t-1}^{2}+\\sum_{s=1}^{t-1}\\lVert\\widetilde{\\mathbf{g}}_{s}-\\nabla f_{s-1}(\\widehat{\\mathbf{x}}_{s})\\rVert_{2}^{2}}},\\,\\,\\operatorname*{min}_{s\\in[t]}\\frac{1}{4\\widehat{L}_{s-1}}\\Bigg\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the key modification is that we add $B_{t-1}^{2}$ in the denominator to facilitate the tuning analysis and we denote by $\\widehat{L}_{t}=\\ell_{t}(2\\|\\nabla f_{t}(\\widehat{\\mathbf{x}}_{t+1})\\|)$ . The following theorem presents the guarantee. ", "page_idx": 18}, {"type": "text", "text": "Theorem 5. Under Assumptions $I\\cdot2_{\\!\\!\\!\\!_{\\!}}$ , assuming online functions are convex, OMD presented in Eq. (14) with the step sizes in Eq. (15) ensures that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{REG}_{T}\\leq\\frac{5D}{2}\\sqrt{2V_{T}}+4\\widehat{L}_{\\mathrm{max}}D^{2}+5\\widehat{G}_{\\mathrm{max}}D,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{V_{T}=\\sum_{t=2}^{T}\\operatorname*{sup}_{\\mathbf x\\in\\mathcal X}\\|\\nabla f_{t}(\\mathbf x)-\\nabla f_{t-1}(\\mathbf x)\\|_{2}^{2}}\\end{array}$ is the gradient variations, $\\widehat{L}_{\\mathrm{max}}=\\operatorname*{max}_{t\\in[T]}\\widehat{L}_{t}$ is the maximum smoothness constant over the optimization trajectory, and $\\widehat{G}_{\\mathrm{max}}$ is the maxim u m empirically estimated Lipschitz constant. ", "page_idx": 19}, {"type": "text", "text": "Proof. First, we prove that the clipping technique incurs a constant in the regret bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle-\\langle\\widetilde{\\mathbf{g}}_{t},\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle=\\displaystyle\\sum_{t=1}^{T}\\frac{B_{t}-B_{t-1}}{B_{t}}\\langle\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle}\\\\ &{\\displaystyle\\leq D\\displaystyle\\sum_{t=1}^{T}(B_{t}-B_{t-1})=D(B_{T}-B_{0})=\\mathcal{O}(\\widehat{G}_{\\operatorname*{max}}D).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the following analysis, we will focus on the regret associated with the clipped gradient $\\widetilde{\\mathbf{g}}_{t}$ . Following the standard analysis of OMD (Lemma 3), and with simple calculations, we arrive a t : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\widetilde{\\mathbf{g}}_{t},\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle\\le\\underbrace{\\frac{D^{2}}{2\\eta_{T}}}_{\\mathrm{TERM-A}}+\\underbrace{\\sum_{t=1}^{T}\\eta_{t}\\Vert\\widetilde{\\mathbf{g}}_{t}-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\Vert_{2}^{2}}_{\\mathrm{TERM-B}}-\\underbrace{\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\Vert\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\Vert_{2}^{2}}_{\\mathrm{TERM-C}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the step size tuning, TERM-A can be bounded as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TERM-A}\\le2\\widehat{L}_{\\operatorname*{max}}D^{2}+\\displaystyle\\frac{D}{2}\\sqrt{B_{T}^{2}+\\sum_{s=1}^{T}\\lVert\\widetilde{\\mathbf z}_{s}-\\nabla f_{s-1}(\\widehat{\\mathbf x}_{s})\\rVert_{2}^{2}}}\\\\ &{\\le2\\widehat{L}_{\\operatorname*{max}}D^{2}+\\widehat{G}_{\\operatorname*{max}}D+\\displaystyle\\frac{D}{2}\\sqrt{\\sum_{s=2}^{T}\\lVert\\widetilde{\\mathbf z}_{s}-\\nabla f_{s-1}(\\widehat{\\mathbf x}_{s})\\rVert_{2}^{2}}}\\\\ &{=2\\widehat{L}_{\\operatorname*{max}}D^{2}+\\widehat{G}_{\\operatorname*{max}}D+\\displaystyle\\frac{D}{2}\\sqrt{\\sum_{s=2}^{T}\\frac{B_{s-1}^{2}}{B_{s}^{2}}\\lVert\\nabla f_{s}(\\mathbf x_{s})-\\nabla f_{s-1}(\\widehat{\\mathbf x}_{s})\\rVert_{2}^{2}}}\\\\ &{\\le2\\widehat{L}_{\\operatorname*{max}}D^{2}+\\widehat{G}_{\\operatorname*{max}}D+\\displaystyle\\frac{D}{2}\\sqrt{\\sum_{s=2}^{T}\\lVert\\nabla f_{s}(\\mathbf x_{s})-\\nabla f_{s-1}(\\widehat{\\mathbf x}_{s})\\rVert_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For TERM-B in (16), by the self-confident tuning lemma (Lemma 6) we obtain, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{TERM-B}=D\\sum_{t=1}^{T}\\frac{\\|\\widetilde{\\mathbf{g}}_{t}-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}^{2}}{\\sqrt{B_{t-1}^{2}+\\sum_{s=1}^{t-1}\\lVert\\widetilde{\\mathbf{g}}_{s}-\\nabla f_{s-1}(\\widehat{\\mathbf{x}}_{s})\\rVert_{2}^{2}}}\\le D\\sum_{t=1}^{T}\\frac{\\|\\widetilde{\\mathbf{g}}_{t}-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\|_{2}^{2}}{\\sqrt{\\sum_{s=1}^{t}\\lVert\\widetilde{\\mathbf{g}}_{s}-\\nabla f_{s-1}(\\widehat{\\mathbf{x}}_{s})\\rVert_{2}^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combine TERM-A-VAR in (17), TERM-B-VAR in (18), and negative term TERM-C in (16): ", "page_idx": 19}, {"type": "text", "text": "TERM-A-VAR $^+$ TERM-B-VAR TERM-C ", "page_idx": 19}, {"type": "equation", "text": "$$\n=\\frac{5D}{2}\\sqrt{2\\sum_{t=2}^{T}\\lVert\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\rVert_{2}^{2}}+2\\sum_{t=2}^{T}\\lVert\\nabla f_{t-1}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat{\\mathbf{x}}_{t})\\rVert_{2}^{2}}-\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\lVert\\mathbf{x}_{t}-\\widehat{\\mathbf{x}}_{t}\\rVert_{2}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\frac{5D}{2}\\sqrt{2\\sum_{t=2}^{T}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}^{2}}+\\frac{5D}{2}\\sqrt{2\\sum_{t=2}^{T}\\|\\nabla f_{t-1}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\widehat\\mathbf{x}_{t})\\|_{2}^{2}}}\\\\ &{\\qquad-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\|\\mathbf{x}_{t}-\\widehat\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ &{\\leq\\displaystyle\\frac{5D}{2}\\sqrt{2V_{T}}+\\frac{5D}{2}\\sqrt{2\\sum_{t=2}^{T}\\widehat\\cal L_{t-1}^{2}\\|\\mathbf{x}_{t}-\\widehat\\mathbf{x}_{t}\\|_{2}^{2}}-\\sum_{t=2}^{T}2\\big(\\displaystyle\\operatorname*{max}_{s\\in[t-1]}\\widehat\\cal L_{s}\\big)\\|\\mathbf{x}_{t}-\\widehat\\mathbf{x}_{t}\\|_{2}^{2}}\\\\ &{\\leq\\displaystyle\\frac{5D}{2}\\sqrt{2V_{T}}+\\frac{25}{16}\\widehat\\cal L_{\\operatorname*{max}}D^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the forth line we apply Lemma 1, and ub the final line, we apply the AM-GM inequality, $2{\\sqrt{a b}}-a\\leq b$ . Combing each component together, we conclude the proof as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{\\star})\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\widetilde{\\mathbf g}_{t},\\mathbf{x}_{t}-\\mathbf{x}_{\\star}\\rangle+2\\widehat{G}_{\\operatorname*{max}}D}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{5D}{2}\\sqrt{2V_{T}}+4\\widehat{L}_{\\operatorname*{max}}D^{2}+5\\widehat{G}_{\\operatorname*{max}}D.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Omitted Details for Section 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present the omitted details for Section 3. Appendix C.1 discusses how set the optimism $\\mathbf{\\nabla}m_{t}$ efficiently via a binary search. Appendix C.2 provides the omitted discussion for the challenge in universal online learning with exp-concave functions. Proofs for the theorems in Section 3 are included in Appendix C.3 and Appendix C.4. Appendix C.5 gives a simple universal algorithm under the global smoothness condition, which improves the optimality and efficiency of the method by Yan et al. [2023], at a cost of additional function value queries. ", "page_idx": 20}, {"type": "text", "text": "C.1 Discussion on Optimism ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this part, we discuss how to set the optimism $\\mathbf{\\nabla}m_{t}$ that involves the decision $\\scriptstyle\\mathbf{\\delta}p_{t}$ . We present the steps for incorporating the optimism and generating the decision $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ in Algorithm 1 for reference: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{w}_{t,i}=w_{t,i}\\exp(\\eta_{t,i}m_{t,i}),\\quad p_{t,i}=\\frac{\\eta_{t,i}\\widetilde{w}_{t,i}}{\\sum_{j\\in[N]}\\eta_{t,j}\\widetilde{w}_{t,j}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For more general consideration, we set optimism the as $\\begin{array}{r}{m_{t,i}=f(\\sum_{i\\in[N]}p_{t,i}\\mathbf{x}_{t,i})-h(\\mathbf{x}_{t,i})}\\end{array}$ where $f:\\mathcal{X}\\mapsto\\mathbb{R}$ is a convex and continuous function and $\\mathbf{x}_{i}\\in\\mathcal{X}$ is a decision available before setting the optimism. Notice that $\\widetilde{w}_{i}$ requires $\\scriptstyle{p_{t}}$ to update while $\\scriptstyle\\mathbf{\\delta}p_{t}$ is produced based on $\\widetilde{w}_{t,i}$ , resulting in a circular argument. Fo l lowing Wei et al. [2016], the $\\scriptstyle{p_{t}}$ can be solved via the   binary search technique. We define $\\begin{array}{r}{\\alpha=f(\\sum_{i\\in[N]}p_{t,i}\\mathbf{x}_{i})}\\end{array}$ , then the weight $\\widetilde{w}_{t,i}$ is a function of $\\alpha$ as $\\widetilde{w_{t,i}}(\\alpha)=$ $w_{t,i}\\exp(\\eta_{t,i}(\\alpha-f(\\mathbf{x}_{t,i})))$ . Furthermore, with the update form  ulation in (19), the decisi o n $p_{t,i}$ is a function of $\\alpha$ as well, with the formulation $\\begin{array}{r}{p_{t,i}(\\alpha)=\\frac{\\eta_{t,i}\\widetilde{w}_{t,i}(\\alpha)}{\\sum_{j\\in[N]}\\eta_{t,j}\\widetilde{w}_{t,j}(\\alpha)}}\\end{array}$ . By introducing a function $\\begin{array}{r}{g(\\alpha)=f(\\sum_{i\\in[N]}p_{t,i}(\\alpha)\\mathbf{x}_{i})}\\end{array}$ , solving the decision $\\textstyle p_{t}$ is equal to solving . ", "page_idx": 20}, {"type": "text", "text": "Below, we prove the existence of a solution to $g(\\alpha)=\\alpha$ . Provided the lower bound $\\underline{{f}}$ of function $f(\\cdot)$ , and by the convexity of the function $f(\\cdot)$ , the searching range of $\\alpha$ is restricted \u00afto $\\underline{{f}}\\,\\le\\,\\alpha\\,\\le$ $\\operatorname*{max}_{i\\in[N]}\\left\\{f(\\mathbf{x}_{i})\\right\\}$ , and thus $\\alpha$ is bounded. The continuity of the function $f(\\cdot)$ implies the c\u00afontinuity of the function $g(\\alpha)$ as well. The choice of $\\alpha=\\underline{{f}}$ results in $\\begin{array}{r}{g(\\alpha)-\\alpha=f(\\sum_{i\\in[N]}p_{t,i}(\\alpha)\\mathbf{x}_{i})\\textrm{-}}\\end{array}$ $\\underline{{f}}\\ \\geq\\ 0$ , and the choice of $\\alpha\\;=\\;\\operatorname*{max}_{i\\in[N]}\\left\\{\\,f(\\mathbf{x}_{i})\\right\\}$ implies $\\begin{array}{r}{g(\\alpha)\\,-\\,\\alpha\\,\\leq\\,\\sum_{i\\in[N]}{\\dot{p_{t,i}}}(\\alpha)f({\\mathbf x}_{i})\\,-\\,}\\end{array}$ $\\mathrm{max}_{i\\in[N]}\\left\\{f(\\mathbf{x}_{i})\\right\\}\\,\\leq\\,0$ , indicating that a solution to $g(\\alpha)\\,=\\,\\alpha$ exists. By using a binary search within $\\left[\\dot{\\underline{{f}}},\\operatorname*{max}_{i\\in[N]}\\left\\{f(\\mathbf{x}_{i})\\right\\}\\right]$ , we can approach $\\alpha$ within an error $\\mathcal{O}(1/T)$ in ${\\mathcal{O}}(\\log T)$ iterations. ", "page_idx": 20}, {"type": "text", "text": "The above argument requires the lower bound of the function $f(\\cdot)$ to determine the searching range over $\\alpha$ . When considering a simpler case where $f(\\mathbf{x}_{i})\\ =\\ \\ell_{i}$ and $\\begin{array}{r l}{f(\\sum_{i\\in[N]}p_{t,i}\\mathbf{x}_{i})}&{{}{\\stackrel{*}{=}}}\\end{array}$ $\\textstyle\\sum_{i\\in[N]}p_{t,i}\\ell_{i}$ , we can omit the requirement for the lower bound because $\\alpha$ falls within $[\\operatorname*{min}_{i\\in[N]}\\{\\ell_{i}\\}$ , $\\operatorname*{max}_{i\\in[N]}\\left\\{\\ell_{i}\\right\\}\\right]$ , because of the simpler structure of linear functions. ", "page_idx": 21}, {"type": "text", "text": "C.2 Challenge for Exp-Concave Functions in Universal Online Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Section 3.4, we present Algorithm 2 that achieves gradient-variation bounds for convex and strongly convex functions simultaneously. However, it does not guarantee such bounds for exp-concave functions. Besides the challenges discussed in Section B.2, a new obstacle arises in designing the meta-algorithm. We require optimism to satisfy $\\langle\\pmb{p}_{t},\\pmb{m}_{t}\\rangle\\mathrm{~\\pmb~\\leq~\\pmb~}0,$ , since we pass the meta-algorithm with heterogeneous inputs, and therefore we set $m_{t,i}^{\\phantom{\\dagger}}\\ =\\ f_{t-1}(\\mathbf{x}_{t})\\ -$ $\\bar{f}_{t-1}(\\mathbf x_{t,i})$ for all the base-learners. This optimism design is suitable for strongly convex functions, as the term $\\sqrt{\\sum_{t}(f_{t-1}(\\mathbf{x}_{t})-f_{t-1}(\\mathbf{x}_{t,i}))^{2}}$ introduced by optimism can be bounded by $\\widehat{G}_{\\mathrm{max}}\\sqrt{\\sum_{t}\\Vert\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\Vert_{2}^{2}}$ , cancelled by the negative ter $\\begin{array}{r}{\\mathrm{n}-\\sum_{t}\\lambda\\|\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\|_{2}^{2}}\\end{array}$ from strong convexity. H owever, for exp-concave functions, the negative term $-\\langle\\nabla f_{t}({\\bf x}_{t}),{\\bf x}_{t}-{\\bf x}_{t,i}\\rangle^{2}$ from exp-concavity may not be sufficient to cancel $\\sqrt{\\sum_{t}(f_{t-1}(\\mathbf{x}_{t})-f_{t-1}(\\mathbf{x}_{t,i}))^{2}}$ . We leave this as an open problem for future exploration. ", "page_idx": 21}, {"type": "text", "text": "C.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we prove a slightly generalized version of Theorem 3, which does not specify optimism and instead imposes conditions only on $\\bar{\\pmb{r}}_{t}$ . One can verify that the setting of optimism in Theorem 3 satisfies the requirement of Theorem 6. ", "page_idx": 21}, {"type": "text", "text": "Theorem 6. By setting $\\bar{r}_{t,i}$ such that $\\begin{array}{r}{\\sum_{i=1}^{N}p_{t,i}\\bar{r}_{t,i}\\leq0,}\\end{array}$ , Algorithm 1 ensures that, for any $i_{\\star}\\in[N]$ the regret $\\begin{array}{r}{\\sum_{t=1}^{T}\\langle p_{t},\\ell_{t}\\rangle-\\sum_{t=1}^{T}\\ell_{t,i_{\\star}}}\\end{array}$ can be bounded by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{O}\\Bigg(\\sqrt{\\sum_{t=1}^{T}(r_{t,i_{\\star}}-m_{t,i_{\\star}})^{2}}\\cdot\\big(\\log(N)+\\log(B_{T}+\\log T)\\big)+B_{T}\\Bigg),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{B_{T}=\\operatorname*{max}\\{B_{0},\\operatorname*{max}_{t\\in[T]}\\lVert\\boldsymbol{r}_{t}-\\boldsymbol{m}_{t}\\rVert_{\\infty}\\}.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. First, we demonstrate that the clipping technique [Chen et al., 2021; Cutkosky, 2019] incurs a constant in the regret bound: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}r_{t,i_{\\star}}-\\bar{r}_{t,i_{\\star}}=\\sum_{t=1}^{T}r_{t,i_{\\star}}-m_{t,i_{\\star}}-\\frac{B_{t-1}}{B_{t}}(r_{t,i_{\\star}}-m_{t,i_{\\star}})=\\sum_{t=1}^{T}\\frac{B_{t}-B_{t-1}}{B_{t}}(r_{t,i_{\\star}}-m_{t,i_{\\star}})}\\\\ &{\\displaystyle\\leq\\sum_{t=1}^{T}\\frac{B_{t}-B_{t-1}}{B_{t}}|r_{t,i_{\\star}}-m_{t,i_{\\star}}|\\leq\\displaystyle\\sum_{t=1}^{T}\\frac{B_{t}-B_{t-1}}{B_{t}}\\|r_{t}-m_{t}\\|_{\\infty}\\leq B_{T}-B_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In below, we focus on the analysis associated with clipped regret $\\bar{r}_{t,i_{\\star}}$ . Following previous work [Wei et al., 2016], we define $\\begin{array}{r}{W_{t}=\\sum_{i=1}^{N}w_{t,i}}\\end{array}$ to represent the summation of weights at time $t$ The quantity $W_{t}$ can be realized as the p otential to be analyzed. Next, we consider to upper bound ln WT +1. ", "page_idx": 21}, {"type": "text", "text": "By the inequality $x\\leq x^{\\alpha}+(\\alpha-1)/e$ for $x>0,\\alpha\\geq0$ , for any $i\\in[N]$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{T+1,i}\\leq\\left(w_{T+1,i}\\right)^{\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}}+\\frac{1}{e}\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Based on the updates in Line 7 of Algorithm 1, we bound the first term on the right-hand side as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(w_{T+1,i}\\right)^{\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}}=w_{T,i}\\exp\\left(\\eta_{T,i}\\bar{r}_{T,i}-\\eta_{T,i}^{2}\\left(\\bar{r}_{T,i}-m_{T,i}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\widetilde{w}_{T,i}\\exp\\left(\\eta_{T,i}\\left(\\bar{r}_{T,i}-m_{T,i}\\right)-\\eta_{T,i}^{2}\\left(\\bar{r}_{T,i}-m_{T,i}\\right)^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq\\tilde{w}_{T,i}\\left(1+\\eta_{T,i}\\left(\\bar{r}_{T,i}-m_{T,i}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last inequality is by $\\exp(x-x^{2})\\leq1+x$ for $x\\geq-1/2$ . This is a crucial condition needed to be verified for Lipschitz-adaptive meta-algorithm. By the tuning of learning rates, and the clipping technique, we control the range of $\\eta_{T,i}(\\bar{r}_{T,i}-m_{T,i})$ well: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta_{T,i}\\vert\\bar{r}_{T,i}-m_{T,i}\\vert=\\eta_{T,i}\\frac{B_{T-1}}{B_{T}}\\vert r_{T,i}-m_{T,i}\\vert\\leq\\eta_{T,i}B_{T-1}\\leq\\frac{B_{T-1}}{2B_{T-1}}=\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which meets the criterion for applying the mentioned inequality. By plugging inequality (22) into (21), we can further analyze the weights for all experts at time $T$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\sum_{i=1}^{N}w_{T+1,i}\\leq\\displaystyle\\sum_{i=1}^{N}\\widetilde w_{T,i}\\left(1+\\eta_{T,i}\\left(\\bar{r}_{T,i}-m_{T,i}\\right)\\right)+\\displaystyle\\sum_{i=1}^{N}\\frac1\\epsilon\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right)}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i=1}^{N}\\widetilde w_{T,i}\\left(1-\\eta_{T,i}m_{T,i}\\right)+\\sum_{i=1}^{N}\\eta_{T,i}\\widetilde w_{T,i}\\bar{r}_{T,i}+\\sum_{i=1}^{N}\\frac1\\epsilon\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right)}\\\\ {\\displaystyle}&{\\displaystyle\\leq\\sum_{i=1}^{N}\\widetilde w_{T,i}\\exp(-\\eta_{T,i}m_{T,i})+\\sum_{i=1}^{N}\\eta_{T,i}\\widetilde w_{T,i}\\bar{r}_{T,i}+\\sum_{i=1}^{N}\\frac1\\epsilon\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right)}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i=1}^{N}w_{T,i}+\\sum_{j=1}^{N}\\eta_{T,j}\\widetilde w_{T,j}\\sum_{i=1}^{N}p_{T,i}\\bar{r}_{T,i}+\\sum_{i=1}^{N}\\frac1\\epsilon\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right)}\\\\ {\\displaystyle}&{\\displaystyle\\leq\\sum_{i=1}^{N}w_{T,i}+\\sum_{i=1}^{N}\\epsilon\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we apply $1-x\\leq\\exp(-x)$ for any $x\\in\\mathbb{R}$ , and the last inequality is by the assumption in the theorem statement that $\\begin{array}{r}{\\sum_{i=1}^{N}p_{t,i}\\bar{r}_{t,i}\\leq0}\\end{array}$ for any $t\\in[T]$ . ", "page_idx": 22}, {"type": "text", "text": "Now we are ready to upper bound $W_{T+1}$ in an inductive style: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{W_{T+1}=\\displaystyle\\sum_{i=1}^{N}w_{T+1,i}\\leq\\displaystyle\\sum_{i=1}^{N}w_{T,i}+\\sum_{i=1}^{N}\\frac{1}{e}\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right)=W_{T}+\\displaystyle\\sum_{i=1}^{N}\\frac{1}{e}\\left(\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1\\right)}}\\\\ {{\\leq W_{1}+\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{N}\\frac{1}{e}\\left(\\frac{\\eta_{t,i}}{\\eta_{t+1,i}}-1\\right)=N+\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{N}\\frac{1}{e}\\left(\\frac{\\eta_{t,i}}{\\eta_{t+1,i}}-1\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality is by the induction. It remains to analyze the last term, the deviations of the learning rates. We present the following analysis tailored for the new learning rate setting, $\\forall i\\in[N]$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta_{T,i}}{\\eta_{T+1,i}}-1=\\sqrt{\\frac{1+\\sum_{t=1}^{T}(\\bar{r}_{t,i}-m_{t,i})^{2}+4B_{T}^{2}}{1+\\sum_{t=1}^{T-1}(\\bar{r}_{t,i}-m_{t,i})^{2}+4B_{T-1}^{2}}}-1}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{1+\\frac{4B_{T}^{2}-4B_{T-1}^{2}+(\\bar{r}_{T,i}-m_{T,i})^{2}}{1+\\sum_{t=1}^{T-1}(\\bar{r}_{t,i}-m_{t,i})^{2}+4B_{T-1}^{2}}}-1}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{2}\\cdot\\frac{4B_{T}^{2}-4B_{T-1}^{2}+(\\bar{r}_{T,i}-m_{T,i})^{2}}{1+\\sum_{t=1}^{T-1}(\\bar{r}_{t,i}-m_{t,i})^{2}+4B_{T-1}^{2}}\\qquad\\qquad\\qquad\\qquad(\\sqrt{1+x}\\leq1+\\frac{1}{2}x)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}\\cdot\\frac{\\phi_{T,i}}{1+4B_{0}^{2}+\\sum_{t=1}^{T-1}\\phi_{t,i}}\\qquad\\qquad\\qquad(\\phi_{t,i}\\triangleq4B_{t}^{2}-4B_{t-1}^{2}+(\\bar{r}_{t,i}-m_{t,i})^{2}\\geq0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma 5 with the choices of $f(x)\\,=\\,1/x,a_{0}\\,=\\,1+4B_{0}^{2},a_{t}\\,=\\,\\phi_{t,i}$ in the lemma statement, summing up the preceding inequality from 1 to $T$ results in: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\frac{\\eta_{t,i}}{\\eta_{t+1,i}}-1\\right)\\leq\\frac{2B_{T}^{2}}{1+4B_{0}^{2}}+\\frac{1}{2}\\ln\\left(1+4B_{T}^{2}+\\sum_{t=1}^{T}(\\bar{r}_{t,i}-m_{t,i})^{2}\\right)-\\frac{1}{2}\\ln(1+4B_{0}^{2})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq\\frac{2B_{T}^{2}}{1+4B_{0}^{2}}+\\frac{1}{2}\\ln\\left(\\frac{1+(T+4)B_{T}^{2}}{1+4B_{0}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now combining (23) and (24), we can upper bound $\\ln W_{T+1}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ln W_{T+1}\\leq\\ln\\left(1+\\frac{B_{T}^{2}}{1+4B_{0}^{2}}+\\frac{1}{2e}\\ln\\left(\\frac{1+T B_{T}^{2}}{1+4B_{0}^{2}}\\right)\\right)+\\ln N.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In another direction, we lower bound $\\ln W_{T+1}\\geq\\ln w_{T+1,i_{\\star}}$ with an inductive argument: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\eta_{T+1,i\\star}}\\ln w_{T+1,i\\star}=\\frac{1}{\\eta_{T,i\\star}}\\left(\\ln w_{T,i\\star}+\\eta_{T,i\\star}\\bar{r}_{T,i\\star}-\\eta_{T,i\\star}^{2}\\big(\\bar{r}_{T,i\\star}-m_{T,i\\star}\\big)^{2}\\right)}\\\\ {\\displaystyle=\\frac{1}{\\eta_{T,k}}\\ln w_{T,i\\star}-\\eta_{T,i\\star}\\big(\\bar{r}_{T,i\\star}-m_{T,i\\star}\\big)^{2}+\\bar{r}_{T,i\\star}}\\\\ {\\displaystyle=\\frac{1}{\\eta_{1,i\\star}}\\ln w_{1,i\\star}-\\sum_{t=1}^{T}\\eta_{t,i\\star}\\big(\\bar{r}_{t,i\\star}-m_{t,i\\star}\\big)^{2}+\\sum_{t=1}^{T}\\bar{r}_{t,i\\star}}\\\\ {\\displaystyle=-\\sum_{t=1}^{T}\\eta_{t,i\\star}\\big(\\bar{r}_{t,i\\star}-m_{t,i\\star}\\big)^{2}+\\sum_{t=1}^{T}\\bar{r}_{t,i\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearranging the above equality with notice of Eq. (20), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}r_{t,i_{\\star}}\\leq\\sum_{t=1}^{T}\\bar{r}_{t,i_{\\star}}+B_{T}}\\\\ &{\\displaystyle\\leq\\sum_{t=1}^{T}\\eta_{t,i_{\\star}}\\bigl(\\bar{r}_{t,i_{\\star}}-m_{t,i_{\\star}}\\bigr)^{2}+\\frac{1}{\\eta_{T+1,i_{\\star}}}\\left(\\ln\\left(1+\\frac{B_{T}^{2}}{1+4B_{0}^{2}}+\\frac{1}{2e}\\ln\\left(\\frac{1+T B_{T}^{2}}{1+4B_{0}^{2}}\\right)\\right)+\\ln N\\right)+B_{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second term is in the order of ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{O}\\Bigg(\\sqrt{\\sum_{t=1}^{T}(r_{t,i_{\\star}}-m_{t,i_{\\star}})^{2}}\\cdot(\\log(B_{T}+\\log(T B_{T}))+\\log N)\\Bigg).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As for the first term, by applying Lemma 6, it can be bounded as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}{\\eta_{t,i_{*}}(\\bar{r}_{t,i_{*}}-m_{t,i_{*}})^{2}}=\\displaystyle\\sum_{t=1}^{T}{\\frac{(\\bar{r}_{t,i_{*}}-m_{t,i_{*}})^{2}}{\\sqrt{1+4B_{t}^{2}+\\sum_{s=1}^{t-1}(\\bar{r}_{s,i_{*}}-m_{s,i_{*}})^{2}}}}}\\\\ {\\displaystyle\\leq\\sum_{t=1}^{T}{\\frac{(\\bar{r}_{t,i_{*}}-m_{t,i_{*}})^{2}}{\\sqrt{1+\\sum_{s=1}^{t}(\\bar{r}_{s,i_{*}}-m_{s,i_{*}})^{2}}}}\\leq2\\sqrt{1+\\sum_{t=1}^{T}(\\bar{r}_{t,i_{*}}-m_{t,i_{*}})^{2}}}\\\\ {\\displaystyle=2\\sqrt{1+\\sum_{t=1}^{T}{\\frac{B_{t-1}^{2}}{B_{t}^{2}}}(r_{t,i_{*}}-m_{t,i_{*}})^{2}}\\leq2\\sqrt{1+\\sum_{t=1}^{T}(r_{t,i_{*}}-m_{t,i_{*}})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, the proof is complete. ", "page_idx": 23}, {"type": "text", "text": "C.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. We first decompose the static regret based on the performance of base-learner $i_{\\star}$ into two parts as presented in Eq. (8). ", "page_idx": 23}, {"type": "text", "text": "The base-regret is guaranteed by the corresponding base-learner via Theorem 1 and Theorem 2. And we mainly focus on the analysis of the meta-regret by leveraging Theorem 6. First we are required to verify the condition that $\\begin{array}{r}{\\sum_{i\\in[N]}p_{t,i}\\bar{r}_{t,i}\\leq0}\\end{array}$ . Without loss of generality, we assume the 1-st baselearner is for convex funct ions. Recall that we set $r_{t,i}=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle$ for strongly convex functions learners $i\\in[2,N]$ , $r_{t,1}=f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,1})$ for the convex function learner, and optimism $m_{t,i}=f_{t-1}(\\mathbf{x}_{t})-f_{t-1}(\\mathbf{x}_{t,i})$ for each base-learner $\\boldsymbol{i}\\in[N]$ , therefore we have: ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[N]}p_{t,i}\\bar{r}_{t,i}=\\Big(1-\\frac{B_{t-1}}{B_{t}}\\Big)\\Big(f_{t-1}(\\mathbf{x}_{t-1})-\\displaystyle\\sum_{i\\in[N]}p_{t,i}f_{t-1}(\\mathbf{x}_{t,i})\\Big)}\\\\ &{\\qquad\\qquad+\\left(p_{t,1}(f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,1}))+\\displaystyle\\sum_{i\\in[2,N]}p_{t,i}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\leq\\Big(1-\\frac{B_{t-1}}{B_{t}}\\Big)\\Big(\\displaystyle\\sum_{i\\in[N]}p_{t,i}f_{t-1}(\\mathbf{x}_{t,i})-\\displaystyle\\sum_{i\\in[N]}p_{t,i}f_{t-1}(\\mathbf{x}_{t,i})\\Big)}\\\\ &{\\quad\\mathrm+\\left(p_{t,1}\\langle f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,1}\\rangle+\\displaystyle\\sum_{i\\in[2,N]}p_{t,i}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, Theorem 6 is applicable in analyzing the meta-regret for universal online learning. In follows, we prove the regret bounds for convex functions and strongly convex functions respectively. ", "page_idx": 24}, {"type": "text", "text": "Convex functions. By Theorem 1, the base-regret is bounded by $O(\\sqrt{V_{T}})$ , as for the meta-regret, by the setting of inputs and optimism, by Theorem 6, it is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{META-REG}\\leq\\mathcal{O}\\Bigg(\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\Big(\\big(f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,1})\\big)-\\big(f_{t-1}(\\mathbf{x}_{t})-f_{t-1}(\\mathbf{x}_{t,1})\\big)\\Big)^{2}}\\cdot C_{T}+B_{T}\\Bigg)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ =\\mathcal{O}\\Bigg(\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\Big(\\big(f_{t}(\\mathbf{x}_{t})-f_{t-1}(\\mathbf{x}_{t})\\big)-\\big(f_{t}(\\mathbf{x}_{t,1})-f_{t-1}(\\mathbf{x}_{t,1})\\big)\\Big)^{2}}\\cdot C_{T}+B_{T}\\Bigg)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ =\\mathcal{O}\\Bigg(\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\Big(\\big\\langle\\nabla f_{t}(\\xi_{t,1})-\\nabla f_{t-1}(\\xi_{t,1}),\\mathbf{x}_{t}-\\mathbf{x}_{t,1}\\big\\rangle\\Big)^{2}}\\cdot C_{T}+B_{T}\\Bigg)}\\\\ &{\\ \\ \\ \\ \\ \\ \\leq\\mathcal{O}\\Bigg(D\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\operatorname*{sup}_{t\\in\\mathcal{X}}\\lVert\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\rVert_{2}^{2}}\\cdot C_{T}+B_{T}\\Bigg)=\\mathcal{O}\\Bigg(\\sqrt{V_{T}}\\cdot C_{T}+B}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we denote by $C_{T}={\\mathcal{O}}(\\log(B_{T}+\\log(B_{T}T)))$ ) and in the third line we apply the mean value theorem. Combining the base-regret and meta-regret together, we concludes that the static regret bound for convex functions is bounded by $\\mathcal{O}(\\sqrt{V_{T}}\\cdot\\log(B_{T}\\!+\\!\\log(B_{T}T)))$ , where $B_{T}=\\mathcal{O}(\\widehat{G}_{\\operatorname*{max}}\\bar{D})$ with $\\widehat{G}_{\\mathrm{max}}$ denoting the maximum Lipschitz constant. ", "page_idx": 24}, {"type": "text", "text": "Strongly convex functions. For $\\lambda_{\\star}$ -strong convex functions with $\\lambda_{\\star}\\in[1/T,1]$ , by the construction of the curvature coefficient pool $\\mathcal{H}$ , there exists $i_{\\star}\\in[2,N]$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda_{i\\star}\\leq\\lambda_{\\star}\\leq2\\lambda_{i\\star}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With this specific $i_{\\star}$ -th base-learner, the base-regret can be upper bounded by $\\mathcal{O}((\\log V_{T})/\\lambda_{\\star})$ by Theorem 2, up to a multiplicative constant of 2. The meta-regret can be bounded as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{4ETA-REG}\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle-\\frac{\\lambda_{\\star}}{2}\\|\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\|_{2}^{2}}\\\\ &{\\leq\\mathcal{O}\\Bigg(\\sqrt{\\displaystyle\\sum_{t=1}^{T}(\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle-(f_{t-1}(\\mathbf x_{t})-f_{t-1}(\\mathbf x_{t,i_{\\star}})))^{2}}\\cdot C_{T}-\\frac{\\lambda_{\\star}}{2}\\|\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\|_{2}^{2}+B_{T}\\Bigg)}\\\\ &{\\leq\\mathcal{O}\\Bigg(\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle^{2}+\\langle\\nabla f_{t-1}(\\xi_{t,i_{\\star}}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle^{2}}\\cdot C_{T}-\\frac{\\lambda_{\\star}}{2}\\|\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\|_{2}^{2}+B_{T}\\Bigg)}\\\\ &{\\leq\\mathcal{O}\\Bigg(\\widehat{G}_{\\operatorname*{max}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\|_{2}^{2}}\\cdot C_{T}-\\frac{\\lambda_{\\star}}{2}\\|\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\|_{2}^{2}}+B_{T}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Input: curvature coefficient pools $\\mathcal{H}_{\\mathrm{sc}},\\mathcal{H}_{\\mathrm{exp}}$ , number of base-learners $N$ , optimistic Adapt-MLProd [Wei et al., 2016] as the meta-algorithm, base-algorithms Acvx, Asc, Aexp.   \n1: Initialization: Pass $N$ to the meta-algorithm, initialize a base-learner with $\\mathcal{A}_{\\mathrm{cvx}}$ ; for $\\lambda\\in\\mathcal{H}_{\\mathrm{sc}}$ , initialize a base-learner with $\\mathcal{A}_{\\mathrm{sc}}$ ; for $\\alpha\\in\\mathcal{H}_{\\mathrm{exp}}$ , initialize a base-learner with $\\mathcal{A}_{\\mathrm{exp}}$ .   \n2: for $t=1$ to $T$ do   \n3: Obtain $\\scriptstyle\\mathbf{\\delta}p_{t}$ from meta-algorithm, $\\mathbf{x}_{t,i}$ from each base-learner $i\\in[N]$ ;   \n4: Submit $\\begin{array}{r}{{\\bf x}_{t}=\\sum_{i\\in[N]}p_{t,i}{\\bf x}_{t,i}}\\end{array}$ ;   \n5: Receive $\\nabla f_{t}(\\mathbf{x}_{t})$ and send it to each base-learner for update;   \n6: For strongly convex and exp-concave functions learners: set $r_{t,i}=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i}\\rangle$ ; 7: For convex functions learner: set $r_{t,1}=f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,1})$ ;   \n8: Send $\\mathbf{\\boldsymbol{r}}_{t}$ to the meta-algorithm;   \n9: Send $m_{t+1,1}=f_{t}(\\mathbf{x}_{t})-f_{t}(\\mathbf{x}_{t,1})$ and $m_{t+1,i}=0,i\\in[2,N]$ to the meta-algorithm. 10: end for ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\leq\\mathcal{O}\\Bigg(\\frac{\\widehat{G}_{\\mathrm{max}}^{2}C_{T}^{2}}{\\lambda_{\\star}}+B_{T}\\Bigg)=\\mathcal{O}\\Bigg(\\frac{\\widehat{G}_{\\mathrm{max}}^{2}(\\log(B_{T}+\\log(B_{T}T)))^{2}}{\\lambda_{\\star}}+B_{T}\\Bigg),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use $\\widehat{G}_{\\mathrm{max}}$ to denote the maximum Lipschitz constant on the optimization trajectory, and $B_{T}\\,=\\,\\mathcal{O}(\\widehat{G}_{\\mathrm{max}}D)$ . In the fourth line, we again utilize the mean value theorem. The fifth line follows fro m  Lemma 7, which ensures that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\langle\\nabla f_{t-1}(\\xi_{t,i_{\\star}}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i_{\\star}}\\right\\rangle\\right|\\leq\\operatorname*{max}\\left\\{\\left|\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t,i_{\\star}}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i_{\\star}}\\right\\rangle\\right|,\\left|\\left\\langle\\nabla f_{t-1}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}_{t,i_{\\star}}\\right\\rangle\\right|\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "thus, our result depends on the Lipschitz constant on the optimization trajectory. In the last step, we apply the AM-GM inequality. The above statements show that the meta-regret is bounded by constants. With the base-regret guarantee, the proof for strongly convex functions is complete. ", "page_idx": 25}, {"type": "text", "text": "C.5 A Simple Universal Algorithm under Global Smoothness ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "As a byproduct, our techniques can be used to design a simpler two-layer universal algorithm that achieves the optimal gradient-variation regret bounds for convex, strongly convex, and exp-concave functions simultaneously, thereby improving upon the results of Yan et al. [2023]. The crux involves leveraging the function-variation-to-gradient-variation technique for convex functions, and following the strategy of Zhang et al. [2022a] to demonstrate that the meta-regret is bounded by constants for strongly convex and exp-concave functions, at a cost of ${\\mathcal{O}}(\\log T)$ times function value queries per round. Given the global smoothness constant and the Lipschitz constant, our algorithm does not need to be Lipschitz-adaptive, thus suitable for more general optimism settings. ", "page_idx": 25}, {"type": "text", "text": "In Algorithm 3, we present this idea. In contrast to Algorithm 2, which is designed under the generalized smoothness, this algorithm in addition can guarantee gradient-variation bound for expconcave functions. In below, we present the theoretical guarantees for Algorithm 3. ", "page_idx": 25}, {"type": "text", "text": "Corollary 3. Under Assumption 2, and assuming the loss functions are $L$ -smooth and $G$ -Lipschitz, we set $N=2\\lceil\\log_{2}T\\rceil+1$ . The curvature coefficient pools are defined as $\\begin{array}{r}{\\mathcal{H}_{s c}=\\mathcal{H}_{e x p}=\\{2^{i-1}/T:}\\end{array}$ $i\\in[(N-1)/2]\\}$ . By selecting suitable base-algorithms, for $\\lambda,\\alpha\\in[1/T,1]$ , Algorithm 3 ensures the following results simultaneously: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{R E G}_{T}\\leq\\left\\{\\begin{array}{l l}{\\mathcal{O}(\\sqrt{V_{T}\\cdot(\\log\\log T)}),}&{\\mathrm{(con}\\nu e x),}\\\\ {\\mathcal{O}\\left(\\frac{1}{\\lambda}\\log V_{T}+\\frac{G^{2}\\log\\log T}{\\lambda}\\right),}&{\\mathrm{(\\lambda-}s t r o n g l y\\;c o n\\nu e x),}\\\\ {\\mathcal{O}\\left(\\frac{d}{\\alpha}\\log V_{T}+\\frac{\\log\\log T}{\\alpha}\\right),}&{\\mathrm{(}\\alpha\\mathrm{-}e x p\\mathrm{-}c o n c a\\nu e).}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. When assuming the Lipschitz constant $G$ , optimistic Adapt-ML-Prod [Wei et al., 2016] can ensure the meta-regret bounded by $\\begin{array}{r l}{\\lefteqn{\\mathcal{O}(\\sqrt{\\sum_{t=1}^{t}(r_{t,i}-m_{t,i})^{2}\\cdot\\log\\log T})}\\quad}&{{}}\\end{array}$ , thus the dependence of logarithmic terms is improved compared to Theorem 4. The proofs for convex functions and strongly convex functions are nearly identical to the proofs for Theorem 4 in Appendix C.4; thus, we omit them here. We highlight the importance of the function-variation-to-gradient-variation technique, bounding the meta-regret of order $\\mathcal{O}(\\sqrt{V_{T}\\cdot\\log\\log T})$ without the cancellation-based analysis. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Next, we show that the meta-regret for $\\alpha_{\\star}$ -exp-concave functions is bounded by a constant. By the construction of the curvature pool $\\mathcal{H}_{\\mathrm{exp}}$ , there exists base-learner $i_{\\star}$ with the input curvature $\\alpha_{i_{\\star}}$ satisfying that $\\alpha_{i_{\\star}}\\leq\\alpha_{\\star}\\leq2\\alpha_{i_{\\star}}$ . Decompose the regret against this specific base-learner and by the definition of exp-concave functions, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\normalfont~dETA-REG}=\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-f_{t}(\\mathbf x_{t,i_{\\star}})\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle-\\frac{\\alpha}{2}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle^{2}}\\\\ &{\\leq\\mathcal{O}\\left(\\sqrt{\\displaystyle\\sum_{t=1}^{T}(r_{t,i_{\\star}}-m_{t,i_{\\star}})^{2}\\cdot\\log\\log T}-\\frac{\\alpha}{2}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle^{2}\\right)}\\\\ &{=\\mathcal{O}\\left(\\sqrt{\\displaystyle\\sum_{t=1}^{T}(\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle)^{2}\\cdot\\log\\log T}-\\frac{\\alpha}{2}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x_{t,i_{\\star}}\\rangle^{2}\\right)\\leq\\mathcal{O}\\left(\\frac{\\log\\log T}{\\alpha}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second-to-last line follows from the settings that $r_{t,i_{\\star}}=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}\\!-\\!\\mathbf{x}_{t,i_{\\star}}\\rangle$ and $m_{t,i_{\\star}}=$ 0, and we apply the AM-GM inequality in the final step. Therefore, by choosing the base-algorithm that ensures the regret bound of $\\bar{\\mathcal{O}}(\\frac{d}{\\alpha_{\\star}}\\log V_{T})$ , we complete the proof for this theorem. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D Omitted Details for Section 4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This section provides the omitted proofs for our two applications. ", "page_idx": 26}, {"type": "text", "text": "D.1 Proof of Corollary 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. We prove this theorem in a black-box manner, thanks to the gradient-variation bound we derive. By Theorem 4, for convex functions, Algorithm 2 ensures: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{R E G}_{T}\\leq\\mathcal{O}\\Bigg(\\sqrt{\\sum_{t=1}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\lVert\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\rVert_{2}^{2}+\\sum_{t=2}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\lVert\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\rVert_{2}^{2}}\\Bigg),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we decompose $\\|\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}$ as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\|\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\|_{2}^{2}+\\|\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\|_{2}^{2}+\\|\\nabla F_{t-1}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, by taking expectation on both sides and leveraging the concavity of the square root, we have: $\\mathbb{E}[\\mathrm{REG}_{T}]\\leq\\mathcal{O}\\Bigg(\\sqrt{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\Vert\\nabla f_{t}(\\mathbf{x})-\\nabla F_{t}(\\mathbf{x})\\Vert_{2}^{2}\\right]+\\sum_{t=2}^{T}\\mathbb{E}\\left[\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{sup}}\\Vert\\nabla F_{t}(\\mathbf{x})-\\nabla F_{t-1}(\\mathbf{x})\\Vert_{2}^{2}\\right]\\Bigg)},$ which is in order of $\\mathcal{O}\\big(\\sqrt{\\Sigma_{I}^{2}}+\\sqrt{\\widetilde{\\sigma}_{I}^{2}}\\big)$ . For strongly convex functions, as s hown in Eq. (13) in the proof of Theorem 2, the multiplicative factor $\\widehat{G}_{\\mathrm{max}}^{\\ \\^}$ can be replaced by a more refined factor $\\mathrm{max}_{t\\in[T]}\\|\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\|_{2}$ . Then Algori th m 2 can ensure the following bound for strongly convex functions: $\\mathsf{R E G}_{T}\\leq\\mathcal{O}\\left(\\operatorname*{max}_{t\\in[T]}\\lVert\\nabla f_{t}(\\mathbf{x}_{t})-\\nabla f_{t-1}(\\mathbf{x}_{t})\\rVert_{2}^{2}\\cdot\\log\\left(\\sum_{t=2}^{T}\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}\\lVert\\nabla f_{t}(\\mathbf{x})-\\nabla f_{t-1}(\\mathbf{x})\\rVert_{2}^{2}\\right)\\right).$ ", "page_idx": 26}, {"type": "text", "text": "By applying a similar argument as in Eq. (26) to decompose the gradient variation, taking the expectation on both sides, and leveraging the concavity of the logarithm, we conclude the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D.2 Proof of Corollary 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. The step sizes for optimistic OMD in (10) is $\\begin{array}{r}{\\eta_{t}\\,=\\,\\operatorname*{min}\\{D,\\ \\operatorname*{min}_{s\\in[t]}\\,\\frac{1}{4\\ell_{s-1}(2\\|F_{s-1}(\\widehat{\\mathbf{z}}_{s})\\|_{2})}\\}}\\end{array}$ . By the convexity and the concavity for the objective function $f(\\cdot,\\cdot)$ , for any $\\mathbf{z}=(\\mathbf{x},\\mathbf{y})\\in{\\mathcal{Z}}$ , we can linearize the gap for an $\\varepsilon$ -approximate solution as $\\begin{array}{r}{f(\\bar{\\mathbf{x}}_{T},\\mathbf{y})-f(\\mathbf{x},\\bar{\\mathbf{y}}_{T})\\leq\\frac{1}{T}\\sum_{t=1}^{T}\\langle F(\\mathbf{z}_{t}),\\mathbf{z}_{t}-\\mathbf{z}\\rangle}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Following the proof of Lemma 2, we can demonstrate that optimistic OMD in (10) ensures: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle F(\\mathbf{z}_{t}),\\mathbf{z}_{t}-\\mathbf{z}\\rangle\\leq\\mathcal{O}\\left(\\frac{\\operatorname*{max}_{t\\in[T]}\\lVert\\mathbf{z}_{t}-\\mathbf{z}\\rVert_{2}^{2}}{\\eta_{T}}+\\sum_{t=1}^{T}\\eta_{t}\\lVert F(\\mathbf{z}_{t})-F(\\widehat{\\mathbf{z}}_{t})\\rVert_{2}^{2}-\\sum_{t=1}^{T}\\frac{1}{\\eta_{t}}\\lVert\\mathbf{z}_{t}-\\widehat{\\mathbf{z}}_{t}\\rVert_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first term on the right-hand side is in the order of $\\mathcal{O}(\\widehat{L}_{\\mathrm{max}}D^{2})$ by the step size configuration, where $\\widehat{L}_{\\mathrm{max}}$ denotes the maximum smoothness constan t on the optimization trajectory. By the $\\ell$ -smoot h ness in Definition 2, the second term can be bounded as $\\eta_{t}\\|F(\\mathbf{z}_{t})\\,-\\,\\mathbf{\\dot{\\boldsymbol{F}}}(\\mathbf{\\widehat{z}}_{t})\\|_{2}^{2}\\ \\leq$ $\\mathcal{O}(\\eta_{t}\\ell_{s-1}(2\\|F_{s-1}(\\widehat{\\mathbf z}_{s})\\|_{2})^{2}\\cdot\\|\\mathbf z_{t}-\\widehat{\\mathbf z}_{t}\\|_{2}^{2})$ , which can be further cancelled by the negative term s. Therefore, $\\textstyle\\sum_{t=1}^{T}\\langle F(\\mathbf{z}_{t}),\\mathbf{z}_{t}-\\mathbf{z}\\rangle$ is boun ded by a constant, thus, the convergence rate to an $\\varepsilon$ -approximate solut ion is $\\mathcal{O}(1/T)$ , implying a fast convergence rate. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "E Supporting Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1 Lemmas for Optimistic OMD ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first present the generic lemma for optimistic OMD with dynamic regret [Zhao et al., 2024], which encompasses the standard regret by setting $\\begin{array}{r}{\\mathbf{u}_{1}=\\ldots=\\mathbf{u}_{T}=\\mathbf{x}_{\\star}\\in\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\end{array}$ . Lemma 3 (Theorem 1 of Zhao et al. [2024]). Optimistic OMD specialized at Eq. (3) satisfies $\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{u}_{t}\\rangle\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t})-M_{t},\\mathbf{x}_{t}-\\widehat\\mathbf{x}_{t+1}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\big(\\mathcal{D}_{\\psi_{t}}(\\mathbf{u}_{t},\\widehat\\mathbf{x}_{t})-\\mathcal{D}_{\\psi_{t}}(\\mathbf{u}_{t},\\widehat\\mathbf{x}_{t+1})\\big)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\sum_{t=1}^{T}\\big(\\mathcal{D}_{\\psi_{t}}(\\widehat\\mathbf{x}_{t+1},\\mathbf{x}_{t})+\\mathcal{D}_{\\psi_{t}}(\\mathbf{x}_{t},\\widehat\\mathbf{x}_{t})\\big)\\,,}\\end{array}$ where $\\mathbf{u}_{1},\\dots,\\mathbf{u}_{T}\\in\\mathcal{X}$ are arbitrary comparators in the feasible domain. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "The next lemma, known as the stability lemma, establishes an upper bound on the proximity between successive decisions in terms of the gradient utilized for updates. ", "page_idx": 27}, {"type": "text", "text": "Lemma 4 (Proposition 7 of Chiang et al. [2012]). Consider the following two updates: (i) $\\begin{array}{r}{\\mathbf{x}\\;=\\;\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\left\\{\\langle\\mathbf{g},\\mathbf{x}\\rangle+\\mathcal{D}_{\\psi}(\\mathbf{x},\\mathbf{c})\\right\\}}\\end{array}$ , and (ii) $\\begin{array}{r}{\\mathbf{x}^{\\prime}\\,=\\,\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\left\\{\\langle\\mathbf{g}^{\\prime},\\mathbf{x}\\rangle+\\mathcal{D}_{\\psi}(\\mathbf{x},\\mathbf{c})\\right\\}}\\end{array}$ . When the regularizer $\\psi\\;:\\;\\mathcal{X}\\;\\mapsto\\;\\mathbb{R}$ is $\\lambda$ -strongly convex function with respect to norm $\\lVert\\cdot\\rVert$ , we have $\\lambda\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|\\le\\|\\mathbf{g}-\\mathbf{g}^{\\prime}\\|_{*}$ . ", "page_idx": 27}, {"type": "text", "text": "E.2 Self-Confident Tuning Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this part, we provide some useful lemmas when analyzing the self-confident tuning strategy. ", "page_idx": 27}, {"type": "text", "text": "Lemma 5 (Extension of Lemma 14 Gaillard et al. [2014]). Let $a_{0}~>~0$ and $a_{t}\\,\\in\\,[0,B]$ be real numbers for all $t\\ \\in\\ [T]$ and let $f~:~(0,+\\infty)~\\mapsto~[0,+\\infty)$ be a nonincreasing function. Then $\\begin{array}{r}{\\sum_{t=1}^{T}a_{t}f\\left(\\sum_{s=0}^{t-1}a_{s}\\right)\\leq B\\cdot f(a_{0})+\\int_{a_{0}}^{\\sum_{t=0}^{T}a_{t}}f(u)\\mathrm{d}u.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Lemma 6 (Lemma 3.5 of Auer et al. [2002]). Let $a_{1},\\dots,a_{T}$ and $\\delta$ be non-negative real numbers. $\\begin{array}{r}{\\sum_{t=1}^{T}\\frac{a_{t}}{\\sqrt{\\delta+\\sum_{s=1}^{t}a_{s}}}\\leq2\\Big(\\sqrt{\\delta+\\sum_{t=1}^{T}a_{t}}-\\sqrt{\\delta}\\Big).}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "E.3 Technical Lemma ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 7. Let $f:\\mathcal{X}\\mapsto\\mathbb{R}$ be a convex, twice differentiable function. Then for any $\\mathbf{x},\\mathbf{y}\\in\\operatorname{int}\\mathcal{X}$ and $\\lambda\\in[0,1]$ , we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\langle\\nabla f(\\lambda{\\mathbf x}+(1-\\lambda){\\mathbf y}),{\\mathbf x}-{\\mathbf y}\\rangle|\\leq\\operatorname*{max}\\left\\{|\\langle\\nabla f({\\mathbf x}),{\\mathbf x}-{\\mathbf y}\\rangle|,|\\langle\\nabla f({\\mathbf y}),{\\mathbf x}-{\\mathbf y}\\rangle|\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Define a function that $\\phi(t)\\ =\\ f(t\\mathbf{x}\\,+\\,(1\\,-\\,t)\\mathbf{y})$ . For this univariate convex function, we have $\\phi^{\\prime}(t)\\;=\\;\\langle\\nabla f(t{\\bf x}\\,+\\,(1\\,-\\,t){\\bf y}),{\\bf x}\\,-\\,{\\bf y}\\rangle$ . By the convexity of $\\phi(t)$ , there is $|\\phi^{\\prime}(t)|\\ \\leq$ max $\\{|\\phi^{\\prime}(1)|,|\\phi^{\\prime}(0)|\\}$ , concluding the proof. \u5382 ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In the abstract, we present the contributions sequentially through ordinal words and we discuss the contributions further in the introduction part. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: One limitation of our work is the requirement for multiple gradient queries Another limitation is that we cannot obtain the gradient-variation results for exp-concave functions due to the technical reasons. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The assumptions are provided in Section 2. The main theoretical results are provided from Section 2 to Section 4. All proofs can be found in the Appendix and, specifically from Appendix B to Appendix D, which contain the proofs for the main results. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not include experiments, and no data or code will be provided. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper only focuses on online learning theory, and we are not aware of any potential societal impacts. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not include experiments, and there are no risks for misuse of data or models. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not include experiments, and we do not use existing assets. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not include experiments, and we do not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip flie. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper focuses on online learning theory and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper focuses on online learning theory and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]