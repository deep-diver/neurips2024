[{"figure_path": "DHVqAQ9DHy/figures/figures_2_1.jpg", "caption": "Figure 1: Overall illustration of posterior node relabeling. To relabel the node label, we compute the posterior distribution of the label given neighborhood labels. Note that the node features are not considered in the relabeling process.", "description": "This figure illustrates the process of posterior node relabeling. It starts with a target graph containing a target node and its neighbors. The global statistics of node labels are used to compute conditional and prior distributions. These distributions are then used to calculate the posterior distribution of the label for the target node, which is used for relabeling.  The node features are not used in this specific step of relabeling.", "section": "3.1 Posterior label smoothing"}, {"figure_path": "DHVqAQ9DHy/figures/figures_5_1.jpg", "caption": "Figure 2: Loss curve of GCN trained on PosteL labels and ground truth labels on the Squirrel dataset.", "description": "The figure shows the training, validation, and test loss curves for a Graph Convolutional Network (GCN) trained on two different sets of labels: ground truth labels and PosteL (Posterior Label Smoothing) labels.  The PosteL labels, generated using the proposed method, incorporate local neighborhood information.  The plot demonstrates that while the training loss is higher with PosteL labels, both the validation and test losses converge to lower values compared to the ground truth labels, suggesting that PosteL helps to prevent overfitting and improve generalization.", "section": "4.2 Analysis"}, {"figure_path": "DHVqAQ9DHy/figures/figures_6_1.jpg", "caption": "Figure 1: Overall illustration of posterior node relabeling. To relabel the node label, we compute the posterior distribution of the label given neighborhood labels. Note that the node features are not considered in the relabeling process.", "description": "This figure illustrates the process of posterior node relabeling, a crucial step in the proposed label smoothing method.  It shows how the posterior probability distribution for a node's label is calculated, based on the labels of its neighboring nodes. The global label statistics (prior distribution) and conditional probabilities between adjacent nodes (likelihood) are used to compute the posterior distribution. The node's own features are not directly used in this relabeling step.", "section": "3.1 Posterior label smoothing"}, {"figure_path": "DHVqAQ9DHy/figures/figures_6_2.jpg", "caption": "Figure 4: t-SNE plots of the final layer representation of the Chameleon and Squirrel datasets. For each dataset, the left figure displays the representations trained on the ground truth labels, while the right figure displays the representations trained on the PosteL labels.", "description": "This figure shows the visualization of node embeddings using t-SNE for the Chameleon and Squirrel datasets.  The left plots in each case show embeddings generated using the ground truth labels, while the right plots show embeddings generated using the PosteL (Posterior Label smoothing) method.  The visualization helps to illustrate how PosteL affects the learned representations by comparing the clustering of nodes based on ground truth labels versus the smoothed labels.", "section": "4.2 Analysis"}, {"figure_path": "DHVqAQ9DHy/figures/figures_7_1.jpg", "caption": "Figure 5: The impact of the iterative pseudo labeling: loss curves of GCN on the Cornell dataset.", "description": "The figure shows the impact of iterative pseudo labeling on the training, validation, and testing loss curves of a Graph Convolutional Network (GCN) model trained on the Cornell dataset. Five lines are presented, each representing a different number of iterations in the pseudo-labeling process. The 'w/o iteration' line represents the model trained without pseudo-labeling. The other lines ('Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4') illustrate the model performance with 1, 2, 3, and 4 pseudo-labeling iterations, respectively. It shows that as the number of iterations increases, the validation and test losses decrease, while the training loss slightly increases. This suggests that iterative pseudo labeling enhances model generalization by preventing overfitting. The optimal number of iterations appears to be around 4, which is consistent with the analysis performed in the paper.", "section": "4.2 Analysis"}, {"figure_path": "DHVqAQ9DHy/figures/figures_20_1.jpg", "caption": "Figure 1: Overall illustration of posterior node relabeling. To relabel the node label, we compute the posterior distribution of the label given neighborhood labels. Note that the node features are not considered in the relabeling process.", "description": "This figure illustrates the process of posterior node relabeling.  It shows how the posterior distribution of a node's label is calculated based on its neighbors' labels, without considering node features. The global statistics of node labels (prior distribution and conditional distributions) are used in the calculation. The process involves computing the likelihood of the neighborhood labels given a potential label for the target node, combining this with the prior probability, to derive the posterior distribution. This posterior distribution is then used as the soft label to relabel the target node.", "section": "3.1 Posterior label smoothing"}, {"figure_path": "DHVqAQ9DHy/figures/figures_21_1.jpg", "caption": "Figure 2: Loss curve of GCN trained on PosteL labels and ground truth labels on the Squirrel dataset.", "description": "This figure compares the training, validation, and testing loss curves of a Graph Convolutional Network (GCN) trained using two different labeling methods: PosteL labels and ground truth labels.  The PosteL labels are generated using the proposed Posterior Label Smoothing method, while ground truth labels represent the actual class labels of the data. The comparison aims to demonstrate how PosteL label smoothing impacts the model's training and generalization performance on the Squirrel dataset.", "section": "4.2 Analysis"}, {"figure_path": "DHVqAQ9DHy/figures/figures_22_1.jpg", "caption": "Figure 1: Overall illustration of posterior node relabeling. To relabel the node label, we compute the posterior distribution of the label given neighborhood labels. Note that the node features are not considered in the relabeling process.", "description": "This figure illustrates the process of posterior node relabeling, a key step in the proposed PosteL method.  It shows how the posterior distribution of a node's label is calculated based on its neighbors' labels. The process uses both local (neighborhood) and global (overall graph statistics) label information. The node's features are not used in this relabeling stage. The posterior distribution is then used to create soft labels for training the model.", "section": "3.1 Posterior label smoothing"}, {"figure_path": "DHVqAQ9DHy/figures/figures_23_1.jpg", "caption": "Figure 1: Overall illustration of posterior node relabeling. To relabel the node label, we compute the posterior distribution of the label given neighborhood labels. Note that the node features are not considered in the relabeling process.", "description": "This figure illustrates the process of posterior node relabeling. It shows how the posterior distribution of a node's label is calculated based on its neighborhood labels. The process involves using global statistics of node labels to estimate the prior distribution and conditional distributions between adjacent nodes. The posterior distribution is then computed using Bayes' rule, and finally, the node label is relabeled using this posterior distribution. The figure highlights that node features are not considered during this relabeling process.", "section": "3.1 Posterior label smoothing"}]