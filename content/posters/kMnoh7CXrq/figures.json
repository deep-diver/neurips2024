[{"figure_path": "kMnoh7CXrq/figures/figures_1_1.jpg", "caption": "Figure 1: DenseFormer architecture. The diagram in (a) shows the DenseFormer architecture with two transformer layers and a dilation of 1. After the first (resp. second) block, the past and current intermediary representations {X0, X1} (resp. {X0, X1, X2}) are averaged using the first (resp. second) DWA weights [\u03b11,0, \u03b11,1] (resp. [\u03b12,0, \u03b12,1, \u03b12,2]). The DWA weights are supported by red arrows. Those weights are represented in matrix form in (b), for a 12 layers DenseFormer. A DWA module at depth i has i + 1 weights, represented in red. Increasing the dilation sparsifies this matrix, reducing the computational overhead without degrading the perplexity, see Section 3.2 for more details.", "description": "This figure illustrates the DenseFormer architecture.  Part (a) shows a simplified representation with two transformer blocks and how the Depth Weighted Average (DWA) module combines outputs from current and previous blocks.  Part (b) displays the DWA weights in matrix form for a 12-layer DenseFormer, demonstrating how the weights are learned and the effect of dilation on sparsity.", "section": "3 Method"}, {"figure_path": "kMnoh7CXrq/figures/figures_4_1.jpg", "caption": "Figure 2: (a): DWA weights with dilation and DWA period: For a 12 layers DenseFormer, the a weights are sparsified using dilation k and DWA periodicity p (referred to as kxp). Compared to Fig. 1b, only certain rows have some weights other than the upper diagonal weights (which correspond to the regular transformer information flow). Increasing the dilation and period sparsifies the a matrix, reducing the computational overhead without degrading the perplexity, see Sections 3.2 and 3.3 for more details. (b): Speed and performance trade-off: Comparison of speed and performance trade-off between the standard Transformer architecture and DenseFormer. The number of blocks in each architecture is reported next to the data-point. All DenseFormer models on this plot use a dilation factor of 4. We show results using a DWA period of 1 and 5. Comparing perplexities: Considering only the perplexity (y-axis), a 48 block DenseFormer performs similarly as a much deeper 72 block Transformer. Comparing trade-offs: A 48 block 4x5-DenseFormer matches the better perplexity of a 72 block Transformer while being 1.4\u00d7 faster at inference.", "description": "This figure shows the effects of dilation and DWA period on the sparsity of the DWA weights and the resulting speed-performance trade-off between DenseFormer and standard Transformer models.  Part (a) visualizes how increasing dilation and period reduces the number of non-zero weights in the DWA matrix. Part (b) illustrates that DenseFormer achieves comparable perplexity to much deeper Transformers while significantly improving inference speed, demonstrating the efficiency gains from the proposed architectural modifications.", "section": "3 Method"}, {"figure_path": "kMnoh7CXrq/figures/figures_4_2.jpg", "caption": "Figure 2: (a): DWA weights with dilation and DWA period: For a 12 layers DenseFormer, the a weights are sparsified using dilation k and DWA periodicity p (referred to as kxp). Compared to Fig. 1b, only certain rows have some weights other than the upper diagonal weights (which correspond to the regular transformer information flow). Increasing the dilation and period sparsifies the a matrix, reducing the computational overhead without degrading the perplexity, see Sections 3.2 and 3.3 for more details. (b): Speed and performance trade-off: Comparison of speed and performance trade-off between the standard Transformer architecture and DenseFormer. The number of blocks in each architecture is reported next to the data-point. All DenseFormer models on this plot use a dilation factor of 4. We show results using a DWA period of 1 and 5. Comparing perplexities: Considering only the perplexity (y-axis), a 48 block DenseFormer performs similarly as a much deeper 72 block Transformer. Comparing trade-offs: A 48 block 4x5-DenseFormer matches the better perplexity of a 72 block Transformer while being 1.4\u00d7 faster at inference.", "description": "This figure shows the effect of dilation and DWA period on the sparsity of the DWA weight matrix and the resulting speed-performance trade-off.  Part (a) illustrates how increasing dilation and period reduces the number of non-zero weights, while part (b) demonstrates that DenseFormer achieves comparable or better perplexity than standard Transformers with significantly faster inference speed, especially with dilation and period optimization.  The improvements in speed are achieved without sacrificing perplexity.", "section": "Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_6_1.jpg", "caption": "Figure 3: Training and inference efficiency of kxp-DenseFormer vs. Transformer. For 48 block models, we compare in (a) the different perplexity/inference speed trade-offs reached by a regular Transformer and kxp-DenseFormers. In the top right corner, the Transformer baseline is the model with the worst perplexity but the fastest at inference. In contrast, the 1x1-DenseFormer in the bottom left corner, is reaching the best perplexity but incurs a cost in inference speed. By varying the dilation k and DWA period p, some kxp-DenseFormer models (e.g. 4x5) provide most of the perplexity improvement of the original DenseFormer while significantly reducing the time overhead. A similar analysis holds when looking at the training speed in (b). In (c), we show the perplexity decreasing during training. The x-axis is time. To compensate for the computational overhead of DenseFormer, we train the Transformer baseline for more iterations, such that the two methods have the same training time budget. We observe how our 4x5-DenseFormer is reaching a better perplexity faster than the baseline.", "description": "This figure compares the training and inference efficiency of the DenseFormer model against the standard Transformer model for 48 blocks.  It demonstrates that DenseFormer achieves a better perplexity/inference speed trade-off. By adjusting the dilation (k) and DWA period (p) hyperparameters, a balance can be struck between perplexity and inference/training speed. The figure shows that 4x5-DenseFormer provides better perplexity than the Transformer baseline in all three aspects (inference speed, training speed, and training time).", "section": "Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_7_1.jpg", "caption": "Figure 4: Visualization of DWA Learned Weights. Each row shows the weights \\(\\mathbf{a}\\) learned by a DWA module at a given depth. While the heatmaps are averaged across 3 runs with different seeds, those patterns are very consistent across seeds. In (a) and (b), strikingly similar patterns can be observed in both 48 and 72 layer DenseFormers. In (c), we show the learned weights for a 48 block DenseFormer trained with a dilation of 4. Despite the sparsity, we still observe a very similar pattern to those learned by the non-dilated models.", "description": "This figure visualizes the learned weights of the Depth Weighted Average (DWA) modules in DenseFormer models with different depths and dilation factors.  The heatmaps show that the learned weight patterns are surprisingly consistent across different random seeds and model configurations.  The consistent patterns, especially the diagonal and the upper-diagonal weights, reveal how the model effectively reuses information from previous layers. Even with a sparsity-inducing dilation factor of 4, the weight patterns maintain a similar structure.", "section": "Analyzing the Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_8_1.jpg", "caption": "Figure 5: Cosine similarity between the output of each DWA module and the initial embedding vectors. The results are averaged over three seeds, for DenseFormer models with 48 blocks and no dilation (corresponding to the weights in Fig. 1a). The model initially maintains a high correlation between the input and the output of each DWA module. That correlation decreases in later layers. Intuitively, we can hypothesize that this is the stage where the model is preparing to output the next token. A very similar plot can be observed for 72 block models in Appendix B.1.", "description": "This figure shows the cosine similarity between the output of each Depth Weighted Average (DWA) module and the initial embedding vectors across different depths of the network.  The results are averaged across three different random seeds. It shows that the correlation between the DWA module outputs and the input embeddings is initially high but decreases as the depth increases, suggesting a shift in the model's focus from processing the current token to preparing for the next.", "section": "Analyzing the Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_13_1.jpg", "caption": "Figure 1: DenseFormer architecture. The diagram in (a) shows the DenseFormer architecture with two transformer layers and a dilation of 1. After the first (resp. second) block, the past and current intermediary representations {X0, X1} (resp. {X0, X1, X2}) are averaged using the first (resp. second) DWA weights [w1,0, w1,1] (resp. [w2,0, w2,1, w2,2]). The DWA weights are supported by red arrows. Those weights are represented in matrix form in (b), for a 12 layers DenseFormer. A DWA module at depth i has i + 1 weights, represented in red. Increasing the dilation sparsifies this matrix, reducing the computational overhead without degrading the perplexity, see Section 3.2 for more details.", "description": "This figure illustrates the DenseFormer architecture, a modification of the standard Transformer architecture.  Panel (a) shows a simplified version with two transformer blocks, highlighting how the Depth Weighted Average (DWA) module combines outputs from current and previous blocks.  Panel (b) shows a matrix representation of the DWA weights for a 12-layer DenseFormer, illustrating how increasing the dilation parameter reduces the computational cost by sparsifying the weight matrix. The DWA weights, represented by red arrows in (a), are learned during training and determine how much information from previous layers is incorporated into the current representation.", "section": "3 Method"}, {"figure_path": "kMnoh7CXrq/figures/figures_14_1.jpg", "caption": "Figure 3: Training and inference efficiency of kxp-DenseFormer vs. Transformer. For 48 block models, we compare in (a) the different perplexity/inference speed trade-offs reached by a regular Transformer and kxp-DenseFormers. In the top right corner, the Transformer baseline is the model with the worst perplexity but the fastest at inference. In contrast, the 1x1-DenseFormer in the bottom left corner, is reaching the best perplexity but incurs a cost in inference speed. By varying the dilation k and DWA period p, some kxp-DenseFormer models (e.g. 4x5) provide most of the perplexity improvement of the original DenseFormer while significantly reducing the time overhead. A similar analysis holds when looking at the training speed in (b). In (c), we show the perplexity decreasing during training. The x-axis is time. To compensate for the computational overhead of DenseFormer, we train the Transformer baseline for more iterations, such that the two methods have the same training time budget. We observe how our 4x5-DenseFormer is reaching a better perplexity faster than the baseline.", "description": "This figure compares the training and inference efficiency of DenseFormer against the standard Transformer architecture. It shows the trade-offs between perplexity and inference/training speed for different models. By adjusting the dilation and DWA period parameters, DenseFormer achieves a balance between improved perplexity and reduced computational overhead. The figure also demonstrates that DenseFormer converges faster to lower perplexity during training than the Transformer baseline.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_16_1.jpg", "caption": "Figure 6: Performance after dropping small DWA weights. The figure shows the performance when the model is trained with no sparsity induced (dilation and period of 1) and the DWA weights are later sparsified based on their magnitude at inference. One can see that the perplexity quickly explodes after only sparsifying 15% of the weights. This observation suggests that even though many of the DWA weights are small (as can be seen in Fig. 4) they still play an important role in the output of the model.", "description": "The plot shows the perplexity of a DenseFormer and a standard Transformer model as a function of the sparsity of the DWA weights. The x-axis shows the percentage of the smallest weights that were dropped, and the y-axis shows the perplexity. The plot shows that as the sparsity of the DWA weights increases, the perplexity of both models increases, but the perplexity of the DenseFormer increases more significantly. This suggests that even though many of the DWA weights are small, they still play an important role in the performance of the model.", "section": "B.1 Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_16_2.jpg", "caption": "Figure 4: Visualization of DWA Learned Weights. Each row shows the weights \\(\\mathbf{a}\\) learned by a DWA module at a given depth. While the heatmaps are averaged across 3 runs with different seeds, those patterns are very consistent across seeds. In (a) and (b), strikingly similar patterns can be observed in both 48 and 72 layer DenseFormers. In (c), we show the learned weights for a 48 block DenseFormer trained with a dilation of 4. Despite the sparsity, we still observe a very similar pattern to those learned by the non-dilated models.", "description": "This figure visualizes the learned weights of the Depth Weighted Average (DWA) modules in DenseFormer for different model depths and with dilation.  It shows that the learned weight patterns are surprisingly consistent across different runs with varying random seeds and model configurations (with or without dilation). The consistent patterns observed suggest a structured reuse of activations from distant layers in the network. The figure provides visual evidence supporting the effectiveness of DWA in enhancing information flow.", "section": "Analyzing the Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_17_1.jpg", "caption": "Figure 5: Cosine similarity between the output of each DWA module and the initial embedding vectors. The results are averaged over three seeds, for DenseFormer models with 48 blocks and no dilation (corresponding to the weights in Fig. 1a). The model initially maintains a high correlation between the input and the output of each DWA module. That correlation decreases in later layers. Intuitively, we can hypothesize that this is the stage where the model is preparing to output the next token. A very similar plot can be observed for 72 block models in Appendix B.1.", "description": "This figure shows the cosine similarity between the output of each Depth Weighted Average (DWA) module and the initial embedding vectors across different depths of the network.  The results demonstrate a high initial correlation which gradually decreases in the later layers. This suggests that the model uses input information primarily in earlier layers, and later stages focus on generating the next token.", "section": "Analyzing the Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_17_2.jpg", "caption": "Figure 9: Rapid convergence of DWA weights during training. The DWA weights are rapidly converging to their final pattern. After 5000 iterations, the weight pattern already looks very similar to the one in Fig. 4.", "description": "This figure visualizes the learned weights of the Depth Weighted Average (DWA) modules during the training process.  It shows how quickly the weights converge to their final, stable pattern.  The heatmaps illustrate the weights at different training steps (0, 1000, 2000, 3000, 4000, 5000, 6000 iterations). After 5000 iterations, the weight pattern closely resembles the stable pattern observed in Figure 4, indicating a rapid learning process.", "section": "B.1 Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_17_3.jpg", "caption": "Figure 10: Effect of the Dilation Factor k on Speed and Performance. Part (a) shows the degradation in perplexity as we increase the dilation factor of kx1-DenseFormer models. A noticeable drop in performance occurs for larger dilation factors, e.g. after k = 4. However, surprisingly, 12-Dilated DenseFormer still outperforms the Transformer baseline. As shown in (b), while the perplexity is not so impacted by dilation, the inference speed is significantly improved. Interestingly, the speed gain also plateaus for larger values of k, e.g. roughly k = 4 for 48 blocks. The gain increases with the depth of the DenseFormer, and the plateau threshold occurs later for deeper models.", "description": "This figure shows the impact of varying the dilation factor (k) on the performance of 48-block and 72-block kx1-DenseFormer models in terms of perplexity and inference speed.  The left panel shows a minimal decrease in perplexity until dilation factor 4 after which it decreases significantly. The right panel demonstrates the increase in inference speed relative to a dilation of 1. The gains are larger and the plateau is reached later for the 72-block model. This indicates that small dilation factors offer a good trade-off between perplexity and inference speed.", "section": "Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_17_4.jpg", "caption": "Figure 13: Speed and performance trade-off. Comparison of speed and performance trade-off between the standard Transformer architecture and 4x1-DenseFormer. The number of blocks in each architecture is reported next to the data-point. All DenseFormer models on this plot use a dilation factor of 4. Comparing perplexities: Considering only the perplexity (y-axis), a 48 layer DenseFormer strikingly outperforms much deeper Transformer baselines. Comparing trade-offs: A 48 layer 4-Dilated DenseFormer matches the better perplexity of a 90 layer Transformer while being 1.6x faster at inference.", "description": "This figure compares the speed and performance trade-off between the standard Transformer and the 4x1-DenseFormer architectures with varying numbers of blocks (48, 72, 84, and 90).  It highlights that the 48-block DenseFormer achieves comparable or better perplexity than much deeper Transformers (up to 90 blocks) while maintaining significantly faster inference speeds.  This demonstrates the efficiency gains of the DenseFormer architecture.", "section": "Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_18_1.jpg", "caption": "Figure 11: Effect of the DWA period p on Speed and Performance. Part (a) shows the degradation in perplexity as we increase the DWA period of 4xp-DenseFormer models. Surprisingly, a 4x20-DenseFormer still outperforms the Transformer baseline. As shown in (b), while the perplexity is not so impacted, the inference speed is significantly improved.", "description": "This figure analyzes the impact of varying the DWA period (p) on the performance of 4xp-DenseFormers (where the dilation factor k is fixed at 4) compared to standard Transformers.  Part (a) shows that increasing p leads to a slight increase in perplexity, but even with a DWA period of 20, the 4x20-DenseFormer still outperforms the Transformer baseline. Part (b) demonstrates a significant increase in inference speed as p increases, indicating a favorable speed-perplexity trade-off.", "section": "B.2 Analysis of Dilation and DWA Period"}, {"figure_path": "kMnoh7CXrq/figures/figures_18_2.jpg", "caption": "Figure 13: Speed and performance trade-off. Comparison of speed and performance trade-off between the standard Transformer architecture and 4x1-DenseFormer. The number of blocks in each architecture is reported next to the data-point. All DenseFormer models on this plot use a dilation factor of 4. Comparing perplexities: Considering only the perplexity (y-axis), a 48 layer DenseFormer strikingly outperforms much deeper Transformer baselines. Comparing trade-offs: A 48 layer 4-Dilated DenseFormer matches the better perplexity of a 90 layer Transformer while being 1.6x faster at inference.", "description": "This figure compares the speed and performance trade-off between the standard Transformer architecture and the 4x1-DenseFormer architecture.  It shows that for the same level of perplexity, the 4x1-DenseFormer is significantly faster than the standard Transformer.  Furthermore, it illustrates that a 48-layer 4x1-DenseFormer achieves comparable perplexity to a much deeper (90-layer) standard Transformer, but with substantially faster inference speeds.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_19_1.jpg", "caption": "Figure 3: Training and inference efficiency of kxp-DenseFormer vs. Transformer. For 48 block models, we compare in (a) the different perplexity/inference speed trade-offs reached by a regular Transformer and kxp-DenseFormers. In the top right corner, the Transformer baseline is the model with the worst perplexity but the fastest at inference. In contrast, the 1x1-DenseFormer in the bottom left corner, is reaching the best perplexity but incurs a cost in inference speed. By varying the dilation k and DWA period p, some kxp-DenseFormer models (e.g. 4x5) provide most of the perplexity improvement of the original DenseFormer while significantly reducing the time overhead. A similar analysis holds when looking at the training speed in (b). In (c), we show the perplexity decreasing during training. The x-axis is time. To compensate for the computational overhead of DenseFormer, we train the Transformer baseline for more iterations, such that the two methods have the same training time budget. We observe how our 4x5-DenseFormer is reaching a better perplexity faster than the baseline.", "description": "This figure compares the training and inference efficiency of DenseFormer against the standard Transformer architecture using three different metrics: perplexity vs. inference speed, perplexity vs. training speed, and perplexity over training time.  The results demonstrate that DenseFormer offers superior performance in terms of perplexity across all three metrics, particularly when using dilation and period hyperparameters for optimization.  It showcases the trade-off between speed and perplexity and the ability of DenseFormer to achieve higher performance (lower perplexity) with significantly less time overhead during both training and inference compared to a standard Transformer.", "section": "Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_20_1.jpg", "caption": "Figure 13: Speed and performance trade-off. Comparison of speed and performance trade-off between the standard Transformer architecture and 4x1-DenseFormer. The number of blocks in each architecture is reported next to the data-point. All DenseFormer models on this plot use a dilation factor of 4. Comparing perplexities: Considering only the perplexity (y-axis), a 48 layer DenseFormer strikingly outperforms much deeper Transformer baselines. Comparing trade-offs: A 48 layer 4-Dilated DenseFormer matches the better perplexity of a 90 layer Transformer while being 1.6x faster at inference.", "description": "This figure compares the speed and performance trade-off between the standard Transformer and the 4x1-DenseFormer architecture with different numbers of blocks (48, 72, 84, and 90). The results show that the 48-layer DenseFormer significantly outperforms deeper Transformer models in terms of perplexity and achieves comparable performance to a 90-layer Transformer while being 1.6 times faster during inference.  This demonstrates the efficiency gains of the DenseFormer architecture.", "section": "Results"}, {"figure_path": "kMnoh7CXrq/figures/figures_21_1.jpg", "caption": "Figure 4: Visualization of DWA Learned Weights. Each row shows the weights \\(\\mathbf{a}\\) learned by a DWA module at a given depth. While the heatmaps are averaged across 3 runs with different seeds, those patterns are very consistent across seeds. In (a) and (b), strikingly similar patterns can be observed in both 48 and 72 layer DenseFormers. In (c), we show the learned weights for a 48 block DenseFormer trained with a dilation of 4. Despite the sparsity, we still observe a very similar pattern to those learned by the non-dilated models.", "description": "This figure visualizes the learned weights of the Depth Weighted Average (DWA) modules in DenseFormer for different model depths and dilation settings.  The heatmaps show consistent patterns across multiple runs with varying random seeds.  The visualization highlights the stable and structured reuse of activations from different layers, even with sparsity introduced through dilation.", "section": "Analyzing the Information Flow"}, {"figure_path": "kMnoh7CXrq/figures/figures_22_1.jpg", "caption": "Figure 15: Comparison with ELC-Bert. Part (a) Shows that ELC-Bert does not improve upon the baseline, while DenseFormer is consistently better. In (b), we observe that ELC-Bert is better in low data-regime, which is the setting for which it was developed. Yet the DenseFormer is catching up fast and outperforms ELC-Bert after approx. 1.2k iterations.", "description": "This figure compares the performance of DenseFormer with ELC-BERT, a concurrent work, under different data regimes (low data vs. non-low data). In the low-data regime (part b), ELC-BERT initially outperforms DenseFormer, but DenseFormer quickly catches up and surpasses ELC-BERT's performance after approximately 1200 steps. In the non-low data regime (part a), DenseFormer consistently outperforms both ELC-BERT and the baseline Transformer.", "section": "B.7 Comparison with ELC-BERT"}, {"figure_path": "kMnoh7CXrq/figures/figures_22_2.jpg", "caption": "Figure 15: Comparison with ELC-Bert. Part (a) Shows that ELC-Bert does not improve upon the baseline, while DenseFormer is consistently better. In (b), we observe that ELC-Bert is better in low data-regime, which is the setting for which it was developed. Yet the DenseFormer is catching up fast and outperforms ELC-Bert after approx. 1.2k iterations.", "description": "This figure compares the performance of DenseFormer against ELC-BERT, another concurrent work in the same area, under different data regimes.  Part (a) shows that in the non low data regime, ELC-BERT performs similarly to the baseline, while DenseFormer consistently performs better. Part (b) shows that ELC-BERT outperforms DenseFormer in a low data regime (as expected, since this is the setting for which ELC-BERT was developed), but DenseFormer quickly catches up and surpasses ELC-BERT's performance after roughly 1200 iterations.", "section": "B.7 Comparison with ELC-BERT"}]