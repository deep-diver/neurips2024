[{"type": "text", "text": "Bridging Inter-task Gap of Continual Self-supervised Learning with External Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recent research on Self-Supervised Learning (SSL) has demonstrated its ability to   \n2 extract high-quality representations from unlabeled samples. However, in continual   \n3 learning scenarios where training data arrives sequentially, SSL\u2019s performance   \n4 tends to deteriorate. This study focuses on Continual Contrastive Self-Supervised   \n5 Learning (CCSSL) and highlights that the absence of contrastive learning on inter  \n6 task data, due to the unavailability of historical samples, leads to a significant drop   \n7 in performance. To tackle this issue, we introduce a simple and effective method   \n8 called BGE, which Bridges the inter-task Gap of CCSSL using External data from   \n9 publicly available datasets. BGE enables the contrastive learning of each task data   \n10 with external data, allowing relationships between them to be passed along the tasks,   \n11 thereby facilitating implicit inter-task data comparisons. To overcome the limitation   \n12 of the external data selection and maintain its effectiveness, we further propose   \n13 the One-Propose-One algorithm to collect more relevant and diverse high-quality   \n14 samples from the chosen external data while flitering out distractions from the out  \n15 of-distribution data. Experiments show that BGE can generate better discriminative   \n16 representation in CCSSL, especially for inter-task data, and improve classification   \n17 results with various external data compositions. Additionally, the proposed method   \n18 can be seamlessly integrated into existing continual learning methods yielding   \n19 significant performance improvement. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 In recent years, deep neural networks [13, 22, 35] have achieved great success, but plenty of works   \n22 are under the assumption that all data are available simultaneously for training. In practical scenarios,   \n23 acquiring the entire dataset at once is often challenging due to data being constantly updated. In this   \n24 case, training the network continually suffers from catastrophic forgetting [38], meaning that the   \n25 network severely forgets old task knowledge after learning the new one. Hence, continual learning   \n26 investigates methods to train networks incrementally while mitigating catastrophic forgetting.   \n27 Although continual learning has been widely studied and numerous effective methods [32, 36, 40]   \n28 have been proposed, most existing research remains focused on supervised learning, with Continual   \n29 Contrastive Self-Supervised Learning (CCSSL) receiving relatively little attention. However, studying   \n30 CCSSL is equally significant.   \n31 To prevent catastrophic forgetting, prior CCSSL works CaSSLe [16], PFR [18], and POCON [19]   \n32 use knowledge distillation, while CPPF [11] incorporates prototype clustering. In this paper, we   \n33 highlight an important but generally overlooked issue in these works: Comparisons of inter-task   \n34 data are absent. Specifically, a widely accepted opinion in continual learning is that if the sum of   \n35 each task\u2019s loss is minimized, then continual learning\u2019s performance reaches its upper bound: joint   \n36 learning. However, in CCSSL, even if each task\u2019s loss is minimized, there is still a gap between joint   \n37 learning. Because joint learning requires any sample pair in the entire dataset to participate in the   \n38 contrastive loss computation. In contrast, in continual learning, inter-task data are unavailable to each   \n39 other, meaning this aspect of the contrastive loss is never computed and optimized. This omission   \n40 increases the likelihood of inter-task class confusion, as illustrated in Figure 1 Right, despite classes   \n41 from four different tasks having distinctly different semantics, they still show confusion in prior   \n42 methods Fine-tune and CaSSLe [16]. In contrast, our method and joint training consider inter-task   \n43 comparisons and can better distinguish them.   \n44 Since we could not directly use data from other tasks for inter-task comparisons, we would like to   \n45 compensate for these comparisons with the help of external data. Some prior works [31, 52, 56]   \n46 have explored using external data for continual learning. GD [31] and ZSCL [56] use external   \n47 data for distillation to stabilize the feature space, while requiring extensive external data and high   \n48 computational costs. ST [52] employs external data as additional training data, but as a supervised   \n49 method, it requires pseudo-labels, making it less robust to out-of-distribution (OOD) data. Tang et   \n50 al. [45] enhance exemplar diversity with external data. Existing methods focus on using external   \n51 data in supervised learning, but given that CCSSL does not require labels for training, we propose   \n52 using external data in CCSSL, which avoids the need for pseudo-labels and is more generalizable and   \n53 robust to OOD data. Besides, our motivation is to improve feature space by compensating for absent   \n54 comparisons rather than merely stabilizing it, and it does not require extensive external data.   \n55 In summary, we propose incorporating publicly available external data into training to compensate for   \n56 the absent inter-task comparisons, as shown in Figure 1 Left. When the external dataset is sufficiently   \n57 large, it is reasonable to assume a high probability that some external data share similar features with   \n58 the task data, even if they are in different classes. By incorporating these high-quality external data   \n59 into CCSSL, the data from each task can be compared with them. enables the inter-data relationship to   \n60 be passed along the tasks, thereby constructing implicit inter-task comparisons. Further, considering   \n61 that external data in open-world scenarios may contain extensive OOD data that is not beneficial for   \n62 task training, we propose the One-Propose-One (OPO) sampling algorithm, to sample high-quality   \n63 external data that are relevant to tasks and sufficiently diverse without any hyperparameters.   \n64 Experiments demonstrate that BGE can be seamlessly integrated into existing methods, resulting   \n65 in significant performance improvement. We also point out that although it may seem unsurprising   \n66 that network performance improves with more training data, this improvement is not due to richer   \n67 input features, because when we add equal external data into joint training, the performance doesn\u2019t   \n68 improve even sometimes decreases. Instead, BGE compensates for the absent comparisons caused by   \n69 inter-task data unavailability, which is much more meaningful in continual learning. Our contributions   \n70 can be summarized as follows:   \n71 \u2022 We point out that existing methods overlook the issue of inter-task data comparisons, and   \n72 propose BGE to incorporate external data into training to address this gap.   \n73 \u2022 We propose the One-Propose-One (OPO) sampling algorithm to sample external data that   \n74 are relevant to tasks and sufficiently diverse, while also filtering out OOD data that are not   \n75 beneficial for learning.   \n76 \u2022 Experiments show that BGE can be seamlessly integrated into existing CCSSL methods and   \n77 consistently yields significant improvement. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "UGKgoAZuuL/tmp/87b5ef154a1ee020c25b29ab0aad5896f02e217c948dbc7f3594141954439deb.jpg", "img_caption": ["Figure 1: Left: Overview of our method BGE. In typical CCSSL methods, the inter-task data pairs are incomparable. We employ an external dataset to complement these missing comparisons, effectively bridging the inter-task gap. Right: t-SNE [47] visualization of four classes belonging to different tasks in continual learning. Compared to prior methods Fine-tune and CaSSLe [16], we make the inter-task data more separable. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "78 2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "79 Self-Supervised Learning (SSL) SSL trains the network without the need for supervised signals.   \n80 One of the prominent branches is contrastive learning [5, 8\u201310, 21, 23, 53]. The objective of   \n81 contrastive learning can be roughly explained as reducing the distance between positive pairs while   \n82 enlarging it between negative pairs. SimCLR [8] simply follows this objective but requires a large   \n83 batch size. MoCo [10, 23] introduces a momentum encoder and a negative sample dictionary to   \n84 solve this problem. SwAV [5] and Barlow Twins [53] introduces prototype comparisons and cross  \n85 decorrelation loss, respectively. Then BYOL [21] and SimSiam [9] can conduct contrastive learning   \n86 without negative samples. However, all these methods assume that a large dataset is available for   \n87 pre-training, which is often impractical in real-world scenarios where data acquisition is incremental.   \n88 Therefore, we research a continual method, which is more practical.   \n89 Since no labeling requirement, incorporating external data into SSL is straightforward. Prior long  \n90 tailed SSL works [3, 28] leverage external data to balance head and tail classes. Instead, we extend the   \n91 exploration to continual learning, aiming to use external data to compensate for the absent inter-task   \n92 comparisons while further preventing catastrophic forgetting.   \n93 Continual learning Continual learning allows the network to learn from sequentially arriving data   \n94 and prevent catastrophic forgetting. Existing continual learning methods can be categorized into   \n95 three groups, which are 1) Regularization-based methods [1, 14, 29, 32, 34, 50, 54] add additional   \n96 regularization constraints such as knowledge distillation [14, 32, 50] or limiting important parameters   \n97 update [1, 29, 34, 54] to network training. 2) Replay-based methods [4, 26, 40, 43, 55] save few   \n98 representative data from old tasks called exemplars to recover the distribution of old data when the   \n99 new task is trained. 3) Architecture-based methods [15, 36, 37, 41, 51], which adjust the architecture   \n100 or parameters of the network during each task training. Currently, most continual learning methods   \n101 still focus on supervised learning. While some of them [6, 33, 44] draw on the idea of contrastive   \n102 learning, there are still few works consider continual learning without any supervision. Among them,   \n103 CaSSLe [16], PFR[18], and POCON[19] use distillation, and CPPF[11] adds clustering to form   \n104 a more complete framework. Sy-CON [7] also reveals the distinction between CCSSL and joint   \n105 training, but it only additionally passes current task data into the old network to get more diverse   \n106 intra-task negative features, which still fails to provide effective inter-task comparisons. Thus it   \n107 underperforms in most contrastive learning frameworks. Compared to them, we introduce external   \n108 data to facilitate implicit inter-task comparisons to solve the problem of absent inter-task comparisons. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "109 3 Proposed method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "110 3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 Contrastive Self-Supervised Learning (CSSL) In Self-Supervised Learning (SSL), the dataset $D$   \n112 contains only $n$ image inputs $\\{x_{1},x_{2},...,x_{n}\\}$ without labels. SSL trains a network $f_{\\theta}$ parameterized   \n113 by $\\theta$ to map these inputs to embeddings $\\{z_{1},z_{2},...,z_{n}\\}$ . Many well-known SSL works [5, 8, 21, 23,   \n114 53] use contrastive learning framework. In contrastive learning, a random augmentation function   \n115 $A$ is pre-designed. Given an input $x$ , two augmented views $(x_{a},x_{b})$ are obtained by applying $A$   \n116 twice. Subsequently, embeddings $z_{a}=f_{\\theta}(x_{a})$ and $z_{b}=f_{\\theta}(x_{b})$ are passed through a projector $h_{\\theta^{\\prime}}$   \n117 parameterized by $\\theta^{\\prime}$ to get $z_{a}^{\\prime}=h_{\\theta^{\\prime}}(z_{a})$ , $z_{b}^{\\prime}=h_{\\theta^{\\prime}}(z_{b})$ , which are involved in $\\mathcal{L}_{S S L}$ . In essence,   \n118 $\\mathcal{L}_{S S L}$ expects the network to output similar embeddings for two views of the same input (i.e. positive   \n119 pair), while ensuring that embeddings from views of different inputs (i.e. negative pair) are dissimilar.   \n120 Continual CSSL (CCSSL) In CCSSL setting, The overall dataset $D$ is divided into multiple tasks.   \n121 Assuming that $T$ tasks $\\{\\tau_{1},\\tau_{2},...,\\tau_{T}\\}$ are to be learned, $D$ can be divided into $\\{D_{1},D_{2},...,D_{T}\\}$ ,   \n122 where $D_{i}\\cap D_{j}=\\emptyset,\\forall i,j\\in\\{1:T\\}$ . Also as SSL, for each task $\\tau,D_{t}$ is only composed of $n_{t}$   \n123 images $\\{x_{1},x_{2},...,x_{n_{t}}\\}$ without labels. Continual learning requires the network to learn knowledge   \n124 as each task\u2019s data arrives sequentially, with dataset $D_{i}$ only available at $\\mathcal{T}_{i}$ . The optimization   \n125 objective is to continually train the network parameter $\\theta$ to satisfy every task, which is defined as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\na r g m i n\\sum_{\\theta}^{T}\\mathbb{E}_{(x_{a},x_{b})\\sim A(D_{t})}\\mathcal{L}_{S S L}\\big(h_{\\theta^{\\prime}}\\big(f_{\\theta}(x_{a})\\big),h_{\\theta^{\\prime}}\\big(f_{\\theta}(x_{b})\\big)\\big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 3.2 Revising and improving CCSSL via external data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 Typical contrastive learning paradigms [8, 23, 53] can be generalized as reducing distances between   \n128 positive pairs and enlarging them between negative pairs on feature hyperspheres. Adjusting the   \n129 interrelationships of sample pairs in this way enables the network to effectively represent features   \n130 [27, 49]. However, in CCSSL, the data is divided by tasks. During the learning process of task $\\mathcal{T}_{t}$ , data   \n131 from other tasks are unavailable. This prevents adequate tuning of inter-sample relationships, resulting   \n132 in suboptimal network training. We identify two reasons for this suboptimality: 1) The network   \n133 rapidly forgets knowledge about old data due to catastrophic forgetting, so their features cannot   \n134 be well extracted in subsequent tasks. 2) Insufficient learning about each task occurs because data   \n135 from one task cannot act as negative samples for another task. While prior works address problem 1   \n136 through techniques like distillation [16, 18, 19] and clustering [11], problem 2 remains underexplored.   \n137 However, we argue that this is unreasonable, and solving problem 2 is equally important.   \n138 Prior works [20, 32] widely agree that in the ideal case, continual learning can perform up to joint   \n139 learning, wherein no forgetting occurs and each task reaches optimality. However, in CSSL, even if   \n140 no forgetting occurs, there is still an optimization gap between continual and joint learning due to the   \n141 absence of inter-task data comparisons in the training objective. Unlike supervised learning which   \n142 guides the network through labels, CSSL relies on data interactions for network learning. When data   \n143 is incomplete, the training objective also becomes incomplete. For better comprehension, we can   \n144 decompose the joint training contrastive loss into two terms as in Eq. 2, representing the comparisons   \n145 of intra-task and inter-task data, denoted as $\\mathcal{L}_{i n t r a}$ and $\\mathcal{L}_{i n t e r}$ , respectively. $\\mathcal{L}_{i n t r a}$ is the training   \n146 objective of the conventional CCSSL, also referred to as $\\mathcal{L}_{c o n t i n u a l}$ . However, for input $x\\in D_{t}$   \n147 in task $\\mathcal{T}_{t}$ , negative samples come exclusively from $D_{t}$ rather than the overall dataset $D$ , making   \n148 direct comparisons between inter-task data infeasible. Consequently, $\\mathcal{L}_{i n t e r}$ can not be computed and   \n149 optimized in continual learning forever, resulting in a $\\mathcal{L}_{i n t e r}$ gap between $\\mathcal{L}_{c o n t i n u a l}$ and $\\mathcal{L}_{j o i n t}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{L}_{j o i n t}=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\widetilde{\\mathbb{E}}_{(x_{a},x_{b})\\sim A(D_{t})}\\mathcal{L}_{S S L}\\left(h_{\\theta^{\\prime}}\\left(f_{\\theta}\\left(x_{a}\\right)\\right),h_{\\theta^{\\prime}}\\left(f_{\\theta}\\left(x_{b}\\right)\\right)\\right)\\right.}}\\\\ &{\\quad\\quad+\\underbrace{\\mathbb{E}_{\\underset{x_{b}\\sim A(D_{t})}{x_{a}\\sim A(D_{t})},}\\mathcal{L}_{S S L}\\left(h_{\\theta^{\\prime}}\\left(f_{\\theta}\\left(x_{a}\\right)\\right),h_{\\theta^{\\prime}}\\left(f_{\\theta}\\left(x_{b}\\right)\\right)\\right)}_{\\mathcal{L}_{i n t e r}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "150 We argue that the lack of optimization for $\\mathcal{L}_{i n t e r}$ leads to confusion between inter-task data. Figure 1   \n151 Right compares the t-SNE visualizations of features from 4 CIFAR100 classes under joint and 10   \n152 tasks continual training (4 classes belong to different tasks during continual training). Compared to   \n153 the joint-trained network, the continually trained network shows poor clustering and severe class   \n154 boundary confusion. More experiments about inter-task confusion can be found at Appendix A.2.1.   \n155 Despite CaSSLe [16] employing distillation to consolidate old knowledge, the issue of inter-task class   \n156 boundary confusion remains. To address the overlooked problem of $\\mathcal{L}_{i n t e r}$ , a straightforward idea   \n157 is to save exemplars for each task. However, this may raise serious privacy concerns. We therefore   \n158 explore an alternative method to optimize $\\mathcal{L}_{i n t e r}$ without exemplars and protect the discriminative   \n159 class boundaries. Figure 1c shows the feature distribution of our method, with all 4 inter-task classes   \n160 better distinguished, and the overall distribution closer to joint training.   \n161 To compensate for $\\mathcal{L}_{i n t e r}$ , bridging the gap of inter-task comparisons is essential. This requires   \n162 introducing additional comparisons into each task, implying extra data incorporation. Under the   \n163 constraints of continual learning, simultaneous access to data from multiple tasks is infeasible.   \n164 Therefore, the idea emerges to incorporate publicly available external data into CCSSL to address the   \n165 lack of inter-task comparisons. Each task\u2019s data can be directly compared with external data, enabling   \n166 relationships between data to be passed along the task sequence. Moreover, using external data better   \n167 protects privacy, and the costs of obtaining unlabeled data from public data sources are extremely low.   \n168 We thus propose our method BGE, meaning Bridging the inter-task comparison Gap with External   \n169 data, as shown in Figure 1 Left. BGE incorporates external data into each task\u2019s training except   \n170 the first one, and resamples part of them after each task using our sampling algorithm ( detailed in   \n171 Section 3.3). This external data acts as a bridge for inter-task comparisons, constructing implicit   \n172 comparisons for inter-task data. For task $\\mathcal{T}_{t}$ , with $D_{e}^{t-1}$ as the external data sampled after task $\\mathcal{T}_{t-1}$ ,   \n173 the training objective is defined as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}=\\mathbb{E}_{(x_{a},x_{b})\\sim A\\left(D_{t}\\cup D_{e}^{t-1}\\right)}\\mathcal{L}_{S S L}\\left(h_{\\theta^{\\prime}}\\left(f_{\\theta}\\left(x_{a}\\right)\\right),h_{\\theta^{\\prime}}\\left(f_{\\theta}\\left(x_{b}\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 Incorporating external data aligns the optimization objective of continual learning more closely with   \n175 Eq. 2, enhancing the mutual understanding of inter-task classes. ", "page_idx": 4}, {"type": "text", "text": "176 3.3 One-Propose-One (OPO) sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 While abundant external data features generally cover in-task data comprehensively, incorporating all   \n178 external data into continual learning is impractical due to computational constraints. Additionally,   \n179 open-world external data may include substantial task-irrelevant out-of-distribution (OOD) data,   \n180 which is unhelpful for training. Therefore, a sampling algorithm is needed to select high-quality   \n181 external data. We observe that $\\mathcal{L}_{i n t e r}$ includes comparisons of current task data $D_{t}$ with both old task   \n182 data $D_{1:t-1}$ and future task data $D_{t+1:T}$ . So sampled external data should ideally proxy for both old   \n183 and future task data. To represent old data, sampled data should have similar features to them, while   \n184 representing future data requires imaginative sampling. Therefore, our sampling algorithm is based   \n185 on both proximity and diversity considerations, and integrates these two aspects into a single objective   \n186 without any hyperparameters. We noted that prior sampling algorithms [3, 28] for long-tailed learning   \n187 also consider proximity and diversity, but they require hyperparameters selection.   \n188 We measure proximity using the cosine distance between sample features. On the other hand, prior   \n189 work [49] indicates that to avoid collapse, contrastive learning methods tend to map all inputs to   \n190 a uniform distribution within the feature hypersphere (i.e. uniformity). Thus we assume that the   \n191 entire distribution of the current task data approximately covers the hypersphere, ensuring diversity.   \n192 Based on the above, we propose a sampling algorithm called One-Propose-One $(O P O)$ as depicted   \n193 in Algorithm 1. After training each task $\\mathcal{T}_{t}$ , OPO constructs the external dataset $D_{e}^{t}$ , which is then   \n194 incorporated in training task $\\mathcal{T}_{t+1}$ . Specifically, OPO considers that each in-task sample can equally   \n195 propose an external sample with the closest feature distance to itself and has not been proposed.   \n196 Given the current task budget $K_{t}$ , we collect all proposed samples as a candidate set $D_{c}$ , and select   \n197 the $K_{t}$ minimum distance samples to be added to the external dataset $D_{e}^{t}$ . We follow iCaRL [40]\u2019s   \n198 exemplar update algorithm, maintaining an equal budget for each task within the total budget $K$ .   \n199 OPO ensures proximity and diversity without hyperparameters, maintaining similarity to old data and   \n200 adequate coverage of future data features. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "201 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "202 4.1 Experimental setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "203 Dataset setup We conduct experiments with the following datasets: 1) CIFAR100 [30], which   \n204 contains 100 classes, each with 500 train images and 100 test images. Each image is $32\\!\\times\\!32$ pixels.   \n205 We follow the class incremental learning setting to split the classes equally by the number of tasks.   \n206 Experiments are conducted under 4 tasks and 10 tasks settings, wherein each task contains 25 classes ", "page_idx": 4}, {"type": "text", "text": "Input: current task $\\mathbf{ID}\\ t$ , current task dataset $D_{t}$ , entire external dataset $D_{o u t}$ , last task sampled external dataset $D_{e}^{t-1}$ , model $f$ , total budget $K$ , cosine distance metric $c o s(\\cdot,\\cdot)$   \nOutput: sampled external dataset $D_{e}^{t}$   \n1: Calculate current task budget $\\begin{array}{r}{K_{t}=\\frac{K}{t}}\\end{array}$ , Adjust $D_{e}^{t-1}=\\mathrm{REDUCEDATA}(D_{e}^{t-1},K_{t})$ [40]   \n2: Create candidate set $D_{c}=\\{\\}$   \n3: while $\\mid D_{c}\\mid<K_{t}$ do   \n4: for each $x\\in D_{t}$ do   \n5: $\\begin{array}{r l}&{u=a r g\\overline{{m}}i n_{x^{\\prime}\\in(D_{o u t}-D_{e}^{t-1})}^{\\nu}c o s(f(x),f(x^{\\prime})),d_{u}=m i n_{x_{i}\\in D_{t}}c o s(f(x_{i}),f(u))}\\\\ &{D_{c}=D_{c}\\cup\\{u\\},D_{o u t}=D_{o u t}-\\{u\\}}\\end{array}$   \n6:   \n7: end for   \n8: end while   \n9: $D_{c}^{\\prime}=\\mathrm{SORT}(D_{c},k e y=d_{u})\\,[:K_{t}],\\,D_{e}^{t}=D_{e}^{t-1}\\cup D_{c}^{\\prime}$   \n10: return $D_{e}^{t}$ ", "page_idx": 5}, {"type": "text", "text": "207 and 10 classes. 2) ImageNet100 [46], which consists of 100 classes selected from ImageNet [12],   \n208 with a total of 130K images of $224\\!\\times\\!224$ pixels. It is equally split under 5 tasks and 10 tasks settings.   \n209 External dataset setup For CIFAR100, the selected external datasets include CIFAR10,   \n210 Places $365_{t e s t}$ (the test set of Places365 [57]) and ImageNet-R [24], among them, Places $365_{t e s t}$ and   \n211 ImageNet-R are OOD for CIFAR100. CIFAR10 contains 50,000 images with $32\\!\\times\\!32$ pixels in 10   \n212 classes. Places365 is a scene recognition dataset with its test set containing 328,500 images of various   \n213 scenes. ImageNet-R contains 24,000 images featuring art, cartoons, and other styles. We resize both   \n214 Place $365_{t e s t}$ and ImageNet-R to $32\\!\\times\\!32$ pixels. We consider three compositions of external datasets,   \n215 CIFAR (CIFAR10), CP $({\\mathrm{CIFAR}}10{+}{\\mathrm{Places}}365_{t e s t})$ ) and CPI (CIFAR10+Places $365_{t e s t}$ +ImageNet-R)   \n216 For ImageNet100, the external datasets include ImageNet900, Places365 and DomainNet [39].   \n217 ImageNet900 is all data in ImageNet excluding ImageNet100, totaling 1.1 million images. Places365   \n218 contains 1.8 million images, and DomainNet contains 0.6 million images of 6 domains. They are also   \n219 used here as OOD data. All data are $224\\!\\times\\!224$ pixels. We consider three compositions of external   \n220 datasets, IN (ImageNet-900), INP (ImageNet9 $^{100+}$ Places365) and IND (ImageNet900+DomainNet).   \n221 Baselines We compare the original performance of existing exemplar-free CCSSL methods to their   \n222 performance when with BGE. The methods we compare include 1) Fine-Tune (FT): Sequentially   \n223 training the network with data from each task without additional prevention of catastrophic forgetting.   \n224 2) CaSSLe [16]: Introducing a distillation loss between the current model and the old model in   \n225 the form of contrastive loss. 3) PFR [18]: Addressing catastrophic forgetting based on functional   \n226 regularization [17]. We slightly optimized its network structure and training procedure.   \n227 Training and evaluation setup Unless specified otherwise, all experiments employ Barlow Twins   \n228 [53] as the contrastive learning framework and Resnet18 [22] as the backbone. The sampling budget   \n229 is uniformly set at 10K. For evaluation, we follow [16, 18, 19] to report the linear evaluation accuracy   \n230 of the final network across all classes as the evaluation metric. For other setups see Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "231 4.2 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "232 Performance improvement on prior methods We compare the performance improvement BGE   \n233 yields to the base methods when using different external data compositions. Table 1 shows that   \n234 on CIFAR100, BGE can consistently and significantly improve base methods. It is worth noting   \n235 that as the number of tasks increases, BGE yields even greater improvement, with improvement of   \n236 $1.5\\%{-3.5\\%}$ for 4 tasks and $2.5\\%-7\\%$ for 10 tasks. This is also in line with our motivation, as an   \n237 increasing number of tasks results in more missing inter-task data comparisons.   \n238 Moreover, across different external dataset compositions, we observe that CIFAR yields the most   \n239 significant improvement. This is attributed to the CIFAR10 dataset best matches the distribution of   \n240 CIFAR100, thereby offering highly relevant features, even if their classes do not intersect. When in  \n241 corporating datasets like Places365 or ImageNet-R, which are OOD for CIFAR100, the improvement   \n242 decreases. Thanks to our OPO sampling algorithm can well resist the harm of OOD data (detailed in   \n243 Section 4.3). On ImageNet100, the performance improvement is shown in Table 2, showcasing a   \n244 similar improvement regularity to that observed on CIFAR100. BGE achieves $1.5\\%{-4\\%}$ improvement   \n245 for 5 tasks and $5\\%-7.5\\%$ improvement for 10 tasks. More experiments see Appendix A.2.7.   \n246 We also emphasize that although it might seem intuitive that network performance would improve   \n247 with richer data because of richer features, BGE yielded improvement does not simply stem from   \n248 using more data. In Table 1 and Table 2, we incorporate an equal amount of external data into   \n249 joint training. However, the results do not improve, and may even decrease when the external data   \n250 contains OOD samples. We believe this is because incorporating irrelevant external data into the   \n251 training process causes the model to allocate some capacity to learning these unrelated data, thereby   \n252 weakening its focus on the in-task data. Hence, the learning of external data can not directly contribute   \n253 to the learning of in-task data.   \n254 Long task sequence experiments We conduct experiments with 100 tasks on CIFAR100, which   \n255 means one task only contains one class, to verify the effectiveness of BGE on long task sequences.   \n256 We set the sampling budget to 1000. Figure 2 shows the performance of different base methods   \n257 with or without BGE as the learned tasks increase. On one hand, BGE improves the final network   \n258 performance, especially evident in FT and PFR. On the other hand, the network\u2019s performance   \n259 increases even more rapidly with BGE, indicating that the network\u2019s generalization ability to unseen   \n260 tasks is higher. This stems from BGE can both overcome catastrophic forgetting and compare with   \n261 future tasks it guessed, thus accumulating more knowledge in the early training stages. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/a4edd8256dfe491580a8eb3ce6981e91a19e238cde223150220bc1eb783787e1.jpg", "table_caption": ["Table 1: Comparison of BGE\u2019s performance improvement on CIFAR100. CIFAR, CP, and CPI are different external dataset compositions. Performance was evaluated by linear evaluation accuracy of the final network. We equally divided classes into 4 tasks and 10 tasks. BGE consistently improves base methods across different external dataset compositions. As for Joint training, ED represents adding equivalent external data, which does not improve the performance. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/a2c045d70d72e59389b193467a59dc2fb937f368eefa74f90a6f5f3bdc6696e2.jpg", "table_caption": ["Table 2: Performance improvement yielded by BGE on ImageNet100. IN, INP, and IND are different external dataset compositions. ED represents adding equivalent external data in joint training. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "UGKgoAZuuL/tmp/d2e76d8526af4d2750f8e41fa56012b99f9624f955fd143de1ed0c36915b6c8a.jpg", "img_caption": ["Figure 2: Performance improvement of BGE at CIFAR100 100 tasks setting. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/ca2b566b946b7436e4eec9d64ae5468b560ec54334c47ae9f25a40171765ecf6.jpg", "table_caption": ["Table 3: Accuracy on CIFAR100 and ImageNet100 with different sampling algorithms. Bold indicates better performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "262 4.3 Ablation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "263 Sampling algorithm Table 3 shows the effect of OPO sam  \n264 pling compared to random sampling for FT and PFR improve  \n265 ment when external datasets contain OOD data. OPO algo  \n266 rithm consistently provides more improvement than random   \n267 sampling. However, we also observed that when all external   \n268 data are in-distribution (ID), the improvement from OPO algo  \n269 rithm is not stable. This suggests that external data quality is   \n270 sufficiently high, making random sampling sufficient for our   \n271 needs. To validate this, we calculated the Fr\u00e9chet Inception   \n272 Distance (FID) scores [25] between the in-task dataset and   \n273 external datasets obtained by different sampling algorithms   \n274 under CIFAR and CPI compositions, as shown in Figure 3.   \n275 A lower FID score indicates greater similarity between two   \n276 datasets, and vice versa. Figure 3 shows that with the CIFAR   \n277 composition, the FID score is lower, and the effect of the OPO   \n278 algorithm is little, indicating that this dataset is already of   \n279 high quality. In contrast, under CPI, the FID score is higher when random sampling, while shows a   \n280 significant decrease when OPO sampling. It indicates that the OPO algorithm adjusts the distribution   \n281 of the external dataset considerably to make it more compatible with the in-task dataset. Therefore   \n282 OPO algorithm will have more advantages when the external dataset contains OOD data.   \n283 Besides, we observed that the advantage of OPO sampling algorithm is more significant on the   \n284 ImageNet100 dataset. We believe this can be attributed to two factors: 1) Higher image pixels contain   \n285 more information, and fewer images will satisfy the proximity. 2) With a larger quantity of external   \n286 data, there are more potentially high-quality data, facilitating better sampling.   \n289 provided by BGE contribute more to performance improve  \n290 ment. We conduct experiments based on CaSSLe [16] on the   \n291 CIFAR100 4 tasks setting. Because this experiment requires   \n292 explicitly calculating the loss incurred by each positive and   \n293 negative pair, we convert the framework to SimCLR [8]. We   \n294 masked the additional positive or negative pairs in Table 4.   \n295 The results show that both types of pairs improve performance   \n296 individually, and negative pairs yield more significant improve  \n297 ment, supporting our emphasis that the impact of absent inter-task comparisons is severe but neglected.   \n298 But positive pairs also yield performance improvement, which is because high-quality external data   \n299 have feature intersections with in-task data, proving that external data can prevent catastrophic   \n300 forgetting as well. With the synergistic effect of both, the improvement reaches the highest.   \n301 Experiments with only OOD external data In the experiments presented in Table 1 and Table 2,   \n302 all external data contain some amount of ID data. To assess BGE\u2019s performance without any ID data   \n303 in the external dataset, we conduct experiments on CIFAR100 4 tasks based on PFR, as shown in   \n304 Table 5. The external dataset is only composed of ImageNet-R or Places $365_{t e s t}$ . In joint training,   \n305 these data are detrimental. While in continual training, BGE consistently improves the base method   \n306 by nearly $2\\%$ , regardless of the composition of OOD data used. It indicates that the performance   \n307 improvement from BGE does not only come from imitating in-task data features, but also from   \n308 introducing similar additional comparisons into each task itself, which is beneficial for constructing   \n309 implicit inter-task comparisons. Even if the external data has few recognizable similar features to   \n310 the in-task data, the network can still try its best to mine valuable knowledge from external data to   \n311 compensate for inter-task comparisons.   \n312 BGE with more types of datasets We validate the effective  \n313 ness of BGE across more aspects of external datasets. Table 6   \n314 presents the results when using GenImage [58], a dataset of gen  \n315 erated images; CC3M [42], a dataset sourced from the Internet;   \n316 and CUB200 [48], a fine-grained bird dataset as external dataset.   \n317 Experiments with GenImage and CC3M demonstrate BGE\u2019s effec  \n318 tiveness with both model-generated and real-world Internet data,   \n319 demonstrating its practical value. Since CUB200 is fine-grained   \n320 and lacking in diversity, it is extremely unfriendly to BGE, yet   \n321 BGE can still improve the base method. ", "page_idx": 7}, {"type": "image", "img_path": "UGKgoAZuuL/tmp/9c9143380bac714e0467d163ed05e0b11fe30907785f0b26386540d13edff390.jpg", "img_caption": ["Figure 3: FID score of different sampling algorithms when CIFAR and CPI as external data. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/7b007c663c8de39ea8332894851489512266bc26206ed870404219e4a0d9a37e.jpg", "table_caption": ["287 Effect of additional positive and negative pairs We fur- Table 4: Comparison of additiona 288 ther investigate whether additional positive or negative pairs positive and negative pairs\u2019 effects. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/c4ad7ef8222addde081efa3494324faec494eaf95b9a6b0695ac2c04d728c0e2.jpg", "table_caption": ["Table 5: Effectiveness of BGE when external data are totally OOD. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/57d4bf22153b83c9b8ce2115576591269640cd1213e7d7012590c7ca7322284a.jpg", "table_caption": ["Table 6: Performance of BGE when choosing more types of datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "322 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "323 In this paper, we address a commonly overlooked but severe issue in Continual Contrastive Self  \n324 Supervised Learning (CCSSL): the lack of inter-task comparisons. To tackle this, we propose our   \n325 method BGE to incorporate external data into training, bridging the inter-task gap and facilitating   \n326 implicit inter-task data comparisons. We also design the One-Propose-One sampling algorithm to   \n327 select high-quality external data and fliter out irrelevant OOD data. BGE can be seamlessly integrated   \n328 into existing methods and yield significant improvement. ", "page_idx": 8}, {"type": "text", "text": "329 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "330 [1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses:   \n331 Learning what (not) to forget. In Proceedings of the European conference on computer vision   \n332 (ECCV), pages 139\u2013154, 2018.   \n333 [2] S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel. Deep vit features as dense visual descriptors.   \n334 arXiv preprint arXiv:2112.05814, 2(3):4, 2021.   \n335 [3] J. Bai, Z. Liu, H. Wang, J. Hao, Y. Feng, H. Chu, and H. Hu. On the effectiveness of out  \n336 of-distribution data in self-supervised long-tail learning. arXiv preprint arXiv:2306.04934,   \n337 2023.   \n338 [4] J. Bang, H. Kim, Y. Yoo, J.-W. Ha, and J. Choi. Rainbow memory: Continual learning with a   \n339 memory of diverse samples. In Proceedings of the IEEE/CVF conference on computer vision   \n340 and pattern recognition, pages 8218\u20138227, 2021.   \n341 [5] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of   \n342 visual features by contrasting cluster assignments. Advances in neural information processing   \n343 systems, 33:9912\u20139924, 2020.   \n344 [6] H. Cha, J. Lee, and J. Shin. Co2l: Contrastive continual learning. In Proceedings of the   \n345 IEEE/CVF International conference on computer vision, pages 9516\u20139525, 2021.   \n346 [7] S. Cha and T. Moon. Sy-con: Symmetric contrastive loss for continual self-supervised represen  \n347 tation learning. arXiv preprint arXiv:2306.05101, 2023.   \n348 [8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning   \n349 of visual representations. In International conference on machine learning, pages 1597\u20131607.   \n350 PMLR, 2020.   \n351 [9] X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the   \n352 IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \n353 [10] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive   \n354 learning. arXiv preprint arXiv:2003.04297, 2020.   \n355 [11] X. Chen, Z. Sun, K. Yan, S. Ding, and H. Lu. Combining past, present and future: A self  \n356 supervised approach for class incremental learning. arXiv preprint arXiv:2311.08764, 2023.   \n357 [12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical   \n358 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages   \n359 248\u2013255. Ieee, 2009.   \n360 [13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,   \n361 M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for   \n362 image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n363 [14] A. Douillard, M. Cord, C. Ollion, T. Robert, and E. Valle. Podnet: Pooled outputs distillation for   \n364 small-tasks incremental learning. In Computer vision\u2013ECCV 2020: 16th European conference,   \n365 Glasgow, UK, August 23\u201328, 2020, proceedings, part XX 16, pages 86\u2013102. Springer, 2020.   \n366 [15] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wier  \n367 stra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint   \n368 arXiv:1701.08734, 2017.   \n369 [16] E. Fini, V. G. T. Da Costa, X. Alameda-Pineda, E. Ricci, K. Alahari, and J. Mairal. Self  \n370 supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on   \n371 Computer Vision and Pattern Recognition, pages 9621\u20139630, 2022.   \n372 [17] S. Garg and Y. Liang. Functional regularization for representation learning: A unified theoretical   \n373 perspective. Advances in Neural Information Processing Systems, 33:17187\u201317199, 2020.   \n374 [18] A. Gomez-Villa, B. Twardowski, L. Yu, A. D. Bagdanov, and J. Van de Weijer. Continually   \n375 learning self-supervised representations with projected functional regularization. In Proceedings   \n376 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3867\u20133877,   \n377 2022.   \n378 [19] A. Gomez-Villa, B. Twardowski, K. Wang, and J. van de Weijer. Plasticity-optimized comple  \n379 mentary networks for unsupervised continual learning. In Proceedings of the IEEE/CVF Winter   \n380 Conference on Applications of Computer Vision, pages 1690\u20131700, 2024.   \n381 [20] D. Goswami, Y. Liu, B. Twardowski, and J. van de Weijer. Fecam: Exploiting the heterogeneity   \n382 of class distributions in exemplar-free continual learning. Advances in Neural Information   \n383 Processing Systems, 36, 2024.   \n384 [21] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch,   \n385 B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new ap  \n386 proach to self-supervised learning. Advances in neural information processing systems, 33:   \n387 21271\u201321284, 2020.   \n388 [22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In   \n389 Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013   \n390 778, 2016.   \n391 [23] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual   \n392 representation learning. In Proceedings of the IEEE/CVF conference on computer vision and   \n393 pattern recognition, pages 9729\u20139738, 2020.   \n394 [24] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Para  \n395 juli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution   \n396 generalization. In Proceedings of the IEEE/CVF international conference on computer vision,   \n397 pages 8340\u20138349, 2021.   \n398 [25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two   \n399 time-scale update rule converge to a local nash equilibrium. Advances in neural information   \n400 processing systems, 30, 2017.   \n401 [26] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin. Learning a unified classifier incrementally   \n402 via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n403 recognition, pages 831\u2013839, 2019.   \n404 [27] W. Huang, M. Yi, X. Zhao, and Z. Jiang. Towards the generalization of contrastive self  \n405 supervised learning. arXiv preprint arXiv:2111.00743, 2021.   \n406 [28] Z. Jiang, T. Chen, T. Chen, and Z. Wang. Improving contrastive learning on imbalanced data   \n407 via open-world sampling. Advances in Neural Information Processing Systems, 34:5997\u20136009,   \n408 2021.   \n409 [29] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,   \n410 J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in   \n411 neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n412 [30] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n413 [31] K. Lee, K. Lee, J. Shin, and H. Lee. Overcoming catastrophic forgetting with unlabeled data in   \n414 the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages   \n415 312\u2013321, 2019.   \n416 [32] Z. Li and D. Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and   \n417 machine intelligence, 40(12):2935\u20132947, 2017.   \n418 [33] H. Lin, B. Zhang, S. Feng, X. Li, and Y. Ye. Pcr: Proxy-based contrastive replay for online   \n419 class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer   \n420 Vision and Pattern Recognition, pages 24246\u201324255, 2023.   \n421 [34] X. Liu, M. Masana, L. Herranz, J. Van de Weijer, A. M. Lopez, and A. D. Bagdanov. Rotate   \n422 your networks: Better weight consolidation and less catastrophic forgetting. In 2018 24th   \n423 International Conference on Pattern Recognition (ICPR), pages 2262\u20132268. IEEE, 2018.   \n424 [35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In   \n425 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n426 11976\u201311986, 2022.   \n427 [36] A. Mallya and S. Lazebnik. Packnet: Adding multiple tasks to a single network by iterative   \n428 pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,   \n429 pages 7765\u20137773, 2018.   \n430 [37] A. Mallya, D. Davis, and S. Lazebnik. Piggyback: Adapting a single network to multiple tasks   \n431 by learning to mask weights. In Proceedings of the European conference on computer vision   \n432 (ECCV), pages 67\u201382, 2018.   \n433 [38] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The   \n434 sequential learning problem. In Psychology of learning and motivation, volume 24, pages   \n435 109\u2013165. Elsevier, 1989.   \n436 [39] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source   \n437 domain adaptation. In Proceedings of the IEEE/CVF international conference on computer   \n438 vision, pages 1406\u20131415, 2019.   \n439 [40] S.-A. Rebuff,i A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and   \n440 representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern   \n441 Recognition, pages 2001\u20132010, 2017.   \n442 [41] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with   \n443 hard attention to the task. In International conference on machine learning, pages 4548\u20134557.   \n444 PMLR, 2018.   \n445 [42] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed,   \n446 image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual   \n447 Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages   \n448 2556\u20132565, 2018.   \n449 [43] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. Advances   \n450 in neural information processing systems, 30, 2017.   \n451 [44] Z. Song, Y. Zhao, Y. Shi, P. Peng, L. Yuan, and Y. Tian. Learning with fantasy: Semantic-aware   \n452 virtual contrastive constraint for few-shot class-incremental learning. In Proceedings of the   \n453 IEEE/CVF conference on computer vision and pattern recognition, pages 24183\u201324192, 2023.   \n454 [45] Y.-M. Tang, Y.-X. Peng, and W.-S. Zheng. Learning to imagine: Diversify memory for   \n455 incremental learning using unlabeled data. In Proceedings of the IEEE/CVF Conference on   \n456 Computer Vision and Pattern Recognition, pages 9549\u20139558, 2022.   \n457 [46] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV   \n458 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16,   \n459 pages 776\u2013794. Springer, 2020.   \n460 [47] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning   \n461 research, 9(11), 2008.   \n462 [48] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011   \n463 dataset. 2011.   \n464 [49] T. Wang and P. Isola. Understanding contrastive representation learning through alignment   \n465 and uniformity on the hypersphere. In International conference on machine learning, pages   \n466 9929\u20139939. PMLR, 2020.   \n467 [50] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu. Large scale incremental learning.   \n468 In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n469 374\u2013382, 2019.   \n470 [51] S. Yan, J. Xie, and X. He. Der: Dynamically expandable representation for class incremen  \n471 tal learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n472 recognition, pages 3014\u20133023, 2021.   \n473 [52] L. Yu, X. Liu, and J. Van de Weijer. Self-training for class-incremental semantic segmentation.   \n474 IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n475 [53] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny. Barlow twins: Self-supervised learning via   \n476 redundancy reduction. In International conference on machine learning, pages 12310\u201312320.   \n477 PMLR, 2021.   \n478 [54] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In   \n479 International conference on machine learning, pages 3987\u20133995. PMLR, 2017.   \n480 [55] M. Zhai, L. Chen, and G. Mori. Hyper-lifelonggan: Scalable lifelong learning for image   \n481 conditioned generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n482 Pattern Recognition, pages 2246\u20132255, 2021.   \n483 [56] Z. Zheng, M. Ma, K. Wang, Z. Qin, X. Yue, and Y. You. Preventing zero-shot transfer   \n484 degradation in continual learning of vision-language models. In Proceedings of the IEEE/CVF   \n485 International Conference on Computer Vision, pages 19125\u201319136, 2023.   \n486 [57] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image   \n487 database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,   \n488 2017.   \n489 [58] M. Zhu, H. Chen, Q. Yan, X. Huang, G. Lin, W. Li, Z. Tu, H. Hu, J. Hu, and Y. Wang. Genimage:   \n490 A million-scale benchmark for detecting ai-generated image. Advances in Neural Information   \n491 Processing Systems, 36, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "492 A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "493 A.1 Experimental details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "494 We use SGD optimizer with warmup cosine scheduler to train the network with batchsize of 256. For   \n495 CIFAR100, we train 500 epochs per task with a learning rate of 0.3 and weight decay of 1e-4 for   \n496 FT and CaSSLe[16]. For PFR[18], we use the learning rate as 0.4. For ImageNet100, we train 400   \n497 epochs per task with a learning rate of 0.4 and weight decay of 1e-4.   \n498 We use one RTX 3090 for CIFAR100 experiments and one A40 for ImageNet100 experiments. For   \n499 CIFAR100 experiments, it takes about 5 hours in 4 tasks setting and 8 hours in 10 tasks setting. For   \n500 ImageNet100 experiments, it takes about 17 hours in 5 tasks setting and 27 hours in 10 tasks setting. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "501 A.2 More experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "502 A.2.1 BGE\u2019s improvement to inter-task confusion ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "503 We categorize the results of classification errors into two types, inter-task confusion (the wrong   \n504 prediction belongs to a different task than the target) and intra-task confusion (the wrong prediction   \n505 belongs to the same task as the target). Under the CIFAR100 4 tasks setting, we compare the   \n506 probability of each of the two types of confusion occurring for the class contained in the last task for   \n507 the three baseline methods, as shown in Table 7. Ideally, the ratio of intra-task confusion to inter-task   \n508 confusion should be 1:3, since the ratio of the number of current task classes to the total number   \n509 of previous task classes is 1:3. However, the inter-task confusion in Table 7 is 5 to 7 times higher   \n510 than the intra-task confusion, suggesting that the lack of $\\mathcal{L}_{i n t e r}$ optimization has a severe impact on   \nperformance, while BGE improves this and decreases inter-task confusion. ", "page_idx": 13}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/70a44992cefc6c1508136bdb6f0394f19b9ef27b5ac334e11c5c3b6df93bd6b7.jpg", "table_caption": ["Table 7: Comparison of intra-task confusion and inter-task confusion. $\\downarrow$ means the value is the lower the better. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "511 ", "page_idx": 13}, {"type": "text", "text": "512 A.2.2 Experiments on the method without negative samples ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "513 While the results in Table 4 indicate that the effectiveness of BGE mainly stems from additional   \n514 negative samples, we conducted experiments using the contrastive learning framework BYOL, which   \n515 calculates contrastive loss without the need of negative samples, as shown in Table 8. The results   \n516 indicate that our method still achieves improvement, demonstrating its applicability even in methods   \nwithout negative samples. ", "page_idx": 13}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/0f3be38ff1a854419418e4947a0860ec1a4d7d11de826c0ea85827c317c9148e.jpg", "table_caption": ["Table 8: Performance improvement yielded by BGE in BYOL. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "518 A.2.3 Visualization of sample algorithm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "519 We visualize the relationship between external and in-task samples obtained by different sampling   \n520 algorithms under CIFAR and CPI compositions, as shown in Figure 4. When CIFAR10 as external   \n521 data, the distributions of random and OPO samples are similar, both covering the entire area effectively.   \n522 While in the CPI setting, random sampling fails to cover the entire area, in contrast, the OPO algorithm   \n523 achieves superior proximity and diversity, consequently leading to greater performance improvement.   \nThis observation corroborates our discussion about the sampling algorithm in Section 4.3. ", "page_idx": 14}, {"type": "image", "img_path": "UGKgoAZuuL/tmp/738707e3225ce356da479c0de691f20fba912c70ed12891483da0d1fb4c2bc6c.jpg", "img_caption": ["Figure 4: Comparison of external data sampled by different algorithms. When the entire external data quality is high (CIFAR), there is little difference between random and OPO sampling. When the data contains many OOD data (CPI), OPO outperforms random in sampling relevant and diverse samples. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "524 ", "page_idx": 14}, {"type": "text", "text": "525 A.2.4 Self-supervised learning feature characteristics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "526 Previous work [2] points out that self-supervised trained networks map inputs together according   \n527 to feature characteristics rather than according to labels as supervised trained networks tend to do.   \n528 Inspired by them, we validate that we adopted network also has such characteristics. Table 9 shows   \n529 the average number of one sample\u2019s $\\mathbf{k}\\cdot$ -nearest neighbors belonging to the class of this sample for   \n530 networks trained in the supervised or self-supervised manner. It is evident that supervised networks   \n531 consistently have more same-class neighbors, indicating that they cluster images based on labels. In   \n532 contrast, self-supervised networks are less influenced by image classes, which is advantageous for   \nincorporating external data. ", "page_idx": 14}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/1b97121aa7539df6af94fa0f9e6bc9b18b8fecf6a7a66412ded26f405130ccef.jpg", "table_caption": ["Table 9: Statistics on how many of the $\\mathbf{k}$ -nearest neighbors of a sample belong to the same class as this sample in self-supervised and supervised networks. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "533 ", "page_idx": 14}, {"type": "text", "text": "534 Table 10 presents the class statistics of the top 100 nearest neighbors of the \"willow tree\" class on the   \n535 CIFAR100 dataset, as learned by self-supervised and supervised networks. Self-supervised learning   \n536 results in a lower proportion of same-class neighbors, indicating less influence from class labels.   \n537 Additionally, the neighbors of other classes in the self-supervised network exhibit features more   \n538 similar to the \"willow tree\" class.   \n539 This insight suggests that external data, despite having different actual classes with in-task data,   \n540 can proxy for the in-task data in self-supervised learning due to shared features. Thus giving us   \n541 confidence that using external data in self-supervised learning as in BGE can yield good results and   \n542 justify our cosine distance based sampling algorithm. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "543 A.2.5 Fairness alignment", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "544 Introducing external data incurs additional iterations and new knowledge. To ensure fairness, we   \n545 train the base method PFR for more epochs and use pre-training with external data to initialize the   \n546 weights for in-task data training. Experimental results, as shown in Table 11, reveal that training ", "page_idx": 14}, {"type": "text", "text": "Table 10: The class name and average number of the top 5 classes with the highest number of the top 100 neighbors of the \"willow tree\" class. ", "page_idx": 15}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/069af1ef5215cc4b167af4a372c3cca0ae2399b8b41806d93f837428f470ad6c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "547 for more epochs and pre-training with external data do not lead to performance improvement. This highlights the effectiveness of BGE under fairer conditions. ", "page_idx": 15}, {"type": "text", "text": "Table 11: Comparison of the performance improvement of BGE and other factors to ensure fairness. ", "page_idx": 15}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/88807d69ec624775c6b3eda6c03ab0ec0936478238978ed90cb701b1e693c484.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "548 ", "page_idx": 15}, {"type": "text", "text": "549 A.2.6 Experiment statistical significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "550 Due to limited computational resources, we report the mean and standard deviation of three random   \n551 trials for only the primary experiments in Tables 12 and 13. The performance of the BGE on the three   \n552 base methods when using CIFAR and CPI as external dataset compositions under the CIFAR100   \n553 4 tasks and 10 tasks setting is shown in Table 12. Table 13 shows the performance of BGE using   \n554 different sampling algorithms with CPI as the external dataset, also in the CIFAR100 4 tasks and 10   \ntasks setting, across the same three baseline methods. ", "page_idx": 15}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/0ce5b3bfbf089ad33877fd90649b9c524465e7ac9322ccdb1c0ddabca9d5623b.jpg", "table_caption": ["Table 12: Results with multiple runs. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/e70542c1a9d733c48aac939ad128a99d2777db63cdec3425e5cc5bb11918700d.jpg", "table_caption": ["Table 13: Results with multiple runs. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "555 ", "page_idx": 15}, {"type": "text", "text": "556 A.2.7 Full experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "557 We present here the full set of experiments, encompassing various base methods, sampling bud  \n558 gets, sampling methods, and compositions of external datasets, demonstrating the performance   \n559 improvement of BGE on CIFAR100 (Table 14) and ImageNet100 (Table 15). ", "page_idx": 15}, {"type": "table", "img_path": "UGKgoAZuuL/tmp/7490845dc7ff9c80d04275c578aa6d0ab92b17c0a6120b452965f10f9470d2d2.jpg", "table_caption": ["Table 14: Full experiment results on CIFAR100 dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "560 A.3 Limitations and future directions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "561 There are still limitations to BGE, such as increased data volume for training, leading to additional   \n562 computational costs. For future directions, we believe BGE can inspire further research into continual   \n563 learning from the perspective of inter-task data relationships. Additionally, BGE\u2019s use of external   \n564 data instead of exemplars to compensate for inter-task comparisons enhances privacy preservation,   \n565 offering a pathway for future work to address privacy concerns associated with using exemplars. We   \n566 research methods to allow the network to learn continually, which have no negative impact on society,   \n567 and at the same time, we proposed method facilitates privacy protection and has a positive impact on   \n568 society. ", "page_idx": 16}, {"type": "text", "text": "569 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "77 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n78 made in the paper.   \n79 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n80 contributions made in the paper and important assumptions and limitations. A No or   \n81 NA answer to this question will not be perceived well by the reviewers.   \n82 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n83 much the results can be expected to generalize to other settings.   \n84 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n85 are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix A.3. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "1 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n2 the paper has limitations, but those are not discussed in the paper.   \n3 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n4 \u2022 The paper should point out any strong assumptions and how robust the results are to   \nviolations of these assumptions (e.g., independence assumptions, noiseless settings,   \n6 model well-specification, asymptotic approximations only holding locally). The authors   \n7 should reflect on how these assumptions might be violated in practice and what the   \n8 implications would be.   \n9 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n0 only tested on a few datasets or with a few runs. In general, empirical results often   \ndepend on implicit assumptions, which should be articulated.   \n2 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n3 For example, a facial recognition algorithm may perform poorly when image resolution   \n4 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n5 used reliably to provide closed captions for online lectures because it fails to handle   \n6 technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n8 and how they scale with dataset size.   \n9 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n0 address problems of privacy and fairness.   \n1 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n2 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n3 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n4 judgment and recognize that individual actions in favor of transparency play an impor  \n5 tant role in developing norms that preserve the integrity of the community. Reviewers   \n6 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "617 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "618 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n619 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "621 Justification: We do not include theoretical results.   \n622 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "633 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "634 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n635 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n636 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We realease our code to prove reproducibility. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "2 Question: Does the paper provide open access to the data and code, with sufficient instruc3 tions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "675 Answer: [Yes]   \n676 Justification: We release our code, and related information can be found at README.md in   \n677 our code supplemental material.   \n678 Guidelines:   \n679 \u2022 The answer NA means that paper does not include experiments requiring code.   \n680 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n681 public/guides/CodeSubmissionPolicy) for more details.   \n682 \u2022 While we encourage the release of code and data, we understand that this might not be   \n683 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n684 including code, unless this is central to the contribution (e.g., for a new open-source   \n685 benchmark).   \n686 \u2022 The instructions should contain the exact command and environment needed to run to   \n687 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n688 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n689 \u2022 The authors should provide instructions on data access and preparation, including how   \n690 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n691 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n692 proposed method and baselines. If only a subset of experiments are reproducible, they   \n693 should state which ones are omitted from the script and why.   \n694 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n695 versions (if applicable).   \n696 \u2022 Providing as much information as possible in supplemental material (appended to the   \n697 paper) is recommended, but including URLs to data and code is permitted.   \n698 6. Experimental Setting/Details   \n699 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n700 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n701 results?   \n702 Answer: [Yes]   \n703 Justification: We specify all the training and test details in section 4.1 and Appendix A.1.   \n704 Guidelines:   \n705 \u2022 The answer NA means that the paper does not include experiments.   \n706 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n707 that is necessary to appreciate the results and make sense of them.   \n708 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n709 material.   \n710 7. Experiment Statistical Significance   \n711 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n712 information about the statistical significance of the experiments?   \n713 Answer: [Yes]   \n714 Justification: we report error bars in Appendix A.2.6.   \n715 Guidelines:   \n716 \u2022 The answer NA means that the paper does not include experiments.   \n717 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n718 dence intervals, or statistical significance tests, at least for the experiments that support   \n719 the main claims of the paper.   \n720 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n721 example, train/test split, initialization, random drawing of some parameter, or overall   \n722 run with given experimental conditions).   \n723 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n724 call to a library function, bootstrap, etc.)   \n725 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n726 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n727 of the mean.   \n728 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n729 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n730 of Normality of errors is not verified.   \n731 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n732 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n733 error rates).   \n734 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n735 they were calculated and reference the corresponding figures or tables in the text.   \n736 8. Experiments Compute Resources   \n737 Question: For each experiment, does the paper provide sufficient information on the com  \n738 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n739 the experiments?   \n740 Answer: [Yes]   \n741 Justification: We provide sufficient information on the computer resources in Appendix A.1.   \n742 Guidelines:   \n743 \u2022 The answer NA means that the paper does not include experiments.   \n744 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n745 or cloud provider, including relevant memory and storage.   \n746 \u2022 The paper should provide the amount of compute required for each of the individual   \n747 experimental runs as well as estimate the total compute.   \n748 \u2022 The paper should disclose whether the full research project required more compute   \n749 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n750 didn\u2019t make it into the paper).   \n751 9. Code Of Ethics   \n752 Question: Does the research conducted in the paper conform, in every respect, with the   \n753 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n754 Answer: [Yes]   \n755 Justification: We have read the NeurIPS Code of Ethics, and conduct research with it.   \n756 Guidelines:   \n757 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n758 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n759 deviation from the Code of Ethics.   \n760 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n761 eration due to laws or regulations in their jurisdiction).   \n762 10. Broader Impacts   \n763 Question: Does the paper discuss both potential positive societal impacts and negative   \n764 societal impacts of the work performed?   \n765 Answer: [Yes]   \n766 Justification: We discuss the societal impacts in Appendix A.3.   \n767 Guidelines:   \n768 \u2022 The answer NA means that there is no societal impact of the work performed.   \n769 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n770 impact or why the paper does not address societal impact.   \n771 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n772 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n773 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n774 groups), privacy considerations, and security considerations.   \n775 \u2022 The conference expects that many papers will be foundational research and not tied   \n776 to particular applications, let alone deployments. However, if there is a direct path to   \n777 any negative applications, the authors should point it out. For example, it is legitimate   \n778 to point out that an improvement in the quality of generative models could be used to   \n779 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n780 that a generic algorithm for optimizing neural networks could enable people to train   \n781 models that generate Deepfakes faster.   \n782 \u2022 The authors should consider possible harms that could arise when the technology is   \n783 being used as intended and functioning correctly, harms that could arise when the   \n784 technology is being used as intended but gives incorrect results, and harms following   \n785 from (intentional or unintentional) misuse of the technology.   \n786 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n787 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n788 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n789 feedback over time, improving the efficiency and accessibility of ML).   \n790 11. Safeguards   \n791 Question: Does the paper describe safeguards that have been put in place for responsible   \n792 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n793 image generators, or scraped datasets)?   \n794 Answer: [NA]   \n795 Justification: The paper poses no such risks.   \n796 Guidelines:   \n797 \u2022 The answer NA means that the paper poses no such risks.   \n798 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n799 necessary safeguards to allow for controlled use of the model, for example by requiring   \n800 that users adhere to usage guidelines or restrictions to access the model or implementing   \n801 safety filters.   \n802 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n803 should describe how they avoided releasing unsafe images.   \n804 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n805 not require this, but we encourage authors to take this into account and make a best   \n806 faith effort.   \n807 12. Licenses for existing assets   \n808 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n809 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n810 properly respected?   \n811 Answer: [Yes]   \n812 Justification: All existing assets we use are cited in section 4   \n813 Guidelines:   \n814 \u2022 The answer NA means that the paper does not use existing assets.   \n815 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n816 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n817 URL.   \n818 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n819 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n820 service of that source should be provided.   \n821 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n822 package should be provided. For popular datasets, paperswithcode.com/datasets   \n823 has curated licenses for some datasets. Their licensing guide can help determine the   \n824 license of a dataset.   \n825 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n826 the derived asset (if it has changed) should be provided.   \n827 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n828 the asset\u2019s creators.   \n829 13. New Assets   \n830 Question: Are new assets introduced in the paper well documented and is the documentation   \n831 provided alongside the assets?   \n832 Answer: [NA]   \n833 Justification: The paper does not release new assets.   \n834 Guidelines:   \n835 \u2022 The answer NA means that the paper does not release new assets.   \n836 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n837 submissions via structured templates. This includes details about training, license,   \n838 limitations, etc.   \n839 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n840 asset is used.   \n841 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n842 create an anonymized URL or include an anonymized zip file.   \n843 14. Crowdsourcing and Research with Human Subjects   \n844 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n845 include the full text of instructions given to participants and screenshots, if applicable, as   \n846 well as details about compensation (if any)?   \n847 Answer: [NA]   \n848 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n849 Guidelines:   \n850 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n851 human subjects.   \n852 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n853 tion of the paper involves human subjects, then as much detail as possible should be   \n854 included in the main paper.   \n855 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n856 or other labor should be paid at least the minimum wage in the country of the data   \n857 collector.   \n858 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n859 Subjects   \n860 Question: Does the paper describe potential risks incurred by study participants, whether   \n861 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n862 approvals (or an equivalent approval/review based on the requirements of your country or   \n863 institution) were obtained?   \n864 Answer: [NA]   \n865 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n866 Guidelines:   \n867 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n868 human subjects.   \n869 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n870 may be required for any human subjects research. If you obtained IRB approval, you   \n871 should clearly state this in the paper.   \n872 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n873 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n874 guidelines for their institution.   \n875 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n876 applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]