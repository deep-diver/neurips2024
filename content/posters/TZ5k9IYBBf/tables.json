[{"figure_path": "TZ5k9IYBBf/tables/tables_1_1.jpg", "caption": "Table 1: (Left) Online Continual Learning. Performance comparison of RanDumb on the PEC setup [75] and VAE-GC [63]. Setup and numbers borrowed from PEC [75]. RanDumb outperforms the best OCL method. (Right) Offline Continual Learning. Performance comparison with ImageNet21K ViT-B16 model using 2 initial classes and 1 new class per task. RanPAC-imp is an improved version of the RanPAC code which mitigates the instability issues in RanPAC. RanDumb nearly matches performance of joint for both online and offline, demonstrating the inefficacy of current benchmarks.", "description": "This table presents a comparison of RanDumb's performance against other state-of-the-art online and offline continual learning methods.  The left side shows RanDumb's superiority in online learning scenarios, outperforming existing methods on various benchmarks. The right side demonstrates that RanDumb achieves near-identical performance to the joint training approach in offline learning, challenging the effectiveness of the continual representation learning techniques used in other methods.", "section": "1 Introduction"}, {"figure_path": "TZ5k9IYBBf/tables/tables_3_1.jpg", "caption": "Table 1: (Left) Online Continual Learning. Performance comparison of RanDumb on the PEC setup [75] and VAE-GC [63]. Setup and numbers borrowed from PEC [75]. RanDumb outperforms the best OCL method. (Right) Offline Continual Learning. Performance comparison with ImageNet21K ViT-B16 model using 2 initial classes and 1 new class per task. RanPAC-imp is an improved version of the RanPAC code which mitigates the instability issues in RanPAC. RanDumb nearly matches performance of joint for both online and offline, demonstrating the inefficacy of current benchmarks.", "description": "This table presents a comparison of RanDumb's performance against other state-of-the-art methods in both online and offline continual learning scenarios.  The left side shows RanDumb significantly outperforming existing online continual learning methods on the PEC benchmark. The right side demonstrates RanDumb achieving comparable performance to the best joint training methods in offline continual learning, even when using pre-trained models, thus challenging the effectiveness of current continual representation learning techniques.", "section": "1 Introduction"}, {"figure_path": "TZ5k9IYBBf/tables/tables_4_1.jpg", "caption": "Table 1: (Left) Online Continual Learning. Performance comparison of RanDumb on the PEC setup [75] and VAE-GC [63]. Setup and numbers borrowed from PEC [75]. RanDumb outperforms the best OCL method. (Right) Offline Continual Learning. Performance comparison with ImageNet21K ViT-B16 model using 2 initial classes and 1 new class per task. RanPAC-imp is an improved version of the RanPAC code which mitigates the instability issues in RanPAC. RanDumb nearly matches performance of joint for both online and offline, demonstrating the inefficacy of current benchmarks.", "description": "This table presents a comparison of RanDumb's performance against other state-of-the-art online and offline continual learning methods on various benchmark datasets.  The left side shows results for online learning, comparing RanDumb to VAE-GC and the best-performing method from the PEC benchmark. The right side shows results for offline continual learning, comparing RanDumb to RanPAC-imp and joint training (a non-continual learning approach).  The results demonstrate that RanDumb significantly outperforms existing methods in both online and offline settings, highlighting the sub-optimality of continually learned representations.", "section": "1 Introduction"}, {"figure_path": "TZ5k9IYBBf/tables/tables_4_2.jpg", "caption": "Table 3: Benchmark B.1 (Ref: Table adopted from OnPro [68], OCM[25]) We compare RanDumb in many-classes per task setting referred as \u2018Dataset (num_tasks/num_classes_per_task)\u2019. We categorize memory buffer sizes with \u2018M\u2019. RanDumb outperforms the competing approaches without heavy-augmentations by 3-20% margins despite being exemplar free. Only in one case, it is second best.", "description": "This table presents a comparison of RanDumb's performance against other methods on Benchmark B.1, which involves multiple classes per task.  The results show that RanDumb significantly outperforms most competing methods, especially those without extensive data augmentation, achieving improvements ranging from 3% to 20%. RanDumb only falls short of one method in a single instance.", "section": "3 Experiments"}, {"figure_path": "TZ5k9IYBBf/tables/tables_5_1.jpg", "caption": "Table 4: (Left) Benchmark B.2 (Ref: Table from OnPro [68]) We compare RanDumb with contrastive representation learning based approaches which additionally use sophisticated augmentations. We observe that RanDumb often outperforms these sophisticated methods despite all of these factors on small-exemplar settings. (Right) Benchmark C (Ref: Table 2 from [60]). We compare RanDumb with latest rehearsal-free methods. RanDumb outperforms them by 4% margin.", "description": "This table presents a comparison of RanDumb's performance against other continual learning methods on two benchmarks: B.2 and C. Benchmark B.2 compares RanDumb to methods that use contrastive representation learning and sophisticated augmentations, focusing on small-exemplar settings. Benchmark C compares RanDumb to rehearsal-free methods, demonstrating its superior performance by a 4% margin.", "section": "3 Experiments"}, {"figure_path": "TZ5k9IYBBf/tables/tables_6_1.jpg", "caption": "Table 3: Benchmark B.1 (Ref: Table adopted from OnPro [68], OCM[25]) We compare RanDumb in many-classes per task setting referred as \u2018Dataset (num_tasks/num_classes_per_task)\u2019. We categorize memory buffer sizes with \u2018M\u2019. RanDumb outperforms the competing approaches without heavy-augmentations by 3-20% margins despite being exemplar free. Only in one case, it is second best.", "description": "This table presents a comparison of RanDumb's performance against other methods on benchmark B.1, which involves multiple classes per task. RanDumb consistently outperforms most competing methods by a significant margin (3-20%), even without employing heavy augmentations or exemplar storage, showcasing its effectiveness in low-memory scenarios.", "section": "3 Experiments"}, {"figure_path": "TZ5k9IYBBf/tables/tables_7_1.jpg", "caption": "Table 6: (Left) Analysis of RanDumb: We study contributions of decorrelation, random embedding, and data augmentation. We further vary the embedding sizes and regularisation parameter. Finally, we compare with alternate embeddings. (Right) Architectures (Ref: Table 1 from Mirzadeh et al. [45]) RanDumb surpasses continual representation learning across a wide range of architectures, achieving close to 94% of the joint performance.", "description": "This table presents the ablation study of RanDumb's components (decorrelation, random embedding, data augmentation) and its performance variation with different embedding sizes and regularization parameters. It also compares RanDumb's performance with alternative embedding methods and shows RanDumb's superior performance across various architectures in continual representation learning, achieving nearly 94% of the performance of the joint model.", "section": "3.2 Analysis of RanDumb"}, {"figure_path": "TZ5k9IYBBf/tables/tables_8_1.jpg", "caption": "Table 7: Benchmark F We compare RanDumb with prompt-tuning approaches using ViT-B/16 ImageNet-21K/1K pretrained models using 2 init classes and 1 class per task setting. Most prompt-tuning based methods collapse and RanDumb achieves either state-of-the-art or second-best performance. RanPAC-imp is an improved version of the RanPAC mitigating the instability issues identified in a previous version of this work.", "description": "This table presents the results of comparing RanDumb's performance against various prompt-tuning approaches on benchmark F.  The benchmark uses pre-trained ViT-B/16 models with ImageNet-21K/1K weights, a setup of two initial classes followed by one class per task.  The results show that many prompt-tuning methods struggle, while RanDumb consistently achieves either state-of-the-art or very close to state-of-the-art results, highlighting its effectiveness even when compared to methods that fine-tune pre-trained models.  The inclusion of RanPAC-imp, an improved version of RanPAC, adds context to the performance comparisons.", "section": "3.3 Should we learn representations when strong pre-trained features are available?"}]