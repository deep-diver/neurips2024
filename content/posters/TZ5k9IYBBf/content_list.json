[{"type": "text", "text": "Random Representations Outperform Online Continually Learned Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ameya Prabhu1\u2217 Shiven Sinha2\u2217 Ponnurangam Kumaraguru2 Philip H.S. Torr1 Ozan Sener3+ Puneet K. Dokania1+ 1University of Oxford 2IIIT-Hyderabad 3Apple ", "page_idx": 0}, {"type": "text", "text": "https://github.com/drimpossible/RanDumb ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning has primarily focused on the issue of catastrophic forgetting and the associated stability-plasticity tradeoffs. However, little attention has been paid to the efficacy of continually learned representations, as representations are learned alongside classifiers throughout the learning process. Our primary contribution is empirically demonstrating that existing online continually trained deep networks produce inferior representations compared to a simple pre-defined random transforms. Our approach projects raw pixels using a fixed random transform, approximating an RBF-Kernel initialized before any data is seen. We then train a simple linear classifier on top without storing any exemplars, processing one sample at a time in an online continual learning setting. This method, called RanDumb, significantly outperforms state-of-the-art continually learned representations across all standard online continual learning benchmarks. Our study reveals the significant limitations of representation learning, particularly in low-exemplar and online continual learning scenarios. Extending our investigation to popular exemplar-free scenarios with pretrained models, we find that training only a linear classifier on top of pretrained representations surpasses most continual fine-tuning and prompt-tuning strategies. Overall, our investigation challenges the prevailing assumptions about effective representation learning in online continual learning. ", "page_idx": 0}, {"type": "image", "img_path": "TZ5k9IYBBf/tmp/57a3dbcd9ee7012833594542628911a75ad76ed1a397341c0611a5c190f8dfcf.jpg", "img_caption": ["Figure 1: Our primary analysis in this work is ablating the deep feature extractor (bottom center) by replacing it with a random projection (top center) to isolate the effect of online continual representation learning in the deep network. RanDumb (top) maps raw pixels to a high-dimensional space using random Fourier projections $\\left(\\varphi\\right)$ , then decorrelates the features using the Mahalanobis distance [43] and classifies based on the nearest class mean. We demonstrate that random projections not only match but consistently outperform continually learned representations, highlighting the low quality of the learned representations. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Continual learning aims to develop models capable of learning from non-stationary data streams, inspired by the lifelong learning abilities exhibited by humans and the prevalence of such real-world applications (see Verwimp et al. [64] for a survey). It is characterized by sequentially arriving tasks, coupled with additional computational and memory constraints [33, 38, 54, 62, 49]. ", "page_idx": 1}, {"type": "text", "text": "Building on the foundations of supervised deep learning, the prevalent approach in continual learning has been to jointly train representations alongside classifiers. This approach simply follows from the assumption that learned representations are expected to outperform fixed representation functions such as kernel classifiers, as demonstrated in supervised deep learning [34, 23, 57]. However, this assumption is never validated in continual learning, with scenarios having limited updates where networks might not be trained until convergence, such as online continual learning (OCL). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we study the efficacy of representations derived from continual learning algorithms. Surprisingly, our findings suggest that these representations might not be as beneficial as presumed. To test this, we introduce a simple baseline method named RanDumb, which combines a random representation function with a straightforward linear classifier, illustrated in detail in Figure 1. Our empirical evaluations, summarized in Table 1 (left, top), reveal that despite replacing the representation learning with a pre-defined random representation, RanDumb surpasses current stateof-the-art methods in latest online continual learning benchmarks [75]. ", "page_idx": 1}, {"type": "text", "text": "We further expand our evaluations to scenarios incorporating methods that use pre-trained feature extractors [67]. By substituting our random projections with these feature extractors and retaining the linear classifier, RanDumb again outperforms leading methods as shown in Table 1 (right, top). ", "page_idx": 1}, {"type": "text", "text": "1.1 Technical Summary: Construction of RanDumb and Empirical Findings ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Design. RanDumb first projects input pixels into a high-dimensional space using a fixed kernel based on random Fourier basis, which is a low-rank data-independent approximation of the RBF Kernel [52]. Then, we use a simple linear classifier which first normalizes distances across different feature dimensions (anisotropy) with Mahalanobis distance [43] and then uses nearest class means for classification [44]. In scenarios with pretrained feature extractors, we use the fixed pretrained model as embedder and learn a linear classifier as described above, similar to Hayes and Kanan [27]. ", "page_idx": 1}, {"type": "text", "text": "Key Properties. RanDumb needs no storage of exemplars and requires only one pass over the data in a one-sample-per-timestep fashion. Furthermore, it only requires online estimation of the sample covariance matrix and nearest class mean. ", "page_idx": 1}, {"type": "text", "text": "Key Finding 1: Poor Representation Learning. We compare RanDumb with leading methods: VAEGC [63] in Table 1 (left, middle) and SLCA [78] in Table 1 (right, middle). The primary distinction ", "page_idx": 1}, {"type": "text", "text": "Table 1: (Left) Online Continual Learning. Performance comparison of RanDumb on the PEC setup [75] and VAE-GC [63]. Setup and numbers borrowed from PEC [75]. RanDumb outperforms the best OCL method. (Right) Offline Continual Learning. Performance comparison with ImageNet21K ViT-B16 model using 2 initial classes and 1 new class per task. RanPAC-imp is an improved version of the RanPAC code which mitigates the instability issues in RanPAC. RanDumb nearly matches performance of joint for both online and offline, demonstrating the inefficacy of current benchmarks. ", "page_idx": 1}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/049b582fe78b5c3e99f6bed012c10d569320a11119e8af030eff75db78dfc52b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "TZ5k9IYBBf/tmp/d1f8f62e90a5ae37883e9fb5e6d3d79a6a60e55127c9842d9ab7754896709bd0.jpg", "img_caption": ["Figure 2: RanDumb projects the datapoints to a high-dimensional space to create a clearer separation between classes. Subsequently, it corrects the anisotropy across feature dimensions, scaling them to be unit variance each. This allows cosine similarity to accurately separates classes. The figure is adapted from [48]. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "between them is their representation: RanDumb uses a fixed function (random/pretrained network), whereas VAE-GC and SLCA further continually trained deep networks. RanDumb consistently surpasses VAE-GC and SLCA by wide margins of $5.15\\%$ . This shows that state-of-the-art online continual learning algorithms fail to learn effective representations across standard exemplar-free continual learning benchmarks. ", "page_idx": 2}, {"type": "text", "text": "Finding 2: Over-Constrained Benchmarks. Given the demonstrated limitations of existing continual representation learning methods, an important question arises: Can better methods learn more effective representations? To explore this, we evaluated the performance of RanDumb against joint training, models trained without continual learning constraints, in both online and offline settings, as shown in Table 1 (left, bottom) and Table 1 (right, bottom). Our straightforward baseline, RanDumb, bridges $70\u201390\\%$ of the performance gap relative to the respective joint classifiers in both scenarios. This significant recovery of performance by such a simple method suggests that if our goal is to advance the study of representation learning, current benchmarks may be overly restrictive and not conducive to truly effective representation learning. ", "page_idx": 2}, {"type": "text", "text": "We highlight that the goal in our work is not to introduce a state-of-the-art continual learning method, but challenge prevailing assumptions and open a discussion on the efficacy of representation learning in continual learning algorithms, especially in online and low-exemplar scenarios. ", "page_idx": 2}, {"type": "text", "text": "2 RanDumb: Mechanism & Intuitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "RanDumb has two main elements: random projection and the dumb learner. We illustrate the mechanism of RanDumb using three toy examples in Figure 2. To classify a test sample $\\mathbf{x}_{\\mathrm{test}}$ , we start with a simple classifier, the nearest class mean (NCM). It predicts the class among $C$ classes by highest value of the similarity function $f$ among class means $\\mu_{i}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{\\mathrm{pred}}=\\operatorname*{arg\\,max}_{i\\in\\{1,\\ldots,|C|\\}}f\\big(\\mathbf{x}_{\\mathrm{test}},\\mu_{i}\\big),\\quad\\mathrm{where}\\quad f\\big(\\mathbf{x}_{\\mathrm{test}},\\mu_{i}\\big):=\\mathbf{x}_{\\mathrm{test}}^{\\mathbf{\\mu}}^{\\top}\\mu_{i}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $\\mu_{i}$ are the class-means in the pixel space: $\\begin{array}{r}{\\mu_{i}=\\frac{1}{|C_{i}|}\\sum_{\\mathbf{x}\\in C_{i}}\\mathbf{x}}\\end{array}$ . RanDumb adds two additional components to this classifier: 1) Kernelization and 2) Decorrelation. ", "page_idx": 2}, {"type": "text", "text": "Kernelization: Classes are typically not linearly separable in the pixel space, unlike in the feature space of deep models. Hence, we randomly project the pixels into a high-dimensional representation space, computing all distances between the data and class-means in this embedding space. This phenomena is illustrated on three toy examples to build intuitions in Figure 2 (Embed). We use an RBF-Kernel, which for two points $\\mathbf{x}$ xand $\\mathbf{y}$ yis defined as: $K_{\\mathrm{RBF}}(\\mathbf{x},\\mathbf{y})=\\exp\\left(-\\gamma\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\\right)$ where $\\gamma$ is a scaling parameter. However, calculating the RBF kernel is not possible due to the online continual learning constraints preventing computation of pairwise-distance between all points. We use a data-independent approximation, random Fourier projection $\\phi(\\mathbf{x})$ , as given in [52]: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nK_{\\mathrm{RBF}}(\\mathbf{x},\\mathbf{y})\\approx\\phi(\\mathbf{x})^{T}\\phi(\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the random Fourier features $\\phi(\\mathbf{x})$ are defined by first sampling $D$ Dvectors $\\{\\omega_{1},\\hdots,\\omega_{D}\\}$ {\\omea }_1, \\ldots , \\mathb {\\omea }_D\\} gfgfrom a Gaussian distribution with mean zero and covariance matrix $2\\gamma\\mathbf{I}$ ,I where $\\mathbf{I}$ Iis the identity matrix. Then $\\phi(\\mathbf{x})$ is a $2D$ -dimensional feature, defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{x})=\\frac{1}{\\sqrt{D}}\\left[\\cos(\\omega_{1}^{T}\\mathbf{x}),\\sin(\\omega_{1}^{T}\\mathbf{x}),..,\\cos(\\omega_{D}^{T}\\mathbf{x}),\\sin(\\omega_{D}^{T}\\mathbf{x})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We keep these $\\omega$ bases fixed throughout online learning. Thus, we obtain our modified similarity function from Equation 1 as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{\\mathrm{test}},\\mu_{i}):=\\phi(\\mathbf{x}_{\\mathrm{test}})^{\\top}\\bar{\\mu}_{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bar{\\mu}_{i}}$ are the class-means in the kernel space: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mu}_{i}=\\frac{1}{|C_{i}|}\\sum_{\\mathbf{x}\\in C_{i}}\\phi(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Decorrelation: Projected raw pixels have feature dimensions with different variances (anisotropic). Hence, instead of naively computing $\\phi(\\mathbf{x}_{\\mathrm{test}})}^{\\top}\\bar{\\mu}_{i}$ , we further decorrelate the feature dimensions using a Mahalonobis distance with the shrinked covariance matrix S using OAS shrinkage [15], inverse obtained by least squares minimization $(\\mathbf{S}+\\lambda\\mathbf{I})$ . We illustrate this phenomena as well on three toy examples in Figure 2 (Decorrelate) to build intuitions. Our similarity function finally is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{\\mathrm{test}},\\mu_{i}):=(\\phi(\\mathbf{x}_{\\mathrm{test}})-\\bar{\\mu}_{i})^{T}\\mathbf{S}^{-1}(\\phi(\\mathbf{x}_{\\mathrm{test}})-\\bar{\\mu}_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Online Computation. Our random projection is fixed before seeing any data. During continual learning, we only perform online update on the running class mean and empirical covariance matrix2. ", "page_idx": 3}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We compare RanDumb with algorithms across online continual learning benchmarks with an emphasis on exemplar-free and low-exemplar storage regime. All numbers in tables with the caption (Ref: table and citation) except our method are taken from the aforementioned table in the cited paper. ", "page_idx": 3}, {"type": "text", "text": "Benchmarks. The benchmarks which we used in our experiments are summized in Table on the right. We aim for a comprehensive coverage and show results on four standard online continual learning benchmarks (A, B, D, E) which reflect the latest trends (\u201922- \u201924) across exemplar-free, contrastivetraining3, meta-continual learning, and network-expansion based approaches re", "page_idx": 3}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/44ecb279c48e0fd44b8038cbf3137fc81c1b0d44922013d0d90883a6f93dd28c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "spectively. We also evaluate on a rehearsal-free offline continual learning benchmark C. These benchmarks are ordered by increasingly relaxed constraints, moving further away from the training scenario of RanDumb.Benchmark A closely matches RanDumb with one class per timestep and no stored exemplars. Benchmark B, D, E progressively relax the constraints on exemplars and classes per timestep. Benchmark C and E remove the online constraint by allowing unrestricted training and sample access within a task without exemplar-storage of past tasks. Benchmark F allows using large pretrained models, modified by us with one class per task, i.e. testing learning over longer timespans. ", "page_idx": 3}, {"type": "text", "text": "We further test on exemplar-free scenarios in offline continual learning using Benchmark F [67] with the challenging one-class per task constraint borrowed from [75]. This benchmark allows ", "page_idx": 3}, {"type": "text", "text": "Table 2: Benchmark A (Ref: Table 1 from PEC [75]). We compare RanDumb in a 1-class per task setting referred as \u2018Dataset (num_tasks/1)\u2019. We observe that RanDumb outperforms all approaches across all datasets by $2{-}6\\%$ margins, with an exception of latest work PEC [75] on CIFAR10. ", "page_idx": 4}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/60c945ff5417e349a8b0801b58edd645ea5537b2c3edd865130dd7c58f7b834a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/57b402b2d7354aea51afc45f77c3c5266db1815f713ee150a186fe4a9028c72b.jpg", "table_caption": ["Table 3: Benchmark B.1 (Ref: Table adopted from OnPro [68], OCM[25]) We compare RanDumb in many-classes per task setting referred as \u2018Dataset (num_tasks/num_classes_per_task)\u2019. We categorize memory buffer sizes with $\\mathbf{\\omega}^{\\star}\\mathbf{M}^{\\star}$ . RanDumb outperforms the competing approaches without heavyaugmentations by $3{-}20\\%$ margins despite being exemplar free. Only in one case, it is second best. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "using pretrained models along with unrestricted training time and access to all class samples at each timestep. However, RanDumb is restricted to learning from a single pass seeing only one sample at a time. RanDumb only learns a linear classifier over a given pretrained model in Benchmark F. ", "page_idx": 4}, {"type": "text", "text": "We use LAMDA-PILOT [61] codebase for all methods, except RanPAC and SLDA for which use their codebases. We use the original hyperparameters. We only change initial classes to 2 and number of classes per task to 1 and test using both ImageNet21K and ImageNet1K ViT-B/16 models. ", "page_idx": 4}, {"type": "text", "text": "Implementation Details (RanDumb). We evaluate RanDumb using five datasets: MNIST, CIFAR10, CIFAR100, TinyImageNet200, and miniImageNet100. For the latter two, we downscale all images to ", "page_idx": 4}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/b26a0e998f8848b1c5faccae9cccbb440c5f622f4220c4eaefdf6059800b0d87.jpg", "table_caption": ["Table 4: (Left) Benchmark B.2 (Ref: Table from OnPro [68]) We compare RanDumb with contrastive representation learning based approaches which additionally use sophisticated augmentations. We observe that RanDumb often outperforms these sophisticated methods despite all of these factors on small-exemplar settings. (Right) Benchmark C (Ref: Table 2 from [60]). We compare RanDumb with latest rehearsal-free methods. RanDumb outperforms them by $4\\%$ margin. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "$32\\mathtt{x}32$ . We augment each datapoint with filpped version, hence two images are seen by the classifier at each timestep (except for MNIST and Benchmark F). We normalize all images and flatten them into vectors, obtaining 784-dim input vectors for MNIST and 3072-dim input vectors for all the other. For Benchmark F, we compare RanDumb on seven datasets used in LAMDA-PILOT, replacing ObjectNet with Stanford Cars as ObjectNet license prohibits training models. We use the 768-dimensional features from the same pretrained ViT-B models used in this benchmark. We measure accuracy on the test set of all past seen classes after completing the full one-pass. We take the average accuracy after the last task on all past tasks [75, 25, 67]. In Benchmark A and F, since we have one class per task, the average accuracy across past tasks is the same regardless of the task ordering. In Benchmarks A-E, all datasets have the same number of samples, hence similarly the average accuracy across past tasks is the same regardless of the task ordering. We used the Scikit-Learn implementation of Random Fourier Features [52] with 25K embedding size, $\\gamma=1.0$ . We use progressively increasing ridge regression parameter $(\\lambda)$ with dataset complexity, $\\lambda=10^{-6}$ for MNIST, $\\bar{\\lambda}=10^{-5}$ for CIFAR10/100 and $\\lambda=1\\dot{0}^{-4}$ for TinyImageNet200/miniImageNet100. ", "page_idx": 5}, {"type": "text", "text": "3.1 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Benchmark A (single-class per task). We assess continual learning models in the challenging setup of one class per timestep, closely mirroring our training assumptions, and present our results in Table 2. Comparing across rows, and see that RanDumb improves over prior state-of-the-art across all datasets with $2{-}6\\%$ margins. The only exception is PEC on CIFAR10, where RanDumb underperforms by $3.3\\%$ . Nonetheless, it outperforms the second-best model, GDumb with a 500 memory size, by $4.9\\%$ . ", "page_idx": 5}, {"type": "text", "text": "Benchmark B.1 (many-classes per task). We present our results comparing with non-contrastive methods in Table 3. We notice that scenario allows two classes per task and relaxes the memory constraints for online continual learning methods, allowing for higher accuracies compared to Benchmark A. Despite that, RanDumb outperforms latest OCL algorithms on MNIST, CIFAR10 and CIFAR100\u2014often by margins exceeding $10\\%$ . The lone exception is GDumb achieving a higher performance with 2K memory samples on TinyImageNet, indicating that this already is in the high-memory regime. ", "page_idx": 5}, {"type": "text", "text": "Benchmark B.2 (many-classes per task, with contrastive losses and data augmentations). We additionally compare our performance with the latest OCL approaches using contrastive losses with sophisticated data augmentations. As shown in in Table 4 (Left), these advancements provide large performance improvements over methods from Benchmark B.1. To compensate, we compare on lower exemplar budgets. The best approach, OnPro [68], outperforms RanDumb on CIFAR10 by $2.2\\%$ and TinyImageNet by $0.3\\%$ , but falls significantly short on CIFAR100 by $5.9\\%$ . Overall, RanDumb achieves strong results compared to representation learning using state-of-the-art contrastive learning approaches customized to continual learning, despite storing no exemplars. ", "page_idx": 5}, {"type": "text", "text": "Benchmark C (rehearsal-free). We compare against offline rehearsal-free continual learning approaches in Table 4 (Right) on CIFAR100. Despite online training, RanDumb outperforms PredKD by over $4\\%$ margins. ", "page_idx": 5}, {"type": "text", "text": "Table 5: (Left) Benchmark D (Ref: Table 2 from VR-MCL [70]) We compare RanDumb with metacontinual learning approaches operating in a high memory setting, allowing buffer sizes up to 1K exemplars. RanDumb outperforms all methods except VR-MCL on TinyImageNet. RanDumb also surpasses all prior work by a substantial $9.1\\%$ on CIFAR100. Allowing generous replay buffers shifts scenarios to a high exemplar regime where GDumb performs the best on CIFAR10. Yet RanDumb competes favorably even under these conditions. (Right) Benchmark E (Ref: Table 1 from SEDEM $I74J)$ We compare RanDumb with network expansion based approaches. Despite allowing access to much larger memory buffers, RanDumb matches the performance of best method SEDEM on MNIST, while exceeding it by $0.3\\%$ on CIFAR10 and $3.8\\%$ on CIFAR100. ", "page_idx": 6}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/8c298b50143671e9336d08f04e05742ca9f70695c5769fc3d9a90c3556ffc5bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Benchmark D (meta-continual learning). We compare performance of RanDumb against metacontinual learning methods, which require large exemplars with buffer sizes of 1K in Table 5 (left). RanDumb achieves strong performance under these conditions, exceeding all prior work by a large margin of $9.1\\%$ on CIFAR100 and outperforms all but VR-MCL approach on the TinyImageNet dataset. GDumb performs the best on CIFAR10, indicating this is already in a large-exemplar regime uniquely unsuited for RanDumb. ", "page_idx": 6}, {"type": "text", "text": "Benchmark E (network-expansion). We compare RanDumb against network expansion-based online continual learning methods in Table 5 (right). These approaches grow model capacity to mitigate forgetting while dealing with shifts in the data distribution, and are allowed larger memory buffers. RanDumb matches the performance of the state-of-the-art method SEDEM [74] on MNIST, while exceeding it by $0.3\\%$ on CIFAR10 and $3.8\\%$ on CIFAR100. ", "page_idx": 6}, {"type": "text", "text": "3.2 Analysis of RanDumb ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Ablating Components of RanDumb. We ablate the contribution of only using Random Fourier features for embedding and decorrelation to the overall performance of RanDumb in Table 6 (left, top). Ablating the decorrelation and relying solely on random Fourier features, colloquially dubbed Kernel-NCM, has performance drops ranging from $6.25\\%$ across the datasets. Replacing random Fourier features with raw features, ie. the SLDA baseline, leads to pronounced drop in performance ranging from $3{-}14\\%$ across the datasets. Moreover, ablating both components results in the base nearest class mean classifier, and exhibits the poorest performance with an average reduction of $17\\%$ . Therefore, both decorrelation and random embedding are crucial for RanDumb. ", "page_idx": 6}, {"type": "text", "text": "Impact of Embedding Dimensions. We vary the dimensions of the random Fourier features ranging from compressing 3K input dimensions to 1K to projecting it to 25K dimensions and evaluate its impact on performance in Figure 3. Surprisingly, the random projection to a 3x compressed 1K dimensional space allows for significant performance improvement over not using embedding, given in Table 6 (left, top). Furthermore, increasing the dimension from 1K to 25K results in improvements of $3.6\\%$ , $10.4\\%$ , $7.0\\%$ , and $2.5\\%$ on MNIST, CIFAR10, CIFAR100, and TinyImageNet respectively. Increasing the embedding sizes beyond 15K, however, only results in modest improvements of $0.1\\%$ , $1.4\\%$ , $1.1\\%$ and $0.2\\%$ on the same datasets, indicating 15K dimensions would be a good point for a performance-computational cost tradeoff. ", "page_idx": 6}, {"type": "image", "img_path": "TZ5k9IYBBf/tmp/a88e15481288a0bc89cb12c81b123ff98a1080168c07e7a5a970a7d6c2f63a50.jpg", "img_caption": ["Figure 3: Accuracy of RanDumb with respect to embedding dimensionality across datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/003db6a6689d023a3449e1329606ffd7964db39d1b5665f3c7941c0803dfe1da.jpg", "table_caption": ["Table 6: (Left) Analysis of RanDumb: We study contributions of decorrelation, random embedding, and data augmentation. We further vary the embedding sizes and regularisation parameter. Finally, we compare with alternate embeddings. (Right) Architectures (Ref: Table 1 from Mirzadeh et al. $[45J)$ RanDumb surpasses continual representation learning across a wide range of architectures, achieving close to $94\\%$ of the joint performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Impact of Flip Augmentation. We evaluate the impact of adding the filp augmentation on the performance of RanDumb in Table 6 (left, middle). Note that MNIST was not augmented. Augmentation provided large gains of $3.1\\%$ on CIFAR10, $1.7\\%$ on CIFAR100, and $0.4\\%$ on TinyImageNet. We did not augment the data further with RandomCrop transform as done with standard augmentations. ", "page_idx": 7}, {"type": "text", "text": "Impact of Varying Ridge Parameter. All prior experiments use a ridge parameter $(\\lambda)$ that increases with dataset complexity: $\\lambda=10^{-6}$ for MNIST, $10^{-5}$ for CIFAR10 and CIFAR100, and $10^{-4}$ for TinyImageNet and miniImageNet. Table 6 (left, middle) shows the effect of varying $\\lambda$ on RanDumb\u2019s performance. With a smaller $\\lambda=10^{-6}$ , CIFAR10, CIFAR100, TinyImageNet and miniImageNet all exhibit minor drops of $0.1\\%{-1.7\\%}$ , $0.8\\%$ , $0.8\\%$ . Increasing shrinkage to a $\\bar{\\lambda}=10^{-4}$ reduces CIFAR10 and CIFAR100 performance more substantially by $3\\%$ and $2.5\\%$ versus their optimal $\\lambda=10^{-5}$ . On the other hand, this larger $\\lambda$ leads to improvements of $0.5\\%$ and $1.8\\%$ on TinyImageNet and miniImageNet. This aligns with the trend that datasets with greater complexity benefit from more regularisation, with the optimal $\\lambda$ balancing under- and over-regularisation effects. ", "page_idx": 7}, {"type": "text", "text": "Comparison with Extreme Learning Machines. We compared our random Fourier features with random projections based extreme learning machines, as recently adapted to continual learning by $\\mathrm{RP+}$ ReLU [41] in Table 6 (left, bottom) with their best embedding size. Our method performs significantly better on each dataset, averaging a gain of $3.4\\%$ . ", "page_idx": 7}, {"type": "text", "text": "Comparisons across Architectures. In table 6 (right), we compare whether using random Fourier features as embeddings outperforms models across various architectures for continual representation learning. We use experience replay (ER) baseline in the task-incremental CIFAR100 setup ", "page_idx": 7}, {"type": "image", "img_path": "TZ5k9IYBBf/tmp/7ee426bc2de556bb440d94be6e38b20faa531e842ab8288c56df584a176f6e68.jpg", "img_caption": ["Figure 4: In previous experiments, models were trained from scratch, so we used random projection. Here, with a pretrained backbone, RanDumb starts with the frozen pretrained backbone to explore whether continual representation learning is necessary. By comparing this frozen backbone (RanDumb) with a continually trained one, we show that using the pretrained features consistently matches the best continually learned representations, similarly challenging the value of continual representation learning. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "(for details, see Mirzadeh et al. [45] as it differs significantly from earlier setups). Our comparison spanned various architectures. The findings revealed that RanDumb surpassed the performance of nearly all considered architectures, and achieved close to $94\\%$ of the joint multitask performance. This suggests that RanDumb outperforms continual representation learning across architectures. ", "page_idx": 8}, {"type": "text", "text": "Conclusion. Overall, both random embedding and decorrelation are critical components in the performance of RanDumb. Using random Fourier features is substantially better than RanPAC. Lastly, one can substantially reduce the embedding dimension without a large drop in performance for large gains in computational cost, additional augmentation may further significantly help performance and optimal shrinkage parameter increases with dataset complexity. RanDumb outperforms continual representation learning across a wide range of architectures. ", "page_idx": 8}, {"type": "text", "text": "3.3 Should we learn representations when strong pre-trained features are available? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Say for a specific application (e.g., where the test data distribution is more or less known during training), practitioners should use strong pretrained models as a starting point as they are likely to perform better. However, we still ask the key question of whether representation learning is necessary by fixing the pretrained backbone and only training the linear classifier, as illustrated in Figure 4 in the next benchmark. ", "page_idx": 8}, {"type": "text", "text": "Benchmark F. We compare performance of approaches which do not further train the deep network like RanDumb against popular contin", "page_idx": 8}, {"type": "text", "text": "Table 7: Benchmark F We compare RanDumb with prompt-tuning approaches using ViT-B/16 ImageNet-21K/1K pretrained models using 2 init classes and 1 class per task setting. Most prompttuning based methods collapse and RanDumb achieves either state-of-the-art or second-best performance. RanPAC-imp is an improved version of the RanPAC mitigating the instability issues identified in a previous version of this work. ", "page_idx": 8}, {"type": "table", "img_path": "TZ5k9IYBBf/tmp/bafcefa0f7bd4912af89f55b5452d5bc44d4bba84a8b6abaa104f2026dfa658d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ual finetuning and prompt-tuning approaches in Table 7. We discover that prompt-tuning approaches completely collapse under large timesteps and approaches which do not finetune their pretrained model achieve strong performance, even under challenging one class per timestep constraint. Note that RanPAC [41] adds a RP+ReLU and finetunes in a first-session adaptation fashion over RanDumb, yet fails to achieve higher accuracies. ", "page_idx": 9}, {"type": "text", "text": "Overall, despite RanDumb being exemplar-free, it outperforms nearly all online continual learning methods across various tasks when exemplar storage is limited. We specifically benchmark on lower exemplar sizes to complement settings in which GDumb does not perform well. ", "page_idx": 9}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Random Representations. There have been extensive theoretical and empirical investigations into random representations in machine learning, compressed sensing, and other fields, often utilizing extreme learning machines [56, 14, 21] (see [30, 29] for a survey). Other investigations include efficient kernel methods using Fourier features and Nystr\u00f6m approximations [52, 69], and extensions to efficiently parameterize linear classifiers [2]. They are also embedded into deep networks [17, 35, 72, 16]. We tailored the already successful random fourier representations [52] to the problem at hand and applied to the online continual learning problem for the first time. ", "page_idx": 9}, {"type": "text", "text": "Continual Representation Learning. There are various works focusing on continual representation learning itself [53, 20, 39, 28], but they address the problem of alleviating the stability-plasticity dilemma in high-exemplar and offline continual learning scenarios where models are trained until convergence. In comparison, we focus on online and low-examplar regime. ", "page_idx": 9}, {"type": "text", "text": "Representation Learning Free Methods in CL. Several works have developed the idea of using fixed pretrained networks after adapting on the first task across various settings [50, 41, 24]. Our work contributes to this growing evidence, however, we do not perform first-task adaptation [47], and propose OAS-shrinked SLDA as structurally simplest but highly accurate continual linear classifier without any extra bells-and-whistles. Moreover, we are the first work to introduce a representation learning free method with random features for continually learning from scratch. ", "page_idx": 9}, {"type": "text", "text": "Equivalent formulations to RanDumb. If the classes are equiprobable, which is the case for most datasets here, nearest class mean classifier with the Mahalanobis distance metric is equivalent to linear discriminant analysis (LDA) classifier [42]. Hence, one could say RanDumb is exactly equivalent to a Streaming LDA classifier with an approximate RBF Kernel. Alternatively, one could think of the decorrelation operation as explicitly decorrelating the features with ZCA whitening [7]. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our investigation reveals a surprising result \u2014 simply using random embedding (RanDumb) consistently outperforms learned representations from methods specifically designed for online continual training. Furthermore, using random/pretrained features also recovers $70\u201390\\%$ of the gap to joint learning, leaving limited room for improvement in representation learning techniques on standard benchmarks. Overall, our investigation questions our understanding of how to effectively design and train models that require efficient continual representation learning, and necessitates a re-investigation of the widely explored problem formulation itself. We believe adoption of computationally bounded scenarios without memory constraints and corresponding benchmarks [51, 50, 22] could be a promising way forward. ", "page_idx": 9}, {"type": "text", "text": "Limitations & Future Directions. We currently do not provide theory or justification for why training dynamics of continual learning algorithms fails to effectively learn good representations; doing so would provide deeper insights into continual learning algorithms. Moreover, our proposed method, RanDumb with random Fourier features is limited in scope towards low-exemplar scenarios and online-continual learning. Extending studies on representation learning to high-exemplar and offline continual learning scenarios might be exciting directions to investigate. ", "page_idx": 9}, {"type": "text", "text": "Social Impact. RanDumb is an algorithm solely designed to perform a scientific study and we do not recommend use of RanDumb for any application in real-world production systems, hence no direct societal impact or explicit limitations on use in production systems is discussed. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "AP is funded by Meta AI Grant No. DFR05540. PT thanks the Royal Academy of Engineering. PT and PD thank FiveAI for their support. This work is supported in part by a UKRI grant: Turing AI Fellowship EP/W002981/1 and an EPSRC/MURI grant: EP/N019474/1. The authors would like to thank Arvindh Arun, Kalyan Ramakrishnan and Shashwat Goel for helpful feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In ICCV, 2021.   \n[2] Nir Ailon and Bernard Chazelle. The fast johnson\u2013lindenstrauss transform and approximate nearest neighbors. SIAM Journal on computing, 2009.   \n[3] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. In NeurIPS, 2019.   \n[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In NeurIPS, 2019.   \n[5] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. In ICLR, 2022.   \n[6] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. In CVPR, 2021.   \n[7] Anthony Bell and Terrence J Sejnowski. Edges are the\u2019independent components\u2019 of natural scenes. In NeurIPS, 1996.   \n[8] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Classincremental continual learning into the extended der-verse. TPAMI, 2022.   \n[9] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020.   \n[10] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky. New insights on reducing abrupt representation change in online continual learning. In ICLR, 2022.   \n[11] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In ICCV, 2021.   \n[12] Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In ICLR, 2019.   \n[13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. Continual learning with tiny episodic memories. In ICML-W, 2019.   \n[14] CL Philip Chen. A rapid supervised learning neural network for function interpolation and approximation. IEEE Transactions on Neural Networks, 1996.   \n[15] Yilun Chen, Ami Wiesel, Yonina C Eldar, and Alfred O Hero. Shrinkage algorithms for mmse covariance estimation. IEEE transactions on signal processing, 2010.   \n[16] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In ICCV, 2015.   \n[17] Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. NeurIPS, 2009.   \n[18] Aristotelis Chrysakis and Marie-Francine Moens. Online bias correction for task-free continual learning. In ICLR, 2023.   \n[19] Matthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from nonstationary data streams. In ICCV, 2021.   \n[20] Enrico Fini, Victor G Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[21] Dmitriy Fradkin and David Madigan. Experiments with random projections for machine learning. In KDD, 2003.   \n[22] Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, and Fartash Faghri. Tic-clip: Continual training of clip models. ArXiv, 2023.   \n[23] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.   \n[24] Dipam Goswami, Yuyang Liu, Bart\u0142omiej Twardowski, and Joost van de Weijer. Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning. NeurIPS, 2023.   \n[25] Yiduo Guo, Bing Liu, and Dongyan Zhao. Online continual learning through mutual information maximization. In ICML, 2022.   \n[26] Gunshi Gupta, Karmesh Yadav, and Liam Paull. Look-ahead meta learning for continual learning. NeurIPS, 2020.   \n[27] Tyler L Hayes and Christopher Kanan. Lifelong machine learning with deep streaming linear discriminant analysis. In CVPR-W, 2020.   \n[28] Timm Hess, Eli Verwimp, Gido M van de Ven, and Tinne Tuytelaars. Knowledge accumulation in continually learned representations and the issue of feature forgetting. arXiv preprint arXiv:2304.00933, 2023.   \n[29] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: theory and applications. Neurocomputing, 2006.   \n[30] Guang-Bin Huang, Dian Hui Wang, and Yuan Lan. Extreme learning machines: a survey. International journal of machine learning and cybernetics, 2011.   \n[31] Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny. A simple baseline that questions the use of pretrained-models in continual learning. In NeurIPS-W, 2022.   \n[32] Xisen Jin, Junyi Du, and Xiang Ren. Gradient based memory editing for task-free continual learning. In NeurIPS, 2021.   \n[33] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 2017.   \n[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 2017.   \n[35] Quoc Le, Tam\u00e1s Sarl\u00f3s, Alex Smola, et al. Fastfood-approximating kernel expansions in loglinear time. In ICML, 2013.   \n[36] Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model for task-free continual learning. In ICLR, 2020.   \n[37] Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 2017.   \n[38] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, 2017.   \n[39] Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Representational continuity for unsupervised continual learning. arXiv preprint arXiv:2110.06976, 2021.   \n[40] Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner. Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning. In CVPR, 2021.   \n[41] Mark D McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, and Anton van den Hengel. Ranpac: Random projections and pre-trained models for continual learning. In NeurIPS, 2023.   \n[42] Geoffrey J McLachlan. Discriminant analysis and statistical pattern recognition. John Wiley & Sons, 2005.   \n[43] Goeffrey J McLachlan. Mahalanobis distance. Resonance, 4(6):20\u201326, 1999.   \n[44] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. TPAMI, 2013.   \n[45] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. Architecture matters in continual learning. arXiv preprint arXiv:2202.00275, 2022.   \n[46] Sudhanshu Mittal, Silvio Galesso, and Thomas Brox. Essentials for class incremental learning. In CVPR, 2021.   \n[47] Aristeidis Panos, Yuriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, and Richard E Turner. First session adaptation: A strong replay-free baseline for class-incremental learning. arXiv preprint arXiv:2303.13199, 2023.   \n[48] Karl Ezra Pilario, Mahmood Shafiee, Yi Cao, Liyun Lao, and Shuang-Hua Yang. A review of kernel methods for feature extraction in nonlinear process monitoring. Processes, 2020. doi: 10.3390/pr8010024.   \n[49] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In ECCV, 2020.   \n[50] Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. Online continual learning without the storage constraint. arXiv preprint arXiv:2305.09253, 2023.   \n[51] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, and Adel Bibi. Computationally budgeted continual learning: What does matter? In CVPR, 2023.   \n[52] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. NeurIPS, 2007.   \n[53] Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell. Continual unsupervised representation learning. NeurIPS, 32, 2019.   \n[54] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In CVPR, 2017.   \n[55] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019.   \n[56] Wouter F Schmidt, Martin A Kraaijveld, Robert PW Duin, et al. Feed forward neural networks with random weights. In ICPR, 1992.   \n[57] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00ebl Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013.   \n[58] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. In AAAI, 2021.   \n[59] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attentionbased prompting for rehearsal-free continual learning. In CVPR, 2023.   \n[60] James Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, and Zsolt Kira. A closer look at rehearsal-free continual learning. In CVPR-W, 2023.   \n[61] Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Pilot: A pre-trained model-based continual learning toolbox. arXiv preprint arXiv:2309.07117, 2023.   \n[62] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In NeurIPS-W, 2018.   \n[63] Gido M Van De Ven, Zhe Li, and Andreas S Tolias. Class-incremental learning with generative classifiers. In CVPR-W, 2021.   \n[64] Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L Hayes, Eyke H\u00fcllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H Lampert, et al. Continual learning: Applications and the road forward. arXiv preprint arXiv:2311.11908, 2023.   \n[65] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. In ECCV, 2022.   \n[66] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In European Conference on Computer Vision (ECCV), 2022.   \n[67] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[68] Yujie Wei, Jiaxin Ye, Zhizhong Huang, Junping Zhang, and Hongming Shan. Online prototype learning for online continual learning. In ICCV, 2023.   \n[69] Christopher Williams and Matthias Seeger. Using the nystr\u00f6m method to speed up kernel machines. NeurIPS, 2000.   \n[70] Yichen Wu, Long-Kai Huang, Renzhen Wang, Deyu Meng, and Ying Wei. Meta continual learning revisited: Implicitly enhancing online hessian approximation via variance reduction. In ICLR, 2024. URL https://openreview.net/forum?id=TpD2aG1h0D.   \n[71] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, 2019.   \n[72] Zichao Yang, Marcin Moczulski, Misha Denil, Nando De Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. In ICCV, 2015.   \n[73] Fei Ye and Adrian G Bors. Continual variational autoencoder learning via online cooperative memorization. In ECCV, 2022.   \n[74] Fei Ye and Adrian G Bors. Self-evolved dynamic expansion model for task-free continual learning. In ICCV, 2023.   \n[75] Micha\u0142 Zaj a\u02dbc, Tinne Tuytelaars, and Gido M van de Ven. Prediction error-based classification for classincremental learning. ICLR, 2024.   \n[76] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017.   \n[77] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning using online variational bayes. arXiv preprint arXiv:1803.10123, 2018.   \n[78] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei. Slca: Slow learner with classifier alignment for continual learning on a pre-trained model. In ICCV, 2023.   \n[79] Da-Wei Zhou, Qi-Wei Wang, Han-Jia Ye, and De-Chuan Zhan. A model or 603 exemplars: Towards memory-efficient class-incremental learning. arXiv preprint arXiv:2205.13218, 2022.   \n[80] Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need. arXiv preprint arXiv:2303.07338, 2023.   \n[81] Fei Zhu, Zhen Cheng, Xu-yao Zhang, and Cheng-lin Liu. Class-incremental learning via dual augmentation. NeurIPS, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper along with important assumptions. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Limitation Section is provided in the supplementary material. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: Rahimi and Recht [52] from our references details the theory for why random fourier representations perform so well quite beautifully. The random representations do not change (no continual aspect), hence the theory can be applied as-is in our case with no changes. We do not claim any novel theoretical contributions. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: RanDumb is fairly simple to implement. We dedicated half a page towards explaining hyperparameters and other information needed to reproduce all of our results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide code in the supplementary material to reproduce our results. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We use standard datasets and splits, we provide hyperparameters in experimental details along with ablations in experiment sections to understand the contribution of each component in our algorithm. ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We state here that the only random component being the random fourier features kernel across, otherwise our method is simple and exactly reproducible. We conducted experiments with three different initialisations corresponding to seeds of the random kernel in sklearn to investigate this and different kernel initialisations lead to around $\\pm0.2$ variation in the reported results. ", "page_idx": 14}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Detailed in Section 3 in Implementation Details. For easy access, we restate: All experiments were conducted on a CPU server with a153 48-core Intel Xeon Platinum 8268 CPU and 392GB of RAM, requiring less than 30 minutes per experiment. ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have read the ethics guidelines and confirm that we do not use human subjects, use existing datasets, explicitly discuss social impacts. ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The primary contribution in RanDumb was not to introduce a novel stateof-the-art continual learning method, but challenge prevailing assumptions and open a discussion on the efficacy of representation learning in continual learning algorithms. As such, we do not recommend use of RanDumb for deployment in real-world production systems, hence no direct societal impact or explicit limitations on use in production systems is discussed. ", "page_idx": 15}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: We do not have any high-risk model or dataset introduced. ", "page_idx": 15}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: MNIST, CIFAR-10, CIFAR-100, tinyImageNet and miniImagenet are cited appropriately. The licenses for these datasets is not explicitly released, hence we do not include that information. ", "page_idx": 15}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Yes, we provide RanDumb with proper documentation under a GPL3 license. ", "page_idx": 15}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects was performed. ", "page_idx": 15}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: No crowdsourcing or research with human subjects was performed. ", "page_idx": 16}, {"type": "text", "text": "A Conceptual and Methodological Differences from GDumb [49] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our core claim is that random representations from raw image pixels consistently outperform deeplearning-based representations designed for online continual learning. In contrast, GDumb\u2019s central claim is that continual learning methods need not actually minimize forgetting of previous online samples as their performance can be entirely recovered simply by a baseline using the latest memory. ", "page_idx": 17}, {"type": "text", "text": "The key differences between thee two works is as follows: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Forgetting of Online Samples (GDumb): GDumb argues that continual learning methods suffer from forgetting online samples and addresses this by relying entirely on memory. This dismisses the value of directly learning from online data, as GDumb does not retain or utilize information from online samples. RanDumb, in contrast, exclusively learns from online samples without using memory, emphasizing the significance of ongoing data streams for performance.   \n\u2022 Inadequate Representation Learning (RanDumb): GDumb\u2019s approach to representation learning mirrors the experience replay (ER) baseline but does not address the quality of the learned representations. RanDumb explicitly focuses on the inadequacy of these representations and ablates their role to highlight their impact on continual learning. This reveals a key distinction in how each method evaluates representation quality in online learning settings. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, the experimental setups for RanDumb and GDumb highlight their complementary nature: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Memory Settings: RanDumb primarily targets low-memory environments, whereas GDumb excels in high-memory scenarios. For example, in rehearsal-free settings without exemplar storage, a common trend in continual learning, GDumb would inherently produce random performance due to its dependence on memory. RanDumb, on the other hand, thrives in these low-memory contexts, providing an alternative solution when memory is constrained. \u2022 Complementary Nature: RanDumb and GDumb occupy complementary spaces within the continual learning landscape. RanDumb performs well in benchmarks where GDumb falters, and vice versa. As GDumb has been acknowledged as a valuable baseline, we argue that RanDumb similarly deserves recognition in the continual learning literature for its distinct strengths. ", "page_idx": 17}, {"type": "text", "text": "In summary, the only commonality between RanDumb and GDumb is that they serve as simple baselines. The points above underscore the fundamental distinctions between the two methods and the specific aims behind their development, as emphasized here and in the title of this work. ", "page_idx": 17}, {"type": "text", "text": "B Online Continual Learning: Our Setting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Current Problem formulation. We formally define the online continual learning (OCL) problem as follows. In classification settings, we aim to continually learn a function $f\\colon\\mathcal X\\rightarrow\\mathcal X$ , parameterized by $\\theta_{t}$ at time $t$ . OCL is an iterative process where each step consists of a learner receiving information and updating its model. For RanDumb, at each step $t$ of the interaction, ", "page_idx": 18}, {"type": "text", "text": "1. One data point $(x_{t},y_{t})\\sim\\pi_{t}$ sampled from a non-stationary distribution $\\pi_{t}$ is revealed.   \n2. Learner updates the model $\\theta_{t+1}$ using a compute budget, $B_{t}^{l e a r n}$ and discards the datapoint. ", "page_idx": 18}, {"type": "text", "text": "Simplifications by Compared Approaches. Traditional online continual learning literature makes several concessions over this which makes the problem easier by allowing the datapoint to be saved for more timesteps. Training deep networks requires those simplifications as more data per batch helps stabilize the gradient updates. Typically, compared approaches store samples across for 10 timesteps, and performs an update with that batch of samples before discarding it. Most works further relax this by storing a memory buffer of samples indefinitely. ", "page_idx": 18}, {"type": "text", "text": "Drawbacks. Traditional online continual learning setups cannot effectively test for rapid adaptation because they use a class-incremental setup. Online learning is generally intended to enable quick adaptation to changing data and label distributions in a data stream. We believe a better formulation for online continual learning is described in [50]. ", "page_idx": 18}]