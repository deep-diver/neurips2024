[{"figure_path": "KYHVBsEHuC/figures/figures_3_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model. It is divided into three main phases: 1.pcap pre-processing: This stage involves splitting pcap files into sessions (bidirectional flows), further splitting sessions into unidirectional flows (src-to-dst & dst-to-src), converting each byte to its corresponding hex number and tokenizing using WordPiece. 2.Pre-training: This phase uses BERT to generate embeddings for the tokens, utilizing two pre-training tasks: Masked Unidirectional Flow Model (predicting masked tokens) and Same Sequence-origin Prediction (predicting transmission order). 3.Fine-tuning with Diffusion Model: This phase fine-tunes the diffusion model using a forward process with packet sequence concatenation strategy and targeted noising, followed by a reverse process with normal packet guidance.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_16_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: 1.pcap pre-processing: This phase involves splitting pcap files into sessions and unidirectional flows, converting bytes to hex numbers and tokenizing them for BERT model processing. 2. pre-training: The pre-trained BERT model is used with two pre-training tasks: Masked Unidirectional Flow Model and Same Sequence-origin Prediction. 3. fine-tuning with diffusion model: This phase uses a diffusion model to generate adversarial packets. It involves embedding both normal and malicious packet sequences, contextually relevant pairing, concatenation, and targeted noising to the malicious packet sequences. The diffusion model uses a reverse process with normal packet guidance and classifier-free guidance.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_17_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: pcap pre-processing, pre-training, and fine-tuning with diffusion models.  The pcap pre-processing phase involves preparing the data by splitting pcap files into sessions and unidirectional flows, tokenizing bytes into hex numbers, and incorporating special tokens. The pre-training phase uses a pre-trained BERT model and two pre-training tasks (Masked Unidirectional Flow Model and Same Sequence-origin Prediction) to learn the contextual relationships between traffic bytes. The fine-tuning phase uses a diffusion model to generate adversarial packets, which blends adversarial packets into genuine network traffic by using a concatenation strategy and targeted noising.  The model employs a reverse process with normal packet guidance, utilizing the pre-trained BERT model for denoising and maintaining packet integrity.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_17_2.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "This figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: pcap pre-processing, pre-training, and fine-tuning with diffusion models.  The pcap pre-processing phase involves preparing the data by splitting pcap files into sessions and unidirectional flows and tokenizing the packet data.  The pre-training phase uses a pre-trained BERT model to learn contextual representations of network traffic by performing masked unidirectional flow prediction and same sequence-origin prediction tasks.  The fine-tuning phase uses a diffusion model to generate adversarial packets that mimic normal packets, which involves a forward process with packet sequence concatenation and targeted noising, and a reverse process with normal packet guidance.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_18_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: 1. pcap pre-processing: This phase involves preparing the raw pcap files (packet captures) for training by splitting them into sessions and unidirectional flows, converting bytes to hex numbers, tokenizing them, and adding special tokens. 2. pre-training: This phase pre-trains a BERT model using two tasks: Masked Unidirectional Flow Model (predicting masked tokens in the unidirectional flows) and Same Sequence-origin Prediction (predicting the origin of a sequence of packets). 3. fine-tuning with diffusion models: This phase uses the pre-trained BERT model with diffusion model to generate adversarial packets. Malicious packets are contextually paired with relevant normal packets, noise is added only to the malicious packets, and the model is trained to seamlessly blend the adversarial packets into the benign network traffic.  The model uses both forward and reverse diffusion processes with classifier-free guidance. ", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_18_2.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall architecture of DiffuPac, which is divided into three main phases: \n1. **Pcap Preprocessing:** This stage involves cleaning and preparing the raw pcap files, such as splitting them into sessions, unidirectional flows, tokenization, and embedding.\n2. **Pre-training:** This phase uses a pre-trained BERT model to learn contextual relationships in network traffic by employing Masked Unidirectional Flow Model and Same Sequence-origin Prediction.\n3. **Fine-tuning with Diffusion Model:** This stage fine-tunes the diffusion model by concatenating malicious and normal packet sequences and applying targeted noising, aiming to generate realistic adversarial packets that can bypass NIDS.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_19_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "This figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases:  pcap pre-processing, pre-training, and fine-tuning with a diffusion model. The pcap pre-processing phase involves preparing the data by splitting pcap files into sessions, then into unidirectional flows.  Pre-training uses a BERT model to learn contextual relationships in network traffic. The fine-tuning phase integrates the BERT model with the diffusion model for adversarial packet generation.  The figure shows the flow of data through each phase, highlighting key components and processes involved in generating adversarial packets.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_22_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: \n1. **pcap pre-processing**: This stage involves processing raw pcap files to extract relevant packet sequences, splitting sessions into unidirectional flows, tokenizing each byte into its corresponding hex number, and incorporating special tokens for training tasks.\n2. **Pre-training**:  A pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is used. The pre-training process uses two tasks: Masked Unidirectional Flow Model and Same Sequence-origin Prediction. The Masked Unidirectional Flow Model involves masking tokens and predicting the original tokens, while the Same Sequence-origin Prediction aims to determine whether packets belong to source-to-destination or destination-to-source sequences.\n3. **Fine-tuning with Diffusion Models**: The pre-trained BERT model is integrated with a diffusion model. This phase involves concatenating malicious packets with contextually relevant normal packets and applying targeted noising only to malicious packets. The targeted noising uses a classifier-free guidance approach to address the real-world constraint of limited attacker knowledge. Finally, the diffusion model undergoes fine-tuning to refine the generation of adversarial packets that evade detection while maintaining practicality.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_22_2.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the DiffuPac model, which consists of three main phases: pcap pre-processing, pre-training, and fine-tuning with diffusion models.  The pcap pre-processing phase involves splitting pcap files into sessions and unidirectional flows, tokenizing each byte to its hex number representation, and incorporating special tokens for training tasks.  The pre-training phase uses a pre-trained BERT model to learn contextual relationships between traffic bytes, involving masked unidirectional flow prediction and same sequence-origin prediction tasks.  The fine-tuning phase integrates the pre-trained BERT model with a diffusion model to generate adversarial packets that evade detection by NIDS. The process combines contextual understanding of network behaviors from the pre-trained BERT model with the generative capabilities of diffusion models to seamlessly blend adversarial packets into genuine network traffic.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_22_3.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "This figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: 1) pcap pre-processing, where raw pcap files are processed to extract unidirectional flows; 2) pre-training, where a pre-trained BERT model is used to learn contextual relationships between traffic bytes, using masked unidirectional flow and same sequence origin prediction; and 3) fine-tuning with diffusion model, where the model is fine-tuned to generate adversarial packets that can mimic normal packets and bypass NIDS. The figure also shows the data flow between different components of the model and the different embedding techniques used during the pre-training and fine-tuning phases. ", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_22_4.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "This figure illustrates the overall framework of the proposed DiffuPac model.  It's divided into three main phases:\n\n1.  **Pcap Preprocessing:** This initial step involves preparing the raw network traffic data. The pcap files are processed and categorized to facilitate subsequent training and analysis. The data is then tokenized and converted into a format suitable for the BERT model. \n2.  **Pre-training:**  A pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is used. This phase focuses on training the BERT model to understand contextual relationships within network flows. Two main pre-training tasks are used: Masked Unidirectional Flow Model (predicting masked tokens to learn underlying patterns) and Same Sequence-origin Prediction (predicting transmission order to understand directionality). \n3.  **Fine-tuning with Diffusion Model:** This phase involves integrating the pre-trained BERT model with a diffusion model to generate adversarial packets.  The diffusion model is trained using a concatenation strategy combining malicious and normal packet sequences.  Targeted noising is applied only to the malicious packets to seamlessly blend the adversarial packets into the genuine network traffic.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_22_5.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "This figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: pcap pre-processing, pre-training, and fine-tuning with diffusion models.  The pre-processing phase involves preparing the network traffic data by splitting it into sessions and unidirectional flows, converting bytes into hex numbers, and then tokenizing them. The pre-training phase uses a pre-trained BERT model to learn contextual representations from the network traffic data, focusing on masked token prediction and same-sequence-origin prediction tasks. Finally, the fine-tuning phase integrates the pre-trained BERT model with a diffusion model for adversarial packet generation. This phase involves contextually relevant pairing of normal and malicious packets and targeted noising to the malicious packets before applying classifier-free guidance.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_23_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the DiffuPac model, which consists of three main phases: pcap pre-processing, pre-training, and fine-tuning with diffusion models.  The pcap pre-processing phase involves the preparation of the network traffic data, including the splitting of the pcap files into sessions and unidirectional flows and the tokenization of the packet sequences. The pre-training phase uses a pre-trained BERT model for the task of masked unidirectional flow modeling and same sequence-origin prediction. The fine-tuning phase employs a diffusion model to generate adversarial packets by concatenating malicious packet sequences with contextually relevant normal packet sequences and applying targeted noising to the malicious packets. The figure shows the flow of data and the interactions between different components of the model.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_24_1.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: \n1. **pcap pre-processing**: This involves splitting pcap files into sessions (bidirectional flows), further splitting them into unidirectional flows (source-to-destination and destination-to-source), and converting each byte to its corresponding hex number and tokenization.\n2. **Pre-training**: This phase utilizes a pre-trained BERT model (Bidirectional Encoder Representations from Transformers) with two pre-training tasks: Masked Unidirectional Flow Model and Same Sequence-origin Prediction to learn the contextual relationships between traffic bytes.\n3. **Fine-tuning with Diffusion Model**: This phase uses a diffusion model to generate adversarial packets, which involves a forward process (with packet sequence concatenation strategy and targeted noising) and a reverse process (with normal packet guidance) to seamlessly blend adversarial packets into genuine network traffic.", "section": "3 DiffuPac"}, {"figure_path": "KYHVBsEHuC/figures/figures_24_2.jpg", "caption": "Figure 1: Proposed architecture is divided into three phases: pcap pre-processing, pre-training and fine-tuning with diffusion models.", "description": "The figure illustrates the overall framework of the proposed DiffuPac model, which consists of three main phases: pcap pre-processing, pre-training, and fine-tuning with diffusion models.  The pre-processing phase involves preparing the raw packet capture (pcap) files for training by splitting them into sessions, unidirectional flows, and tokenizing the byte sequences. The pre-training phase focuses on training a Bidirectional Encoder Representations from Transformers (BERT) model to learn contextual relationships within network traffic using masked language modeling and next sentence prediction tasks. The fine-tuning phase integrates the pre-trained BERT with a diffusion model for adversarial packet generation.  The process involves contextually relevant pairing of malicious and normal packet sequences, targeted noising of malicious packets, and classifier-free guidance via the diffusion model to generate adversarial packets that blend seamlessly with genuine network traffic.", "section": "3 DiffuPac"}]