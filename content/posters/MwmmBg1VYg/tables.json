[{"figure_path": "MwmmBg1VYg/tables/tables_2_1.jpg", "caption": "Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. =ImageNet [11], =Flowers102 [35], =StanfordCars [21],=Caltech101 [13].", "description": "This table presents the results of evaluating various visually-grounded language models (VLMs) and CLIP models on four standard image classification benchmarks: ImageNet, Flowers102, Stanford Cars, and Caltech101.  The table shows that the VLMs significantly underperform compared to the CLIP models, highlighting a key weakness of VLMs in image classification.", "section": "2 VLMs are Bad at Image Classification"}, {"figure_path": "MwmmBg1VYg/tables/tables_4_1.jpg", "caption": "Table 2: Analysis of VLMs from the inference perspective. (Top) We explore prompt variation such as wording, label order, chain-of-thought and find it has limited impact on the performance. (Bottom) We leverage the probabilistic inference strategy, which improves the performance but still fails to close the gap between VLMs and CLIPs. Results are from the official validation set.", "description": "This table presents the results of experiments that investigate the impact of different inference strategies on the performance of two VLMs (LLaVA1.5-7B and BLIP2-2.7B) on ImageNet classification.  The top half shows the effect of varying the prompt wording and order of labels in the prompt. The bottom half explores the use of probabilistic inference techniques to improve the accuracy of the models.  Despite these attempts, the VLMs still underperform CLIP.", "section": "3.1 Inference"}, {"figure_path": "MwmmBg1VYg/tables/tables_5_1.jpg", "caption": "Table 3: Analysis of VLMs from the training perspective. (Left) We conduct feature probing experiments on the VLM's last layer and find that the information required for classification is mostly preserved in the VLM's latent space. (Right) We fine-tune VLMs on the classification datasets using the text generation objective and find that the text generation training objective is as effective as the traditional cross-entropy for learning classification, which eliminates the VLM-CLIP performance gap, with VLMs now being the state-of-the-art classifier. Results are from the official validation set.", "description": "This table presents the results of probing experiments and fine-tuning experiments performed to analyze the training aspect of VLMs.  The left part shows that information crucial for classification is mostly preserved in the VLM's latent space. The right part demonstrates that fine-tuning VLMs on classification datasets using the text generation objective achieves state-of-the-art classification results, closing the gap between VLMs and CLIP.", "section": "3.2 Training"}, {"figure_path": "MwmmBg1VYg/tables/tables_7_1.jpg", "caption": "Table 4: Analysis of data types. We fine-tune the VLM on the caption-focused data generated by GPT4 using the same experimental settings as Table 3 and find that data is the main determining factor for VLM performance, and the data type does not matter much.", "description": "This table presents the results of fine-tuning a VLM (LLaVA1.5-7B) on different types of data: classification data and captioning data.  The goal is to determine if the type of data used for fine-tuning significantly affects the VLM's performance on image classification tasks (ImageNet, Flowers102, StanfordCars). The table compares the zero-shot performance of LLaVA1.5-7B with the performance after fine-tuning on classification data and captioning data, showing that the *quantity* of data is more important than the *type* of data.", "section": "4 Improving VLM with Classification Data"}, {"figure_path": "MwmmBg1VYg/tables/tables_8_1.jpg", "caption": "Table 5: Evaluations of VLMs on ImageWikiQA. ImageWikiQA is a multiple-choice question-answering dataset collected by feeding the Wikipedia pages of ImageNet classes to GPT-4. We find that current VLMs perform poorly in answering these questions, suggesting that their poor classification performance is a fundamental limitation for more advanced capabilities. Integrating classification data into VLM training enhances both their classification and overall capabilities.", "description": "This table presents the results of evaluating various visually grounded language models (VLMs) on the ImageWikiQA dataset.  ImageWikiQA is a newly created dataset designed to test both the classification abilities and more advanced reasoning capabilities of VLMs.  The results show that even advanced models perform poorly, highlighting the limitations of current VLMs in applying classification knowledge to more complex tasks.  The table also shows improved performance when classification data is included in the VLM training.", "section": "4 Improving VLM with Classification Data"}, {"figure_path": "MwmmBg1VYg/tables/tables_14_1.jpg", "caption": "Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models.  ImageNet [11], =Flowers102 [35], =StanfordCars [21],=Caltech101 [13].", "description": "This table presents the results of evaluating various visually-grounded language models (VLMs) and CLIP models on four standard image classification benchmark datasets: ImageNet, Flowers102, StanfordCars, and Caltech101.  The table shows that the VLMs significantly underperform compared to the CLIP models in terms of classification accuracy, highlighting a major gap in performance despite often using CLIP as a vision encoder.  The results emphasize the poor performance of VLMs in image classification tasks.", "section": "2 VLMs are Bad at Image Classification"}, {"figure_path": "MwmmBg1VYg/tables/tables_14_2.jpg", "caption": "Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. =ImageNet [11], =Flowers102 [35], =StanfordCars [21],=Caltech101 [13].", "description": "This table presents the results of evaluating various visually-grounded language models (VLMs) and CLIP models on four standard image classification benchmark datasets: ImageNet, Flowers102, StanfordCars, and Caltech101.  The table highlights the significant underperformance of VLMs compared to CLIP models, demonstrating that even with substantially more parameters, VLMs struggle to achieve comparable accuracy in image classification.  The results are categorized by whether the evaluation was in an 'open-world' (class labels not provided) or 'closed-world' (class labels provided) setting.", "section": "2 VLMs are Bad at Image Classification"}, {"figure_path": "MwmmBg1VYg/tables/tables_14_3.jpg", "caption": "Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. =ImageNet [11], =Flowers102 [35], =StanfordCars [21],=Caltech101 [13].", "description": "This table presents the results of evaluating various visually grounded language models (VLMs) and CLIP models on four standard image classification benchmarks: ImageNet, Flowers102, Stanford Cars, and Caltech101.  The table highlights the significant underperformance of VLMs compared to CLIP models, even though many VLMs utilize CLIP as their vision encoder.  The results are presented as accuracy percentages for each benchmark and model, illustrating the substantial gap in performance.", "section": "2 VLMs are Bad at Image Classification"}, {"figure_path": "MwmmBg1VYg/tables/tables_15_1.jpg", "caption": "Table 9: Analysis of the label set size. This is the table version of Figure 2.", "description": "This table shows the performance of three different models (LLaVA1.5-7B, BLIP2-2.7B, and CLIP-L) on four datasets (ImageNet, Flowers, Cars, and Caltech) with varying numbers of candidate classes (2, 5, 20, and 100).  It provides a quantitative comparison of the models' performance as the number of classes considered in the classification task is reduced. The table complements Figure 2 which presents the same data graphically, showing the accuracy of each model in relation to the number of candidate classes considered.", "section": "B.2 Label Set Size Details"}, {"figure_path": "MwmmBg1VYg/tables/tables_16_1.jpg", "caption": "Table 10: Probing the last token or the average token results in much better performance than probing other token positions. Experiments are done using LLaVA1.5-7B on the Flowers102 dataset.", "description": "This table presents the results of a feature probing experiment.  The experiment tested different positions within the LLaVA1.5-7B model's output to determine which position(s) contained the most information relevant for classification. The results indicate that probing the last token or average token yielded significantly better performance (accuracy) compared to other token positions.  This suggests that critical information for classification is concentrated towards the end of the model's processing.", "section": "B.4 Information Loss Details"}, {"figure_path": "MwmmBg1VYg/tables/tables_17_1.jpg", "caption": "Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models.  ImageNet [11],  Flowers102 [35],  StanfordCars [21], =Caltech101 [13].", "description": "This table presents the results of evaluating various visually-grounded language models (VLMs) and CLIP models on four standard image classification benchmarks (ImageNet, Flowers102, StanfordCars, and Caltech101).  The results highlight the significant underperformance of VLMs compared to CLIP models, despite VLMs often utilizing CLIP as their vision encoder and having significantly more parameters.  The table reveals the accuracy scores of each model on each benchmark in both open-world and closed-world settings.", "section": "2 VLMs are Bad at Image Classification"}, {"figure_path": "MwmmBg1VYg/tables/tables_18_1.jpg", "caption": "Table 2: Analysis of VLMs from the inference perspective. (Top) We explore prompt variation such as wording, label order, chain-of-thought and find it has limited impact on the performance. (Bottom) We leverage the probabilistic inference strategy, which improves the performance but still fails to close the gap between VLMs and CLIPs. Results are from the official validation set.", "description": "This table presents the results of experiments conducted to analyze the impact of different inference strategies on the performance of visually-grounded language models (VLMs) in image classification. The top part of the table shows the effect of prompt variations (wording, label order, chain-of-thought prompting) on two different VLMs (LLaVA1.5-7B and BLIP2-2.7B), revealing a limited impact on performance.  The bottom half shows results using a probabilistic inference strategy, which improves performance but does not eliminate the gap between VLMs and CLIP (a strong baseline model).", "section": "3.1 Inference"}, {"figure_path": "MwmmBg1VYg/tables/tables_22_1.jpg", "caption": "Table 13: Performance of LLaVA1.5-7B before and after fine-tuning on TextVQA, POPE, and MMVet datasets. Fine-tuning resulted in consistent performance across all benchmarks.", "description": "This table presents the performance comparison of the original LLaVA1.5-7B model and its further fine-tuned version across three different datasets: TextVQA, POPE (Popular and Adverse subsets), and MMVet.  The fine-tuning process did not significantly alter the model's performance, demonstrating its robustness across diverse tasks.", "section": "C.3 Performance of Fine-tuned VLM on Other Datasets"}, {"figure_path": "MwmmBg1VYg/tables/tables_24_1.jpg", "caption": "Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. =ImageNet [11], =Flowers102 [35], =StanfordCars [21],=Caltech101 [13].", "description": "This table presents the results of evaluating various visually-grounded language models (VLMs) and CLIP models on four standard image classification benchmark datasets: ImageNet, Flowers102, StanfordCars, and Caltech101.  The table shows that VLMs significantly underperform CLIP models in image classification across all datasets, highlighting a key limitation of VLMs in this fundamental task.", "section": "2 VLMs are Bad at Image Classification"}]