{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is a foundational vision encoder used extensively in the VLMs studied and compared in the target paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-01", "reason": "LLaVA, a key VLM architecture analyzed in the target paper, is introduced and described here."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "BLIP-2, another prominent VLM architecture, is introduced here, and its comparison with LLaVA provides key insights in the target paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper's findings on few-shot learning capabilities of language models are relevant to the target paper's investigation into VLM limitations."}, {"fullname_first_author": "Jia Deng", "paper_title": "ImageNet: A large-scale hierarchical image database", "publication_date": "2009-06-01", "reason": "ImageNet, a benchmark dataset heavily used in the target paper, is introduced and described here, providing essential context for evaluating VLM performance."}]}