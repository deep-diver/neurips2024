[{"figure_path": "x69O84Df2G/figures/figures_6_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "This figure presents the average estimation error of optimal policies across four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms). The estimation error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the set of optimal policies in the estimated MDP. The figure shows three plots: the first one combines the results for both the canonical basis of rewards and the random rewards, the second one uses only canonical basis rewards, and the third one uses only the random rewards. Shaded areas represent the 95% confidence intervals.", "section": "3.4 Numerical Results on Tabular MDPs"}, {"figure_path": "x69O84Df2G/figures/figures_30_1.jpg", "caption": "Figure 2: On the left: an MDP with two states and two actions; each tuple (a, p, r) consists of (1) the action a that triggers the transition, (2) the transition probability p and (3) the reward r. On the right: the minimum sub-optimality gap \u2206e as a function of \u03b8 \u2208 [0, 1] (note the discontinuity in \u03b8 = 23/26).", "description": "This figure shows a simple Markov Decision Process (MDP) with two states and two actions, along with a plot illustrating the minimum sub-optimality gap.  The MDP is represented as a graph, where each edge shows an action, its transition probability, and the reward received. The reward function depends on a parameter \u03b8 (theta), and its minimum sub-optimality gap demonstrates the non-convexity and discontinuity issues in the multi-reward best policy identification problem.", "section": "B.4.1 Non-convexity of the minimum sub-optimality gap"}, {"figure_path": "x69O84Df2G/figures/figures_48_1.jpg", "caption": "Figure 4: Forked Riverswim environment [53]. Each tuple (a,p) represents the action a that triggers the transition and the probability p of that event.", "description": "This figure shows a diagram of the Forked Riverswim environment.  It's a variation of the standard Riverswim environment, designed to be more challenging for exploration algorithms. Unlike the linear chain of states in the standard Riverswim, the Forked Riverswim has branching paths. The agent starts in state s0 and can move left (upstream), or right (downstream) in the main chain or choose to move to a different path at intermediate stages.  The tuple (a,p) associated with each transition indicates that action a is taken with probability p. This environment increases the sample complexity and tests an agent's ability to generalize across different paths within the environment.", "section": "3.4 Numerical Results on Tabular MDPs"}, {"figure_path": "x69O84Df2G/figures/figures_49_1.jpg", "caption": "Figure 4: Forked Riverswim environment [53]. Each tuple (a, p) represents the action a that triggers the transition and the probability p of that event.", "description": "The Forked Riverswim environment is a variation of the traditional Riverswim environment, designed to test more complex exploration strategies. In this variant, the state space branches into multiple paths, resembling a river that forks. At intermediate states the agent can switch between the forks, while the end states are not connected. This variant requires the agent to make more sophisticated decisions to explore the environment. This setup increases the sample complexity and challenges the agent's ability to generalize across different paths within the environment.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_49_2.jpg", "caption": "Figure 5: Double Chain environment [22]. Each tuple (a, p) represents the action a that triggers the transition and the probability p of that event.", "description": "The figure shows a diagram of the Double Chain environment, a reinforcement learning benchmark.  It consists of two separate chains of states, unlike the Forked Riverswim environment.  The agent cannot transition between the chains, and intermediate states are transient. Each state has two actions:  one moving towards the end of the chain (probability p) and one moving towards the start of the chain (probability 1-p). The figure visually represents the states as nodes and the actions and transition probabilities as labeled edges.", "section": "D.1.1 Environments Details"}, {"figure_path": "x69O84Df2G/figures/figures_50_1.jpg", "caption": "Figure 6: NArms environment. Each tuple (a, p) represents the action a that triggers the transition and the probability p of that event. In the figure the notation ano:no+n indicates all the actions in {ano,...,ano+n}. In state so the probability to remain in so for any action ai is P(so|so,ai) = 1-po/(i + 1), with the exception that P(so|so, ao) = 0.", "description": "This figure depicts the NArms environment, a simplified version of the 6Arms environment.  The environment is a Markov Decision Process (MDP) with n+1 states. The agent starts in state s0 and chooses one of n actions (arms) a0,...,an-1. Each action transitions the agent to state si with probability pi, where pi is defined by a parameter p0.  The figure shows the state transition probabilities, indicating the action and its corresponding transition probability. The probability of remaining in state s0 is 1 - p0/(i+1) except for action a0, where it's 0. This environment challenges the agent's exploration capabilities due to unequal transition probabilities for each action.", "section": "3.4 Numerical Results on Tabular MDPs"}, {"figure_path": "x69O84Df2G/figures/figures_51_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "This figure shows the policy estimation error for four different environments (Riverswim, Forked Riverswim, DoubleChain, NArms) using four different algorithms (MR-NaS, RF-UCRL, ID3AL, MR-PSRL). The error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the set of optimal policies in the estimated MDP. Three plots are provided, each showing the error over different reward sets. The first combines the canonical basis rewards and random rewards, while the second and third show results for the canonical and random rewards alone, respectively.", "section": "3.4 Numerical Results on Tabular MDPs"}, {"figure_path": "x69O84Df2G/figures/figures_53_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "The figure displays the average estimation error of optimal policies for three different reward settings.  The error is calculated as the symmetric difference between the sets of optimal policies in the true MDP and the estimated MDP. The three plots show the error for: (1) both the canonical basis of rewards and randomly generated rewards, (2) only the canonical basis, and (3) only the randomly generated rewards.  Each plot shows results for four environments: Riverswim, Forked Riverswim, DoubleChain, and NArms. The shaded areas represent the 95% confidence intervals, indicating uncertainty in the average error.", "section": "Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_54_1.jpg", "caption": "Figure 9: Average value of Avisit,t = maxs tvisit (s') \u2013 mins tvisit(s), where tvisit(s) is the time step the algorithm last visited state s.", "description": "This figure displays the average deviation in visitation time across different states for four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms).  The deviation, Avisit,t,  is calculated as the difference between the maximum and minimum times that the algorithm has visited any state up to time t.  It measures how much the algorithm favors some states over others during exploration. Smaller values indicate more uniform exploration across the state space.", "section": "Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_54_2.jpg", "caption": "Figure 10: Top: average value of the least visited state-action pair mins,a Nt(s, a). Bottom: normalized average value of the entropy of Nt over time (see text for a definition).", "description": "This figure compares four different reinforcement learning algorithms (MR-NaS, RF-UCRL, ID3AL, and MR-PSRL) across four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms).  The top row shows the average number of times the least visited state-action pair was visited over time, illustrating the exploration behavior of each algorithm. The bottom row displays the normalized entropy of the number of state-action visits over time, providing a measure of how evenly each algorithm explores the state-action space. The plots show how the different algorithms explore different aspects of the environments and the trade-off between exploration and exploitation.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_54_3.jpg", "caption": "Figure 11: Diagram of the Cartpole swing-up problem.", "description": "The Cartpole swing-up problem involves balancing a pole attached to a cart on a frictionless track.  The cart's position is controlled by applying force, and the goal is to swing the pole up from a hanging position and maintain its balance. This is a challenging reinforcement learning problem due to the sparsity of the reward signal (reward is only obtained when the pole is balanced), and the penalty for movement. The figure illustrates the setup, showing the cart, pole, angle (\u03b8), angular velocity (\u03b8\u0307), and the goal position of the upright pole.", "section": "Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_56_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "This figure presents the results of policy estimation error for four different environments: Riverswim, Forked Riverswim, DoubleChain, and NArms.  Three plots are shown, each comparing the performance of four different algorithms (MR-NAS, RF-UCRL, ID3AL, and MR-PSRL) in estimating optimal policies. The first plot combines results using both the canonical basis rewards and random rewards, while the second and third plots show results using only canonical basis rewards and random rewards, respectively.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_56_2.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of et,r = |\u03a0*(M)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "The figure presents the policy estimation error over different sets of rewards and environments. It shows the average value of et,r (the symmetric difference between the set of optimal policies in the true MDP and the estimated MDP) and its 95% confidence interval. The first plot combines the results for the canonical basis of rewards and random rewards, while the other two plots consider each set separately. The results demonstrate that MR-NaS outperforms the baselines in estimating optimal policies, particularly when focusing on the canonical basis of rewards.", "section": "Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_57_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of  et,r = |\u03a0*(M\u2084)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "This figure shows the policy estimation error of the Multi-Reward Navigate and Stop (MR-NaS) algorithm and three baselines (RF-UCRL, ID3AL, and MR-PSRL) across four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms). The error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the estimated MDP. Three plots are shown, corresponding to (1) combining the results for canonical and random rewards, (2) using only canonical rewards, and (3) only using random rewards.  The figure displays the mean and 95% confidence intervals for each algorithm across 100 seeds.", "section": "Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_57_2.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure shows the average estimation error for four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms) across four different algorithms (MR-NAS, RF-UCRL, ID3AL, and MR-PSRL).  The x-axis represents the number of steps taken. The y-axis represents the average estimation error, showing how frequently the algorithms incorrectly identified the optimal policy.  Shaded regions denote 95% confidence intervals based on 100 independent simulation runs. This illustrates the relative performance of the algorithms in terms of accurately identifying the best policy for a given reward function, with MR-NaS generally showing superior performance.", "section": "3.4 Numerical Results on Tabular MDPs"}, {"figure_path": "x69O84Df2G/figures/figures_59_1.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure displays the average estimation error of the optimal policies over time steps for four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms) and four different algorithms (MR-NaS, RF-UCRL, ID3AL, and MR-PSRL).  The shaded region represents the 95% confidence interval calculated from 100 independent runs. Each plot shows the average estimation error for each algorithm across the various time steps in the corresponding environment. The x-axis denotes the number of steps performed by the algorithms, and the y-axis shows the average estimation error. The different colours and lines denote different algorithms.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_60_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "This figure shows the estimation error of optimal policies for different reward sets and algorithms. The plots show the average value of et,r, which represents the symmetric difference between the optimal policy sets of the true MDP and the estimated MDP at time t, for a given reward r, along with 95% confidence intervals. The figure includes three subplots showing results for the canonical basis, the random rewards and a combination of both. This helps evaluate the algorithms' performance in various scenarios.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_61_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of  et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)| (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "The figure displays the average estimation error for the optimal policies across various hard exploration tabular environments. The error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the set of optimal policies in the estimated MDP.  The figure compares the performance of MR-NAS against three baselines (RF-UCRL, ID3AL, and MR-PSRL) across three different reward allocation scenarios: 1) Canonical basis rewards combined with random rewards; 2) canonical basis rewards only; and 3) random rewards only. The results show the average estimation error, as well as 95% confidence intervals, over 100 seeds, for a specified number of steps.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_62_1.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure displays the average estimation error of optimal policies across four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms) and four different algorithms (MR-NaS, RF-UCRL, ID3AL, and MR-PSRL).  The error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the estimated MDP.  The shaded areas represent 95% confidence intervals, based on 100 independent runs. The figure shows the error over time (number of steps).", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_62_2.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure displays the average estimation error of the optimal policies over time steps for four different environments: Riverswim, Forked Riverswim, DoubleChain, and NArms.  The error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the set of optimal policies in the estimated MDP.  The shaded area shows the 95% confidence intervals based on 100 independent runs of each algorithm for each environment.  The plot shows the algorithms' progress in accurately identifying the optimal policies. Note that the reward r is a random variable.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_63_1.jpg", "caption": "Figure 21: Downlink link adaptation diagram.", "description": "The figure shows a diagram of a downlink link adaptation system. It shows multiple UEs transmitting data to an eNB (evolved NodeB) via a wireless link. The RL agent at the eNB receives CQI (Channel Quality Indicator) feedback and ACK/NACK (Acknowledgement/Negative Acknowledgement) indicating successful/failed data transmissions from the UEs.  Based on the received feedback, the RL agent dynamically adjusts the MCS (Modulation and Coding Scheme) for transmission. The MCS determines the transmission rate and reliability, thus balancing throughput, spectral efficiency, and reliability.", "section": "D.3 Radio network control problems"}, {"figure_path": "x69O84Df2G/figures/figures_65_1.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure presents the average estimation error for four different environments (Riverswim, Forked Riverswim, DoubleChain, and NArms) across four different algorithms (MR-NaS, RF-UCRL, ID3AL, and MR-PSRL). The estimation error is calculated over T time steps and considers a random reward r.  Shaded regions represent 95% confidence intervals across 100 independent experimental runs. The figure shows the performance of MR-NaS compared to other baseline methods across various environments.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_66_1.jpg", "caption": "Figure 7: Estimation error of the optimal policies. The plots depict the average value of  et,r = |\u03a0*(Mr)\u2206\u03a0*(Mt,r)|  (r is a random quantity) and its 95% confidence interval. In the first plot we combine together the results for the canonical basis Rcanonical and the random rewards Rrnd. In the second plot we only consider the canonical basis of rewards Rcanonical, whilst in the third plot we only consider the random rewards Rrnd.", "description": "This figure shows the estimation error of the optimal policies for four different environments: Riverswim, Forked Riverswim, DoubleChain, and NArms. The estimation error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the set of optimal policies in the estimated MDP. The figure shows three different plots, one for each of the three sets of rewards used in the experiments: canonical rewards, random rewards, and a combination of both.", "section": "3.4 Numerical Results on Tabular MDPs"}, {"figure_path": "x69O84Df2G/figures/figures_66_2.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure displays the average estimation error over a series of time steps (t) for four different hard-exploration tabular environments: Riverswim, Forked Riverswim, DoubleChain, and NArms.  The error is calculated for each environment using four different algorithms: MR-NaS, RF-UCRL, ID3AL, and MR-PSRL. The shaded area represents the 95% confidence interval, calculated from 100 independent runs, making the results statistically robust. Each plot shows how the estimation error decreases as the number of steps increases, indicating the convergence of the algorithms.  The comparison allows for evaluation of the relative efficiency of each algorithm in identifying optimal policies across the different environments.", "section": "3.4 Numerical Results on Tabular MDPS"}, {"figure_path": "x69O84Df2G/figures/figures_66_3.jpg", "caption": "Figure 1: Average estimation error over t = 1,..., T steps (note that r is a random variable). Shaded area indicates 95% confidence intervals over 100 seeds.", "description": "This figure shows the average estimation error of optimal policies over time steps for four different environments: Riverswim, Forked Riverswim, DoubleChain, and NArms.  The error is calculated as the symmetric difference between the set of optimal policies in the true MDP and the estimated MDP.  The shaded area represents the 95% confidence interval based on 100 independent simulation runs.  The figure helps illustrate how the different algorithms perform in identifying optimal policies across different environments, highlighting the speed and accuracy of convergence.", "section": "3.4 Numerical Results on Tabular MDPS"}]