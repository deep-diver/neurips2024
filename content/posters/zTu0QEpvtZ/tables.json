[{"figure_path": "zTu0QEpvtZ/tables/tables_8_1.jpg", "caption": "Table 2: The difference between images generated under varied a with the ones of a = 0. The results are averaged over 30K generated images, and saved latency is evaluated on one V100 GPU.", "description": "This table presents the results of an experiment to evaluate the effect of removing text prompt at different stages of the denoising process in Stable Diffusion. The experiment varied the starting step (a) at which the text prompt was removed, and compared the results to the baseline (a=0).  Metrics include Image-CLIPScore (higher is better), L1-distance (lower is better), saved latency (higher is better), and FID (lower is better).  The results are shown for different samplers (DDIM and DPM-Solver) and Stable Diffusion model versions (v1.5, v2.1, and Pixart-Alpha).", "section": "6 Application"}, {"figure_path": "zTu0QEpvtZ/tables/tables_16_1.jpg", "caption": "Table 3: The alignment of generated results image under different constructed text prompt sets. Here \"Sem + EOS\" is the original text prompt, and serves as baseline here. Besides that, the CLIPScore (Image) is the image-level alignment of generated images with the ones under \"Sem + EOS\".", "description": "This table shows the results of text-image alignment using three metrics (CLIPScore, BLIP-VQA, MiniGPT4-CoT) for different text prompt variations. The original prompt is \"Sem + EOS\", while other prompts replace semantic tokens ([Sem]) or the end-of-sentence token ([EOS]) with either zero vectors or random vectors.  The table compares the alignment scores (higher scores indicate better alignment) of generated images with the corresponding text prompts to evaluate the relative importance of semantic tokens and the [EOS] token in the generation process.", "section": "5 The Working Mechanism of Text Prompt"}, {"figure_path": "zTu0QEpvtZ/tables/tables_17_1.jpg", "caption": "Table 4: The alignment of generated image with its source and target prompts, under switched [EOS] on Key or Value substitution. Here KV-Sub is the complete substitution as in Section 5, which serves as a baseline here.", "description": "This table presents the results of an experiment where the [EOS] token in text prompts was switched, and either the key (K) or value (V) in the cross-attention module was substituted.  The alignment of generated images with both the source and target prompts was measured using three metrics: CLIPScore (Text), BLIP-VQA, and MiniGPT-CoT.  The KV-Sub column represents a baseline where both K and V were substituted, providing a comparison point for the other substitution methods. The results show the impact of substituting either K or V individually on the generated images' alignment with the source and target prompts.", "section": "5.1 [EOS] Contains More Information"}]