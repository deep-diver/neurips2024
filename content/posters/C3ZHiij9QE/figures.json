[{"figure_path": "C3ZHiij9QE/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our VLMimic. (a) Typical VIL methods struggle to generalize to unseen environments, and (b) current methods naively utilize VLMs as planners, encounter difficulties in generating low-level actions. (c) VLMimic grounds human videos to obtain action movements, and learns skills with fine-grained actions, while the skill adapter updates skills for generalization. (d) Our method achieves superior performance given a limited collection of human videos.", "description": "This figure illustrates the core idea of the VLMimic framework by contrasting it with traditional VIL methods and VLM-based planning approaches. Panel (a) shows that typical VIL methods struggle to generalize to unseen environments, highlighting the limitations of learning from limited data. Panel (b) demonstrates how current methods, which use VLMs as high-level planners, encounter challenges in generating the low-level actions necessary for robot control. Panel (c) presents the VLMimic method, which directly learns fine-grained actions from human videos using a skill learner and skill adapter. The skill learner extracts object-centric movements and learns skills with fine-grained actions from human videos, while the skill adapter helps generalize to unseen environments. Finally, panel (d) shows the superior performance of VLMimic compared to other methods using limited human video data.", "section": "1 Introduction"}, {"figure_path": "C3ZHiij9QE/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our VLMimic. (a) The human-object interaction grounding module parses videos into multiple segments and captures object-centric movements. Then, (b) a skill learner extracts knowledge from action motions and derives skills. In novel scenes, (c) a skill adapter updates the learned skills to facilitate adaptation.", "description": "This figure illustrates the three main modules of the VLMimic framework:  Human-object interaction grounding, skill learner with hierarchical representation, and skill adapter with iterative comparison.  The grounding module processes human demonstration videos to identify object-centric interactions. The skill learner leverages these interactions and hierarchical representations to extract knowledge and generate skills. Finally, the skill adapter updates these skills iteratively in new, unseen environments. This entire process enables VLMimic to generalize robot skills from limited human demonstrations.", "section": "3 VLMimic"}, {"figure_path": "C3ZHiij9QE/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of Human-object interaction grounding module. (a) It recognizes tasks and related objects from human videos, (b) parses videos into multiple segments based on this information, and subsequently (c) identifies object-centric interactions within each segment.", "description": "This figure illustrates the three main steps of the Human-object Interaction Grounding module.  First, (a) Task Recognition uses keyframes from the video to identify the task and involved objects. Second, (b) Video Parsing segments the video into subtasks using motion trajectories.  Finally, (c) Interaction Extraction extracts object-centric interactions for each subtask (e.g., grasping, manipulation).  The output provides a structured representation of the human-object interactions in the video, suitable for subsequent skill learning.", "section": "3.1 Human-object Interaction Grounding"}, {"figure_path": "C3ZHiij9QE/figures/figures_8_1.jpg", "caption": "Figure 4: Configuration of various viewpoints.", "description": "This figure shows four different viewpoints used to test the robustness of the VLMimic approach against changes in perspective. Each image shows the robot arm performing a manipulation task from a different angle.  The images illustrate that the method is effective even with shifts in camera position, demonstrating the model's robustness.", "section": "4.4 Robustness against viewpoint variance"}, {"figure_path": "C3ZHiij9QE/figures/figures_15_1.jpg", "caption": "Figure 6: Visualization of the make-a-pie task.", "description": "This figure visualizes the steps involved in the \"make a pie\" task, a subtask within the long-horizon tasks section of the paper.  The images show a robotic arm performing each step of the task: (0) initial state, (1) grasping the bowl, (2) pouring sauce onto the pie, (3) grasping the brush, (4) spreading sauce, (5) opening the microwave, (6) placing the pan in the microwave, (7) closing the microwave, and (8) turning on the microwave. This sequence of images demonstrates the fine-grained actions that VLMimic is capable of learning and executing.", "section": "C.1 Kitchen"}, {"figure_path": "C3ZHiij9QE/figures/figures_16_1.jpg", "caption": "Figure 7: Visualization of the wash-pan task.", "description": "This figure shows a step-by-step visualization of a robot performing the task of washing a pan.  It highlights the sequence of actions involved, from initially placing the pan in the sink to the final placement of the washed pan on the rack. Each step is depicted with an image of the robot in the respective pose.", "section": "C.1 Kitchen"}, {"figure_path": "C3ZHiij9QE/figures/figures_16_2.jpg", "caption": "Figure 8: Visualization of the make-cucumber-slices task.", "description": "The figure shows a sequence of images illustrating the steps involved in making cucumber slices using a robotic arm. The initial state shows the robot arm, a cucumber in a refrigerator, a cutting board, and a knife. The steps involved are: placing the cutting board on the table, opening the refrigerator, placing the cucumber on the cutting board, closing the refrigerator, removing the knife from the knife rack, cutting the cucumber, and finally placing the knife back in the rack.", "section": "3.1 Human-object Interaction Grounding"}, {"figure_path": "C3ZHiij9QE/figures/figures_17_1.jpg", "caption": "Figure 9: Visualization of the make-coffee task.", "description": "This figure shows a step-by-step visualization of the robot performing the \"Make Coffee\" task.  It highlights the sequence of actions involved, from grasping the coffee capsule to turning on the coffee machine. Each step is illustrated with a separate image, providing a clear visual representation of the task's sub-goals.", "section": "C.2 Table"}, {"figure_path": "C3ZHiij9QE/figures/figures_18_1.jpg", "caption": "Figure 10: Visualization of the clean-table task.", "description": "The figure shows a sequence of images illustrating the steps involved in the \"Clean table\" task.  The task involves cleaning up a table with several items scattered on it, such as fruits, cups, and brushes. The robot arm performs a series of actions to put these items back into designated places (e.g., a plate, drawer) and then sweeps the table with a dust brush.  The images depict the robot arm manipulating the items and completing the steps sequentially.", "section": "C.2 Table"}, {"figure_path": "C3ZHiij9QE/figures/figures_18_2.jpg", "caption": "Figure 11: Visualization of the Chemistry Lab task.", "description": "This figure visualizes the steps involved in a chemistry experiment task.  The initial setup shows a test tube, two beakers, two conical flasks, and a funnel on a retort stand. The steps shown depict the robot manipulating these items: pouring liquids between containers, shaking a flask, and using a funnel. Each sub-figure corresponds to a subtask in a longer sequence of actions required to complete the experiment.", "section": "3.1 Human-object Interaction Grounding"}, {"figure_path": "C3ZHiij9QE/figures/figures_19_1.jpg", "caption": "Figure 12: Manipulated objects in SE setting.", "description": "This figure shows the objects used in the seen environment (SE) experiments for the VLMimic robotic manipulation tasks.  The image depicts various kitchen and household items arranged on a table, including a microwave oven, a toaster oven, a coffee machine, bowls, cups, plates, utensils, and fruits.  The Franka Emika robot arm is also visible, positioned to interact with the objects.  The scene demonstrates a setup for testing the robot's manipulation abilities in a familiar environment.", "section": "4 Experiments"}, {"figure_path": "C3ZHiij9QE/figures/figures_19_2.jpg", "caption": "Figure 13: Manipulated objects in US setting.", "description": "This figure shows the various objects used in the unseen environment (US) experiments for real-world manipulation tasks.  It provides a visual representation of the setup, illustrating the complexity and diversity of objects the robot had to interact with during testing.  The image shows a variety of everyday household items arranged on a table, including a microwave, oven, containers, and various tools, demonstrating the challenges of generalizing to unseen environments.", "section": "4.2 Real-world Manipulation Tasks"}, {"figure_path": "C3ZHiij9QE/figures/figures_20_1.jpg", "caption": "Figure 15: Visualization of manipulation task results in unseen environments.", "description": "This figure shows the results of 14 manipulation tasks performed by the robot in unseen environments.  Each task is shown in a pair of images, with the left image showing the initial state and the right image showing the final state after the robot has completed the task. The tasks involve a range of actions, such as opening drawers and ovens, putting objects on plates, brushing pans, and pouring liquids. The figure demonstrates the ability of the VLMimic model to generalize to unseen environments, successfully completing the tasks despite the differences in the environments and objects.", "section": "4.2 Real-world Manipulation Tasks"}, {"figure_path": "C3ZHiij9QE/figures/figures_21_1.jpg", "caption": "Figure 15: Visualization of manipulation task results in unseen environments.", "description": "This figure shows the results of various manipulation tasks performed by a robot in unseen environments.  The tasks are visually depicted, showing the robot interacting with different objects and completing tasks like opening drawers, stacking blocks, and pouring liquids. The images provide a visual representation of the robot's successful execution of these manipulation tasks in settings it has not previously encountered, showcasing the generalization capabilities of the proposed VLMimic approach.", "section": "4.2 Real-world Manipulation Tasks"}]