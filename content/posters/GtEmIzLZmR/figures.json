[{"figure_path": "GtEmIzLZmR/figures/figures_1_1.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows accuracy-fairness trade-off curves for the COMPAS dataset.  The black and red curves represent the trade-off from the same optimally trained model tested on two different data splits, highlighting the variability due to randomness in data splits.  The blue curve shows a suboptimal model. The green shaded region represents the range of acceptable fairness violations (permissible trade-off region) for each accuracy level, according to the proposed method.  Pink indicates a suboptimal accuracy-fairness trade-off (worse than achievable), and blue shows areas of unlikely achievable trade-offs (better than achievable). This illustrates the dataset-dependent nature of the fairness-accuracy trade-off and demonstrates the method's ability to quantify the uncertainty in estimated trade-offs.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_6_1.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows the accuracy-fairness trade-off curves for the COMPAS dataset.  Three curves are presented: one for an optimally trained model evaluated on two different data splits (black and red), and one for a sub-optimally trained model (blue). The shaded regions represent the range of permissible (green), suboptimal (pink), and practically unachievable (blue) trade-offs, illustrating how dataset characteristics affect the achievable fairness-accuracy balance. The figure highlights the challenges of using uniform fairness requirements across diverse datasets.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_7_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure displays the accuracy-fairness trade-off curves for four different datasets (Adult, COMPAS, CelebA, and Jigsaw) across three fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  The black and red curves represent the empirical trade-offs from optimally trained models evaluated on two different data splits. The blue curve shows the trade-off from a suboptimally trained model. The green area represents the permissible trade-off region determined by the authors' method, while the pink and blue areas show suboptimal and unlikely-to-be-achieved regions, respectively. The figure demonstrates how the trade-offs differ across datasets and fairness metrics.", "section": "Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_20_1.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows accuracy-fairness trade-off curves for the COMPAS dataset.  Three curves are plotted, demonstrating different levels of optimality in model training.  The black and red curves are from the same optimally trained model tested on different data splits, illustrating variance due to sampling. The blue curve is from a suboptimally trained model.  Colored regions highlight ranges of permissible fairness violations for various accuracy levels, indicating potentially achievable, suboptimal, and unreachable trade-offs.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_20_2.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows accuracy-fairness trade-off curves for the COMPAS dataset.  Multiple curves are shown, illustrating the impact of different training methods and data splits on the trade-off.  The key takeaway is that the optimal trade-off is dataset dependent and varies with accuracy levels. The figure also visualizes regions representing permissible, suboptimal, and unlikely-to-be-achieved fairness-accuracy combinations.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_24_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure presents the accuracy-fairness trade-off curves for four real-world datasets: Adult, COMPAS, CelebA, and Jigsaw.  For each dataset, the figure shows the trade-off curves for different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds) obtained using various methods, including the proposed YOTO method, and several baseline methods.  The confidence intervals (CIs) generated by four different methods (Hoeffding's, asymptotic, Bernstein, and bootstrap) are also shown.  The use of a 10% data split for calibration and two separately trained models for sensitivity analysis is highlighted in the caption. This visualization helps compare the performance of different fairness methods across different data modalities and highlights the data-dependent nature of the accuracy-fairness trade-off.", "section": "Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_24_2.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows the accuracy-fairness trade-off curves for the COMPAS dataset. Three curves are presented: black and red curves, obtained from the same optimally trained model but evaluated on different data splits; and a blue curve, obtained from a suboptimally trained model.  The green shaded area represents the range of permissible fairness violations for each accuracy level, providing a benchmark for evaluating model fairness. The pink area highlights suboptimal accuracy-fairness trade-offs, while the blue area shows trade-offs that are unlikely to be achievable. The figure demonstrates how the accuracy-fairness trade-off varies depending on the model's training and the data used for evaluation.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_24_3.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows the accuracy-fairness trade-off curves for the COMPAS dataset. Three curves are shown: optimal model on two different data splits (black and red), and a suboptimal model (blue). The green shaded region represents the permissible fairness violations for each accuracy level, given by the proposed method.  The pink region highlights suboptimal trade-offs, and the blue region shows unlikely-to-be-achieved trade-offs. This illustrates the dataset-dependent nature of the accuracy-fairness trade-off and the uncertainty in estimating this trade-off.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_24_4.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows accuracy-fairness trade-offs for the COMPAS dataset. Three curves are presented: an optimal model evaluated on two different data splits (black and red), and a suboptimal model (blue).  The shaded regions illustrate the range of achievable fairness violations for each accuracy level, differentiating between permissible (green), suboptimal (pink), and unlikely (blue) trade-offs. This highlights the dataset-dependent nature of the accuracy-fairness tradeoff and the uncertainty involved in estimating optimal tradeoffs.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_25_1.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows the accuracy-fairness trade-off curves for the COMPAS dataset.  Three curves illustrate the trade-off obtained using different training methods: optimal training (black and red curves, showing variation due to different data splits), and suboptimal training (blue curve). The green area represents the range of permissible fairness violations for each accuracy level, providing a benchmark for acceptable model performance. Pink shaded area indicates suboptimal performance, while the blue shaded area represents ranges unlikely to be achievable.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_25_2.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure displays accuracy-fairness trade-off curves for the COMPAS dataset.  Multiple curves are shown to illustrate the impact of different training methods and data splits on the trade-off. The black and red curves represent results from the same optimally trained model, evaluated on different data splits to highlight the impact of sampling variability. The blue curve shows results from a suboptimally trained model.  Three regions are highlighted: a green region representing the permissible fairness-accuracy tradeoffs, a pink region showing suboptimal trade-offs, and a blue region representing trade-offs unlikely to be achievable. This visualization emphasizes the dataset-specific nature of the fairness-accuracy trade-off and the importance of accounting for uncertainty.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_26_1.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure displays accuracy-fairness trade-off curves for the COMPAS dataset.  Multiple curves illustrate the impact of model training methods and data splits on the trade-off.  The green shaded area represents the achievable range of fairness given a certain accuracy; the pink area shows suboptimal trade-offs and the blue area shows unachievable trade-offs.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_28_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure shows the accuracy-fairness trade-off curves for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  For each dataset and metric, multiple methods are compared, including the proposed YOTO method. The shaded regions represent confidence intervals for the optimal achievable trade-offs, calculated with a 95% confidence level. The results highlight the data-dependent nature of the accuracy-fairness trade-off.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_32_1.jpg", "caption": "Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)", "description": "This figure shows the accuracy-fairness trade-off curves for the COMPAS dataset.  The black and red curves represent the trade-off obtained from the same optimally trained model evaluated on two different data splits, demonstrating the impact of data variability. The blue curve shows the trade-off of a suboptimally trained model. The green shaded region represents the permissible range of fairness violations at each accuracy level, the pink region represents suboptimal trade-offs, and the blue region represents the unlikely-to-be-achieved area. This visualization highlights the dataset-dependent nature of the accuracy-fairness trade-off and the uncertainty involved in estimating it.", "section": "1 Introduction"}, {"figure_path": "GtEmIzLZmR/figures/figures_33_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure displays the accuracy-fairness trade-off curves for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  The curves are generated using various state-of-the-art (SOTA) fairness methods, including the proposed YOTO method. The shaded regions represent confidence intervals, showing the uncertainty in the estimated trade-off curves. The figure highlights how the optimal accuracy-fairness trade-off varies significantly across different datasets and fairness metrics.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_33_2.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure shows the accuracy-fairness trade-offs for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds). The black and red curves represent the trade-offs obtained from an optimal model evaluated on different data splits.  The blue curve represents the trade-off from a suboptimal model. The green shaded area shows the permissible range of fairness violations for each accuracy level, the pink region represents suboptimal accuracy-fairness trade-offs, and the blue region highlights trade-offs unlikely to be achieved. The figure also demonstrates how the authors' method provides confidence intervals (CIs) to quantify uncertainty in estimates and avoid false conclusions due to estimation errors.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_33_3.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure presents the accuracy-fairness trade-off curves for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  The curves show the minimum fairness violation achievable for each accuracy level. The figure also displays confidence intervals (CIs) representing the uncertainty in the estimated trade-off curve, due to finite-sampling and estimation errors. These CIs provide a range of 'permissible' accuracy-fairness trade-offs.  The figure demonstrates how the trade-offs vary across different datasets and fairness metrics and highlights the importance of dataset-specific fairness considerations.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_34_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure presents the accuracy-fairness trade-offs for four different real-world datasets: Adult, COMPAS, CelebA, and Jigsaw.  For each dataset, it shows the trade-off curves for several fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds) and different fairness methods.  The figure highlights the range of permissible fairness violations using confidence intervals calculated by the proposed method. The confidence intervals consider both finite-sampling and approximation errors, providing a robust framework for auditing fairness.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_34_2.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure presents accuracy-fairness trade-off curves for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  The curves show the minimum fairness violation achievable for each accuracy level. The shaded areas represent confidence intervals constructed using different methods (Hoeffding\u2019s, asymptotic, Bernstein\u2019s, and bootstrap), showing the uncertainty in the estimated trade-off curves. The figure demonstrates that the optimal trade-offs can differ significantly across different datasets, highlighting the importance of considering dataset characteristics when setting fairness guidelines.", "section": "Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_34_3.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure shows the accuracy-fairness trade-off curves for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  The curves represent the results of several state-of-the-art fairness methods and the authors' YOTO method.  Confidence intervals are shown to account for uncertainty in the estimates, highlighting the dataset-dependent nature of the accuracy-fairness trade-off and the uncertainty involved in estimating it. The figure shows that the YOTO method provides both reliable and informative intervals while offering significant computational savings compared to training multiple models. The sensitivity analysis is used to check for any suboptimality in YOTO's performance.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_35_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure shows the accuracy-fairness trade-off curves for four real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using four different fairness metrics (Demographic Parity, Equalized Opportunity, Equalized Odds).  The curves are generated using several state-of-the-art (SOTA) fairness methods and the proposed YOTO method.  The confidence intervals (CIs) obtained using different methods (Hoeffding's, Bernstein's, asymptotic, and bootstrap) are also shown for comparison. The figure highlights the dataset-dependent nature of the accuracy-fairness trade-off and demonstrates the effectiveness of the proposed YOTO method in constructing reliable confidence intervals.", "section": "Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_35_2.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure visualizes the accuracy-fairness trade-offs for four real-world datasets (Adult, COMPAS, CelebA, Jigsaw) using different fairness metrics (Demographic Parity, Equalized Opportunity, Equalized Odds).  The results show confidence intervals (CIs) calculated using four different methods (Hoeffding's, asymptotic, Bernstein's, bootstrap), and they are compared against several state-of-the-art (SOTA) fairness methods (KDE-fair, logsig, linear, reductions, RTO, adversary, separate). The figure highlights the dataset-dependent nature of these trade-offs and demonstrates the effectiveness of the proposed method in constructing reliable and informative confidence intervals.", "section": "Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_35_3.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure visualizes the accuracy-fairness trade-offs for four different real-world datasets (Adult, COMPAS, CelebA, and Jigsaw) using four different fairness metrics (Demographic Parity, Equalized Opportunity, Equalized Odds).  The results are shown with 95% confidence intervals calculated using four different methods (Hoeffding\u2019s, Bernstein\u2019s, asymptotic, and bootstrap).  The YOTO model's performance is compared against several state-of-the-art (SOTA) fairness methods. The sensitivity analysis (|M|=2) is used to distinguish between sub-optimality due to finite samples and inherent limitations of the SOTA method.  Each subplot represents a dataset and each line style/color shows performance with a particular fairness metric and SOTA method.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_36_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure displays the accuracy-fairness trade-offs for four datasets (Adult, COMPAS, CelebA, and Jigsaw) across three fairness metrics (Demographic Parity, Equalized Opportunity, and Equalized Odds).  It compares the performance of the YOTO model with other state-of-the-art fairness methods. The confidence intervals generated by YOTO (representing the range of permissible fairness violations for each accuracy level) are shown as shaded green regions, providing a robust assessment of the model's fairness and avoiding false conclusions due to estimation errors. Suboptimal and unachievable regions are also shown.", "section": "5 Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_37_1.jpg", "caption": "Figure 3: Results on four real-world datasets where Dcal is a 10% data split. Here, \u03b1 = 0.05 and we use |M| = 2 separately trained models for sensitivity analysis.", "description": "This figure presents the accuracy-fairness trade-off curves and their corresponding confidence intervals for four real-world datasets: Adult, COMPAS, CelebA, and Jigsaw.  Each dataset is evaluated using three different fairness metrics: Demographic Parity, Equalized Opportunity, and Equalized Odds. The confidence intervals, calculated using four different methods (Hoeffding's, asymptotic, Bernstein, and bootstrap), provide a range of permissible fairness violations for each accuracy level.  The YOTO method's trade-off curve is compared to those of several other state-of-the-art fairness methods. The figure also uses a sensitivity analysis (|M|=2) which refines the confidence intervals by taking into account the potential sub-optimality of the YOTO model. The use of 10% of data as Dcal for calibration is specified.", "section": "Experiments"}, {"figure_path": "GtEmIzLZmR/figures/figures_38_1.jpg", "caption": "Figure 29: Plot showing how \u2206(hx) decreases (relative to the ground truth trade-off value \u03c4^(acc(hx))) as the training data size |Dtr| increases. Here, we plot the worst (i.e. largest) value of max\u03bb\u2208\u039b(acc(hx),\u03c4^(acc(hx)) achieved by our YOTO model over a grid of \u03bb values in [0, 5].", "description": "This figure shows how the difference between the YOTO model's fairness trade-off and the optimal trade-off decreases as the size of the training data increases.  The y-axis represents the maximum difference (across different regularization parameters lambda) between the YOTO model's fairness tradeoff and the optimal fairness tradeoff at a given accuracy, relative to the optimal fairness tradeoff.  The x-axis shows the training dataset size. Three lines represent three different fairness metrics: Demographic Parity (DP), Equalized Opportunity (EOP), and Equalized Odds (EO). The shaded area represents the confidence interval.", "section": "F.7 Synthetic data experiments"}]