[{"figure_path": "rpjh69DUX2/figures/figures_9_1.jpg", "caption": "Figure 1: Results of the meta-test on Frozen Lake, where Alg(1) is applied. Left: Average accumulated reward across all test tasks v.s. number of policy adaptation steps; Right: Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(1).", "description": "This figure displays the results of the meta-test on the Frozen Lake environment using Alg(1) in the BO-MRL framework.  The left panel shows the average accumulated reward across all test tasks plotted against the number of policy adaptation steps. The BO-MRL approach is compared against baselines like random initialization and MAML.  The right panel focuses on the expected optimality gap, comparing BO-MRL's performance to the upper bound of the accumulated reward achieved with a single application of Alg(1). This illustrates BO-MRL's efficiency in leveraging data collected from a single policy adaptation.", "section": "6.1 Verification of theoretical results"}, {"figure_path": "rpjh69DUX2/figures/figures_9_2.jpg", "caption": "Figure 2: Average accumulated reward across all test tasks during the meta-test under the practical algorithm of BO-MRL on the locomotion tasks.", "description": "This figure compares the performance of the proposed BO-MRL algorithm against several baseline methods (MAML, E-MAML, ProMP, and MAML-TRPO) across four locomotion tasks in the MuJoCo simulator.  The tasks involve Half-Cheetah with goal directions and velocities, and Ant with goal directions and velocities. The x-axis represents the number of policy adaptation steps, and the y-axis shows the average accumulated reward across all test tasks. The figure demonstrates that the proposed BO-MRL algorithm consistently achieves higher average accumulated reward than the baseline methods for all four tasks, showcasing its superior performance in high-dimensional environments.", "section": "High-dimensional Experiment"}, {"figure_path": "rpjh69DUX2/figures/figures_14_1.jpg", "caption": "Figure 3: Results of the meta-test on Frozen Lake, where Alg(2) is applied. Left: Average accumulated reward across all test tasks v.s. number of policy adaptation steps; Right: Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(2).", "description": "This figure shows the results of applying Algorithm 1 with the Alg(2) within-task algorithm to the Frozen Lake environment.  The left panel displays the average accumulated reward achieved across all test tasks, plotted against the number of policy adaptation steps.  This demonstrates the performance of the proposed BO-MRL method compared to baselines like MAML and random initialization. The right panel focuses on the expected optimality gap, comparing BO-MRL's performance to the upper bound derived from the theoretical analysis.  This comparison shows how close the BO-MRL method gets to the optimal policy.", "section": "6.1 Verification of theoretical results"}, {"figure_path": "rpjh69DUX2/figures/figures_14_2.jpg", "caption": "Figure 4: Results of BO-MRL on Frozen Lake, where Alg(3) is applied. Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(3).", "description": "This figure displays the results of the BO-MRL algorithm on the Frozen Lake environment, specifically using the Alg(3) within-task algorithm.  It compares the expected optimality gap (the difference between the optimal policy and the policy obtained by the algorithm) for three scenarios: no policy adaptation, a single one-time adaptation using Alg(3), and one step of policy gradient adaptation.  The results are shown separately for high and low task variance distributions.  A horizontal dashed line represents the theoretical upper bound derived in the paper for the one-time Alg(3) policy adaptation. This figure visually verifies the theoretical upper bounds and demonstrates the performance improvement of the BO-MRL approach.", "section": "6.1 Verification of theoretical results"}, {"figure_path": "rpjh69DUX2/figures/figures_15_1.jpg", "caption": "Figure 2: Average accumulated reward across all test tasks during the meta-test under the practical algorithm of BO-MRL on the locomotion tasks.", "description": "This figure displays the average accumulated rewards obtained during meta-testing on four MuJoCo locomotion tasks using the practical BO-MRL algorithm.  The tasks are Half-cheetah with goal velocity, Half-cheetah with goal direction, Ant with goal velocity, and Ant with goal direction.  Results are shown for different numbers of policy adaptation steps (1, 2, and 3). The BO-MRL algorithm's performance is compared against several baseline meta-reinforcement learning methods (MAML-TRPO and ProMP).  The graph illustrates how BO-MRL's performance improves over the baselines, and how that improvement increases with more policy adaptation steps, showing better generalization and adaptation ability.", "section": "High-dimensional Experiment"}]