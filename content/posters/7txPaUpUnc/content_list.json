[{"type": "text", "text": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dan Braun\u2217 Jordan Taylor\u2020 Nicholas Goldowsky-Dill ", "page_idx": 0}, {"type": "text", "text": "Lee Sharkey\u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network\u2019s internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the dataset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e_sae. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sparse Autoencoders (SAEs) are a popular method in mechanistic interpretability [Sharkey et al., 2022, Cunningham et al., 2023, Bricken et al., 2023]. They have been proposed as a solution to the problem of superposition, the phenomenon by which networks represent more \u2018features\u2019 than they have neurons. \u2018Features\u2019 are directions in neural activation space that are considered to be the basic units of computation in neural networks. SAE dictionary elements (or \u2018SAE features\u2019) are thought to approximate the features used by the network. SAEs are typically trained to reconstruct the activations of an individual layer of a neural network using a sparsely activating, overcomplete set of dictionary elements (directions). It has been shown that this procedure identifies ground truth features in toy models [Sharkey et al., 2022]. ", "page_idx": 0}, {"type": "text", "text": "However, current SAEs focus on the wrong goal: They are trained to minimize mean squared reconstruction error (MSE) of activations (in addition to minimizing their sparsity penalty). The issue is that the importance of a feature as measured by its effect on MSE may not strongly correlate with how important the feature is for explaining the network\u2019s performance. This would not be a problem if the network\u2019s activations used a small, finite set of ground truth features \u2013 the SAE would simply identify those features, and thus optimizing MSE would have led the SAE to learn the functionally important features. In practice, however, Bricken et al. [2023] observed the phenomenon of feature splitting, where increasing dictionary size while increasing sparsity allows SAEs to split a feature into multiple, more specific features, representing smaller and smaller portions of the dataset. In the limit of large dictionary size, it would be possible to represent each individual datapoint as its own dictionary element. Since minimizing MSE does not explicitly prioritize learning features based on how important they are for explaining the network\u2019s performance, an SAE may waste much of its fixed capacity on learning less important features. This is perhaps responsible for the observation that, when measuring the causal effects of circuits made from SAE features on network performance, a significant amount is mediated by the reconstruction residual errors (i.e. everything not explained by the SAE) and not mediated by SAE features [Marks et al., 2024]. ", "page_idx": 0}, {"type": "image", "img_path": "7txPaUpUnc/tmp/074a001cdee73da9f45a93df699755bf02233c32a6891474512690cb7de70e5d.jpg", "img_caption": ["Figure 1: Top: Diagram comparing the loss terms used to train each type of SAE. Each arrow is a loss term which compares the activations represented by circles. $\\mathrm{SAE}_{\\mathrm{local}}$ uses MSE reconstruction loss between the SAE input and the SAE output. $S A E_{\\mathrm{e2e}}$ uses KL-divergence on the logits. $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ (end-to-end $^+$ downstream reconstruction) uses KL-divergence in addition to the sum of the MSE reconstruction losses at all future layers. All three are additionally trained with a $L_{1}$ sparsity penalty (not pictured). ", "Bottom: Pareto curves for three different types of GPT2-small layer 6 SAEs as the sparsity coefficient is varied. E2e-SAEs require fewer features per datapoint (i.e. have a lower $L_{0}$ ) and fewer features over the entire dataset (i.e. have a low number of alive dictionary elements). GPT2-small has a CE loss of 3.139 over our evaluation set. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Given these issues, it is therefore natural to ask how we can identify the functionally important features used by the network. We say a feature is functionally important if it is important for explaining the network\u2019s behavior on the training distribution. If we prioritize learning functionally important features, we should be able to maintain strong performance with fewer features used by the SAE per datapoint as well as fewer overall features. ", "page_idx": 1}, {"type": "text", "text": "To optimize SAEs for these properties, we introduce a new training method. We still train SAEs using a sparsity penalty on the feature activations (to reduce the number of features used on each datapoint), but we no longer optimize activation reconstruction. Instead, we replace the original activations with the SAE output (Figure 1) and optimize the KL divergence between the original output logits and the output logits when passing the SAE output through the rest of the network, thus training the SAE end-to-end (e2e). We use $\\mathrm{SAE_{e2e}}$ to denote an SAE trained with KL divergence and a sparsity penalty. ", "page_idx": 1}, {"type": "text", "text": "By contrast, we use $\\mathrm{SAE}_{\\mathrm{local}}$ to denote our baseline SAEs, trained only to reconstruct the activations at the current layer with a sparsity penalty. ", "page_idx": 2}, {"type": "text", "text": "One risk with this method is that it may be possible for the outputs of $\\mathrm{SAE_{e2e}}$ to take a different computational pathway through subsequent layers of the network (compared with the original activations) while nevertheless producing a similar output distribution. For example, it might learn a new feature that exploits a particular transformation in a downstream layer that is unused by the regular network or that is used for other purposes. To reduce this likelihood, we also add terms to the loss for the reconstruction error between the original model and the model with the SAE at downstream layers in the network (Figure 1). We use $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ to denote SAEs trained with KL divergence, a sparsity penalty, and downstream reconstruction loss. We use $e2e\\;S A E s$ to refer to the family of methods introduced in this work, including both $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ . ", "page_idx": 2}, {"type": "text", "text": "Previous work has used the performance explained \u2013 measured by cross-entropy loss difference when replacing the original activations with SAE outputs \u2013 as a measure of SAE quality [Cunningham et al., 2023, Bricken et al., 2023, Bloom, 2024]. It\u2019s reasonable to ask whether our approach runs afoul of Goodhart\u2019s law (\u201cWhen a measure becomes a target, it ceases to be a good measure\u201d). We contend that mechanistic interpretability should prefer explanations of networks (and the components of those explanations, such as features) that explain more network performance over other explanations. Therefore, optimizing directly for quantitative proxies of performance explained (such as CE loss difference, KL divergence, and downstream reconstruction error) is preferred. ", "page_idx": 2}, {"type": "text", "text": "We train each SAE type on language models (GPT2-small [Radford et al., 2019] and Tinystories-1M [Eldan and Li, 2023]), and present three key findings: ", "page_idx": 2}, {"type": "text", "text": "1. For the same level of performance explained, $\\mathrm{SAE}_{\\mathrm{local}}$ requires activating more than twice as many features per datapoint compared to $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ and $\\mathrm{SAE}_{\\mathrm{e2e}}$ (Section 3.1). 2. $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ performs equally well as $S\\mathrm{AE}_{\\mathrm{e2e}}$ in terms of the number of features activated per datapoint (Section 3.1), yet its activations take pathways through the network that are much more similar to $\\mathrm{SAE}_{\\mathrm{local}}$ (Sections 3.2, 3.3). 3. $\\mathrm{SAE}_{\\mathrm{local}}$ requires more features in total over the dataset to explain the same amount of network performance compared with $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ (Section 3.1). ", "page_idx": 2}, {"type": "text", "text": "These findings suggest that e2e SAEs are more efficient in capturing the essential features that contribute to the network\u2019s performance. Moreover, our automated-interpretability and qualitative analyses reveal that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ features are at least as interpretable as $\\mathrm{SAE}_{\\mathrm{local}}$ features, demonstrating that the improvements in efficiency do not come at the cost of interpretability (Section 3.4). These gains nevertheless come at the cost of longer wall-clock time to train (Appendix H). ", "page_idx": 2}, {"type": "text", "text": "As a supplementary investigation, we tested e2e SAEs on a set of subject-verb agreement tasks from Finlayson et al. [2021]. These tests had inconclusive results (Appendix I), indicating that further work is needed to identify which downstream tasks would benefit most from e2e SAEs. ", "page_idx": 2}, {"type": "text", "text": "In addition to this article, we also provide: A library for training all SAE types presented in this article (https://github.com/ApolloResearch/e2e_sae); a Weights and Biases [Biewald, 2020] report that links to training metrics for all runs (https://api.wandb.ai/links/sparsify/ evnqx8t6); a Neuronpedia [Lin and Bloom, 2023] page for interacting with the features in a subset of SAEs (those presented in Tables 2 and 3) (https://www.neuronpedia.org/gpt2sm-apollojt) as well as a repository for downloading these SAEs directly (https://huggingface.co/ apollo-research/e2e-saes-gpt2). ", "page_idx": 2}, {"type": "text", "text": "2 Training end-to-end SAEs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our experiments train SAEs using three kinds of loss function (Figure 1), which we evaluate according to several metrics (Section 2.5): ", "page_idx": 2}, {"type": "text", "text": "1. $L_{\\mathrm{local}}$ trains SAEs to reconstruct activations at a particular layer (Section 2.2); 2. $L_{\\mathrm{e}2\\mathrm{e}}$ trains SAEs to learn functionally important features (Section 2.3); 3. $L_{\\mathrm{e}2\\mathrm{e}+\\mathrm{downstream}}$ trains SAEs to learn functionally important features that optimize for faithfulness to the activations of the original network at subsequent layers (Section 2.4). ", "page_idx": 2}, {"type": "text", "text": "2.1 Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose we have a feedforward neural network (such as a decoder-only Transformer [Radford et al., 2018]) with $L$ layers and vectors of hidden activations $a^{(l)}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a^{(0)}(x)=x}\\\\ &{a^{(l)}(x)=f^{(l)}(a^{(l-1)}(x)),\\;\\mathrm{for}\\;l=1,\\ldots,L-1}\\\\ &{\\quad\\quad y=\\mathrm{softmax}\\left(f^{(L)}(a^{(L-1)}(x))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We use SAEs that consist of an encoder network (an affine transformation followed by a ReLU activation function) and a dictionary of unit norm features, represented as a matrix $D$ , with associated bias vector $b_{d}$ . The encoder takes as input network activations from a particular layer $l$ . The architecture we use is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Enc}\\left(a^{(l)}(x)\\right)=\\mathrm{ReLU}\\left(W_{e}a^{(l)}(x)+b_{e}\\right)}\\\\ &{\\mathrm{SAE}\\left(a^{(l)}(x)\\right)=D^{\\top}\\mathrm{Enc}\\left(a^{(l)}(x)\\right)+b_{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the dictionary $D$ and encoder weights $W_{e}$ are both $\\mathbb{N}_{-}$ _dict_elements $\\times\\ d$ _hidden) matrices, $b_{e}$ is a N_dict_elements-dimensional vector, while $b_{d}$ and $\\bar{a}^{(l)}(x)$ are d_hidden-dimensional vectors. ", "page_idx": 3}, {"type": "text", "text": "2.2 Baseline: Local SAE training loss $\\mathbf{(}L_{\\mathrm{local}})$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The standard, baseline method for training SAEs is $\\mathrm{SAE}_{\\mathrm{local}}$ training, where the output of the SAE is trained to reconstruct its input using a mean squared error loss with a sparsity penalty on the encoder activations (here an L1 loss): ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\mathrm{local}}=L_{\\mathrm{reconstuction}}+L_{\\mathrm{sparsity}}=||a^{(l)}(x)-\\mathrm{SAE}_{\\mathrm{local}}(a^{(l)}(x))||_{2}^{2}+\\phi||\\mathrm{Enc}(a^{(l)}(x))||_{1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r}{\\phi=\\frac{\\lambda}{\\dim\\left(a^{(l)}\\right)}}\\end{array}$ is a sparsity coefficient $\\lambda$ scaled by the size of the input to the SAE (see Appendix D for details on hyperparameters). ", "page_idx": 3}, {"type": "text", "text": "2.3 Method 1: End-to-end SAE training loss $\\left(L_{\\mathrm{e}2\\mathrm{e}}\\right)$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For $\\mathrm{SAE_{e2e}}$ , we do not train the SAE to reconstruct activations. Instead, we replace the model activations with the output of the SAE and pass them forward through the rest of the network: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{a}^{(l)}(x)=\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}}\\big(a^{(l)}(x)\\big)}\\\\ &{\\hat{a}^{(k)}(x)=f^{(k)}(\\hat{a}^{(l)}(x))\\,\\,\\mathrm{for}\\,\\,k=l,\\ldots,L-1}\\\\ &{\\quad\\quad\\hat{y}=\\mathrm{softmax}\\left(f^{(L)}(\\hat{a}^{(L-1)}(x))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We train the SAE by penalizing the $\\mathrm{KL}$ divergence between the logits produced by the model with the SAE activations and the original model: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\mathrm{e2e}}=L_{\\mathrm{KL}}+L_{\\mathrm{sparsity}}=K L(\\hat{y},y)+\\phi||\\mathrm{Enc}(a^{(l)}(x))||_{1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Importantly, we freeze the parameters of the model, so that only the SAE is trained. This contrasts with Tamkin et al. [2023], who train the model parameters in addition to training a \u2018codebook\u2019 (which is similar to a dictionary). ", "page_idx": 3}, {"type": "text", "text": "2.4 Method 2: End-to-end with downstream layer reconstruction SAE training loss (Le2e+downstream) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A reasonable concern with the $L_{\\mathrm{e}2\\mathrm{e}}$ is that the model with the SAE inserted may compute the output using an importantly different pathway through the network, even though we\u2019ve frozen the original model\u2019s parameters and trained the SAE to replicate the original model\u2019s output distribution. To counteract this possibility, we also compare an additional loss: The end-to-end with downstream reconstruction training loss $\\left(L_{\\mathrm{e}2\\mathrm{e+downstream}}\\right)$ additionally minimizes the mean squared error between the activations of the new model at downstream layers and the activations of the original model: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{\\mathrm{e2e+downstream}}={\\cal L}_{\\mathrm{KL}}+{\\cal L}_{\\mathrm{sparsity}}+{\\cal L}_{\\mathrm{downstream}}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=K L(\\hat{y},y)+\\phi||\\mathrm{Enc}(a^{(l)})||_{1}+\\displaystyle{\\frac{\\beta_{l}}{{\\cal L}-l}\\sum_{k=l+1}^{L-1}}\\ ||\\hat{a}^{(k)}(x)-a^{(k)}(x)||_{2}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta_{l}$ is a hyperparameter that controls the downstream reconstruction loss term (Appendix D). ", "page_idx": 4}, {"type": "text", "text": "$L_{\\mathrm{e}2\\mathrm{e}+\\mathrm{downstream}}$ thus has the desirable properties of 1) incentivizing the SAE outputs to lead to similar computations in downstream layers in the model and 2) allowing the SAE to \u201cclear out\" some of the non-functional features by not training on a reconstruction error at the layer with the SAE. Note, however, the inclusion of the intermediate reconstruction terms means that $L_{\\mathrm{e}2\\mathrm{e}+}$ downstream may encourage the SAE to learn features that are less functionally important. ", "page_idx": 4}, {"type": "text", "text": "2.5 Experimental metrics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We record several key metrics for each trained SAE: ", "page_idx": 4}, {"type": "text", "text": ". Cross-entropy loss increase between the original model and the model with SAE: We measure the increase in cross-entropy (CE) loss caused by using activations from the inserted SAE rather than the original model activations on an evaluation set. We sometimes refer to this as \u2018amount of performance explained\u2019, where a low CE loss increase means more performance explained. All other things being equal, a better SAE recovers more of the original model\u2019s performance. ", "page_idx": 4}, {"type": "text", "text": "2. L0: How many SAE features activate on average for each datapoint. All other things being equal, a better SAE needs fewer features to explain the performance of the model on a given datapoint. ", "page_idx": 4}, {"type": "text", "text": "3. Number of alive dictionary elements: The number of features in training that have not \u2018died\u2019 (which we define to mean that they have not activated over a set of $500\\mathbf{k}$ tokens of data). All other things being equal, a better SAE needs a smaller number of alive features to explain the performance of model over the dataset. ", "page_idx": 4}, {"type": "text", "text": "We also record the reconstruction loss at downstream layers. This is the mean squared error between the activations of the original model and the model with the SAE at all layers following the insertion of the SAE (i.e. downstream layers). If reconstruction loss at downstream layers is low, then the activations take a similar pathway through the network as in the original model. This minimizes the risk that the SAEs are learning features that take different computational pathways through the downstream layers compared to the original model. Finally, following Bills et al. [2023], we perform automated-interpretability scoring and qualitative analysis on a subset of the SAEs, to verify that improved quantitative metrics does not sacrifice the interpretability of the learned features. ", "page_idx": 4}, {"type": "text", "text": "We show results for experiments performed on GPT2-small\u2019s residual stream before attention layer 6.3 Results for layers 2, 6, and 10 of GPT2-small and some runs on a model trained on the TinyStories dataset [Eldan and Li, 2023] can be found in Appendices A.1 and A.2, respectively. They are qualitatively similar to those presented in the main text. For our GPT2-small experiments, we train SAEs with each type of loss function on $400\\mathbf{k}$ samples of context size 1024 from the Open Web Text dataset [Gokaslan and Cohen, 2019] over a range of sparsity coefficients $\\lambda$ . Our dictionary is fixed at 60 times the size of the residual stream (i.e. $60\\times768=46080$ initial dictionary elements). Hyperparameters, along with sweeps over dictionary size and number of training examples, are shown in Appendices D and E, respectively. ", "page_idx": 4}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 End-to-end SAEs are a Pareto improvement over local SAEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We compare the trained SAEs according to CE loss increase, $L_{0}$ , and number of alive dictionary elements. The learning rates for each SAE type were selected to be Pareto-optimal according to their ", "page_idx": 4}, {"type": "text", "text": "$L_{0}$ vs CE loss increase curves.4 Each experiment uses a range of sparsity coefficients $\\lambda$ . In Figure 1, we see that both $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ achieve better CE loss increase for a given $L_{0}$ or for a given number of alive dictionary elements. This means they need fewer features to explain the same amount of network performance for a given datapoint or for the dataset as a whole, respectively. For similar results at other layers see Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "This difference is large: For a given $L_{0}$ , both $\\mathrm{SAE}_{\\mathrm{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ have a CE loss increase that is less than $45\\%$ of the CE loss increase of $\\mathrm{SAE}_{\\mathrm{local}}$ .5 $\\mathrm{SAE}_{\\mathrm{local}}$ must therefore be learning features that are not maximally important for explaining network performance. ", "page_idx": 5}, {"type": "text", "text": "This improved performance comes at the expense of increased compute costs (2-3.5 times longer runtime, see Appendix H). We test to see if additional compute improves our $\\mathrm{SAE}_{\\mathrm{local}}$ baseline in Appendix E. We find neither increasing dictionary size from $60*768$ to $100*768$ nor increasing training samples from $400\\mathbf{k}$ to $800\\mathbf{k}$ noticeably improves the Pareto frontier, implying that our e2e SAEs maintain their advantage even when compared against $\\mathrm{SAE}_{\\mathrm{local}}$ dictionaries trained with more compute. ", "page_idx": 5}, {"type": "text", "text": "For comparability, our subsequent analyses focus on 3 particular SAEs that have approximately equivalent CE loss increases (Table 1). ", "page_idx": 5}, {"type": "table", "img_path": "7txPaUpUnc/tmp/21b2f165c75f9cc83216544ae5a18ab4467eeac7cb347ac092d707c9394f2a4c.jpg", "table_caption": ["Table 1: Three SAEs from layer 6 with similar CE loss increases are analyzed in detail. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.2 End-to-end SAEs have worse reconstruction loss at each layer despite similar output distributions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Even though $\\mathrm{SAE}_{\\mathrm{e2e}}\\mathrm{s}$ explain more performance per feature than $\\mathrm{SAE_{local}s}$ , they have much worse reconstruction error of the original activations at each subsequent layer (Figure 2). This indicates that the activations following the insertion of $\\mathrm{SAE_{e2e}}$ take a different path through the network than in the original model, and therefore potentially permit the model to achieve its performance using different computations from the original model. This possibility motivated the training of $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}\\mathrm{S}$ . ", "page_idx": 5}, {"type": "text", "text": "In later layers, the reconstruction errors of $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ are extremely similar (Figure 2). $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ therefore has the desirable properties of both learning features that explain approximately as much network performance as $S\\mathrm{AE}_{\\mathrm{e2e}}$ (Figure 1) while having reconstruction errors that are much closer to $\\mathrm{SAE}_{\\mathrm{local}}$ . There remains a difference in reconstruction at layer 6 between $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ . This is not surprising given that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ is not trained with a reconstruction loss at this layer. In Appendix B, we examine how much of this difference is explained by feature scaling. In Appendix G.3, we find a specific example of a direction with low functional importance that is faithfully represented in $\\mathrm{SAE}_{\\mathrm{local}}$ but not $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Differences in feature geometries between SAE types ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.3.1 End-to-end SAEs have more orthogonal features than $\\mathbf{SAE_{local}}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Bricken et al. [2023] observed \u2018feature splitting\u2019, where a locally trained SAEs learns a cluster of features which represent similar categories of inputs and have dictionary elements pointing in similar directions. A key question is to what extent these subtle distinctions are functionally important for the network\u2019s predictions, or if they are only helpful for reconstructing functionally unimportant patterns in the data. ", "page_idx": 5}, {"type": "image", "img_path": "7txPaUpUnc/tmp/126ab514b6e2e682617e2795c46348ed79b163a4c02878fdd97c83079927842e.jpg", "img_caption": ["Figure 2: Reconstruction mean squared error (MSE) at later layers for our set of GPT2-small layer 6 SAEs with similar CE loss increases (Table 1). $\\mathrm{SAE}_{\\mathrm{local}}$ is trained to minimize MSE at layer 6, $S\\mathrm{AE}_{\\mathrm{e2e}}$ was trained to match the output probability distribution, $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ was trained to match the output probability distribution and minimize MSE in all downstream layers. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We have already seen that $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ learn smaller dictionaries compared with $\\mathrm{SAE}_{\\mathrm{local}}$ for a given level of performance explained (Figure 1). In this section, we explore if this is due to less feature splitting. We measure the cosine similarities between each SAE dictionary feature and nextclosest feature in the same dictionary. While this does not account for potential semantic differences between directions with high cosine similarities, it serves as a useful proxy for feature splitting, since split features tend to be highly similar directions [Bricken et al., 2023]. We find that $\\mathrm{SAE}_{\\mathrm{local}}$ has features that are more tightly clustered, suggesting higher feature splitting (Figure 3a). Compared to $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ the mean cosine similarity is 0.04 higher (bootstrapped $95\\%$ CI $[0.037-0.043];$ ; compared to $\\mathrm{SAE_{e2e}}$ the difference is 0.166 $95\\%$ CI $[0.163-0.168])$ . We measure this for all runs in our Pareto frontiers and find that this difference is not explained by $\\mathrm{SAE}_{\\mathrm{local}}$ having more alive dictionary elements than e2e SAEs (Appendix A.5). ", "page_idx": 6}, {"type": "text", "text": "3.3.2 $\\mathbf{SAE_{e2e}}$ features are not robust across random seeds, but $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ and $\\bf S A E_{l o c a l}$ are ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We find that $\\mathrm{SAE_{local}s}$ trained with one seed learn similar features as $\\mathrm{SAE_{local}s}$ trained with a different seed (Figure 3b). The same is true for two $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}\\mathrm{s}$ . However, features learned by $S\\mathrm{AE}_{\\mathrm{e2e}}$ are quite different for different seeds. This suggests there are many different sets of $\\mathrm{SAE_{e2e}}$ features that achieve the same output distribution, despite taking different paths through the network. ", "page_idx": 6}, {"type": "text", "text": "3.3.3 $\\mathbf{SAE_{e2e}}$ and $\\mathbf{SAE_{e2e+ds}}$ features do not always align with $\\bf S\\bf A\\bf E_{\\mathrm{local}}$ features ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The cosine similarity plots between $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ (Figure 3c top) reveal that the average similarity between the most similar features is low, and includes a group of features that are very dissimilar. $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ learns features that are much more similar to $\\mathrm{SAE}_{\\mathrm{local}}$ , although the cosine similarity plot is bimodal, suggesting that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ learns a set of directions that very different to those identified by $\\mathrm{SAE}_{\\mathrm{local}}$ (Figure 3c bottom). ", "page_idx": 6}, {"type": "text", "text": "It is encouraging that $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ features are somewhat similar, since this indicates that $\\mathrm{SAE}_{\\mathrm{local}}\\mathrm{s}$ may serve as good initializations for training $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}\\mathrm{S}$ , reducing training time. ", "page_idx": 6}, {"type": "text", "text": "3.4 Interpretability of learned directions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Using the automated-interpretability library [Lin, 2024] (an adaptation of Bills et al. [2023]), we generate automated explanations of our SAE features by prompting gpt-4-turbo-2024-04- 09 [OpenAI et al., 2024] with five max-activating examples for each feature, before generating \u201cinterpretability scores\u201d by tasking gpt-3.5-turbo to use that explanation to predict the SAE feature\u2019s true activations on a random sample of 20 max-activating examples. For each SAE we generate automated-interpretabilty scores for a random sample of features $n=198$ to 201 per SAE). We then measure the difference between average interpretability scores. This interpretability score is an imperfect metric of interpretability, but it serves as an unbiased verification and is therefore useful for ensuring that we are not trading better training losses for significantly less interpretable features. ", "page_idx": 6}, {"type": "image", "img_path": "7txPaUpUnc/tmp/acd83881a08017a85b1f07f17e30ee4c1fb46d0f1bdf5ec3ca508c5498f7e0ea.jpg", "img_caption": ["Figure 3: Geometric comparisons for our set of GPT2-small layer 6 SAEs with similar CE loss increases (Table 1). For each dictionary element, we find the max cosine similarity between itself and all other dictionary elements. In 3a we compare to others directions in the same SAE, in 3b to directions in an SAE of the same type trained with a different random seed, in 3c to directions in the $\\mathrm{SAE}_{\\mathrm{local}}$ with similar CE loss increase. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For pairs of SAEs with similar $L_{0}$ (listed in Table 3), we find no difference between the average interpretability scores of $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ . If we repeat the analysis for pairs with similar CE loss increases, we find the $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ features to be more interpretable than $\\mathrm{SAE}_{\\mathrm{local}}$ features in Layers 2 $p=0.0053)$ and 6 ( $p=0.0005$ ) but no significant difference in layer 10. For additional automated-interpretability analysis, see Appendix A.7. ", "page_idx": 7}, {"type": "text", "text": "We also provide some qualitative, human-generated interpretations of some groups of features for different SAE types in Appendix G. Features from the SAEs in Table 2 and Table 3 can be viewed interactively at https://www.neuronpedia.org/gpt2sm-apollojt. ", "page_idx": 7}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Using sparse autoencoders and sparse coding in mechanistic interpretability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "When Elhage et al. [2022] identified superposition as a key bottleneck to progress in mechanistic interpretability, the field found a promising scalable solution in SAEs [Sharkey et al., 2022]. SAEs have since been used to interpret language models [Cunningham et al., 2023, Bricken et al., 2023, Bloom, 2024] and have been used to improve performance of classifiers on downstream tasks [Marks et al., 2024]. Earlier work by Yun et al. [2021] concatenated together the residual stream of a language model and used sparse coding to identify an undercomplete set of sparse \u2018factors\u2019 that spanned multiple layers. This echoes even earlier work that applied sparse coding to word embeddings and found sparse linear structure [Faruqui et al., 2015, Subramanian et al., 2017, Arora et al., 2018]. Similar to our work is Tamkin et al. [2023], who trained sparse feature codebooks, which are similar to SAEs, and trained them end-to-end. However, to achieve adequate performance, they needed to train the model parameters alongside the sparse codebooks. Here, we only trained the SAEs and left the interpreted model unchanged. ", "page_idx": 7}, {"type": "text", "text": "4.2 Identifying problems with and improving sparse autoencoders ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Although useful for mechanistic interpretability, current SAE approaches have several shortcomings. One issue is the functional importance of features, which we have aimed to address here. Some work has noted problems with SAEs, including Anders and Bloom [2024], who found that SAE features trained on a language model with a given context length failed to generalize to activations collected from activations in longer contexts. Other work has addressed \u2018feature suppression\u2019 [Wright and Sharkey, 2024], also known as \u2018shrinkage\u2019 [Jermyn et al., 2024], where SAE feature activations systematically undershoot the \u2018true\u2019 activation value because of the sparsity penalty. While Wright and Sharkey [2024] approached this problem using finetuning after SAE training, Jermyn et al. [2024] and Riggs and Brinkmann [2024] explored alternative sparsity penalties during training that aimed to reduce feature suppression (with mixed success). Farrell [2024], taking an approach similar to Jermyn et al. [2024], has explored different sparsity penalties, though here not to address shrinkage, but instead to optimize for other metrics of SAE quality. Rajamanoharan et al. [2024] introduce Gated SAEs, an architectural variation for the encoder which both addresses shrinkage and improves on the Pareto frontier of $L_{0}$ vs CE loss increase. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Methods for evaluating the quality of trained SAEs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "One of the main challenges in using SAEs for mechanistic interpretability is that there is no known \u2018ground truth\u2019 against which to benchmark the features learned by SAEs. Prior to our work, several metrics have been used, including: Comparison with ground truth features in toy data; activation reconstruction loss; $L_{1}$ loss; number of alive dictionary elements; similarity of SAE features across different seeds and dictionary sizes [Sharkey et al., 2022]; $L_{0}$ ; KL divergence (between the output distributions of the original model and the model with SAE activations) upon causal interventions on the SAE features [Cunningham et al., 2023]; reconstructed negative log likelihood of the model with SAE activations inserted [Cunningham et al., 2023, Bricken et al., 2023]; feature interpretability Cunningham et al. [2023] (as measured by automatic interpretability methods [Bills et al., 2023]); and task-specific comparisons [Makelov et al., 2024]. In our work, we use (1) $L_{0}$ , (2) number of alive dictionary elements, (3) the average KL divergence between the output distribution of the original model and the model with SAE activations, and (4) the reconstruction error of activations in layers that follow the layer where we replace the original model\u2019s activations with the SAE activations. ", "page_idx": 8}, {"type": "text", "text": "4.4 Methods for identifying the functional importance of sparse features ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In our work, we optimize for functional importance directly, but previous work measured functional importance post hoc using different approaches. Cunningham et al. [2023] used activation patching [Vig et al., 2020], a form of causal mediation analysis, where they intervened on feature activations and found the output distribution was more sensitive (had higher KL divergence with the original model\u2019s distribution) in the direction of SAE features than other directions, such as PCA directions. With the same motivation, Marks et al. [2024] use a similar, approximate, but more efficient, method of causal mediation analysis [Nanda, 2022, Sundararajan et al., 2017]. Unlike our work, these works use the measures of functional importance to construct circuits of sparse features. Bricken et al. [2023] used logit attribution, measuring the effect the feature has on the output logits. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we introduce end-to-end dictionary learning as a method for training SAEs to identify functionally important features in neural networks. By optimizing SAEs to minimize the KL divergence between the output distributions of the original model and the model with SAE activations inserted, we demonstrate that e2e SAEs learn features that better explain network performance compared to the standard locally trained SAEs. ", "page_idx": 8}, {"type": "text", "text": "Our experiments on GPT2-small and Tinystories-1M reveal several key findings. First, for a given level of performance explained, e2e SAEs require activating significantly fewer features per datapoint and fewer total features over the entire dataset. Second, $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ , which has additional loss terms for the reconstruction errors at downstream layers in the model, achieves a similar performance explained to $\\mathrm{SAE_{e2e}}$ while maintaining activations that follow similar pathways through later layers compared to the original model. Third, the improved efficiency of e2e SAEs does not come at the cost of interpretability, as measured by automated-interpretability scores and qualitative analysis. ", "page_idx": 8}, {"type": "text", "text": "These results suggest that standard, locally trained SAEs are capturing information about dataset structure that is not maximally useful for explaining the algorithm implemented by the network. By directly optimizing for functional importance, e2e SAEs offer a more targeted approach to identifying the essential features that contribute to a network\u2019s performance. ", "page_idx": 8}, {"type": "text", "text": "6 Impact statement ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This article proposes an improvement to methods used in mechanistic interpretability. Mechanistic interpretability, and interpretability broadly, promises to let us understand the inner workings of neural networks. This may be useful for debugging and improving issues with neural networks. For instance, it may enable the evaluation of a model\u2019s fairness or bias. Interpretability may relatedly be useful for improving the trust-worthiness of AI systems, potentially enabling AI\u2019s use in certain high stakes settings, such as healthcare, finance, and justice. However, increasing the trust-worthiness of AI systems may be dual use in that may also enable its use in settings such as military applications. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Evan Anders and Joseph Bloom. Examining language model performance with reconstructed activations using sparse autoencoders. https://www.lesswrong.com/posts/ 8QRH8wKcnKGhpAu2o/examining-language-model-performance-with-reconstructed, 2024.   \nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of word senses, with applications to polysemy. 2018. URL https://arxiv.org/abs/1601. 03764.   \nLukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb. com/. Software available from wandb.com.   \nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/ index.html, 2023.   \nJoseph Bloom. Open source sparse autoencoders for all residual stream layers of gpt2 small. https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/ open-source-sparse-autoencoders-for-all-residual-stream, 2024.   \nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. URL https://transformer-circuits.pub/ 2023/monosemantic-features/index.html.   \nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. URL https://arxiv.org/abs/2309.08600.   \nRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023.   \nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition, 2022.   \nEoin Farrell. Experiments with an alternative method to promote sparsity in sparse autoencoders. https://www.lesswrong.com/posts/cYA3ePxy8JQ8ajo8B/ experiments-with-an-alternative-method-to-promote-sparsity, 2024.   \nManaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah Smith. Sparse overcomplete word vector representations, 2015.   \nMatthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. Causal analysis of syntactic agreement mechanisms in neural language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1828\u20131843, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.144. URL https://aclanthology.org/2021.acl-long.144. ", "page_idx": 9}, {"type": "text", "text": "Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. ", "page_idx": 10}, {"type": "text", "text": "Adam Jermyn, Adly Templeton, Joshua Batson, and Trenton Bricken. Tanh penalty in dictionary learning. https://transformer-circuits.pub/2024/feb-update/index.html#: \\~:text $=$ handle%20dying%20neurons.-,Tanh%20Penalty%20in%20Dictionary% 20Learning,-Adam%20Jermyn%2C%20Adly, 2024. ", "page_idx": 10}, {"type": "text", "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. ", "page_idx": 10}, {"type": "text", "text": "Johnny Lin. Automatic interpretability. https://github.com/hijohnnylin/ automated-interpretability, 2024. ", "page_idx": 10}, {"type": "text", "text": "Johnny Lin and Joseph Bloom. Analyzing neural networks with dictionary learning, 2023. URL https://www.neuronpedia.org. Software available from neuronpedia.org. ", "page_idx": 10}, {"type": "text", "text": "Aleksandar Makelov, George Lange, and Neel Nanda. Towards principled evaluations of sparse autoencoders for interpretability and control. https://openreview.net/forum?id= MHIX9H8aYF, 2024. ", "page_idx": 10}, {"type": "text", "text": "Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. ", "page_idx": 10}, {"type": "text", "text": "Leland McInnes, John Healy, Nathaniel Saul, and Lukas Gro\u00dfberger. Umap: Uniform manifold approximation and projection. Journal of Open Source Software, 3(29):861, 2018. doi: 10.21105/joss.00861. URL https://doi.org/10.21105/joss.00861. ", "page_idx": 10}, {"type": "text", "text": "Neel Nanda. Attribution patching: Activation patching at industrial scale. https://www. neelnanda.io/mechanistic-interpretability/attribution-patching, 2022. ", "page_idx": 10}, {"type": "text", "text": "Neel Nanda and Joseph Bloom. Transformerlens. https://github.com/TransformerLensOrg/ TransformerLens, 2022. ", "page_idx": 10}, {"type": "text", "text": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. Gpt-4 technical report, 2024. ", "page_idx": 10}, {"type": "text", "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. ", "page_idx": 10}, {"type": "text", "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. ", "page_idx": 10}, {"type": "text", "text": "Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, J\u00e1nos Kram\u00e1r, Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders, 2024. ", "page_idx": 10}, {"type": "text", "text": "Logan Riggs and Jannik Brinkmann. Improving sae\u2019s by sqrt()-ing l1 and removing lowest activating features. https://www.lesswrong.com/posts/YiGs8qJ8aNBgwt2YN/ improving-sae-s-by-sqrt-ing-l1-and-removing-lowest, 2024. ", "page_idx": 10}, {"type": "text", "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders, Dec 2022. URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/ interim-research-report-taking-features-out-of-superposition. ", "page_idx": 10}, {"type": "text", "text": "Anant Subramanian, Danish Pruthi, Harsh Jhamtani, Taylor Berg-Kirkpatrick, and Eduard Hovy. Spine: Sparse interpretable neural embeddings, 2017. ", "page_idx": 10}, {"type": "text", "text": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks, 2017. ", "page_idx": 10}, {"type": "text", "text": "Alex Tamkin, Mohammad Taufeeque, and Noah D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks. 2023. ", "page_idx": 10}, {"type": "text", "text": "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber. Causal mediation analysis for interpreting neural nlp: The case of gender bias, 2020. ", "page_idx": 10}, {"type": "text", "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing, 2020. ", "page_idx": 11}, {"type": "text", "text": "Benjamin Wright and Lee Sharkey. Addressing feature suppression in saes, Feb   \n2024. URL https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/ addressing-feature-suppression-in-saes. Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors,   \n2021. URL https://arxiv.org/abs/2103.15949. ", "page_idx": 11}, {"type": "text", "text": "A Additional results on other layers and models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Pareto curves for SAEs at other layers ", "page_idx": 12}, {"type": "image", "img_path": "7txPaUpUnc/tmp/63def393aebcd5be36d38e36e5ba3edba999b8461b1a94c475ebd801547ccf69.jpg", "img_caption": [], "img_footnote": ["Figure 4: Performance of all SAE types on GPT2-small\u2019s residual stream at layers 2, 6 and 10. GPT2-small has a CE loss of 3.139 over our evaluation set. "], "page_idx": 12}, {"type": "text", "text": "A.2 Pareto curves for TinyStories-1M ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We also tested our methods on Tinystories-1M, a $1M$ parameter model trained on short, simple stories [Eldan and Li, 2023]. Figure 5 shows our key results generalising to the residual stream halfway through the model (before the $5^{\\mathrm{th}}$ of 8 layers). ", "page_idx": 13}, {"type": "text", "text": "Note that most of our Tinystories-1M runs were for $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE_{e2e}}$ , and we did not perform several of the analyses that we performed for GPT2-small elsewhere in this report. But the clear improvement in $L_{0}$ and alive_dict_elements vs CE loss increase was apparent for $\\mathrm{SAE_{e2e}}$ vs $\\mathrm{SAE}_{\\mathrm{local}}$ . More results can be found at https://api.wandb.ai/links/sparsify/yk5etolk. Future work would test that these results hold on more models of different sizes and architectures, as well as on SAEs trained not just on the residual stream. ", "page_idx": 13}, {"type": "image", "img_path": "7txPaUpUnc/tmp/63c216e733c05b7b6fbf5cdaa2a610bb782ee51d25addc28665f2675d09647db.jpg", "img_caption": ["Figure 5: Tinystories-1M runs comparing $\\mathrm{SAE}_{\\mathrm{local}}$ , $S\\mathrm{AE}_{\\mathrm{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ on the residual stream before the $5^{\\mathrm{th}}$ of 8 layers. Tinystories-1M has a CE loss of 2.306 over our evaluation set. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Comparison of runs with similar $L_{0}$ , CE loss increase, or number of alive dictionary elements ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 2: Comparison of runs with similar CE loss increase for each layer. $\\lambda$ represents the sparsity coefficient and GradNorm is the mean norm of all SAE weight gradients measured from 10k training samples onwards. ", "page_idx": 13}, {"type": "table", "img_path": "7txPaUpUnc/tmp/f1898f83c12411a3378f4eb38c0484346e66e767b6b8027a386a1badca94f3c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "7txPaUpUnc/tmp/2745ea26cd2693d14fface6702ce50392e6e5fd4f8194dcb25aafa2d15b74942.jpg", "table_caption": ["Table 3: Comparison of runs with similar $L_{0}$ for each layer. $\\lambda$ represents the sparsity coefficient and GradNorm is the mean norm of all SAE weight gradients measured from 10k training samples onwards. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7txPaUpUnc/tmp/b5b838752c8ef026aef5b6f08413cb94dcb44b094d8c66907ef9669e0fcafbc6.jpg", "img_caption": ["Figure 6: Reconstruction mean squared error (MSE) at later layers for our three SAEs with similar CE loss increase for layers 2, 6, and 10. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Downstream MSE for all layers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 6 shows that layers 2 and 10 also have the property that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ has a very similar reconstruction loss to $\\mathrm{SAE}_{\\mathrm{local}}$ at downstreams layers, and $\\mathrm{SAE_{e2e}}$ has a much higher reconstruction loss. ", "page_idx": 14}, {"type": "text", "text": "A.5 Feature splitting geometry ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 3.3.1 we showed that at layer 6 $,S\\mathrm{AE_{\\mathrm{local}}}$ is less orthogonal than $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ , indicating a higher level of feature splitting. In Figure 7 we extend the analysis to runs on other layers. ", "page_idx": 14}, {"type": "text", "text": "In almost all cases we find that $\\mathrm{SAE_{e2e}}$ contains the most orthogonal dictionaries, followed by $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ and then $\\mathrm{SAE}_{\\mathrm{local}}$ . Perhaps surprisingly, as the number of alive dictionary elements decrease for each SAE type, we see an increase in the mean of the within-SAE similarities, indicating less feature splitting. One hypothesis for this result is that the the orthogonality of the dictionary depends much more on the output performance (as measured by CE loss difference) or sparsity (as measured by $L_{0}$ ) of the model with the SAE than on the number of alive dictionary elements, though further analysis is needed. ", "page_idx": 14}, {"type": "text", "text": "A.6 Cross-type similarity at other layers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 3.3.2 we show that downstream and local SAEs have more similar decoder directions than e2e and local SAEs. In Figure 8 we show this is true for layers 2, 6, and 10. ", "page_idx": 14}, {"type": "image", "img_path": "7txPaUpUnc/tmp/90b41e5a2475f78f57e886e398b2538f6cc3f2f880c9833eb2cfdbc3dfaf7ae4.jpg", "img_caption": ["Figure 7: Mean over all SAE dictionary elements of the cosine similarity to the next-closest element in the same dictionary. Plotted against $L_{0}$ , CE loss increase, and number of alive dictionary elements for all SAE types on runs with a variety of sparsity coefficients for GPT2-small "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "7txPaUpUnc/tmp/59f3ccd93616fa7250da47be27c6c94d5127e3f09076c7b7d07d0a0bfec64038.jpg", "img_caption": ["Figure 8: For runs with similar CE loss increase in layers 2, 6, 10, for each $S\\mathrm{AE}_{\\mathrm{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ dictionary direction, we take the max cosine similarity over all $\\mathrm{SAE}_{\\mathrm{local}}$ directions. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.7 Auto-interpretability ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Section 3.4 we claim that when comparing auto-interpretability scores we find no difference between pairs of similar $L_{0}$ , but do find $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ is more interpretable than $\\mathrm{SAE}_{\\mathrm{local}}$ in layers 2 and 6. These results are presented in more detail in Figure 9 and Table 4. ", "page_idx": 16}, {"type": "image", "img_path": "7txPaUpUnc/tmp/c85eca22872a4cf63bb21b9b9ebfdf35651afe7bab5dfba329fd20b1783ffedd.jpg", "img_caption": ["Figure 9: Comparison of auto-interpretability scores between $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ for runs with similar $L_{0}$ (see Table 3) and similar CE loss increases (see Table 2). Error bars are a bootstraped $95\\%$ confidence interval for the true mean auto-interpretability scores. Measured on approximately $200(\\pm2\\bar{)}$ randomly selected features per dictionary. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 4: Estimates of the difference between the mean auto-interpretability scores for $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ (Figure 9). A positive difference indicates $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ is more interpretable. For each comparison we use bootstrapping to compute a $95\\%$ confidence interval and a two-tailed $\\mathbf{p}\\mathrm{.}$ -value that the means are equal. ", "page_idx": 16}, {"type": "table", "img_path": "7txPaUpUnc/tmp/8bf5f802e4c7399cb63a8306c1baca011356fe47051852eca54f6cec487cc248.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Analysis of reconstructed activations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We saw in Appendix A.4 that our e2e-trained SAEs are much worse at reconstructing the exact activation compared to locally-trained SAEs. We performed some initial analysis of why this is. ", "page_idx": 17}, {"type": "text", "text": "B.1 Scale ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A common problem with SAEs is \u201cfeature-supression\u201d, where the SAE output has considerably smaller norm than the input [Wright and Sharkey, 2024, Rajamanoharan et al., 2024]. We observe this as well, as shown in Figure 10 for an $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ in layer 6. Note the cluster of activations with original norm around 3000; these are the activations at position 0. ", "page_idx": 17}, {"type": "image", "img_path": "7txPaUpUnc/tmp/5b6733497622f434a6230ebba73e09771ca363c9aa546ac2aee8f135b48014c0.jpg", "img_caption": ["Figure 10: A scatterplot showing the $L_{2}$ -norm of the input and output activations for out $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ in layer 6. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "7txPaUpUnc/tmp/4c954c94eb9b703742bcaeba0aec1c1bc0efc05f5c55734d404a350a29417664.jpg", "table_caption": ["Table 5: $L_{2}$ Ratio for the SAEs of similar CE loss increase, as in Table 2. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We can measure suppression with the metric: ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{2}{\\mathrm{~Ratio}}=\\mathbb{E}_{x\\in{\\mathcal{D}}}{\\frac{||{\\hat{a}}(x)||}{||a(x)||}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is presented in Table 5 for all of the similar CE loss increase SAEs in Table 2. Generally, $\\mathrm{SAE_{e2e}}$ has the most feature-suppression. This is as layer-norm is applied to the residual stream before the activations are used, which can allow the network to re-normalize the downscaled activations and keep similar outputs. The downscaled activations will still disrupt the normal ratio between the residual stream before the SAE is applied and the outputs of future layers that are added to the residual stream. ", "page_idx": 17}, {"type": "text", "text": "B.2 Direction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Both $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ do significantly worse at reconstructing the directions of the original activations than $\\mathrm{SAE}_{\\mathrm{local}}$ (Figure 11). Note, however, that we are comparing runs with similar CE ", "page_idx": 17}, {"type": "text", "text": "loss increases. $\\mathrm{SAE}_{\\mathrm{local}}$ is the only one of the three that is trained directly on reconstructing these activations, and achieves this reconstruction with significantly higher average $L_{0}$ . ", "page_idx": 18}, {"type": "text", "text": "Overall, $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ and $\\mathrm{SAE_{e2e}}$ reconstruct the activation direction in the current layer similarly well, with $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ doing better at layer 6 and but worse at layer 10. ", "page_idx": 18}, {"type": "image", "img_path": "7txPaUpUnc/tmp/38b3c012b63e75efcd2f76fcd68825ea154977a00fd49b879b42ac82a814e578.jpg", "img_caption": ["Figure 11: Distribution of cosine similarities between the original and reconstructed activations, for our SAEs with similar CE loss increases (Table 2). We measure 100 sequences of length 1024. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 Explained variance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "How much of the reconstruction error seen earlier (Section 3.2) is due to feature shrinkage? One way to investigate this is to normalize the activations of the SAE output before comparing them to the original activation.6 In Figure 12, we compare the explained variance for the reconstructed activations of each type of SAE in layer 6, both with and without normalizing the activations first. Normalizing the activations greatly improves the explained variance of our e2e SAEs. Despite this, the overall story and relative shapes of the curves are similar. ", "page_idx": 18}, {"type": "text", "text": "C Effect of gradient norms on the number of alive dictionary elements ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "One of our goals is to reduce the total number of features needed over a dataset (i.e. the alive dictionary elements), thereby reducing the computational overhead of any method that makes use of these features. We showed in Figure 4 that $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ consistently use fewer dictionary elements for the same amount of performance when compared with $\\mathrm{SAE}_{\\mathrm{local}}$ . We also see that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ uses fewer elements than $\\mathrm{SAE_{e2e}}$ for layers 2 and 6 but not layer 10. ", "page_idx": 19}, {"type": "text", "text": "Notice in Table 2, however, that the number of alive dictionary elements is negatively correlated with the norm of the gradients during training. This begs the question: If we increase the learning rate, is it possible to maintain performance in $L_{0}$ vs CE loss increase while also decreasing the number of alive dictionary elements? ", "page_idx": 19}, {"type": "image", "img_path": "7txPaUpUnc/tmp/79fc9e3e28d183c06cbcff8be02777abafab77a1b1835b65ba9c32db8f4332e0.jpg", "img_caption": ["Figure 13: Varying the learning rate for $\\mathrm{SAE}_{\\mathrm{local}}$ on layers 2, 6 and 10. All other parameters are the same as the local runs listed in the similar CE loss increase Table 2. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "In Figures 13, 14, 15, we show the effect that varying the learning rate has on performance for $\\mathrm{SAE}_{\\mathrm{local}}$ , $\\mathrm{SAE_{e2e}}$ , and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ , respectively. In all cases, we see that learning rates higher than our default of 0.0005 require fewer dictionary elements for the same level of performance on CE loss increase. We also see that these runs with higher learning rates (up to a limit) can have a better $L_{0}$ vs CE loss increase frontier at high sparsity levels and is similar or worse at low sparsity levels. ", "page_idx": 19}, {"type": "image", "img_path": "7txPaUpUnc/tmp/44caa67ed852bc6f0486399bacaac34663881bb7c768f1782e41411104361ea7.jpg", "img_caption": ["Figure 14: Varying the learning rate for $S\\mathrm{AE}_{\\mathrm{e2e}}$ on layers 2, 6 and 10. All other parameters are the same as the local runs listed in the similar CE loss increase Table 2. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "This effect appears to be more pronounced for $\\mathrm{SAE}_{\\mathrm{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ than $\\mathrm{SAE}_{\\mathrm{local}}$ , indicating that e2e SAEs may require even fewer alive dictionary elements compared to $\\mathrm{SAE_{local}s}$ than what is presented in the figures in the main text. ", "page_idx": 20}, {"type": "text", "text": "While not shown in these figures, a downside of using learning rates larger than 0.0005 is that it can cause the $L_{0}$ metric to steadily increase during training after an initial period of decreasing. This occurred for all of our SAE types, and was especially apparent in later layers. Due to this instability, we persisted with a learning rate of 0.0005 for our main experiments. We expect that training tweaks such as using a sparsity schedule could help remedy this issue and allow for using higher learning rates. ", "page_idx": 20}, {"type": "image", "img_path": "7txPaUpUnc/tmp/7647191e396c4436e29668178ac011ae4907928f7cc7518b6318838381d710e7.jpg", "img_caption": ["Figure 15: Varying the learning rate for $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ on layers 2, 6 and 10. All other parameters are the same as the local runs listed in the similar CE loss increase Table 2. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D Experimental details and hyperparameters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our architectural and training design choices were selected with the goal of maximizing $L_{0}$ vs CE loss increase Pareto frontier of $\\mathrm{SAE}_{\\mathrm{local}}$ . We then used the same design choices for $\\mathrm{SAE_{e2e}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ . Much of our design choice iteration took place on the smaller Tinystories-1m due to time and cost constraints. ", "page_idx": 22}, {"type": "text", "text": "Our SAE encoder and decoder both have a regular, trainable bias, and use Kaiming initialization. To form our dictionary elements, we transform our decoder to have unit norm on every forward pass. We do not employ any resampling techniques [Bricken et al., 2023] as it is unclear how these methods affect the types of features that are found, especially when aiming to find functional features with e2e training. We clip the gradients norms of our parameters to a fixed value (10 for GPT2-small). This only affects the very large grad norms at the start of training and the occasional spike later in training. We do not have strong evidence that this is worthwhile to do on GPT2-small, and it does comes at a computational cost. ", "page_idx": 22}, {"type": "text", "text": "We train for $400\\mathbf{k}$ samples of context size 1024 on Open Web Text with an effective batch size of 16. We use a learning rate of $5e-4$ , with a warmup of 20k samples, a cosine schedule decaying to $10\\%$ of the max learning rate, and the Adam optimizer [Kingma and Ba, 2017] with default hyperparameters. ", "page_idx": 22}, {"type": "text", "text": "For $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ , we multiply our KL loss term by a value of 0.5 in our implementation. Note that if we instead fixed this value to 1 and varied the other loss coefficients, we would also need to vary other coefficients such as learning rate and effective batch size accordingly, which may have been difficult. This said, fixing this parameter to 1 and having fewer overall hyperparemeters may be a better option going forward, as it turns out to be difficult to tune the other coefficients in this setting anyway. We set the total_coeff (i.e., the coefficient that multiplies the downstream reconstruction MSE, denoted $\\beta$ in Equation 1) to 2.5 for layers 2 and 6, and to 0.05 for layer 10. Note from Equation 1 that this coefficient gets split evenly among all downstream layers. It\u2019s likely that a different weighting of these parameters is more desirable, but we did not explore this for this report. ", "page_idx": 22}, {"type": "text", "text": "It\u2019s worth noting that we did not iterate heavily on loss function design for $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ , so it\u2019s likely that other configurations have better performance (e.g. having different downstream reconstruction loss coefficients depending on the layer, and/or including the reconstruction loss at the layer containing the SAE). ", "page_idx": 22}, {"type": "text", "text": "Note that in our loss formulation (Section 2), we divide our sparsity coefficient $\\lambda$ by the size of the residual stream $\\dim(a^{(l)}(x))$ . This is done in an attempt to make our sparsity coefficient robust to changes in model size. The idea is that the $L_{1}$ score for an optimal SAE will be a function of the size of the residual stream. However, we did not explore this relationship in detail and expect that other functions of residual stream size (and perhaps dictionary size) are more suitable for scaling the sparsity coefficient. ", "page_idx": 22}, {"type": "text", "text": "For GPT2-small we stream the dataset https://huggingface.co/datasets/ apollo-research/Skylion007-openwebtext-tokenizer-gpt2 which is a tokenized version of OpenWebText ([Gokaslan and Cohen, 2019]) (released under the license CC0-1.0). The tokenization process is the same as was used in GPT2 training, with a \u2018BOS\u2019 token between documents. ", "page_idx": 22}, {"type": "text", "text": "We evaluate our models on 500 samples of the Open Web Text dataset (a different seed to that used for training). We consider a dictionary element alive if it activates at all on 500k training tokens. ", "page_idx": 22}, {"type": "text", "text": "Note that information from all of our runs are accessible in this Weights and Biases ([Biewald, 2020]) report, including the weights, configs and numerous metrics tracked throughout training. The SAEs from these runs can be loaded and further analysed in our library https://github.com/ ApolloResearch/e2e_sae/. ", "page_idx": 22}, {"type": "text", "text": "We used NVIDIA A100 GPUs with 80GB VRAM (although the GPU was saturated when using smaller batch sizes that used 40GB VRAM or less). ", "page_idx": 22}, {"type": "text", "text": "Our library imports from the TransformerLens library [Nanda and Bloom, 2022] (released under the MIT License), which is used to download models via HuggingFace\u2019s Transformers library [Wolf et al., 2020] (released under the Apache License 2.0). GPT2-small is released under the MIT license. The Tinystories-1M model is released under the Apache License 2.0 and it\u2019s accompanying dataset is released under CDLA-Sharing-1.0. ", "page_idx": 22}, {"type": "text", "text": "E Varying initial dictionary size and number of training samples ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Varying initial dictionary size ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Figure 16 we show the effect of varying the initial dictionary size for our layer 6 similar CE loss increase SAEs in Table 2. For all SAE types, we see $L_{0}$ vs CE loss increase improve with diminishing returns as the dictionary size is scaled up, capping out at a dictionary size of roughly 60. This comes at the cost of having more alive dictionary elements with increasing dictionary size. ", "page_idx": 23}, {"type": "text", "text": "It\u2019s worth mentioning that, after preliminary investigation on Tinystories-1M, it\u2019s possible to reduce the dictionary ratio to 5 times the residual stream and still achieve a good $L_{0}$ vs CE loss increase tradeoff, as well as reducing the number of alive dictionary elements. See this Weights and Biases report for details https://wandb.ai/sparsify/tinystories-1m-ratio/ reports/Scaling-dict-size-tinystories-blocks-4-layerwise--Vmlldzo3MzMzOTcw. ", "page_idx": 23}, {"type": "image", "img_path": "7txPaUpUnc/tmp/c48ee4c529adb1e150c912032b463aa896e6a9330ff816329d0ba7603f5f44a7.jpg", "img_caption": ["Figure 16: Sweep over the SAE dictionary size for layer 6 (where \u2018ratio\u2019 is the size of the initial dictionary divided by the residual stream size of 768). All other parameters are the same as in the similar CE loss increase runs in Table 2. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.2 Varying number of training samples ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Figure 17 we analyse the effect of varying the number of training samples for each SAE type on layer 6 of our similar CE loss increase SAEs. For $\\mathrm{SAE}_{\\mathrm{local}}$ , training for 50k samples is clearly insufficient. The difference between training on $200\\mathrm{k}$ , $400\\mathbf{k}$ , and $800\\mathbf{k}$ samples is quite minimal for both $L_{0}$ vs CE loss increase and alive_dict_elements vs CE loss increase. ", "page_idx": 24}, {"type": "text", "text": "For $\\mathrm{SAE_{e2e}}$ , we see improvements to $L_{0}$ vs CE loss increase when increasing from 50k to $800\\mathbf{k}$ samples but with diminishing returns. In contrast to $\\mathrm{SAE}_{\\mathrm{local}}$ , we see a steady improvement in alive_dict_elements vs CE loss increase as we increase the number of samples. Note that training $\\mathrm{SAE_{e2e}}$ or $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ for $800\\mathbf{k}$ samples takes approximately 23 hours on a single A100. ", "page_idx": 24}, {"type": "text", "text": "For $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ , the $L_{0}$ vs CE loss increase and alive_dict_elements vs CE loss increase improves up until $400\\mathbf{k}$ samples where performance maxes out. ", "page_idx": 24}, {"type": "image", "img_path": "7txPaUpUnc/tmp/9617a0c47eb8b8f30af76a14237ff89f175867ecadc68f04301f91611c622e32.jpg", "img_caption": ["Figure 17: Sweep over number of samples trained on layer 6. All other parameters are the same as in the similar CE loss increase runs in Table 2. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "F Robustness of features to different seeds ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We show in Figure 18 that, for a variety of sparsity coefficients and layers, our training runs are robust to the random seed. Note that the seed is responsible for both SAE weight initialization as well as the dataset samples used in training and evaluation. ", "page_idx": 25}, {"type": "image", "img_path": "7txPaUpUnc/tmp/6a0dabe221b68ef4684f6514e7c57092a7797774dc162d4d8327cecad18e11cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 18: A sample of SAEs for layers 2, 6 and 10 for all run types showing the robustness of SAE training to two different seeds. ", "page_idx": 25}, {"type": "text", "text": "G Analysis of UMAP plots ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To explore the qualitative differences between the features learned by $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ , we first visualize the SAE features using UMAP [McInnes et al., 2018] (Figures 19, 20). ", "page_idx": 26}, {"type": "text", "text": "G.1 UMAP of layer 6 SAEs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Although there is substantial overlap between the features from both types of SAE in the plot, there are some distinct regions that are dense with $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ features but void of $\\mathrm{SAE}_{\\mathrm{local}}$ , and vice versa. We look at the features in these regions along with features in other identified regions of interest such as small mixed clusters in layer 6 of GPT2-small in more detail. We label the regions of interest from A to G in Figure 19, and provide human-generated overview of these features below. Features from this UMAP plot can be explored interactively at https://www.neuronpedia.org/gpt2sm-apollojt. For each region, we also share links to lists of features in that region which go to an interactive dashboards on Neuronpedia. ", "page_idx": 26}, {"type": "image", "img_path": "7txPaUpUnc/tmp/533cee56a0fb99888d6aa0950a28067ed6cfb0a07623f7657bd01cf1a89e3dbe.jpg", "img_caption": ["Figure 19: UMAP plot of $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ features for layer 6 on runs with similar CE loss increase in GPT2-small. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Region A $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ features (18). $\\mathbf{SAE_{local}}$ features (91)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Many of these features appear to be late-context positional features, or miscellaneous tokens that only activate in particularly late context positions. It may be the case that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ has fewer positional features than local, as indicated by the $.8\\;\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ vs 91 $\\mathrm{SAE}_{\\mathrm{local}}$ local features in this region (and similar in surrounding reasons). This said, we have not ruled out whether positional features for $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ are found elsewhere in the UMAP plot. ", "page_idx": 27}, {"type": "text", "text": "Region B $\\mathbf{SAE_{e2e+ds}}$ features (48). $\\bf{S A E_{l o c a l}}$ features (2)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This region mostly contains features which activate on $<$ |endoftext $|>$ tokens, in addition to some newline and double newline. These are tokens that mark the beginning of a new context. Seemingly $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ contains many more distinct features for $<$ |endoftext $|>$ than $\\mathrm{SAE}_{\\mathrm{local}}$ . ", "page_idx": 27}, {"type": "text", "text": "Region C ( $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ features (20). $\\mathbf{SAE_{local}}$ features (31)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Region C potentially suggests more feature splitting happening in $\\mathrm{SAE}_{\\mathrm{local}}$ than $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ . For $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ , each feature activates most strongly on tokens \u201cby\u201d or \u201cfrom\u201d in a broad range of contexts. For $\\mathrm{SAE}_{\\mathrm{local}}$ , each feature activates most strongly in fine-grained contexts, such as \u201cgoes by\u201d vs \u201cled by\u201d vs \u201c. By\u201d vs \u201cstop by\u201d vs \u201c<media>, by author\u201d vs \u201cdespised by\u201d vs \u201covertaken by\u201d vs \u201cissued by\u201d vs \u201cstep-by-step / case-by-case / frame-by-frame\u201d vs \u201cPosted by\u201d vs \u201cDirected by\u201d vs \u201ckilled by\u201d vs \u201cby\u201d. ", "page_idx": 27}, {"type": "text", "text": "Region D ( $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ features (11). $\\mathbf{SAE_{local}}$ features (19)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "These features all activate on \u201cat\u201d in various contexts. As in Region C, the $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ features appear less fine-grained. Examples of $\\mathrm{SAE}_{\\mathrm{local}}$ features not present in $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ : \u201cAnnounced at\u201d or \u201cpresented at\u201d or \u201crevealed at\u201d feature (https://www.neuronpedia.org/gpt2-small/6-res_ scl-ajt/40197). \u201cAt\u201d in technical contexts (https://www.neuronpedia.org/gpt2-small/ 6-res_scl-ajt/34541). ", "page_idx": 27}, {"type": "text", "text": "Region E $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ features (3). $\\bf S A E_{\\mathrm{local}}$ features (67)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "All features appear to boost starting words which would come after a paragraph or a full stop to start a new idea, such as \u201cFinally\u201d, \u201cMoreover\u201d, \u201cSimilarly\u201d, \u201cFurthermore\u201d, \u201cRegardless\u201d, \u201cHowever\u201d and so on. They seem to be differentiated by perhaps activating in different contexts. For example https://www.neuronpedia.org/gpt2-small/6-res_scl-ajt/4284 activates on full stops and newlines in technical contexts so it can predict things like \u201cAdditionally\u201d, \u201cHowever\u201d, and \u201cSpecifically\u201d. On the other hand, https://www.neuronpedia.org/gpt2-small/6-res_ scl-ajt/13519 activates on full stops in baking recipes so it can predict things like \u201cThen\u201d, \u201cAfterwards\u201d, \u201cAlternatively\u201d, \u201cDepending\u201d and so on. ", "page_idx": 27}, {"type": "text", "text": "Region F ( $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ features (19). $\\mathbf{SAE_{local}}$ features (8)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "These seem mostly similar to Region E. It\u2019s not clear what distinguishes the regions looking at the feature dashboards alone. ", "page_idx": 27}, {"type": "text", "text": "Region G $\\mathbf{\\{SAE_{e2e+ds}}}$ features (41). $\\bf S A E_{\\mathrm{local}}$ features (71)) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The features in both SAEs seem to activate on fairly specific different words or phrases. There is no obvious distinguishing features. It\u2019s possible that $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ features tend to activate more specifically and on fewer tokens than the corresponding $\\mathrm{SAE}_{\\mathrm{local}}$ features. An example of this can be seen when comparing https://www.neuronpedia.org/gpt2-small/6-res_ scefr-ajt/13910 (a $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ feature), with https://www.neuronpedia.org/gpt2-small/ 6-res_scl-ajt/45568 (a $\\mathrm{SAE}_{\\mathrm{local}}$ feature). ", "page_idx": 27}, {"type": "text", "text": "G.2 UMAP of layer 2 and layer 10 SAEs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Figure 20, we show UMAP plots for layers 2 and 10. We interpret a single region from layer 10 in the next section. ", "page_idx": 27}, {"type": "image", "img_path": "7txPaUpUnc/tmp/b2565e9b68064e2b18c19d69540ed3a2badf0e8380f580ecf9966b0b8c94c805.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 20: UMAP of $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ features for layers 2 and 10 on runs with similar CE loss increase in GPT2-small. ", "page_idx": 28}, {"type": "text", "text": "G.3 Region H in layer 10 ( $\\mathbf{S}\\mathbf{A}\\mathbf{E_{e2e+ds}}$ features (2). $\\mathbf{SAE_{local}}$ features (593)) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "While some individual features in this region are interpretable, there is no obvious uniting theme semantically. There is, however, a geometric connection. In particular, these are features that point away from the 0th PCA direction in the original model\u2019s activations (Figure 21). ", "page_idx": 28}, {"type": "image", "img_path": "7txPaUpUnc/tmp/db8fd8ed09dd2998f700a4db52e9479d16fdba448c2dc20480dc00fb36d8857b.jpg", "img_caption": ["Figure 21: The UMAP plot for $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ and $\\mathrm{SAE}_{\\mathrm{local}}$ directions, with points colored by their cosine similarity to the 0th PCA direction. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "The 0th PCA direction is nearly exactly the direction of the outlier activations at position 0 (see also Appendix B). Activations in this direction are tri-modal, with large outliers at position 0 and smaller outliers at end-of-text tokens (Figure 22). ", "page_idx": 28}, {"type": "text", "text": "We can measure how well an SAE preserves a particular direction by measuring the correlation between the input and output components in that direction. Our $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ faithfully reconstructs the activations in this direction at position 0 $r=0.996)$ ), but not at other positions $r=0.262)$ . This is a particularly poor reconstruction compared to $\\mathrm{SAE}_{\\mathrm{local}}$ or other PCA directions (Figure 23a). ", "page_idx": 28}, {"type": "image", "img_path": "7txPaUpUnc/tmp/79955a6e664c0c6544f7d284fc298562161bd362ff598ff77dcf34591475cab2.jpg", "img_caption": ["Figure 22: A histogram of the 0th PCA component of the activations before layer 10. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "7txPaUpUnc/tmp/6fc3c0fb6677c6d911f122736ab7006f0e69c736f72b9ccd5bf9bfcb43a5a436.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "7txPaUpUnc/tmp/2073f2a554ce2c10c0edef69d15f33cf6bfd5a7975bc12fe75140678c0039fb1.jpg", "img_caption": ["Figure 23: For each PCA direction before layer 10 we measure two qualities. The first is how faithfully $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE}_{\\mathrm{e2e+ds}}$ reconstruct that direction by measuring correlation coefficient. The second is how functionallyimportant the direction is, as measured by how much the output of the model changes when resample ablating the direction. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "$\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ \u2019s poor reconstruction of the activations in this direction implies that the differences may not be functionally relevant. We can measure this by resample ablating the activation in this direction at all non-zero positions. This means we perform the following intervention in a forward hook: ", "page_idx": 29}, {"type": "equation", "text": "$$\na(x)_{i}\\gets a(x)_{i}-P a(x)_{i}+P a(x^{\\prime})_{j}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where $a(\\boldsymbol{x})_{i}$ is the activation at position $i>0$ , $a(\\boldsymbol{x}^{\\prime})_{j}$ is the resampled activation for a different input $x^{\\prime}$ and position $j>0$ , and $P$ is a projection matrix onto the 0th PCA direction. ", "page_idx": 29}, {"type": "text", "text": "After performing this ablation, the kl-divergence from the original activations is only 0.01. This difference is smaller than repeating the experiment for any other direction in the first $30~\\mathrm{PCA}$ directions (Figure 23b). ", "page_idx": 29}, {"type": "text", "text": "This means that the exact value of this component of the activation (at positions $>\\mathrm{~0~}$ ) is mostly functionally irrelevant for the model. $\\mathrm{SAE}_{\\mathrm{local}}$ still captures the direction faithfully, as it is purely trained to minimize MSE. While $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ fails to preserve this direction accurately, this seems to allow it to have a cleaner dictionary, avoiding $\\mathrm{SAE}_{\\mathrm{local}}$ \u2019s cluster of features that point partially away from this direction. ", "page_idx": 29}, {"type": "text", "text": "H Training time ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The training time for each type of SAE in GPT2-small is shown in Table 6. We see that e2e SAEs are $2{-}3.5\\mathrm{x}$ slower than $\\mathrm{SAE}_{\\mathrm{local}}$ . Note that one can reduce training time with little performance cost by training on fewer that $400\\mathbf{k}$ samples (Figure 17) and/or using an initial dictionary ratio of less than $60\\mathrm{x}$ the residual stream size (Figure 16). Using locally trained SAEs as initialization for e2e SAEs or training multiple SAEs at different layers concurrently are also possible solutions. ", "page_idx": 30}, {"type": "text", "text": "Table 6: Training times for different layers and SAE training methods using a single NVIDIA A100 GPU on the residual stream of GPT2-small at layer 6. All SAEs are trained on 400k samples of context length 1024, with a dictionary size of $60\\mathrm{x}$ the residual stream size of 768. ", "page_idx": 30}, {"type": "table", "img_path": "7txPaUpUnc/tmp/19a198844496c34e827929a6690ad7da6a4b5323d3e52f637a98afe6d2e5dcbf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "7txPaUpUnc/tmp/8a588cff58fdd3029b58b99e556d2dd0702174a6f00de8c766cec7a99b676370.jpg", "table_caption": ["Table 7: Faithfulness on subject-verb agreement when replacing the activations with SAE outputs. ", "(b) Similar L0 "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "I Faithfulness of SAEs on subject verb agreement task ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Our main evaluation metrics presented in Section 3 measure the functional importance of the features learned by the SAEs on the next-token language modeling task used to train the model. We also experimented with evaluating the SAEs on a downstream task: how faithfully the dictionaries represent the information the model uses to perform subject-verb agreement. This task is directly inspired by the analysis in Marks et al. [2024]. ", "page_idx": 31}, {"type": "text", "text": "I.1 Methodology ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We use datasets from Finlayson et al. [2021] with 4 variations of a subject-verb agreement task: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Simple: The parent/s is/are \u2022 Across participle phrase (PP): The secretary/secretaries near the cars has/have \u2022 Within relative clause (RC): The athlete that the manager/managers likes/like \u2022 Across RC: The athlete/athletes that the managers like do/does ", "page_idx": 31}, {"type": "text", "text": "For each template, we use 1000 datapoints with different subjects and verbs. For each input, we can compute the logit difference that the model assigns to the correct and incorrect forms of the verb. ", "page_idx": 31}, {"type": "text", "text": "Following Marks et al. [2024] we compute the faithfulness of this logit difference when intervening on the network\u2019s activations. Let $m$ represent the mean logit difference between the correct and incorrect verb forms across the dataset. Let $M$ be the original model and $\\tilde{M}$ be the model under some intervention. We measure the faithfulness of the intervention as $\\frac{m(\\tilde{M})-m(\\mathcal{D})}{m(M)-m(\\mathcal{D})}$ where $\\mathcal{Q}$ represents ablating the entire residual stream. ", "page_idx": 31}, {"type": "text", "text": "A faithfulness of $0\\%$ thus means $\\tilde{M}$ performs no better than random, while a faithfulness of $100\\%$ means the intervention does not change performance. Faithfulness numbers greater than $100\\%$ mean the model is, on average, more confident in the correct verb with the intervention than without. ", "page_idx": 31}, {"type": "text", "text": "I.2 Faithfulness with complete SAEs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We first test the faithfulness of the models with the SAEs inserted (Table 7). An SAE which preserves functionally relevant features in the activations would have faithfulness close to $100\\%$ on all tasks the model is trained to perform. ", "page_idx": 31}, {"type": "text", "text": "While all SAEs preserve most of the logit-difference, there is significant variation across SAE types, layers, and tasks. The local SAEs in layer 10 have the worst faithfulness, although $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{ds}}$ in layer 6 also has poor faithfulness across participial phrases. ", "page_idx": 31}, {"type": "text", "text": "I.3 Faithfulness with a small number of features ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We are also interested in if end-to-end training helps concentrate the functional relevance into a few specific SAE features. We thus ranked SAE features in Layer 6 Similar $L_{0}$ SAEs by their indirect ", "page_idx": 31}, {"type": "image", "img_path": "7txPaUpUnc/tmp/f4844d4c65571031adc0984a2cad5e8586d1eb91f88fab6589a3706555287f8f.jpg", "img_caption": ["Number of SAE features preserved "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 24: Faithfulness when mean-ablating all SAE features except the top $k$ with largest indirect effect.   \nExperiments done with the similar $L_{0}$ SAEs on layer 6. ", "page_idx": 32}, {"type": "text", "text": "effect, measured by the change in faithfulness score when mean ablating that feature.7 We then mean ablate all SAE features except the $k$ with largest indirect effect (Figure 24). We see $\\mathrm{SAE}_{\\mathrm{e2e}}$ needs comparatively few features to achieve high faithfulness scores, but $\\mathrm{SAE}_{\\mathrm{local}}$ and $\\mathrm{SAE}_{\\mathrm{e}2\\mathrm{e}+\\mathrm{d}\\mathrm{s}}$ have roughly similar curves. ", "page_idx": 32}, {"type": "text", "text": "I.4 Discussion ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Despite certain (SAE type, layer) combinations showing superior performance, there are no clear patterns between SAE types across tasks. These results indicate that e2e SAEs do not provide an obvious benefit on the selected downstream tasks. While e2e SAEs demonstrate benefits for the language modeling task on the full OpenWebText distribution, further work would be needed to find specific tasks and sub-distributions in which they provide the most benefit. Or, perhaps these sort of task-specific results are very noisy and it would be necessary to aggregate across many tasks and templates to differentiate between SAE training methodologies. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All claims made are supported in the main text with additional support in the Appendices. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The main limitation, longer training times, is discussed in the results (Section 3) and Appendix H. We also mention in Appendix A.2 how the results may be strengthened if trained on a wider variety of model sizes and architectures. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No theoretical results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The core methods are laid out in Section 2.1, and detailed hyperparameters and design details are given in Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Repository attached to the submission (https://github.com/ ApolloResearch/e2e_sae) ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All details are presented in Appendix D ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide statistical significance testing and report confidence intervals across our geometrics comparisons and auto-interpretability. We also provide some results showing the robustness to seeds (Appendix F), though we do not run statistical significance testing on pareto differences between our methods over various seeds due to computational feasibility. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Details on compute resources given at the end of Appendix D and in Appendix H. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We conform to all aspects of the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See our impact statement (Section 6). ", "page_idx": 36}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not release data or models that have a high risk of misuse. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The sources and licenses of all dataset, model and core libraries used are given in Appendix D. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Codebase for reproducing results provided alongside this submission. No other assets provided. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: No crowdsourcing nor experiments with human subjects were undertaken. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: N/A ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}]