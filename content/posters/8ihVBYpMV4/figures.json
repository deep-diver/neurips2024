[{"figure_path": "8ihVBYpMV4/figures/figures_1_1.jpg", "caption": "Figure 1: An illustrative example of autoformalization. The mathematical statement from the MATH dataset is translated into a formal version by GPT-4. Only two formalization results (No.2 and No.3) are correct, while the others fail in the grounding (0.\\overline6 \u2192 2/3).", "description": "The figure shows an example of how GPT-4 translates a natural language mathematical statement into a formal Isabelle language version.  The prompt is to formalize the statement \"Find the product of 0.666... and 6. The answer is 4.\"  GPT-4 generates four different Isabelle code versions as possible translations. Only two of these correctly represent the mathematical statement, showing the inherent challenge of accurately translating natural language into formal mathematical language, highlighting the unreliability of the grounding step (converting recurring decimals to their fractional equivalents).", "section": "1 Introduction"}, {"figure_path": "8ihVBYpMV4/figures/figures_1_2.jpg", "caption": "Figure 2: Pass@k curves for GPT-4 autoformalization on the MATH (left) and miniF2F (right) datasets. The results show that LLMs can achieve higher coverage of correct formal statements with an increasing number of generated candidates up to k = 10. Beyond this point, the improvement gradually diminishes as k continues to increase.", "description": "This figure displays two graphs illustrating the pass@k accuracy for GPT-4 on the MATH and miniF2F datasets.  The x-axis represents the number of generations (k), and the y-axis shows the pass@k accuracy.  The left graph shows MATH dataset results, and the right graph shows miniF2F dataset results.  Both graphs show a clear upward trend in accuracy as the number of generations increases, but the rate of improvement slows significantly after around k=10.", "section": "1 Introduction"}, {"figure_path": "8ihVBYpMV4/figures/figures_3_1.jpg", "caption": "Figure 3: The overview of our autoformalization framework. In the framework, symbolic equivalence is constructed among formalized statements, and semantic consistency is computed between the informalized statements and the original statement. The scores from these two evaluations are combined to rank and select the final formalization results.", "description": "This figure illustrates the workflow of the proposed autoformalization framework.  It starts with a mathematical statement in natural language.  The framework then uses LLMs to generate multiple formalization candidates in a formal language. Symbolic equivalence compares these candidates to identify logically equivalent ones, and semantic consistency measures the similarity between re-informalized candidates and the original statement using embeddings.  Finally, scores from both methods are combined to select the best formalization result.", "section": "3 Methodology"}, {"figure_path": "8ihVBYpMV4/figures/figures_7_1.jpg", "caption": "Figure 4: Performance curve of log-comb for different values of \u03b1. The formalization results are generated by GPT-4. The results show that the combination can further improve the autoformalization accuracy with a large sweet spot.", "description": "This figure shows the performance of the log-combination strategy for various values of \u03b1 (alpha), a hyperparameter controlling the trade-off between symbolic equivalence and semantic consistency.  The x-axis represents different values of \u03b1, ranging from 0 to 1. The y-axis shows the 1@k accuracy (percentage of problems correctly formalized among the top k generated results), achieved by GPT-4. The curve demonstrates that a specific range of \u03b1 values yields significantly higher 1@k accuracy than other values, suggesting a synergistic relationship between the two self-consistency methods.  The optimal range of \u03b1 for improved performance is highlighted.", "section": "3.3 Combination of Two Scores"}, {"figure_path": "8ihVBYpMV4/figures/figures_7_2.jpg", "caption": "Figure 5: The performance of our proposed combination strategy (log-comb) on the MATH (left) and miniF2F (right) datasets. The results show that the log-comb further boost the autoformalization performance across various LLMs on the two datasets.", "description": "This figure displays the performance of the proposed log-comb combination strategy on two datasets, MATH and miniF2F.  The results demonstrate that log-comb consistently improves the autoformalization accuracy for various LLMs. The left panel shows the results on the MATH dataset, while the right panel shows the results on the miniF2F dataset. The y-axis represents the 1@k accuracy, while the x-axis lists the LLMs. The bars for each LLM represent the performance of SemCo, SymEq, and log-comb respectively. The results highlight the synergistic effect of combining symbolic equivalence and semantic consistency to boost autoformalization performance.", "section": "4.2 Empirical Results"}, {"figure_path": "8ihVBYpMV4/figures/figures_8_1.jpg", "caption": "Figure 6: Performance of SymEq using different ATP settings, with formalization results generated by GPT-4. The results indicate that the performance improvement is very narrow by increasing the capability of ATPs.", "description": "This figure shows the performance of the SymEq method (symbolic equivalence) using different ATP (Automated Theorem Prover) settings. The formalization results were generated using the GPT-4 language model.  The key finding is that increasing the power of the ATP doesn't significantly improve the performance of SymEq, suggesting that the limitations of the approach lie elsewhere, and not solely in the capabilities of the theorem prover.", "section": "4.2 Empirical Results"}, {"figure_path": "8ihVBYpMV4/figures/figures_16_1.jpg", "caption": "Figure 7: Performance curves of linear- (left) and quad- (right) comb across various values of \u03b1. The formalization results are generated by GPT-4. The results show that both combination strategies successfully improve autoformalization accuracy, while the effective range of quad-comb is smaller.", "description": "This figure displays the performance curves for linear and quadratic combination strategies, varying the hyperparameter \u03b1 (alpha) from 0 to 1.  The curves represent the 1@k accuracy (the proportion of tasks where at least one of the top-k generated formalizations is correct) achieved by each combination method when applied to GPT-4's autoformalization results.  The figure highlights that both methods generally improve accuracy but that the quadratic method has a narrower range of effective alpha values compared to the linear method.", "section": "4.2 Empirical Results"}]