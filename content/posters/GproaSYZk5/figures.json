[{"figure_path": "GproaSYZk5/figures/figures_3_1.jpg", "caption": "Figure 1: Compilation of an LSRL program to a Linear RNN. An example of a simple LSRL program that takes a sequence of 0s and 1s as an input and outputs 1 if there have been more 1s than 0s and 0 otherwise. The LSRL compiler follows the rules in App. A to simplify the computation DAG into a path graph. The resulting path graph can be represented as a Linear RNN with one layer.", "description": "This figure demonstrates how a simple Linear State Recurrent Language (LSRL) program can be compiled into a Linear Recurrent Neural Network (RNN). The LSRL program checks if there are more 1s than 0s in the input sequence and outputs 1 if true, 0 otherwise. The compilation process simplifies the Directed Acyclic Graph (DAG) representation of the program into a linear path, which directly maps to a single-layer Linear RNN architecture.  The figure visually illustrates the program's structure, the transformation steps, and the resulting RNN with its node types (Input, Concat, ReLU, Linear, LinState).", "section": "3 Linear State Recurrent Language (LSRL)"}, {"figure_path": "GproaSYZk5/figures/figures_4_1.jpg", "caption": "Figure 2: Intuition behind the LSRL program for universal in-context approximation for continuous functions in Lst. 1. Our target function f has input dimension din = 2 and output dimension dout = 1. Each input dimension is split into two parts, hence 8 = 1/2. We illustrated an example input sequence of length 5: one for the query and four for the prompt tokens corresponding to each of the discretisation cells. The query (q1, q2) falls in the cell corresponding to the third prompt token. We show how the two LinState variables in the program are updated after each step. Most notably, how the state holding the output y is updated after p3 is processed.", "description": "This figure shows how a simple LSRL program approximates a continuous function. The program discretizes the input space into cells and uses prompt tokens to represent the function's value in each cell.  The query is processed, and the program updates its internal state (LinState variables) based on whether the query falls within a cell and if so, adds the corresponding function value to the output state. The figure illustrates this process step-by-step for a 2D input, 1D output function.", "section": "4.1 Approximating continuous functions in Cvec"}, {"figure_path": "GproaSYZk5/figures/figures_5_1.jpg", "caption": "Figure 2: Intuition behind the LSRL program for universal in-context approximation for continuous functions in Lst. 1. Our target function f has input dimension din = 2 and output dimension dout = 1. Each input dimension is split into two parts, hence 8 = 1/2. We illustrated an example input sequence of length 5: one for the query and four for the prompt tokens corresponding to each of the discretisation cells. The query (q1, q2) falls in the cell corresponding to the third prompt token. We show how the two LinState variables in the program are updated after each step. Most notably, how the state holding the output y is updated after p3 is processed.", "description": "This figure illustrates how a simple LSRL program approximates a continuous 2D function. The input space is discretized into cells, and each cell's function value is encoded in the prompt. The query is processed, and the program finds the cell containing the query, outputting the corresponding function value. The figure shows the state updates of the LinState variables in the program.", "section": "4.1 Approximating continuous functions in Cvec"}, {"figure_path": "GproaSYZk5/figures/figures_7_1.jpg", "caption": "Figure 3: Intuition behind the LSRL program for universal in-context approximation for discrete functions in Lst. 2. Our keys and values have length n=3 and represent countries and capitals, e.g., AUStria\u2192VIEnna, BULgaria\u2192SOFia, and so on. The query is CAN for Canada and the final n outputs are OTT (Ottawa). We show the values of some of the variables in Lst. 2 at each step, with the LinState variables being marked with arrows. For cleaner presentation we are tokenizing letters as 0?, 1A, 2B, etc. Vertical separators are for illustration purposes only.", "description": "This figure illustrates how the LSRL program (Listing 2) works for approximating discrete functions. The program receives a query (CAN for Canada) and a sequence of key-value pairs representing country-capital mappings. The figure demonstrates how the program updates its internal state variables to identify the correct key (CAN) in the prompt and output the corresponding value (OTT for Ottawa).  The visualization simplifies token representation for clarity.", "section": "4 Universal In-Context Approximation with Linear RNNs"}, {"figure_path": "GproaSYZk5/figures/figures_8_1.jpg", "caption": "Figure 4: Robustness of the various f_ifelse implementations to model parameter noise. We show how the performance of the two universal approximation programs in Lsts. 1 and 2 deteriorates as we add Gaussian noise of various magnitudes to the non-zero weights of the resulting compiled models. As expected, the original f_ifelse implementation in Eq. (7) exhibits numerical precision errors at the lowest noise magnitude. For the token sequence case, numerical precision errors are present in all samples even in the no-noise setting. Hence, the original f_ifelse implementation is less numerically robust while the implementations with multiplicative gating are the most robust. For Lst. 1 (approximating Cvec) we report the Euclidean distance between the target function value and the estimated one over 10 queries for 25 target functions. For Lst. 2 we report the percentage of wrong token predictions over 5 queries for 25 dictionary maps. Lower values are better in both cases.", "description": "This figure shows the impact of adding Gaussian noise to the model parameters on the performance of different implementations of the conditional operator. The results indicate that the original implementation is highly sensitive to noise, while versions with multiplicative gates demonstrate significantly improved robustness.  The performance is measured by the average difference from target function values for continuous function approximation and wrong tokens for functions on token sequences.", "section": "Stable Universal In-Context Approximation with Gated Linear RNNs"}, {"figure_path": "GproaSYZk5/figures/figures_14_1.jpg", "caption": "Figure 1: Compilation of an LSRL program to a Linear RNN. An example of a simple LSRL program that takes a sequence of 0s and 1s as an input and outputs 1 if there have been more 1s than 0s and 0 otherwise. The LSRL compiler follows the rules in App. A to simplify the computation DAG into a path graph. The resulting path graph can be represented as a Linear RNN with one layer.", "description": "This figure demonstrates how a simple Linear State Recurrent Language (LSRL) program is compiled into a Linear Recurrent Neural Network (RNN).  The LSRL program is designed to take a sequence of binary inputs (0s and 1s) and output 1 if the cumulative sum of 1s exceeds the sum of 0s, and 0 otherwise. The figure shows the original directed acyclic graph (DAG) representation of the LSRL program, a simplified path graph version, and the equivalent one-layer Linear RNN.  This illustrates the compiler's role in transforming LSRL code into a recurrent neural network architecture.", "section": "3 Linear State Recurrent Language (LSRL)"}, {"figure_path": "GproaSYZk5/figures/figures_24_1.jpg", "caption": "Figure 2: Intuition behind the LSRL program for universal in-context approximation for continuous functions in Lst. 1. Our target function f has input dimension din = 2 and output dimension dout = 1. Each input dimension is split into two parts, hence 8 = 1/2. We illustrated an example input sequence of length 5: one for the query and four for the prompt tokens corresponding to each of the discretisation cells. The query (q1, q2) falls in the cell corresponding to the third prompt token. We show how the two LinState variables in the program are updated after each step. Most notably, how the state holding the output y is updated after p3 is processed.", "description": "This figure illustrates how the LSRL program for universal in-context approximation of continuous functions works.  It shows a 2D input space discretized into cells, with each cell represented by a prompt token in the input sequence. A query point falls within one of the cells, and the program identifies the corresponding prompt token to determine the function's output. The diagram tracks the changes in two internal LinState variables as the program processes the input sequence, highlighting how the output variable is updated.", "section": "4.1 Approximating continuous functions in Cvec"}]