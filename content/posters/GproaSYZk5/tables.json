[{"figure_path": "GproaSYZk5/tables/tables_6_1.jpg", "caption": "Figure 3: Intuition behind the LSRL program for universal in-context approximation for discrete functions in Lst. 2. Our keys and values have length n=3 and represent countries and capitals, e.g., AUStria\u2192VIEnna, BULgaria\u2192SOFia, and so on. The query is CAN for Canada and the final n outputs are OTT (Ottawa). We show the values of some of the variables in Lst. 2 at each step, with the LinState variables being marked with arrows. For cleaner presentation we are tokenizing letters as 0?, 1A, 2B, etc. Vertical separators are for illustration purposes only.", "description": "This figure illustrates the LSRL program's operation for discrete function approximation.  It uses a sequence of tokens representing countries and their capitals as input. The figure traces how the program's internal variables change over time, highlighting the process of query matching and value retrieval to demonstrate the in-context approximation of the function.", "section": "4 Universal In-Context Approximation with Linear RNNs"}, {"figure_path": "GproaSYZk5/tables/tables_24_1.jpg", "caption": "Figure 4: Robustness of the various f_ifelse implementations to model parameter noise. We show how the performance of the two universal approximation programs in Lsts. 1 and 2 deteriorates as we add Gaussian noise of various magnitudes to the non-zero weights of the resulting compiled models. As expected, the original f_ifelse implementation in Eq. (7) exhibits numerical precision errors at the lowest noise magnitude. For the token sequence case, numerical precision errors are present in all samples even in the no-noise setting. Hence, the original f_ifelse implementation is less numerically robust while the implementations with multiplicative gating are the most robust. For Lst. 1 (approximating Cvec) we report the Euclidean distance between the target function value and the estimated one over 10 queries for 25 target functions. For Lst. 2 we report the percentage of wrong token predictions over 5 queries for 25 dictionary maps. Lower values are better in both cases.", "description": "This figure displays the robustness of different  implementations of the conditional operator *f_ifelse* to parameter noise.  It shows how the performance of universal approximation programs degrades as Gaussian noise is added to model parameters. The original *f_ifelse* is less robust than implementations with multiplicative gating, especially with low noise magnitudes. Results are shown for both continuous functions and functions over token sequences, with performance measured by Euclidean distance and wrong token percentage, respectively.", "section": "5 Stable Universal In-Context Approximation with Gated Linear RNNs"}, {"figure_path": "GproaSYZk5/tables/tables_24_2.jpg", "caption": "Figure 4: Robustness of the various  f_ifelse  implementations to model parameter noise. We show how the performance of the two universal approximation programs in Lsts. 1 and 2 deteriorates as we add Gaussian noise of various magnitudes to the non-zero weights of the resulting compiled models. As expected, the original  f_ifelse  implementation in Eq. (7) exhibits numerical precision errors at the lowest noise magnitude. For the token sequence case, numerical precision errors are present in all samples even in the no-noise setting. Hence, the original  f_ifelse  implementation is less numerically robust while the implementations with multiplicative gating are the most robust. For Lst. 1 (approximating  Cvec ) we report the Euclidean distance between the target function value and the estimated one over 10 queries for 25 target functions. For Lst. 2 we report the percentage of wrong token predictions over 5 queries for 25 dictionary maps. Lower values are better in both cases.", "description": "This figure shows the impact of adding Gaussian noise to model parameters on the performance of two universal approximation programs.  The original implementation of a conditional operator (f_ifelse) shows significant numerical instability, especially at low noise levels, while versions using multiplicative gating are more robust. The results are presented for both continuous functions and functions over token sequences, measuring the error differently for each.", "section": "5 Stable Universal In-Context Approximation with Gated Linear RNNs"}, {"figure_path": "GproaSYZk5/tables/tables_24_3.jpg", "caption": "Figure 4: Robustness of the various f_ifelse implementations to model parameter noise. We show how the performance of the two universal approximation programs in Lsts. 1 and 2 deteriorates as we add Gaussian noise of various magnitudes to the non-zero weights of the resulting compiled models. As expected, the original f_ifelse implementation in Eq. (7) exhibits numerical precision errors at the lowest noise magnitude. For the token sequence case, numerical precision errors are present in all samples even in the no-noise setting. Hence, the original f_ifelse implementation is less numerically robust while the implementations with multiplicative gating are the most robust. For Lst. 1 (approximating Cvec) we report the Euclidean distance between the target function value and the estimated one over 10 queries for 25 target functions. For Lst. 2 we report the percentage of wrong token predictions over 5 queries for 25 dictionary maps. Lower values are better in both cases.", "description": "This figure displays the robustness experiments of different implementations of the f_ifelse function against parameter noise.  The experiments test two universal approximation programs (from Listings 1 and 2) using continuous functions and token sequences as inputs. The results, displayed as plots, show how different implementations of the f_ifelse functions (original, optimized, step-based, and multiplicative) degrade under different levels of Gaussian noise added to model parameters. It demonstrates the superior numerical stability of the multiplicative gating versions of the conditional operator.", "section": "5 Stable Universal In-Context Approximation with Gated Linear RNNs"}]