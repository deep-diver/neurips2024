{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is a foundation for many large language models and is extensively discussed in relation to in-context learning."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduced the concept of in-context learning, a key topic explored in the provided research paper."}, {"fullname_first_author": "Soham De", "paper_title": "RecurrentGemma: Moving past transformers for efficient open language models", "publication_date": "2024-04-01", "reason": "This paper introduced the concept of Linear Recurrent models, which are the main focus of the provided research paper."}, {"fullname_first_author": "Yihan Wang", "paper_title": "Universality and limitations of prompt tuning", "publication_date": "2023-12-01", "reason": "This paper explored the theoretical limits of in-context learning with transformers, a relevant comparison to the present paper's work on recurrent models."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023-12-01", "reason": "This paper examined the optimization process used by transformers during in-context learning, providing a theoretical context for the research."}]}