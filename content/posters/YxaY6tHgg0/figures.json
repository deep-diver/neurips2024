[{"figure_path": "YxaY6tHgg0/figures/figures_1_1.jpg", "caption": "Figure 1: We use an MLP layer as an example. Left: Regular pruning methods have to follow structural dependence thus their flexibility is limited. Right: Our dimension-independent structural pruning method breaks the structural dependence via index operations and thus largely improves the flexibility for pruning.", "description": "This figure illustrates the difference between regular structural pruning and the proposed dimension-independent structural pruning method.  In regular methods, the pruning process must respect the structural dependencies within the network (e.g., residual connections), limiting flexibility.  The proposed method breaks this dependence using index operations to enable each block to utilize varying subsets of the feature maps, significantly increasing pruning flexibility.", "section": "1 Introduction"}, {"figure_path": "YxaY6tHgg0/figures/figures_2_1.jpg", "caption": "Figure 2: Our method, DISP-LLM, applies different selection matrices to the input and output dimension of the Attention layer and MLP layer (S1/S2: Attention in/out; S3/S4/S5: MLP in/middle/out). When pruning the model, we add \u201cIndex Selection\u201d before Layer Norm and we replace addition with \"Index Add.\" S1,..., S5 are applied for pruning weight matrices.", "description": "This figure illustrates the DISP-LLM method, showing how different selection matrices (S1 to S5) are applied to the input and output dimensions of the attention and MLP layers within a transformer block.  It highlights the modifications made during the pruning process: index selection is added before layer normalization, and addition operations are replaced with index addition operations.  These changes break the structural dependence in standard pruning methods, enhancing flexibility. The figure shows the process both during the hypernetwork training phase and during the actual pruning phase.", "section": "3 Preliminary"}, {"figure_path": "YxaY6tHgg0/figures/figures_3_1.jpg", "caption": "Figure 3: Comparison of the projection matrices for structural pruning. We use Win and Wout in Fig. 1 as an example. Left: SliceGPT employs orthogonal projection matrices, and it has to insert the projection matrices into the residual connections. Middle: Regular structural pruning methods remove structures based on their dependence, requiring to use the unified selection matrix S for all blocks, which limits flexibility. Right: Our method breaks the structural dependence, allowing the use of different selection matrices Sin and Sout for the embedding dimension, significantly improving the flexibility of pruning.", "description": "This figure compares three different structural pruning methods: SliceGPT, regular structural pruning, and the proposed dimension-independent structural pruning (DISP-LLM). It highlights the key difference in how these methods handle projection matrices and structural dependence, illustrating the improved flexibility of DISP-LLM in selecting different subsets of features for different layers, without introducing additional parameters.", "section": "3 Preliminary"}, {"figure_path": "YxaY6tHgg0/figures/figures_7_1.jpg", "caption": "Figure 4: The pruned model architecture along the embedding dimension (model dimension) for the LLaMA-2 7B model when the pruning ratio equals 50%.", "description": "This figure visualizes the results of applying the DISP-LLM pruning method to the LLaMA-2 7B model at a 50% pruning ratio.  The left panel shows a heatmap representing the pruning decisions across both the embedding dimension (horizontal axis) and the depth of the model (vertical axis). Darker blue indicates preserved weights, while lighter teal indicates pruned weights. The right panel provides a histogram summarizing the pruning rate across the embedding dimension and shows the overall pruning ratio.", "section": "4 Dimension-Independent Large Language Model"}, {"figure_path": "YxaY6tHgg0/figures/figures_8_1.jpg", "caption": "Figure 4: The pruned model architecture along the embedding dimension (model dimension) for the LLaMA-2 7B model when the pruning ratio equals 50%.", "description": "This figure visualizes how the model's embedding dimension is pruned at a 50% pruning ratio for the LLaMA-2 7B model.  It shows which parts of the embedding dimension are preserved and which are pruned across different layers of the model, illustrating the dimension-independent nature of the proposed pruning method. The color coding represents the preserved and pruned elements in the embedding dimension, demonstrating the flexibility in pruning different parts of the embedding dimension for different layers.", "section": "4 Dimension-Independent Large Language Model"}, {"figure_path": "YxaY6tHgg0/figures/figures_8_2.jpg", "caption": "Figure 6: Model width after pruning for the LLaMA-2 7B model when the pruning ratio equals 50%.", "description": "This figure visualizes the width of each layer in the LLaMA-2 7B model after applying the proposed dimension-independent structural pruning method with a 50% pruning ratio.  The x-axis represents the layer number, and the y-axis shows the preserved rate (the proportion of features retained) for each layer.  Different colors represent different selection matrices (S1 to S5) used in different parts of the transformer block. The graph reveals the varying widths assigned to different layers after pruning, highlighting the flexibility of the proposed method. Some layers are pruned more aggressively than others.", "section": "4 Dimension-Independent Large Language Model"}, {"figure_path": "YxaY6tHgg0/figures/figures_13_1.jpg", "caption": "Figure 7: Left: SliceGPT inserts QlQl+1 to the residual connection and brings additional parameters. It also modifies the weights and Layer Norms within the original model. The selection matrix S is omitted for consistency. Right: Our method, DISP-LLM, applies different selection matrices to the input and output dimension of the Attention layer and MLP layer (S1/S2: Attention in/out; S3/S4/S5: MLP in/middle/out).", "description": "This figure compares the SliceGPT method and the proposed DISP-LLM method. SliceGPT adds projection matrices (Q) to the residual connection which increases the number of parameters. The DISP-LLM method, on the other hand, adds index selection and index addition operations which does not add extra parameters. Both methods use different selection matrices for different layers to improve flexibility.", "section": "3.3 Residual Connections Limit the Flexibility of Structural Pruning"}, {"figure_path": "YxaY6tHgg0/figures/figures_13_2.jpg", "caption": "Figure 4: The pruned model architecture along the embedding dimension (model dimension) for the LLaMA-2 7B model when the pruning ratio equals 50%.", "description": "This figure visualizes the pruning decisions made by the DISP-LLM method on the LLaMA-2 7B model when 50% of the parameters are pruned.  The heatmap shows the embedding dimension on the horizontal axis and the depth (number of layers) on the vertical axis. Each cell represents a parameter, and is colored blue if the parameter is preserved and teal if it's pruned. The figure demonstrates how DISP-LLM selectively prunes parameters across different layers and embedding dimensions, leading to a flexible and effective model compression technique.  Note that the lower portion of the figure shows more consistent pruning across the embedding dimension, while the upper portion exhibits more scattered pruning decisions.", "section": "4 Dimension-Independent Large Language Model"}, {"figure_path": "YxaY6tHgg0/figures/figures_13_3.jpg", "caption": "Figure 4: The pruned model architecture along the embedding dimension (model dimension) for the LLaMA-2 7B model when the pruning ratio equals 50%.", "description": "This figure visualizes the model architecture after applying the dimension-independent structural pruning method with a 50% pruning ratio on the LLaMA-2 7B model. It shows how different layers of the model retain varying numbers of features across the embedding dimension, highlighting the flexibility of this pruning technique. The preserved and pruned parts in each layer along the embedding dimension are clearly depicted.", "section": "4 Dimension-Independent Large Language Model"}, {"figure_path": "YxaY6tHgg0/figures/figures_14_1.jpg", "caption": "Figure 10: Expected compression rate vs. actual compression rate of our method and Slice-GPT on the LLaMA-7B model.", "description": "This figure compares the expected and actual compression rates achieved by DISP-LLM and SliceGPT for the LLaMA-7B language model. The x-axis represents the expected compression rate (percentage of parameters intended to be preserved), while the y-axis shows the actual compression rate achieved.  The figure visually demonstrates the discrepancy between the expected and actual compression rate in SliceGPT, highlighting a significant difference. In contrast, DISP-LLM exhibits a much closer alignment between expected and actual compression rates, indicating its improved accuracy and reliability in pruning.", "section": "A.5 Additional Results"}, {"figure_path": "YxaY6tHgg0/figures/figures_15_1.jpg", "caption": "Figure 5: The training dynamics when learning the hypernetwork are shown in Figs. 5a, 5b, 5e, 5f. The results of different settings are in Figs. 5c, 5d, throughput is in Fig. 5g, and cost is in Fig. 5h.", "description": "This figure shows the training dynamics of the hypernetwork used in the DISP-LLM method.  It illustrates the loss curves for both the language modeling loss (L) and regularization loss (R),  under different pruning ratios (p) and hyperparameter settings. The effect of using the hypernetwork versus not using it, and the impact of different hyperparameter choices are visually demonstrated.  Finally, the figure also displays the resulting throughput improvement and cost reduction achieved by the method.", "section": "5.4 Analysis"}, {"figure_path": "YxaY6tHgg0/figures/figures_15_2.jpg", "caption": "Figure 4: The pruned model architecture along the embedding dimension (model dimension) for the LLaMA-2 7B model when the pruning ratio equals 50%.", "description": "This figure visualizes how the proposed dimension-independent structural pruning method affects the LLaMA-2 7B model's architecture when 50% of its parameters are pruned.  The x-axis represents the embedding dimension, while the y-axis shows the preserved rate (proportion of parameters retained). The color gradient illustrates the pruning decisions across the embedding dimension and depth of the model, revealing which dimensions are more or less heavily pruned.  This demonstrates the method's flexibility in selecting subnetworks with varying widths along the embedding dimension, unlike traditional pruning approaches.", "section": "4 Dimension-Independent Large Language Model"}]