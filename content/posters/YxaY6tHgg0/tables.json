[{"figure_path": "YxaY6tHgg0/tables/tables_4_1.jpg", "caption": "Table 1: Perplexities of different structural pruning methods on WikiText-2. Our method is the only one that does not update model weights. SliceGPT does not directly update model weights, however, it applies orthogonal transformation matrices to the weights.", "description": "This table compares the perplexity scores achieved by various structural pruning methods on the WikiText-2 benchmark.  It showcases the performance of different methods across various Large Language Models (LLMs) at different pruning ratios. Notably, it highlights that the proposed DISP-LLM method achieves comparable or better results without updating the model weights, unlike other methods that do require weight updates.  SliceGPT, while not directly updating weights, uses orthogonal transformations, introducing additional parameters.", "section": "5 Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_6_1.jpg", "caption": "Table 1: Perplexities of different structural pruning methods on WikiText-2. Our method is the only one that does not update model weights. SliceGPT does not directly update model weights, however, it applies orthogonal transformation matrices to the weights.", "description": "This table presents the perplexity scores achieved by different structural pruning methods on the WikiText-2 dataset for several language models. The pruning methods are compared based on their performance at different pruning ratios (percentage of parameters removed).  The table highlights that the proposed DISP-LLM method is unique in not updating the model weights during pruning, which is a significant difference from other methods.  The results demonstrate the effectiveness of DISP-LLM even when compared to methods that do update model weights.", "section": "5 Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of our method against semi-structure pruning methods on WikiText-2.", "description": "This table compares the performance of the proposed DISP-LLM method against several semi-structured pruning methods on the WikiText-2 benchmark.  It shows the test perplexity (PPL) achieved by each method at a 50% pruning ratio. The table also indicates whether each method updates model weights during pruning and whether it maintains the original structure of the model.  The results highlight DISP-LLM's competitive performance compared to other methods, even without weight updates.", "section": "5 Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_7_1.jpg", "caption": "Table 3: Zero-shot performance of the compressed LLaMA 7B, LLaMA-2 7B and Phi models. The structure of DISP-LLM is based on the WikiText dataset, and the structure of DISP-LLM Alpaca is based on the Alpaca dataset.", "description": "This table presents the zero-shot performance results on five different tasks (WinoGrande, HellaSwag, ARC-e, ARC-C, and PIQA) for various compressed language models.  The models are compressed using different methods (DISP-LLM, LLM-Pruner, SliceGPT, K-OBD, and LLM Surgeon) at different pruning ratios (20% and 50%). The table shows the accuracy and accuracy normalized results for each method, task, and pruning ratio.  The DISP-LLM results are shown separately for models trained on WikiText and Alpaca datasets to highlight the impact of training data on performance.", "section": "5.3 Zero-shot Performance"}, {"figure_path": "YxaY6tHgg0/tables/tables_9_1.jpg", "caption": "Table 4: The impact of the Hypernetwork architecture on the Phi-1.5 model. Performance is measured by PPL (perplexity).", "description": "This table presents an ablation study on the impact of different hypernetwork architectures on the performance of the DISP-LLM method using the Phi-1.5 model. It compares the perplexity (PPL) scores achieved with three different hypernetwork configurations: one without a hypernetwork, one without Bi-GRU layers within the hypernetwork, and one with the full hypernetwork architecture. The comparison is made across different compression rates (0%, 10%, 20%, 30%, 40%, 50%).  The results show the impact of the hypernetwork design on the model's performance, highlighting the benefit of using the full hypernetwork.", "section": "5.4 Analysis"}, {"figure_path": "YxaY6tHgg0/tables/tables_15_1.jpg", "caption": "Table 1: Perplexities of different structural pruning methods on WikiText-2. Our method is the only one that does not update model weights. SliceGPT does not directly update model weights, however, it applies orthogonal transformation matrices to the weights.", "description": "This table compares the perplexity scores achieved by several structural pruning methods on the WikiText-2 benchmark across different LLMs (OPT and LLaMA).  It highlights the performance of the proposed DISP-LLM method against state-of-the-art methods, emphasizing DISP-LLM's unique characteristic of not updating model weights during pruning, unlike other methods which either update weights directly or indirectly through transformation matrices. The table shows perplexity results for various pruning ratios.", "section": "5 Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_15_2.jpg", "caption": "Table 6: Time costs of our method.", "description": "This table presents the time taken and the number of GPUs used to train the hypernetwork for different sized language models.  The models listed are LLaMA 7B and LLaMA 13B, and their corresponding LLaMA-2 versions. The training time varied depending on the model size, with the larger 13B models requiring substantially more time and GPUs.", "section": "5 Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_16_1.jpg", "caption": "Table 3: Zero-shot performance of the compressed LLaMA 7B, LLaMA-2 7B and Phi models. The structure of DISP-LLM is based on the WikiText dataset, and the structure of DISP-LLM Alpaca is based on the Alpaca dataset.", "description": "This table presents the zero-shot performance results on five different tasks (WinoGrande, HellaSwag, ARC-e, ARC-c, and PIQA) for three different language models (LLaMA 7B, LLaMA-2 7B, and Phi models) using the DISP-LLM method and comparing it to other state-of-the-art structural pruning methods.  The results are shown for different pruning ratios (20% and 50%) and indicate whether the model weights were updated during pruning. The \"DISP-LLM Alpaca\" results use a hypernetwork trained on the Alpaca dataset instead of the WikiText dataset.", "section": "5. Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_16_2.jpg", "caption": "Table 3: Zero-shot performance of the compressed LLaMA 7B, LLaMA-2 7B and Phi models. The structure of DISP-LLM is based on the WikiText dataset, and the structure of DISP-LLM Alpaca is based on the Alpaca dataset.", "description": "This table presents the zero-shot performance of several compressed large language models (LLMs) across five different tasks: WinoGrande, HellaSwag, ARC-e, ARC-C, and PIQA.  The LLMs are compressed using different methods, including DISP-LLM (the authors' method) and baseline methods like LLM-Pruner and SliceGPT.  Two versions of DISP-LLM are shown: one trained on the WikiText dataset and another trained on the Alpaca dataset. The table shows the accuracy (acc) and accuracy normalized by the dense model (acc-norm) for each model and task. The pruning ratios (20% and 50%) indicate the proportion of parameters removed during the compression process.", "section": "5.3 Zero-shot Performance"}, {"figure_path": "YxaY6tHgg0/tables/tables_17_1.jpg", "caption": "Table 9: Zero-shot performance of the compressed LLaMA 13B model.", "description": "This table presents the zero-shot performance results of the compressed LLaMA 13B model across various pruning methods.  It includes the average accuracy across five zero-shot tasks (WinoGrande, HellaSwag, ARC-e, ARC-c, PIQA) for different pruning ratios (0%, 20%, 50%).  The table shows results with and without fine-tuning and compares different pruning approaches (Magnitude, LLM Pruner - Channel, LLM Pruner - Block, and DISP-LLM).  The \"W Update?\" column indicates whether model weights were updated during the pruning process.  The results highlight the performance of DISP-LLM compared to other state-of-the-art methods, especially when considering that DISP-LLM does not update weights.", "section": "5.2 Language Modeling"}, {"figure_path": "YxaY6tHgg0/tables/tables_17_2.jpg", "caption": "Table 11: Throughput of the pruned model.", "description": "This table presents the throughput (Tokens/seconds) of the LLaMA-2 13B model under different pruning ratios (0%, 20%, 30%, 40%, 50%).  It shows how the speed of token processing changes as more parameters are removed from the model through pruning.  Higher numbers indicate faster processing.", "section": "5 Experiments"}, {"figure_path": "YxaY6tHgg0/tables/tables_17_3.jpg", "caption": "Table 12: Impact of \u03bb on Phi-1.5", "description": "This table shows the impact of the regularization parameter \u03bb on the performance of the Phi-1.5 model when 50% of parameters are pruned.  The results demonstrate that a \u03bb value of 6 or higher yields stable performance, whereas smaller \u03bb values result in unstable performance (indicated by 'NC' for non-convergence). This suggests that the appropriate magnitude of \u03bb is crucial for the effectiveness of the pruning method.", "section": "5.4 Analysis"}, {"figure_path": "YxaY6tHgg0/tables/tables_18_1.jpg", "caption": "Table 13: PPL vs. pruning ratio trade-off for the Phi-2 model.", "description": "This table presents the results of experiments conducted on the Phi-2 model to analyze the trade-off between perplexity (PPL) and different pruning ratios. It demonstrates how the model's performance, measured by PPL, changes as the pruning ratio increases from 10% to 50%.  The results are essential for understanding the impact of structural pruning on the model's accuracy and efficiency.", "section": "5.3 Zero-shot Performance"}, {"figure_path": "YxaY6tHgg0/tables/tables_18_2.jpg", "caption": "Table 14: Zero-shot task performance vs pruning ratio trade-off for the LLaMA-2 7B model with the WikiText dataset", "description": "This table shows the trade-off between the pruning ratio and the average zero-shot task accuracy for the LLaMA-2 7B model.  It demonstrates how performance changes as different percentages of parameters are pruned from the model using the WikiText dataset for evaluation.", "section": "5.2 Language Modeling"}, {"figure_path": "YxaY6tHgg0/tables/tables_18_3.jpg", "caption": "Table 3: Zero-shot performance of the compressed LLaMA 7B, LLaMA-2 7B and Phi models. The structure of DISP-LLM is based on the WikiText dataset, and the structure of DISP-LLM Alpaca is based on the Alpaca dataset.", "description": "This table presents the zero-shot performance results for three different large language models (LLaMA 7B, LLaMA-2 7B, and Phi models) after applying the proposed dimension-independent structural pruning method (DISP-LLM).  It compares the performance of the pruned models against several baselines across five different zero-shot tasks (WinoGrande, HellaSwag, ARC-e, ARC-C, and PIQA) at different pruning ratios (20% and 50%).  The table also shows results for DISP-LLM trained on the Alpaca dataset (DISP-LLM Alpaca), highlighting that the pruning strategy can be adapted to different training data.  The \"W Update?\" column indicates whether the model weights were updated during the pruning process.", "section": "5.2 Language Modeling"}, {"figure_path": "YxaY6tHgg0/tables/tables_19_1.jpg", "caption": "Table 1: Perplexities of different structural pruning methods on WikiText-2. Our method is the only one that does not update model weights. SliceGPT does not directly update model weights, however, it applies orthogonal transformation matrices to the weights.", "description": "This table compares the perplexity scores achieved by various structural pruning methods on the WikiText-2 dataset across different model sizes (OPT and LLaMA-2). It highlights that the proposed DISP-LLM method outperforms other state-of-the-art methods while not requiring model weight updates, a key advantage in terms of computational efficiency and ease of deployment.  It also shows the impact of pruning ratios on model performance.", "section": "5 Experiments"}]