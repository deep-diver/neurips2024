[{"figure_path": "5cIRdGM1uG/figures/figures_0_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "This figure compares different methods for achieving length generalization in the task of integer addition.  The x-axis represents the length of the operands used for addition. The y-axis shows the exact match accuracy. The figure shows that the proposed position coupling method significantly outperforms other methods, achieving over 95% accuracy even when tested on operands with lengths that are up to 6.67x longer than the training data. The figure also provides a comparison with NoPE (no positional embedding), random start APE (Absolute Positional Embedding with a random starting position ID), and index hinting (a method that uses positional markers in the input sequence).", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_3_1.jpg", "caption": "Figure 2: Position coupling for decimal integer addition task, displaying 653 + 49 = 702 with appropriate input formats. The starting position ID \u20186\u2019 is an arbitrarily chosen number.", "description": "This figure illustrates the position coupling method for the decimal integer addition task.  The input sequence is preprocessed using reversed response format, zero-padding and BOS/EOS tokens. The tokens are grouped into three categories: first operand and '+', second operand, and '=' and response (sum). The figure displays how position IDs are assigned to the same significance digits in each group. The same position ID is given to relevant digits to help the model to understand the structure of addition irrespective of the length of the input sequence.", "section": "3 Position Coupling: A Method for Length Generalization"}, {"figure_path": "5cIRdGM1uG/figures/figures_4_1.jpg", "caption": "Figure 3: Ablation on the trained operand lengths (1-layer 4-head models).", "description": "This figure shows the result of ablation studies on the trained operand lengths for 1-layer 4-head Transformer models.  The models were trained on integer addition problems with a maximum operand length varying from 10 to 40 digits, and their performance was evaluated on addition problems with operand lengths up to 200 digits. The graph shows the exact match accuracy (median over 8 runs) as a function of operand length, with different lines representing models trained on different operand lengths.  The shaded regions represent 95% confidence intervals.  The results indicate that longer training sequences lead to longer generalizable lengths in the addition task.", "section": "4.1 Results"}, {"figure_path": "5cIRdGM1uG/figures/figures_4_2.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares several methods for improving length generalization in the integer addition task, including the proposed Position Coupling.  The x-axis shows the operand length, and the y-axis shows the exact-match accuracy.  The plot demonstrates that Position Coupling significantly outperforms existing methods (NOPE, Random-Start APE, Index Hinting) in terms of accuracy for longer sequences.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_5_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows the result of the experiment on the integer addition task. It compares the performance of different methods for length generalization in the integer addition task. The methods compared are position coupling (the proposed method), index hinting with APE, index hinting with NoPE, and APE with a random starting position ID. The x-axis represents the operand length, and the y-axis represents the exact-match accuracy. The figure shows that position coupling achieves more than 95% accuracy for up to 200-digit additions when trained only on up to 30-digit additions.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_5_2.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows the results of different methods for improving the length generalization of arithmetic Transformers.  The x-axis represents the operand length, and the y-axis represents the exact-match accuracy.  The plot compares the performance of position coupling (the proposed method) against several baseline methods, including different positional encoding schemes and index hinting, on the task of integer addition. Position coupling demonstrates significantly better length generalization than the baselines.", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_7_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows a comparison of different methods for achieving length generalization in the integer addition task.  The x-axis represents the operand length, and the y-axis represents the exact-match accuracy.  The plot compares the performance of the proposed method (position coupling) against existing techniques like index hinting with and without positional encoding (APE and NoPE). The results demonstrate that position coupling significantly outperforms the other methods, achieving over 95% accuracy for operand lengths up to 200 digits when trained only on lengths up to 30 digits.", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_8_1.jpg", "caption": "Figure 8: N \u00d7 2 multiplication task, trained on sequences of length 1\u201340.", "description": "The figure shows the result of the N \u00d7 2 multiplication task.  The x-axis represents the operand length (N), while the y-axis shows the exact-match accuracy (%).  Different lines represent different models using position coupling with varying numbers of layers (1-4 layers) and two baselines (NoPE and Random-Start APE). The plot demonstrates the ability of the position coupling method to generalize to longer sequences than those seen during training, exceeding the performance of baseline approaches.", "section": "Applying Position Coupling Beyond Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_9_1.jpg", "caption": "Figure 2: Position coupling for decimal integer addition task, displaying 653 + 49 = 702 with appropriate input formats. The starting position ID \u20186\u2019 is an arbitrarily chosen number.", "description": "This figure illustrates the concept of position coupling for the decimal integer addition task.  It shows how position IDs are assigned to tokens in an input sequence representing an addition problem (653 + 49 = 702).  The key is that tokens representing digits of the same place value (ones, tens, hundreds, etc.) in different numbers (operands and sum) are assigned the same position ID. This embedding of task structure helps the model generalize better to longer addition problems than it was trained on.  The figure demonstrates this by representing the input as \"$653+049=2070$\", where the response is reversed and zero-padded.  A random starting position ID (6 in this example) is used to prevent overfitting to specific positional encodings.", "section": "3 Position Coupling: A Method for Length Generalization"}, {"figure_path": "5cIRdGM1uG/figures/figures_9_2.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares several methods for length generalization in the integer addition task, including the proposed position coupling method.  The x-axis shows operand length, and the y-axis represents the exact-match accuracy.  The plot showcases how position coupling significantly outperforms other methods in handling longer sequences.", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_16_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows the results of experiments on the integer addition task.  Different methods for achieving length generalization are compared, including the proposed position coupling method and existing methods such as index hinting.  The graph plots the exact-match accuracy against the operand length.  The results show that position coupling significantly improves length generalization capabilities.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_18_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares the performance of different methods for achieving length generalization in the integer addition task.  The methods include Position Coupling (the proposed method), Index Hinting with and without positional encoding (NoPE), and existing methods like NOPE and Random-Start APE.  The graph displays the exact-match accuracy for different operand lengths, illustrating the superior performance and length generalization capabilities of Position Coupling.", "section": "1 Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_18_2.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows the result of different methods for length generalization in integer addition.  The x-axis represents the length of operands, and the y-axis represents the exact-match accuracy.  The graph compares the performance of position coupling (the proposed method) with other methods like NOPE, random APE, and index hinting, demonstrating that position coupling significantly improves length generalization for integer addition task.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_19_1.jpg", "caption": "Figure 3: Ablation on the trained operand lengths (1-layer 4-head models).", "description": "The figure shows the ablation study on the trained operand lengths for 1-layer 4-head Transformer models trained with position coupling.  The models were trained on different lengths of addition problems (1-10, 1-20, 1-30, and 1-40 digits), and their performance was evaluated on addition problems with operand lengths up to 200 digits. The graph shows the exact match accuracy (%) median over 8 runs, with 95% confidence intervals represented by the light shaded area.  The results demonstrate that longer training sequences (more digits) lead to longer generalizable lengths, meaning the models trained on longer sequences can successfully generalize to longer sequences during testing. The 95% line indicates that the model is considered to successfully generalize to a certain length if its median EM accuracy exceeds 95%.", "section": "4.1 Results"}, {"figure_path": "5cIRdGM1uG/figures/figures_19_2.jpg", "caption": "Figure 4: Ablation on the number of layers (trained with position coupling).", "description": "This figure shows the ablation study on the number of layers in the Transformer model when trained with position coupling.  The x-axis represents the query length (number of digits in the addition problem), and the y-axis represents the exact-match accuracy (median over 8 runs).  Different colored lines represent different numbers of layers in the model (1-layer to 6-layer).  The shaded region indicates the training length. The figure demonstrates how the accuracy changes with varying numbers of layers and how the length generalization capability differs for different model depths.", "section": "4.1 Results"}, {"figure_path": "5cIRdGM1uG/figures/figures_20_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows the comparison of different methods for length generalization in integer addition.  It plots the exact-match accuracy against the operand length for various Transformer models and training methods, including position coupling (the proposed method), index hinting, and models without positional encoding. Position coupling demonstrates significantly better length generalization compared to others, achieving over 95% accuracy on 200-digit addition, trained only on data up to 30 digits.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_20_2.jpg", "caption": "Figure 2: Position coupling for decimal integer addition task, displaying 653 + 49 = 702 with appropriate input formats. The starting position ID \u20186\u2019 is an arbitrarily chosen number.", "description": "This figure illustrates the concept of position coupling for the task of decimal integer addition.  It shows how position IDs are assigned to the tokens in the input sequence (\"query=response\"). Instead of assigning unique IDs to each token, position coupling groups \"relevant\" tokens, such as digits of the same significance in the numbers being added, and assigns them the same position ID. This helps the transformer model learn the structure of the addition task and generalize to longer sequences.", "section": "3 Position Coupling: A Method for Length Generalization"}, {"figure_path": "5cIRdGM1uG/figures/figures_20_3.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares several methods for improving length generalization in the integer addition task, including the proposed position coupling method.  It shows the exact-match accuracy for various models trained on different lengths of input sequences (1-30 digits) and tested on much longer sequences (up to 200 digits).  The results demonstrate the superior length generalization capability of the proposed position coupling approach compared to existing methods such as index hinting with and without absolute positional embeddings.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_21_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "This figure compares different methods for achieving length generalization in the integer addition task, focusing on the exact-match accuracy. It shows that the proposed 'position coupling' method significantly outperforms existing methods like index hinting and those without positional encoding, achieving high accuracy even with significantly longer sequences than those used for training.", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_21_2.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "This figure compares the performance of different methods for achieving length generalization in the integer addition task.  The x-axis represents the operand length, and the y-axis represents the exact-match accuracy.  The figure shows that position coupling significantly outperforms other methods, such as NoPE (no positional encoding) and index hinting, achieving over 95% accuracy even with operand lengths up to 200 digits when trained on much shorter sequences (up to 30 digits).  Error bars showing 95% confidence intervals are also included.", "section": "Experiments on the Addition Task"}, {"figure_path": "5cIRdGM1uG/figures/figures_22_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows the results of an experiment on integer addition using different length generalization methods.  The x-axis represents the operand length, and the y-axis represents the exact-match accuracy.  The figure compares the performance of position coupling (the proposed method) against several baselines, including NoPE (no positional encoding), APE (absolute positional embedding) with and without a random start, and Index Hinting. Position coupling shows significantly better length generalization than the baselines, achieving over 95% accuracy even for additions with 200 digits (trained on 1-30 digits).", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_30_1.jpg", "caption": "Figure 2: Position coupling for decimal integer addition task, displaying 653 + 49 = 702 with appropriate input formats. The starting position ID \u20186\u2019 is an arbitrarily chosen number.", "description": "This figure illustrates the concept of position coupling for the decimal integer addition task.  It shows how position IDs are assigned to tokens in the input sequence (\"query\"), which includes the two operands, the addition operator, the equals sign, and the reversed sum.  Crucially, position coupling assigns the same position ID to digits of the same significance (e.g., ones digits, tens digits) in the different parts of the input. This helps the model to learn the inherent structure of the addition task and generalize to longer sequences than those seen during training. The choice of starting position ID (6 in this example) is arbitrary. Zero-padding and reversed format are used for data formatting.", "section": "3 Position Coupling: A Method for Length Generalization"}, {"figure_path": "5cIRdGM1uG/figures/figures_33_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares the performance of different methods for length generalization in the integer addition task.  It shows the exact-match accuracy for various models trained on different lengths of integer addition problems and tested on increasingly longer sequences.  The key takeaway is that the proposed \"position coupling\" method significantly outperforms other methods in generalizing to much longer sequences than seen during training.", "section": "1 Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_33_2.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure shows a comparison of different methods for achieving length generalization in the integer addition task.  The x-axis represents the operand length, and the y-axis represents the exact-match accuracy.  The plot compares the performance of position coupling (the proposed method) against several baselines, including NoPE (no positional encoding), random-start APE (absolute positional embedding with a randomized starting point), and index hinting. The results demonstrate that position coupling significantly outperforms the baselines in terms of length generalization, achieving over 95% accuracy even for operand lengths of up to 200 digits (a 6.67x extrapolation of the training length).", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_36_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares different methods for length generalization in the integer addition task.  It shows the exact-match accuracy of various models (including the proposed 'position coupling' method) trained on shorter sequences (1-30 digits) and tested on increasingly longer sequences (up to 200 digits).  The results demonstrate the effectiveness of position coupling in achieving high accuracy even on much longer sequences than those seen during training.", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_37_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "This figure compares the performance of several methods for length generalization in the integer addition task. The methods compared are: Position Coupling (the proposed method), NOPE (no positional encoding), Random-Start APE (absolute positional embedding with a random starting position ID), and Index Hinting. The plot shows the exact-match accuracy for each method at different operand lengths, demonstrating the superior performance of Position Coupling in generalizing to longer sequences.", "section": "Introduction"}, {"figure_path": "5cIRdGM1uG/figures/figures_41_1.jpg", "caption": "Figure 1: Methods for Length Generalization in the Integer Addition Task. We report exact-match (EM) accuracies (markers: medians over experiments; light area: 95% confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With our proposed position coupling, we achieve more than 95% exact-match accuracy for up to 200-digit additions with decoder-only Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)).", "description": "The figure compares several methods for achieving length generalization in the integer addition task, plotting exact-match accuracy against operand length.  It shows that the proposed 'Position Coupling' method significantly outperforms other methods like index hinting and those without positional encoding, achieving high accuracy even with significantly longer sequences than those seen during training.", "section": "Introduction"}]