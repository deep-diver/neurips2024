[{"figure_path": "MNg331t8Tj/tables/tables_5_1.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the performance of various data augmentation methods on five fine-grained visual classification (FGVC) datasets.  The methods are categorized as traditional (e.g., CAL-Aug, RandAug, CutMix) and generative (e.g., Real Guidance, ALIA, SaSPA).  The table shows the test accuracy for each method on each dataset. The highest accuracy for each dataset is highlighted in bold, and the highest validation accuracy achieved by traditional methods is underlined. SaSPA, the authors' proposed method, generally outperforms the other augmentation methods.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_7_1.jpg", "caption": "Table 2: Classification performance on the contextually biased Aircraft dataset [30], detailing overall, in-domain (ID) and out-of-domain (OOD) accuracies for each augmentation method.", "description": "This table presents the classification performance results on the contextually biased Aircraft dataset. It compares different data augmentation methods, showing their overall accuracy, in-domain accuracy (ID Acc.), and out-of-domain accuracy (OOD Acc.).  In-domain accuracy refers to performance on data similar to the training data, while out-of-domain accuracy assesses generalization to unseen data.  The table helps to illustrate the effectiveness of various augmentation techniques in mitigating bias and improving generalization.", "section": "4.4 Mitigating Contextual Bias (Airbus vs. Boeing)"}, {"figure_path": "MNg331t8Tj/tables/tables_7_2.jpg", "caption": "Table 3: Comparison to concurrent work diff-mix [60]. Test accuracy on 3 FGVC datasets. \u2020 indicates values taken from the diff-mix paper. TI - Textual Inversion, DB - DreamBooth, X- No fine-tuning.", "description": "This table compares the proposed SaSPA method to the diff-mix method [60] across three fine-grained visual classification (FGVC) datasets. It shows the test accuracy achieved by both methods, with and without using additional augmentation techniques like CutMix and fine-tuning strategies such as Textual Inversion and DreamBooth.  The table highlights the performance of SaSPA, showing its competitive results even without using any fine-tuning.", "section": "4.5 Comparing SaSPA with Concurrent Work diff-mix"}, {"figure_path": "MNg331t8Tj/tables/tables_8_1.jpg", "caption": "Table 4: Ablation Study: Effects of different generation strategies on various FGVC Datasets. \u2018Subj.\u2019 means subject representation is used. \u2018Edges=Subj.\u2019 indicates that the real image used to extract the edges is the same as the subject reference image. \u2018Art.\u2019 indicates that half the prompts are appended with artistic styles. For each dataset, bold indicates the highest validation accuracy, and underline indicates the second highest. Ticks under each column mean the component is used.", "description": "This table presents an ablation study evaluating the impact of different components of the SaSPA method on the performance of fine-grained visual classification (FGVC) tasks across multiple datasets.  It examines the individual and combined effects of edge guidance, Img2Img generation, subject representation, artistic prompt styles, and the relationship between the edge map and subject images. The results highlight the optimal combination of components for achieving high accuracy in FGVC.", "section": "4.6 Effect of Different Generation Strategies on Performance"}, {"figure_path": "MNg331t8Tj/tables/tables_16_1.jpg", "caption": "Table 5: Effect of amount of real data used (as a fraction of the complete dataset) and \u03b1 values on validation accuracy when augmenting with SaSPA", "description": "This table shows the validation accuracy achieved by SaSPA under different augmentation ratios (\u03b1) and varying amounts of real training data. The results demonstrate the interaction between \u03b1 and the amount of real data, highlighting the diminishing returns of augmentation and the optimal \u03b1 values under different real data proportions.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_17_1.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the performance of various data augmentation methods on five fine-grained visual classification (FGVC) datasets.  The methods include traditional augmentation techniques (e.g., CAL-Aug, RandAug, CutMix), generative methods using real image guidance, and the proposed SaSPA method. The table highlights the test accuracy for each method on each dataset, with the best result for each dataset shown in bold and the best result achieved by a traditional method underlined.  This allows for a direct comparison of the performance gains achieved by different augmentation approaches on full FGVC datasets.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_17_2.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the performance of various data augmentation methods on five fine-grained visual classification (FGVC) datasets.  The methods are categorized as traditional (e.g., CAL-Aug, RandAug, CutMix) and generative (e.g., SaSPA, ALIA, Real Guidance).  For each dataset, the highest test accuracy is highlighted in bold, and the best validation accuracy achieved by traditional methods is underlined. The table allows readers to assess the effectiveness of the different augmentation strategies for improving the performance of FGVC models.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_18_1.jpg", "caption": "Table 8: Results on the test set of three FGVC datasets for ViT and ResNet101 architectures", "description": "This table compares the performance of different augmentation methods (Best Trad Aug, Real Guidance, ALIA, and SaSPA) on three fine-grained visual classification (FGVC) datasets using two different network architectures: ViT and ResNet101.  The results show the test accuracy for each method on each dataset and architecture, highlighting the effectiveness of SaSPA in improving the performance across different FGVC datasets and architectures.", "section": "4.2 Fine-grained Visual Classification"}, {"figure_path": "MNg331t8Tj/tables/tables_19_1.jpg", "caption": "Table 9: Additional datasets. We report test accuracy on two additional FGVC datasets: Stanford Dogs and The Oxford-IIIT Pet Dataset. The highest values for each dataset are shown in bold.", "description": "This table presents the test accuracy achieved by various augmentation methods (CAL-Aug, Real Guidance, ALIA, and SaSPA) on two additional fine-grained visual classification (FGVC) datasets: Stanford Dogs and Oxford-IIIT Pet.  The highest accuracy for each dataset is highlighted in bold, demonstrating the relative performance of each method on these datasets.", "section": "4.2 Fine-grained Visual Classification"}, {"figure_path": "MNg331t8Tj/tables/tables_19_2.jpg", "caption": "Table 10: Comparison of prompt strategies across two FGVC datasets. The highest values are highlighted in bold, while the second highest are underlined.", "description": "This table compares the performance of different prompt generation strategies (Captions, LE, ALIA (GPT), Ours (GPT), Ours (GPT) + Art) on two FGVC datasets (Aircraft and Cars).  The results show that the proposed method (Ours (GPT) and Ours (GPT) + Art) outperforms other strategies.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_20_1.jpg", "caption": "Table 11: Validation accuracy on the Aircraft dataset using 100 and 200 prompts generated by our method.", "description": "This table shows the validation accuracy achieved on the Aircraft dataset using two different numbers of prompts (100 and 200) generated by the SaSPA method. The results indicate whether using more prompts significantly improves the accuracy of the method.", "section": "B.9 Will More Prompts Improve Performance?"}, {"figure_path": "MNg331t8Tj/tables/tables_20_2.jpg", "caption": "Table 12: Combined FID and accuracy results for various generative augmentation methods across four FGVC datasets.", "description": "This table shows the Fr\u00e9chet Inception Distance (FID) and accuracy scores for various generative augmentation methods across four fine-grained visual classification (FGVC) datasets. FID measures the similarity between the distribution of generated images and real images.  The table compares Real Guidance, ALIA, and SaSPA, highlighting the trade-off between FID (image realism) and accuracy.  Lower FID values indicate higher similarity to real data, while higher accuracy indicates better performance on the classification task.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_20_3.jpg", "caption": "Table 13: Combined diversity score and accuracy results for various generative augmentation methods across five FGVC datasets.", "description": "This table presents a comparison of different generative augmentation methods across five fine-grained visual classification (FGVC) datasets.  It shows both the diversity score (using LPIPS to measure perceptual differences between original and augmented images) and the resulting accuracy achieved by each method.  The higher the diversity score, the more varied the augmentations produced. The accuracy reflects the performance of a downstream classification model trained using the augmented datasets.", "section": "4.2 Fine-grained Visual Classification"}, {"figure_path": "MNg331t8Tj/tables/tables_21_1.jpg", "caption": "Table 14: Validation accuracy of our method with different base models. Generations do not include BLIP-diffusion.", "description": "This table presents the validation accuracy results of the SaSPA method using different base diffusion models (Stable Diffusion v1.5, SD XL Turbo, SD XL) with and without edge guidance. It helps to analyze the impact of different base models and edge guidance on the overall performance of SaSPA.", "section": "4.6 Effect of Different Generation Strategies on Performance"}, {"figure_path": "MNg331t8Tj/tables/tables_21_2.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the performance of different data augmentation methods on five fine-grained visual classification (FGVC) datasets.  It shows the test accuracy for each method, highlighting the best-performing method for each dataset in bold.  The table is divided into traditional augmentation methods and generative augmentation methods.  The highest validation accuracy achieved by traditional methods is also underlined for comparison.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_22_1.jpg", "caption": "Table 16: Higher resolution results. Comparison of our method (SaSPA) with the best augmentation method per dataset. All results are using 448x448 resolution, and reported on the test set of each dataset.", "description": "This table shows the results of experiments conducted using images with higher resolution (448x448) compared to other parts of the paper. It compares the performance of SaSPA with the best-performing traditional augmentation method for each of the datasets used in the study, providing a comparison of test accuracy scores.  The table helps assess the impact of higher resolution on the effectiveness of SaSPA and traditional augmentation methods.", "section": "4.2 Fine-grained Visual Classification"}, {"figure_path": "MNg331t8Tj/tables/tables_22_2.jpg", "caption": "Table 17: Validation accuracy on the Aircraft dataset using different conditioning types of ControlNet.", "description": "This table shows the validation accuracy achieved on the Aircraft dataset when using different edge detection methods (Canny and HED) for conditioning the ControlNet model during image generation.  The results compare the performance of using Canny edges versus HED edges as the conditioning input for the ControlNet model.", "section": "4.6 Effect of Different Generation Strategies on Performance"}, {"figure_path": "MNg331t8Tj/tables/tables_23_1.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the test accuracy of different data augmentation methods across five fine-grained visual classification (FGVC) datasets.  The methods include traditional augmentation techniques (no augmentation, CAL-Aug, RandAug, CutMix, combinations thereof), generative methods leveraging real images (Real Guidance, ALIA), and the proposed SaSPA method.  The highest accuracy for each dataset is highlighted in bold, and the highest validation accuracy achieved by traditional methods is underlined to provide context for the performance of the generative methods.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_23_2.jpg", "caption": "Table 19: Dataset Statistics for Contextually Biased Planes", "description": "This table shows the number of training, validation, and testing samples for Airbus and Boeing planes, categorized by their background (sky, grass, road).  It's used to illustrate the class imbalance and contextual bias present in the dataset when evaluating the model's performance on mitigating contextual biases.", "section": "4.4 Mitigating Contextual Bias (Airbus vs. Boeing)"}, {"figure_path": "MNg331t8Tj/tables/tables_24_1.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the test accuracy of different data augmentation methods across five fine-grained visual classification (FGVC) datasets.  It contrasts traditional methods (no augmentation, CAL-Aug, RandAug, CutMix, combinations thereof) with generative methods (real guidance, ALIA, SaSPA with and without BLIP-diffusion). The best performing method for each dataset is highlighted in bold, while the best performing traditional method is underlined.", "section": "4 Experiments"}, {"figure_path": "MNg331t8Tj/tables/tables_25_1.jpg", "caption": "Table 1: Results on full FGVC Datasets. This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in bold, while the highest validation accuracies achieved by traditional augmentation methods are underlined.", "description": "This table compares the performance of various data augmentation techniques on five fine-grained visual classification (FGVC) datasets.  It shows the test accuracy achieved by different methods, including traditional augmentation techniques (no augmentation, CAL-Aug, RandAug, CutMix, and combinations), generative methods (using real images as guidance, ALIA, SaSPA w/o BLIP diffusion), and the proposed SaSPA method. The highest test accuracy for each dataset is highlighted in bold, and the best validation accuracy achieved by a traditional method is underlined, providing a basis for comparison.", "section": "4 Experiments"}]