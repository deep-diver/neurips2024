{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a crucial foundation for SaSPA's filtering mechanism, which leverages CLIP to ensure generated images maintain class fidelity."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper introduces Stable Diffusion, a key component of SaSPA and a foundation for many other text-to-image diffusion models used in the paper's comparisons."}, {"fullname_first_author": "Dongxu Li", "paper_title": "BLIP-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing", "publication_date": "2024-01-01", "reason": "This paper introduces BLIP-diffusion, the core generation model within SaSPA's pipeline; its subject-driven generation is vital to SaSPA's ability to produce class-consistent and high-quality images."}, {"fullname_first_author": "Lvmin Zhang", "paper_title": "Adding conditional control to text-to-image diffusion models", "publication_date": "2023-10-01", "reason": "This paper introduces ControlNet, a key component that conditions the generation process within SaSPA, ensuring generated images maintain structural consistency with real images."}, {"fullname_first_author": "Lisa Dunlap", "paper_title": "Diversify your vision datasets with automatic diffusion-based augmentation", "publication_date": "2023-12-01", "reason": "This paper is highly relevant as it explores generative augmentation methods for FGVC and directly addresses the trade-off between fidelity and diversity in generating augmented images, a core challenge also tackled by SaSPA."}]}