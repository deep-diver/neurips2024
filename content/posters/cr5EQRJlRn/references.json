{"references": [{"fullname_first_author": "Austin, J.", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper introduces a novel approach to program synthesis using large language models, a topic highly relevant to the current research on LLMs and their applications."}, {"fullname_first_author": "Liu, J.", "paper_title": "Bitdelta: Your fine-tune may only be worth one bit", "publication_date": "2024-02-10", "reason": "This paper introduces a novel 1-bit quantization method for delta weights, which is directly compared against in the current research."}, {"fullname_first_author": "Luo, H.", "paper_title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct", "publication_date": "2023-08-09", "reason": "This paper proposes a method for improving the mathematical reasoning capabilities of LLMs, which is one of the tasks evaluated in the current research."}, {"fullname_first_author": "Ryu, S.", "paper_title": "Efficient storage of fine-tuned models via low-rank approximation of weight residuals", "publication_date": "2023-05-18", "reason": "This paper proposes a low-rank approximation method for compressing delta weights, which is directly compared against in the current research."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, a large language model that serves as a backbone for several of the fine-tuned models evaluated in the current research."}]}