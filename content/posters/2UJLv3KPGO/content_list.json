[{"type": "text", "text": "Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tian Xie ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xueru Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science and Engineering the Ohio State University Columbus, OH 43210 xie.1379@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Computer Science and Engineering the Ohio State University Columbus, OH 43210 zhang.12807@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As machine learning (ML) models are increasingly used in social domains to make consequential decisions about humans, they often have the power to reshape data distributions. Humans, as strategic agents, continuously adapt their behaviors in response to the learning system. As populations change dynamically, ML systems may need frequent updates to ensure high performance. However, acquiring highquality human-annotated samples can be highly challenging and even infeasible in social domains. A common practice to address this issue is using the model itself to annotate unlabeled data samples. This paper investigates the long-term impacts when ML models are retrained with model-annotated samples when they incorporate human strategic responses. We first formalize the interactions between strategic agents and the model and then analyze how they evolve under such dynamic interactions. We find that agents are increasingly likely to receive positive decisions as the model gets retrained, whereas the proportion of agents with positive labels may decrease over time. We thus propose a refined retraining process to stabilize the dynamics. Last, we examine how algorithmic fairness can be affected by these retraining processes and find that enforcing common fairness constraints at every round may not benefti the disadvantaged group in the long run. Experiments on (semi-)synthetic and real data validate the theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As machine learning (ML) is increasingly used to automate human-related decisions (e.g., in lending, hiring, college admission), there is a growing concern that these decisions are vulnerable to human strategic behaviors. With the knowledge of decision policy, humans may adapt their behavior strategically in response to ML models, e.g., by changing their features at costs to receive favorable outcomes. A line of research called Strategic Classification studies such problems by formulating mathematical models to characterize strategic interactions and developing algorithms robust to strategic behavior [1, 2]. Among existing works, most studies focus on one-time model deployment where an ML model is trained and applied to a fixed population of strategic agents once. ", "page_idx": 0}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/42e934aaf7d91fae5640944bb0dbabcb653e031c5b12964146dbeeffd81adc7d.jpg", "img_caption": ["Figure 1: Illustration of updating the training data from $t$ to $t\\!+\\!1$ during the retraining process with strategic feedback "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "However, practical ML systems often need to be retrained periodically to ensure high performance on the current population. As the ML model gets updated, human behaviors also change accordingly. ", "page_idx": 0}, {"type": "text", "text": "To prevent the potential adverse outcomes, it is critical to understand how the strategic population is affected by the model retraining process. Traditionally, the data used for retraining models can be constructed manually with human annotations (e.g., ImageNet). However, acquiring a large amount of human-annotated samples can be highly difficult and even infeasible, especially in human-related applications, e.g., in automated hiring where an ML model is used to identify qualified applicants, even an experienced interviewer needs time to label an applicant. ", "page_idx": 1}, {"type": "text", "text": "Motivated by a recent practice of automating data annotation for retraining large-scale ML models [3, 4], we study strategic classification in a sequential framework where an ML model is periodically retrained by a decision-maker with both human and model-annotated samples. These updated models are deployed sequentially on agents who may modify their features to receive favorable outcomes. Since ML models affect agent behavior and the agent strategic feedback can further be captured when retraining the future model, their interactions drive both to change dynamically over time. However, it remains unclear how the two evolve under such dynamics and what long-term effects one may have on the other. ", "page_idx": 1}, {"type": "text", "text": "To further illustrate our problem, consider an example of college admission where new students from a population apply each year. In the $t$ -th year, an ML model $f_{t}$ is learned from a training dataset $\\mathcal{S}_{t}$ and used to make admission decisions. For students who apply in the $(t+1)$ -th year, they will best respond to the model $f_{t}$ in the previous year (e.g., preparing the ap", "page_idx": 1}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/b9edbbac4ceacb23e675ba6880a53ff82d7e4ac4761ddb20833bc0e8b2a0425d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: Evolution of the student distribution and ML model at $t=0$ (left), $t=5$ (middle), and $t=14$ (right): each student has two features. At each time, a classifier is retrained with both human and modelannotated samples, and students best respond to be admitted (Fig. 1). Over time, the learned classifier (black lines) deviates from ground truth (green lines). ", "page_idx": 1}, {"type": "text", "text": "plication package in a way that maximizes the chance of getting admitted). Meanwhile, the college retrains the classifier $f_{t+1}$ using a new training dataset $\\boldsymbol{S}_{t+1}$ consisting of previous training data $\\mathcal{S}_{t}$ , new human-annotated samples, and new model-annotated samples (i.e., previous applicants annotated by the most recent model $f_{t.}$ ). The model $f_{t+1}$ is then used to make admission decisions in the $(t+1)$ -th year. This process continues over time and we demonstrate how the training dataset $\\mathcal{S}_{t}$ is updated to $\\boldsymbol{S}_{t+1}$ in Fig. 1. Under such dynamics, both the ML system and the strategic population change over time and may lead to unexpected long-term consequences. An illustrating example is given in Fig. 2. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we examine the evolution of the ML model and the agent data distribution. We ask: 1) How does the agent population evolve when the model is retrained with strategic feedback? 2) How is the ML system affected by the agent\u2019s strategic response? 3) If agents come from multiple social groups, how can model retraining further impact algorithmic fairness? Can imposing group fairness constraints during model training bring long-term societal benefits? ", "page_idx": 1}, {"type": "text", "text": "Compared to prior studies on strategic classification under sequential settings [5, 6, 7] that mainly focused on developing a classifier robust to strategic agents, we study the long-term impacts of model retraining with agent strategic feedback. Instead of assuming actual labels are available while retraining, we consider more practical scenarios with model-annotated samples. Although the risks on accuracy [3] and fairness [8] of using model-annotated samples to retrain models have been highlighted, ours is the first to incorporate strategic feedback from human agents. In App. C, we discuss more related works in detail. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We formulate the problem of model retraining with human strategic feedback (Sec. 2). ", "page_idx": 1}, {"type": "text", "text": "2. We theoretically characterize the evolution of the expected acceptance rate (i.e., the proportion of agents receiving positive classifications), qualification rate (i.e., the proportion of agents with positive labels), and the classifier bias (i.e., the discrepancy between acceptance rate and qualification rate) under the retraining process. We show that the acceptance rate increases over time under retraining, while the actual qualification rate may decrease under certain conditions. The dynamics of classifier bias are more complex depending on the systematic bias of humanannotated samples. Finally, we propose an approach to stabilize the dynamics (Sec. 3). ", "page_idx": 1}, {"type": "text", "text": "3. We consider settings where agents come from multiple social groups and investigate how intergroup fairness can be affected by the model retraining process; we also investigate the long-term effects of fairness intervention at each round of model retraining (Sec. 4). 4. We conduct experiments on (semi-)synthetic and real data to verify the theorems (Sec. 5, App. E, ", "page_idx": 2}, {"type": "text", "text": "App. F). ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a population of agents who are subject to certain ML decisions (e.g., admission/hiring decisions) and join the decision-making system in sequence. Each agent has observable continuous features $X\\in\\bar{\\mathbb{R}}^{d}$ and a hidden binary label $Y\\in\\{0,1\\}$ indicating its qualification state (\"1\" being qualified and $\"0\"$ being unqualified). Let $P_{X Y}$ be the joint distribution of $(X,Y)$ which is fixed over time, and $P_{X}$ , $P_{Y\\mid X}$ be the corresponding marginal and conditional distributions. Assume $P_{X}$ , $P_{Y\\mid X}$ are continuous with non-zero probability mass everywhere in their domain. For agents who join the system at time $t$ , the decision-maker uses a classifier $f_{t}:\\mathbb{R}^{d}\\rightarrow\\{0,1\\}$ to make decisions. Note that the decision-maker does not know $P_{X Y}$ and can only learn $f_{t}$ from the training dataset at $t$ [9]. ", "page_idx": 2}, {"type": "text", "text": "Agent best response. Agents who join the system at time $t$ can adapt their behaviors based on the latest classifier $f_{t-1}$ and change their features $X$ strategically. We denote the resulting data distribution as $P_{X Y}^{t}$ . Specifically, given original features $X=x$ , agents have incentives to change their features at costs to receive positive classification outcomes, i.e., by maximizing utility ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=\\arg\\operatorname*{max}_{z}\\ \\left\\{f_{t-1}(z)-c(x,z)\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where distance function $c(\\boldsymbol{x},\\boldsymbol{z})\\ge0$ measures the cost for an agent to change features from $x$ to $z$ . In this paper, we consider $c(\\bar{x},z)=(z-x)^{T}B(z-x)$ for some $d\\times d$ positive semidefinite matrix $B$ , allowing heterogeneous costs for different features. After agents best respond, their data distribution changes from $P_{X Y}$ to $P_{X Y}^{t}$ . In this paper, we term $P_{X Y}$ agent prior-best-response distribution aadnad $\\bar{P}_{X Y}^{t}$ i rp boesth-abveisotr-sr easnpdo tnhseei r driesstrpiobnustieos na.r eW dee lcaoynesdi d[e1r0 ]n:a tthuerayl  ascett tbiansgesd t hoant  t(hie)  laagteesntt sc lnaesesidf tieir $f_{t-1}$ they are aware of, not the one they receive; (ii) agent behaviors are benign and feature changes can genuinely affect their underlying labels, so feature-label relationship $\\bar{P_{Y|X}^{t}}=P_{Y|X}$ is fixed over time [9, 11]. ", "page_idx": 2}, {"type": "text", "text": "Human-annotated samples and systematic bias. At each round $t$ , we assume the decision-maker can draw a limited number of unlabeled samples from the prior-best-response distribution $P_{X}$ .1 With some prior knowledge (possibly biased), the decision-maker can annotate these features and generate human-annotated samples $\\textstyle S_{o,t}$ . We assume the quality of human annotations is consistent, so $\\textstyle S_{o,t}$ at any $t$ is drawn from a fixed probability distribution $D_{X Y}^{o}$ with marginal distribution $D_{X}^{o}=P_{X}$ . Because human annotations may not be the same as true labels, $D_{Y\\mid X}^{o}$ can be biased compared to $P_{Y\\mid X}$ . We define such difference as the decision-maker\u2019s systematic bias, formally stated below. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Systematic bias). Define $\\mu(D^{o},P)\\,:=\\,\\mathbb{E}_{x\\sim P x}[D_{Y|X}^{o}(1|x)-P_{Y|X}(1|x)]$ . The decision-maker has a systematic bias if $\\mu(D^{o},P)>0$ (overestimation) or $<0$ (underestimation). ", "page_idx": 2}, {"type": "text", "text": "Def. 2.1 implies that the decision-maker has a systematic bias when it labels a larger (or smaller) proportion of agents as qualified compared to the ground truth. In App. B, we present numerous examples of systematic bias in real applications where the decision-maker has different systematic biases towards different demographic groups. Generally, the systematic bias may or may not exist and we study both scenarios in the paper. ", "page_idx": 2}, {"type": "text", "text": "Model-annotated samples. In addition to human-annotated samples, the decision-maker at each round $t$ can also utilize the most recent classifier $f_{t-1}$ to generate model-annotated samples for training $f_{t}$ . Specifically, let $\\{x_{t-1}^{i}\\}_{i=1}^{N}$ be $N$ post-best-response features ((1)) acquired from the agents coming at $t-1$ , the decision-maker uses $f_{t-1}$ to annotate the samples and obtain modelannotated samples $\\boldsymbol{S_{m,t-1}}=\\{\\boldsymbol{x}_{t-1}^{i},\\boldsymbol{f_{t-1}(x_{t-1}^{i})}\\}_{i=1}^{\\bar{N}}$ . Both human and model-annotated samples are used to retrain the classifier at $t$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Classifier\u2019s retraining process. With the human and model-annotated samples, we next introduce how the model is retrained by the decision-maker over time. Denote the training dataset at $t$ as $\\mathcal{S}_{t}$ . Initially, the decision-maker trains $f_{0}$ with a human-annotated training dataset $\\mathcal{S}_{0}=\\mathcal{S}_{o,0}$ . Then the decision-maker updates $f_{t}$ every round to make decisions about agents, and it learns $f_{t}$ using empirical risk minimization (ERM) with training dataset $\\mathcal{S}_{t}$ . Similar to studies in strategic classification [2], we consider linear classifier in the form of $f_{t}(x)=\\mathbf{1}(h_{t}(x)\\geq\\theta)$ where $h_{t}:\\overline{{\\mathbb{R}}}\\rightarrow[0,1]$ is the scoring function (e.g., logistic function) and $h_{t}\\in\\mathcal{H}$ . At each round $t\\geq1$ , $\\mathcal{S}_{t}$ consists of three components: existing training samples $\\mathcal{S}_{t-1}$ , $N$ new model-annotated and $K$ new human-annotated samples: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{t}=S_{t-1}\\cup S_{m,t-1}\\cup S_{o,t-1},\\ \\forall t\\geq1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since annotating agents is usually time-consuming and expensive, we have $N\\gg K$ in practice. The complete retraining process is shown in Alg. 1 (App. A). ", "page_idx": 3}, {"type": "text", "text": "Given the post-best-response distribution $P_{X Y}^{t}$ , we can define the associated qualification rate as the probability that agents are qualified, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ(P^{t})=\\mathbb{E}_{(x,y)\\sim P_{X Y}^{t}}\\left[y\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the classifier $f_{t}$ deployed on marginal feature distribution $P_{X}^{t}$ , we define acceptance rate as the probability that agents are classified as positive, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nA(f_{t},P^{t})=\\mathbb{E}_{x\\sim P_{X}^{t}}[f_{t}(x)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since $\\mathcal{S}_{t}$ is randomly sampled at all $t$ , the resulting classifier $f_{t}$ and agent best response are also random. Denote $D_{X Y}^{t}$ as the probability distribution of sampling from $\\mathcal{S}_{t}$ and recall that $D_{X Y}^{o}$ is the distribution for human-annotated $\\textstyle S_{o,t}$ , we can further define the expectations of $Q(P^{t}),\\bar{A}\\bar{(}f_{t},P^{t})$ over the training dataset: ", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{t}:=\\mathbb{E}_{S_{t-1}}[Q(P^{t})];\\quad a_{t}:=\\mathbb{E}_{S_{t}}\\left[A(f_{t},P^{t})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q_{t}$ is the expected actual qualification rate of agents after they best respond, note that the expectation is taken with respect to $\\mathcal{S}_{t-1}$ because the distribution $\\overline{{P_{X Y}^{t}}}$ is the result of agents responding to $f_{t-1}$ which is trained with $\\mathcal{S}_{t-1}$ ; $a_{t}$ is the expected acceptance rate of agents at time ", "page_idx": 3}, {"type": "text", "text": "Dynamics of qualification rate $\\pmb{\\&}$ acceptance rate. Under the model retraining process, both the model $f_{t}$ and agent distribution $P_{X Y}^{t}$ change over time. One goal is to understand how the agents and the ML model interact and impact each other in the long run. Specifically, we are interested in the dynamics of the following variables: ", "page_idx": 3}, {"type": "text", "text": "1. Qualification rate $q_{t}$ : it measures the qualification of agents and indicates the social welfare. ", "page_idx": 3}, {"type": "text", "text": "2. Acceptance rate $a_{t}$ : it measures the likelihood that an agent can receive positive outcomes and indicates the applicant welfare. ", "page_idx": 3}, {"type": "text", "text": "3. Classifier bias $\\Delta_{t}~=~|a_{t}-\\mathit{q}_{t}|$ : it is the discrepancy between the acceptance rate and the true qualification rate, measuring how well the decision-maker can approximate agents\u2019 actual qualification rate and can be interpreted as decision-maker welfare. ", "page_idx": 3}, {"type": "text", "text": "In the rest of the paper, we study the dynamics of $q_{t},a_{t},\\Delta_{t}$ and we aim to answer the following questions: 1) How do the qualification rate $q_{t}$ , acceptance rate $a_{t}$ , and classifier bias $\\Delta_{t}$ evolve under the dynamics? 2) How can the evolution of the system be affected by the decision-maker\u2019s retraining process? 3) What are the impacts of the decision-maker\u2019s systematic bias? 4) If we further consider agents from multiple social groups, how can the retraining process affect inter-group fairness? ", "page_idx": 3}, {"type": "text", "text": "3 Dynamics of the Agents and Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we examine the evolution of qualification rate $q_{t}$ , acceptance rate $a_{t}$ , and classifier bias $\\Delta_{t}$ . We aim to understand how applicant welfare (Sec. 3.1), social welfare (Sec. 3.2), and decision-maker welfare (Sec. 3.3) are affected by the retraining process in the long run. We first introduce some assumptions used for the theorems. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1. Hypothesis class $\\mathcal{H}$ can perfectly learn the training data distribution $D_{Y\\mid X}^{t}$ , i.e., $\\exists h_{t}^{*}\\in\\mathcal{H}$ such that $h_{t}^{*}(x)=D_{Y\\mid X}^{t}(1|x)$ . ", "page_idx": 4}, {"type": "text", "text": "With Assumption 3.1, we avoid the effects of learning error on the system dynamics; this allows us to focus on the dynamic interactions between strategic agents and ML system. Although the theoretical analysis relies on the assumption, our experiments in Sec. 5 and App. F show consistent results with theorems. We further assume the monotone likelihood ratio property holds for $D_{X Y}^{o}$ and $P_{X Y}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. Let $x[m]$ be the $m^{t h}$ dimension of $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , then $D_{Y\\mid X}^{o}(1|x)$ and $P_{Y\\mid X}(1|x)$ are continuous and monotonically increasing in $x[m]$ , $\\forall m=1,\\cdot\\cdot\\cdot,d$ while other dimensions are fixed. ", "page_idx": 4}, {"type": "text", "text": "Note that the Assumption 3.2 is mild and widely used in previous literature [12, 13]. It can be satisfied by many distributional families such as exponential, Gaussian, and mixtures of exponential/Gaussian. It implies that agents are more likely to be qualified as feature value increases. ", "page_idx": 4}, {"type": "text", "text": "3.1 Applicant welfare: dynamics of acceptance rate ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first examine the dynamics of $a_{t}\\,=\\,\\mathbb{E}_{S_{t}}\\left[A(f_{t},P^{t})\\right]$ . Intuitively, under Assumption 3.1, all classifiers can fit the training data well. Then the model-annotated samples $S_{m,t-1}$ generated from post-best-response agents would have a higher qualification rate than the qualification rate of training data $\\mathcal{S}_{t-1}$ . As a result, the training data $\\mathcal{S}_{t}$ augmented with $\\mathcal{S}_{m,t-1}$ has a higher proportion of qualified agents than the qualification rate of $\\mathcal{S}_{t-1}$ , thereby producing a more \"generous\" classifier $f_{t}$ with a larger $a_{t}$ . This reinforcing process can be formally stated in Thm. 3.3. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Evolution of $a_{t}$ ). Under the retraining process, the acceptance rate of the agents that join the system increases over time, i.e., $a_{t}>a_{t-1}$ , $\\forall t\\geq1$ . ", "page_idx": 4}, {"type": "text", "text": "We prove Thm. 3.3 by mathematical induction in App. G.3 and Fig. 3 illustrates the theorem. When agents best respond, the decision-maker tends to accept more agents. We can further show that when the number of model-annotated samples $N$ is large compared to the number of human-annotated samples $K$ , the classifier will ultimately accept all agents in the long run (Prop. 3.4). Proposition 3.4. For any $P_{X Y},D^{o},B$ , there exists a threshold $\\lambda\\,>\\,0$ such that $\\operatorname*{lim}_{t\\to\\infty}a_{t}\\;=\\;1$ whenever $\\begin{array}{r}{\\frac{K}{N}<\\lambda}\\end{array}$ . ", "page_idx": 4}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/0ccbe85b88e6e92fbaa973e55784afeb561e329b5318985301dfbf7941eddd8b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Increasing acceptance rate from $a_{t}$ to $a_{t+1}$ . Unqualified/qualified agents are shown as circles/squares, while the admitted/rejected agents are shown in red/blue. New agents coming at $t+1$ are shown in hollow. The left plot shows the training set $\\mathcal{S}_{t}$ containing 2 unqualified (red circle) and 2 qualified agents (blue square) and $a_{t}$ is 0.5. The middle plot shows the agents coming at $t$ best respond to $f_{t-1}$ . After the responses, 3 of 4 agents are qualified (blue square) and 1 is still unqualified (blue circle). However, all 4 agents are annotated as \"qualified\" (blue). The right plot shows the training set $\\boldsymbol{S}_{t+1}$ containing all points of the left and middle plot, plus two new human-annotated points (hollow points). All blue points are labeled as 1 and the red points are 0. So $\\boldsymbol{S}_{t+1}$ has more samples with a positive label (0.7), resulting in $f_{t+1}$ accepting a higher proportion of agents. ", "page_idx": 4}, {"type": "text", "text": "The specific value of $\\lambda$ in Prop. 3.4 depends on $P_{X Y},D^{o},B$ , which is difficult to find analytically. Nonetheless, we illustrate in Sec. 5 that when $\\begin{array}{r}{\\frac{K}{N}=0.05}\\end{array}$ , $a_{t}$ tends to approach 1 in numerous datasets. Since the human-annotated samples are often difficult to attain (due to time and labeling costs), the condition in Prop. 3.4 is easy to satisfy in practice. ", "page_idx": 4}, {"type": "text", "text": "3.2 Social welfare: dynamics of qualification rate ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we study the dynamics of qualification rate $q_{t}=\\mathbb{E}_{S_{t-1}}[Q(P^{t})]$ . Unlike the acceptance rate $a_{t}$ which always increases during the retraining process, the evolution of $q_{t}$ is more complicated and depends on agent prior-best-response distribution $P_{X Y}$ . ", "page_idx": 4}, {"type": "text", "text": "Specifically, let $q_{0}=Q(P)=\\mathbb{E}_{(x,y)\\sim P_{X Y}}[y]$ be the initial qualification rate, then the difference between $q_{t}$ and $q_{0}$ can be interpreted as the amount of improvement (i.e., increase in label) agents gain from their strategic behavior at $t$ . This is determined by (i) the proportion of agents that decide to change their features at costs (depends on $P_{X}$ ), and (ii) the improvement agents can expect upon changing features (depends on $P_{Y\\mid X},$ . Thus, the dynamics of $q_{t}$ depend on $P_{X Y}$ . Despite the intricate nature of dynamics, we can still derive a condition under which $q_{t}$ decreases monotonically. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Evolution of $q_{t}$ ). Consider the setting where the $d$ -dimensional feature space $X\\in$ $\\mathbb{R}^{d}$ where $F_{X}(x),P_{X}(x),P_{Y|X}(1|x)$ are the cumulative distribution function, probability density function and the labeling function when $Y=1$ . Denote $\\mathcal{T}=\\{x|f_{0}(x)=0\\}$ as the half-space in $\\mathbb{R}^{d}$ determined by the classifier $f_{0}$ . Under the retraining process, $\\forall t\\geq1$ , $q_{t+1}\\leq q_{t}$ if either of the following conditions holds: (i) $F_{X}$ and $P_{Y\\mid X}(1|x)$ are convex on $\\mathcal{I}$ ; $(i i)$ for each dimension $x[i],i\\in[d],\\,F_{X}(x)$ and $P_{Y\\mid X}(1|x)$ are convex with respect to $x[i]$ . ", "page_idx": 5}, {"type": "text", "text": "Note that $q_{t+1}\\leq q_{t}$ in Thm. 3.5 holds only for $t\\geq1$ . Because agent behavior can only improve their labels, prior-best-response $q_{0}$ always serves as the lower bound of $q_{t}$ . The half-space $\\mathcal{I}$ in Thm. 3.5 specifies the region in feature space where agents have incentives to change their features. The convexity of $F_{X}$ and $P_{Y\\mid X}(1|x)$ ensure that as $f_{t}$ evolves from $t=1$ : (i) fewer agents choose to improve their features, and (ii) agents expect less improvement from feature changes. Thus, $q_{t}$ decreases over time. Conditions in Thm. 3.5 can be satisfied by common distributions $P_{X}$ (e.g., Uniform, $\\mathrm{Beta}(\\alpha,1)$ with $\\alpha>1$ ) and labeling functions $P_{Y\\mid X}(1|x)$ (e.g., linear function, quadratic functions with degree greater than 1). The proof and a more general analysis are shown in App. G.5. We also show that Thm. 3.5 is valid under diverse experimental settings (Sec. 5, App. E, App. F). ", "page_idx": 5}, {"type": "text", "text": "3.3 Decision-maker welfare: dynamics of classifier bias ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Sec. 3.1 and 3.2 show that as the classifier $f_{t}$ gets updated over time, agents are more likely to get accepted $a_{t}$ increases). However, their true qualification rate $q_{t}$ (after the best response) may actually decrease. It indicates that the decision-maker\u2019s misperception about agents varies over time. Thus, this section studies the dynamics of classifier bias $\\Delta_{t}^{-}=|\\bar{a_{t}}-q_{t}|$ . Our results show that the evolution of $\\Delta_{t}$ is largely affected by the decision-maker\u2019s systematic bias $\\mu(D^{o},P)$ as defined in Def. 2.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6 (Evolution of $\\Delta_{t}$ ). Starting from $t=1$ at the retraining process and under conditions in Thm. 3.5:   \n1. If $\\mu(D^{o},P)=0$ , i.e., the systematic bias does not exist, then $\\Delta_{t}$ increases over time.   \n2. If $\\mu(D^{o},P)>0,$ , i.e., the decision-maker overestimates agent qualification, then $\\Delta_{t}$ increases over time.   \n3. If $\\mu(D^{o},P)<0,$ , i.e., the decision-maker underestimates agent qualification, $\\Delta_{t}$ either monotonically decreases or first decreases but then increases. ", "page_idx": 5}, {"type": "text", "text": "Thm. 3.6 highlights the potential risks of the model retraining process and is proved in App. G.6. Originally, the purpose of retraining the classifier was to ensure accurate decisions on the targeted population. However, when agents behave strategically, the retraining may lead to adverse outcomes by amplifying the classifier bias. Meanwhile, though systematic bias is usually an undesirable factor to eliminate when learning ML models, it may help mitigate classifier bias to improve the decision-maker welfare in the retraining process, i.e., $\\Delta_{t}$ decreases when $\\mu(D^{o},P)<0$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 Intervention to stabilize the dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Sec. 3.1- 3.3 show that as the model is retrained from strategic agents, $a_{t},q_{t},\\Delta_{t}$ are unstable and may change monotonically over time. Next, we introduce an effective approach to stabilizing the system. ", "page_idx": 5}, {"type": "text", "text": "From the above analysis, we know that one reason that makes $q_{t},\\ a_{t}$ , $\\Delta_{t}$ evolve is agent\u2019s best response, i.e., agents improve their features strategically to be accepted by the most recent model, which leads to a higher qualification rate of model-annotated samples (and the resulting training data), eventually causing $a_{t}$ to deviate from $q_{t}$ . Thus, to mitigate such deviation, we can improve the quality of model annotation. Our method is proposed based on this idea, which uses a probabilistic sampler [3] when producing model-annotated samples. ", "page_idx": 5}, {"type": "text", "text": "Specifically, at each time $t$ , instead of adding $\\boldsymbol{S_{m-1,o}}=\\{\\boldsymbol{x}_{t-1}^{i},\\boldsymbol{f_{t-1}}(\\boldsymbol{x}_{t-1}^{i})\\}_{i=1}^{N}$ (samples annotated by the model $f_{t-1})$ to training data $\\mathcal{S}_{t}$ (2), we use the probabilistic model $h_{t-1}(x)$ to annotate each sample according to the following: For each sample $x$ , we label it as 1 with probability $h_{t-1}(x)$ , and as $O$ otherwise. Here $h_{t-1}(x)\\approx D_{Y\\mid X}^{t-1}(1|x)$ is the estimated posterior probability learned from $\\mathcal{S}_{t-1}$ (e.g., using logistic model). We call the procedure refined retraining process if model-annotated samples are generated in this way based on a probabilistic sampler. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Fig. 3 also illustrates the above idea: agents best respond to $f_{t-1}$ (middle plot) to improve and $f_{t}$ will label both as 1. By contrast, a probabilistic sampler $h_{t}$ only labels a fraction of them as 1. This alleviates the influence of agents\u2019 best responses to stabilize the dynamics of $a_{t},q_{t},\\Delta_{t}$ . Prop. D.2 and App. F.3 provide proofs and more experiments for the refined retraining process. ", "page_idx": 6}, {"type": "text", "text": "4 Impacts on Algorithmic Fairness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we further consider agents from multiple social groups and investigate how group fairness can be affected by the model retraining process. Similar to prior studies in fair ML [12, 14, 15], we assume the decision-maker knows the group identity of each agent and uses group-dependent classifiers to make decisions. WLOG, we present the results for any pair of two groups $i,j$ . Among the two groups, we define the group with a smaller acceptance rate under unconstrained optimal classifiers as disadvantaged group. ", "page_idx": 6}, {"type": "text", "text": "4.1 Impacts of systematic bias & model retraining ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first consider the situation where two groups have no innate difference: they have the same prior-best-response feature distribution and the same cost matrix $B$ to change features. However, the decision-maker has a systematic bias in favor of group $i$ more than group $j$ , making $i$ the advantaged group and $j$ the disadvantaged group. We consider the fairness metric demographic parity (DP) [16], which measures unfairness as the difference in acceptance rate across two groups. Extension to other fairness metrics such as equal opportunity [17] is discussed in App. D.2. Thm. 4.1 below shows the long-term impacts of refined retraining process (i.e., model-annotated samples generated with probabilistic sampler $h_{t}$ ) and original model retraining process (i.e., model-annotated samples generated with model $f_{t.}$ ) on group unfairness. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Impacts of model retraining on unfairness). When groups i, j have no innate difference, but $j$ is disadvantaged due to systematic bias: (i) If applying original retraining process to both groups with $\\frac{K}{N}$ satisfying Prop. 3.4, then group $j$ will stay disadvantaged until all agents in both groups are accepted to achieve perfect fairness; (ii) If applying refined retraining process to both groups, then group $j$ will stay disadvantaged and the unfairness remains the same in the long run; (iii) If applying refined retraining process to group iii but the original process to group $\\pmb{\\mathscr{j}}$ with $\\frac{K}{N}$ satisfying Prop. 3.4, then unfairness first decreases after certain rounds of retraining until group $j$ becomes advantaged, then unfairness increases. ", "page_idx": 6}, {"type": "text", "text": "Thm. 4.1 shows that original and refined retraining processes impact differently on group fairness as proved in App. G.9. Specifically, the original retraining process ultimately attains \"trivial\" perfect fairness by accepting all agents, whereas the refined retraining process stabilizes the dynamics but unfairness always exists. Interestingly, applying disparate retraining strategies to two groups may result in perfect fairness in the middle of retraining process. ", "page_idx": 6}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/b4e9df1d5b18e0bc4485106d2355886ca24e7fc871cd3c38d20db43a2a62be6d.jpg", "img_caption": ["Figure 4: Unfairness (DP) when the refined retraining process is applied to both groups (left), original retraining process is applied to both groups (middle), refined retraining process is only applied to group $i$ (right) under dataset 2. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "This suggests that it may be beneficial for the decision-maker to monitor the fairness measure during the retraining and execute an early stopping mechanism to attain almost perfect DP fairness, i.e., stop retraining models early once unfairness reaches the minimum. As shown in the right plot of Fig. 4, when refined retraining is only applied to group $i$ , unfairness is minimized at $t=5$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Impact of short-term fairness intervention ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Sec. 4.1 proposed a solution of \"disparate retraining with early stopping\" to mitigate unfairness. A more common method to maintain fairness throughout the retraining process is to enforce certain fairness constraints every time when updating the models. Next, we consider this method where refined retraining process is applied to both groups (to stabilize dynamics) and a fairness constraint is imposed at each round of model retraining. Unlike Sec. 4.1, we consider a general setting where two groups may have different feature distributions and different cost matrices $B$ , but feature-label relation $P_{Y\\mid X}(1|x)$ is the same across groups. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Finding fair models. For each group $s$ , we use superscript $s$ to denote the group-specific distributions/metrics listed in Sec. 2 (e.g., $P_{X Y}^{s,t}$ ). At each round $t$ , the decision-maker first trains a linear model $f_{t}^{s}=\\mathbf{1}(h_{t}^{s}(x)\\geq\\theta)$ for group $s$ . According to Assumption 3.1 and Prop. D.2, $h_{t}^{s}$ is expected to be $D_{Y|X}^{o,s}(1|x)=P_{Y|X}^{s}(1|x)+\\mu_{s}$ , where $\\mu_{s}$ is the systematic bias towards group $s$ . Denote $\\theta_{t}^{s}$ as the original optimal threshold at $t$ without the fairness intervention, and let $a_{t}^{s}$ be the acceptance rate for group $s$ under $\\theta_{t}^{s}$ , then the decision-maker can tune the thresholds to get fair-optimal thresholds $\\widetilde{\\theta}_{t}^{s}$ (a pair of thresholds satisfying the fairness constraint with the largest aggregated accuracy). ", "page_idx": 7}, {"type": "text", "text": "Noisy agent best response. To ensure there always exists a pair of thresholds that satisfy DP fairness constraint (i.e., equal acceptance rate), each group\u2019s post-best-response distribution needs to be continuous. However, when agents modify their features based on (1), the aggregate response necessarily exhibits discontinuities [18]. To tackle this issue, we consider the noisy best response model proposed by Jagadeesan et al. [18], which assumes the agents only have imperfect and noisy information of decision threshold. Formally, given decision threshold $\\theta$ , each agent best responds to $\\theta+\\epsilon$ where $\\epsilon$ is a noise independently sampled from a zero mean distribution with finite variance $\\sigma^{2}$ Under noisy best response, we investigate the impacts of fairness intervention. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 (Impact of fairness intervention). Suppose group $j$ is disadvantaged from round 0 to $t_{\\perp}$ , i.e., group $j$ has a smaller acceptance rate than group $i$ under unconstrained optimal thresholds $\\{\\theta_{\\tau}^{i},\\theta_{\\tau}^{j}\\}$ , $\\forall\\tau\\leq t$ . Let $\\sigma_{t}^{2}$ be the variance of the noisy best response at $t$ . We have the following: ", "page_idx": 7}, {"type": "text", "text": "$(i)$ Without fairness intervention, $\\forall\\sigma_{t}$ , there always exists feature distributions and cost matrices under which group $j$ switches to be advantaged at $t+1$ ; ", "page_idx": 7}, {"type": "text", "text": "(ii) With fairness intervention, $i f\\sigma_{t}<(\\theta_{t}^{j}-\\widetilde{\\theta}_{t}^{j})\\sqrt{a_{0}^{i}-a_{t}^{j}}$ , then group $j$ always remains disadvantaged at $t+1$ . ", "page_idx": 7}, {"type": "text", "text": "Thm. 4.2 shows that without fairness intervention, the originally disadvantaged group $j$ can filp to be advantaged. In contrast, the fairness intervention helps maintain the disadvantaged and advantaged groups when the agent\u2019s perception of the decision rule is sufficiently accurate. Note that the bound on $\\sigma$ is well-defined. Since group $j$ is disadvantaged, $a_{0}^{i}>a_{0}^{j}$ always holds. If $\\sigma_{t}<(\\theta_{t}^{j}-\\widetilde{\\theta}_{t}^{j})\\sqrt{a_{0}^{i}-a_{t}^{j}}$ holds, then it is guaranteed that $a_{0}^{i}>a_{t+1}^{j}$ (see App. G.10 for details). This result implies that the disadvantaged group, by losing their chance to become advantaged, may not benefit from fairness intervention in the long run. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on two synthetic (Uniform, Gaussian), one semi-synthetic (German Credit [19]), and one real dataset (Credit Approval [20]) to validate the dynamics of $a_{t},q_{t},\\Delta_{t}$ and the unfairness 2. Note that only the Uniform dataset satisfies all assumptions and the conditions in our theoretical analysis, while the Gaussian and German Credit datasets violate the conditions in Thm. 3.5. The Credit Approval dataset violates all assumptions and conditions of the main paper. The decision-maker trains logistic regression models for all experiments using stochastic gradient descent (SGD) over $T$ steps. We present the experimental results of the Gaussian and German Credit datasets to illustrate the dynamics of $a_{t},q_{t},\\Delta_{t}$ in this section, while the results for Uniform and Credit Approval data are similar and shown in App. E. ", "page_idx": 7}, {"type": "text", "text": "Gaussian data. We consider a synthetic dataset with Gaussian distributed $P_{X}$ . $P_{Y\\mid X}$ is logistic and satisfies Assumption 3.2 but not the conditions of Thm. 3.5. We assume agents have two independent features $X_{1},X_{2}$ and are from two groups $i,j$ with different sensitive attributes but identical joint distribution $P_{X Y}$ . Their cost matrix is $B={\\left[\\begin{array}{l l}{5}&{0}\\\\ {0}&{5}\\end{array}\\right]}$ and the initial qualification rate is $q_{0}=0.5$ . We assume the decision-maker has a systematic bias by overestimating (resp. underestimating) the qualification of agents in the advantaged group $i$ (resp. disadvantaged group $j$ ), which is modeled as increasing $D_{Y\\mid X}^{o}(1|x)$ to be 0.1 larger (resp. smaller) than $P_{Y\\mid X}(1|x)$ for group $i$ (resp. group $j$ ). For the retraining process, we let $\\begin{array}{r}{r=\\frac{K}{N}=0.05}\\end{array}$ (i.e., the number of model-annotated samples $N=2000$ , which is sufficiently large compared to the number of human-annotated samples $K=100)$ ). Table 1 summarizes the dataset information, and the joint distributions are visualized in App. F.1. ", "page_idx": 7}, {"type": "table", "img_path": "2UJLv3KPGO/tmp/4faef6446452e266d088f4958e6023ee6d2896b00b9c62947f5c023dc078e899.jpg", "table_caption": ["Table 1: Gaussian Dataset Setting "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We verify the results in Sec. 3 by illustrating the dynamics of $a_{t},q_{t},\\Delta_{t}$ for both groups (Fig. 5a). Since our evolution results are in expectation, we perform $n=100$ independent runs of experiments for every parameter configuration and show the averaged outcomes. The results are consistent with Thm. 3.3, 3.5 and 3.6: (i) acceptance rate $a_{t}$ (red curves) increases monotonically; (ii) qualification rate $q_{t}$ decreases monotonically starting from $t=1$ (since strategic agents only best respond from $t=1$ ); (iii) classifier bias $\\Delta_{t}$ evolves differently for different groups and it may reach the minimum after a few rounds of retraining. ", "page_idx": 8}, {"type": "text", "text": "We further test the robustness of system dynamics against agent noisy response, where we assume agents estimate their outcomes as ${\\widehat{f}}_{t}(x)=f_{t}(x)+\\epsilon$ with $\\epsilon\\sim\\mathcal{N}(0,0.1)$ . We present the dynamics of $a_{t},q_{t},\\Delta_{t}$ for both groups in Fig. 6a which are similar to Fig. 5a, demonstrating the robustness of our theorems. ", "page_idx": 8}, {"type": "text", "text": "German Credit dataset [19]. This dataset includes features for predicting individuals\u2019 credit risks. It has 1000 samples and 19 numeric features, which are used to construct a larger-scaled dataset. Specifically, we fti a kernel density estimator for all 19 features to generate 19-dimensional features, the corresponding labels are sampled from the distribution $P_{Y\\mid X}$ which is estimated from data by fitting a logistic classifier with 19 features. Given this dataset, the first 10 features are used to train the classifiers. The attribute \"sex\" is regarded as the sensitive attribute. The systematic bias is created by increasing/decreasing $P_{Y\\mid X}$ by 0.06. Other parameters $n,r,T,q_{0}$ are the same as Table 1. Since $P_{Y\\mid X}$ is a logistic function, Assumption 3.2 can be satisfied easily as illustrated in App. F.1. ", "page_idx": 8}, {"type": "text", "text": "We verify the results in Sec. 3 by illustrating the dynamics of $a_{t},q_{t},\\Delta_{t}$ for both groups (Fig. 5b). The results are consistent with Thm. 3.3, 3.5 and 3.6: (i) acceptance rate $a_{t}$ (red curves) always increases; (ii) qualification rate $q_{t}$ (blue curves) decreases starting from $t=1$ (since strategic agents only best respond from $t=1$ ); (iii) classifier bias $\\Delta_{t}$ (black curves) evolve differently for different groups. Finally, similar to Fig. 5b, Fig. 6b demonstrates the results are still robust under the noisy setting. ", "page_idx": 8}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6ee6f048dab29ab3a6554d62fde849d3407711dc64006ccfcddd62a852591ed2.jpg", "img_caption": ["Figure 6: Dynamics of $a_{t},q_{t},\\Delta_{t}$ under the noisy setting "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Additional experiments. We provide more results in App. F to: (i) verify Thm. 3.3 and Thm. 3.6; (ii) Visualize the influence of each factor including training rounds, cost matrices, the ratio between human-annotated and model-annotated samples, whether the agents are strategic, and whether certain assumptions are violated; (iii) visualize the evolution of unfairness when different retraining strategies are applied to different groups. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion & Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper studies the dynamics where strategic agents interact with an ML system retrained over time with model-annotated and human-annotated samples. We rigorously studied the evolution of applicant welfare, decision-maker welfare, and social welfare. Such results highlight the potential risks of retraining classifiers when agents are strategic. The paper also provides a comprehensive analysis on the fairness dynamics associated with the retraining process, revealing that the fairness intervention may not bring long-term benefits. To ease the negative social impacts, we provide mechanisms to stabilize the dynamics and an early stopping mechanism to maintain fairness. However, our theoretical results rely on certain assumptions and we should first verify these conditions before adopting the results of this paper, which may be challenging in real-world applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This material is based upon work supported by the U.S. National Science Foundation under award IIS2202699 and IIS-2416895, by OSU President\u2019s Research Excellence Accelerator Grant, and grants from the Ohio State University\u2019s Translational Data Analytics Institute and College of Engineering Strategic Research Initiative. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, page 111\u2013122, 2016.   \n[2] Sagi Levanon and Nir Rosenfeld. Generalized strategic classification and the case of aligned incentives. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, pages 12593\u201312618, 2022.   \n[3] Rohan Taori and Tatsunori Hashimoto. Data feedback loops: Model-driven amplification of dataset biases. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 33883\u201333920, 2023.   \n[4] George Alexandru Adam, Chun-Hao Kingsley Chang, Benjamin Haibe-Kains, and Anna Goldenberg. Error amplification when updating deployed machine learning models. In Proceedings of the Machine Learning for Healthcare Conference, Durham, NC, USA, pages 5\u20136, 2022.   \n[5] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, page 55\u201370, 2018.   \n[6] Saba Ahmadi, Hedyeh Beyhaghi, Avrim Blum, and Keziah Naggita. The strategic perceptron. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 6\u201325, 2021.   \n[7] Yiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers. Advances in Neural Information Processing Systems, 33:15265\u201315276, 2020.   \n[8] Sierra Wyllie, Ilia Shumailov, and Nicolas Papernot. Fairness feedback loops: training on synthetic data amplifies bias. In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 2113\u20132147, 2024.   \n[9] Ozgur Guldogan, Yuchen Zeng, Jy-yong Sohn, Ramtin Pedarsani, and Kangwook Lee. Equal improvability: A new fairness notion considering the long-term impact. In The Eleventh International Conference on Learning Representations, 2022.   \n[10] Tijana Zrnic, Eric Mazumdar, Shankar Sastry, and Michael Jordan. Who leads and who follows in strategic classification? Advances in Neural Information Processing Systems, pages 15257\u201315269, 2021.   \n[11] Jon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategically? page 1\u201323, 2020.   \n[12] Xueru Zhang, Mohammad Mahdi Khalili, Kun Jin, Parinaz Naghizadeh, and Mingyan Liu. Fairness interventions as (Dis)Incentives for strategic manipulation. In Proceedings of the 39th International Conference on Machine Learning, pages 26239\u201326264, 2022.   \n[13] Reilly Raab and Yang Liu. Unintended selection: Persistent qualification rate disparities and interventions. Advances in Neural Information Processing Systems, pages 26053\u201326065, 2021.   \n[14] Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng Zhang. How do fair decisions fare in long-term qualification? In Advances in Neural Information Processing Systems, pages 18457\u201318469, 2020.   \n[15] Lydia T Liu, Ashia Wilson, Nika Haghtalab, Adam Tauman Kalai, Christian Borgs, and Jennifer Chayes. The disparate equilibria of algorithmic decision making when individuals invest rationally. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 381\u2013391, 2020.   \n[16] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 259\u2013268, 2015.   \n[17] Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, 2016.   \n[18] Meena Jagadeesan, Celestine Mendler-D\u00fcnner, and Moritz Hardt. Alternative microfoundations for strategic classification. In Proceedings of the 38th International Conference on Machine Learning, pages 4687\u20134697, 2021.   \n[19] Hans Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: https://doi.org/10.24432/C5NC77.   \n[20] Quinlan Quinlan. Credit Approval. UCI Machine Learning Repository, 2017. DOI: https://doi.org/10.24432/C5FS30.   \n[21] Quinn Capers IV, Daniel Clinchot, Leon McDougle, and Anthony G Greenwald. Implicit racial bias in medical school admissions. Academic Medicine, 92(3):365\u2013369, 2017.   \n[22] AJ Alvero, Noah Arthurs, Anthony Lising Antonio, Benjamin W Domingue, Ben Gebre-Medhin, Sonia Giebel, and Mitchell L Stevens. Ai and holistic review: informing human reading in college admissions. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 200\u2013206, 2020.   \n[23] Elias Bareinboim and Judea Pearl. Controlling selection bias in causal inference. In Artificial Intelligence and Statistics, pages 100\u2013108, 2012.   \n[24] J Michelle Brock and Ralph De Haas. Discriminatory lending: Evidence from bankers in the lab. American Economic Journal: Applied Economics, 15(2):31\u201368, 2023.   \n[25] Omer Ben-Porat and Moshe Tennenholtz. Best response regression. In Advances in Neural Information Processing Systems, 2017.   \n[26] Mark Braverman and Sumegha Garg. The role of randomness and noise in strategic classification. CoRR, abs/2005.08377, 2020.   \n[27] Zachary Izzo, Lexing Ying, and James Zou. How to learn when data reacts to your model: Performative gradient descent. In Proceedings of the 38th International Conference on Machine Learning, pages 4641\u20134650, 2021.   \n[28] Wei Tang, Chien-Ju Ho, and Yang Liu. Linear models are robust optimal under strategic behavior. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 2584\u20132592, 13\u201315 Apr 2021.   \n[29] Itay Eilat, Ben Finkelshtein, Chaim Baskin, and Nir Rosenfeld. Strategic classification with graph neural networks, 2022.   \n[30] Lydia T. Liu, Nikhil Garg, and Christian Borgs. Strategic ranking. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 2489\u20132518, 2022.   \n[31] Tosca Lechner and Ruth Urner. Learning losses for strategic classification. arXiv preprint arXiv:2203.13421, 2022.   \n[32] Guy Horowitz and Nir Rosenfeld. Causal strategic classification: A tale of two shifts, 2023.   \n[33] Keegan Harris, Hoda Heidari, and Steven Z Wu. Stateful strategic regression. Advances in Neural Information Processing Systems, pages 28728\u201328741, 2021.   \n[34] Yahav Bechavod, Chara Podimata, Steven Wu, and Juba Ziani. Information discrepancy in strategic learning. In International Conference on Machine Learning, pages 1691\u20131715, 2022.   \n[35] Kun Jin, Xueru Zhang, Mohammad Mahdi Khalili, Parinaz Naghizadeh, and Mingyan Liu. Incentive mechanisms for strategic classification and regression problems. In Proceedings of the 23rd ACM Conference on Economics and Computation, page 760\u2013790, 2022.   \n[36] Yatong Chen, Jialu Wang, and Yang Liu. Strategic recourse in linear classification. CoRR, abs/2011.00355, 2020.   \n[37] Nika Haghtalab, Nicole Immorlica, Brendan Lucier, and Jack Z Wang. Maximizing welfare with incentive-aware evaluation mechanisms. arXiv preprint arXiv:2011.01956, 2020.   \n[38] Tal Alon, Magdalen Dobson, Ariel Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent evaluation mechanisms. Proceedings of the AAAI Conference on Artificial Intelligence, 34:1774\u20131781, 2020.   \n[39] Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In International Conference on Artificial Intelligence and Statistics, pages 1234\u20131242, 2021.   \n[40] Tian Xie, Xuwei Tan, and Xueru Zhang. Algorithmic decision-making under agents with persistent improvement. arXiv preprint arXiv:2405.01807, 2024.   \n[41] Tian Xie and Xueru Zhang. Non-linear welfare-aware strategic learning, 2024. URL https: //arxiv.org/abs/2405.01810.   \n[42] John Miller, Smitha Milli, and Moritz Hardt. Strategic classification is causal modeling in disguise. In Proceedings of the 37th International Conference on Machine Learning, 2020.   \n[43] Yonadav Shavit, Benjamin L. Edelman, and Brian Axelrod. Causal strategic linear regression. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920, 2020.   \n[44] Keegan Harris, Dung Daniel T Ngo, Logan Stapleton, Hoda Heidari, and Steven Wu. Strategic instrumental variable regression: Recovering causal relationships from strategic responses. In International Conference on Machine Learning, pages 8502\u20138522, 2022.   \n[45] Tom Yan, Shantanu Gupta, and Zachary Lipton. Discovering optimal scoring mechanisms in causal strategic prediction, 2023.   \n[46] Juan Perdomo, Tijana Zrnic, Celestine Mendler-D\u00fcnner, and Moritz Hardt. Performative prediction. In Proceedings of the 37th International Conference on Machine Learning, pages 7599\u20137609, 2020.   \n[47] Moritz Hardt, Meena Jagadeesan, and Celestine Mendler-D\u00fcnner. Performative power. In Advances in Neural Information Processing Systems, 2022.   \n[48] Nir Rosenfeld, Anna Hilgard, Sai Srivatsa Ravindranath, and David C Parkes. From predictions to decisions: Using lookahead regularization. In Advances in Neural Information Processing Systems, pages 4115\u20134126, 2020.   \n[49] Melissa Hall, Laurens van der Maaten, Laura Gustafson, and Aaron Adcock. A systematic study of bias amplification. arXiv preprint arXiv:2201.11706, 2022.   \n[50] Julius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label errors on model disparity metrics. In The Eleventh International Conference on Learning Representations, 2022.   \n[51] Ali Shirali, Rediet Abebe, and Moritz Hardt. A theory of dynamic benchmarks. In The Eleventh International Conference on Learning Representations, 2022.   \n[52] Klas Leino, Emily Black, Matt Fredrikson, Shayak Sen, and Anupam Datta. Feature-wise bias amplification. arXiv preprint arXiv:1812.08999, 2018.   \n[53] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston. Queens are powerful too: Mitigating gender bias in dialogue generation. arXiv preprint arXiv:1911.03842, 2019.   \n[54] Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations. pages 5310\u20135319, 2019.   \n[55] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. arXiv preprint arXiv:1707.09457, 2017.   \n[56] David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. Advances in neural information processing systems, 28, 2015.   \n[57] Danielle Ensign, Sorelle A Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkatasubramanian. Runaway feedback loops in predictive policing. In Conference on fairness, accountability and transparency, pages 160\u2013171. PMLR, 2018.   \n[58] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. Feedback loop and bias amplification in recommender systems. In Proceedings of the 29th ACM international conference on information & knowledge management, pages 2145\u20132148, 2020.   \n[59] George Alexandru Adam, Chun-Hao Kingsley Chang, Benjamin Haibe-Kains, and Anna Goldenberg. Hidden risks of machine learning applied to healthcare: Unintended feedback loops between models and future data causing model degradation. In Proceedings of the 5th Machine Learning for Healthcare Conference, volume 126 of Proceedings of Machine Learning Research, pages 710\u2013731. PMLR, 2020.   \n[60] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.   \n[61] Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining of generative models on their own data. arXiv preprint arXiv:2310.00429, 2023.   \n[62] Sven Schmit and Carlos Riquelme. Human interaction with recommendation systems. In International Conference on Artificial Intelligence and Statistics, pages 862\u2013870, 2018.   \n[63] Ayan Sinha, David F Gleich, and Karthik Ramani. Deconvolving feedback loops in recommender systems. Advances in neural information processing systems, 29, 2016.   \n[64] Ray Jiang, Silvia Chiappa, Tor Lattimore, Andr\u00e1s Gy\u00f6rgy, and Pushmeet Kohli. Degenerate feedback loops in recommender systems. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, page 383\u2013390, 2019.   \n[65] Vivek Gupta, Pegah Nokhiz, Chitradeep Dutta Roy, and Suresh Venkatasubramanian. Equalizing recourse across groups, 2019.   \n[66] Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 6196\u20136200, 2019.   \n[67] Sagi Levanon and Nir Rosenfeld. Strategic classification made practical. In International Conference on Machine Learning, pages 6243\u20136253, 2021.   \n[68] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL https://archive. ics.uci.edu/ml/datasets/credit+approval.   \n[69] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems, 34:6478\u20136490, 2021.   \nAlgorithm 1 retraining process   \nInput: Joint distribution $D_{X Y}^{o}$ for any $\\textstyle S_{o,t}$ , Hypothesis class $\\mathcal{F}$ , the number of the initial training samples and agents coming per round $N$ , the number of decision-maker-labeled samples per round $K$ .   \nOutput: Model deployments over time $f_{0},f_{1},f_{2},\\ldots$   \n1: $\\dot{S}_{0}=S_{o,0}\\sim\\dot{D_{X Y}^{o}}=\\{x_{0}^{i}\\}_{i=1}^{N}$   \n2: At $t=0$ , deploy $f_{0}\\sim\\mathcal{F}(S_{0})$   \n34:: for $t\\in\\{1,\\ldots\\infty\\}$ $N$ kdnoowledge of $f_{t-1}$ and best respond to it, resulting in $\\{x_{t}^{i}\\}_{i=1}^{N}$   \n5: $\\begin{array}{r}{S_{m,t}=\\{x_{t-1}^{i},f_{t-1}(x_{t-1}^{i})\\}_{i=1}^{N}}\\end{array}$ consists of the model-labeled samples from round $t-1$ .   \n6: $S_{o,t}\\sim D_{X Y}^{o}$ consists of the new $K$ decision-maker-labeled samples.   \n7: $S_{t}=S_{t-1}\\cup S_{m,t}\\cup S_{o,t}$   \n8: Deploy $f_{t}\\sim\\mathcal{F}(\\boldsymbol{S}_{t})$ on the incoming $N$ agents who best respond to $f_{t-1}$ with the resulting joint distribution $P_{X Y}^{t}$ .   \n9: end for ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Motivating examples of the systematic bias ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Def. 2.1 highlights the systematic nature of the decision-maker\u2019s bias. This bias is quite ubiquitous when labeling is not a trivial task. It almost always the case when the decision-maker needs to make human-related decisions. We provide the following motivating examples of systematic bias with supporting literature in social science: ", "page_idx": 14}, {"type": "text", "text": "1. College admissions: consider experts in the admission committee of a college that obtains a set of student data and wants to label all students as \"qualified\" or \"unqualified\". The labeling task is much more complex and subjective than the ones in computer vision/natural language processing which have some \"correct\" answers. Therefore, the experts in the committee are prone to bring their \"biases\" towards a specific population sharing the same sensitive attribute into the labeling process including: ", "page_idx": 14}, {"type": "text", "text": "(a) Implicit bias: the experts may have an implicit bias they are unaware of to favor/discriminate against students from certain groups. For instance, a famous study [21] reveals admission committee members at the medical school of the Ohio State University unconsciously have a \"better impression\" towards white students; Alvero et al. [22] finds out that even when members in an admission committee do not access the sensitive attributes of students, they unconsciously infer them and discriminate against students from the minority group. ", "page_idx": 14}, {"type": "text", "text": "(b) Selection bias: the experts may have insufficient knowledge of the under-represented population due to the selection bias [23] because only a small portion of them were admitted before. Thus, experts may expect a lower qualification rate from this population, resulting in more conservative labeling practices. The historical stereotypes created by selection bias are difficult to erase. ", "page_idx": 14}, {"type": "text", "text": "2. Loan applications: consider experts in a big bank that obtains data samples from some potential applicants and wants to label them as \"qualified\" or \"unqualified\". Similarly, the experts are likely to have systematic bias including: ", "page_idx": 14}, {"type": "text", "text": "(a) Implicit bias: similarly, Brock and De Haas [24] conduct a lab-in-the-field experiment with over 300 Turkish loan officers to show that they bias against female applicants even if they have identical profiles as male applicants. ", "page_idx": 14}, {"type": "text", "text": "(b) Selection bias: when fewer female applicants are approved historically, the experts have less knowledge on females (i.e., whether they will actually default or repay), thereby tending to stay conservative. ", "page_idx": 14}, {"type": "text", "text": "C Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Strategic Classification ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Strategic classification without label changes. Our work is mainly based on an extensive line of literature on strategic classification [1, 2, 5, 6, 7, 12, 14, 18, 25, 26, 27, 28, 29, 30, 31, 32]. These works assume the agents are able to best respond to the policies of the decision-maker to maximize their utilities. Most works modeled the strategic interactions between agents and the decision-maker as a repeated Stackelberg game where the decision-maker leads by publishing a classifier and the agents immediately best respond to it. The earliest line of works focused on the performance of regular linear classifiers when strategic behaviors never incur label changes [1, 5, 7, 25], while the later literature added noise to the agents\u2019 best responses [18], randomized the classifiers [26] and limited the knowledge of the decision-maker [28]. Levanon and Rosenfeld [2] proposed a generalized framework for strategic classification and a strategic hinge loss to better train strategic classifiers, but the strategic behaviors are still not assumed to cause label changes. ", "page_idx": 15}, {"type": "text", "text": "Strategic classification with label changes. Several other lines of literature enable strategic behaviors to cause label changes. The first line of literature mainly focuses on incentivizing improvement actions where agents have budgets to invest in different actions and only some of them cause the label change (improvement) [11, 13, 33, 34, 35, 36, 37, 38, 39, 40, 41]. The other line of literature focuses on causal strategic learning [32, 42, 43, 44, 45]. These works argue that every strategic learning problem has a non-trivial causal structure which can be explained by a structural causal model, where intervening on causal nodes causes improvement and intervening on non-causal nodes means manipulation. ", "page_idx": 15}, {"type": "text", "text": "Performative prediction. Several works consider performative prediction as a more general setting where the feature distribution of agents is a function of the classifier parameters. Perdomo et al. [46] first formulated the prediction problem and provided iterative algorithms to find the stable points of the model parameters. Izzo et al. [27] modified the gradient-based methods and proposed the Perfgrad algorithm. Hardt et al. [47] elaborated the model by proposing performative power. ", "page_idx": 15}, {"type": "text", "text": "Retraining under strategic settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Most works on learning algorithms under strategic settings consider developing robust algorithms that the decision-maker only trains the classifier once [1, 2, 18, 28], while Performative prediction[46] focuses on developing online learning algorithms for strategic agents. There are only a few works [32, 48] which permit retraining the Strategic classification models. However, all these algorithms assume that the decision-maker has access to a new training dataset containing both agents\u2019 features and labels at each round. There is no work considering the dynamics under the retraining process with model-annotated samples. Also, few works [14, 15] discussed the long-term fairness issues under sequential strategic settings. Liu et al. [15] modeled strategic behaviors in a completely different way where each individual decides whether or not (a binary choice) to acquire the desired qualification based on a cost drawn from a fixed distribution. Moreover, they assumed the decision maker has perfect knowledge of the agent distribution, i.e., can draw infinitely many examples from the agent population. Instead, our work focuses on a practical setting where the decision maker must learn from samples and acquiring human-annotated samples is quite expensive, motivating the use of model-annotated samples as well. Zhang et al. [14] also simplified the modeling of the qualification changes of agents by using fixed transition matrices. ", "page_idx": 15}, {"type": "text", "text": "C.2 Bias amplification during retraining ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "There has been an extensive line of study on the computer vision field about how machine learning models amplify the dataset bias, while most works only focus on the one-shot setting where the machine learning model itself amplifies the bias between different groups in one training/testing round [49, 50]; Another work theoretically studies dynamic benchmarking [51] to propose a retraining scheme to increase model accuracy with adversarial human-annotation. In recent years, another line of research focuses on the amplification of dataset bias under model-annotated data where ML models label new samples on their own and add them back to retrain themselves[4, 52, 53, 54, 55, 56, 57, 58, 59]. These works study the bias amplification in different practical fields including resource allocation [57], computer vision [54], natural language processing [55], generative models [60, 61] and clinical trials [59]. The most related work is [3] which studied the influence of retraining in the non-strategic setting. Also, there is a work [4] touching on the data feedback loop under performative setting, but it focused on empirical experiments under medical settings where the feature distribution shifts are mainly caused by treatment and the true labels in historical data are highly accessible. Besides, the data feedback loop is also related to recommendation systems. where extensive works have studied how the system can shape users\u2019 preferences and disengage the minority population [58, 62, 63, 64]. However, previous literature did not touch on the retraining process. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "C.3 Machine learning fairness in strategic classification ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Several works have considered how different fairness metrics [1, 9, 16, 65] are influenced in strategic classification [12, 14, 15, 66]. The most related works [12, 15] studied how strategic behaviors and the decision-maker\u2019s awareness can shape long-term fairness. They deviated from our paper since they never considered retraining and the strategic behaviors never incurred label changes. ", "page_idx": 16}, {"type": "text", "text": "D Additional Discussions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 The source of Human-annotated samples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As stated in footnote 1, for the source of human-annotated samples, we consider natural settings where the human-annotated samples are drawn from fixed $P_{X}$ and the process is independent of the decision-making process, i.e., the agents who best respond to $f_{t-1}$ are classified by model $f_{t}$ and the decision-maker never confuses them with human annotations. Instead, human annotation is a separate process for the decision-maker to obtain additional information about the whole population (e.g., by first acquiring data from public datasets or third parties, and then labeling them to estimate the population distribution $P_{X Y}$ ). Here we provide an additional \\*\\*example\\*\\*: In a university admission scenario, the admission office uses the model $f_{t}$ to assign decisions at $t$ and obtain modelannotated samples for $t+1$ . But for human-annotated samples, the admission committee hopes to acquire a more objective knowledge of \"what kinds of students are successful\" from the whole student population. Thus, in the hope of avoiding additional sampling/selection bias, the committee seeks human annotations from third-party education researchers/experts. The experts can provide qualifications of the public/historical student samples drawn from the whole student population including students who are not applicants for this specific university or even do not attend a university. ", "page_idx": 16}, {"type": "text", "text": "In this section, we additionally consider the situation where all human-annotated samples at $t$ are drawn from the post-best-response distribution $P_{X}^{t}$ . This will change (4) to the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{q^{\\prime}}}_{t}=\\frac{t N+(t-1)K}{(t+1)N+t K}\\cdot\\overline{{q^{\\prime}}}_{t-1}+\\frac{N}{(t+1)N+t K}\\cdot a_{t-1}^{\\prime}\\,+\\frac{K}{(t+1)N+t K}\\cdot q_{t-1}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $q^{\\prime}$ and $a^{\\prime}$ denote the new qualification rate and acceptance rate, and $q_{t-1}^{*}$ stands for the qualification rate of the human annotations on features drawn from $P_{X}^{t-1}$ . Note that the only difference lies in the third term of the RHS which changes from $\\overline{{q}}_{0}$ to $q_{t-1}^{*}$ . Our first observation is that $q_{t-1}^{*}$ is never smaller than $\\overline{{q}}_{0}$ because the best response will not harm agents\u2019 qualifications. With this observation, we can derive Prop. D.1. ", "page_idx": 16}, {"type": "text", "text": "Proposition D.1. $a_{t}^{\\prime}\\geq a_{t}$ holds for any $t\\geq1$ . If Prop. 3.4 further holds, we also have $a_{t}^{\\prime}\\rightarrow1$ . ", "page_idx": 16}, {"type": "text", "text": "Prop. D.1 can be proved easily by applying the observation stated above. However, note that unlike $a_{t}$ , $a_{t}^{\\prime}$ is not necessarily monotonically increasing. ", "page_idx": 16}, {"type": "text", "text": "D.2 Additional discussions on fairness ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Other fairness metrics. It is difficult to derive concise results considering other fairness metrics including equal opportunity and equal improvability because the data distributions play a role in determining these metrics. However, Theorem 3.5 states the true qualification rate of the agent population is likely to decrease, suggesting the retraining process may do harm to improvability. Meanwhile, when the acceptance rate $a_{t}$ increases for the disadvantageous group, the acceptance rate of the qualified individuals will be likely to be better, but it is not guaranteed because the feature distribution of the qualified individuals also changes because we assume the strategic behaviors are causal which may incur label changes. ", "page_idx": 16}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/476b323d2b5a663b966c0fc04bf2c4127d6da18818de265b05bdac2523bfd259.jpg", "img_caption": ["Figure 7: Visualization of distribution: Uniform data (left) and Gaussian data (right) "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.3 Additional Discussion on the training dataset $\\mathcal{S}_{t}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$\\mathcal{S}_{t}$ reuses the accumulated data instead of only fine-tuning the model using the available data at the current round. This is more reasonable because: (i) The distribution shifts caused by agent best responses are not known by the decision maker, only using current data may result in forgetting the previous samples that may still be useful. Since agents\u2019 best responses will change $Y$ and will not break $P_{Y\\mid X}$ , previous samples can provide useful information, especially on the feature domain that current samples do not cover; (ii) the sample size at a single round may be too small even for fine-tuning (e.g., for a college admission example, if we only have tens of people applying during some application round). ", "page_idx": 17}, {"type": "text", "text": "Moreover, only using current data does not qualitatively change the theoretical results. Since the theoretical results demonstrate monotonic trends of $a_{t},q_{t}$ , only using the most recent data will not alter the trends. We perform an additional set of simulations where the decision maker only uses samples from the most recent round $(S_{m,t-1},S_{o,t-1})$ on Gaussian Data and Uniform-linear Data with the experimental setups same as the ones in the paper (Section 5 for Gaussian Data and App. E for Uniform-linear Data), and report the average of $a_{t},q_{t}$ . Dynamics of $a_{t},q_{t}$ are still similar. ", "page_idx": 17}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/32ddd3d5cda5dcf09e4026f12c823bed92fcba82c3340e32080e9c58dc860729.jpg", "img_caption": ["Figure 8: Dynamics of $a_{t},q_{t},\\Delta_{t}$ when $\\mathcal{S}_{t}$ only contains the most recent samples "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.4 Additional Discussions on refined retraining process ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide the following proposition to illustrate how the refined retraining process leverages a probabilistic sampler to stabilize the dynamics. ", "page_idx": 17}, {"type": "text", "text": "Proposition D.2. If the decision-maker uses a probabilistic sampler $h_{t-1}(x)\\,=\\,D_{Y\\mid X}^{t-1}(1|x)$ to produce model-annotated samples at $t,$ , then $D_{Y\\mid X}^{t}=D_{Y\\mid X}^{o}$ . ", "page_idx": 17}, {"type": "text", "text": "The proof details are in App. G.7. Prop. D.2 illustrates the underlying conditional distribution $D_{Y\\mid X}^{t}$ is expected to be the same as $D_{Y\\mid X}^{o}$ , meaning that the classifier $f_{t}$ always learns the distribution of human-annotated data, thereby only preserving the systematic bias. However, there is no way to deal with the systematic bias in refined retraining process. ", "page_idx": 17}, {"type": "text", "text": "D.5 Additional Discussions on the linearity of the model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our model is built upon previous works on strategic classification (e.g., [2, 12, 32]) where the decision policy was assumed to be transparent and interpretable since human agents expect to face a linear classifier which they can understand, especially under high-stake situations such as job hiring, college admission and loan application. Meanwhile, as pointed out by Zhang et al. [12], Raab and Liu [13], Levanon and Rosenfeld [67], in practical circumstances, the decision-maker can first fit a non-linear model to learn the embeddings of agents\u2019 preliminary features and then produce the embedded features. The decision-maker then uses a linear classifier on the new set of features and agents also best respond with respect to new features. Under this setting, nonlinearity is involved but our results still hold. Practical examples include (i) FICO credit score [12]: FICO credit score is based on a complex set of features, and the decision-maker (e.g., a bank) simply uses a threshold of the score to assign decisions; (ii) Spam classification [67]: The decision-maker classifies spam using a linear classifier, but the features are produced as an embedding by a neural network based on number of words in the post, number of phone numbers in the post and number of followers of the user. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E Main Experiments for other datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide results on a Uniform dataset and a real dataset [20] with settings similar to Sec. 5. ", "page_idx": 18}, {"type": "text", "text": "Uniform data. All settings are similar to the Gaussian dataset except that $P_{X}$ and $P_{Y\\mid X}(1|x)$ change as shown in Table 2. ", "page_idx": 18}, {"type": "text", "text": "We first verify the results in Sec. 3 by illustrating the dynamics of $a_{t},q_{t},\\Delta_{t}$ for both groups (Fig. 9). Since our analysis neglects the algorithmic bias and the evolution results are in expectation, we perform $n\\,=\\,100$ independent runs of experiments for every parameter configuration and show the averaged outcomes. The results are consistent with Thm. 3.3, 3.5 and 3.6: (i) acceptance rate $a_{t}$ (red curves) increases monotonically; (ii) qualification rate $q_{t}$ decreases monotonically starting from $t=1$ (since strategic agents only best respond from $t=1$ ); (iii) classifier bias $\\Delta_{t}$ evolves differently for different groups and it may reach the minimum after a few rounds of retraining. ", "page_idx": 18}, {"type": "table", "img_path": "2UJLv3KPGO/tmp/2ee24ae82f6e89f8920dcb49d749a70bb2ef5332b59de7cbc41df2529975a818.jpg", "table_caption": ["Table 2: Gaussian Dataset Setting "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/250a5bc48ca99ce356cee95fc681c811a746a20c7e39442a28c317fb0df73896.jpg", "img_caption": ["Figure 9: $a_{t},\\,q_{t}$ , $\\Delta_{t}$ for group $i$ (left) and $j$ (right) "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Next, we present the results of a set of complementary experiments on real data [20] where we directly fit $P_{X\\mid Y}$ with Beta distributions. The fitting results slightly violate Assumption 3.2. Also $D_{X}^{o}$ is not equal to $P_{X}$ . More importantly, logistic models cannot fit these distributions well and produce non-negligible algorithmic bias, thereby violating Assumption 3.1. The following experiments demonstrate how the dynamics change when situations are not ideal. ", "page_idx": 18}, {"type": "table", "img_path": "2UJLv3KPGO/tmp/16c4373d6c89453242a4a04bb39a20c1803bf16a69f65a9885dce13275c6e097.jpg", "table_caption": ["Table 3: Description of credit approval dataset "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Credit approval dataset [20]. We consider credit card applications and adopt the data in UCI Machine Learning Repository processed by Dua and Graff [68]. The dataset includes features of agents from two social groups $i,j$ and their labels indicate whether the credit application is successful. We first preprocess the dataset by normalizing and only keeping a subset of features (two continuous $X_{1},X_{2})$ and labels, then we fti conditional distributions $P_{X_{k}|Y}$ for each group using Beta distributions (Fig. 10) and calculate prior-best-response qualification rates $q_{0}^{i},q_{0}^{j}$ from the dataset. The details are summarized in Table 3. All other parameter settings are the same as the ones of synthetic datasets in Sec. 5. ", "page_idx": 18}, {"type": "text", "text": "We first illustrate the dynamics of $a_{t},q_{t},\\Delta_{t}$ for both groups under different $r$ . The results are shown in Fig. 11 and are approximately aligned with Thm. 3.3, 3.5 and 3.6: (i) acceptance rate $a_{t}$ (red curves) has increasing trends; (ii) qualification rate $q_{t}$ (blue curves) decreases starting from $t=1$ (since strategic agents only best respond from $t=1$ ); (iii) classifier bias $\\Delta_{t}$ (black curves) evolve differently for different groups. ", "page_idx": 18}, {"type": "text", "text": "Next, we illustrate situation (iii) of Thm. 4.1 on this dataset, where the evolutions of unfairness and $\\Delta_{t}$ are shown in Fig. 12. Though the dynamics are still approximately aligned with the theoretical results, the changes are not smooth. However, this is not surprising because several assumptions are violated, and the overall trends still stay the same. ", "page_idx": 19}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/50edaf3b6c083bc1d0e28d131270cdf2d8dfda499d7ea163b74ce964d24cba25.jpg", "img_caption": ["Figure 10: Visualization of distribution for Credit Approval dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/a6bd402f4b3447da58f8dabd75d6129c1ea60d4f729c976e2ca5b128122fc902.jpg", "img_caption": ["(b) Group $j$ in Credit Approval data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "Figure 11: Dynamics of $a_{t},q_{t},\\Delta_{t}$ for Credit Approval dataset "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/e03d26158f5bc655aacc4306814299e679e16b58eed9562abf41622ab5539b3b.jpg", "img_caption": ["(a) Dynamics of unfairness in Credit Approval data: $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6c169749d38ad9f40e6575628461dcfd6c9d54280b21b2b9c72c003bb0191ccc.jpg", "img_caption": ["(b) Dynamics of $\\Delta_{t}$ in Credit Approval data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "Figure 12: Dynamics of unfairness and $\\Delta_{t}$ for Credit Approval dataset "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Additional Results on Synthetic/Semi-synthetic Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide comprehensive experimental results conducted on two synthetic datasets and one semi-synthetic dataset mentioned in Sec. 5 of the main paper. Specifically, App. F.1 gives the details of experimental setups; App. F.2 demonstrates additional results to verify Theorem 3.3 to Theorem 3.6 under different $r$ (i.e., ratios of human-annotated examples available at each round). The section also gives results on how $a_{t},q_{t},\\Delta_{t}$ change under a long time horizon or when there is no systematic bias; App. F.3 further demonstrates the results under refined retraining process; App. F.4 provides fairness dynamics under various values of $r$ on different datasets; App. F.5 illustrates how the results in the main paper still hold when strategic agents have noisy best responses; App. F.6 compares the situations when agents are non-strategic with the ones when they are strategic, demonstrating how agents\u2019 strategic behaviors produce more extreme dynamics of $a_{t},q_{t},\\Delta_{t}$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "F.1 Additional Experimental Setups ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Generally, we run all experiments on a MacBook Pro with Apple M1 Pro chips, memory of 16GB and Python 3.9.13. All experiments are randomized with seed 42 to run $n$ rounds. Error bars are provided in App. H. All experiments train $f_{t}$ with a logistic classifier using SGD as its optimizer. Specifically, we use SGDClassifier with logloss to fit models. ", "page_idx": 20}, {"type": "text", "text": "Synthetic datasets. The basic description of synthetic datasets 1 and 2 is shown in Sec. 5 and App.   \nE. We further provide the visualizations of their distributions in Fig. 7. ", "page_idx": 20}, {"type": "text", "text": "German Credit dataset. There are 2 versions of the German Credit dataset according to UCI Machine Learning Database [19], and we are using the one where all features are numeric. Firstly, we produce the sensitive features by ignoring the marital status while only focusing on sex. Secondly, we use MinMaxScaler to normalize all features. The logistic model itself can satisfy Assumption 3.2 with minimal operations: if feature $i$ has coefficients smaller than 0, then just negate it and the coefficients will be larger than 0 and satisfy the assumption. ", "page_idx": 20}, {"type": "text", "text": "F.2 Additional Results to Verify Thm. 3.3 to Thm. 3.6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Dynamics under different $r$ . Although experiments in Sec. 5 and App. E already demonstrate the validity of Thm. 3.3, Prop. 3.4 and Thm. 3.6, the ratio $r$ of human-annotated examples is subject to change in reality. Therefore, we first provide results for $r\\in\\{0,0.05,0.1,0.3\\}$ in all 3 datasets. $r$ only has small values since human-annotated examples are likely to be expensive to acquire. Fig. 17 shows all results under different $r$ values. Specifically, Fig. 17a and 17b show results for synthetic dataset 1, Fig. 17c and 17d show results for synthetic dataset 2, while Fig. 17e and 17f show results for German Credit data. On every row, $r=0.3,0.1,0.05,0$ from the left to the right. All figures demonstrate the robustness of the theoretical results, where $a_{t}$ always increases and $q_{t}$ decreases starting from $t=1$ . $\\Delta_{t}$ also has different dynamics as specified in Thm. 3.6. ", "page_idx": 20}, {"type": "text", "text": "Dynamics under a long time horizon to verify Prop. 3.4. Prop. 3.4 demonstrates that when $r$ is small enough, $a_{t}$ will increase towards 1. Therefore, we provide results in all 3 datasets when $r=0$ to see whether $a_{t}$ increases to be close to 1. As Fig. 15 shows, $a_{t}$ is close to 1 after tens of rounds, validating Prop. 3.4. ", "page_idx": 20}, {"type": "text", "text": "Dynamics under a different $B$ . Moreover, individuals may incur different costs to alter different features, so we also provide the dynamics of $a_{t},q_{t},\\Delta_{t}$ when the cost matrix $B={\\left[\\begin{array}{l l}{3}&{0}\\\\ {0}&{6}\\end{array}\\right]}$ in two synthetic datasets. Fig. 16 shows the differences in costs of changing different features do not affect the theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Dynamics when all samples are human-annotated. Though this is unlikely to happen under the Strategic Classification setting as justified in the main paper, we provide an illustration when all training examples are human-annotated (i.e., $r=1$ ) when humans systematically overestimate the qualification in both synthetic datasets. Theoretically, the difference between $a_{t}$ and $q_{t}$ should be relatively consistent, which means $\\Delta_{t}$ is only due to the systematic bias. Fig. 15d verifies this. ", "page_idx": 20}, {"type": "text", "text": "F.3 Additional Results on refined retraining process ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide more experimental results demonstrating how refined retraining process stabilizes the dynamics of $a_{t},q_{t},\\Delta_{t}$ but still preserves the systematic bias. Specifically, we produce plots similar to Fig. 17 in Fig. 18, but the only difference is that we use probabilistic samplers for ", "page_idx": 20}, {"type": "text", "text": "model-annotated examples. From Fig. 18, it is obvious the deviations of $a_{t}$ from $q_{t}$ have the same directions and approximately the same magnitudes as the systematic bias. ", "page_idx": 21}, {"type": "text", "text": "F.4 Additional Results on Fairness ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide additional results on the dynamics of unfairness and classifier bias under different $r$ (the same settings in App. F.2). We aim to illustrate situation (iii) in Thm. 4.1 where the refined retraining process is applied on the advantaged group $i$ while the original process is applied on the disadvantaged group $j$ . From Fig. 19a, 19c and 19e, we can see unfairness reaches a minimum in the middle of the retraining process, suggesting the earlier stopping of retraining brings benefits. From Fig. 19b, 19d and 19f, we can see $\\Delta_{t}$ for the disadvantaged group $j$ reaches a minimum in the middle of the retraining process, but generally not at the same time when unfairness reaches a minimum. ", "page_idx": 21}, {"type": "text", "text": "F.5 Additional Results on Noisy Best Responses ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Following the discussion in Sec. 5, we provide dynamics of $a_{t},q_{t},\\Delta_{t}$ of both groups under different $r$ similar to App. F.2 but when the agents have noisy knowledge. The only difference is that the agents\u2019 best responses are noisy in that they only know a noisy version of classification outcomes: $\\tilde{\\widehat{f_{t}}}(x)=f_{t}(x)+\\epsilon$ , where $\\epsilon$ is a Gaussian noise with mean 0 and standard deviation 0.1. Fig.20 shows that Thm. 3.3 to Thm. 3.6 are still valid. ", "page_idx": 21}, {"type": "text", "text": "F.6 Comparisons between Strategic and Non-strategic Situations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we show the absence of strategic behaviors may result in much more consistent dynamics of $a_{t},q_{t},\\Delta_{t}$ as illustrated in Fig. 21. ", "page_idx": 21}, {"type": "text", "text": "F.7 Additional Results on a Dataset with Non-Linearity. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "ACSIncome-CA dataset [69] is a larger dataset consisting of over $150K$ records for agents and their annual income. The decision-maker wants to predict whether a person has an annual income $>50000$ . We assume the decision-maker first trains a 2-d embedding using a neural network and 53 original features then regards the embedding as the new feature. We divide the agents into 2 groups based on their ages. Similar to the credit approval dataset, we then fit Beta distributions on the 2 groups and then verify the monotonic likelihood assumption (Fig. 13). We then plot the dynamics of $a_{t},q_{t},\\Delta_{t}$ for both groups when the systematic bias is either positive or negative. The results show that similar trends still hold for this large dataset (Fig. 14). ", "page_idx": 21}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6fd0fd37f366ac2ea63992406e22a855315deec62a9629d8a7240a94a503c1e3.jpg", "img_caption": ["Figure 13: Feature distribution of Income Dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/f17cafdd75331d7b97ade6039b98baeec71fad22312aacf9c9f540aa44a3b952.jpg", "img_caption": ["Figure 14: Dynamics of $a_{t},q_{t},\\Delta_{t}$ of the income dataset: the left plot is for group $i$ and the right plot is for group $j$ . We run 10 trials for each experiment and agent cost matrices are the same as the experiments in the main paper. The results are similar to the ones in the main paper. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/67279a47bbd80a756508668ea6607db42a5c149ac42738584a35097301c1c7f0.jpg", "img_caption": ["(a) Uniform data when $r=0$ and $T$ is large: Group $i$ (left), Group $j$ (right) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/431bed5564f43e74ca8bd06b5cab6ca292548e2344d39c3befc10deca2043e00.jpg", "img_caption": ["(c) German Credit data when $r\\,=\\,0$ and $T$ is large: Group $i$ (left), Group $j$ (right) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/2b3e69324468ce1eecfe6678e16f4e47592f0361a5db75545f6467bbca680a37.jpg", "img_caption": ["(b) Gaussian data when $r=0$ and $T$ is large: Group $i$ (left), Group $j$ (right) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/51a42861c9fb918bbb7151bf1821351104563467d8be5f19bf140ce7524541f7.jpg", "img_caption": ["(d) Dynamics when all data is human-annotated (i.e, $r=1_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}$ ): Uniform data (left), Gaussian data (right). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 15: Dynamics of $a_{t},q_{t},\\Delta_{t}$ on all datasets when $r=0$ and $T$ is large or when all examples are annotated by humans. ", "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6aeef5fbd31d65f1b44b5f2b20fa3e44c9d88d413eef06ae157223aa1d3bb049.jpg", "img_caption": ["(a) Uniform data with a different $B$ : Group $i$ (left), Group $j$ (right) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/7c50926bbe16bbfb9155195e3ca611c6d63deeb22066cd06fe3b7c96329104c6.jpg", "img_caption": ["(b) Gaussian data with a different $B$ : Group $i$ (left), Group $j$ (right) "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 16: Dynamics of $a_{t},q_{t},\\Delta_{t}$ on synthetic datasets. Except $B$ , all other settings are as same as in Sec. 5. ", "page_idx": 22}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/be7e6f0d27662e90e8036f2204195814c415592c12795f48724674f74d7130ec.jpg", "img_caption": ["(a) Group i in Uniform data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/69e340e7a57c76d10762b200b117971e2125424d195fb9f90c0f81f4d5500cb9.jpg", "img_caption": ["(b) Group $j$ in Uniform data : r = 0.3, 0.1, 0.05, 0 from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/1da20f037a625fb999d58c1433ac46f6de022a400d91f727cb487f28ceecb84f.jpg", "img_caption": ["(c) Group $i$ in Gaussian data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/ef6f1e87fb39db614c910a01f51cce83cb3ad854dc4080daf1a95ba6e67f435a.jpg", "img_caption": ["(d) Group $j$ in Gaussian data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/296c710c819d1efcd86c32b0cefa7609f0c7375a9cc365ca3704c8afcce8880c.jpg", "img_caption": ["(e) Group $i$ in German Credit data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/884ae59e138092c5735fdadf2212694147d798ef92467f7529284c02b92ddb8a.jpg", "img_caption": ["(f) Group $j$ in German Credit data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost ", "Figure 17: Dynamics of $a_{t},q_{t},\\Delta_{t}$ on all datasets. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/01a0b6377be1be75cf06d2e241f46a19c5f1e2af49e1e13880fc8701b3138f57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(a) refined retraining process for Group $i$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 24}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/3050699dc24aa3d0ece00896b0f3a2cc2edab35885927df04771e176758f8060.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(b) refined retraining process for Group $j$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 24}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/3726ae0945274df4fd4cb0bf5b98fef5811ea5a88bb67b465ddf169c1245bd90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(c) refined retraining process for Group $i$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 24}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/a1f89c92eee442b159b9ece22f167cd9b5ac205759252510fe6c93612ca22c36.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(d) refined retraining process for Group $j$ in Gaussian data: $r\\,=\\,0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 24}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/685009c9af62d8865e67fe98e54f5baea61fbca243da0cfe041397dafce3b858.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(e) refined retraining process for Group $i$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 24}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/dc7ac71db8e449311d8291736834e909e673d1d0b14c17f613da4677a93ebc19.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(f) refined retraining process for Group $j$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right). ", "page_idx": 24}, {"type": "text", "text": "Figure 18: Illustrations of refined retraining process on all 3 datasets. ", "page_idx": 24}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6ffa8b2b9c03769a2551a38080cc72b57b97ffc5f4862ce9b015d4344771d678.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "(a) Dynamics of unfairness in Uniform data : $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 25}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/79af5099950e8b3de7b3547e6a1be5d4635a4d85ecd74b5938c54ae455e9257c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "(b) Dynamics of $\\Delta_{t}$ in Uniform data : $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 25}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/e57fcb33b81b18d02cc59918fec63a82f52378e6a08d4e719cee609b0e422f29.jpg", "img_caption": ["(c) Dynamics of unfairness in Gaussian data : $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/10326b65f6b768bf6c01e914c961055bedebfd1e6a0ea7a9d8ab7a6298222f2d.jpg", "img_caption": ["(d) Dynamics of $\\Delta_{t}$ in Gaussian data : $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/0fe10e9b4d2a44c8e332a21f850ee188d8b83a75090456853ce1ab25b0018ed8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "(e) Dynamics of unfairness in German Credit data : $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 25}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/13fb9e13d10513e82ae63bf1195b40253fb07233d300723b5d2684b5237ca586.jpg", "img_caption": ["Figure 19: Dynamics of unfairness and $\\Delta_{t}$ of all datasets. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "(f) Dynamics of $\\Delta_{t}$ in German Credit data : $r\\,=\\,0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 25}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/ce88dac99e3320e13b874af11bbaec2798a1ac529ce2b3c8a18d5c97e1d81a48.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(a) Noisy retraining process for Group $i$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 26}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/edbfa539e50ebf28d5a3ce23d05fb41177b4a81cb42f80e54530669b552d9a96.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(b) Noisy retraining process for Group $j$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 26}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/0556509e4c3f3b72358cdc70dfca979e7b14746b8bdf2f2428a19e117eb9ee16.jpg", "img_caption": ["(c) Noisy retraining process for Group $i$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/a3621c25f4cf91fe66331698a6bfe305a90e236f34f628b8836b32f10276a040.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(d) Noisy retraining process for Group $j$ in Gaussian data: $r\\,=\\,0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 26}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/a32eea2390e076c5a800d65b0a910b35f5b87ec328a83981847cf18111d51b5d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(e) Noisy retraining process for Group $i$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 26}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/1c4c6b313a614acca928a7942bc35e6446d0e7e4d91cf7376b2b302b41a612e7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(f) Noisy retraining process for Group $j$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 26}, {"type": "text", "text": "Figure 20: Illustrations of noisy retraining process on all 3 datasets. ", "page_idx": 26}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/e38f537f059ab849961e1c102487f1bff04d613492c265de419baaceccbd005f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/10f5525f5c7c4ab6a227f1dc6095cd01d06396cedfcf25d9ef5af31c5534842e.jpg", "img_caption": ["(b) Nonstrategic retraining process for Group $j$ in Uniform data "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/561b2c7b60cab899db3608b07dd3cd7f4a01b4382c812a48ba4c5410d0dfe085.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/eaa8b532790736fdd74dc3a3a14fb425e996fa429e0fdcc6d391a1e768bdb2ae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/c1b00e96d1719f4231f9372a2924a0b7a5053b789d50c6a826419ed62fe11f8b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/2a3ab84388ead3eb79f868b08aa38815f2444544e8dd058e733fbff9e2158157.jpg", "img_caption": ["(f) Nonstrategic retraining process for Group $j$ in German Credit data "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 21: Illustrations of nonstrategic retraining process on all 3 datasets: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 27}, {"type": "text", "text": "G Derivations and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "G.1 Dynamics of the Expected Qualification Rate of $\\mathcal{S}_{t}$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Let us begin by defining $\\overline{{q}}_{t}:=\\mathbb{E}_{S_{t}}[Q(S_{t})]$ which is the expected qualification rate of the training dataset at $t$ . While it is difficult to derive the dynamics of $a_{t}$ and $q_{t}$ explicitly, we can first work out the dynamics of $\\overline{{q}}_{t}$ using the law of total probability (details in App. G.1), i.e., ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{q}}_{t}=\\frac{t N+(t-1)K}{(t+1)N+t K}\\cdot\\overline{{q}}_{t-1}+\\frac{N}{(t+1)N+t K}\\cdot a_{t-1}\\;+\\frac{K}{(t+1)N+t K}\\cdot\\overline{{q}}_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To derive $\\overline{{q}}_{t}:=\\mathbb{E}_{S_{t}}[Q(S_{t})]$ , we first refer to (2) to get $|S_{t}|=(t+1)N+t K$ . Then, by the law of total probability, the expected qualification rate of $\\boldsymbol{S}_{t}$ equals to the weighted sum of the expected qualification rate of $\\mathcal{S}_{t-1}$ , $S_{o,t-1}$ and the expectation of $f_{t-1}(x)$ over $S_{m,t-1}$ as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{t N+(t-1)K}{(t+1)N+t K}\\,\\mathbb{E}_{\\mathcal{S}_{t-1}}[Q(\\mathcal{S}_{t-1})]+\\frac{N}{(t+1)N+t K}+\\mathbb{E}_{\\mathcal{S}_{t-1}}[A(f_{t-1},P^{t-1})]+\\frac{K}{(t+1)N+t K}\\,\\mathbb{E}_{\\mathcal{S}_{o,t-1}}[Q(\\mathcal{S}_{o,t-1})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The second expectation is exactly the definition of $a_{t-1}$ . Moreover, note that $Q(S_{o,t-1})=Q(S_{o,0})=$ $Q(S_{0})=\\overline{{q}}_{0}$ for any $t>0$ , the third expectation is exactly $\\overline{{q}}_{0}$ , so the above equation is exactly (4). ", "page_idx": 28}, {"type": "text", "text": "G.2 Derivation of factors influencing $a_{t},q_{t}$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As stated in Sec.2, we can get the factors influencing the evolution of $a_{t},\\,q_{t}$ by finding all sources affecting $P_{X Y}^{t}$ and the expectation of $f_{t}(x)$ over $P_{X}^{t}$ . ", "page_idx": 28}, {"type": "text", "text": "We first work out the sources influencing $f_{t}$ and the expectation of $f_{t}(x)$ over $P^{t}(X)$ : ", "page_idx": 28}, {"type": "text", "text": "\u2022 $\\overline{{q}}_{t}$ : since $f_{t}$ is trained with $\\mathcal{S}_{t}$ , $\\overline{{q}}_{t}$ is a key factor influencing the classifier. \u2022 $h_{t}$ may not model $D_{Y\\mid X}^{t}$ accurately enough. However, we ignore this by Assumption 3.1, where realizability is a common assumption in theoretical proof, while the experiments do not ignore this source. \u2022 $\\delta_{B R}^{t}$ : we now know factors influencing the expectation of $f_{t}(x)$ over $D_{X}^{t}$ , then the only left factor is the ones accounting for the difference between $D_{X}^{t}$ and $P_{X}^{t}$ . Note that only the best responses of agents can change the marginal distribution $P_{X}$ . XWe then Xdenote it as $\\delta_{B R}^{t}$ . ", "page_idx": 28}, {"type": "text", "text": "Then, with $P_{X Y}$ known, $P_{X Y}^{t}$ is only influenced by $f_{t-1}$ (i.e., the agents\u2019 best responses to the classifier at $t-1$ ). $f_{t-1}$ is also dependent on the above factors. So we get all factors. ", "page_idx": 28}, {"type": "text", "text": "G.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "proving the following lemma: ", "page_idx": 28}, {"type": "text", "text": "Lemma G.1. Assume $t\\geq2$ and the following conditions hold: $(i)\\,\\overline{{q}}_{t}>\\overline{{q}}_{t-1}\\geq\\overline{{q}}_{t-2}$ ; $(i i)\\forall x\\in X$ , $D_{Y\\mid X}^{t}(1|x)\\;\\geq\\;D_{Y\\mid X}^{t-1}(1|x),$ ; $(i i i)\\;\\forall x,\\overline{{f}}_{t-1}(x)\\;\\geq\\;\\overline{{f}}_{t-2}(x)$ . Let $\\overline{{f}}_{t-1}\\,=\\,\\mathbb{E}_{S_{t-1}\\sim D_{X Y}^{t-1}}[f_{t-1}],\\overline{{f}}_{t}\\,=$ $\\mathbb{E}_{S_{t}\\sim D_{X Y}^{t}}[f_{t}]$ , we have the following results:   \n$I.\\ \\forall x$ , $\\overline{{f}}_{t}(x)\\geq\\overline{{f}}_{t-1}(x)$ .   \n2. There exists a non-zero measure subset of $x$ values that satisfies the strict inequality. ", "page_idx": 28}, {"type": "text", "text": "Proof. We first prove (i). Note that we assume $h_{t}$ models $D_{Y\\mid X}^{t}$ well in Assumption 3.1, so $\\overline{{f}}_{t-1}$ (resp. $\\overline{{f}}_{t})$ outputs 1 if $D_{Y\\mid X}^{t-1}(1|x)$ (resp. $D_{Y\\mid X}^{t}(1|x))$ is larger than some threshold $\\theta$ . Then, according to the above condition (ii), $\\forall\\theta$ , if $D_{Y|X}^{t-1}(1|x)^{\\cdot}>\\theta,D_{Y|X}^{t}(1|x)\\geq D_{Y|X}^{t-1}(1|x)>\\theta$ . This demonstrates that $\\overline{{f}}_{t-1}(x)=1$ implies $\\overline{{f}}_{t}(x)=1$ and (i) is proved. ", "page_idx": 28}, {"type": "text", "text": "Next, according to (2), if $\\overline{{q}}_{t}>\\overline{{q}}_{t-1}$ , this means $\\mathbb{E}_{y\\sim D_{Y}^{t}}[y]>\\mathbb{E}_{y\\sim D_{Y}^{t}}[y]$ . Since $D_{Y}^{t}=D_{X}^{t}{\\cdot}D_{Y\\mid X}^{t}$ , either there exists at least one non-zero measure subset of $x$ values satisfying $D_{Y\\mid X}^{t}(1|x)>D_{Y\\mid X}^{t-1}(1|x)$ or $D_{X}^{t}$ is more \"skewed\" to the larger values of $x$ (because of monotonic likelihood assumption 3.2). For the second possibility, note that the only possible cause for the feature distribution in the training dataset $D_{X}^{t}$ to gain such a skewness is agents\u2019 strategic behaviors. However, since $\\overline{{f}}_{t-1}(x)\\geq\\overline{{f}}_{t-2}(x)$ always holds, $\\overline{{f}}_{t-1}$ sets a lower admission standard where some $x$ values that are able to best respond to $\\overline{{f}}_{t-2}$ and improve will not best respond to $\\overline{{f}}_{t-1}$ , thereby impossible to result in a feature distribution shift to larger $x$ values while keeping the conditional distribution unchanged. Thus, only the first possibility holds. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Then we prove Theorem 3.3 using mathematical induction to prove a stronger version: ", "page_idx": 29}, {"type": "text", "text": "Lemma G.2. When $t>1$ , $\\overline{{q}}_{t}\\,>\\,\\overline{{q}}_{t-1}$ , $D_{Y\\mid X}^{t}(1\\vert x)\\,\\geq\\,D_{Y\\mid X}^{t-1}(1\\vert x),$ , $\\overline{{f}}_{t}(x)\\geq\\overline{{f}}_{t-1}(x),$ , and finally $a_{t}>a_{t-1}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. $t$ starts from 2, but we need to prove the following claim: $\\overline{{q}}_{1}$ is \"almost equal\" to $\\overline{{q}}_{0}$ , so are $D_{Y\\mid X}^{1}$ and $D_{Y\\mid X}^{0}$ . ", "page_idx": 29}, {"type": "text", "text": "Firstly, according to the law of total probability, we can derive $\\overline{{q}}_{1}$ as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\overline{{q}}}_{1}={\\frac{N}{2N+K}}\\cdot{\\overline{{q}}}_{0}+{\\frac{N}{2N+K}}\\cdot a_{0}+{\\frac{K}{2N+K}}\\cdot{\\overline{{q}}}_{0}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The first and the third element are already multiples of $\\overline{{q}}_{0}$ . Also, we know $D_{X}^{o}\\;=\\;P_{X}$ . Then, since $D_{Y\\mid X}^{t}(1|x)$ falls in $\\mathcal{H}$ and agents at $t\\,=\\,0$ have no chance to best respond, we have $a_{0}\\,=$ $\\mathbb{E}_{X\\sim P_{X}}[\\dot{D}_{Y|X}^{o}(1|x)]=\\overline{{q}}_{0}$ . Thus, $a_{0}$ is also equal to $\\overline{{q}}_{0}$ , and the claim is proved. Still, as Assumption 3.1 assumes realizability of $h_{t}$ , ${\\overline{{f}}}_{1}$ is the same as $\\overline{{f}}_{0}$ . ", "page_idx": 29}, {"type": "text", "text": "Next, we can prove the lemma by induction: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Base case: Similar to Eq.5, we are able to derive $\\overline{{q}}_{2}$ as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\overline{{{q}}}_{2}=\\frac{2N+K}{3N+2K}\\cdot\\overline{{{q}}}_{1}+\\frac{N}{3N+2K}\\cdot a_{1}+\\frac{K}{3N+2K}\\cdot\\overline{{{q}}}_{0}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Based on the claim above, we can just regard $\\overline{{q}}_{0}$ as $\\overline{{q}}_{1}$ . Then we may only focus on the second term. Since $\\overline{{q}}_{1}$ and $\\overline{{q}}_{0}$ are \"almost equal\" and both distributions should satisfy the monotonic likelihood assumption 3.2, we can conclude ${\\overline{{f}}}_{0},{\\overline{{f}}}_{1}$ are \"almost identical\". Then the best responses of agents to $\\overline{{f}}_{0}$ will also make them be classified as 1 by ${\\overline{{f}}}_{1}$ , and this will directly ensure the second term to be $A(f_{1},P^{1})>A(f^{1},P)=A(f^{0},P)$ . The \"larger than\" relationship is because strategic best responses at the first round will only enable more agents to be admitted. Thus, the first and the third term stay the same as $\\overline{{q}}_{1}$ while the second is larger, so we can claim $\\overline{{q}}_{2}>\\overline{{q}}_{1}$ . Moreover, the difference between D2XY and D1XY are purely produced by the best responses at $t\\,=\\,1$ , which will never decrease the conditional probability of $y\\,=\\,1$ . Thus, $D_{Y|X}^{2}(1|x)\\,\\geq\\,D_{Y|X}^{1}(1|x)$ . Together with ${\\overline{{f}}}_{1}\\,=\\,{\\overline{{f}}}_{0}$ , all three conditions in Lemma G.1 are satisfied. we thereby claim that for every $x$ admitted by $\\overline{{f}}_{1}$ , $\\overline{{f}}_{2}(x)\\geq\\overline{{f}}_{1}(x)$ and there exists some $x$ satisfying the strict inequality. Note that $P^{1}$ is expected to be the same as $P^{2}$ since $f_{1}=f_{0}$ . Thus, $\\dot{A}(f_{2},P^{2})>A(f_{1},\\dot{P}^{2})\\stackrel{.}{=}A(f_{1},P^{1})$ , which is $a_{2}>a_{1}$ . The base case is proved. ", "page_idx": 29}, {"type": "text", "text": "2. Induction step: To simplify the notion, we can write: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{\\overline{{{q}}}_{t}}}&{{=}}&{{\\displaystyle{\\frac{t N+(t-1)K}{(t+1)N+t K}\\cdot\\overline{{{q}}}_{t-1}+\\frac{N}{(t+1)N+t K}\\cdot a_{t-1}+\\frac{K}{(t+1)N+t K}\\cdot\\overline{{{q}}}_{0}}}}\\\\ {{}}&{{=}}&{{A_{t}\\cdot\\overline{{{q}}}_{t-1}+B_{t}\\cdot a_{t-1}+C_{t}\\cdot\\overline{{{q}}}_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $\\overline{{q}}_{t-1}$ can also be decomposed into three terms: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{\\overline{{{q}}}_{t-1}}}&{{=}}&{{\\displaystyle{\\frac{(t-1)N+(t-2)K}{t N+(t-1)K}\\cdot\\overline{{{q}}}_{t-2}+\\frac{N}{t N+(t-1)K}\\cdot a_{t-2}+\\frac{K}{t N+(t-1)K}\\cdot\\overline{{{q}}}_{0}}}}\\\\ {{}}&{{=}}&{{A_{t-1}\\cdot\\overline{{{q}}}_{t-1}+B_{t-1}\\cdot a_{t-2}+C_{t-1}\\cdot\\overline{{{q}}}_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since the expectation in the second term is just $a_{t-1}$ , and we already know $a_{t-1}>a_{t-2}$ , we know $B_{t}\\cdot a_{t-1}+C_{t}\\cdot\\overline{{q}}_{0}>B_{t}\\cdot a_{t-2}+C_{t}\\cdot\\overline{{q}}_{0}$ . Note that $\\begin{array}{r}{\\frac{B_{t}}{B_{t-1}}\\,=\\,\\frac{C_{t}}{C_{t-1}}}\\end{array}$ = CtC\u2212t1 , we let the ratio be $m\\,<\\,1$ . Then sinc $\\mathrm{~e~}B_{t-1}\\cdot a_{t-2}+C_{t-1}\\cdot\\overline{{q}}_{0}\\,>\\,\\left(B_{t-1}+C_{t-1}\\right)\\cdot\\overline{{q}}_{t-1}$ due to $\\overline{{q}}_{t-1}\\,>\\,\\overline{{q}}_{t-2}$ , we can derive $B_{t}\\cdot a_{t-1}+C_{t}\\cdot\\overline{{q}}_{0}\\,>\\,m\\cdot(B_{t-1}+C_{t-1})\\cdot\\overline{{q}}_{t-1}\\,=\\,(B_{t}+C_{t})\\cdot\\overline{{q}}_{t-1}$ 1. Then $\\overline{{q}}_{t}>\\left(A_{t}+B_{t}+C_{t}\\right)\\cdot\\overline{{q}}_{t-1}=\\overline{{q}}_{t-1}$ . The first claim is proved. As $a_{t-1}>a_{t-2}$ and $D^{o}$ stays the same, any agent will not have a less probability of being qualified in $\\mathcal{S}_{t}$ compared to in $\\mathcal{S}_{t-1}$ , demonstrating $D_{Y\\mid X}^{t}(1\\vert x)\\ge D_{Y\\mid X}^{t-1}(1\\overbar{|x})$ still holds. And similarly, we can apply Lemma G.1 to get $\\overline{{f}}_{t}(x)\\geq\\overline{{f}}_{t-1}(x)$ and $a_{t}>a_{t-1}$ . \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Now we already prove Lemma G.2, which already includes Theorem 3.3. We also want to note here, the proof of Theorem 3.3 does not rely on the initial $\\overline{{q}}_{0}$ which means it holds regardless of the systematic bias. ", "page_idx": 30}, {"type": "text", "text": "G.4 Proof of Proposition 3.4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We prove the proposition by considering two extreme cases: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "(i) When $\\begin{array}{r}{\\frac{K}{N}\\to0}\\end{array}$ , this means we have no decision-maker annotated sample coming in each round and all new samples come from the deployed model. We prove $\\textstyle\\operatorname*{lim}_{t\\to\\infty}a_{t}=1$ by contradiction: firstly, by Monotone Convergence Theorem, the limit must exist because $a_{t}\\,>\\,a_{t-1}$ and $a_{t}<1$ . Let us assume the limit is $\\overline{{a}}<1$ . Then, since $K=0$ , when $t\\to\\infty$ , $\\overline{{q}}_{t}$ will also approach $\\overline{{a}}$ , this means the strategic shift $\\delta_{B R}^{t}$ approaches 0. However, this shift only approaches 0 when all agents are accepted by $f_{t-1}$ because otherwise there will be a proportion of agents benefiting from best responding to $f_{t-1}$ and result in a larger $\\overline{{q}}_{t+1}$ Thus, the classifier at $t+1$ will admit more people and the stability is broken. This means the only possibility is $l i m_{t\\rightarrow\\infty}a_{t-1}=1$ and produces a conflict. ", "page_idx": 30}, {"type": "text", "text": "(ii) When $\\begin{array}{r}{\\frac{K}{N}\\to\\infty}\\end{array}$ , the problem shrinks to retrain the classifier to fit $D_{X Y}^{o}$ , this will make $a_{t}=a_{0}$ . Thus, there exists some threshold $\\lambda$ , when $\\begin{array}{r}{\\frac{K}{N}<\\lambda}\\end{array}$ , $l i m_{t\\rightarrow\\infty}a_{t}=1$ . In practice, the $\\lambda$ could be very small. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "G.5 Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Firstly, since $P_{X Y}^{t}$ differs from $P_{X Y}$ only because agents\u2019 best respond to $f_{t}$ , we can write $q_{t}=q_{0}\\!+\\!r_{t}$ where $r_{t}$ is the difference of qualification rate caused by agents\u2019 best responses to $f_{t}$ . Qualitatively, $r_{t}$ is completely determined by two sources: (i) the proportion of agents who move their features when they best respond; (ii) the increase in the probability of being qualified for each agent. Specifically, each agent that moves its features increases its probability of being qualified from its initial point to a point at the decision boundary of $f_{t}$ . For an agent with initial feature $x$ and improved feature $x^{*}$ , its improvement can be expressed as $U(x)=P_{Y|X}(1|x^{*})-P_{Y|X}(1|x)$ . ", "page_idx": 30}, {"type": "text", "text": "Next, denote the Euclidean distance between $x$ and the decision boundary of $f_{t}$ as $d_{x,t}$ . Noticing that the agents will best respond to the decision classifier if and only if the Euclidean distance between her feature vector and the decision boundary is less than or equal to some constant $C$ no matter where the boundary is (Lemma 2 in [2]) and what the cost matrix $B$ is, we can express the total improvement at $t$ as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\nI(t)=\\int_{d_{x,t}\\leq C}P_{X}(x)\\cdot U(x)\\,d x\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "According to the proof of Theorem 3.3, all agents with feature vector $x$ who are admitted by $f_{0}$ will be admitted by $f_{t}$ $t>0_{\\mathrm{,}}$ ), making all agents who possibly improve must belong to $\\mathcal{I}$ . When $F_{X}$ is convex in $\\mathcal{I}$ or it is convex in each dimension separately, we know $P_{X}$ as its derivative will be non-decreasing in each of the $d$ dimensions within $\\mathcal{I}$ . Similarly, when $P_{Y\\mid X}(1|x)$ is convex in each of its dimensions or the whole $\\mathcal{I},U$ is also non-decreasing in each of the $d$ dimensions within $\\mathcal{I}$ . Then note that $f_{t}$ always lies below $f_{t-1}$ , therefore, for each agent who improves at $t$ having feature vector $x_{i,t}$ , we can find an agent at $t-1$ with corresponding $x_{i,t-1}$ such that both $P_{X}(x_{i,t-1})\\geq P_{X}(x_{i,t})$ and $U(x_{i,t-1})\\geq U(\\bar{x}_{i,t})$ . This will ensure $I(t)\\leq I(t-1)$ . Thus, $q_{t}$ decreases starting from 1. ", "page_idx": 30}, {"type": "text", "text": "G.6 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 $\\mu(D^{o},P)\\ge0$ : according to Def. 2.1, $a_{0}=\\mathbb{E}_{P_{X}}[D_{Y|X}^{o}(1|x)]>\\mathbb{E}_{P_{X}}[P_{Y|X}(1|x)]=Q(P)=$ $q_{0}$ . Now that $a_{0}\\mathrm{~-~}q_{0}\\mathrm{~\\geq~}0$ . Based on Thm. 3.3 and Thm. 3.5, $a_{t}$ is increasing, while $q_{0}$ is decreasing, so $\\Delta_{t}$ is always increasing. \u2022 $\\mu(D^{o},P)<0$ : similarly we can derive $a_{0}-q_{0}<0$ , so $\\Delta_{0}=|a_{0}-q_{0}|=q_{0}-a_{0}$ . So while $a_{0}-q_{0}$ is still increasing, $\\Delta_{t}$ will first decrease. Moreover, according to Prop. 3.4, if $\\frac{K}{N}$ is small enough, $\\Delta_{t}$ will eventually exceed 0 and become larger again. Thus, $\\Delta_{t}$ either decreases or first decreases and then increases. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "G.7 Proof of Proposition D.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Firstly, $S_{0}=S_{o,0}$ and $S_{1}=S_{0}\\cup S_{o,1}\\cup S_{m,0}$ . Obviously, the first two sets are drawn from $D_{X Y}^{o}$ . Consider ${\\cal S}_{m,0}$ , since it is now produced by labeling features from $P_{X}=D_{X}^{o}$ with $D_{Y|X}^{o},S_{m,0}$ is also drawn from $D_{X Y}^{o}$ . Thus, $D_{Y\\mid X}^{1}=D_{Y\\mid X}^{o}$ . ", "page_idx": 31}, {"type": "text", "text": "Then we prove the cases when $t>1$ using mathematical induction as follows: ", "page_idx": 31}, {"type": "text", "text": "1. Base case: We know $S_{2}=S_{1}\\cup S_{o,2}\\cup S_{m,1}$ . The first two sets on rhs are drawn from $D_{X Y}^{o}$ . The labeing in the third set is produced by $h_{1}(x)=D_{Y\\mid X}^{1}=D_{Y\\mid X}^{o}$ . Thus, the base case is proved. 2. Induction step: $S_{t}=S_{t-1}\\cup S_{o,t}\\cup S_{m,t-1}$ . Similarly, we only need to consider the third set. But Note that $h_{t}(x)=D_{Y\\mid X}^{t-1}=D_{Y\\mid X}^{o}$ , the induction step is easily completed. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "G.8 Proof of Proposition G.3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "At round $t$ , we know $A_{\\theta_{t}^{i}}^{i t}>A_{\\theta_{t}^{j}}^{j t}$ . To reach demographic parity, the acceptance rates need to be the same, so at least one of the following situations must happen: (i) $\\widetilde{\\theta_{t}^{i}}>\\theta_{t}^{i}$ and $\\widetilde{\\theta}_{t}^{j}>\\theta_{t}^{j}$ ; (ii) $\\widetilde{\\theta_{t}^{i}}<\\theta_{t}^{i}$ and $\\widetilde{\\theta}_{t}^{j}<\\theta_{t}^{j}$ ; (iii) $\\widetilde{\\theta_{t}^{i}}\\geq\\theta_{t}^{i}$ and $\\widetilde{\\theta}_{t}^{j}\\leq\\theta_{t}^{j}$ . Next we prove that (i) and (ii) cannot be true by contradiction. Suppose (i) holds, then we can find ${\\overline{{\\theta}}}_{t}^{j}\\ <\\ \\theta_{t}^{j}$ such that $\\ell(\\overline{{\\theta}}_{t}^{j},S_{t}^{j})\\ \\in\\ \\big(\\ell(\\theta_{t}^{j},S_{t}^{j}),\\ell(\\widetilde{\\theta}_{t}^{j},S_{t}^{j})\\big)$ and $A_{\\overline{{\\theta}}_{t}^{j}}^{j t}\\in\\big(A_{\\theta_{t}^{j}}^{j t},A_{\\theta_{t}^{i}}^{i t}\\big)$ . We can indeed find this $\\overline{{\\theta}}_{t}^{j}$ because $\\ell,A$ are continuous w.r.t. $\\theta$ . Now noticing that $A_{\\widetilde{\\theta_{t}^{i}}}^{i t}=A_{\\widetilde{\\theta_{t}^{j}}}^{j t}>A_{\\theta_{t}^{j}}^{j t}$ but $A_{\\overline{{\\theta}}_{t}^{j}}^{j t}<A_{\\theta_{t}^{j}}^{j t}$ Aj\u03b8tj , we will know we can find \u03b8it \u2208 \u03b8ti, \u03b8 ti to satisfy demographic parity together with $\\overline{{\\theta}}_{t}^{j}$ . Since $P_{Y\\mid X}(1|x)$ satisfies monotonic likelihood and $\\theta_{t}^{i}$ is the optimal point, $\\ell(\\overline{{\\theta}}_{t}^{i},S_{t}^{i})<\\ell(\\widetilde{\\theta}_{t}^{i},S_{t}^{i})$ must hold. Thus, $(\\overline{{\\theta}}_{t}^{i},\\overline{{\\theta}}_{t}^{j})$ satisfy demographic parity and have a lower loss than $(\\widetilde{\\theta}_{t}^{i},\\widetilde{\\theta}_{t}^{j})$ . Moreover, the pair satisfies (iii), which produces a conflict. ", "page_idx": 31}, {"type": "text", "text": "Similarly, we can prove (ii) cannot hold by contradiction, thereby proving (iii) must hold. ", "page_idx": 31}, {"type": "text", "text": "G.9 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Situation (i) is directly derived from Thm. 3.3, where we can just regard the trajectory of the acceptance rate of group $j$ as moving the one of group $i$ vertically downward; Situation (ii) is derived from Prop. D.2, where the systematic bias causes unfairness and it keeps stable; Situation (iii) can be derived from Thm. 3.6, where the acceptance rate of group $i$ stays stable, while the one for $j$ monotonically increases. ", "page_idx": 31}, {"type": "text", "text": "G.10 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The first fact is that both $\\theta_{t}^{i},\\theta_{t}^{j}$ stay stable as $0.5-\\mu_{i}$ and $0.5-\\mu_{j}$ when we use accuracy as the metric. Therefore, to understand which group is disadvantaged/advantaged at $t+1$ , we just need to track the agents who best respond to $\\widetilde{\\theta}_{t}^{i},\\widetilde{\\widetilde{\\theta}}_{t}^{j}$ . First, we need the following proposition which is proved in App. G.8: ", "page_idx": 31}, {"type": "text", "text": "Proposition G.3. If P tXiY , P are continuous, then $\\widetilde{\\theta_{t}^{i}}\\geq\\theta_{t}^{i}$ and $\\widetilde{\\theta}_{t}^{j}\\leq\\theta_{t}^{j}$ . ", "page_idx": 31}, {"type": "text", "text": "Note that we want to sacrifice the least accuracy to reach demographic parity. Since the agent best response will never make them worse off, we will know $a_{t}^{i}$ corresponding to the acceptance rate of group $i$ under the optimal threshold $\\theta_{t}^{i}$ is always larger than or equal to $a_{0}^{i}$ . Thus, it suffices to show that under the condition of Thm. 4.2, $a_{t}^{j}<a_{0}^{i}$ . ", "page_idx": 32}, {"type": "text", "text": "Suppose $a_{t}^{j}<a_{0}^{i}$ , then we can use Chebyshev\u2019s Inequality to bound the probability that an unadmitted agent $z$ in group $j$ reaches $\\theta_{t}^{j}$ after its best response. Denote this event as $E$ and the threshold agent $z$ feels is Iz, where E(Iz) = \u03b8 tj. Then we can write P(E) < P(Iz \u2212E(Iz) \u2265\u03b8tj \u2212\u03b8 tj) < (\u03b8tj \u03c3\u2212t2\u03b8 tj )2. Thus, when \u03c3t <\u221a\u03b8tja i0\u2212\u2212\u03b8 tjatj, we know P(E) < ai0 \u2212atj, and from the simple fact atj+1 < atj + P(E), we know $j$ will stay disadvantaged at $t+1$ . ", "page_idx": 32}, {"type": "text", "text": "However, when we do not add the fairness intervention, just consider the situation where all unadmitted agents in group $j$ at $t=0$ improve, while no one in group $i$ can improve. This will flip the disadvantaged/advantaged group. ", "page_idx": 32}, {"type": "text", "text": "H All plots with error bars ", "text_level": 1, "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/900cda436682f1d74b47b51c81c495404e102606464fb620eed34f0ccfa6e9b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "(a) Uniform data with a different $B$ : Group $i$ (left), Group $j$ (right) ", "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/2e06b6030cf05f9cac52a4f4771239a92a30fb9e01a84988b136f6ec9ae46514.jpg", "img_caption": ["Figure 22: Error bar version of Fig. 16. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "(b) Gaussian data with a different $B$ : Group $i$ (left), Group $j$ (right) ", "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/e77a72c6216270f5bb25d9bc6b6008376a960599ba63296d18a213d36574ae9b.jpg", "img_caption": ["Figure 23: Error bar version of Fig. 4 "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/ab34c0c703bc01713a5e67ddbde31d43e3fe12d8a9ed6a3c619cbcd138880f10.jpg", "img_caption": ["(a) $a_{t},\\,q_{t}$ , $\\Delta_{t}$ for group $i$ (left) and $j$ (right) ", "Figure 24: Error bar version of Fig. 5 "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/0199add3056192fef714d37c7ccef7efedc44fbb5566273b2954c0d5a51f0e4f.jpg", "img_caption": ["(b) unfairness (left) and classifier bias $\\Delta_{t}$ (right) "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/1eb00878086a92183daf8b6c8596c2fa1e901d60efd49e149a209807d12d4032.jpg", "img_caption": ["(a) $a_{t},\\,q_{t},$ , $\\Delta_{t}$ for group $i$ (left) and $j$ (right) "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/3301863506c1ed8bc9bbd69aeac26a08202418aa3292ec5633f1ae0511450476.jpg", "img_caption": ["Figure 25: Error bar version of Fig. 9 ", "(b) unfairness (left) and classifier bias $\\Delta_{t}$ (right) "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/38eb5c572cca839bddcab0c83619b2b2e5722b93701483f3c319ffb19efdefed.jpg", "img_caption": ["(a) Gaussian: $a_{t}$ , $q_{t}$ , $\\Delta_{t}$ for group $i$ (left) and $j$ (right) "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/db29d9ce64266d6e79557f7afb8df9eb7a834b7828a2029d30850a379e020f89.jpg", "img_caption": ["Figure 26: Error bar version of Fig. 6 "], "img_footnote": ["(b) German: at, $q_{t}$ , $\\Delta_{t}$ for group $i$ (left) and $j$ (right) "], "page_idx": 33}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/7d8b9469de942c792cc1e677d7423c322d223d7a72c8fa704cc12fb60a24cbd3.jpg", "img_caption": ["(a) Uniform-linear: group $i$ (left) and $j$ (right) "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/0ca1d0c29d2ba99cfbdfdede3aa8b16cbc42f906f1babfdab9b88a2dc742ad7a.jpg", "img_caption": ["Figure 27: Error bar version of Fig. 8. ", "(b) Gaussian: group i (left) and $j$ (right) "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/f9219ee8e6015583bf6aa6c6bfd2e4562fcf0c479b480e4e61803686698d623e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "(a) Uniform data when $r=0$ and $T$ is large: Group $i$ (left), Group $j$ (right) ", "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/d8c349456919a69d3419c0fd699ac1ca4930d1fe5c1f17bd792e368ed14a5b77.jpg", "img_caption": ["(c) German Credit data when $r\\,=\\,0$ and $T$ is large: Group $i$ (left), Group $j$ (right) "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/3c82b811608e8d8636ebba1320dbedea2d5ff3de9805f4d93712c260a328cb3c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "(b) Gaussian data when $r=0$ and $T$ is large: Group $i$ (left), Group $j$ (right) ", "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/b979e1cb55869610d7f64dbb18509d53e986d02181935d5094fd519fc5826d32.jpg", "img_caption": ["Figure 28: Error bar version of Fig. 15 ", "(d) Dynamics when all data is human-annotated (i.e, $r=1_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}$ ) on synthetic datasets. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/72a794ef3712de3cb516053a3944e2c36f42863113b92dc432bcee0676d57ea5.jpg", "img_caption": ["(b) Group $j$ in Credit Approval data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "Figure 29: Error bar version of Fig. 11 "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/62d3202ed41187f66dd07d8a41161ca042ec83cf26dea828470d0ea0c15e04da.jpg", "img_caption": ["Figure 30: Error bar version of Fig. 14. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/dd1557e6eb8310f1bab4b348416533a5540f554bc844454e9dbab1fd8dd3a25b.jpg", "img_caption": ["(a) Group i in Uniform data : r = 0.3, 0.1, 0.05, 0 from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/b4f1caf0131a555850206d6924ac6c9bddf1902743c20a622b7adfe65fdcbd9a.jpg", "img_caption": ["(b) Group $j$ in Uniform data : r = 0.3, 0.1, 0.05, 0 from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/67e375727283c2dc31c59a28608038d962395df2129de63bbd03589f04477c4d.jpg", "img_caption": ["(c) Group $i$ in Gaussian data : r = 0.3, 0.1, 0.05, 0 from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/332e113fa70faf4a6a96dc2d040d3c95d03007feeeb12c13e5875acb2d81df65.jpg", "img_caption": ["(d) Group $j$ in Gaussian data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/dd1a8e0e38dc2676b653db4434031529d5619a257d712f9bb638d0db34fa1322.jpg", "img_caption": ["(e) Group $i$ in German Credit data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/1c7093efc1645a13c5acb3d5b56cd10552d99af217275d1ee5fc8bb100810d26.jpg", "img_caption": ["(f) Group $j$ in German Credit data : $r=0.3,0.1,0.05,0$ from the leftmost to the rightmost ", "Figure 31: Error bar version of Fig. 17. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/68baad2de0b12568eb05deacef01ecf26098b61db6fba7ff3d68696eea0512af.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "(a) refined retraining process for Group $i$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 36}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/385c6c7610eb7be53780b9dd4d08f680d76525198b9a63a6921473db8aaa9b16.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "(b) refined retraining process for Group $j$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 36}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/af624beb08cbe4854c6c546a9d0c469b878c2a5cf801c6020eee3d15f6f147c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "(c) refined retraining process for Group $i$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 36}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/821a079d894eab9b1c2b3967382aec8aa2a5d68e7d0f504ad382c1e15f1ef698.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "(d) refined retraining process for Group $j$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 36}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/5152fba5cb15c8a946877de87b57507c3e66fd2a4cc550e1f17ba8d488291a87.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "(e) refined retraining process for Group $i$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 36}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/bda0b29c0db4d6799a5c8e983703bcb262913826199cd7aac464d874112d27b8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "(f) refined retraining process for Group $j$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right). ", "page_idx": 36}, {"type": "text", "text": "Figure 32: Error bar version of Fig. 18 (a) Dynamics of unfairness in Uniform data : $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 36}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/86f7305e302ff944458ccce14ae5594dac985a37e18e3c88a2a66efcc838ab10.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/9ed9b625ce2a9c2a3664b0beff4900b9907d1e0037fbf2b5fc3ca49282c3d344.jpg", "img_caption": ["(b) Dynamics of $\\Delta_{t}$ in Uniform data : $r\\,=\\,0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/f248b70fa79ebbdda4292f783cfc15255488b08d8749367a3aa33149c02f4420.jpg", "img_caption": ["(c) Dynamics of unfairness in Gaussian data : $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/f4bf6b074768bbbcb6297e74619fda5d1a7a8697c16e1f1c0cce866417c2c619.jpg", "img_caption": ["(d) Dynamics of $\\Delta_{t}$ in Gaussian data : $r\\,=\\,0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/55346fda6eb1312241399ece92d01fe147c6df4d1a0ff03788b9aa5604dcd140.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "(e) Dynamics of unfairness in German Credit data : $r\\:=\\:0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 37}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/2bf513b72b1f29c03e55e15ab66b043d9e48129bc54adac9bc6f6fda8f38dd64.jpg", "img_caption": ["Figure 33: Error bar version of Fig. 19 "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "(f) Dynamics of $\\Delta_{t}$ in German Credit data : $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 37}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/fe0b3c2f1307c36a06cc8cacddd0d3126b9fa3878cd914522975f2c4a140dd9d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "(a) Noisy retraining process for Group $i$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 38}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/c375640b0318d236c4f25691cdf6fbde54d2aa4f28a6eafa4065ef1e180daf46.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "(b) Noisy retraining process for Group $j$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 38}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6d9922741ab8d9a1a731ec83ddee9f6ee6a3714b94756c3244b872ab223c79c1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "(c) Noisy retraining process for Group $i$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 38}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/7e0fc82fa8d7ebfb737df0d842737ac6ab803487e55cb442845e240b1af2f7bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/6523dca31aa8095a23e66308ffc64f2250ffe7fc410747172ba161de8f7cbe38.jpg", "img_caption": ["(e) Noisy retraining process for Group $i$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/58e0938bb8a1688549e0477647441d70004d191526d71c560f65ef4099f3d277.jpg", "img_caption": ["Figure 34: Error bar version of Fig. 20 "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "(f) Noisy retraining process for Group $j$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 38}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/93ff5365d78d914121e5be9e405973cc8d548bdf084aa43834dddb4cdcdf9281.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(a) Nonstrategic retraining process for Group $i$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 39}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/ef7460e07ae77c508d975c72a7c158cc24697bd2ee896d0c222d34c0fb4d3d65.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(b) Nonstrategic retraining process for Group $j$ in Uniform data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 39}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/d764070e7c4b23e62362c68f233b8deb0b509e19d4524276c70aea9f7569f21c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(c) Nonstrategic retraining process for Group $i$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 39}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/26d82ba5bb5947396b62103e08e3a48bc74a304dd650e6602c7db7837150981c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(d) Nonstrategic retraining process for Group $j$ in Gaussian data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 39}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/8132aae9506eeef4ebf7fb0cee91aa084d693007fc7abecb0c1d2e37ab8445cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(e) Nonstrategic retraining process for Group $i$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 39}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/29907f12589f0064d56f6ca6b9363f053f0ffb70dbb56836a518f9a42ea1a759.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(f) Nonstrategic retraining process for Group $j$ in German Credit data: $r=0.1$ (left), 0.05 (middle) and 0 (right) ", "page_idx": 39}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/de0b5e07d80ca323aef23969675955435a7f1cdd849bf128c2971954a60f11b9.jpg", "img_caption": ["(a) Dynamics of unfairness in Credit Approval data: $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "2UJLv3KPGO/tmp/c2778559c086d1a6883def95b14b1e58740728f8a720059ee722bcbd7fc0a7e7.jpg", "img_caption": ["(b) Dynamics of $\\Delta_{t}$ in Credit Approval data: $r=0.1$ (left), 0.05 (middle) and 0 (right) "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Although all our theoretical results are expressed in terms of expectation, we provide error bars for all plots in the main paper, App. E and F if randomness is applicable. All the above figures demonstrate expectations as well as error bars $\\pm\\,1$ standard deviation). Overall, the experiments have reasonable standard errors. However, experiments in the Credit Approval dataset [20] (Fig. 36) incur larger standard errors, which is not surprising because the dataset violates several assumptions. Finally, note that we conduct 50-100 randomized trials for every experiment and we should expect much lower standard errors if the numbers of trials become large. ", "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The abstract summarizes the paper\u2019s contribution appropriately. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We have a conclusion section to discuss the limitations. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: See main paper and Appendix where all assumptions and proofs are clearly stated. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See supplementary materials. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The code and dataset are in the supplemental material. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: All details are in experiment section and appendix and supplementary materials. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: We have standard errors in the plots for all experiments in App. H. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: See App. F. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: See Conclusion Section. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 44}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: No such risks. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We cite all papers, datasets appropriately. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]