[{"figure_path": "y8P633E5HQ/tables/tables_7_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments on six datasets (Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor).  It compares the performance of the proposed att-Node-level NLSFs against several state-of-the-art spectral graph neural networks (GNNs) such as GCN, GAT, SAGE, ChebNet, and others. The table shows the classification accuracy (with standard deviations) achieved by each method on each dataset.  The results demonstrate that the att-Node-level NLSFs generally outperform or achieve comparable performance to existing GNNs across various datasets, showcasing their effectiveness in node-level classification tasks.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_8_1.jpg", "caption": "Table 2: Node classification performance on original and filtered Chameleon and Squirrel datasets.", "description": "This table presents a comparison of node classification performance on the original and filtered versions of the Chameleon and Squirrel datasets.  The filtering process removes duplicate nodes identified in the original datasets.  The table shows the classification accuracy (with 95% confidence intervals) of several GNN methods (ResNet+SGC, ResNet+adj, GCN, GPRGNN, FSGNN, GloGNN, FAGCN, and att-Node-level NLSFs) on both the original and filtered datasets.  The results highlight the impact of data leakage due to duplicate nodes in the original datasets and demonstrate the robustness of the proposed att-Node-level NLSFs to this issue.", "section": "5.2 Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting"}, {"figure_path": "y8P633E5HQ/tables/tables_9_1.jpg", "caption": "Table 3: Graph classification accuracy.", "description": "This table presents the results of graph classification experiments using various methods, including geometric graph convolutional networks (GCNs) and the proposed nonlinear spectral filters (NLSFs).  The accuracy is reported for eight different benchmark datasets,  showing the performance of different methods in classifying graph structures.  The table allows for a comparison of the proposed method (att-Pooling-NLSFs) to several state-of-the-art baseline algorithms, including WL, GK, GCN, GAT, SAGE, DiffPool, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, and ARMA.", "section": "5.3 Graph Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_17_1.jpg", "caption": "Table 4: Classification accuracy on the MNIST and perturbed MNIST using NLSFs.", "description": "This table presents the MNIST classification accuracy achieved using NLSFs on both clean and perturbed MNIST datasets.  The \"Ours\" row indicates the accuracy obtained by the proposed method, showing high accuracy on both datasets, highlighting the robustness of NLSFs to perturbations.", "section": "B.2 Robust Graph Functional Shifts in Image Translation and MNIST Classification on Perturbed Grids"}, {"figure_path": "y8P633E5HQ/tables/tables_29_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments conducted on six different datasets (Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor).  The accuracy of the proposed att-Node-level NLSFs is compared against several other state-of-the-art graph neural networks (GNNs), including GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, ARMA, JacobiConv, BernNet, Specformer, and OptBasisGNN.  The table shows the mean accuracy and standard deviation across multiple trials for each model and dataset, providing a comprehensive comparison of performance.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_30_1.jpg", "caption": "Table 6: Graph classification datasets statistics.", "description": "This table presents the statistics of eight graph classification datasets used in the paper's experiments.  For each dataset, it lists the number of graphs, the number of classes, the minimum and maximum number of nodes per graph, the average number of nodes per graph, and the number of features per node.  The datasets cover various domains, including bioinformatics (MUTAG, PTC, ENZYMES, PROTEINS, NCI1) and social networks (IMDB-B, IMDB-M, COLLAB).", "section": "5.3 Graph Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_31_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments on six datasets (Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor).  The table compares the performance of the proposed att-Node-level NLSFs against several state-of-the-art GNN methods, showing classification accuracy with 95% confidence intervals across 10 random dataset splits. The results illustrate the superior performance of att-Node-level NLSFs on various datasets.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_32_1.jpg", "caption": "Table 8: Full Performance comparison of node classification on original and filtered Chameleon and Squirrel datasets in dense split setting. The baseline results used for comparison are taken from [77].", "description": "This table compares the performance of different graph neural network models on node classification tasks using the Chameleon and Squirrel datasets.  It's notable that these datasets were re-evaluated in a later study [77] to address issues related to data leakage in the original dense split setting.  This table thus presents classification results on both the original and the filtered versions of the datasets, allowing for a more reliable comparison. The models compared include various GNN architectures, and the results are presented as average accuracy with standard deviation.", "section": "5.2 Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting"}, {"figure_path": "y8P633E5HQ/tables/tables_32_2.jpg", "caption": "Table 9: An ablation study investigated the effect of Node-level NLSFs on node classification, comparing the use of Index NLSFs and Value NLSFs. The symbol (\u2191) denotes an improvement using Laplacian attention.", "description": "This ablation study compares the performance of Node-level NLSFs using different Laplacians (combinatorial Laplacian L and normalized Laplacian N) and parameterizations (Index and Value).  It shows the impact of Laplacian attention on improving classification accuracy. The results are presented for six different datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor. The (\u2191) symbol indicates that Laplacian attention improved the result.", "section": "3.5 Laplacian Attention NLSFs"}, {"figure_path": "y8P633E5HQ/tables/tables_33_1.jpg", "caption": "Table 10: An ablation study investigated the effect of Graph-NLSFs on graph classification, comparing the use of Index NLSFs and Value NLSFs. The symbol (\u2191) denotes an improvement using Laplacian attention.", "description": "This table presents the results of an ablation study on graph classification using Graph-level NLSFs. It compares the performance of Index NLSFs and Value NLSFs, both with and without Laplacian attention, across eight benchmark datasets. The goal is to analyze the impact of different parameterizations and the Laplacian attention mechanism on graph classification accuracy.  The results show that Laplacian attention generally improves performance.", "section": "F.3 Ablation Study on att-Node-level NLSFs, att-Graph-NLSF, and att-Pooling-NLSFS"}, {"figure_path": "y8P633E5HQ/tables/tables_33_2.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments on six datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.  The table compares the performance of the proposed att-Node-level NLSFs against several other state-of-the-art Graph Neural Networks (GNNs) including GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, ARMA, JacobiConv, BernNet, Specformer, and OptBasisGNN. The accuracy for each method is presented as an average across 10 random splits, accompanied by a 95% confidence interval.  The results showcase the superior performance of att-Node-level NLSFs compared to the baseline GNN models on several of the datasets.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_34_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments using various Graph Neural Networks (GNNs) on six benchmark datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.  The table shows the average classification accuracy (with standard deviation) achieved by each GNN on each dataset. The datasets represent different types of graphs and levels of difficulty, allowing for a comprehensive comparison of GNN performance.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_34_2.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments on six different datasets (Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor).  The accuracy of several graph neural network (GNN) models is compared, including GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, ARMA, JacobiConv, BernNet, Specformer, OptBasisGNN, and the proposed att-Node-level NLSFs.  The table shows the mean accuracy and standard deviation for each method on each dataset, providing a quantitative comparison of the performance of different GNN architectures for node classification.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_35_1.jpg", "caption": "Table 14: Graph classification performance using Diagonal NLSFs, including index-by-index Index-NLSFs and band-by-band Value-NLSFs, along with their variants using Laplacian attention. The symbol (\u2191) denotes an improvement using Laplacian attention.", "description": "This table presents the results of graph classification experiments using diagonal NLSFs, specifically focusing on index-by-index Index-NLSFs and band-by-band Value-NLSFs.  It compares the performance of these methods with and without Laplacian attention. The results are presented for several benchmark datasets, illustrating the impact of the different approaches on classification accuracy. The use of the Laplacian attention mechanism is highlighted by the (\u2191) symbol to indicate an improvement in performance compared to methods without this attention.", "section": "F.6 Index-by-Index Index-NLSFs and Band-by-Band Value-NLSFs"}, {"figure_path": "y8P633E5HQ/tables/tables_35_2.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification on six datasets (Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor).  It compares the accuracy of the proposed att-Node-level NLSFs against several other state-of-the-art graph neural network models (GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, ARMA, JacobiConv, BernNet, Specformer, OptBasisGNN). The results show the average accuracy and standard deviation over ten random splits, demonstrating the superior performance of att-Node-level NLSFs on several of the datasets, especially the more challenging heterophilic datasets (Chameleon, Squirrel, Actor).", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_36_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments on six datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.  The table compares the accuracy of the proposed Node-level NLSFs (with Laplacian attention) against several existing state-of-the-art spectral graph neural networks (GNNs).  Each dataset's results are presented with average accuracy and a 95% confidence interval over ten random splits. The performance of the NLSFs is highlighted and contrasted with the results of GCN, GAT, SAGE, ChebNet, and other GNNs.", "section": "5.1 Semi-Supervised Node Classification"}, {"figure_path": "y8P633E5HQ/tables/tables_37_1.jpg", "caption": "Table 1: Semi-supervised node classification accuracy.", "description": "This table presents the results of semi-supervised node classification experiments on six datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.  The table compares the performance of the proposed att-Node-level NLSFs against several other state-of-the-art Graph Neural Networks (GNNs), including GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, ARMA, JacobiConv, BernNet, Specformer, and OptBasisGNN. The accuracy is presented as the average classification accuracy \u00b1 standard deviation over 10 random dataset splits, showcasing the superior performance of the att-Node-level NLSFs, especially on the heterophilic datasets (Chameleon, Squirrel, Actor).", "section": "5.1 Semi-Supervised Node Classification"}]