[{"type": "text", "text": "Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ya-Wei Eileen Lin\u2020 Ronen Talmon\u2020 Ron Levie\u2021 ", "page_idx": 0}, {"type": "text", "text": "Viterbi Faculty of Electrical and Computer Engineering, Technion \u2021Faculty of Mathematics, Technion ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space. We remark that each linear fliter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral fliters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many fields, such as chemistry [37], biology [36, 3], social science [9], and computer graphics [97], data can be described by graphs. In recent years, there has been a tremendous interest in the development of machine learning models for graph-structured data [98, 13]. This young field, often called graph machine learning (graph-ML) or graph representation learning, has made significant contributions to the applied sciences, e.g., in protein folding, molecular design, and drug discovery [45, 2, 70, 89], and has impacted the industry with applications in social media, recommendation systems, traffic prediction, computer graphics, and natural language processing, among others. ", "page_idx": 0}, {"type": "text", "text": "Geometric Deep Learning (GDL) [13] is a design philosophy for machine learning models where the model is constructed to inherently respect symmetries present in the data, aiming to reduce model complexity and enhance generalization. By incorporating knowledge of these symmetries into the model, it avoids the need to expend parameters and data to learn them. This inherent respect for symmetries is automatically generalized to test data, thereby improving generalization [6, 76, 25]. For instance, convolutional neural networks (CNNs) respect the translation symmetries of 2D images, with weight sharing due to these symmetries contributing significantly to their success [54]. Respecting node re-indexing in a scalable manner revolutionized machine learning on graphs [13, 12] and has placed GNNs as a main general-purpose tool for processing graph-structured data. Moreover, within GNNs, respecting the 3D Euclidean symmetries of the laws of physics (rotations, reflections, and translations) led to state-of-the-art performance in molecule processing [24]. ", "page_idx": 0}, {"type": "text", "text": "Our Contribution. In this paper, we focus on GDL for graph-ML. We consider extensions of shift symmetries from images to general graphs. Since graphs do not have a natural notion of domain translation, as opposed to images, we propose considering functional translations instead. We model the group of translations on graphs as the group of all unitary operators on signals that commute with the graph shift operator. Such unitary operators are called graph functional shifts. Note that each linear filter layer of a standard spectral GNN commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose non-linear spectral fliters (NLSFs) that are fully equivariant to graph functional shifts and have universal approximation properties. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In Sec. 3, we introduce our NLSFs based on new notions of analysis and synthesis that map signals between their node-space representations and spectral representations. Our transforms are related to standard graph Fourier and inverse Fourier transforms but differ from them in one important aspect. One key property of our analysis transform is that it is independent of a specific choice of Laplacian eigenvectors. Hence, our spectral representations are transferable between graphs. In comparison, standard graph Fourier transforms are based on an arbitrary choice of eigenvectors, and therefore, the standard frequency domain is not transferable. To achieve transferability, standard graph Fourier methods resort to linear filter operations based on functional calculus. Since we do not have this limitation, we can operate on the frequency coefficients with arbitrary nonlinear functions such as multilayer perceptrons. In Sec. 4, we present theoretical results of our NLSFs, including the universal approximation and expressivity properties. In Sec. 5, we demonstrate the efficacy of our NLSFs in node and graph classification benchmarks, where our method outperforms existing spectral GNNs. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation. For $N\\in\\mathbb N$ , we denote $[N]=\\{1,\\dots,N\\}$ . We denote matrices by boldface uppercase letter $\\mathbf{B}$ , vectors (assumed to be columns) by lowercase boldface $\\mathbf{b}$ , and the entries of matrices and vectors are denoted with the same letter in lower case, e.g., $\\mathbf{B}=(b_{i,j})_{i,j\\in[N]}$ . Let $G=([N],{\\mathcal{E}},\\mathbf{A},\\mathbf{X})$ be an undirected graph with a node set $[N]$ , an edge set $\\mathcal{E}\\subset[N]\\times[N]$ , an adjacency matrix ${\\bf A}\\in\\mathbb{R}^{N\\times N}$ representing the edge weights, and a node feature matrix (also called a signal) $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ containing $d$ -dimensional node attributes. Let $\\mathbf{D}$ be the diagonal degree matrix of $G$ , where the diagonal element $d_{i,i}$ is the degree of node $i$ . Denote by $\\Delta$ any normal graph shift operator (GSO). For example, $\\Delta$ could be the combinatorial graph Laplacian or the normalized graph Laplacian given by ${\\bf L}={\\bf D}-{\\bf A}$ and ${\\bf N}={\\bf D}^{-\\frac{1}{2}}{\\bf L}{\\bf D}^{-\\frac{1}{2}}$ , respectively. Let $\\begin{array}{r}{\\Delta=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{\\top}}\\end{array}$ be the eigendecomposition of $\\Delta$ , where $\\mathbf{V}$ is the eigenvector matrix and $\\mathbf{\\calA}=\\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{N})$ is the diagonal matrix with eigenvalues $(\\lambda_{i})_{i=1}^{N}$ ordered by $|\\lambda_{1}|\\le\\dots\\le|\\lambda_{N}|$ . An eigenspace is the span of all eigenvectors corresponding to the same eigenvalue. Let $\\mathbf{P}_{i}=\\mathbf{P}_{\\Delta;i}$ denote the projection upon the $i$ -th eigenspace of $\\Delta$ in increasing order of $|\\lambda|$ . We denote the Euclidean norm by $\\lVert\\mathbf{X}\\rVert_{2}$ . We define the channel-wise signal norm $\\|\\mathbf{X}\\|_{\\mathrm{sig}}$ of a feature matrix $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ as the vector $\\|\\mathbf{\\bar{X}}\\|_{\\mathrm{sig}}=(\\|\\mathbf{X}_{:,j}\\|_{2})_{j=1}^{d}\\in\\mathbb{R}^{d}$ , where $\\mathbf{X}_{:,j}$ is the $j$ -th column on and $0\\leq a\\leq1$ . We abbreviate multilayer perceptrons by MLP. ", "page_idx": 1}, {"type": "text", "text": "2.1 Linear Graph Signal Processing ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Spectral GNNs define convolution operators on graphs via the spectral domain. Given a self-adjoint graph shift operator $(G S O)\\,\\Delta$ , e.g., a graph Laplacian, the Fourier modes of the graph are defined to be the eigenvectors $\\{\\mathbf{v}_{i}\\}_{i=1}^{N}$ of $\\Delta$ and the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{N}$ are the frequencies. A spectral fliter is defined to directly satisfy the \u201cconvolution theorem\u201d [11] for graphs. Namely, given a signal $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ and a function $\\mathbf{Q}:\\dot{\\mathbb{R}}\\to\\mathbb{R}^{d^{\\prime}\\times d}$ , the operator $\\mathbf{Q}(\\Delta):\\mathbb{R}^{N\\times d}\\rightarrow\\mathbb{R}^{N\\times d^{\\prime}}$ defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{Q}(\\Delta)\\mathbf{X}:=\\sum_{i=1}^{N}\\mathbf{v}_{i}\\mathbf{v}_{i}^{\\top}\\mathbf{X}\\mathbf{Q}(\\lambda_{i})^{\\top},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "is called a filter. Here, $d^{\\prime}$ is the number of output channels. Spectral GNNs, e.g., [21, 49, 61, 5], are graph convolutional networks where convolutions are via Eq. (1), with a trainable function $\\mathbf{Q}$ at each layer, and a nonlinear activation function. ", "page_idx": 1}, {"type": "text", "text": "2.2 Equivariant GNNs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Equivariance describes the ability of functions $f$ to respect symmetries. It is expressed as $f(H_{\\kappa}x)=$ $H_{\\kappa}f(x)$ , where $\\mathcal{K}\\ni\\kappa\\mapsto H_{\\kappa}$ is an action of a symmetry group $\\kappa$ on the domain of $f$ . GNNs [83, 14], including spectral GNNs [22, 49] and subgraph GNNs [31, 4], are inherently permutation equivariant w.r.t. the ordering of nodes. This means that the network\u2019s operations are unaffected by the specific arrangement of nodes, a property stemming from passive symmetries [95] where transformations are applied to both the graph signals and the graph domain. This permutation equivariance is often compared to the translation equivariance in CNNs [56, 57], which involves active symmetries [44]. The key difference between the two symmetries lies in the domain: while CNNs operate on a fixed domain with signals transforming within it, graphs lack a natural notion of domain translation. To address this, we consider graph functional shifts as the symmetry group, defined by unitary operators that commute with the graph shift operator. This perspective allows for our NLSFs to be interpreted as an extension of active symmetry within the graph context, bridging the gap between the passive and active symmetries inherent to GNNs and CNNs, respectively. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Nonlinear Spectral Graph Filters ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present new concepts of analysis and synthesis under which the spectral domain is transferrable between graphs. Following these concepts, we introduce new GNNs that are equivariant to functional symmetries \u2013 symmetries of the Hilbert space of signals rather than symmetries in the domain of definition of the signal [64]. ", "page_idx": 2}, {"type": "text", "text": "3.1 Translation Equivariance of CNNs and GNNs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For motivation, we start with the grid graph $R$ with node set $[M]^{2}$ and circular adjacency $\\mathbf{B}$ , we define the translation operator $\\mathbf{T}_{m,n}$ by $[m,n]$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{\\Gamma}_{m,n}\\in\\mathbb{R}^{M^{2}\\times M^{2}};\\quad\\left(\\mathbf{T}_{m,n}\\mathbf{x}\\right)_{i,j}=\\mathbf{x}_{l,k}\\quad\\mathrm{where}\\quad l=\\left(i-m\\right)\\bmod M\\quad\\mathrm{and}\\quad k=\\left(j-n\\right)\\bmod M\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that any $\\mathbf{T}_{m,n}$ is a unitary operator that commutes with the grid Laplacian $\\Delta_{R}$ , i.e., $\\mathbf{T}_{m,n}\\mathbf{\\Delta}\\Delta_{R}=$ $\\Delta_{R}\\mathbf{T}_{m,n}$ , and therefore it belongs to the group of all unitary operators $\\mathcal{U}_{R}$ that commute with the grid Laplacian $\\pmb{\\Delta}_{R}$ . In fact, the space of isotropic convolution operators (with $90^{o}$ rotation and reflection symmetric filters) can be seen as the space of all normal operators1 that commute with unitary operators from $\\mathcal{U}_{R}$ [18]. Applying a non-linearity after the convolution retains this equivariance, and hence, we can build multi-layer CNNs that commute with $\\mathcal{U}_{R}$ . By the universal approximation theorem [19, 34, 58], this allows us to approximate any continuous function that commutes with $\\mathcal{U}_{R}$ . ", "page_idx": 2}, {"type": "text", "text": "Note that such translation equivariance cannot be extended to general graphs. Achieving equivariance to graph functional shifts through linear spectral convolutional layers $\\mathbf{Q}(\\mathbf{\\DeltaA})$ is straightforward, since these layers commute with the space of all unitary operators $\\mathcal{U}_{\\Delta}$ that commute with $\\Delta$ . However, introducing non-linearity $\\rho(\\mathbf{Q}(\\bar{\\Delta})\\mathbf{X})$ breaks the symmetry. That is, there exists $\\mathbf{U}\\in\\mathcal{U}_{\\Delta}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho\\left(\\mathbf{Q}(\\Delta)\\mathbf{U}\\mathbf{X}\\right)\\neq\\mathbf{U}\\rho\\left(\\mathbf{Q}(\\Delta)\\mathbf{X}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\rho$ is any non-linear activation function, e.g., ReLU, Sigmoid, etc. ", "page_idx": 2}, {"type": "text", "text": "This means that multi-layer spectral GNNs do not commute with $\\mathcal{U}_{G}$ , and are hence not appropriate as approximators of general continuous functions that commute with $\\mathcal{U}_{G}$ (see App. A for an example illustrating how non-linear activation functions break the functional symmetry). Instead, we propose in this paper a multi-layer GNN that is fully equivariant to $\\mathcal{U}_{G}$ , which we show to be universal: it can approximate any continuous graph-signal function (w.r.t. some metric) commuting with $\\mathcal{U}_{G}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Graph Functional Symmetries and Their Relaxations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define the symmetry group of graph functional shifts as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Graph Functional Shifts). The space of graph functional shifts is the unitary subgroup $\\mathcal{U}_{\\Delta}$ , where a unitary matrix U is in $\\mathcal{U}_{\\Delta}$ iff it commutes with the GSO \u2206, namely, $\\mathbf{U}\\Delta=\\Delta\\mathbf{U}$ . ", "page_idx": 2}, {"type": "text", "text": "It is important to note that functional shifts, in general, are not induced from node permutations. Instead, functional shifts are related to the notion of functional maps [73] used in shape correspondence and are general unitary operators that are not permutation matrices in general. The value of the functionally translated signal at a given node can be a mixture of the content of the original signal at many different nodes. For example, the functional shift can be a combination of shifts of different frequencies at different speeds. See App. B for illustrations and examples of functional translations. ", "page_idx": 2}, {"type": "text", "text": "A fundamental challenge with the symmetry group in Def. 1 is its lack of transferability between different graphs. Hence, we propose to relax this symmetry group. Let $g_{1},\\dotsc,g_{S}:\\mathbb{R}\\rightarrow\\mathbb{R}$ be the indicator functions of the intervals $\\{[l_{s},l_{s+1}]\\}_{s=1}^{S}$ , which constitute a partition of the frequency band $[l_{1},l_{S}]\\subset\\mathbb{R}$ . The operators $g_{j}(\\Delta)$ , interpreted via functional calculus Eq. (1), are projections of the signal space upon band-limited signals. Namely, $\\begin{array}{r}{g_{j}(\\Delta)=\\sum_{i:\\lambda_{i}\\in[l_{j},l_{j+1}]}\\mathbf{v}_{i}\\mathbf{v}_{i}^{\\top}}\\end{array}$ . In our work, we consider filters $g_{j}$ that are supported on the dyadic sub-bands $\\left[\\lambda_{N}r^{S-j+1},\\lambda_{N}r^{S-j}\\right]$ , where $0<r<1$ is the decay rate. See Fig. 5 in App. F for an illustrated example. Note that for $j^{\\bar{}}\\bar{=}\\,^{1}$ , the sub-band falls in $[0,\\bar{\\lambda_{N}}r^{S-1}]$ . The total band $[0,l_{S}]$ is $[0,\\lambda_{N}]$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Relaxed Functional Shifts). The space of relaxed functional shifts with respect to the fliter bank $\\{g_{j}\\}_{j=1}^{K}$ (of indicators) is the unitary subgroup $\\mathcal{U}_{\\Delta}^{g}$ , where a unitary matrix $\\mathbf{U}$ is in $\\mathcal{U}_{\\Delta}^{g}$ iff it commutes with $g_{j}(\\Delta)$ for all $j$ , namely, $\\mathbf{U}g_{j}(\\pmb{\\Delta})=g_{j}(\\pmb{\\Delta})\\mathbf{U}$ . ", "page_idx": 3}, {"type": "text", "text": "Similarly, we can relax functional shifts by restricting to the leading eigenspaces. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Leading Functional Shifts). The space of leading functional shifts is the unitary subgroup $\\mathcal{U}_{\\Delta}^{J}$ , where a unitary $\\mathbf{U}$ is in $\\mathcal{U}_{\\Delta}^{J}$ iff it commutes with the eigenspace projections $\\{\\mathbf{P}_{j}\\}_{j=1}^{J}$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Analysis and Synthesis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use the terminology of analysis and synthesis, as in signal processing [68], to describe transformations of signals between their graph and spectral representations. Here, we consider two settings: the eigenspace projections case and the filter bank (of indicators) case. The definition of the frequency domain depends on a given signal $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ , where its projections to the eigenspaces of $\\Delta$ are taken as the Fourier modes. Spectral coefficients are modeled as matrices $\\mathbf{R}$ that mix the Fourier modes, allowing to synthesize signals of general dimensions. ", "page_idx": 3}, {"type": "text", "text": "Spectral Index Case. We first define analysis and synthesis using the spectral index up to frequency $J$ . Let $\\begin{array}{r}{\\mathbf{P}_{J+1}=\\mathbf{I}-\\sum_{j=1}^{J}\\mathbf{P}_{j}}\\end{array}$ be the orthogonal complement to the first $J$ eigenprojections. Let $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ be the graph signal and $\\mathbf{R}\\in\\mathbb{R}^{(J+1)d\\times(J+1)\\widetilde{d}}$ the spectral coefficients to be synthesized, whered  represents number of output channels. The analysis and synthesis are defined respectively by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{A}^{\\mathrm{ind}}(\\Delta,\\mathbf{X})=\\left(\\|\\mathbf{P}_{i}\\mathbf{X}\\|_{\\mathrm{sig}}\\right)_{i=1}^{J+1}\\in\\mathbb{R}^{(J+1)d}\\mathrm{~and~}\\,\\mathcal{S}^{\\mathrm{ind}}(\\mathbf{R};\\Delta,\\mathbf{X})=\\left[\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J+1}\\mathbf{R},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $0~\\leq~a~\\leq~1$ and $0~<~e~\\ll~1$ are parameters that promote stability, and the power $\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}$ as well as the division in Eq. (2) are element-wise operations on each entry. Here, $\\left[\\mathbf{P}_{j}\\mathbf{X}/(\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig~}}^{a}+e)\\right]_{j=1}^{J+1}\\in\\mathbb{R}^{N\\times(J+1)d}$ denotes concatenation. We remark that the orthogonal complement in the $(J+1)$ -th filter alleviates the loss of information due to projecting to the low-frequency bands, and therefore, the full spectral range of the signal can be captured. This is particularly important for heterophilic graphs, which rely on high-frequency components for accurate label representation. The term index stems from the fact that eigenvalues are treated according to their index when defining the projections ${\\mathbf{P}}_{j}$ . Note that the synthesis here differs from classic signal processing as it depends on a given signal on the graph. When treating $\\Delta$ and $\\mathbf{X}$ as fixed, this synthesis operation is denoted by $S_{\\Delta,\\mathbf{X}}^{\\mathrm{ind}}(\\mathbf{R}):=S^{\\mathrm{ind}}(\\mathbf{R};\\mathbf{\\bar{\\Delta}X})$ . We similarly denote $\\mathcal{A}_{\\Delta}^{\\mathrm{ind}}(\\mathbf{X}):=\\mathcal{A}^{\\mathrm{ind}}(\\Delta,\\mathbf{X})$ ", "page_idx": 3}, {"type": "text", "text": "Filter Bank Case. Similarly, we define the analysis and synthesis in the fliter bank up to band $g_{K}$ as follows. Let $\\begin{array}{r}{g_{K+1}(\\pmb{\\Delta})=\\mathbf{I}-\\sum_{j=1}^{K}g_{j}(\\pmb{\\Delta})}\\end{array}$ denote the orthogonal complement to the first $K$ bands. Let $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ be the graph signal, and let $\\mathbf{R}\\in\\mathbb{R}^{(K+1)d\\times(K+1)\\widetilde{d}}$ represent the spectral coefficients to be synthesized, whered  refers to the general dimension. The analysis and synthesis in the filter bank case are defined by ", "page_idx": 3}, {"type": "text", "text": "$4^{\\mathrm{val}}(\\Delta,\\mathbf{X})=\\left(\\left\\lVert g_{i}(\\Delta)\\mathbf{X}\\right\\rVert_{\\mathrm{sig}}\\right)_{i=1}^{K+1}\\in\\mathbb{R}^{(K+1)d}\\mathrm{~and~}S^{\\mathrm{val}}(\\mathbf{R};\\Delta,\\mathbf{X})=\\left[\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\left\\lVert g_{j}(\\Delta)\\mathbf{X}\\right\\rVert_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{K+1}\\mathbf{R},$ rTehsep etcetrimve lvya,l uweh reerfee $a,e$ oa rheo aws  ebiegfeonrvea.l uHese raer,e $\\left[g_{j}(\\Delta)\\mathbf{X}/(\\lVert g_{j}(\\Delta)\\mathbf{X}\\rVert_{\\mathrm{sig}}^{a}+e)\\right]_{j=1}^{K+1}\\in\\mathbb{R}^{N\\times(K+1)d}$ . projections $g_{j}(\\Delta)$ . As before, we denote $S_{\\Delta,\\mathbf{X}}^{\\mathrm{val}}$ and $\\mathcal{A}_{\\Delta}^{\\mathrm{val}}$ . ", "page_idx": 3}, {"type": "image", "img_path": "y8P633E5HQ/tmp/c0589559c826f0924b7b80af341576b906b415c0e301cecc2107fc4ecb12d890.jpg", "img_caption": ["Figure 1: Illustration of nonlinear spectral filters for equivariant machine learning on graphs. Given a graph $G$ , the node features $\\mathbf{X}$ are projected onto eigenspaces (analysis $\\mathcal{A}$ ). The function $\\Psi$ map a sequence of frequency coefficients to a sequence of frequency coefficients. The coefficients are synthesized to the graph domain using the using $\\boldsymbol{S}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "In App. C.1, we present a special case of diagonal synthesis whered  = d. In App. D.3, we show that the diagonal synthesis is stably invertible. ", "page_idx": 4}, {"type": "text", "text": "3.4 Definitions of Nonlinear Spectral Filters ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We introduce three novel types of non-linear spectral fliters (NLSF): Node-level NLSFs, Graph-level NLSFs, and Pooling-NLSFs. Fig. 1 illustrates our NLSFs for equivariant machine learning on graphs. ", "page_idx": 4}, {"type": "text", "text": "Node-level NLSFs. To be able to transfer NLSFs between different graphs and signals, one key property of NLSF is that they do not depend on the specific basis chosen in each eigenspace. This independence is facilitated by the synthesis process, which relies on the input signal $\\mathbf{X}$ . Following the spectral index and fliter bank cases in Sec. 3.3, we define the Index NLSFs and Value NLSFs by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{\\mathrm{(ind)}}(\\Psi_{\\mathrm{ind}}(A_{\\Delta}^{\\mathrm{(ind)}}(\\mathbf{X})))=\\left[\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J+1}\\left[\\Psi_{\\mathrm{ind}}\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)\\right]_{i=1}^{J+1},\\qquad(4)}\\\\ &{\\Theta_{\\mathrm{val}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{\\mathrm{(val)}}(\\Psi_{\\mathrm{val}}(A_{\\Delta}^{\\mathrm{(val)}}(\\mathbf{X})))=\\left[\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\left\\|g_{j}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{K+1}\\left[\\Psi_{\\mathrm{val}}\\left(\\left\\|g_{i}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)\\right]_{i=1}^{K+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Psi_{\\mathrm{ind}}~:~\\mathbb{R}^{(J+1)d}~\\rightarrow~\\mathbb{R}^{(J+1)d\\times(J+1)\\widetilde{d}}$ and $\\Psi_{\\mathrm{val}}~:~\\mathbb{R}^{(K+1)d}~\\rightarrow~\\mathbb{R}^{(K+1)d\\times(K+1)\\widetilde{d}}$ are called nonlinear frequency responses, and $\\widetilde{d}$ is the output dimension. To adjust the feature output dimension, we apply an MLP with shared weights to all nodes after the NLSF. In the case when $\\widetilde{d}=d$ and the fliters operator diagonally (i.e., the product and division are element-wise in synthesis), we refer to it as diag-NLSF. See App. C for more details. ", "page_idx": 4}, {"type": "text", "text": "Graph-level NLSFs. We first introduce the Graph-level NLSFs that are fully spectral, where the NLSFs map a sequence of frequency coefficients to an output vector. Specifically, the Index-based and Value-based Graph-level NLSFs are given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi_{\\mathrm{ind}}(\\Delta,\\mathbf{X})\\!=\\!\\widehat{\\Psi}_{\\mathrm{ind}}\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)\\ \\mathrm{~and}\\ \\ \\Phi_{\\mathrm{val}}(\\Delta,\\mathbf{X})\\!=\\!\\widehat{\\Psi}_{\\mathrm{val}}\\left(\\left\\|g_{i}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widehat{\\Psi}_{\\mathrm{ind}}:\\mathbb{R}^{(J+1)d}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ , $\\widehat{\\Psi}_{\\mathrm{val}}:\\mathbb{R}^{(K+1)d}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ , and $d^{\\prime}$ is the output dimension. ", "page_idx": 4}, {"type": "text", "text": "Pooling-NLSFs. We introduce another type of graph-level NLSFs by first representing each graph in a Node-level NLSFs as in Eq. (4) and Eq. (5). The final graph representation is obtained by applying a nonlinear activation function followed by a readout function to these node-level representations. We consider four commonly used pooling methods, including mean, sum, max, and $L_{p}$ -norm pooling, as the readout function for each graph. We apply an MLP after readout function to obtain a $d^{\\prime}$ - dimensional graph-level representation. We term these graph-level NLSFs as Pooling-NLSFs. ", "page_idx": 4}, {"type": "image", "img_path": "y8P633E5HQ/tmp/5bd93b993119f0ab0fb6a6b75c050b090244461a9209a32538ab5e0c34199198.jpg", "img_caption": ["Figure 2: Illustration of Laplacian attention NLSFs. An attention mechanism is applied to both Index NLSFs and Value NLSFs, enabling the adaptive selection of the most appropriate parameterization. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.5 Laplacian Attention NLSFs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To understand which Laplacian and parameterization (index v/s value) of the NLSF are preferable in different settings, we follow the random geometric graph analysis outlined in [74]. Specifically, we consider a setting where random geometric graphs are sampled from a metric-probability space $\\boldsymbol{S}$ . In such a case, the graph Laplacian approximates continuous Laplacians on the metric spaces under some conditions. We aim for our NLSFs to produce approximately the same outcome for any two graphs sampled from the same underlying metric space $\\boldsymbol{S}$ , ensuring that the NLSF is transferable. In App. D.5, we show that if the nodes of the graph are sampled uniformly from $\\boldsymbol{S}$ , then using the graph Laplacian $\\mathbf{L}$ in Index NLSFs yields a transferable method. Conversely, if the nodes of the graph are sampled non-uniformly, and any two balls of the same radius in $\\boldsymbol{S}$ have the same volume, then utilizing the normalized graph Laplacian $\\mathbf{N}$ in Value NLSFs is a transferable method. Given that graphs may fall between these two boundary cases, we present an architecture that chooses between the Index NLSFs with respect to $\\mathbf{L}$ and Value NLSFs with respect to $\\mathbf{N}$ , as illustrated in Fig. 2. While the above theoretical setting may not be appropriate as a model for every real-life graph dataset, it suggests that index NLSF may be more appropriate with $\\mathbf{L}$ , value NLSFs with $\\mathbf{N}$ , and different graphs are more appropriately analyzed by different balances between these two cases. ", "page_idx": 5}, {"type": "text", "text": "In the Laplacian attention architecture, a soft attention mechanism is employed to dynamically choose between the two parameterizations, given by ", "page_idx": 5}, {"type": "equation", "text": "$\\begin{array}{r}{(\\Theta_{\\mathrm{ind}}(\\mathbf{L},\\mathbf{X}),\\Theta_{\\mathrm{val}}(\\mathbf{N},\\mathbf{X}))=\\alpha\\Theta_{\\mathrm{ind}}(\\mathbf{L},\\mathbf{x})\\|(1-\\alpha)\\Theta_{\\mathrm{val}}(\\mathbf{N},\\mathbf{X}),}\\end{array}$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $0\\leq\\alpha\\leq1$ is obtained using a softmax function to normalize the scores into attention weights, balancing each NLSFs\u2019 contribution. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Properties of Nonlinear Spectral Filters ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the desired theoretical properties of our NLSFs at the node-level and graph-level. ", "page_idx": 5}, {"type": "text", "text": "4.1 Complexity of NLSFs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "NLSFs are implemented by computing the eigenvectors of the GSO. Most existing spectral GNNs avoid direct eigendecomposition due to its perceived inefficiency. Instead, they use fliters implemented by applying polynomials [49, 22] or rational functions [61, 5] to the GSO in the spatial domain. However, power iteration-based eigendecomposition algorithms, e.g., variants of the Lanczos method, can be highly efficient [80, 55]. For matrices with $E$ non-zero entries, the computational complexity of one iteration for finding $J$ eigenvectors corresponding to the smallest or largest eigenvalues (called leading eigenvectors) is $O(J E)$ . In practice, these eigendecomposition algorithms converge quickly due to their super-exponential convergence rate, often requirin\u221ag only a few iterations, which makes them as efficient as message passing networks of signals with J channels. ", "page_idx": 5}, {"type": "text", "text": "This makes NLSFs applicable to node-level tasks on large sparse graphs, as demonstrated empirically in App. F.5, since they rely solely on the leading eigenvectors. In Sec. 4.4, we show that using the leading eigenvectors can approximate GSOs well in the context of learning on graphs. Note that we can precompute the spectral projections of the signal before training. For node-level tasks, such as semi-supervised node classification, the leading eigenvectors only need to be pre-computed once, with a complexity of $O(J E)$ . This During the learning phase, each step of the architecture search and hyperparameter optimization takes $O(N J d)$ complexity for analysis and synthesis, and $O(J^{2}d^{2})$ ", "page_idx": 5}, {"type": "text", "text": "for the MLP in the spectral domain, which is faster than the complexity $O(E d^{2})$ of message passing or standard spectral methods if $N J<E d$ . Empirical studies on runtime analysis are in App. F. ", "page_idx": 6}, {"type": "text", "text": "For dense matrices, the computational complexity of a full eigendecomposition is $O(N^{b})$ per iteration, where $N^{b}$ is the complexity of matrix multiplication. This is practical for graph-level tasks on relatively small and dense graphs, which is typical for many graph classification datasets. In these cases, the eigendecomposition of all graphs in the dataset can be performed as a pre-computation step, significantly reducing the complexity during the learning phase. ", "page_idx": 6}, {"type": "text", "text": "4.2 Equivariance of Node-level NLSFs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We demonstrate the node-level equivariance of our NLSFs, ensuring that our method respects the functional shift symmetries. The proof is given in App. D.1. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Index NLSFs in Eq. (4) are equivariant to the graph functional shifts $\\mathcal{U}_{\\Delta}$ , and Value NLSFs in Eq. (5) are equivariant to the relaxed graph functional shifts $\\mathcal{U}_{\\Delta}^{g}$ . ", "page_idx": 6}, {"type": "text", "text": "4.3 Universal Approximation and Expressivity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we discuss the approximation power of NLSFs. ", "page_idx": 6}, {"type": "text", "text": "Node-Level Universal Approximation. We begin with a setting where a graph is given as a fixed domain, and the data distribution consists of multiple signals defined on this graph. An example of this setup is a spatiotemporal graph [17], e.g., traffic networks, where a fixed sensor system defines a graph and the different signals represent the sensor readings collected at different times. ", "page_idx": 6}, {"type": "text", "text": "In App. D.2.1, we prove the following lemma, which shows that linear NLSFs exhaust the space of linear operators that commute with graph functional shifts. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. A linear operator $\\mathbb{R}^{N\\times d}\\rightarrow\\mathbb{R}^{N\\times d}$ commutes with $\\mathcal{U}_{\\Delta}^{J}$ (resp. $\\mathcal{U}_{\\Delta}^{g}$ ) iff it is a NLSF based on a linear function $\\Psi$ in Eq. (4) (resp. Eq. (5)). ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 shows a close relationship between functions that commute with functional shifts and those defined in the spectral domain. This motivates the following construction of a pseudo-metric on $\\mathbb{R}^{N\\times d}$ . In the case of relaxed functional shifts, we define the standard Euclidean metric $\\mathrm{\\dist_{E}}$ in the spectral domain $\\mathbb{R}^{(K+1)\\times d}$ . We pull back the Euclidean metric to the spatial domain to define a signal pseudo-metric. Namely, for two signals $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ , their distance is defined by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{dist}_{\\Delta}\\!\\left(\\mathbf{X},\\mathbf{X}^{\\prime}\\right):=\\mathrm{dist}_{\\mathrm{E}}\\!\\left(A(\\Delta,\\mathbf{X}),A(\\Delta^{\\prime},\\mathbf{X}^{\\prime})\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This pseudo metric can be made into a metric by considering each equivalence class of signals with zero distance as a single point in the space. As MLPs $\\Psi$ can approximate any continuous function $\\mathbb{R}^{(K+1)\\times d}\\,\\rightarrow\\,\\mathbb{R}^{(K+\\overbar{1})\\times\\hat{d}}$ (the universal approximation theorem [19, 34, 58]), node-level NLSFs can approximate any continuous function that maps (equivalence classes of) signals to (equivalence classes of) signals. For details, see App. D.2.2. A similar analysis applies to hard functional shifts. ", "page_idx": 6}, {"type": "text", "text": "Graph-Level Universal Approximation. The above analysis also motivates the construction of a graph-signal metric for graph-level tasks. For graphs with $d_{\\cdot}$ -channel signals, we consider again the standard Euclidean metric $\\mathrm{{dist}_{E}}$ in the spectral domain $\\mathbb{R}^{(K+1)\\times d}$ . We define the distance between any two graphs with GSOs and signals $(\\Delta,\\mathbf{X})$ and $(\\Delta^{\\prime},\\mathbf{X}^{\\prime})$ to be ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{dist}\\big((\\Delta,\\mathbf{X}),(\\Delta^{\\prime},\\mathbf{X}^{\\prime})\\big):=\\mathrm{dist}_{\\mathrm{E}}\\big(\\mathcal{A}(\\Delta,\\mathbf{X}),\\mathcal{A}(\\Delta^{\\prime},\\mathbf{X}^{\\prime})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This definition can be extended into a metric by considering the space $\\mathcal{G}$ of equivalence classes of graph-signals with distance 0. As before, this distance inherits the universal approximation properties of standard MLPs. Namely, any continuous function $\\mathcal{G}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ with respect to dist can be approximated by NLSFs based on MLPs. Additional details are in App. D.2.3. ", "page_idx": 6}, {"type": "text", "text": "Graph-Level Expressivity of Pooling-NLSFs. In App. D.2.4, we show that Pooling-NLSFs are more expressive than graph-level NLSF when any $L_{p}$ norm is used in Eq. (4) and Eq. (5) with $p\\neq2$ , both in the definition of the NLSF and as the pooling method. Specifically, for every graph-level NLSF, there is a Pooling-NLSF that coincides with it. Additionally, there are graph signals $(\\Delta,\\mathbf{X})$ and $(\\Delta^{\\prime},\\mathbf{X}^{\\prime})$ for which a Pooling-NLSF can attain different values, whereas any graph-level NLSF must attain the same value. Hence, Pooling-NLSFs have improved discriminative power compared to graph-level NLSFs. Indeed, as shown in Tab. 3, Pooling-NLSFs outperform Graph-level NLSFs in practice, which can be attributed to their increased expressivity. We refer to App. D.2.5 for additional discussion on graph-level expressivity. ", "page_idx": 6}, {"type": "table", "img_path": "y8P633E5HQ/tmp/0f053ce43ac5bed367e51a3a5d567a2827336a70191fb6bb87a295e31a851a9e.jpg", "table_caption": ["Table 1: Semi-supervised node classification accuracy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 Uniform Approximation of GSOs by Their Leading Eigenvectors ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since NLSFs on large graphs are based on the leading eigenvectors of $\\Delta$ , we justify its low-rank approximation in the following. While approximating matrices with low-rank matrices might lead to a high error in the spectral and Frobenius norms, we show that such an approximation entails a uniformly small error in the cut norm. We define and interpret the cut norm in App. D.4.1, and explain why it is a natural graph similarity measure for graph machine learning. ", "page_idx": 7}, {"type": "text", "text": "The following theorem is a corollary of the Constructive Weak Regularity Lemma presented in [29].   \nIts proof is presented in App. D.4. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1. Let M be a symmetric matrix with entries bounded by $|m_{i,j}|\\,\\leq\\,\\alpha_{}$ , and let $J\\,\\in\\,\\mathbb{N}$ . Suppose m is sampled uniformly from $[J]$ , and let $R\\geq1$ s.t. $J/R\\in\\mathbb{N}$ . Let $\\phi_{1},...,\\phi_{m}$ be the leading eigenvectors of M, with eigenvalues $\\mu_{1},\\ldots,\\mu_{m}$ ordered by their magnitudes $|\\mu_{1}|\\geq\\ldots\\geq$ $|\\mu_{m}|$ . Define $\\begin{array}{r}{\\mathbf{C}=\\sum_{k=1}^{m}\\mu_{k}\\boldsymbol{\\phi}_{k}\\boldsymbol{\\phi}_{k}^{\\intercal}}\\end{array}$ . Then, with probability $\\textstyle1-{\\frac{1}{R}}$ (w.r.t. the choice of $m$ ), ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{M}-\\mathbf{C}\\rVert_{\\Omega}<\\frac{3\\alpha}{2}\\sqrt{\\frac{R}{J}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that the bound in Thm. 1 is uniformly small, independently of $\\mathbf{M}$ and its dimension $N$ . This theorem justifies using the leading eigenvectors when working with the adjacency matrix as the GSO. For a justification when working with other GSOs see App. D.4. ", "page_idx": 7}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the NLSFs on node and graph classification tasks. Additional implementation details are in App. E, and additional experiments, including runtime analysis and ablation studies, are in App. F. ", "page_idx": 7}, {"type": "text", "text": "5.1 Semi-Supervised Node Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first demonstrate the main advantage of the proposed Node-level NLSFs over existing GNNs with convolution design on semi-supervised node classification tasks. We test three citation networks [84, 100]: Cora, Citeseer, and Pubmed. In addition, we explore three heterophilic graphs: Chameleon, Squirrel, and Actor [79, 90]. For more comprehensive descriptions of these datasets, see App. E. ", "page_idx": 7}, {"type": "text", "text": "We compare the Node-level NLSFs using Laplacian attention with existing spectral GNNs for nodelevel predictions, including GCN [49], ChebNet [22], ChebNetII [41], CayleyNet [61], APPNP [50], GPRGNN [16], ARMA [5], JacobiConv [96], BernNet [42], Specformer [7], and OptBasisGNN [38]. We also consider GAT [93] and SAGE [39]. For datasets splitting on citation graphs (Cora, Citeseer, and Pubmed), we apply the standard splits following [100], using 20 nodes per class for training, ", "page_idx": 7}, {"type": "text", "text": "500 nodes for validation, and 1000 nodes for testing. For heterophilic graphs (Chameleon, Squirrel, and Actor), we use the sparse splitting as in [16, 41], allocating $2.5\\%$ of samples for training, $2.5\\%$ for validation, and $95\\%$ for testing. We measure the classification quality by computing the average classification accuracy with a $95\\%$ confidence interval over 10 random splits. We utilize the source code released by the authors for the baseline algorithms and optimize their hyperparameters using Optuna [1]. Each model\u2019s hyperparameters are fine-tuned to achieve the highest possible accuracy. Detailed hyperparameter settings are provided in App. E. ", "page_idx": 8}, {"type": "text", "text": "Tab. 1 presents the node classification accuracy of our NLSFs using Laplacian attention and the various competing baselines. We see that att-Node-level NLSFs outperform the competing models on the Cora, Citeseer, and Chameleon datasets. Notably, it shows remarkable performance on the densely connected Squirrel graph, outperforming the baselines by a large margin. This can be explained by the sparse version in Eq. (21) of Thm. 1, which shows that the denser the graphs is, the better its rank- $J$ approximation. For the Pubmed and Actor datasets, att-Node-level NLSFs yield the second-best results, which are comparable to the best results obtained by APPNP. ", "page_idx": 8}, {"type": "text", "text": "5.2 Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Recently, the work in [77] identi- Table 2: Node classification performance on original and fied the presence of many duplicate filtered Chameleon and Squirrel datasets. ", "page_idx": 8}, {"type": "text", "text": "nodes across the train, validation, and test splits in the dense split setting of the Chameleon and Squirrel [75]. This results in train-test data leakage, causing GNNs to inadvertently fit the test splits during training, thereby making performance results on Chameleon and Squirrel less reliable. To further validate the performance of our Node-level NLSFs ", "page_idx": 8}, {"type": "table", "img_path": "y8P633E5HQ/tmp/2e0c30b4072a37570720cfff62f71d6ee5b2866342a34ba1b0db0d5cdbd56a9e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "on these datasets in the dense split setting, we use both the original and flitered versions of Chameleon and Squirrel, which do not contain duplicate nodes, as suggested in [77]. We use the same random splits as in [77], dividing the datasets into $48\\%$ for training, $32\\%$ for validation, and $20\\%$ for testing. ", "page_idx": 8}, {"type": "text", "text": "Tab. 2 presents the classification performance comparison between the original and flitered Chameleon and Squirrel. The baseline results are taken from [77], and we include the following competitive models: ResNet+SGC [77], ResNet+adj [77], GCN [49], GPRGNN [16], FSGNN [69], GloGNN [62], and FAGCN [8]. The detailed comparisons are in App. F. We see in the table that the attNode-level NLSFs consistently outperform the competing baselines on both the original and flitered datasets. att-Node-level NLSFs demonstrate less sensitivity to node duplicates and exhibit stronger generalization ability, further validating the reliability of the Chameleon and Squirrel datasets in the dense split setting. We note that compared to the dense split setting, the sparse split setting in Tab. 1 is more challenging, resulting in lower classification performance. A similar trend of significant performance difference between the two settings of Chameleon and Squirrel is also observed in [41]. ", "page_idx": 8}, {"type": "text", "text": "5.3 Graph Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further illustrate the power of NLSFs on eight graph classification benchmarks [47]. Specifically, we consider five bioinformatics datasets [10, 53, 86]: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, where MUTAG, PTC, and NCI1 are characterized by discrete node features, while ENZYMES and PROTEINS have continuous node features. Additionally, we examine three social network datasets: IMDB-B, IMDB-M, and COLLAB. The unattributed graphs are augmented by adding node degree features following [26]. For more details of these datasets, see App. E. ", "page_idx": 8}, {"type": "text", "text": "In graph classification tasks, a readout function is used to summarize node representations for each graph. The final graph-level representation is obtained by aggregating these node-level summaries and is then fed into an MLP with a (log)softmax layer to perform the graph classification task. We compare our NLSFs with two kernel-based approaches: GK [87] and WL [86], as well as nine GNNs: GCN [49], GAT [93], SAGE [39], ChebNet [22], ChebNetII [41], CayleyNet [61], APPNP [50], GPRGNN [16], and ARMA [5]. Additionally, we consider the hierarchical graph pooling model ", "page_idx": 8}, {"type": "table", "img_path": "y8P633E5HQ/tmp/fc5a569e05ad7114457e8d66a3ef80a24d2f6fca3ec61426f50bea61a96fe2c1.jpg", "table_caption": ["Table 3: Graph classification accuracy. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "DiffPool [101]. For dataset splitting, we apply the random split following [94, 101, 67, 104], using $80\\%$ for training, $10\\%$ for validation, and $10\\%$ for testing. This random splitting process is repeated 10 times, and we report the average performance along with the standard deviation. For the baseline algorithms, we use the source code released by the authors and optimize their hyperparameters using Optuna [1]. We fine-tune the hyperparameters of each model to achieve the highest possible accuracy. Detailed hyperparameter settings for both the baselines and our method are provided in App. E. ", "page_idx": 9}, {"type": "text", "text": "Tab. 3 presents the graph classification accuracy. Notably, the att-Graph-level NLSFs (i.e., NLSFs without the synthesis and pooling process) perform the second best on the ENZYMES and PROTEINS. Additionally, att-Pooling-NLSFs consistently outperform att-Graph-level NLSFs, indicating that the node features learned in our Node-level NLSFs representation are more expressive, corroborating our theoretical findings in Sec. 4.3. Furthermore, our att-Pooling-NLSFs consistently outperform all baselines on the MUTAG, PTC, ENZYMES, PROTEINS, IMDB-M, and COLLAB datasets. For NCI1 and IMDB-B, att-Pooling-NLSFs rank second and are comparable to ChebNetII. ", "page_idx": 9}, {"type": "text", "text": "6 Summary ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented an approach for defining non-linear filters in the spectral domain of graphs in a transferable and equivariant way. Transferability between different graphs is achieved by using the input signal as a basis for the synthesis operator, making the NLSF depend only on the eigenspaces of the GSO and not on an arbitrary choice of the eigenvectors. While different graph-signals may be of different sizes, the spectral domain is a fixed Euclidean space independent of the size and topology of the graph. Hence, our spectral approach represents graphs as vectors. We note that standard spectral methods do not have this property since the coefficients of the signal in the frequency domain depend on an arbitrary choice of eigenvectors, while our representation depends only on the eigenspaces. We analyzed the universal approximation and expressivity power of NLSFs through metrics that are pulled back from this Euclidean vector space. From a geometric point of view, our NLSFs are motivated by respecting graph functional shift symmetries, making them related to Euclidean CNNs. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future Work. One limitation of NLSFs is that, when deployed on large graphs, they only depend on the leading eigenvectors of the GSO and their orthogonal complement. However, important information can also lie within any other band. In future work, we plan to explore NLSFs that are sensitive to eigenvalues that lie within a set of bands of interest, which can be adaptive to the graph. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of YEL and RT was supported by the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No. 802735-ERC-DIFFOP. The work of RL was supported by the Israel Science Foundation grant #1937/23: Analysis of graph deep learning using graphon theory. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2623\u20132631, 2019. [2] K. Atz, F. Grisoni, and G. Schneider. Geometric deep learning on molecular representations. Nature Machine Intelligence, 3:1023\u20131032, 2021. [3] A.-L. Barabasi and Z. N. Oltvai. Network biology: understanding the cell\u2019s functional organization. Nature reviews genetics, 5(2):101\u2013113, 2004. [4] B. Bevilacqua, F. Frasca, D. Lim, B. Srinivasan, C. Cai, G. Balamurugan, M. M. Bronstein, and H. Maron. Equivariant subgraph aggregation networks. In International Conference on Learning Representations, 2022.   \n[5] F. M. Bianchi, D. Grattarola, L. Livi, and C. Alippi. Graph neural networks with convolutional ARMA filters. IEEE transactions on pattern analysis and machine intelligence, 44(7):3496\u2013 3507, 2021.   \n[6] A. Bietti, L. Venturi, and J. Bruna. On the sample complexity of learning with geometric stability. arXiv preprint arXiv:2106.07148, 2021. [7] D. Bo, C. Shi, L. Wang, and R. Liao. Specformer: Spectral graph neural networks meet transformers. In The Eleventh International Conference on Learning Representations, 2023.   \n[8] D. Bo, X. Wang, C. Shi, and H. Shen. Beyond low-frequency information in graph convolutional networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 3950\u20133957, 2021.   \n[9] S. P. Borgatti, A. Mehra, D. J. Brass, and G. Labianca. Network analysis in the social sciences. science, 323(5916):892\u2013895, 2009.   \n[10] K. M. Borgwardt, C. S. Ong, S. Sch\u00f6nauer, S. Vishwanathan, A. J. Smola, and H.-P. Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005.   \n[11] R. Bracewell and P. B. Kahn. The fourier transform and its applications. American Journal of Physics, 34(8):712\u2013712, 1966.   \n[12] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veli\u02c7ckovi\u00b4c. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. preprint arXiv:2104.13478, 2021.   \n[13] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18\u201342, 2017.   \n[14] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.   \n[15] D. Burago, S. O. Ivanov, and Y. Kurylev. Spectral stability of metric-measure laplacians. Israel Journal of Mathematics, 232:125\u2013158, 2019.   \n[16] E. Chien, J. Peng, P. Li, and O. Milenkovic. Adaptive universal generalized pagerank graph neural network. In International Conference on Learning Representations, 2020.   \n[17] A. Cini and I. Marisca. Torch Spatiotemporal, 3 2022.   \n[18] T. Cohen and M. Welling. Group equivariant convolutional networks. In ICML, pages 2990\u20132999. PMLR, 2016.   \n[19] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.   \n[20] A. Daigavane, S. Kim, M. Geiger, and T. Smidt. Symphony: Symmetry-equivariant pointcentered spherical harmonics for molecule generation. arXiv preprint arXiv:2311.16199, 2023.   \n[21] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS. Curran Associates Inc., 2016.   \n[22] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29, 2016.   \n[23] L. Du, X. Shi, Q. Fu, X. Ma, H. Liu, S. Han, and D. Zhang. Gbk-gnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily. In Proceedings of the ACM Web Conference 2022, pages 1550\u20131558, 2022.   \n[24] A. Duval, S. V. Mathis, C. K. Joshi, V. Schmidt, S. Miret, F. D. Malliaros, T. Cohen, P. Li\u00f2, Y. Bengio, and M. Bronstein. A hitchhiker\u2019s guide to geometric gnns for 3d atomic systems, 2024.   \n[25] B. Elesedy and S. Zaidi. Provably strict generalisation benefit for equivariant models. ICML, 2021.   \n[26] F. Errica, M. Podda, D. Bacciu, and A. Micheli. A fair comparison of graph neural networks for graph classification. In International Conference on Learning Representations, 2019.   \n[27] P. Esser, L. Chennuru Vankadara, and D. Ghoshdastidar. Learning theory can (sometimes) explain generalisation in graph neural networks. Advances in Neural Information Processing Systems, 34:27043\u201327056, 2021.   \n[28] M. Fey and J. E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.   \n[29] B. Finkelshtein, \u02d9I. \u02d9I. Ceylan, M. Bronstein, and R. Levie. Learning on large graphs using intersecting communities. arXiv preprint arXiv:2405.20724, 2024.   \n[30] M. Finzi, S. Stanton, P. Izmailov, and A. G. Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In International Conference on Machine Learning, pages 3165\u20133176. PMLR, 2020.   \n[31] F. Frasca, B. Bevilacqua, M. Bronstein, and H. Maron. Understanding and extending subgraph GNN by rethinking their symmetries. Advances in Neural Information Processing Systems, 35:31376\u201331390, 2022.   \n[32] A. M. Frieze and R. Kannan. Quick approximation to matrices and applications. Combinatorica, 1999.   \n[33] F. Fuchs, D. Worrall, V. Fischer, and M. Welling. SE(3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:1970\u2013 1981, 2020.   \n[34] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183\u2013192, 1989.   \n[35] V. Garg, S. Jegelka, and T. Jaakkola. Generalization and representational limits of graph neural networks. In International Conference on Machine Learning, pages 3419\u20133430. PMLR, 2020.   \n[36] T. Gaudelet, B. Day, A. R. Jamasb, J. Soman, C. Regep, G. Liu, J. B. Hayter, R. Vickers, C. Roberts, J. Tang, et al. Utilizing graph machine learning within drug discovery and development. Briefings in bioinformatics, 22(6):bbab159, 2021.   \n[37] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017.   \n[38] Y. Guo and Z. Wei. Graph neural networks with learnable and optimal polynomial bases. In International Conference on Machine Learning, pages 12077\u201312097. PMLR, 2023.   \n[39] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017.   \n[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[41] M. He, Z. Wei, and J.-R. Wen. Convolutional neural networks on graphs with chebyshev approximation, revisited. Advances in Neural Information Processing Systems, 35:7264\u20137276, 2022.   \n[42] M. He, Z. Wei, H. Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021.   \n[43] W. Hoeffding. Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pages 409\u2013426, 1994.   \n[44] N. Huang, R. Levie, and S. Villar. Approximately equivariant graph networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[45] J. M. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Z\u00eddek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. A. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583 \u2013 589, 2021.   \n[46] N. Keriven and G. Peyr\u00e9. Universal invariant and equivariant graph neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \n[47] K. Kersting, N. M. Kriege, C. Morris, P. Mutzel, and M. Neumann. Benchmark data sets for graph kernels. 2016.   \n[48] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[49] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016.   \n[50] J. Klicpera, A. Bojchevski, and S. G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations, 2019.   \n[51] M. Kofinas, B. Knyazev, Y. Zhang, Y. Chen, G. J. Burghouts, E. Gavves, C. G. M. Snoek, and D. W. Zhang. Graph neural networks for learning equivariant representations of neural networks. 2024.   \n[52] N. J. Korevaar and R. M. Schoen. Sobolev spaces and harmonic maps for metric space targets. Communications in Analysis and Geometry, 1:561\u2013659, 1993.   \n[53] N. Kriege and P. Mutzel. Subgraph matching kernels for attributed graphs. arXiv preprint arXiv:1206.6483, 2012.   \n[54] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.   \n[55] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. 1950.   \n[56] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.   \n[57] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Proceedings of 2010 IEEE international symposium on circuits and systems, pages 253\u2013256. IEEE, 2010.   \n[58] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861\u2013 867, 1993.   \n[59] R. Levie. A graphon-signal analysis of graph neural networks. In NeurIPS, 2023.   \n[60] R. Levie, W. Huang, L. Bucci, M. Bronstein, and G. Kutyniok. Transferability of spectral graph convolutional neural networks. Journal of Machine Learning Research, 22(272):1\u201359, 2021.   \n[61] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein. Cayleynets: Graph convolutional neural networks with complex rational spectral filters. IEEE Transactions on Signal Processing, 67(1):97\u2013109, 2018.   \n[62] X. Li, R. Zhu, Y. Cheng, C. Shan, S. Luo, D. Li, and W. Qian. Finding global homophily in graph neural networks when meeting heterophily. In International Conference on Machine Learning, pages 13242\u201313256. PMLR, 2022.   \n[63] D. Lim, F. Hohne, X. Li, S. L. Huang, V. Gupta, O. Bhalerao, and S. N. Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887\u201320902, 2021.   \n[64] D. Lim, J. Robinson, S. Jegelka, and H. Maron. Expressive sign equivariant networks for spectral geometric learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[65] L. Lov\u00e1sz and B. Szegedy. Szemer\u00e9di\u2019s lemma for the analyst. GAFA Geometric And Functional Analysis, 2007.   \n[66] L. M. Lov\u00e1sz. Large networks and graph limits. In volume 60 of Colloquium Publications, 2012.   \n[67] Y. Ma, S. Wang, C. C. Aggarwal, and J. Tang. Graph convolutional networks with eigenpooling. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 723\u2013731, 2019.   \n[68] S. G. Mallat. A theory for multiresolution signal decomposition: the wavelet representation. IEEE transactions on pattern analysis and machine intelligence, 11(7):674\u2013693, 1989.   \n[69] S. K. Maurya, X. Liu, and T. Murata. Simplifying approach to node classification in graph neural networks. Journal of Computational Science, 62:101695, 2022.   \n[70] O. M\u00e9ndez-Lucio, M. Ahmad, E. A. del Rio-Chanona, and J. K. Wegner. A geometric deep learning approach to predict binding conformations of bioactive molecules. Nature Machine Intelligence, 3:1033\u20131039, 2021.   \n[71] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5115\u20135124, 2017.   \n[72] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020.   \n[73] M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, and L. Guibas. Functional maps: A flexible representation of maps between shapes. ACM Transactions on Graphics (ToG), 31(4):1\u201311, 2012.   \n[74] R. Paolino, A. Bojchevski, S. G\u00fcnnemann, G. Kutyniok, and R. Levie. Unveiling the sampling density in non-uniform geometric graphs. In The Eleventh International Conference on Learning Representations, 2023.   \n[75] H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations, 2020.   \n[76] M. Petrache and S. Trivedi. Approximation-generalization trade-offs under (approximate) group equivariance. Advances in Neural Information Processing Systems, 36, 2024.   \n[77] O. Platonov, D. Kuznedelev, M. Diskin, A. Babenko, and L. Prokhorenkova. A critical look at the evaluation of gnns under heterophily: Are we really making progress? In The Eleventh International Conference on Learning Representations, 2023.   \n[78] O. Puny, D. Lim, B. Kiani, H. Maron, and Y. Lipman. Equivariant polynomials for graph neural networks. In International Conference on Machine Learning, pages 28191\u201328222. PMLR, 2023.   \n[79] B. Rozemberczki, C. Allen, and R. Sarkar. Multi-scale attributed node embedding. Journal of Complex Networks, 9(2):cnab014, 2021.   \n[80] Y. Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011.   \n[81] R. Sato, M. Yamada, and H. Kashima. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM international conference on data mining (SDM), pages 333\u2013341. SIAM, 2021.   \n[82] V. G. Satorras, E. Hoogeboom, and M. Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR, 2021.   \n[83] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61\u201380, 2008.   \n[84] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[85] J.-P. Serre et al. Linear representations of finite groups, volume 42. Springer, 1977.   \n[86] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.   \n[87] N. Shervashidze, S. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt. Efficient graphlet kernels for large graph comparison. In Artificial intelligence and statistics, pages 488\u2013495. PMLR, 2009.   \n[88] Y. Shi, Z. Huang, S. Feng, H. Zhong, W. Wang, and Y. Sun. Masked label prediction: Unified message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509, 2020.   \n[89] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia, C. R. MacNair, S. French, L. A. Carfrae, Z. Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688\u2013702, 2020.   \n[90] J. Tang, J. Sun, C. Wang, and Z. Yang. Social influence analysis in large-scale networks. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 807\u2013816, 2009.   \n[91] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds. arXiv preprint arXiv:1802.08219, 2018.   \n[92] L. N. Trefethen and D. Bau. Numerical Linear Algebra, Twenty-fifth Anniversary Edition. Society for Industrial and Applied Mathematics, 2022.   \n[93] P. Velic\u02c7kovic\u00b4, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[94] P. Velickovic, W. Fedus, W. L. Hamilton, P. Li\u00f2, Y. Bengio, and R. D. Hjelm. Deep graph infomax. ICLR, 2(3):4, 2019. [95] S. Villar, D. W. Hogg, W. Yao, G. A. Kevrekidis, and B. Sch\u00f6lkopf. Towards fully covariant machine learning. arXiv preprint arXiv:2301.13724, 2023.   \n[96] X. Wang and M. Zhang. How powerful are spectral graph neural networks. In International conference on machine learning, pages 23341\u201323362. PMLR, 2022.   \n[97] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):1\u201312, 2019.   \n[98] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4\u201324, 2020.   \n[99] K. Xu, M. Zhang, J. Li, S. S. Du, K.-i. Kawarabayashi, and S. Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848, 2020.   \n[100] Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.   \n[101] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems, 31, 2018.   \n[102] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra. Graph neural networks with heterophily. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11168\u201311176, 2021.   \n[103] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in neural information processing systems, 33:7793\u20137804, 2020.   \n[104] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pages 2069\u20132080, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Non-linear Activation Functions Break the Functional Symmetry ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We offer a construction for complex-valued signals and first order derivative as the graph Laplacian, which is easy to extend to the real case and second order derivative. Consider a circular 1D grid with 100 nodes and the first order central difference as the Laplacian. Here, the Laplacian eigenvectors are the standard Fourier modes. In this case, one can see that a graph functional shift is any operator that is diagonal in the frequency domain and multiplies each frequency by any complex number with the unit norm. Consider the nonlinearity that takes the real part of the signal and then applies ReLU, which we denote in short by ReLU. Consider a graph signal $x=(e^{i\\pi10\\bar{n}/100}+e^{i\\pi20n/1\\bar{0}0})_{n=0}^{99}$ . We consider a graph functional shift $S$ that shifts frequencies 10 and 20 at a distance of 5, and every other frequency is not shifted. Namely, frequency 10 is multiplied by $e^{-i\\pi50/100}=-i$ , frequency 20 by $e^{-i\\pi=-i\\pi100/100}=-1$ , and every other frequency is multiplied by 1. Consider also the classical shift $D$ that translates the whole signal by 5 uniformly. Since $x$ consists only of the frequencies 10 and 20, it is easy to see that $S x=D x$ . Hence, ${\\mathrm{ReLU}}(S x)={\\mathrm{ReLU}}(D x)={\\dot{D}}({\\mathrm{ReLU}}({\\bar{x}}))$ . Conversely, if we apply $\\mathrm{ReLU}(x)$ and only then shift, note that $\\mathrm{ReLU}(x)$ consists of many frequencies in addition to 10 and 20. For example, by nonnegativity of ReLU, $\\mathrm{ReLU}(x)$ has a nonzero DC component (zeroth frequency). Now, $S(\\dot{\\mathrm{ReLU}}(x))$ only translates the 10 and 20 frequencies, so we have $S(\\mathrm{ReLU}(x))\\neq D(\\mathrm{ReLU}(x))=\\mathrm{ReLU}(S(x))$ . ", "page_idx": 16}, {"type": "text", "text": "B Illustrating Functional Translations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we present an additional discussion and illustrations on the new notions of symmetry. ", "page_idx": 16}, {"type": "text", "text": "B.1 Functional Translation of a Gaussian Signal with Different Speeds at Different Frequencies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To illustrate the concepts of relaxed symmetry and functional translation, we present a toy example involving the classical translation and a functional translation of a Gaussian signal on a 2D grid with a standard deviation of 1. ", "page_idx": 16}, {"type": "text", "text": "The classical translation involves moving the entire signal uniformly across the grid. This uniform movement can be represented as $I^{\\prime}(x,\\bar{y})=I(x-t_{x},\\bar{y}-t_{y})$ , where $t_{x}$ and $t_{y}$ are the translation amounts in the $x$ and $y$ directions, respectively, and $x-t_{x}$ and $y-t_{y}$ are circular translations, namely, subtractions modulo the size of the circular grid. For simplicity, we consider $t_{x}=t_{y}=t$ . For instance, if $t\\,=\\,5$ , the Gaussian is shifted by 5 units in both the $x$ and $y$ directions. Fig. 3 top-row shows the classical translation for $t=0$ , $t=5$ , $t=10$ , and $t\\,=\\,15$ . We see that every part of the signal shifts at the same rate and direction, preserving the overall shape of the Gaussian signal. Note that classical translation is equivalent to modulation of the frequency domain, i.e. $\\widehat{I^{\\prime}}(u,v)=\\widehat{I}(u,v)e^{-i2\\pi(u t_{x}+v t_{y})}$ , where $\\ensuremath{\\widehat{I}}(\\ensuremath{\\boldsymbol{u}},\\ensuremath{\\boldsymbol{v}})$ is the Fourier transform of $I(x,y)$ . Therefore, we can view the classical translation as a specific type of functional translation. ", "page_idx": 16}, {"type": "text", "text": "Next, we illustrate a functional translation that is based on a frequency-dependent movement. Specifically, the Gaussian signal is decomposed into low and high-frequency components based on two indicator band-pass filters. In our example, the translation parameter $t$ is different for the low and high-frequency components: low frequencies are shifted by $t_{l o w}$ while high frequencies are shifted by $t_{h i g h}$ . This functional translation is defined via modulations in the frequency domain given by $\\widehat{I_{l o w}^{\\prime}}(u,v)\\;=\\;\\widehat{I}_{l o w}(u,v)e^{-i2\\pi(u t_{x,l o w}+v t_{y,l o w})}$ for low-frequency components and $\\widehat{I}_{h i g h}^{\\prime}(u,v)\\,=\\,\\widehat{I}_{h i g h}(u,v)e^{-i2\\pi(u t_{x,h i g h}+v t_{y,h i g h})}$ for high-frequency components. The combined functionally translated signal in the frequency domain is then $\\widehat{I}^{\\prime}=(\\widehat{I}_{l o w}^{\\prime},\\widehat{I}_{h i q h}^{\\prime})$ . Fig. 3 bottom-row demonstrates the functional translation for $t=0$ , $t=5$ , $t=10$ , and $t=15$ . We observe that the low-frequency components (smooth parts) of the signal move at one speed, while high-frequency components move at another. This demonstrates that functional symmetries are typically more rich than domain symmetries. ", "page_idx": 16}, {"type": "image", "img_path": "y8P633E5HQ/tmp/a6df1ccde5d42c006468eb29420759198ae328a6829c2b850555e4d799e59158.jpg", "img_caption": ["Figure 3: Comparison of classical and functional translation of a Gaussian signal. Top Row (Classical Translation): The Gaussian signal moves uniformly across the grid without changing shape. Bottom Row (Functional Translation): The Gaussian signal translates as low-frequency components move at different speeds than high-frequency components, demonstrating relaxed symmetry. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Robust Graph Functional Shifts in Image Translation and MNIST Classification on Perturbed Grids ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present another toy example to illustrate that functional translations are more stable than hard symmetries of the graph, namely, graph automorphisms (isomorphisms from the graph to itself). Automorphisms are another analog to translations on general graphs, which competes with our notion of functional shifts. Consider as an example the standard 2D circular grid (discretizing the torus). The automorphisms of the grid include all translations. However, this notion of hard symmetry is very sensitive to graph perturbations. If one adds or deletes even one edge in the graph, the automorphism group becomes much smaller and does not contain the translations anymore. ", "page_idx": 17}, {"type": "text", "text": "In contrast, we claim that functional shifts are not so sensitive to graph perturbations. To demonstrate this empirically, we conducted the following toy experiment. We add a Gaussian noise to the edge weights of the 2D grid to create a perturbed graph. Given a standard domain shift, we optimize the coefficients of a functional shift so it is as close as possible to the classical domain shift of the clean grid in Frobenius error. Fig. 4 presents an example of classically and functionally shifted image of the digit 5. The original digit (left), a classical domain translation of the clean grid (middle), and a functional translation constructed from the perturbed graph (right) to match the classical translation. We see that standard translations can be approximated by a graph functional shift on a perturbed grid. ", "page_idx": 17}, {"type": "text", "text": "We further demonstrate the robustness to graph perturbations of NLSFs trained on MNIST classification. The experimental setup follows previous work in [22, 71, 61]. We compare the NLSF on the clean grid (i.e., without Gaussian noise) to the NLSF on the perturbed grid. Tab. 4 presents the classification accuracy of the NLSF. We see that the classification performance is almost unaffected by the perturbation, indicating that NLSFs on the perturbed grid can roughly express CNN-like operations (translation equivariant functions). ", "page_idx": 17}, {"type": "image", "img_path": "y8P633E5HQ/tmp/0c478f0d9c731ced45d822d8178829c1b4d788a177972e4358ec9c98ab4a55d3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "y8P633E5HQ/tmp/d3277995b5124d63397837dda47016b7d6cbe0964fe6ae3174bb125c9d123df8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: Approximate a standard translation by functional translation on a perturbed graph. ", "page_idx": 17}, {"type": "text", "text": "Table 4: Classification accuracy on the MNIST and perturbed MNIST using NLSFs. ", "page_idx": 17}, {"type": "text", "text": "C Special Cases of NLSFs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We present two special cases of NLSFs as follows. ", "page_idx": 18}, {"type": "text", "text": "C.1 Diagonal NLSFs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Sec. 3, we introduced the NLSFs with the output dimension $\\hat{d}$ , which is a tunable hyperparameter. Here, we present a special case when $\\widetilde{d}=d$ such that the multiplication and division in synthesis are operated diagonally. Specifically, the  diagonal analysis and synthesis in the spectral index case and in the filter bank case are defined respectively by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{A}^{\\mathrm{ind,\\,diag}}(\\Delta,\\mathbf{X})=\\Big(\\left\\lVert\\mathbf{P}_{j}\\mathbf{X}\\right\\rVert_{\\mathrm{sig}}\\Big)_{j=1}^{J+1}\\in\\mathbb{R}^{(J+1)\\times d}\\mathrm{~and~}}\\\\ &{\\mathcal{A}^{\\mathrm{val,\\,diag}}(\\Delta,\\mathbf{X})=\\Big(\\left\\lVert g_{j}(\\Delta)\\mathbf{X}\\right\\rVert_{\\mathrm{sig}}\\Big)_{j=1}^{K+1}\\in\\mathbb{R}^{(K+1)\\times d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{S^{\\mathrm{ind,\\,diag}}(\\mathbf{R};\\Delta,\\mathbf{X})=\\sum_{j=1}^{J+1}\\mathbf{r}_{j}\\odot\\,\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}\\,\\mathrm{and}}}\\\\ {\\displaystyle{S^{\\mathrm{val,\\,diag}}(\\mathbf{R};\\Delta,\\mathbf{X})=\\sum_{j=1}^{K+1}\\mathbf{r}_{j}\\odot\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\|g_{j}(\\Delta)\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the product and division are element-wise along the channel direction. That is, $\\mathbf{r}_{j}\\;\\odot\\;(\\cdot)$ $\\begin{array}{r l r}{\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}}&{=}&{\\left[r_{j_{1}}\\frac{\\mathbf{P}_{j}\\mathbf{X}_{:,1}}{\\|\\mathbf{P}_{j}\\mathbf{X}_{:,1}\\|_{2}^{a}+e},\\ldots,r_{j_{d}}\\frac{\\mathbf{P}_{j}\\mathbf{X}_{:,d}}{\\|\\mathbf{P}_{j}\\mathbf{X}_{:,d}\\|_{2}^{a}+e}\\right]}\\end{array}$ in the spectral index case (resp. $\\mathbf{r}_{j}\\;\\left(\\cdot\\right)$ $\\begin{array}{r l r}{\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\|g_{j}(\\Delta)\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}}&{=}&{\\left[r_{j_{1}}\\frac{g_{j}(\\Delta)\\mathbf{X}_{:,1}}{\\|g_{j}(\\Delta)\\mathbf{X}_{:,1}\\|_{2}^{a}+e},\\dots,r_{j_{d}}\\frac{g_{j}(\\Delta)\\mathbf{X}_{:,d}}{\\|g_{j}(\\Delta)\\mathbf{X}_{:,d}\\|_{2}^{a}+e}\\right]}\\end{array}$ in the filter bank case). Here, $\\mathbf{R}\\,=\\,(\\mathbf{r}_{j})_{j=1}^{\\bar{J}+1}\\,\\in\\,\\mathbb{R}^{(J+1)\\times d}$ in the spectral index case (resp. $\\mathbf{\\bar{R}}\\,=\\,(\\mathbf{r}_{j})_{j=1}^{K+1}\\,\\in\\,\\mathbb{R}^{(K+1)\\times d}$ in the filter bank case) are the spectral coefficients to be synthesized and $a,e$ are as before. ", "page_idx": 18}, {"type": "text", "text": "For Node-level diag-NLSFs, we define the Index diag-NLSFs and Value diag-NLSFs as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\mathrm{ind,\\,diag}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{(\\mathrm{ind},\\mathrm{diag})}(\\Psi(A_{\\Delta}^{\\mathrm{(ind)}}(\\mathbf{X})))=\\displaystyle\\sum_{j=1}^{J+1}\\left[\\Psi\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)_{i=1}^{J+1}\\right]_{j}\\;\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e},\\;\\;(11)}\\\\ &{\\Theta_{\\mathrm{val},\\,\\mathrm{diag}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{(\\mathrm{val},\\,\\mathrm{diag})}(\\Psi(A_{\\Delta}^{\\mathrm{(val)}}(\\mathbf{X})))=\\displaystyle\\sum_{j=1}^{K+1}\\left[\\Psi\\left(\\left\\|g_{i}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)_{i=1}^{K+1}\\right]_{j}\\;\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\left\\|g_{j}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Psi:\\mathbb{R}^{(J+1)d}\\rightarrow\\mathbb{R}^{(J+1)d}$ in $\\Theta_{\\mathrm{ind,\\,diag}}$ and $\\Psi:\\mathbb{R}^{(K+1)d}\\rightarrow\\mathbb{R}^{(K+1)d}$ in $\\Theta_{\\mathrm{val,\\,diag}}$ . To adjust the feature output dimension at each node, we apply an MLP with shared weights to all nodes after the NLSF. We present the empirical study of diag-NLSFs in App. F.7. ", "page_idx": 18}, {"type": "text", "text": "C.2 Leading NLSFs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Sec. 3.3, we introduce the NLSFs with the orthogonal complement. Specifically, the $(J+1)$ -th fliter in the Index NLSFs is given by $\\begin{array}{r}{\\mathbf{P}_{J+1}=\\mathbf{I}-\\sum_{j=1}^{J}\\mathbf{P}_{j}}\\end{array}$ , and the $(K+1)$ -th fliter in Vale NLSFs is defined as $\\begin{array}{r}{g_{K+1}(\\pmb{\\Delta})=\\mathbf{I}-\\sum_{j=1}^{K}g_{j}(\\pmb{\\Delta})}\\end{array}$ . To explore the effects of the orthogonal complement, we focus on the leading NLSFs in both the Index NLSFs and Vale NLSFs, considering only the first $J$ and $K$ fliters without including their orthogonal complements. In this case, the analysis and synthesis in the spectral index case and in the filter bank case are respectively given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{A}}^{\\mathrm{ind,\\,lead}}({\\bf R};\\Delta,{\\bf X})=\\left(\\|{\\bf P}_{i}{\\bf X}\\|_{\\mathrm{sig}}\\right)_{i=1}^{J}\\in\\mathbb{R}^{J d}\\mathrm{~and~}}\\\\ &{{\\mathcal{A}}^{\\mathrm{val,\\,lead}}({\\bf R};\\Delta,{\\bf X})=\\left(\\|g_{i}(\\Delta){\\bf X}\\|_{\\mathrm{sig}}\\right)_{i=1}^{K}\\in\\mathbb{R}^{K d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S^{\\mathrm{ind,\\,lead}}({\\bf R};\\Delta,{\\bf X})=\\left[\\frac{{\\bf P}_{j}{\\bf X}}{\\left\\|{\\bf P}_{j}{\\bf X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J}{\\bf R}\\;\\mathrm{and}}\\\\ &{S^{\\mathrm{val,\\,lead}}({\\bf R};\\Delta,{\\bf X})=\\left[\\frac{g_{j}(\\Delta){\\bf X}}{\\left\\|g_{j}(\\Delta){\\bf X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{K}{\\bf R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ${\\mathbf{P}}_{j}$ and $g_{j}(\\Delta)$ are defined as in Sec. 3.2. The lead-NLSFs are then defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\mathrm{ind,\\,lead}}\\big(\\Delta,\\mathbf{X}\\big)=S_{\\Delta,\\mathbf{X}}^{(\\mathrm{ind,\\,lead})}\\big(\\Psi_{\\mathrm{ind}}\\big(\\mathcal{A}_{\\Delta}^{\\mathrm{(ind)}}(\\mathbf{X})\\big)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\Bigg[\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\lVert\\mathbf{P}_{j}\\mathbf{X}\\rVert_{\\mathrm{sig}}^{a}+e}\\Bigg]_{j=1}^{J}\\left[\\Psi_{\\mathrm{ind}}\\left(\\lVert\\mathbf{P}_{i}\\mathbf{X}\\rVert_{\\mathrm{sig}}\\right)\\right]_{i=1}^{J},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\mathrm{val,\\,ind}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{(\\mathrm{val,\\,ind})}(\\Psi_{\\mathrm{val}}(\\mathcal{A}_{\\Delta}^{\\mathrm{(val)}}(\\mathbf{X})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left[\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\left\\|g_{j}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{K}\\left[\\Psi_{\\mathrm{val}}\\left(\\left\\|g_{i}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)\\right]_{i=1}^{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Psi_{\\mathrm{ind}}:\\mathbb{R}^{J d}\\to\\mathbb{R}^{J d\\times J\\hat{d}}$ , $\\Psi_{\\mathrm{val}}:\\mathbb{R}^{K d}\\rightarrow\\mathbb{R}^{K d\\times K\\widetilde{d}}$ , and $\\widetilde{d}$ is the output dimension. To adjust the feature output dimension, we apply an MLP with shared we ights to all nodes after the NLSF. ", "page_idx": 19}, {"type": "text", "text": "Similar to App. C.1, when considering $\\widetilde{d}=d$ such that the multiplication and division in synthesis are operated diagonally, we have the lea d-diag-NLSFs defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\mathrm{ind,\\,lead,\\,diag}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{(\\mathrm{ind},\\,\\mathrm{lead},\\,\\mathrm{diag})}(\\Psi({\\mathcal A}_{\\Delta}^{(\\mathrm{ind})}(\\mathbf{X})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=1}^{J}\\left[\\Psi\\left(\\|\\mathbf{P}_{i}\\mathbf{X}\\|_{\\mathrm{sig}}\\right)_{i=1}^{J}\\right]_{j}\\,\\,\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Theta_{\\mathrm{val,\\,lead,\\,diag}}(\\Delta,\\mathbf{X})=S_{\\Delta,\\mathbf{X}}^{(\\mathrm{val},\\mathrm{lead},\\,\\mathrm{diag})}(\\Psi(\\mathcal{A}_{\\Delta}^{\\mathrm{(val)}}(\\mathbf{X})))}&{}\\\\ {=\\displaystyle\\sum_{j=1}^{K}\\left[\\Psi\\left(\\left\\|g_{i}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)_{i=1}^{K}\\right]_{j}\\,\\frac{g_{j}(\\Delta)\\mathbf{X}}{\\left\\|g_{j}(\\Delta)\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Psi\\,:\\,\\mathbb{R}^{J d}\\,\\rightarrow\\,\\mathbb{R}^{J d}$ in $\\Theta_{\\mathrm{ind}}$ , lead, diag and $\\Psi\\,:\\,\\mathbb{R}^{K d}\\,\\rightarrow\\,\\mathbb{R}^{K d}$ in $\\Theta_{\\mathrm{val}}$ , lead, diag. We present the empirical study of lead-NLSFs and lead-diag-NLSFs in App. F.7. ", "page_idx": 19}, {"type": "text", "text": "D Theoretical Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We note that the numbering of the statements corresponds to the numbering used in the paper, and we have also included several separately numbered propositions and lemmas that are used in supporting the proofs presented. We restate the claim of each statement for convenience. ", "page_idx": 19}, {"type": "text", "text": "D.1 Equivariance of NLSFs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start with two simple lemmas that characterize the graph functional shifts. The lemmas directly follow the fact that two normal operators commute iff the projections upon their eigenspaces commute. ", "page_idx": 19}, {"type": "text", "text": "Projection to Eigenspaces Case. Note that ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{R}^{N}=\\left(\\prod_{j=1}^{J+1}\\mathbf{P}_{j}\\mathbb{R}^{N}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\prod$ denotes direct products of linear spaces and the $(J\\!+\\!1)$ -th fliter is the orthogonal complement, given by $\\begin{array}{r}{\\mathbf{P}_{J+1}=\\mathbf{I}-\\sum_{j=1}^{J}\\mathbf{P}_{j}}\\end{array}$ . Denote by $\\bigoplus$ the direct sum of operators. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.1. U is in $\\mathcal{U}_{\\Delta}^{J}$ iff it has the form $\\mathbf{U}=\\left(\\bigoplus_{j=1}^{J+1}\\mathbf{U}_{j}\\right)$ , where $\\mathbf{{U}}_{j}$ is a unitary operator in $\\mathbf{P}_{j}\\mathbb{R}^{N}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\mathbf{U}\\in\\mathcal{U}_{\\Delta}^{J}$ . Since $\\mathbf{U}$ commute with ${\\mathbf{P}}_{j}$ for $j=1,\\dots,J$ , and with $\\mathbf{I}$ , it also commutes with $\\begin{array}{r}{\\mathbf{P}_{J+1}=\\mathbf{I}-\\sum_{j=1}^{J}\\mathbf{P}_{j}}\\end{array}$ . Therefore, we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{U}=\\sum_{j=1}^{J+1}\\mathbf{P}_{j}\\mathbf{U}=\\sum_{j=1}^{J+1}\\mathbf{P}_{j}^{2}\\mathbf{U}=\\sum_{j=1}^{J+1}\\mathbf{P}_{j}\\mathbf{U}\\mathbf{P}_{j}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, when restricting ${\\bf P}_{j}{\\bf U}{\\bf P}_{j}$ to an operator in $\\mathbf{P}_{j}\\mathbb{R}^{N}$ , it is unitary. Indeed, for every $\\mathbf{v}=\\mathbf{P}_{j}\\mathbf{v}\\in$ $\\mathbf{P}_{j}\\mathbb{R}^{N}$ and $\\mathbf{u}=\\mathbf{P}_{j}\\mathbf{u}\\in\\mathbf{P}_{j}\\mathbb{R}^{N}$ , since $\\mathbf{U}$ is unitary and ${\\mathbf{P}}_{j}$ is self-adjoint and satisfies ${\\bf P}_{j}^{2}={\\bf P}_{j}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\mathbf{P}_{j}\\mathbf{U}\\mathbf{P}_{j}\\mathbf{v},\\mathbf{u}\\right\\rangle=\\left\\langle\\mathbf{v},\\mathbf{P}_{j}\\mathbf{U}^{-1}\\mathbf{P}_{j}\\mathbf{u}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\mathbf{P}_{j}\\mathbf{U}^{-1}\\mathbf{P}_{j}$ is the inverse of ${\\bf P}_{j}{\\bf U}{\\bf P}_{j}$ in $\\mathbf{P}_{j}\\mathbb{R}^{N}$ , since for every $\\mathbf{v}\\in\\mathbf{P}_{j}\\mathbb{R}^{N}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{P}_{j}\\mathbf{U}^{-1}\\mathbf{P}_{j}\\mathbf{P}_{j}\\mathbf{U}\\mathbf{P}_{j}\\mathbf{v}=\\mathbf{P}_{j}\\mathbf{U}^{-1}\\mathbf{U}\\mathbf{P}_{j}\\mathbf{v}=\\mathbf{P}_{j}\\mathbf{v}=\\mathbf{v},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and similarly $\\mathbf{P}_{j}\\mathbf{U}\\mathbf{P}_{j}\\mathbf{P}_{j}\\mathbf{U}^{-1}\\mathbf{P}_{j}\\mathbf{v}\\,=\\,\\mathbf{v}$ . Here, an invertible normal operator commutes with an orthogonal projection if and only if its inverse does. ", "page_idx": 20}, {"type": "text", "text": "The other direction is trivial. ", "page_idx": 20}, {"type": "text", "text": "Projection to Bands Case. Note that ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{R}^{N}=\\left(\\prod_{j=1}^{K+1}g_{j}(\\pmb{\\Delta})\\mathbb{R}^{N}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the $(K+1)$ -th filter is the orthogonal complement, given by $\\begin{array}{r}{g_{K+1}(\\pmb{\\Delta})=\\mathbf{I}-\\sum_{j=1}^{K}g_{j}(\\pmb{\\Delta})}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2. U is in $\\mathcal{U}_{\\Delta}^{g}$ iff it has the form $\\mathbf{U}=\\Big(\\bigoplus_{j=1}^{K+1}\\mathbf{U}_{j}\\Big).$ , where $\\mathbf{{U}}_{j}$ is a unitary operator in gj(\u2206)RN. ", "page_idx": 20}, {"type": "text", "text": "The proof is analogous to the proof of Lemma D.1. ", "page_idx": 20}, {"type": "text", "text": "D.1.1 Proof of Propositions 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proposition 1. Index NLSFs in Eq. (4) are equivariant to the graph functional shifts $\\mathcal{U}_{\\Delta}$ , and Value NLSFs in Eq. (5) are equivariant to the relaxed graph functional shifts $\\mathcal{U}_{\\Delta}^{g}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. We start with Index-NLSFs. Consider the Index NLSF defined as in Eq. (4) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{X})=\\left[\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\lVert\\mathbf{P}_{j}\\mathbf{X}\\rVert_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J+1}\\left[\\Psi_{\\mathrm{ind}}\\left(\\lVert\\mathbf{P}_{i}\\mathbf{X}\\rVert_{\\mathrm{sig}}\\right)\\right]_{i=1}^{J+1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We need to show that for any unitary operator $\\mathbf{U}\\in\\mathcal{U}_{\\Delta}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{U}\\mathbf{X})=\\mathbf{U}\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consider $\\mathbf{U}\\in\\mathcal{U}_{\\Delta}$ and apply it to the graph signal $\\mathbf{X}$ . The Index NLSF with the transformed input is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{UX})=\\left[\\frac{\\mathbf{P}_{j}\\mathbf{UX}}{\\left\\|\\mathbf{P}_{j}\\mathbf{UX}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J+1}\\left[\\Psi_{\\mathrm{ind}}\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{UX}\\right\\|_{\\mathrm{sig}}\\right)\\right]_{i=1}^{J+1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbf{U}\\in\\mathcal{U}_{\\Delta}$ , it commutes with ${\\mathbf{P}}_{j}$ for all $j$ , i.e., $\\mathbf{UP}_{j}=\\mathbf{P}_{j}\\mathbf{U}$ . Using this commutation property, we can rewrite the norm and the projections as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\lVert\\mathbf{P}_{j}\\mathbf{U}\\mathbf{X}\\right\\rVert_{\\mathrm{sig}}=\\left\\lVert\\mathbf{U}\\mathbf{P}_{j}\\mathbf{X}\\right\\rVert_{\\mathrm{sig}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In addition, since $\\mathbf{U}$ is a unitary matrix, it preserves the norm. Therefore, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{UP}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}=\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substituting these expressions back into the definition of the Index NLSFs gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{U}\\mathbf{X})=\\left[\\frac{\\mathbf{U}\\mathbf{P}_{j}\\mathbf{X}}{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J+1}\\left[\\Psi_{\\mathrm{ind}}\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)\\right]_{i=1}^{J+1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that $\\mathbf{U}$ appears linearly in the numerator of the fraction. Hence, we can factor it out by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{UX})=\\mathbf{U}\\left[\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\lVert\\mathbf{P}_{j}\\mathbf{X}\\rVert_{\\mathrm{sig}}^{a}+e}\\right]_{j=1}^{J+1}\\left[\\Psi_{\\mathrm{ind}}\\left(\\lVert\\mathbf{P}_{i}\\mathbf{X}\\rVert_{\\mathrm{sig}}\\right)\\right]_{i=1}^{J+1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The expression inside the summation is exactly the original Index NLSFs applied to $\\mathbf{X}$ , so ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{U}\\mathbf{X})=\\mathbf{U}\\Theta_{\\mathrm{ind}}(\\Delta,\\mathbf{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we have shown that applying the unitary operator $\\mathbf{U}$ to the graph signal $\\mathbf{X}$ results in the Index NLSFs being transformed by the same unitary operator $\\mathbf{U}$ , proving the equivariance property. The proof for value-NLSF follows the same steps. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.2 Expressivity and Universal Approximation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we focus on value parameterized NLSFs. The analysis for index-NLSF is equivalent. ", "page_idx": 21}, {"type": "text", "text": "D.2.1 Proof of Lemma $^{1}$ - Linear Node-level NLSF Exhaust the Linear Operators that Commute with Functional Shifts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We next show that node-level linear NLSFs exhaust the space of linear operators that commute with graph functional shifts (on the fixed graph). ", "page_idx": 21}, {"type": "text", "text": "Lemma 1. A linear operator $\\mathbb{R}^{N\\times d}\\rightarrow\\mathbb{R}^{N\\times d}$ commute with $\\mathcal{U}_{\\Delta}^{J}$ (resp. $\\mathcal{U}_{\\Delta}^{g}$ ) iff it is a NLSF based on a linear function $\\Psi$ in Eq. (4) (resp. Eq. (5)). ", "page_idx": 21}, {"type": "text", "text": "Proof. For simplicity, we restrict the analysis to the case of 1D signals ( $[d=1]$ ). The extension to a general dimension $d$ is natural. ", "page_idx": 21}, {"type": "text", "text": "By Lemma D.2, the unitary operators $\\mathbf{U}$ in $\\mathcal{U}_{\\Delta}^{g}$ are exhausted by the operators of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{U}=\\left(\\bigoplus_{j=1}^{K+1}\\mathbf{U}_{j}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{{U}}_{j}$ is any unitary operator in $g_{j}(\\pmb{\\Delta})\\mathbb{R}^{N}$ . Hence, since the unitary representation $\\mathbf T\\mapsto\\mathbf T$ of the group of unitary operators in $\\mathbb{R}^{m}$ (for any $m\\in\\mathbb{N}$ ) is irreducible, by Schur\u2019s lemma [85] any linear operator $\\mathbf{B}$ that commutes with all operators of $\\mathcal{U}_{\\Delta}^{g}$ must be a scalar times the identity when projected to $g_{j}(\\pmb{\\Delta})\\mathbb{R}^{N}$ . Namely, $\\mathbf{B}$ has the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\left(\\bigoplus_{j=1}^{K+1}(b_{j}\\mathbf{I}_{j})\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ${\\bf{I}}_{j}$ is the identity operator in $g_{j}(\\pmb{\\Delta})\\mathbb{R}^{N}$ . This means that linear node-level NLSFs exhaust the space of linear operators that commute with $\\mathcal{U}_{\\Delta}^{g}$ . ", "page_idx": 21}, {"type": "text", "text": "The case of hard graph functional shifts is treated similarly. ", "page_idx": 21}, {"type": "text", "text": "D.2.2 Node-Level Universal Approximation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The above analysis motivates the following construction of a metric on $\\mathbb{R}^{N}$ . Given the filter bank $\\{g_{j}\\}_{j=1}^{K+1}$ , on graph with $d$ -dimensional signals, we define the standard Euclidean metric $\\mathrm{dist_{E}}$ in the spectral domain $\\mathbb{R}^{(K+1)\\times d}$ . We pull back the Euclidean metric to the spatial domain to define a graph-signal metric. Namely, for two signals $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ , their distance is defined to be ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{dist}_{\\Delta}\\left(\\mathbf{X},\\mathbf{X}^{\\prime}\\right):=\\operatorname{dist}_{\\mathrm{E}}\\left(\\boldsymbol{A}(\\Delta,\\mathbf{X}),\\boldsymbol{\\mathcal{A}}(\\Delta^{\\prime},\\mathbf{X}^{\\prime})\\right)\\!.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This defines a pseudometric on the space of graph-signals. To obtain a metric, we consider each equivalence class of signals with zero distance as a single point. Namely, we define the space $\\dot{\\mathcal{N}}_{\\Delta}:=\\mathbb{R}^{N}/\\mathrm{dist}_{\\Delta}$ to be the space of signals with 1D features modulo dist. In $\\mathbf{N}_{\\Delta}$ , the pseudometric dist becomes a metric, and $A_{\\Delta}/\\mathrm{dist}_{\\Delta}$ an isometry of metric spaces2. ", "page_idx": 22}, {"type": "text", "text": "Now, since MLPs $\\Psi$ can approximate any continuous function $\\mathbb{R}^{(K+1)\\times d}\\rightarrow\\mathbb{R}^{(K+1)\\times d^{\\prime}}$ (by the universal approximation theorem), and by the fact that $A_{\\Delta}/\\mathrm{dist}_{\\Delta}$ is an isometry, node-level NLSFs based on MLPs in the spectral domain can approximate any continuous function from signals with $d$ features to signal with $d^{\\prime}$ features $(\\mathcal{N}_{\\pmb{\\Delta}})^{d}\\rightarrow(\\mathcal{N}_{\\pmb{\\Delta}})^{d^{\\prime}}$ . ", "page_idx": 22}, {"type": "text", "text": "D.2.3 Graph-Level Universal approximation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The above analysis also motivates the construction of a graph-signal metric for graph-level tasks. For graphs with $d$ -channel signals, we define the standard Euclidean metric $\\mathrm{{dist}_{E}}$ in the spectral domain $\\mathbb{R}^{(K+1)\\times d}$ . We pull back the Euclidean metric to the spatial domain to define a graph-signal metric. Namely, for two graphs with Laplacians and signals $(\\Delta,\\mathbf{X})$ and $(\\Delta^{\\prime},\\mathbf{X}^{\\prime})$ , their distance is defined to be ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{dist}\\bigl((\\Delta,\\mathbf{X}),(\\Delta^{\\prime},\\mathbf{X}^{\\prime})\\bigr):=\\operatorname{dist}_{\\mathrm{E}}\\bigl(A(\\Delta,\\mathbf{X}),A(\\Delta^{\\prime},\\mathbf{X}^{\\prime})\\bigr)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This defines a pseudometric on the space of graph-signals. To obtain a metric, we consider equivalence classes of graph-signals with zero distance as a single point. Namely, we define the space $\\mathcal{G}$ to be the space of graph-signal modulo dist. In $\\mathcal{G}$ , the function dist becomes a metric, and $\\boldsymbol{\\mathcal{A}}$ an isometry. ", "page_idx": 22}, {"type": "text", "text": "By the isometry property, $\\mathcal{G}$ inherits any approximation property from $\\mathbb{R}^{(K+1)\\times d}$ . For example, since MLPs can approximate any continuous function $\\mathbb{R}^{(K+1)\\tilde{\\times}d}\\stackrel{\\cdot}{\\rightarrow}\\mathbb{R}^{d^{\\prime}}$ , the space of NLSFs based on MLPs $\\Psi$ has a universality property: any continuous function $\\mathcal{G}\\rightarrow\\mathbb{R}^{d^{\\prime}}$ with respect to dist can be approximated by a NLSF based on an MLP. ", "page_idx": 22}, {"type": "text", "text": "D.2.4 Graph-Level Expressivity of Pooling-NLSFs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We now show that pooling-NLSF are more expressive than graph-level NLSF if the norm in Eq. (4) and Eq. (5) is $L_{p}$ with $p\\neq2$ . ", "page_idx": 22}, {"type": "text", "text": "First, we show that there are graph-signals that graph-level-NLSFs cannot separate and PoolingNLSFs can. Consider an index NLSF with norm $L_{1}$ normalize by $1/N$ . Namely, for $\\mathbf{a}\\in\\mathbb{R}^{N}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{a}\\right\\|_{1}=\\frac{1}{N}\\sum_{n=1}^{N}\\left|a_{n}\\right|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The general case is similar. ", "page_idx": 22}, {"type": "text", "text": "For the two graphs, take the graph Laplacian ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{r r}{{1}}&{{-1}}\\\\ {{-1}}&{{1}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with eigenvalues $0,1$ , and corresponding eigenvectors $(1,1)$ and $(1,-1)$ . Take the graph Laplacian ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{r r r}{{1}}&{{-1}}&{{0}}\\\\ {{-1}}&{{2}}&{{-1}}\\\\ {{0}}&{{-1}}&{{1}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with eigenvalues $0,1,3$ . The first two eigenvectors are $(1,1,1)$ and $(1,0,-1)$ respectively. ", "page_idx": 22}, {"type": "text", "text": "Consider an Index-NLSF based on two eigenprojections $\\mathbf{P}_{1},\\mathbf{P}_{2}$ . As the signal of the first graph take $(1,1)+(1,-1)$ , and for the second graph take $(1,1,1)+\\textstyle{\\frac{3}{2}}(1,0,-1)$ . Both graph-signals have the same spectral coefficients $(1,1)$ , so graph-level NLSF cannot separate them. Suppose that the NLSF is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{val}}(\\mathbf{\\DeltaA},\\mathbf{X})=(\\|\\mathbf{P}_{1}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e)\\frac{\\mathbf{P}_{1}\\mathbf{X}}{\\|\\mathbf{P}_{1}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}\\ +\\ (\\|\\mathbf{P}_{2}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e)\\frac{\\mathbf{P}_{2}\\mathbf{X}}{\\|\\mathbf{P}_{2}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The outcome of the corresponding Pooling NLSF on the two graphs is ", "page_idx": 23}, {"type": "equation", "text": "$$\n1=\\|(1+1,1-1)\\|_{1}\\neq\\|(1+3/2,1,1-3/2)\\|_{1}=\\frac{4}{3}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, Pooling-NLSFs separate these inputs, while graph-level-NLSFs do not. ", "page_idx": 23}, {"type": "text", "text": "Next, we show that Pooling-NLSFs are at least as expressive as graph-level NLSFs. In particular, any graph-level NLSF can be expressed as a pooling NLSF. ", "page_idx": 23}, {"type": "text", "text": "Let $\\Psi$ be a graph-level NLSF. Define the node-level NLSF that chooses one spectral index $j$ with a nonzero value (e.g., the band with the largest coefficient) and projects upon it the value $\\Psi(\\Delta,\\mathbf{X})(\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e\\widetilde{)}/\\left\\|\\mathbf{P}_{j}X\\right\\|_{\\mathrm{sig}}$ . Hence, before pooling, the NLSF gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Theta(\\Delta,\\mathbf{X})=\\Psi(\\Delta,\\mathbf{X})(\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e)\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}/\\left\\|\\mathbf{P}_{j}X\\right\\|_{\\mathrm{sig}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $j$ depends on the spectral coefficients (e.g., it is the index of the largest spectral coefficient). Hence, after pooling, the Pooling NLSF returns $\\Psi(\\Delta,\\mathbf{X})$ , which coincides with the output of the graph-level NLSF. ", "page_idx": 23}, {"type": "text", "text": "Now, let us show that for $p=2$ , they have the same expressivity. For any NLSF, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Theta(\\Delta,\\mathbf{X})=\\left\\|\\displaystyle\\sum_{j=1}^{J}\\left[\\Psi\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)_{i=1}^{J}\\right]_{j}\\;\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}\\right\\|_{\\mathrm{sig}}^{2}}}\\\\ &{}&{=\\displaystyle\\sum_{j=1}^{J}\\left[\\Psi\\left(\\left\\|\\mathbf{P}_{i}\\mathbf{X}\\right\\|_{\\mathrm{sig}}\\right)_{i=1}^{J}\\right]_{j}^{2}\\;\\frac{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{2}}{\\left\\|\\mathbf{P}_{j}\\mathbf{X}\\right\\|_{\\mathrm{sig}}^{a}+e}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is a generic fully spectral NLSF. ", "page_idx": 23}, {"type": "text", "text": "D.2.5 Discussion on Graph-Level Expressivity of NLSFs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We offer additional discussion on the expressivity of graph-level NLSFs. Note that graph-level NLSFs are bounded by the expressive power of MPNNs with random positional encodings. For example, in [81], the authors showed that random features improve GNN expressivity, distinguishing certain structures that deterministic GNNs cannot. The Lanczos algorithm for computing the leading $J$ eigenvectors can be viewed as a message-passing algorithm with a randomly initialized $J$ -channel signal. ", "page_idx": 23}, {"type": "text", "text": "The example in App. D.2.4 can be written as a standard linear graph spectral filter, followed by the pooling (readout) function. This shows that graph-level NLSFs (without synthesis and pooling) are not more expressive than standard spectral GNNs with pooling. In the other direction, there are no graph-signals that can be separated by a graph-level NLSF but not by a spectral GNN. Given two graph-signals that an NLSF can separate, we can build a specific standard spectral GNN that gives the same output as the NLSF specifically for the two given graph-signals. However, several considerations should be noted: ", "page_idx": 23}, {"type": "text", "text": "1. the feature dimension of this GNN would is as large as the combined number of eigenvalues of the two graphs times the number of features,   \n2. this GNN is designed to fit these two specific graphs, and will not work for other graphs, and,   \n3. if implemented via polynomial fliters, the polynomial order would have to be the combined number of eigenvalues of the two graphs, which makes it very unstable. ", "page_idx": 23}, {"type": "text", "text": "Let us explain the architecture next. Given the two graphs $(G_{1},f_{1})$ with $N_{1}$ vertices and $\\left(G_{2},f_{2}\\right)$ with $N_{2}$ vertices, with node features of dimension $D$ , define band-pass filters that separate the combined set of eigenvalues of the two given graphs. Concatenate these filters to build a single multi-channel filter. Namely, this filter maps the signal to the sequence of band-pass filtered signal \u2013 each band at a different channel, for a total of $D(N_{1}+N_{2})$ channels. If the band-pass filters are based on polynomials, each polynomial would be chosen to be zero on all eigenvalues except for one. Apply $L_{2}$ -norm pooling on each channel to obtain a single feature of dimension $D(N_{1}+N_{2})$ . This gives the sequence of norms of the signal at the bands, where, for the two given graphs, the two pooled signals of the two graphs are supported on disjoint sets of indices. Hence, the linear spectral fliter contains the frequency information of the NLSF. Then, one can apply the MLP of the NLSF to the channels corresponding to the first graph, and to the channels corresponding to the second graph. This means that the linear spectral GNN gives the exact same outputs on $(G_{1},f_{1})$ and $\\left(G_{2},f_{2}\\right)$ as the NLSF. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Regarding pooling-NLSF, we do not offer an answer to the question whether standard spectral GNNs are more powerful (assuming an unlimited budget) than pooling-NLSFs, or vice-versa. More practically meaningful questions would be: ", "page_idx": 24}, {"type": "text", "text": "1. Compare the expressivity of standard spectral GNNs to NLSFs for a given budget of parameters.   \n2. How many graph-signal can a single spectral GNN separate, vs a single NLSF, of the same budget. ", "page_idx": 24}, {"type": "text", "text": "We leave these questions as open problems for future research. We note that, in practice, NLSFs with the same budget as standard spectral GNNs perform better. ", "page_idx": 24}, {"type": "text", "text": "D.3 Stable Invertibility of Synthesis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We present the analysis for diagonal synthesis defined from App. C.1. In the fixed graph setting, given any 1D signal $\\mathbf{X}$ such that $g_{j}(\\Delta)\\mathbf{X}\\neq0$ for every $j\\in[K+1]$ , we show that the synthesis operator Sv\u2206al,, Xdi is stably invertible. ", "page_idx": 24}, {"type": "text", "text": "Since we consider 1D signal $\\mathbf{X}$ , the diagonal synthesis in fliter bank case in Eq. (10) can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\nS_{\\Delta,\\mathbf{X}}^{\\mathrm{val,\\,diag}}=\\mathbf{H}\\mathbf{r}^{\\prime}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{R}^{N\\times(K+1)}\\ni\\mathbf{H}=\\left[\\frac{g_{1}(\\pmb{\\Delta})\\mathbf{X}}{\\lVert g_{1}(\\pmb{\\Delta})\\mathbf{X}\\rVert_{2}^{a}+e},\\dots,\\frac{g_{K+1}(\\pmb{\\Delta})\\mathbf{X}}{\\lVert g_{K+1}(\\pmb{\\Delta})\\mathbf{X}\\rVert_{2}^{a}+e}\\right],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $(g_{j_{1}}(\\Delta)\\mathbf{X})^{\\top}(g_{j_{2}}(\\Delta)\\mathbf{X})=0$ for all $j_{1},j_{2}\\in[K+1]$ , the matrix $\\mathbf{H}^{\\top}\\mathbf{H}$ is a diagonal matrix with entries $\\begin{array}{r}{\\left\\|\\frac{g_{1}(\\Delta)\\mathbf{X}}{\\|g_{1}(\\Delta)\\mathbf{X}\\|_{2}^{a}+e}\\right\\|_{2}^{2}}\\end{array}$ . Therefore, the singular values of $\\mathbf{H}$ are ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma_{j}=\\frac{\\left\\|g_{1}(\\pm)\\mathbf{X}\\right\\|_{2}}{\\left\\|g_{1}(\\pm)\\mathbf{X}\\right\\|_{2}^{a}+e}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the right singular vectors are the standard basis elements ${\\bf{e}}_{j}$ in $\\mathbb{R}^{K+1}$ , and the left singular vectors are ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{g_{j}(\\pmb{\\Delta})\\mathbf{X}}{\\|g_{j}(\\pmb{\\Delta})\\mathbf{X}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $j\\in[K+1]$ . Hence, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|S_{\\pmb{\\Delta},\\mathbf{X}}^{\\mathrm{val,\\,diag}}\\right\\|_{2}=\\operatorname*{max}_{j}\\sigma_{j}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\left(S_{\\Delta,\\mathbf{X}}^{\\mathrm{val,\\,diag}}\\right)^{-1}\\right\\|_{2}=\\frac{1}{\\operatorname*{min}_{j}\\sigma_{j}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Suppose that $a=1$ and $e=0$ . In this case, Sv\u2206al,, Xdiagis an isometry from the spectral domain to a subspace of the signal space $\\mathbb{R}^{N}$ . Analysis is the adjoint of synthesis. This analysis can be extended to the index parametrization case for diagonal synthesis, and to higher dimensional signals. ", "page_idx": 24}, {"type": "text", "text": "D.4 Uniform Approximation of Graphs with $J$ Eigenvectors ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we develop the setting under which the low-rank approximation of GSOs with their leading eigenvectors can be interpreted as a uniform approximation (Sec. 4.4). ", "page_idx": 24}, {"type": "text", "text": "D.4.1 Cut Norm ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The cut norm of ${\\bf M}\\in\\mathbb{R}^{N\\times N}$ is defined to be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{M}\\right\\|_{\\Omega}:=\\frac{1}{N^{2}}\\operatorname*{sup}_{S,T\\subset[N]}\\bigg|\\sum_{i\\in S}\\sum_{j\\in T}m_{i,j}\\bigg|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The distance between two matrices in cut norm is defined to be $\\|\\mathbf{M}-\\mathbf{M}^{\\prime}\\|_{\\Omega}$ . ", "page_idx": 25}, {"type": "text", "text": "The cut norm has the following interpretation, which has precise formulation in terms of the weak regularity lemma [32, 65]. Any pair of (deterministic) graphs are close to each other in cut norm if and only if they can be described as pseudo-random graphs sampled from the same stochastic block model. Hence, the cut norm is a meaningful notion of graph similarity for practical graph machine learning, where graphs are noisy and can represent the same underlying phenomenon even if they have different sizes and topologies. In addition, the distance between non-isomorphic graphons is always positive in cut norm [66]. In this context, the work in [59] showed that GNNs with normalized sum aggregation cannot separate graphs that have zero distance in the cut norm. This means that the cut norm is sufficiently discriminative for practical machine learning on graphs. ", "page_idx": 25}, {"type": "text", "text": "D.4.2 The Constructive Weak Regularity Lemma in Hilbert Spaces ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The following lemma, called the constructive weak regularity lemma in Hilbert spaces, was proven in [29]. It is an extension of the classical respective result from [65]. ", "page_idx": 25}, {"type": "text", "text": "We define the Frobenius norm normalized by $1/N$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Vert\\mathbf{M}\\Vert_{\\mathrm{F}}=\\sqrt{\\frac{1}{N}\\sum_{i,j=1}^{N}\\left|m_{i,j}\\right|^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma D.3. [[29]] Let $\\{\\kappa_{j}\\}_{j\\in\\mathbb{N}}$ be a sequence of nonempty subsets of a real Hilbert space $\\mathcal{H}$ and let $\\delta\\geq0$ . Let $J>0$ , let $R\\geq1$ such that $J/R\\in\\mathbb{N}$ , and let $g\\in\\mathcal H$ . Let m be randomly uniformly sampled from $[J]$ . Then, in probability $\\textstyle1-{\\frac{1}{R}}$ (with respect to the choice of $m$ ), any vector of the form ", "page_idx": 25}, {"type": "equation", "text": "$$\ng^{*}=\\sum_{j=1}^{m}\\gamma_{j}\\,f_{j}\\,\\quad\\,s u c h\\,t h a t\\,\\quad\\,\\gamma=(\\gamma_{j})_{j=1}^{m}\\in\\mathbb R^{m}\\quad a n d\\quad\\mathbf f=(f_{j})_{j=1}^{m}\\in K_{1}\\times\\ldots\\times K_{m}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "that gives a close-to-best Hilbert space approximation of g in the sense that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|g-g^{*}\\|\\leq(1+\\delta)\\operatorname*{inf}_{\\gamma,\\mathbf{f}}\\,\\|g-\\sum_{i=1}^{m}\\gamma_{i}f_{i}\\|,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the infimum is over $\\gamma\\in\\mathbb{R}^{m}$ and $\\mathbf{f}\\in K_{1}\\times\\ldots\\times K_{m},$ , also satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{K}_{m+1},\\quad|\\langle w,g-g^{*}\\rangle|\\leq\\|w\\|\\,\\|g\\|\\,\\sqrt{\\frac{R}{J}+\\delta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.4.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 1. Let M be a symmetric matrix with entries bounded by $|m_{i,j}|\\leq\\alpha$ , and let $J\\in\\mathbb{N}$ Suppose m is sampled uniformly from $[J]$ , and let $R\\geq1$ s.t. $J/R\\in\\mathbb{N}$ . Consider $\\phi_{1},...,\\phi_{m}$ as the leading eigenvectors of M, with eigenvalues $\\mu_{1},\\ldots,\\mu_{m}$ ordered by their magnitudes $|\\mu_{1}|\\geq\\ldots\\geq$ $|\\mu_{m}|$ . Define $\\begin{array}{r}{\\mathbf{C}=\\sum_{k=1}^{m}\\mu_{k}\\boldsymbol{\\phi}_{k}\\boldsymbol{\\phi}_{k}^{\\intercal}}\\end{array}$ . Then, with probability $\\textstyle1-{\\frac{1}{R}}$ (w.r.t. the choice of $m$ ), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{M}-\\mathbf{C}\\rVert_{\\Omega}<\\frac{3\\alpha}{2}\\sqrt{\\frac{R}{J}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let us use Lemma D.3, with $\\mathcal{H}\\,=\\,\\mathbb{R}^{N\\times N}$ , and $\\kappa_{j}\\,=\\,\\kappa$ the set of symmetric rank one matrices of the form $\\mathbf{v}\\mathbf{v}^{\\top}$ where $\\mathbf{v}\\,\\in\\,\\mathbb{R}^{N}$ is a column vector. Denote by $y_{m}$ the space of linear combinations of $m$ elements of $\\kappa$ , which is the space of symmetric matrices of rank bounded by $m$ . For the Hilbert space norm, we take the Frobenius norm. In the setting of the lemma, we take $\\boldsymbol{g}=\\mathbf{M}$ , and $g^{\\ast}\\in\\mathcal{V}_{m}$ , and $\\delta=0$ . By the lemma, with probability $1-1/R$ , any Frobenius minimizer $\\mathbf{C}$ , namely, that satisfies $\\begin{array}{r}{\\|\\mathbf{M}-\\mathbf{C}\\|_{\\mathrm{F}}=\\operatorname*{min}_{\\mathbf{C^{\\prime}}\\in\\mathcal{D}_{m}}\\|\\mathbf{M}-\\mathbf{C^{\\prime}}\\|_{\\mathrm{F}}}\\end{array}$ , also satisfies ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\langle{\\bf Y},{\\bf M}-{\\bf C}\\right\\rangle\\le\\left\\|{\\bf Y}\\right\\|_{\\mathrm{F}}\\left\\|{\\bf M}\\right\\|_{\\mathrm{F}}\\sqrt{\\frac{R}{J}}\\le\\alpha\\left\\|{\\bf Y}\\right\\|_{\\mathrm{F}}\\sqrt{\\frac{R}{J}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for every $\\mathbf{Y}\\in\\mathcal{Y}_{m}$ . Hence, for every choice of subset $S,T\\subset[N]$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\left|\\displaystyle\\sum_{|\\epsilon>\\delta/\\epsilon>T}(m_{i,j}-c_{i,j})\\right|}\\\\ &{=\\displaystyle\\frac{1}{2}\\left|\\displaystyle\\sum_{|\\epsilon>\\delta/\\epsilon>T}(m_{i,j}-c_{i,j})+\\sum_{i\\in T}\\sum_{j\\in S}(m_{i,j}-c_{i,j})\\right|}\\\\ &{=\\displaystyle\\frac{1}{2}\\left|\\displaystyle\\sum_{|\\epsilon<\\delta/T}\\sum_{j\\in S\\cup T}(m_{i,j}-c_{i,j})-\\sum_{i\\in S}\\sum_{\\ell\\in S}(m_{i,j}-c_{i,j})-\\sum_{i\\in T}\\sum_{j\\in T}(m_{i,j}-c_{i,j})\\right|}\\\\ &{\\le\\left\\|\\displaystyle\\frac{1}{2}\\left\\langle\\mathbf{1}_{S\\cup T}\\mathbf{1}_{S\\cup T}^{\\top},\\mathbf{M}-\\mathbf{C}\\right\\rangle\\right\\|_{\\mathrm{F}}+\\left\\|\\frac{1}{2}\\left\\langle\\mathbf{1}_{S}\\mathbf{1}_{S}^{\\top},\\mathbf{M}-\\mathbf{C}\\right\\rangle\\right\\|_{\\mathrm{F}}+\\left\\|\\frac{1}{2}\\left\\langle\\mathbf{1}_{T}\\mathbf{1}_{T}^{\\top},\\mathbf{M}-\\mathbf{C}\\right\\rangle\\right\\|_{\\mathrm{F}}}\\\\ &{~~<\\displaystyle\\frac{3\\alpha_{i}}{2}\\sqrt{\\frac{R}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where for a set $S\\subset[N]$ , denote by $\\mathbb{1}_{S}\\in\\mathbb{R}^{N}$ the column vector with 1 at coordinates in $S$ and 0 otherwise. ", "page_idx": 26}, {"type": "text", "text": "Hence, we also have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{M}-\\mathbf{C}\\rVert_{\\Omega}\\leq\\frac{3\\alpha}{2}\\sqrt{\\frac{R}{J}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lastly, note that by the best rank- $^m$ approximation theorem (Eckart\u2013Young\u2013Mirsky Theorem [92, Thm. 5.9]), any Frobenius minimizer $\\mathbf{C}$ is the projection upon the $m$ leading eigenvectors of M (or some choice of these eigenvectors in case of multiplicity higher than 1). ", "page_idx": 26}, {"type": "text", "text": "Now, one can use the adjacency matrix A as $\\mathbf{M}$ in Thm. 1. When working with sparse matrices of $E\\ll N^{2}$ edges, to achieve a meaningful scaling of cut distance, we re-normalize the cut norm and define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{M}\\right\\|_{\\square}^{(E)}=\\frac{N^{2}}{E}\\left\\|\\mathbf{M}\\right\\|_{\\square}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With this norm, Thm. 1 gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mathbf{M}-\\mathbf{C}\\|_{\\Omega}^{(E)}<\\frac{3\\alpha N^{2}}{2E}\\sqrt{\\frac{R}{J}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "While this bound is not uniformly small in $N$ , it is still independent of $\\mathbf{M}$ . In contrast, the error bounds for spectral and Frobenius norms do depend on the specific properties of $\\mathbf{M}$ . ", "page_idx": 26}, {"type": "text", "text": "Now, if we want to apply Thm. 1 to other GSOs $\\Delta$ , we need to make some assumptions. Note that when the GSO is a Laplacian, we take as the leading eigenvectors the ones correspoding to the smallest eigenvalues, not the largest ones. To make the theorem applicable, we need to reorder the eigenvectors of $\\Delta$ . This can be achieved by applying a decreasing function $h$ to $\\Delta$ , such as $h(\\Delta)\\ =\\Delta^{-1}$ . The role of $h$ is to amplify the eigenspaces of $\\Delta$ in which most of the energy of signals interest (the ones that often appear in the dataset) is concentrated. Under the assumption that $h(\\Delta)$ has entries bounded by some not-too-high $\\alpha>0$ , one can now justify approximating GSOs by low-rank approximations based on the smallest eigenvectors. ", "page_idx": 26}, {"type": "text", "text": "D.5 NLSFs on Random Geometric Graphs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we consider the $L_{2}[N]$ norm normalized by $1/N^{1/2}$ , namely, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathbf{v}\\|_{2}={\\sqrt{{\\frac{1}{N}}\\sum_{n=1}^{N}|v_{n}|^{2}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We follow a similar analysis to [74], mostly skipping the rigorous Monte-Carlo error rate proofs, as these are standard. ", "page_idx": 27}, {"type": "text", "text": "Let $\\boldsymbol{S}$ be a compact metric space with metric $\\mathrm{d}$ and with a Borel probability measure, that we formally call the standard measure $d x$ . Let $r\\,>\\,0$ , and denote by $B_{r}(x)$ the ball of radius $r$ about $x\\in S$ . Let $V(x)$ be the volume of $B_{r}(x)$ with respect to the standard measure (note that $V(x)$ need not be constant). We consider an underlying Laplacian on the space $\\boldsymbol{S}$ defined on signals $f:S\\rightarrow\\mathbb{R}$ by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathscr{L}f(x)=C\\int_{B_{r}(x)}(f(x)-f(y))d y,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we assume $C=1$ in the analysis below without loss of generality. In case the integral in Eq. (22) is normalized by $1/r^{2}V(x)$ , this Laplacian was called $\\rho$ -Laplacian in [15], which we denote by ${\\mathcal{L}}_{r}$ . Such a Laplacian is related to Korevaar-Schoen type energies [52], in which the limit case of the radius $r$ going to 0 is considered. It was shown in [15] that the $\\rho$ -Laplacian is self-adjoint with spectrum supported inside some interval $[0,Q]$ , for some $Q>0$ , where for some $0<R<Q$ , the part of the spectrum in $[0,R)\\cup(R,Q]$ is discrete (consisting of isolated eigenvalues with finite multiplicities). The intuition behind this result is that, for the $\\rho$ -Laplacian $\\mathcal{L}_{r}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}f=\\frac{1}{r^{2}}f-\\frac{1}{V(x)r^{2}}\\int_{B_{r}(x)}f(y)d y.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first term in the right-hand-side of Eq. (23) is a scaled version of the identity operator, and the second term is a compact self-adjoint integral operator, and hence has a discrete spectrum with accumulation point at 0, or no accumulation point. After showing that the sum of these two operators is self-adjoint, we end up with only one accumulation point of the spectrum of ${\\mathcal{L}}_{r}$ . ", "page_idx": 27}, {"type": "text", "text": "We show below that $\\mathcal{L}$ is self-adjoint under the assumption that $V(x)$ is bounded from above. In this case it must also be positive semi-definite (the proof is equivalent to the positivity of the combinatorial graph Laplacian). Consider the decomposition ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}f(x)=C V(x)f(x)-C\\int_{B_{r}(x)}f(y)d y.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The second term of Eq. (24) is a compact integral operator, and hence only has an accumulation point at 0, or has no accumulation point. The first term is a multiplication operator by the real-valued function $V(x)$ , so it is self-adjoint and its spectrum is the closure of $\\{V({\\bar{x}})\\mid x\\in{\\bar{S}}\\}$ . We suppose that $V(x)$ is bounded from below by some $R^{\\prime}>0$ and also bounded from above. This shows that $\\mathcal{L}$ is bounded and self-adjoint (as a sum of two bounded self-adjoint operators). Under these assumptions, it is reasonable to further assume that the spectrum of $\\mathcal{L}$ in the interval $[0,R)$ , for some $R>0$ , is discrete, with accumulation point at $R$ . This happens, for example, if $V(x)=V$ is constant. ", "page_idx": 27}, {"type": "text", "text": "We now assume that the signal $f$ consists only of frequencies in $[0,R)$ . This means that we can project $\\mathcal{L}$ upon this part of the spectrum, giving a self-adjoint operator with discrete spectrum, and accumulation point of the eigenvalues only at $R$ . ", "page_idx": 27}, {"type": "text", "text": "Let $w\\,:\\,S\\,\\rightarrow\\,\\mathbb{R}$ be a measurable weight function with $\\begin{array}{r}{\\int_{S}w(x)d x\\,=\\,1}\\end{array}$ . Suppose that $w(x)$ is continuous and varies slowly over $\\boldsymbol{S}$ . Namely, we assume that $w(x)\\approx w(y)$ for every $y\\in B_{r}(x)$ . To generate a random geomertic graph of $N$ nodes, we sample $N$ points $\\mathbf{x}=\\{x_{n}\\}_{n=1}^{N}$ independently from the weighted probability measure $w(x)d x$ . We connect node $x_{j}$ to $x_{j}$ by an edge if and only if $\\mathrm{d}(x_{i},x_{j})\\,\\stackrel{!}{\\leq}\\,r$ to obtain the adjacency matrix A. Let $\\mathbf{L}$ be the combinatorial Laplacian with respect to A, and $\\mathbf{N}$ the normalized symmetric Laplacian. The number of samples inside the ball of radius $r$ around $x$ is approximately $w(x)V(x)N$ . Hence, the degree of the node $x_{n}$ is approximately $w(x_{n})V(x_{n})N$ . ", "page_idx": 27}, {"type": "text", "text": "We consider the leading $J$ eigenvectors the ones corresponding to the smallest eigenvalues of $\\mathcal{L}$ in the interval $[0,R)$ , where $J\\ll N$ . We order the eigenvalues in increasing order. Let $\\mathcal{P}\\mathcal{W}(J)$ be the space spanned by the $J$ leading eigenvectors of $\\mathcal{L}$ , called the Paley-Wiener space of $\\mathcal{L}$ . Let $p_{J}$ be the projection upon the Paley-Wiener space of $\\mathcal{L}$ . Let $R_{N}:L^{2}(S)\\to L^{2}[N]$ be the sampling operator, defined by $(\\bar{R}_{N}f)_{j}=f(x_{j})$ . ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Let $f:S\\to\\mathbb{R}$ be a bounded signal over the metric space and suppose that $f\\in\\mathcal{P}\\mathcal{W}(J)$ . Denote $\\mathbf{f}=(f(x_{n}))_{n=1}^{N}=R_{N}f$ . By Monte Carlo theory, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n(\\mathbf{L}\\mathbf{f})_{j}\\approx w(x_{j})N\\int_{B_{r}(x_{j})}(f(x)-f(y))d y=w(x_{j})N\\mathcal{L}f(x_{j}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So, in case the sampling is uniform. i.e., $w(x)=1$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{L}\\approx N{\\mathcal{L}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "pointwise. To make this precise, let us recall Hoeffding\u2019s Inequality [43]. ", "page_idx": 28}, {"type": "text", "text": "Theorem D.1 (Hoeffding\u2019s Inequality [43]). Let $Y_{1},\\ldots,Y_{N}$ be independent random variables such that $a\\leq Y_{n}\\leq b$ almost surely. Then, for every $k>0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{N}\\sum_{n=1}^{N}(Y_{n}-\\mathbb{E}[Y_{n}])\\right|\\geq k\\right)\\leq2\\exp\\left(-\\frac{2k^{2}N}{(b-a)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using Hoeffding\u2019s Inequality, one can now show that there is an event of probability more than $1-p$ in which for every $j\\in[N]$ , the error between $\\begin{array}{r}{\\big(\\frac{1}{N}{\\bf L}R_{N}f\\big)_{j}}\\end{array}$ and $\\mathcal{L}f$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|R_{N}\\mathcal{L}f-\\frac{1}{N}\\mathbf{L}R_{N}f\\right\\|=\\mathcal{O}\\left(J\\sqrt{\\frac{\\log(1/p)+\\log(N)+\\log(2)}{N}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The fact that different graphs of different sizes $N$ approximate $\\mathcal{L}$ with different scaling factors means that $\\mathbf{L}$ is not value transferable. Let us show that $\\mathbf{L}$ is index transferable. ", "page_idx": 28}, {"type": "text", "text": "We now evoke the transferability theorem \u2013 a slight reformulation of Section 3 of Theorem 4 in [60]. ", "page_idx": 28}, {"type": "text", "text": "Theorem D.2. Let $\\mathcal{L}$ be a compact operator on $L^{2}(S)$ and $\\mathbf{L}$ an operator on $\\mathbb{C}^{N}$ . Let $J\\in\\mathbb{N}$ and let $R_{N}$ be the sampling operator defined above. Let $\\mathcal{P}\\mathcal{W}(J)$ be the space spanned by the $J$ leading eigenvectors of $\\mathcal{L}$ , and let $f\\in\\mathcal{P}\\mathcal{W}(J)$ . Let $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ be Lipschitz with Lipschitz constant $D$ . Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|R_{N}g(\\mathcal{L})f-\\frac{1}{N}g(\\mathbf{L})R_{N}f\\right\\|\\leq D\\sqrt{J}\\left\\|R_{N}\\mathcal{L}f-\\frac{1}{N}\\mathbf{L}R_{N}f\\right\\|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For every leading eigenvalue $\\lambda_{j}$ of $\\mathcal{L}$ let $\\gamma_{i_{j}}$ be the leading eigenvalue of $\\mathcal{L}$ closest to $\\lambda$ , where, if there are repeated eigenvalues, $i_{j}$ is chosen arbitrarily from the eigenvalues of $\\mathbf{L}$ that best approximate $\\lambda_{i}$ . Let ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\delta=\\operatorname*{min}\\{\\alpha,\\beta\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\alpha=\\operatorname*{min}_{j\\in[J]}\\operatorname*{min}\\big\\{\\operatorname*{min}_{\\stackrel{i_{j}\\neq i\\in[N]}{\\jmath}}\\,|\\lambda_{j}-\\gamma_{i}|\\,\\,,\\,\\operatorname*{min}_{\\stackrel{j\\neq m\\in[J]}{\\jmath\\neq m\\in[J]}}\\,|\\lambda_{j}-\\lambda_{m}|\\,\\,,\\,\\operatorname*{min}_{\\stackrel{j\\neq m\\in[J]}{\\jmath\\neq m\\in[J]}}\\,\\big|\\gamma_{i_{j}}-\\lambda_{m}\\big|\\,\\,\\big\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}_{i\\in[J],n\\in[N]}\\left|\\gamma_{i}-\\gamma_{n}\\right|\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and suppose that $\\delta\\,>\\,0$ . For each $j~\\in~[N]$ there exists a Lipschitz continuous function $g_{j}$ with Lipschitz constant $\\delta^{-1}$ such that $g_{j}(\\lambda_{j})=g_{j}(\\gamma_{i_{j}})=1$ and $\\gamma$ is zero on all other leading eigenvalues of $\\mathcal{L}$ and all other eigenvalues of $\\mathbf{L}$ . Hence, ", "page_idx": 28}, {"type": "equation", "text": "$$\ng_{j}(\\mathcal{L})f=p_{j}^{\\mathcal{L}}f,\\quad g_{j}(\\mathbf{L})R_{N}f=p_{i_{j}}^{\\mathbf{L}}\\mathbf{f},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $p_{j}^{\\mathcal{L}}$ is the projection upon the space spanned by the $j$ -th eigenvector of $\\mathcal{L}$ , and $p_{i_{j}}^{\\mathbf{L}}$ is the projection upon the space spanned by the $i_{j}$ -th eigenvector of $\\mathbf{L}$ . ", "page_idx": 28}, {"type": "text", "text": "Now, by the transferability theorem, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|{R_{N}p_{j}^{\\mathcal{L}}f-p_{i_{j}}^{\\mathbf{L}}\\mathbf{f}}\\right\\|\\leq D\\sqrt{J}\\left\\|{R_{N}\\mathcal{L}f-\\frac{1}{N}\\mathbf{L}}\\mathbf{f}\\right\\|=\\mathcal{O}(\\sqrt{J/N}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By induction over $j$ , with base $j=1$ , we must have $i_{j}=j$ for every $j\\in[J]$ . ", "page_idx": 28}, {"type": "text", "text": "Now, note that by standard Monte Carlo theory (evoking Hoeffding\u2019s inequality again and intersecting events), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\left\\|R_{N}p_{j}^{\\mathcal{L}}f\\right\\|_{2}^{2}-\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}^{2}\\right|=\\mathcal{O}(J^{-1/2})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "table", "img_path": "y8P633E5HQ/tmp/2c3a527c4492b468eb469ab8fc6a5acd22db788884afe2c63a5fd4840e8f4bb0.jpg", "table_caption": ["Table 5: Node classification datasets statistics. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "in high probability. Hence, by the fact that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|R_{N}p_{j}^{\\mathcal{L}}f\\right\\|_{2}^{2}-\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}^{2}=\\left(\\left\\|R_{N}p_{j}^{\\mathcal{L}}f\\right\\|_{2}-\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}\\right)\\left(\\left\\|R_{N}p_{j}^{\\mathcal{L}}f\\right\\|_{2}+\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $\\left\\|{R_{N}p_{j}^{\\mathcal{L}}f}\\right\\|_{2}+\\left\\|{p_{j}^{\\mathcal{L}}f}\\right\\|_{2}$ is bounded from below by the constant $\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\left\\|R_{N}p_{j}^{\\mathcal{L}}f\\right\\|_{2}-\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}\\right|=\\mathcal{O}(J^{-1/2}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This shows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|p_{j}^{\\mathcal{L}}f\\right\\|_{2}\\approx\\left\\|p_{j}^{\\mathbf{L}}\\mathbf{f}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which shows index transferability. Namely, for two graphs of $N$ and $N^{\\prime}$ nodes sampled from $\\boldsymbol{S}$ , with corresponding Laplacians $\\mathbf{L}$ and $\\mathbf{L}^{\\prime}$ , by the triangle inequality, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|p_{j}^{\\mathbf{L}}\\mathbf{f}\\right\\|_{2}\\approx\\left\\|p_{j}^{\\mathbf{L}^{\\prime}}\\mathbf{f}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, we show value transferability for $\\mathbf{N}$ . Here, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{N}f(x_{j})\\approx\\frac{1}{V(x_{j})}\\mathcal{L}f(x_{j}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, if $V(x)=V$ is constant, we have $\\mathbf{N}\\approx{\\mathcal{L}}$ up to a constant that does not depend on $N$ . In this case, by a similar analysis to the above, $\\mathbf{N}$ is value transferable. We note that $\\mathbf{N}$ is also index transferable, but value transferability is guaranteed in a more general case, where we need not assume a separable spectrum. ", "page_idx": 29}, {"type": "text", "text": "E Additional Details on Experimental Study ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We describe the experimental setups and additional details of our experiments in Sec. 5. The experiments are performed on NVIDIA DGX A100. ", "page_idx": 29}, {"type": "text", "text": "E.1 Semi-Supervised Node Classification ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We provide a detailed overview of the experimental settings for semi-supervised node classification tasks, along with the validated hyperparameters used in our benchmarks. ", "page_idx": 29}, {"type": "text", "text": "Datasets. We consider six datasets in node classification tasks, including Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor. The detailed statistics of the node classification benchmarks are summarized in Tab. 5. For datasets splitting on citation graphs (Cora, Citeseer, and Pubmed), we follow the standard splits from [100], using 20 nodes per class for training, 500 nodes for validation, and 1000 nodes for testing. For heterophilic graphs (Chameleon, Squirrel, and Actor), we adopt the sparse splitting method from [16, 41], allocating $2.5\\%$ of samples for training, $2.5\\%$ for validation, and $95\\%$ for testing. The classification quality is assessed by computing the average classification accuracy across 10 random splits, along with a $95\\%$ confidence interval. ", "page_idx": 29}, {"type": "text", "text": "Baselines. For GCN, GAT, SAGE, ChebNet, ARMA, and APPNP, we use the implementations from the PyTorch Geometric library [28]. For other baselines, we use the implementations released by the respective authors. ", "page_idx": 29}, {"type": "table", "img_path": "y8P633E5HQ/tmp/64c73868ae6beb9f60c3701efb0e7291d32a924bd820c019c71ed213407813ec.jpg", "table_caption": ["Table 6: Graph classification datasets statistics. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Hyperparameters Settings. The hidden dimension is set to be either 64 or 128 for all models and datasets. We implement our proposed Node-level NLSFs using PyTorch and optimize the model with the Adam optimizer [48]. To determine the optimal dropout probability, we search within the range $[0,0.9]$ in increments of 0.1. The learning rate is examined within the set $\\{1e^{-1},5e^{-2},1e^{-2},5e^{-3},1e^{-3}\\}.$ . We explore weight decay values within the set $\\{1e^{-2},5e^{-3}$ , 1e\u22123, $5e^{-4}$ , $1e^{-4}$ , $5e^{-5}$ , $1e^{-5}$ , $5e^{-6}$ , 1e\u22126, 0.0}. Furthermore, the number of layers is varied from 1 to 10. The number of leading eigenvectors $J$ in Index NLSFs is set within $\\lbrack1,1e^{2}\\rbrack$ . The decay rate in the Value NLSFs is determined using dyadic sampling within the set { 14, 13, 12, 23, 34}, the sampling resolution $S$ within $[1,1e^{2}]$ , and the number of the bands in Value NLSFs $K\\le S$ . For hyperparameter optimization, we conduct a grid search using Optuna [1] for each dataset. An early stopping criterion is employed during training, stopping the process if the validation loss does not decrease for 200 consecutive epochs. ", "page_idx": 30}, {"type": "text", "text": "E.2 Graph Classification ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We provide a comprehensive description of the experimental settings for graph classification tasks and the validated hyperparameters used in our benchmarks. The results are reported in the main paper. ", "page_idx": 30}, {"type": "text", "text": "Problem Setup. Consider a set of $Z$ graphs $\\mathcal{G}_{Z}=\\{G_{1},G_{2},\\ldots,G_{Z}\\}$ , where in each graph $G_{i}=$ $([N_{i}],\\mathcal{E}_{i},\\mathbf{A}_{i},\\mathbf{X}_{i})$ , we have $N_{i}$ nodes for each graph $G_{i}$ , $\\mathcal{E}_{i}$ represents the edge set, $\\mathbf{A}_{i}\\bar{\\in}\\,\\mathbf{\\bar{\\mathbb{R}}}^{N_{i}\\times N_{i}}$ denotes the edge weights, and $\\mathbf{X}_{i}\\in\\mathbb{R}^{N_{i}\\times d}$ represents the node feature matrix with $d$ -dimensional node attributes. Let $\\bar{\\mathbf{Y}}_{Z}\\in\\mathbb{R}^{Z\\times C}$ be the label matrix with $C$ classes such that $y_{i,j}=1$ if the graph $G_{i}$ belongs to the class $j$ , and $y_{i,j}=0$ otherwise. Given a set of $Z^{\\prime}$ graphs $\\mathcal{G}_{Z^{\\prime}}\\subset\\mathcal{G}$ , where $Z^{\\prime}<Z$ , with the label information $\\mathbf{Y}_{Z^{\\prime}}\\in\\mathbb{R}^{Z^{\\prime}\\times C}$ , our goal is to classify the set of unseen graph labels of $\\mathcal{G}_{Z}\\setminus\\mathcal{G}_{Z^{\\prime}}$ . ", "page_idx": 30}, {"type": "text", "text": "Datasets. We consider eight datasets [72] for graph classification tasks, including five bioinformatics: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, and three social network datasets: IMDBB, IMDB-M, and COLLAB. The detailed statistics of the graph classification benchmarks are summarized in Tab. 6. We use the random split from [94, 101, 67, 104], using $80\\%$ for training, $10\\%$ for validation, and $10\\%$ for testing. This process is repeated 10 times, and we report the average performance and standard deviation. ", "page_idx": 30}, {"type": "text", "text": "Baselines. For GCN, GAT, SAGE, ChebNet, ARMA, APPNP, and DiffPool, we use the implementations from the PyTorch Geometric library [28]. For other baselines, we use the implementations released by the respective authors. ", "page_idx": 30}, {"type": "text", "text": "Hyperparameters Settings. The dimension of node representations is set to 128 for all methods and datasets. We implement the proposed Pooling-NLSFs and Graph-level NLSFs using PyTorch and optimize the model with the Adam optimizer [48]. A readout function is applied to aggregate the node representations for each graph, utilizing mean, add, max, or RMS poolings. The learning rate and weight decay are searched within $\\{1e^{-1},1e^{-2},1e^{-3},1e^{-4},1e^{-5}\\}$ , the pooling ratio within [0.1, 0.9] with step 0.1, the number of layers within [1, 10] with step 1, the number of leading eigenvectors $J$ in Index NLSFs within $\\left[1,1e^{2}\\right]$ , the decay rate in the Value NLSFs using dyadic sampling within $\\textstyle\\left\\{{\\frac{1}{4}},{\\frac{1}{3}},{\\frac{1}{2}},{\\frac{2}{3}},{\\frac{3}{4}}\\right\\}$ , the sampling resolution $S$ within $[1,1e^{2}]$ , and the number of the bands in Value NLSFs within $K\\leq S$ . The graph-level representation is then fed into an MLP with a (log)softmax classifier, using a cross-entropy loss function for predictions over the labels. Specifically, the MLP consists of three fully connected layers with 256, 128, and 64 neurons, respectively, followed by a (log)softmax classifier. We conduct a grid search on the hyperparameters for each dataset using Optuna [1]. An early stopping criterion is employed during training, stopping the process if the validation loss does not decrease for 100 consecutive epochs. ", "page_idx": 30}, {"type": "table", "img_path": "y8P633E5HQ/tmp/2c05c5e3f1e19a3017dac0038a683a1bc39499a370d15098b3bb0585362f2dbd.jpg", "table_caption": ["Table 7: Semi-supervised node classification accuracy with random split, following the experimental protocol established by [41]. Results marked with \u2217are taken from [41]. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "F Additional Experimental Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Here, we present additional experiments on node and graph classification benchmarks, ablation studies, runtime analysis, and uniform sub-bands. ", "page_idx": 31}, {"type": "text", "text": "F.1 Semi-Supervised Node Classification Following [41] Protocol ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We present additional experimental results for semi-supervised node classification using random splits, adhering to the protocol established by [41]. The results, summarized in Tab. 7, demonstrate the classification accuracy across six benchmark datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor. Our att-Node-level NLSFs achieve the highest accuracy on four out of the six datasets, outperforming other models significantly. On Pubmed, it records a close second-highest accuracy, slightly behind APPNP. Our method achieves a competitive second place in the Actor dataset. att-Node-level NLSFs demonstrate substantial improvements, particularly in challenging datasets like Chameleon and Squirrel. The comparison models, including GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, and ARMA, varied in their effectiveness, with ChebNetII and APPNP also showing strong results on several datasets. These findings highlight the efficacy of the att-Node-level NLSFs in semi-supervised node classification tasks. ", "page_idx": 31}, {"type": "text", "text": "F.2 Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Following [77], we conduct additional experiments on the original and filtered Chameleon and Squirrel datasets in the dense split setting. We use the same random splits as in [77], dividing the datasets into $48\\%$ for training, $32\\%$ for validation, and $20\\%$ for testing. We compare the Nodelevel NLSFs using Laplacian attention with GCN [49], SAGE [39], GAT [93], GT [88], $\\mathrm{H_{2}G C N}$ [103], CPGNN [102], GPRGNN [16], FSGNN [69], GloGNN [62], FAGCN [8], GBKGNN [23], JacobiConv [96], and ResNet [40] with GNN models [77]. The study by [103] demonstrates the benefits of separating ego- and neighbor-embeddings in the GNN aggregation step when dealing with heterophily. Therefore, [77] also adopts this approach for the GNN aggregation step in GAT and GT models, denoted as \u201csep.\u201d The baseline results used for comparison are taken from [77]. Tab. 8 presents the full performance comparison on the original and filtered datasets. Note that Tab. 8 is the same as Tab. 2 but with more baseline methods. att-Node-level NLSFs achieve the highest accuracy on both the flitered Chameleon and Squirrel datasets. Additionally, att-Node-level NLSFs demonstrate strong performance on the original Chameleon dataset, achieving the highest ", "page_idx": 31}, {"type": "text", "text": "Table 8: Full Performance comparison of node classification on original and flitered Chameleon and Squirrel datasets in dense split setting. The baseline results used for comparison are taken from [77]. ", "page_idx": 32}, {"type": "table", "img_path": "y8P633E5HQ/tmp/41cd4f70b2cb160551f275209fd13b2e87bdd9236708582f47aa62ba1895fd95.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "y8P633E5HQ/tmp/df97247e74b855cf37f96d79b456a821b29d27e97be3c6e0b0e3f957f14f9ea3.jpg", "table_caption": ["Table 9: An ablation study investigated the effect of Node-level NLSFs on node classification, comparing the use of Index NLSFs and Value NLSFs. The symbol (\u2191) denotes an improvement using Laplacian attention. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "accuracy and the second-highest accuracy on the original Squirrel dataset. att-Node-level NLSFs show less sensitivity to node duplicates and exhibit stronger generalization ability, further validating the reliability of the Chameleon and Squirrel datasets in the dense split setting. ", "page_idx": 32}, {"type": "text", "text": "F.3 Ablation Study on att-Node-level NLSFs, att-Graph-NLSF, and att-Pooling-NLSFs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Sec. 3.5, we note that using the graph Laplacian L in Index NLSFs and the normalized graph Laplacian $\\mathbf{N}$ in Value NLSFs is transferable. Since real-world graphs often fall between these two boundary cases, we present the Laplacian attention NLSFs that operate between them at both the node-level and graph-level. Indeed, as demonstrated in Sec. 5, the proposed att-Node-level NLSFs, att-Graph-level NLSFs, and att-Pooling-NLSFs outperform existing spectral GNNs. ", "page_idx": 32}, {"type": "text", "text": "We conduct an ablation study to evaluate the contribution and effectiveness of different components within the att-Node-level NLSFs, att-Graph-level-NLSFs, and att-Pooling-NLSFs on node and graph classification tasks. Specifically, we compare the Index NLSFs and Value NLSFs using both the graph Laplacian $\\mathbf{L}$ and the normalized graph Laplacian $\\mathbf{N}$ to understand their individual and Laplacian attention impact on these tasks. ", "page_idx": 32}, {"type": "text", "text": "The ablation study of att-Node-level NLSPs for node classification is summarized in Tab. 9. We investigate the performance on six node classification benchmarks as in Sec. 5, including Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor. Tab. 9 shows that using the graph Laplacian $\\mathbf{L}$ in Index NLSFs (denoted as $\\Theta_{\\mathrm{ind}}(\\mathbf{L},\\cdot))$ and the normalized graph Laplacian $\\mathbf{N}$ in Value NLSFs (denoted as $\\Theta_{\\mathrm{val}}(\\mathbf{N},\\cdot)$ ) has superior node classification accuracy compared to using the normalized graph Laplacian $\\mathbf{N}$ in Index NLSFs (denoted as $\\Theta_{\\mathrm{ind}}(\\mathbf{N},\\cdot)$ ) and the graph Laplacian $\\mathbf{L}$ in Value ", "page_idx": 32}, {"type": "table", "img_path": "y8P633E5HQ/tmp/c1cd24cef375ba4633de26c4b1ee9ee35466316477596dcb2c12fe4bfe1200a0.jpg", "table_caption": ["Table 10: An ablation study investigated the effect of Graph-NLSFs on graph classification, comparing the use of Index NLSFs and Value NLSFs. The symbol (\u2191) denotes an improvement using Laplacian attention. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "y8P633E5HQ/tmp/107e82261cc4668a16ae64d2e024f1d48515fa3cac7dabd4b2428c30be626f08.jpg", "table_caption": ["Table 11: An ablation study investigated the effect of Pooling-NLSFs on graph classification, comparing the use of Index NLSFs and Value NLSFs. The symbol (\u2191) denotes an improvement using Laplacian attention. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "NLSFs (denoted as $\\Theta_{\\mathrm{val}}(\\mathbf{L},\\cdot))$ . This is in line with our theoretical findings in App. D.5. Moreover, the att-Node-level NLSFs using the Laplacian attention $\\mathsf{a t t}\\bigl(\\Theta_{\\mathrm{ind}}\\bigl(\\mathbf{L},\\cdot\\bigr),\\Theta_{\\mathrm{val}}\\bigl(\\mathbf{N},\\cdot\\bigr)\\bigr)$ yield the highest accuracies across all datasets, corroborating the findings in Sec. 3.5. We also note that without Laplacian attention, the Node-level NLSFs $\\Theta_{\\mathrm{ind}}(\\mathbf{L},\\cdot)$ and $\\Theta_{\\mathrm{val}}(\\mathbf{N},\\cdot))$ alone still achieve more effective classification performance compared to existing baselines, as shown in Tab. 1. ", "page_idx": 33}, {"type": "text", "text": "Tab. 10 demonstrates the ablation study of Graph-level NLSFs for graph classification tasks. We examine the eight graph datasets as in Sec. 5, including MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B, IMDB-M, and COLLAB. Similar to the above, we investigate the Index and Value settings using graph Laplacian $\\mathbf{L}$ and normalized graph Laplacian $\\mathbf{N}$ , including $\\Phi_{\\mathrm{ind}}(\\mathbf{L},\\cdot)$ , $\\Phi_{\\mathrm{ind}}(\\mathbf{N},\\cdot)$ , $\\Phi_{\\mathrm{val}}(\\mathbf{L},\\cdot)$ , and $\\Phi_{\\mathrm{val}}(\\mathbf{N},\\cdot)$ , along with their variants using Laplacian attention. Here, we see that att-Graph-level NLSFs do not show significant improvement over the standard Graph-level NLSFs. Notably, $\\Phi_{\\mathrm{val}}(\\mathbf{N},\\cdot)$ outperforms other models in social network datasets (IMDB-B, IMDB-M, and COLLAB), where the node features are augmented by the node degree. We plan to investigate the limited graph attribution in future work. ", "page_idx": 33}, {"type": "text", "text": "The ablation study of att-Pooling-NLSFs for graph classification is reported in Tab. 11. Similar to Tab. 10, we consider eight graph classification benchmarks: MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B, IMDB-M, and COLLAB. Tabl. 11 demonstrates that using the graph Laplacian $\\mathbf{L}$ in Index Pooling-NLSFs (denoted as $\\Theta_{\\mathrm{ind}}^{\\mathbb{P}}(\\mathbf{L},\\cdot))$ and the normalized graph Laplacian $\\mathbf{N}$ in Value Pooling-NLSFs (denoted as $\\Theta_{\\mathrm{val}}^{\\mathsf{P}}(\\mathbf{N},\\cdot))$ achieves superior graph classification accuracy compared to using the normalized graph Laplacian $\\mathbf{N}$ in Index Pooling-NLSFs (denoted as $\\Theta_{\\mathrm{ind}}^{\\mathsf{P}}(\\mathbf{N},\\cdot))$ and the graph Laplacian $\\mathbf{L}$ in Value Pooling-NLSFs (denoted as $\\bar{\\Theta_{\\mathrm{val}}^{\\mathrm{p}}}(\\mathbf{L},\\cdot))$ . This finding aligns with our theoretical results in App. D.5. Moreover, the att-Pooling-NLSFs using the Laplacian attention $\\mathsf{a t t}^{\\mathsf{P}}(\\Theta_{\\mathrm{ind}}(\\mathbf{L},\\cdot),\\Theta_{\\mathrm{val}}(\\bar{\\mathbf{N}},\\cdot))$ yield the highest accuracies across all datasets, corroborating the findings in Sec. 3.5. In addition, att-Pooling-NLSFs consistently outperform att-Graph-level NLSFs as shown in Tab. 10, indicating that the node features learned in our Node-level NLSFs representation are more expressive. This supports our theoretical findings in Sec. 4.3. ", "page_idx": 33}, {"type": "text", "text": "The ablation study demonstrates that the Laplacian attention between the Index and Value NLSFs significantly enhances classification accuracy for both node and graph classification tasks across various datasets, outperforming existing baselines. ", "page_idx": 33}, {"type": "table", "img_path": "y8P633E5HQ/tmp/fc9d016c6eb04cd4fc49e2b2640df321ca039a730191e0e968ca4376d37e85cd.jpg", "table_caption": ["Table 12: Average running time per epoch(ms)/average total running time(s). "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "y8P633E5HQ/tmp/538e605a29b31680dfcb71d02def43df0a20b3505f098283bd21481cee23e933.jpg", "table_caption": ["Table 13: Experimental results on large heterophilic graphs. The results for BernNet, ChebNet, ChebNetII, and GPRGNN are taken from [41], while the results for OptBasisGNN are taken from [38]. All other competing results are taken from [63]. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "F.4 Runtime Analysis ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In our NLSFs, the eigendecomposition can be calculated once for each graph and reused in the training process. This step is essential as the cost of the forward pass during model training often surpasses the initial eigendecomposition preprocessing cost. Note that the computation time for eigendecomposition is considerably less than the time needed for model training. For medium and large graphs, the computation time is further reduced when partial eigendecomposition is utilized, making it more efficient than the training times of competing baselines. To evaluate the computational complexity of our NLSFs compared to baseline models, we report the empirical training time in Tab. 12. Our Node-level NLSFs showcase competitive running times, with moderate values per epoch and total running times that are comparable to the most efficient models like GCN, GPRGNN, and APPNP. Notably, Node-level NLSFs are particularly efficient for the Cora, Citeseer, and Pubmed datasets. For the dense Squirrel graph, our Node-level NLSFs exhibit efficient performance with a moderate running time, outperforming several models that struggle with significantly higher times. ", "page_idx": 34}, {"type": "text", "text": "F.5 Scalability to Large-Scale Datasets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "To demonstrate the scalability of our method, we conduct additional tests on five large heterophilic graphs: Penn94, Pokec, Genius, Twitch-Gamers, and Wiki datasets from [63]. The experimental setup is in line with previous work by [16, 63, 41]. We use the same hyperparameters for our NLSFs as reported in App. E. Tab. 13 presents the classification accuracy. We see that NLSFs outperform the competing methods on the Penn94, Pokec, Genius, and Wiki datasets. For the Twitch-Gamers dataset, NLSFs yield the second-best results. Our additional experiments show that our method could indeed scale to handle large-scale graphs effectively. ", "page_idx": 34}, {"type": "text", "text": "F.6 Index-by-Index Index-NLSFs and Band-by-Band Value-NLSFs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Our primary objective in this work is to introduce new GNNs that are equivariant to functional symmetries, based on a novel spectral domain that is transferable between graphs. We emphasize ", "page_idx": 34}, {"type": "table", "img_path": "y8P633E5HQ/tmp/0fab47109f03310fa653c552b891298878ab3fa26ca1b020e2268b2821b298ee.jpg", "table_caption": ["Table 14: Graph classification performance using Diagonal NLSFs, including index-by-index IndexNLSFs and band-by-band Value-NLSFs, along with their variants using Laplacian attention. The symbol (\u2191) denotes an improvement using Laplacian attention. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "y8P633E5HQ/tmp/3abba7e5fa92802e60cb8bbc5cd318131f3c41681be6afda21116e297e2f65cd.jpg", "table_caption": ["Table 15: Semi-supervised node classification accuracy using NLSFs, diag-NLSFs, lead-NLSFs, and lead-diag-NLSFs. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "the unique aspects of our method rather than providing an exhaustive list of operators in this group, which, while important, is a key direction for future research. ", "page_idx": 35}, {"type": "text", "text": "Here, we present a new type of NLSFs: index-by-index Index-NLSFs and band-by-band Value-NLSFs. We denote them as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Gamma_{\\mathrm{ind}}(\\Delta,\\mathbf{X})=\\displaystyle\\sum_{j=1}^{J}\\gamma_{j}\\left(\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}\\right)\\frac{\\mathbf{P}_{j}\\mathbf{X}}{\\|\\mathbf{P}_{j}\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e}\\mathrm{~and~}}\\\\ {\\displaystyle\\Gamma_{\\mathrm{val}}(\\Delta,\\mathbf{X})=\\displaystyle\\sum_{j=1}^{K}\\gamma_{j}\\left(\\|g_{j}(\\Delta)\\mathbf{X}\\|_{\\mathrm{sig}}\\right)\\frac{g_{j}(\\mathbf{L})\\mathbf{X}}{\\|g_{p}(\\Delta)\\mathbf{X}\\|_{\\mathrm{sig}}^{a}+e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $a,e$ are as before in Sec. 3.3. Note that these operators are also equivariant to our group actions. ", "page_idx": 35}, {"type": "text", "text": "We investigate index-by-index Index-NLSFs and band-by-band Value-NLSFs in graph classification tasks as described in Sec. 5, including MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B, IMDB-M, and COLLAB datasets. Unlike Graph-NLSFs, which are fully spectral and map a sequence of frequency coefficients to an output vector, index-by-index Index-NLSFs and band-by-band ValueNLSFs do not possess such a sequential spectral form. In index-by-index Index-NLSFs and bandby-band Value-NLSFs, the index-by-index (or band-by-band) frequency response is projected back to the graph domain. Consequently, for graph classification tasks, we apply the readout function as defined for Pooling-NLSFs in Sec. 3.4. ", "page_idx": 35}, {"type": "text", "text": "Following App. F.3, we examine index-by-index Index-NLSFs and band-by-band Value-NLSFs settings using graph Laplacian L and normalized graph Laplacian N, including $\\Gamma_{\\mathrm{ind}}^{\\mathsf{P}}(\\mathbf{L},\\cdot)$ , $\\Gamma_{\\mathrm{ind}}^{\\mathsf{P}}(\\mathbf{N},\\cdot)$ $\\Gamma_{\\mathrm{val}}^{\\mathsf{P}}(\\bar{\\bf L},\\cdot)$ , and $\\Gamma_{\\mathrm{val}}^{\\mathsf{P}}(\\mathbf{N},\\cdot)$ , along with their variants using Laplacian attention, where P denotes the pooling function as in Tab. 11. ", "page_idx": 35}, {"type": "text", "text": "Tab. 14 presents the graph classification accuracy using index-by-index Index-NLSFs and band-byband Value-NLSFs. It shows that incorporating Laplacian attention consistently improves classification performance, aligning with the findings in App. F.3. We note that index-by-index Index-NLSFs and band-by-band Value-NLSFs perform comparably to existing baselines in graph classification benchmarks. However, index-by-index Index-NLSFs and band-by-band Value-NLSFs are generally less effective compared to Pooling-NLSFs, as shown in Tab. 11. ", "page_idx": 35}, {"type": "image", "img_path": "y8P633E5HQ/tmp/9f13141f1dc218333755c5e0010741920322fc4aaa2886f321436e7b5a010b03.jpg", "img_caption": ["Figure 5: Illustration of dyadic sub-bands for $\\begin{array}{r}{r=\\frac{1}{2}}\\end{array}$ and $S=4$ . "], "img_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "y8P633E5HQ/tmp/484922503cd205a8cdd869a6b0c816346d33ef196492dac8723d04b7c6ddb46b.jpg", "table_caption": ["Table 16: Graph classification performance using Graph-NLSFs with uniform sub-bands. The symbol $(\\uparrow)$ denotes an improvement using Laplacian attention. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "F.7 Node Classification Using Diag-NLSFs and Lead-NLSFs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In App. C, we presented the diag-NLSFs, consideringd  = d, and lead-NLSFs for leading fliters that do not include orthogonal complements. We explore t he leading fliters, orthogonal complement, and diagonal operation in our NLSFs. The results of these investigations are summarized in Tab. 15 , which shows the node classification accuracy achieved using various configurations, including NLSFs, diag-NLSFs, lead-NLSFs, and lead-diag-NLSFs. We see that incorporating both the orthogonal complement and the multi-channel approach yields the highest classification accuracy. ", "page_idx": 36}, {"type": "text", "text": "F.8 Uniform Sub-Bands ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Our primary objective in this work is to introduce new GNNs that are equivariant to functional symmetries, based on a novel spectral domain transferable between graphs using analysis and synthesis. Our NLSFs in Sec. 3.4 consider filters $g_{j}$ that are supported on the dyadic sub-bands $\\left[\\lambda_{N}r^{S-j+1},\\lambda_{N}r^{S-j}\\right]$ . The closer to the low-frequency range, the denser the sub-bands become. We illustrate an example of filters $g_{j}$ supported on dyadic sub-bands with $\\textstyle r={\\frac{1}{2}}$ and $S=4$ in Fig. 5, showing that sub-bands become denser as they approach the low-frequency range. Our primary goal is to highlight the unique aspects of our method rather than sub-band separation, which, while crucial, is an important consideration across spectral GNNs. Therefore, we also present uniform sub-bands, where fliters $g_{j}$ are supported on the uniform sub-bands $\\left[(j-1)\\frac{\\lambda_{N}}{S},j\\frac{\\lambda_{N}}{S}\\right]$ . Note that the modifications required for our NLSFs are minimal, and most steps can be seamlessly applied with filters supported on the uniform sub-bands. ", "page_idx": 36}, {"type": "text", "text": "We evaluate our NLSFs with uniform sub-bands on graph classification tasks. We consider the graph benchmarks as in Sec. 5, including five bioinformatics datasets: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, and three social network datasets: IMDB-B, IMDB-M, and COLLAB. Note the sub-bands only affect our Value-NLSFs, where Index-NLSFs remain the same as in Sec. 3.4. ", "page_idx": 36}, {"type": "text", "text": "We report the graph classification accuracy of Graph-NLSFs and Pooling-NLSFs using uniform sub-bands in Tab. 16 and Tab. 17, respectively, where the $\\Phi_{\\mathrm{ind}}(\\mathbf{L},\\cdot)$ , $\\Phi_{\\mathrm{ind}}(\\mathbf{N},\\cdot)$ , $\\Theta_{\\mathrm{ind}}(\\mathbf{\\bar{L}},\\cdot)$ , and $\\Theta_{\\mathrm{ind}}(\\mathbf{N},\\cdot)$ are the index-based NLSFs and therefore they are the same as in Tab. 10 and Tab. 11. Interestingly, the Laplacian attention does not yield significant improvements for either Graph-level NLSFs or Pooling-NLSFs when considering uniform sub-bands. Moreover, we observe that the graph classification performance is generally worse than when using the dyadic sub-bands reported in Tab. 10 and Tab. 11. We emphasize the importance of considering the spectral support of filters $g_{j}$ . Empirically, we found that using the dyadic grid is more effective, which is why we focus on it and report those results in the main paper. However, exploring other sub-bands remains an important tcahsoki cfoers  fouft F aonr di .e ,I nw te hpel lainm tiot  iwnvheesnt u, baansde $\\{a r^{j}-b\\}_{j=1}^{K}$ efrogr eost\u201dh teor $1<r<2$ $b>0$ $r\\rightarrow1$ $a\\to\\infty$ $b\\rightarrow\\infty$ the uniform grid. ", "page_idx": 36}, {"type": "table", "img_path": "y8P633E5HQ/tmp/15c69724689511ae9d84147d4823b35dd5330ddabb21754c0329d5e028c46435.jpg", "table_caption": ["Table 17: Graph classification performance using Pooling-NLSFs with uniform sub-bands. The symbol (\u2191) denotes an improvement using Laplacian attention. "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "G Additional Related Equivariant GNNs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Equivariant GNNs are designed to handle graph data with symmetries. The output of an equivariant GNN respects the same transformation applied to the input. Depending on the application, the transformation could involve translations, reflections, or permutations, to name but a few [82, 46, 78]. Due to respecting the symmetries, equivariant GNNs can reduce model complexity and improve generalization [35, 27], which can be applied to test data and produce more interpretable representations [99, 51]. When symmetry plays a critical role, such as in physical simulations, molecular modeling, and protein folding, equivariant GNNs have been demonstrated to be effective [20]. For example, [91, 33] employ spherical harmonics to handle 3D molecular structures, and [82] simplify computations for explicit rotation and translation matrices by focusing on relative distances between nodes. In addition, [30] embeds symmetry transformations by parameterizing convolution operations over Lie groups, such as rotations, translations, and scalings, through Lie algebra. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We point out that our NLSFs are fully equivariant to graph functional shifts and show that they have universal approximation properties, where the proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We list out our contributions in Sec. 1. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We present the limitation discussion of our method in Sec. 6 (lines 359-363). Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Right before and/or after each stated theoretical result, we indicate that the proof and the theoretical analysis can be found in App. D. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Detailed descriptions of the methodology are provided in the manuscript and App. E. The method can be re-implemented, and results are reproducible. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide detailed descriptions of our method\u2019s implementation within the paper and report the hyper-parameters in App. E. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide comprehensive information on data splits, hyperparameters, and the criteria for their selection. Additionally, we describe the type of optimizer used and other relevant training details. All this information is thoroughly detailed in App. E. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: In Sec. 5, we report the node classification quality by computing the average classification accuracy along with a $95\\%$ confidence interval. Additionally, we present the mean and standard deviation for the graph classification accuracy to provide a comprehensive understanding of the experimental results. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 40}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Detailed descriptions of the type of computer resources and time of execution are provided in App. E and App. F. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have respected the NeurIPS code of Ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of graph machine learning. The paper has no foreseeable societal impact. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide credit to the original papers of all external data, code, and models used in this work, ensuring that their contributions are acknowledged and their licensing terms are followed. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 42}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We provide detailed descriptions of our method\u2019s implementation within the paper and report the hyper-parameters in App. E. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: No crowdsourcing nor research with human subjects was involved in this work. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: No crowdsourcing nor research with human subjects was involved in this work. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]