{"references": [{"fullname_first_author": "Martin Arjovsky", "paper_title": "Invariant risk minimization", "publication_date": "2019-07-01", "reason": "This paper introduces the concept of Invariant Risk Minimization (IRM), a crucial technique for addressing distribution shifts in machine learning, which is central to the methodology of GraphMETRO."}, {"fullname_first_author": "Shurui Gui", "paper_title": "GOOD: A graph out-of-distribution benchmark", "publication_date": "2022-01-01", "reason": "The GOOD benchmark provides a standardized evaluation framework for out-of-distribution generalization in graph neural networks, allowing for systematic comparison of GraphMETRO's performance against state-of-the-art methods."}, {"fullname_first_author": "Thomas N. Kipf", "paper_title": "Semi-supervised classification with graph convolutional networks", "publication_date": "2017-01-01", "reason": "This paper introduces Graph Convolutional Networks (GCNs), a foundational model in graph neural networks, which is extensively used in GraphMETRO for both node and graph classification tasks."}, {"fullname_first_author": "Michael I. Jordan", "paper_title": "Hierarchical mixtures of experts and the EM algorithm", "publication_date": "1994-01-01", "reason": "This paper introduces the Mixture of Experts (MoE) architecture which is fundamental to the design of GraphMETRO, enabling it to model the diverse nature of real-world graph data and handle complex distribution shifts."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This paper introduces the sparsely-gated Mixture of Experts (MoE) layer, a highly scalable and efficient architecture for handling large-scale data that directly inspired the MoE architecture used in GraphMETRO."}]}