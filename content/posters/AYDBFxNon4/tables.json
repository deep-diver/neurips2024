[{"figure_path": "AYDBFxNon4/tables/tables_19_1.jpg", "caption": "Table S1: Comparison of mechanisms of induction heads and CMR.", "description": "This table compares the mechanisms of three types of attention heads (K-composition, Q-composition, and CMR) in Transformer models.  It details the representation in the residual stream before and after the first and second layer attention heads, specifying the query, key, optimal match conditions, activation functions, and value and output for each layer of each attention head type. It highlights the similarities and differences in how information is processed and how the outputs contribute to the residual stream.", "section": "Supplementary Materials"}, {"figure_path": "AYDBFxNon4/tables/tables_20_1.jpg", "caption": "Table S2: Details of compute resources used to compute induction head metrics. All models were pretrained and accessible through the TransformerLens library [25] with MIT License. The numbers in the \"Computing time\" column indicate the total number of minutes it took to compute all scores for all heads across all checkpoints where available.", "description": "This table details the computational resources used for the experiments in the paper, focusing on the computation time needed for each model.  It shows that smaller models (GPT2-small and smaller Pythia models) could be run on a standard CPU, while larger models (larger Pythia models and others) required high-RAM CPUs or TPUs for processing.  The computing times ranged from less than a minute for GPT2-small to over 3 hours for some larger Pythia models.  The TransformerLens library was used for all models.", "section": "H Experiment compute resources"}]