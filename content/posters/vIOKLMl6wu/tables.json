[{"figure_path": "vIOKLMl6wu/tables/tables_3_1.jpg", "caption": "Table 1: Data taxonomy of GenQA, detailing the data type, name, size, and instruction prompts of each dataset.", "description": "This table presents a breakdown of the datasets used for the GenQA task in the LOVA\u00b3 framework. It categorizes datasets based on their data type (Generic VQA, Multi-choice VQA, Multi-turn VQA, REC, REG), provides the dataset name and size, and includes example instruction prompts used for data generation. The table helps illustrate the diversity of data sources and task types incorporated into the GenQA training process to enhance the model's ability to generate high-quality question-answer pairs from images.", "section": "3.1 Data Collection for GenQA"}, {"figure_path": "vIOKLMl6wu/tables/tables_7_1.jpg", "caption": "Table 3: Results on five generic tasks including VQAv2 [26], GQA [30], VizWiz [27], ScienceQA [50], and POPE [39]. The first two columns represent the results on held-in datasets marked as *, and the last three columns represent the held-out datasets. The best result on each subtask is bolded.", "description": "This table presents the results of five generic visual question answering (VQA) tasks comparing the performance of LOVA3 and other state-of-the-art models.  The tasks include VQAv2, GQA, VizWiz, ScienceQA, and POPE, each with different characteristics and difficulty levels. The table shows that LOVA3 consistently outperforms other models across all five datasets, demonstrating its effectiveness.", "section": "4.2 Main Results"}, {"figure_path": "vIOKLMl6wu/tables/tables_7_2.jpg", "caption": "Table 4: Results on multimodal benchmarks, including MME [19] and SEED-Bench [36], MM-Bench [45] and LLava-Bench [43]", "description": "This table presents the results of the LOVA\u00b3 model and other state-of-the-art (SOTA) models on four widely used multimodal benchmarks: MME, SEED-Bench, MMBench, and LLaVA-Bench.  The table shows the performance of each model on various subtasks within each benchmark, allowing for a comparison of their overall multimodal reasoning capabilities.  The inclusion of LOVA\u00b3's results highlights the impact of the proposed training framework on various multimodal reasoning tasks.", "section": "4 Experiments"}, {"figure_path": "vIOKLMl6wu/tables/tables_8_1.jpg", "caption": "Table 5: Multimodal reasoning ability on MM-Vet [92]. Rec denotes Recognition; Know denotes knowledge; Gen denotes Language generation; and Spat denotes Spatial awareness.", "description": "This table presents the results of several multimodal large language models on the MM-Vet benchmark.  The MM-Vet benchmark assesses multimodal reasoning capabilities across various sub-tasks. The table shows the performance of each model across different aspects like Recognition, OCR, Knowledge, Generation, and Spatial Awareness. The improvements achieved by LOVA3 over the baseline model are highlighted. The table allows for comparing the relative strengths of different models in various multimodal reasoning tasks.", "section": "4.2 Main Results"}, {"figure_path": "vIOKLMl6wu/tables/tables_8_2.jpg", "caption": "Table 6: Ablation studies on different finetuning datasets. The model is LOVA\u00b3-7B.", "description": "This table presents the results of ablation studies performed on the LOVA\u00b3-7B model.  It shows the impact of including different combinations of GenQA-Generic, GenQA-Grounding, and EvalQA datasets during model training.  The performance is measured across several key multimodal benchmarks (GQA, VizWiz, ScienceQA, POPE, and MME). By comparing the performance across rows, one can observe the effect of each dataset on the overall performance, revealing the contribution of each training task to the final model's abilities.", "section": "4.3 Ablation Study"}, {"figure_path": "vIOKLMl6wu/tables/tables_8_3.jpg", "caption": "Table 3: Results on five generic tasks including VQAv2 [26], GQA [30], VizWiz [27], ScienceQA [50], and POPE [39]. The first two columns represent the results on held-in datasets marked as *, and the last three columns represent the held-out datasets. The best result on each subtask is bolded.", "description": "This table presents the results of five generic visual question answering (VQA) tasks: VQAv2, GQA, VizWiz, ScienceQA, and POPE.  The performance of the LOVA\u00b3 model is compared against several other state-of-the-art (SOTA) models. The table is divided into two parts: held-in datasets (marked with *) and held-out datasets.  The best result for each task is highlighted in bold.  The results demonstrate the improvement achieved by LOVA\u00b3 over existing methods on these standard VQA benchmarks.", "section": "4.2 Main Results"}, {"figure_path": "vIOKLMl6wu/tables/tables_9_1.jpg", "caption": "Table 8: Results of multimodal large language models on the test set of EvalQABench (ours).", "description": "This table presents the performance of various multimodal large language models on the EvalQABench test set, specifically focusing on the models' ability to assess the correctness of visual question-answer pairs.  The metrics used are Accuracy, Precision, F1 Score, and the percentage of \"No\" answers.  The inclusion of the \"No (%)\" column helps assess the bias of the models towards either positive or negative classifications. The table is organized to compare Vision Language Pretraining Models and Multimodal Large Language Models separately, highlighting the performance difference between them and the improvement achieved by the proposed LOVA\u00b3 model.", "section": "4.5 Benchmark of EvalQABench"}, {"figure_path": "vIOKLMl6wu/tables/tables_17_1.jpg", "caption": "Table 9: 665K instruction data of LLaVA1.5. The content is from LLaVA1.5 for convenient browsing.", "description": "This table lists the datasets used in the instruction-tuning stage of the LLaVA1.5 model.  It shows the name of each dataset, its size (number of samples), and the instructions used to generate the training data. The datasets cover various vision-language tasks, including general VQA (Visual Question Answering), multi-choice VQA, and image captioning, providing a diverse set of training examples for the model.", "section": "Additional Details of Implementation"}, {"figure_path": "vIOKLMl6wu/tables/tables_17_2.jpg", "caption": "Table 10: Hyperparameters of LOVA\u00b3 are the same as the LLaVA1.5.", "description": "This table shows the hyperparameters used for training the LOVA\u00b3 model.  The values are identical to those used for training the LLaVA1.5 model, demonstrating a consistent approach in the experimental setup.  The hyperparameters include settings related to batch size, learning rate (and its scheduling), warmup ratio, weight decay, number of epochs, the optimizer employed (AdamW), and the DeepSpeed stage.", "section": "3.4 Training"}, {"figure_path": "vIOKLMl6wu/tables/tables_18_1.jpg", "caption": "Table 11: Data amount details of creating EvalQABench training set.", "description": "This table shows the number of samples at each stage of the EvalQABench dataset creation.  It starts with 100,000 raw VQA pairs.  Then negative answers are generated, manually filtered, and corrected. Feedback is then generated and further filtered. The final count is 41,592 training samples.", "section": "3.2 Data Creation for EvalQA"}, {"figure_path": "vIOKLMl6wu/tables/tables_19_1.jpg", "caption": "Table 12: Statistic of question types of EvalQABench training set.", "description": "This table presents the statistical distribution of question types within the EvalQABench training dataset. It shows the number and proportion of questions categorized into nine types: Object, Yes/No, Counting, Color, Attribute, Number, Relation, Action, and Other.  The 'Other' category encompasses diverse question types not easily classified into the other categories.", "section": "3.2 Data Creation for EvalQA"}]