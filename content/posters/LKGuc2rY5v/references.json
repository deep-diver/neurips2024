{"references": [{"fullname_first_author": "Nathan Kallus", "paper_title": "Double reinforcement learning for efficient off-policy evaluation in Markov decision processes", "publication_date": "2020-00-00", "reason": "This paper is foundational to the current work, introducing the Double Reinforcement Learning (DRL) estimator for efficient off-policy evaluation, which the current paper generalizes for robust settings."}, {"fullname_first_author": "Dylan J Foster", "paper_title": "Orthogonal statistical learning", "publication_date": "2023-00-00", "reason": "This paper introduces the concept of orthogonal statistical learning, which is crucial for constructing estimators that are robust to errors in nuisance function estimation, a key property of the proposed estimator in the current paper."}, {"fullname_first_author": "Victor Chernozhukov", "paper_title": "Double/debiased machine learning for treatment and structural parameters", "publication_date": "2018-00-00", "reason": "This paper lays the groundwork for using double machine learning techniques in causal inference, which are instrumental in addressing the challenges of nuisance function estimation in the context of robust off-policy evaluation."}, {"fullname_first_author": "Masatoshi Uehara", "paper_title": "Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper addresses the challenge of handling long horizons in off-policy evaluation, which is relevant to the current paper's focus on infinite-horizon robust Markov Decision Processes."}, {"fullname_first_author": "Garud N Iyengar", "paper_title": "Robust dynamic programming", "publication_date": "2005-00-00", "reason": "This paper is foundational for the study of robust Markov decision processes, providing a theoretical basis for the robust optimization approach adopted in the current work."}]}