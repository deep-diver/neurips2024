[{"figure_path": "SCEdoGghcw/figures/figures_2_1.jpg", "caption": "Figure 1: We find SAE features that detect interpretable board state properties (BSP) with high precision (i.e., above 0.95). This figure illustrates three distinct chessboard states, each an example of a BSP associated with a high activation of a particular SAE feature. Left: A board state detector identifies a knight on square f3, owned by the player to move. Middle: A rook threat detector indicates an immediate threat posed by a rook to a queen regardless of location and piece threatened. Right: A pin detector recognizes moves that resolve a check on a diagonal by creating a pin, again, regardless of location and piece pinned.", "description": "This figure shows three examples of chessboard states, each illustrating a different board state property (BSP) detected by a sparse autoencoder (SAE) feature. The left panel shows a BSP detector identifying a knight on F3. The middle panel shows a rook threat detector recognizing an immediate threat to a queen. The right panel shows a pin detector identifying a move that resolves a check by creating a pin.  These examples highlight the SAE's ability to learn interpretable features related to chess strategy.", "section": "2.2 Sparse autoencoders"}, {"figure_path": "SCEdoGghcw/figures/figures_5_1.jpg", "caption": "Figure 2: Comparison of the coverage and board reconstruction metrics for chess SAE quality on Gboard state. The coverage score reports the mean F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. L0, with the scheme color corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes.", "description": "This figure compares the performance of four different SAE training methods (Standard, Standard w/ p-annealing, Gated SAE, Gated SAE w/ p-annealing) on two metrics: coverage and board reconstruction.  The x-axis represents the L0 norm (sparsity), while the y-axis shows the performance on each metric. The plots show how the coverage and reconstruction performance change with increasing sparsity for each training method.  Different shapes represent different hyperparameter settings for each method. The figure helps to understand the tradeoff between sparsity and performance for various SAE training strategies on the Gboard state properties.", "section": "3 Measuring autoencoder quality for chess and Othello models"}, {"figure_path": "SCEdoGghcw/figures/figures_6_1.jpg", "caption": "Figure 2: Comparison of the coverage and board reconstruction metrics for chess SAE quality on Gboard state. The coverage score reports the mean F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. L0, with the scheme color corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes.", "description": "This figure compares different methods for training sparse autoencoders (SAEs) on chess game data, evaluating their performance using two new metrics: coverage and board reconstruction.  The graphs show how these metrics (as well as existing metrics like loss recovered and L0 norm) relate to the sparsity of the SAE features (lower L0 means more sparse). Different shapes represent different training methods, allowing for a comparison of their effectiveness across multiple measures of performance.", "section": "3 Measuring autoencoder quality for chess and Othello models"}, {"figure_path": "SCEdoGghcw/figures/figures_8_1.jpg", "caption": "Figure 2: Comparison of the coverage and board reconstruction metrics for chess SAE quality on Gboard state. The coverage score reports the mean F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. Lo, with the scheme color corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes.", "description": "This figure compares the performance of different SAE training methods on chess data using two metrics: coverage and board reconstruction.  The plots show how these metrics change with the L0 norm (sparsity) of the SAE features.  The color-coding helps to understand how the two metrics relate to one another, and the different shapes indicate the different SAE training methods.  It demonstrates that p-annealing generally improves performance compared to standard SAE training.", "section": "3 Measuring autoencoder quality for chess and Othello models"}, {"figure_path": "SCEdoGghcw/figures/figures_19_1.jpg", "caption": "Figure 5: Comparison of the relative reconstruction bias metric \u03b3 quantifying feature activation shrinkage across a suite of SAEs. \u03b3 < 1 indicates shrinkage. A perfectly unbiased SAE would have \u03b3 = 1.", "description": "This figure compares the relative reconstruction bias (\u03b3) for different SAE training methods across various levels of sparsity (L\u2080).  The relative reconstruction bias measures how much the SAE underestimates (shrinkage) or overestimates the feature activations.  The lower the \u03b3 value, the more shrinkage is observed. The figure shows that p-annealing and Gated SAEs achieve similar levels of improved relative reconstruction bias compared to the Standard SAE, particularly in the case of Othello, while chess shows less difference.", "section": "E Relative Reconstruction Bias"}, {"figure_path": "SCEdoGghcw/figures/figures_20_1.jpg", "caption": "Figure 6: Additional examples of learned SAE features. We show the full board state of a chosen game in which the SAE latent has a high activation. The PGN-string (model input) which represents the game history is shown below the board. Tokens that activate SAE features are marked in blue, where darker shades correspond to higher feature activations. Moves that create the considered board state are highlighted in yellow.", "description": "This figure shows three examples of chess board states and their corresponding PGN strings, highlighting tokens that trigger specific SAE features. The intensity of the blue color indicates the strength of the feature activation, demonstrating how these features correspond to specific board configurations or game states.", "section": "G Additional examples of learned SAE features"}, {"figure_path": "SCEdoGghcw/figures/figures_20_2.jpg", "caption": "Figure 6: Additional examples of learned SAE features. We show the full board state of a chosen game in which the SAE latent has a high activation. The PGN-string (model input) which represents the game history is shown below the board. Tokens that activate SAE features are marked in blue, where darker shades correspond to higher feature activations. Moves that create the considered board state are highlighted in yellow.", "description": "This figure shows three examples of chess board states where specific SAE features have high activation.  Each example illustrates a different interpretable board property (BSP), such as the availability of an en passant move, the presence of a knight on a particular square, and a mirrored knight position. The PGN sequences leading to each state are also displayed, highlighting the relevant moves.", "section": "Additional examples of learned SAE features"}, {"figure_path": "SCEdoGghcw/figures/figures_20_3.jpg", "caption": "Figure 1: We find SAE features that detect interpretable board state properties (BSP) with high precision (i.e., above 0.95). This figure illustrates three distinct chessboard states, each an example of a BSP associated with a high activation of a particular SAE feature. Left: A board state detector identifies a knight on square f3, owned by the player to move. Middle: A rook threat detector indicates an immediate threat posed by a rook to a queen regardless of location and piece threatened. Right: A pin detector recognizes moves that resolve a check on a diagonal by creating a pin, again, regardless of location and piece pinned.", "description": "This figure shows three examples of chessboard states, each highlighting a different interpretable board state property (BSP) detected by a sparse autoencoder (SAE).  The left panel shows a BSP detector identifying a knight on square f3. The middle panel shows a BSP detector indicating a rook's threat to a queen. The right panel shows a BSP detector identifying a pin that resolves a check.", "section": "2.2 Sparse autoencoders"}]