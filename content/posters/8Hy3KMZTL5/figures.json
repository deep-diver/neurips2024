[{"figure_path": "8Hy3KMZTL5/figures/figures_3_1.jpg", "caption": "Figure 1: A schematic representation of H-CLIP. In the H-CLIP framework, we propose a partial orthogonal fine-tuning strategy, where each pre-trained weight matrix is paired with a tuned block-diagonal transformation matrix, some of which are orthogonal to preserve generalization. Then, we introduce a dual cross-relation communication mechanism to facilitate communication among all matrices, enabling alignment between different modalities.", "description": "This figure illustrates the architecture of H-CLIP, a method for parameter-efficient fine-tuning of CLIP for open-vocabulary semantic segmentation.  It shows how partial orthogonal fine-tuning (POF) is applied to both image and text encoders of CLIP.  Each pre-trained weight matrix is augmented with a learnable block-diagonal transformation matrix. Some matrices in the text encoder are constrained to be orthogonal to maintain generalization ability. A dual cross-relation communication (DCRC) module is included to encourage interactions among all learnable matrices, bridging the gap between image and text modalities. The output of this process feeds into a decoder to generate the final segmentation.", "section": "4 Methodology"}, {"figure_path": "8Hy3KMZTL5/figures/figures_6_1.jpg", "caption": "Figure 2: Comparison of qualitative reults on ADE20K [55] with 150 categories. we compare Our method with CAT-Seg [7].", "description": "This figure presents a qualitative comparison of the proposed H-CLIP model's semantic segmentation performance against CAT-Seg [7] on the ADE20K dataset with 150 categories. The comparison is shown through three example images, displaying the ground truth segmentation, CAT-Seg's output, and H-CLIP's output side-by-side for each image.  This allows for a visual assessment of the relative strengths and weaknesses of each method in terms of accuracy and detail of segmentation.", "section": "5.2 Main Results"}, {"figure_path": "8Hy3KMZTL5/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of qualitative results on ADE20K [55] with 150 categories. We compare Our method with CAT-Seg [7].", "description": "This figure shows a qualitative comparison of the proposed H-CLIP model's performance against the CAT-Seg [7] method on the ADE20K dataset with 150 categories. Three sample images are presented with their ground truth segmentations, the results from CAT-Seg [7], and the results from the proposed H-CLIP model.  Visual inspection of the segmentations allows for a comparison of the accuracy and detail achieved by each method.  The goal is to demonstrate the improved segmentation quality of H-CLIP.", "section": "5.2 Main Results"}, {"figure_path": "8Hy3KMZTL5/figures/figures_7_2.jpg", "caption": "Figure 2: Comparison of qualitative results on ADE20K [55] with 150 categories. We compare our method with CAT-Seg [7].", "description": "This figure displays a qualitative comparison of the proposed H-CLIP model's semantic segmentation results against CAT-Seg on the ADE20K dataset, focusing on 150 categories.  It visually demonstrates the model's performance by showing the ground truth segmentation, the CAT-Seg segmentation results, and the H-CLIP segmentation results for several sample images.  The comparison highlights the differences in accuracy and detail between the methods.", "section": "5.2 Main Results"}]