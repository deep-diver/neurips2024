[{"figure_path": "qCJ1dq5M7N/tables/tables_7_1.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents the quantitative results of LoRA and FouRA on text-to-image style transfer tasks.  It shows the LPIPS diversity and HPSv2 scores for both methods across different adapter strengths (\u03b1 = 1, 0.8, 0.6) and for two base models (Stable Diffusion-v1.5 and Realistic Vision-v3.0). The results are averages over 30 random seeds and demonstrate that FouRA generally outperforms LoRA in terms of both image diversity and perceived quality.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_7_2.jpg", "caption": "Table 1: Merging two adapters for Blue Fire and Paintings with strengths ab and ap.", "description": "This table shows the results of merging two adapters (Blue Fire and Paintings) using both LoRA and FouRA methods.  The HPSv2 score, a metric measuring image quality, alignment with the prompt, and aesthetic coherence, is presented for various combinations of adapter strengths (\u03b1b and \u03b1p). It demonstrates that FouRA consistently achieves higher HPSv2 scores than LoRA, indicating better image quality and style preservation even when merging multiple adapters.", "section": "3.5 Combining multiple adapters"}, {"figure_path": "qCJ1dq5M7N/tables/tables_8_1.jpg", "caption": "Table 3: Performance on Commonsense Reasoning benchmarks: Evaluation on eight Commonsense Reasoning benchmarks with the Llama-3(8B) model.", "description": "This table presents the results of the Commonsense Reasoning benchmark evaluation.  It compares the performance of LoRA and FouRA adapters, at different ranks (16 and 32), when fine-tuned on an Llama-3(8B) model.  The results are shown for eight different commonsense reasoning tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA). The 'Average' column provides an overall performance score.", "section": "5.4 Commonsense Reasoning Tasks"}, {"figure_path": "qCJ1dq5M7N/tables/tables_8_2.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents the quantitative results of LoRA and FouRA models on text-to-image style transfer tasks.  The results are averaged across 30 different random seeds, ensuring statistical robustness.  The key metrics reported are LPIPS Diversity (higher is better, indicating more diverse generated images) and HPSv2 score (higher is better, indicating higher image quality and better alignment with the prompt/style).  The table is broken down by the adapter strength (\u03b1) and dataset used.  The comparison allows for a direct assessment of FouRA's performance relative to the established LoRA baseline.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_9_1.jpg", "caption": "Table 5: Individual gain with FouRA components. Gains from each individual component of FouRA. All results are with rank 64 and a = 0.8 on the paintings adapter.", "description": "This table shows the ablation study results for different components of FouRA, comparing the performance with LoRA. It demonstrates the individual contribution of each component (Fourier transform, frozen dynamic mask, inference-adaptive mask) to the overall performance improvement in terms of HPS and LPIPS diversity metrics. The results highlight the importance of each component in achieving the final performance gain of FouRA over LoRA.", "section": "5.6 Ablation Studies"}, {"figure_path": "qCJ1dq5M7N/tables/tables_15_1.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table compares the performance of LoRA and FouRA on text-to-image style transfer tasks using different base models (Stable Diffusion v1.5 and Realistic Vision 3.0) and adapter strengths (\u03b1 = 1, 0.8, 0.6).  The metrics used for comparison are LPIPS diversity (higher is better, indicating more diverse image generation) and HPSv2 score (higher is better, representing better image quality and prompt alignment).  The results highlight FouRA's superior performance in generating more diverse and higher-quality images across all settings.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_16_1.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents a comparison of LoRA and FouRA performance on text-to-image style transfer tasks using the Stable Diffusion v1.5 and Realistic Vision 3.0 models.  The results are averaged across 30 different random seeds.  The table shows LPIPS Diversity and HPSv2 scores for different adapter strengths (\u03b1) and models (LoRA and FouRA) on two datasets (Paintings and Blue Fire). LPIPS diversity measures the diversity of generated images, while HPSv2 assesses image quality and alignment with the prompt.  The rank of the adapter is fixed at 64.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_17_1.jpg", "caption": "Table 3: Performance on Commonsense Reasoning benchmarks: Evaluation on eight Commonsense Reasoning benchmarks with the Llama-3(8B) model.", "description": "This table presents the results of the Commonsense Reasoning benchmark experiments.  It compares the performance of LoRA and FouRA adapters on eight different commonsense reasoning tasks using the Llama-3(8B) model. The table shows the number of trainable parameters for each model along with performance metrics on each task (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA), and a final average score across all tasks.  This allows for a direct comparison of the effectiveness of LoRA and FouRA approaches for this specific task.", "section": "5.4 Commonsense Reasoning Tasks"}, {"figure_path": "qCJ1dq5M7N/tables/tables_17_2.jpg", "caption": "Table C.2: GLUE Benchmark", "description": "This table presents the details of the GLUE benchmark datasets used in the paper's experiments.  It shows the number of training and validation samples for each of the six tasks (CoLA, SST-2, MRPC, STS-B, MNLI, and QNLI) and specifies the evaluation metric used for each task.  The table provides essential information for understanding and reproducing the experimental results related to the GLUE benchmark evaluations.", "section": "C.1.2 GLEU"}, {"figure_path": "qCJ1dq5M7N/tables/tables_18_1.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents the results of experiments comparing LoRA and FouRA on text-to-image style transfer tasks.  The experiments used adapters with a rank of 64, and results are averaged across 30 different random seeds to ensure reliability. The table shows LPIPS Diversity and HPSv2 scores for both LoRA and FouRA across different adapter strengths (\u03b1 = 1, 0.8, 0.6) and for two different base models (Stable Diffusion-v1.5 and Realistic Vision-v3.0).  It allows for a comparison of the performance of the two methods in terms of image diversity and overall image quality.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_21_1.jpg", "caption": "Table E.1: Performance on unseen classes. Shows that on unseen classes FouRA generalizes better on unseen categories.", "description": "This table presents the performance of LoRA and FouRA on unseen classes from the Bluefire and Paintings datasets.  The results are measured using the HPSv2 score (higher is better) at different adapter strengths (\u03b1 = 1.0, 0.8, 0.6). The table shows that FouRA consistently outperforms LoRA on unseen data, demonstrating its superior generalization capability.", "section": "E.1.1 Performance on Unseen Concepts for Text-to-Image Stylization"}, {"figure_path": "qCJ1dq5M7N/tables/tables_21_2.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents the quantitative results of LoRA and FouRA on text-to-image style transfer tasks.  The results are averaged over 30 different random seeds and show the LPIPS diversity and HPSv2 scores for each model and adapter strength (\u03b1). LPIPS diversity measures the diversity of generated images while HPSv2 evaluates image quality and alignment with the prompt and style.  The table shows that FouRA significantly outperforms LoRA on both metrics across different adapter strengths.  All adapters used were rank 64.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_22_1.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents the results of evaluating LoRA and FouRA on text-to-image style transfer tasks.  The experiments used adapters with a rank of 64 and the results are averaged across 30 different random seeds. The table shows LPIPS Diversity and HPSv2 scores for different adapter strengths (\u03b1) across two datasets and two different base models.  Higher LPIPS Diversity indicates greater diversity in generated images, while higher HPSv2 scores suggest better image quality and better alignment with the prompt and style.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_23_1.jpg", "caption": "Table 2: Evaluation of LoRAs on Text-to-Image tasks. Adapters are rank 64. Results are averaged over 30 seeds.", "description": "This table presents the performance comparison between LoRA and FouRA for text-to-image style transfer tasks.  It shows LPIPS Diversity and HPSv2 scores for both methods across different adapter strengths (\u03b1) and datasets (Paintings and Blue-Fire). Higher LPIPS Diversity indicates greater image diversity, while higher HPSv2 scores signify better image quality and alignment with the prompts and styles.  The results are averaged over 30 random seeds to ensure statistical reliability.", "section": "5.2 Text-to-Image Stylized Generation"}, {"figure_path": "qCJ1dq5M7N/tables/tables_28_1.jpg", "caption": "Table G.1: Evaluation of DeBERTa-V3 on the GLUE benchmarks, averaged over 3 seeds.", "description": "This table presents the performance comparison of three different adapters (LoRA, SORA, and FouRA) on six GLUE benchmark tasks. The results are averaged across three random seeds, providing a measure of the models' performance on each task.  The table shows the accuracy for each model on the respective GLUE tasks.", "section": "G FouRA on General Language Understanding Tasks"}]