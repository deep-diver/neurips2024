[{"figure_path": "oNMnR0NJ2e/figures/figures_1_1.jpg", "caption": "Figure 1: Soft labels are crucial for dataset distillation Left: Synthetic images by different distillation methods. Right: Student test Accuracy comparison between different distillation methods and random baseline from training data (both with hard labels or with soft labels). For soft/hard label generation details, see Appendix A.2.", "description": "The figure on the left shows example synthetic images generated by different dataset distillation methods.  The figure on the right is a graph comparing the student test accuracy of different distillation methods (using both soft and hard labels) against a random baseline using training data with hard labels.  The graph shows that soft labels are crucial for the success of dataset distillation.", "section": "3 Soft Labels are Crucial for Distillation"}, {"figure_path": "oNMnR0NJ2e/figures/figures_4_1.jpg", "caption": "Figure 2: Expert test accuracy v.s. student test accuracy v.s. soft label entropy. The quality of soft labels (measured by student accuracy) depends on expert accuracy (left) and label entropy (right).", "description": "This figure shows the relationship between expert test accuracy, student test accuracy, and the average entropy of soft labels.  The left panel demonstrates that student performance improves as expert accuracy increases, indicating that the quality of soft labels is tied to expert performance. The right panel shows that as the average entropy of soft labels decreases, student accuracy initially improves, but then plateaus and eventually decreases at very low entropy values. This suggests there's an optimal level of entropy, where sufficient uncertainty in the labels aids student learning, but excessive certainty hinders generalization.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_5_1.jpg", "caption": "Figure 3: Importance of i-th label by performing label swapping test. Swap the i-th label (sorted by softmax value) with the last label. Top labels contain structured information and the non-top labels contain noise.", "description": "This figure shows the results of an experiment designed to assess the impact of label order on model performance in a dataset distillation setting.  Specifically, it shows how swapping the i-th label (ranked by its softmax probability) with the least probable label affects the model's relative performance.  The results across different images-per-class (IPC) budgets (1, 10, 50) demonstrate that the most probable labels carry structured information crucial for model learning, while less probable labels contribute mainly noise.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_5_2.jpg", "caption": "Figure 4: (Expert Epoch, Temp) grid search on TinyImageNet IPC=10. Temperature smoothing does not fully resolve the issue that later epoch experts yield sub-optimal labels for a given data budget.", "description": "This figure shows the results of a grid search experiment conducted on the TinyImageNet dataset with 10 images per class (IPC=10). The experiment investigated the impact of two hyperparameters: the expert epoch (the training epoch at which the expert model's predictions are used as soft labels) and the temperature used for softmax probability smoothing.  The heatmap displays the student model's test accuracy for different combinations of expert epoch and temperature.  The results indicate that even with temperature smoothing, using soft labels generated from later training epochs (overfit expert) does not necessarily lead to improved student performance, suggesting that label quality is not solely determined by entropy.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_5_3.jpg", "caption": "Figure 5: Soft labels generated by expert at different epochs. The structured information in soft labels changes over the course of training.", "description": "This figure shows the soft labels generated by an expert model at different epochs for a single example image (a German Shepherd).  It highlights how the probability distribution across classes changes as the expert model trains. Early epochs show a flatter distribution, reflecting less certainty, while later epochs show a sharper distribution, with higher probability assigned to the correct class.  This illustrates the concept of structured information evolving within soft labels over the course of training.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_6_1.jpg", "caption": "Figure 6: Data-Knowledge Scaling Law for TinyImageNet. Left: Trading knowledge (amount of information in soft labels) with data using one expert. Right: Establishing the Pareto-optimal front for data-efficient learning.", "description": "This figure presents two graphs illustrating the trade-off between knowledge (soft labels from an expert model) and data (number of images per class) in achieving good student model accuracy on the TinyImageNet dataset.  The left graph shows how, for a given expert model at a specific epoch, using more of the top-ranked soft labels (higher K) allows for improved student accuracy even with smaller datasets. The right graph illustrates the Pareto frontier, displaying the optimal balance between dataset size and student accuracy across multiple expert epochs.  Different colors represent different expert epochs, demonstrating that the optimal balance varies depending on the expert's training stage.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_7_1.jpg", "caption": "Figure 7: Zero-shot learning in the absence of knowledge v.s. data. The student model can achieve good performances when data is absent but much worse performances when knowledge is absent.", "description": "This figure shows the results of a zero-shot learning experiment where either the images or labels for a specific class were removed from the training data. The student model's performance is compared across three different data budget settings (IPC1, IPC10, and IPC50) under three conditions: (1) a control group where all data and labels are available; (2) a group where images for the target class are removed but the labels for that class are still included in other datapoints; and (3) a group where the labels for the target class are removed but the corresponding images are still included in the training data.The results indicate that the student model can still perform reasonably well when the image data is absent, but its performance severely suffers when class labels are absent. This demonstrates the significance of soft labels in data-efficient learning.", "section": "Learning from (almost) no data"}, {"figure_path": "oNMnR0NJ2e/figures/figures_8_1.jpg", "caption": "Figure 8: Normalized JSD between BPTT-learned labels and ensemble-expert labels on TinyImageNet. Despite not training any experts, distillation method (BPTT) recovers the same labels as those generated by early stopped experts.", "description": "This figure visualizes the Jensen-Shannon Distance (JSD) between soft labels generated by the Back-Propagation Through Time (BPTT) method and those generated by an ensemble of experts on the TinyImageNet dataset.  The heatmaps show the normalized JSD for each image across different expert epochs.  The results indicate that BPTT, without explicit expert training, produces labels similar to those from early-stopped experts, suggesting that BPTT implicitly captures the same label information.", "section": "5.2 Learning soft labels through data distillation methods"}, {"figure_path": "oNMnR0NJ2e/figures/figures_14_1.jpg", "caption": "Figure 9: Selecting images with cross-entropy criteria further improves the soft label baseline performances. We select training images based on cross-entropy scores using epoch-tuned experts, and report the relative student accuracy change when compared to random selection.", "description": "This figure shows the results of an experiment where the authors used cross-entropy to select training images for their soft label baseline method.  They divided the training images into ten quantiles based on their cross-entropy scores (lower scores indicating easier samples). The figure plots the change in student test accuracy compared to the random selection baseline for different cross-entropy quantiles and for three different data budgets (images per class, IPC). The results show a small but consistent improvement when selecting the easiest samples (quantile 1), while selecting the hardest samples significantly hurts performance. This suggests that using cross-entropy for image selection, in addition to soft labels, can further boost the performance of the soft label baseline approach.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_15_1.jpg", "caption": "Figure 10: Visualizing softmax probabilities for each class in Tiny ImageNet Soft labels are generated by pre-trained experts, and they exhibit structures related to semantic similarity.", "description": "This figure visualizes the soft labels generated by a pre-trained expert model on the TinyImageNet dataset.  The heatmap shows the average softmax probabilities for each class, excluding the highest probability class (the diagonal is zeroed out). The resulting visualization highlights clusters of classes with semantic similarities, reflecting relationships between classes as represented in the WordNet ontology. This suggests that the soft labels capture structured semantic information beyond individual class labels, a key finding of the paper.", "section": "4 Good Soft Labels Require Structured Information"}, {"figure_path": "oNMnR0NJ2e/figures/figures_16_1.jpg", "caption": "Figure 11: Empirical Data-Knowledge Scaling Law with expert at epoch 46. We repeat the scaling law experiment in Section 4.3 using a later-epoch expert. The comparison between two experiments shows that more data is needed for the student to fully recover a later-epoch expert.", "description": "This figure shows the results of an experiment designed to explore the relationship between the amount of data and the amount of knowledge needed to achieve a certain level of accuracy in a student model.  The experiment uses a later-epoch expert (epoch 46) compared to the earlier epoch expert used in Figure 6.  The results show that using a later-epoch expert requires significantly more data to achieve the same level of accuracy as using an earlier epoch expert. The data-knowledge scaling law is investigated by varying the size of the dataset (IPC) and the amount of knowledge retained from the expert (top K softmax values).  The figure demonstrates a trade-off between data and knowledge, highlighting how additional knowledge can compensate for smaller datasets, but that this compensation diminishes as the dataset size increases.", "section": "4.3 Trading data with knowledge"}, {"figure_path": "oNMnR0NJ2e/figures/figures_17_1.jpg", "caption": "Figure 8: Normalized JSD between BPTT-learned labels and ensemble-expert labels on TinyImageNet. Despite not training any experts, distillation method (BPTT) recovers the same labels as those generated by early stopped experts.", "description": "This figure shows the Jensen-Shannon Distance (JSD) between labels generated by the BPTT method and labels generated by an ensemble of experts, for different expert epochs and image counts per class (IPC).  The normalized JSD is used to compare the distributions. The results suggest that the BPTT method, without explicitly training experts, is able to recover labels similar to those from early-stopped experts, indicating the information contained in the labels may be a key factor for dataset distillation.", "section": "5.2 Learning soft labels through data distillation methods"}]