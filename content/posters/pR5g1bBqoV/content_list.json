[{"type": "text", "text": "$\\mu\\mathbf{P}^{2}$ : Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Moritz Haas1 Jin Xu2 Volkan Cevher3,4 Leena Chennuru Vankadara4 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1University of T\u00fcbingen, T\u00fcbingen AI Center\u2217 2University of Oxford\u2217   \n3LIONS, EPFL\u2217 4AGI Foundations, Amazon ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets. As models are continually scaled up to improve performance, a rigorous understanding of SAM\u2019s scaling behaviour is paramount. To this end, we study the infinite-width limit of neural networks trained with SAM, using the Tensor Programs framework. Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters. In contrast, we identify a stable parameterization with layerwise perturbation scaling, which we call Maximal Update and Perturbation Parameterization $(\\mu\\mathrm{P^{2}})$ , that ensures all layers are both feature learning and effectively perturbed in the limit. Through experiments with MLPs, ResNets and Vision Transformers, we empirically demonstrate that $\\mu\\mathrm{P^{2}}$ is the first parameterization to achieve hyperparameter transfer of the joint optimum of learning rate and perturbation radius across model scales. Moreover, we provide an intuitive condition to derive $\\mu\\mathrm{P^{2}}$ for other perturbation rules like Adaptive SAM and SAM-ON, also ensuring balanced perturbation effects across all layers. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sharpness Aware Minimization (SAM) (Foret et al., 2021) and its variants (Kwon et al., 2021; M\u00fcller et al., 2024) improves generalization performance across a wide range of neural architectures and datasets (Chen et al., 2021; Kaddour et al., 2022). In the SAM formulation, we minimize a given loss $L$ between our prediction and the data $y$ as a function of the architecture\u2019s weights $W$ , where an adversary simultaneously maximizes the same loss by perturbing the weights within a budget $\\rho$ . ", "page_idx": 0}, {"type": "text", "text": "A standard SAM update for an $L$ -hidden layer multi layer perceptron (MLP) is given by ", "page_idx": 0}, {"type": "equation", "text": "$$\nW_{t+1}^{l}=W_{t}^{l}-\\eta_{l}\\nabla_{W^{l}}\\mathcal{L}\\left(f\\left(\\xi_{t};W_{t}+\\varepsilon_{t}\\right),y_{t}\\right),\\;\\;\\mathrm{with}\\;\\;\\;\\varepsilon_{t}^{l}=\\rho\\cdot\\frac{\\nabla_{W^{l}}\\mathcal{L}\\left(f\\left(\\xi_{t};W_{t}\\right),y_{t}\\right)}{\\left\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\left(f\\left(\\xi_{t};W_{t}\\right),y_{t}\\right)\\right\\|_{F}},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathbf{W}=[W^{1},\\ldots,W^{L+1}]$ , $t$ is the iteration count and $\\varepsilon_{t}^{l}$ denotes the perturbation in the $l_{\\cdot}$ -th MLP layer with width $n\\in\\mathbb N$ , and where we define an $L$ -hidden layer MLP iteratively via ", "page_idx": 0}, {"type": "equation", "text": "$$\nh^{1}(\\xi):=W^{1}\\xi,\\qquad x^{l}(\\xi):=\\phi(h^{l}(\\xi)),\\qquad h^{l+1}(\\xi):=W^{l+1}x^{l}(\\xi),\\qquad f(\\xi):=W^{L+1}x^{L}(\\xi),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "for inputs $\\xi\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ with trainable weight matrices $W^{1}\\in\\mathbb{R}^{n\\times d_{i n}}$ , $W^{l}\\in\\mathbb{R}^{n\\times n}$ for $l\\in[2,L]$ , and $W^{L+1}\\in\\mathbb{R}^{d_{\\mathrm{out}}\\times n}$ . We call $h^{l}$ preactivations, $x^{l}$ activations, and $f(\\xi)$ output function. Despite the inherent difficulty of non-convex, non-concave optimization, SAM is quite successful in practice. ", "page_idx": 0}, {"type": "text", "text": "On the other hand, the steadily growing scale of foundation models has sparked considerable interest in scaling laws of model size and dataset size (Kaplan et al., 2020; Zhai et al., 2022). To rigorously understand learning dynamics under width scaling, Yang and Hu (2021) have recently provided general infinite-width theory for SGD, which has since been shown to be a good model for understanding the properties of large models (Vyas et al., 2024). Yang and Hu (2021) show that standard parameterizations (SP), including He or LeCun initialization (He et al., 2015; LeCun et al., 2002) with a global learning rate, do not learn features in the infinite-width limit. ", "page_idx": 0}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/52e4a9b8834fe2e6eaf5abf846d5958269e352cead4dfbdbfb035eb05cb06917.jpg", "img_caption": ["Figure 1: Left (Only $\\mu\\mathbf{P}^{2}$ transfers both $\\eta$ and $\\rho$ ): Test accuracy as a function of learning rate $\\eta$ and perturbation radius $\\rho$ of a 3-layer MLP trained with SAM on CIFAR10 for various widths and in different parameterizations (see subplot title), averaged over 3 independent runs. $\\mathbf{\\dot{\\mu}}\\times\\mathbf{\\dot{\\mu}}$ denotes the optimum. Blue contours (the darker, the wider) denote the region within $1\\%$ of the optimal test accuracy smoothened with a Gaussian filter. Grey regions (the lighter, the wider) denote the unstable regime below $30\\%$ test accuracy. \u2018naive\u2019 denotes no perturbation scaling, \u2018global\u2019 denotes global perturbation scaling $\\rho=\\Theta(n^{-1/2})$ . Right $(\\mu{\\bf P}^{2}$ achieves the best generalization performance): Same as left but sliced at the optimal learning rate of each parameterization for width 4096. Dashed horizontal lines denote the base optimizer SGD in SP (green) and in $\\mu\\mathrm{P}$ (blue), respectively. Average and $2\\sigma$ -CI from 16 independent runs. SP-LP denotes SP with layerwise perturbation scaling. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Instead, a different scaling of layerwise initialization variances and learning rates, termed Maximal Update Parameterization $(\\mu{\\mathrm{P}})$ , is necessary to achieve feature learning in all layers in wide networks. A crucial practical benefit of $\\mu\\mathrm{P}$ is the transferability of the optimal learning rate across model scales (Yang et al., 2022). This can drastically reduce computational costs as it allows to tune hyperparameters on smaller representative models and then to train the large model only once. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this paper, we adopt a scaling perspective to understand SAM\u2019s learning dynamics. Using the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023), this work provides the first infinite-width theory for SAM with important practical consequences: ", "page_idx": 1}, {"type": "text", "text": "1. We show that training an MLP with the standard (SAM) update rule is equivalent to applying perturbations only in the last layer in the infinite-width limit, even if the perturbation radius is properly tuned. This holds for any width-dependent scaling of layerwise initialization variances and learning rates, including SP and $\\mu\\mathrm{P}.$   \n2. We demonstrate that the optimal perturbation radius can shift significantly in $\\mu\\mathrm{P}$ (Figure 1).   \n3. We postulate that jointly transferring the optimal learning rate $\\eta$ and perturbation radius $\\rho$ requires width-independent feature learning and effective perturbations in every layer in the infinite-width limit. We show that this can be achieved with layerwise scalings of the perturbation radius, and provide a complete characterization of perturbation scaling parameterizations into four regimes: unstable, vanishing, nontrivial and effective perturbations.   \n4. We derive the Maximal Update and Perturbation Parameterization $(\\mu\\mathrm{P^{2}})$ that achieves both feature learning and effective perturbations in all layers in the infinite-width limit. We empirically demonstrate that $\\bar{\\mu}\\bar{\\mathbf{P}}^{2}$ is the first parameterization to achieve hyperparameter transfer in both learning rate $\\eta$ and perturbation radius $\\rho$ (Figure 1).   \n5. We provide a versatile (spectral) scaling condition $(*)$ applicable to architectures such as ResNets and Vision Transformers (ViTs), and to various SAM variants like SAM-ON and Adaptive SAM (ASAM), and any SAM updates modeled in a Tensor Program. ", "page_idx": 1}, {"type": "text", "text": "2 Background and related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We here provide a short summary of related work. A more detailed account is provided in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Sharpness Aware Minimization. SAM was motivated as an inductive bias towards flatter minima and it provably reduces properties of the Hessian that are related to sharpness in simpler settings (Bartlett et al., 2023; Wen et al., 2023; Monzio Compagnoni et al., 2023). However a full understanding of why SAM works so well remains elusive (Andriushchenko et al., 2023b; Wen et al., 2024). For example, applying SAM on only the normalization layers (SAM-ON) often improves generalization further despite increasing sharpness (M\u00fcller et al., 2024). A plethora of SAM variants have recently been proposed with the purpose of even stronger performance or reducing SAM\u2019s computational and memory complexity. We focus on two variants of Adaptive SAM (ASAM) (Kwon et al., 2021) which achieve the overall strongest results in M\u00fcller et al. (2024) (see Appendix F.4 for more details). ", "page_idx": 2}, {"type": "text", "text": "Tensor Programs. We build on the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023; Yang et al., 2022, 2023b), which covers many modern deep learning architectures, optimization algorithms and arbitrary abc-parameterizations. Each abc-parameterization is essentially defined by a layerwise scaling of initialization variance and learning rate as a function of network width. Beyond pure infinite-width limits, the simple\u221a1L- scaling allows depth-scaling in ResNets and unlocks hyperparameter transfer across depths (Hayou et al., 2021; Li et al., 2021; Bordelon et al., 2023; Yang et al., 2023b). Noci et al. (2022, 2024) provide infinite width and depth analyses for Transformers with the goal of preventing rank collapse. ", "page_idx": 2}, {"type": "text", "text": "Look-LayerSAM (Liu et al., 2022) already considers layerwise perturbation scaling with the goal of preserving good performance under large batch training. However, achieving $\\mu\\mathrm{P^{2}}$ with LookLayerSAM requires nontrivial layerwise learning rate and perturbation rescaling (see Appendix B). ", "page_idx": 2}, {"type": "text", "text": "3 SAM induces vanishing perturbations in wide neural networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section shows that under the standard (SAM) update rule, weight perturbations induced by SAM vanish in the infinite-width limit in every layer except the output layer. We later demonstrate that other SAM variants also selectively perturb other subsets of layers. For enhanced readability of some formulae, we use colors to distinguish four regimes of perturbation behaviour: Unstable, vanishing, nontrivial and effective perturbations. ", "page_idx": 2}, {"type": "text", "text": "While our theory covers any stable parameterization including He and LeCun initializations, for concreteness and for the clarity of exposition, we first present our results for MLPs under $\\mu\\mathrm{P}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}^{\\prime1}\\sim\\mathcal{N}(0,1/d_{i n}),\\,\\,W^{l}\\in{\\mathbb R}^{n\\times n}\\sim\\mathcal{N}(0,1/n)\\,\\mathrm{for}\\,l\\in[2,L],\\,\\,W^{L+1}\\sim\\mathcal{N}(0,1/n^{2})}\\\\ &{\\mathrm{\\Lambda}^{\\prime}\\mathrm{erwise~SGD~learning~rates}\\,\\,\\,\\,\\,\\eta_{1}=\\eta n,\\,\\,\\eta_{l}=\\eta,\\,\\,\\mathrm{for}\\,\\,l\\in[2,L],\\,\\,\\eta_{L+1}=\\eta n^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By analyzing the infinite-width behaviour of the SAM update rule, we show that the training dynamics under standard (SAM) become unstable as the network width increases. This result is first stated informally below in Proposition 1 and then more formally in the next section. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 (Instability of standard SAM parameterization in wide neural networks). Under $\\mu P$ with the standard (SAM) update rule and default perturbation given in (SAM), the output function becomes unbounded after the first update step in the infinite-width limit for any fixed, positive learning rate $\\eta>0$ and perturbation radius $\\rho>0$ . ", "page_idx": 2}, {"type": "text", "text": "Hence, to achieve stable optimization, it is necessary to introduce some width-dependent perturbation scaling $\\rho n^{-d}$ for some suitable $d>0$ . To understand the layerwise scaling behaviour of SAM under this scaling, we define the notion of vanishing perturbations. ", "page_idx": 2}, {"type": "text", "text": "Vanishing perturbations. The weight perturbation $\\varepsilon^{l}$ perturbs the $l$ -th layer\u2019s activations as ", "page_idx": 2}, {"type": "equation", "text": "$$\nx^{l}+\\tilde{\\delta}x^{l}=\\phi((W^{l}+\\varepsilon^{l})(x^{l-1}+\\tilde{\\delta}x^{l-1})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tilde{\\delta}\\boldsymbol{x}^{l}$ denotes the perturbation of the $l$ -th layer\u2019s activations accumulated from the weight perturbations $\\{\\varepsilon^{l^{\\prime}}\\}_{l^{\\prime}\\in[l]}$ in all previous layers. We say a layer $l$ has vanishing perturbations if $\\tilde{\\delta}\\boldsymbol{x}^{l}\\,\\rightarrow\\,\\boldsymbol{0}$ as the width approaches infinity. This occurs if the weight perturbations in all previous layers are too small when measured in spectral norm, that is if $\\|\\varepsilon^{l^{\\prime}}\\|_{*}/\\|W^{l^{\\prime}}\\|_{*}\\to0$ for all $l^{\\prime}\\in[l]$ . ", "page_idx": 2}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/08ebeacc437190dfba06298f31a59f69db13929c5989aaaa5cc54eb2cc4a17b2.jpg", "img_caption": ["Figure 2: ((SAM) effectively only perturbs the last layer) Layerwise weight perturbations (top) and normalized activation updates $\\bar{\\|}\\bar{\\Delta}x^{l}\\|_{2}$ (bottom) for SAM, last-layer SAM and SGD as a baseline across widths after training a 3-layer MLP in $\\mu\\mathrm{P}$ with global perturbation scaling $\\rho=\\Theta(n^{-1/2})$ for 20 epochs on CIFAR10. Average and CI are computed from 4 independent runs. Perturbations are normalized by the weight spectral norm to measure their effect on the layer\u2019s output. Activation updates are normalized by $\\bar{\\sqrt{\\dim(\\Delta x^{l})}}$ to measure coordinatewise updates. We provide more neural network statistics in Appendix H.1. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Informally, Proposition 2 below shows that for every choice of a decay parameter $d\\,>\\,0$ , either the training dynamics of SAM are unstable or all the hidden layers of the network have vanishing perturbations in the limit. The formal results are stated in the next section. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Global perturbation scaling is unstable or induces vanishing perturbations). Fix $\\rho>0$ and $t\\in\\mathbb{N}$ . Let ${\\mathring{f}}_{t}$ denote the infinite-width limit of the output function after training an MLP of width $n$ with the SAM update rule (SAM) with perturbation radius $\\rho n^{-d}$ for $t$ steps. If $d<1/2$ , then output perturbations blow up, and ${\\mathring{f}}_{t}$ is unstable. If $d>1/2$ , then the perturbations in all layers vanish and $\\mathring{f}_{t}$ corresponds to the limit after $t$ steps of SGD. If $d=1/2,$ , then only the last layer is effectively perturbed, all other layers have vanishing perturbations. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 shows statistics of an MLP trained with (SAM) with global width-dependent scaling $\\rho n^{-1/2}$ versus the same MLP trained with SAM where only the last-layer weights are perturbed and $\\varepsilon^{l}=0$ for all $l\\in[L]$ . As predicted by Proposition 2, both training algorithms produce equivalent training dynamics, already at moderate width, and last-layer perturbations are scaled correctly. ", "page_idx": 3}, {"type": "text", "text": "Remark 3 (Practical implications). According to Proposition 2, for sufficiently wide models, any performance gains from standard SAM are primarily due to applying the SAM update rule to the last layer even with a properly tuned perturbation radius. This implies that, when applying the standard SAM update rule (SAM), one can remove the inner backward pass beyond the last layer and nearly recover the computational cost of SGD. However, it may be undesirable for optimal generalization if only the last layer is perturbed (Figure 1). \u25c0 ", "page_idx": 3}, {"type": "text", "text": "Layerwise perturbation scaling. In the next section, we show that correcting the (SAM) update rule to achieve effective perturbations in every single layer requires introducing additional hyperparameters \u2014 layerwise width-dependent scaling of the perturbation radius. This is similar in spirit to $\\mu\\mathrm{P}$ which corrects standard parameterization by introducing layerwise scaling of the learning rates. We postulate that achieving width-independent scaling of both updates and perturbations is a necessary condition for hyperparameter transfer under SAM. We also lay all theoretical foundations and derive the stable parameterization, we call maximal update and perturbation parameterization $(\\mu\\mathrm{P^{2}})$ that achieves both feature learning and effective perturbations in all layers in the infinite-width limit. ", "page_idx": 3}, {"type": "text", "text": "Figure 1 shows that $\\mu\\mathrm{P^{2}}$ is indeed the first parameterization to achieve hyperparameter transfer in the optimal joint choice of $(\\eta,\\rho)$ , while also achieving the best generalization performance. ", "page_idx": 3}, {"type": "text", "text": "General perturbation scaling condition. For intuitive understanding and a generalization to other perturbation rules, a simple condition for achieving effective perturbations in any layer follows from our results: in every layer, perturbations should scale like updates in $\\mu\\mathrm{P}$ . ", "page_idx": 3}, {"type": "text", "text": "The reason is that both updates and perturbations are gradient-based $\\nabla_{W^{l}}\\mathcal{L}=\\nabla_{h^{l}}\\mathcal{L}\\cdot(x^{l-1})^{\\top}$ , and thus low-rank and correlated with the incoming activations $x^{l-1}$ . Therefore updates and perturbations introduce the same LLN-like scaling factors, and require the same layerwise scaling corrections. Like Yang et al. (2023a), we can rephrase this condition in terms of weight spectral norms to: For a weight matrix $W_{t}^{l}\\in\\mathbb{R}^{\\mathtt{f a n\\_o u t}\\times\\mathtt{f a n\\_i n}}$ , its update $\\delta W_{t}^{l}$ and its perturbation $\\varepsilon_{t}^{l}$ , it should hold at all times $t$ that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\varepsilon_{t}^{l}\\|_{*}=\\Theta\\left(\\|\\delta W_{t}^{l}\\|_{*}\\right)=\\Theta\\left(\\|W_{t}^{l}\\|_{*}\\right)=\\Theta\\left(\\sqrt{\\mathtt{f}\\mathtt{a}_{\\mathtt{n}-}\\mathtt{o u t}/\\mathtt{f}\\,\\mathtt{a}_{\\mathtt{n}-}\\mathtt{i n}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with big-O notation that only tracks dependence on network width (Definition C.1). We discuss the spectral perspective in more detail in Appendix F.7. ", "page_idx": 4}, {"type": "text", "text": "4 Sharpness Aware Minimization in the infinite-width limit ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Characterization of layerwise perturbation scaling: Unstable, vanishing, nontrivial and effective perturbations ", "page_idx": 4}, {"type": "text", "text": "To systematically and rigorously understand the width-scaling behaviour of neural networks trained under the SAM update rule, we propose a new class of parameterizations, which we refer to as bcdparameterizations. Motivated by the analysis in Section 3, the class of bcd-parameterizations naturally extends abc-parameterizations (Yang and Hu, 2021) by including layerwise scaling of the perturbation radius. By setting all weight multiplier exponents $a_{l}=0$ , we do not need to modify the MLP architecture and recover representatives of each abc-parameterization that capture their essence and condense all equations: Ignoring numerical considerations (Blake et al., 2024), each abc-parameterization is essentially a layerwise initialization and learning rate scaling. The effects of weight multipliers on SAM are more nuanced than for SGD or Adam (see Remark 12 and Appendix F.6). ", "page_idx": 4}, {"type": "text", "text": "To study the infinite-width behaviour of networks trained with SAM in any bcd-parameterization, we utilize the theoretical framework of $\\Nu_{\\mathrm{E}}\\otimes$ OR\u22a4programs (Yang et al., 2023b). We write the two forward and backward passes for each SAM update (ascent/perturbation step then descent/update step) using the NE\u2297OR\u22a4computation rules and rigorously track all relevant scalings as provided by the NE $\\otimes$ OR\u22a4master theorem. All proofs are provided in Appendix E. The full formal result statements can be found in Appendix D. Further theoretical considerations and generalizations around perturbation scaling are provided in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "Assumptions. For clarity of exposition, we present our main results for MLPs. Their extension to other architectures is discussed in Appendix F.5. For all of the results in this section, we assume that the used activation function is either tanh or $\\sigma$ -gelu for $\\sigma>0$ sufficiently small. For small enough $\\sigma\\,>\\,0$ , $\\sigma$ -gelu (Definition C.9) approximates ReLU arbitrarily well. We also assume constant training time as width $n\\to\\infty$ . We assume batch size 1 for clarity, but our results can be extended without further complications to arbitrary fixed batch size as well as differing fixed batch sizes for the ascent/perturbation and the descent/update step, as sometimes used for SAM (Foret et al., 2021). Considering small perturbation batch size is practical, as it has been observed to enhance SAM\u2019s generalization properties (Andriushchenko and Flammarion, 2022). ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (bcd-parametrization). A bcd-parametrization $\\{b_{l}\\}_{l\\in[L+1]}\\;\\cup\\;\\;\\{c_{l}\\}_{l\\in[L+1]}\\;\\;\\mid$ \u222a $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ defines the training of an MLP with SAM in the following way: ", "page_idx": 4}, {"type": "text", "text": "(a) Initialize weights iid as $W_{i j}^{l}\\sim\\mathcal{N}(0,n^{-2b_{l}})$ . (b) Train the weights using the SAM update rule with layerwise learning rates, ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{t+1}^{l}=W_{t}^{l}-\\eta n^{-c_{l}}\\nabla_{W^{l}}\\mathcal{L}\\left(f\\left(\\xi_{t};W_{t}+\\varepsilon_{t}\\right),y_{t}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the scaled perturbation $\\varepsilon_{t}$ via layerwise perturbation radii, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varepsilon_{t}:=\\rho n^{-d}\\frac{v_{t}}{\\|v_{t}\\|},\\quad\\mathrm{with}\\quad v_{t}=(v_{t}^{1},\\dots,v_{t}^{L+1}),\\quad v_{t}^{l}:=n^{-d_{l}}\\cdot\\nabla_{W^{l}}\\mathcal{L}\\big(f(\\xi_{t};W_{t}),y_{t}\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "W.l.o.g. we set $\\|v_{t}\\|=\\Theta(1)$ , which prevents nontrivial width-dependence from the denominator. This imposes the constraints: $d_{1}\\,\\geq\\,1/2-\\operatorname*{min}(b_{L+1},c_{L+1})$ , $d_{l}\\,\\geq\\,1-\\operatorname*{min}(b_{L+1},c_{L+1})$ for $l\\in$ $[2,L]$ , and $d_{L+1}\\,\\geq\\,1/2$ , with at least one equality required to hold (see Appendix E.1.3). The normalization $v_{t}/\\lVert v_{t}\\rVert$ removes one degree of freedom from $\\{d_{l}\\}_{l\\in[L+1]}$ via the equivalence $\\{d_{l}^{\\prime}\\}_{l\\in[L+1]}\\cong\\{d_{l}\\}_{l\\in[L+1]}$ iff there exists a $C\\in\\mathbb{R}$ such that $d_{l}^{\\prime}=d_{l}+C$ for all $l\\in[L+1]$ . ", "page_idx": 4}, {"type": "text", "text": "Stability. To ensure that the training dynamics of SAM are well-behaved with scale, we require bcd-parameterizations to satisfy conditions of stability. Perturbed weights $\\tilde{W}^{l}=W^{l}+\\varepsilon^{l}$ induce perturbed activations $x^{l}+\\tilde{\\delta}x^{l}$ and a perturbed output function $\\tilde{f}_{t}(\\xi):=f_{\\tilde{W}_{t}}(\\xi)$ . We call a bcdparameterization stable (Definition C.3) if the hidden activations have width-independent scaling $\\Theta(1)$ at initialization and during training, and neither the updates nor the perturbations $\\tilde{\\delta}\\boldsymbol{x}^{l}$ of the activations or output logits $\\tilde{f}_{t}-f_{t}$ blow up at any point in training. ", "page_idx": 5}, {"type": "text", "text": "For stating the conditions that characterize the class of stable $b c d$ -parameterizations, we define the maximal feature perturbation scaling $\\tilde{r}$ of a bcd-parameterization as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{r}:=\\operatorname*{min}(b_{L+1},c_{L+1})+d+\\operatorname*{min}_{l=1}^{L}(d_{l}-\\mathbb{I}(l\\neq1)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similar to the maximal feature update scaling $r$ from Yang and Hu (2021), $\\tilde{r}$ describes how much the last hidden-layer activations are perturbed as a function of width, $x^{L}+\\tilde{\\delta}x^{L}=\\Theta(n^{-\\tilde{r}})$ . Hidden-layer activation perturbations do not explode with width if and only if $\\tilde{r}\\geq0$ . The output perturbations not to blow up if and only if $d+d_{L+1}\\geq1$ and $b_{L+1}+\\tilde{r}\\ge1$ . In particular, this implies that any stable $b c$ -parameterization together with naive perturbation scaling $d_{l}=d=0$ for all $l\\in[L+1]$ is unstable due to blowup in the last layer. We formally state the stability characterization in Theorem D.2. Ideally, we will later require width-independent perturbation scaling which is attained iff $\\tilde{r}=0$ . ", "page_idx": 5}, {"type": "text", "text": "Effective SGD dynamics. Within the class of stable parameterizations, there are parameterizations in which perturbations in the output vanish in the infinite-width limit at any point during training. In other words, SAM training dynamics collapses to SGD dynamics with scale. We are mostly interested in the opposing class of parameterizations with non-vanishing perturbations. We characterize this class in Theorem 6 and refer to them as perturbation nontrivial (Definition 5). ", "page_idx": 5}, {"type": "text", "text": "Definition 5 (Perturbation nontriviality). We say that a stable bcd-parametrization is perturbation nontrivial if there exists a training routine, $t_{\\mathrm{~\\rightmoon~}}\\in\\mathrm{~\\mathbb~N_{0}~}$ and $\\xi_{\\mathrm{~\\scriptsize~\\in~\\mathbb~{~R~}~}}\\mathbb{R}^{d_{\\mathrm{in}}}$ such that $\\tilde{\\delta}f_{t}(\\xi):=f_{\\tilde{W}_{t}}(\\xi)-f_{W_{t}}(\\xi)=\\Omega(1)$ . Otherwise, the bcd-parametrization is perturbation trivial. $\\blacktriangleleft$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 6 (Perturbation nontriviality characterization). A stable bcd-parametrization is perturbation nontrivial if and only if $d+d_{L+1}=1\\,o r\\,\\mathrm{min}(b_{L+1},c_{L+1})+\\tilde{r}=1.$ . ", "page_idx": 5}, {"type": "text", "text": "For the class of stable and perturbation nontrivial $b c d$ -parameterizations, SAM learning is both stable and deviates from SGD dynamics. A natural question to ask here is: what should be the ideal SAM behaviour in the infinite-width limit? To address this question, we make the following crucial distinction between non-vanishing and effective perturbations. ", "page_idx": 5}, {"type": "text", "text": "Non-vanishing versus effective perturbations. Recall that the weight perturbation $\\varepsilon^{l}$ perturbs the $l$ -th layer\u2019s activations as ", "page_idx": 5}, {"type": "equation", "text": "$$\nx^{l}+\\tilde{\\delta}x^{l}=\\phi((W^{l}+\\varepsilon^{l})(x^{l-1}+\\tilde{\\delta}x^{l-1})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\delta}\\boldsymbol{x}^{l}$ denotes the perturbation of the $l$ -th layer\u2019s activations accumulated from the weight perturbations $\\{\\varepsilon^{l^{\\prime}}\\}_{l^{\\prime}\\in[l]}$ in all previous layers. Therefore, perturbations $\\tilde{\\delta}x^{l}$ can stem both from weight perturbations $\\varepsilon^{l^{\\prime}}$ in a previous layer $l^{\\prime}<l$ and/or from weight perturbations $\\varepsilon^{l}$ in the current layer $l$ . Intuitively, if we perturb a layer, we want this to affect the next layer\u2019s activations and thereby have a nontrivial effect on the output function. Otherwise one can simply set the layer\u2019s perturbations to 0 by design and not change the learning algorithm in the infinite-width limit. This motivates the definition of effective perturbations, which demands the weight perturbations of the current layer to contribute non-vanishingly. From the weight perspective $(*)$ , effective $l$ -th layer perturbations are achieved if and only if weight perturbations scale like the weights in spectral norm, $\\lVert\\epsilon^{l}\\rVert_{*}/\\lVert W^{l}\\rVert_{*}\\,=\\,\\Theta(1)$ . Without an effective perturbation $\\varepsilon^{l}$ of the $l$ -th layer, this layer does not inherit SAM\u2019s inductive bias towards low spectral norm of the Hessian or enhanced sparsity and does not improve generalization performance. We provide empirical evidence for these claims in Appendix H.2. Therefore a distinction between non-vanishing and effective perturbations is crucial. ", "page_idx": 5}, {"type": "text", "text": "Definition 7 (Non-vanishing perturbations). For $l\\in[L]$ , we say that a stable parameterization has non-vanishing perturbations in the $l$ -th layer if there exists a $t\\in\\mathbb{N}$ such that $\\tilde{\\delta}x_{t}^{l}=\\Omega(1)$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 8 (Effective perturbations). For $l\\,\\in\\,[L+1]$ , we say that a stable parameterization effectively perturbs the $l$ -th layer if there exists a $t\\in\\mathbb{N}$ such that $\\varepsilon_{t}^{l}(x_{t}^{l-1}+\\tilde{\\delta}x_{t}^{l-1})=\\Theta(1)$ , where $\\boldsymbol{x}_{t}^{0}+\\tilde{\\delta}\\boldsymbol{x}_{t}^{0}=\\boldsymbol{\\xi}_{t}$ . \u25c0 ", "page_idx": 5}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/e3eb75b27ba59e4618134010f962990279cb4802f32d12d662c7fbf81e88a947.jpg", "img_caption": ["Figure 3: (Perturbation phase characterization of bcd-parameterizations) Given a choice of layerwise initialization and learning rate scalings $\\{b_{l},c_{l}\\}_{l\\in[L+1]}$ , the maximal feature perturbation scaling $\\tilde{r}$ and the last-layer perturbation scaling $d+d_{L+1}$ completely determine whether a bcdparameterization is unstable, has effective SGD dynamics, effective perturbations in some but not all layers or effective perturbations in all layers. In SP or NTP (left), there does not exist a choice of perturbation scalings that achieves effective perturbations in all layers, whereas in $\\mu\\mathrm{P}$ (right), there is a unique choice as provided in Theorem 11. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 9 provides a characterization of stable $b c d\\cdot$ -parameterizations with vanishing perturbations in any given layer. ", "page_idx": 6}, {"type": "text", "text": "Theorem 9 (Vanishing perturbation characterization). For any $l_{0}\\in[L].$ , the following statements are equivalent: ", "page_idx": 6}, {"type": "text", "text": "(a) A stable bcd-parametrization has vanishing perturbations in layer $l_{0}$ . (b) A stable bcd-parametrization has vanishing perturbations in layer l for all $1\\leq l\\leq l_{0}$ . (c) $\\begin{array}{r}{\\tilde{r}_{l_{0}}:=\\operatorname*{min}(\\bar{b}_{L+1},c_{L+1})+d+\\operatorname*{min}_{m=1}^{l_{0}}(\\bar{d_{m}}-\\mathbb{I}(m\\neq1))>0.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "It follows from Theorem 9 that any stable bcd-parameterization that performs updates in the original gradient direction (i.e., $d_{l}=C$ for all $l\\in[L+1]$ for some $C\\in\\mathbb{R}$ ) has vanishing perturbations in all input and hidden layers $l\\in[L]$ , and the last layer $l=L+1$ is effectively perturbed if and only if $d=1/2$ . This covers the case of both standard and maximal update parameterizations with global scaling of the perturbation radius discussed in Section 3. Negating the conditions of Theorem 9 implies that a stable $b c d$ -parameterization has non-vanishing perturbations in layer $l_{0}$ if and only if $\\tilde{r}_{l_{0}}=0$ . Achieving effective perturbations is a stronger requirement for which Theorem 10 provides the necessary and sufficient conditions. ", "page_idx": 6}, {"type": "text", "text": "Theorem 10 (Effective perturbation characterization). For $l\\in[L]$ , a stable bcd-parametrization effectively perturbs the l-th layer if and only $i f\\operatorname*{min}(b_{L+1},c_{L+1})+{\\bar{d}}+d_{l}-\\mathbb{I}(l\\neq1)=0$ . ", "page_idx": 6}, {"type": "text", "text": "A stable bcd-parametrization effectively perturbs the last layer if and only if $d+d_{L+1}=1$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Maximal Update and Perturbation Parameterization $(\\mu\\mathbf{P}^{2})$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We postulate that just as the optimal learning rate transfers across widths under $\\mu\\mathrm{P}$ for SGD and Adam due to non-vanishing width-independent feature evolution in all layers, the optimal learning rate and perturbation radius may be jointly transferable across widths if additionally the weight perturbations induce width-independent perturbations of the activations in all layers. Here, we show that, for every stable initialization and learning rate scaling with $b_{L+1}\\geq1$ , there exists a unique stable layerwise perturbation scaling which effectively perturbs every single layer. We term this layerwise perturbation scaling $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ the Maximal Perturbation Parameterization (MPP). This concludes the phase characterization of perturbation scaling behaviours (Figure 3). ", "page_idx": 6}, {"type": "text", "text": "Theorem 11 (Maximal Perturbation Parameterization (MPP)). Consider any stable bcdparametrization $\\{b_{l}\\}_{l\\in[L+1]}\\cup\\{c_{l}\\}_{l\\in[L+1]}\\cup\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ . If $b_{L+1}\\,<\\,1$ , then there does not exist a stable choice of $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ that achieves effective perturbations before the last layer. If $b_{L+1}\\geq1$ , then up to the equivalence $d_{l}^{\\prime}=d_{l}+C,\\,C\\in\\mathbb{R},\\,\\forall l\\in[L+1],$ , the unique stable choice $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ that effectively perturbs all layers $l\\in[L+1]$ is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nd=-1/2,\\qquad d_{l}=\\left\\{\\begin{array}{l l}{1/2-\\operatorname*{min}(b_{L+1},c_{L+1})}&{l=1,}\\\\ {3/2-\\operatorname*{min}(b_{L+1},c_{L+1})}&{l\\in[2,L],}\\\\ {3/2}&{l=L+1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/604ab2079fc81fb7ebaf90924d727d6f7282004d4394126265bcd753b42b85ba.jpg", "table_caption": [], "table_footnote": ["Table 1: (Layerwise perturbation scaling for effective perturbations in $\\mu\\mathbf{P}_{.}$ ) Without layerwise perturbation scaling (left), each SAM variant perturbs a different subset of layers at large width $n\\to\\infty$ , but we provide the unique layerwise perturbation rescaling $\\mu\\mathrm{P^{2}}$ (right) that achieves effective perturbations in all layers. This parameterization transfers both the optimal $\\eta$ and $\\rho$ across widths. "], "page_idx": 7}, {"type": "text", "text": "Maximal Update and Perturbation Parameterization $\\mu\\mathbf{P}^{2}$ . To achieve feature learning in every layer and hyperparameter transfer in the learning rate, $\\mu\\mathrm{P}$ is the unique2 choice of layerwise initialization variance and learning rate scalings $\\{b_{l},c_{l}\\}_{l\\in[L+1]}$ (Yang and Hu, 2021). Together with Theorem 11, this shows that there exists a unique2 bcd-parameterization that achieves both feature learning and effective perturbations in all layers, we call maximal update and perturbation parametrization, $\\mu\\mathrm{P^{2}}$ for short. Now that we have found a parameterization that achieves widthindependent scaling of both activation updates and activation perturbations, $\\mu\\mathrm{P^{2}}$ fulfills essential necessary conditions for hyperparameter transfer to occur in both $\\eta$ and $\\rho$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 12 (Achieving $\\mu\\mathbf{P}^{2}$ with weight multipliers). Appendix F.6 covers the extension of our results to nontrivial weight multipliers. We show that, for each choice of weight multipliers $\\{a_{l}\\}_{l\\in[L+1]}$ , there is a unique2 choice of bcd-hyperparameters that achieves effective perturbations in all layers. But unlike for SGD or Adam, these parameterizations lead to slightly different training algorithms, because differing subsets of layers contribute non-vanishingly to the joint gradient normalization term $\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|_{F}$ in (SAM). The term $\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|_{F}$ couples all layers so that there do not exist layerwise but only layer-coupled equivalence classes for (SAM). Most importantly, instead of adapting (SAM), we can adapt the architecture with the weight multipliers $n^{-a_{l}}\\cdot\\dot{W}^{l}$ with ", "page_idx": 7}, {"type": "equation", "text": "$$\na_{l}=-1/2\\cdot\\mathbb{I}(l=1)+1/2\\cdot\\mathbb{I}(l=L+1)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$(a{-}\\mu P^{2})$ ", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "to achieve effective perturbations in all layers with naive perturbation scaling such that all layers contribute non-vanishingly to the joint gradient norm (Appendix F.6). Unfortunately, this choice of weight multipliers is not compatible unit scaling considerations (Blake et al., 2024). \u25c0 ", "page_idx": 7}, {"type": "text", "text": "Alternative perturbation scaling definitions. Scaling equivalent to $(a{-}\\mu P^{2})$ can be achieved without multipliers by scaling the numerator and denominator terms in (LP) independently, and choosing to scale all denominator terms to be width-independent (see perturbation rule (DP) and Appendix F.7 for more details). The ablations in Appendix H.4 suggest that this has a negligible effect on the optimal generalization performance of $\\bar{\\mu}\\mathrm{P^{2}}$ , but can be more stable given suboptimal hyperparameters. Gradient normalization in each layer separately is uncommon and performs slightly worse (Appendix H.5). Appendix F.2 discusses further considerations that led to Definition 4. ", "page_idx": 7}, {"type": "text", "text": "Trivial, lazy, and feature learning regimes. A small last-layer initialization variance $b_{L+1}\\geq1$ is required for stable feature learning. Theorem 11 shows that $b_{L+1}\\geq1$ is also required for effective hidden-layer perturbations. Beyond this condition, the choice of $\\{b_{l}\\}$ and $\\{c_{l}\\}$ is decoupled from that of perturbation scalings $\\{\\bar{d}_{l}\\}\\cup\\{d\\}$ for stable $b c d$ -parameterizations, because the scale of the activations of a layer $l$ is entirely determined by the scale of initialization $b_{l}$ and learning rates $c_{l}$ , given stability. Consequently, whether a parameterization is trivial, in the lazy regime, or in the feature learning regime is independent of the choice of $d_{l}$ \u2019s provided that all stability constraints are met. A complete characterization of these regimes for the class of $b c$ -parameterizations has been provided in Yang and Hu (2021) and remains unchanged for the class of stable $b c d\\!.$ -parameterizations. For completeness, formal definitions and the corresponding results are stated in Appendices C and D. ", "page_idx": 7}, {"type": "text", "text": "4.3 Generalizations to other architectures and SAM variants ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Generalization to other architectures. Our results can be extended to other common layer types, that are representable as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ program, including all ResNet and Transformer components (Appendix F.5). All considered layer types behave like input, hidden or output layers. Most importantly, normalization layer weights and biases scale like input layer weights to the input 1. ", "page_idx": 8}, {"type": "text", "text": "Generalization to other SAM variants. We would like to find the correct layerwise perturbation scaling without writing out the NE\u2297OR\u22a4program for every perturbation rule individually. Formally justified by our proof in Appendix E, we rephrase our equivalent spectral scaling condition $(*)$ from Section 3 to: maximal stable perturbations are achieved in $\\mu\\mathrm{P}$ if and only if $\\varepsilon^{l}=\\Theta(\\delta W^{l})$ . This condition holds as soon as weight updates $\\delta W^{l}$ and perturbations $\\varepsilon^{l}$ are both correlated with the incoming activations $x^{l-1}$ , for example if both are gradient-based. Table 1 summarizes the application of this condition to two ASAM variants that perform well empirically but cannot be written as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program. Additional details are provided in Appendix F.4. We demonstrate that these scalings perform well and transfer hyperparameters in the next section. Note that for hidden layers in $\\mu\\mathrm{P^{2}}$ , it holds that $\\varepsilon^{l}=\\Theta(n^{-1})$ but $W^{l}=\\Theta(n^{-1/2})$ entrywise, due to large initialization, showing that it is crucial to compare perturbations to updates or to measure weight scalings in spectral norm. ", "page_idx": 8}, {"type": "text", "text": "5 The maximal update and perturbation parameterization $\\mu\\mathbf{P}^{2}$ achieves hyperparameter transfer and improved generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide experimental results showing that $\\mu\\mathrm{P^{2}}$ is the first parameterization to achieve hyperparameter transfer in both $\\eta$ and $\\rho$ across architectures, and that $\\mu\\mathrm{P^{2}}$ also improves generalization \u2013 even after multi-epoch training to convergence. We train MLPs and ResNets (He et al., 2016) on CIFAR10 (Krizhevsky et al., 2009) and Vision Transformers (ViTs) (Dosovitskiy et al., 2021) on Imagenet1K (Deng et al., 2009). While we directly implement bcd-parameterizations for MLPs and ResNets in PyTorch (Paszke et al., 2019), we use the mup-package (Yang et al., 2022) as a basis for ViT experiments. Pseudocode and a spectral derivation of our $\\mu\\mathrm{{P}^{\\breve{2}}}$ -implementation for ViTs, which is equivalent to $(a{-}\\mu P^{2})$ , are provided in Appendix F.7. All experimental details are stated in Appendix G and all supplemental experiments can be found in Appendix H. ", "page_idx": 8}, {"type": "text", "text": "Comparing candidate parameterizations in MLPs. Figure 1 shows test accuracy as a function of learning rate and perturbation radius for MLPs of varying width. While previous $\\mu\\mathrm{P}$ -literature mostly focuses on the more immediate transfer in training error, for SAM it is crucial to consider optimality in test error as the perturbation radius acts as a regularizer, so that optimality in test error typically coincides with suboptimal training error. In SP, the regime of stable learning rates shrinks with width. In $\\mu\\mathrm{P}$ without perturbation scaling, the regime of stable perturbation radii shrinks. In $\\mu\\mathrm{P}$ with global perturbation scaling, the regime of stable $\\rho$ remains invariant under width scaling, but there is no significant improvement of SAM beyond SGD, so that the optimal perturbation radius fluctuates within its stable regime due to noise. Only $\\mu\\mathrm{P^{2}}$ consistently achieves hyperparameter transfer across widths, and achieves significant improvement over its base optimizer SGD in $\\mu\\mathrm{P}$ at scale. The full hyperparameter landscapes are provided in Appendix H.3. ", "page_idx": 8}, {"type": "text", "text": "$\\rho$ -transfer in ViTs. Figure 4 shows that the optimal perturbation radius transfers for ViT-S/16 on Imagenet1K trained with SAM in $\\mu\\mathrm{P^{2}}$ . While Andriushchenko and Flammarion (2022, Appendix E.3) observe diminishing benefits of SAM at large widths in SP, here the improvements beyond the base optimizer AdamW in $\\mu\\mathrm{P}$ are particularly large. ", "page_idx": 8}, {"type": "text", "text": "$\\rho$ -transfer for SAM variants in $\\mu\\mathbf{P}^{2}$ . Figure 6 shows that training a ResNet-18 in $\\mu\\mathrm{P^{2}}$ achieves hyperparameter transfer in $\\rho$ for all considered SAM variants with varying width. $\\mu\\mathrm{P}$ with global perturbation scaling $\\textstyle{\\left|\\mu\\mathbf{P}\\right|}$ -global) has a width-invariant stability threshold in $\\rho$ and the optimal $\\rho$ clearly shifts toward that threshold. It would be interesting to see whether this shift continues with larger width and leads to suboptimal performance of $\\mu\\mathrm{P}$ -global in wider ", "page_idx": 8}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/74196d1c79a6c19423d75ce3694743eb0e249c2c9d0fb3e627b30b3cc84df9b0.jpg", "img_caption": ["Figure 4: ( $\\bar{\\rho}$ -transfer in ViTs) Training a ViT with SAM in $\\mu\\mathrm{P^{2}}$ on ImageNet1K from scratch for 100 epochs yields $\\rho$ - transfer and large improvements over AdamW in $\\mu\\mathrm{P}$ (dashed lines). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/a0924740ede23f13e955f6d1e66d5fe3dc03e3faa43d85488749a1357be99aaa.jpg", "img_caption": ["Figure 6: $\\dot{\\boldsymbol{\\rho}}$ -transfer of ASAM variants in $\\mu\\mathbf{P}^{2}$ ) Test error as a function of perturbation radius $\\rho$ after 200 epochs of training a ResNet-18 in $\\mu\\mathrm{P^{2}}$ on CIFAR10 with various SAM variants (see subplot title). CI over 2 independent runs. Darker lines correspond to larger width multipliers. Other hyperparameters are tuned at base width multiplier 0.5. $\\mu\\mathrm{{P^{2}}}$ achieves transfer in $\\rho$ and large improvements over the base optimizer (dashed lines) SGD in $\\mu\\mathrm{P}$ with momentum and weight decay. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/db52a2b68b297ee9df23f6b8318c31774b842cde7cea69756bb1b68b2e615078.jpg", "table_caption": ["Table 2: (Performance of $\\overline{{\\mu\\mathbf{P}^{2}}}$ ) Average test accuracy $\\pm$ standard deviation across 4 runs ( $^{+}$ improvement of SAM over SGD) for ResNet-18 with width multiplier 4 on CIFAR10 using SGD as a base optimizer. In bold, all parameterizations within a $2\\sigma$ -CI from the best-performing variant SAM-ON in $\\overline{{\\mu}}\\mathrm{P^{2}}$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "ResNets. Table 2 shows that all SAM variants perform similarly well in $\\mu\\mathrm{P^{2}}$ , some slightly outperforming the best-performing variant SAM-ON in SP. This suggests that for ResNets, even with a proper layerwise balance, normalization layer perturbations may suffice, and performance differences in SP are primarily caused by varying degrees to which the normalization layers are perturbed. ", "page_idx": 9}, {"type": "text", "text": "Without providing an explanation, M\u00fcller et al. (2024, Section 5.3) observe that only SAM-ON and elementwise ASAM sufficiently perturb normalization layers in SP. Table 1 (left) explains these observations by showing that only these two SAM variants effectively perturb normalization layers under global perturbation scaling. Table 1 (right) also provides full control over which layers to perturb. For transferring the optimal $\\rho$ with SAM-ON in $\\mu\\mathrm{P},$ our theory predicts the global scaling $\\rho=\\Theta(n^{1/2})$ which is confirmed by our empirical observations (Figure 6). However, properly understanding the role of normalization layer perturbations remains an important question for future work. Note that we report results after fine-tuning all hyperparameters. The performance gain of $\\mu\\mathrm{P^{2}}$ over ", "page_idx": 9}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/15c43e53cdc7c1b12dbc74e045b4b322c0619f7832ca9a251cc72796ade19674.jpg", "img_caption": ["Figure 5: (Stable training dynamics) SAM in $\\mu\\mathrm{P^{2}}$ stabilizes training dynamics for a ResNet-18 with width multiplier 2. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "SP and $\\mu\\mathrm{P}$ -global is likely much higher in larger models, for which fine-tuning is infeasible and the lack of feature learning and effective perturbations is more pronounced. Even under optimal HPs, $\\mu\\mathrm{P^{2}}$ appears to stabilize SAM\u2019s training dynamics compared to SP (Figure 5). ", "page_idx": 9}, {"type": "text", "text": "6 Future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study may serve as an inspiration of how scaling theory can be used to understand and improve training procedures in minimax optimization and beyond. To reach a fully practical theory of deep learning, it will be necessary to take data distributions and training dynamics into account in more detail than it is possible with current Tensor Program theory (Everett et al., 2024). Existing Tensor Program theory assumes constant batch size and training time, and does not make statements about generalization. For example, we observe that ResNets in SP can sometimes display HP transfer in $\\eta$ and $\\rho$ after training to convergence (Appendix H.3.2). This contradicts the infinite-width theory from Yang and Hu (2021) which predicts output blowup under large learning rates, and it shows that the exact conditions which enable hyperparameter transfer in practice are not fully understood. It also remains unclear how to optimally adapt (SAM) when increasing network depth. We plan to address some of these questions in upcoming work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In International Conference on Machine Learning (ICML), 2022. Cited on page 5, 9, 17, 37, 38, 48. ", "page_idx": 10}, {"type": "text", "text": "Maksym Andriushchenko, Dara Bahri, Hossein Mobahi, and Nicolas Flammarion. Sharpness-aware minimization leads to low-rank features. arXiv:2305.16292, 2023a. Cited on page 38, 53.   \nMaksym Andriushchenko, Francesco Croce, Maximilian M\u00fcller, Matthias Hein, and Nicolas Flammarion. A modern look at the relationship between sharpness and generalization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning (ICML), 2023b. Cited on page 3, 17.   \nSanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. Cited on page 17.   \nPeter L. Bartlett, Philip M. Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. Journal of Machine Learning Research (JMLR), 24(316):1\u201336, 2023. Cited on page 3, 17.   \nJeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between two neural networks and the stability of learning. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020. Cited on page 17.   \nTamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication attempt. arXiv:2404.10102, 2024. Cited on page 17.   \nCharlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y Prince, Bj\u00f6rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, and Douglas Orr. u- $\\mu\\mathrm{P}$ : The unit-scaled maximal update parametrization. arXiv:2407.17465, 2024. Cited on page 5, 8, 45, 49.   \nBlake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit. arXiv:2309.16620, 2023. Cited on page 3, 16.   \nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. arXiv:2106.01548, 2021. Cited on page 1, 17.   \nJeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations (ICLR), 2020. Cited on page 17.   \nYan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware minimization. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Cited on page 17, 39.   \nYann N Dauphin, Atish Agarwala, and Hossein Mobahi. Neglected hessian component explains mysteries in sharpness regularization. arXiv:2401.10809, 2024. Cited on page 17.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. Cited on page 9, 51, 70.   \nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. Cited on page 17.   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Cited on page 9.   \nKatie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. Cited on page 10, 49.   \nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations (ICLR), 2021. Cited on page 1, 5, 17, 37, 48.   \nBoris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018. Cited on page 16.   \nSoufiane Hayou and Greg Yang. Width and depth limits commute in residual networks. In International Conference on Machine Learning (ICML), 2023. Cited on page 17.   \nSoufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith Rousseau. Stable resnet. In International Conference on Artificial Intelligence and Statistics, 2021. Cited on page 3, 16.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In IEEE international conference on computer vision (ICCV), 2015. Cited on page 2, 51.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Cited on page 9.   \nSepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Comput., 9(1):1\u201342, 1997. Cited on page 17.   \nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv:2203.15556, 2022. Cited on page 17.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural Tangent Kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Cited on page 16.   \nYiding Jiang\\*, Behnam Neyshabur\\*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations (ICLR), 2020. Cited on page 17.   \nJean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. When do flat minima optimizers work? Advances in Neural Information Processing Systems (NeurIPS), 35, 2022. Cited on page 1, 17.   \nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020. Cited on page 1, 17.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Cited on page 9, 51, 70.   \nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning (ICML), 2021. Cited on page 1, 3, 17, 37, 39, 48.   \nYann LeCun, L\u00e9on Bottou, Genevieve B Orr, and Klaus-Robert M\u00fcller. Efficient backprop. In Neural networks: Tricks of the trade. Springer, 2002. Cited on page 2.   \nMufan Li, Mihai Nica, and Dan Roy. The future is log-gaussian: Resnets and their infinite-depth-andwidth limit at initialization. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Cited on page 3, 16. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Cited on page 3, 17. ", "page_idx": 12}, {"type": "text", "text": "Philip M. Long and Peter L. Bartlett. Sharpness-aware minimization and the edge of stability. arXiv:2309.12488, 2023. Cited on page 17. ", "page_idx": 12}, {"type": "text", "text": "Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018. Cited on page 16. ", "page_idx": 12}, {"type": "text", "text": "Enea Monzio Compagnoni, Luca Biggio, Antonio Orvieto, Frank Norbert Proske, Hans Kersting, and Aurelien Lucchi. An SDE for modeling SAM: Theory and insights. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. Cited on page 3, 17, 39. ", "page_idx": 12}, {"type": "text", "text": "Maximilian M\u00fcller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that sharpness-aware minimization needs. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Cited on page 1, 3, 10, 17, 35, 37, 39, 41, 48, 51, 52. ", "page_idx": 12}, {"type": "text", "text": "Radford M. Neal. Priors for Infinite Networks. Springer New York, 1996. Cited on page 16. ", "page_idx": 12}, {"type": "text", "text": "Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems (NeurIPS), 35, 2022. Cited on page 3, 16. ", "page_idx": 12}, {"type": "text", "text": "Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan Roy. The shaped transformer: Attention models in the infinite depth-and-width limit. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Cited on page 3, 16. ", "page_idx": 12}, {"type": "text", "text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Cited on page 9, 70. ", "page_idx": 12}, {"type": "text", "text": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in Neural Information Processing Systems (NeurIPS), 29, 2016. Cited on page 16. ", "page_idx": 12}, {"type": "text", "text": "David Samuel. (Adaptive) SAM Optimizer (PyTorch). https://github.com/davda54/sam, 2022. Cited on page 37, 48, 49, 70. ", "page_idx": 12}, {"type": "text", "text": "Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv:1611.01232, 2016. Cited on page 16. ", "page_idx": 12}, {"type": "text", "text": "Sungbin Shin, Dongyeop Lee, Maksym Andriushchenko, and Namhoon Lee. The effects of overparameterization on sharpness-aware minimization: An empirical and theoretical analysis. arXiv:2311.17539, 2023. Cited on page 17. ", "page_idx": 12}, {"type": "text", "text": "Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Cited on page 2, 16, 60. ", "page_idx": 12}, {"type": "text", "text": "Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How sharpness-aware minimization minimizes sharpness? In The Eleventh International Conference on Learning Representations (ICLR), 2023. Cited on page 3, 17. ", "page_idx": 12}, {"type": "text", "text": "Kaiyue Wen, Zhiyuan Li, and Tengyu Ma. Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Cited on page 3, 17. ", "page_idx": 12}, {"type": "text", "text": "Jonathan Wenger, Felix Dangel, and Agustinus Kristiadi. On the disconnect between theory and practice of overparametrized neural networks. arXiv:2310.00137, 2023. Cited on page 16. ", "page_idx": 13}, {"type": "text", "text": "Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arXiv:2006.03677, 2020. Cited on page 51.   \nLechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generalization in deep neural networks. In International Conference on Machine Learning (ICML), 2020. Cited on page 16.   \nGreg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. Cited on page 2, 3, 16, 42.   \nGreg Yang. Tensor programs iii: Neural matrix laws. arXiv:2009.10685, 2021. Cited on page 24.   \nGreg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In International Conference on Machine Learning (ICML), 2021. Cited on page 2, 3, 5, 6, 8, 10, 16, 18, 20, 21, 29, 31, 32, 33, 41.   \nGreg Yang and Etai Littwin. Tensor programs ivb: Adaptive optimization in the infinite-width limit. arXiv:2308.01814, 2023. Cited on page 2, 3, 16, 17, 19, 41.   \nGreg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv:2203.03466, 2022. Cited on page 2, 3, 9, 16, 19, 43, 51, 57, 60, 70.   \nGreg Yang, James B. Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv:2310.17813, 2023a. Cited on page 5, 17, 39, 47.   \nGreg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks. arXiv:2310.02244, 2023b. Cited on page 3, 5, 16, 17, 43.   \nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations (ICLR), 2020. Cited on page 17.   \nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Cited on page 1, 17. ", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Appendix Contents. ", "page_idx": 14}, {"type": "text", "text": "A Notation 16 ", "page_idx": 14}, {"type": "text", "text": "B Detailed related work 16 ", "page_idx": 14}, {"type": "text", "text": "C Definitions 18 ", "page_idx": 14}, {"type": "text", "text": "D Extensive main results 19 ", "page_idx": 14}, {"type": "text", "text": "E Proof of main results 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Tensor program formulation 23   \nE.2 The infinite-width limit 29   \nE.3 Concluding the proof of all main results 31   \nE.4 Analytic expression of the features after first SAM update . 33 ", "page_idx": 14}, {"type": "text", "text": "F Generalizations and further perturbation scaling considerations 35 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Overview over choices of $d_{l}$ and $d$ 35   \nF.2 Other ways to introduce layerwise perturbation scaling 37   \nF.3 Extension to SAM without gradient normalization . 38   \nF.4 Extension to Adaptive SAM 39   \nF.5 Representing general architectures and adaptive optimizers as Tensor Programs 41   \nF.6 Influence of width-dependent weight multipliers on bcd-parameterizations . 43   \nF.7 The spectral perspective on $\\mu P^{2}$ 47 ", "page_idx": 14}, {"type": "text", "text": "G Experimental details 51 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H Supplemental experiments 53 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H.1 SAM is approximately LL-SAM in $\\mu\\mathrm{P}$ with global perturbation scaling 53   \nH.2 Propagating perturbations from the first layer does not inherit SAM\u2019s benefits 53   \nH.3 Hyperparameter transfer 57   \nH.4 Gradient norm contributions have negligible effects on generalization performance 63   \nH.5 SAM with layerwise gradient normalization . . 64   \nH.6 Test error over the course of training 65 ", "page_idx": 14}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/eefb48f487cf53245add3944e6d39854f394b12643979283cdb29b70d638835b.jpg", "table_caption": [], "table_footnote": ["Table A.1: (Notation) Overview over notation used in the main paper (top) and in the appendix (bottom). "], "page_idx": 15}, {"type": "text", "text": "B Detailed related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Signal propagation. Our work can be seen as scaling theory with the goal of preventing both vanishing and exploding signals in forward and backward passes, where the analysis of SAM requires considering stability of perturbations in each layer as well. In this sense, we build on a rich literature, often restricted to an analysis at initialization (Schoenholz et al., 2016; Poole et al., 2016; Hanin and Rolnick, 2018; Xiao et al., 2020). For scaling neural networks to infinite depth, residual connections have been found to be beneficial for stabilizing signal propagation while retaining expressivity. The simple $\\textstyle{\\frac{1}{\\sqrt{L}}}$ -scaling allows depth-scaling in ResNets and unlocks hyperparameter transfer (Hayou et al., 2021; Li et al., 2021; Bordelon et al., 2023; Yang et al., 2023b). Noci et al. (2022, 2024) provide infinite width and depth analyses for Transformers with the goal of preventing rank collapse and attaining a limit that has behaviour consistent with that of moderately large networks. ", "page_idx": 15}, {"type": "text", "text": "Tensor Programs. After kernel-based approaches to understand infinite-width limits of neural networks (Neal, 1996; Jacot et al., 2018) and applications of mean-field theory (Mei et al., 2018), the Tensor Program series (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023; Yang et al., 2022, 2023b) marks the first important break through in the theory of large neural networks. The framework covers many modern deep learning architectures, optimization algorithms and arbitrary abc-parameterizations, where each abc-parameterization is essentially defined by a layerwise scaling of initialization variance and learning rate as a function of network width. Yang and Hu (2021) propose the maximal update parameterization $(\\mu{\\sf P})$ and show that it is the unique stable parameterization that achieves feature learning in all layers in the limit of infinite width. In this framework, training neural networks with a global learning rate $\\eta>0$ for all layers and with He or LeCun initialization falls under the category of so called standard parameterization (SP). The neural tangent parameterization (NTP), studied in the neural tangent kernel literature, differs but does not achieve feature learning in any layer, and is therefore less useful to describe the behaviour of finite width networks than $\\mu\\mathrm{P}$ (Wenger et al., 2023; Vyas et al., 2024). Yang and Littwin (2023) characterize stable learning with adaptive optimizers at infinite width into a feature learning versus a (nonlinear) operator regime. SAM is not covered by the update rule definition in Yang and Littwin (2023) since the nested application of the gradient w.r.t. the weights is not a coordinatewise optimizer anymore. Yang et al. (2023a) show that $\\mu\\mathrm{P}$ is equivalent to the spectral scaling conditions on the weights $\\Vert\\Delta W^{l}\\Vert=\\Theta(\\sqrt{n_{l}/n_{l-1}})$ and $\\|\\Delta W^{l}\\|\\,=\\,\\Theta(\\sqrt{n_{l}/n_{l-1}})$ . Hence Bernstein et al. (2020) would have achieved their goal of an optimizer with automatic update scaling, if they had normalized by the spectral instead of the Frobenius norm and multiplied by $\\sqrt{\\mathtt{f}\\mathtt{a n}_{-}\\mathrm{out}/\\mathtt{f}\\,\\mathtt{a n}_{-}\\mathrm{in}}$ in each layer. While recent works have considered joint limits of infinite width and depth (Yang et al., 2023b; Hayou and Yang, 2023), the data distribution has not been taken into account in Tensor Program literature. The study of scaling laws of jointly scaling model size, data set size and training time has predominantly been empirical (Kaplan et al., 2020; Zhai et al., 2022; Hoffmann et al., 2022; Besiroglu et al., 2024). Developing theory to inform Pareto optimal trade offs in a principled manner constitutes an important direction for future work. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Sharpness Aware Minimization. Sharpness aware minimization (SAM) (Foret et al., 2021) has shown to be extremely effective and robust in improving generalization performance across a wide range of architectures and settings (Chen et al., 2021; Kaddour et al., 2022). SAM was motivated as an inductive bias towards flatter minima and it has been understood to have an gradient-norm adaptive edge of stability at which it drifts towards minima with smaller spectral norm of the Hessian (Long and Bartlett, 2023; Bartlett et al., 2023). However a full understanding of why SAM works so well remains elusive. While correlations between flatness and generalization have been observed in some settings (Hochreiter and Schmidhuber, 1997; Jiang\\* et al., 2020), other studies have questioned the usefulness of sharpness as a measure for generalization, especially for modern architectures (Dinh et al., 2017; Andriushchenko et al., 2023b; Wen et al., 2024). Applying SAM on only the normalization layers often even improves generalization in vision tasks depsite increasing sharpness (M\u00fcller et al., 2024). Adaptive SAM (ASAM) (Kwon et al., 2021) is a variant of SAM derived from a sharpness definition that is invariant to weight rescalings with respect to a chosen normalization operator that leave the output function invariant. The results in M\u00fcller et al. (2024) suggest that two of the most promising normalization operators are elementwise normalization $T_{w}^{\\stackrel{\\cdot}{l}}(x)^{\\stackrel{\\smile}{=}}|W^{l}|\\odot x$ and layerwise normalization $T_{w}^{l}(x)=\\|W^{l}\\|_{F}\\cdot x$ . We state the resulting update rules and a scaling analysis in Appendix F.4. A variant of SAM that is often studied theoretically because of its simplicity does not normalize the gradient of the perturbation. Our theory covers this variant too (Appendix F.3), but Dai et al. (2024) argue that normalizing the gradients for the perturbation is crucial. Monzio Compagnoni et al. (2023) find that unnormalized SAM gets stuck around saddles while SAM slowly escapes through additional Hessian-induced noise. This suggests that the additional effort of analysing the original SAM update rule with gradient normalization is necessary for practically useful theory. Dauphin et al. (2024) draw connections between SAM and other second order optimizers like gradient penalties and weight noise. They show that SAM is able to effectively use second order information implicitly using ReLU, whereas the other two methods close the gap to SAM when using GeLU since they require the localized second order information that GeLU provides in contrast to ReLU. Wen et al. (2023) show that worst-case, ascent and average case sharpness are biased towards minimizing the maximal eigenvalue, minimal non-zero eigenvalue and trace of the Hessian, respectively. With an architecture-agnostic analysis, they show that 1-SAM minimizes the trace of Hessian like average-case sharpness, for small enough $\\eta$ and $\\rho$ . Similarly, the theoretical results by Andriushchenko and Flammarion (2022) rely on the assumption that learning rate $\\eta$ and perturbation radius $\\rho$ are chosen sufficiently close to 0. Arguably, the empirically optimal choice of $\\eta$ and $\\rho$ lies outside of this gradient flow-like regime and has qualitatively different properties (see e.g. edge of stability literature (Cohen et al., 2020; Arora et al., 2022)). ", "page_idx": 16}, {"type": "text", "text": "Scaling theory for SAM. Shin et al. (2023) suggest that the generalisation improvement by SAM continues to increase with growing overparametrization. This corroborates empirical observations that performance monotonically improves with scale, and understanding the infinite-width limit is not only of theoretical interest but entails immediate practical benefits. ", "page_idx": 16}, {"type": "text", "text": "Liu et al. (2022) introduce Look-LayerSAM with layerwise perturbation scaling for preserving good performance under large batch training for enhanced training parallelization. They use LAMB (You et al., 2020) for layerwise learning rate scaling for large batch training. The update scaling strategy in these kinds of algorithms follows ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{t+1}^{l}=W_{t}^{l}-\\eta_{t}\\phi(\\|W_{t}^{l}\\|_{F})\\frac{\\nabla_{W^{l}}\\mathcal{L}}{\\|\\nabla_{W^{l}}L\\|_{F}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with some $\\phi:\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ and where $\\nabla_{W}\\boldsymbol{\\mathcal{L}}$ may be replaced by Adam\u2019s $\\frac{m_{t}}{\\sqrt{v_{t}}+\\epsilon}$ . In practice, often simple functions like $\\phi(x)=\\operatorname*{max}(c,\\operatorname*{min}(x,C))$ or $\\phi(x)=x$ are used. The idea is to ensure that the update has the same order of magnitude as the weights. Look-LayerSAM follows an analogous approach for layerwise perturbation scaling. A derivation of $\\mu\\mathrm{P}$ for LAMB could also yield feature learning in all layers in the infinite-width limit as well as hyperparameter transfer. It certainly requires layerwise learning rate scaling. In the case $\\phi(x)=x$ , following a heuristic scaling derivation as in Appendix F.4 leads to layerwise learning rate scalings $\\eta_{1}=\\eta_{L+1}=\\Theta(1)$ and $\\eta_{l}=\\Theta(n^{-1/2})$ for hidden layers $l\\in[2,L]$ . With a bounded function like $\\phi(x)=\\operatorname*{max}(c,\\operatorname*{min}(x,C))$ , the scalings become $\\eta_{1}=\\Theta(n^{1/2})$ , $\\eta_{L+1}=\\Theta(n^{-1/2})$ and $\\eta_{l}=\\Theta(1)$ for hidden layers $l\\in[2,L]$ . We leave a closer investigation of feature learning and hyperparameter transfer with LAMB and Look-LayerSAM in SP and $\\mu\\mathrm{P}$ to future work. ", "page_idx": 17}, {"type": "text", "text": "C Definitions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we collect all definitions that do not appear in the main text. With minor modifications, we adopt all definitions from Yang and Hu (2021). If not stated otherwise, limits are taken with respect to width $n\\to\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "Definition C.1 (Big-O Notation). Given a sequence of scalar random variables $c=\\{c_{n}\\in\\mathbb{R}\\}_{n=1}^{\\infty}$ we write $c\\,=\\,\\Theta\\left(n^{-a}\\right)$ if there exist constants $A,B$ such that for almost every instantiation of $c=\\{c_{n}\\in\\mathbb{R}\\}_{n=1}^{\\infty}$ , for $n$ large enough, $A n^{-a}\\leq|c_{n}|\\leq B n^{-a}$ . Given a sequence of random vectors $x=\\{x_{n}\\in\\mathbb{R}^{n}\\}_{n=1}^{\\infty}$ , we say $x$ has coordinates of size $\\Theta\\left(n^{-a}\\right)$ and write $x=\\Theta\\left(n^{-a}\\right)$ to mean the scalar random variable sequence $\\left\\{{\\sqrt{\\left\\|x_{n}\\right\\|^{2}/n}}\\right\\}_{r}$ is $\\Theta\\left(n^{-a}\\right)$ . For the definition of $c=O(n^{-a})$ n and $c=\\Omega(n^{-a})$ , adapt the above definition of $c=\\Theta(n^{-a})$ by replacing $A n^{-a}\\leq|c_{n}|\\leq B n^{-a}$ with $|c_{n}|\\le B n^{-a}$ and $A n^{-a}\\leq|c_{n}|$ , respectively. We write $x_{n}=o(n^{-a})$ if $n^{a}\\cdot\\sqrt{\\left\\|x_{n}\\right\\|^{2}/n}\\rightarrow0$ almost surely. \u25c0 ", "page_idx": 17}, {"type": "text", "text": "Definition C.2 (Training routine). A training routine is a combination of base learning rate $\\eta\\geq0$ , perturbation radius $\\rho\\geq0$ , training sequence $\\{(\\xi_{t},y_{t})\\}_{t\\in\\mathbb{N}}$ and a continuously differentiable loss function $\\mathcal{L}(f(\\xi),y)$ using the SAM update rule with layerwise perturbation scaling (LP). \u25c0 ", "page_idx": 17}, {"type": "text", "text": "In addition to the stability conditions from the corresponding SGD result, we demand that the activation perturbations do not blow up. Otherwise the perturbations would strictly dominate both the initialization and the updates which makes the perturbation too strong and is avoided in practice. ", "page_idx": 17}, {"type": "text", "text": "Definition C.3 (Stability). We say a bcd-parametrization of an $L$ -hidden layer MLP is stable if ", "page_idx": 17}, {"type": "text", "text": "1. For every nonzero input $\\xi\\in\\mathbb{R}^{d_{\\mathrm{in}}}\\backslash\\{0\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{0}^{l},x_{0}^{l}=\\Theta_{\\xi}(1),\\;\\forall l\\in[L],\\quad\\mathrm{and}\\quad\\mathbb{E}f_{0}(\\xi)^{2}=O_{\\xi}(1),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the expectation is taken over the random initialization. ", "page_idx": 17}, {"type": "text", "text": "2. For any training routine, any time $t\\in\\mathbb{N},\\,l\\in[L],\\,\\xi\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{t}^{l}(\\xi)-h_{0}^{l}(\\xi),x_{t}^{l}(\\xi)-x_{0}^{l}(\\xi)=O_{*}(1),\\quad\\mathrm{and}\\quad f_{t}(\\xi)=O_{*}(1),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the hidden constant in $O_{*}$ can depend on the training routine, $t,\\xi,\\mathit{l}$ and the initial function $f_{0}$ . ", "page_idx": 17}, {"type": "text", "text": "3. For any training routine, any time $t~\\in~\\ensuremath{\\mathbb{N}}_{0},~l~\\in~[L],~\\xi~\\in~\\ensuremath{\\mathbb{R}}^{d_{\\mathrm{in}}}$ , for the perturbed (pre-)activation $\\bar{\\tilde{h}}_{t}^{l}:=h^{l}\\big(\\tilde{W}_{t}\\big),\\tilde{x}_{t}^{l}:=x^{l}\\big(\\tilde{W}_{t}\\big)$ and output function $\\tilde{f}_{t}(\\tilde{W}_{t})$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{h}_{t}^{l}(\\xi)-h_{t}^{l}(\\xi),\\tilde{x}_{t}^{l}(\\xi)-x_{t}^{l}(\\xi)=O_{*}(1),\\quad\\mathrm{and}\\quad\\tilde{f}_{t}(\\xi)=O_{*}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the hidden constant in $O_{*}$ can depend on the training routine, $t,\\xi,\\mathit{l}$ and the initial function $f_{0}$ . ", "page_idx": 17}, {"type": "text", "text": "Definition C.4 (Nontriviality). We say a $b c d$ -parametrization is trivial if for every training routine, $f_{t}(\\xi)-f_{0}(\\xi)\\to0$ almost surely for $n\\to\\infty$ , for every time $t>0$ and input $\\xi\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ . Otherwise the bcd-parametrization is nontrivial. \u25c0 ", "page_idx": 17}, {"type": "text", "text": "Definition C.5 (Feature learning). We say a bcd-parametrization admits feature learning in the l-th layer if there exists a training routine, a time $t>0$ and input $\\xi$ such that $x_{t}^{l}(\\xi)-x_{0}^{l}(\\bar{\\xi})=\\Omega_{\\ast}(1)$ , where the constant may depend on the training routine, the time $t$ , the input $\\xi$ and the initial function $f_{0}$ but not on the width $n$ . \u25c0 ", "page_idx": 18}, {"type": "text", "text": "Definition C.6 (Vanishing perturbations). Let $l\\in[L]$ . We say that a stable $b c d$ -parametrization has vanishing perturbations in the $l$ -th layer if for any training routine, $t\\in\\ensuremath{\\mathbb{N}}_{0}$ and $\\boldsymbol{\\xi}\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ , it holds that $\\tilde{x}_{t}^{l}-x_{t}^{l}=o(1)$ , and it has vanishing perturbations in the output if for any training routine, $t\\in\\ensuremath{\\mathbb{N}}_{0}$ and $\\boldsymbol{\\xi}\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ it holds that $\\tilde{\\delta}f_{t}(\\xi):=f_{\\tilde{W}_{t}}(\\xi)-f_{W_{t}}(\\xi)=o(1)$ . \u25c0 ", "page_idx": 18}, {"type": "text", "text": "Definition C.7 (Perturbation nontriviality). Let $l\\in[L]$ . We say that a stable bcd-parametrization is perturbation nontrivial with respect to the $l$ -th layer if and only if it does not have vanishing perturbations in the $l$ -th layer. A stable $b c d\\cdot$ -parametrization is perturbation nontrivial with respect to the output if it does not have vanishing perturbations in the output. \u25c0 ", "page_idx": 18}, {"type": "text", "text": "Definition C.8 (Effective perturbations). Let $l\\in[L+1]$ . We say that a stable $b c d$ -parametrization effectively perturbs the $l$ -th layer if there exists a training routine, $t\\,\\in\\,\\mathbb{N}$ and $\\xi\\in\\mathbb{R}^{d_{\\mathrm{in}}}$ such that $\\tilde{\\delta}W_{t}^{l}\\tilde{x}_{t}^{l-1}(\\dot{\\xi})=\\Theta(1)$ where $\\tilde{\\delta}\\dot{W}_{t}^{l}$ is defined in (LP) and $\\tilde{x}_{t}^{0}=x_{t}^{0}=\\xi_{t}$ . \u25c0 ", "page_idx": 18}, {"type": "text", "text": "Definition C.9 (\u03c3-gelu). Define \u03c3-gelu to be the function x  \u2192x2 1 + erf \u03c3\u22121x  +\u03c3 e\u22122\u03c3\u221a\u22122\u03c0x2 ", "page_idx": 18}, {"type": "text", "text": "In order to apply the Tensor Program Master Theorem, all Nonlin and Moment operations in the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program, which do not only contain parameters as inputs, are required to be pseudoLipschitz in all of their arguments. For training with SGD, this is fulfilled as soon as $\\phi^{\\prime}$ is pseudoLipschitz. Both tanh as well as $\\sigma$ -gelu fulfill this assumption. ", "page_idx": 18}, {"type": "text", "text": "Definition C.10 (Pseudo-Lipschitz). A function $f:\\mathbb{R}^{k}\\rightarrow\\mathbb{R}$ is called pseudo-Lipschitz of degree $d$ if there exists a $C>0$ such that $\\begin{array}{r}{|f(x)-f(y)|\\leq C\\|x-y\\|(1+\\sum_{i=1}^{k}|x_{i}|^{d}+|y_{i}|^{d})}\\end{array}$ . We say $f$ is pseudo-Lipschitz if it is so for any degree $d$ . \u25c0 ", "page_idx": 18}, {"type": "text", "text": "D Extensive main results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Using the formal definitions from Appendix C, here we provide the full formal statements of all of our main theoretical results together with further details and implications. The proof of all statements is provided in Appendix E. Since SAM evaluates the gradients on perturbed weights, it is not covered by the update rule definition in Yang and Littwin (2023) and an infinite-width analysis requires explicitly deriving the corresponding $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program, scalings and infinite-width limits. ", "page_idx": 18}, {"type": "text", "text": "Recall that our definition of bcd-parameterizations extends $a b c$ -parameterizations by setting the maximal perturbation scaling to $n^{\\dot{-}d}$ and allowing relative downweighting $n^{-d_{l}}$ of the global scaling in each layer $l$ . The perturbation scaling does not affect the choice of layerwise initialization variance scalings $b_{l}$ and the layerwise learning rate scalings $c_{l}$ . Common $b c$ -parametrizations for SGD are summarized in Table D.1. SAM with SGD as a base optimizer requires the same scalings. Similarly, SAM with Adam as a base optimizer requires the same scalings as Adam (Yang et al., 2022, Table 3). Recall that, for convenience, we require width-independent denominator scaling $\\|v_{t}\\|=\\Theta(1)$ of the scaled gradient for the perturbation (LP), which imposes the constraints ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{1}\\geq1/2-\\operatorname*{min}(b_{L+1},c_{L+1}),\\quad d_{l}\\geq1-\\operatorname*{min}(b_{L+1},c_{L+1})\\mathrm{~for~}l\\in[2,L],\\quad d_{L+1}\\geq1/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "All (pre-)activation and function outputs can be thought of as outputs given a fixed input $\\xi\\in\\mathbb{R}^{d_{\\mathrm{in}}}\\backslash\\{0\\}$ with $d_{i n}\\in\\mathbb{N}$ fixed, e.g. $f_{t}:=f_{W_{t}}:=f_{W_{t}}(\\xi)$ . For the perturbed weights we write $\\tilde{W}_{t}:=W_{t}+\\tilde{\\delta}W_{t}$ , with $\\tilde{\\delta}W_{t}$ defined in (LP) as $\\varepsilon_{t}^{l}$ . Here we write weight perturbations as $\\tilde{\\delta}W_{t}^{l}$ instead of $\\varepsilon_{t}^{l}$ to show the resemblance to weight updates $\\delta W_{t}^{l}$ . Perturbed activations and function outputs at time $t$ are written as $\\tilde{x}_{t}^{l}(\\xi)=x_{\\tilde{W}_{t}}^{l}(\\xi)$ and $\\tilde{f}_{t}(\\xi)=f_{\\tilde{W}_{t}^{l}}(\\xi)$ . Recall that for all of the results in this section we make the following smoothness assumption on the activation function. ", "page_idx": 18}, {"type": "text", "text": "Assumption 1 (Smooth activation function). The used activation function is either tanh or $\\sigma$ -gelu for $\\sigma>0$ sufficiently small. \u25c0 ", "page_idx": 18}, {"type": "text", "text": "We define the maximal feature update scale of a bcd-parameterization ", "page_idx": 18}, {"type": "equation", "text": "$$\nr:=\\operatorname*{min}(b_{L+1},c_{L+1},d+d_{L+1})+\\operatorname*{min}_{l=1}^{L}(c_{l}-\\mathbb{I}(l\\neq1)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as well as the maximal feature perturbation scale of a $b c d$ -parameterization ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{r}:=\\operatorname*{min}(b_{L+1},c_{L+1})+d+\\operatorname*{min}_{l=1}^{L}(d_{l}-\\mathbb{I}(l\\neq1)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Stability requires the constraints (a-c) from SGD and additional perturbation stability constraints (d-e) that include the layerwise perturbation scales $\\{d_{l}\\}_{l=1,\\dots,L+1}$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem D.2 (Stability characterization). $A$ bcd-parametrization is stable if and only if all of the following are true: ", "page_idx": 19}, {"type": "text", "text": "(a) (Stability at initialization, $h_{0}^{l},x_{0}^{l}=\\Theta(1)$ for all $l_{:}$ , $f_{0}=O(1),$ ) $b_{1}=0,$ , $b_{l}=1/2$ for $l\\in[2,L]$ and $b_{L+1}\\geq1/2$ .   \n$(b)$ (Features do not blow up during training, i.e. $\\Delta x_{t}^{l}=O(1)$ for all l) $r\\geq0$ .   \n(c) (Output function does not blow up during training, i.e. $\\Delta W_{t}^{L+1}x_{t}^{L},W_{0}^{L+1}\\Delta x_{t}^{L}=O(1))$ $c_{L+1}\\geq1$ and $b_{L+1}+r\\ge1$ .   \n(d) (Feature perturbations do not blow up, i.e. $\\tilde{\\delta}x_{t}^{l}=O(1)$ for all l) $\\tilde{r}\\geq0$ .   \n(e) (Output function perturbations do not blow up during training, i.e. $\\widetilde{\\delta}W_{t}^{L+1}\\widetilde{x}_{t}^{L},W_{t}^{L+1}\\widetilde{\\delta}x_{t}^{L}=$ $O(1),$ ) $d+{\\cal d}_{L+1}\\geq1\\;a n d\\;b_{L+1}+\\tilde{r}\\geq1.$ ", "page_idx": 19}, {"type": "text", "text": "The nontriviality and feature learning characterizations from SGD remain unaltered. This is because in the definition of $r$ , it holds that $d\\!+\\!d_{L+1}\\geq1$ (from perturbation stability), and $\\operatorname*{min}(b_{L+1},c_{L+1})\\leq1$ already had to hold for nontriviality in SGD, so that stable perturbation scaling does not affect $r$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem D.3 (Nontriviality characterization). A stable bcd-parametrization is nontrivial if and only $f c_{L+1}=1\\,o r\\operatorname*{min}(b_{L+1},c_{L+1})+r=1.$ . ", "page_idx": 19}, {"type": "text", "text": "As for nontriviality, the conditions under which a stable, nontrivial parameterization is feature learning in the infinite-width limit are decoupled from the choice of perturbation scalings $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ Hence the conditions are the same as for SGD. Below we provide a slightly refined result in terms of the maximal feature update scale $r_{l_{0}}$ of a $b c d$ -parameterization up to layer $l_{0}$ (as provided in the Appendix of Yang and Hu (2021)). ", "page_idx": 19}, {"type": "text", "text": "Theorem D.4 (Feature learning characterization). For any $l_{0}\\in[L]$ , the following statements are equivalent: ", "page_idx": 19}, {"type": "text", "text": "(a) A stable, nontrivial bcd-parametrization admits feature learning in layer $l_{0}$ . (b) A stable, nontrivial bcd-parametrization admits feature learning in layer $l$ for all $l\\geq l_{0}$ . (c) $r_{l_{0}}:=\\operatorname*{min}(b_{L+1},c_{L+1},\\stackrel{\\cdot}{d}+d_{L+1})+\\operatorname*{min}_{m=1}^{l_{0}}(c_{m}-\\mathbb{I}(m\\not=1))=0.$ ", "page_idx": 19}, {"type": "text", "text": "Consequently, a stable, nontrivial bcd-parametrization admits feature learning (at least in the last layer activations) if and only if $r=0$ . ", "page_idx": 19}, {"type": "text", "text": "Remark D.5 (Effective feature learning). As for perturbations, feature learning in later layers can be caused by weight updates in earlier layers that propagate through the network. One could demand effective feature learning in the l-th layer as \u03b4W tlxlt\u2212 $\\delta\\bar{W}_{t}^{l}x_{t}^{\\bar{l}-\\bar{1}}=\\Theta(1)$ and it would occur if and only if $\\operatorname*{min}(b_{L+1},c_{L+1},d+d_{L+1})+c_{l}-\\mathbb{I}(l\\neq1)=0.$ . \u25c0 ", "page_idx": 19}, {"type": "text", "text": "As for nontriviality, perturbation nontriviality in the output is attained if the constraints for $\\widetilde{\\delta}W_{t}^{L+1}\\widetilde{x}_{t}^{L}$ or $W_{t}^{l}\\tilde{\\delta}x_{t}^{L}$ are exactly satisfied. ", "page_idx": 19}, {"type": "text", "text": "Theorem D.6 (Perturbation nontriviality characterization). Let $\\textit{l}\\in\\textit{}[L]$ . A stable bcdparametrization is perturbation nontrivial with respect to the $l$ -th layer if and only if ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{r}_{l}:=\\operatorname*{min}(b_{L+1},c_{L+1})+d+\\operatorname*{min}_{m=1}^{l}(d_{m}-\\mathbb{I}(m\\neq1))=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A stable bcd-parametrization is perturbation nontrivial with respect to the output if and only if $d+d_{L+1}=1$ or $\\operatorname*{min}(b_{L+1},c_{L+\\bar{1}})+\\tilde{r}=1$ . ", "page_idx": 19}, {"type": "text", "text": "The converse formulation of the perturbation-nontriviality results characterizes the regime of vanishing perturbations. ", "page_idx": 19}, {"type": "text", "text": "Corollary D.7 (Vanishing perturbation characterization). For any $l_{0}\\in[L]$ , the following statements are equivalent: ", "page_idx": 20}, {"type": "text", "text": "(a) A stable bcd-parametrization has vanishing perturbations in layer $l_{0}$ . $(b)$ $A$ stable bcd-parametrization has vanishing perturbations in layer $l$ for all $1\\leq l\\leq l_{0}$ . (c) $\\begin{array}{r}{\\tilde{r}_{l_{0}}:=\\operatorname*{min}(b_{L+1},c_{L+1})+d+\\operatorname*{min}_{m=1}^{l_{0}}(d_{m}-\\mathbb{I}(m\\neq1))>0.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "A stable bcd-parametrization has vanishing perturbations with respect to all layers and the output function if and only if $d_{L+1}>1/2$ and $\\Tilde{r}>\\operatorname*{max}(0,1-b_{L+1})$ . This case reduces to the results in Yang and Hu (2021). ", "page_idx": 20}, {"type": "text", "text": "For perturbation nontriviality it suffices that the perturbation in any of the previous layers is scaled correctly. For effective perturbations, we need the correct scaling in exactly that layer. ", "page_idx": 20}, {"type": "text", "text": "Theorem D.8 (Effective perturbation characterization). For $l\\in[L]$ , a stable bcd-parametrization effectively performs SAM in the l-th layer if and only $i f\\operatorname*{min}(b_{L+1},\\dot{c}_{L+1})+d+d_{l}-\\mathbb{I}(l\\neq1)=0$ . A stable bcd-parametrization effectively performs SAM in the last layer if and only if $d+d_{L+1}=1$ . The above understanding of all update and perturbation scalings allows us to extract the most important consequences of different choices of perturbation scaling on the learning dynamics. Beyond vanishing hidden layer perturbations, the following theorem shows that the joint gradient norm $\\|v_{t}\\|$ can be approximated efficiently without an additional backward pass under global perturbation scaling. ", "page_idx": 20}, {"type": "text", "text": "Theorem D.9 (Global Perturbation Scaling). Given any stable bcd-parametrization $\\{b_{l}\\}_{l\\in[L+1]}\\cup$ $\\{c_{l}\\}_{l\\in[L+1]}\\cup\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ . The parametrization performs updates in the original gradient direction if and only if $d_{l}=C$ for all $l\\in[L+1]$ for some $C\\in\\mathbb{R}$ . In this case, the parametrization has vanishing perturbations in all hidden layers $l\\in[L]$ , and the last layer $l=L+1$ is effectively perturbed if and only if $d=1/2$ . If $b_{L+1}>1/2$ (as in $\\mu P_{\\mathrm{,}}$ ), the gradient norm is dominated by the last layer and simplifies to, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|v_{t}\\|=\\Theta(n^{1/2-C}),\\qquad\\|v_{t}\\|-\\mathcal{L}^{\\prime}(f_{t}(\\xi_{t}),y_{t})\\|x_{t}^{L}\\|=o(n^{1/2-C}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "One might suspect that it is desirable to let all layers contribute non-vanishingly to the gradient norm in the denominator of (LP). The following proposition shows that this should be avoided with our definition of $b c d$ -parameterizations. Of course, if we add even more hyperparameters by decoupling numerator and denominator scalings, we can set all contributions to $\\Theta(1)$ , which is what we do in Appendix F.7. ", "page_idx": 20}, {"type": "text", "text": "Proposition D.10 (Balancing gradient norm contributions). Given any stable bcd-parametrization $\\{b_{l}\\}_{l\\in[L+1]}\\cup\\{c_{l}\\}_{l\\in[L+1]}\\cup\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ . If all layers contribute to the gradient norm nonvanishingly in the limit, i.e. $\\|v_{t}^{l}\\|=\\Theta(\\|v_{t}\\|)$ for all $l\\in[L+1],t\\in\\mathbb{N}_{0}$ , then the parametrization has vanishing perturbations in all hidden layers $l\\in[L]$ . Such a parametrization effectively performs SAM in the last layer $l=L+1$ if and only if $d=1/2$ . ", "page_idx": 20}, {"type": "text", "text": "The following theorem provides the unique correct perturbation scaling for any stable bcparameterization with $b_{L+1}\\geq1$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem D.11 (Perturbation Scaling Choice for Effective Perturbations). Given any stable bcd-parametrization $\\{b_{l}\\}_{l\\in[L+1]}\\cup\\{c_{l}\\}_{l\\in[L+1]}\\cup\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ . If $b_{L+1}<1$ , then there does not exist a stable choice of $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ that achieves effective perturbations before the last layer. If $b_{L+1}\\geq1$ , then up to the equivalence $d_{l}^{\\prime}=d_{l}+C,\\,C\\in\\mathbb{R},\\,\\forall l\\in[L+1]$ , the unique stable choice $\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ with effective perturbations in all layers $l\\in[L+1]$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\nd=-1/2,\\qquad d_{l}=\\left\\{\\begin{array}{l l}{1/2-\\operatorname*{min}(b_{L+1},c_{L+1})}&{l=1,}\\\\ {3/2-\\operatorname*{min}(b_{L+1},c_{L+1})}&{l\\in[2,L],}\\\\ {3/2}&{l=L+1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In this parameterization, the first layer dominates the gradient norm as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|v_{t}\\|=\\Theta(1),\\qquad\\left|\\|v_{t}^{1}\\|-\\|v_{t}\\|\\right|=\\Theta(n^{-1/2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Table D.2 summarizes the consequences of Theorem D.11. Together with Theorem D.11, the following proposition suggests that $b_{L+1}=1$ is a good choice. However $b_{L+1}>1$ can also induce effective perturbations, as long as $d$ and $d_{L+1}$ are chosen correctly. ", "page_idx": 20}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/f06a5fab98a2d024de13734bfb2a8856f15a06b32a466c15d93bd8449b79a1b6.jpg", "table_caption": [], "table_footnote": ["Table D.1: ( $b c$ -parametrizations) Overview over common implicitly used $\\overline{{b c}}$ -parametrizations for training MLPs without biases in standard parametrization (SP), standard parametrization with maximal stable nonadaptive LR $c\\,=\\,1$ (SP (stable)), neural tangent parametrization (NTP) and maximal update parametrization $(\\mu{\\bf P})$ . "], "page_idx": 21}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/52020694407be1c79f5915b1dab2ab93bccaf8efb20a2d75eb01799b47a89587.jpg", "table_caption": [], "table_footnote": ["Table D.2: (Perturbation scalings) Overview over important choices of the global perturbation scaling $\\rho n^{-d}$ and the layerwise perturbation scalings $n^{\\dot{-}d_{l}}$ for training MLPs without biases with SAM: Naive scaling without width dependence (Naive), maximal stable global scaling along the original gradient direction (Global) and the unique scaling that achieves effective perturbations in all layers (Effective). An extensive overview that characterizes all possible choices of perturbation scaling is provided in Appendix F.1. Recall the gradient scaling $c_{\\nabla}:=\\operatorname*{min}(b_{L+1},c_{L+1})$ . "], "page_idx": 21}, {"type": "text", "text": "Proposition D.12 (Effects of last-layer initialization $b_{L+1}$ on all perturbations). If a stable bcdparametrization with $\\operatorname*{min}(b_{L+1},c_{L+1})\\leq1$ is perturbation nontrivial with respect to any hidden layer $l\\in[L]$ , it is also perturbation nontrivial with respect to the output. ", "page_idx": 21}, {"type": "text", "text": "Lastly, the following proposition shows that effective perturbations from the first layer propagate through the entire network. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.13 (Perturbations propagate through the forward pass). All stable bcdparametrizations with $d_{1}\\,=\\,-\\operatorname*{min}(b_{L+1},c_{L+1})\\,-\\,d$ effectively perturb the first layer and are perturbation nontrivial in all layers. ", "page_idx": 21}, {"type": "text", "text": "Remark D.14 (Efficiency gains). The above results may be used for efficiency gains. Given any stable bcd-parametrization, we can compute the maximal layer $l_{0}$ such that $\\tilde{r}_{l_{0}}~>~0$ , and in wide networks do not have to compute SAM perturbations before layer $l_{0}+1$ ; as soon as $b_{L+1}\\,>\\,1/2$ (as for $\\mu\\mathrm{P})$ , the gradient norm for the SAM update rule is approximately given by $\\|\\nabla L_{t}\\|\\approx\\dot{\\mathcal{L}^{\\prime}}(f_{t}(\\xi_{t}),\\dot{y}_{t})\\|x_{t}^{L}\\|$ , which can directly be computed without an additional backward pass. The practical recommendation from our experiments however is to either use $\\mu\\mathrm{P^{2}}$ or to completely abstain from perturbations. \u25c0 ", "page_idx": 21}, {"type": "text", "text": "Remark D.15 (SAM without gradient normalization). For the SAM update rule without gradient normalization simply set $d\\,=\\,0$ and remove the gradient norm constraints (D.1) to arrive at the adapted $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program and bcd-constraints. Note that standard parametrization gets even more unstable without dividing by $\\|\\nabla L\\|=\\Theta(n^{1/2})$ , now requiring $d_{L+1}\\geq1$ for stability. Similar to the previous results, this shows that unawareness of $b c d$ -parametrizations requires strongly scaling down $\\rho$ for stability, while vasting computation on vanishing perturbations before the last layer. More details can be found in Appendix F.3. \u25c0 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E Proof of main results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we derive the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program that corresponds to training a MLP without biases with SAM. For simplicity and clarity of the proof, we prove the one-dimensional case $d_{i n}\\,=\\,1$ , $d_{o u t}=1$ , but an extension to arbitrary but fixed $d_{i n}$ , $d_{o u t}$ is straightforward. Recall Assumption 1 that allows us to apply the Tensor Program Master Theorem and explicitly state the infinite-width limit of training MLPs with SAM in Appendix E.2. ", "page_idx": 22}, {"type": "text", "text": "E.1 Tensor program formulation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1.1 Tensor Program initialization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We initialize the matrices $W_{0}^{2},\\dots,W_{0}^{L}$ as $(W_{0}^{l})_{\\alpha\\beta}\\sim\\mathcal{N}(0,1/n)$ , which absorbs $b_{l}=1/2$ . ", "page_idx": 22}, {"type": "text", "text": "We initialize the input layer matrix $W_{0}^{1}\\,\\in\\,\\mathbb{R}^{n\\times1}$ and normalized output layer matrix $\\hat{W}_{0}^{L+1}\\,=$ $W_{0}^{L+1}n^{b_{L+1}}\\in\\mathbb{R}^{1\\times n}$ as $(W_{0}^{1})_{\\alpha},(\\hat{W}_{0}^{L+1})_{\\alpha}\\sim\\mathcal{N}(0,1)$ , as initial vectors should have a distribution that is $\\Theta(1)$ . ", "page_idx": 22}, {"type": "text", "text": "In the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ formulation, we write all quantities as $\\theta_{z}z$ , where $\\theta_{z}$ denotes their scaling $n^{C}$ for some $C\\in\\mathbb{R}$ and $z$ therefore has a $\\Theta(1)$ distribution. The stability, nontriviality and feature learning conditions then stem from requiring either $\\theta_{z}\\rightarrow0$ or $\\theta_{z}=1$ depending on $z$ and its desired scale. ", "page_idx": 22}, {"type": "text", "text": "E.1.2 First forward pass ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We denote a definition of a Tensor Program (TP) or $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ computation $\\operatorname{as}:=$ . Compared to MLPs trained with SGD nothing changes in the first forward pass, ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{0}^{1}(\\xi):=W_{0}^{1}\\xi\\quad\\mathrm{(NL)},\\quad x_{0}^{l}:=\\phi(h_{0}^{l})\\quad\\mathrm{(NL)},\\quad h_{0}^{l+1}:=W_{0}^{l+1}x_{0}^{l}.\\quad\\mathrm{(MatMul)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the case of MuP, $f_{0}(\\xi)=W_{0}^{L+1}x_{0}^{L}(\\xi)\\rightarrow0$ defines a scalar in the TP. ", "page_idx": 22}, {"type": "text", "text": "Observe the scalings $x_{0}^{1}=\\Theta(h_{0}^{1})=\\Theta(n^{-b_{1}}),x_{0}^{l}=\\Theta(h_{0}^{l})=\\Theta(n^{1/2-b_{l}})$ for $l\\in[2,L]$ due to CLT, independence at initialization and $x_{0}^{l}=\\Theta(h_{0}^{l})=\\Theta(1)$ by stability. Hence stability at initialization inductively requires $b_{1}=0$ , $b_{l}=1/2$ for $l\\in[2,L]$ and $b_{L+1}\\geq1/2$ . ", "page_idx": 22}, {"type": "text", "text": "E.1.3 First backward pass ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The chain rule of the derivative remains the same, we just evaluate on different weights compared to standard SGD. We denote the adversarially perturbed weights by $\\tilde{W}_{t}^{l}$ and the normalized perturbations by $\\tilde{\\delta}W_{t}^{l}$ . Before computing the updates we have to compute a full backward pass to determine these perturbed weights for each layer, and then compute a forward pass with these perturbed weights to compute the perturbed preactivations $\\tilde{h}_{t}^{l}$ that we will need for computing the SAM update. Therefore the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program for SAM maintains a perturbed copy of all preactivations, activations, lastlayer weights and logits just for computing the updates of the actual parameters. ", "page_idx": 22}, {"type": "text", "text": "Under MuP, the loss derivative with respect to the function remains $\\chi_{0}:=\\mathcal{L}^{\\prime}(f_{0}(\\xi_{0}),y_{0})\\rightarrow\\mathring{\\chi}_{0}:=$ $\\mathcal{L}^{\\prime}(0,y_{0})$ . For the weight perturbation, we need to perform a SGD backward pass, ", "page_idx": 22}, {"type": "equation", "text": "$$\nd x_{0}^{L}:=\\hat{W}_{0}^{L+1},\\quad d h_{0}^{l}:=d x_{0}^{l}\\odot\\boldsymbol{\\phi}^{\\prime}(h_{0}^{l}),\\quad d x_{0}^{l-1}:=(W_{0}^{l})^{T}d h_{0}^{l},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $d\\boldsymbol{z}:=\\,\\theta_{\\nabla}^{-1}\\nabla_{z}f$ . For SGD (and for SAM, as we will see later) all gradients have scaling $\\theta_{\\nabla}:=n^{-b_{L+1}}$ in the first step, whereas we overload the notation $\\theta_{\\nabla}:=\\,n^{-\\,\\operatorname*{min}\\left(b_{L+1},c_{L+1}\\right)}$ for all later steps. For clarity of presentation assume $b_{L+1}\\geq c_{L+1}$ here, the other case follows analogously. For the first step this can be understood from ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{x^{L}}\\,f_{0}=W_{0}^{L+1}=\\Theta(n^{-b_{L+1}}),\\qquad\\nabla_{h^{L}}\\,f_{0}=\\nabla_{x^{L}}\\,f_{0}\\odot\\phi^{\\prime}(h_{0}^{L})=\\Theta(n^{-b_{L+1}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since $h_{0}^{L}=\\Theta(1)$ by the stability assumption, and this scale $\\Theta(n^{-b_{L+1}})$ propagates through all layers via the chain rule and remains stable in later backward passes. For hidden layer gradients, observe ", "page_idx": 22}, {"type": "text", "text": "that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{x^{L-1}}f_{t}=}&{}&{(W_{t}^{L})^{T}\\nabla_{h^{L}}f_{t}=(W_{0}^{L}+\\Delta W_{t}^{L})^{T}\\nabla_{h^{L}}f_{t}}\\\\ {=}&{}&{\\Theta\\left((W_{0}^{L})^{T}\\nabla_{h^{L}}f_{t}-n^{-c L}\\displaystyle\\sum_{s=0}^{t-1}((\\nabla_{h^{L}}f_{s})^{T}\\nabla_{h^{L}}f_{t})x_{s}^{L-1}\\right)}\\\\ {=}&{}&{\\Theta(n^{1-2b_{L}}\\theta_{\\nabla}-n^{-c L}\\theta_{\\nabla}^{2}n)=\\Theta(\\theta_{\\nabla}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where first term\u2019s scale stems from the products $(W_{0}^{L})^{T}W_{0}^{L}v=\\Theta(n^{1-2b_{L}}v)$ due to Yang (2021), $b_{L}=1/2$ for stability at initialization and $b_{L+1}+c_{L}\\ge1$ for update stability during training $(r\\geq0)$ ). If we allowed the second term to strictly dominate, the gradient scale would explode iteratively in the backward pass. ", "page_idx": 23}, {"type": "text", "text": "The gradient norm. Before computing the weight perturbations, we need to compute the gradient norm for the SAM update. The gradient norm at time $t$ in each layer $l\\in[2,L]$ is given by the scalar, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\gamma^{-2}}{\\nabla}\\left\\|\\frac{\\partial L_{t}}{\\partial W^{l}}\\right\\|^{2}=\\sum_{i,j=1}^{n}\\left(\\chi_{t}(d h_{t}^{l})_{i}(x_{t}^{l-1})_{j}\\right)^{2}=\\chi_{t}^{2}\\|d h_{t}^{l}(x_{t}^{l-1})^{T}\\|_{F}^{2}=\\chi_{t}^{2}\\big((d h_{t}^{l})^{T}d h_{t}^{l}\\big)\\big((x_{t}^{l-1})^{T}x_{t}^{l-1}\\big),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\chi_{t}=\\mathcal{L}^{\\prime}(f_{t}(\\xi_{t}),y_{t})$ and we used $\\partial h^{l}/\\partial W_{i j}^{l}=(x_{j}^{l-1}\\delta_{i k})_{k=1,\\dots,n}.$ ", "page_idx": 23}, {"type": "text", "text": "Hence the gradient norm of all weights jointly is given by the unnormalized scalar ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla_{w}L_{t}\\|^{2}=\\chi_{t}^{2}\\left(n\\theta_{\\nabla}^{2}\\frac{(d h_{t}^{1})^{T}d h_{t}^{1}}{n}(\\xi_{t}^{T}\\xi_{t})+\\sum_{l=2}^{L}n^{2}\\theta_{\\nabla}^{2}\\frac{(d h_{t}^{l})^{T}d h_{t}^{l}}{n}\\frac{(x_{t}^{l-1})^{T}x_{t}^{l-1}}{n}+n\\frac{(x_{t}^{L})^{T}x_{t}^{L}}{n}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with scaling $\\theta_{\\|\\nabla\\|}^{2}=\\Theta(n^{2}\\theta_{\\nabla}^{2}+n)=\\Theta(n)$ , because stability at initialization requires $b_{L+1}\\geq1/2$ so that $n^{2}\\theta_{\\nabla}^{2}\\leq n$ . Note that the first layer contributes vanishingly to the gradient norm, the hidden layer gradients only if $b_{L+1}=1/2$ (equivalently $f_{0}=\\Theta(1)^{!}$ ) and the last-layer activations always in dominating order. So in $\\mu\\mathrm{P}_{:}$ , in the limit, $\\|\\dot{\\nabla_{w}}\\bar{L}_{t}\\|=\\dot{\\mathcal{L}^{\\prime}}(f_{t}(\\xi_{t}),y_{t})\\|x_{t}^{L}\\|$ . This means that the unscaled gradient always aligns with the last-layer activation. For learning in $\\mu\\mathrm{P}_{:}$ , this dominance is corrected by the layerwise learning rates. ", "page_idx": 23}, {"type": "text", "text": "The squared norm of the rescaled gradient is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|v_{t}\\|^{2}=\\chi_{t}^{2}\\bigg(}&{}&{n\\theta_{\\nabla}^{2}n^{-2d_{1}}\\frac{(d h_{t}^{1})^{T}d h_{t}^{1}}{n}(\\xi_{t}^{T}\\xi_{t})}\\\\ &{}&{+\\displaystyle\\sum_{l=2}^{L}n^{2}\\theta_{\\nabla}^{2}n^{-2d_{l}}\\frac{(d h_{t}^{l})^{T}d h_{t}^{l}}{n}\\frac{(x_{t}^{l-1})^{T}x_{t}^{l-1}}{n}+n n^{-2d_{L+1}}\\frac{(x_{t}^{L})^{T}x_{t}^{L}}{n}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with scaling $\\begin{array}{r}{\\theta_{v}^{2}=\\Theta(n^{1-2d_{1}}\\theta_{\\nabla}^{2}+\\sum_{l=2}^{L}n^{2-2d_{l}}\\theta_{\\nabla}^{2}+n^{1-2d_{L+1}})}\\end{array}$ . For simplicity, set $\\theta_{v}=1$ . This raises the constraints $n^{1-2d_{1}}\\theta_{\\nabla}^{2}\\le1,n^{2-2d_{l}}\\theta_{\\nabla}^{2}\\le1$ for $l\\in[2,L]$ and $n^{1-2d_{L+1}}\\leq1$ , which can be rewritten as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{1}\\geq1/2-\\operatorname*{min}(b_{L+1},c_{L+1}),\\quad d_{l}\\geq1-\\operatorname*{min}(b_{L+1},c_{L+1})\\mathrm{~for~}l\\in[2,L],\\quad d_{L+1}\\geq1/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where at least one equality is demanded to hold in order to attain $\\theta_{v}=1$ . If one of the equalities holds, the respective layer contributes to the norm non-vanishingly in the limit. ", "page_idx": 23}, {"type": "text", "text": "Thus, applying the square root and dividing by $\\theta_{v}=1$ the square root of (E.2) defines a normalized TP scalar. ", "page_idx": 23}, {"type": "text", "text": "Perturbations. Stability implies that also the perturbed (pre-)activations and output function remain $\\Theta(1)$ and $O(1)$ respectively. Otherwise a SAM training step would induce blowup in the updates. We call this weaker property of just the perturbations perturbation stability. ", "page_idx": 23}, {"type": "text", "text": "Definition E.1 (Perturbation stability). We call a bcd-parametrization perturbation stable if and only if $\\tilde{h}_{t}^{l},\\tilde{x}_{t}^{l}=\\Theta(1)$ for all $l\\in[L]$ and $t\\in\\mathbb{N}$ and $\\tilde{\\delta}f_{t}=\\bar{O}(1)$ for all $t\\in\\mathbb{N}$ . ", "page_idx": 23}, {"type": "text", "text": "Mathematically we get the normalized weight perturbations for $l\\in\\{2,\\ldots,L\\}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\delta}W_{0}^{L+1}:=\\frac{\\rho\\;\\chi_{0}\\;x_{0}^{L}}{\\|v_{0}\\|},\\quad\\tilde{\\delta}W_{0}^{l}=\\frac{\\rho\\;\\chi_{0}\\;d h_{0}^{l}\\;(x_{0}^{l-1})^{T}}{\\|v_{0}\\|},\\quad\\tilde{\\delta}W_{0}^{1}=\\frac{\\rho\\;\\chi_{0}\\;d h_{0}^{1}\\;\\xi_{0}^{T}}{\\|v_{0}\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which scale as $\\tilde{\\theta}_{L+1}:=\\tilde{\\theta}_{W^{L+1}}:=n^{-(d+d_{L+1})}$ , $\\Theta(n^{(d+d_{l})-b_{L+1}})$ and $\\Theta(n^{-(d+d_{1})-b_{L+1}})$ respectively. But the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program computation rules do not allow to compute matrices $\\tilde{\\delta}W_{0}^{l},l\\in[L]$ , therefore we use the weight updates to directly compute the preactivation and activation changes analogous to the $t$ -th forward pass. For all $t\\geq0$ , we write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{h}_{t}^{l}=h_{t}^{l}+\\tilde{\\theta}_{l}\\tilde{\\delta}h_{t}^{l},\\qquad\\tilde{x}_{t}^{l}=x_{t}^{l}+\\tilde{\\theta}_{l}\\tilde{\\delta}x_{t}^{l},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with the perturbations for $l\\in[2,L]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\tilde{\\delta}h_{0}^{1}(\\xi):=}&{}&{+\\frac{\\rho\\chi_{0}(\\xi_{0}^{T}\\xi)d h_{0}^{1}}{||v_{0}||},}\\\\ {\\tilde{\\delta}x_{t}^{l}:=}&{}&{\\tilde{\\theta}_{l}^{-1}(\\phi(h_{t}^{l}+\\tilde{\\theta}_{l}\\tilde{\\delta}h_{t}^{l})-\\phi(h_{t}^{l})),}\\\\ {\\tilde{\\theta}_{l}\\tilde{\\delta}h_{0}^{l}:=}&{}&{\\tilde{\\theta}_{l-1}W_{0}^{l}\\tilde{\\delta}x_{0}^{l-1}+(\\tilde{W}_{0}^{l}-W_{0}^{l})\\tilde{x}_{0}^{l-1}}\\\\ {=}&{}&{\\tilde{\\theta}_{l-1}W_{0}^{l}\\tilde{\\delta}x_{0}^{l-1}+\\rho\\tilde{\\theta}_{W^{l}}\\frac{\\chi_{0}}{||v_{0}||}\\frac{(x_{0}^{l-1})^{T}\\tilde{x}_{0}^{l-1}}{n}d h_{0}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which defines a NonLin operation with the vectors $W_{0}^{l}\\tilde{\\delta}x_{0}^{l-1}$ and $d h_{0}^{l}$ and everything else treated as scalars, and with first backward pass scalings $\\tilde{\\theta}_{W^{1}}:=\\bar{n}^{-(d+d_{1})}\\theta_{\\nabla}$ , $\\tilde{\\theta}_{W^{l}}:=\\,n^{1-(d+d_{l})}\\theta_{\\nabla}$ and $\\begin{array}{r}{\\tilde{\\theta}_{l}\\,:=\\,\\operatorname*{max}(\\tilde{\\theta}_{l-1},\\tilde{\\theta}_{W^{l}})\\,=\\,\\operatorname*{max}_{m=1}^{l}\\overline{{\\tilde{\\theta}_{W^{m}}}}}\\end{array}$ , where we used that x\u02dcl0\u22121 = \u0398(1) due to perturbation stability. Note that these scalings may implicitly increase when $t\\,>\\,0$ since $\\theta_{\\nabla}\\,=\\,n^{-b_{L+1}}$ gets replaced by $\\theta_{\\nabla}=n^{-\\operatorname*{min}\\left(b_{L+1},c_{L+1}^{-}\\right)}$ . ", "page_idx": 24}, {"type": "text", "text": "The activation perturbations can then simply be defined via the NonLin operation, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\delta}x_{0}^{l}:=\\tilde{\\theta}_{l}^{-1}(\\phi(h_{0}^{l}+\\tilde{\\theta}_{l}\\tilde{\\delta}h_{0}^{l})-\\phi(h_{0}^{l})),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with the same scaling as $\\tilde{\\delta}h_{0}^{l}$ . ", "page_idx": 24}, {"type": "text", "text": "The perturbation of the scalar output function can simply be defined via the NonLin operation, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\delta}f_{0}:=\\tilde{W}_{0}^{L+1}\\tilde{x}_{0}^{L}-W_{0}^{L+1}x_{0}^{L}=\\tilde{\\theta}_{L+1}^{\\prime}\\frac{\\tilde{\\delta}W_{0}^{L+1}\\tilde{x}_{0}^{L}}{n}+\\tilde{\\theta}_{L\\nabla}^{\\prime}\\frac{\\hat{W}_{0}^{L+1}\\tilde{\\delta}x_{0}^{L}}{n},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\tilde{\\theta}_{L+1}^{\\prime}:=n\\tilde{\\theta}_{W^{L+1}}$ and $\\tilde{\\theta}_{L\\nabla}^{\\prime}:=n\\theta_{\\nabla}\\tilde{\\theta}_{L}$ . ", "page_idx": 24}, {"type": "text", "text": "SAM Update. Finally, we can compute the SAM updates as follows. In the case $\\operatorname*{min}(b_{L+1},c_{L+1})\\leq$ $d+d_{L+1}$ the weight perturbation scale is dominated by the weight scale, so that ", "page_idx": 24}, {"type": "equation", "text": "$$\nd x_{S A M,0}^{L}:=\\hat{W}_{0}^{L+1}+\\tilde{\\theta}_{(L+1)/\\nabla}\\;\\tilde{\\delta}W_{0}^{L+1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\tilde{\\theta}_{(L+1)/\\nabla}:=\\tilde{\\theta}_{L+1}/\\theta_{\\nabla}\\le1$ , whereas if $\\operatorname*{min}(b_{L+1},c_{L+1})>d+d_{L+1}$ we write ", "page_idx": 24}, {"type": "equation", "text": "$$\nd x_{S A M,0}^{L}:=\\tilde{\\theta}_{\\nabla/(L+1)}\\hat{W}_{0}^{L+1}+\\tilde{\\delta}W_{0}^{L+1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\theta_{\\nabla/(L+1)}:=\\theta_{\\nabla}/\\widetilde\\theta_{L+1}\\le1$ . In any case, the scaling of $d x_{S A M,0}^{L}$ and all other SAM gradients is $\\theta_{S A M}:=\\operatorname*{max}(\\theta_{\\nabla},n^{-(d+d_{L+1})})=n^{-\\operatorname*{min}(b_{L+1},c_{L+1},d+d_{L+1})}$ . The other SAM gradients are given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d h_{S A M,0}^{l}:=}&{}&{d x_{S A M,0}^{l}\\odot\\phi^{\\prime}(\\tilde{h}_{0}^{l})}\\\\ {d x_{S A M,0}^{l-1}:=}&{}&{(\\tilde{W}_{0}^{l})^{T}d h_{S A M,0}^{l}=(W_{0}^{l}+\\tilde{\\theta}_{W^{l}}\\tilde{\\delta}W_{0}^{l})^{T}d h_{S A M,0}^{l}}\\\\ {=}&{}&{(W_{0}^{l})^{T}d h_{S A M,0}^{l}+\\rho\\theta_{S A M}\\tilde{\\theta}_{W^{l}}\\frac{\\chi_{0}}{\\lVert v_{0}\\rVert}\\frac{(d h_{0}^{l})^{T}d h_{S A M,0}^{l}}{n}x_{0}^{l-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last line define a NonLin operation in the vectors $(W_{0}^{l})^{T}d h_{S A M,t}^{l}$ and $x_{0}^{l-1}$ and everything else treated as scalars. Consequently, $\\nabla_{h_{0}^{l}}f|_{\\tilde{W}_{0}}$ is of the same scale as $\\nabla_{x_{0}^{l}}f|_{\\tilde{W}_{0}}$ and $\\nabla_{x_{0}^{l-1}}f|_{\\tilde{W}_{0}}$ is of the scale m $\\operatorname{tax}(\\theta_{S A M},\\tilde{\\theta}_{W^{l}}\\theta_{S A M})=\\theta_{S A M}$ since $\\tilde{\\theta}_{W^{l}}\\leq1$ is required for perturbation stability. Note that for SAM\u2019s weight updates the loss derivative is also evaluated on the perturbed weights, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\chi}_{0}:=\\mathcal{L}^{\\prime}(\\tilde{W}_{0}^{L+1}\\tilde{x}_{0}^{L},y_{0}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Constraints on the output function. Assuming $\\tilde{x}_{0}^{L}=\\Theta(1)$ (perturbation stability), we get $\\tilde{\\chi}_{0}=$ $O(1)$ if and only if $\\tilde{\\delta}W_{0}^{L+1}={\\cal O}(n^{-1})$ if and only if $d+d_{L+1}\\geq1$ . ", "page_idx": 25}, {"type": "text", "text": "We have $\\tilde{\\chi}_{0}=\\Theta(1)$ if and only if $\\tilde{f}_{0}=\\tilde{W}_{0}^{L+1}\\tilde{x}_{0}^{L}=\\Theta(1)$ . This can either be caused by changes in the last-layer weights, by non-vanishing initial function $W_{0}^{L+1}x_{0}^{L}$ (if and only if $b_{L+1}=1/2)$ or by $W_{0}^{L+1}\\tilde{\\delta}x_{0}^{L}=\\Theta(1)$ , which holds if and only if $b_{L+1}+\\tilde{r}_{L}=1$ (analogously, $W_{0}^{L+1}\\tilde{\\delta}x_{0}^{L}={\\cal O}(1)$ if and only if $b_{L+1}+\\tilde{r}_{L}\\,\\geq\\,1)$ . The first case requires $\\tilde{\\delta}W_{0}^{L+1}=\\Theta(n^{-1})$ , since $\\tilde{\\delta}W_{0}^{L+1}$ and $\\tilde{x}_{0}^{L}$ are highly correlated. $\\tilde{\\delta}W_{0}^{L+1}=\\Theta(n^{-1})$ is fulfilled if and only if $d+d_{L+1}=1$ (the analogue to $c_{L+1}\\geq1$ for stability and $c_{L+1}=1$ for nontriviality). ", "page_idx": 25}, {"type": "text", "text": "Hence perturbation stability of the output function holds only if $d+d_{L+1}\\geq1$ and $b_{L+1}+\\tilde{r}_{L}\\ge1$ .   \nThen, perturbation nontriviality holds if and only if $d+d_{L+1}=1$ or $b_{L+1}+\\tilde{r}_{L}=1$ . ", "page_idx": 25}, {"type": "text", "text": "In the $t$ -th backward pass, $b_{L+1}+\\tilde{r}_{L}\\;\\geq\\;1$ will be replaced by the slightly stronger constraint $b_{L+1}+\\tilde{r}\\ge1$ . ", "page_idx": 25}, {"type": "text", "text": "E.1.4 $t$ -th forward pass ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Formally, we sum the updates in each step, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{W}_{t}^{L+1}:=\\hat{W}_{0}^{L+1}+\\theta_{L+1/\\nabla}(\\delta W_{1}^{L+1}+\\cdot\\cdot\\cdot+\\delta W_{t}^{L+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\delta W_{t+1}^{L+1}:=-\\eta\\ \\tilde{\\chi}_{t}\\ (\\tilde{x}_{t}^{L})^{T}$ denotes the normalized change in the weights $W^{L+1}$ (as a row vector) of scaling $\\theta_{L+1}=\\theta_{W^{L+1}}=n^{-c_{L+1}}$ under perturbation stability and nontriviality so that $\\hat{W}_{t}^{L+1}$ scales as $\\theta_{\\nabla}~=~n^{-\\operatorname*{min}\\left(b_{L+1},c_{L+1}\\right)}$ . $\\delta W_{t+1}^{L+1}$ should not be confused with $\\tilde{\\delta}W_{t+1}^{L+1}$ which denotes the perturbation of the weights at time $t+1$ . For every nontrivial stable parametrization we have $\\tilde{\\chi}_{t}\\,=\\,\\Theta(1)$ and $\\tilde{x}_{t}^{L}\\,=\\,\\Theta(1)$ which requires $\\tilde{\\theta}_{L}\\ \\leq\\ \\overset{\\cdot}{1}$ . In the case $c_{L+1}\\,<\\,b_{L+1}$ , we write W\u02c6 tL+1 : $\\hat{W}_{t}^{L+1}\\,:=\\,n^{-b_{L+1}+c_{L+1}}\\hat{W}_{0}^{L+1}+(\\delta W_{1}^{L+1}+\\cdot\\cdot\\cdot+\\delta W_{t}^{L+1})$ with the same scaling $\\theta_{\\nabla{\\mathbf{\\tau}}}=$ $n^{-\\operatorname*{min}(b_{L+1},c_{L+1})}$ . ", "page_idx": 25}, {"type": "text", "text": "For preactivations and activations we also sum the changes from each step, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t}^{l}:=h_{0}^{l}+\\theta_{l}(\\delta h_{1}^{l}+\\cdot\\cdot\\cdot+\\delta h_{t}^{l}),\\qquad x_{t}^{l}:=x_{0}^{l}+\\theta_{l}(\\delta x_{1}^{l}+\\cdot\\cdot\\cdot+\\delta x_{t}^{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the fact that ", "page_idx": 25}, {"type": "equation", "text": "$$\nW_{t}^{1}-W_{t-1}^{1}=-\\eta\\tilde{\\chi}_{t-1}\\theta_{W^{1}}d h_{S A M,t-1}^{1}\\xi_{t-1}^{T},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "yields the normalized preactivation updates ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\delta h_{t}^{1}(\\xi):=-\\eta\\tilde{\\chi}_{t-1}d h_{S A M,t-1}^{1}\\xi_{t-1}^{T}\\xi\\quad\\mathrm{(NL)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with scaling $\\theta_{1}=\\theta_{W^{1}}=n^{-c_{1}}\\theta_{S A M}=n^{-c_{1}-\\mathrm{min}(b_{L+1},c_{L+1},d+d_{L+1})}\\ ,$ as for SGD under perturbation stability and nontriviality where $\\tilde{\\chi}_{t-1}=\\Theta(1)$ . ", "page_idx": 25}, {"type": "text", "text": "For $l\\in[2,L]$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\nW_{t}^{l}-W_{t-1}^{l}=-\\eta\\tilde{\\chi}_{t-1}\\theta_{W^{l}}\\frac{1}{n}d h_{S A M,t-1}^{l}(\\tilde{x}_{t-1}^{l-1})^{T},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with the right scaling $\\theta_{W^{l}}=n^{1-c_{l}-\\operatorname*{min}(b_{L+1},c_{L+1},d+d_{L+1})}$ as for SGD under perturbation stability $\\tilde{x}_{t-1}^{l-1}=\\Theta(1)$ , so that we get $\\delta h_{t}^{l}$ using a telescope sum, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\theta_{l}\\delta h_{t}^{l}=}&{}&{W_{t}^{l}x_{t}^{l-1}-W_{t-1}^{l}x_{t-1}^{l-1}=W_{t-1}^{l}(x_{t}^{l-1}-x_{t-1}^{l-1})+(W_{t}^{l}-W_{t-1}^{l})x_{t}^{l-1}}\\\\ {=}&{}&{\\theta_{l-1}\\left(W_{0}^{l}\\delta x_{t}^{l-1}+\\displaystyle\\sum_{s=1}^{t-1}(W_{s}^{l}-W_{s-1}^{l})\\delta x_{t}^{l-1}\\right)+(W_{t}^{l}-W_{t-1}^{l})x_{t}^{l-1}}\\\\ &{=}&{\\theta_{l-1}\\left(W_{0}^{l}\\delta x_{t}^{l-1}-\\eta\\theta_{W}{!}\\displaystyle\\sum_{s=1}^{t-1}\\tilde{\\chi}_{s-1}\\displaystyle\\frac{(\\tilde{x}_{s-1}^{l-1})^{T}\\delta x_{t}^{l-1}}{n}d h_{S A M,s-1}^{l}\\right)}\\\\ &{}&{\\quad-\\eta\\theta_{W}\\nu_{t}\\tilde{\\chi}_{t-1}\\displaystyle\\frac{(\\tilde{x}_{t-1}^{l-1})^{T}x_{t}^{l-1}}{n}d h_{S A M,t-1}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which defines a NonLin operation with the vectors $W_{0}^{l}\\delta x_{t}^{l-1},d h_{S A M,0}^{l},d h_{S A M,t-1}^{l}$ and everything else treated as scalars. The scaling is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\theta_{l}=\\operatorname*{max}(\\theta_{l-1},\\theta_{W^{l}}\\theta_{l-1},\\theta_{W^{l}})=\\operatorname*{max}_{m=1}^{l}\\theta_{W^{m}}=n^{-r_{l}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{l}:=\\operatorname*{min}(b_{L+1},c_{L+1},d+d_{L+1})+\\operatorname*{min}_{m=1}^{l}(c_{m}-\\mathbb{I}(m\\neq1)),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\theta_{W^{l}}\\leq1$ for all $l\\in[L]$ for stability. Note that for $l_{1}\\leq l_{2}$ , it holds that $\\theta_{l_{1}}\\leq\\theta_{l_{2}}$ , which explains the sufficiency of $\\theta_{L}=n^{-r_{L}}=n^{-r}$ for the stability of the activation updates. ", "page_idx": 26}, {"type": "text", "text": "Activations with the same scaling $\\theta_{l}$ can then simply be defined via the NonLin operation ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\delta x_{t}^{l}:=\\theta_{l}^{-1}(\\phi(h_{t-1}^{l}+\\theta_{l}\\delta h_{t}^{l})-\\phi(h_{t-1}^{l})).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The updates of the output function are scalars defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\delta f_{t}:=\\theta_{L+1}^{\\prime}\\frac{\\delta W_{t}^{L+1}x_{t}^{L}}{n}+\\theta_{L\\nabla}^{\\prime}\\frac{\\hat{W}_{t-1}^{L+1}\\delta x_{t}^{L}}{n},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where \u03b8\u2032L+1 $\\theta_{L+1}^{\\prime}=n\\theta_{L+1}=n^{1-c_{L+1}}$ and $\\theta_{L\\nabla}^{\\prime}=n\\theta_{\\nabla}\\theta_{L}=n^{1-\\operatorname*{min}\\left(b_{L+1},c_{L+1}\\right)-r_{L}}$ , where we will see why $W_{t-1}^{L+1}=\\Theta(n^{-\\operatorname*{min}(b_{L+1},c_{L+1})})$ in the next paragraph. This leads to the constraints $c_{L+1}\\geq1$ and $b_{L+1}+r\\ge1$ for the stability of the output function, where equality in either constraint leads to nontriviality. ", "page_idx": 26}, {"type": "text", "text": "E.1.5 $t$ -th backward pass", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Perturbations. Due to linearity and stability, the last layer remains ", "page_idx": 26}, {"type": "equation", "text": "$$\nd x_{t}^{L}:=\\hat{W}_{t}^{L+1},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with scaling \u03b8\u2207= n\u2212min(bL+1,cL+1). ", "page_idx": 26}, {"type": "text", "text": "As in the first backward pass, we use the weight updates to directly compute the preactivation and activation perturbations similar to the $t$ -th forward pass but performing SGD instead of SAM in the last step. The SGD backward pass for the perturbation is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d h_{t}^{l}:=}&{}&{d x_{t}^{l}\\odot\\phi^{\\prime}(h_{t}^{l}),}\\\\ {d x_{t}^{l-1}:=}&{}&{(W_{t}^{l})^{T}d h_{t}^{l}}\\\\ {=}&{}&{\\left(W_{0}^{l}-\\eta\\theta_{W^{l}}\\displaystyle\\sum_{s=1}^{t}\\tilde{\\chi}_{s-1}\\displaystyle\\frac{1}{n}d h_{S A M,s-1}^{l}(\\tilde{x}_{s-1}^{l-1})^{T}\\right)^{T}d h_{t}^{l}}\\\\ {=}&{}&{W_{0}^{l}d h_{t}^{l}-\\eta(n^{1-c_{l}}\\theta_{S A M}\\theta\\nabla)\\displaystyle\\sum_{s=1}^{t}\\tilde{\\chi}_{s-1}\\displaystyle\\frac{(d h_{S A M,s-1}^{l})^{T}d h_{t}^{l}}{n}\\tilde{x}_{s-1}^{l-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with scaling $\\operatorname*{max}(\\theta_{\\nabla},n^{1-c_{l}}\\theta_{S A M}\\theta_{\\nabla})=\\theta_{\\nabla}$ , since $n^{1-c_{l}}\\theta_{S A M}\\leq1$ is implied by $r\\geq0$ required for the stability of (pre-)activation updates. ", "page_idx": 26}, {"type": "text", "text": "We write $\\chi_{t}=\\mathcal{L}^{\\prime}(f_{t}(\\xi_{t}),y_{t})$ for the derivative of the loss with respect to the unperturbed function (which is $\\Theta(1)$ under stability and nontriviality), and get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\tilde{\\delta}h_{t}^{1}(\\xi):=}&{}&{+\\frac{\\rho\\chi_{t}(\\xi_{t}^{T}\\xi)d h_{t}^{1}}{\\|v_{t}\\|},}\\\\ {\\tilde{\\theta}_{l}\\tilde{\\delta}h_{t}^{l}:=}&{}&{\\tilde{\\theta}_{l-1}W_{t}^{l}\\tilde{\\delta}x_{t}^{l-1}+(\\tilde{W}_{t}^{l}-W_{t}^{l})\\tilde{x}_{t}^{l-1}}\\\\ {=}&{}&{\\tilde{\\theta}_{l-1}\\left(W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1}+\\displaystyle\\sum_{s=1}^{t}(W_{s}^{l}-W_{s-1}^{l})\\tilde{\\delta}x_{t}^{l-1}\\right)+(\\tilde{W}_{t}^{l}-W_{t}^{l})\\tilde{x}_{t}^{l-1}}\\\\ {=}&{}&{\\tilde{\\theta}_{l-1}\\left(W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1}-\\eta(n^{1-c_{l}}\\theta_{S A M})\\displaystyle\\sum_{s=1}^{t}\\tilde{\\chi}_{s-1}\\displaystyle\\frac{(\\tilde{x}_{s-1}^{l-1})^{T}\\tilde{\\delta}x_{t}^{l-1}}{n}d h_{S A M,s-1}^{l}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n+\\rho\\widetilde{\\theta}_{W^{l}}\\frac{\\chi_{t}}{\\lVert v_{t}\\rVert}\\frac{(x_{t}^{l-1})^{T}\\widetilde{x}_{t}^{l-1}}{n}d h_{t}^{l},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which defines a NonLin operation with the vectors $W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1},d h_{S A M,0}^{l},\\dots,d h_{S A M,t-1}^{l},d h_{t}^{l}$ and where we can now define the definitive scalings $\\begin{array}{r c c c c l}{{\\tilde{\\theta}_{1}}}&{{:=}}&{{\\tilde{\\theta}_{W^{1}}}}&{{:=}}&{{n^{-(d+d_{1})}\\theta_{\\nabla}}}&{{=}}\\end{array}$ $n^{-(\\mathrm{min}\\left(b_{L+1},c_{L+1}\\right)+d+d_{1})}$ , $\\begin{array}{r c l}{{\\tilde{\\theta}_{W^{l}}}}&{{:=}}&{{n^{1-(d+d_{l})}\\theta_{\\nabla}}}&{{=}}&{{n^{-(\\operatorname*{min}(b_{L+1},c_{L+1})+d+(d_{l}-1))}}}\\end{array}$ and $\\begin{array}{r l}{\\tilde{\\theta}_{l}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}(\\tilde{\\theta}_{l-1},n^{1-c_{l}}\\theta_{S A M}\\tilde{\\theta}_{l-1},\\tilde{\\theta}_{W^{l}})=\\operatorname*{max}_{m=1}^{l}\\tilde{\\theta}_{W^{m}}=n^{-\\tilde{r}_{l}}}\\end{array}$ with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{r}_{l}:=\\operatorname*{min}(b_{L+1},c_{L+1})+d+\\operatorname*{min}_{m=1}^{l}(d_{m}-\\mathbb{I}(m\\neq1)),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used that $n^{1-c_{l}}\\theta_{S A M}\\leq1$ due to $r\\geq0$ for stability and $\\tilde{x}_{t}^{l-1}=\\Theta(1)$ due to perturbation stability. Perturbation stability of the hidden layer (pre-)activations $\\tilde{\\delta}h^{l},\\tilde{\\delta}x^{l}={\\cal O}(1)$ for all $l\\in[L]$ holds if and only if $\\tilde{r}:=\\tilde{r}_{L}\\ge0$ since $\\tilde{r}_{l}\\geq\\tilde{r}_{L}$ for all $l\\leq L$ . ", "page_idx": 27}, {"type": "text", "text": "The activation perturbations $\\tilde{\\delta}x_{t}^{l}$ and the perturbation of the output function ${\\tilde{\\delta}}f_{t}$ can be defined exactly as in the first backward pass, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\tilde{\\delta}x_{t}^{l}:=}&{}&{\\tilde{\\theta}_{l}^{-1}(\\phi(h_{t}^{l}+\\tilde{\\theta}_{l}\\tilde{\\delta}h_{t}^{l})-\\phi(h_{t}^{l})),}\\\\ {\\tilde{\\delta}f_{t}:=}&{}&{\\tilde{W}_{t}^{L+1}\\tilde{x}_{t}^{L}-W_{t}^{L+1}x_{t}^{L}=\\tilde{\\theta}_{L+1}^{\\prime}\\frac{\\tilde{\\delta}W_{t}^{L+1}\\tilde{x}_{t}^{L}}{n}+\\tilde{\\theta}_{L\\nabla}^{\\prime}\\frac{\\hat{W}_{t}^{L+1}\\tilde{\\delta}x_{t}^{L}}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\begin{array}{r}{\\tilde{\\delta W}_{t}^{L+1}\\,:=\\,\\frac{\\rho\\,\\chi_{t}\\,\\,x_{t}^{L}}{\\|v_{t}\\|}}\\end{array}$ and the same scalings $\\tilde{\\theta}_{l}$ , $\\tilde{\\theta}_{L+1}^{\\prime}\\,=\\,n^{1-(d+d_{L+1})}$ and $\\tilde{\\theta}_{L\\nabla}^{\\prime}\\,=\\,n\\theta_{\\nabla}\\tilde{\\theta}_{L}\\,=$ n1\u2212min(bL+1,cL+1)\u2212r\u02dc since W tL $W_{t}^{L+1}=W_{0}^{L+1}+\\Delta W_{t}^{L+1}=\\operatorname*{max}(n^{-b_{L+1}},n^{-c_{L+1}}),$ which yields the slightly stronger constraint (than in the first backward pass) $\\operatorname*{min}(b_{L+1},c_{L+1})\\!+\\!\\widetilde{r}\\geq1$ for perturbation stability and either $\\tilde{\\theta}_{L+1}^{\\prime}=1$ or $\\operatorname*{min}(b_{L+1},c_{L+1})+\\tilde{r}=1$ for perturbation nontriviality. ", "page_idx": 27}, {"type": "text", "text": "SAM Update. For each $l\\in\\{1,\\ldots,L\\}$ , as in the first backward pass, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nd x_{S A M,t}^{L}:=\\hat{W}_{t}^{L+1}+\\tilde{\\theta}_{(L+1)/\\nabla}\\ \\tilde{\\delta}W_{t}^{L+1},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with scaling $\\theta_{S A M}=n^{-\\operatorname*{min}(b_{L+1},c_{L+1},d_{L+1}+1/2)}$ as well as ", "page_idx": 27}, {"type": "equation", "text": "$$\nd h_{S A M,t}^{l}:=d x_{S A M,t}^{l}\\odot\\phi^{\\prime}(\\tilde{h}_{t}^{l}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $d x_{S A M,t}^{l}$ we again use a telescope sum over the weight changes, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d x_{S A M,t}^{l-1}:=}&{}&{(\\tilde{W}_{t}^{l})^{T}d h_{S A M,t}^{l}=(W_{0}^{l}+\\theta_{W^{l}}\\displaystyle\\sum_{s=1}^{t}\\delta W_{s}^{l}+\\tilde{\\theta}_{W^{l}}\\tilde{\\delta}W_{t}^{l})^{T}d h_{S A M,t}^{l}}\\\\ {=}&{}&{(W_{0}^{l})^{T}d h_{S A M,t}^{l}-\\eta(n^{1-c_{l}}\\theta_{S A M})\\displaystyle\\sum_{s=1}^{t}\\tilde{\\chi}_{s-1}\\displaystyle\\frac{(d h_{S A M,s-1}^{l})^{T}d h_{S A M,t}^{l}}{n}\\tilde{x}_{s-1}^{l-1}}\\\\ &{}&{+\\rho(n^{1/2-d_{l}}\\theta_{\\nabla})\\displaystyle\\frac{\\chi_{t}}{\\|v_{t}\\|}\\displaystyle\\frac{(d h_{t}^{l})^{T}d h_{S A M,t}^{l}}{n}x_{t}^{l-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which defines a NonLin operation in the vectors $(W_{0}^{l})^{T}d h_{S A M,t}^{l},\\tilde{x}_{0}^{l-1},\\dots,\\tilde{x}_{t-1}^{l-1},x_{t}^{l-1}$ and everything else treated as scalars. Note that the scalings remain $\\theta_{S A M}$ , since $\\nabla_{x_{t}^{l-1}}f|_{\\tilde{W}_{t}}~~=$ $\\Theta(\\operatorname*{max}(\\theta_{S A M},n^{1-c_{l}}\\theta_{S A M}^{2},n^{1/2-d_{l}}\\theta_{\\nabla}\\theta_{S A M}))=\\Theta(\\theta_{S A M})$ under stability, nontriviality, perturbation stability and perturbation nontriviality. ", "page_idx": 27}, {"type": "text", "text": "Finally define the loss derivative on the perturbed output function ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{\\chi}_{t}:=\\mathcal{L}^{\\prime}(\\tilde{W}_{t}^{L+1}\\tilde{x}_{t}^{L},y_{t}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and compute the normalized change in $W^{L+1}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\delta W_{t+1}^{L+1}:=-\\eta\\tilde{\\chi}_{t}\\tilde{x}_{t}^{L}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E.2 The infinite-width limit ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we apply the Master Theorem\u2019s computation rules to derive the marginal distributions $Z$ corresponding to the vectors of the program constructed above. According to the Master Theorem, each such vector $z$ will have roughly iid coordinates distributed like $Z^{z}$ in the large $n$ limit. ", "page_idx": 28}, {"type": "text", "text": "We assume stability holds, so that $\\theta\\rightarrow\\overset{\\circ}{\\theta}\\in\\{0,1\\}$ for all scalars $\\theta$ in the program. For the first forward pass, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{h_{0}^{1}(\\xi)}=\\xi Z^{W_{0}^{1}},\\qquad Z^{x_{0}^{l}(\\xi)}=\\phi\\big(Z^{h_{0}^{l}(\\xi)}\\big),\\qquad Z^{h_{0}^{l+1}(\\xi)}=Z^{W_{0}^{l+1}x_{0}^{l}(\\xi)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If $b_{L+1}>1/2$ then $\\mathring{f}_{0}=0$ , otherwise if $b_{L+1}=1/2$ then $\\mathring{f}_{0}$ converges to a nontrivial Gaussian. For the details we refer to Appendix H.4.1 in Yang and $\\operatorname{Hu}$ (2021), as at initialization their results still hold here. ", "page_idx": 28}, {"type": "text", "text": "For the first SGD backward pass, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{d x_{0}^{L}(\\xi)}=Z^{\\hat{W}_{0}^{L+1}},\\qquad Z^{d h_{0}^{l}(\\xi)}=Z^{d x_{0}^{l}(\\xi)}\\phi^{\\prime}\\bigl(Z^{h_{0}^{l}(\\xi)}\\bigr),\\qquad Z^{d x_{0}^{l-1}(\\xi)}=Z^{(W_{0}^{l})^{T}d h_{0}^{l}(\\xi)},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\dot{Z}^{d x_{0}^{l}(\\xi)}=0$ and $Z^{d x_{0}^{l}(\\xi)}=\\hat{Z}^{d x_{0}^{l}(\\xi)}$ for all $\\xi\\in\\mathcal X$ . ", "page_idx": 28}, {"type": "text", "text": "For general $t>0$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Z^{d x_{t}^{L}(\\xi)}=}&{{}}&{Z^{\\hat{W}_{t}^{L+1}},}\\\\ {Z^{d h_{t}^{l}(\\xi)}=}&{{}}&{Z^{d x_{t}^{l}(\\xi)}\\phi^{\\prime}(Z^{h_{t}^{l}(\\xi)}),}\\\\ {Z^{d x_{t}^{l-1}(\\xi)}=}&{{}}&{Z^{(W_{0}^{l})^{T}d h_{t}^{l}(\\xi)}-\\eta\\mathring\\theta_{W^{l}}\\sum_{s=1}^{t}\\mathring\\chi_{s-1}\\mathbb{E}[Z^{d h_{S A M,s-1}^{l}}Z^{d h_{t}^{l}}]Z^{\\tilde{x}_{s-1}^{l-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathring{\\tilde{\\chi}}_{s}=\\mathcal{L}^{\\prime}(\\mathring{\\tilde{f}_{s}}(\\xi_{s}),y_{s})$ for $s<t$ , and $Z^{(W_{0}^{l})^{T}d h_{t}^{l}(\\xi)}$ is a $\\Theta(1)$ random variable distributed as ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{(W_{0}^{l})^{T}d h_{t}^{l}(\\xi)}=\\hat{Z}^{(W_{0}^{l})^{T}d h_{t}^{l}(\\xi)}+\\sum_{v\\in\\mathcal{V}:\\;W_{0}^{l}v\\in\\mathcal{V}}Z^{v}\\;\\mathbb{E}\\frac{\\partial Z^{d h_{t}^{l}(\\xi)}}{\\partial\\hat{Z}^{W_{0}^{l}v}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For all $t\\geq0$ , the limit of the gradient norm is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\tilde{v}\\|=\\mathring\\chi_{t}\\left(\\mathring{\\theta}_{\\|v^{1}\\|}^{2}\\mathbb{E}[Z^{(d h_{t}^{1})^{2}}](\\xi_{t}^{T}\\xi_{t})+\\sum_{l=2}^{L}\\mathring\\theta_{\\|v^{l}\\|}^{2}\\mathbb{E}[Z^{(d h_{t}^{l})^{2}}]\\mathbb{E}[Z^{(x_{t}^{l-1})^{2}}]+\\mathring\\theta_{\\|v^{L+1}\\|}^{2}\\frac{(x_{t}^{L})^{T}x_{t}^{L}}{n}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathring{\\chi}_{t}=\\mathcal{L}^{\\prime}(\\mathring{f}_{t}(\\xi_{t}),y_{t})$ , $\\theta_{\\parallel v^{1}\\parallel}^{2}:=n^{1-2d_{1}}\\theta_{\\nabla}^{2}$ , $\\theta_{\\parallel v^{l}\\parallel}^{2}:=n^{2-2d_{l}}\\theta_{\\nabla}^{2}$ for $l\\in[2,L]$ and $\\theta_{\\parallel v^{L+1}\\parallel}^{2}:=$ $n^{1-2d_{L+1}}$ , and where $\\mathring{\\theta}_{||v^{L+1}||}^{2}\\,=\\,1$ if and only if $d_{L+1}\\,=\\,1/2$ and \u03b8\u02da\u22252vL+1\u2225 = 0 if and only if $d_{L+1}>1/2$ , while $\\mathring{\\theta}_{||v^{l}||}^{2}=1$ if and only if $2d_{l}=1+\\mathbb{I}(l>1)-2\\operatorname*{min}(b_{L+1},c_{L+1})$ and $\\mathring{\\theta}_{\\|v^{l}\\|}^{2}=0$ if and only if $2d_{l}>1+\\mathbb{I}(l>1)-2\\operatorname*{min}(b_{L+1},c_{L+1})$ . ", "page_idx": 28}, {"type": "text", "text": "For the last-layer weight perturbations (for $\\theta_{\\nabla}\\geq\\tilde{\\theta}_{L+1}$ , else $Z^{\\hat{\\Tilde{W}}_{t}^{L+1}}=Z^{\\Tilde{\\delta}W_{t}^{L+1}})$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{\\hat{W}_{t}^{L+1}}=Z^{\\hat{W}_{t}^{L+1}}+\\mathring{\\tilde{\\theta}}_{(L+1)/\\nabla}Z^{\\tilde{\\delta}W_{t}^{L+1}},\\qquad Z^{\\tilde{\\delta}W_{t}^{L+1}}=\\frac{\\rho\\,\\mathring{\\chi}_{t}}{\\|\\mathring{v}\\|}Z^{x_{t}^{L}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that $\\mathring{\\chi}_{t}$ cancels itself out and we purely get a perturbation in distribution $Z^{x_{t}^{L}}$ scaled to have standard deviation $\\rho$ . ", "page_idx": 28}, {"type": "text", "text": "For all $t\\geq0$ and $l\\in[1,L]$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{\\tilde{h}_{t}^{l}}=Z^{h_{t}^{l}}+\\mathring{\\tilde{\\theta}}_{l}Z^{\\tilde{\\delta}h_{t}^{l}},\\qquad Z^{\\tilde{x}_{t}^{l}}=Z^{x_{t}^{l}}+\\mathring{\\tilde{\\theta}}_{l}Z^{\\tilde{\\delta}x_{t}^{l}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where for $l=1$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{\\tilde{\\delta}h_{t}^{1}(\\xi)}=+\\frac{\\rho\\mathring{\\chi}_{t}(\\xi_{t}^{T}\\xi)}{\\|\\mathring{v}\\|}Z^{d h_{t}^{1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If ${\\overset{\\circ}{\\tilde{\\theta}}}_{l}=0$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\nZ^{\\tilde{\\delta}x_{t}^{l}}=\\phi^{\\prime}(Z^{h_{t}^{l}})Z^{\\tilde{\\delta}h_{t}^{l}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "otherwise $\\mathring{\\tilde{\\theta}}_{l}=1$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\nZ^{\\tilde{\\delta}x_{t}^{l}}=\\phi(Z^{\\tilde{h}_{t}^{l}})-\\phi(Z^{h_{t}^{l}}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $l\\geq2$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Z^{\\tilde{\\delta}h_{t}^{l}}=}&{}&{\\mathring{\\tilde{\\theta}}_{(l-1)/l}\\ Z^{W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1}}-\\eta\\mathring{\\theta}_{W^{l}(\\tilde{l}-1)/\\tilde{l}}\\displaystyle\\sum_{s=1}^{t}\\mathring{\\tilde{\\chi}}_{s-1}\\mathbb{E}[Z^{\\tilde{x}_{s-1}^{l-1}}Z^{\\tilde{\\delta}x_{t}^{l-1}}]Z^{d h_{s A M,s-1}^{l}}}\\\\ &{}&{+\\rho\\mathring{\\tilde{\\theta}}_{W^{l}/l}\\displaystyle\\frac{\\mathring{\\tilde{\\chi}}_{t}}{\\lVert\\tilde{\\boldsymbol{v}}\\rVert}\\mathbb{E}[Z^{x_{t}^{l-1}}Z^{\\tilde{x}_{t}^{l-1}}]Z^{d h_{t}^{l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where \u03b8\u02dc(l\u22121)/l $\\begin{array}{r}{\\tilde{\\theta}_{(l-1)/l}=\\frac{\\tilde{\\theta}_{l-1}}{\\tilde{\\theta}_{l}}}\\end{array}$ \u03b8\u02dcl\u03b8\u02dc\u2212l1 , \u03b8W l(l\u02dc\u22121)/l\u02dc = $\\begin{array}{r}{\\theta_{W^{l}(\\tilde{l}-1)/\\tilde{l}}=\\frac{\\theta_{W^{l}}\\tilde{\\theta}_{l-1}}{\\tilde{\\theta}_{l}}}\\end{array}$ $\\begin{array}{r}{\\tilde{\\theta}_{W^{l}/l}=\\frac{\\tilde{\\theta}_{W^{l}}}{\\tilde{\\theta}_{l}}}\\end{array}$ , and $Z^{W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1}}$ has the decomposition ", "page_idx": 29}, {"type": "equation", "text": "$$\nZ^{W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1}}=\\hat{Z}^{W_{0}^{l}\\tilde{\\delta}x_{t}^{l-1}}+\\sum_{\\substack{v\\in\\mathcal{V}\\colon(W_{0}^{l})^{T}v\\in\\mathcal{V}}}Z^{v}\\;\\mathbb{E}\\frac{\\partial Z^{\\tilde{\\delta}x_{t}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}v}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The perturbed output function has the limit $\\stackrel{\\circ}{\\tilde{f}}_{t}:=\\tilde{f}_{t}+\\tilde{\\tilde{\\delta}}f_{t}$ with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathring{\\delta}f_{t}:=\\mathring{\\tilde{\\theta}}_{L+1}^{\\prime}\\mathbb{E}\\big[Z^{\\tilde{\\delta}W_{t}^{L+1}}Z^{\\tilde{x}_{t}^{L}}\\big]+\\mathring{\\tilde{\\theta}}_{L\\nabla}^{\\prime}\\mathbb{E}\\big[Z^{\\hat{W}_{t}^{L+1}}Z^{\\tilde{\\delta}x_{t}^{L}}\\big],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so that we can define $\\mathring{\\tilde{\\chi}}_{t}=\\mathcal{L}^{\\prime}\\big(\\mathring{\\tilde{f}}_{t}(\\xi_{t}),y_{t}\\big)$ or equivalently $\\tilde{\\tilde{\\chi}}_{t}=\\mathcal{L}^{\\prime}(\\tilde{\\tilde{\\theta}}_{L+1}\\tilde{\\tilde{\\theta}}_{L}\\mathbb{E}[Z^{\\hat{\\tilde{W}}_{t}^{L+1}}Z^{\\tilde{x}_{t}^{L}}],y_{t}).$ . For the SAM gradients we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Z^{d x_{S A M,t}^{L}}=}&{}&{Z^{\\hat{W}_{t}^{L+1}}+\\overset{\\circ}{\\hat{\\theta}}_{(L+1)/\\nabla}Z^{\\hat{\\delta}W_{t}^{L+1}},}\\\\ {Z^{d h_{S A M,t}^{l}}=}&{}&{Z^{d x_{S A M,t}^{l}}\\cdot\\phi^{\\prime}(Z^{\\hat{h}_{t}^{l}})}\\\\ {Z^{d x_{S A M,t}^{l-1}}=}&{}&{Z^{(W_{0}^{l})^{T}d h_{S A M,t}^{l}}-\\eta\\mathring{\\theta}_{W^{l}}\\overset{\\circ}{\\underset{s=1}{\\sum}}\\mathring{\\tilde{\\chi}}_{s-1}\\mathbb{E}[Z^{d h_{S A M,s-1}^{l}}Z^{d h_{S A M,t}^{l}}]Z^{\\hat{x}_{s-1}^{l-1}}}\\\\ &{}&{+\\rho\\mathring{\\theta}_{W^{l}}\\overset{\\circ}{\\underset{\\parallel}{\\prod}}\\mathbb{E}[Z^{d h_{t}^{l}}Z^{d h_{S A M,t}^{l}}]Z^{x_{t}^{l-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $Z^{(W_{0}^{l})^{T}d h_{S A M,t}^{l}}$ is given by ", "page_idx": 29}, {"type": "equation", "text": "$$\nZ^{(W_{0}^{l})^{T}d h_{S A M,t}^{l}}=\\hat{Z}^{(W_{0}^{l})^{T}d h_{S A M,t}^{l}}+\\sum_{v\\in\\mathcal{V}:\\;W_{0}^{l}v\\in\\mathcal{V}}Z^{v}\\;\\mathbb{E}\\frac{\\partial Z^{d h_{S A M,t}^{l}}}{\\partial\\hat{Z}^{W_{0}^{l}v}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now SAM\u2019s (pre-)activation updates are given by ", "page_idx": 29}, {"type": "equation", "text": "$$\nZ^{h_{t}^{l}}=Z^{h_{0}^{l}}+\\mathring\\theta_{l}(Z^{\\delta h_{1}^{l}}+\\cdot\\cdot\\cdot+Z^{\\delta h_{t}^{l}}),\\qquad Z^{x_{t}^{l}}=Z^{x_{0}^{l}}+\\mathring\\theta_{l}(Z^{\\delta x_{1}^{l}}+\\cdot\\cdot\\cdot+Z^{\\delta x_{t}^{l}}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with, for $l\\in[2,L]$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Z^{\\delta h_{t}^{1}}(\\xi)=}&{}&{-\\eta\\mathring{\\tilde{\\chi}}_{t-1}{\\left(\\xi_{t-1}^{T}\\xi\\right)}Z^{d h_{S A M,t-1}^{1}},}\\\\ {Z^{\\delta h_{t}^{l}}=}&{}&{\\mathring{\\theta}_{(l-1)/l}\\left(Z^{W_{0}^{l}\\delta x_{t}^{l-1}}-\\eta\\mathring{\\theta}_{W^{l}}\\sum_{s=1}^{t-1}\\mathring{\\tilde{\\chi}}_{s-1}\\mathbb{E}[Z^{\\tilde{x}_{s-1}^{l-1}}Z^{\\delta x_{t}^{l-1}}]Z^{d h_{S A M,s-1}^{l}}\\right)}\\\\ &{}&{-\\eta\\mathring{\\theta}_{W^{l}/l}\\mathring{\\tilde{\\chi}}_{t-1}\\mathbb{E}[Z^{\\tilde{x}_{t-1}^{l-1}}Z^{x_{t}^{l-1}}]Z^{d h_{S A M,t-1}^{l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\theta_{(l-1)/l}:=\\theta_{l-1}/\\theta_{l},\\theta_{W^{l}/l}:=\\theta_{W^{l}}/\\theta_{l}$ and $Z^{W_{0}^{l}\\delta x_{t}^{l-1}}$ has the decomposition ", "page_idx": 29}, {"type": "equation", "text": "$$\nZ^{W_{0}^{l}\\delta x_{t}^{l-1}}=\\hat{Z}^{W_{0}^{l}\\delta x_{t}^{l-1}}+\\sum_{\\substack{v\\in\\mathcal{V}\\colon(W_{0}^{l})^{T}v\\in\\mathcal{V}}}Z^{v}\\;\\mathbb{E}\\frac{\\partial Z^{\\delta x_{t}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}v}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If $\\stackrel{\\circ}{\\theta}_{l}=0$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\nZ^{\\delta x_{t}^{l}}=\\phi^{\\prime}(Z^{h_{t-1}^{l}})Z^{\\delta h_{t}^{l}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "otherwise $\\mathring{\\theta}_{l}=1$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\nZ^{\\delta x_{t}^{l}}=\\phi(Z^{h_{t}^{l}})-\\phi(Z^{h_{t-1}^{l}}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The last-layer SAM weight update is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\nZ^{\\hat{W}_{t}^{L+1}}=Z^{\\hat{W}_{0}^{L+1}}+\\mathring{\\theta}_{L+1/\\nabla}(Z^{\\delta W_{1}^{L+1}}+\\cdot\\cdot\\cdot+Z^{\\delta W_{t}^{L+1}}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with Z\u03b4W tL+1 $Z^{\\delta W_{t}^{L+1}}=-\\eta\\mathring{\\tilde{\\chi}}_{t-1}Z^{\\tilde{x}_{t-1}^{L}}$ ", "page_idx": 30}, {"type": "text", "text": "For $t>0$ , the SAM function update is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathring{f}_{t}=\\mathring{f}_{0}+\\mathring{\\delta}f_{1}+\\cdot\\cdot\\cdot+\\mathring{\\delta}f_{t},}\\\\ &{}&{\\mathring{\\delta}f_{t}=\\mathring{\\theta}_{L+1}^{\\prime}\\mathbb{E}[Z^{\\delta W_{t}^{L+1}}Z^{x_{t}^{L}}]+\\mathring{\\theta}_{L\\nabla}^{\\prime}\\mathbb{E}[Z^{\\hat{W}_{t-1}^{L+1}}Z^{\\delta x_{t}^{L}}].\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E.3 Concluding the proof of all main results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "After writing out the NE $\\otimes$ OR\u22a4program and its limit, as well as tracking all scalings, the main results stated in Appendix D all follow from the Tensor Program Master Theorem and from the characterization results in Yang and Hu (2021) in the following way. ", "page_idx": 30}, {"type": "text", "text": "Formally Yang and Hu (2021) show feature learning for SGD with small enough learning rate $\\eta>0$ by proving $\\partial_{\\eta}^{2}\\mathbb{E}\\big(Z^{x_{1}^{L}(\\xi_{0})}\\big)^{2}\\neq0$ at $\\eta=0$ , and they show that learning does not occur in the kernel regime by showing $\\partial_{\\eta}^{3}\\mathring{f}_{1}\\neq0$ , hence ${\\mathring{f}}_{1}-{\\mathring{f}}_{0}$ is not linear in $\\eta$ . ", "page_idx": 30}, {"type": "text", "text": "Both $\\mathbb{E}(Z^{x_{1}^{L}(\\xi_{0})})^{2}$ and $\\mathring{f}_{1}$ are defined via $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ computations and can be written as a composition of additions, multiplications, the expectation operator, applications of $\\phi$ and $\\phi^{\\prime}$ , overall applications of infinitely differentiable, pseudo-Lipschitz functions to (Gaussian) random variables, $\\eta$ and $\\rho$ . Consequently $\\mathbb{E}(Z^{x_{1}^{L}(\\xi_{0})})^{2}$ and $\\mathring{f}_{1}$ are infinitely often differentiable as a function of both $\\eta$ and $\\rho$ , where differentiating the expectation operator is covered in Yang and Hu (2021, Lemma H.39). Since Yang and Hu (2021) cover the case $\\rho=0$ , their proofs immediately show the correctness of the derived scalings for SAM as long as $\\eta>0$ and $\\rho>0$ are chosen small enough. Both the gradient evaluation for the perturbation as well as the gradient evaluation for the updates stay arbitrarily close to those of SGD if $\\rho>0$ is chosen small enough. The conditions for stability, nontriviality, feature learning, perturbation nontriviality and effective perturbations now follow from considering the respective scaling. ", "page_idx": 30}, {"type": "text", "text": "E.3.1 Proof of Theorem D.2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "A bcd-parameterization is stable if and only if all scalings in the Tensor Program have the limit $\\mathring{\\theta}\\in\\,\\{0,1\\}$ , where $\\mathring{\\theta}=1$ is required for activations at initialization (for which nothing changes compared to SGD). Potential cancellations are taken care of for sufficiently small $\\eta>0$ and $\\rho>0$ by the argument above. Now collecting all constraints that are already stated in the Tensor Program formulation at the respective step concludes the proof. ", "page_idx": 30}, {"type": "text", "text": "E.3.2 Proof of Theorem D.3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "A stable $b c d$ -parameterization is nontrivial if and only if $\\mathring{f}_{t}\\,=\\,\\Theta(1)$ if and only if $\\mathring{\\theta}_{L+1}^{\\prime}=1$ or $\\mathring{\\theta}_{L\\nabla}^{\\prime}=1$ . ", "page_idx": 30}, {"type": "text", "text": "E.3.3 Proof of Theorem D.4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "A stable bcd-parametrization is feature learning in layer $l$ if and only if the feature update scaling $\\mathring{\\theta}_{l}=1$ where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\theta_{l}=n^{-r_{l}},\\quad r_{l}:=\\operatorname*{min}(b_{L+1},c_{L+1},d_{L+1}+1/2)+\\operatorname*{min}_{m=1}^{l}(c_{m}-\\mathbb{I}(m\\neq1)).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence a stable $b c d$ -parametrization is feature learning in layer $l$ if and only if $r_{l}=0$ . ", "page_idx": 30}, {"type": "text", "text": "Since for all $l_{1}\\leq l_{2}$ , it holds that $r_{l_{1}}\\ge r_{l_{2}}\\ge0$ , we get the equivalence for any $l_{0}\\in[L]$ : A stable $b c d$ -parametrization is feature learning in layer $l_{0}$ if and only if it is feature learning in layer $l$ for all $l\\geq l_{0}$ if and only if ${r}_{l_{0}}=0$ . ", "page_idx": 31}, {"type": "text", "text": "E.3.4 Proof of Theorem D.6 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Given a stable bcd-parametrization, perturbation triviality is fulfilled if and only if $\\mathring{\\tilde{\\theta}}_{L+1}^{\\prime}=0$ and $\\mathring{\\tilde{\\theta}}_{L\\nabla}^{\\prime}=0$ , where $\\tilde{\\theta}_{L+1}^{\\prime}=n^{1/2-d_{L+1}}$ and $\\tilde{\\theta}_{L\\nabla}^{\\prime}=n\\theta_{\\nabla}\\tilde{\\theta}_{L}=n^{1-\\operatorname*{min}\\left(b_{L+1},c_{L+1}\\right)-\\tilde{r}}$ , hence if and only if $d_{L+1}>1/2$ and $\\operatorname*{min}(b_{L+1},c_{L+1})+\\tilde{r}>1$ . ", "page_idx": 31}, {"type": "text", "text": "In that case, $\\mathring{\\tilde{f}}_{t}=\\mathring{f}_{t}$ , but $\\mathring{f}_{t}$ may still be affected by non-vanishing SAM perturbations in $\\delta W_{t}^{L+1}$ and $\\delta x_{t}^{L}$ . Only when all SAM perturbations vanish are we effectively only using SGD. By definition, the perturbation scale in the $l$ -th layer vanishes if and only if $\\mathring{\\tilde{\\theta}}_{l}\\,=\\,0$ , where $\\tilde{\\theta}_{l}\\ =\\ n^{-\\tilde{r}_{l}}$ with $\\tilde{r}_{l}\\,=\\,\\mathrm{min}(b_{L+1},c_{L+1})+1/2+\\mathrm{min}_{m=1}^{l}(d_{m}\\,-\\,\\mathbb{I}(m\\,\\neq\\,1))$ , hence if and only if $\\tilde{r}_{l}\\;>\\;0$ . Since $\\tilde{r}_{l}\\,\\geq\\,\\tilde{r}_{L}\\,=\\,\\tilde{r}$ for all $l\\leq L$ , we get $\\mathring{\\tilde{\\theta}}_{l}=0$ for all $l\\in[L]$ if and only if $\\tilde{r}>0$ . Similarly, for any reference layer $l_{0}\\in[L]$ , we get $\\overset{\\circ}{\\widetilde{\\theta}}_{l}=0$ for all $l\\leq l_{0}$ if and only if $\\tilde{r}_{l_{0}}>0$ . In words, for any $l_{0}\\in[L]$ , we have vanishing perturbations in layer $l_{0}$ if and only if we have vanishing perturbations until layer $l_{0}$ if and only if $\\tilde{r}_{l_{0}}>0$ . ", "page_idx": 31}, {"type": "text", "text": "Altogether, a stable bcd-parametrization has vanishing perturbations if and only if $\\tilde{r}>0$ , $d_{L+1}>1/2$ and $\\operatorname*{min}(b_{L+1},c_{L+1})+\\tilde{r}>\\;1$ . This case reduces to the results in Yang and Hu (2021) in the limit. Since stability requires $c_{L+1}\\geq1$ and $\\tilde{r}\\geq0$ , we can rewrite the equivalence conditions as $d_{L+1}\\geq1/2$ and $\\Tilde{r}>\\operatorname*{max}(0,1-b_{L+1})$ . ", "page_idx": 31}, {"type": "text", "text": "E.3.5 Proof of Theorem D.8 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Recall $\\tilde{\\theta}_{W^{1}}:=n^{-(d+d_{1})}\\theta_{\\nabla}$ , $\\tilde{\\theta}_{W^{l}}:=n^{1-(d+d_{l})}\\theta_{\\nabla}$ and, for the last layer $\\tilde{\\theta}_{W^{L+1}}:=n^{-(d+d_{L+1})}$ . ", "page_idx": 31}, {"type": "text", "text": "As opposed to perturbation nontriviality, we are not only interested in $\\tilde{\\theta}_{l}\\,=\\,\\mathrm{max}(\\tilde{\\theta}_{l-1},\\tilde{\\theta}_{W^{l}})\\,=$ $\\operatorname*{max}_{m=1}^{l}\\tilde{\\theta}_{W^{m}}\\rightarrow1$ , but in a non-vanishing contribution of the perturbations in layer $l$ , i.e. ${\\stackrel{\\circ}{\\textstyle7}}\\theta_{W^{l}}=1$ or, for the last layer, $\\mathring{\\boldsymbol{\\theta}}_{L+1}=1$ . ", "page_idx": 31}, {"type": "text", "text": "E.3.6 Proof of Theorem D.9 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The limit of the gradient norm is defined as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program scalar (E.3). Note that for $b_{L+1}\\,>\\,1/2$ , the last-layer scaling strictly dominates all other scalings leading to the simplified gradient norm formula. ", "page_idx": 31}, {"type": "text", "text": "Now consider an arbitrary stable choice of layerwise initialization variances $\\{b_{l}\\}_{l\\in[L+1]}$ and learning rates $\\{c_{l}\\}_{l\\in[L+1]}$ . To fulfill the gradient norm constraints (D.1), we have to choose $\\dot{d}_{l}=C=1/2$ for all $l\\in[L+1]$ , because stability requires $\\operatorname*{min}(b_{L+1},c_{L+1})\\geq1/2$ . Now stability of the output function perturbations requires $d\\geq1/2$ , where $d>1/2$ yields vanishing perturbations and $d=\\Bar{1}/2$ yields effective last-layer SAM through the term $\\widetilde{\\delta}W_{t}^{L+1}\\widetilde{x}_{t}^{L}$ t . After choosing $d\\,\\geq\\,1/2$ , we get $\\tilde{r}\\geq\\operatorname*{min}(b_{L+1},c_{L+1})\\geq1/2>0$ which implies vanishing perturbations in all hidden layers. ", "page_idx": 31}, {"type": "text", "text": "E.3.7 Proof of Proposition D.10 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To achieve non-vanishing gradient norm contribution of the last layer in (D.1), we need to choose $d_{L+1}=1/2$ , which requires $d\\geq1/2$ for stability of the output function perturbations. Achieving non-vanishing gradient norm contributions of all layers requires $d_{1}=1/\\bar{2}-\\operatorname*{min}(b_{L+1},c_{L+1})$ and $d_{l}=1-\\operatorname*{min}(b_{L+1},c_{L+1})$ for $l\\in[2,L]$ , which results in $\\tilde{r}=d\\geq1/2>0$ which implies vanishing perturbations in all hidden layers. ", "page_idx": 31}, {"type": "text", "text": "E.3.8 Proof of Theorem D.11 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Given a stable bcd-parametrization, we know $d+d_{L+1}\\geq1$ , so that the feature learning constraint $r$ is not affected by any stable choice of $d\\cup\\{d_{l}\\}_{l\\in[L+1]}$ . The maximal stable choice of layerwise initialization variances $\\{b_{l}\\}_{l\\in[L+1]}$ and learning rates $\\left\\{c_{l}\\right\\}_{l\\in[L+1]}$ that constitute $\\mu\\mathrm{P}$ is therefore unaffected by the perturbation scalings $d\\cup\\{d_{l}\\}_{l\\in[L+1]}$ . ", "page_idx": 31}, {"type": "text", "text": "Stability of the output function perturbations requires $b_{L+1}+\\tilde{r}\\,\\geq\\,1$ . Hence if $b_{L+1}<1$ , then $\\tilde{r}\\geq1-b_{L+1}>0$ , which implies vanishing perturbations in all hidden layers. ", "page_idx": 32}, {"type": "text", "text": "From now on consider $b_{L+1}\\geq1$ . Recall $c_{\\nabla}:=\\operatorname*{min}(b_{L+1},c_{L+1})$ . In \u00b5P, $,c_{\\nabla}=1$ , but effective perturbations in all layers can be achieved more generally for $c_{\\nabla}\\geq1$ . Choosing $d_{1}=1/2-c_{\\nabla}$ saturates the gradient norm constraint (D.1). To reach effective perturbations already in the first layer $\\tilde{r}_{1}\\;=\\;c_{\\nabla}\\,+\\,d\\,+\\,d_{1}\\;=\\;0$ , we need $d\\ =\\ -1/2$ . For perturbation stability and last-layer effective perturbations, we need $d+d_{L+1}=1$ which requires $\\bar{d}_{L+1}=3/2$ . Achieving perturbation stability and effective perturbations in all hidden layers requires $\\tilde{\\theta}_{W^{l}}=1$ which is equivalent to $c_{\\nabla}+\\dot{d}+d_{l}-\\mathbb{I}(l\\neq\\bar{1})=0$ . For $l\\in[2,L]$ , we therefore need $d_{l}\\,=\\,3/2-c_{\\nabla}$ . This choice of $\\{d_{l}\\}_{l\\in[L+1]}$ achieves effective perturbations in all layers. ", "page_idx": 32}, {"type": "text", "text": "To show uniqueness we iterate through all possibilities of saturating the norm bound constraint (D.1). We have considered the cases $d_{L+1}=1/2$ in (b) leading to vanishing perturbations in all hidden layers and $d_{1}=1/2-c_{\\nabla}$ in (c) with only one choice for effective perturbations in all layers. Lastly consider $d_{l}=1-c_{\\nabla}$ for $l\\in[2,L]$ for non-vanishing gradient contribution of the hidden layers. Note that all hidden layers play the same role in all relevant constraints. Effective perturbations in any hidden layer $l\\in[2,L]$ requires $\\tilde{\\theta}_{W^{l}}=1$ for which we need $d=0$ . But then, as $d_{1}\\geq1/2-c_{\\nabla}$ , it holds that $\\tilde{r}_{1}\\geq1/2$ implying vanishing perturbations in the first layer. This shows the uniqueness of (1). ", "page_idx": 32}, {"type": "text", "text": "For the gradient norm statements, note that the gradient norm $\\|v_{t}\\|$ can be written as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ computation rule (E.2) where the layer scalings in this parameterization are $\\Theta(1)$ for the input layer, $\\Theta(n^{-1/2})$ for hidden layers and $\\Theta(n^{-1})$ for the output layer. Now the Tensor Program master theorem immediately implies the result. ", "page_idx": 32}, {"type": "text", "text": "E.3.9 Proof of Proposition D.12 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Perturbation nontriviality with respect to any hidden layer is equivalent to $\\tilde{r}\\ =\\ 0$ . Since $\\operatorname*{min}(b_{L+1},c_{L+1})\\leq1$ , we get $\\operatorname*{min}(\\bar{b_{L+1}},c_{L+1})^{\\prime}{+\\tilde{r}}\\leq1$ . Since stability requires $\\operatorname*{min}(b_{L+1},c_{L+1})+$ $\\tilde{r}\\geq1$ , we get $\\operatorname*{min}(b_{L+1},c_{L+1})+\\tilde{r}=1$ , which implies perturbation nontriviality with respect to the output. ", "page_idx": 32}, {"type": "text", "text": "E.3.10 Proof of Proposition D.13 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The constraint is the same constraint as in Theorem D.8, which implies effective perturbations in the first layer. Now $\\tilde{r}_{l}\\,\\leq\\,\\tilde{r}_{1}\\,=\\,0$ implies perturbation nontriviality in all hidden layers due to Theorem D.6. ", "page_idx": 32}, {"type": "text", "text": "E.4 Analytic expression of the features after first SAM update ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Below we state the analytic expression of the first SAM update, but leave a closer analysis of its fine-grained dynamics in comparison to SGD to future work. Before looking into the effective perturbation regime, we restate Lemma H.37 in Yang and Hu (2021) with a more detailed proof. ", "page_idx": 32}, {"type": "text", "text": "First, we define $\\ell\\in[L]$ as the unique index that satisfies $\\theta_{L}=\\cdot\\cdot\\cdot=\\theta_{\\ell}=1>\\theta_{\\ell-1}\\ge\\cdot\\cdot\\cdot\\ge\\theta_{1}$ . In words, $\\ell$ is the first layer in which feature learning occurs. Analogously, we define $\\tilde{\\ell}\\in[L]$ as the unique index that satisfies $\\begin{array}{r}{1=\\frac{\\tilde{\\theta}_{L}}{\\tilde{\\theta}_{L}}=\\cdot\\cdot:=\\frac{\\tilde{\\theta}_{\\tilde{\\ell}}}{\\tilde{\\theta}_{L}}>\\frac{\\tilde{\\theta}_{\\tilde{\\ell}-1}}{\\tilde{\\theta}_{L}}\\geq\\cdot\\cdot\\geq\\frac{\\tilde{\\theta}_{1}}{\\tilde{\\theta}_{L}}.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Lemma E.2 (Features after first SGD step). Defining $:=Z^{h_{t}^{l}},\\,\\gamma^{l}(\\eta)=\\mathbb{E}\\phi(Z_{0}^{l})\\phi(Z_{1}^{l})\\,f\\!o r\\,l\\ge1,$ , $\\gamma^{0}=\\xi_{0}^{T}\\xi$ and $\\gamma_{11}^{l}(\\eta)=\\mathbb{E}\\phi^{\\prime}(Z_{0}^{l})\\phi^{\\prime}(Z_{1}^{l})$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nZ_{1}^{\\ell-1}=Z_{0}^{\\ell-1},\\ldots,Z_{1}^{1}=Z_{0}^{1},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and, for all $l\\geq\\ell$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\nZ_{1}^{l}=Z_{0}^{l}+\\mathbb{I}_{l>\\ell}\\hat{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}+\\eta\\beta^{l}Z^{d x_{0}^{l}}\\phi^{\\prime}(Z_{0}^{l}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\beta^{l}$ is defined recursively by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\beta^{l}=\\beta^{l}(\\eta)=-\\mathring\\chi_{0}\\gamma^{l-1}(\\eta)+\\beta^{l-1}(\\eta)\\gamma_{11}^{l-1}(\\eta),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with $\\beta^{\\ell-1}=0$ . Note that $\\beta^{l}(0)<0$ for all $l\\geq\\ell$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. By the defining infinite-width equations, assuming $\\stackrel{\\circ}{\\theta}_{W^{l}/l}=1$ (so minimal stable choice of $c_{l}$ ), ", "page_idx": 33}, {"type": "equation", "text": "$$\nZ_{1}^{l}=Z_{0}^{l}+\\mathring{\\theta}_{(\\ell-1)/\\ell}Z^{W_{0}^{l}\\delta x_{1}^{l-1}}-\\eta\\mathring{\\chi}_{0}\\gamma^{l-1}Z^{d x_{0}^{l}}\\phi^{\\prime}(Z_{0}^{l}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "At $l=\\ell$ , we get $\\mathring{\\theta}_{(\\ell-1)/\\ell}=0$ , whereas for $l>\\ell$ we get $\\stackrel{\\circ}{\\theta}_{(l-1)/l}=1$ , which results in $\\mathring{\\theta}_{(\\ell-1)/\\ell}=\\mathbb{I}_{l>\\ell}$ . Now, for $l>\\ell$ , the second term decomposes into $\\hat{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}$ and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\dot{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}=Z^{d h_{0}^{l}}\\mathbb{E}\\frac{\\partial Z^{\\delta x_{1}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{0}^{l}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since by induction hypothesis, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\angles{z^{\\delta x_{1}^{l-1}}}{=\\phi(Z_{1}^{l-1})}-\\phi(Z_{0}^{l-1})=\\phi\\left(Z_{0}^{l-1}+\\mathbb{I}_{l>\\ell}\\hat{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}+\\eta\\beta^{l-1}Z^{d x_{0}^{l-1}}\\phi^{\\prime}(Z_{0}^{l-1})\\right)-\\phi(Z_{0}^{l-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $Z^{d x_{0}^{l-1}}=Z^{(W_{0}^{l})^{T}d h_{0}^{l}}$ is the only dependence on $\\hat{Z}^{(W_{0}^{l})^{T}d h_{0}^{l}}$ , we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\partial Z^{\\delta x_{1}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{0}^{l}}}=\\phi^{\\prime}(Z_{1}^{l-1})\\eta\\beta^{l-1}\\phi^{\\prime}(Z_{0}^{l-1}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging the derivative back into the defining equation and noticing that $Z^{d h_{0}^{l}}\\ =\\ Z^{d x_{0}^{l}}\\phi^{\\prime}(Z_{0}^{l})$ concludes the proof. ", "page_idx": 33}, {"type": "text", "text": "An analogous analysis for the perturbation at initialization shows. ", "page_idx": 33}, {"type": "text", "text": "Lemma E.3 (Feature perturbation at initialization). The perturbation trivial layers fulfill ", "page_idx": 33}, {"type": "equation", "text": "$$\nZ^{\\tilde{h}_{0}^{\\tilde{\\ell}-1}}=Z^{h_{0}^{\\tilde{\\ell}-1}},\\ldots,Z^{\\tilde{h}_{0}^{1}}=Z^{h_{0}^{1}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and, for all $l\\geq\\tilde{\\ell}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\nZ^{\\tilde{h}_{0}^{l}}=Z^{h_{0}^{l}}+\\mathbb{I}_{l>\\tilde{\\ell}}\\hat{Z}^{W_{0}^{l}\\tilde{\\delta}x_{0}^{l-1}}+\\rho\\tilde{\\beta}^{l}Z^{d x_{0}^{l}}\\phi^{\\prime}(Z^{h_{0}^{l}}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\tilde{\\beta}^{l}$ independent of $\\eta$ is defined recursively by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{\\beta}^{l}=\\tilde{\\beta}^{l}(\\rho)=\\frac{\\tilde{\\chi}_{0}}{\\|\\nabla\\tilde{L}_{0}\\|}\\mathbb{E}[\\phi(Z^{h_{0}^{l-1}})\\phi(Z^{\\tilde{h}_{0}^{l-1}})]+\\tilde{\\beta}^{l-1}\\mathbb{E}[\\phi^{\\prime}(Z^{h_{0}^{l-1}})\\phi^{\\prime}(Z^{\\tilde{h}_{0}^{l-1}})]\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\tilde{\\beta}^{\\tilde{\\ell}-1}=0$ . Note that $\\tilde{\\beta}^{l}(0)>0$ for all $l\\geq\\tilde{\\ell}$ . ", "page_idx": 33}, {"type": "text", "text": "Remark E.4. If $\\tilde{\\ell}=1$ , in the definition of $\\tilde{\\beta}^{l}$ replace $\\mathbb{E}[\\phi(Z^{h_{0}^{l-1}})\\phi(Z^{\\tilde{h}_{0}^{l-1}})]$ by $\\xi_{0}^{T}\\xi$ . ", "page_idx": 33}, {"type": "text", "text": "Now we are ready to state the closed form expression for the first SAM update. ", "page_idx": 33}, {"type": "text", "text": "Lemma E.5 (Features after first SAM update). Defining $Z_{t}^{l}:=Z^{h_{t}^{l}}$ and $\\tilde{Z}_{t}^{l}:=Z^{\\tilde{h}_{t}^{l}}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nZ_{1}^{\\ell-1}=Z_{0}^{\\ell-1},\\ldots,Z_{1}^{1}=Z_{0}^{1},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and, for all $l\\geq\\ell$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\nZ_{1}^{l}=Z_{0}^{l}+\\mathbb{I}_{l>\\ell}\\hat{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}+\\eta\\beta^{l}Z^{d x_{S A M,0}^{l}}\\phi^{\\prime}(\\tilde{Z}_{0}^{l})+\\eta\\gamma^{l}Z^{d h_{0}^{l}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\beta^{l}$ is defined recursively by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{l}=\\beta^{l}(\\eta)=-\\mathring{\\chi}_{0}\\mathbb{E}[\\phi(\\tilde{Z}_{0}^{l-1})\\phi(Z_{1}^{l-1})]+\\beta^{l-1}(\\eta)\\mathbb{E}[\\phi^{\\prime}(Z_{1}^{l-1})\\phi^{\\prime}(\\tilde{Z}_{0}^{l-1})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\beta^{\\ell-1}=0,$ , and $\\gamma^{l}=\\gamma^{l}(\\eta)$ is recursively defined by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\gamma^{l}:=\\beta^{l-1}\\rho\\tilde{\\beta}^{l-1}\\mathbb{E}[\\phi^{\\prime}(Z_{1}^{l-1})\\phi^{\\prime}(Z_{0}^{l-1})\\phi^{\\prime\\prime}(\\tilde{Z}_{0}^{l-1})Z^{d x_{S A M,0}^{l-1}}]+\\gamma^{l-1}\\mathbb{E}[\\phi^{\\prime}(Z_{0}^{l-1})\\phi^{\\prime}(Z_{1}^{l-1})],\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\gamma^{\\ell-1}=\\gamma^{\\ell}=0$ . ", "page_idx": 33}, {"type": "text", "text": "Remark E.6. If $\\ell=1$ , in the definition of $\\beta^{l}$ replace $\\mathbb{E}[\\phi(\\tilde{Z}_{0}^{l-1})\\phi(Z_{1}^{l-1})]$ by $(\\xi_{t-1}^{T}\\xi)$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. By the defining infinite-width equations, for $l\\geq\\ell$ , assuming $\\stackrel{\\circ}{\\theta}_{W^{l}/l}=1$ (so minimal stable choice of $c_{l}$ ), ", "page_idx": 34}, {"type": "equation", "text": "$$\nZ_{1}^{l}=Z_{0}^{l}+\\mathring{\\theta}_{(l-1)/l}Z^{W_{0}^{l}\\delta x_{1}^{l-1}}-\\eta\\mathring{\\chi}_{0}\\mathbb{E}[\\phi(\\tilde{Z}_{0}^{l-1})\\phi(Z_{1}^{l-1})]Z^{d x_{S A M,0}^{l}}\\phi^{\\prime}(\\tilde{Z}_{0}^{l}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "At $l=\\ell$ , we get $\\mathring{\\theta}_{(\\ell-1)/\\ell}=0$ and $\\mathring{\\theta}_{W^{\\ell}/\\ell}=1$ , whereas for $l>\\ell$ we get $\\mathring{\\theta}_{(l-1)/l}=1$ and $\\stackrel{\\circ}{\\theta}_{W^{l}/l}=1$ (under minimal stable choice of $c_{l}$ ), which results in $\\mathring{\\theta}_{(l-1)/l}=\\mathbb{I}_{l>\\ell}$ . Now, for $l>\\ell$ , the second term decomposes into $\\hat{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}$ and $\\dot{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}$ . For the rest of the proof it remains to analyse $\\dot{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}$ . Since by induction hypothesis, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Z^{\\delta x_{1}^{l-1}}=}&{}&{\\phi(Z_{1}^{l-1})-\\phi(Z_{0}^{l-1})}\\\\ &{=}&{\\phi\\left(Z_{0}^{l-1}+\\mathbb{I}_{l>\\ell}\\hat{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}+\\eta\\beta^{l-1}Z^{d x_{S A M,0}^{l-1}}\\phi^{\\prime}(\\tilde{Z}_{0}^{l-1})+\\eta\\gamma^{l-1}Z^{d h_{0}^{l-1}}\\right)-\\phi(Z_{0}^{l-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{Z^{d x_{S A M,0}^{l-1}}\\,=\\,Z^{(W_{0}^{l})^{T}d h_{S A M,0}^{l}}+\\rho\\bar{\\tilde{\\theta}}_{W^{l}}\\frac{\\tilde{\\chi}_{0}}{\\|\\nabla\\tilde{L}_{0}\\|}\\mathbb{E}[Z^{d h_{0}^{l}}Z^{d h_{S A M,0}^{l}}]Z^{x_{0}^{l-1}}}\\end{array}$ with the second term independent of $(W_{0}^{l})^{T}$ and by Lemma E.3 we know $\\tilde{Z}_{0}^{l-1}\\ =\\ Z_{0}^{l-1}\\,+\\,\\mathbb{I}_{l-1>\\tilde{\\ell}}\\hat{Z}^{W_{0}^{l-1}\\tilde{\\delta}x_{0}^{l-2}}\\,+\\,$ $\\rho\\tilde{\\beta}^{l-1}Z^{d x_{0}^{l-1}}\\phi^{\\prime}(Z_{0}^{l-1})$ , where only the last term with $Z^{d x_{0}^{l-1}}=Z^{(W_{0}^{l})^{T}d h_{0}^{l}}$ influences $\\dot{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}$ , we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\dot{Z}^{W_{0}^{l}\\delta x_{1}^{l-1}}=Z^{d h_{0}^{l}}\\mathbb{E}\\frac{\\partial Z^{\\delta x_{1}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{0}^{l}}}+Z^{d h_{S A M,0}^{l}}\\mathbb{E}\\frac{\\partial Z^{\\delta x_{1}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{S A M,0}^{l}}},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial Z^{\\delta x_{1}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{S A M,0}^{l}}}=\\phi^{\\prime}(Z_{1}^{l-1})\\eta\\beta^{l-1}\\phi^{\\prime}(\\tilde{Z}_{0}^{l-1}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and, using $Z^{d h_{0}^{l-1}}=Z^{d x_{0}^{l-1}}\\phi^{\\prime}(Z_{0}^{l-1})=Z^{(W_{0}^{l})^{T}d h_{0}^{l}}\\phi^{\\prime}(Z_{0}^{l-1})$ , yields ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\frac{\\partial Z^{\\delta x_{1}^{l-1}}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{0}^{l}}}=}&{}&{\\phi^{\\prime}(Z_{1}^{l-1})\\left(\\eta\\beta^{l-1}Z^{d x_{S A M,0}^{l-1}}\\phi^{\\prime\\prime}(\\tilde{Z}_{0}^{l-1})\\frac{\\partial\\tilde{Z}_{0}^{l-1}}{\\partial\\hat{Z}^{(W_{0}^{l})^{T}d h_{0}^{l}}}+\\eta\\gamma^{l-1}\\phi^{\\prime}(Z_{0}^{l-1})\\right)}\\\\ {=}&{}&{\\phi^{\\prime}(Z_{1}^{l-1})\\left(\\eta\\beta^{l-1}Z^{d x_{S A M,0}^{l-1}}\\phi^{\\prime\\prime}(\\tilde{Z}_{0}^{l-1})\\rho\\tilde{\\beta}^{l-1}\\phi^{\\prime}(Z_{0}^{l-1})+\\eta\\gamma^{l-1}\\phi^{\\prime}(Z_{0}^{l-1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Plugging Eq. (E.5) back into the defining equation (E.4) and noticing that $Z^{d h_{S A M,0}^{l}}\\ =$ $Z^{d x_{S A M,0}^{l}}\\phi^{\\prime}(\\tilde{Z}_{0}^{l})$ as well as $Z^{d h_{0}^{l}}=Z^{d x_{0}^{l}}\\phi^{\\prime}(Z_{0}^{l})$ concludes the proof. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "F Generalizations and further perturbation scaling considerations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "F.1 Overview over choices of $d_{l}$ and $d$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Since for some combinations of architectures and datasets it turns out that performing SAM on a subset of layers performs better than effective perturbations in all layers (M\u00fcller et al., 2024), we would like to know how to choose $d$ and $d_{l}$ to adjust which layers should be effectively perturbed and which should have vanishing weight perturbations. In practice, simply set all perturbations that should vanish to 0 by design, and use the global scaling $d$ and relative scalings $d_{l}$ from $\\mu\\mathrm{P^{2}}$ for the perturbed layers. This section is instead interested in a complete characterization of all possible choices of $\\{\\bar{d}_{l}\\}_{l\\in[L+1]}$ and $d$ . The heuristic derivation only requires the gradient norm constraints (D.1) and the perturbation stability constraints that require $\\tilde{\\delta}W^{1}=O(1)$ and $\\tilde{\\delta}W^{l}=O(n^{-1})$ for $l>1$ given by ", "page_idx": 34}, {"type": "equation", "text": "$$\nd_{l}\\geq\\left\\{\\!\\!\\begin{array}{l l}{-c_{\\nabla}-d\\,}&{\\mathrm{if}\\ l\\ \\mathrm{is\\input{-}l i k e},}\\\\ {1-c_{\\nabla}-d\\,}&{\\mathrm{if}\\ l\\ \\mathrm{is\\hidden{-}l i k e},}\\\\ {1-d\\,}&{\\mathrm{if}\\ l\\ \\mathrm{is\\output{-}l i k e},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where a layer is effectively perturbed if and only if equality holds in the respective perturbation stability inequality. This heuristic claim yields the characterization of all phases of the choices of perturbation scalings $d$ and $d_{l}$ in Table F.1 and allows us to formulate a simple rule of how to choose $d$ and $d_{l}$ given the information which layers should be effectively perturbed, and which should have vanishing weight perturbations. ", "page_idx": 34}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/1214ec8d53ed1ac90b1140f4925844c494742a084f11d8c15aea82b6be54291e.jpg", "table_caption": [], "table_footnote": ["Table F.1: (Characterization of perturbation scalings) Overview over the regimes of all possible choices of $d$ and $d_{l}$ . A layer is effectively perturbed if and only $d_{l}$ satisfies (F.1). At least one layer must satisfy equality in its gradient norm constraint (D.1). This table summarizes which layers can exhibit effective perturbations, and which may dominate the gradient norm, given a choice of $d$ . The choice $d<-1/2$ results in perturbation blowup $\\tilde{r}\\,<\\,0$ . At the critical $d=-1/2$ (respectively, $d=0$ ; $d=1/2$ ) a input-like (respectively hidden-like; output-like) layer is effectively perturbed if and only if it dominates the gradient norm. Consequently $d=-1/2$ implies effective perturbations in at least one input-like layer. "], "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "Choice of perturbation scaling from list of layers to effectively perturb. We denote the set of all layers by $\\mathcal{L}$ , whereas the subset of layers, which we want to effectively perturb, is denoted by $\\mathcal{L}_{S A M}\\subseteq\\mathcal{L}$ . ", "page_idx": 35}, {"type": "text", "text": "1. If there exists an input-like layer $l\\in\\mathcal{L}_{S A M}$ , set $d=-1/2$ . Input-like layers are effectively perturbed if and only if $d_{l}=1/2-c_{\\nabla}$ . Hidden-like (respectively, output-like) layers are effectively perturbed if and only if $d_{l}=3/2\\!-\\!c_{\\nabla}$ (respectively, $d_{l}=3/2$ ). For all layers that have vanishing weight perturbations, do not perturb these weights or choose $d_{l}>1/2-c_{\\nabla}$ for input-like, $d_{l}>3/2-c_{\\nabla}$ for hidden-like and $d_{l}>3/2$ for output-like layers.   \n2. If all input-like layers should have vanishing weight perturbations but there exists a hiddenlike layer $l\\in\\mathcal{L}_{S A M}$ , set $d=0$ . Hidden-like layers are effectively perturbed if and only if $d_{l}=1-c_{\\nabla}$ . Output-like layers are effectively perturbed if and only if $d_{L+1}=1$ . For all layers that have vanishing weight perturbations, do not perturb these weights, or set $d_{l}>c_{\\nabla}$ for input-like, $d_{l}>1-c_{\\nabla}$ for hidden-like and $d_{l}>1$ for output-like layers (as required by the perturbation stability and gradient norm constraints).   \n3. If both all input-like and all hidden-like layers have vanishing weight perturbations, but there exists some output-like layer $l\\in\\mathcal{L}_{S A M}$ , then set $d=\\Bar{1}/2$ . Output-like layers are effectively perturbed if and only if $d_{l}\\,=\\,1/2$ . For all layers that have vanishing weight perturbations, do not perturb these weights or set $d_{l}\\geq1/2-c_{\\nabla}$ for input-like, $d_{l}\\geq1-c_{\\nabla}$ for hidden-like and $d_{l}>1/2$ for output-like layers (as required by the perturbation stability and gradient norm constraints).   \n4. If $\\mathcal{L}_{S A M}=\\emptyset$ , then set $d>1/2$ or simply perform SGD. ", "page_idx": 35}, {"type": "text", "text": "Example F.1 (First-layer-only effective perturbations). Instead of simply using the rule set above, we derive the necessary choice of perturbation scaling from the scaling equalities and the norm constraints (D.1). To achieve first-layer effective perturbations, but trivial weight perturbations in all other layers, we need $\\tilde{\\theta}_{W^{1}}=1$ and ${\\overset{\\circ}{\\tilde{\\theta}}}_{W^{l}}=0$ , for which we will choose ${\\tilde{\\theta}}_{W^{l}}=n^{-1}$ . This requires setting ", "page_idx": 35}, {"type": "equation", "text": "$$\nd_{1}=-(c_{\\nabla}+d),\\qquad d_{l}=2-c_{\\nabla}-d,\\qquad d_{L+1}=2-d,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where one of the constraints (D.1) has to be fulfilled. Plugging the above $d_{l}$ -choices into (D.1) results in the constraints $l\\le-1/2,d\\le1,d\\le3/2$ , hence choose $d=-1/2$ so that only the first layer contributes non-vanishingly to the gradient norm. Note that $\\tilde{r}{}=0$ and output perturbation nontriviality holds if and only if $\\operatorname*{min}(b_{L+1},c_{L+1})=1$ (as in $\\mu\\mathrm{P}$ ). We apply this perturbation scaling in Appendix H.2 to show that propagating perturbations from early layers are not enough to inherit SAM\u2019s inductive bias that leads to improved generalization performance. \u25c0 ", "page_idx": 35}, {"type": "text", "text": "F.2 Other ways to introduce layerwise perturbation scaling ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Before presenting alternative ways how layerwise perturbation scaling could be accomplished, let us collect desirable properties that a definition should fulfill: ", "page_idx": 36}, {"type": "text", "text": "\u2022 Layerwise perturbation scaling should enable stable, effective perturbations in every layer.   \n\u2022 The perturbation step should require at most one additional forward and backward pass in each update step.   \n\u2022 The adapted optimization algorithm should recover the original (SAM) algorithm when not using layerwise perturbation scaling. ", "page_idx": 36}, {"type": "text", "text": "We start with the simplest case where the perturbations are normalized in each layer separately. ", "page_idx": 36}, {"type": "text", "text": "Remark F.2 (SAM with layerwise gradient normalization). For (SAM) with layerwise gradient normalization of the perturbations ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{\\varepsilon}^{l}=\\rho_{l}\\cdot\\nabla_{W^{l}}\\mathcal{L}/\\|\\nabla_{W^{l}}\\mathcal{L}\\|,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\|\\cdot\\|$ may denote the Frobenius or the spectral norm (equivalent under limited perturbation batch size), the spectral scaling condition $(*)$ immediately yields the correct layerwise perturbation scaling $\\rho_{l}\\overset{!}{=}\\Theta(\\sqrt{\\mathtt{f a n\\mathrm{_{-}o u t}/f a n\\mathrm{_{-}i n}}}).$ . \u25c0 ", "page_idx": 36}, {"type": "text", "text": "However, in practice, perturbations are usually globally normalized across layers, according to the GitHub repositories provided by Foret et al. (2021); Samuel (2022); Kwon et al. (2021); Andriushchenko and Flammarion (2022); M\u00fcller et al. (2024). Preliminary ablations in Appendix H.5 suggest that layer-coupled SAM with global normalization slightly outperforms SAM with layerwise gradient normalization. As our goal in this paper is to study (SAM) as applied in practice, we consider SAM with joint gradient normalization. ", "page_idx": 36}, {"type": "text", "text": "A first alternative to Definition 4 could scale perturbations after the joint gradient normalization. Opposed to Definition 4, for this variant the perturbation norm, i.e. the radius of the adversarial ascent ball, is not guaranteed to be $\\rho n^{-d}$ , but $\\bar{\\rho}(\\sum_{l\\in[L+1]}\\rho_{l}^{2})^{1/2}$ . The correct perturbation scaling for this version more immediately follows from the condition that perturbations scale like updates. ", "page_idx": 36}, {"type": "text", "text": "Remark F.3 (Layerwise perturbation scaling after joint gradient normalization). For (SAM) with joint gradient normalization of the perturbations ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\varepsilon^{l}=\\rho_{l}\\cdot\\frac{\\nabla_{W^{l}}\\mathcal{L}}{\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "the correct perturbation scaling in $\\mu\\mathrm{P}$ is given by $\\rho_{l}\\overset!\\!\\perp\\Theta(n^{1/2}\\cdot\\mathtt{f a n\\_o u t/f a n\\_i n}).$ ", "page_idx": 36}, {"type": "text", "text": "To understand this scaling rule, note that for $b_{L+1}\\,>\\,1/2$ (such as in $\\mu\\mathrm{P}_{.}$ ), the last layer always dominates the gradient norm (see Eq. (E.2) for the TP argument), resulting in the scaling ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|_{F}\\approx\\mathcal{L}^{\\prime}(f_{t}(\\xi_{t}),y_{t})\\|x^{L}\\|=\\Theta(n^{1/2}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, compared to SAM without gradient normalization (Appendix F.3), $\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|_{F}$ always contributes the scaling $n^{1/2}$ . Noting that perturbations should scale like updates, and updates receive the layerwise learning rates $\\eta_{l}\\overset{!}{=}\\Theta(\\mathtt{f a n\\mathrm{_{-}o u t/f a n\\mathrm{_{-}i n)}}}$ concludes the derivation. \u25c0 ", "page_idx": 36}, {"type": "text", "text": "In Definition 4, we accept the additional layer-coupling complications that the layerwise gradient scaling before the joint gradient normalization entails in order to analytically control the perturbation radius to $\\rho n^{-d}$ . To simplify the analysis as much as possible, we will first ensure width-independence of the normalization, so that the layerwise perturbation scaling is not affected by the normalization term. Then, layerwise perturbations should be scaled like updates. ", "page_idx": 36}, {"type": "text", "text": "Another alternative to layerwise perturbation scaling as in Definition 4 is motivated by the observation, that in $\\mu\\mathrm{P^{2}}$ with Definition 4, only the first layer dominates the joint gradient norm (Theorem D.11). To let all layers contribute width-independently to the joint gradient norm, we can introduce even more hyperparameters (with limited benefit) by decoupling the numerator and denominator scalings in the perturbation. Opposed to Definition 4, the perturbation norm is again not analytically set with the choice of $\\rho$ , but may be smaller. Empirically, we do not observe performance differences due to denominator contribution scaling (Appendix H.4). This is the perturbation scaling we implement for ViTs (see Algorithm 1 for details). ", "page_idx": 36}, {"type": "text", "text": "Remark F.4 (SAM with decoupled perturbation numerator and denominator scaling). For (SAM) with perturbations ", "text_level": 1, "page_idx": 37}, {"type": "equation", "text": "$$\n\\varepsilon^{l}=\\rho n^{-d_{l}}\\frac{\\nabla_{W^{l}}\\mathcal{L}}{\\|v\\|}\\quad\\mathrm{with}\\quad\\|v\\|^{2}=\\sum_{l=1}^{L+1}n^{-2\\tilde{d}_{l}}\\|\\nabla_{W^{l}}\\mathcal{L}\\|^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with layerwise perturbation radii $\\rho\\cdot n^{-d_{l}}$ and separate gradient norm scaling $n^{-\\tilde{d}_{l}}$ . ", "page_idx": 37}, {"type": "text", "text": "In all alternatives, nontrivial layerwise perturbation scaling is necessary for effective perturbations in every layer, which necessarily changes the direction away from the original gradient direction. Such a layerwise gradient rescaling can also be achieved by adapting the architecture with widthdependent weight multipliers. The multipliers $(a{-}\\mu P^{2})$ achieve effective perturbations without layerwise perturbation scaling such that all layers contribute non-vanishingly to the joint gradient norm. They rescale the gradients equivalently to (DP) when scaling all denominator terms to be width-independent. See Appendix F.6 for all details about weight multipliers. ", "page_idx": 37}, {"type": "text", "text": "Adapting the TP-based analysis. Our TP-based analysis covers all of the above perturbation scaling alternatives with minor adjustments. We just have to replace the normalized TP scalar (E.2). If we want to express $\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|_{F}$ , we just drop all perturbation scaling terms $n^{-d_{l}}$ . For the examples of (LN) and (DP), we replace (E.2) in each layer separately by the normalized TP scalars, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\nabla_{W^{1}}\\mathcal{L}_{t}\\|:=\\chi_{t}\\left(\\frac{(d h_{t}^{1})^{T}d h_{t}^{1}}{n}(\\xi_{t}^{T}\\xi_{t})\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with scaling $\\theta_{\\parallel\\nabla_{1}\\parallel}=n^{1/2}\\theta_{\\nabla}$ for the first layer, where $\\theta_{\\nabla}$ is overloaded to denote $\\theta_{\\nabla}=n^{-b_{L+1}}$ in the first step and $\\dot{\\theta_{\\nabla}}=n^{-\\operatorname*{min}\\left(b_{L+1},c_{L+1}\\right)}$ in all later steps (in $\\mu\\mathrm{P}_{i}$ , $\\theta_{\\nabla}=n^{-1}$ always), ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\Vert\\nabla_{W^{l}}\\mathcal{L}_{t}\\right\\Vert:=\\chi_{t}\\left(\\frac{(d h_{t}^{l})^{T}d h_{t}^{l}}{n}\\frac{(x_{t}^{l-1})^{T}x_{t}^{l-1}}{n}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with scaling $\\theta_{\\parallel\\nabla_{L+1}\\parallel}=n\\theta_{\\nabla}$ for all hidden layers $l\\in[2,L]$ , and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\nabla_{W^{L+1}}\\mathcal{L}_{t}\\|:=\\chi_{t}\\left(\\frac{(x_{t}^{L})^{T}x_{t}^{L}}{n}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with scaling $\\sqrt{n}$ for the output layer, with respective normalized limits ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathring{\\chi}_{t}(\\mathbb{E}[Z^{(d h_{t}^{1})^{2}}](\\xi_{t}^{T}\\xi_{t}))^{1/2},\\quad\\mathring{\\chi}_{t}(\\mathbb{E}[Z^{(d h_{t}^{l})^{2}}]\\mathbb{E}[Z^{(x_{t}^{l-1})^{2}}])^{1/2},\\quad\\mathring{\\chi}_{t}(\\mathbb{E}[Z^{(x_{t}^{L})^{2}}])^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\mathring{\\chi}_{t}=\\mathcal{L}^{\\prime}\\big(\\mathring{f}_{t}(\\xi_{t}),y_{t}\\big)$ . ", "page_idx": 37}, {"type": "text", "text": "The adapted scalings can then be tracked as before to derive the maximal stable layerwise perturbation scaling. Consider for example input layers in (LN). In $\\mu\\mathrm{P},$ we know $\\|\\nabla_{W^{1}}\\dot{\\mathcal{L}_{t}}\\|=\\Theta\\bar{(n^{-1/2})}$ and $\\nabla_{W^{1}}\\bar{\\mathcal{L}}_{t}\\,=\\,\\Theta(n^{-1})$ entrywise. Effective perturbations are achieved with $\\varepsilon^{1}\\,=\\,\\Theta(1)$ , so choose $\\rho_{l}=n^{1/2}$ as expected from $(*)$ . Proceed similarly for all layers and perturbation scaling variants. ", "page_idx": 37}, {"type": "text", "text": "F.3 Extension to SAM without gradient normalization ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Andriushchenko and Flammarion (2022) and Andriushchenko et al. (2023a) consider the SAM update without normalizing the gradient in the adversarial ascent. The corresponding update rule is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\nW_{t}=W_{t}-\\eta\\nabla_{w}\\mathcal{L}\\big(f(\\xi_{t};W_{t}+\\rho v_{t},y_{t})\\big),y_{t}),\\qquad v_{t}=\\nabla_{w}\\mathcal{L}\\big(f(\\xi_{t};W_{t}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program for this update rule with arbitrary $v_{t}^{l}=n^{-c_{l}}\\nabla_{w}\\mathcal{L}(f(\\xi_{t};W_{t})$ is also easily adapted from the above derivation. Just note that the gradient norm appears in an equation if and only if the perturbation radius $\\rho n^{-d}$ appears. Without dividing by $\\|v_{t}\\|$ , the parameter $d$ becomes superfluous. Simply set $d\\,=\\,0$ and remove the gradient norm constraints (D.1) to arrive at the $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ program and bcd-constraints for the update rule without gradient normalization. ", "page_idx": 37}, {"type": "text", "text": "Perturbation scaling $d_{l}$ plays a similar role as learning rate scaling $c_{l}$ as both scale similar gradients. We get effective perturbations in the $l_{\\cdot}$ -th layer from the equation $d_{l}+\\operatorname*{min}(b_{L+1},c_{L+1}\\bar{)}\\,=\\,c_{l}\\,+$ $\\operatorname*{min}(b_{L+1},c_{L+1},d_{L+1})$ in $\\mu\\mathrm{P},$ which yields $d_{l}\\,=\\,c_{l}$ for all $l\\in[L]$ (since $d_{L+1}=1$ for stability). In particular, in $\\mu\\mathbf{P}_{:}$ , the correct layerwise perturbation scaling of unnormalized gradients is given by the rule $\\frac{\\mathbf{fan\\;out}}{\\mathbf{fan\\;in}}$ or the squared weight (update) spectral norm $\\lVert W^{l}\\rVert_{*}^{2}$ (Yang et al., 2023a), which could be efficiently approximately tracked with a running power iteration. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "Note that Dai et al. (2024) argue that the normalizing the gradients for the perturbation is crucial (in standard parametrization) due to a stabilizing effect and an enhanced drift along manifolds of minima. Monzio Compagnoni et al. (2023) find that unnormalized SAM gets stuck around saddles while SAM slowly escapes through additional Hessian-induced noise. This suggests that the additional effort of analysing the original SAM update rule with gradient normalization is necessary for practically useful theory. From this paper\u2019s point of view, the gradient normalization may be adding stability via the $n^{-1/2}$ contribution which allows to scale down $\\rho$ less aggressively in practice. ", "page_idx": 38}, {"type": "text", "text": "F.4 Extension to Adaptive SAM ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Adaptive SAM (ASAM) (Kwon et al., 2021) is motivated by a sharpness definition that is invariant to parameter rescaling operators that leave the output function invariant, and can provide a further improvement over SAM of $0.5\\%$ to $1\\%$ , depending on the considered vision dataset and model (M\u00fcller et al., 2024). Here we consider the two examples of elementwise rescaling operators (with $p=2$ ) and layerwise rescaling operators (with $p=2$ ), which are the best performing SAM variant in most settings in M\u00fcller et al. (2024). ", "page_idx": 38}, {"type": "text", "text": "Proposition F.5. Neither elementwise ASAM, which performs (SAM) but using the perturbation rule (F.4), nor layerwise ASAM, which performs (SAM) but using the perturbation rule (F.6), can be written as a NE $\\otimes$ OR\u22a4program. ", "page_idx": 38}, {"type": "text", "text": "Proof sketch. Elementwise ASAM requires an elementwise multiplication of matrices, and layerwise ASAM requires calculating the Frobenius norm of a matrix. A NonLin operation only takes vectors as arguments, so NE $\\otimes$ OR\u22a4calculations with a matrix require its multiplication with a vector. But then a single coordinate of the resulting vector contains a mixture of an entire row of that matrix. Since we are only allowed to define random vectors and matrices, and the $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ master theorem states that coordinates of NE\u2297OR\u22a4vectors behave iid-like, this mixture cannot be disentangled by choosing a structured vector. Hence, already at initialization, the square of individual entries/the Frobenius norm of a random matrix cannot be exactly recovered by a function of matrix-vector products with NE\u2297OR\u22a4vectors. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Although ASAM is not formally covered by our theory, we still expect that the ASAM perturbations are correlated with the gradient and therefore with the incoming activations, so that heuristically we can still expect LLN-like behaviour and apply our scaling condition. If the perturbation rules still behave LLN-like, then Table 1 summarizes which layers are effectively perturbed under global scaling and provides the unique maximal perturbation scalings for all considered SAM variants. The correct perturbation scaling in $\\mu\\mathrm{P}$ for other perturbation rules that behave LLN-like can always be derived following the same steps: ", "page_idx": 38}, {"type": "text", "text": "1. In $\\mu\\mathrm{P}_{:}$ , it always holds that ", "page_idx": 38}, {"type": "equation", "text": "$$\nW^{l}=\\left\\{\\begin{array}{l l}{\\displaystyle\\Theta(1)}&{l=1,}\\\\ {\\displaystyle\\Theta(n^{-1/2})}&{l\\in[2,L],\\quad\\mathrm{~and~}\\quad\\nabla_{W^{l}}\\mathcal{L}=\\left\\{\\Theta(\\theta_{\\nabla})=\\Theta(n^{-1})\\quad l\\leq L,\\right.}\\\\ {\\displaystyle\\Theta(n^{-1})}&{\\left.l=L+1,}\\end{array}\\right.\\,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "2. Assuming the normalization term in the denominator is scaled to $\\Theta(1)$ , track the layerwise scalings of the numerator. Maximal stable perturbations are always achieved with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\delta}W_{t}^{l}=\\binom{\\Theta(1)}{\\Theta(n^{-1})}\\quad l>1.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This yields constraints for achieving maximal stable perturbations in each layer. 3. Now replace the norm constraints (D.1) by tracking the scalings of each layer\u2019s contribution to the update rule\u2019s total normalization term. 4. To ensure normalization term scaling $\\Theta(1)$ , iterate through the layers $l$ : ", "page_idx": 38}, {"type": "text", "text": "(a) choose $d_{l}$ to satisfy its norm constraint,   \n(b) choose $d$ to induce maximal stable perturbations in that layer,   \n(c) choose all other $d_{l^{\\prime}}$ , ${{l}^{\\prime}}\\neq l$ , minimal to both satisfy its norm constraint as well as $\\tilde{\\delta}W_{t}^{l}=\\binom{O(1)}{O(n^{-1})}\\quad l>1$ , perturbation stability . ", "page_idx": 39}, {"type": "text", "text": "5. From the above configurations, choose the unique one that yields maximal stable perturbations in all layers.3 ", "page_idx": 39}, {"type": "text", "text": "F.4.1 Elementwise ASAM ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "If we want to be invariant to elementwise rescaling operators $T_{w}^{l}(x)=|W^{l}|\\odot x$ where $x,W^{l}\\in$ $\\mathbb{R}^{m\\times n}$ and $\\odot$ denotes elementwise multiplication, the resulting ASAM perturbation rule (where we introduce (layer-wise) perturbation scalings $\\{d\\}\\cup\\{d_{l}\\}_{l\\in[L+1]}\\}$ replaces (LP) and is given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tilde{\\delta}W_{t}^{l}:=\\rho n^{-d}\\frac{n^{-d_{l}}|W^{l}|\\odot|W^{l}|\\odot\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})}{\\|\\nabla_{A S A M}^{e l e m}\\|},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with normalization ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\nabla_{A S A M}^{e l e m}\\|:=\\sum_{l=1}^{L+1}n^{-d_{l}}\\left\\||W^{l}|\\odot\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})\\right\\|_{F},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the absolute values $|W^{l}|$ are computed and multiplied elementwise. To find the correct perturbation scalings, we track the typical elementwise scaling of each quantity as before. ", "page_idx": 39}, {"type": "text", "text": "Elementwise ASAM in $\\mu\\mathbf{P}.$ In $\\mu\\mathrm{P},$ the layerwise weights and gradients scale as (F.2). For $\\|\\nabla_{A S A M}^{e l e m}\\|=O(1)$ , we therefore replace the constraints (D.1) by the constraints ", "page_idx": 39}, {"type": "equation", "text": "$$\nd_{l}\\ge1/2-c_{\\nabla},\\;\\mathrm{for}\\,l\\in[L],\\qquad d_{L+1}\\ge-1/2,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we can choose $\\{d_{l}\\}_{l\\in[L+1]}$ to achieve equality in at least one constraint to achieve $\\|\\nabla_{A S A M}^{e l e m}\\|=\\Theta(1)$ . ", "page_idx": 39}, {"type": "text", "text": "The layerwise perturbations scale as $\\begin{array}{r}{\\tilde{\\delta W}_{t}^{l}=n^{-d}\\left\\{\\Theta(n^{-d_{1}}\\theta_{\\nabla})\\begin{array}{r l}&{l=1,}\\\\ &{l\\in[2,L],}\\\\ &{l=L+1.}\\end{array}\\right.}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Stable and nontrivial perturbations in each layer are achieved under condition (F.3), which induces the constraints for optimal layerwise perturbation scaling ", "page_idx": 39}, {"type": "equation", "text": "$$\nd+d_{l}=-c_{\\nabla},\\;\\mathrm{for}\\,l\\in[L],\\qquad d+d_{L+1}=-1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Irrespective which of the above norm constraints (F.5) we satisfy, we need $d=-1/2$ to achieve optimal layerwise perturbation scaling. Hence $d=d_{L+1}=-1/2$ and $d_{l}=1/2-c_{\\nabla}$ for $l\\in[L]$ is the unique choice of $\\{d\\}\\cup\\{d_{l}\\}_{l\\in[L+1]}$ modulo norm scaling equivalence that achieves $\\Theta(1)$ perturbation scaling in all layers. With this choice all layers contribute non-vanishingly to the gradient norm. In $\\mu\\mathbf{P}\\ c_{\\nabla}\\,=\\,1$ , so that $d_{l}\\,=\\,-1/2$ for all $l^{\\mathrm{~\\scriptsize~\\cdot~}}\\in\\mathrm{~[}L+1\\mathrm{]}$ , so that ASAM does not require layerwise rescaling of the gradients, but upscaling of the perturbation by $n^{1/2}$ to achieve nontrivial perturbations in any layer. This may explain why ASAM often outperforms SAM in large models: By only requiring global scaling, ASAM achieves maximal stable perturbations in all layers if the perturbation radius is tuned globally at every width. ", "page_idx": 39}, {"type": "text", "text": "If instead of a global gradient norm $\\|\\nabla_{A S A M}^{e l e m}\\|$ , one would want to normalize in each layer separately with $\\|\\nabla_{A S A M}^{e l e m,l}\\|\\;:=\\;n^{-d_{l}}\\||W^{l}|\\;\\odot\\;\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})\\|_{F}$ , the layerwise perturbation scalings become \u03b4\u02dcW tl = n\u2212d \u0398(n\u2212\u221231//22) l = 1, A . gain, to achieve maximal stable perturbations in all layers we need $d=-1/2$ and no layerwise adaptation of the gradient norm. ", "page_idx": 39}, {"type": "text", "text": "F.4.2 Layerwise ASAM ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "ASAM with layerwise rescaling as in M\u00fcller et al. (2024) employs the layerwise transformations $T_{w}^{l}(x)=\\|W^{l}\\|_{F}\\cdot x$ . This ASAM perturbation rule replaces (LP) and is given by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\tilde{\\delta}W_{t}^{l}:=\\rho n^{-d}\\frac{n^{-d_{l}}\\|W^{l}\\|_{F}^{2}\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})}{\\|\\nabla_{A S A M}^{l a y e r}\\|},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with normalization ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|\\nabla_{A S A M}^{l a y e r}\\|:=\\sum_{l=1}^{L+1}n^{-d_{l}}\\|W^{l}\\|_{F}\\|\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})\\|_{F}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Layerwise ASAM in $\\mu\\mathrm{P}.$ In $\\mu\\mathrm{P},$ , we have $\\|W^{l}\\|_{F}=\\left\\{\\Theta(n^{1/2})\\;\\;\\;\\;\\;l=1,\\atop\\Theta(n^{1/2})\\;\\;\\;\\;l\\in[2,L],\\atop\\Theta(n^{-1/2})}\\right.$ ", "page_idx": 40}, {"type": "text", "text": "Hence, the norm constraints (D.1) are now replaced by ", "page_idx": 40}, {"type": "equation", "text": "$$\nd_{1}\\geq1-c_{\\nabla},\\qquad d_{l}\\geq3/2-c_{\\nabla}\\quad{\\mathrm{for~}}l\\in[2,L],\\qquad d_{L+1}\\geq0.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The scale of the perturbation numerator now scales as $\\begin{array}{r}{\\tilde{\\delta W}_{t}^{l}=n^{-d}\\left\\{\\Theta(n^{-d_{1}}n\\theta_{\\nabla})\\right.\\quad\\,\\,\\,\\,l=1,}\\\\ {\\Theta(n^{-d_{l}}n\\theta_{\\nabla})\\quad\\quad\\,\\,\\,\\,l\\in[2,L],}\\\\ {\\Theta(n^{-d_{L+1}}n^{-1})\\quad l=L+1.}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "In $\\mu\\mathrm{P},$ achieving maximal stable perturbations (F.3) is therefore equivalent to satisfying the constraints ", "page_idx": 40}, {"type": "equation", "text": "$$\nd+d_{1}=0,\\quad\\quad d+d_{l}=1\\quad\\mathrm{for}\\,l\\in[2,L],\\qquad d+d_{L+1}=0.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now we can simultaneously satisfy the first- and last-layer norm constraints with $d_{1}\\ =\\ 0$ and $d_{L+1}=0$ , while achieving effective perturbations in all layers with $d=0$ and $d_{l}=1$ . Satisfying the norm constraint in the hidden layers with $d_{l}=1/2$ would imply vanishing perturbations in the first and last layer (by requiring $d\\geq1/2$ ). ", "page_idx": 40}, {"type": "text", "text": "F.5 Representing general architectures and adaptive optimizers as Tensor Programs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Here, we lay out explicitly how to write some of the building blocks in ResNets and ViTs in a Tensor Program and provide further scaling considerations. According to Yang and Hu (2021), it is straightforward to generalize scaling conditions that induce feature learning in MLPs to these other common neural network building blocks. Since perturbations should always scale like updates, the conditions for stable feature learning and those for stable effective perturbations are analogous. ", "page_idx": 40}, {"type": "text", "text": "One potential complication in the case of SAM would be a contribution to the joint gradient normalization $\\|v_{t}\\|$ that differs from the classical input, hidden or output layer contribution. But we will see that these contributions do not differ for any of the considered layer types. ", "page_idx": 40}, {"type": "text", "text": "Layernorm. The Layernorm operation is defined as ", "page_idx": 40}, {"type": "equation", "text": "$$\nh_{t}^{l+1}=\\gamma_{t}^{l}\\frac{x_{t}^{l}-\\nu_{t}^{l}}{\\sigma_{t}^{l}+\\varepsilon}+\\beta_{t}^{l},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\varepsilon>0$ is a small positive constant, $\\gamma_{t}^{l},\\beta_{t}^{l}$ are learnable parameters and $\\begin{array}{r}{\\nu_{t}^{l}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{t}^{l})_{i}}\\end{array}$ is an Avg operation as in Yang and Littwin (2023, Def. 2.6.1) and $\\begin{array}{r}{\\sigma_{t}^{l}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{t}^{l}-\\nu_{t}^{l})^{2}}}\\end{array}$ is a composition of Nonlin, Avg and Nonlin. The parameters $\\gamma_{t}^{l},\\beta_{t}^{l}$ can be seen as input weights to the input 1. They should be initialized as $\\gamma_{0}^{l}=1$ and $\\beta_{0}^{l}=0$ . In the forward pass, the layernorm preserves stability $h_{t}^{l+1}=\\Theta(1)$ when $\\gamma_{t}^{l}+\\beta_{t}^{l}=\\Theta(1)$ except for the Lebesgue nullset of learning rates for which they exactly cancel each other out. Recall the notation $\\bar{d z^{}}=\\theta_{z}^{-1}\\partial f/\\partial z$ , where $\\theta_{z}=n^{C}$ for some $C\\in\\mathbb{R}$ denotes the width-dependent scaling. The derivatives are ", "page_idx": 40}, {"type": "equation", "text": "$$\nd\\beta_{t}^{l}=d h_{t}^{l+1},\\qquad d\\gamma_{t}^{l}=d h_{t}^{l+1}\\frac{x_{t}^{l}-\\nu_{t}^{l}}{\\sigma_{t}^{l}+\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "These gradients coincide both in shape and scaling with the scaling we expect for an input layer, resulting in the same gradient spectral/Frobenius norm scaling. Continuing the backward pass, using $\\begin{array}{r}{\\frac{\\partial\\sigma_{t}^{l}}{\\partial x_{t}^{l}}=\\frac{\\bar{x}_{t}^{l}-\\nu_{t}^{l}}{n\\sigma_{t}^{l}}}\\end{array}$ , we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d x_{t}^{l}=}&{}&{d h_{t}^{l+1}\\gamma_{t}^{l}\\left(\\frac{1}{\\sigma_{t}^{l}+\\varepsilon}(I-\\frac{1}{n})-\\frac{x_{t}^{l}-\\nu_{t}^{l}}{(\\sigma_{t}^{l}+\\varepsilon)^{2}}\\frac{\\partial\\sigma_{t}^{l}}{\\partial x_{t}^{l}}\\right)}\\\\ {=}&{}&{d h_{t}^{l+1}\\gamma_{t}^{l}\\left(\\frac{1}{\\sigma_{t}^{l}+\\varepsilon}(I-\\frac{1}{n}11^{T})-\\frac{x_{t}^{l}-\\nu_{t}^{l}}{(\\sigma_{t}^{l}+\\varepsilon)^{2}}\\frac{(x_{t}^{l}-\\nu_{t}^{l})^{T}}{n\\sigma_{t}^{l}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which preserves the order as long as $\\gamma_{t}^{l}=\\Theta(1)$ , since $x_{t}^{l}=\\Theta(1)$ , we know $\\nu_{t}^{l},\\sigma_{t}^{l}=\\Theta(1)$ . ", "page_idx": 41}, {"type": "text", "text": "Note that Layernorm removes the necessity to avoid blowup in the activations $\\boldsymbol{x}_{t}^{l}$ in the forward pass (ignoring potential numerical issues), and always rescales to $\\Theta(\\operatorname*{max}(\\gamma_{t}^{l},\\beta_{t}^{l}))$ . However, in the backward pass, a scaling $x_{t}^{l}=\\Theta(n^{c})$ , with $c>0$ , results in $d x_{t}^{l}=\\Theta(n^{-c}d h_{t}^{l+1}\\gamma_{t}^{l})$ , hence vanishing gradients. The gradients would only stabilize if $\\phi^{\\prime}(h_{t}^{l})=\\Theta(h_{t}^{\\bar{l}})$ , but no popular activation function has a scale equivariant derivative. Yang (2019) shows how to write Batchnorm and Average Pooling as a Tensor Program. ", "page_idx": 41}, {"type": "text", "text": "Convolutions. Convolutional layers can be seen as a collection of dense weight matrices where width corresponds to the number of channels (Yang, 2019). With kernel positions $k e r$ , input channels $[n^{l}]$ and output channels $[n^{l+1}]$ , the weights of a stride-1 convolution are given by $\\{W_{i\\alpha\\beta}^{l}\\}_{i\\in k e r,\\alpha\\in[n^{l+1}],\\beta\\in[n^{l}]}$ , so that for each $i\\;\\in\\;k e r$ , $W_{i}^{l}\\,\\in\\,\\mathbb{R}^{n^{l+1}\\times n^{l}}$ is a dense matrix. With $\\{x_{i\\alpha}^{l}\\}_{i\\in p o s^{l},\\alpha\\in[n^{l}]}$ , the convolution operation is given by ", "page_idx": 41}, {"type": "equation", "text": "$$\n(W^{l}*x)_{i\\alpha}=\\sum_{\\beta,j:j+i\\in p o s^{l}}W_{j\\alpha\\beta}^{l}x_{i+j,\\beta}^{l},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which performs MatMul and Avg and where $k e r,p o s^{l}$ are assumed to be of fixed size. For $k e r$ of fixed size, convolutional weights scale like hidden layer weight matrices, also in Frobenius norm contributing to $\\|v_{t}\\|$ . ", "page_idx": 41}, {"type": "text", "text": "Residual connections. A residual connection propagates the current activation forward, skipping an arbitrarily complex nonlinear block $f_{t}^{l}:\\bar{\\mathbb{R}^{n_{l}}}\\ \\stackrel{=}-\\mathbb{R}^{n_{l+1}}$ in between, where $f_{t}^{l}$ can depend on time-dependent parameters like a weight matrix. The forward pass can be written as ", "page_idx": 41}, {"type": "equation", "text": "$$\nx_{t}^{l}=x_{t}^{l-1}+f_{t}^{l}(x_{t}^{l-1}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "If $\\boldsymbol{x}_{t}^{l}=\\boldsymbol{\\Theta}(1)$ for all layers $l$ holds in the model without residual connections, it also holds in the model with residual connections. At fixed depth, $f_{t}^{l}\\,=\\,o(1)$ should be avoided, as it would hold that xlt+1= in the infinite-width limit and the layer would be superfluous. The derivative of the activations becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\nd x_{t}^{l-1}=d x_{t}^{l}+d x_{t}^{l}\\frac{\\partial f_{t}^{l}}{\\partial x_{t}^{l-1}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the second term stays the same as without the residual connection. For the example of $f_{t}^{l}$ being a fully connected layer we get $d x_{t}^{l-1}=d x_{t}^{l}+(W_{t}^{l})^{T}\\left(d x_{t}^{l}\\odot\\phi^{\\prime}(W_{t}^{l}x_{t}^{l-1})\\right)$ . In this example, the derivative with respect to the weights becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{t}}{\\partial W_{t}^{l}}=d x_{t}^{l}\\frac{\\partial x_{t}^{l}}{\\partial W_{t}^{l}}=d x_{t}^{l}\\frac{\\partial f_{t}^{l}}{\\partial W_{t}^{l}}=(d x_{t}^{l}\\odot\\phi^{\\prime}(W_{t}^{l}x_{t}^{l-1}))(x_{t}^{l-1})^{T},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the residual connection does not alter the functional dependence on $d x_{t}^{l}$ and $\\boldsymbol{x}_{t}^{l}$ compared to a MLP, but implicitly influences the weight gradient since $d x_{t}^{l}$ and $\\boldsymbol{x}_{t}^{l}$ are altered. As for the forward pass, the gradient scaling $d x_{t}^{l}$ gets stabilized in the backward pass so tha t \u2202\u2202xlf \u2212t1 is now allowed to be vanishing with width. Again, we are not aware of an architecture in which that would be desirable. Since a residual connection does not introduce learnable parameters, it interferes in $\\|v_{t}\\|$ only implicitly through the stabilized gradients in earlier layers, which can contribute non-vanishingly to $\\|v_{t}\\|$ even if later layers are wrongly scaled and their scaling is not adapted. ", "page_idx": 41}, {"type": "text", "text": "Adam as a base optimizer. When using Adam or similar adaptive optimizers as a base optimizer, the learning rate should scale as $\\Theta(1)$ for input-like layers and biases, and $\\Theta(n^{-1})$ for hidden and output layers (Yang et al., 2022). Yang et al. (2023b) provide proofs for arbitrary optimizers that perform generalized, nonlinear outer products. In the example of Adam, the update rule can be written as ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "equation", "text": "$$\n\\phi(u_{\\alpha}^{1},\\dots,u_{\\alpha}^{k},v_{\\beta}^{1},\\dots,v_{\\beta}^{k})=\\sum_{i}\\gamma_{i}u_{\\alpha}^{i}v_{\\beta}^{k}/\\left(\\sum_{i}\\omega_{i}(u_{\\alpha}^{i}v_{\\beta}^{i})^{2}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\gamma_{i},\\omega_{i}$ are the weights that stem from the moving averages. By using a learning rate of $n^{-1}$ and using the fact that both $u$ and $v$ have approximately iid coordinates of order $\\Theta(1)$ , the law of large numbers yields $\\Theta(1)$ updates of the form ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{\\beta=1}^{n}\\phi\\big(u_{\\alpha}^{1},\\dots,u_{\\alpha}^{k},v_{\\beta}^{1},\\dots,v_{\\beta}^{k}\\big)x_{\\beta}=\\mathbb{E}\\phi\\big(u_{\\alpha}^{1},\\dots,u_{\\alpha}^{k},Z^{v^{1}},\\dots,Z^{v^{k}}\\big)Z^{x}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Any other learning rate scaling would either result in blowup or vanishing updates. ", "page_idx": 42}, {"type": "text", "text": "Adaptive optimizers have not been used for the ascent/perturbation step. In the descent/update step, nothing changes compared to unperturbed optimization as long as we ensure stable perturbations. ", "page_idx": 42}, {"type": "text", "text": "F.6 Influence of width-dependent weight multipliers on bcd-parameterizations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Our definition of bcd-parameterizations is convenient because it purely adapts the learning algorithm but not the architecture. We can also adapt the architecture by using layerwise width-dependent weight multipliers to effectively perturb all layers without any perturbation scaling. The reason is that layerwise weight multipliers scale the layerwise gradients. Here, we study how the introduction of weight multipliers affects $b c d$ -parameterizations. ", "page_idx": 42}, {"type": "text", "text": "In this section, we consider $L$ -hidden layer MLPs with weight multipliers $\\{a_{l}\\}_{l\\in[L+1]}$ , width $n\\in\\mathbb N$ , inputs $\\xi\\,\\in\\,\\mathbb{R}^{d_{\\mathrm{in}}}$ , and with outputs $f(\\xi)\\,:=\\,n^{-a_{L+1}}W^{L+1}x^{L}(\\xi)$ where the activations $x^{L}(\\xi)$ are defined via the iteration ", "page_idx": 42}, {"type": "equation", "text": "$$\nh^{1}(\\xi):=n^{-a_{1}}W^{1}\\xi,\\qquad x^{l}(\\xi):=\\phi(h^{l}(\\xi)),\\qquad h^{l+1}(\\xi):=n^{-a_{l+1}}W^{l+1}x^{l}(\\xi).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We define abcd-parameterizations in the same way as $b c d$ -parameterizations, but instead of MLPs we use MLPs with weight multipliers $\\{a_{l}\\}_{l\\in[L+1]}$ . ", "page_idx": 42}, {"type": "text", "text": "Definition F.6 (abcd-parametrization). An abcd-parametrization $\\{a_{l}\\}_{l\\in[L+1]}\\cup\\{b_{l}\\}_{l\\in[L+1]}\\cup$ $\\{c_{l}\\}_{l\\in[L+1]}\\cup\\{d_{l}\\}_{l\\in[L+1]}\\cup\\{d\\}$ defines the training of an MLP with weight multipliers $\\{a_{l}\\}_{l\\in[L+1]}$ with SAM in the following way: ", "page_idx": 42}, {"type": "text", "text": "(a) Initialize weights iid as $W_{i j}^{l}\\sim\\mathcal{N}(0,n^{-2b_{l}})$ ", "page_idx": 42}, {"type": "text", "text": "(b) Train the weights using the SAM update rule with layerwise learning rates, ", "page_idx": 42}, {"type": "equation", "text": "$$\nW_{t+1}^{l}=W_{t}^{l}-\\eta n^{-c_{l}}\\nabla_{W^{l}}\\mathcal{L}\\left(f\\left(\\xi_{t};W_{t}+\\varepsilon_{t}\\right),y_{t}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with the scaled perturbation $\\varepsilon_{t}$ via layerwise perturbation radii, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\varepsilon_{t}:=\\rho n^{-d}\\frac{v_{t}}{\\|v_{t}\\|},\\quad\\mathrm{with}\\quad v_{t}=(v_{t}^{1},\\dots,v_{t}^{L+1}),\\quad v_{t}^{l}:=n^{-d_{l}}\\cdot\\nabla_{W^{l}}\\mathcal{L}\\big(f(\\xi_{t};W_{t}),y_{t}\\big),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "W.l.o.g. we set $\\|v_{t}\\|=\\Theta(1)$ , which prevents nontrivial width-dependence from the denominator. This imposes the constraints: ", "page_idx": 42}, {"type": "equation", "text": "$$\nd_{1}+a_{1}\\geq1/2-c_{\\nabla},\\qquad d_{l}+a_{l}\\geq1-c_{\\nabla},\\qquad d_{L+1}+a_{L+1}\\geq1/2,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with at least one equality required to hold, where $l\\in[2,L]$ , and where $\\nabla_{x^{L}}f=n^{-a_{L+1}}W^{L+1}=$ $\\Theta(n^{-c}\\nabla)$ with $c_{\\nabla}=\\mathrm{min}(b_{L+1}+a_{L+1},c_{L+1}+2a_{L+1})$ . The normalization $v_{t}/\\lVert v_{t}\\rVert$ removes one degree of freedom from $\\{d_{l}\\}_{l\\in[L+1]}$ via the equivalence $\\{d_{l}^{\\prime}\\}_{l\\in[L+1]}\\cong\\{d_{l}\\}_{l\\in[L+1]}$ iff there exists a $C\\in\\mathbb{R}$ such that $d_{l}^{\\prime}=d_{l}+C$ for all $l\\in[L+1]$ . \u25c0 ", "page_idx": 42}, {"type": "text", "text": "F.6.1 abcd-equivalence classes ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Update scalings behave as in SGD. The weight multiplier $n^{-a_{l}}$ scales the gradient $\\nabla_{W^{l}}f$ by $n^{-a_{l}}$ . In the following forward pass, another multiplication of the weight updates with $n^{-a_{l}}$ leads to the activation update scaling $\\bar{\\,n}^{-2a_{l}}$ . This can be counteracted by adapting the learning rate scaling. For abc-parameterizations and SGD training, this induces the layerwise equivalence between parameterizations with $(a_{l},b_{l},c_{l})$ or with $(a_{l}+\\bar{\\theta}_{l},b_{l}-\\theta_{l},c_{l}-2\\theta_{l})$ . The extension of all of our results to Adam as a base optimizer is straightforward, since learning rate scalings and perturbation scalings are decoupled. For Adam, $c_{l}$ should be adapted to $c_{l}-\\theta_{l}$ . ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "Again, perturbations with joint gradient normalization complicate matters compared to SGD and Adam. Keeping the gradient norm scalings invariant under $a_{l}\\mapsto a_{l}+\\theta$ would require $d_{l}\\mapsto d_{l}-\\theta$ , but keeping the activation perturbation scaling invariant would require $d_{l}\\mapsto d_{l}-2\\theta$ as for updates. Consequently, an exact equivalence between abcd-parameterizations at finite width requires $\\theta$ to be the same for all layers and the conflicting gradient norm in the denominator and perturbation scaling in the numerator to be accounted for by adapting the global perturbation scaling $d\\mapsto d-\\theta$ (together with $d_{l}\\mapsto d_{l}-\\theta)$ . In other words, (SAM) with layer-coupling gradient normalization (LP) does not have layerwise analytical equivalence classes at finite width. Below, we provide two alternative perturbation rules that resolve these complications and recover layerwise equivalence classes. The following lemma formally states the layer-coupled equivalence relation for the perturbation rule (LP). All proofs are provided at the end of this section. ", "page_idx": 43}, {"type": "text", "text": "Lemma F.7 (abcd-equivalence classes). Let $f_{t}(\\xi)$ denote the output of a MLP in a stable abcdparameterization with weight multipliers $\\{a_{l}\\}_{l\\in[L+1]}$ after $t$ steps of training with the SAM update rule with layerwise perturbation scaling (LP) using a fixed sequence of batches and evaluated on input $\\xi$ . Then for any $\\theta\\in\\mathbb{R}$ and any $C\\in\\mathbb{R}$ , $f_{t}(\\xi)$ stays fixed for all $t$ and $\\xi$ if, for all $l\\in[L+1]$ , ", "page_idx": 43}, {"type": "text", "text": "Remark F.8 (Infinite-width equivalences). In the infinite-width limit, abcd-parameterizations remain equivalent under $(a_{l}+\\theta_{l},b_{l}-\\theta_{l},c_{l}-2\\theta_{l},d_{l}-2\\theta_{l},d)$ layerwise as long as the set of layers that contribute to the gradient norm non-vanishingly remains invariant. The gradient norm constraints for $\\|v^{l}\\|=O(1)$ become ", "page_idx": 43}, {"type": "equation", "text": "$$\nd_{1}+a_{1}\\geq1/2-c_{\\nabla},\\qquad d_{l}+a_{l}\\geq1-c_{\\nabla},\\qquad d_{L+1}+a_{L+1}\\geq1/2,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\nabla_{{x^{L}}}f=n^{-a_{L+1}}W^{L+1}=\\Theta(n^{-c_{\\nabla}})$ with $c_{\\nabla}=\\mathrm{min}(b_{L+1}+a_{L+1},c_{L+1}+2a_{L+1})$ remains invariant under equivalence transformations. ", "page_idx": 43}, {"type": "text", "text": "Remark F.9 (SAM with layerwise gradient normalization). As the layer coupling is induced by the joint gradient normalization in the perturbations, layerwise gradient normalization simplifies the analysis. For (SAM) with layerwise gradient normalization (LN) of the perturbations global perturbation scaling $d$ is superfluous, and there exist layerwise equivalence classes: For any $\\{\\bar{\\theta}_{l}\\}_{l\\in[L+1]}\\subset\\mathbb{R}$ , ", "page_idx": 43}, {"type": "text", "text": "To understand this equivalence, observe that any layerwise gradient scaling is cancelled out by the normalization $\\nabla_{W^{l}}\\bar{\\mathcal{L}}/\\|\\nabla_{W^{l}}\\mathcal{L}\\|$ . Only the $n^{-a_{l}}$ factor from subsequent forward passes has to be counteracted. \u25c0 ", "page_idx": 43}, {"type": "text", "text": "Remark F.10 (SAM with decoupled perturbation numerator and denominator scaling). A perturbation rule with joint gradient normalization and layerwise equivalence classes can be achieved by introducing even more hyperparameters and decoupling numerator and denominator scalings of each layer. For (SAM) with perturbations (DP) with layerwise perturbation radii $\\rho\\cdot n^{-d_{l}}$ and separate gradient norm scaling $\\bar{n}^{-\\tilde{d_{l}}}$ , global perturbation scaling $d$ is superfluous, and there exist layerwise equivalence classes: For any $\\{\\theta_{l}\\}_{l\\in[L+1]}\\subset\\mathbb{R}$ , ", "page_idx": 43}, {"type": "text", "text": "This perturbation rule also allows us to recover an analytical equivalence between trivial weight multipliers $a_{l}=0$ for all $l$ , and any other weight multipliers. ", "page_idx": 43}, {"type": "text", "text": "F.6.2 $\\mu P^{2}$ under non-trivial weight multipliers. ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Our goal here is to find the weight multipliers that simplify the necessary perturbation scaling for effective perturbations in all layers as much as possible. The non-existence of layerwise equivalence classes in abcd-parameterizations from (LP) is not an issue if we are interested in effective perturbation properties and recovering $\\mu\\mathrm{P^{2}}$ for arbitrary weight multipliers $\\{a_{l}\\}_{l\\in[L+1]}$ , as the equivalence breaks due to varying gradient norm contributions, which are inconsequential for achieving effective perturbations. ", "page_idx": 43}, {"type": "text", "text": "As we aim to reproduce $\\mu\\mathrm{P^{2}}$ , we restrict ourselves to the $\\mu\\mathrm{P}$ equivalence class of abcparameterizations. We do not allow layerwise perturbation scaling and are interested in the maximal stable choice of global perturbation scaling $\\rho\\dot{n}^{-d}$ to at least achieve non-vanishing perturbations in some layers. The following lemma shows even more: The choice ", "page_idx": 44}, {"type": "equation", "text": "$$\na_{l}=-1/2\\cdot\\mathbb{I}(l=1)+1/2\\cdot\\mathbb{I}(l=L+1)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "achieves effective perturbations in all layers with the naive (SAM) update rule with naive perturbation scaling $\\rho\\cdot n^{0}$ , and all layers contribute non-vanishingly to the joint gradient norm. Hence this seems to be a natural choice of weight multipliers for SAM. However, it is in conflict with unit scaling considerations (Blake et al., 2024). Effectively, naive learning rate and perturbation scaling with these multipliers is equivalent to (DP) where all denominator terms are scaled to be width independent, as implemented by Algorithm 1, which resembles our implementation for ViTs. Our ablations in Appendix H.4 suggest that gradient norm contributions have a negligible effect on generalization performance. ", "page_idx": 44}, {"type": "text", "text": "Lemma F.11 (Naive perturbation scaling can effectively perturb all layers). Consider an abcdparameterization where $\\{(a_{l},b_{l},c_{l})\\}_{l\\in[L+1]}$ are chosen from the $\\mu P$ equivalence class, and where there is some $C\\in\\mathbb{R}$ such that $d_{l}=C$ for all $l\\in[L+1]$ . This reduces to training a MLP with weight multipliers with (SAM) with global perturbation scaling $\\rho n^{-d}$ for some $d\\in\\mathbb R$ . Effective perturbations in all layers are achieved and all layers contribute non-vanishingly to the gradient norm if and only if ", "page_idx": 44}, {"type": "equation", "text": "$$\na_{1}=-d-1/2,\\qquad a_{l}=-d\\quad f o r\\,l\\in[2,L],\\qquad a_{L+1}=-d+1/2.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Achieving $\\mu\\mathrm{P^{2}}$ with the current implementation of the mup-package requires both an adaptation of the architecture and of the learning algorithm, as the following lemma shows. Hence the package is not particularly suited for SAM learning in $\\mu\\mathrm{P^{2}}$ when the goal is simple perturbation scaling. ", "page_idx": 44}, {"type": "text", "text": "Lemma F.12 (Effective perturbations with the mup-package). Consider an abcd-parameterization where $\\{(a_{l},b_{l},c_{l})\\}_{l\\in[L+1]}$ are chosen from the \u00b5P equivalence class, and with the weight multipliers $a_{L+1}=\\mathbb{I}(l=L+1)$ as in the mup-package. ", "page_idx": 44}, {"type": "text", "text": "(a) (mup-package global scaling effectively perturbs hidden layers) Under global scaling $d_{l}=C$ , $C\\in\\mathbb{R},$ , for all $l\\in[L+1]$ , maximal stable perturbations are achieved with $d=0$ . In this parameterization, hidden layers are effectively perturbed, but input and output layers are not effectively perturbed.   \n(b) $\\mu\\mathbf{P}^{2}$ with the mup-package) Effective perturbations in all layers are achieved with the choice $d=d_{1}=d_{L+1}=-1/2$ and $d_{l}=1/2$ for $l\\in[2,L]$ . ", "page_idx": 44}, {"type": "text", "text": "The following lemma covers the general case how to achieve $\\mu\\mathrm{P^{2}}$ given arbitrary weight multipliers. ", "page_idx": 44}, {"type": "text", "text": "Lemma F.13 ( $\\mu\\mathbf{P}^{2}$ with arbitrary weight multipliers). Consider an abcd-parameterization where $\\{(a_{l},b_{l},c_{l})\\}_{l\\in[L+1]}$ are chosen from the $\\mu P$ equivalence class. Then effective perturbations in all layers are achieved with the choice $d=\\operatorname*{min}_{l\\in[L+1]}(-a_{l}-1/2\\mathbb{I}(l=1)+1/2\\mathbb{I}(l=L+1))$ ), and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{1}=-1-d-2a_{1},\\qquad d_{l}=-d-2a_{l},\\,f o r\\,l\\in[2,L],\\qquad d_{L+1}=1-d-2a_{L+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The following lemma shows that weight multipliers that achieve $\\mu\\mathrm{P^{2}}$ with naive perturbation scaling under perturbations with layerwise normalization (LN) are exactly the same as the ones for (LP). ", "page_idx": 44}, {"type": "text", "text": "Lemma F.14 ((LN) with naive perturbation scaling can effectively perturb all layers). Consider (SAM) with layerwise normalization (LN). Assume $\\{(a_{l},b_{l},c_{l})\\}_{l\\in[L+1]}$ are chosen from the $\\mu P$ equivalence class, and assume there is some $C\\in\\mathbb{R}$ such that $d_{l}=C$ for all $l\\in[L+1]$ . Then all layers are effectively perturbed if the multipliers are chosen as ", "page_idx": 44}, {"type": "equation", "text": "$$\na_{1}=-1/2-C,\\qquad a_{l}=-C,\\qquad a_{L+1}=1/2-C.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma F.14. As derived in Appendix F.7, under $a_{l}=0$ for all $l\\in[L+1]$ , all layers are effectively perturbed if and only if $d_{l}=-1/2\\cdot\\mathbb{I}(l=1)+1/2\\cdot\\mathbb{I}(l=L+1)$ . Now we can exploit the layerwise equivalence relation to enforce $d_{l}=C$ in each layer by adapting all $a_{l}$ . \u53e3 ", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma $F.I I$ . In general, in the abc-equivalence class of $\\mu\\mathrm{P},$ the $l$ -th layer\u2019s gradient norm is scaled by $n^{-a_{l}}$ . This induces the generalized gradient norm constraints for $\\bigl\\|\\nabla_{W}L\\bigr\\|^{\\!\\!^{\\circ}}=\\Theta(1)$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\nd_{1}+a_{1}\\geq-1/2,\\qquad d_{l}+a_{l}\\geq0,\\qquad d_{L+1}+a_{L+1}\\geq1/2.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Effective perturbations are achieved when $\\rho n^{-d-d_{l}-a_{l}}\\nabla_{W^{l}}L=\\Theta(n^{-\\mathbb{I}(l>1)})$ , which induces the perturbation stability constraints ", "page_idx": 45}, {"type": "equation", "text": "$$\nd+d_{1}+2a_{1}\\ge-1,\\qquad d+d_{l}+2a_{l}\\ge0,\\qquad d+d_{L+1}+2a_{L+1}\\ge1,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with effective perturbations whenever the equality of the respective layer holds. ", "page_idx": 45}, {"type": "text", "text": "Under global scaling, the gradient norm constraints become, for some $C\\in\\mathbb{R}$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\nC+a_{1}\\geq-1/2,\\qquad C+a_{l}\\geq0,\\qquad C+a_{L+1}\\geq1/2,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and the conditions for effective perturbations become ", "page_idx": 45}, {"type": "equation", "text": "$$\nd+C+2a_{1}\\geq-1,\\qquad d+C+2a_{l}\\geq0,\\qquad d+C+2a_{L+1}\\geq1.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "As $d+C$ is a common term in all layers, we get the relations $a_{l}=a_{1}+1/2$ , $a_{L+1}=a_{1}+1$ , so that all gradient norm constraints are simultaneously satisfied with $C=-a_{l}$ and effective perturbations are achieved in all layers with $d=-a_{l}$ . \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma F.12. Under the choice $a_{l}=\\mathbb{I}(l=L+1)$ , the gradient norm constraints become ", "page_idx": 45}, {"type": "equation", "text": "$$\nd_{1}\\geq-1/2,\\qquad d_{l}\\geq0,\\qquad d_{L+1}\\geq-1/2,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and the conditions for effective perturbations become ", "page_idx": 45}, {"type": "equation", "text": "$$\nd+d_{1}\\geq-1,\\qquad d+d_{l}\\geq0,\\qquad d+d_{L+1}\\geq-1.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof of (a): ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Satisfying the gradient norm constraints with global scaling requires $d_{l}=0$ for all $l\\in[L+1]$ , then the minimal stable choice of $d$ is $d=0$ which only effectively perturbs hidden layers. ", "page_idx": 45}, {"type": "text", "text": "Proof of (b): ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The choice $d=-1/2$ and $d_{1}=-1/2$ saturates the gradient norm constraint and achieves effective perturbations in the input layer. Then the choice $d_{l}\\bar{=}1/2$ and $d_{L+1}=-1/2$ satisfies the gradient norm constraints and achieves effective perturbations in all layers. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma $F7$ . To understand the influence of weight multipliers on updates and perturbations, first note that under an equivalence transformation of all abcd-parameters w.l.o.g from $a_{l}=0$ for all $l\\in[L+1]$ , the scalings of $h^{l},x^{l}$ and of $n^{-a_{l}}W^{l}$ remain invariant. This implies that the scalings of $\\nabla_{x^{L}}f=n^{-a_{L+1}}W^{L+1}$ , $\\nabla_{h^{l}}f=\\nabla_{x^{l}}f\\odot\\phi^{\\prime}(h^{l})$ and $\\nabla_{x^{l}}f$ for all $l\\in[L]$ also remain invariant. Hence the weight gradients, $\\nabla_{W^{L+1}}f=n^{-a_{L+1}}x^{L}$ and $\\nabla_{W^{l}}f=n^{-a_{l}}\\nabla_{h^{l}}f\\cdot(x^{l-1})^{\\top}$ are scaled by $n^{-a_{l}}$ in each layer. ", "page_idx": 45}, {"type": "text", "text": "In the following forward pass, we get ", "page_idx": 45}, {"type": "equation", "text": "$$\nh^{l}=n^{-a_{l}}(W^{l}+\\Delta W^{l})x^{l-1}=n^{-a_{l}}(W^{l}-\\eta n^{-c_{l}}\\nabla_{W^{l}}\\mathcal{L})x^{l-1},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "so that activation/output updates and perturbations of layer $l$ are scaled by $n^{-2a_{l}}$ . ", "page_idx": 45}, {"type": "text", "text": "Again, a complication compared to SGD or Adam arises through the gradient normalization of SAM\u2019s weight perturbation. If the gradients are simply normalized layerwise $\\bar{\\pmb\\varepsilon}^{l}=\\rho\\!\\cdot\\!n^{-d_{l}}\\!\\cdot\\!\\nabla_{W^{l}}\\mathcal{L}/\\|\\nabla_{W^{l}}\\mathcal{L}\\|$ , the $n^{-a\\i}$ -term from the backward pass cancels out, and only in the forward pass we get a scaling $n^{-a\\i}$ . Hence an exact layerwise equivalence still exists for SAM with layerwise gradient normalization: ", "page_idx": 45}, {"type": "equation", "text": "$$\n(a_{l},b_{l},c_{l},d_{l})\\mathrm{~is~equivalent~to~}(a_{l}+\\theta_{l},b_{l}-\\theta_{l},c_{l}-2\\theta_{l},d_{l}-\\theta_{l}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Under joint gradient normalization (SAM), as we consider in our definition of $b c d$ -parameterizations, keeping the gradient norm scalings invariant under $a_{l}\\mapsto a_{l}+\\theta$ would require $d_{l}\\mapsto d_{l}-\\theta$ , but keeping the perturbation scaling invariant would require $d_{l}\\mapsto d_{l}-2\\theta$ as for updates. Consequently, due to the layer coupling of joint gradient normalization $\\|\\nabla_{\\mathbf{W}}\\mathcal{L}\\|$ , an exact equivalence between abcd-parameterizations at finite width requires $\\theta$ to be the same for all layers and the conflicting gradient norm in the denominator and perturbation scaling in the numerator to be accounted for by $d_{l}\\mapsto d_{l}-\\theta$ and $d\\mapsto d-\\theta$ . ", "page_idx": 45}, {"type": "text", "text": "In the infinite-width limit, abcd-parameterizations remain equivalent under $(a_{l}+\\theta_{l},b_{l}-\\theta_{l},c_{l}-$ $2\\theta_{l},d_{l}\\,-\\,2\\theta_{l},d)$ layerwise as long as the set of layers that contributes to the gradient norm nonvanishingly remains invariant. The gradient norm constraints for $\\|v^{l}\\|=O(1)$ become ", "page_idx": 46}, {"type": "equation", "text": "$$\nd_{1}+a_{1}\\geq1/2-c_{\\nabla},\\qquad d_{l}+a_{l}\\geq1-c_{\\nabla},\\qquad d_{L+1}+a_{L+1}\\geq1/2,\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\nabla_{x^{L}}f=n^{-a_{L+1}}W^{L+1}=\\Theta(n^{-c_{\\nabla}})$ with $c_{\\nabla}=\\mathrm{min}(b_{L+1}+a_{L+1},c_{L+1}+2a_{L+1})$ remains invariant under equivalence transformations. \u53e3 ", "page_idx": 46}, {"type": "text", "text": "Proof of Lemma F.13. First, the choices of $d_{l}$ ensure that the constraints for effective perturbations from the proof of Lemma F.11 are saturated in each layer. It is left to show, that these choices satisfy the $\\|\\nabla_{W}L\\|\\,=\\,\\Theta(1)$ -constraints. For input layers, since $-d\\,\\geq\\,a_{1}+1/2$ , it holds that $d_{1}+a_{1}\\geq-1/2$ . For hidden layers, since $-d\\ge a_{l}$ , it holds that $d_{l}+a_{l}\\geq0$ . For output layers, since $-d\\ge a_{L+1}-1/2$ , it holds that $d_{L+1}+a_{L+1}\\geq1/2$ . Observe that the minimizer in the definition of $d$ saturates its gradient norm constraint. \u53e3 ", "page_idx": 46}, {"type": "text", "text": "F.7 The spectral perspective on $\\mu P^{2}$ ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "While Tensor Programs allow to track the transformations of vectors like activations, Yang et al. (2023a) provide an equivalent formulation in terms of weight matrix spectral norms. They find that the spectral norm measures the effect of a weight update on the activations, under certain non-cancellation assumptions and limited batch size. For all MLP layers, they show that $\\mu\\mathrm{P}$ is equivalent to achieving the condition ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{t}^{l}\\|_{*}=\\Theta\\left(\\sqrt{\\frac{\\mathtt{f a n}_{-}\\mathrm{out}}{\\mathtt{f a n}_{-}\\mathrm{in}}}\\right)\\quad\\mathrm{and}\\quad\\|\\Delta W_{t}^{l}\\|_{*}=\\Theta\\left(\\sqrt{\\frac{\\mathtt{f a n}_{-}\\mathrm{out}}{\\mathtt{f a n}_{-}\\mathrm{in}}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "at all times $t$ , where $W_{t}^{l}:\\mathbb{R}^{{\\tt f a n\\_i n}}\\rightarrow\\mathbb{R}^{{\\tt f a n\\_o u t}}$ . This condition is achieved with initialization $\\sigma_{l}$ , SGD learning rate $\\eta_{l}$ and Adam learning rate \u03b7lAdamchosen as, ", "page_idx": 46}, {"type": "equation", "text": "$$\nr_{l}=\\Theta\\left(\\frac{1}{\\sqrt{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathbf{in}}}\\operatorname*{min}\\left\\{1,\\sqrt{\\frac{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathbf{out}}{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathbf{in}}}\\right\\}\\right),\\quad\\eta_{l}=\\Theta\\left(\\frac{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathbf{out}}{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathbf{in}}\\right),\\quad\\eta_{l}^{A d a m}=\\Theta\\left(\\frac{1}{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathbf{in}}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This generalizes $\\mu\\mathrm{P}$ to varying widths inside the network. For varying widths, we adopt the notation $W^{l}:\\mathbf{\\bar{R}}^{n_{l-1}}\\rightarrow\\mathbf{\\bar{R}}^{n_{l}}$ with $n_{0}=d_{i n}$ and $n_{L+1}=d_{o u t}$ , whereas fan_in and fan_out always adapt to the weight matrix under consideration. ", "page_idx": 46}, {"type": "text", "text": "To understand why the spectral norm is desirable, note that $\\Delta W^{l}=\\eta_{l}\\nabla_{h^{l}}\\mathcal{L}(x^{l-1})^{\\top}$ is low rank and aligned with the incoming activations. For batch size 1, we even have rank-1 updates with $\\|\\Delta W^{l}\\|_{*}=\\eta_{l}\\|\\nabla_{h^{l}}\\mathcal{L}\\|_{2}\\|x^{l-1}\\|_{2}$ , aligned with the incoming activations $x^{l-1}$ , hence $\\Vert\\Delta\\dot{W}^{l}x^{l-1}\\Vert_{2}=$ $\\lVert\\Delta W^{l}\\rVert_{*}\\lVert x^{l-1}\\rVert_{2}$ . This allows to achieve $\\|\\Delta x^{l}\\|_{2}\\,=\\,\\Theta(\\sqrt{n_{l}})$ irrespective of the layer type with $\\|\\Delta W_{t}^{l}\\|_{*}=\\Theta(\\sqrt{n_{l}/n_{l-1}})$ . ", "page_idx": 46}, {"type": "text", "text": "Our simple condition that perturbations should scale like updates, which is rigorously justified by our Tensor Program based proof in Appendix E, now allows to derive the correct perturbation scalings using the spectral weight perspective. ", "page_idx": 46}, {"type": "text", "text": "Layerwise perturbations. As a simple starting point, consider a variant of (SAM) that does not globally normalize the gradient of all layers jointly, but uses layerwise normalization (LN), resulting in the layerwise perturbation rule, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\pmb{\\varepsilon}^{l}=\\rho_{l}\\cdot\\nabla_{W^{l}}\\mathcal{L}/\\|\\nabla_{W^{l}}\\mathcal{L}\\|,\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\left\\Vert\\cdot\\right\\Vert$ may denote either the spectral or the Frobenius norm (equivalent under limited perturbation batch size). Without the global normalization, the scalings of all layers are not coupled, and the spectral condition $\\|\\epsilon^{l}\\|_{*}=\\Theta(\\sqrt{\\mathtt{f a n\\_o u t}/\\mathtt{f a n\\_i n}})$ immediately requires choosing ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\rho_{l}=\\rho\\cdot\\sqrt{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathrm{out}/\\mathbf{f}\\mathbf{a}\\mathrm{n}_{-}\\mathrm{in}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for effective perturbations in layer $l$ with width-independent hyperparameter $\\rho\\geq0$ . ", "page_idx": 46}, {"type": "text", "text": "Perturbations with global gradient normalization. Perturbations that are globally normalized across layers have usually been implemented practice according to the GitHub repositories provided by Foret et al. (2021); Samuel (2022); Kwon et al. (2021); Andriushchenko and Flammarion (2022); M\u00fcller et al. (2024). Since we are interested in analysing (SAM) as it is applied in practice, we study variants with joint gradient normalization in more detail. Preliminary ablations in Appendix H.5 suggest that layer-coupled SAM with global normalization slightly outperforms SAM with layerwise gradient normalization. To simplify the analysis as much as possible, we will first ensure widthindependence of the normalization, so that the layerwise perturbation scaling is not affected by the normalization term. Then, layerwise perturbations should again be scaled like updates. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "Separate denominator scalings. If we allow to scale each denominator term separately from the corresponding numerator term (DP), the perturbation radius in each layer for the numerator can be scaled like updates, $\\begin{array}{r}{\\rho_{l}=\\Theta\\left(\\frac{\\tt f a n\\tt_{-}o u t}{\\tt f a n\\tt_{-}i n}\\right)}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "Now, to ensure $\\Theta(1)$ in the denominator, each input and hidden-like gradient norm $\\|\\nabla_{W^{l}}\\mathcal{L}\\|_{F}$ , $l\\in[L]$ , achieves width-independence if it scaled by $\\sqrt{\\mathtt{f}\\mathtt{a n}_{-}\\mathrm{out}/\\mathtt{f}\\,\\mathtt{a n}_{-}\\mathrm{in}}$ . The same rule applies to biases when understanding them as weights $\\mathbb{R}\\to\\mathbb{R}^{n_{l}}$ to the input 1. These scalings are derived in the next paragraph. The last-layer gradient norm $\\|\\nabla_{W^{L+1}}\\mathcal{L}\\|_{F}$ should be scaled as $(n_{L}n_{L+1})^{-1/2}$ , and \u2225\u2207bL+1L\u2225F as nL+1 . ", "page_idx": 47}, {"type": "text", "text": "If we care about the correct width-independent constants, observe that the learning rate scaling $\\eta_{L+1}=\\Theta(\\mathtt{f a n\\_o u t}/\\mathtt{f a n\\_i n})$ induces $\\|W^{L+1}\\|=\\|\\Delta W^{L+1}\\|=\\Theta(\\sqrt{n_{L+1}^{3}/n_{L}})$ . If we wanted to achieve $\\Delta W^{L+1}=\\Theta(\\sqrt{n_{L+1}/n_{L}})$ we would need $\\eta_{L+1}=\\Theta(1/\\mathtt{f a n}_{-}\\mathrm{in})$ . As $n_{L+1}=d_{o u t}$ is width-independent, $\\sqrt{\\mathtt{f}\\mathtt{a n}_{-}\\mathrm{out}/\\mathtt{f}\\,\\mathtt{a n}_{-}\\mathrm{in}}$ would result in the same width-dependent scaling for the last layer, but ignoring large constants can introduce a significant width-independent spectral distortion. For example in ImageNet1K, $n_{L+1}$ is large. By tuning input, hidden and output multipliers such constant distortions may be corrected. The multiplier used in the mup-package does not correct this distortion. Using a base width at which SP is recovered may also cement such spectral distortions, if no multipliers are tuned. ", "page_idx": 47}, {"type": "text", "text": "Derivation of gradient norm $\\|\\nabla_{W^{l}}\\mathcal{L}\\|_{F}$ scalings. In this paragraph, $\\|\\cdot\\|$ may denote the Frobenius or spectral norm. As all matrices are of limited rank, both norms scale equivalently. As a first step, $\\begin{array}{r}{\\|\\nabla_{h^{l}}\\mathcal{L}\\|=\\Theta(\\frac{1}{\\sqrt{n_{l}}})}\\end{array}$ can be reconstructed from ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Theta(\\sqrt{n_{l}/n_{l-1}})=\\|\\Delta W^{l}\\|_{*}=\\eta_{l}\\|\\nabla_{h^{l}}\\mathcal{L}\\|\\|x^{l-1}\\|_{2}=n_{l}/n_{l-1}\\cdot\\sqrt{n_{l-1}}\\|\\nabla_{h^{l}}\\mathcal{L}\\|.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Now, for input and hidden layers, $\\|\\nabla_{W^{l}}{\\mathcal{L}}\\|=\\|\\nabla_{h^{l}}{\\mathcal{L}}\\|\\|x^{l-1}\\|_{2}=\\Theta(\\sqrt{n_{l-1}/n_{l}})$ . Multiplying by the inverse yields width-independent scaling. The output layer gradient $\\nabla_{W^{L+1}}\\mathcal{L}\\in\\mathbb{R}^{n_{L+1}\\times n_{L}}$ is given by $(\\bar{\\nabla}_{W^{L+1}}\\mathcal{L})_{i j}=x_{j}^{L}\\overset{\\cdot}{=}\\Theta(1)$ , so that $\\|\\nabla_{W^{L+1}}\\bar{\\mathcal{L}}\\|=\\Theta(\\bar{\\sqrt{n_{L}n_{L+1}}})$ . Biases before the last layer follow the scheme $\\|\\nabla_{b^{l}}{\\mathcal{L}}\\|=\\|\\nabla_{h^{l}}{\\mathcal{L}}\\|=\\Theta({\\sqrt{1/n_{l}}})=\\Theta({\\sqrt{\\mathtt{f}\\mathtt{a n}_{-}\\mathrm{in}/\\mathtt{f}\\mathtt{a n}_{-}\\mathrm{out}}})$ . The last layer bias $\\|\\nabla_{b^{L+1}}\\mathcal{L}\\|=\\sqrt{n_{L+1}}$ scales width-independently as it should, but needs to be scaled by a different constant $1/\\sqrt{\\mathbf{f}\\mathbf{an}_{-}\\mathsf{o u t}}$ than earlier layers. ", "page_idx": 47}, {"type": "text", "text": "Extensions to ASAM. As ASAM cannot be written as a NE\u2297OR\u22a4program, its scaling can only be derived heuristically. As provided in Table 1 and derived in Appendix F.4, elementwise ASAM scales all layer types correctly in relation to each other, and it suffices to rescale the global perturbation radius by $\\sqrt{n_{L}}$ , assuming all width dimensions scale proportionally. For SAM-ON, we only perturb input-like layers such as normalization layers. As the conditions for correct scaling remain the same, the above scalings for input layers in SAM also apply to SAM-ON. ", "page_idx": 47}, {"type": "text", "text": "For layerwise ASAM, first note that $\\|W_{t}^{l}\\|_{F}=\\Theta(\\|W_{0}^{l}\\|_{F})=\\Theta(\\sqrt{n_{l}})$ for input and hidden layers $l~\\in~[L]$ . As the numerator contains $\\|W_{t}^{l}\\|_{F}^{2}$ , it requires the layerwise perturbation scaling $\\frac{1}{\\mathtt{f a n\\_i n}}$ . In the denominator, width independence is achieved with the multiplier $\\sqrt{\\frac{1}{\\mathtt{f a n\\_i n}}}$ , since $\\begin{array}{r}{\\|W^{l}\\|_{F}\\|\\nabla_{W^{l}}\\mathcal{L}\\|_{*}=\\sqrt{n_{l}}\\sqrt{\\frac{n_{l-1}}{n_{l}}}=\\sqrt{n_{l-1}}}\\end{array}$ . Again, the output layer requires a special treatment. Due to its small initialization, it holds that \u2225W L+1\u22252F = \u2225\u2206W L+1\u22252F = \u0398(n3nLL+1 ) . For perturbations that fulfill the spectral condition $\\begin{array}{r}{\\rho_{L+1}\\|W^{L+1}\\|_{F}^{2}\\|\\nabla_{W^{l+1}}\\mathcal{L}\\|_{*}=\\Theta(\\sqrt{\\frac{n_{L+1}}{n_{L}}})}\\end{array}$ , we need to choose $\\rho_{L+1}=\\rho\\cdot\\frac{1}{n_{L+1}^{3}}$ (width-independent, but very small). The last-layer denominator term scales as $\\begin{array}{r}{\\|W^{L+1}\\|_{F}\\|\\nabla_{W^{l+1}}\\mathcal{L}\\|_{*}=\\Theta(\\sqrt{\\frac{n_{L+1}^{3}}{n_{L}}}\\cdot\\sqrt{n_{L}n_{L+1}})=\\Theta(n_{L+1}^{2})}\\end{array}$ n3nLL+1\u00b7 \u221anLnL+1) = \u0398(n2L+1), which is width independent, but can be a large constant, as for ImageNet1K. The output bias numerator exactly conforms with the correct scaling $\\|\\nabla_{b^{L+1}}\\mathcal{L}\\|_{F}^{2}=n_{L+1}^{\\texttt{-}}=\\mathtt{f a n\\_o u t}/\\mathtt{f a n\\_i n}=\\Theta(1)$ . ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "Note that weight decay may break statements like $\\|W_{t}^{l}\\|_{F}=\\Theta(\\|W_{0}^{l}\\|_{F})$ over long training. Everett et al. (2024) have recently observed more generally that scalings may evolve differently over long training than predicted by pure infinite-width TP theory, because alignments evolve dynamically between CLT- and LLN-like behaviour. ", "page_idx": 48}, {"type": "text", "text": "Using the mup-package. The mup-package introduces the output layer weight multiplier $n_{L}^{-1}$ so that input and output layer learning rates may be scaled by the same width-dependent factor. Hence, only the last-layer scalings change. The scalings of $n_{L}^{-1}\\bar{W}^{L+1}$ and $n_{L}^{-1}\\Delta W^{\\bar{L}+1}$ remain the unique ones that achieve $\\mu\\mathrm{P},$ but $\\nabla_{W^{L+1}}\\mathcal{L}$ is scaled by $n_{L}^{-1}$ . This requires adapting the last-layer learning rate $\\eta_{L+1}$ to scale like input layers. For SAM, the last-layer perturbation radius can now be scaled like input layers. That is, assuming proportionally growing width $n$ , in the numerator $\\rho_{L+1}=\\rho_{1}=\\rho\\cdot n$ and $\\rho_{l}=\\rho$ for $l\\in[2,L]$ , and the gradient norm contributions should be scaled by $\\sqrt{n}$ for input and output layers, and by 1 for hidden layers. The Tensor Program perspective on weight multipliers can be found in Appendix F.6. The correct width-independent constants are achieved with the last-layer numerator scaling $\\rho_{L+1}=\\rho\\cdot n_{L}$ and the last-layer denominator scaling $\\sqrt{n_{L}/n_{L+1}}$ , since $\\nabla_{W^{L+1}}\\mathcal{L}=\\Theta(\\sqrt{n_{L+1}/n_{L}})$ and for the numerator we get an additional $n_{L}^{-1}$ in the forward pass. ", "page_idx": 48}, {"type": "text", "text": "For SAM-ON nothing changes, as only input-like layers are perturbed. For elementwise ASAM, ignoring width-independent constants, nothing changes as the weight multiplier $n_{L}^{-1}$ increases the weight scaling $W^{L+1}$ and decreases the gradient scaling $\\nabla_{W L+1}\\mathcal{L}$ by the same amount. The additional $n_{L}^{-1}$ -factor in the numerator is cancelled out by the additional $W^{L+1}$ -factor. For the correct width-independent constants with decoupled numerator and denominator scaling, we would scale the denominator by $\\sqrt{n_{L}/n_{L+1}^{3}}$ with or without weight multiplier, and scale the numerator by $\\rho_{L+1}=\\rho\\cdot n_{L}/n_{L+1}^{2}$ with or without weight multiplier. For the example of layerwise ASAM, we still get for the denominator $\\|W^{L+1}\\|_{F}\\|\\nabla_{W^{l+1}}\\mathcal{L}\\|_{*}=\\Theta(n_{L+1}^{2})$ , again because the weights $W^{L+1}$ are scaled up by $n_{L}$ and the gradient is scaled down by the same amount. In the numerator, the upscaling of the weights also cancels out the downscaling of the gradient and additional $n_{L}^{-1}$ in the subsequent forward pass, leading to an unchanged $\\rho_{L+1}=\\rho\\cdot n_{L+1}^{-3}$ , which is width-independent but potentially leads to numerical issues. ", "page_idx": 48}, {"type": "text", "text": "Code for $\\mu\\mathbf{P}^{2}$ with separate denominator scalings. Algorithm 1 provides a PyTorch code example that implements the above $\\mu\\mathrm{P^{2}}$ scalings for SAM, scaling the gradient norm contributions of all layers to $\\Theta(1)$ (equivalent to $(a{-}\\mu P^{2})$ together with naive perturbation and learning rate scaling). We adapt the popular SAM implementation Samuel (2022) using the mup-package. This code resembles our implementation for the ViT experiments. In the mup-package, \u2018vector-like\u00b4 parameters scale as $n\\times$ constant or constant $\\times\\ n$ and include input and output weights. The last-layer multiplier $n_{L}^{-1}$ is chosen so that input and output layers can be scaled by the same width-dependent factor. On the other hand, \u2018matrix-like\u00b4 parameters scale as $n\\times n$ and include hidden weights. The implementation uses a base width at which $\\mu\\mathrm{P^{2}}$ and SP are equivalent; all width-dependent scalings then scale with width-multipliers width/base_width. This allows to immediately transfer well-performing settings from SP to $\\dot{\\mu}\\mathrm{P^{2}}$ . ", "page_idx": 48}, {"type": "text", "text": "Let us recapitulate how the $\\mu\\mathrm{P^{2}}$ scaling in the following code arises. The crucial variables to track are factor, group[\"rho\"] and group[\"gradnorm_scaling\"]. For limited batch size, the spectral and Frobenius norm of gradients scale equivalently, and we get, for all $l\\in[L]$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{W^{l}}\\mathcal{L}\\|_{F}=\\Theta(\\|\\nabla_{W^{l}}\\mathcal{L}\\|_{*})=\\Theta\\left(\\sqrt{\\frac{{\\bf f}\\mathrm{an}_{-}\\mathrm{in}}{{\\bf f}\\mathrm{an}_{-}\\mathrm{out}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We want to scale each weight\u2019s contribution in the denominator to be width-independent, hence need the factor $\\scriptstyle{\\sqrt{\\tt f a c t o r}}$ with factor $=$ fan_out/fan_in. For the numerator, the spectral condition $(*)$ demands $\\begin{array}{r}{\\|\\rho_{l}\\cdot\\nabla_{W^{l}}\\mathcal L\\|_{*}\\overset{!}{=}\\Theta(\\sqrt{\\frac{\\mathtt{f a n}_{-}\\mathrm{out}}{\\mathtt{f a n}_{-}\\mathrm{in}}})}\\end{array}$ , so that we need to scale the weight\u2019s perturbation radius to $\\rho_{l}=\\rho\\cdot$ factor. Since the mup-package sets the last-layer weight multiplier such that input and output layers can be scaled in the same way, the implementation is short. For optimal numerical properties however, this choice of multipliers is sub-optimal (Blake et al., 2024). ", "page_idx": 48}, {"type": "text", "text": "import math , torch", "page_idx": 49}, {"type": "text", "text": "from mup import MuAdamW   \n# specify parameterization   \nparameterization $=$ \u2019mupp \u2019 # \u2019sp -naive \u2019, \u2019mup -naive \u2019   \n# for \u2019mup -global \u2019 use \u2019mup -naive \u2019 and scale rho accordingly   \n# specify model and hyperparameters   \nmodel , lr , rho , weight_decay , last_layer_weight_name $=$ ...   \n# adapt SAM to allow gradient norm scaling of each weight tensor   \nclass SAM(torch.optim.Optimizer): ... def grad_norm(self): grads $=$ [] for i, group in enumerate(self.param_groups): for p in group[\"params\"]: grads.append ((group[\"gradnorm_scaling\"] $^*$ p.grad).norm( $p\\mathtt{=}2$ )) norm $=$ torch.stack(grads).norm( $p\\!=\\!2$ ) return norm @torch.no_grad () def first_step(self): # perturbation step before the weight update grad_norm $=$ self.grad_norm () for group in self.param_groups: scale $=$ group[\"rho\"] / (grad_norm + 1e -12) for p in group[\"params\"]: if p.grad is None: continue self.state[p][\"old_p\"] $=$ p.data.clone () e_w = p.grad $^*$ scale.to(p) p.add_(e_w) # climb to the local maximum \" $^{1}\\mathrm{~\\boldmath~{~\\pi~}~}+\\mathrm{~\\boldmath~{~e~}~}(\\mathrm{~\\boldmath~{~\\pi~}~})$ \"   \n# set width -dependent rho and gradient norm scaling for each weight   \nparam_groups $=$ []   \nfor name , p in model. named_parameters (): if p.infshape.ninf () $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad0$ or \u2019naive \u2019 in parameterization : factor $\\mathit{\\Theta}=\\mathit{\\Theta}_{1}$ elif p.infshape.ninf () $\\circleddash\\ \\ 1$ : # vector -like for d in p.infshape: if d.base_dim is not None: factor $=$ d.dim / d.base_dim #width break elif p.infshape.ninf () $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad2$ : # matrix -like factor $=$ (p.infshape[0]. ${\\tt d i m/p}$ .infshape[1].dim) $^*$ (p.infshape [1]. base_dim/ p.infshape [0]. base_dim) # fan_out/fan_in else: raise NotImplementedError $\\begin{array}{r l r}{\\mathbf{g}\\mathbf{r}\\circ\\mathbf{u}\\mathbf{p}}&{{}=}&{\\left\\{\\begin{array}{l l}\\end{array}\\right.}\\end{array}$ \"params\": [p], \"lr\": lr , \"rho\": rho $^*$ factor, \" gradnorm_scaling \": math.sqrt(factor), } param_groups.append(group) ", "page_idx": 49}, {"type": "text", "text": "Algorithm 1: Pytorch implementation of $\\mu\\mathrm{P^{2}}$ for SAM using the mup-package. Key changes from the original implementation that correct the layerwise perturbation scaling are highlighted with gray boxes. This code decouples the scalings of numerator and denominator terms following (DP), and scales the gradient norm contributions of all layers by group[\"gradnorm_scaling\"] in the denominator to be width-independent. The numerator terms group[\"rho\"] of all weight tensors are scaled to achieve effective perturbations. This scaling is equivalent to $(a{-}\\mu P^{2})$ together with naive perturbation and learning rate scaling. ", "page_idx": 50}, {"type": "text", "text": "G Experimental details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "If not mentioned otherwise, experiments use the settings specified in this section. ", "page_idx": 50}, {"type": "text", "text": "Implementation details. For MLPs, we exactly implement our Definition 4 of bcd-parameterizations to precisely validate our theoretical results. For ResNets and ViTs, the width varies inside the network, so that we implement the spectral scaling rules derived in Appendix F.7. Like the mup-package, we introduce a base width at which SP and $\\mu\\mathrm{P}$ are equivalent, allowing to immediately transfer setups that perform well in SP. We use the mup-package only for ViTs, and our implementation of $\\mu\\bar{\\bf P}^{2}$ resembles the pseudocode provided in Algorithm 1. For ResNets, we use no width-dependent lastlayer multiplier. At initialization, $\\mu\\mathrm{P}$ differs from SP only through a smaller last layer initialization. For MLPs we exactly implement the bcd-parameterization with $b_{L+1}=1$ . For ResNets and Vits, we initialize the last layer to 0 in $\\mu\\mathrm{P}_{i}$ which corresponds to $b_{L+1}\\to\\infty$ and which recovers the limit behaviour $f_{0}\\rightarrow0$ already at finite width. We are working on making Python code to reproduce all of our experiments publicly available. ", "page_idx": 50}, {"type": "text", "text": "MLPs. We train 3-layer MLPs without biases with ReLU activation function for 20 epochs with constant learning rate, using SGD as base optimizer as specified in Definition 4, but allow for SGD batchsize larger than 1, defaulting to batch size 64. We evaluate the test accuracy after every epoch and use the snapshot across training with the best accuracy. This is necessary as the test accuracy is not monotonically increasing across training, while the training accuracy is. For ResNets we do not observe such harmful overfitting. For the standard parametrization, we use He initialization (He et al., 2015) and don\u2019t tune multipliers to mimic standard training procedures. For $\\mu\\mathrm{P}_{i}$ , we resort to the optimal multipliers from Yang et al. (2022). We then find the optimal learning rate and perturbation radius for each bcd-parametrization and SAM variant separately. ", "page_idx": 50}, {"type": "text", "text": "ResNets. For ResNet18 experiments, we augment the CIFAR10 data with random crops and random horizontal flips, set labelsmoothing to 0.1 and use a cosine learning rate schedule. ResNets in $\\mu\\mathrm{P}$ have base width 0.5, gradient norm scaling according to Definition 4 and their last layer is initialized to 0. For SP, we again adopt the standard hyperparameters from M\u00fcller et al. (2024) by using a momentum of 0.9, weight decay 0.0005, an output multiplier of 1.0, and individually tuned learning rate and perturbation radius for each SAM variant. For $\\mu\\mathrm{P},$ at base width multiplier 0.5 compared to the original width, for each SAM variant, we perform a random grid search over the hyperparameters learning rate, perturbation radius, output multiplier $[2^{-8},2^{-7},\\bar{.}\\ldots,2^{8}]$ , weight decay $[0,\\bar{1}0^{-5},10^{-4},5\\cdot10^{-4},10^{-3},10^{-2}]$ and momentum $[0,0.1,0.4,\\dot{0}.7,0.$ 9]. Learning rate and perturbation radius grids were either set to $[2^{-10},2^{-9},\\dots,2^{\\bar{1}}]$ or centered around recommendations from the literature. The optimal hyperparameter configurations found from at least 150 runs for each SAM variant are summarized in Table G.1. Learning rates and perturbation radii were further tuned with the experiments from Appendix H.3.2. ", "page_idx": 50}, {"type": "text", "text": "ViTs. We train ViT-S/16 with 6 layers and 12 attention heads on ImageNet1K (Deng et al., 2009) and a ViT-S/4 with 12 layers and 12 attention heads on CIFAR100 (Krizhevsky et al., 2009) (see Appendix H.6), again adopting the hyperparameter settings from M\u00fcller et al. (2024). This means we use AdamW as a base optimizer with warmup and a cosine learning rate decay. For CIFAR100, we use random crops, random horizontal flips and AutoAugment as data augmentations. For Imagenet we use the original preprocessing from Huggingface vit-base-patch16-224 (Wu et al., 2020). For $\\mu\\mathrm{P},$ , we tune multipliers at a basewidth 384, initialize the last layer and query weights to 0. By using the $\\mu\\mathrm{P}$ package, the relative perturbation scalings change as explained in Appendix F.7 and Appendix F.6. Global and naive perturbation scaling in $\\mu\\mathrm{P}$ now coincide. Here, instead of the original perturbation scaling Definition 4, we scale the gradient norm contributions of all layers in the denominator to $\\Theta(1)$ . The hyperparameter choices for ViTs on CIFAR100 and ImageNet are summarized in Table G.2. For $\\mu\\mathrm{P},$ the learning rate, perturbation radius, input multiplier, output multiplier and weight decay were tuned using 3 independent runs of Nevergrad NGOpt with budget 56 on ImageNet. The same multipliers are used on CIFAR100. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "Figures. Whenever multiple runs with independent random seeds are used for training, confidence bands cover the interval from the empirical $2.5\\%$ - to the empirical $97.5\\%$ -quantile. The line then denotes the average of all runs. When confidence bands are given, but the number of independent runs is not specified, the number of runs defaults to 4. ", "page_idx": 51}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/4a98de605efcbd3a83ecce21fabfeb960d901c2997f7248a7f06b15e13363dfa.jpg", "table_caption": ["Computational resources. We ran all of our experiments on Amazon EC2 G5 instances each containing up to 8 NVIDIA A10G GPUs. On a single GPU, our $\\mu\\mathrm{P^{2}}$ -SAM training script for MLPs of width 4096 on CIFAR10 takes 502 seconds to run in total (25 seconds per epoch), where data handling takes most of the time. The training times for ResNets and ViTs are presented in Table G.3. "], "table_footnote": [], "page_idx": 51}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/5df746da0ecf104ed0ab1914b1c69d5ccbbc2774c852f552a60db22eeb744e3e.jpg", "table_caption": ["Table G.1: (ResNet-18 hyperparameters for CIFAR10) Hyperparameters for SP are taken from M\u00fcller et al. (2024). Learning rate and perturbation radius are tuned using the experiments in Appendix H.3.2. ResNets in $\\mu\\mathrm{P}$ have base width 0.5, gradient norm scaling according to Definition 4 and their last layer is initialized to 0. "], "table_footnote": [], "page_idx": 51}, {"type": "text", "text": "Table G.2: (Vision Transformer hyperparameters) Hyperparameters for SP are taken from M\u00fcller et al. (2024) using AdamW as a base optimizer. ViTs in $\\mu\\mathrm{P}$ have base width 384, last layer and query weights are initialized to 0 and gradient norm contributions of all layers are scaled to $\\dot{\\Theta}(1)$ . ", "page_idx": 51}, {"type": "table", "img_path": "pR5g1bBqoV/tmp/dc588ee00ec830f1972ef00a39c0c9068d77d43698d8cfc4c9aee7e7096c2bef.jpg", "table_caption": [], "table_footnote": ["Table G.3: (Training time per epoch) Training time (in seconds) per epoch of the entire data loading and training pipeline of SAM in $\\mathrm{\\Delta\\muP^{2}}$ on a single NVIDIA A10G GPU. "], "page_idx": 51}, {"type": "text", "text": "H Supplemental experiments ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "This section provides more extensive empirical evaluations to validate the claims of the main paper. By naive perturbation scaling (naive) we denote parameterizations that do not adapt any perturbation scalings $;d=d_{l}=0$ for all $l^{\\prime}$ ). Global perturbation scaling (global) denotes the maximal stable scaling $n^{-d}$ of the global perturbation radius that achieves effective perturbations in some layers without layerwise perturbation scaling ( $d_{l}=0$ for all $l^{'}$ ). ", "page_idx": 52}, {"type": "text", "text": "H.1 SAM is approximately LL-SAM in $\\mu\\mathbf{P}$ with global perturbation scaling ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Figure H.1 compares SAM in $\\mu\\mathrm{P}$ under global perturbation scaling ( $\\textstyle{\\left.\\mu\\mathbf{P}\\right.}$ -global) with SAM under global perturbation scaling where only the last-layer weights are perturbed (LL-SAM) by showing more neural network statistics that are related to SAM\u2019s inductive bias and to learning in general. From top-left to bottom right, the statistics are: Frobenius norm of the layerwise weight perturbation (which is closely related to spectral norm as perturbations are low rank); Frobenius norm of the layerwise weight perturbation normalized by the weight spectral norm to upper bound the influence of the perturbations on the output; spectral norm of the weight updates across training scaled by the spectral condition $n^{1/2}$ , 1 and $\\bar{n^{-1/2}}$ for input, hidden and output layers respectively; norm of the activation updates for each layer normalized by the square root of the layer\u2019s output dimension to measure coordinatewise update scaling; layerwise effective feature ranks measured as in Andriushchenko et al. (2023a) by the minimal amount of singular values to make up $99\\%$ of the variance of the activations in a given layer; gradient norm, Hessian spectral norm and Hessian trace of loss with respect to weights; training accuracy, test accuracy after optimally stopping. ", "page_idx": 52}, {"type": "text", "text": "Observe that, especially for large widths, global perturbation scaling effectively only perturbs the last layer, as predicted by Theorem 11. Last-layer SAM is more similar to $\\mu\\mathrm{P}$ -global SAM than SGD on all of the tracked statistics, in particular at large widths. Only perturbing the last layer still affects the gradients in earlier layers so that weight updates and activations change in all layers. We find that SAM in $\\mu\\mathrm{P}$ with global scaling does not consistently improve generalization performance over SGD, whereas $\\mu\\mathrm{P^{2}}$ does improve over SGD for all widths (Figure H.3). Last-layer perturbation norms coincide by design with the global perturbation radius $n^{-\\bar{d}}\\rho$ and their effect on the activations stays $\\Theta(1)$ with increasing width as measured in relation to weight spectral norm. Formally the last-layer perturbation norm converges due to ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\|\\tilde{W}^{L+1}-W^{L+1}\\|_{F}=n^{-d}\\rho\\|\\frac{\\chi_{t}x_{t}^{L}}{\\|v_{t}\\|}\\|_{F}\\to n^{-d}\\rho\\|\\frac{x_{t}^{L}}{\\|x_{t}^{L}\\|}\\|_{F}=n^{-d}\\rho\\to0,\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the loss derivative $\\chi_{t}$ always cancels out due to the normalization and the global gradient norm $\\|v_{t}\\|$ is dominated by the last-layer gradient norm due to the global scaling (Theorem 11). Normalizing the weight perturbations by the weight spectral norm measures the influence of the perturbations on the activations. Note that this influence is also vanishing. Feature ranks stay close to initialization, since random initialization has high rank and training does low effective rank updates. Here we do not observe that SAM reduces the feature rank compared to SGD. The Hessian spectral norm and trace are quite noisy. The last-layer Hessian spectral norm explodes with width in $\\mu\\mathrm{P},$ because last-layer learning rate is scaled as $n^{-1}$ , hence the edge of stability explodes. ResNets in $\\mu\\mathrm{P}$ are more stable, their Hessian spectral norm even shrinks with width (not shown). ", "page_idx": 52}, {"type": "text", "text": "Contrast the results for $\\mu\\mathrm{P}$ -global with the results for $\\mu\\mathrm{P^{2}}$ in Figure H.2 for a comparison with SGD in $\\mu\\mathrm{P}.$ The Hessian spectral norm is reduced by SAM as you would expect. Additionally $\\mu\\mathrm{P^{2}}$ shows low variability in performance and all other statistics. SAM in $\\mu\\mathrm{P^{2}}$ does not reduce the feature rank compared to SGD in $\\mu\\mathrm{P}.$ This suggests that the conclusions drawn by Andriushchenko et al. (2023a) do not apply to MLPs in $\\mu\\mathrm{P}.$ . ", "page_idx": 52}, {"type": "text", "text": "H.2 Propagating perturbations from the first layer does not inherit SAM\u2019s benefits ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Here we apply a parametrization that only effectively perturbs the first layer weights (derived in Example F.1). Figure H.2 shows that effective first-layer SAM loses both $\\mu\\dot{\\mathbf{P}}^{2}$ SAM\u2019s improvement in test accuracy as well as SAM\u2019s inductive bias towards smaller gradient norm and Hessian norm, i.e. lower sharpness in MLPs. This performance deterioration occurs although the perturbation of first-layer SAM has an effect of the same order of magnitude as $\\mu\\mathrm{P^{2}}$ on weight and activation updates in all layers. This shows that mere propagation of weight perturbations from earlier layers cannot replace effective weight perturbations in each layer in order to benefit from SAM. It is crucial to correctly adjust the layerwise perturbation scaling, and to distinguish between effective perturbations and perturbation nontriviality in each layer. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "SAM in $\\mu\\mathrm{P^{2}}$ , on the other hand, achieves the correct perturbation and update scaling, has lower final gradient and Hessian spectral norm, improves test accuracy over SGD and has overall lower variance between training runs. ", "page_idx": 53}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/dd90289639fb23cce3202e5f9f5b942763e61d71b3010b295a7e6d0dd57718df.jpg", "img_caption": ["Figure H.1: Several neural network statistics for SAM (blue), LL-SAM (green) and SGD as a baseline (orange) across width after training a 3-layer MLP in $\\mu\\mathrm{P}$ -global for 20 epochs with the optimal learning rate 0.3432 and perturbation radius 0.2154. The statistics are explained in the text of Appendix H.1. "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/421c0cfb42bbb3beb1c38138d0e4874d9228559a48b860cea58fd2dc7a6b6d67.jpg", "img_caption": ["Figure H.2: Same neural network statistics as in Figure H.1 but SAM-SGD in $\\mu\\mathrm{P^{2}}$ (blue) versus MUP with perturbations scaled to only effectively perturb the first layer weights (green) with SGD in $\\mu\\mathrm{P}$ as a baseline. The first-layer perturbation parameterization performs worse than $\\mu\\mathrm{P^{2}}$ and results in gradient norm and Hessian norm similar to that of SGD, larger than those of SAM. While the spectral norm of the weights converges to a similar quantity as for $\\bar{\\mu}\\mathrm{P^{2}}$ , the effect of the weight changes on the hidden activation updates behaves more like SGD. Feature ranks all look similar. "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "H.3 Hyperparameter transfer ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "In this section, we provide supplemental evidence that $\\mu\\mathrm{P^{2}}$ is the unique parameterization that robustly achieves hyperparameter transfer both for the optimal learning rate and the optimal perturbation radius across neural architectures and datasets. ", "page_idx": 56}, {"type": "text", "text": "H.3.1 MLPs ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Figure H.3 shows that in $\\mu\\mathrm{P^{2}}$ the optimal hyperparameters in terms of test accuracy transfer in both learning rate and perturbation radius at sufficient width. In $\\mu\\mathrm{P^{2}}$ , SAM improves over SGD more than in SP, and the overall best test accuracy is achieved with the widest MLPs in $\\mu\\mathrm{P^{2}}$ . ", "page_idx": 56}, {"type": "text", "text": "While other works focus on hyperparameter transfer in training loss, we are ultimately interested in transfer with respect to test accuracy. Especially under harmful overfitting, the test accuracy is affected by nontrivial interactions between the learning rate and the perturbation radius. While the joint optimum is slightly shifting towards larger learning rate and perturbation radius for small widths, it remains remarkably stable for sufficient width $\\geq1024$ . Note that slight shifts in the optimal learning rate due to finite width biases have also been observed in earlier works (Yang et al., 2022). ", "page_idx": 56}, {"type": "text", "text": "Figure H.4 shows that global perturbation scaling does transfer the same perturbation instability threshold, whereas in $\\mu\\mathrm{P}$ -naive every fixed perturbation radius becomes unstable at sufficient width (Figure H.7). But in $\\mu\\mathrm{P}$ -global we do not observe a benefit of SAM over SGD. While the optimal learning rate with respect to the training accuracy transfers, the optimal learning rate with respect to the validation error is smaller for MLPs of moderate widths due to harmful overfitting. How to control for nonmonotonic dependence of the test error on the training error is an important question for future work. Figure H.5 also shows $\\mu\\mathrm{P}$ -global but with a different choice of input and output multipliers. With these multipliers, networks with width at most 256 perform better in terms of test accuracy than with the other multiplier choice in Figure H.4, but these multipliers have worse width scaling properties. To the best of our knowledge, the issue that optimally tuned hyperparameters on small models may scale worse than slightly suboptimal hyperparameters has not been stated before. This raises the question when and how can we use small models to predict the optimal choice of all hyperparameters jointly in large models. ", "page_idx": 56}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/8e10d129992b34a1652c4088bff7e4ddb09742874e5ba10a121121d3fee028b1.jpg", "img_caption": ["Figure H.4: Training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch SAM training as a function of learning rate (left) and perturbation radius (right) in $\\mu\\mathrm{P}$ -global with the same base learning rate and perturbation radius as in Figure H.9. For global perturbation scaling, we do not observe a benefit of SAM over SGD. "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/0bb85adde1f9bce29284f723870b352712b37b1ca9ad050da8455410c395564b.jpg", "img_caption": ["Figure H.3: Training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch SAM training as a function of learning rate (left) with perturbation radius $\\rho\\,=\\,0.2154$ , and as a function of perturbation radius (right) with learning rate $\\eta=0.4529$ in $\\mu\\mathrm{P^{2}}$ . The optimal learning rate transfers. The smaller the perturbation radius the better the training accuracy. For sufficiently wide MLPs, the validation-optimal perturbation radius transfers as well and SAM reduces harmful overfitting. "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/904adb82bcfcf8156a1399f91feaaef085492eda7244a671e929fadc48596324.jpg", "img_caption": ["Figure H.5: Same as Figure H.4 but with input multiplier 0.0305 and small output multiplier 0.0098. Note that networks with width at most 256 perform better in terms of test accuracy than with the other multiplier choice in Figure H.4, but the multipliers here have worse width scaling properties. To the best of our knowledge, the issue that optimally tuned hyperparameters on small models may scale worse than slightly suboptimal hyperparameters has not been stated before. This raises the question when and how can we use small models to predict the optimal hyperparameters of large models. "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/617ae0943ddbd369d5dfd434476d18da74bca3b43e2be0aef0998dbe2259b281.jpg", "img_caption": ["Figure H.6: Mean (over 3 runs) of training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch SAM training of a MLP in SP-naive as a function of learning rate and perturbation radius. Neither the optimal learning rate nor the optimal perturbation radius transfers. Every fixed learning rate becomes unstable in sufficiently wide networks. Optimal training and test accuracy are reached on differing hyperparameters due to harmful overfitting in SGD. "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/5b1f479d1782d9addd914c0fc637070d1229a22e34588a47511903ca727ffc71.jpg", "img_caption": ["Figure H.7: Mean (over 3 runs) of training accuracy (top) and of test accuracy (bottom) after optimally stopping 20 epoch SAM training of a MLP in $\\mu\\mathrm{P}$ -naive as a function of learning rate and perturbation radius. The optimal hyperparameters do not transfer. Every fixed perturbation radius becomes unstable in sufficiently wide networks. "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/4c0260d9e3efb4ba8d530e9cc3e8ed12c774cddf8aaf9c9ec6c5991a5826e2ae.jpg", "img_caption": ["Figure H.8: Mean (over 3 runs) of training accuracy (top) and of test accuracy (bottom) after optimally stopping 20 epoch SAM training of a MLP in $\\mu\\mathrm{P}$ -global as a function of learning rate and perturbation radius. The global scaling of the perturbation radius by $n^{-1/2}$ compared to $\\mu\\mathrm{P}$ -naive (Figure H.7) makes the stable regime invariant to width. But the suboptimal layerwise perturbation scaling that only perturbs the last layer does not consistently improve over SGD $(\\rho=0,$ ). "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/3cd2f6190d7af731c427f5095af18a35aee1fb8fff9ad1d9926bbdcd24e7b309.jpg", "img_caption": ["Figure H.9: Mean (over 3 runs) of training accuracy (top) and of test accuracy (bottom) after optimally stopping 20 epoch SAM training of a MLP in $\\mu\\mathrm{P^{2}}$ as a function of learning rate and perturbation radius. At sufficient width, the optimal hyperparameters are stable in terms of test accuracy, even under severe overfitting. "], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "In this section, we plot averages and $\\sigma$ -CI from 2 independent runs. ", "page_idx": 59}, {"type": "text", "text": "ResNets in $\\mu\\mathrm{P^{2}}$ transfer both the optimal learning rate and perturbation radius for SAM (Figure H.11), SAM-ON (Figure H.13) and elementwise ASAM (Figure H.14), as well as different alternatives of scaling the gradient norm contributions to SAM\u2019s denominator (Figure H.17). This suggests correctness of the derived scalings. At width multipliers 2 and 4, $\\mu\\mathrm{P^{2}}$ achieves the same or slightly better test accuracy than SP in all SAM variants. ", "page_idx": 59}, {"type": "text", "text": "Figure H.12 shows ResNets trained with SAM in different parameterizations. In ResNets of practical scale, $\\rho$ remains quite stable in $\\mu\\mathrm{P^{2}}$ but surprisingly also in SP-NAIVE. In $\\mu\\mathrm{P}_{i}$ for naive perturbation scaling the regime of stable perturbation radii shrinks, for global perturbation scaling, the optimal perturbation radius shifts, approaching its maximal stable value, which stays invariant to width scaling. Here, it would be interesting to see whether even larger width would lead to suboptimal performance of $\\mu\\mathrm{P}$ -global. $\\mu\\mathrm{P^{2}}$ is most robust to the choice of $\\rho$ and achieves the best test accuracy. Surprisingly, ResNets in SP have very stable hyperparameter transfer across most SAM variants too, as soon as we tune momentum, weight decay and labelsmoothing (Figure H.10). This is in line with previous empirical observations (Figure 16, Yang et al., 2022) but contradicts pure infinite-width theory. Because we are training to convergence, pure infinite-width theory does not adequately describe the training dynamics anymore (Vyas et al., 2024). We plan to study this phenomenon in more detail in upcoming work. The infinite-width theory implies that scaling the width further would eventually break the learning rate transfer. ", "page_idx": 59}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/d8f1ac513e68e90b63dfc47657caf6e7eeff9730ad812991227a5276199a658d.jpg", "img_caption": ["(a) No momentum, weight decay or labelsmoothing "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/b0feacedb22d4909b4f4913c364fa2fe901ffafe2d76a00691b2ee238a68a19e.jpg", "img_caption": ["(b) Tuned momentum and weight decay "], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "Figure H.10: Training accuracy (top) and test accuracy (bottom) after optimally stopping 100 epoch SAM training as a function of learning rate and perturbation radius in SP-naive without regularization (left) and with tuned regularization (right) using momentum 0.9, weightdecay 0.0005 and labelsmoothing 0.1. CI denote the minimal and maximal value from 4 independent runs. Without regularization, the optimal learning rate shrinks with width. Given the learning rate, the optimal perturbation radius seems quite stable, but since the optimal learning rate shifts, the performance scales worse than for $\\mu\\mathrm{P^{2}}$ with the fixed learning rate that is tuned on the small model. With optimal regularization, both optimal learning rate and perturbation radius remain remarkably stable. We plan to investigate this mechanism in an upcoming work. ", "page_idx": 60}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/0b0f193988f5d7ec87804d138778242d0f806509f32cb8b43438e8918e88d934.jpg", "img_caption": [], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "Figure H.11: Training accuracy (top) and test accuracy (bottom) after optimally stopping 200 epoch SAM training as a function of learning rate and of perturbation radius in SP (left) and in $\\mu\\mathrm{P^{2}}$ (right) with optimized momentum 0.9, weight decay $5\\cdot1\\bar{0}^{-4}$ and labelsmoothing 0.1 for both $\\mu\\mathrm{P^{2}}$ and SP. In $\\mu\\mathrm{P^{2}}$ , the base learning rate is $\\eta=\\bar{2}^{-4}$ and the base perturbation radius is $\\rho=2^{-4}$ , in SP $\\textit{\\eta}=0.05$ and $\\rho=0.1$ , respectively. Observe monotonic improvement with width in both training and test error. Optimal hyperparameters transfer across widths, surprisingly in both $\\mu\\mathrm{P^{2}}$ and SP. ", "page_idx": 60}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/d91a19e3763d89e91eea662a304e91f2b0f44a803548b2bf7eba4897e1bfd188.jpg", "img_caption": ["Figure H.12: Test accuracy after optimally stopping 200 epoch SAM training as a function of perturbation radius in various parameterizations. Dashed lines denote the base optimizer SGD with tuned momentum and weight decay in the respective parameterization. "], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "H.3.3 ASAM variants ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "As we are not aware of any use of ASAM with MLPs in the literature and since the amount of necessary experiments for ViTs exceeds our computational budget, we only show that ResNets trained with the all of the discussed SAM variants in $\\mu\\mathrm{P^{2}}$ transfer the optimal $(\\eta,\\rho)$ . ", "page_idx": 61}, {"type": "text", "text": "For the examples of elementwise ASAM and SAM-ON the global perturbation scaling $n^{1/2}$ suffices to reach $\\mu\\mathrm{P^{2}}$ . The stability of the optimal perturbation radius in the applied scaling $n^{1/2}$ shows that in $\\mu\\mathrm{P}$ with naive perturbation scaling the optimal perturbation radius would grow as $n^{1/2}$ . ", "page_idx": 61}, {"type": "text", "text": "See the previous section, for a discussion of the remarkable stability of ResNets in SP. For the example of elementwise ASAM in SP, the optimal perturbation radius seems to grow. ", "page_idx": 61}, {"type": "text", "text": "For layerwise ASAM (Figure H.15), the optimal perturbation radius seems to grow in both SP and $\\mu\\dot{\\mathbf{P}}^{2}$ , suggesting that our scaling condition does not perfectly apply to this variant, although $\\mu\\mathrm{P^{2}}$ $97.09_{\\pm0.03}(+0.83))$ still outperforms SP $(96.86{\\scriptstyle\\pm0.05}(+0.83))$ in terms of the optimal test accuracy. As Frobenius norms of weights are the only component that is not representable as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program, these Frobenius norms appear to scale differently than heuristically predicted over the course of training. ", "page_idx": 61}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/831be1d8179d63fc8a9667a6eb76ca33b01cbf20b4eb2e9d5cf925d23185e9ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "Figure H.13: Same as Figure H.11 but for SAM-ON in SP without perturbation scaling (left) and in $\\mu\\bar{\\mathbf{P}}^{2}$ (right). Both optimal learning rate and perturbation radius are remarkably stable in both $\\mu\\mathrm{P^{2}}$ and SP. Since $\\mu\\mathrm{P^{2}}$ for SAM-ON is just $\\mu\\mathrm{P}$ with global perturbation scaling $n^{1/2}$ , transfer here implies that $\\mu\\mathrm{P}$ with width-independent scaling would not transfer. ", "page_idx": 61}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/d81f4cade18b518c50899630ca1763f906d4e6b9ddcb119cefdc3f1d3e8251ea.jpg", "img_caption": ["Figure H.14: Same as Figure H.11 but for elementwise ASAM in SP without perturbation scaling (left) and in $\\mu\\mathrm{P^{2}}$ (right). Observe a consistent HP landscape in $\\mu\\mathrm{P^{2}}$ but growing optimal perturbation radius in SP without perturbation scaling. "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/20db2827816745214b95fbbd41381badff46c7b27a766c2c871c1b48960aded5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 62}, {"type": "text", "text": "Figure H.15: Same as Figure H.11 but for layerwise ASAM in SP without perturbation scaling (left) and in $\\mu\\mathrm{P^{2}}$ (right). For layerwise ASAM, both $\\mu\\mathrm{P^{2}}$ and SP seem to transfer the optimal learning rate as well as perturbation radius. ", "page_idx": 62}, {"type": "text", "text": "H.4 Gradient norm contributions have negligible effects on generalization performance ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "In this section we provide ablations concerning the question which layers should contribute nonvanishingly to the gradient norm in the denominator of the layerwise SAM perturbation rule (LP). ", "page_idx": 62}, {"type": "text", "text": "For MLPs, in Figure H.16 we scale all contributions to $\\Theta(1)$ , and then set the contribution of individual layers to zero, one by one. We observe no significant effect on the optimal test loss or hyperparameter transfer for MLPs. Any layer\u2019s contribution to the gradient normalization in the denominator of the SAM update rule can be set to 0 without a significant effect on the test loss. This raises the question which effect the gradient normalization has in $\\mu\\mathrm{P}.$ Does it contribute a scaling correction in SP, but may be dropped entirely in $\\mu\\mathrm{P}$ ? ", "page_idx": 62}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/9a96c31a880ae8cc9a9275b7f5cfd3f00e02e43eb2723d792f6dfa41b49ece97.jpg", "img_caption": ["Figure H.16: Scaling the gradient norm contributions of all layers to $\\Theta(1)$ (first row) and then setting the first layer gradient norm to 0 (2nd row), respectively the hidden layer (3rd row), last-layer (4th row). Each individual layer seems to have vanishing contribution to the optimal test error. "], "img_footnote": [], "page_idx": 62}, {"type": "text", "text": "For ResNets, Figure H.17(a) shows accuracies when rescaling all layers\u2019 gradient norms to $\\Theta(1)$ , and Figure H.17(b) shows the results when using the original global gradient norm rescaled to $\\Theta(1)$ . Again, both methods achieve similar optimal test accuracy. The first variant shows cleaner hyperparameter transfer and monotonous improvement with width. When comparing to our original definition (LP) in Figure H.11, optimal performance is similar but rescaling all layers\u2019 gradient norm contributions to $\\Theta(1)$ may even produce a slightly more stable hyperparameter-loss landscape for ResNets. ", "page_idx": 62}, {"type": "text", "text": "", "page_idx": 63}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/85ebac069838ecec6ca04e27ae6137e20bd53697c7b567578fbbd79c646abfe0.jpg", "img_caption": ["(a) Rescaling all of SAM\u2019s denominator terms to $\\Theta(1)$ ", "(b) Global $\\|\\nabla L\\|$ scaling "], "img_footnote": [], "page_idx": 63}, {"type": "text", "text": "Figure H.17: Same as Figure H.11 but with scaling of the gradient norms in the SAM perturbation (LP) denominator that scales all terms to $\\Theta(1)$ (left) and only global denominator scaling $\\frac{\\|\\nabla L\\|}{n_{L}}$ (right). All denominator scalings achieve similar optimal accuracy, show HP transfer in learning rate and monotonic test accuracy improvement with width. In global denominator scaling, the optimal $\\rho$ shifts with width. ", "page_idx": 63}, {"type": "text", "text": "H.5 SAM with layerwise gradient normalization ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Here we consider SAM without the gradient normalization over all layers jointly. Instead we apply the layerwise perturbation rule presented in Appendix F.7, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon_{t}^{l}=\\rho_{l}\\cdot\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})/\\|\\nabla_{W^{l}}\\mathcal{L}(f(\\xi_{t};W_{t}),y_{t})\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "In SP, we consider a global constant $\\rho_{l}=\\rho$ , whereas for $\\mu\\mathrm{P^{2}}$ the spectral condition $(*)$ requires $\\rho_{l}=\\rho\\cdot\\sqrt{\\mathbf{f}\\mathbf{a}\\mathbf{n}_{-}\\mathsf{o u t}/\\mathtt{f}\\mathbf{a}\\mathbf{n}_{-}\\mathrm{in}}.$ . ", "page_idx": 63}, {"type": "text", "text": "Overall, SAM without layer coupling performs decently, but is outperformed by the original SAM in particular in ResNets, in $\\bar{\\mu}\\mathrm{P^{2}}$ and at large width. But note that for ResNets we adopt the hyperparameters tuned for the original SAM with layer coupling, so that these ablations only serve as preliminary experiments. ", "page_idx": 63}, {"type": "text", "text": "MLPs. In SP, we observe very similar performance as under the original SAM with layer coupling (Figure H.6). This may be because the last layer dominates the perturbation in both versions of SAM. ", "page_idx": 63}, {"type": "text", "text": "SAM without layer coupling achieves similar optimal generalization in $\\mu\\mathrm{P^{2}}$ at each width compared to Figure H.9. The regime of stable $(\\eta,\\rho)$ stays width-independent, but does not transfer the optimum consistently. This suggests complex or noisy dependence of the training dynamics on $\\rho$ . ", "page_idx": 63}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/fc121bf222e00fab5f737570163a45c24afea66e77bcbc3b6f64d4ca6b1ba358.jpg", "img_caption": ["Figure H.18: (SAM with layerwise normalization in MLPs) Test accuracy as a function of learning rate $\\eta$ and perturbation radius $\\rho$ for an optimally-stopped MLP trained with SAM with layerwise normalization. "], "img_footnote": [], "page_idx": 63}, {"type": "text", "text": "ResNets. Figure H.19 shows that decoupled SAM has decent performance, but is worse than original SAM with global normalization (Figure H.12) in both SP and $\\mu\\mathrm{P^{2}}$ , in particular at large width. As expected, $\\rho$ transfers in $\\mu\\mathrm{P^{2}}$ . ", "page_idx": 64}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/7bd1b695654c8cc0ebb12ecd3bc95df7e24a17e389f956894b8ae7c4628c965e.jpg", "img_caption": ["Figure H.19: (SAM with layerwise normalization in ResNets) Test accuracy as a function of perturbation radius $\\rho$ for ResNets trained with SAM with layerwise normalization. "], "img_footnote": [], "page_idx": 64}, {"type": "text", "text": "H.6 Test error over the course of training ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Figure H.20 shows the test error of ResNets and ViTs over the course of training. $\\mu\\mathrm{P^{2}}$ always achieves the best final test accuracy. In ResNets it also achieves a decent test accuracy the fastest and removes training instabilities of SAM in SP. While SGD in $\\mu\\mathrm{P}$ alone cannot compete with SAM in SP, SAM in $\\mu\\mathrm{P^{2}}$ uniformly dominates over the entire course of training. Our theory suggests that in $\\mu\\mathrm{P^{2}}$ the gradients are scaled correctly from the beginning, whereas in SP they have to self-stabilize first, which slows down convergence. We plan a closer analysis in an upcoming work. ", "page_idx": 64}, {"type": "text", "text": "In ViTs, $\\mu\\mathrm{P}$ generally achieves decent accuracy faster than SP, since gradient norms are already scaled correctly at initialization. SAM converges slower than the base optimizer AdamW in favor of drifting towards a better generalizing local minimum or saddle point. For ViTs at this moderate scale, SAM in SP catches up to SAM in $\\bar{\\mu}\\mathrm{P^{2}}$ at the end of training. ", "page_idx": 64}, {"type": "image", "img_path": "pR5g1bBqoV/tmp/42dda4bcadfc4fda16a9173ba665cfa526844c22f51089d20deea524ce183a18.jpg", "img_caption": ["Figure H.20: Training a ResNet-18 with width multiplier 2 on CIFAR10 (left) and a ViT with width multiplier 2 on CIFAR100 (right). SGD and AdamW are the respective base optimizers. "], "img_footnote": [], "page_idx": 64}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: In the abstract and introduction we state our main contributions while acknowledging related work. All main claims are theoretically proven and/or empirically verified. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 65}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: Limitations are discussed in the future work section as well as in the section in the appendix that is related to the respective limitation. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 65}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 65}, {"type": "text", "text": "Justification: We state all assumptions in the main paper and Appendix C, and provide all formal proofs in Appendix E. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 65}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 66}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: All experimental details are disclosed in Appendix G. Our perturbation scaling rules are clearly stated in the main paper. Their implementation with flexible fan_in and fan_out is explained in Appendix F.7, together with pseudocode for implementing our proposed scaling rule. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 66}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 66}, {"type": "text", "text": "Answer: [No] ", "page_idx": 66}, {"type": "text", "text": "Justification: We only propose width-dependent scaling of hyperparameters of an existing optimization algorithm. This can be easily implemented by following the scaling rules that we clearly specify in the main paper. In Appendix F.7 we even provide a code example that ", "page_idx": 66}, {"type": "text", "text": "contains the essential modifications.We are working on making Python code to reproduce all of our experiments publicly available.   \nGuidelines: ", "page_idx": 67}, {"type": "text", "text": "", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 67}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes]   \nJustification: All experimental details are disclosed in the main paper or Appendix G. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 67}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: As stated in Appendix G, we repeat all main experiments with multiple independent runs and report confidence bands within the empirical $\\bar{2}.5\\%$ - to $97.5\\%$ -quantiles. When we repeat experiments on Vision Transformers that we have also conducted on MLPs or ResNets, we do not use multiple runs due to limitations in computational resources. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 67}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 68}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 68}, {"type": "text", "text": "Justification: We provide the type of GPU used and number of GPU seconds required for each experiment in Appendix G.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 68}, {"type": "text", "text": "", "page_idx": 68}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: We provide a theoretical analysis of a widely used optimization algorithm, point out the algorithm\u2019s limitations in large models and propose a correction. We do not foresee any ethical concerns.   \nGuidelines: ", "page_idx": 68}, {"type": "text", "text": "", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 68}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 68}, {"type": "text", "text": "Justification: This paper provides fundamental research toward understanding and improving existing optimization algorithms for neural networks. We do not release any model or data and do not consider generative models. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the ", "page_idx": 68}, {"type": "text", "text": "technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 69}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 69}, {"type": "text", "text": "Justification: We only use standard vision architectures and vision datasets in our experiments and do not release any data or models.   \nGuidelines: ", "page_idx": 69}, {"type": "text", "text": "", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 69}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Justification: We cite the standard CIFAR10, CIFAR100 (Krizhevsky et al., 2009) and ImageNet1K (Deng et al., 2009) datasets following the standard practice. We also cite the Python assets PyTorch (Paszke et al., 2019), mup (Yang et al., 2022) and the GitHub repository implementing SAM (Samuel, 2022) that we use as a basis for our experiments. Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 69}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not release new assets.   \nGuidelines: \u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 69}, {"type": "text", "text": "", "page_idx": 70}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 70}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 70}, {"type": "text", "text": "ustification: The paper does not involve crowdsourcing nor research with human subjects.   \nuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 70}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 70}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 70}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 70}]