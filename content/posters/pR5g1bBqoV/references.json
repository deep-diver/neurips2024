{"references": [{"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2021-00-00", "reason": "This paper introduces Sharpness-Aware Minimization (SAM), a technique that is the central focus of the current paper's analysis and improvements."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tensor programs iv: Feature learning in infinite-width neural networks", "publication_date": "2021-00-00", "reason": "This paper's theoretical framework on infinite-width neural networks is leveraged to analyze SAM's behavior and limitations in the current paper."}, {"fullname_first_author": "Maksym Andriushchenko", "paper_title": "Towards understanding sharpness-aware minimization", "publication_date": "2022-00-00", "reason": "This paper provides a crucial analysis and discussion of SAM, which is directly referenced and built upon in the current research."}, {"fullname_first_author": "Maximilian M\u00fcller", "paper_title": "Normalization layers are all that sharpness-aware minimization needs", "publication_date": "2024-00-00", "reason": "This paper presents a significant empirical study on SAM variants that motivates a key aspect of the current paper's investigation:  comparing various SAM variants under different parameterizations."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tensor programs iii: Neural matrix laws", "publication_date": "2021-00-00", "reason": "This paper provides another layer of the theoretical foundation for the current paper's analysis of neural network scaling behavior."}]}