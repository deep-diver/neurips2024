[{"type": "text", "text": "Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andreas Schlaginhaufen Maryam Kamgarpour SYCAMORE, EPFL SYCAMORE, EPFL andreas.schlaginhaufen@epfl.ch maryam.kamgarpour@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert\u2019s true reward. Past work has addressed this problem only under the assumption of full access to the expert\u2019s policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert\u2019s policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has achieved remarkable success in various domains such as robotics [Hwangbo et al., 2019], autonomous driving [Lu et al., 2023], or fine-tuning of large language models [Stiennon et al., 2020]. Despite these advances, a key challenge lies in designing appropriate reward functions that reflect the desired outcomes and align with human values. Misaligned rewards can lead to suboptimal behaviors [Ngo et al., 2022], undermining the potential benefits of RL in practical scenarios. Inverse reinforcement learning (IRL), also known as inverse optimal control [Kalman, 1964] or structural estimation [Rust, 1994], addresses this problem by inferring a reward from demonstrations of an expert acting optimally in a Markov decision process (MDP). ", "page_idx": 0}, {"type": "text", "text": "Compared to behavioral cloning [Pomerleau, 1988], which directly fits a policy to the expert\u2019s demonstrations, IRL is believed to provide a more transferable description of the expert\u2019s task $[\\mathrm{Ng}$ et al., 2000], as recovering the expert\u2019s underlying reward would enable us to train a policy in a new environment with different dynamics. However, it is also known that the reward corresponding to some optimal policy is not unique [Ng et al., 1999], making it difficult to recover the expert\u2019s true underlying reward. This raises the question: Is a reward recovered via IRL transferable to a new environment in the sense that its optimal policy aligns with the expert\u2019s true reward? For example, in autonomous driving, could we effectively reuse a reward learned from demonstrations of one car in a given city to train or fine-tune a policy for another car in another city? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Ensuring transferability is challenging, as neither the optimal policy corresponding to a reward nor the reward corresponding to an optimal policy is unique. This leads to trivial solutions to the IRL problem, such as constant rewards that make all policies optimal. Common approaches to address this challenge include characterizing the entire set of rewards for which the expert is optimal [Metelli et al., 2021], or assuming the expert is optimal with respect to an entropy regularized RL problem [Ziebart, 2010], leading to many popular IRL and imitation learning algorithms [Ho and Ermon, 2016, Fu et al., 2017, Garg et al., 2021]. Entropy regularization results in a unique and more uniform optimal policy, serving as a model for the expert\u2019s bounded rationality [Ortega et al., 2015]. ", "page_idx": 1}, {"type": "text", "text": "In the entropy-regularized setting, several recent works study the set of rewards for which a given expert policy is optimal. In particular, Cao et al. [2021], Skalse et al. [2023] show that under entropy regularization, the expert\u2019s reward can be identified up to so-called potential shaping transformations $[\\mathrm{Ng}$ et al., 1999]. The authors of [Schlaginhaufen and Kamgarpour, 2023] extend this result to more general steep regularization. Furthermore, they show that to guarantee transferability to any transition law, the expert\u2019s reward needs to be identified up to a constant. The latter can be achieved either by restricting the reward class, e.g., to state-only rewards [Amin et al., 2017], or by learning from multiple experts with the same reward but different transition laws, given that a specific rank condition is satisfied [Cao et al., 2021, Rolland et al., 2022]. However, the above results cannot be applied directly in practice, as they rely on having full access to the experts\u2019 policies, whereas in practice, we typically only have a finite set of demonstrations available. ", "page_idx": 1}, {"type": "text", "text": "Contributions We consider the framework of regularized IRL [Jeon et al., 2021] and address the transferability of rewards recovered from a finite set of expert demonstrations. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We define a novel notion of transferability (Definition 3.1), to address the practical limitation of not having perfect access to the experts\u2019 policies. Furthermore, we show that when learning from finite data, the conditions developed under full access to the experts\u2019 policies are not sufficient to guarantee transferability (Example 3.3).   \n\u2022 Instead of a binary rank condition, we propose to use principal angles to characterize the similarity and dissimilarity between transition laws (Definition 3.8). Based on these principal angles, we then establish two key transferability results: 1) a guarantee for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws (Theorem 3.10), and 2) a guarantee for transferability to local changes in the transition law when learning from a single expert (Theorem 3.11).   \n\u2022 Assuming oracle access to a probably approximately correct (PAC) algorithm for the forward RL problem, we provide a PAC algorithm for the IRL problem, which in ${\\mathcal O}(K^{2}/\\hat{\\varepsilon}^{2})$ steps recovers a reward for which, with high probability, all $K$ experts are $\\hat{\\varepsilon}_{}$ -optimal (Theorem 4.1). Together with our results on transferability, this establishes end-to-end guarantees for learning transferable rewards from a finite set of expert demonstrations. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We experimentally validate our results in a gridworld environment (Section 5).1 ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation Given $x$ and $y$ in some Euclidean vector space $\\nu$ , we denote the $p$ -norm by $\\Vert{\\boldsymbol{x}}\\Vert_{p}$ , the orthogonal projection onto a closed convex set $\\mathcal{X}\\subset\\mathcal{V}$ by $\\Pi_{\\mathcal{X}}(x)=\\arg\\operatorname*{min}_{y\\in\\mathcal{X}}\\|x-y\\|_{2}$ , and the standard dot product by $\\langle x,y\\rangle$ . For a linear operator $A$ , we denote its image and rank by $\\operatorname{im}A$ and rank $A$ , respectively. Given two sets $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , we denote $\\mathcal{X}+\\mathcal{Y}$ for their Minkowski sum and $\\mathcal{V}^{\\mathcal{X}}$ for the set of all functions mapping from $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ . Additionally, we denote $\\Delta_{\\mathcal{X}}$ for the probability simplex over $\\mathcal{X}$ and $\\mathbb{1}$ for the indicator function. The interior int $\\mathcal{X}$ , the relative interior relint $\\mathcal{X}$ , the relative boundary relbd $\\mathcal{X}$ , and the convex hull conv $\\mathcal{X}$ of some set $\\mathcal{X}$ are defined in Appendix A, along with an overview of all other notations. ", "page_idx": 1}, {"type": "text", "text": "Regularized MDPs We consider a regularized MDP [Geist et al., 2019] defined by a tuple $(S,A,P,\\nu_{0},r,\\gamma,h)$ . Here, $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ represent finite state and action spaces with $|{\\cal S}|,\\dot{|{\\cal A}|}>1$ , $\\nu_{0}\\,\\in\\,\\Delta_{\\mathcal{S}}$ the initial state distribution, $P\\in\\Delta_{S}^{S\\times A}$ the transition law, $r\\,\\in\\,\\mathbb{R}^{S\\times A}$ the reward, and $\\gamma\\in(0,1)$ the discount factor. Furthermore, $h:\\mathcal{X}\\to\\mathbb{R}$ is a strictly convex regularizer that is defined on a closed convex set $\\mathcal{X}\\subseteq\\mathbb{R}^{A}$ with relint $\\Delta_{\\mathcal{A}}\\subseteq\\operatorname{int}\\mathcal{X}$ . Starting from some initial state $s_{0}\\sim\\nu_{0}$ the agent can at each step in time $t$ , choose an action $a_{t}\\in\\mathcal A$ , will arrive in state $s_{t+1}\\sim P(\\cdot|s_{t},a_{t})$ , and receives reward $r(s_{t},a_{t})$ . The goal is to find a Markov policy $\\pi\\in\\Delta_{\\mathcal{A}}^{\\mathcal{S}}$ maximizing the regularized objective $\\begin{array}{r}{\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left[r(s_{t},a_{t})-h\\left(\\pi(\\cdot|s_{t})\\right)\\right]\\right]}\\end{array}$ . Following the classical linear programming approach to MDPs [Puterman, 2014], this can be cast equivalently as the convex optimization problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathcal{M}}J(r,\\mu),\\quad\\mathrm{with}\\quad J(r,\\mu):=\\langle r,\\mu\\rangle-\\bar{h}(\\mu),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{M}$ denotes the set of occupancy measures, $\\begin{array}{r}{\\mu^{\\pi}(s,a):=(1\\!-\\!\\gamma)\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{1}(s_{t}=s,a_{t}=a)\\right]\\!,}\\end{array}$ , and we have $\\bar{h}(\\mu)\\,:=\\,\\mathbb{E}_{(s,a)\\sim\\mu}\\,[h(\\pi^{\\mu}(\\cdot|s))]$ , with $\\pi^{\\mu}$ being the policy corresponding to $\\mu$ (see Appendix A). The set of occupancy measures is characterized by the Bellman flow constraints ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{M}=\\big\\{\\mu\\in\\mathbb{R}_{+}^{S\\times A}:(E-\\gamma P)^{\\top}\\mu=(1-\\gamma)\\nu_{0}\\big\\}\\subseteq\\Delta_{S\\times A},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $E:\\mathbb{R}^{S}\\,\\rightarrow\\,\\mathbb{R}^{S\\times A}$ and $P:\\mathbb{R}^{S}\\,\\rightarrow\\,\\mathbb{R}^{S\\times A}$ are the linear operators mapping $\\boldsymbol{v}\\in\\mathbb{R}^{s}$ to $(E v)(s,a)=v(s)$ and $\\begin{array}{r}{(P v)(s,a)=\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)v(s^{\\prime})}\\end{array}$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Due to the strict convexity of $h$ , the regularized MDP problem has a unique optimal policy [Geist et al., 2019], hence guaranteeing the uniqueness of the optimal occupancy measure in (O-RL). In addition, we assume that the gradients of $h$ become unbounded towards the relative boundary of the simplex as detailed in Assumption 2.1 below. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1 (Steep regularization). Suppose that $h:\\mathcal{X}\\to\\mathbb{R}$ is differentiable in int $\\mathcal{X}$ and that $\\begin{array}{r}{\\operatorname*{lim}_{l\\to\\infty}\\|\\nabla h(p_{l})\\|_{2}=\\infty}\\end{array}$ if $(p_{l})_{l\\in\\mathbb{N}}$ is a sequence in int $\\mathcal{X}$ converging to a point $p\\in{\\mathrm{relbd}}\\,\\Delta_{A}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1 ensures that the optimal policy is non-vanishing, and together with Assumption 2.2 below, we also have that the optimal occupancy measure is non-vanishing. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.2 (Exploration). Let $\\begin{array}{r}{\\nu(s):=\\sum_{a}\\mu(s,a)\\geq\\nu_{\\operatorname*{min}}>0}\\end{array}$ for any $s\\in S$ and $\\mu\\in\\mathcal{M}$ . ", "page_idx": 2}, {"type": "text", "text": "One way to guarantee Assumption 2.2 is to impose a lower bound on the initial state distribution $\\nu_{0}$ . In the following, it will be convenient to denote the optimal solution to (O-RL) for the reward $r$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{R L}(r):=\\underset{\\mu\\in\\mathcal{M}}{\\mathrm{argmax}}\\,J(r,\\mu),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the suboptimality of some occupancy measure $\\mu$ for the reward $r$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(r,\\mu):=\\operatorname*{max}_{\\mu^{\\prime}\\in\\mathcal{M}}J(r,\\mu^{\\prime})-J(r,\\mu).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "That is, $\\mu=\\mathsf{R L}(r)$ if and only if $\\ell(r,\\mu)=0$ . ", "page_idx": 2}, {"type": "text", "text": "iRt ewmilalr ko f2t.e3n.  bAe su sweef uali tmo  teox apnliacliytlzye  sthpee ctirfayn stfheer adbeiplietny doefn rceyw oanr .t o Wnee wd ot rsaon sbityi oand dlianwgs $P\\in\\Delta_{S}^{S\\times A}$ $P$   \ne.g. we write $\\mathcal{M}_{P}$ , $\\mathsf{R L}_{P}$ , and $\\ell_{P}$ . However, for better readability, we drop these subscripts whenever   \nthere is no potential for confusion. ", "page_idx": 2}, {"type": "text", "text": "Inverse reinforcement learning Given a dataset of trajectories sampled from an expert $\\mu^{\\tt E}$ that is optimal for some reward $r^{\\mathsf{E}}$ , the goal in IRL is to recover a reward $\\hat{r}\\in\\mathcal{R}$ , within a predefined reward class $\\mathcal{R}\\subseteq\\mathbb{R}^{S\\times A}$ , such that the expert is optimal for $\\hat{r}$ . That is, ideally, we aim to find a reward in the feasible reward set ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{I R L}(\\mu^{\\mathsf{E}}):=\\left\\{r\\in\\mathcal{R}:\\mu^{\\mathsf{E}}\\in\\mathsf{R L}(r)\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, since we don\u2019t have direct access to the expert\u2019s policy but only to a finite set of demonstrations, the best we can hope for is an algorithm that with high probability outputs a reward $\\hat{r}\\in\\mathcal{R}$ such that $\\ell(\\hat{r},\\mu^{\\mathsf{E}})$ is small \u2013 i.e. an algorithm that is PAC [Syed and Schapire, 2007]. ", "page_idx": 2}, {"type": "text", "text": "Reward equivalence The reward corresponding to an optimal occupancy is not unique. For example, all rewards in the affine subspace $r+\\mathcal{U}$ , where ${\\bar{\\mathcal{U}}}:=\\operatorname{im}(E^{*}-\\gamma{\\bar{P}})$ is the subspace of so-called potential shaping transformations, correspond to the same optimal occupancy measure $[\\mathrm{Ng}$ et al., 1999]. From a geometric perspective, the subspace $\\mathcal{U}=\\mathrm{im}(E-\\gamma P)$ lies perpendicular to the set of occupancy measures $\\mathcal{M}$ . Therefore, adding an element of $\\boldsymbol{\\mathcal{U}}$ to the reward leaves the performance difference between any two occupancy measures invariant. Hence, it is often convenient to consider these rewards as equivalent [Kim et al., 2021] and to measure distances between rewards in the resulting quotient space. Given a linear subspace $\\mathcal{V}\\subset\\mathbb{R}^{S\\times A}$ , the quotient space $\\mathbb{R}^{S\\times A}/\\mathcal{V}$ is the set of all equivalence classes $[r]_{\\mathcal{V}}:=\\left\\{r^{\\prime}\\in\\mathbb{R}^{S\\times A}:r^{\\prime}-r\\in\\mathcal{V}\\right\\}$ , which is itself a vector space with addition and multiplication operation defined by $[r]\\nu+[r^{\\prime}]\\nu^{'}=[r+r^{\\prime}]\\nu$ and $c[r]_{\\mathcal{V}}=[c r]_{\\mathcal{V}}$ for $c\\in\\mathbb R$ . Intuitively, $\\mathring{\\mathbb{R}}^{S\\times A}/\\nu$ is the vector space obtained by collapsing $\\nu$ to zero, or in other words, it is isomorphic to the orthogonal complement of $\\mathcal{V}$ . We endow $\\mathbb{R}^{S\\breve{\\times}A}/\\nu$ with the quotient norm $\\begin{array}{r}{\\|[r]\\nu\\|_{2}:=\\operatorname*{min}_{v\\in\\mathcal{V}}\\|r+v\\|_{2}^{-}=\\|\\Pi_{\\mathcal{V}^{\\perp}}r\\|_{2}}\\end{array}$ and we say that $r$ and $r^{\\prime}$ are close in $\\mathbb{R}^{\\lambda\\times A}/\\nu$ if $\\big\\|[r]\\nu-[r^{\\bar{\\prime}}]\\nu\\big\\|_{2}$ is small. Moreover, the expert\u2019s reward is said to be identifiable up to some equivalence class $[\\cdot]_{\\nu}$ if $\\mathsf{I R L}(\\mu^{\\mathsf{E}})\\subseteq[r^{\\mathsf{E}}]\\nu$ . In this paper, we will consider the equivalence relations induced by constant shifts, i.e., $\\mathcal{V}=\\mathbf{1}:=\\left\\{r\\in\\mathbb{R}^{S\\times A}:r(s,a)=c\\in\\mathbb{R}\\right\\}$ , and by potential shaping transformations, i.e., $\\mathcal{V}=\\mathcal{U}$ . Note that since 1 is a subspace of $\\boldsymbol{\\mathcal{U}}$ and $\\boldsymbol{\\mathcal{U}}$ is $|{\\cal S}|$ -dimensional, $[r]_{1}$ is a strict subset of $[r]_{\\mathcal{U}}$ whenever $\\vert{\\cal S}\\vert>1$ . ", "page_idx": 3}, {"type": "text", "text": "3 Transferability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our main results on transferability in IRL. To this end, we first introduce the problem of learning $\\varepsilon$ -transferable rewards from multiple experts acting in different environments. ", "page_idx": 3}, {"type": "text", "text": "3.1 Problem formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{R}\\subseteq\\mathbb{R}^{S\\times A}$ be a compact reward class, and suppose we are given access to $K$ expert data sets, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}_{k}^{\\mathsf{E}}=\\Bigl\\{\\Bigl(s_{0}^{k,i},a_{0}^{k,i},\\cdot\\cdot\\cdot,s_{H^{\\mathsf{E}}-1}^{k,i},a_{H^{\\mathsf{E}}-1}^{k,i}\\Bigr)\\Bigr\\}_{i=0}^{N^{\\mathsf{E}}-1}\\,,\\quad k=0,\\cdot\\cdot\\,,K-1,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "consisting of trajectories sampled independently from the experts $\\mu_{P^{0}}^{\\mathsf{E}},\\ldots,\\mu_{P^{K-1}}^{\\mathsf{E}}$ . Each expert is optimal for the same unrevealed reward $r^{\\mathsf{E}}\\in\\mathcal{R}$ , but under different transition laws, $P^{0},\\ldots,P^{K-1}$ . Our goal is to recover a reward r\u02c6 \u2208R that is transferable across a set of transition laws P \u2286\u2206SS\u00d7A. Specifically, the optimal occupancy measure corresponding to $\\hat{r}$ should remain approximately optimal for $r^{\\mathsf{E}}$ under every transition law in $\\mathcal{P}$ . This yields the following definition of $\\varepsilon$ -transferability. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 $\\varepsilon$ -transferability). Fix some $\\varepsilon\\,>\\,0$ . We say that $\\hat{r}$ is $\\varepsilon$ -transferable to some set of transition laws $\\mathcal{P}\\subseteq\\Delta_{S}^{S\\times A}$ if $\\ell_{P}(r^{\\mathsf{E}},\\mathsf{R L}_{P}(\\hat{r}))\\,\\leq\\,\\varepsilon$ for all $P\\,\\in\\,\\mathcal P$ . We say that $\\hat{r}$ is exactly transferable to $\\mathcal{P}$ if it is $\\varepsilon$ -transferable to $\\mathcal{P}$ with $\\varepsilon=0$ . ", "page_idx": 3}, {"type": "text", "text": "The error margin of $\\varepsilon$ is crucial, as exact transferability is unrealistic when learning from finite expert data. Moreover, note that Definition 3.1 is a definition of uniform transferability, as it requires $\\hat{r}$ to be $\\varepsilon$ -transferable to any $P\\,\\in\\,\\mathcal P$ with the same fixed $\\varepsilon$ . In the following, we will analyze the transferability of a reward $\\hat{r}$ for which all experts are $\\hat{\\varepsilon}_{}$ -optimal for some $\\hat{\\varepsilon}>0$ . That is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{P^{k}}(\\hat{r},\\mu_{P^{k}}^{\\mathsf{E}})\\leq\\hat{\\varepsilon},\\quad k=0,\\ldots,K-1.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, we aim to establish appropriate conditions for choosing $\\hat{\\varepsilon}$ so as to guarantee $\\varepsilon$ - transferability to some set of transition laws $\\mathcal{P}$ . In Section 4, we will then provide an IRL algorithm that, with high probability, outputs a reward $\\hat{r}$ such that (3) holds. ", "page_idx": 3}, {"type": "text", "text": "Remark 3.2. As discussed in Appendix J, the assumption of perfect expert optimality with respect to $r^{\\mathsf{E}}$ can be relaxed to allow for a misspecification error. All our results remain applicable in this setting but include an additional error term due to the experts\u2019 suboptimality. ", "page_idx": 3}, {"type": "text", "text": "3.2 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Most previous work has focused on reward identifiability. For a single expert, Cao et al. [2021], Skalse et al. [2023], Schlaginhaufen and Kamgarpour [2023] show that under Assumption 2.1 (steepness) ", "page_idx": 3}, {"type": "text", "text": "the feasible reward set (2) can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{I R L}(\\mu^{\\mathsf{E}})=\\left(\\nabla\\bar{h}(\\mu^{\\mathsf{E}})+\\mathcal{U}\\right)\\cap\\mathcal{R}=[r^{\\mathsf{E}}]_{\\mathcal{U}}\\cap\\mathcal{R}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, steepness ensures that the expert\u2019s reward is identifiable up to potential shaping. To identify the reward up to a constant, we can either restrict the reward class, e.g. to state-only rewards as explored by Amin et al. [2017], or learn from multiple experts [Cao et al., 2021, Rolland et al., 2022]. In particular, when we are given access to two experts, $\\mu_{P^{0}}^{\\bar{\\mathsf{E}}}$ and $\\mu_{P^{1}}^{\\mathsf{E}}$ , we can identify the experts\u2019 reward up to the intersection ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{I R L}_{P^{0}}(\\mu_{P^{0}}^{\\mathsf{E}})\\cap\\mathsf{I R L}_{P^{1}}(\\mu_{P^{1}}^{\\mathsf{E}})=[r^{\\mathsf{E}}]_{\\mathcal{U}_{P^{0}}}\\cap[r^{\\mathsf{E}}]_{\\mathcal{U}_{P^{1}}}\\cap\\mathcal{R}=\\left(r^{\\mathsf{E}}+\\mathcal{U}_{P^{0}}\\cap\\mathcal{U}_{P^{1}}\\right)\\cap\\mathcal{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "That is, for the unrestricted reward class, $\\mathcal{R}=\\mathbb{R}^{S\\times A}$ , the reward is identifiable up to a constant if and only if $\\mathcal{U}_{P^{0}}\\cap\\mathcal{U}_{P^{1}}=\\mathbf{1}$ . Or equivalently, if and only if the rank condition ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{rank}\\left(\\left[E-\\gamma P^{0},\\qquad E-\\gamma P^{1}\\right]\\right)=2|S|-1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is satisfied [Rolland et al., 2022]. Moreover, Schlaginhaufen and Kamgarpour [2023] show that identifying the expert\u2019s reward up to a constant is a necessary and sufficient condition for exact transferability to any full-dimensional set $\\mathcal{P}\\subseteq\\Delta_{S}^{S\\times A}$ (a set $\\mathcal{P}$ whose interior, with respect to the subspace topology on $\\Delta_{S}^{S\\times A}$ [Bourbaki, 1966], is non-empty). ", "page_idx": 4}, {"type": "text", "text": "Limitations The above results assume perfect access to the expert\u2019s policy, which isn\u2019t realistic. In practice, we can only learn a reward for which the experts are approximately optimal. In Example 3.3 below, we show that under approximate optimality of the experts, the learned reward can perform very poorly in a new environment, even if the rank condition in Equation (5) is satisfied. ", "page_idx": 4}, {"type": "text", "text": "Example 3.3. We consider a two-state, two-action MDP with $\\ensuremath{\\boldsymbol{S}}=\\ensuremath{\\boldsymbol{\\mathcal{A}}}=\\{0,1\\}$ , uniform initial state distribution, discount rate $\\gamma=0.9$ , and Shannon entropy regularization $h=-\\mathcal{H}$ (see Appendix C). Suppose the expert reward is $r^{\\mathsf{E}}(s,a)\\,=\\,\\mathbb{1}\\{s\\,=\\,1\\}$ and consider the transition laws, $P^{0}$ and $P^{1}$ , defined by $P^{0}(\\bar{0}|s,a)=0.75$ and $P^{1}(0|s,a)=0.25+\\beta\\cdot\\mathbb{1}\\left\\{s=0,a=0\\right\\}$ for some $\\beta\\in[0,0.75]$ . Also, consider the two experts $\\mu_{P^{0}}^{\\mathsf{E}}=\\mathsf{R L}_{P^{0}}(r^{\\mathsf{E}})$ and $\\mu_{P^{1}}^{\\mathsf{E}}=\\mathsf{R L}_{P^{1}}(r^{\\mathsf{E}})$ , and suppose we recovered the reward $\\hat{r}(s,a)\\,=\\,-r^{\\mathsf{E}}$ . Then, as detailed in Appendix E, the following holds: 1) We have $\\ell_{P^{0}}(\\hat{r},\\mu_{P^{0}}^{\\mathsf{E}})\\,=\\,0$ and $\\ell_{P^{1}}(\\hat{r},\\mu_{P^{1}}^{\\mathsf{E}})=\\mathcal{O}(\\beta)$ . That is, for small $\\beta$ , the reward $\\hat{r}$ is a good solution to the IRL problem, as both experts are approximately optimal under $\\hat{r}$ . 2) The rank condition (5) between $P^{0}$ and $P^{1}$ is satisfied for any $\\beta~>~0$ . 3) For a new transition law $P$ defined by $P(0|s,a)=\\mathbb{1}\\left\\{s=1,a=0\\right\\}$ , we have $\\ell_{P}(\\dot{r}^{\\mathsf{E}},\\mathsf{R L}_{P}(\\hat{r}))\\approx4.81$ , i.e. $\\mathsf{R L}_{P}(\\hat{r})$ performs poorly under the experts\u2019 reward. ", "page_idx": 4}, {"type": "text", "text": "3.3 Theoretical insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To establish a sufficient condition for $\\varepsilon$ -transferability, our goal is to bound the suboptimality of an optimal occupancy measure, $\\mathsf{R L}(r)$ , for some reward $r^{\\prime}$ , in terms of reward distances measured in the quotient space $\\mathbb{R}^{\\check{S}\\times\\mathcal{A}}/\\mathcal{U}$ . To this end, we first establish the relationship between the suboptimality in Equation (1) and the Bregman divergence corresponding to the occupancy measure regularization. ", "page_idx": 4}, {"type": "text", "text": "Bregman divergences The Bregman divergence [Teboulle, 1992] associated to $\\bar{h}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{\\bar{h}}(\\mu,\\mu^{\\prime})=\\bar{h}(\\mu)-\\bar{h}(\\mu^{\\prime})-\\langle\\nabla\\bar{h}(\\mu^{\\prime}),\\mu-\\mu^{\\prime}\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proposition 3.4. Under Assumptions 2.1 and 2.2, we have $\\ell(r^{\\prime},\\mu)=D_{\\bar{h}}(\\mu,\\mathsf{R L}(r^{\\prime}))$ for any $\\mu\\in\\mathcal{M}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.4 above demonstrates that the suboptimality of an occupancy measure $\\mu$ for the reward $r^{\\prime}$ coincides with the Bregman divergence between $\\mu$ and the optimal occupancy measure under $r^{\\prime}$ . This generalizes [Mei et al., 2020, Lemma 26] from entropy regularization to any steeply regularized MDP. The proof is presented in Appendix D.6. ", "page_idx": 4}, {"type": "text", "text": "Reward approximation Next, we show that under strong convexity and local Lipschitz gradients, the Bregman divergence between two optimal occupancy measures is bounded in terms of reward distances in $\\mathbb{R}^{S\\times A}\\breve{/}\\mathcal{U}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.5 (Regularity). Suppose the following holds: ", "page_idx": 5}, {"type": "text", "text": "a) The regularizer $\\bar{h}$ is $\\eta$ -strongly convex over the set of occupancy measures $\\mathcal{M}$ . That is, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{h}(\\mu^{\\prime})\\geq\\bar{h}(\\mu)+\\langle\\nabla\\bar{h}(\\mu),\\mu^{\\prime}-\\mu\\rangle+\\frac{\\eta}{2}\\left\\Vert\\mu^{\\prime}-\\mu\\right\\Vert_{2}^{2},\\quad\\forall\\mu,\\mu^{\\prime}\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "b) The gradient $\\nabla\\bar{h}$ is locally Lipschitz continuous over relint $\\mathcal{M}$ . That is, for any closed convex subset $\\kappa\\subset$ relint $\\mathcal{M}$ there exists $L_{\\cal K}>0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla\\bar{h}(\\mu)-\\nabla\\bar{h}(\\mu^{\\prime})\\right\\|_{2}\\leq L_{K}\\left\\|\\mu-\\mu^{\\prime}\\right\\|_{2},\\quad\\forall\\mu,\\mu^{\\prime}\\in\\mathcal{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We will show later that Assumption 3.5 is met for Shannon and Tsallis entropy regularization (see Proposition D.9). Under the above assumption, the following lemma establishes the desired upper and lower bound on the Bregman divergence between two optimal occupancy measures with respect to reward distances measured in $\\mathbb{R}^{S\\times\\overleftarrow{A}}/\\mathcal{U}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.6. Suppose Assumptions 2.1,2.2, and 3.5 hold, and let $r,r^{\\prime}\\in\\mathcal{R}$ . Then, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{\\mathcal{R}}}{2}\\left\\|[r]_{\\mathcal{U}}-[r^{\\prime}]_{\\mathcal{U}}\\right\\|_{2}^{2}\\leq\\ell(r^{\\prime},\\mathsf{R L}(r))=D_{\\bar{h}}\\left(\\mathsf{R L}(r),\\mathsf{R L}(r^{\\prime})\\right)\\leq\\frac{1}{2\\eta}\\left\\|[r]_{\\mathcal{U}}-[r^{\\prime}]_{\\mathcal{U}}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some problem-dependent constant $\\sigma_{\\mathscr R}>0$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 3.7. The proof of Lemma 3.6 hinges on the duality between equivalence classes of rewards and optimal occupancy measures (see Appendix B). The main idea is to leverage duality of Bregman divergences, and a dual smoothness and strong convexity result in Proposition D.7. A key challenge arises because, by Assumption 2.1, the regularizer cannot be globally smooth. This results in a problem-dependent dual strong convexity constant $\\sigma_{\\mathcal{R}}$ [Goebel and Rockafellar, 2008]. In Proposition D.9, we will provide a lower bound on $\\sigma_{\\mathcal{R}}$ for the specific choices of Shannon and Tsallis entropy regularization. For more details, we refer to the full proof in Appendix D.6. ", "page_idx": 5}, {"type": "text", "text": "The above lemma has two key implications: First, the lower bound in (6) implies that if we recover a reward $\\hat{r}$ for which all experts are approximately optimal, then the distance between $\\hat{r}$ and $r^{\\mathsf{E}}$ can be bounded in the quotient spaces $\\mathbb{R}^{\\lambda\\times\\mathcal{A}}/\\mathcal{U}_{P^{k}}$ . Second, the upper bound shows that to control the performance of $\\mathsf{R L}_{P}(\\hat{r})$ in a new environment $P$ , we need to tightly bound the distance between $\\hat{r}$ and $r^{\\mathsf{E}}$ in $\\mathbb{R}^{S\\times A}/\\mathcal{U}_{P}$ . As distances in $\\mathbb{R}^{S\\times A}/\\mathcal{U}_{P}$ are bounded by distances in $\\mathbb{R}^{S\\times A}/\\mathbf{1}$ , this can be achieved by bounding the distance between $\\hat{r}$ and $r^{\\mathsf{E}}$ in $\\mathbb{R}^{S\\times A}/\\mathbf{1}$ . However, revisiting Example 3.3 in light of Lemma 3.6 shows that even though $\\hat{r}$ and $r^{\\mathsf{E}}$ are close in $\\mathbb{R}^{S\\times A}/{\\mathcal{U}}_{P^{k}}$ , this does not guarantee their proximity in $\\mathbb{R}^{S\\times A}/\\mathbf{1}$ and $\\mathbb{R}^{S\\times\\bar{A}}/\\bar{\\mathcal{U}}_{P}$ . ", "page_idx": 5}, {"type": "text", "text": "Example 3.3 (continued). Recall the definition $\\mathcal{U}_{P^{k}}=\\mathrm{im}(E-\\gamma P^{k})$ . Given that in Example 3.3 we have $\\ell_{P^{0}}(\\hat{r},\\mu_{P^{0}}^{\\mathsf{E}})=0$ and $\\ell_{P^{1}}(\\hat{r},\\mu_{P^{1}}^{\\mathsf{E}})=\\mathcal{O}(\\beta)$ , Lemma 3.6 ensures that $\\hat{r}$ and $r^{\\mathsf{E}}$ coincide in $\\mathbb{R}^{S\\times A}/{\\mathcal{U}}_{P^{0}}$ , and for small $\\beta$ , they are close in $\\mathbb{R}^{S\\times A}/\\mathcal{U}_{P^{1}}$ . However, as illustrated in Figure1(a) this doesn\u2019t ensure that $\\hat{r}$ and $r^{\\mathsf{E}}$ are close in $\\mathbb{R}^{S\\times A}/\\mathbf{1}$ and $\\mathbb{R}^{S\\times A}/\\mathcal{U}_{P}$ . In particular, it can be computed that $\\left\\|[\\hat{r}]_{\\mathcal{U}_{P}}-[r^{\\mathsf{E}}]_{\\mathcal{U}_{P}}\\right\\|_{2}\\approx1.51$ , which by Lemma 3.6 explains the poor transferability to $P$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 Sufficient conditions for transferability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With Lemma 3.6 in place, we are set to present our results on $\\varepsilon$ -transferability. Example 3.3 indicates that a sufficient condition for learning transferable rewards from experts $[K=2]$ ) should not rely solely on the binary rank condition (5), which only checks if $\\mathcal{U}_{P^{0}}\\cap\\mathcal{U}_{P^{1}}={\\bf1}$ . Instead, we should consider the relative orientation between $\\mathcal{U}_{P^{0}}$ and $\\mathcal{U}_{P^{1}}$ . To formalize this, we need to introduce the concept of principal angles between linear subspaces, as outlined in Definition 3.8 below. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.8 (Principal angles [Gal\u00e1ntai, 2013]). Let $\\mathcal{V},\\mathcal{W}\\subseteq\\mathbb{R}^{n}$ be two subspaces of dimension $m\\leq n$ . The principal angles $0\\le\\theta_{1}(\\mathcal{V},\\mathcal{W})\\le\\ldots\\le\\theta_{m}(\\mathcal{V},\\mathcal{W})=:\\theta_{\\operatorname*{max}}(\\mathcal{V},\\mathcal{W})\\le\\pi/2$ between $\\nu$ and $\\mathcal{W}$ are defined recursively via ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cos(\\theta_{i}(\\mathcal{V},\\mathcal{W}))=\\operatorname*{max}_{v\\in\\mathcal{V},\\,w\\in\\mathcal{W}}\\langle v,w\\rangle\\mathrm{~s.t.~}\\|v\\|_{2}=\\|w\\|_{2}=1,\\,\\langle v,v_{j}\\rangle=\\langle w,w_{j}\\rangle=0,\\,j=1,\\dots,i-1,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $v_{j},w_{j}$ are the maximizers corresponding to the angle $\\theta_{j}$ . For two transition laws $P,P^{\\prime}$ , we define $\\mathring{\\theta_{i}}(P;P^{\\prime}):=\\theta_{i}(\\mathcal{U}_{P},\\mathcal{U}_{P^{\\prime}})$ and refer to $\\theta_{i}(P,P^{\\prime})$ as the $i$ -th principal angles between $P$ and $P^{\\prime}$ . ", "page_idx": 5}, {"type": "image", "img_path": "l5wEQPcDab/tmp/8cb13407356bd273b0bc827528c87d0f06b67934bd99fd8a2efb193f21d46986.jpg", "img_caption": ["(a) Rewards in Example 3.3. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "l5wEQPcDab/tmp/7d0d77a8118055ab1bdd70dc797bf8cca55ccb3ab086c52ee9793c974c17cce6.jpg", "img_caption": ["$(b)$ Proof sketch Theorem 3.10. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: $(a)$ illustrates the equivalence classes $[\\hat{r}]_{\\mathcal{U}}$ and $[r^{\\mathsf{E}}]_{\\mathcal{U}}$ , corresponding to the transition laws $P^{0},P^{1},P$ from Example 3.3, for a small $\\beta$ , in $\\mathbf{\\dot{R}}^{\\bar{S}\\times A}/\\mathbf{1}$ . The blue lines correspond to $P^{0}$ , the red lines to $P^{1}$ , and the gray lines to $P$ . Furthermore, the shaded areas illustrate the approximation error around $[r^{\\mathsf{E}}]_{\\mathcal{U}_{P^{k}}}$ , as guaranteed by Lemma 3.6. $(b)$ illustrates the uncertainty set for the recovered reward when learning from two experts, as discussed in the proof sketch of Theorem 3.10. ", "page_idx": 6}, {"type": "text", "text": "Principal angles are the natural generalization of angles between two lines or planes to higher dimensional subspaces. For principal angles between transition laws, we have the following proposition. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.9. Let $P,P^{\\prime}\\,\\in\\,\\Delta_{S}^{S\\times A}$ and $H_{\\gamma}\\,=\\,1/(1-\\gamma)$ . Then, we have $\\theta_{1}(P,P^{\\prime})\\,=\\,0$ and $\\sin\\left(\\theta_{\\mathrm{max}}(P,P^{\\prime})\\right)\\leq\\gamma H_{\\gamma}\\sqrt{|S|/|A|}\\,\\left\\|P-P^{\\prime}\\right\\|$ , where $\\lVert\\cdot\\rVert$ denotes the spectral norm. ", "page_idx": 6}, {"type": "text", "text": "The proof can be found in Appendix D.7. The above result shows that while the first principal angle between two transition laws is always zero, all principal angles are small if the transition laws are close to one another. In Example 3.3, we have $\\dot{\\sin}(\\theta_{2}(\\dot{P}^{0},\\bar{P^{1}}))=\\mathcal{O}(\\beta)$ , indicating that the second and in this case maximal principal angle is small when $\\beta$ is small (see Appendix E). The following result shows that when learning from two experts, the transferability error is directly controlled by the second principal angle between the experts\u2019 transition laws. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.10. Let $K=2$ , $\\theta_{2}(P^{0},P^{1})>0,$ , and suppose that Assumptions 2.1,2.2, and 3.5 hold. If $\\ell_{P^{k}}(\\hat{r},\\mu_{P^{k}}^{E})\\leq\\hat{\\varepsilon}$ for $k=0,1$ , then $\\hat{r}$ is $\\varepsilon$ -transferable to $\\mathcal{P}=\\Delta_{S}^{S\\times A}$ with ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varepsilon=\\hat{\\varepsilon}/\\left[\\eta\\sigma_{\\mathcal{R}}\\sin\\left(\\theta_{2}({P^{0}},{P^{1}})\\,/2\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Sketch of proof. The main idea of the proof is illustrated in Figure 1(b). First, it follows from Lemma 3.6 that $\\hat{r}$ and $r^{\\mathsf{E}}$ are $\\bar{\\varepsilon}=\\sqrt{2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}}}$ -close in $\\mathbb{R}^{S\\times A}/{\\mathcal{U}}_{P^{k}}$ for $k=0,1$ , respectively. From Figure 1(b) we see \u2013 using basic trigonometry \u2013 that this implies that $\\hat{r}$ and $r^{\\mathsf{E}}$ are at least $\\Delta=$ $\\bar{\\varepsilon}/\\sin(\\theta/2)$ -close in $\\mathbb{R}^{S\\times\\overleftarrow{A}}/\\mathbf{1}$ . As shown in the full proof in Appendix F, the relevant angle, $\\theta$ , is the second principal angle $\\theta_{2}(\\dot{P}^{0},P^{1})$ . The result then follows from the upper bound in Lemma 3.6. ", "page_idx": 6}, {"type": "text", "text": "Some observations are in order. First, the above theorem shows that the larger the second principal angle between the two experts\u2019 transition laws, the better the recovered reward transfers to a new environment. Second, observe that $\\theta_{2}(P^{0},P^{1})>0$ is equivalent to the rank condition (5), as the second principal angle between two subspaces is non-zero if and only if their intersection is at most one-dimensional. Therefore, for exact transferability, Theorem 3.10 requires the rank condition (5) to be satisfied and $\\hat{\\varepsilon}\\;=\\;0$ , recovering the results by Cao et al. [2021], Rolland et al. [2022], Schlaginhaufen and Kamgarpour [2023]. But in contrast to past results, Theorem 3.10 applies to more realistic scenarios, where $\\hat{\\varepsilon}$ is merely small, not zero. Finally, we note that Theorem 3.10 can be trivially generalized to $K\\geq2$ experts by replacing $\\theta_{2}(P^{0},P^{1})$ with the maximum of $\\theta_{2}(P^{k},P^{l})$ over $0\\,\\le\\,k\\,\\le\\,l\\,\\le\\,K\\,-\\,1$ . However, such bounds may be loose for $K>2$ , potentially leaving considerable room for improvement in this setting. ", "page_idx": 6}, {"type": "text", "text": "Local transferability When learning a reward $\\hat{r}$ from a single expert $\\mathcal{K}=1$ ), Schlaginhaufen and Kamgarpour [2023] show that, without reducing the dimension of the reward class, $\\hat{r}$ cannot be exactly transferable to any neighborhood of the expert\u2019s transition law $P_{0}$ . However, Theorem 3.11 below shows that by allowing for an $\\varepsilon$ of error, we can guarantee transferability to a neighborhood of $P_{0}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.11. Let $K=1$ , $D:=\\operatorname*{max}_{r,r^{\\prime}\\in\\mathcal{R}}\\left\\|r-r^{\\prime}\\right\\|_{2}$ , and suppose that Assumptions 2.1,2.2, and 3.5 hold. If \u2113P 0(r\u02c6, \u00b5E) \u2264\u03b5\u02c6, then r\u02c6 is \u03b5P -transferable to P \u2208\u2206SS\u00d7 with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varepsilon_{P}=2\\operatorname*{max}\\left\\{2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}},D^{2}\\sin\\left(\\theta_{\\operatorname*{max}}(P^{0},P)\\right)^{2}\\right\\}/\\eta.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above theorem (which is proven in Appendix $\\mathrm{G}$ ) shows that the reward learned from a single expert transfers to transition laws that are sufficiently close to the expert\u2019s, where the closeness is measured in terms of the maximal principal angle. In other words, while a large second principal angle between two experts\u2019 transition laws, as per Theorem 3.10, ensures that the reward recovered from these two experts is transferable to arbitrary transition laws, a small largest principal angle between two transition laws ensures that a reward recovered in one environment can be successfully transferred to the other environment. ", "page_idx": 7}, {"type": "text", "text": "Remark 3.12. As discussed in Appendix $\\mathrm{H}$ , we can compute the principal angles using a singular value decomposition. Moreover, given estimates $\\hat{P}^{0},\\hat{P}^{1}$ of the transition laws $P^{0},P^{1}$ , the error in the estimate of $\\sin\\theta_{i}(P^{0},P^{1})$ scales with $\\mathcal{O}(\\operatorname*{max}\\{||P^{0}-\\hat{P}^{0}||,||P^{1}-\\hat{P}^{1}||\\})$ . ", "page_idx": 7}, {"type": "text", "text": "Regularizers To provide more insights about Theorems 3.10 and 3.11, we provide explicit values for the primal and dual strong convexity constants, $\\eta$ and $\\sigma_{\\mathcal{R}}$ , respectively. To this end, we focus on the Shannon entropy regularization $h(p)=-\\tau\\mathcal{H}(p)$ and the Tsallis- $1/2$ entropy regularization $h(p)=-\\tau\\mathcal{H}_{1/2}(p)$ as defined in Appendix C. While the Shannon entropy regularization is commonly used in IRL [Ziebart, 2010, Ho and Ermon, 2016], the Tsallis- $1/2$ entropy is more often adopted in the multi-armed bandit literature Zimmert and Seldin [2021]. Both regularizations satisfy Assumption 2.1 as well as Assumption 3.5 with the constants detailed in Proposition D.9 in the appendix. In general, the Tsallis entropy leads to a slightly smaller strong convexity constant $\\eta$ , but avoids an exponential dependence on the effective horizon $H_{\\gamma}=1/(1-\\bar{\\gamma})$ in $\\sigma_{\\mathcal{R}}$ . Below, we summarize the implications of Proposition D.9 for $\\varepsilon$ -transferability of a reward $\\hat{r}$ recovered from two experts. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3.13. Suppose the conditions in Theorem 3.10 hold. Furthermore, let $H_{\\gamma}:=1/(1-\\gamma)$ , $R:=\\operatorname*{max}_{r\\in{\\mathcal{R}}}\\left\\|r\\right\\|_{\\infty}$ , $D=\\operatorname*{max}_{r,r^{\\prime}\\in\\mathcal{R}}{\\|r-r^{\\prime}\\|_{2}},$ , and $\\tau<D$ . Then, for the Shannon entropy $\\hat{r}$ is $\\varepsilon$ -transferable to $\\mathcal{P}=\\Delta_{S}^{S\\times A}$ with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varepsilon=\\frac{2H_{\\gamma}^{2}D|S||A|^{2+H_{\\gamma}}\\exp{\\left(\\frac{2R H_{\\gamma}}{\\tau}\\right)}}{\\nu_{m i n}^{2}\\tau\\sin{(\\theta_{2}(P^{0},P^{1})/2)^{2}}}\\hat{\\varepsilon},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and for the Tsallis entropy with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varepsilon=\\frac{4\\sqrt{2}H_{\\gamma}^{5}D|S||A|^{2}\\left(2R/\\tau+3\\sqrt{|A|}\\right)^{3}}{\\nu_{m i n}^{2}\\tau\\sin\\left(\\theta_{2}(P^{0},P^{1})/2\\right)^{2}}\\hat{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We observe that transferability generally becomes more challenging with decreasing regularization parameter $\\tau$ , i.e. if the expert\u2019s policy becomes more deterministic. Furthermore, we see that it is easier to recover a transferable reward in a Tsallis entropy-regularized MDP. Corollary 3.13 also shows that the constant between $\\varepsilon$ and $\\hat{\\varepsilon}$ tends to be large, meaning that we need to recover a reward for which the experts are $\\hat{\\varepsilon}$ -optimal with a very small $\\hat{\\varepsilon}$ to guarantee $\\varepsilon$ -transferability for a reasonable $\\varepsilon$ . However, it\u2019s important to note that our results provide sufficient conditions for the worst case, and it remains for future work to determine under what conditions these constants can be improved. ", "page_idx": 7}, {"type": "text", "text": "Remark 3.14. Our results in this section, especially Proposition 3.4 and Lemma 3.6, are critically relying on the steepness of the regularization (Assumption 2.1), which is essential to ensure that the expert\u2019s reward can be identified up to the equivalence class of potential shaping transformations. Although we can still upper bound the suboptimality $\\ell(r^{\\mathsf{E}},{\\mathsf{R L}}(\\hat{r}))$ in terms of the distance between $\\hat{r}$ to $r^{\\mathsf{E}}$ in $\\mathbb{R}^{S\\times A}/\\mathcal{U}$ without this assumption (see Proposition D.10), we no longer have a lower bound as in Lemma 3.6, which is essential for establishing closeness of $\\hat{r}$ and $r^{\\mathsf{E}}$ in $\\mathbb{R}^{S\\times A}/\\mathcal{U}$ . Hence, we expect it to be difficult to obtain guarantees similar to those in Theorem 3.10 and 3.11 for the unregularized setting, without either reducing the dimension of the reward class [Amin et al., 2017] or making specific assumptions about the feasible reward sets [Metelli et al., 2021, Assumption 4.1]. ", "page_idx": 7}, {"type": "text", "text": "4 Algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To provide end-to-end guarantees for recovering transferable rewards from a finite set of expert demonstrations, we analyze the convergence and sample complexity (in terms of expert demonstrations) of an algorithm for recovering a reward for which all $K$ experts are approximately optimal. To this end, we focus on the reward class $\\mathcal{R}=\\left\\{r\\in\\mathbb{R}^{S\\times A}:\\lVert r\\rVert_{1}^{\\textit{\\textbf{\\cdot}}}\\leq1\\right\\}$ . Furthermore, we assume oracle access to a $(\\varepsilon,\\delta)$ -PAC algorithm for the forward problem (O-RL). That is, a polynomial-time algorithm, $A^{\\varepsilon,\\delta}$ , that outputs a policy $\\pi=\\mathsf{A}^{\\varepsilon,\\delta}(r)$ such that with probability at least $1-\\delta$ it holds that $\\ell(r,\\mu^{\\pi})\\leq\\varepsilon$ (see e.g. [Lan, 2023] for a specific example). The key idea of our meta-algorithm is to learn a reward minimizing the sum of the suboptimalities of the $K$ experts $\\mu_{P^{0}}^{\\mathsf{E}},\\ldots,\\mu_{P^{K-1}}^{\\mathsf{E}^{-}}$ . This leads us to the following multi-expert IRL problem ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r\\in\\mathcal{R}}\\sum_{k=0}^{K-1}\\ell_{P^{k}}(r,\\hat{\\mu}_{\\mathcal{D}_{k}^{\\mathsf{E}}}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\mu}_{\\mathcal{D}_{k}^{\\mathbb{E}}}(s,a):=(1-\\gamma)/N^{\\mathbb{E}}\\sum_{i=0}^{N^{\\mathbb{E}}-1}\\sum_{t=0}^{H^{\\mathbb{E}}-1}\\gamma^{t}\\mathbb{1}\\{s_{t}^{k,i}=s,a_{t}^{k,i}=a\\}}\\end{array}$ is an empirical expert occupancy measure. To solve Problem (O-IRL), we propose the projected gradient descent scheme as detailed in Algorithm 1 below, where $\\tt r o l l o u t_{P^{k}}(\\pi,N,H)$ samples $N$ independent trajectories of length $H$ from policy $\\pi$ . Using a stochastic online gradient descent analysis, Theorem 4.1 shows that any PAC algorithm for the forward problem yields a PAC algorithm for the inverse problem. ", "page_idx": 8}, {"type": "table", "img_path": "l5wEQPcDab/tmp/52f34a0a5cfeb06efbbc857242b75ce7b72f5556e1203bd3823ffa8509dfbbf4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. Suppose that $N^{E}=\\Omega\\big(K\\log(|S||A|/\\hat{\\delta})/\\hat{\\varepsilon}^{2}\\big)$ and $H^{E}=\\Omega\\big(\\log(K/\\hat{\\varepsilon})/\\log(1/\\gamma)\\big)$ . Running Algorithm $^{\\,l}$ for $T=\\Omega\\big(K^{2}/\\hat{\\varepsilon}^{2}\\big)$ iterations with step-size $\\alpha=1/(K{\\sqrt{T}})$ , where $\\delta_{o p t}=$ $\\mathcal{O}\\big(\\hat{\\delta}\\hat{\\varepsilon}^{2}/K^{3}\\big)$ , $\\varepsilon_{o p t}\\,=\\,{\\mathcal O}(\\hat{\\varepsilon}/K),$ , $N\\,=\\,\\Omega\\big(K\\log(K|S||A|/(\\hat{\\delta}\\hat{\\varepsilon}))/\\hat{\\varepsilon}^{2}\\big)$ , and $H\\,=\\,H^{E}$ , it holds with probability at least $1-\\hat{\\delta}$ that $\\ell_{P^{k}}(\\hat{r},\\mu_{P^{k}}^{E})\\leq\\hat{\\varepsilon}$ , for $k=0,\\ldots,K-1$ . ", "page_idx": 8}, {"type": "text", "text": "The above result generalizes [Syed and Schapire, 2007, Theorem 2] by considering multiple experts and by proving convergence in terms of the expert suboptimality. We refer to Appendix I for the proof and the precise constants. Theorem 4.1 shows that with $\\dot{\\Omega}(K/\\hat{\\varepsilon}^{2})$ demonstrations of each expert, we recover in $\\Omega(K^{2}/\\hat{\\varepsilon}^{2})$ steps of Algorithm 1 a reward $\\hat{r}$ for which all experts are $\\hat{\\varepsilon}.$ -optimal. Together with Theorem 3.10 and 3.11, this provides a bound on the sample and time complexity of recovering in $\\varepsilon$ -transferable rewards in regularized IRL. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate our results experimentally, we adopt a stochastic variant of the WindyGridworld environment [Sutton and Barto, 2018]. In this environment, the agent moves to the intended grid cell with a probability of $(1-\\beta)$ and is pushed one step further in the direction of the wind with a probability of $\\beta$ . Using Algorithm 1, we recover a reward $\\hat{r}$ from demonstrations of two experts, both exposed to the same wind strength $\\beta$ but different wind directions \u2013 North and East. The experiments are repeated for a varying number of expert demonstrations $N^{\\mathsf{E}}\\in\\{10^{3},10^{4},10^{5},10^{6}\\}$ and wind strengths $\\beta\\in\\{0.01,\\bar{0}.1,\\bar{0}.5,1.0\\}$ . We then test the transferability to two different environments: one with South wind, $P^{\\mathrm{South}}$ , and a zero-wind environment with cyclically shifted actions, $P^{\\mathrm{Shifted}}$ . Figure 2(a) shows that the second principal angle between the two experts\u2019 transition laws $P_{0}$ and ", "page_idx": 8}, {"type": "image", "img_path": "l5wEQPcDab/tmp/d226ec619b38ed8b0ebd10a35bf2382b714d10411b9776d0b7705160bc035c00.jpg", "img_caption": ["Figure 2: $(a)$ shows the second principal angle between the experts, for varying wind strength $\\beta$ . $(b)$ shows the distance between $\\hat{r}$ and $r^{\\tt E}$ in $\\mathbb{R}^{\\breve{S}\\times\\mathcal{A}}/\\mathbf{1}$ for a varying number of expert demonstrations $N^{\\mathsf{E}}$ and wind strength $\\beta$ . $(c)$ and $(d)$ show the transferability to $P^{\\mathrm{South}}$ and $P^{\\mathrm{Shifted}}$ in terms of $\\ell_{P^{\\mathrm{South}}}(r^{\\mathsf{E}},\\mathsf{R L}_{P^{\\mathrm{South}}}(\\bar{r}))$ and $\\ell_{P^{\\mathrm{Shifted}}}(r^{\\mathsf{E}},\\mathsf{R L}_{P^{\\mathrm{Shifted}}}(\\hat{r}))$ , respectively. The circles indicate the median and the shaded areas the 0.2 and 0.8 quantiles over 10 independent realizations of the expert data. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "$P_{1}$ increases with increasing wind strength. Moreover, Figure 2(b)-(d) show that both the closeness between $\\hat{r}$ and $r^{\\mathsf{E}}$ in $\\mathbb{R}^{S\\times\\tilde{A}}/\\mathbf{1}$ and the transferability to $\\bar{P}^{\\mathrm{South}}$ and $P^{\\mathrm{Shifted}}$ improve with a larger second principal angle, as expected from Theorem 3.10. For a more detailed discussion of the experiments we refer to Appendix K. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary In this paper, we investigated the transferability of rewards in regularized IRL. We showed that the conditions established under full access to the experts\u2019 policies do not guarantee transferability when learning a reward from a finite set of expert demonstrations. To address this issue, we proposed using principal angles as a more refined measure of the similarity and dissimilarity of transition laws. Assuming a strongly convex and locally smooth regularization, we then showed that if we recover a reward for which at least two experts are nearly optimal, and their environments are sufficiently different in terms of the second principal angle between their transition laws, then the recovered reward is universally transferable. Furthermore, we showed that if two environments are sufficiently similar in terms of the maximal principal angle between their transition laws, rewards learned in one environment can be effectively transferred to the other environment. Additionally, we provided explicit constants for the Shannon and Tsallis entropy, as well as a PAC algorithm for recovering a reward for which all experts are approximately optimal. As a result, we established end-to-end guarantees for learning transferable rewards in regularized IRL. Additionally, we experimentally validated our results through gridworld experiments. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work Our results provide only sufficient conditions for transferability. It would be valuable to investigate necessary conditions to check whether our bounds are tight. Furthermore, extending our analysis to lower-dimensional reward classes could reduce the complexity of learning transferable rewards. Although our paper focuses on discrete state and action spaces, an exciting avenue for future research would be to extend our results to continuous state and action spaces, which are more commonly encountered in practice. We expect that our proof methods can be generalized to this setting, but the analysis will be more intricate due to the infinite-dimensional reward and occupancy measure spaces. Finally, as our work is mainly theoretical, experimental validation on real-world applications could provide valuable insight into the practical aspects and challenges of transferability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments Andreas Schlaginhaufen is funded by a PhD fellowship from the Swiss Data Science Center. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, 2004.   \nK. Amin, N. Jiang, and S. Singh. Repeated inverse reinforcement learning. Advances in neural information processing systems, 2017.   \nA. Beck. First-order methods in optimization. SIAM, 2017.   \nN. Bourbaki. Elements of mathematics: General topology. Hermann, 1966.   \nS. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.   \nH. Cao, S. Cohen, and L. Szpruch. Identifiability in inverse reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.   \nT. M. Cover. Elements of information theory. John Wiley & Sons, 1999.   \nC. Dann, C.-Y. Wei, and J. Zimmert. Best of both worlds policy optimization. arXiv preprint arXiv:2302.09408, 2023.   \nZ. Drmac. On principal angles between subspaces of euclidean space. SIAM Journal on Matrix Analysis and Applications, 2000.   \nJ. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.   \nA. Gal\u00e1ntai. Projectors and projection methods. Springer Science & Business Media, 2013.   \nD. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. Iq-learn: Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 2021.   \nM. Geist, B. Scherrer, and O. Pietquin. A theory of regularized markov decision processes. In International Conference on Machine Learning. PMLR, 2019.   \nR. Goebel and R. T. Rockafellar. Local strong convexity and local lipschitz continuity of the gradient of convex functions. Journal of Convex Analysis, 2008.   \nJ. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 2016.   \nW. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 1963.   \nJ. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 2019.   \nW. Jeon, C.-Y. Su, P. Barde, T. Doan, D. Nowrouzezahrai, and J. Pineau. Regularized inverse reinforcement learning. In International Conference on Learning Representations, 2021.   \nS. Ji-Guang. Perturbation of angles between linear subspaces. Journal of Computational Mathematics, 1987.   \nR. E. Kalman. When Is a Linear Control System Optimal? Journal of Basic Engineering, 1964.   \nK. Kim, S. Garg, K. Shiragur, and S. Ermon. Reward identification in inverse reinforcement learning. In Proceedings of the 38th International Conference on Machine Learning, pages 5496\u20135505, 2021.   \nA. V. Knyazev and M. E. Argentati. Principal angles between subspaces in an a-based scalar product: algorithms and perturbation estimates. SIAM Journal on Scientific Computing, 2002.   \nG. Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical programming, 2023.   \nY. Lu, J. Fu, G. Tucker, X. Pan, E. Bronstein, R. Roelofs, B. Sapp, B. White, A. Faust, S. Whiteson, et al. Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023.   \nJ. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax policy gradient methods. In International conference on machine learning. PMLR, 2020.   \nA. M. Metelli, G. Ramponi, A. Concetti, and M. Restelli. Provably efficient learning of transferable rewards. In International Conference on Machine Learning. PMLR, 2021.   \nA. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, 1999.   \nA. Y. Ng, S. Russell, et al. Algorithms for inverse reinforcement learning. In Icml, 2000.   \nR. Ngo, L. Chan, and S. Mindermann. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626, 2022.   \nP. A. Ortega, D. A. Braun, J. Dyer, K.-E. Kim, and N. Tishby. Information-theoretic bounded rationality. arXiv preprint arXiv:1512.06789, 2015.   \nR. Ouhamma and M. Kamgarpour. Learning nash equilibria in zero-sum markov games: A single time-scale algorithm under weak reachability. arXiv preprint arXiv:2312.08008, 2023.   \nR. Penrose. A generalized inverse for matrices. In Mathematical proceedings of the Cambridge philosophical society. Cambridge University Press, 1955.   \nD. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1988.   \nM. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \nR. T. Rockafellar. Convex Analysis. Princeton University Press, 1970.   \nR. T. Rockafellar and R. J.-B. Wets. Variational analysis. Springer Science & Business Media, 2009.   \nP. Rolland, L. Viano, N. Sch\u00fcrhoff, B. Nikolov, and V. Cevher. Identifiability and generalizability from multiple experts in inverse reinforcement learning. Advances in Neural Information Processing Systems, 2022.   \nJ. Rust. Structural estimation of markov decision processes. Handbook of econometrics, pages 3081\u20133143, 1994.   \nA. Schlaginhaufen and M. Kamgarpour. Identifiability and generalizability in constrained inverse reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning. PMLR, 2023.   \nJ. M. V. Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in policy optimisation and partial identifiability in reward learning. In International Conference on Machine Learning. PMLR, 2023.   \nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 2020.   \nR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nU. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. Advances in neural information processing systems, 2007.   \nM. Teboulle. Entropic proximal mappings with applications to nonlinear programming. Mathematics of Operations Research, 1992.   \nB. D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.   \nJ. Zimmert and Y. Seldin. Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits. The Journal of Machine Learning Research, 2021.   \nM. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th international conference on machine learning (icml-03), 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Notations 15 ", "page_idx": 13}, {"type": "text", "text": "B Conjugate duality in regularized IRL 16 ", "page_idx": 13}, {"type": "text", "text": "C Regularizers 16 ", "page_idx": 13}, {"type": "text", "text": "D Technical Lemmas 17 ", "page_idx": 13}, {"type": "text", "text": "D.1 Lipschitz continuity from policies to occupancy measures 17   \nD.2 Strong convexity . . . 18   \nD.3 Lipschitz gradients . . . . 19   \nD.4 Dual smoothness and strong convexity . . 22   \nD.5 Regularity constants . . . 24   \nD.6 Suboptimality bounds . . . 24   \nD.7 Perturbation bounds . 25 ", "page_idx": 13}, {"type": "text", "text": "E Proof of claim in Example 3.3 26 ", "page_idx": 13}, {"type": "text", "text": "F Proof of Theorem 3.10 28 ", "page_idx": 13}, {"type": "text", "text": "G Proof of Theorem 3.11 30 ", "page_idx": 13}, {"type": "text", "text": "H Estimating principal angles 30 ", "page_idx": 13}, {"type": "text", "text": "I Proof of Theorem 4.1 30 ", "page_idx": 13}, {"type": "text", "text": "J Suboptimal experts 33 ", "page_idx": 13}, {"type": "text", "text": "K Experimental details 34 ", "page_idx": 13}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Overview Here, we provide an overview of some of the most important notations. However, every notation is defined when it is introduced as well. ", "page_idx": 14}, {"type": "text", "text": "Table 1: Notations. ", "page_idx": 14}, {"type": "table", "img_path": "l5wEQPcDab/tmp/6bf9fc1cbe0f3d084978c501bdf63a606221310210f9b299ee73ffd85d7531a4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Additional definitions In the following, we briefly recall some additional definitions. To this en   \nwe denote $\\mathcal{B}(x,r):=\\{x\\in\\mathbb{R}^{n}:\\|x\\|_{2}<r\\}$ for an open ball of radius $r$ with center $x$ .   \nDefinition A.1 (Interior). The interior of a set $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ is defined as $\\operatorname{int}\\mathcal{X}:=\\{x\\in\\mathcal{X}:\\mathcal{B}(x,r)\\subseteq\\mathcal{X}$ for some $r>0\\}$ .   \nDefinition A.2 (Affine hull). The affine hull of a set $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ is defined as af $\\begin{array}{r}{\\mathcal{X}:=\\left\\{\\theta_{1}x_{1}+.\\,.\\,.+\\theta_{k}x_{k}:x_{1},\\dots,x_{k}\\in\\mathcal{X},\\theta_{1}+.\\,.\\,.+\\theta_{k}=1\\right\\}.}\\end{array}$   \nDefinition A.3 (Relative interior). The relative interior of a set $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ is defined as relint $\\mathcal{X}:=\\{\\boldsymbol{x}\\in\\mathcal{X}:B(\\boldsymbol{x},\\boldsymbol{r})\\cap$ aff ${\\mathcal{X}}\\subseteq{\\mathcal{X}}$ for some $r>0\\}$ .   \nDefinition A.4 (Relative boundary). The relative boundary of a closed set $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ is defined as relbd $\\mathcal{X}:=\\mathcal{X}\\setminus\\operatorname{relint}\\mathcal{X}$ .   \nDefinition A.5 (Convex hull). The convex hull of a set $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ is defined as conv $\\mathcal{X}:=\\left\\{\\theta_{1}x_{1}+.\\,.\\,.\\,+\\theta_{k}x_{k}:x_{1},\\dots,x_{k}\\in\\mathcal{X},\\theta_{1}+.\\,.\\,.\\,+\\theta_{k}=1,\\theta_{i}\\geq0,i=1,\\dots,k\\right\\},$ . ", "page_idx": 14}, {"type": "text", "text": "B Conjugate duality in regularized IRL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we first recall some background from convex analysis and then briefly discuss the duality between reward equivalence classes and optimal occupancy measures. ", "page_idx": 15}, {"type": "text", "text": "Definitions We recall a few definitions related to convex functions. In convex analysis it is standard to consider extended real value functions $f:\\mathbb{R}^{n}\\rightarrow\\overline{{\\mathbb{R}}}:=[-\\infty,\\infty]$ , where convex functions defined on some subset $\\mathcal{X}\\subset\\mathbb{R}^{n}$ are extended over the entire space by setting their value to $+\\infty$ outside of their domain. The effective domain is defined as do $\\operatorname{m}f:=\\{x:f(x)<\\infty\\}$ , and a convex function $f$ is said to be proper if $f>-\\infty$ and $\\operatorname{dom}f\\neq\\emptyset$ . Furthermore, $f$ is referred to as closed if its epigraph $\\{(x,y):x\\in\\operatorname{dom}f,y\\geq f(x)\\}$ is a closed set.2 In particular, $f$ is closed if it is continuous on $\\operatorname{dom}f$ and $\\operatorname{dom}f$ is a closed set [Boyd and Vandenberghe, 2004]. Lastly, we recall two key concepts in convex analysis \u2013 the subdifferential and the convex conjugate of some convex function. ", "page_idx": 15}, {"type": "text", "text": "Definition B.1 (Subdifferential). A subgradient of $f:\\mathbb{R}^{n}\\rightarrow\\overline{{\\mathbb{R}}}$ at some point $x\\in\\mathbb{R}^{n}$ is a vector $g\\in\\mathbb{R}^{n}$ such that $f(x^{\\prime})\\geq f(x)+g^{\\top}\\,({\\bar{x^{}}}-x)$ for all $x^{\\prime}\\in\\mathcal{X}$ . The subdifferential $\\partial f(x)$ at $x\\in\\mathscr{X}$ is the set of all subgradients at $x$ , where $\\partial f(x)$ is defined to be empty if $x\\not\\in\\operatorname{dom}f$ . ", "page_idx": 15}, {"type": "text", "text": "Definition B.2 (Convex Conjugate). The convex conjugate of $f:\\mathbb{R}^{n}\\,\\rightarrow\\,\\overline{{\\mathbb{R}}}$ is the function $f^{*}$ : $\\mathbb{R}^{n}\\to{\\overline{{\\mathbb{R}}}}$ defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{*}(y)=\\operatorname*{sup}_{x}\\langle y,x\\rangle-f(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Key results Next, we list two key results from convex analysis. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.3 ([Rockafellar, 1970]). A function $f\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\overline{{\\mathbb{R}}}$ is differentiable at some point $x\\in\\operatorname{dom}f$ if and only if $\\partial f(x)$ is singleton. In this case we have $\\partial f(x)=\\{\\nabla f(x)\\}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem B.4 ([Rockafellar, 1970]). For any proper convex function $f:\\mathbb{R}^{n}\\rightarrow\\overline{{\\mathbb{R}}}$ it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{*}(y)=\\langle y,x\\rangle-f(x)\\quad\\iff\\quad y\\in\\partial f(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If additionally $f$ is closed, then ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{*}(y)=\\langle y,x\\rangle-f(x)\\quad\\iff\\quad y\\in\\partial f(x)\\quad\\iff\\quad x\\in\\partial f^{*}(y).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Duality in IRL Let $f:\\mathbb{R}^{S\\times A}\\rightarrow\\overline{{\\mathbb{R}}}$ be given by $f:=\\bar{h}+\\delta_{\\mathcal{M}}$ , where $\\delta_{\\mathcal{M}}$ is a characteristic function defined as $\\delta_{\\mathcal{M}}(\\mu)=0$ if $\\mu\\in\\mathcal{M}$ and $\\delta_{\\mathcal{M}}(\\mu)=\\infty$ , otherwise. Since $f$ is closed proper convex, Theorem B.3 and B.4 imply that for a strictly convex $\\bar{h}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathsf{R L}(r)=\\nabla f^{*}(r)\\quad\\mathrm{~and~}\\quad|\\mathsf{R L}(\\mu)=\\partial f(\\mu)\\cap\\mathcal{R}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Additionally, under Assumption 2.1 and Slater\u2019s condition, which is ensured by Assumption 2.2, we have $\\partial f(\\mu)\\dot{=}\\nabla\\bar{h}(\\mu)+\\mathcal{U}$ [Schlaginhaufen and Kamgarpour, 2023]. Hence, $\\mu$ is optimal for $r$ if and only if $r\\in[\\nabla\\bar{h}(\\mu)]\\bar{u}$ . Therefore, there is a one-to-one mapping between elements of the quotient space $\\mathbb{R}^{S\\times A}/\\mathcal{U}$ , i.e. reward equivalence classes, and corresponding optimal occupancy measures in $\\mathcal{M}$ . This mapping is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla f^{*}:\\mathbb{R}^{S\\times A}/\\mathcal{U}\\rightarrow\\mathcal{M},[r]\\mathcal{u}\\mapsto\\nabla f^{*}(r)=\\mathsf{R L}(r),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and its inverse by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial f:\\mathcal{M}\\to\\mathbb{R}^{S\\times A}/\\mathcal{U},\\mu\\mapsto\\partial f(\\mu)=\\nabla\\bar{h}(\\mu)+\\mathcal{U}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Regularizers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We dedicate this section to discuss optimal policies in regularized MDPs and to recall their explicit form for Shannon and Tsallis entropy regularization. ", "page_idx": 15}, {"type": "text", "text": "Optimal policies Throughout the appendix, it will convenient to use the notation $\\pi_{s}:=\\pi(\\cdot|s)$ . Given some proper closed strongly convex policy regularizer $h$ , it can be shown [Geist et al., 2019] that the optimal policy, $\\pi^{*}$ , satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi_{s}^{*}=\\nabla h^{*}(q_{s}^{*})=\\underset{\\pi_{s}\\in\\Delta_{\\cal A}}{\\mathrm{argmax}}\\langle\\pi_{s},q_{s}^{*}\\rangle-h(\\pi_{s}),\\;\\forall s\\in{\\cal S},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $q_{s}^{*}\\in\\mathbb{R}^{A}$ is the optimal $q$ -function defined via ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{s}^{*}(a):=q^{*}(s,a):=\\operatorname*{max}_{\\pi\\in\\Delta_{\\mathcal{A}}^{s}}\\mathbb{E}_{\\boldsymbol\\pi}\\left[r(s_{t},a_{t})+\\sum_{t\\geq1}\\gamma^{t}\\left[r(s_{t},a_{t})-h(\\pi_{s})\\right]\\bigg|s_{0}=s,a_{0}=a\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we discuss the explicit form of $\\pi_{s}^{*}=\\nabla h^{*}(q_{s}^{*})$ for the specific cases of Shannon and Tsallis- $\\cdot1/2$ entropy regularization. ", "page_idx": 16}, {"type": "text", "text": "Shannon entropy For some $\\tau\\,>\\,0$ , we define the Shannon entropy regularizer as $h:=-\\tau\\mathcal{H}$ , where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\pi_{s}):=-\\sum_{a}\\pi_{s}(a)\\log\\pi_{s}(a),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is the Shannon entropy satisfying $0\\leq\\mathcal{H}\\leq\\log|\\mathcal{A}|$ . It can be shown that $h$ is $\\tau$ -strongly convex with respect to $\\left\\Vert\\cdot\\right\\Vert_{1}$ [Cover, 1999], and the optimal policy satisfies [Geist et al., 2019] ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s)=\\frac{\\exp\\left(q^{*}(s,a)/\\tau\\right)}{\\sum_{a^{\\prime}}\\exp\\left(q^{*}(s,a^{\\prime})/\\tau\\right)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Tsallis entropy For some parameter $\\alpha\\in\\mathbb R$ , the Tsallis entropy, $\\mathcal{H}_{\\alpha}$ , is defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\alpha}(\\pi_{s}):=\\frac{1}{\\alpha-1}\\left(1-\\sum_{a}\\pi_{s}(a)^{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the limit $\\alpha\\rightarrow1$ , the Tsallis entropy equals the Shannon entropy defined above. However, in this paper, we use Tsallis entropy to refer to the choice $\\alpha=1/2$ , which is often adopted as regularization in multi-armed bandit and, more recently, policy optimization algorithms [Zimmert and Seldin, 2021, Dann et al., 2023]. That is, we consider $\\begin{array}{r}{h(\\pi_{s})=-\\tau\\mathcal{H}_{1/2}(\\pi_{s})=-2\\tau\\left(\\sum_{a}\\sqrt{\\pi_{s}(a)}-1\\right)}\\end{array}$ for some $\\tau>0$ . We have $0\\leq-h\\leq2\\tau\\left(\\sqrt{|A|}-1\\right)$ and it can be shown that the optimal policy satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s)=\\left(\\frac{\\tau}{x_{s}-q^{*}(s,a)}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $x_{s}$ is a normalization parameter such that $\\begin{array}{r}{\\sum_{a}\\pi^{*}(a|s)\\,=\\,1}\\end{array}$ [Zimmert and Seldin, 2021]. Furthermore, since $h$ has diagonal Hessian $\\nabla^{2}h(\\pi_{s})_{a,a}=\\tau/(2\\pi_{s}(a)^{3/2})$ , it is $\\tau/2$ -strongly convex with respect to $\\lVert\\cdot\\rVert_{2}$ and hence also $\\tau/(2|A|)$ -strongly convex with respect to $\\left\\Vert\\cdot\\right\\Vert_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "D Technical Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we show several new technical results that are required for the proofs of our main theorems. ", "page_idx": 16}, {"type": "text", "text": "D.1 Lipschitz continuity from policies to occupancy measures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition D.1. Let Assumption 2.2 hold. For any $\\mu_{1},\\mu_{2}\\in\\mathcal{M}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(1-\\gamma\\right)\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}\\leq\\operatorname*{max}_{s}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}\\leq\\frac{2}{\\nu_{m i n}}\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. To show the first inequality, we decompose ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mu_{1}-\\mu_{2}\\|_{1}\\leq\\displaystyle\\sum_{s,a}|\\nu_{1}(s)(\\pi^{\\mu_{1}}(a|s)-\\pi^{\\mu_{2}}(a|s))|+\\displaystyle\\sum_{s,a}|(\\nu_{1}(s)-\\nu_{2}(s))\\pi^{\\mu_{2}}(a|s)|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\operatorname*{max}_{s}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}+\\|\\nu_{1}-\\nu_{2}\\|_{1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used the triangle and H\u00f6lder\u2019s inequality. From the Bellman flow constraints, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nu(s)=\\gamma\\sum_{s^{\\prime},a^{\\prime}}P(s|s^{\\prime},a^{\\prime})\\mu(s^{\\prime},a^{\\prime})+(1-\\gamma)\\nu_{0}(s),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nu_{1}-\\nu_{2}\\|_{1}=\\gamma\\displaystyle\\sum_{s}\\left\\vert\\sum_{s^{\\prime},a^{\\prime}}P(s|s^{\\prime},a^{\\prime})(\\mu_{1}(s^{\\prime},a^{\\prime})-\\mu_{2}(s^{\\prime},a^{\\prime}))\\right\\vert}\\\\ &{\\qquad\\qquad\\leq\\gamma\\displaystyle\\sum_{s^{\\prime},a^{\\prime}\\leq\\nu}\\sum_{s}P(s|s^{\\prime},a^{\\prime})\\,|\\mu_{1}(s^{\\prime},a^{\\prime})-\\mu_{2}(s^{\\prime},a^{\\prime})|}\\\\ &{\\qquad\\qquad=\\gamma\\,\\|\\mu_{1}-\\mu_{2}\\|_{1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we again used the triangle inequality. Hence, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s}\\left\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\right\\|_{1}\\geq\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}-\\left\\|\\nu_{1}-\\nu_{2}\\right\\|_{1}\\geq\\left(1-\\gamma\\right)\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To show the second inequality, we use the reverse triangle inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mu_{1}-\\mu_{2}\\|_{1}\\geq\\displaystyle\\sum_{s,a}|\\nu_{1}(s)(\\pi^{\\mu_{1}}(a|s)-\\pi^{\\mu_{2}}(a|s))|-\\displaystyle\\sum_{s,a}|(\\nu_{1}(s)-\\nu_{2}(s))\\pi^{\\mu_{2}}(a|s)|}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s}\\nu_{1}(s)\\,\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}-\\|\\nu_{1}-\\nu_{2}\\|_{1}\\,,}\\\\ &{\\qquad\\qquad\\geq\\nu_{\\operatorname*{min}}\\operatorname*{max}_{s}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}-\\gamma\\,\\|\\mu_{1}-\\mu_{2}\\|_{1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the last step we used again that $\\left\\|\\nu_{1}-\\nu_{2}\\right\\|_{1}\\leq\\gamma\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}$ . By rearranging terms we arrive at the desired inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}\\leq\\frac{1+\\gamma}{\\nu_{\\operatorname*{min}}}\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}\\leq\\frac{2}{\\nu_{\\operatorname*{min}}}\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.2 Strong convexity ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Next, we show that strong convexity of the policy regularizer translates into strong convexity in the occupancy measure. ", "page_idx": 17}, {"type": "text", "text": "Proposition D.2 (Strong convexity). Let Assumption 2.2 hold and suppose that $h$ is $\\eta_{h}$ -strongly convex with respect to the $\\left\\|\\cdot\\right\\|_{1}$ norm. Then, $\\bar{h}$ is $\\eta_{h}\\nu_{m i n}/H_{\\gamma}^{2}$ -strongly convex with respect to $\\left\\Vert\\cdot\\right\\Vert_{1}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We need to show that for $\\alpha\\in(0,1),\\bar{\\alpha}=1-\\alpha$ and any two $\\mu_{1},\\mu_{2}\\in\\mathcal{M}$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{h}(\\alpha\\mu_{1}+\\bar{\\alpha}\\mu_{2})\\leq\\alpha\\bar{h}(\\mu_{1})+\\bar{\\alpha}\\bar{h}(\\mu_{2})-\\frac{\\alpha\\bar{\\alpha}\\eta}{2}\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $\\eta=\\eta_{h}\\nu_{\\mathrm{min}}/H_{\\gamma}^{2}$ . To this end, we start similarly as in the proof of strict convexity by Schlaginhaufen and Kamgarpour [2023], but use $\\nu(s)\\geq\\nu_{\\mathrm{min}}$ and strong convexity of $h$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{s\\neq s}\\left(\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)\\right)h\\left(\\frac{\\alpha\\mu_{1}(s)+\\tilde{\\alpha}\\mu_{2}(s)}{\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)}\\right)}\\\\ &{=\\displaystyle\\sum_{s\\neq s}\\left(\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)\\right)h\\left(\\frac{\\alpha\\mu_{1}(s)}{\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)}\\frac{\\nu_{1}(s)}{\\nu_{1}(s)}+\\frac{\\tilde{\\alpha}\\mu_{2}(s)\\cdot\\nu_{2}(s)}{\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)}\\frac{\\nu_{2}(s)}{\\nu_{2}(s)}\\right)}\\\\ &{=\\displaystyle\\sum_{s\\neq s}\\left(\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)\\right)h\\left(\\frac{\\alpha\\nu_{1}(s)}{\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)}\\frac{\\pi^{\\mu_{1}}}{s^{\\alpha_{1}}}+\\frac{\\tilde{\\alpha}\\nu_{2}(s)}{\\frac{1-\\tilde{\\alpha}\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)}{1-\\tilde{\\beta}\\nu_{2}}}\\pi_{\\quad\\times}^{\\mu_{2}}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{s\\neq s}\\left(\\alpha\\nu_{1}(s)+\\tilde{\\alpha}\\nu_{2}(s)\\right)\\left(\\beta_{s}h\\left(\\frac{\\alpha\\mu_{1}^{\\alpha}}{\\alpha\\nu_{1}}\\right)+(1-\\beta_{s})h\\left(\\frac{\\pi^{\\mu_{2}}}{\\alpha\\nu_{1}}\\right)-\\frac{\\beta_{s}(1-\\beta_{s})\\eta_{1}}{2}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}^{2}\\right)}\\\\ &{=\\displaystyle\\sum_{s\\neq s}\\left(\\alpha\\nu_{1}(s)h\\left(\\frac{\\mu^{\\mu_{1}}}{\\alpha\\nu_{1}}\\right)+\\tilde{\\alpha}\\nu_{2}(s)h\\left(\\pi_{s}^{\\mu_{2}}\\right)-\\frac{\\alpha\\tilde{\\alpha}\\eta_{h}}{2}h\\left(s\\right)\\frac{\\nu_\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From here on, we use that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s}\\frac{\\nu_{1}(s)\\nu_{2}(s)}{\\alpha\\nu_{1}(s)+\\bar{\\alpha}\\nu_{2}(s)}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}^{2}}\\\\ &{\\displaystyle=\\sum_{s}\\underbrace{\\frac{\\operatorname*{max}\\left\\{\\nu_{1}(s),\\nu_{2}(s)\\right\\}}{\\alpha\\nu_{1}(s)+\\bar{\\alpha}\\nu_{2}(s)}}_{\\geq1}\\underbrace{\\operatorname*{min}\\left\\{\\nu_{1}(s),\\nu_{2}(s)\\right\\}}_{\\geq\\nu_{m i n}}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}^{2}}\\\\ &{\\displaystyle\\geq\\sum_{s}\\nu_{\\operatorname*{min}}\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\|_{1}^{2}}\\\\ &{\\displaystyle\\geq\\nu_{\\operatorname*{min}}\\operatorname*{max}\\left\\|\\pi_{s}^{\\mu_{1}}-\\pi_{s}^{\\mu_{2}}\\right\\|_{1}^{2}}\\\\ &{\\displaystyle\\geq\\nu_{\\operatorname*{min}}\\langle H_{x}^{2}\\|\\mu_{1}-\\mu_{2}\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used Proposition D.1 in the last step. Plugging the inequality (8) back into (7) concludes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D.3 Lipschitz gradients ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we show how we can get bounds on the Lipschitz constant $L_{\\mathcal{K}}$ . To this end, we first need to lower bound the optimal policies. ", "page_idx": 18}, {"type": "text", "text": "Policy lower bounds The following proposition establishes a lower bound for optimal policies with Shannon and Tsallis entropy regularization. ", "page_idx": 18}, {"type": "text", "text": "Proposition D.3. Let $H_{\\gamma}\\,=\\,1/(1-\\gamma)$ and $r_{\\mathrm{max}}:=\\|r\\|_{\\infty}$ . Then, we have the following lower bounds: ", "page_idx": 18}, {"type": "text", "text": "Proof. Recall the formula for the optimal policies in Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Part $a$ ): Since, $-r_{\\mathrm{max}}H_{\\gamma}\\leq q^{*}(s,a)\\leq\\left(r_{\\mathrm{max}}+\\tau\\log\\left|A\\right|\\right)H_{\\gamma}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\pi^{*}(a|s)\\ge\\frac{\\exp\\big(-r_{\\mathrm{max}}H_{\\gamma}/\\tau\\big)}{\\big|A\\big|\\exp\\big(\\big(r_{\\mathrm{max}}+\\tau\\log|A|\\big)\\,H_{\\gamma}/\\tau\\big)}}}\\\\ &{}&{=\\frac{\\exp\\big(-r_{\\mathrm{max}}H_{\\gamma}/\\tau\\big)}{\\big|A\\big|^{1+H_{\\gamma}}\\exp\\big(r_{\\mathrm{max}}H_{\\gamma}/\\tau\\big)}=\\frac{\\exp\\big(-2r_{\\mathrm{max}}H_{\\gamma}/\\tau\\big)}{\\big|A\\big|^{1+H_{\\gamma}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Part $b$ ): The proof of b) is similar to [Ouhamma and Kamgarpour, 2023, Lemma 8]. However, our settings are slightly different. Recall that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s)=\\left(\\frac{\\tau}{x_{s}-q^{*}(s,a)}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Ouhamma and Kamgarpour [2023, Lemma 10] we have $\\tau\\leq x_{s}-\\|q_{s}^{*}\\|_{\\infty}\\leq\\tau\\sqrt{|A|}$ . Furthermore, it holds that $-r_{\\mathrm{max}}H_{\\gamma}\\leq q^{*}(s,a)\\leq\\left(r_{\\mathrm{max}}+2\\tau\\sqrt{|A|}\\right)H_{\\gamma}$ . Hence, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n0<x_{s}-q^{*}(s,a)\\leq\\tau\\sqrt{|A|}+\\left(r_{\\operatorname*{max}}+2\\tau\\sqrt{|A|}\\right)H_{\\gamma}+r_{\\operatorname*{max}}H_{\\gamma}\\leq\\left(2r_{\\operatorname*{max}}+3\\tau\\sqrt{|A|}\\right)H_{\\gamma},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which yields the desired lower bound. ", "page_idx": 19}, {"type": "text", "text": "We also highlight the following result, which shows that if the policy $\\pi^{\\mu}$ is lower bounded on some set $\\kappa\\subset\\mathcal{M}$ , then it is also lower bounded on its convex hull conv $\\kappa$ . ", "page_idx": 19}, {"type": "text", "text": "Proposition D.4. Suppose $\\mu=\\alpha\\mu_{1}+(1-\\alpha)\\mu_{2}$ with $\\alpha\\in(0,1)$ and $\\mu_{1},\\mu_{2}\\in\\mathcal{M}$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{s}^{\\mu}=\\underbrace{\\frac{\\alpha\\nu_{1}(s)}{\\alpha\\nu_{1}(s)+\\bar{\\alpha}\\nu_{2}(s)}}_{\\beta_{s}}\\pi_{s}^{\\mu_{1}}+\\underbrace{\\frac{\\bar{\\alpha}\\nu_{2}(s)}{\\alpha\\nu_{1}(s)+\\bar{\\alpha}\\nu_{2}(s)}}_{1-\\beta_{s}}\\pi_{s}^{\\mu_{2}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\beta_{s}\\in(0,1)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof follows immediately from the proof of Proposition D.2. ", "page_idx": 19}, {"type": "text", "text": "Hessian upper bounds In the following, we establish upper bounds for the Hessians of the occupancy measure regularizations, $\\bar{h}$ , resulting from Shannon and Tsallis entropy regularization of the policy. In particular, we aim to upper-bound the maximum norm of the Hessian in terms of the smallest entry of the policy. ", "page_idx": 19}, {"type": "text", "text": "Proposition D.5. Let Assumption 2.2 hold. Consider $\\mu\\in\\mathcal{M}$ and let $\\pi_{m i n}=\\operatorname*{min}_{s,a}\\pi^{\\mu}(a|s)>0$ . Then, the Hessian of $\\bar{h}$ is upper bounded as follows: ", "page_idx": 19}, {"type": "text", "text": "Here, $\\|\\cdot\\|_{\\infty}$ denotes the maximum norm $\\left\\|A\\right\\|_{\\infty}=\\operatorname*{max}_{i j}\\left|A_{i j}\\right|$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. As shown by Schlaginhaufen and Kamgarpour [2023, Proposition B.2], the gradient of $\\bar{h}$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\bar{h}(\\mu)(s,a)=h(\\pi_{s}^{\\mu})+\\nabla h(\\pi_{s}^{\\mu})(a)-\\langle\\nabla h(\\pi_{s}^{\\mu}),\\pi_{s}^{\\mu}\\rangle.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\pi^{\\mu}(s,a)}{\\partial\\mu(s^{\\prime},a^{\\prime})}=\\delta_{s,s^{\\prime}}\\cdot\\frac{\\delta_{a,a^{\\prime}}-\\pi^{\\mu}(a|s)}{\\nu(s)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the above two formulas, we can calculate the Hessians explicitly. ", "page_idx": 19}, {"type": "text", "text": "Part a): For the Shannon entropy it holds by (9) that $\\nabla\\bar{h}(\\mu)(s,a)=\\tau\\log\\pi^{\\mu}(a|s)$ . Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\bar{h}(\\mu)}{\\partial\\mu(s^{\\prime},a^{\\prime})\\partial\\mu(s,a)}=\\tau\\cdot\\frac{1}{\\pi^{\\mu}(a|s)}\\cdot\\delta_{s,s^{\\prime}}\\cdot\\frac{\\delta_{a,a^{\\prime}}-\\pi^{\\mu}(a|s)}{\\nu(s)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\nabla^{2}\\bar{h}(\\mu)_{(s^{\\prime},a^{\\prime}),(s,a)}\\right|=\\left|\\frac{\\partial^{2}\\bar{h}(\\mu)}{\\partial\\mu(s^{\\prime},a^{\\prime})\\partial\\mu(s,a)}\\right|\\leq\\frac{\\tau}{\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Part $^b$ ): For the Tsallis entropy it holds by (9) that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla\\bar{h}(\\mu)(s,a)=-\\tau\\left(\\sum_{a^{\\prime\\prime}}\\sqrt{\\pi^{\\mu}(a^{\\prime\\prime}|s)}+\\frac{1}{\\sqrt{\\pi^{\\mu}(a|s)}}-2\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the second derivative is bounded as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial^{2}\\tilde{h}(\\mu)}{\\partial\\mu(s^{\\prime},a^{\\prime})\\partial\\mu(s,a)}\\bigg|=}&{\\!\\!\\!\\!-\\tau\\cdot\\delta_{s,s^{\\prime}}\\cdot\\left(\\sum_{\\alpha^{\\prime}}\\frac{1}{2\\sqrt{\\pi^{\\mu}(a^{\\prime}|s)}}\\frac{\\delta_{a^{\\prime},a^{\\prime\\prime}}-\\pi^{\\mu}(a^{\\prime\\prime}|s)}{\\nu(s)}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\frac{1}{2\\pi^{\\mu}(a|s)^{3/2}}\\frac{\\delta_{a,a^{\\prime}}-\\pi^{\\mu}(a|s)}{\\nu(s)}\\right)\\bigg|}\\\\ &{=\\frac{\\tau\\cdot\\delta_{s,s^{\\prime}}}{2\\nu(s)}\\left|\\frac{1}{\\sqrt{\\pi^{\\mu}(a^{\\prime}|s)}}-\\sum_{\\alpha^{\\prime}}\\sqrt{\\pi^{\\mu}(a^{\\prime\\prime}|s)}-\\frac{\\delta_{a,a^{\\prime}}}{\\pi^{\\mu}(a|s)^{3/2}}+\\frac{1}{\\sqrt{\\pi^{\\mu}(a|s)}}\\right|}\\\\ &{\\leq\\frac{\\tau}{2\\nu_{\\mathrm{min}}}\\left(\\underbrace{\\frac{1}{\\sqrt{\\pi^{\\mu}(a^{\\prime}|s)}}-\\sum_{\\alpha^{\\prime}}\\sqrt{\\pi^{\\mu}(a^{\\prime\\prime}|s)}}_{(A)}\\right)+\\underbrace{\\left|\\frac{\\delta_{a,a^{\\prime}}}{\\pi^{\\mu}(a|s)^{3/2}}-\\frac{1}{\\sqrt{\\pi^{\\mu}(a|s)}}\\right|}_{(B)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, since $\\begin{array}{r}{\\sum_{a}\\sqrt{\\pi^{\\mu}(a|s)}\\leq\\sqrt{A}\\leq1/\\sqrt{\\pi_{\\operatorname*{min}}}}\\end{array}$ , we have $(A)+(B)\\leq1/\\sqrt{\\pi_{\\mathrm{min}}}+1/\\pi_{\\mathrm{min}}^{3/2}\\leq2/\\pi_{\\mathrm{min}}^{3/2}$ which yield s the desired result ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\nabla^{2}\\bar{h}(\\mu)_{(s^{\\prime},a^{\\prime}),(s,a)}\\right|\\leq\\tau/\\left(\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min}}^{3/2}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lipschitz gradients Next, we provide explicit Lipschitz constants $L_{\\mathcal{K}}$ for $\\nabla\\bar{h}$ corresponding to Shannon and Tsallis entropy regularization. ", "page_idx": 20}, {"type": "text", "text": "Proposition D.6 (Lipschitz gradients). Consider some closed convex set $\\kappa\\subset\\mathcal{M}$ and suppose $\\begin{array}{r}{\\pi_{m i n}=\\operatorname*{min}_{\\mu\\in K}\\operatorname*{min}_{s,a}\\pi^{\\mu}(a|s)>0}\\end{array}$ . Then, the gradient of $\\bar{h}$ is Lipschitz continuous over $\\kappa$ i.e. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla\\bar{h}(\\mu_{1})-\\nabla\\bar{h}(\\mu_{2})\\right\\|_{2}\\leq L_{K}\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{2},\\quad\\forall\\mu_{1},\\mu_{2}\\in\\mathcal{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the respective Lipschitz constants are as follows: ", "page_idx": 20}, {"type": "text", "text": "a) If $\\begin{array}{r}{\\dot{h}=-\\tau\\mathcal{H}}\\end{array}$ , then $L_{K}=\\tau|S||A|/(\\nu_{m i n}\\pi_{m i n})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Defining $h=\\mu_{2}-\\mu_{1}$ , Lipschitz continuity follows from ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\nabla\\bar{h}(\\mu_{1})-\\nabla\\bar{h}(\\mu_{2})\\right|\\|_{2}\\stackrel{(i)}{\\leq}\\sqrt{|{\\mathcal{S}}||{\\mathcal{A}}|}\\left||\\nabla\\bar{h}(\\mu_{1})-\\nabla\\bar{h}(\\mu_{2})\\right|\\right|_{\\infty}}&{}\\\\ {\\stackrel{(i i)}{=}\\sqrt{|{\\mathcal{S}}||{\\mathcal{A}}|}\\left|\\left|\\int_{0}^{1}\\nabla^{2}\\bar{h}(\\mu_{1}+t h)h d t\\right|\\right|_{\\infty}}&{}\\\\ {\\stackrel{(i i i)}{\\leq}\\sqrt{|{\\mathcal{S}}||{\\mathcal{A}}|}\\int_{0}^{1}\\left||\\nabla^{2}\\bar{h}(\\mu_{1}+t h)h|\\right|_{\\infty}d t}&{}\\\\ {\\stackrel{(i v)}{\\leq}\\sqrt{|{\\mathcal{S}}||{\\mathcal{A}}|}\\int_{0}^{1}\\left||\\nabla^{2}\\bar{h}(\\mu_{1}+t h)\\right|\\right|_{\\infty}\\|h\\|_{1}\\,d t}&{}\\\\ {\\stackrel{(v)}{\\leq}\\sqrt{|{\\mathcal{S}}||{\\mathcal{A}}|}\\int_{0}^{1}\\left||\\nabla^{2}\\bar{h}(\\mu_{1}+t h)\\right|\\|_{\\infty}\\|h\\|_{1}\\,d t}&{}\\\\ {\\stackrel{(v)}{\\leq}||{\\mathcal{S}}||{\\mathcal{A}}|\\displaystyle|\\operatorname*{max}_{0\\leq t\\leq1}\\left||\\nabla^{2}\\bar{h}(\\mu_{1}+t h)\\right|\\right|_{\\infty}\\|\\mu_{1}-\\mu_{2}\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, we used in $(i)$ and $(v)$ that $\\left\\|x\\right\\|_{1}\\leq{\\sqrt{n}}\\left\\|x\\right\\|_{2}\\leq n\\left\\|x\\right\\|_{\\infty}$ for $x\\in\\mathbb{R}^{n}$ , and in $(i i)$ we applied the fundamental theorem of calculus. Moreover, $(i i i)$ follows from $\\left|\\int f\\right|\\leq\\int\\left|f\\right|$ , and $(i v)$ from H\u00f6lder\u2019s inequality. Now, by convexity $\\mu_{1},\\mu_{2}\\in\\mathcal{K}$ implies that $\\mu_{1}+t h\\in K$ for $t\\in[0,1]$ . Hence, plugging in the upper bounds from Proposition D.5 concludes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "D.4 Dual smoothness and strong convexity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Next, we show that the convex conjugate, $f^{*}$ , of the extended real value function $f:=\\bar{h}+\\delta_{\\mathcal{M}}$ (see Appendix B) is \u2013 if understood as a mapping from $\\mathbb{R}^{S\\times A}/\\mathcal{U}$ to $\\mathbb{R}$ \u2013 both smooth and strongly convex on $\\mathcal{R}$ with respect to the quotient norm. While it is well-known that global smoothness and strong convexity are dual properties [Rockafellar and Wets, 2009, Proposition 12.60], the key challenge for proving dual strong convexity is that $\\bar{h}$ has only locally Lipschitz gradients (see Proposition D.6). Proposition D.7 below shows that $\\eta$ -strong convexity of $\\bar{h}$ implies dual $1/\\eta$ -smoothness and locally Lipschitz gradients imply dual $\\sigma_{\\mathcal{R}}$ -strong convexity on $\\mathcal{R}$ for some $\\sigma_{\\mathscr R}>0$ . Moreover, we provide explicit lower bounds on $\\sigma_{\\mathcal{R}}$ for Shannon and Tsallis entropy entropy regularization. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.7. Let $f^{*}$ be the convex conjugate of $f:=\\bar{h}+\\delta_{\\mathcal{M}}$ . Then, the following holds: ", "page_idx": 21}, {"type": "text", "text": "a) Suppose that $\\bar{h}$ is $\\eta$ -strongly convex over $\\mathcal{M}$ , that is for all $\\mu,\\mu^{\\prime}\\in\\mathcal{M}$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{h}(\\mu^{\\prime})\\geq\\bar{h}(\\mu)+\\langle\\nabla\\bar{h}(\\mu),\\mu^{\\prime}-\\mu\\rangle+\\frac{\\eta}{2}\\left\\Vert\\mu-\\mu^{\\prime}\\right\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then we have for all $r,r^{\\prime}\\in\\mathbb{R}^{S\\times A}$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\nf^{*}(r^{\\prime})\\leq f(r)+\\langle\\nabla f^{*}(r),r^{\\prime}-r\\rangle+\\frac{1}{2\\eta}\\left\\|[r^{\\prime}]\\varkappa-[r]\\varkappa\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "b) Suppose that for any closed convex subset $\\kappa\\subset$ relint $\\mathcal{M}$ , there is some $L_{\\cal K}>0$ such that for all $\\mu,\\mu^{\\prime}\\in\\mathcal{K}$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla\\bar{h}(\\mu)-\\nabla\\bar{h}(\\mu^{\\prime})\\right\\|_{2}\\leq L\\kappa\\left\\|\\mu-\\mu^{\\prime}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then we have for all $r,r^{\\prime}\\in\\mathcal{R}$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\nf^{*}(r^{\\prime})\\geq f^{*}(r)+\\left\\langle\\nabla f^{*}(r),r^{\\prime}-r\\right\\rangle+\\frac{\\sigma_{\\mathcal{R}}}{2}\\left\\|[r^{\\prime}]\\boldsymbol{u}-[r]\\boldsymbol{u}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $\\sigma_{\\mathscr R}>0$ . ", "page_idx": 21}, {"type": "text", "text": "c) Let $H_{\\gamma}:=1/(1-\\gamma)$ , $R:=\\operatorname*{max}_{r\\in{\\mathcal{R}}}\\left\\|r\\right\\|_{\\infty}$ , $D=\\operatorname*{max}_{r,r^{\\prime}\\in\\mathcal{R}}\\left\\|r-r^{\\prime}\\right\\|_{2}$ , and suppose that $\\tau<D$ , then for the Shannon entropy the inequality (11) holds with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma_{\\mathcal{R}}=\\frac{\\nu_{m i n}\\exp\\left(\\frac{-2R H_{\\gamma}}{\\tau}\\right)}{2D|\\boldsymbol{S}||\\boldsymbol{A}|^{2+H_{\\gamma}}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and for the Tsallis entropy with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma_{\\mathcal{R}}=\\frac{\\nu_{m i n}}{2\\sqrt{2}D|S||A|\\left(\\left(2R/\\tau+3\\sqrt{|A|}\\right)H_{\\gamma}\\right)^{3}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Part $a$ ): Since $f$ is $\\eta$ -strongly convex with respect to $\\left\\|\\cdot\\right\\|_{2}$ , the convex conjugate $f^{*}$ is $1/\\eta$ - smooth with respect to the dual norm in $\\mathbb{R}^{S\\times A}/\\mathcal{U}$ [Rockafellar and Wets, 2009, Proposition 12.60], which is equivalent to (10). ", "page_idx": 21}, {"type": "text", "text": "Part $^b$ ): The show b), we closely follow [Goebel and Rockafellar, 2008, Theorem 4.1], but we need to account for the quotient spaces. We define the sets $\\begin{array}{r}{K=\\nabla f^{*}({\\mathcal R})\\,=\\,{\\mathsf{R L}}({\\mathcal R})}\\end{array}$ and ${\\sf K}_{\\epsilon}\\,=$ $\\operatorname{conv}(K)+\\epsilon(\\mathcal{B}\\cap\\operatorname{aff}(\\mathcal{M}))$ , where $\\dot{\\boldsymbol{B}}\\subset\\mathbb{R}^{S\\times A}$ denotes the closed unit ball with respect to $\\lVert\\cdot\\rVert_{2}$ and $\\epsilon>0$ is chosen such that $\\mathcal{K}_{\\epsilon}\\subset\\mathrm{relint}\\,\\mathcal{M}$ . Moreover, we let $L$ be the Lipschitz constant of $\\nabla\\bar{h}$ over $\\kappa_{\\epsilon}$ . Now, consider $r\\in\\mathcal{R}$ and $\\mu=\\nabla f^{*}(r)$ . Then, for any $r^{\\prime}\\in\\mathcal{R}$ , we have ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{*}(r^{\\prime})=\\displaystyle\\operatorname*{sup}_{\\hat{\\mu}\\in\\mathcal{K}_{r}}\\left[\\langle r^{\\prime},\\hat{\\mu}\\rangle-f(\\hat{\\mu})\\right]}\\\\ &{\\overset{(i)}{\\geq}\\displaystyle\\operatorname*{sup}_{\\hat{\\mu}\\in\\mathcal{K}_{r}}\\left[\\langle r^{\\prime},\\hat{\\mu}\\rangle-f(\\hat{\\mu})\\right]}\\\\ &{\\overset{(i i)}{\\geq}\\displaystyle\\operatorname*{sup}_{\\hat{\\mu}\\in\\mathcal{K}_{r}}\\left[\\langle r^{\\prime},\\hat{\\mu}\\rangle-f(\\mu)-\\langle r,\\hat{\\mu}-\\mu\\rangle-\\frac{L}{2}\\left\\Vert\\vec{\\mu}-\\mu\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\overset{(i i i)}{=}\\displaystyle\\langle r,\\mu\\rangle-f(\\mu)+\\displaystyle\\operatorname*{sup}_{\\hat{\\mu}\\in\\mathcal{K}_{r}}\\left[\\langle r^{\\prime}-r,\\hat{\\mu}\\rangle-\\frac{L}{2}\\left\\Vert\\vec{\\mu}-\\mu\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\overset{(i v)}{=}f^{*}(r)+\\displaystyle\\operatorname*{sup}_{\\hat{\\mu}\\in\\mathcal{K}_{r}}\\left[\\langle r^{\\prime}-r,\\hat{\\mu}\\rangle-\\frac{L}{2}\\left\\Vert\\vec{\\mu}-\\mu\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\overset{(e)}{=}f^{*}(r)+\\displaystyle\\operatorname*{sup}_{\\hat{\\mu}\\in\\mathcal{K}_{r}}\\left[\\langle r^{\\prime}-r,\\hat{\\mu}\\rangle-\\frac{L}{2}\\left\\Vert\\vec{\\mu}-\\mu\\right\\Vert_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, $(i)$ follows from $K_{\\epsilon}\\subset\\mathbb{R}^{S\\times A}$ , $(i i)$ from the fact that Lipschitz gradients imply that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(\\bar{\\mu})\\leq f(\\mu)+\\langle g,\\bar{\\mu}-\\mu\\rangle+\\frac{L}{2}\\left\\|\\bar{\\mu}-\\mu\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $g\\in\\partial f(\\mu)$ [Beck, 2017, Lemma 5.7] and $r\\in\\partial f(\\mu)$ (see Theorem B.4). Moreover, $(i i i)$ and $(v)$ follow from rearranging terms, and $(i v)$ from $f^{*}(\\ddot{r})=\\langle r,\\mu\\rangle-f(\\mu)$ . Now, consider any $\\alpha>0$ such that $\\sigma_{\\mathscr R}=2(\\alpha-L\\dot{\\alpha}^{2}/2)>0$ and $\\alpha D\\leq\\epsilon$ . Setting $\\bar{\\mu}\\in\\mathcal{K}_{\\epsilon}$ in the above supremum to $(\\bar{\\mu}-\\mu)=\\alpha\\Pi_{\\mathcal{U}^{\\perp}}(r^{\\prime}-r)$ yields the desired result ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{*}(r^{\\prime})\\geq f^{*}(r)+\\langle r^{\\prime}-r,\\nabla f^{*}(r)\\rangle+\\alpha\\langle r^{\\prime}-r,\\Pi_{\\mathcal{U}^{\\bot}}(r^{\\prime}-r)\\rangle-\\displaystyle\\frac{L\\alpha^{2}}{2}\\left\\|\\Pi_{\\mathcal{U}^{\\bot}}(r^{\\prime}-r)\\right\\|_{2}^{2}}\\\\ &{\\qquad=f^{*}(r)+\\langle r^{\\prime}-r,\\nabla f^{*}(r)\\rangle+\\displaystyle\\frac{\\sigma_{\\mathcal{R}}}{2}\\left\\|[r^{\\prime}]u-[r]u\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that we indeed have $\\bar{\\mu}\\in\\mathcal{K}_{\\epsilon}$ as $\\mu\\in\\mathcal{K}$ and $\\left\\|\\mu-\\bar{\\mu}\\right\\|_{2}\\leq\\alpha\\left\\|r-r^{\\prime}\\right\\|_{2}\\leq\\alpha D\\leq\\epsilon.$ ", "page_idx": 22}, {"type": "text", "text": "Part $c$ ): To get an explicit constant for $\\sigma_{\\mathcal{R}}$ , we need to appropriately choose $\\epsilon$ and calculate the corresponding Lipschitz constant. To this end, we first recall that according to Proposition D.3 and D.4 policies corresponding to occupancy measures in conv $\\kappa$ are, for Shannon and Tsallis entropy, lower bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{min},\\,\\mathrm{Sh}}=\\frac{\\exp\\left(-2R H_{\\gamma}/\\tau\\right)}{|A|^{1+H_{\\gamma}}}\\quad\\mathrm{and}\\quad\\pi_{\\mathrm{min},\\,\\mathrm{Ts}}=\\left(\\left(2R/\\tau+3\\sqrt{|A|}\\right)H_{\\gamma}\\right)^{-2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "respectively. Furthermore, for any $\\mu\\in\\mathcal{K}_{\\epsilon}$ , we have by Proposition D.1 and equivalence of norms ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi^{\\mu}(a|s)\\geq\\pi_{\\mathrm{min}}-\\frac{2\\sqrt{|S||A|}}{\\nu_{\\mathrm{min}}}\\epsilon=\\pi_{\\mathrm{min}}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, by setting $\\epsilon=\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min}}/(4\\sqrt{|S||A|})$ , we have $\\pi^{\\mu}(a|s)\\geq\\pi_{\\operatorname*{min}}^{\\prime}=\\pi_{\\operatorname*{min}}/2$ for any $\\mu\\in\\mathcal{K}_{\\epsilon}$ . As for the Lipschitz constant over $\\kappa_{\\epsilon}$ , we have by Proposition D.6 ", "page_idx": 22}, {"type": "equation", "text": "$$\nL_{\\mathrm{Sh}}=\\frac{\\tau|S||\\mathcal{A}|}{\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min,\\,Sh}}^{\\prime}}\\quad\\mathrm{and}\\quad L_{\\mathrm{Ts}}=\\frac{\\tau|S||\\mathcal{A}|}{\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min,\\,Ts}}^{\\prime}3/2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for the Shannon and Tsallis entropy, respectively. Now, we need to ensure that $\\alpha\\,>\\,0$ such that $\\sigma_{\\mathscr R}=2(\\alpha-L\\alpha^{2}/2)>0$ and $\\alpha\\leq\\epsilon/D$ . To that end, we set for both regularizations $\\alpha=\\tau/(L D)$ , which in light of (13) ensures that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha=\\frac{\\tau}{L D}\\leq\\frac{\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min}}^{\\prime}}{D|S||A|}\\leq\\frac{\\nu_{\\mathrm{min}}\\pi_{\\mathrm{min}}^{\\prime}}{2D\\sqrt{|S||A|}}=\\frac{\\epsilon}{D},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $|S|,|A|\\geq2$ . Moreover, we get the dual strong convexity constant ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma_{\\mathcal{R}}=2\\left(\\frac{\\tau}{L D}-\\frac{\\tau^{2}}{2L D^{2}}\\right)=\\frac{2\\tau}{L D}\\left(1-\\frac{\\tau}{2D}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is larger than $\\tau/L D$ for $\\tau<D$ . We can therefore choose $\\sigma_{\\mathscr{R}}\\,=\\,\\tau/L D$ as a dual strong convexity constant. Plugging in the Lipschitz constants for the two regularizations yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{\\mathcal{R},\\mathrm{{Sh}}}=\\frac{\\pi_{\\operatorname*{min},\\mathrm{{Sh}}}^{\\prime}\\nu_{\\operatorname*{min}}}{D|S||A|}=\\frac{\\pi_{\\operatorname*{min},\\mathrm{{Sh}}}\\nu_{\\operatorname*{min}}}{2D|S||A|}=\\frac{\\exp\\left(-2R H_{\\gamma}/\\tau\\right)\\nu_{\\operatorname*{min}}}{2D|S||A|^{2+H_{\\gamma}}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for the Shannon entropy, and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{\\mathcal{R},\\mathrm{Ts}}=\\frac{\\pi_{\\mathrm{min},\\mathrm{Ts}}^{\\prime}{}^{3/2}\\nu_{\\mathrm{min}}}{D|S||A|}=\\frac{\\pi_{\\mathrm{min},\\mathrm{Ts}}^{3/2}\\nu_{\\mathrm{min}}}{2\\sqrt{2}D|S||A|}=\\frac{\\nu_{\\mathrm{min}}}{2\\sqrt{2}D|S||A|\\left(\\left(2R/\\tau+3\\sqrt{|A|}\\right)H_{\\gamma}\\right)^{3}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for the Tsallis entropy. ", "page_idx": 23}, {"type": "text", "text": "Remark D.8 (Large $\\tau$ regime). Note that if $\\tau\\geq2D/\\sqrt{|S||A|}$ , we can set $\\sigma_{\\mathscr R}=\\alpha=1/L$ , while still satisfying the condition (14). This leads to the strong convexity constants ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{\\mathcal{R},\\mathrm{Sh}}=\\frac{\\exp\\left(-2R H_{\\gamma}/\\tau\\right)\\nu_{\\mathrm{min}}}{2\\tau|S||A|^{2+H_{\\gamma}}},~\\sigma_{\\mathcal{R},\\mathrm{Ts}}=\\frac{\\nu_{\\mathrm{min}}}{2\\sqrt{2}\\tau|S||A|\\left(\\left(2R/\\tau+3\\sqrt{|A|}\\right)H_{\\gamma}\\right)^{3}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.5 Regularity constants ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the following Proposition, we summarize the regularity constants for Shannon and Tsallis entropy regularization. We highlight that these constants are lower bounds for $\\eta$ and $\\sigma_{\\mathcal{R}}$ . ", "page_idx": 23}, {"type": "text", "text": "Proposition D.9. Let $H_{\\gamma}\\,:=\\,1/(1-\\gamma)$ , $R:=\\left.\\operatorname*{max}_{r\\in{\\mathcal{R}}}\\left\\|r\\right\\|_{\\infty}\\right.$ , and $\\begin{array}{r}{D\\,=\\,\\mathrm{max}_{r,r^{\\prime}\\in{\\mathcal{R}}}\\left\\|r-r^{\\prime}\\right\\|_{2}}\\end{array}$ . Suppose that $\\tau<D$ , then for the Shannon entropy, Assumption 3.5 holds with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta=\\tau\\nu_{m i n}/H_{\\gamma}^{2}\\quad a n d\\quad\\sigma_{\\mathcal{R}}=\\frac{\\exp\\left(-2R H_{\\gamma}/\\tau\\right)\\nu_{m i n}}{2D|\\mathcal{S}||\\mathcal{A}|^{2+H_{\\gamma}}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and for the Tsallis entropy with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta=\\tau\\nu_{m i n}/(2H_{\\gamma}^{2}|A|)\\quad a n d\\quad\\sigma_{\\mathcal{R}}=\\frac{\\nu_{m i n}}{2\\sqrt{2}D|S||A|\\left(\\left(2R/\\tau+3\\sqrt{|A|}\\right)H_{\\gamma}\\right)^{3}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The derivation for $\\eta$ is given in Proposition D.2 and for $\\sigma_{\\mathcal{R}}$ in Proposition D.7 above. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D.6 Suboptimality bounds ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition 3.4. Under Assumptions 2.1 and 2.2, we have $\\ell(r^{\\prime},\\mu)=D_{\\bar{h}}(\\mu,\\mathsf{R L}(r^{\\prime}))$ for any $\\mu\\in\\mathcal{M}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\mu=\\mathsf{R L}(r)$ . We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(r,\\mu^{\\prime})=\\langle r,\\mu-\\mu^{\\prime}\\rangle-\\bar{h}(\\mu)+\\bar{h}(\\mu^{\\prime})}\\\\ &{\\qquad\\qquad=\\langle\\nabla\\bar{h}(\\mu),\\mu-\\mu^{\\prime}\\rangle-\\bar{h}(\\mu)+\\bar{h}(\\mu^{\\prime})}\\\\ &{\\qquad\\qquad=D_{\\bar{h}}(\\mu^{\\prime},\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second equality holds, as by (4) we have $r-\\nabla\\bar{h}(\\mu)\\in\\mathcal{U}$ , and $\\mu-\\mu^{\\prime}\\in\\mathcal{U}^{\\perp}$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 3.6. Suppose Assumptions 2.1,2.2, and 3.5 hold, and let $r,r^{\\prime}\\in\\mathcal{R}$ . Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{\\mathcal{R}}}{2}\\left\\|[r]_{\\mathcal{U}}-[r^{\\prime}]_{\\mathcal{U}}\\right\\|_{2}^{2}\\leq\\ell(r^{\\prime},\\mathsf{R L}(r))=D_{\\bar{h}}\\left(\\mathsf{R L}(r),\\mathsf{R L}(r^{\\prime})\\right)\\leq\\frac{1}{2\\eta}\\left\\|[r]_{\\mathcal{U}}-[r^{\\prime}]_{\\mathcal{U}}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some problem-dependent constant $\\sigma_{\\mathscr R}>0$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $f:=\\bar{h}+\\delta_{\\mathcal{M}}$ and $\\mu=\\mathsf{R L}(r),\\mu^{\\prime}=\\mathsf{R L}(r^{\\prime})$ . We then have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\bar{h}}(\\mu,\\mu^{\\prime})\\overset{(i)}{=}f(\\mu)-f(\\mu^{\\prime})-\\langle r^{\\prime},\\mu-\\mu^{\\prime}\\rangle}\\\\ &{\\overset{(i i)}{=}f^{*}(r^{\\prime})-\\langle r^{\\prime},\\mu^{\\prime}\\rangle-f^{*}(r)+\\langle r,\\mu\\rangle-\\langle r^{\\prime},\\mu-\\mu^{\\prime}\\rangle}\\\\ &{\\overset{(i i i)}{=}f^{*}(r^{\\prime})-f^{*}(r)-\\langle r^{\\prime}-r,\\nabla f^{*}(r)\\rangle=D_{f^{*}}(r^{\\prime},r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, $(i)$ follows from the definition of $f$ and $r^{\\prime}\\in[\\nabla\\bar{h}(\\mu^{\\prime})]u$ , in $(i i)$ we use that $f(\\mu)=\\left\\langle r,\\mu\\right\\rangle-$ $f^{*}(r)$ and $f(\\mu^{\\prime})=\\langle r^{\\prime},\\mu^{\\prime}\\rangle-f^{\\ast}(r^{\\prime})$ , and $(i i i)$ follows from rearranging terms and $\\mu=\\nabla f^{*}(r)$ . The result then follows from dual strong convexity and smoothness as established in Proposition D.7. ", "page_idx": 24}, {"type": "text", "text": "Note that without steep regularization it is impossible to lower bound the suboptimality in terms of reward distances in $\\mathbb{R}^{\\hat{S}\\times\\tilde{A}}/\\mathcal{U}$ (Proposition 3.4 doesn\u2019t hold). However, we still have the following upper bound. ", "page_idx": 24}, {"type": "text", "text": "Proposition D.10. Consider an arbitrary regularization and let $\\mu\\in\\mathsf{R L}(r),\\mu^{\\prime}\\in\\mathsf{R L}(r^{\\prime})$ . Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell(r,\\mu^{\\prime})\\leq2\\left\\|[r]\\_{\\mathcal{U}}-[r^{\\prime}]_{\\mathcal{U}}\\right\\|_{\\infty}\\leq2\\left\\|[r]_{\\mathcal{U}}-[r^{\\prime}]_{\\mathcal{U}}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{r^{\\prime\\prime}:=\\mathrm{argmin}_{\\tilde{r}\\in[r^{\\prime}]_{\\mathcal{U}}}\\left\\lVert\\tilde{r}-r\\right\\rVert_{\\infty}}\\end{array}$ , then the following holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(r,\\mu^{\\prime})=\\displaystyle\\operatorname*{max}_{\\mu\\in\\mathcal{M}}J(r,\\mu)-J(r,\\mu^{\\prime})}\\\\ &{\\qquad\\quad\\overset{(i)}{\\leq}\\displaystyle\\left|\\operatorname*{max}_{\\mu\\in\\mathcal{M}}J(r,\\mu)-\\operatorname*{max}_{\\mu\\in\\mathcal{M}}J(r^{\\prime\\prime},\\mu)\\right|+|J(r^{\\prime\\prime},\\mu^{\\prime})-J(r,\\mu^{\\prime})|}\\\\ &{\\qquad\\quad\\overset{(i i)}{\\leq}\\displaystyle\\operatorname*{max}_{\\mu\\in\\mathcal{M}}|\\langle r-r^{\\prime\\prime},\\mu\\rangle|+|\\langle r-r^{\\prime\\prime},\\mu^{\\prime}\\rangle|}\\\\ &{\\qquad\\quad\\overset{(i i i)}{\\leq}2\\|r-r^{\\prime\\prime}\\|_{\\infty}\\overset{(i v)}{=}2\\|[r]u-[r^{\\prime}]u\\|_{\\infty}\\leq2\\left\\|[r]u-[r^{\\prime}]u\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $(i)$ follows from the triangle inequality and optimality of $\\mu^{\\prime}$ , $(i i)$ from $\\left|\\operatorname*{max}{f}-\\operatorname*{max}{g}\\right|\\,\\leq$ max $|f-g|$ and simplifying, $(i i i)$ from H\u00f6lder\u2019s inequality, and $(i v)$ from the definition of $r^{\\prime\\prime}$ and the quotient norm. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "D.7 Perturbation bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Next, we provide a bound quantifying the change in the quotient norm when changing the generating subspace. ", "page_idx": 24}, {"type": "text", "text": "Proposition D.11. Consider $x,y\\in\\mathbb{R}^{n}$ and two subspaces $\\mathcal{V},\\mathcal{W}\\subset\\mathbb{R}^{n}$ of dimension $m<n$ . Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|[x]_{\\mathcal{W}}-[y]_{\\mathcal{W}}\\|_{2}\\leq\\|\\Pi_{\\mathcal{W}}-\\Pi_{\\mathcal{V}}\\|\\cdot\\|x-y\\|_{2}+\\|[x]_{\\mathcal{V}}-[y]_{\\mathcal{V}}\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\|\\Pi_{\\mathcal{W}}-\\Pi_{\\mathcal{V}}\\|=\\sin\\left(\\theta_{\\operatorname*{max}}(\\mathcal{V},\\mathcal{W})\\right)$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. The result follows from the triangle inequality and the definition of the spectral norm: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|[x]w-[y]w\\|_{2}=\\|\\Pi_{\\mathcal{W}^{\\perp}}(x-y)\\|_{2}}&{}\\\\ {=\\|(\\Pi_{\\mathcal{W}^{\\perp}}-\\Pi_{\\mathcal{V}^{\\perp}})(x-y)+\\Pi_{\\mathcal{V}^{\\perp}}(x-y)\\|_{2}}&{}\\\\ {\\leq\\|(\\Pi_{\\mathcal{W}^{\\perp}}-\\Pi_{\\mathcal{V}^{\\perp}})(x-y)\\|_{2}+\\|\\Pi_{\\mathcal{V}^{\\perp}}(x-y)\\|_{2}}&{}\\\\ {=\\|(\\Pi_{\\mathcal{W}}-\\Pi_{\\mathcal{V}})(x-y)\\|_{2}+\\|[x]\\nu-[y]\\nu\\|_{2}}&{}\\\\ {\\leq\\|\\Pi_{\\mathcal{W}}-\\Pi_{\\mathcal{V}}\\|\\cdot\\|x-y\\|_{2}+\\|[x]\\nu-[y]\\nu\\|_{2}\\,.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, for a proof of $\\|\\Pi_{\\mathcal{W}}-\\Pi_{\\mathcal{V}}\\|=\\sin\\left(\\theta_{\\operatorname*{max}}(\\mathcal{V},\\mathcal{W})\\right)$ we refer to [Drmac, 2000]. ", "page_idx": 24}, {"type": "text", "text": "The following proposition shows that the maximal principal angle between two transition laws can be upper bounded by the spectral norm difference of the transition laws. ", "page_idx": 24}, {"type": "text", "text": "Proposition 3.9. Let $P,P^{\\prime}\\,\\in\\,\\Delta_{S}^{s\\times A}$ and $H_{\\gamma}\\,=\\,1/(1-\\gamma)$ . Then, we have $\\theta_{1}(P,P^{\\prime})\\,=\\,0$ and $\\sin\\left(\\theta_{\\mathrm{max}}(P,P^{\\prime})\\right)\\leq\\gamma H_{\\gamma}\\sqrt{|S|/|A|}\\,\\left\\|P-P^{\\prime}\\right\\|$ , where $\\lVert\\cdot\\rVert$ denotes the spectral norm. ", "page_idx": 24}, {"type": "text", "text": "Before proceeding with the proof of Proposition 3.9, we need the following technical result. ", "page_idx": 24}, {"type": "text", "text": "Proposition D.12. For any $P\\in\\Delta_{S\\times A}^{S}$ the smallest singular value of $E-\\gamma P$ satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}\\,(E-\\gamma P)\\geq\\sqrt{|A|/|S|}(1-\\gamma).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The main idea of the proof is to use that $\\begin{array}{r}{\\sigma_{\\mathrm{min}}(A)=\\operatorname*{min}_{x\\neq0}\\left\\|A x\\right\\|_{2}/\\left\\|x\\right\\|_{2}}\\end{array}$ for any matrix $A\\in\\mathbb{R}^{n\\times m}$ . We first lower bound $\\sigma_{\\operatorname*{min}}(I-\\gamma P_{a})=\\sigma_{\\operatorname*{min}}\\left(I-\\gamma(P_{a})^{\\top}\\right)$ , where $P_{a}$ denotes the state transition matrix under action $a$ . Let $x\\in\\mathbb{R}^{S}$ , then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(I-\\gamma(P_{a})^{\\top}\\right)x\\right\\|_{2}\\geq1/\\sqrt{|S|}\\left\\|\\left(I-\\gamma(P_{a})^{\\top}\\right)x\\right\\|_{1}}\\\\ &{~~~~~~~~~~~~~~~~~~\\geq1/\\sqrt{|S|}\\left(\\left\\|x\\right\\|_{1}-\\gamma\\left\\|(P_{a})^{\\top}x\\right\\|_{1}\\right)}\\\\ &{~~~~~~~~~~~~~~~~~~~~~\\geq(1-\\gamma)/\\sqrt{|S|}\\left\\|x\\right\\|_{1}\\geq(1-\\gamma)/\\sqrt{|S|}\\left\\|x\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the third inequality follows from ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|(P_{a})^{\\top}x\\right\\|_{1}=\\sum_{s}\\left|\\sum_{s^{\\prime}}P_{a}(s|s^{\\prime})x(s^{\\prime})\\right|\\leq\\sum_{s}\\sum_{s^{\\prime}}P_{a}(s|s^{\\prime})\\left|x(s^{\\prime})\\right|=\\left\\|x\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, $\\sigma_{\\mathrm{min}}(I-\\gamma P_{a})\\geq(1-\\gamma)/\\sqrt{|S|}$ . Therefore, we have for any $x\\in\\mathbb{R}^{S}$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|(E-\\gamma P)x\\right\\|_{2}^{2}=\\sum_{a}\\left\\|(I-\\gamma P_{a})x\\right\\|_{2}^{2}\\geq|A|/|S|(1-\\gamma)^{2}\\left\\|x\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which yields the desired result. ", "page_idx": 25}, {"type": "text", "text": "We are now ready to prove Proposition 3.9. ", "page_idx": 25}, {"type": "text", "text": "Proof of Proposition 3.9. The first principal angle $\\theta_{1}(P,P^{\\prime})=0$ is zero as we always have $\\mathbf{1}\\subseteq\\mathcal{U}$ . The bound on the maximal angle follows from a well-known perturbation result for orthogonal projections. Namely, if $A,B\\,\\,\\,\\bar{\\in}\\,\\,\\mathbb{R}^{n\\times m}$ are matrices of the same rank and $\\Pi_{A},\\Pi_{B}$ denote the orthogonal projections onto their column span, then we have [Ji-Guang, 1987] ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\Pi_{A}-\\Pi_{B}\\|\\leq\\operatorname*{min}\\left\\{\\left\\|A^{\\dagger}\\right\\|,\\left\\|B^{\\dagger}\\right\\|\\right\\}\\left\\|A-B\\right\\|,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $A^{\\dagger}$ denotes the Moore-Penrose inverse [Penrose, 1955]. Recall that $\\mathcal{U}_{P}\\,=\\,\\mathrm{im}(E-\\gamma P)$ , $\\sin\\left(\\theta_{\\operatorname*{max}}(P,P^{\\prime})\\right)=\\left\\|\\Pi_{\\mathcal{U}_{P}}-\\Pi_{\\mathcal{U}_{P^{\\prime}}}\\right\\|$ , and by Proposition D.12 ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|(E-\\gamma P)^{\\dagger}\\right\\|=\\left(\\sigma_{\\operatorname*{min}}\\left(E-\\gamma P\\right)\\right)^{-1}\\leq{\\sqrt{|S|/|A|}}H_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sin\\left(\\theta_{\\mathrm{max}}(P,P^{\\prime})\\right)\\leq\\sqrt{|S|/|A|}\\cdot\\gamma\\cdot H_{\\gamma}\\cdot\\left\\|P-P^{\\prime}\\right\\|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E Proof of claim in Example 3.3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We recall Example 3.3 from the main paper. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Example 3.3. We consider a two-state, two-action MDP with $\\ensuremath{\\boldsymbol{S}}=\\ensuremath{\\boldsymbol{\\mathcal{A}}}=\\{0,1\\}$ , uniform initial state distribution, discount rate $\\gamma=0.9$ , and Shannon entropy regularization $h=-\\mathcal{H}$ (see Appendix C). Suppose the expert reward is $r^{\\mathsf{E}}(s,a)\\,=\\,\\mathbb{1}\\{s\\,=\\,1\\}$ and consider the transition laws, $\\bar{P^{0}}$ and $P^{1}$ , defined by $P^{0}(\\bar{0}|s,a)=0.75$ and $P^{1}(0|s,a)=0.25+\\beta\\cdot\\mathbb{1}\\left\\{s=0,a=0\\right\\}$ for some $\\beta\\in[0,0.75]$ . Also, consider the two experts $\\mu_{P^{0}}^{\\mathsf{E}}=\\mathsf{R L}_{P^{0}}(r^{\\mathsf{E}})$ and $\\mu_{P^{1}}^{\\mathsf{E}}=\\mathsf{R L}_{P^{1}}(r^{\\mathsf{E}})$ , and suppose we recovered the reward $\\hat{r}(s,a)=-r^{\\mathsf{E}}$ . Then, the following holds: 1) We have $\\ell_{P^{0}}(\\hat{r},\\mu_{P^{0}}^{\\mathsf{E}})=0$ and $\\ell_{P^{1}}(\\hat{r},\\mu_{P^{1}}^{\\mathsf{E}})=$ $\\mathcal{O}(\\beta)$ . That is, for small $\\beta$ , the reward $\\hat{r}$ is a good solution to the IRL problem, as both experts are approximately optimal under $\\hat{r}$ . 2) The rank condition (5) between $\\bar{P}^{0}$ and $P^{1}$ is satisfied for any $\\beta\\,>\\,0$ . 3) For a new transition law $P$ defined by $P(0|s,a)\\:=\\:\\mathbb{1}\\left\\{s=1,a=0\\right\\}$ , we have $\\ell_{P}(r^{\\mathsf{E}},\\mathsf{R L}_{P}(\\hat{r}))\\approx4.81$ , i.e. RL $_P\\!\\left({\\hat{r}}\\right)$ performs poorly under the experts\u2019 reward. ", "page_idx": 25}, {"type": "text", "text": "In the following we prove the claims 1. and 2., while 3. is computed via regularized dynamic programming [Geist et al., 2019].3 Furthermore, we illustrate the occupancy measure spaces corresponding to $P^{0}$ and $P^{1}$ for different $\\beta$ in Figure 3. ", "page_idx": 25}, {"type": "image", "img_path": "l5wEQPcDab/tmp/94d18cfe6a22fdd4cfceb52517e7b65d4c75b1a294119e2e1ed7f8b69aeb825c.jpg", "img_caption": ["Figure 3: The set of occupancy measures $\\mathcal{M}_{P^{0}}$ and $\\mathcal{M}_{P^{1}}$ are illustrated in $\\mathbb{R}^{S\\times A}/\\mathbf{1}\\cong\\mathbf{1}^{\\perp}$ . For a two-state-two-action MDP, the set of occupancy measures is given by the intersection of a twodimensional affine subspace (a plane in $\\mathbb{R}^{S\\times\\lambda}/\\mathbf{1})$ with the probability simplex in $\\mathbb{R}^{4}$ (a tetrahedron in $\\mathbb{R}^{S\\times A}/\\mathbf{1}$ ). We see that for a small $\\beta$ , the sets $\\mathcal{M}_{P^{0}}$ and $\\mathcal{M}_{P^{1}}$ are approximately parallel. That is, the angle between their normal vectors, which span the potential shaping spaces $\\mathcal{U}_{P^{0}}$ and $\\mathcal{U}_{P^{1}}$ , is small. In contrast, for a large $\\beta$ the orientation of $\\mathcal{M}_{P^{0}}$ and $\\mathcal{M}_{P^{1}}$ is very different, resulting in a large angle between the corresponding normal vectors. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Consider the transition law $P^{\\prime}$ defined by $P^{\\prime}(0|s,a)=0.25.$ . First, we observe that while $\\left\\|\\boldsymbol{P}^{0}-\\boldsymbol{P}^{\\prime}\\right\\|$ is large, the potential shaping spaces $\\mathcal{U}_{P^{0}}$ and $\\mathcal{U}_{P^{\\prime}}$ coincide. To see this note that we have $P^{\\prime}(\\cdot|s,a)=P^{0}(\\cdot|s,a)+\\Delta$ , where $\\Delta=[-0.5,0.5]^{\\top}$ . Hence, we have for any $x\\in\\mathbb{R}^{2}$ that ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\boldsymbol{E}-\\gamma\\boldsymbol{P}^{\\prime})\\boldsymbol{x}=(\\boldsymbol{E}-\\gamma\\boldsymbol{P}^{0})\\boldsymbol{x}-\\gamma\\langle\\Delta,\\boldsymbol{x}\\rangle\\mathbf{1}_{4}=(\\boldsymbol{E}-\\gamma\\boldsymbol{P}^{0})\\left(x-\\frac{\\gamma\\langle\\Delta,\\boldsymbol{x}\\rangle}{1-\\gamma}\\mathbf{1}_{2}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ${\\mathbf{1}}_{n}$ denotes the all-one vector in $\\mathbb{R}^{n}$ . Therefore, $\\operatorname{span}(E-\\gamma P^{\\prime})=\\operatorname{span}(E-\\gamma P^{0})$ Moreover, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|P^{1}-P^{\\prime}\\right\\|\\leq{\\sqrt{\\sum_{s,s^{\\prime},a}(P^{1}(s^{\\prime}|s,a)-P^{\\prime}(s^{\\prime}|s,a))^{2}}}={\\sqrt{2}}\\beta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In light of Propositions D.10, D.11, and 3.9, this implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{P^{1}}\\bigl(\\hat{r},\\mathsf{R L}_{P^{1}}(r^{\\mathsf{E}})\\bigr)\\leq2\\left\\|[\\hat{r}]\\boldsymbol{u}_{P^{1}}-[r^{\\mathsf{E}}]\\boldsymbol{u}_{P^{1}}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\left\\|\\Pi_{\\mathcal{U}_{P^{1}}}-\\Pi_{\\mathcal{U}_{P^{\\prime}}}\\right\\|\\left\\|\\hat{r}-r^{\\mathsf{E}}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\gamma\\cdot H_{\\gamma}\\cdot\\|P-P^{\\prime}\\|\\leq2\\sqrt{2}\\gamma\\cdot H_{\\gamma}\\cdot\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. We need to show that $P^{0}$ and $P^{1}$ are satisfying the rank condition ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{rank}\\left(\\left[E-\\gamma P^{0},\\qquad E-\\gamma P^{1}\\right]\\right)=2|S|-1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the same reasoning as above, we can equivalently show the rank condition for the transition laws $P^{0^{\\prime}},P^{1^{\\prime}}$ defined by $P^{0^{\\prime}}(0|s,a)=1$ and $P^{1^{\\prime}}(0|s,a)=\\beta\\cdot\\mathbb{1}\\left\\{s=0,a=0\\right\\}$ . ", "page_idx": 26}, {"type": "text", "text": "To this end, we choose the matrix representation ", "page_idx": 27}, {"type": "equation", "text": "$$\nE=\\left[\\!\\!\\begin{array}{c}{{I}}\\\\ {{I}}\\end{array}\\right]\\quad\\mathrm{and}\\quad P=\\left[\\!\\!\\begin{array}{c}{{P_{a_{0}}}}\\\\ {{P_{a_{1}}}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $I\\in\\mathbb{R}^{|S|\\times|S|}$ is the identity matrix and $P_{a_{0}},P_{a_{1}}\\in\\mathbb{R}^{|S|\\times|S|}$ are the state transition matrices corresponding to the actions $0,1$ , respectively. Let $C=\\left[E-\\gamma P^{0^{\\prime}},\\right.\\;\\;\\;\\;\\;\\;E-\\gamma P^{1^{\\prime}}\\right]$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n{P^{0}}^{\\prime}=\\left[\\!\\!{\\begin{array}{r r}{1}&{0}\\\\ {1}&{0}\\\\ {1}&{0}\\\\ {1}&{0}\\end{array}}\\!\\!\\right]\\quad{\\mathrm{and}}\\quad{P^{1}}^{\\prime}=\\left[\\!\\!{\\begin{array}{r r}{\\beta}&{1-\\beta}\\\\ {0}&{1}\\\\ {0}&{1}\\\\ {0}&{1}\\end{array}}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\nC=\\left[\\!\\!\\begin{array}{c c c c}{1-\\gamma}&{0}&{1-\\beta\\gamma}&{-\\gamma+\\beta\\gamma}\\\\ {-\\gamma}&{1}&{0}&{1-\\gamma}\\\\ {1-\\gamma}&{0}&{1}&{-\\gamma}\\\\ {-\\gamma}&{1}&{0}&{1-\\gamma}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It\u2019s straightforward to see that the vector $\\ensuremath{[1\\mathrm{~\\ensuremath~{~1~}~}-1\\mathrm{~\\ensuremath~{~-1~}~}]}^{\\intercal}$ lies in the kernel of $C$ , but there is a $3\\times3$ submatrix with non-zero determinant: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left({\\left[\\!\\!\\begin{array}{l l l}{1-\\gamma}&{0}&{1-\\beta\\gamma}\\\\ {-\\gamma}&{1}&{0}\\\\ {1-\\gamma}&{0}&{1}\\end{array}\\!\\!\\right]}\\right)=1\\cdot[(1-\\gamma)-(1-\\gamma)(1-\\beta\\gamma)]=\\beta\\gamma(1-\\gamma)>0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In other words, we have rank $C=3$ for any $\\beta>0$ . ", "page_idx": 27}, {"type": "text", "text": "F Proof of Theorem 3.10 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 3.10. Let $K=2$ , $\\theta_{2}(P^{0},P^{1})>0,$ , and suppose that Assumptions 2.1,2.2, and 3.5 hold. If $\\ell_{P^{k}}(\\hat{r},\\mu_{P^{k}}^{E})\\leq\\hat{\\varepsilon}$ for $k=0,1,$ , then $\\hat{r}$ is $\\varepsilon$ -transferable to $\\mathcal{P}=\\Delta_{S}^{S\\times A}$ with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\varepsilon=\\hat{\\varepsilon}/\\left[\\eta\\sigma_{\\mathcal{R}}\\sin\\left(\\theta_{2}(P^{0},P^{1})\\,/2\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The proof of Theorem 3.10 hinges on Lemma 3.6 and the following reward approximation result. Lemma F.1. Let $\\left\\|[r^{E}]_{\\mathcal{U}_{P^{k}}}-[\\hat{r}]_{\\mathcal{U}_{P^{k}}}\\right\\|_{2}\\leq\\bar{\\varepsilon}$ for $k=0,1$ . Then, i $^{\\circ}\\theta_{2}(P^{0},P^{1})>0,$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\big\\|[r^{E}]_{\\mathbf{1}}-[\\hat{r}]_{\\mathbf{1}}\\big\\|_{2}\\leq\\frac{\\bar{\\varepsilon}}{\\sin\\big(\\theta_{2}(P^{0},P^{1})/2\\big)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma $F.l$ . Throughout this proof, we will use the short-hand notation $\\mathcal{U}_{k}:=\\mathcal{U}_{P^{k}}$ for $k=0,1$ . Recall that since $\\mathbf{1}\\subseteq\\mathcal{U}_{0}\\cap\\mathcal{U}_{1}$ , we have $\\theta_{1}(\\mathcal{U}_{0},\\mathcal{U}_{1})=0$ and by assumption we also have $\\theta_{2}(\\mathcal{U}_{0},\\mathcal{U}_{1})>0$ , which implies that $\\mathcal{U}_{0}\\cap\\mathcal{U}_{1}=\\mathbf{1}$ . Furthermore, since for $k=0,1$ we can rewrite $\\mathbb{R}^{S\\times A}$ as the orthogonal sum $\\mathbb{R}^{S\\times A}=\\mathcal{U}_{k}\\cap\\mathbf{1}^{+}\\oplus\\mathcal{U}_{k}^{\\perp}\\oplus\\mathbf{1}$ , we can uniquely decompose $\\boldsymbol{r}^{\\mathsf{E}}-\\boldsymbol{\\hat{r}}$ into $r^{\\mathsf{E}}-{\\hat{r}}=x_{k}+y_{k}+z.$ , where $x_{k}\\in\\mathcal{U}_{k}\\cap{\\bf1^{+}}$ , $y_{k}\\in\\mathcal{U}_{k}^{\\perp}$ , $\\boldsymbol{z}\\in\\mathbf{1}$ , for $k=0,1$ . Then, it holds that $x_{0}+y_{0}=x_{1}+y_{1}$ . Since $\\Big|\\Big|[r^{\\mathsf{E}}]_{P^{k}}-\\big[\\hat{r}\\big]_{P^{k}}\\,\\Big|\\Big|_{P^{k},2}=\\|y_{k}\\|_{2}$ , the Assumption of Lemma F.1 implies that $\\|y_{k}\\|_{2}\\leq\\bar{\\varepsilon}$ . For the 2-distance between the equivalence classes $[r^{\\mathsf{E}}]_{\\mathbf{1}}$ and $[\\hat{r}]_{\\mathbf{1}}$ the Pythagorean theorem implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|[r^{\\mathsf{E}}]_{1}-[\\hat{r}]_{1}\\right\\|_{1,2}^{2}=\\|x_{0}\\|_{2}^{2}+\\|y_{0}\\|^{2}=\\|x_{1}\\|_{2}^{2}+\\|y_{1}\\|_{2}^{2}\\leq}&{\\underset{\\underset{\\theta_{k}\\in\\mathcal{U}_{k}\\cap\\Omega_{k}\\in\\mathcal{U}_{k}^{+},\\ldots}{\\operatorname*{max}}}{\\operatorname*{max}},\\;\\;\\alpha_{0}^{2}+\\beta_{0}^{2},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{\\|u_{k}\\|_{2}=\\|v_{k}\\|_{2}=1}{\\alpha_{k}\\in\\mathbb{R}_{+}\\operatorname*{max}},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\alpha_{0}u_{0}\\!+\\!\\beta_{0}v_{0}\\!=\\!\\alpha_{1}u_{1}\\!+\\!\\beta_{1}v_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the upper bound follows from $x_{0}+y_{0}=x_{1}+y_{1}$ and $\\|y_{k}\\|_{2}\\leq\\bar{\\varepsilon}$ . Next, we want to show that the maximum on the right-hand side of (27) is achieved for $\\beta_{0}=\\beta_{1}=\\bar{\\varepsilon}$ . To see this, note that taking inner products between $u_{0}$ and $u_{1}$ , respectively, and the equation $\\alpha_{0}u_{0}+\\beta_{0}v_{0}=\\alpha_{1}u_{1}+\\beta_{1}v_{1}$ , we arrive at ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\alpha_{0}=\\alpha_{1}\\langle u_{0},u_{1}\\rangle+\\beta_{1}\\langle u_{0},v_{1}\\rangle,\\;\\alpha_{1}=\\alpha_{0}\\langle u_{0},u_{1}\\rangle+\\beta_{0}\\langle u_{1},v_{0}\\rangle,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is for any choice of $\\beta_{k},u_{k},v_{k},k=0,1$ an invertible linear system of equations for $\\alpha_{0},\\alpha_{1}$ with the solutions ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\alpha_{0}=\\frac{\\beta_{0}\\langle u_{0},u_{1}\\rangle\\langle u_{1},v_{0}\\rangle+\\beta_{1}\\langle u_{0},v_{1}\\rangle}{1-\\langle u_{0},u_{1}\\rangle^{2}},\\;\\alpha_{1}=\\frac{\\beta_{1}\\langle u_{1},u_{0}\\rangle\\langle u_{0},v_{1}\\rangle+\\beta_{0}\\langle u_{1},v_{0}\\rangle}{1-\\langle u_{1},u_{0}\\rangle^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\langle u_{0},u_{1}\\rangle<1$ , due to $\\mathcal{U}_{0}\\cap\\mathcal{U}_{1}\\cap\\mathbf{1}^{\\perp}=0$ . As the sign of $\\langle u_{0},u_{1}\\rangle\\langle u_{1},v_{0}\\rangle$ and $\\langle u_{0},v_{1}\\rangle$ can be chosen arbitrarily by an appropriate choice of $v_{0},v_{1}$ , the objective in the right-hand-side of (27) is increasing in $\\beta_{0},\\beta_{1}$ and hence the maximum is achieved for $\\beta_{0}=\\beta_{1}=\\bar{\\varepsilon}$ and $\\alpha:=\\alpha_{0}=\\alpha_{1}=$ 1\u03b5\u00af\u2212\u27e8\u27e8uu00,,vu11\u27e9\u27e9. Therefore, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|F_{\\lambda_{1}-}^{\\lambda_{2}}-F_{\\lambda_{1}\\|_{2}^{2}}\\le\\frac{2}{\\lambda_{2}\\eta_{1}\\eta_{2}}\\tan\\frac{\\alpha_{2}^{\\prime}}{\\eta_{3}}\\Bigg[\\lambda_{1}+\\bigg(\\frac{\\alpha_{1}\\eta_{1}}{1-(\\lambda_{2}\\eta_{1}-\\alpha_{2})\\eta_{1}}\\bigg)^{2}\\Bigg]}&{}\\\\ {\\|\\frac{\\alpha_{2}}{\\eta_{3}\\eta_{1}\\eta_{2}},\\ \\frac{\\alpha_{2}\\eta_{3}}{\\eta_{3}\\eta_{3}\\eta_{1}},\\ [\\lambda_{2}+\\bigg(\\frac{\\alpha_{2}\\eta_{3}}{1-(\\lambda_{3}\\eta_{1}-\\alpha_{2})\\eta_{1}+\\alpha_{3}}\\bigg)^{2}\\Bigg]}&{}\\\\ {\\|\\frac{\\alpha_{2}}{\\eta_{3}\\eta_{1}\\eta_{2}},\\ [\\frac{\\alpha_{2}\\eta_{3}}{\\eta_{3}\\eta_{1}},\\ [1+\\bigg(\\frac{\\alpha_{2}\\eta_{3}}{1-(\\lambda_{3}\\eta_{1}-\\alpha_{2})\\eta_{2}+\\alpha_{3}}\\bigg)^{2}\\Bigg]}&{}\\\\ {\\|\\frac{\\alpha_{2}}{\\eta_{3}\\eta_{1}\\eta_{2}},\\ [1+\\bigg(\\frac{\\alpha_{2}\\eta_{3}}{1-(\\lambda_{3}\\eta_{1}-\\alpha_{2})\\eta_{2}}\\bigg)^{2}\\Bigg]}&{}\\\\ {\\|\\frac{\\alpha_{2}}{\\eta_{3}\\eta_{1}\\eta_{2}},\\ [1+\\bigg(\\frac{\\alpha_{2}\\eta_{3}}{\\eta_{3}\\eta_{1}}\\bigg)^{2}\\bigg]}&{}\\\\ {\\|\\frac{\\alpha_{2}}{\\eta_{3}\\eta_{1}\\eta_{2}},\\ [1+\\bigg(\\frac{\\alpha_{2}\\eta_{3}}{\\eta_{3}\\eta_{1}}\\bigg)^{2}\\bigg]}&{}\\\\ {\\|\\frac{\\alpha_{2}}{\\eta_{3}\\eta_{1}\\eta_{2}},\\ [1+\\bigg(\\frac{\\alpha_{2}\\eta_{3}}{\\eta_{3}\\eta_{1}}\\bigg)^{3}\\bigg]}&{}\\\\\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, we took the maximum over $u_{1},v_{1}$ in $(i)$ , we used that $\\mathrm{max}_{v\\in\\mathcal{V},\\|v\\|_{2}=1}\\langle v,u\\rangle=\\|\\Pi_{\\mathcal{V}}u\\|_{2}$ in $(i i)$ , and $(i i i)$ follows from the Pythagorean theorem. Furthermore, $(i v)$ follows from $u_{0}\\in\\mathbf{1}^{\\perp}$ and $(v)$ from simplifying. In $(v i)$ we then again use $\\mathrm{max}_{v\\in\\mathcal{V},\\|v\\|_{2}=1}\\langle v,\\dot{u}\\rangle=\\|\\Pi_{\\mathcal{V}}u\\|_{2}$ , the definition of the second principal angle (Definition 3.8), and the fact that the first principal vectors lie in 1. Lastly, $(v i i)$ follows from simplifying and $(v i i i)$ from $\\sin(x/2)^{2}=(1-\\cos x)/2$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 3.10. As mentioned in the proof sketch in the main paper, it follows from the lower bound in Lemma 3.6 that $\\begin{array}{r}{\\left\\|[r^{\\mathsf{E}}]\\mathscr{u}_{P^{k}}-[\\hat{r}]\\mathscr{u}_{P^{k}}\\right\\|_{2}\\leq\\sqrt{2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}}}}\\end{array}$ . In light of Lemma F.1, this implies that for any $P\\in\\Delta_{S\\times A}^{S}$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\big\\|\\big[r^{\\mathsf{E}}\\big]_{\\mathcal{U}_{P}}-\\big[\\widehat{r}\\big]_{\\mathcal{U}_{P}}\\big\\|_{2}\\leq\\big\\|[r^{\\mathsf{E}}]_{\\mathbf{1}}-[\\widehat{r}]_{\\mathbf{1}}\\big\\|_{2}\\leq\\frac{\\sqrt{2\\widehat{\\varepsilon}/\\sigma_{\\mathcal{R}}}}{\\sin\\big(\\theta_{2}(P^{0},P^{1})/2\\big)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, applying the upper bound in Lemma 3.6 yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\ell_{P}(r^{\\mathsf{E}},\\mathsf{R L}_{P}(\\hat{r}))\\leq\\frac{1}{2\\eta}\\left\\|[r^{\\mathsf{E}}]_{\\mathcal{U}_{P}}-[\\hat{r}]_{\\mathcal{U}_{P}}\\right\\|_{2}^{2}\\leq\\frac{\\hat{\\varepsilon}}{\\eta\\sigma_{\\mathcal{R}}\\sin\\left(\\theta_{2}(P^{0},P^{1})/2\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "G Proof of Theorem 3.11 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem 3.11. Let $K=1$ , $\\begin{array}{r}{D:=\\operatorname*{max}_{r,r^{\\prime}\\in\\mathcal{R}}\\left\\|r-r^{\\prime}\\right\\|_{2},}\\end{array}$ , and suppose that Assumptions 2.1,2.2, and 3.5 hold. If \u2113P 0(r\u02c6, \u00b5E) \u2264\u03b5\u02c6, then r\u02c6 is \u03b5P -transferable to P \u2208\u2206SS\u00d7 with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\varepsilon_{P}=2\\operatorname*{max}\\left\\{2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}},D^{2}\\sin\\left(\\theta_{\\operatorname*{max}}(P^{0},P)\\right)^{2}\\right\\}/\\eta.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Similar to Theorem 3.10, it follows from Lemma 3.6 that $\\begin{array}{r}{\\|[r^{\\mathsf{E}}]\\boldsymbol{u}_{{\\boldsymbol{P}}^{0}}-[\\hat{r}]\\boldsymbol{u}_{{\\boldsymbol{P}}^{0}}\\|_{2}\\le\\sqrt{2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}}}}\\end{array}$ . By Proposition D.11, we then have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|[r^{\\mathsf{E}}]_{\\mathcal{U}_{P}}-[\\widehat{r}]_{\\mathcal{U}_{P}}\\right\\|_{2}\\leq\\sin\\left(\\theta_{\\operatorname*{max}}(P,P^{0})\\right)\\left\\|r^{\\mathsf{E}}-\\widehat{r}\\right\\|_{2}+\\left\\|[r^{\\mathsf{E}}]_{\\mathcal{U}_{P}0}-[\\widehat{r}]_{\\mathcal{U}_{P}0}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sin\\left(\\theta_{\\operatorname*{max}}(P,P^{0})\\right)D+\\sqrt{2\\widehat{\\varepsilon}/\\sigma_{\\mathcal{R}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, applying Lemma 3.6 again yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{P}\\left(r^{\\mathsf{E}},\\mathsf{R L}_{P}(\\hat{r})\\right)\\leq\\frac{1}{2\\eta}\\left\\|[r^{\\mathsf{E}}]_{\\mathcal{U}_{P}}-[\\hat{r}]_{\\mathcal{U}_{P}}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\left(D\\sin\\left(\\theta_{\\operatorname*{max}}(P,P^{0})\\right)+\\sqrt{2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}}}\\right)^{2}}{2\\eta}}\\\\ &{\\qquad\\qquad\\leq\\frac{2\\operatorname*{max}\\big\\{D^{2}\\sin\\left(\\theta_{\\operatorname*{max}}(P,P^{0})\\right)^{2},\\,2\\hat{\\varepsilon}/\\sigma_{\\mathcal{R}}\\big\\}}{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "H Estimating principal angles ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Consider two full rank matrices $A,B\\in\\mathbb{R}^{n\\times m}$ and let the columns of $U_{A},U_{B}\\in\\mathbb{R}^{n\\times m}$ form an orthonormal basis of $\\mathcal{V}=\\operatorname{im}A$ and $\\mathcal{W}=\\mathrm{im}\\,B$ , respectively. Then, as discussed by [Ji-Guang, 1987] we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{i}=\\cos(\\theta_{i}(\\mathcal{V},\\mathcal{W})),i=1,\\ldots,m,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $1\\geq\\sigma_{1}\\geq...\\geq\\sigma_{m}\\geq0$ denote the singular values of $U_{A}^{\\top}U_{B}$ sorted in decreasing order. Hence, given the transition matrices $P^{0},P^{1}$ , we can compute the principle angles $\\theta_{i}(P^{0},P^{1})$ by first computing orthonormal bases for the column spans of $E-\\gamma P_{i},i=1,2$ , and then computing the singular values as described above. ", "page_idx": 29}, {"type": "text", "text": "Now, suppose that $\\hat{P}^{0},\\hat{P}^{1}$ are empirical estimates of $P^{0},P^{1}$ , then we have by [Ji-Guang, 1987, Theorem 3.1] the following perturbation result ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{i}{\\operatorname*{max}}\\left|\\sin(\\theta_{i}(P^{0},P^{1}))-\\sin(\\theta_{i}(\\hat{P}^{0},\\hat{P}^{1}))\\right|\\le\\left\\|\\Pi_{\\mathcal{U}_{P^{0}}}-\\Pi_{\\mathcal{U}_{\\hat{P}^{0}}}\\right\\|+\\left\\|\\Pi_{\\mathcal{U}_{P^{1}}}-\\Pi_{\\mathcal{U}_{\\hat{P}^{1}}}\\right\\|}&{}\\\\ {\\le\\gamma H_{\\gamma}\\sqrt{|\\mathcal{S}|/|\\mathcal{A}|}\\left(\\left\\|P^{0}-\\hat{P}^{0}\\right\\|+\\left\\|P^{1}-\\hat{P}^{1}\\right\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from Propositions D.11 and 3.9. Hence, we can estimate $\\sin\\theta_{i}(P^{0},P^{1})$ up to an error of $\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\left\\|P^{0}-\\hat{P}^{0}\\right\\|,\\left\\|P^{1}-\\hat{P}^{1}\\right\\|\\right\\}\\right)$ ", "page_idx": 29}, {"type": "text", "text": "I Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem 4.1. Suppose that $N^{E}=\\Omega\\big(K\\log(|S||A|/\\hat{\\delta})/\\hat{\\varepsilon}^{2}\\big)$ and $H^{E}=\\Omega\\big(\\log(K/\\hat{\\varepsilon})/\\log(1/\\gamma)\\big)$ . Running Algorithm $^{\\,l}$ for $T=\\Omega\\big(K^{2}/\\hat{\\varepsilon}^{2}\\big)$ iterations with step-size $\\alpha=1/(K{\\sqrt{T}})$ , where $\\delta_{o p t}=$ ${\\mathcal O}\\big(\\hat{\\delta}\\hat{\\varepsilon}^{2}/K^{3}\\big)$ , $\\varepsilon_{o p t}\\,=\\,{\\mathcal O}(\\hat{\\varepsilon}/K).$ , $N\\,=\\,\\Omega\\big(K\\log(K|S||A|/(\\hat{\\delta}\\hat{\\varepsilon}))/\\hat{\\varepsilon}^{2}\\big)$ , and $H\\,=\\,H^{E}$ , it holds with probability at least $1-\\hat{\\delta}$ that $\\ell_{P^{k}}(\\hat{r},\\mu_{P^{k}}^{E})\\leq\\hat{\\varepsilon}$ , for $k=0,\\ldots,K-1$ . ", "page_idx": 29}, {"type": "text", "text": "The proof of Theorem 4.1 is inspired by [Syed and Schapire, 2007, Theorem 2]. However, in contrast to Syed and Schapire [2007], we consider the regularized problem with multiple experts, we use the suboptimality as the convergence metric, and we use a projected gradient descent update (instead of multiplicative weights). The proof hinges on Hoeffding\u2019s inequality and a regret bound for online gradient descent, which are provided in Theorem I.1 and I.2 below. ", "page_idx": 29}, {"type": "text", "text": "Theorem I.1 (Hoeffding\u2019s inequality [Hoeffding, 1963]). Let $X_{0},\\ldots,X_{M-1}$ be independent random variables with $X_{l}\\in[a,\\bar{b}]$ and let ${\\cal S}_{M}:=X_{0}+...+X_{M-1}$ . Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(|S_{M}-\\mathbb{E}S_{M}|\\geq c\\right)\\leq2\\exp\\left(-{\\frac{2c^{2}}{M(b-a)^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem I.2 (Online gradient descent [Zinkevich, 2003]). Consider some bounded closed convex set $\\mathcal{X}\\subset\\mathbb{R}^{n}$ with $D:=\\operatorname*{max}_{x,x^{\\prime}\\in\\mathcal{X}}\\left\\|x-x^{\\prime}\\right\\|_{2}$ . Moreover, let $\\Pi_{\\mathcal{X}}:\\mathbb{R}^{n}\\rightarrow\\mathcal{X}$ be the orthogonal projection onto $\\mathcal{X}$ . For any sequence of convex differentiable functions $f_{0},\\dotsc,f_{T-1}:\\mathcal{X}\\ \\rightarrow\\mathbb{R}$ satisfying $\\begin{array}{r}{\\operatorname*{max}_{x\\in\\mathcal{X}}\\|\\nabla f_{t}(x)\\|_{2}\\leq G,}\\end{array}$ , the online projected gradient descent update ", "page_idx": 30}, {"type": "equation", "text": "$$\n{x_{t+1}}\\gets\\Pi_{\\mathcal{X}}\\left({x_{t}}-\\alpha\\nabla{f_{t}}(x_{t})\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with step-size $\\alpha=D/(G{\\sqrt{T}})$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}f_{t}(x_{t})-\\operatorname*{min}_{x^{*}\\in\\mathcal{X}}\\sum_{t=0}^{T-1}f_{t}(x^{*})\\leq D G\\sqrt{T}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Theorem 4.1. The proof is in three steps. First, we use Hoeffding\u2019s inequality to prove concentration of the empirical occupancy measures around the true occupancy measures. Then, we use the union bound to upper bound the probability that any of our bounds fails to hold. Finally, we prove the convergence rate of Algorithm 1 using the regret bound in Theorem I.2. ", "page_idx": 30}, {"type": "text", "text": "Step 1: Let $\\mathcal{D}=\\left\\{\\left(s_{0},a_{0},\\ldots,s_{H-1},a_{H-1}\\right)\\right\\}_{i=0}^{N-1}$ be sampled from some policy $\\pi^{\\mu}$ and recall that the corresponding empirical occupancy measure is defined as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{\\mathcal{D}}(s,a)=\\frac{1-\\gamma}{N}\\sum_{i=0}^{N-1}\\sum_{t=0}^{H-1}\\gamma^{t}\\mathbb{1}\\{s_{t}^{i}=s,a_{t}^{i}=a\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It will be convenient to define the truncated occupancy measure ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mu_{H}(s,a)=(1-\\gamma)\\sum_{t=0}^{H-1}\\gamma^{t}\\mathbb{P}_{\\nu_{0}}^{\\pi^{\\mu}}\\{s_{t}^{i}=s,a_{t}^{i}=a\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For $K$ data sets $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{K}$ sampled from $\\pi^{\\mu_{k}}$ we then have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{r\\in\\mathcal{R}}\\sum_{k=0}^{K-1}\\langle r,\\mu_{k}-\\hat{\\mu}_{\\mathcal{D}_{k}}\\rangle\\overset{(i)}{\\leq}\\displaystyle\\operatorname*{max}_{r\\in\\mathcal{R}}\\left\\|r\\right\\|_{1}\\left\\|\\sum_{k=0}^{K-1}\\left(\\mu_{k}-\\hat{\\mu}_{\\mathcal{D}_{k}}\\right)\\right\\|_{\\infty}\\leq\\left\\|\\sum_{k=0}^{K-1}\\left(\\mu_{k}-\\hat{\\mu}_{\\mathcal{D}_{k}}\\right)\\right\\|_{\\infty}}&{}\\\\ {\\overset{(i i i)}{\\leq}\\left\\|\\sum_{k=0}^{K-1}\\left(\\mu_{k}-\\mu_{H,k}\\right)\\right\\|_{\\infty}+\\left\\|\\sum_{k=0}^{K-1}\\left(\\mu_{H,k}-\\hat{\\mu}_{\\mathcal{D}_{k}}\\right)\\right\\|_{\\infty},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(i)$ follows from H\u00f6lder\u2019s inequality, $(i i)$ from our definition of $\\mathcal{R}$ as the 1-norm ball, and (iii) from the triangle inequality. Since $\\|\\mu-\\mu_{H}\\|_{\\infty}\\leq\\gamma^{H}$ , we have $I_{1}\\leq\\gamma^{H}K$ . Moreover, applying Hoeffding\u2019s inequality to the $M=K N$ independent random variables ", "page_idx": 30}, {"type": "equation", "text": "$$\nX_{k N+i}=\\frac{1-\\gamma}{N}\\sum_{t=0}^{H-1}\\gamma^{t}\\mathbb{1}\\{s_{t}^{k,i}=s,a_{t}^{k,i}=a\\},\\ i\\in[N],k\\in[K],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with $X_{i}\\in[0,1/N]$ , we arrive at ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{v}_{\\mathrm{r}}\\left(\\left|S_{M}-\\mathbb{E}S_{M}\\right|\\geq\\varepsilon_{\\mathrm{sta}}/2\\right)=\\mathrm{Pr}\\left(\\left|\\sum_{k=0}^{K-1}\\hat{\\mu}_{\\mathcal{D}_{k}}(s,a)-\\mu_{K,k}(s,a)\\right|\\geq\\varepsilon_{\\mathrm{sta}}/2\\right)\\leq2\\exp\\left(-\\frac{\\varepsilon_{\\mathrm{sta}}^{2}N}{2K}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, applying the union bound over all $|{\\mathcal{S}}||{\\mathcal{A}}|$ components of the occupancy measure yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(I_{2}<\\varepsilon_{\\mathrm{stat}}/2)=1-\\mathrm{Pr}(I_{2}\\geq\\varepsilon_{\\mathrm{stat}}/2)\\geq1-2|S||A|\\exp\\left(-\\frac{\\varepsilon_{\\mathrm{stat}}^{2}N}{2K}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, to ensure that with probability at least $1-\\delta_{\\mathrm{stat}}$ it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r\\in\\mathcal{R}}\\sum_{k=0}^{K-1}\\langle r,\\mu_{k}-\\hat{\\mu}_{\\mathcal{D}_{k}}\\rangle\\leq\\varepsilon_{\\mathrm{stat}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "it suffices to choose ", "page_idx": 31}, {"type": "equation", "text": "$$\nN\\geq\\frac{2K\\log\\left(2|S||\\mathcal{A}|/\\delta_{\\mathrm{stat}}\\right)}{\\varepsilon_{\\mathrm{stat}}^{2}}\\quad\\mathrm{and}\\quad H\\geq\\frac{\\log\\left(2K/\\varepsilon_{\\mathrm{stat}}\\right)}{\\log(1/\\gamma)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This concentration result applies to both empirical occupancy measures generated from the expert data sets $\\mathcal{D}_{k}^{\\mathsf{E}}$ , as well as the data sets $\\mathcal{D}_{k,t}$ generated by Algorithm 1. ", "page_idx": 31}, {"type": "text", "text": "Step 2: When analyzing Algorithm 1 there are three sources of stochasticity. The first two are due to the randomness in the data sets $\\mathcal{D}_{k}^{\\mathsf{E}}$ and $\\mathcal{D}_{k,t}$ , and the third is due to the randomness in the forward RL algorithm, $\\mathsf{A}_{P^{k}}^{\\varepsilon_{\\mathrm{opt}},\\delta_{\\mathrm{opt}}}$ , that upon a query with the reward $r_{t}$ outputs a policy $\\pi_{k,t}$ such that with probability at least $1-\\delta_{\\mathrm{opt}}$ it holds $\\ell_{P^{k}}(r_{t},\\mu^{\\pi_{k,t}})\\leq\\varepsilon_{\\mathrm{opt}}$ . Let\u2019s denote the event that $\\begin{array}{r l}{\\operatorname*{max}_{r\\in\\mathcal{R}}\\sum_{k=0}^{K-1}\\langle r,\\mu_{P^{k}}^{\\mathsf{E}}-\\hat{\\mu}_{\\mathcal{D}_{k}^{\\mathsf{E}}}\\rangle>\\varepsilon_{\\mathrm{stat},\\mathsf{E}}}&{{}}\\end{array}$ by $\\mathcal{E}_{\\mathrm{stat},\\mathsf{E}}$ , the event that $\\begin{array}{r l}{\\operatorname*{max}_{r\\in\\mathcal{R}}\\sum_{k=0}^{K-1}\\langle r,\\mu^{\\pi_{k,t}}-\\hat{\\mu}_{\\mathcal{D}_{k,t}}\\rangle>}&{{}}\\end{array}$ $\\varepsilon_{\\mathrm{stat}}$ by $\\ensuremath{\\mathcal{E}}_{\\mathrm{stat},t}$ , and the event that $\\ell_{P^{k}}(r_{t},\\mu^{\\pi_{k,t}})>\\varepsilon_{\\mathrm{opt}}$ by $\\mathcal{E}_{\\mathrm{opt},k,t}$ . Moreover, let us assume that $\\mathcal{E}_{\\mathrm{stat},\\mathsf{E}}$ happens with probability at most $\\delta_{\\mathrm{stat},\\mathsf{E}},\\,\\mathcal{E}_{\\mathrm{stat},t}$ happens with probability at most $\\delta_{\\mathrm{stat}}$ , and $\\mathcal{E}_{\\mathrm{opt},k,t}$ happens with probability at most $\\delta_{\\mathrm{opt}}$ . By union bound, the probability of the event ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{F}:=\\neg\\mathcal{E}_{\\mathrm{stat},\\mathsf{E}}\\wedge\\bigwedge_{t=0}^{T-1}\\neg\\mathcal{E}_{\\mathrm{stat},t}\\wedge\\bigwedge_{t=0}^{T-1}\\bigwedge_{k=0}^{K-1}\\neg\\mathcal{E}_{\\mathrm{opt},k,t},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "that none of the above events happens is lower bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(\\mathcal{F}\\right)=1-\\operatorname*{Pr}\\left(\\mathcal{E}_{\\mathrm{stat},\\mathsf{E}}\\vee\\bigvee_{t=0}^{T-1}\\mathcal{E}_{\\mathrm{stat},t}\\vee\\bigvee_{t=0}^{T-1}\\bigvee_{k=0}^{K-1}\\mathcal{E}_{\\mathrm{opt},k,t}\\right)}\\\\ &{\\qquad\\qquad\\geq1-\\Bigg(\\!\\operatorname*{Pr}\\left(\\mathcal{E}_{\\mathrm{stat},\\mathsf{E}}\\right)+\\displaystyle\\sum_{t=0}^{T-1}\\operatorname*{Pr}\\left(\\mathcal{E}_{\\mathrm{stat},t}\\right)+\\displaystyle\\sum_{t=0}^{T-1}\\sum_{k=0}^{K-1}\\operatorname*{Pr}\\left(\\mathcal{E}_{\\mathrm{opt},k,t}\\right)\\!\\Bigg)}\\\\ &{\\qquad\\qquad\\geq1-\\left(\\delta_{\\mathrm{stat},\\mathsf{E}}+T\\delta_{\\mathrm{stat}}+K T\\delta_{\\mathrm{opt}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, to ensure that $\\mathcal{F}$ happens with probability at least $1-\\hat{\\delta}$ , it suffices to choose ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\ge\\frac{2K\\log\\Big(6|S||\\mathcal{A}|/\\hat{\\delta}\\Big)}{\\varepsilon_{\\mathrm{stat},\\mathsf{E}}^{2}}\\quad\\mathrm{and}\\quad H\\ge\\frac{\\log\\big(2K/\\varepsilon_{\\mathrm{stat},\\mathsf{E}}\\big)}{\\log\\left(1/\\gamma\\right)},}\\\\ &{N_{t}\\ge\\frac{2K\\log\\Big(6T|S||\\mathcal{A}|/\\hat{\\delta}\\Big)}{\\varepsilon_{\\mathrm{stat}}^{2}}\\quad\\mathrm{and}\\quad\\delta_{\\mathrm{opt}}=\\frac{\\hat{\\delta}}{3K T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Step 3: Note that we can bound $\\begin{array}{r}{\\left\\|g_{t}\\right\\|_{2}\\leq\\left\\|g_{t}\\right\\|_{1}\\leq\\sum_{k=0}^{K-1}\\left\\|\\hat{\\mu}_{\\mathcal{D}_{k}^{\\mathsf{E}}}\\right\\|_{1}+\\left\\|\\hat{\\mu}_{k,t}\\right\\|_{1}\\leq2K=:G}\\end{array}$ and the diameter of $\\mathcal{R}$ is $D=2$ . Hence, given that event $\\mathcal{F}$ happens, we can bound the suboptimalities of the ", "page_idx": 31}, {"type": "text", "text": "$K$ experts under the reward, $\\hat{r}$ , recovered by Algorithm 1 with stepsize $\\alpha=D/(G{\\sqrt{T}})$ as follows ( $\\begin{array}{r l}&{\\mathcal{L}\\mathrm{coart}:=\\frac{\\lambda}{2}\\sum_{k=1}^{\\infty}\\bigg[\\operatorname*{sup}_{i,t\\rightarrow t}\\big(p_{i,t\\rightarrow t}-p_{i-t}\\big)-M_{i}(i)+M_{i}(j)\\bigg]}\\\\ &{:=\\!\\!\\!\\operatorname*{sup}_{i,t\\rightarrow t}+\\frac{1}{2}\\!\\!\\!\\frac{\\Gamma_{w}^{(1)}}{\\Gamma_{w}^{(2)}}\\!\\bigg[\\operatorname*{sup}_{i,t\\rightarrow t}\\!-\\!\\operatorname*{sup}_{i,t}-\\delta_{i}(i)+M_{i}(j)\\bigg]}\\\\ &{:=\\!\\!\\!\\operatorname*{sup}_{i,t\\rightarrow t}+\\frac{\\lambda}{2}\\!\\!\\bigg[\\!\\operatorname*{sup}_{i,t\\rightarrow t}\\!-\\!\\operatorname*{sup}_{i,t}\\!-\\!\\operatorname*{sup}_{i,t}-M_{i}(j)\\!-\\!\\operatorname*{sup}_{i,t}\\!\\bigg]}\\\\ &{\\overset{(a)}{\\leq}\\!\\!\\!\\operatorname*{coart}{_{a}\\leq}\\!\\!+K_{a\\phi}\\!+\\!\\sum_{s=1}^{\\infty}\\!\\bigg[\\!\\operatorname*{sup}_{i,t\\rightarrow t}\\!-\\!\\operatorname*{sup}_{i,t}\\!-M_{i}(j)\\!-\\!\\operatorname*{sup}_{i,t}\\!\\bigg]}\\\\ &{\\overset{(b)}{\\leq}\\!\\!\\!\\operatorname*{coart}{_{a}\\leq}\\!\\!+K_{a\\phi}\\!+\\!\\operatorname*{sup}_{i,t\\rightarrow t}+\\frac{1}{2}\\!\\!\\!\\sum_{k=1}^{\\infty}\\!\\!\\!\\big[\\!\\operatorname*{sup}_{i,t\\rightarrow t}-k\\!\\theta_{i+1}\\!+k\\!(\\frac{\\theta_{i}}{\\Delta})\\!\\big]}\\\\ &{\\overset{(c)}{\\leq}\\!\\!\\!\\operatorname*{coart}{_{a}\\leq}\\!\\!+K_{a\\phi}\\!+\\!\\operatorname*{sup}_{i,t\\rightarrow t}+\\frac{\\lambda}{2}\\!\\!\\frac{(1-\\lambda)^{\\frac{1}{2}}\\!-1}{\\lambda!}\\!\\bigg[\\!\\operatorname*{sup}_{i,t\\rightarrow t}-k\\!\\theta_{i+1}\\!-k\\!(\\frac{\\theta_{i}}{\\Delta})\\!+k\\!(\\frac{\\theta_{i}}{\\Delta})\\!\\bigg]}\\\\ &{:=\\!\\!\\!\\operatorname*{sup}_{i,t\\rightarrow t}+K_{a\\phi}\\!+\\!\\operatorname*{sup}$ $\\begin{array}{r l}&{\\frac{\\sinh(x_{i}-x_{j})}{\\sinh(x_{i}-x_{j})}}\\\\ &{=\\frac{\\sinh(x_{i}-x_{j})}{\\sinh(x_{i}-x_{j})}\\,,}\\\\ &{\\frac{1}{\\sqrt{2}\\pi x_{i}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,}\\\\ &{\\frac{\\sinh(x_{i}-x_{j})}{\\sinh(x_{i}-x_{j})}\\,,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,}\\\\ &{\\frac{\\sinh(x_{i}-x_{i})}{\\sinh(x_{i}-x_{j})}\\,,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,}\\\\ &{\\frac{\\sinh(x_{i}-x_{j})}{\\sinh(x_{i}-x_{j})}\\,,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{=\\alpha_{4\\times4}\\,,\\frac{1}{\\sqrt{2}\\pi x_{i}}\\,\\bigg[\\frac{1}{\\sqrt{2}\\pi x_{i}}\\left(\\gamma_{\\infty,x_{i}}-\\beta_{0,j}\\right)-\\beta_{0,j}(x_{i})+b_{0,j}(x_{i}^{2})\\bigg]}\\\\ &{\\qquad\\quad\\quad\\times\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{=\\alpha_{4\\times4}\\,+\\frac{1}{\\sqrt{2}\\pi x_{i}}\\sum_{i=1}^{N}\\left(\\gamma_{0,i}(x_{i}-x_{i})-b_{0,j}\\right)-b_{0,i}(x_{i}^{2})}\\\\ &{\\frac{\\sinh(x_{i}-x_{j})}{\\sinh(x_{i}-x_{j})}\\,,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\frac{\\sinh(x_{i}-x_{i})}{\\sinh(x_{i}-x_{j})}\\,,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{=\\alpha_{4\\times4}\\,+K_{i}\\Longleftrightarrow(x_{i}-\\frac{1}{\\sqrt{2}\\pi x_{i}})\\,\\Big[(\\gamma_{0,i}(x_{i})-b_{0,j})-b_{0,i}(x_{i})+b_{0,j}(x_{i}^{2})\\Big]}\\\\ &{\\qquad\\quad\\$ T T ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Here, the inequalities $(i),(i v)$ , and $(v i)$ follow from the concentration bound established in step 1. Moreover, inequality $(i i)$ holds since $\\begin{array}{r}{\\hat{r}\\mapsto\\operatorname*{max}_{\\mu\\in\\mathcal{M}_{P^{k}}}\\langle\\hat{r},\\mu-\\mu_{P^{k}}^{\\mathsf{E}}\\rangle-\\bar{h}(\\mu)\\!+\\!\\bar{h}(\\mu_{P^{k}}^{\\mathsf{E}})}\\end{array}$ is the pointwise maximum of affine functions and therefore convex. Furthermore, $(i i i)$ follows from $\\varepsilon_{\\mathrm{opt}}$ -optimality of $\\mu_{k,t},(v)$ from Theorem I.2, and $(v i i)$ from concavity of the mapping $\\mu_{k,t}\\mapsto\\langle r,\\mu_{k,t}\\!-\\!\\mu_{P^{k}}^{\\mathsf{E}}\\rangle\\!-\\!\\bar{h}(\\mu_{k,t})$ Finally, $(v i i i)$ holds because all experts are optimal for the reward $r^{\\mathsf{E}}$ . In conclusion, to ensure that with probability at least $1-\\hat{\\delta}$ it holds that $\\begin{array}{r}{\\ell_{P^{k}}\\bigl(\\hat{r},\\mu_{P^{k}}^{\\mathsf{E}}\\bigr)\\leq\\sum_{k=0}^{K-1}\\ell_{P^{k}}\\bigl(\\hat{r},\\mu_{P^{k}}^{\\mathsf{E}}\\bigr)\\leq\\hat{\\varepsilon}}\\end{array}$ it suffices to choose $\\begin{array}{r l r}{T}&{{}\\!\\!\\!\\!=}&{\\!\\!\\!\\!\\frac{256K^{2}}{\\hat{\\varepsilon}^{2}}}\\end{array}$ 5\u03b56\u02c62K , \u03b1 = $\\begin{array}{r}{\\alpha\\ =\\ \\frac{\\hat{\\varepsilon}}{16K^{2}}}\\end{array}$ 16\u03b5\u02c6K2 , N = 128K log(6|S||A|/\u03b4\u02c6), H = Ht = $\\begin{array}{r}{H\\;=\\;H_{t}\\;=\\;\\frac{\\log(16K/\\hat{\\varepsilon})}{\\log(1/\\gamma)}}\\end{array}$ log(16K/\u03b5\u02c6), N = 128K log(1536\u03b5\u02c6K22|S||A|/(\u03b4\u02c6\u03b5\u02c62)), \u03b4opt = 76\u03b4\u02c68\u03b5\u02c62K3 , \u03b5opt = 4\u03b5\u02c6K . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "J Suboptimal experts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In our problem formulation, we assumed that the $K$ experts are optimal with respect to $r^{\\mathsf{E}}$ , i.e. $\\mu_{P^{k}}^{\\mathsf{E}}=\\mathsf{\\bar{R}L}_{P^{k}}(r^{\\mathsf{E}})$ for $k=0,\\ldots,K-1$ . This assumption can be weakened by requiring that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r\\in\\mathcal{R}}\\left|J(r,\\mu_{P^{k}}^{\\mathsf{E}})-J(r,\\mathsf{R L}_{P^{k}}(r^{\\mathsf{E}}))\\right|\\leq\\varepsilon_{\\mathrm{mis}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\varepsilon_{\\mathrm{mis}}$ is some misspecification error. The transferability results in Theorem 3.10 and 3.11 still apply whenever we recover a reward $\\hat{r}$ such that $\\ell_{P^{k}}(\\boldsymbol{\\hat{r}},\\mathbf{\\tilde{R}}\\mathsf{L}_{P^{k}}(r^{\\mathsf{E}}))\\,\\leq\\,\\hat{\\varepsilon}$ . Moreover, with a straightforward modification of the proof of Theorem 4.1, it follows that with high probability Algorithm 1 recovers a reward $\\hat{r}$ such that $\\ell_{P^{k}}(\\hat{r},\\mathsf{R L}_{P^{k}}(r^{\\mathsf{E}}))\\leq\\hat{\\varepsilon}+2K\\varepsilon_{\\mathrm{mis}}$ . Hence, our end-to-end transferability guarantees apply with $\\hat{\\varepsilon}\\gets\\hat{\\varepsilon}+2K\\varepsilon_{\\mathrm{mis}}$ . However, $\\varepsilon_{\\mathrm{mis}}$ cannot be further reduced by collecting more samples from the expert or MDP. ", "page_idx": 33}, {"type": "text", "text": "K Experimental details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Setup To validate our results experimentally, we are using a stochastic adaption of the WindyGridworld environment [Sutton and Barto, 2018].4 In particular, we consider a 6x6 grid with 4 actions (Up, Down, Left, Right), a wind direction (North, East, South, West), and a wind strength $\\beta\\in[0,1]$ . When the agent takes an action, with probability $(1-\\beta)$ , it moves to the intended grid cell, and with probability $\\beta$ , the wind pushes the agent one step further in the direction of the wind. This means that the transition law is a convex combination of two laws: $(1-\\beta)P^{\\mathrm{Gridworld}}+\\beta P^{\\mathrm{Wind}}$ , where $P$ Gridworld and $P^{\\mathrm{Wind}}$ represent the transition laws for a deterministic Gridworld and a deterministic WindyGridworld. For our experiments, we then consider the pairs of expert transition laws $P_{\\beta}^{0}=(1-\\beta)\\dot{P}^{\\mathrm{Gridworld}}+\\beta P^{\\mathrm{North}}$ and $\\bar{P}_{\\beta}^{1}=(1\\!-\\!\\beta)P^{\\mathrm{Gridworld}}+\\beta P^{\\mathrm{East}}$ with $\\beta$ in $\\{0.01,0.1,0.5,1.0\\}$ As shown in Figure 4(a), the second principal angle between $P_{\\beta}^{0}$ and $P_{\\beta}^{1}$ , calculated using a singular value decomposition [Knyazev and Argentati, 2002], increases as the wind strength $\\beta$ increases. ", "page_idx": 33}, {"type": "text", "text": "Inverse reinforcement learning We observed that under a small second principal angle, the recovered reward heavily depends on both the expert reward and the reward initialization. Hence, we sample 10 independent expert rewards, each generated by first sampling a random set of 10 state-action pairs and then randomly assigning a reward of $\\pm1$ . Using Shannon entropy regularization with $\\tau=0.3$ , we then use soft policy iteration to get expert policies for each combination of expert reward and wind strength $\\beta$ . For each of these expert policies, we then generate expert data sets with $N^{\\mathsf{E}}\\in\\{10^{3},10^{4},\\breve{10}^{5},10^{6}\\}$ trajectories of length $H=100$ . Next, we run Algorithm 1, with soft policy iteration as a subroutine, for $30^{\\prime}000$ iterations, where rewards are initialized by sampling from a standard normal distribution. As a reward class, we choose the $\\left\\Vert\\cdot\\right\\Vert_{1}$ -ball with radius $10^{3}$ (essentially unbounded), as a stepsize $\\alpha=0.05$ for the first $15^{\\prime}000$ iterations and $\\alpha=0.005$ for the second half. Moreover, we sample $N=100$ new trajectories of horizon $H=100$ at each gradient step. Figure 4(b) illustrates the distances between the recovered $\\hat{r}$ and the experts\u2019 reward $r^{\\check{\\mathsf{E}}}$ , measured in $\\bar{\\mathbb{R}}^{S\\times A}/\\mathbf{1}$ It is evident that the recovered reward gets closer to the experts\u2019 reward as the number of expert demonstrations increases. Moreover, we observe that the recovered reward is closer to the experts\u2019 reward when the second principal angle between the experts is larger, as expected from Lemma F.1. ", "page_idx": 33}, {"type": "text", "text": "Transferability We evaluate the transferability of the obtained reward by considering two new environments. First, a south wind setting $P^{\\mathrm{South}}$ with wind strength $\\beta=1$ , and second, a deterministic gridworld $P^{\\mathrm{Shifted}}$ , with cyclically shifted actions, i.e., Right $\\rightarrow$ Down, $\\mathrm{Up}{\\rightarrow}~\\mathrm{Right}$ , Lef $\\rightarrow\\,{\\mathrm{Up}}$ $\\mathrm{Down}{\\to}\\,\\mathrm{L}$ eft. In Figure 4(c) and (d), we illustrate the transferability in terms of $\\ell_{P^{\\mathrm{South}}}\\bar{(r^{\\mathrm{E}},{\\sf R L}_{P^{\\mathrm{South}}}(\\hat{r^{\\mathrm{\\Delta}}}))}$ and $\\ell_{P^{\\mathrm{Shifted}}}(r^{\\mathsf{E}},\\mathsf{R L}_{P^{\\mathrm{Shifted}}}^{-}(\\hat{r}))$ , respectively. We observe that for both environments the transferability improves with a larger second principal angle, thus confirming our theoretical result in Theorem 3.10. The effect is even more pronounced for the shifted environment. While confirming our results, the experiments also reveal a high sample complexity in terms of expert demonstrations. This is to be expected, as IRL aims to match the expert\u2019s empirical occupancy measure, leading to overfitting when there are not enough demonstrations [Ho and Ermon, 2016]. This issue can be mitigated by reducing the dimension of the reward class (see e.g. [Abbeel and Ng, 2004]). ", "page_idx": 33}, {"type": "image", "img_path": "l5wEQPcDab/tmp/61e332c249dacd54a32c9b5e0cb6dd55d0535441145f763f37153ec9a174c48a.jpg", "img_caption": ["Figure 4: $(a)$ shows the second principal angle between $P_{\\beta}^{0}$ and $P_{\\beta}^{1}$ for varying wind strength $\\beta$ . Furthermore, $(b)$ shows the distance between $\\hat{r}$ and $r^{\\mathsf{E}}$ in $\\mathbb{R}^{S\\times A}/\\mathbf{1}$ for a varying number of expert demonstrations $N^{\\mathsf{E}}$ and wind strength $\\beta$ . Moreover, $(c)$ and $(d)$ show the transferability to $P^{\\mathrm{South}}$ and $P^{\\mathrm{Shifted}}$ in terms of $\\ell_{P^{\\mathrm{South}}}(r^{\\mathsf{E}},\\bar{\\mathsf{R L}}_{P^{\\mathrm{South}}}(\\hat{r}))$ and $\\ell_{P^{\\mathrm{Shifted}}}\\big(r^{\\mathsf{E}},\\mathsf{R L}_{P^{\\mathrm{Shifted}}}\\big(\\hat{r}\\big)\\big)$ , respectively. The dots indicate the median and the shaded areas the 0.2 and 0.8 quantiles over the 10 independent realizations. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: As mentioned in the abstract and introduction, our main contributions are sufficient conditions for learning transferable rewards, as given in Theorem 3.10 and 3.11. Furthermore, we also provide an algorithm and experiments in Section 4 and 5. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the limitations in the Limitations and future work paragraph in Section 6. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Yes, all assumptions are stated in the main paper, and proofs are provided in the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All details about our experiments are provided in Appendix K. Furthermore, we also provide the code attached as a .zip file. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same ", "page_idx": 35}, {"type": "text", "text": "dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 36}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All code (for experiments and computations in Example 3.3) is attached as a .zip file. Furthermore, we provide a README.md with instructions how to run the code. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we discuss the chosen stepsizes and parameters in Appendix K. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We repeated our experiments over 10 random seeds and provide quantile plots. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, the hardware details are specified as a footnote in Appendix K. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This work is purely theoretical and did not cause any harm to society. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This work is purely theoretical. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We don\u2019t work with any potentially harmful models or datasets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The experiments are based on our own codebase. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide our code as a .zip flie. We will make it openly available on github later. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This research does not involve any human experts but only synthetically generated ones. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 39}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: No humans are involved in this research. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]