[{"figure_path": "ZfRGRK5Kxl/tables/tables_4_1.jpg", "caption": "Table 1: Winoground-style evaluation of pretrained CLIP models on TripletData.", "description": "This table presents the results of a Winoground-style evaluation performed on the TripletData dataset using several pretrained CLIP models.  Winoground is a benchmark designed to evaluate the compositional reasoning capabilities of vision-language models. The evaluation assesses the model's ability to correctly match image-text pairs that share similar meanings, even when semantic perturbations have been made to the caption. The table shows the Image Score, Text Score, and Group Score for each model.  These scores represent the accuracy of the models in selecting the correct image-text pairs based on image features, textual features, and a combination of both, respectively.  The table also includes the human performance on the Winoground benchmark, providing a baseline for comparison.", "section": "3.2 TripletData: Image-text hard negative data augmentations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_4_2.jpg", "caption": "Table 2: Wordnet synset analysis of captions from CC3M and TripletData.", "description": "This table presents a quantitative analysis comparing the concept coverage of the CC3M dataset and the TripletData dataset (considering only the negative captions). It shows the number of unique WordNet synsets (semantic concepts) and the total number of synsets in each dataset.  The 'TripletData Intersection' column shows the overlap in synsets between the CC3M dataset and the negative captions in TripletData. This analysis helps to understand the extent to which TripletData introduces new concepts or enhances existing ones in comparison to the original dataset.", "section": "Data for Contrastive Pre-training"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_5_1.jpg", "caption": "Table 3: Importance of image-text hard negatives. We measure the importance of various modality-specific hard negatives on SugarCrepe, image-text retrieval, and ImageNet1k. We find that TripletCLIP results into the most optimal solution. Bold number indicates the best performance.", "description": "This table presents the results of experiments evaluating the impact of using different types of negative samples (captions only, images only, both captions and images) during the training of CLIP models. The performance is evaluated on three different tasks: SugarCrepe (a benchmark dataset for compositional reasoning), image-text retrieval, and ImageNet1k (zero-shot image classification). The results show that using both negative captions and negative images (TripletCLIP) leads to the best performance across all three tasks.  The bold numbers highlight the best performing model for each task.", "section": "3.3 TripletCLIP"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_6_1.jpg", "caption": "Table 4: Composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance. \u2020 represents the results taken from SugarCrepe benchmark.", "description": "This table presents a comprehensive comparison of different methods (LaCLIP, LaCLIP+HN, NegCLIP, NegCLIP++, and TripletCLIP) for compositional reasoning on the SugarCrepe benchmark.  It shows the performance of each method across various sub-tasks within the benchmark (Replace, Swap, Add), focusing on object, attribute, and relation aspects.  The \"Overall Avg.\" column provides the average performance across all sub-tasks. Bold numbers highlight the best performance for each category, while underlined numbers indicate the second-best performance. The results are broken down by two datasets (CC3M and CC12M).  The \u2020 symbol indicates results obtained directly from the SugarCrepe benchmark paper, not from the authors' experiments.", "section": "4.2 Compositional reasoning"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_6_2.jpg", "caption": "Table 5: Zero-shot image-text retrieval and classification results. Bold number indicates the best performance and underlined number denotes the second-best performance.", "description": "This table presents the zero-shot performance of different models on image-text retrieval tasks (using MSCOCO and Flickr30k datasets) and zero-shot image classification (using VTAB and ImageNet1k datasets).  The metrics reported include recall at 5 (R@5) for retrieval and top-1 and top-5 accuracy for classification.  The table highlights the significant performance improvements achieved by TripletCLIP compared to the baseline models (LaCLIP, LaCLIP+HN, NegCLIP, and NegCLIP++) in both retrieval and classification tasks.", "section": "4.3 Zero-shot evaluations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_7_1.jpg", "caption": "Table 6: Ablation on filtering high-quality image-text pairs from TripletData. We evaluate the TripletCLIP after applying the filters to ensure the quality similar to DataComp and compare the baselines on three benchmarks. We find that TripletCLIP results in the most optimal solution. Bold number indicates the best performance. \u2020 represents that results are borrowed from DataComp.", "description": "This ablation study investigates the effect of filtering the TripletData to improve data quality.  It compares the performance of TripletCLIP (with and without filtering) against baselines (CLIP and LaCLIP) across three benchmarks: SugarCrepe, Retrieval, and ImageNet1k.  The filtering methods use CLIP scores and image-based CLIP scores to select high-quality image-text pairs. The results demonstrate that even with a reduced dataset size after filtering, TripletCLIP maintains superior performance compared to the baselines, suggesting that the quality of the negative samples is more important than quantity.", "section": "4.1 Experiment Setup"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_8_1.jpg", "caption": "Table 7: Finetuning-based composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance.", "description": "This table presents the results of finetuning various vision-language models on the SugarCrepe benchmark.  The models evaluated include CLIP, CLIP (finetuned), NegCLIP, a baseline model [44], CON-CLIP [50], TSVLC (RB) [12], TSVLC (LLM+RB) [12], DAC [11], and TripletCLIP.  The performance is broken down by several compositionality sub-categories (Replace Object, Replace Attribute, Replace Relation, Swap Object, etc.)  allowing for a detailed comparison of each model's ability to handle different aspects of compositional reasoning.  Bold numbers indicate the best performance in each subcategory and underlined numbers highlight the second-best performance. The \"Overall Avg.\" column shows the average performance across all sub-categories.", "section": "4.2 Compositional reasoning"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_8_2.jpg", "caption": "Table 8: Frozen encoder ablation. LiT style fine-tuning ablations on SugarCrepe, image-text retrieval, and ImageNet1k. Bold number indicates the best performance.", "description": "This table presents the results of ablation experiments on TripletCLIP, specifically focusing on the impact of freezing either the vision or text encoder during fine-tuning.  It compares the performance of LaCLIP and TripletCLIP under different training scenarios: training only the text encoder, training only the vision encoder, and training both. The metrics evaluated include SugarCrepe (a compositional reasoning benchmark), image-text retrieval performance, and ImageNet1k zero-shot classification accuracy. The bold numbers highlight the best performance achieved in each scenario.", "section": "4.3 Zero-shot evaluations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_9_1.jpg", "caption": "Table 9: TripletData as large-scale composition evaluation dataset after [53].", "description": "This table presents the results of evaluating the TripletData dataset on a large-scale composition task, comparing it against several baselines (CLIP, NegCLIP, and NegCLIP++).  The results show the performance across three metrics: Text-Score, Image-Score, and Group-Score.  Importantly, TripletCLIP significantly outperforms all baselines across all metrics. This indicates that TripletData provides a valuable resource for evaluating compositional reasoning capabilities in vision-language models.", "section": "4.3 Zero-shot evaluations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_15_1.jpg", "caption": "Table 10: Detailed pre-training hyper-parameters for CLIP training across various experiments and ablations.", "description": "This table lists the hyperparameters used for pre-training CLIP models in various experimental setups described in the paper.  The hyperparameters are consistent across different datasets (CC3M, CC12M) and training strategies (LiT, Concept Coverage Ablations), enabling a fair comparison of results.  The listed parameters include batch size, optimizer, learning rate, weight decay, Adam \u03b2 and \u03b5, total training steps, and learning rate schedule.  This level of detail allows for reproducibility of the experiments.", "section": "4.1 Experiment Setup"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_16_1.jpg", "caption": "Table 4: Composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance. \u2020 represents the results taken from SugarCrepe benchmark.", "description": "This table presents a comparison of different methods (LaCLIP, LaCLIP+HN, NegCLIP, NegCLIP++, and TripletCLIP) on the SugarCrepe benchmark for compositional reasoning.  The benchmark evaluates the ability of vision-language models to understand and generate responses to various compositional queries.  The table shows the performance of each method across different compositional aspects (Replace, Swap, Add) and reports the average performance.  Bold numbers highlight the best performing method for each aspect and overall, while underlined numbers indicate the second-best performance. Results for a comparison method (DataComp) are also included.  The results demonstrate the significant improvement achieved by the TripletCLIP method over other baselines, particularly in terms of compositional reasoning ability.", "section": "4.2 Compositional reasoning"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_16_2.jpg", "caption": "Table 3: Importance of image-text hard negatives. We measure the importance of various modality-specific hard negatives on SugarCrepe, image-text retrieval, and ImageNet1k. We find that TripletCLIP results into the most optimal solution. Bold number indicates the best performance.", "description": "This table presents the results of an ablation study comparing the performance of different models on three benchmark tasks: SugarCrepe, image retrieval, and ImageNet-1k classification.  Each model was trained with different combinations of hard negative captions and hard negative images, illustrating the impact of each modality on the overall performance. The results show that TripletCLIP, using both modalities, outperforms all other models.", "section": "3.3 TripletCLIP"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_17_1.jpg", "caption": "Table 5: Zero-shot image-text retrieval and classification results. Bold number indicates the best performance and underlined number denotes the second-best performance.", "description": "This table presents the zero-shot performance results of different models on image-text retrieval tasks (using MSCOCO and Flickr30k datasets) and zero-shot image classification (using 18 standard datasets, including ImageNet-1k).  The metrics used are Recall@5 (R@5) for retrieval and top-1 and top-5 accuracy for classification.  The table highlights the superior performance of TripletCLIP compared to various baseline models (LaCLIP, LaCLIP+HN, NegCLIP, NegCLIP++) across all tasks.", "section": "4.3 Zero-shot evaluations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_17_2.jpg", "caption": "Table 14: Ablation on choice pre-trained LLM. We train NegCLIP++ (ViT-B/32) on negative captions generated from various LLMs and report SugarCrepe, Flickr30k Retrieval (R@5), and ImageNet-top5 performances.", "description": "This table presents the results of an ablation study on the choice of pre-trained large language models (LLMs) used for generating negative captions in the NegCLIP++ model.  Three different LLMs were used: Gemma-2b-it, Phi-3-mini-4k-instruct, and Mistral-7b-instruct-v0.2. The table shows the performance of NegCLIP++ trained with negative captions generated by each LLM on three different benchmarks: SugarCrepe (a compositional reasoning benchmark), Flickr30k Retrieval (R@5) (an image-text retrieval task), and ImageNet-top5 (zero-shot classification).  The goal is to determine which LLM produces the most effective negative captions for improving the overall performance of the model.", "section": "4.5 Ablations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_17_3.jpg", "caption": "Table 15: Ablation on CyCLIP with TripletLoss. To evaluate the compatibility of TripletLoss with the CyCLIP, we train the ViT-B/32 models from search on CC3M with 512 batch size and report SugarCrepe, Flickr30k Retrieval (R@5), and ImageNet-top-5 performances.", "description": "This table presents an ablation study evaluating the effectiveness of combining the TripletLoss with the CyCLIP model.  The experiment trains a ViT-B/32 model on the CC3M dataset using a batch size of 512. The results show the performance on three different metrics: SugarCrepe (a compositional reasoning benchmark), Flickr30k Retrieval (R@5) (image-text retrieval at rank 5), and ImageNet1k (top-5) (zero-shot image classification in top 5).  It helps determine if adding the TripletLoss improves the CyCLIP model's performance on these tasks. ", "section": "4.5 Ablations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_18_1.jpg", "caption": "Table 16: Finetuning-based evaluations of the methods on Retrieval and ImageNet-1k benchmarks. Bold number indicates the best performance and underlined number denotes the second-best performance.", "description": "This table presents the results of finetuning various models on image retrieval tasks (using MSCOCO and Flickr30k datasets) and ImageNet-1k zero-shot classification.  The models compared include CLIP (with and without finetuning), NegCLIP, a baseline model from another study [44], TSVLC (with and without LLM-based rule rewriting) [12], DAC [11], and TripletCLIP. The metrics used are Recall@5 for retrieval tasks and top-1 and top-5 accuracy for ImageNet-1k.  The table highlights the performance of TripletCLIP in comparison to other methods after finetuning.", "section": "4.3 Zero-shot evaluations"}, {"figure_path": "ZfRGRK5Kxl/tables/tables_19_1.jpg", "caption": "Table 17: Question Generation. Examples of LLM-generated existence-related questions from captions to evaluate the generated images.", "description": "This table presents examples of questions automatically generated by a large language model (LLM) to assess the quality of images generated by a text-to-image (T2I) model.  For each image caption, five binary yes/no questions are created. The goal is to determine whether the generated images accurately reflect the information contained in the original captions.", "section": "TripletData Analysis"}]