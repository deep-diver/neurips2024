[{"figure_path": "ZfRGRK5Kxl/figures/figures_2_1.jpg", "caption": "Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) represents the positive a image-text pair, and (x', y') represents the corresponding negative image-text pair.", "description": "This figure compares the training workflows of three different models: CLIP, NegCLIP, and TripletCLIP.  CLIP uses only positive image-text pairs (x, y) for training. NegCLIP adds hard negative text captions (y') to the training process, enhancing the contrast. TripletCLIP builds upon NegCLIP by further introducing hard negative images (x') corresponding to the hard negative captions (y'), creating a triplet (x, y, (x',y')) for more robust contrastive learning. The figure visually demonstrates how the negative samples are incorporated into each model's training objective, showcasing the progression of complexity and potential for improved compositional reasoning.", "section": "3 Method"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) represents the positive a image-text pair, and (x', y') represents the corresponding negative image-text pair.", "description": "This figure compares the training workflows of three different models: CLIP, NegCLIP, and TripletCLIP.  It illustrates how each model uses positive and negative image-text pairs during training. CLIP uses a simple positive-negative pair. NegCLIP adds a negative text caption to the positive image-text pair. TripletCLIP adds both negative text and image to the positive pair for a more robust training process.", "section": "3 Method"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_9_1.jpg", "caption": "Figure 3: Average Results of LaCLIP and TripletCLIP for SugarCrepe Compositions, Image-Text Retrieval, and ImageNet1k over increasing concept diversity.", "description": "This figure shows the performance of LaCLIP and TripletCLIP models across three different evaluation metrics: SugarCrepe (compositional reasoning), Image-Text Retrieval, and ImageNet1k (zero-shot classification).  The x-axis represents increasing concept coverage during training, indicating the amount of unique concepts the model has learned.  The y-axis shows the corresponding performance scores for each metric.  The plot demonstrates that TripletCLIP consistently outperforms LaCLIP across all three metrics as concept coverage increases, highlighting the effectiveness of the proposed TripletCLIP training strategy in improving the model's compositional reasoning and generalization capabilities.", "section": "4.4 Finetuning performance"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_15_1.jpg", "caption": "Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) represents the positive a image-text pair, and (x', y') represents the corresponding negative image-text pair.", "description": "This figure compares the training workflows of three different models: CLIP, NegCLIP, and TripletCLIP.  CLIP uses a standard contrastive learning approach with positive image-text pairs. NegCLIP augments this by adding hard negative text captions. TripletCLIP extends this further by incorporating both hard negative text captions and synthetically generated hard negative images, creating a triplet of (positive image-text pair, negative image, negative text). The figure visually represents how each model incorporates positive and negative samples during training to learn robust image-text representations.", "section": "3 Method"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_18_1.jpg", "caption": "Figure 5: Positive vs. Negative modality-specific pair-based similarity distribution of baseline LaCLIP and TripletCLIP. The left plot is the vision embedding similarities between positive and negative images. The right plot is the text embedding similarities between positive and negative captions.", "description": "This figure shows the distribution of cosine similarity scores between positive and negative image-text pairs for both vision and text modalities.  The left plot shows the distribution for vision embeddings (images) and the right plot for text embeddings (captions).  The distributions are compared between baseline LaCLIP and the proposed TripletCLIP method. A more skewed distribution towards lower similarity scores (closer to 0.0) indicates better discrimination between positive and negative pairs; hence, better performance.  Ideally, one would expect a distribution skewed sharply towards 0.0, representing perfect discrimination. The plots illustrate how the TripletCLIP model improves this discrimination, especially for the vision modality.", "section": "E Encoder Representation Distribution Analysis"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_19_1.jpg", "caption": "Figure 5: Positive vs. Negative modality-specific pair-based similarity distribution of baseline LaCLIP and TripletCLIP. The left plot is the vision embedding similarities between positive and negative images. The right plot is the text embedding similarities between positive and negative captions.", "description": "This figure shows the cosine similarity distributions for both vision and text modalities between positive and negative pairs using LaCLIP and TripletCLIP models trained on CC12M dataset. The x-axis represents the cosine similarity, ranging from 0 to 1, where 1 indicates high similarity and 0 indicates low similarity. The y-axis represents the frequency of pairs with a given cosine similarity. Separate distributions are shown for the vision and text modalities in the left and right panels, respectively. The left plot shows cosine similarity between vision embeddings of positive and negative image pairs. The right plot shows the cosine similarity between text embeddings of positive and negative text pairs.", "section": "E Encoder Representation Distribution Analysis"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_22_1.jpg", "caption": "Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) represents the positive a image-text pair, and (x', y') represents the corresponding negative image-text pair.", "description": "This figure compares the training workflows of three different models: CLIP, NegCLIP, and TripletCLIP.  CLIP uses only positive image-text pairs. NegCLIP incorporates negative text pairs alongside positive pairs. TripletCLIP, the proposed method, uses both hard negative image and text pairs, in an alternating fashion, alongside positive image-text pairs, to enhance the compositional reasoning of the model.", "section": "3 Method"}, {"figure_path": "ZfRGRK5Kxl/figures/figures_23_1.jpg", "caption": "Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) represents the positive a image-text pair, and (x', y') represents the corresponding negative image-text pair.", "description": "This figure compares the training workflows of three different models: CLIP, NegCLIP, and TripletCLIP.  CLIP uses only positive image-text pairs for training. NegCLIP incorporates negative text captions to enhance the contrastive learning process. TripletCLIP extends this further by additionally using synthetically generated negative images and their corresponding hard negative captions, creating triplets for contrastive learning.", "section": "3 Method"}]