[{"type": "text", "text": "Scaling laws for learning with real and surrogate data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ayush Jain Andrea Montanari Granica Computing Inc. Granica Computing Inc. granica.ai \u2014 granica.ai ayush.jain@granica.ai ndrea.montanari@granica.ai Eren Sasoglu Granica Computing Inc. \u2014 granica.ai eren.sasoglu@granica.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Collecting large quantities of high-quality data can be prohibitively expensive or impractical, and a bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources, e.g. data collected under different circumstances or synthesized by generative models. We refer to such data as \u2018surrogate data\u2019. We study a weighted empirical risk minimization (ERM) approach for integrating surrogate data into training. We analyze mathematically this method under several classical statistical models, and validate our findings empirically on datasets from different domains. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution. Surprisingly, this can happen even when the surrogate data is unrelated to the original ones. We trace back this behavior to the classical Stein\u2019s paradox. $(i i)$ In order to reap the benefit of surrogate data, it is crucial to use optimally weighted ERM. (iii) The test error of models trained on mixtures of real and surrogate data is approximately described by a scaling law. This scaling law can be used to predict the optimal weighting scheme, and to choose the amount of surrogate data to add. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction and overview ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1.1 Motivation and formulation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider a standard learning setting where we are given $n$ i.i.d. points $\\boldsymbol{z}_{i}$ from a target distribution $\\mathcal{D}$ . Given a family of parametric models governed by the parameters\u2019 vector $\\pmb{\\theta}$ , the goal is to find $\\pmb{\\theta}$ that minimizes the expected test loss $R_{\\mathrm{test}}(\\pmb\\theta)$ incurred by the model predictions, where expectation is taken over the distribution $\\mathcal{D}$ . In many application domains, the available data $Z=(z_{i})_{i\\leq n}$ from the target distribution, referred to as either real or original data, may be difficult or expensive to acquire. One may then attempt to supplement these data with a different, cheaper source. Examples of such cheaper sources are $(i)$ publicly available datasets; $(i i)$ datasets owned by the same research group or company but acquired in different circumstances, e.g. in a different location; $(i i i)$ synthetic data produced by a generative model. ", "page_idx": 0}, {"type": "text", "text": "We will denote the data points obtained from this source by $\\boldsymbol{z}_{i}^{s}$ , and assume we have $m$ of them. We assume the \u2018surrogate\u2019 data $Z^{s}=(z_{i}^{s})_{i\\leq m}$ to be i.i.d. samples with distribution $\\mathcal{D}^{s}$ . In general, we will not assume the distribution $\\mathcal{D}^{s}$ of synthetic data to be close to the original data distribution $\\mathcal{D}$ . However we assume that these distributions are over the same domain. A number of questions arise: $(i)$ How should we use the surrogate data in training? $(i i)$ How many surrogate samples should we add to the original data? $(i i i)$ Can we predict the improvement in test error achieved by adding surrogate samples to the training? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A natural approach would be to add the surrogate data to the original one in the usual training procedure, and indeed many authors have explored this approach (see Section 1.3). Namely, one attempts to minimize the overall empirical risk $\\begin{array}{r}{\\widehat{R}_{n+m}^{\\mathrm{naive}}(\\pmb{\\theta})=\\sum_{i=1}^{n}\\ell(\\pmb{\\theta};\\pmb{z}_{i})+\\sum_{i=1}^{m}\\ell(\\pmb{\\theta};\\pmb{z}_{i}^{s})}\\end{array}$ , where $\\ell(z,\\theta)$ is a train loss function. ", "page_idx": 1}, {"type": "text", "text": "However, a moment of reflection reveals that this approach has serious shortcomings. Consider a simple mean estimation problem, whereby $z_{i}\\sim\\mathsf{N}(\\pmb{\\theta}_{*},\\pmb{I}_{d}),z_{i}^{s}\\sim\\mathsf{N}(\\pmb{\\theta}_{*}^{s},\\pmb{I}_{d}),\\ell(\\pmb{\\theta};z)=\\|\\pmb{\\theta}-z\\|^{2}$ , and $\\overset{\\cdot}{R_{\\mathrm{test}}}(\\pmb{\\theta})=\\lVert\\pmb{\\theta}-\\pmb{\\theta}_{\\ast}\\rVert^{2}$ . A straightforward calculation yields that the test error of the empirical risk minimizer $\\widehat{\\pmb{\\theta}}_{n+m}^{\\mathrm{naive}}:=\\arg\\operatorname*{min}\\widehat{R}_{n+m}^{\\mathrm{naive}}(\\pmb{\\theta})$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{\\mathrm{test}}(\\hat{\\pmb{\\theta}}_{n+m}^{\\mathrm{naive}})=\\left(\\frac{m}{n+m}\\right)^{2}\\|\\pmb{\\theta}_{*}^{s}-\\pmb{\\theta}_{*}\\|^{2}+\\frac{1}{n+m}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "As $m$ increases the variance (the second term) decreases, but the bias due to the difference $\\lVert\\pmb{\\theta}_{*}^{s}-\\pmb{\\theta}_{*}\\rVert$ increases, and the error approaches $\\lVert\\pmb{\\theta}_{*}^{s}-\\pmb{\\theta}_{*}\\rVert^{2}$ , i.e. the model will be only as good as if training only on surrogate data. ", "page_idx": 1}, {"type": "text", "text": "In order to overcome these limitations, we study a weighted ERM approach, and will show that the weight plays a crucial role. Namely, we consider the following regularized empirical risk: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{R}_{n,m}(\\pmb{\\theta};\\alpha):=\\frac{1-\\alpha}{n}\\sum_{i=1}^{n}\\ell(\\pmb{\\theta};\\pmb{z}_{i})+\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\ell(\\pmb{\\theta};\\pmb{z}_{i}^{s})+\\Omega(\\pmb{\\theta})\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\alpha\\in[0,1]$ is the weight of the surrogate dataset and $\\Omega:\\mathbb R^{d}\\to\\mathbb R_{\\geq0}$ is a regularizer, e.g. a ridge $\\Omega(\\pmb\\theta)\\dot{=}\\lambda\\|\\pmb\\theta\\|_{2}^{2}$ . We denote by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha):=\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\widehat{R}_{n,m}(\\pmb{\\theta};\\alpha)}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "the corresponding empirical risk minimizer, and by $R_{\\mathrm{test}}(\\hat{\\pmb\\theta}_{n,m}(\\alpha))$ the corresponding test error. ", "page_idx": 1}, {"type": "text", "text": "For supervised learning tasks, a sample $_{z}$ is represented as $\\boldsymbol{z}=(y,\\boldsymbol{x})$ , where $\\pmb{x}\\in\\mathbb{R}^{d}$ is covariate vector and $y\\in\\mathbb R$ is response variable and $\\pmb{\\theta}$ parametrizes a family of models $f(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{\\theta}})$ that predict the response $y$ given covariate vector $\\textbf{\\em x}$ . We consider losses of the form $\\ell(\\pmb\\theta,z)=L(y,f(\\pmb x;\\pmb\\theta))$ and $R_{\\mathrm{test}}(\\pmb\\theta):=\\mathbb{E}_{z\\sim\\mathcal{D}}L_{\\mathrm{test}}(y,f(\\pmb x;\\pmb\\theta))$ for some functions $L$ and $L_{\\mathrm{test}}$ . We allow for the test loss $L_{\\mathrm{test}}$ to be different from the train loss $L$ , but we will omit the subscript \u2018test\u2019 from the risk $R$ and the loss $L$ whenever clear from the context. ", "page_idx": 1}, {"type": "text", "text": "Figure 1 provides a preview of our results, for a sentiment analysis task. (Technical details provided in Section 4 and Appendix A.3). Each frame corresponds to a different combination of $n$ and $m$ , and we report the test error of our approach as a function of the weight parameter $\\alpha$ (red circles). Solid lines report the prediction of a scaling law that will be one of the main results presented below. ", "page_idx": 1}, {"type": "text", "text": "We observe that the weighted ERM approach systematically achieves better test error than either training only on original data $(\\alpha\\rightarrow0)$ ) or on surrogate data $(\\alpha\\rightarrow1)$ ). Further the error for optimal $\\alpha$ is always monotone decreasing both in $m$ and $n$ , and the approach outperforms the naive unweighted approach. This is shown more clearly in Figure 2, which also shows that the performance of unweighted ERM can degrade with more surrogate data. Also, while scaling laws typically do not capture the dependence on hyperparameters, the scaling law presented below predicts the dependence on $\\alpha$ reasonably well. This is particularly useful, because such a scaling law can be used to tune $\\alpha$ optimally and to predict the amount of surrogate data needed. ", "page_idx": 1}, {"type": "text", "text": "1.2 Summary of results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study the method outlined above both mathematically and via numerical experiments. Our mathematical results are developed in four different settings: $(i)$ The Gaussian sequence model (Section 3.1); $(i i)$ A non-parametric function estimation setting (Section 3.2); (iii) Low-dimensional empirical-risk minimization (Section 3.3); $(i v)$ High dimensional ridge regression (Section 3.4); ", "page_idx": 1}, {"type": "text", "text": "We carry out experiments with the following data sources. (1) Simulated data from linear or Gaussian mixture models: this allows us to explicitly control the distribution shift between the original and surrogate datasets, as well as check our theoretical results in a controlled setting. (2) Real natural language processing (NLP) data for sentiment analysis, with the role of original dataset played by IMDB reviews and the role of surrogate datasets played respectively by Rotten Tomatoes review and Goodreads book reviews. (3) Progression-free survival analysis using Lasso on TCGA PanCancer dataset with female patients data and male patients data as original and surrogate data, respectively. (4) Real image classification data, with CIFAR-10 and CIFAR-100 datasets respectively playing the role of original and surrogate data. Our results support the following conclusions: ", "page_idx": 1}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/6150f2ce51a1b846005667f45eeba4028126474f3a2deee2941d9c2d0bf0e1a5.jpg", "img_caption": ["Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4). "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/270b584d30ab6c4aa5a0fb677a9a83a231e8365aff3aafab52624fa65ede9b47.jpg", "img_caption": ["Figure 2: Performance of unweighted vs weighted ERM approach for the setting in Figure "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Surrogate data improve test error. Including surrogate data in training generally improves the test error on the original data, even if the surrogate data distribution is far from the original one. In agreement with the interpretation of surrogate data as a regularizer (see also Sec. 2), the improvement is generally positive, although its size depend on the data distributions. ", "page_idx": 2}, {"type": "text", "text": "Tuning of $\\alpha$ . The above conclusion holds under the condition that $\\alpha$ can be tuned (nearly) optimally. For each of the theoretical settings already mentioned, we characterize this optimal value. We verify that nearly optimal $\\alpha$ can be effectively selected by minimizing the error on a validation split of the original data. An attractive alternative is to use the scaling law we discuss next. ", "page_idx": 3}, {"type": "text", "text": "Scaling law. We propose a scaling law that captures the behavior of the test error with $n,m,\\alpha$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))-R_{*}\\approx\\alpha^{2}R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)+\\left[\\alpha^{2}\\big(R_{\\mathsf{s u}}^{\\mathsf{e x}}(m)-R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)\\big)^{1/\\beta}+(1-\\alpha)^{2}R_{\\mathsf{o r}}^{\\mathsf{e x}}(n)^{1/\\beta}\\right]^{\\beta}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $R_{*}$ is the minimal (Bayes) error, $R_{\\mathsf{s u}}^{\\mathsf{e x}}(m):=R(\\hat{\\pmb{\\theta}}_{0,m}(1))-R_{*}$ is the excess test error when training on the surrogate data (and testing on original), $R_{\\mathsf{o r}}^{\\mathsf{e x}}(n):=R(\\hat{\\pmb{\\theta}}_{n,0}(0))-R_{*}$ is the excess test error1 when training on original data (and testing on original), and $\\beta$ is a scaling exponent as described in Section 4. The above scaling admits natural generalizations; see Section 5. ", "page_idx": 3}, {"type": "text", "text": "Practical uses of the scaling law. Given data $\\{z_{i}\\}_{i\\leq n}$ and a source of surrogate data, we would like to predict how much the test error can be decreased by including any number $m$ of surrogate samples in the mix. The scaling law (4) suggests a simple approach: (1) Learn models on purely original data to extract the behavior of test loss $R(\\hat{\\pmb{\\theta}}_{n,0}(0))$ .; (2) Learn models on purely surrogate data to extract the behavior of $R(\\hat{\\pmb\\theta}_{0,m}(1))$ . (A relatively small sample is sufficient for this step.) (3) Use the minimum over $\\alpha$ of Eq. (4) to predict the test error at any given pair $n,m$ . ", "page_idx": 3}, {"type": "text", "text": "We can further leverage the scaling law to achieve the desired error by: $(I)$ Using the scaling law to determine the number of surrogate samples needed to achieve the desired performance. $(I I)$ Acquiring the surrogate samples and train the model using weighted ERM with optimal weighting predicted by scaling law. ", "page_idx": 3}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The use of surrogate data to enhance training has attracted increasing research effort, also because of the recent progresses in generative modeling. ", "page_idx": 3}, {"type": "text", "text": "This line of work has largely focused on the techniques to generate synthetic data that are well suited for training. A wide variety of methods have been demonstrated to be useful in generating data for computer vision tasks, ranging from object classification to semantic segmentation $[\\mathrm{RS}\\bar{\\mathrm{M}}^{+}16\\$ , $\\mathrm{JRBM^{+}}\\bar{1}7$ , $\\mathrm{AAMM^{+}18}$ , $\\mathrm{TPA}^{+}18$ , CLCG19, $\\mathrm{HSY}^{+}22$ , $\\mathrm{MPT}^{+}22$ , $\\mathrm{YCFB}^{+}22\\mathrm{j}$ ]. We refer to [SLW20] for a review. More recently, synthetic data have been used for training in natural language processing $[\\mathrm{HNK}^{+}22$ , MHZH22]. ", "page_idx": 3}, {"type": "text", "text": "Scaling laws have been broadly successful in guiding the development of large machine learning models $[\\mathrm{H}\\bar{\\mathrm{N}}\\mathrm{A}^{+}17$ , RRBS19, $\\mathrm{HK}\\dot{\\mathrm{K}}^{+}20$ , $\\mathrm{KMH}^{+}2\\bar{0}$ , $\\mathrm{TD}\\bar{\\bf R}^{+}21$ , HKHM21, $\\mathrm{HBM}^{\\mp}22$ , ANZ22, $\\mathrm{MR}\\bar{\\mathrm{B}}^{+}23]$ . We expect them to be similarly useful for integrating heterogeneous data into training. The change in scaling laws when training on synthetic data was the subject of a recent empirical study $[\\mathrm{FCK}^{+}23]$ . On the other hand, no systematic attempt was made at integrating real and synthetic data. ", "page_idx": 3}, {"type": "text", "text": "In data augmentation [KSH12, SK19], the original samples are supplemented with transformed or noisy version of the same. In contrast, we assume that surrogate data is obtained from a different source than the original one, and the surrogate samples are independent of the original samples. ", "page_idx": 3}, {"type": "text", "text": "The problem we consider was also studied within \u2018domain adaptation\u2019, a subarea of transfer learning [MPRP16, TJJ20]. Among others, $[\\mathrm{BDBC^{+}10}]$ establishes bounds on the generalization error of weighted ERM via uniform convergence. However these bounds do not reveal the full advantage achieved by this approach and are not precise enough to justify the scaling laws that we derive. Recent works in domain adaptation study the behavior of test error error [Has21, KJSJ24, $\\mathrm{YLS}^{+}24]$ and its scaling laws [Has21], but only consider vanilla ERM, a special of weighted ERM considered here. ", "page_idx": 3}, {"type": "text", "text": "2 Regularization, Gaussian mean estimation, Stein paradox ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The role of the parameter $\\alpha$ can be understood by considering the limit $m\\rightarrow\\infty$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{R}_{n,\\infty}(\\pmb{\\theta};\\alpha)=\\frac{1-\\alpha}{n}\\sum_{i=1}^{n}\\ell(\\pmb{\\theta};z_{i})+\\alpha\\,R^{s}(\\pmb{\\theta})+\\Omega(\\pmb{\\theta})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $R^{s}(\\pmb\\theta)=\\mathbb{E}_{\\pmb{z}^{s}\\sim\\mathcal{D}^{s}}\\ell(\\pmb\\theta;\\pmb z^{s})$ is the population risk for surrogate data. This suggests to think of the surrogate data as an additional (highly non-trivial) regularizer, with parameter $\\alpha$ . This leads to a simple yet important insight: adding surrogate data to the original data is beneficial if $\\alpha$ is chosen optimally, and large $m$ reduces statistical fluctuations in this regularizer. This contrasts with the unweighted approach whose test error in general deteriorates for large $m$ . ", "page_idx": 4}, {"type": "text", "text": "As a toy example, reconsider the mean estimation problem mentioned in the introduction: $z_{i}\\sim$ $\\mathsf{N}(\\pmb\\theta_{*},\\pmb I_{d})$ and $z_{i}^{s}\\sim\\mathsf{N}(\\pmb{\\theta}_{*}^{s},\\pmb{I}_{d}),\\ell(\\pmb{\\theta};z)=\\|\\pmb{\\theta}-z\\|^{2}$ and $R_{\\mathrm{test}}(\\pmb\\theta)=\\|\\pmb\\theta-\\pmb\\theta_{\\ast}\\|^{2}$ . We have $\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)=$ $\\textstyle(1-\\alpha)\\sum_{i\\leq n}z_{i}/n+\\alpha\\sum_{i\\leq m}z_{i}^{s}/m$ . In other words, the weighted ERM shrinks the mean of the original data towards the mean of the surrogate data. For a given $\\alpha$ , the resulting test errors are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))=\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+\\left(\\frac{\\alpha^{2}}{m}+\\frac{(1-\\alpha)^{2}}{n}\\right)d\\,,\\quad R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)=\\|\\pmb{\\theta}_{*}-\\pmb{\\theta}_{*}^{s}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and for the optimum value $\\begin{array}{r}{\\alpha_{*}=\\arg\\operatorname*{min}_{\\alpha}R(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))}\\end{array}$ , this yields ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha_{*}))=\\left(\\frac{R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+d/m}{R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+d/m+d/n}\\right)\\cdot\\frac{d}{n}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $1/n$ is the error of training only on original data and the prefactor is always strictly smaller than one. Hence, weighted ERM always achieves better error than training only on original data, regardless of the distance between original and surrogate data, although the improvement is larger for small $R_{\\mathrm{{su}}}^{\\mathsf{e x}}(\\infty)$ . This might seem paradoxical at first. As mentioned above, we are shrinking towards an arbitrary point given by the empirical mean of the surrogate data: how can this help? ", "page_idx": 4}, {"type": "text", "text": "In fact, this is a disguised version of the celebrated Stein paradox [EM77, Ste81]: in estimating a Gaussian mean, a procedure that shrinks the empirical mean towards an arbitrary point by a carefully chosen amount outperforms the naive empirical mean. In our toy example, the naive empirical mean corresponds to estimation purely based on the original data, and we shrink it towards the mean of the surrogate data. Of course, the improvement over empirical mean is only possible if $\\alpha$ is chosen optimally. Equation (6) assumes $\\alpha=\\alpha_{*}$ is chosen by an oracle that knows the value of $R_{\\mathrm{{su}}}^{\\mathsf{e x}}(\\infty)$ . Stein\u2019s analysis implies that in the Gaussian mean problem, $\\alpha$ can be chosen empirically as long as the dimension of $\\pmb{\\theta}$ is $d\\geq3$ . In the settings we are interested in, $\\alpha$ can be chosen via cross-validation. ", "page_idx": 4}, {"type": "text", "text": "3 Theoretical results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Gaussian sequence model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The sequence model captures the behavior of many models in non-parametric statistics while being simpler to analyze [Tsy09, GN21]. It is also known to approximate the behavior of overparametrized linear regression [CM22]. The unknown target is $\\pmb{\\theta}_{\\ast}\\in\\bar{\\mathbb{R}}^{d}$ (with potentially $d=\\infty$ ), and we observe ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{y}_{i}=\\pmb{\\theta}_{*}+\\sigma\\,\\pmb{g}_{i},\\;i\\leq n\\,,\\quad\\pmb{y}_{i}^{s}=\\pmb{\\theta}_{*}^{s}+\\sigma_{s}\\,\\pmb{g}_{i}^{s},\\;i\\leq m\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{*}^{s}$ is also unknown, and $\\pmb{g}_{i},\\pmb{g}_{i}^{s}\\sim\\mathsf{N}(\\mathbb{0},\\pmb{I}_{d})$ are i.i.d. We study the penalized estimator ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{\\theta}}_{n,m}(\\alpha):=\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\left\\{\\frac{(1-\\alpha)}{n}\\sum_{i=1}^{n}\\|\\pmb{y}_{i}-\\pmb{\\theta}\\|_{2}^{2}+\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\|\\pmb{y}_{i}^{s}-\\pmb{\\theta}\\|_{2}^{2}+\\lambda\\|\\pmb{\\theta}\\|_{\\Omega}^{2}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lVert\\pmb{\\theta}\\rVert_{\\Omega}^{2}=\\langle\\pmb{\\theta},\\Omega\\pmb{\\theta}\\rangle$ and $\\Omega\\succeq\\mathbf{0}$ is a regularization weight matrix. We will be concerned with the expected risk ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{n,m}(\\alpha,\\lambda)=\\mathbb{E}\\bigg\\{\\big\\|\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)-\\pmb{\\theta}_{\\ast}\\big\\|^{2}\\bigg\\}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of the next result is presented in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $\\omega_{1}\\leq\\omega_{2}\\leq\\cdots$ be the ordered eigenvalues of $\\Omega$ , and denote by $\\pmb{v}_{i}$ the corresponding eigenvectors. Further denote by $\\theta_{\\ast,>k},\\theta_{\\ast,>k}^{s}$ the projections of $\\boldsymbol{\\theta}_{*}$ , $\\theta_{*,s}$ onto span $\\left(\\pmb{v}_{i}:i>k\\right)$ , and similarly for $\\theta_{\\ast,\\leq k},\\,\\theta_{\\ast,\\leq k}^{s}$ . Assume that $\\omega_{k}\\asymp k^{\\mu}$ , $\\mu>1/2$ , $\\|\\pmb{\\theta}_{*,>k}\\|^{2}\\leq C_{\\theta}k^{-2\\rho}$ , $\\rho\\neq\\mu,$ , and let $\\Delta_{k}$ be such that (for all $k$ ): $\\Delta_{k}:=\\omega_{k}^{-1}|\\langle\\pmb{\\theta}_{*,\\leq k}-\\pmb{\\theta}_{*,\\leq k}^{s},\\pmb{\\theta}_{*,\\leq k}\\rangle\\Omega|\\leq C_{0}k^{-2(\\mu\\wedge\\rho)}$ . Then the following hold: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{n,m}\\bigl(\\alpha,\\lambda_{*}(\\alpha)\\bigr)\\leq\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+C\\cdot\\left[(1-\\alpha)^{2}\\frac{\\sigma^{2}}{n}+\\alpha^{2}\\frac{\\sigma_{s}^{2}}{m}\\right]^{\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$(b)$ If $\\mu>2\\rho-1/2,$ , there exists $C^{\\prime}>0$ and there exist $\\theta_{\\ast},\\theta_{\\ast}^{s}$ satisfying the assumptions in point $(a)$ , such that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\lambda}R_{n,m}\\mathopen{}\\mathclose\\bgroup\\left(\\alpha,\\lambda\\aftergroup\\egroup\\right)\\geq\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+C^{\\prime}\\cdot\\left[(1-\\alpha)^{2}\\frac{\\sigma^{2}}{n}+\\alpha^{2}\\frac{\\sigma_{s}^{2}}{m}\\right]^{\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that since the theorem also implies $R_{\\mathsf{s u}}^{\\mathsf{e x}}(m)-R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)\\asymp(\\sigma_{s}^{2}/m)^{\\beta}$ and $R_{\\mathfrak{o r}}^{\\mathsf{e x}}(m)\\asymp(\\sigma^{2}/n)^{\\beta}$ , this result confirms the scaling law (4). ", "page_idx": 5}, {"type": "text", "text": "3.2 Non-parametric regression in Sobolev classes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we consider the classic non-parametric regression model. We assume that $n=Q^{d}$ for some integer $Q\\geq2$ , and the original data $(\\pmb{x}_{i},y_{i})_{i\\leq n}$ are defined through ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{i}=f_{*}(\\pmb{x}_{i})+\\varepsilon_{i}\\,,\\quad\\varepsilon_{i}\\sim\\mathsf{N}(0,\\sigma^{2})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\varepsilon_{i}$ are independent of $\\pmb{x}_{i}$ and of each other, and $\\{x_{i}\\}_{i\\leq n}$ equally spaced grid points in the $d$ -dimensional unit-cube, i.e. $\\mathcal{X}_{n}=\\{\\pmb{q}/Q:\\;\\pmb{q}\\in[Q]^{d}\\}$ . Surrogate data have a similar distribution, with $m=Q_{s}^{d}$ equally spaced points $\\boldsymbol{x_{i}^{s}}$ in the unit cube, and $y_{i}^{s}=f_{\\ast,s}(\\pmb{x}_{i}^{s})\\substack{+\\varepsilon_{i}^{s}}$ , where $\\varepsilon_{i}^{s}\\sim\\mathsf{N}(0,\\sigma_{s}^{2})$ . We assume that $f_{*}$ has small Sobolev norm, that is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|f_{*}\\|_{r,2}^{2}:=\\int_{[0,1]^{d}}\\big(|f_{*}(t)|^{2}+\\|f_{*}^{(r)}(t)\\|^{2}\\big)\\mathrm{d}t\\leq1\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recall that $\\|f\\|_{r,2}^{2}$ is a special reproducing kernel Hilbert space (RKHS) norm: we expect some of the considerations below to generalize to other RKHS norms. ", "page_idx": 5}, {"type": "text", "text": "Following our general methodology, we use the estimator ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}_{n,m,\\alpha}=\\arg\\operatorname*{min}_{f}\\left\\{\\frac{1-\\alpha}{n}\\sum_{i=1}^{n}\\left(y_{i}-f(x_{i})\\right)^{2}+\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(y_{i}^{s}-f(x_{i}^{s})\\right)^{2}+\\lambda\\|f\\|_{p,2}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We are interested in $R(f)=\\mathbb{E}\\{(f({\\pmb x})-f_{*}({\\pmb x}))^{2}\\}$ , which is the excess squared loss for a test point $\\pmb{x}\\sim\\mathsf{U n i f}([0,1]^{d})$ . ", "page_idx": 5}, {"type": "text", "text": "In order to avoid technical burden we will carry out the analysis for a continuous model, the socalled white noise model, where we observe the function $f$ at all points $x\\in[0,1]^{d}$ , perturbed by $d$ -dimensional white noise: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}Y=f_{*}(\\pmb{x})\\,\\mathrm{d}\\pmb{x}+\\frac{\\sigma}{\\sqrt{n}}\\mathrm{d}B(\\pmb{x})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and similarly for $Y^{s}$ . We use an estimator that naturally generalizes (13) to the continuous case. Our results for the white noise model are as follows. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $\\beta=(2p\\wedge4r)/(d+(2p\\wedge4r))$ . If $r>d/4$ and $\\lambda=(\\delta K_{n,m}\\sigma^{2})^{2r/(d+(2p\\wedge4r))}$ , then for every $\\delta\\in(0,1)$ there exists a constant $C=C(d,\\delta)$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(\\hat{f}_{n,m,\\alpha})\\leq(1+\\delta)\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+C\\left\\{(1-\\alpha)^{2}\\cdot\\frac{\\sigma^{2}}{n}+\\alpha^{2}\\cdot\\frac{\\sigma_{s}^{2}}{m}\\right\\}^{\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with high probability, where $K_{n,m}:=(1-\\alpha)^{2}/n+\\alpha^{2}/m.$ ", "page_idx": 5}, {"type": "text", "text": "Remark 3.1. The white noise model (14) is known to be equivalent to the original model (12) (with deterministic equispaced designs) in the sense of Le Cam, for $r>d/2$ [BL96, Rei08]. While suggestive, this equivalence does not allow us to formally deduce results for the data (12), because it does not apply to the specific estimators of interest here. ", "page_idx": 5}, {"type": "text", "text": "With the given choice of $\\lambda,\\,r.$ , the derivation of (15) also implies $R_{\\mathsf{s u}}^{\\mathsf{e x}}(m)-R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)\\geq C^{\\prime}(\\sigma_{s}^{2}/m)^{\\beta}$ , $R_{\\mathsf{o r}}^{\\mathsf{e x}}(n)\\geq C^{\\prime}(\\sigma/n)^{\\beta}$ (for the least favorable $f$ [Tsy09]). Hence (15) is consistent with the scaling law (4). ", "page_idx": 5}, {"type": "text", "text": "3.3 Low-dimensional asymptotics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We study the estimator of Eqs. (2), (3) under the classical asymptotics $n,m\\rightarrow\\infty$ at $d$ fixed. Since this type of analysis is more standard, we defer it to Appendix B. The main result of this analysis is that the scaling law (4) holds in this setting, with the classical parametric exponent $\\beta=1$ , for $\\alpha\\in[0,\\alpha_{\\mathrm{max}}]$ for a suitable $\\alpha_{\\mathrm{max}}\\in(0,1)$ . Importantly, the interval $\\left[0,\\alpha_{\\mathrm{max}}\\right]$ includes the optimal choice of the weight $\\alpha$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 High-dimensional linear regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we study ridge regression in the high-dimensional regime in which the number of samples is proportional to the number of parameters. Denoting the original data by $(y,X)$ (with $\\pmb{y}\\in\\mathbb{R}^{n}$ the vector of responses and $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ the matrix of covariates), and the surrogate data by $(\\pmb{y}^{s},\\pmb{X}^{s})$ (with $\\pmb{y}^{s}\\in\\mathbb{R}^{m}$ and $X^{s}\\in\\mathbb{R}^{m\\times d})$ , we minimize the regularized empirical risk ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{R}_{n,m}({\\pmb\\theta};{\\boldsymbol\\alpha})=\\frac{1-\\alpha}{2n}\\|{\\pmb y}-{\\pmb X}{\\pmb\\theta}\\|_{2}^{2}+\\frac{\\alpha}{2m}\\|{\\pmb y}^{s}-{\\pmb X}^{s}{\\pmb\\theta}\\|_{2}^{2}+\\frac{\\lambda}{2}\\,\\|{\\pmb\\theta}\\|_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We assume a simple distribution, whereby the rows of $\\mathbf{\\deltaX}$ , $X^{s}$ (denoted by $\\boldsymbol{x}_{i},\\,\\boldsymbol{x}_{i}^{s})$ ) are standard normal vectors and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{y}=X\\pmb{\\theta}_{*}+\\pmb{\\varepsilon}\\,,\\qquad\\pmb{y}^{s}=X^{s}\\pmb{\\theta}_{*}^{s}+\\pmb{\\varepsilon}^{s}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for $\\pmb\\varepsilon\\,\\sim\\,\\mathsf{N}(\\mathbf{0},\\sigma^{2}I_{n})$ , $\\pmb{\\varepsilon}^{s}\\;\\sim\\;\\mathsf{N}(\\mathbf{0},\\sigma_{s}^{2}\\pmb{I}_{m})$ . Note that the two data distributions differ in the true coefficient vectors $\\pmb{\\theta}_{*}$ versus $\\theta_{*}^{s}$ as well as in the noise variance. We will denote by $\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)$ the ridge estimator, $\\begin{array}{r}{\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha)=\\arg\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{R}^{d}}\\widehat{R}_{n,m}(\\pmb{\\theta};\\alpha)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "The excess test error (for square loss) is given by $R(\\hat{\\pmb{\\theta}}):=\\mathbb{E}\\big\\{\\big(\\langle\\pmb{\\mathscr{x}},\\pmb{\\theta}_{*}\\rangle-\\langle\\pmb{\\mathscr{x}},\\hat{\\pmb{\\theta}}\\rangle\\big)^{2}\\big\\}=\\|\\hat{\\pmb{\\theta}}-\\pmb{\\theta}_{*}\\|^{2}$ .   \nThe next result characterizes this error in the proportional asymptotics. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Consider the ridge regression estimator $\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)$ . Let r := \u2225\u03b8\u2217\u22252, $r_{s}:=\\|\\pmb{\\theta}_{*}^{s}\\|_{2}$ and $\\gamma:=\\cos^{-1}(\\langle\\pmb{\\theta}_{*},\\pmb{\\theta}_{*}^{s}\\rangle/(\\lVert\\pmb{\\theta}_{*}\\rVert_{2}\\lVert\\pmb{\\theta}_{*}^{s}\\rVert_{2}))$ . Assume $n,m,d\\rightarrow\\infty$ such that $n/d\\rightarrow\\delta$ , $m/d\\rightarrow\\delta_{s}$ , with $\\delta+\\delta_{s}>1^{2}$ . For $\\mathcal{R}(.)$ defined in Appendix E.1, let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi^{*}(\\alpha),\\xi_{\\perp}^{*}(\\alpha),\\omega^{*}(\\alpha)=\\operatorname*{argmin}_{\\xi,\\xi_{\\perp}\\geq0,\\omega\\geq0}\\mathcal{R}(\\xi,\\xi_{\\perp},\\omega;\\alpha),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "be the unique minimizer. Then for any $\\varepsilon,\\varepsilon_{0}>0$ , there exist $c>0$ such that, for all $n$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\operatorname*{sup}_{\\alpha\\in[\\varepsilon_{0},1-\\varepsilon_{0}]}\\big|R(\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha))-\\mathcal{R}_{\\mathrm{test}}(\\alpha)\\big|\\leq\\varepsilon\\Big)\\geq1-2\\,e^{-c n}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{R}_{\\mathrm{test}}(\\alpha):=(\\xi^{*}(\\alpha)-r)^{2}+(\\xi_{\\perp}^{*}(\\alpha))^{2}+(\\omega^{*}(\\alpha))^{2}$ . Further, we can take $\\varepsilon_{0}=0\\,i f\\,\\delta,\\delta_{s}>1.$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 3.2 (Optimizing $\\alpha$ over the validation set). Note that the concentration of $R(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))$ around the theoretical prediction $\\mathcal{R}_{\\mathrm{test}}(\\alpha)$ in Theorem 3 is uniform over $\\alpha\\in[\\varepsilon_{0},1-\\varepsilon_{0}]$ . This means that we can find the optimal $\\alpha$ by computing $\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)$ over a grid of $\\alpha$ values, estimating $R(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))$ over the validation set and choosing the optimal $\\alpha$ . The uniform guarantee insures that this procedure will achieve risk $\\mathrm{min}_{\\alpha\\in[0,1]}\\,\\mathcal{R}_{\\mathrm{test}}(\\bar{\\alpha})+o_{P}^{\\bigcirc}(1)$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 3.3 (Relation to scaling laws). An analysis of the equations for $(\\xi^{*},\\xi_{\\perp}^{*},\\omega^{*})$ reveals that, for large $\\delta,\\delta_{s}$ , the predicted excess risk behaves as $\\mathcal{R}_{\\mathrm{test}}(\\alpha)=\\alpha^{2}\\mathcal{R}_{s,\\infty}^{*}+\\alpha^{2}C_{1}/\\delta_{s}+(1-\\alpha)^{2}C_{2}/\\delta+$ $o(1/\\delta,1/\\delta_{s})$ (for some constants $\\mathcal{R}_{s,\\infty}^{*},C_{1},C_{2})$ . This matches the low-dimensional asymptotics and our scaling law (4) with $\\beta=1$ . In practice, we find that, for moderate $\\delta,\\delta_{s}$ , the behavior of $\\mathcal{R}_{\\mathrm{test}}(\\alpha)$ is better approximated by a different value of $\\beta$ (see Appendix A.) ", "page_idx": 6}, {"type": "text", "text": "4 Empirical results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present experiments validating that the scaling law (4) is a good approximation both for simulated and real-world data. For simulated data, we select two different distributions for the original and surrogate datasets. The test and validation sets are generated from the same distribution as the original dataset. In case of real-world data, we choose two different datasets as the original and surrogate datasets. We split the original dataset into train, test, and validation sets, while all examples in the surrogate datasets are allocated solely to the train split. ", "page_idx": 6}, {"type": "text", "text": "For each dataset and model discussed in this section, we carry out the same experiment: $(i)$ We use models trained on original data to fit the scaling curve $R(\\hat{\\pmb{\\theta}}_{n,0}(0))\\,=\\,A_{\\mathsf{o r}}+B_{\\mathsf{o r}}n^{-\\beta_{0}}$ and obtain $\\boldsymbol{A}_{\\circ\\boldsymbol{r}}$ and $\\beta_{\\mathsf{o r}}\\,\\left(i i\\right)$ We use models trained on purely surrogate data to fit the scaling curve $R(\\hat{\\pmb{\\theta}}_{0,m}(1))=A_{\\mathsf{s u}}+B_{\\mathsf{s u}}m^{-\\beta_{\\mathsf{s u}}}$ to obtain $A_{\\mathsf{s u}}$ and $\\beta_{\\mathrm{su}}$ . $(i i i)$ Since assume $R_{*}=R(\\hat{\\pmb{\\theta}}_{\\infty,0}(0))$ , we let ", "page_idx": 6}, {"type": "text", "text": "$R_{*}=A_{\\mathsf{o r}}$ and excess risk estimates $R_{\\mathsf{o r}}^{\\mathsf{e x}}(n)=R(\\hat{\\pmb{\\theta}}_{n,0}(0))-A_{\\mathsf{o r}}$ , $R_{\\mathsf{s u}}^{\\mathsf{e x}}(m)=R(\\hat{\\pmb{\\theta}}_{0,m}(1))-A_{\\mathsf{o r}}$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=A_{\\mathsf{s u}}-A_{\\mathsf{o r}}$ , and we use $\\beta=\\beta_{\\mathsf{o r}}$ , the fit exponent obtained from original data); $(i v)$ For each combination of $n,m$ , we use our estimates of $R_{\\mathrm{su}}^{\\ e x}(m)$ , $R_{\\mathsf{o r}}^{\\mathsf{e x}}(n)$ (as measured empirically on the test set), $\\beta,R_{\\mathtt{s u}}^{\\mathtt{e x}}(\\infty)$ , and $R_{*}$ to plot the predicted $R(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))$ as a function of $\\alpha$ using scaling law (4). $(v)$ We then train the model using $n$ original and $m$ surrogate examples with weights $(1-\\alpha)$ and, $\\alpha$ for the two datasets, respectively. We average the results of 10 independent runs to compare it against those predicted by the scaling law. For ridge regression, we also compare with exact high-dimensional asymptotics from Theorem 3. ", "page_idx": 7}, {"type": "text", "text": "Let us emphasize that these plots probe the dependence on the hyperparameter $\\alpha$ . These are much more demanding tests that the usual ones in scaling laws. We generally observe that the scaling law captures well the behavior of the test error for data mixtures. Furthermore, we perform experiments for variety of loss functions to show these scaling laws hold more widely than the theoretical settings we considered. ", "page_idx": 7}, {"type": "text", "text": "Binary classification with Gaussian mixture data This is a simple simulated setting. The original dataset consists of independent and identically distributed examples $(y_{i},\\pmb{x}_{i})\\in\\mathbb{R}\\times\\mathbb{R}^{d}$ , $d=200$ , where $y_{i}$ is uniform over $\\{+1,-1\\}$ , and $\\pmb{x}_{i}\\big|_{y_{i}}\\sim\\mathsf{N}(y_{i}\\pmb{\\theta}_{*},\\pmb{I}_{d})$ , where $\\pmb{\\theta}_{\\ast}\\in\\mathbb{R}^{d}$ , $\\lVert\\pmb{\\theta}_{*}\\rVert=1$ . Surrogate data have the same distribution, with a different unit vector $\\theta_{*,s}$ . This data distribution is parametrized by $d$ and the angle $\\gamma$ between the original and surrogate parameters, $\\cos\\gamma:=\\langle\\pmb{\\theta}_{*},\\hat{\\pmb{\\theta}}_{*,s}\\rangle$ . We use $\\gamma=\\pi/10$ in our experiments. For each $(n,m,\\alpha)$ , we averaged the results over 10 independent runs. ", "page_idx": 7}, {"type": "text", "text": "We use two different models for classification: (1) Logistic regression; (2) A one-hidden layer neural network with 32 hidden ReLU neurons. The results for both models are presented in Appendix A.1. ", "page_idx": 7}, {"type": "text", "text": "Linear regression with Gaussian mixture data For the Gaussian mixture data generation setup described above, we also perform ridge regression. The results (presented in Appendix A.2) demonstrate that classification loss and square loss often have a similar qualitative behavior as a function of weight $\\alpha$ , as seen by comparing the classification loss in Figure 7 and the squared loss in Figure 11 for the same setup. Although our theoretical results do not apply directly to classification loss, we believe that our qualitative conclusions generalize. This is confirmed by the similar behavior between the two losses and the successful prediction of actual risk by scaling laws in our classification experiments. ", "page_idx": 7}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/6d2a37d1561980d5e219a43fd6361b53d0804b60f3f0017c2bc7ce6d2950f717.jpg", "img_caption": ["Figure 3: CIFAR10 and CIFAR100 data. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/fddcaaa2b49642eb0449abca750d81a3bdb108a3ac40794386d4cd41063899ba.jpg", "img_caption": ["Figure 4: Lasso-based Cox regression on TCGA PanCancer dataset. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Sentiment analysis in movie reviews As original data, we use the IMDB dataset (link) which has $25\\mathrm{k}$ reviews for training, each labeled as positive or negative. For validation and testing, we split the IMDB test dataset of $25\\mathrm{k}$ reviews into a validation set of 10k reviews and test set of $15\\mathbf{k}$ reviews. ", "page_idx": 8}, {"type": "text", "text": "We experiment with two different surrogate datasets: 1) Rotten Tomatoes dataset of movie reviews (link): these are data with different distribution but within the same domain. This dataset contains movie reviews and the corresponding sentiments, 2) Goodreads book reviews (link): these are data from a substantially different domain. This dataset has reviews and their ratings. We choose 10k reviews each with a rating of 5 and 1, and label them as positive and negative, respectively. ", "page_idx": 8}, {"type": "text", "text": "We convert reviews into feature vectors with $d=884$ dimensions as explained in Appendix A.3. We use logistic regression and neural network models with the same set of parameters as in the Gaussian mixture experiments (except for the input dimension). ", "page_idx": 8}, {"type": "text", "text": "Results with neural nets and Rotten Tomatoes as synthetic dataset are presented in Figure 1 and the remaining results are in Appendix A.3. ", "page_idx": 8}, {"type": "text", "text": "Image classification with CIFAR10 and CIFAR100 We use 50,000 CIFAR10 training images as original data, its 10 classes for the classification task, and test on the 10,000 CIFAR10 test images. We use 50,000 CIFAR100 training images as surrogate data. We train a 9-layer ResNet model for classification. Appendix A.4 presents details on the data pre-processing and mapping of labels. Results are shown in Figure 3. Note that CIFAR10 and CIFAR100 datasets are quite different from each other, as they have no overlap either in the images or in their label sets. Yet, the test error on training on their mixture is well predicted by the scaling law (4). ", "page_idx": 8}, {"type": "text", "text": "Lasso-based Cox regression on TCGA PanCancer dataset We use the public domain TCGA pancancer dataset $[\\mathrm{GCH}^{+}20]$ (link), with gene expressions as covariates and progression-free survival (PFS) as response. After flitering and feature selection, we are left with 3580 female patients, which we use as original data, and 3640 male patients, which we use as surrogate data. We fti CoxPHFitter model (link) with 500 selected genes and use \u201c1-concordance score\u201d as our loss function. The results are shown in Figure 4. The details of pre-processing and experiment parameters3 are in Appendix A.5. ", "page_idx": 8}, {"type": "text", "text": "High-dimensional ridge regression We simulate the data distribution in Section 3.4, i.e., $y_{i}=$ $\\langle\\pmb{\\theta}_{*},\\pmb{x}_{i}\\rangle+\\varepsilon_{i}$ , $i\\,\\leq\\,n$ ; $y_{i}^{s}\\;=\\;\\langle\\pmb{\\theta}_{\\ast,s},\\pmb{x}_{i}^{s}\\rangle+\\varepsilon_{i}^{s}$ , $i\\,\\leq\\,m$ ; with $\\pmb{x}_{i},\\pmb{x}_{i}^{s}\\;\\sim\\;\\mathsf{N}(\\mathbf{0},\\pmb{I}_{d})$ , $\\varepsilon_{i}\\;\\sim\\;\\mathsf{N}(0,\\sigma^{2})$ , $\\varepsilon_{i}^{s}\\sim\\mathsf{N}(0,\\sigma_{s}^{2})$ , and fti a simple linear model using ridge regression. The results are shown in Figure 5. In our experiments, we use $d=500$ , $\\sigma^{2}=\\sigma_{s}^{2}=\\overline{{1}}$ , $\\lVert\\bar{\\boldsymbol{\\theta}}_{*}\\rVert\\bar{=}\\,\\lVert\\boldsymbol{\\theta}_{*,s}\\rVert=1$ and regularization parameter $\\lambda=2^{-10}$ . Under these settings, the model is parametrized by the angle $\\gamma$ between $\\theta_{*}$ and $\\theta_{*,s}$ , where $\\cos\\gamma:=\\langle\\pmb{\\theta}_{\\ast},\\pmb{\\theta}_{\\ast,s}\\rangle$ . We used $\\gamma=\\pi/6$ and $\\pi/2$ in our experiments.4 ", "page_idx": 8}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/e10d0a9dd9c73f4951a61f11c1fb941b8a34ce40d70c74e53b440d4f3df0cb34.jpg", "img_caption": ["Figure 5: Ridge regression on simulated data. Here $d=500$ , $n=1000$ , $\\sigma^{2}=\\sigma_{s}^{2}=1$ , $\\lVert\\pmb\\theta_{*}\\rVert=$ $\\bar{\\|\\pmb{\\theta}_{*,s}\\|}=1$ , regul. par. $\\lambda=2^{-10}$ , and $m$ varies by column. Top row $\\gamma=\\pi/2$ , bottom row $\\gamma=\\pi/6$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The theoretical predictions of Theorem 3 for these curves in high-dimensional asymptotics $n,m,d\\rightarrow\\infty$ , with $n/d\\rightarrow\\delta,m/d\\rightarrow\\delta_{s}$ are reported as blue lines, and match remarkably well with the empirical data. The simple scaling law (4) nevertheless provides a good approximation of these (more complicated) theoretical formulas. ", "page_idx": 9}, {"type": "text", "text": "Note in particular that in the top row of Figure 5, we have $\\langle\\pmb{\\theta}_{*},\\pmb{\\theta}_{*,s}\\rangle=0$ , i.e. the surrogate data are as far as possible from the original ones. Nevertheless, the induced regularization effect leads to smaller test error on the original distribution. ", "page_idx": 9}, {"type": "text", "text": "We observe proposed scaling law (4) predicts well the behavior of the experiments, across of the datasets above, and for most combinations of original and surrogate examples we have tested. ", "page_idx": 9}, {"type": "text", "text": "Finally, we emphasize that the scaling law is only an empirical approximation of reality. This is clearly illustrated by the example of ridge regression: in this case, we use Theorem 3 to precisely predict the discrepancy between precise asymptotics and scaling law, see Appendix A.6. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conclude by discussing two possible generalizations of the scaling law (4), and its applicability. First, throughout this paper we assumed that $R_{\\mathsf{o r}}^{\\mathsf{e x}}(\\infty)=0$ , namely that we can achieve the Bayes error by training on infinitely many original samples. In practice this will not hold because of the limited model complexity. Following standard scaling laws $[\\mathrm{KMH^{+}}20\\$ , $\\mathrm{HBM}^{+}22]$ , this effect can be accounted for by an additional term $C\\cdot N^{-\\omega}$ , where $N$ is the model size (number of parameters). Second, the scaling law (4) implies as special cases that $R_{\\mathsf{o r}}^{\\mathsf{e x}}(n)\\approx A_{\\mathsf{o r}}n^{-\\beta}$ , $R_{\\mathrm{{su}}}^{\\mathrm{{ex}}}(m)\\approx$ $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)+A_{\\mathsf{s u}}m^{-\\beta}$ . In particular, the exponent $\\beta$ is the same when training on real or surrogate data. In practice, we observe often two somewhat different exponents $\\beta_{0\\up r}\\neq\\beta_{\\mathsf{s u}}$ . In these cases, we set $\\beta=\\beta_{\\mathsf{o r}}$ , and this appears to work reasonably well. However, we can imagine cases in which the difference between $\\beta_{\\mathsf{o r}}$ and $\\beta_{\\mathrm{su}}$ is significant enough (4) will stop being accurate. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are grateful to Joseph Gardi, Germain Kolossov, Marc Laugharn, Kaleigh Mentzer, Rahul Ponnala, and Pulkit Tandon, for several conversations about this work. This work was carried out while Andrea ", "page_idx": 9}, {"type": "text", "text": "Montanari was on leave from Stanford and a Chief Scientist at Granica (formerly known as Project N). The present research is unrelated to AM\u2019s Stanford research. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "$[\\mathrm{AAMM^{+}18}]$ Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger, and Carsten Rother, Augmented reality meets computer vision: Efficient data generation for urban driving scenes, International Journal of Computer Vision 126 (2018), 961\u2013 972.   \n[ANZ22] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai, Revisiting neural scaling laws in language and vision, Advances in Neural Information Processing Systems 35 (2022), 22300\u201322312.   \n$[\\mathbf{BDBC}^{+}10]$ Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan, A theory of learning from different domains, Machine learning 79 (2010), 151\u2013175.   \n[Bir06] Steven Bird, Nltk: the natural language toolkit, Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, 2006, pp. 69\u201372.   \n[BL96] Lawrence D Brown and Mark G Low, Asymptotic equivalence of nonparametric regression and white noise, The Annals of Statistics 24 (1996), no. 6, 2384\u20132398.   \n[CLCG19] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool, Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 1841\u20131850.   \n[CM22] Chen Cheng and Andrea Montanari, Dimension free ridge regression, arXiv:2210.08571 (2022).   \n[EM77] Bradley Efron and Carl Morris, Stein\u2019s paradox in statistics, Scientific American 236 (1977), no. 5, 119\u2013127.   \n$[\\mathrm{FCK}^{+}23]$ Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, and Yonglong Tian, Scaling laws of synthetic images for model training... for now, arXiv preprint arXiv:2312.04567 (2023).   \n$[\\mathrm{GCH}^{+}20]$ Mary J Goldman, Brian Craft, Mim Hastie, Kristupas Repec\u02c7ka, Fran McDade, Akhil Kamath, Ayan Banerjee, Yunhai Luo, Dave Rogers, Angela N Brooks, et al., Visualizing and interpreting cancer genomics data via the xena platform, Nature biotechnology 38 (2020), no. 6, 675\u2013678.   \n[GN21] Evarist Gin\u00e9 and Richard Nickl, Mathematical foundations of infinite-dimensional statistical models, Cambridge University Press, 2021.   \n[Gor85] Yehoram Gordon, Some inequalities for gaussian processes and applications, Israel Journal of Mathematics 50 (1985), no. 4, 265\u2013289.   \n[Has21] Tatsunori Hashimoto, Model performance scaling with multiple data sources, International Conference on Machine Learning, PMLR, 2021, pp. 4107\u20134116.   \n$[\\mathrm{HBM}^{+}22]$ Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al., Training compute-optimal large language models, arXiv preprint arXiv:2203.15556 (2022).   \n[HKHM21] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish, Scaling laws for transfer, arXiv preprint arXiv:2102.01293 (2021).   \n$[\\mathrm{HKK}^{+}20]$ Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al., Scaling laws for autoregressive generative modeling, arXiv preprint arXiv:2010.14701 (2020).   \n$[\\mathrm{HNA^{+}17}]$ Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou, Deep learning scaling is predictable, empirically, arXiv preprint arXiv:1712.00409 (2017).   \n$[\\mathrm{HNK}^{+}22]$ Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, and Mohammad Norouzi, Generate, annotate, and learn: Nlp with synthetic text, Transactions of the Association for Computational Linguistics 10 (2022), 826\u2013842.   \n$[\\mathrm{HSY}^{+}22]$ Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi, Is synthetic data from generative models ready for image recognition?, arXiv preprint arXiv:2210.07574 (2022).   \n$[\\mathbf{J}\\mathbf{R}\\mathbf{B}\\mathbf{M}^{+}17]$ Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan, Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?, 2017 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2017, pp. 746\u2013753.   \n[KJSJ24] Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, and Ruoxi Jia, Performance scaling via optimal transport: Enabling data selection from partially revealed sources, Advances in Neural Information Processing Systems 36 (2024).   \n$[\\mathrm{KMH}^{+}20]$ Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361 (2020).   \n[Kri09] Alex Krizhevsky, Learning multiple layers of features from tiny images, Tech. report, 2009.   \n[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, Imagenet classification with deep convolutional neural networks, Advances in neural information processing systems 25 (2012).   \n$[\\mathbf{MDP}^{+}11]$ Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts, Learning word vectors for sentiment analysis, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (Portland, Oregon, USA) (Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, eds.), Association for Computational Linguistics, June 2011, pp. 142\u2013150.   \n[MHZH22] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han, Generating training data with language models: Towards zero-shot language understanding, Advances in Neural Information Processing Systems 35 (2022), 462\u2013477.   \n[MM21] L\u00e9o Miolane and Andrea Montanari, The distribution of the lasso: Uniform control over sparse balls and adaptive parameter tuning, The Annals of Statistics 49 (2021), no. 4, 2313\u20132335.   \n[MPRP16] Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes, The benefti of multitask representation learning, Journal of Machine Learning Research 17 (2016), no. 81, 1\u201332.   \n$[\\mathbf{MPT}^{+}22]$ Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, and Arnaud de La Fortelle, Lens: Localization enhanced by nerf synthesis, Conference on Robot Learning, PMLR, 2022, pp. 1347\u20131356.   \n$[\\mathbf{M}\\mathbf{R}\\mathbf{B}^{+}23]$ Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel, Scaling data-constrained language models, arXiv preprint arXiv:2305.16264 (2023).   \n[PL05] Bo Pang and Lillian Lee, Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales, Proceedings of the ACL, 2005.   \n[Rei08] Markus Rei\u00df, Asymptotic equivalence for nonparametric regression with multivariate and random design, The Annals of Statistics (2008), 1957\u20131982.   \n[RRBS19] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit, A constructive prediction of the generalization error across scales, International Conference on Learning Representations, 2019.   \n$[\\mathsf{R S M}^{+}16]$ German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez, The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes, Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3234\u20133243.   \n[SK19] Connor Shorten and Taghi M Khoshgoftaar, A survey on image data augmentation for deep learning, Journal of big data 6 (2019), no. 1, 1\u201348.   \n[SLW20] Viktor Seib, Benjamin Lange, and Stefan Wirtz, Mixing real and synthetic data to enhance neural network training\u2013a review of current approaches, arXiv preprint arXiv:2007.08781 (2020).   \n[Ste81] Charles M Stein, Estimation of the mean of a multivariate normal distribution, The annals of Statistics (1981), 1135\u20131151.   \n[TAH18] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi, Precise error analysis of regularized m-estimators in high dimensions, IEEE Transactions on Information Theory 64 (2018), no. 8, 5592\u20135628.   \n$[\\mathrm{TDR}^{+}21]$ Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler, Scale efficiently: Insights from pretraining and finetuning transformers, International Conference on Learning Representations, 2021.   \n[TJJ20] Nilesh Tripuraneni, Michael Jordan, and Chi Jin, On the theory of transfer learning: The importance of task diversity, Advances in neural information processing systems 33 (2020), 7852\u20137862.   \n[TOH15] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi, Regularized linear regression: A precise analysis of the estimation error, Proceedings of Machine Learning Research 40 (2015), 1683\u20131709.   \n$[\\mathrm{TPA}^{+}18]$ Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield, Training deep networks with synthetic data: Bridging the reality gap by domain randomization, Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2018, pp. 969\u2013977.   \n[Tsy09] Alexandre B. Tsybakov, Introduction to nonparametric estimation, Springer, 2009.   \n[vdV00] Aaad W van der Vaart, Asymptotic statistics, Cambridge University Press, 2000.   \n[Ver18] Roman Vershynin, High-dimensional probability: An introduction with applications in data science, vol. 47, Cambridge university press, 2018.   \n[WM18] Mengting Wan and Julian J. McAuley, Item recommendation on monotonic behavior chains, Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018 (Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John O\u2019Donovan, eds.), ACM, 2018, pp. 86\u201394.   \n[WMNM19] Mengting Wan, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley, Finegrained spoiler detection from large-scale review corpora, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers (Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez, eds.), Association for Computational Linguistics, 2019, pp. 2605\u20132610.   \n$[\\mathbf{YCFB}^{+}22]$ Lin Yen-Chen, Pete Florence, Jonathan T Barron, Tsung-Yi Lin, Alberto Rodriguez, and Phillip Isola, Nerf-supervision: Learning dense object descriptors from neural radiance fields, 2022 International Conference on Robotics and Automation (ICRA), IEEE, 2022, pp. 6496\u20136503.   \n$[\\mathrm{YLS^{+}24}]$ Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu, Data mixing laws: Optimizing data mixtures by predicting language modeling performance, arXiv preprint arXiv:2403.16952 (2024). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Details of empirical results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Binary classification with Gaussian mixture data ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/6c6b91ded78e6c0895dcd0a65624a30c6dddac4a360c4a1cec60f5182cbe8f67.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 6: Gaussian mixture data and logistic regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best ftis are shown in black. These gives the estimates $\\beta=0.72$ , $R_{*}=0.157$ , and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=0.013$ . ", "page_idx": 13}, {"type": "text", "text": "We provide details for the models used in the simulations. ", "page_idx": 13}, {"type": "text", "text": "Logistic regression: We use the scikit-learn implementation with the lbfgs solver, ftiting the intercept, with maximum iterations set to $10\\mathbf{k}$ . For each run of each $(n,m,\\alpha)$ combination, we set the $\\ell_{2}$ penalty (parameter C in scikit-learn) to $2^{i},i=-8,...,8$ and $10^{i},i=-6,-5,-4,-3,3,4,5,6$ , and only report the test result for the value that achieves the best validation error. The results of the individual scaling law estimates and the comparison of joint training results with the scaling law predictions are shown in Figures 6 and 7. ", "page_idx": 13}, {"type": "text", "text": "Neural network: The network has one hidden layer with 32 ReLU neurons, and an output neuron using sigmoid. For training, we use the binary cross entropy loss, a constant learning rate of 0.05, and batch size 64. We train the network for 1,000 epochs. Similar to the procedure in logistic regression, we use $\\ell_{2}$ regularization (weight decay) and use the validation set to choose the best regularization parameter from the set $\\{\\stackrel{\\leftarrow}{0},1\\stackrel{\\leftarrow}{0}^{-5},10^{-\\stackrel{\\leftarrow}{4}},10^{-3},2\\cdot10^{-3},4\\cdot10^{-3},10^{-2},2\\cdot10^{-2},4\\cdot10^{-2},10^{-1},2\\cdot1\\}.$ $\\mathbf{\\bar{10^{-1}},4\\cdot10^{-1}}\\mathbf{\\bar{\\Psi}}$ . The results of the individual scaling law estimates and the comparison of joint training results with the scaling law predictions are shown in Figures 8 and 9. ", "page_idx": 13}, {"type": "text", "text": "A.2 Linear regression with Gaussian mixture data ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For the Gaussian mixture data, described in the previous section, we perform weighted ridge regression experiments according to equation (16) and plot the square loss. As before, we choose the best regularizer for the ridge regression of the set $2^{i},i=-8,...,\\bar{8}$ and $10^{i},i=-6,-5,-4,-3,3,4,5,6,$ , and report the test result for the value that achieves the best validation error. The results are presented in Figures 10 and 11. ", "page_idx": 13}, {"type": "text", "text": "A.3 Sentiment analysis in movie reviews ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To convert the movie reviews and book reviews to vectors, we use a combination of two different embedding: We use all the reviews in the training data and then use nltk tagger [Bir06] to find the most frequent 500 adjectives appearing in the samples used for training. Then we use the common Tfidf vectorizer (we used scikit-learn\u2019s implementation of tfidf vectorizer) for which we use the list of these most common 500 adjectives as vocabulary. This gives us a vector of length 500 dimension for each review. In addition, we also apply \u201cParaphrase-MiniLM-L6-v2\u201d sentence transformer which is based on BERT with 6 Transformer Encoder Layers, and return a 384 dimension vector representation of the reviews. For each movie review we concatenate the results of tfidf vectorizer and sentence transformer to get a 884 dimensional representation that we use as our input vector. ", "page_idx": 13}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/a2056ad5317fbd26e1e27fe5b769ea1f826aa253788271a54f081914a1479c8b.jpg", "img_caption": ["Figure 7: Gaussian mixture data and logistic regression. Test error when trained on mixtures of original $\\ln$ varying by row) and surrogate ( $m$ varying by column) data. Black curves: scaling formula (4). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We use logistic regression and neural networks with the same set of parameters as in the Gaussian mixture experiments (except for the input dimension). We plot the average loss over 10 independent runs. ", "page_idx": 14}, {"type": "text", "text": "Results omitted from the main text are presented in Figures 12\u201316. ", "page_idx": 14}, {"type": "text", "text": "A.4 Image classification with CIFAR10 and CIFAR100 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We largely use the model and the training procedure described at https://jovian.ml/aakashns/05bcifar10-resnet. We normalize the images for mean and standard deviation. We train a 9-layer ResNet model for classification, using Adam for optimization, weight decay, and gradient clipping, trained over 16 epochs with a one-cycle learning rate scheduling policy, minimizing cross entropy loss. For each combination of $m,\\,n,$ , and $\\alpha$ , we report the average test error over 10 runs. Since there is no overlap between the label sets of CIFAR10 and CIFAR100, the latter dataset needs to be relabeled. We do this by training a separate 9-layer ResNet model on 10,000 randomly chosen CIFAR10 images from the training set of 50,000 examples (without creating a separate split for them), and use its predictions on CIFAR100 images as labels. ", "page_idx": 14}, {"type": "text", "text": "Scaling curves are presented in Figure 17 and 3. ", "page_idx": 14}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/1d53ad0466c32dec3fbfbd9e661a019fc3ce53a3cc4eef929d59cd066226091d.jpg", "img_caption": ["Figure 8: Gaussian mixture data and neural network. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best ftis are shown in black. These gives the estimates $\\beta=0.79$ , $R_{*}=0.160$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=0.010$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Lasso on TCGA PanCancer dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We used public domain TCGA pancancer dataset. After, flitering samples with incomplete values we are left with 9220 patients, each having 20,531 gene expression values and the outcome was PFS (progression-free survival). Out of these we used a group of 2000 patients, splitted into train and test set of 1000 each to select 500 genes having the largest absolute Cox PH score. We also used the mean and standard deviation of gene expression values of these 2000 patients to normalize the gene expression columns for the remaining 7220 patients. Among the remaining of 7220 patients 3580 were females. We treated the female patients data as original data, and split them into train $(50\\%)$ , test $(25\\%)$ and validation split $(25\\%)$ . The remaining 3640 patients data was used as surrogate dataset. We fit CoxPHFitter model (link) with 500 selected genes and use \u201c1-concordance score\u201d as our loss function. We used the validation split to choose best value of $\\ell_{1}$ penalty parameter from $2^{i},i=2,0,-2,-4,-6,-8,-10,-12,-\\dot{1}4,-16$ in the model. We observed discontinuity at $\\alpha=1$ . To avoid this discontinuity, we approximated $R(\\hat{\\pmb\\theta}_{n,m}(1))$ by $R(\\hat{\\pmb{\\theta}}_{n,m}(1-\\epsilon))$ if $n>0$ and by $R(\\hat{\\pmb{\\theta}}_{m/2,m}(1-\\epsilon))$ if $n=0$ , where we choose $\\epsilon=0.05$ . We plot the average loss over 10 independent runs. The results are presented in Figures 18 and 4. ", "page_idx": 15}, {"type": "text", "text": "A.6 High-dimensional ridge regression ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present additional ridge regression experiments here in Figs. 19\u201330. We plot the average loss over 10 independent runs. In these experiments, as in the main paper, we set $d=500$ , $\\sigma^{2}=\\bar{\\sigma}_{s}^{2}=1$ , $\\lVert\\pmb{\\theta}_{*}\\rVert=1$ , $\\lVert\\bar{\\pmb{\\theta}}_{*,s}\\rVert=1$ , except for the last four Figs. 27\u201330, where we use $\\lVert\\pmb{\\theta}_{*,s}\\rVert=1/2$ . We used angle $\\gamma=\\pi/6$ and $\\pi/2$ in our experiments. ", "page_idx": 15}, {"type": "text", "text": "We consider two methods: (1) Fix $\\lambda$ to a very small value $2^{-10}$ , and (2) For each random draw of datasets select $\\lambda$ that achieves the best validation performance. For the latter method, we try $\\lambda=2^{i}$ , where $i=-10,-8,-6,\\dotsc,8,10$ . For ridge regression simulations, we directly plot the excess test risks, as the parameter $\\theta$ for original data is known and for any $\\hat{\\theta}$ the excess test risk in this model is $\\lVert{\\boldsymbol{\\theta}}-{\\hat{\\boldsymbol{\\theta}}}\\rVert^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "B Low-dimensional asymptotics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Formal statements ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this appendix, we present our results on the estimator of Eqs. (2), (3) under the classical asymptotics $n,m\\rightarrow\\infty$ at $d$ fixed. For simplicity, we assume no regularizer is used in this regime. ", "page_idx": 15}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/8bd9f8ef01df113e0be70de15832ac782272ffdd93a11e87d85aad910f08d68e.jpg", "img_caption": ["Figure 9: Gaussian mixture data and neural network. Test error when training mixture of original ( $n$ varying by row) and surrogate ( $m$ varying by column) data. Black curves: scaling law (4). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Beyond classical regularity assumptions of low-dimensional asymptotics, in this section we will make the following assumption which guarantees that original and surrogate distribution are \u2018not arbitrarily far.\u2019 Recall that $R^{s}(\\pmb\\theta)$ denotes the population error on surrogate data. ", "page_idx": 16}, {"type": "text", "text": "Assumption 1 (Distribution shift for low- $d$ asymptotics). There exists a constant $K_{*}$ such that for all $\\pmb{\\theta}\\in\\mathbf{\\bar{\\mathbb{R}}}^{d}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|R^{s}(\\pmb\\theta)-R(\\pmb\\theta)\\right|\\leq K_{*}\\!\\left(1+R(\\pmb\\theta)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The regularity conditions are similar to the ones in [vdV00]. Here and in the following $\\mathsf{B}(\\pmb\\theta_{*},r)$ is the ball of radius $r$ centered at $\\theta_{*}$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 2 (\u2018Classical\u2019 regularity). ", "page_idx": 16}, {"type": "text", "text": "$(a)$ The original population risk $R(\\pmb\\theta)$ is uniquely minimized at a point $\\theta_{*}$ . ", "page_idx": 16}, {"type": "text", "text": "(b) $\\theta\\mapsto\\ell(\\theta;z)$ is non-negative lower semicontinuous. Further, define the following limit in $[0,\\infty]$ for $\\pmb{u}\\in\\mathbb{S}^{d-1}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell_{\\infty}({\\pmb u};{\\pmb z}):=\\operatorname*{lim}_{{\\pmb\\theta}\\to\\infty\\atop{\\pmb\\theta}/\\|{\\pmb\\theta}\\|_{2}\\to{\\pmb u}}\\ell({\\pmb\\theta};{\\pmb z})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we assume $\\operatorname*{inf}_{\\pmb{u}\\in\\mathbb{S}^{d-1}}\\mathbb{E}\\ell_{\\infty}(\\pmb{u};\\pmb{z})\\geq R(\\pmb{\\theta}_{*})+c f$ or some $c>0$ . ", "page_idx": 16}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/264c569f6b7d9ab460b47d75a56e7476c2061735e5f407114032d3c430ce83a4.jpg", "img_caption": ["Figure 10: Gaussian mixture data and ridge regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best ftis are shown in black. These gives the estimates $\\beta=0.60$ , $R_{*}=0.49$ , and $R_{\\mathtt{s u}}^{\\mathtt{e x}}(\\infty)=0.03$ . "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/d358f91e67860f52d69cfe7f9adee0e5699896d5fce581f95696aeeb5d7b9579.jpg", "img_caption": ["Figure 11: Gaussian mixture data and ridge regression. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/82ee7fe1752f1d826a81a3ee0368d522e1f67a09a31cf333de4be276041dc3ac.jpg", "img_caption": ["Figure 12: IMDB and Rotten Tomatoes data and logistic regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots), together with scaling law ftis (black lines). Best fit parameters are $\\beta=0.27$ , $R_{*}=0.101$ and $R_{\\mathrm{{su}}}^{\\mathrm{{ex}}}(\\infty)=0.148$ . "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/870c6c9233258a7d537c9d749e58390aaa92fb0680e5d72a94fe674ddab4c492.jpg", "img_caption": ["Figure 13: IMDB and Rotten Tomatoes data and logistic regression. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4). "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/cded319f5cf0424220184617b81e1b1506497e8b62360909f3eb3c0963a8ad0f.jpg", "img_caption": ["Figure 14: IMDB and Goodreads book reviews (as surrogate dataset) and logistic regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots), together with scaling law fits (black lines). Best fit parameters are $\\beta=0.27$ , $R_{*}=0.101$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=0.101$ . "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/7b7eee6c746a84e7325e9ff7dff4f05c302afc30e1d5462383cfd538df7bd455.jpg", "img_caption": ["Figure 15: IMDB and Goodreads book reviews and logistic regression. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4). "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/62915034d1edf39b8da90bd933d1df5c95c0a90387e5048a04c330eedeec5ff7.jpg", "img_caption": ["Figure 16: IMDB and Rotten Tomatoes data and neural networks. Scaling law ftis for models trained on original (left plot) and surrogate (right plot) data only (red dots)(as in Fig. 12), together with scaling law fits (black lines). Best fit parameters are $\\beta=0.37$ , $R_{*}=0.145$ and $R_{\\mathrm{{su}}}^{\\mathrm{{ex}}}(\\infty)=0.095$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/fa945e9723d1ae50d5c0e581a69394e4820364b39484299941062200bbf7975e.jpg", "img_caption": ["Figure 17: CIFAR10 and CIFAR100 data: (left) Test error scaling of original data (left) and surrogate data (right). Best fit parameters are $\\beta=0.404$ , $R_{*}=0.0013$ , and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=0.199$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/f1ba810f6a59a579e9259edd43d53793f269a4d70724ff0b3547485abd9866cc.jpg", "img_caption": ["Figure 18: Lasso-based Cox regression on TCGA PanCancer dataset with female patients data as original data and male patients data as surrogate data. Scaling law ftis for models trained on original (left plot) and surrogate (right plot) data only (red dots)(as in Fig. 12.) Best fit parameters are $\\beta=1.55$ , $R_{*}=0.29$ and $R_{\\mathtt{s u}}^{\\mathtt{e x}}(\\infty)=0.29$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/e2244972095cddf06c1156ac8c27a16edc0e268efedae511cc3f2b9e6b423e2e.jpg", "img_caption": ["Figure 19: Ridge regression with $\\gamma=\\pi/2$ , and regularization parameter $\\lambda=2^{-10}$ : Test error scaling of the original data (left), and surrogate data (right). Best curve ftis give the estimates $\\beta=1.57$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)\\bar{=}2.0$ "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/44fdbd989d23976e3a50943e7e1f399fff14d8637ee30dc2fc7bc87f692646ae.jpg", "img_caption": ["Figure 20: Ridge regression with $\\gamma=\\pi/2$ , and regularization parameter $\\lambda=2^{-10}$ "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/d3bd9cd3672f6ebc66ee981ca7b7baf8534a0fc13e4f4312b108c2615c1f21d0.jpg", "img_caption": ["Figure 21: Ridge regression with $\\pi/6$ between $\\theta$ and $\\theta_{s}$ , and regularization parameter $\\lambda=2^{-10}$ : Test error scaling of the original data (left), and surrogate data (right). Best curve fits give the estimates $\\beta=1.57$ and $R_{\\mathtt{s u}}^{\\mathtt{e x}}(\\infty)=0.29$ "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/e638b1b365ece3db0617048682cb7fca56920f725f287e6659d79b3fd074c613.jpg", "img_caption": ["Figure 22: Ridge regression with $\\pi/6$ between $\\theta$ and $\\theta_{s}$ , and regularization parameter $\\lambda=2^{-10}$ "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/d32f2f56ffdd689e6ba0620a7a00b5bb5a1259e8751874df13c094210ce534e9.jpg", "img_caption": ["Figure 23: Ridge regression with $\\pi/2$ between $\\theta$ and $\\theta_{s}$ , and the best regularization parameter: Test error scaling of the original data (left), and surrogate data (right). Best curve fits give the estimates $\\beta=0.94$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)\\,{=}\\,1.0$ "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/b31a756eecc65f658820f9294f44f5291d40656e55feeec6d523bad8975911fa.jpg", "img_caption": ["Figure 24: Ridge regression with $\\gamma=\\pi/2$ , and the best regularization parameter "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/a7d3c0ae61ddeaf192d49a0faed8f726be659004c6fa210b1da61af1231d16c7.jpg", "img_caption": ["Figure 25: Ridge regression with $\\pi/6$ between $\\theta$ and $\\theta_{s}$ , and the best regularization parameter: Test error scaling of the original data (left), and surrogate data (right). Best curve fits give the estimates $\\beta=0.94$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)\\,{=}\\,0.24$ "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/0180df9d4454cecfc3ce3146654554f5fa0d03fd039c72a9da4087db5e60a6d9.jpg", "img_caption": ["Figure 26: Ridge regression with $\\pi/6$ between $\\theta$ and $\\theta_{s}$ , and the best regularization parameter "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/3990604b4e9a95185d83c09d2771ca290cd67f79ebd981a9796b3e6a10866329.jpg", "img_caption": ["Figure 27: Ridge regression with $\\pi/2$ between $\\theta$ and $\\theta_{s}$ , $\\|\\theta\\|\\,=\\,1$ , $\\|\\theta_{s}\\|\\;=\\;1/2$ and the best regularization parameter: Test error scaling of the original data (left), and surrogate data (right). Best curve fits give the estimates $\\beta=0.94$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=1.00$ "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/2422112945e3467e361c14bee0929477379aefa4094bb696e949b0553bb605e4.jpg", "img_caption": ["Figure 28: Ridge regression with $\\pi/2$ between $\\theta$ and $\\theta_{s}$ , $\\|\\theta\\|\\,=\\,1$ , $\\|\\theta_{s}\\|\\;=\\;1/2$ and the best regularization parameter "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/292085bfe17688ab003ec43ce9b55c7fbca39b749b9393e0ca9164d78e12ddf7.jpg", "img_caption": ["Figure 29: Ridge regression with $\\gamma=\\pi/2$ , $\\|\\theta\\|=1$ , $\\|\\theta_{s}\\|=1/2$ , and regularization parameter $\\lambda=2^{-10}$ : Test error scaling of the original data (left), and surrogate data (right). Best curve ftis give the estimates $\\beta=1.57$ and $R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)=1.27$ "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "NAcHv7vtL2/tmp/85583a0bbd83f78b21d35ef7ef41333f87336549681828b9f0c5e899896714bf.jpg", "img_caption": ["Figure 30: Ridge regression with $\\gamma=\\pi/2$ , $\\|\\theta\\|=1$ , $\\|\\theta_{s}\\|=1/2$ , and regularization parameter \u03bb = 2\u221210 "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(c) $\\theta\\mapsto\\ell(\\theta;z)$ is differentiable at $\\boldsymbol{\\theta}_{*}$ almost surely, both under $z\\sim\\mathbb{P}$ and under $z\\sim\\mathbb{P}^{s}$ . Further, there exists $r\\,>\\,0$ such that, letting $\\bar{\\textsf{B}}:=\\,\\mathsf{B}(\\theta_{*},r)$ , the following holds for $a$ constant $C$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta_{1}\\neq\\theta_{2}\\in\\mathtt{B}}\\Big\\{\\frac{|\\ell(\\pmb{\\theta}_{1};z)-\\ell(\\pmb{\\theta}_{2};z)|^{2}}{\\|\\pmb{\\theta}_{1}-\\pmb{\\theta}_{2}\\|_{2}^{2}}\\Big\\}\\leq C<\\infty\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$(d)$ The functions $\\pmb{\\theta}\\mapsto R(\\pmb{\\theta})$ , $\\theta\\mapsto R^{s}(\\theta)$ , are twice differentiable in a neighborhood of $\\pmb{\\theta}_{*}$ , with Lipschitz continuous Hessian. Further $\\nabla^{2}R(\\pmb{\\theta}_{*})\\succ\\mathbf{0}$ (strictly positive definite). ", "page_idx": 27}, {"type": "text", "text": "Proposition B.1. Under Assumption $^{\\,l}$ and Assumption 2, define the following $d\\times d$ matrices ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{H}:=\\nabla^{2}R(\\pmb{\\theta}_{*})=\\mathbb{E}[\\nabla^{2}\\ell(\\pmb{\\theta}_{*};z)]\\,,}\\\\ &{\\pmb{K}:=\\operatorname*{Cov}\\!\\left(\\nabla\\ell(\\pmb{\\theta}_{*};z);\\nabla\\ell(\\pmb{\\theta}_{*};z)\\right),}\\\\ &{\\pmb{K}_{s}:=\\operatorname*{Cov}_{s}\\!\\left(\\nabla\\ell(\\pmb{\\theta}_{*};z^{s});\\nabla\\ell(\\pmb{\\theta}_{*};z^{s})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where Cov, $\\mathrm{Cov}_{s}$ denote the covariances, respectively, with respect to the original data (i.e., with respect to $z\\sim\\mathbb{P}$ ), and with respect to the surrogate data (i.e., with respect to $z^{s}\\sim\\mathbb{P}_{s}$ ). Further define the $d$ -dimensional vector ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pmb{g}^{s}:=\\nabla R^{s}(\\pmb{\\theta}_{\\ast})-\\nabla R(\\pmb{\\theta}_{\\ast})\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then there exists $\\alpha_{\\mathrm{max}}\\in(0,1]$ (depending only on the constants in the assumptions) such that, for all $\\alpha\\in[0,\\alpha_{\\mathrm{max}}],$ , the excess risk of the estimator $\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)$ satisfies (for $D:=\\|g^{s}\\|$ bounded by $a$ constant) ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R\\big(\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha)\\big)-R\\big(\\pmb{\\theta}_{*}\\big)=\\;\\alpha^{2}\\langle\\pmb{g}^{s},{H}^{-1}\\pmb{g}^{s}\\rangle+\\frac{(1-\\alpha)^{2}}{n}\\cdot\\operatorname{Tr}\\big({H}^{-1}\\pmb{K}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;\\frac{\\alpha^{2}}{m}\\cdot\\operatorname{Tr}\\big({H}^{-1}\\pmb{K}_{s}\\big)+O\\Big(\\Big(\\frac{1}{m\\,\\vee n}+D\\alpha^{2}\\Big)\\Big(\\frac{1}{(m\\,\\vee n)^{1/2}}+D\\alpha\\Big)\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "(Here the big $O$ hides dependence on the constants in Assumptions $^{\\,l}$ and 2.) ", "page_idx": 27}, {"type": "text", "text": "Remark B.1. For economy of notation we stated Proposition B.1 in the case in which the excess risk is measured by using the same loss as for training, i.e. $\\ell_{\\mathrm{test}}=\\ell$ . However the same result Eq. (25) applies with minor modifications to the case $\\ell_{\\mathrm{test}}\\neq\\ell$ (and thus, with $R$ replaced by $R^{\\mathrm{test}}$ ), provided $R^{\\mathrm{test}}$ is also twice differentiable with Lipschitz Hessian, and $\\nabla R^{\\mathrm{test}}(\\pmb{\\theta}_{*})=\\mathbf{0}$ . In this case, (25) has to be modified replacing ${\\pmb H}^{-1}$ by $\\pmb{H}^{-1}\\nabla^{2}R^{\\mathrm{test}}(\\pmb{\\theta}_{\\ast})\\pmb{H}^{-1}$ . ", "page_idx": 27}, {"type": "text", "text": "Remark B.2. The error terms in Eq. (25) are negligible under two conditions: $(i)\\;m$ and $n$ are large, which is the classical condition for low-dimensional asymptotics to hold; $(i i)\\,\\|\\pmb{g}^{s}\\|_{2}=\\|\\nabla R^{s}(\\pmb{\\theta}_{*})\\|_{2}\\alpha$ is small. In particular, the latter condition will hold in two cases. First, when $\\lVert\\nabla R^{s}(\\pmb\\theta_{\\ast})\\rVert_{2}$ is of order one (i.e. the distribution shift is large), but $\\alpha$ is small (surrogate data are downweighted). Note that, when the distribution shift is large, and the sample size $n$ is large enough, we expect small $\\alpha$ to be optimal and therefore Eq. (25) covers the \u2018interesting\u2019 regime. ", "page_idx": 27}, {"type": "text", "text": "Second, when $\\lVert\\nabla R^{s}(\\pmb\\theta_{\\ast})\\rVert_{2}$ is small (i.e. the shift is small) and $\\alpha$ is of order one. If in addition we have $\\nabla^{2}R^{s}(\\pmb\\theta_{\\ast})\\approx\\nabla^{2}R^{s}(\\pmb\\theta_{\\ast})$ , it can be shown that the range of validity of Eq. (25) covers the whole interval $\\alpha\\in[0,1]$ . ", "page_idx": 27}, {"type": "text", "text": "Remark B.3. Note that the distribution shift is measured in Eq. (25) by the first term $\\langle g^{s},H^{-1}g^{s}\\rangle$ . The original and surrogate distribution can be very different in other metrics (e.g. in total variation or transportation distance), but as long as $g^{s}$ is small (as measured in the norm defined by ${\\pmb H}^{-1}$ ), surrogate data will reduce test error. ", "page_idx": 27}, {"type": "text", "text": "Note that, within the setting of Proposition B.1, the excess error of training only on original data is $R_{\\mathsf{o r}}^{\\mathsf{e x}}(n):=R(\\widehat{\\pmb{\\theta}}_{n,0}(0))-R(\\pmb{\\theta}_{*})=\\operatorname{Tr}\\!\\big(\\pmb{H}^{-1}\\pmb{K}\\big)/n+o(1/n)$ , while $R_{\\mathsf{s u}}^{\\mathsf{e x}}(m):=R(\\hat{\\pmb{\\theta}}_{n,m}(0))\\mathrm{~-~}$ $\\begin{array}{r}{\\mathfrak{L}(\\pmb{\\theta}_{*})=\\langle\\pmb{g}^{s},\\pmb{H}^{-1}\\pmb{g}^{s}\\rangle+\\operatorname{Tr}\\bigl(\\pmb{H}^{-1}\\pmb{K}_{s}\\bigr)/m+o(1/m).\\;\\mathrm{I}}\\end{array}$ Hence Eq. (B.1) can be recast in the form of our general scaling law (4), namely: ", "page_idx": 27}, {"type": "equation", "text": "$$\nR(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))-R\\big(\\pmb{\\theta}_{*}\\big)\\approx\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+\\Big[\\alpha^{2}\\big(R_{\\mathrm{su}}^{\\mathrm{ex}}(m)-R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)\\big)+(1-\\alpha)^{2}R_{\\mathrm{or}}^{\\mathrm{ex}}(n)\\Big]\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which (as expected) corresponds to the parametric scaling exponent $\\beta=1$ . ", "page_idx": 27}, {"type": "text", "text": "An immediate consequence of Proposition B.1 is that surrogate data do not hurt, and will help if their distribution is close enough to the original one (under the assumption of optimally chosen $\\alpha$ ). ", "page_idx": 27}, {"type": "text", "text": "Corollary B.2. Under the assumptions of Proposition B.1, let $\\overline{{{R}}}_{\\mathsf{o r}}(n)\\,:=\\,\\mathrm{Tr}\\bigl(H^{-1}K\\bigr)/n,$ , and $\\overline{{\\cal R}}_{\\mathrm{su}}(m):=\\langle g^{s},H^{-1}g^{s}\\rangle+\\operatorname{Tr}\\!\\left(H^{-1}K_{s}\\right)/m.$ . For $\\alpha_{n,m}^{*}=\\overline{{R}}_{0\\mathrm{r}}(n)/(\\overline{{R}}_{\\mathrm{su}}(m)+\\overline{{R}}_{0\\mathrm{r}}(n))$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nR\\bigl(\\widehat{\\theta}_{n,m}(\\alpha_{n,m}^{*})\\bigr)-R_{*}=\\mathrm{~}\\bigl(\\overline{{R}}_{\\mathfrak{o r}}(n)^{-1}+\\overline{{R}}_{\\mathfrak{s u}}(m)^{-1}\\bigr)^{-1}+\\Delta_{n,m},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with $\\Delta_{n,m}$ of the same order as the error in Prop. B.1. ", "page_idx": 28}, {"type": "text", "text": "B.2 Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma B.3. Under the assumptions of Proposition B.1 (Assumption 1 and Assumption 2) there exists $\\alpha_{\\mathrm{max}}\\in(0,1],$ , depending only on the constants appearing there such that the following holds: ", "page_idx": 28}, {"type": "text", "text": "(i) The function $\\pmb{\\theta}\\mapsto R(\\pmb{\\theta};\\alpha):=\\left(1-\\alpha\\right)R(\\pmb{\\theta})+\\alpha\\,R^{s}(\\pmb{\\theta})$ has a unique minimizer $\\pmb{\\theta}_{\\ast}(\\alpha)\\in\\mathbb{R}^{d}$ . Further $\\pmb{\\theta}_{*}(\\alpha)\\in\\mathsf{B}(\\pmb{\\theta}_{*},r)$ , and $\\pmb{\\theta}_{*}(\\alpha)\\rightarrow\\pmb{\\theta}_{*}$ as $\\alpha\\downarrow0$ . ", "page_idx": 28}, {"type": "text", "text": "$(i i)$ We have $\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)\\rightarrow\\pmb{\\theta}_{\\ast}$ in probability as $n,m\\rightarrow\\infty$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Fix $r_{0}\\in(0,r]$ By Assumption $2.(a)$ , $\\operatorname*{inf}_{\\theta\\notin\\mathsf{B}(\\theta_{*};r_{0})}R(\\pmb\\theta)>R(\\pmb\\theta_{*})+\\delta_{0}$ for some constant $\\delta_{0}$ . Hence, using Assumption 1, for any $\\pmb{\\theta}\\notin\\mathsf{B}(\\pmb{\\theta}_{\\ast};r)$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R(\\pmb{\\theta};\\alpha)\\geq R(\\pmb{\\theta})-K_{*}\\alpha\\big[1+R(\\pmb{\\theta})\\big]}}\\\\ &{\\geq(1-K_{*}\\alpha)R(\\pmb{\\theta})-K_{*}\\alpha}\\\\ &{\\geq(1-K_{*}\\alpha)(R(\\pmb{\\theta}_{*})+\\delta_{0})-K_{*}\\alpha\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the other hand $R(\\pmb\\theta_{*};\\alpha)\\leq(1+K_{*}\\alpha)R(\\pmb\\theta_{*})+K_{*}\\alpha$ , whence ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{R(\\pmb\\theta;\\alpha)-R(\\pmb\\theta_{*};\\alpha)\\geq(1-K_{*}\\alpha)\\delta_{0}-2K_{*}\\alpha R(\\pmb\\theta_{*})}}\\\\ {{-\\,2K_{*}\\alpha,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which is strictly positive for $\\alpha<\\alpha_{\\mathrm{max}}(r_{0}):=\\delta_{0}/(4K_{*}(1+R(\\pmb\\theta_{*}))$ . Hence the minimum must be achieved in $\\mathsf{B}(\\pmb\\theta_{*};r_{0})$ (note that since $R(\\pmb\\theta)$ , $R_{s}(\\pmb\\theta)$ are lower semicontinuous, the minimum is achieved). ", "page_idx": 28}, {"type": "text", "text": "By Assumption $2.(d)$ , for $r_{0}$ sufficiently small, $\\theta\\mapsto\\nabla R(\\pmb\\theta;\\alpha)$ is strictly convex in $\\mathsf{B}(\\pmb\\theta_{*};r_{0})$ and therefore the minimizer is unique. This proves point $(i)$ . ", "page_idx": 28}, {"type": "text", "text": "Point $(i i)$ follows from a modification of Theorem 5.14 in [vdV00]. Namely, for a diverging sequence $\\{(n(k),m(k))~:~k~\\in~\\mathbb{N}\\}$ , we consider to $\\widehat{R}_{*,k}(\\pmb{u})\\ :=\\ \\widehat{R}_{n(k),m(k)}(c(\\pmb{u})\\pmb{u};\\alpha)$ , where $c(\\pmb{u}):=(1+\\|\\pmb{u}\\|^{2})^{-1/2}$ . This function is lower semicontinuous on the compact set ${\\sf B}({\\bf0};1)$ and converges almost surely to its expectation for every fixed $\\textbf{\\em u}$ in this set, and hence the argument of Theorem $5.14\\;[\\mathrm{vdV}00]$ applies here. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proof of Proposition B.1. By a modification of Theorem 5.39 in [vdV00] (here $\\pmb\\theta_{\\ast}(\\alpha)$ is defined as in Lemma B.3) ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)=\\pmb{\\theta}_{*}(\\alpha)+\\frac{1-\\alpha}{n}\\pmb{H}(\\alpha)^{-1}\\sum_{i=1}^{n}\\left[\\nabla\\ell(\\pmb{\\theta}_{*}(\\alpha);\\boldsymbol{z}_{i})-\\mathbb{E}\\nabla\\ell(\\pmb{\\theta};\\boldsymbol{z})\\right]}\\\\ {\\displaystyle\\qquad\\qquad+\\frac{\\alpha}{m}\\pmb{H}(\\alpha)^{-1}\\sum_{i=1}^{m}\\left[\\nabla\\ell(\\pmb{\\theta}_{*}(\\alpha);\\boldsymbol{z}_{i}^{c})-\\mathbb{E}_{s}\\nabla\\ell(\\pmb{\\theta};\\boldsymbol{z})\\right]+O_{P}(m^{-1}+n^{-1})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\pmb{H}(\\alpha)_{:}=(1-\\alpha)\\nabla^{2}R(\\pmb{\\theta}_{*}(\\alpha))+\\alpha\\nabla^{2}R_{s}(\\pmb{\\theta}_{*}(\\alpha))$ . Note that in the present setting the error is of order $m^{-1}+n^{-1}$ because we assume the Hessian to be Lipschitz continuous. ", "page_idx": 28}, {"type": "text", "text": "The population minimizer $\\pmb\\theta_{\\ast}(\\alpha)$ solves ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{0}=\\nabla R(\\theta_{*}(\\alpha);\\alpha)}\\\\ {\\displaystyle\\quad=\\nabla R(\\theta_{*};\\alpha)+\\nabla^{2}R(\\theta_{*};\\alpha)(\\theta_{*}(\\alpha)-\\theta_{*})+\\int_{0}^{1}\\big[\\nabla^{2}R(\\theta_{t};\\alpha)-\\nabla^{2}R(\\theta_{*};\\alpha)\\big](\\theta_{*}(\\alpha)-\\theta_{*})\\,\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\pmb{\\theta}_{t}=t\\,\\pmb{\\theta}_{*}(\\alpha)+\\left(1-t\\right)\\pmb{\\theta}_{*}$ . Denoting by $L_{2}$ the Lipschitz constant of the Hessian (in operator norm), and recalling that $\\nabla\\bar{R}(\\pmb\\theta_{*})=\\mathbf{0}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{2}R(\\pmb{\\theta}_{*};\\alpha)(\\pmb{\\theta}_{*}(\\alpha)-\\pmb{\\theta}_{*})=-\\alpha\\nabla R_{s}(\\pmb{\\theta}_{*})+\\pmb{u}\\,,}\\\\ {\\|\\pmb{u}\\|_{2}\\le L_{2}\\|\\pmb{\\theta}_{*}(\\alpha)-\\pmb{\\theta}_{*}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Recalling that, by Lemma B.3, $\\pmb{\\theta}_{*}(\\alpha)\\rightarrow\\pmb{\\theta}_{*}$ as $\\alpha\\rightarrow0$ , this implies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{*}(\\alpha)-\\pmb{\\theta}_{*}=-\\pmb{H}^{-1}\\nabla R_{s}(\\pmb{\\theta}_{*})\\alpha+O\\big((\\|\\nabla R_{s}(\\pmb{\\theta}_{*})\\|_{2}\\vee\\|\\nabla R_{s}(\\pmb{\\theta}_{*})\\|_{2}^{2}\\big)\\alpha^{2}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Substituting in Eq. (26), we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\theta}_{n,m}(\\alpha)-\\theta_{*}=-\\,H^{-1}\\nabla R_{s}(\\theta_{*})\\alpha+\\displaystyle\\frac{1-\\alpha}{n}H(\\alpha)^{-1}\\displaystyle\\sum_{i=1}^{n}\\big[\\nabla\\ell(\\theta_{*}(\\alpha);z_{i})-\\mathbb{E}\\nabla\\ell(\\theta;z)\\big]}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\alpha}{m}H(\\alpha)^{-1}\\displaystyle\\sum_{i=1}^{m}\\big[\\nabla\\ell(\\theta_{*}(\\alpha);z_{i}^{c})-\\mathbb{E}_{s}\\nabla\\ell(\\theta;z)\\big]+\\Delta\\,,}\\\\ &{\\qquad\\qquad\\qquad\\|\\Delta\\|\\le\\!C\\Big(\\|\\nabla R_{s}(\\theta_{*})\\|_{2}\\vee\\|\\nabla R_{s}(\\theta_{*})\\|_{2}^{2}\\Big)\\alpha^{2}+\\displaystyle\\frac{C}{m\\wedge n}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The claim follows by substituting the above in ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}R(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha))-R(\\pmb{\\theta})=\\mathbb{E}\\langle\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)-\\pmb{\\theta}_{*},\\pmb{H}(\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)-\\pmb{\\theta}_{*})\\rangle+O\\Big(\\mathbb{E}\\|\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)-\\pmb{\\theta}_{*}\\|^{3}\\Big)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C Gaussian sequence model: Proofs for Section 3.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "C.1 General ridge regression ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We define $\\hat{\\Sigma}=\\boldsymbol{{X}}^{\\top}\\boldsymbol{{X}}/n$ , $\\hat{\\Sigma}_{s}=\\boldsymbol{X}_{s}^{\\sf T}\\boldsymbol{X}_{s}/m$ , and $\\hat{\\Sigma}_{\\alpha}=(1-\\alpha)\\hat{\\Sigma}+\\alpha\\hat{\\Sigma}_{s}$ . We then have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{n,m}(\\alpha,\\lambda)=\\!B_{n,m}(\\alpha,\\lambda)+\\frac{(1-\\alpha)^{2}\\sigma^{2}}{n}\\cdot V_{n,m}(\\alpha,\\lambda)+\\frac{\\alpha^{2}\\sigma_{s}^{2}}{n}\\cdot V_{n,m}^{s}(\\alpha,\\lambda)\\,,}\\\\ &{\\quad B_{n,m}(\\alpha,\\lambda):=\\left\\|\\Sigma^{1/2}(\\Omega+\\hat{\\Sigma}_{\\alpha})^{-1}\\big(\\Omega\\theta_{*}-\\alpha\\hat{\\Sigma}_{s}(\\theta_{*}^{s}-\\theta_{*})\\big)\\right\\|^{2},}\\\\ &{\\quad V_{n,m}(\\alpha,\\lambda):=\\mathrm{Tr}\\Big((\\Omega+\\hat{\\Sigma}_{\\alpha})^{-1}\\hat{\\Sigma}(\\Omega+\\hat{\\Sigma}_{\\alpha})^{-1}\\Sigma\\Big)\\,,}\\\\ &{\\quad V_{n,m}^{s}(\\alpha,\\lambda):=\\mathrm{Tr}\\Big((\\Omega+\\hat{\\Sigma}_{\\alpha})^{-1}\\hat{\\Sigma}_{s}(\\Omega+\\hat{\\Sigma}_{\\alpha})^{-1}\\Sigma\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Without loss of generality, we can assume $\\Omega=\\mathrm{diag}((\\omega_{k})_{k\\geq1})$ with $\\omega_{k}$ non-decreasing. A simple calculation gives the following general expression for the test error: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle R_{n,m}(\\alpha,\\lambda)=\\!{\\cal B}_{n,m}(\\alpha,\\lambda)+s_{n,m}(\\alpha)\\cdot V_{n,m}(\\alpha,\\lambda)\\,,}\\\\ &{\\quad B_{n,m}(\\alpha,\\lambda):=\\!\\displaystyle\\sum_{k=1}^{\\infty}\\!\\left(\\frac{1}{1+\\lambda\\omega_{k}}\\right)^{2}\\left[(\\alpha+\\lambda\\omega_{k})\\theta_{*,k}-\\alpha\\theta_{*,k}^{*}\\right]^{2},}\\\\ &{\\quad V_{n,m}(\\alpha,\\lambda):=\\displaystyle\\sum_{k=1}^{\\infty}\\!\\left(\\frac{1}{1+\\lambda\\omega_{k}}\\right)^{2},}\\\\ &{\\quad s_{n,m}(\\alpha):=(1-\\alpha)^{2}\\frac{\\sigma^{2}}{n}+\\alpha^{2}\\frac{\\sigma_{s}^{2}}{m}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We define (with $k_{1}=0$ if the condition is never verified) ", "page_idx": 29}, {"type": "equation", "text": "$$\nk_{1}:=\\operatorname*{max}\\left\\{k:\\,\\lambda\\omega_{k}\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{0<k\\leq k_{1}}}&{{\\Rightarrow}}&{{0<\\lambda\\omega_{k}\\leq1\\,,}}\\\\ {{k_{1}<k}}&{{\\Rightarrow}}&{{1<\\lambda\\omega_{k}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now estimate various sums by breaking them by the value of $k$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\lambda_{n,m}}{\\lambda_{n}}\\leq\\displaystyle\\sum_{k=1}^{k_{1}}\\big[(\\alpha+\\lambda\\omega_{k})\\theta_{*,k}-\\alpha\\theta_{*,k}^{*}\\big]^{2}+\\displaystyle\\sum_{k=k_{1}+1}^{\\infty}\\frac{1}{(\\lambda\\omega_{k})^{2}}\\big[(\\alpha+\\lambda\\omega_{k})\\theta_{*,k}-\\alpha\\theta_{*,k}^{*}\\big]^{2}}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\sum_{k=1}^{k_{1}}\\big[\\alpha^{2}(\\theta_{*,k}-\\theta_{*,k}^{*})^{2}+2\\alpha(\\theta_{*,k}-\\theta_{*,k}^{*})\\lambda\\omega_{k}\\theta_{*,k}+(\\lambda\\omega_{k})^{2}\\theta_{*,k}^{2}\\big]}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{k=k_{1}+1}^{\\infty}\\Big[\\frac{\\alpha^{2}}{(\\lambda\\omega_{k})^{2}}(\\theta_{*,k}-\\theta_{*,k}^{*})^{2}-\\frac{2\\alpha}{\\lambda\\omega_{k}}(\\theta_{*,k}-\\theta_{*,k}^{*})\\theta_{*,k}+\\theta_{*,k}^{2}\\Big]}\\\\ &{\\leq\\alpha^{2}\\|\\theta_{*,\\leq k_{1}}-\\theta_{*,\\leq k_{1}}^{*}\\|^{2}+\\displaystyle\\frac{2\\alpha}{\\omega_{k}}|\\langle\\theta_{*,\\leq k_{1}}-\\theta_{*,\\leq k_{1}}^{*},\\theta_{*,\\leq k_{1}}\\rangle\\mathbf{a}|+\\displaystyle\\frac{1}{\\omega_{k}^{2}}\\|\\theta_{*,\\leq k_{1}}\\|_{\\Omega^{2}}^{2}}\\\\ &{\\quad\\quad+\\alpha^{2}\\omega_{k+1}^{2}\\|\\theta_{*,>k_{1}}-\\theta_{*,\\leq k_{1}}^{*}\\|_{\\Omega^{2}-2}^{2}+2\\alpha\\omega_{k+1}\\|\\langle\\theta_{*,>k_{1}}-\\theta_{*,\\leq k_{1}}^{*},\\theta_{*,>k_{1}}\\rangle\\mathbf{a}^{-1}\\|+\\|\\theta_{*,\\geq k_{1}}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\nV_{n,m}\\leq k_{1}+\\sum_{k>k_{1}}\\frac{\\omega_{k_{1}+1}^{2}}{\\omega_{k}^{2}}\\leq\\left(k_{1}+c_{\\#}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "since under the assumption $\\omega_{k}\\asymp k^{\\mu}$ , $\\mu>1/2$ , we have $\\begin{array}{r}{\\sum_{k>k_{1}}(\\omega_{k_{1}+1}/\\omega_{k})^{2}\\leq c_{\\#}}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "Recalling the definitions in the theorem, and letting ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\delta_{k}:=\\operatorname*{max}\\left(\\omega_{k+1}\\big|\\langle\\theta_{*,>k}-\\theta_{*,>k}^{s},\\theta_{*,>k}\\rangle_{\\Omega^{-1}}\\big|;\\ \\omega_{k+1}^{2}\\|\\theta_{*,>k}-\\theta_{*,>k}^{s}\\|_{\\Omega^{-2}}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nB_{n,m}\\leq\\alpha^{2}\\|\\pmb\\theta_{*}-\\pmb\\theta_{*}^{s}\\|^{2}+\\|\\pmb\\theta_{*,>k_{1}}\\|^{2}+\\frac{1}{\\omega_{k_{1}}^{2}}\\|\\pmb\\theta_{*,\\leq k}\\|_{\\Omega^{2}}^{2}+3\\delta_{k_{1}}+2\\Delta_{k_{1}}\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "whence ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}_{n,m}(\\alpha,\\lambda)\\le\\alpha^{2}\\|\\theta_{*}-\\theta_{*}^{s}\\|^{2}+\\|\\theta_{*,>k_{1}}\\|^{2}+\\frac{1}{\\omega_{k_{1}}^{2}}\\|\\theta_{*,\\le k_{1}}\\|_{\\Omega^{2}}^{2}+(k_{1}+c_{\\#})\\cdot s_{n,m}(\\alpha)+3\\delta_{k_{1}}+2\\Delta_{k_{1}}}\\\\ &{\\qquad\\qquad=\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+\\|\\theta_{*,>k_{1}}\\|^{2}+\\frac{1}{\\omega_{k_{1}}^{2}}\\|\\theta_{*,\\le k_{1}}\\|_{\\Omega^{2}}^{2}+(k_{1}+c_{\\#})\\cdot s_{n,m}(\\alpha)+3\\delta_{k_{1}}+2\\Delta_{k_{1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Next we specialize to the case $\\|\\pmb{\\theta}_{*,>k}\\|^{2}\\;\\leq\\;C_{\\theta}k^{-2\\rho}$ , $\\omega_{k}\\;\\asymp\\;k^{\\mu}\\;\\mu\\;\\ne\\;\\rho$ . In this case we have $\\omega_{k}^{-2}\\mathopen{}\\mathclose\\bgroup\\left\\|\\theta_{*,\\leq k}\\aftergroup\\egroup\\right\\|_{\\Omega^{2}}^{2}\\leq C k^{-2(\\mu\\wedge\\rho)}$ , and therefore, by suitably adjusting the constant $C$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nR_{n,m}(\\alpha,\\lambda)\\leq\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+C k_{1}^{-2(\\mu\\wedge\\rho)}+(k_{1}+c_{\\#})\\cdot s_{n,m}(\\alpha)+3\\delta_{k_{1}}+2\\Delta_{k_{1}}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now bound $\\delta_{k}$ . By Cauchy-Schwarz and monotonicity of $\\omega$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\omega_{k+1}\\big|\\langle\\theta_{*,>k}-\\theta_{*,>k}^{s},\\theta_{*,>k}\\rangle_{\\Omega^{-1}}\\big|\\leq\\|\\theta_{*,>k}-\\theta_{*,>k}^{s}\\|_{2}\\|\\theta_{*,>k}\\|_{2}\\leq2C_{\\theta}k^{-2\\rho}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and further ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\omega_{k+1}^{2}\\|\\theta_{*,>k}-\\theta_{*,>k}^{s}\\|_{\\Omega^{-2}}^{2}\\leq2\\|\\theta_{*,>k}\\|^{2}+2\\|\\theta_{*,>k}^{s}\\|^{2}\\leq4C_{\\theta}k^{-2\\rho}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\nR_{n,m}(\\alpha,\\lambda)\\leq\\alpha^{2}R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)+C k_{1}^{-2(\\mu\\wedge\\rho)}+(k_{1}+c_{\\#})\\cdot s_{n,m}(\\alpha)+2\\Delta_{k_{1}}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of claim $(a)$ . The stated assumption on $\\Delta_{k}$ imply that (eventually adjusting the constant $C$ ): ", "page_idx": 30}, {"type": "equation", "text": "$$\nR_{n,m}(\\alpha,\\lambda)\\leq\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+C k_{1}^{-2(\\mu\\wedge\\rho)}+(k_{1}+c_{\\#})\\cdot s_{n,m}(\\alpha)\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now select $\\lambda_{\\ast}(\\alpha)$ so that $k_{1}\\,\\asymp\\,s_{n,m}(\\alpha)^{-1+\\beta}$ where $\\beta\\,=\\,2(\\mu\\land\\rho)/(1+2(\\mu\\land\\rho))$ . (this is possible for all $n,m$ large enough under the assumption on $\\omega_{k}$ ), to A straightforward calculation yields: ", "page_idx": 31}, {"type": "equation", "text": "$$\nR_{n,m}(\\alpha,\\lambda_{*}(\\alpha))\\leq\\alpha^{2}R_{\\mathrm{su}}^{\\mathrm{ex}}(\\infty)+C\\cdot s_{n,m}(\\alpha)^{\\beta}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which proves claim $(a)$ . ", "page_idx": 31}, {"type": "text", "text": "Proof of Claim $(b)$ . We choose $\\omega_{k}~=~k^{\\mu}$ , $\\theta_{*,k}\\;=\\;k^{-\\rho^{\\prime}-1/2}$ , $\\theta_{*,k}^{s}\\,=\\,\\theta_{*,k}\\,+\\,a_{k}k^{-\\rho-1/2}$ , with $a_{k}\\sim\\mathsf{U n i f}(\\{-A,+A\\})$ . We will choose $A\\le1$ a sufficiently small numerical constant. Note that, for $\\mu>2\\rho+1/2$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Delta_{k}=k^{-\\mu}\\left|\\sum_{\\ell=1}^{k}a_{\\ell}\\ell^{\\mu-2\\rho-1}\\right|\\leq C A k^{-\\mu+\\varepsilon}\\left|\\sum_{\\ell=1}^{k}\\ell^{2\\mu-4\\rho-2}\\right|^{1/2}\\leq C A k^{-2\\rho-1/2+\\varepsilon^{\\prime}}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where, for any $\\varepsilon>0$ , the first inequality holds with probability at least $1/2$ for all $k>k_{0}(\\varepsilon)$ . We can therefore select the $a_{\\ell}$ , so that $\\dot{\\Delta}_{k}\\leq C^{\\prime\\prime}A k^{-2\\rho-\\dot{\\varepsilon}}$ for some $C^{\\prime\\prime}<\\infty$ . ", "page_idx": 31}, {"type": "text", "text": "Following the calculation at point $(a)$ decompose the bias term as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B_{n,m}=\\sum_{k=1}^{\\infty}\\bigg(\\frac{1}{1+\\lambda\\omega_{k}}\\bigg)^{2}\\big[\\alpha^{2}(\\theta_{*,k}-\\theta_{*,k}^{s})^{2}+(\\lambda\\omega_{k})^{2}\\theta_{*,k}^{2}\\big]+2\\alpha E_{n,m}\\,,}}\\\\ {{\\displaystyle E_{n,m}:=\\sum_{k=1}^{\\infty}\\bigg(\\frac{1}{1+\\lambda\\omega_{k}}\\bigg)^{2}(\\theta_{*,k}-\\theta_{*,k}^{s})\\lambda\\omega_{k}\\theta_{*,k}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that $|E_{n,m}|\\leq\\delta_{k_{1}}+\\Delta_{k_{1}}\\leq C A k_{1}^{-2(\\mu\\wedge\\rho)}$ . Therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{n,m}-\\alpha^{2}\\|\\theta_{*}-\\theta_{*}^{s}\\|^{2}}\\\\ &{\\geq\\displaystyle\\sum_{k=1}^{\\infty}\\Big(\\frac{\\lambda\\omega_{k}}{1+\\lambda\\omega_{k}}\\Big)^{2}\\theta_{*,k}^{2}-\\alpha^{2}\\displaystyle\\sum_{k=1}^{\\infty}\\left[1-\\Big(\\frac{1}{1+\\lambda\\omega_{k}}\\Big)^{2}\\right](\\theta_{*,k}-\\theta_{*,k}^{s})^{2}-C A k_{1}^{-2(\\mu\\wedge\\rho)}}\\\\ &{\\geq\\displaystyle\\frac{1}{4\\omega_{k+1}^{2}}\\|\\theta_{*,\\leq k_{1}}\\|_{\\Omega^{2}}^{2}+\\frac{1}{4}\\|\\theta_{*,>k_{1}}\\|^{2}-\\frac{A}{4\\omega_{k_{1}+1}}\\|\\theta_{*,\\leq k_{1}}\\|_{\\Omega}^{2}-\\frac{A}{4}\\|\\theta_{*,>k_{1}}\\|^{2}-C A k_{1}^{-2(\\mu\\wedge\\rho)}}\\\\ &{\\geq C\\,k_{1}^{-2(\\mu\\wedge\\rho)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By a similar calculation, we also obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\nV_{n,m}\\geq C\\,k_{1}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\nR_{n,m}(\\alpha,\\lambda)\\geq\\alpha^{2}R_{\\mathsf{s u}}^{\\mathsf{e x}}(\\infty)+C k_{1}^{-2(\\mu\\wedge\\rho)}+C k_{1}\\cdot s_{n,m}(\\alpha)\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The proof is completed by minimizing over $k_{1}$ . ", "page_idx": 31}, {"type": "text", "text": "D Analysis of the nonparametric model: Proofs for Section 3.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "This appendix is devoted to proving Theorem 2. Recall that this is established within the white noise model of Eq. (14), which we copy here for the readers\u2019 convenience ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}Y=f_{*}(\\pmb{x})\\,\\mathrm{d}\\pmb{x}+\\frac{\\sigma}{\\sqrt{n}}\\mathrm{d}B(\\pmb{x})\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The adaptation of the estimator (13) to this continuous setting is given explicitly below ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{f}_{n,m,\\alpha}=\\arg\\operatorname*{min}_{f}\\left\\{(1-\\alpha)\\left\\|Y-f\\right\\|_{2}^{2}+\\alpha\\|Y_{s}-f\\|_{2}^{2}+\\lambda\\|f\\|_{p,2}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The proof of Theorem 2 is based on a reduction to a suitable \u2018sequence model\u2019 via the Fourier transform, defined as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\theta(\\pmb{q}):=\\int_{[0,1]^{d}}f(\\pmb{x})\\,e^{-\\iota\\left<\\pmb{q},\\pmb{x}\\right>}\\,\\mathrm{d}\\pmb{x}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for $\\pmb q\\in\\mathcal{Q}_{d}:=\\left\\lbrace2\\pi\\pmb q\\,:\\,\\pmb q\\in\\mathbb{Z}^{d}\\right\\rbrace$ , where $\\iota=\\sqrt{-1}$ . The inverse Fourier transform is defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\nf({\\pmb x})=\\frac{1}{(2\\pi)^{d}}\\sum_{{\\pmb q}\\in\\mathcal{Q}_{d}}\\theta({\\pmb q})\\,e^{\\iota\\langle{\\pmb q},{\\pmb x}\\rangle}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We let $\\boldsymbol{\\theta}_{*}$ , $\\theta_{*,s}$ , and $\\hat{\\theta}_{\\lambda,p,n,m,\\alpha}$ respectively denote the Fourier transform of $f_{\\ast},f_{\\ast,s}$ , and $\\hat{f}_{\\lambda,p,n,m,\\alpha}$ . The Fourier transforms of the observations are given by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{Y}({\\pmb q})=\\theta_{*}({\\pmb q})+\\frac{\\sigma}{\\sqrt{n}}\\,G({\\pmb q})\\,,\\qquad\\hat{Y}_{s}({\\pmb q})=\\theta_{*,s}({\\pmb q})+\\frac{\\sigma_{s}}{\\sqrt{m}}\\,G_{s}({\\pmb q})\\,,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $G(\\pmb q)$ and $G_{s}(\\pmb q)$ are i.i.d. standard Gaussian. It then follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\theta}}_{n,m}(\\alpha)=\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\left\\{(1-\\alpha)\\lVert\\hat{\\pmb{Y}}-\\pmb{\\theta}\\rVert_{2}^{2}+\\alpha\\lVert\\hat{\\pmb{Y}}_{s}-\\pmb{\\theta}\\rVert_{2}^{2}+\\lambda\\lVert\\pmb{\\theta}\\rVert_{p,2}^{2}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we abuse the notation to define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}\\|_{p,2}^{2}:=\\sum_{\\pmb{q}\\in\\mathcal{Q}_{d}}c_{p,\\pmb{q}}\\,|\\theta(\\pmb{q})|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with $c_{p,q}:=1+\\|q\\|^{2r}.$ . Minimizing (50) we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{n,m}(\\pmb{q};\\alpha)=\\frac{1}{1+\\lambda c_{p,\\pmb{q}}}\\big[(1-\\alpha)\\,\\hat{Y}(\\pmb{q})+\\alpha\\,\\hat{Y}_{s}(\\pmb{q})\\big]\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking the inverse Fourier transform and plugging it into the excess risk formula we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R(\\hat{f}_{n,m,\\alpha})=\\sum_{\\pmb{q}\\in\\mathcal{Q}_{d}}\\frac{1}{(1+\\lambda c_{p,\\pmb{q}})^{2}}\\big[\\alpha(\\theta_{*,s}-\\theta_{*})(\\pmb{q})}}\\\\ &{\\quad\\quad\\quad+\\,\\lambda c_{p,\\pmb{q}}\\theta_{*}(\\pmb{q})\\big]^{2}+V_{n,m}\\displaystyle\\sum_{\\pmb{q}\\in\\mathcal{Q}_{d}}\\frac{1}{(1+\\lambda c_{p,\\pmb{q}})^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\nV_{n,m}:=(1-\\alpha)^{2}\\frac{\\sigma^{2}}{n}+\\alpha^{2}\\frac{\\sigma_{s}^{2}}{m}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The convexity of $x\\to x^{2}$ implies ", "page_idx": 32}, {"type": "equation", "text": "$$\n(a+b)^{2}=\\left(\\gamma\\frac{a}{\\gamma}+(1-\\gamma)\\frac{b}{1-\\gamma}\\right)^{2}\\leq\\frac{a^{2}}{\\gamma}+\\frac{b^{2}}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for $\\gamma\\in(0,1)$ and therefore we can upper bound the first sum in (53) by taking $\\gamma=1/(1+\\delta)$ for any $\\delta>0$ , which yields ", "page_idx": 32}, {"type": "equation", "text": "$$\nR(f_{n,m,\\alpha})\\leq(1+\\delta)\\alpha^{2}\\|\\theta_{*,s}-\\theta_{*}\\|_{2}^{2}+\\frac{1+\\delta}{\\delta}\\sum_{\\boldsymbol{q}\\in\\mathcal{Q}_{d}}\\left(\\frac{\\lambda c_{p,\\boldsymbol{q}}}{1+\\lambda c_{p,\\boldsymbol{q}}}\\right)^{2}|\\theta_{*}(\\boldsymbol{q})|^{2}+V_{n,m}\\sum_{\\boldsymbol{q}\\in\\mathcal{Q}_{d}}\\frac{1}{(1+\\lambda c_{p,\\boldsymbol{q}})^{2}}\\|\\theta_{*}(\\boldsymbol{q})\\|_{2}^{2}+\\|\\theta_{*}(\\boldsymbol{q})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "D.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We now upper bound the first sum above. We note that, defining $q_{0}$ via $\\lambda c_{r}(q_{0})=1$ (with an abuse of notation $c_{r}(t)=1+t^{2r})$ , whence $q_{0}\\ge(\\lambda/2)^{-1/2r}$ for all $\\lambda<1$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{q\\in Q_{d}}\\left(\\frac{\\lambda c_{p,q}}{1+\\lambda c_{r,q}}\\right)^{2}\\cdot|\\theta_{*}(q)|^{2}\\leq\\displaystyle\\sum_{q\\in Q_{d},\\|q\\|_{2}\\leq q_{0}}\\lambda^{2}c_{r}(q)^{2}|\\theta_{*}(q)|^{2}+\\displaystyle\\sum_{q\\in Q_{d},\\|q\\|_{2}>q_{0}}|\\theta_{*}(q)|^{2}}\\\\ &{\\qquad\\leq\\lambda^{2}\\displaystyle\\operatorname*{max}_{\\|q\\|_{2}\\leq q_{0}}\\frac{c_{r}(q)^{2}}{c_{s}(q)}\\displaystyle\\sum_{q\\in Q_{d},\\|q\\|_{2}\\leq q_{0}}^{q\\in Q_{d}}|\\theta_{*}(q)|^{2}+\\displaystyle\\operatorname*{max}_{\\|q\\|_{2}>q_{0}}\\frac{1}{c_{s}(q)}\\displaystyle\\sum_{q\\in Q_{d},\\|q\\|_{2}>q_{0}}c_{s}(q)|\\theta_{*}(q)}\\\\ &{\\qquad\\overset{(a)}{\\leq}\\lambda^{2}\\displaystyle\\operatorname*{max}_{\\|q\\|_{2}\\leq q_{0}}\\frac{c_{r}(q)^{2}}{c_{s}(q)}\\displaystyle\\operatorname*{max}_{\\|q\\|_{2}>q_{0}}\\frac{1}{c_{s}(q)}}\\\\ &{\\qquad\\leq\\lambda^{2}\\operatorname*{max}\\left(1,\\frac{c_{r}(q_{0})^{2}}{c_{s}(q_{0})}\\right)+\\displaystyle\\frac{1}{c_{s}(q_{0})}}\\\\ &{\\qquad\\leq C\\operatorname*{max}(\\lambda^{2},\\lambda^{p/r})+C\\lambda^{p/r}\\leq C\\lambda^{2\\wedge(p/r)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where in $(a)$ we used the fact that $\\begin{array}{r}{\\|f_{*}\\|_{2,p}^{2}=\\sum_{\\pmb{q}}c_{s}(\\pmb{q})|\\theta_{*}(\\pmb{q})|}\\end{array}$ . Letting $C_{i}(d)$ be constants depending on $d$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{q\\in\\mathbb{Q}_{d}}\\frac{1}{(1+\\lambda c_{r,q})^{2}}\\leq C_{1}(d)\\int_{\\mathbb{R}^{d}}\\frac{1}{(1+\\lambda c_{r,q})^{2}}\\,{\\mathrm{d}}q}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\leq C_{1}(d)\\int_{\\mathbb{R}^{d}}\\frac{1}{(1+\\lambda\\|q\\|^{2r}))^{2}}\\,{\\mathrm{d}}q}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\leq C_{2}(d)\\int_{0}^{\\infty}\\frac{t^{d-1}}{(1+\\lambda t^{2r})^{2}}{\\mathrm{d}}t}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\leq C_{2}(d)\\int_{0}^{\\lambda^{-1/2r}}t^{d-1}\\,{\\mathrm{d}}t+C_{2}(d)\\lambda^{-2}\\int_{\\lambda^{-1/2r}}^{\\infty}t^{d-1-4r}\\,{\\mathrm{d}}t\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For convergence we requite $r>d/4$ , in which case ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{q\\in\\mathcal{Q}_{d}}\\frac{1}{(1+\\lambda c_{r,q})^{2}}\\leq C_{4}(d)\\lambda^{-d/2r}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "E Analysis of high-dimensional regression: Proofs for Section 3.4 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "E.1 Auxiliary definition for Theorem 3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Our characterization is given in terms of a variational principle. For $\\delta,\\delta_{s}\\in(0,\\infty)$ , define $\\mathcal{R}(\\cdot;\\alpha)$ : $\\mathbb{R}_{\\geq0}^{3}\\to\\mathbb{R}$ via ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\xi,\\xi_{\\perp},\\omega;\\alpha):=-\\omega\\sqrt{\\rho^{2}+\\rho_{s}^{2}}+\\rho\\sqrt{\\delta(\\tau^{2}+\\sigma^{2})}+\\rho_{s}\\sqrt{\\delta_{s}(\\tau_{s}^{2}+\\sigma_{s}^{2})}}\\\\ &{\\qquad\\qquad\\qquad-\\displaystyle\\frac{\\delta\\rho^{2}}{2(1-\\alpha)}-\\frac{\\delta_{s}\\rho_{s}^{2}}{2\\alpha}+\\displaystyle\\frac{\\lambda}{2}\\big(\\xi^{2}+\\xi_{\\perp}^{2}+\\omega^{2}\\big)\\:,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\tau,\\tau_{s}$ are defined by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau^{2}:=(\\xi-r)^{2}+\\xi_{\\perp}^{2}+\\omega^{2}\\,,}\\\\ &{\\tau_{s}^{2}:=(\\xi-r_{s}\\cos\\gamma)^{2}+(\\xi_{\\perp}-r_{s}\\sin\\gamma)^{2}+\\omega^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $\\rho=\\overline{{\\rho}}/\\sqrt{1+t^{2}}$ , $\\rho_{s}=\\overline{{\\rho}}t/\\sqrt{1+t^{2}}$ , with $\\overline{{\\rho}}$ solving the polynomial equation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\rho}}^{2}=\\frac{\\delta(\\tau^{2}+\\sigma^{2})}{\\big(\\delta/(1-\\alpha)+\\omega/\\overline{{\\rho}}\\big)^{2}}+\\frac{\\delta_{s}(\\tau_{s}^{2}+\\sigma_{s}^{2})}{\\big(\\delta_{s}/\\alpha+\\omega/\\overline{{\\rho}}\\big)^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $t$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t=\\frac{\\omega+\\delta\\overline{{\\rho}}/(1-\\alpha)}{\\omega+\\delta_{s}\\overline{{\\rho}}/\\alpha}\\cdot\\sqrt{\\frac{\\delta_{s}\\left(\\tau_{s}^{2}+\\sigma_{s}^{2}\\right)}{\\delta\\left(\\tau^{2}+\\sigma^{2}\\right)}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Theorem 3 states that the asymptotics of the test error is determined by the minimizer of $\\mathcal{R}$ . ", "page_idx": 33}, {"type": "text", "text": "E.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The proof is based on Gordon Gaussian comparison inequality [Gor85, Ver18], and follow a standard route, see e.g. [TOH15, TAH18, MM21]. We will limit ourselves to outlining the main steps of the calculation. Throughout, we consider the case $\\varepsilon_{0}>0$ , $\\delta+\\delta_{s}>1$ because the other one $\\varepsilon_{0}=0$ and $\\delta,\\delta_{s}>1;$ ) is analogous and less interesting. ", "page_idx": 33}, {"type": "text", "text": "We begin by rewriting the ridge cost function in terms of a Lagrangian ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widehat{R}_{n,m}(\\theta;\\alpha)=\\displaystyle\\operatorname*{max}_{u\\in\\mathbb{R}^{n}}\\displaystyle\\operatorname*{max}_{u^{s}\\in\\mathbb{R}^{m}}\\widehat{L}_{n,m}(\\theta,u,u^{s};\\alpha)\\,,}}\\\\ {{\\widehat{L}_{n,m}(\\theta,u,u^{s};\\alpha):=\\langle u,X(\\theta-\\theta_{*})\\rangle+\\langle u^{s},X^{s}(\\theta-\\theta_{*,s})\\rangle-\\langle u,\\varepsilon\\rangle-\\langle u^{s},\\varepsilon^{s}\\rangle}}\\\\ {{-\\displaystyle\\frac{n\\|u\\|_{2}^{2}}{2(1-\\alpha)}-\\displaystyle\\frac{m\\|u^{s}\\|_{2}^{2}}{2\\alpha}+\\displaystyle\\frac{\\lambda}{2}\\,\\|\\theta\\|_{2}^{2}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let $\\Delta(\\pmb{\\theta},\\pmb{u},\\pmb{u}^{s}):=\\|\\pmb{u}\\|_{2}\\|\\pmb{\\theta}-\\pmb{\\theta}_{*}\\|_{2}G+\\|\\pmb{u}^{s}\\|_{2}\\|\\pmb{\\theta}-\\pmb{\\theta}_{*,s}\\|_{2}G_{s}.$ , where $G,G_{s}$ are independent standard normal random variables, independent of $X,X^{s}$ . By Gordon\u2019s inequality [Gor85], we can compare the Gaussian process $\\widehat{L}_{n,m}(\\pmb{\\theta},\\pmb{u},\\pmb{u}^{s};\\alpha)+\\Delta(\\pmb{\\theta},\\pmb{u},\\pmb{u}^{s})$ to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\overset{\\times_{G}}{\\underset{\\substack{n,m}}{\\sum}}(\\theta,u,u^{s};\\alpha):=\\lVert u\\rVert\\langle g,\\theta-\\theta_{*}\\rangle+\\lVert\\theta-\\theta_{*}\\rVert\\langle h,u\\rangle+\\lVert u^{s}\\rVert\\langle g^{s},\\theta-\\theta_{*,s}\\rangle+\\lVert\\theta-\\theta_{*,s}\\rVert\\langle h,u^{s}\\rangle\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n-\\left\\langle\\pmb{u},\\pmb{\\varepsilon}\\right\\rangle-\\left\\langle\\pmb{u}^{s},\\pmb{\\varepsilon}^{s}\\right\\rangle-\\frac{n\\|\\pmb{u}\\|_{2}^{2}}{2(1-\\alpha)}-\\frac{m\\|\\pmb{u}^{s}\\|_{2}^{2}}{2\\alpha}+\\frac{\\lambda}{2}\\,\\|\\pmb{\\theta}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next we define the orthonormal vectors ", "page_idx": 34}, {"type": "equation", "text": "$$\nv_{*}:=\\frac{\\pmb\\theta_{*}}{\\lVert\\pmb\\theta_{*}\\rVert_{2}}\\,,\\qquad\\pmb v_{*}^{\\perp}:=\\frac{P_{\\pmb\\theta_{*}}^{\\perp}\\pmb\\theta_{*,s}}{\\lVert P_{\\pmb\\theta_{*}}^{\\perp}\\pmb\\theta_{*,s}\\rVert_{2}}\\,,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{P_{\\theta_{*}}^{\\bot}=I-P_{\\theta_{*}}:=I-v_{*}v_{*}^{\\top}}\\end{array}$ is the projector orthogonal to $\\pmb{\\theta}_{*}$ . We then decompose ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\pmb\\theta}=\\xi{\\pmb v}_{*}+\\xi_{\\perp}\\,{\\pmb v}_{*}^{\\perp}+{\\pmb\\theta}^{\\perp}\\,,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\langle{\\pmb v}_{*},{\\pmb\\theta}^{\\perp}\\rangle\\,=\\,\\langle{\\pmb v}_{*}^{\\perp},{\\pmb\\theta}^{\\perp}\\rangle\\,=\\,0$ , and define $\\omega:=\\,\\|\\theta^{\\perp}\\|_{2}$ . Defining $\\tau^{2}\\:=\\:\\|\\pmb{\\theta}\\:-\\:\\pmb{\\theta}_{*}\\|_{2}^{2}$ , $\\tau_{s}^{2}\\;=\\;$ $\\lVert\\pmb{\\theta}-\\pmb{\\theta}_{*,s}\\rVert_{2}^{2}$ , Eq. (60) follows. ", "page_idx": 34}, {"type": "text", "text": "With these notations, and letting $\\hat{\\sigma}^{2}=\\lVert\\tau h+\\varepsilon\\rVert_{2}^{2}/n-\\tau^{2},\\hat{\\sigma}_{s}^{2}=\\lVert\\tau_{s}h^{s}+\\varepsilon^{s}\\rVert_{2}^{2}/m-\\tau_{s}^{2}$ , we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widehat{\\mathcal{L}}_{n,m}^{G}(\\theta,\\rho,\\rho_{s};\\alpha):=\\displaystyle\\operatorname*{max}_{u,u^{s}}\\left\\{\\widehat{L}_{n,m}^{G}(\\theta,u,u^{s};\\alpha):\\|u\\|=\\displaystyle\\frac{\\rho}{\\sqrt d},\\,\\|u^{s}\\|=\\displaystyle\\frac{\\rho_{s}}{\\sqrt d}\\right\\},\\qquad\\qquad\\qquad(68\\,\\mathrm{~)}}\\\\ {\\widehat{\\mathcal{L}}_{n,m}^{G}(\\theta,\\rho,\\rho_{s};\\alpha)=\\displaystyle\\frac{\\rho}{\\sqrt d}\\langle g,\\theta-\\theta_{*}\\rangle+\\frac{\\rho_{s}}{\\sqrt d}\\langle g^{s},\\theta-\\theta_{*,s}\\rangle+\\rho\\sqrt{\\delta(\\tau^{2}+\\hat{\\sigma}^{2})}+\\rho_{s}\\sqrt{\\delta_{s}(\\tau_{s}^{2}+\\hat{\\sigma}_{s}^{2})}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(69\\mathrm{~}}\\\\ {-\\displaystyle\\frac{\\delta\\rho^{2}}{2(1-\\alpha)}-\\frac{\\delta_{s}\\rho_{s}}{2\\alpha}+\\frac{\\lambda}{2}\\left(\\xi^{2}+\\xi_{\\perp}^{2}+\\omega^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We finally decompose ${\\pmb g}={\\pmb g}_{\\parallel}+{\\pmb g}_{\\perp}$ where $\\pmb{g}_{\\parallel}\\in\\mathsf{s p a n}(\\pmb{v}_{\\ast},\\pmb{v}_{\\ast}^{\\bot})$ and $\\pmb{g}_{\\parallel}\\perp\\mathsf{s p a n}(\\pmb{v}_{*},\\pmb{v}_{*}^{\\perp})$ , and similarly for $\\pmb{g}_{s}$ , and define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n,m}^{G}(\\boldsymbol{\\xi},\\boldsymbol{\\xi}_{\\bot},\\omega,\\rho,\\rho_{s};\\alpha):=\\operatorname*{min}_{\\theta}\\left\\{\\widehat{\\mathcal{L}}_{n,m}^{G}(\\theta,\\rho,\\rho_{s};\\alpha):\\,\\theta=\\xi v_{*}+\\xi_{\\bot}\\,v_{*}^{\\bot}+\\theta^{\\bot}\\,,\\ \\Vert\\theta^{\\bot}\\Vert=\\omega\\right\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Defining $\\iota$ via $\\|\\rho\\pmb{g}_{\\perp}/\\sqrt{n}+\\rho_{s}\\pmb{g}_{s,\\perp}/\\sqrt{m}\\|=(1+\\iota)\\sqrt{\\rho^{2}+\\rho_{s}^{2}}$ , we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n,m}^{G}(\\xi,\\xi_{\\perp},\\omega,\\rho,\\rho_{s};\\alpha)=-\\,(1+\\iota)\\sqrt{\\rho^{2}+\\rho_{s}^{2}}\\cdot\\omega+\\Delta+\\rho\\sqrt{\\delta(\\tau^{2}+\\hat{\\sigma}^{2})}+\\rho_{s}\\sqrt{\\delta_{s}(\\tau_{s}^{2}+\\hat{\\sigma}_{s}^{2})}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n-\\;\\frac{\\delta\\rho^{2}}{2(1-\\alpha)}-\\frac{\\delta_{s}\\rho_{s}^{2}}{2\\alpha}+\\frac{\\lambda}{2}\\left(\\xi^{2}+\\xi_{\\perp}^{2}+\\omega^{2}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\Delta$ is the contribution of the perpendicular components. Simple concentration estimates imply that for any $\\varepsilon>0$ there exist $c(\\varepsilon)>0$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\big(|\\hat{\\sigma}-\\sigma|\\leq\\varepsilon\\sqrt{\\tau^{2}+\\sigma^{2}},|\\hat{\\sigma}_{s}-\\sigma_{s}|\\leq\\varepsilon\\sqrt{\\tau_{s}^{2}+\\sigma_{s}^{2}}\\big)\\geq1-e^{-c(\\varepsilon)n}\\,,}\\\\ &{}&{\\mathbb{P}\\big(\\Delta|\\leq\\sqrt{(\\rho^{2}+\\rho_{s}^{2})(\\xi^{2}+\\xi_{\\perp}^{2})}\\big)\\geq1-e^{-c(\\varepsilon)n}\\,,}\\\\ &{}&{\\mathbb{P}\\big(|\\iota|\\leq\\varepsilon\\big)\\geq1-e^{-c(\\varepsilon)n}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can then estimate $\\mathcal{L}_{n,m}^{G}(\\xi,\\xi_{\\perp},\\omega,\\rho,\\rho_{s};\\alpha)$ by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{G}(\\xi,\\xi_{\\bot},\\omega,\\rho,\\rho_{s};\\alpha)=-\\,\\sqrt{\\rho^{2}+\\rho_{s}^{2}}\\cdot\\omega+\\rho\\sqrt{\\delta(\\tau^{2}+\\sigma^{2})}+\\rho_{s}\\sqrt{\\delta_{s}(\\tau_{s}^{2}+\\sigma_{s}^{2})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,\\frac{\\delta\\rho^{2}}{2(1-\\alpha)}-\\frac{\\delta_{s}\\rho_{s}^{2}}{2\\alpha}+\\frac{\\lambda}{2}\\left(\\xi^{2}+\\xi_{\\bot}^{2}+\\omega^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Differenti\u221aating with respect to $\\rho$ and $\\rho_{s}$ and setting the derivatives to 0 yields $\\rho\\,=\\,\\overline{{\\rho}}/\\sqrt{1+t^{2}}$ , $\\rho_{s}=\\overline{{\\rho}}t/\\sqrt{1+t^{2}}$ , with $\\overline{{\\rho}},t$ given by Eqs. (61), (62). By computing second derivatives, one obtain that this is a local maximum. Since $\\overset{\\cdot}{\\mathcal{L}^{G}}(\\xi,\\xi_{\\perp},\\omega,\\rho,\\rho_{s};\\alpha)\\to-\\infty$ as $\\rho^{2}+\\rho_{s}^{2}\\to\\infty$ , the maximum over $\\rho,\\rho_{s}$ is either achieved at this point or at the boundary $\\{\\rho=0\\}\\cup\\{\\rho_{s}=0\\}$ . By checking the signs of partial derivatives along this boundary, the only other possibility is $\\rho=\\rho_{s}=0$ . ", "page_idx": 35}, {"type": "text", "text": "For economy of notation, write $F(\\rho,\\rho_{s})\\;:=\\;\\mathcal{L}^{G}(\\xi,\\xi_{\\perp},\\omega,\\rho,\\rho_{s};\\alpha)$ . For any unit vector $\\textit{\\textbf{v}}=$ $(v_{1},v_{2})\\geq0$ , the directional derivative is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left.\\nabla_{v}F(r)\\right|_{r=0}=-\\omega+v_{1}\\sqrt{\\delta(\\tau^{2}+\\sigma^{2})}+v_{2}\\sqrt{\\delta_{s}(\\tau_{s}^{2}+\\sigma_{s}^{2})}\\,}\\\\ &{\\qquad\\qquad\\qquad\\geq\\omega\\,\\big[-1+v_{1}\\sqrt{\\delta}+v_{2}\\sqrt{\\delta_{s}}\\big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By maximizing over the direction, we see that $\\pmb{v}$ can be chosen so that $\\nabla_{v}F(\\mathbf{0})\\geq\\omega[-1+\\sqrt{\\delta+\\delta_{s}}]$ .   \nHence $\\rho=\\rho_{s}=0$ cannot be the global aximum for $\\delta+\\delta_{s}>1$ . ", "page_idx": 35}, {"type": "text", "text": "Hence, we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\xi,\\xi_{\\perp},\\omega;\\alpha)=\\operatorname*{max}_{\\rho,\\rho_{s}\\geq0}\\mathcal{L}^{G}(\\xi,\\xi_{\\perp},\\omega,\\rho,\\rho_{s};\\alpha)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We further note that, for fixed $\\rho,\\rho_{s}>0$ , the function $(\\xi,\\xi_{\\bot},\\omega)\\mapsto\\mathcal{L}^{G}(\\xi,\\xi_{\\bot},\\omega,\\rho,\\rho_{s};\\alpha)$ is jointly strictly convex for $\\lambda>0$ . Hence $(\\xi,\\xi_{\\perp},\\omega)\\mapsto\\mathcal{R}(\\xi,\\xi_{\\perp},\\omega;\\alpha)$ is also strictly convex for $\\lambda\\,>\\,0$ . Therefore, it has a unique minimizer, which we denote by $(\\xi^{*},\\xi_{\\perp}^{*},\\omega^{*})$ . Proceeding as in [MM21], we obtain the following result. ", "page_idx": 35}, {"type": "text", "text": "Proposition E.1. Under the assumptions of Proposition 3, for any $\\varepsilon,\\varepsilon_{0}\\;>\\;0$ there exists $c=$ $c(\\varepsilon,\\varepsilon_{0})>0$ such that, $i f\\alpha\\in\\left[\\varepsilon_{0},1-\\varepsilon_{0}\\right]$ (letting $\\begin{array}{r}{P^{\\perp}:=I-v_{*}v_{*}^{\\top}-v_{*}^{\\perp}(v_{*}^{\\perp})^{\\top})}\\end{array}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big\\{\\big|\\langle v_{*},\\hat{\\theta}_{n,m}\\rangle-\\xi^{*}\\big|\\leq\\varepsilon,\\,\\big|\\langle v_{*}^{\\perp},\\hat{\\theta}_{n,m}\\rangle-\\xi_{\\perp}^{*}\\big|\\leq\\varepsilon,\\,,\\,\\big|\\|P^{\\perp}\\hat{\\theta}_{n,m}\\|-\\omega^{*}\\big|\\leq\\varepsilon\\Big\\}\\geq1-2\\,e^{-c n}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In particular, the last proposition implies (a weaker form of) Theorem 3 whereby the supremum is taken over a finite net. Namely for $\\eta>0$ , we define ", "page_idx": 35}, {"type": "equation", "text": "$$\nN(\\varepsilon_{0},\\eta):=\\left[\\varepsilon_{0},1-\\varepsilon_{0}\\right]\\cap\\eta\\mathbb{Z}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Recalling that, in the present case, $R(\\hat{\\pmb{\\theta}})=\\|\\hat{\\pmb{\\theta}}-\\pmb{\\theta}\\|_{2}^{2}$ , we obtain (after adjusting the constant $c$ ) we have therefore: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{\\alpha\\in N(\\varepsilon_{0},\\eta)}\\big|R(\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha))-\\mathcal{R}_{\\mathrm{test}}(\\alpha)\\big|\\geq\\varepsilon\\Big)\\geq1-2\\,e^{-c n}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, let $\\boldsymbol{X}_{+}\\,\\in\\,\\mathbb{R}^{(m+n)\\times d}$ be the matrix obtained by stacking $\\mathbf{\\deltaX}$ and $X_{s}$ . Given constants $C_{1},C_{2},C_{3}$ , define the good event ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\left\\{C_{1}n\\leq\\lambda_{\\operatorname*{min}}(X_{+}^{\\mathsf{T}}X_{+})\\leq\\lambda_{\\operatorname*{max}}(X_{+}^{\\mathsf{T}}X_{+})\\leq C_{2}n;\\|X^{\\mathsf{T}}y\\|\\leq C_{3}n,\\ \\|X_{s}^{\\mathsf{T}}y_{s}\\|\\leq C_{3}n\\right\\}/\\int_{\\Sigma_{\\mathsf{t}}}\\left\\|y_{s}\\right\\|_{L^{2}(\\Sigma_{\\mathsf{t}})}\\,d s.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By a standard bound on eigenvalues of Wishart matrices [Ver18], for $\\delta+\\delta_{s}>1$ , we can choose $C_{1},C_{2},C_{3}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{G})\\ge1-2e^{-c n}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Further on $\\mathcal{G}$ , $,\\pmb{\\theta}_{n,m}(\\alpha)$ is bounded (in $\\ell_{2}$ norm, and Lipschitz continuous in $\\alpha$ ). As a consequence, for a sufficiently large constant $L$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\big|R(\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha_{1}))-R(\\widehat{\\pmb{\\theta}}_{n,m}(\\alpha_{2}))\\big|\\leq L|\\alpha_{1}-\\alpha_{2}|\\forall\\alpha_{1},\\alpha_{2}\\in[\\varepsilon_{0},1-\\varepsilon_{0}]\\Big)\\geq1-2e^{-c n}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The claim follows by using this estimate together with Eq. (78). ", "page_idx": 35}, {"type": "text", "text": "F Datasets information ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 Imdb reviews datatset: \u2013 Paper: $[\\mathrm{MDP^{+}11}]$ \u2013 Link \u2013 Licence: Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License   \n\u2022 Rotten Tomatoes reviews: \u2013 Paper: [PL05] \u2013 Link \u2013 Data has been scraped from the publicly available website https://www.rottentomatoes.com as of 2020-10-31. \u2013 Licence: CC0: Public Domain   \n\u2022 Goodreads bookreviews \u2013 Papers: [WMNM19], [WM18] \u2013 Link \u2013 License: Unknown   \n\u2022 CIFAR10 and CIFAR100: \u2013 Paper: [Kri09] \u2013 Link \u2013 License: Unknown   \n\u2022 TCGA Pan-Cancer Clinical Data \u2013 Link \u2013 Publicaly availbale, free to use \u2013 Licence: None ", "page_idx": 36}, {"type": "text", "text": "G Compute resources information ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We ran all experiments on a single machine with 2 RTX 4090 GPUs and a 24-core Intel Xeon E5 CPU. All experiments completed in less than 24 hours. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: These claims are justified by our theorems and the experimental results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We discuss it in Section 5. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All assumptions and proofs are clearly mentioned in the paper. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide detailed setup of our experiments ion appendix and the main paper. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: All the datasets used are public datasets. The results can be reproduced using the details we provided. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide these details in the experiment sections in appendix and the main paper. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: Since we observed a good match between the smooth theoretical curves and the empirical results across several dozen different dataset/model combinations, we chose to keep the figures clean by plotting only the average results and omitting the confidence intervals. If preferred, we are happy to replace the existing plot with ones with confidence intervals. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide the details of the compute resources in Appendix G. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have read the code of ethics and our submission abide by that. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning and efficient use of data. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not release any new dataset or model. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All of the third-party dataset used in the paper are well-known, publicly available datasets. In Section F, we cite the original papers, link to the datasets, and when available, cite the licenses. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}]