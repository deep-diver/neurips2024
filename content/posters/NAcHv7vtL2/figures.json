[{"figure_path": "NAcHv7vtL2/figures/figures_2_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of models trained on mixtures of original and surrogate data for sentiment analysis using IMDB and Rotten Tomatoes datasets. The x-axis represents the weight given to surrogate data (\u03b1), ranging from 0 (only original data) to 1 (only surrogate data). The y-axis shows the classification loss.  Different curves represent different amounts of original (n) and surrogate (m) data. The black curves represent the prediction based on a scaling law derived in the paper (Equation 4).  The figure demonstrates that using a mixture of data with optimal weighting leads to lower test error compared to using only original or surrogate data.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_2_2.jpg", "caption": "Figure 2: Performance of unweighted vs weighted ERM approach for the setting in Figure 1", "description": "This figure compares the performance of unweighted and optimally weighted empirical risk minimization (ERM) approaches. The x-axis represents the amount of surrogate data (m), and the y-axis shows the classification loss. The plot reveals that the optimally weighted ERM consistently outperforms the unweighted approach, achieving lower classification loss across different amounts of surrogate data.  This highlights the importance of optimal weighting when integrating surrogate data into model training.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_7_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of models trained on mixtures of original and surrogate data for different combinations of the number of original samples (n) and surrogate samples (m). The x-axis represents the weight parameter \u03b1, which controls the contribution of surrogate data in the training process. The red dots represent the empirical test error obtained from experiments, while the black curves show the predictions of a scaling law (Equation 4 from the paper). The figure demonstrates that integrating surrogate data can significantly reduce the test error, even when the surrogate data is different from the original data. The optimal weight \u03b1 is neither 0 nor 1 (i.e. training only on original or only on surrogate data), suggesting the importance of optimally weighing the contribution of both datasets.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_8_1.jpg", "caption": "Figure 4: Lasso-based Cox regression on TCGA PanCancer dataset. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of Lasso-based Cox regression on the TCGA PanCancer dataset when trained on mixtures of original and surrogate data.  The x-axis represents the weight parameter (alpha) given to the surrogate data, ranging from 0 to 1. Different panels show the results for different combinations of the number of original data points (n) and the number of surrogate data points (m). The red dots represent the experimentally observed test error, while the black curves represent the test error predicted by equation (4), a scaling law derived in the paper. The scaling law aims to capture how the test error changes as a function of alpha, n, and m.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_9_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error achieved when training neural networks on mixtures of original and surrogate data. The x-axis represents the weight parameter (\u03b1) given to the surrogate data in the training process, varying from 0 (only original data) to 1 (only surrogate data). The y-axis shows the classification loss. Different curves represent different combinations of the number of original data points (n) and the number of surrogate data points (m).  The black curves represent the prediction from Equation 4 (a scaling law derived in the paper) and closely follow the experimental results.", "section": "1 Introduction and overview"}, {"figure_path": "NAcHv7vtL2/figures/figures_13_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure displays the test error achieved when training neural networks on mixtures of original and surrogate data.  The x-axis represents the weight parameter \u03b1, which controls the weighting between original and surrogate data. Different points represent different dataset sizes (n for original, m for surrogate). Red dots represent the actual test error obtained through experiments.  The black curves represent the predictions generated by Equation 4, which is a scaling law proposed by the paper to model the relationship between dataset size, weight parameter, and test error.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_14_1.jpg", "caption": "Figure 7: Gaussian mixture data and logistic regression. Test error when trained on mixtures of original (n varying by row) and surrogate (m varying by column) data. Black curves: scaling formula (4).", "description": "This figure displays the results of an experiment on Gaussian mixture data using logistic regression.  It shows the test error achieved when training models on mixtures of real data (n samples) and surrogate data (m samples), for various values of the mixing parameter (alpha, \u03b1).  The x-axis represents alpha (0 to 1), indicating the proportion of surrogate data used in training.  The y-axis represents the classification loss.  Different rows correspond to varying amounts of real data (n), while different columns correspond to varying amounts of surrogate data (m). The black curves represent the prediction from the scaling law (4), which is a mathematical model the authors developed to approximate the test error as a function of n, m, and \u03b1. This scaling law is a core finding of the paper, showing the relationship between the quantity of real and surrogate data and the resulting model performance.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_15_1.jpg", "caption": "Figure 12: IMDB and Rotten Tomatoes data and logistic regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots), together with scaling law fits (black lines). Best fit parameters are \u03b2 = 0.27, R* = 0.101 and R(\u221e) = 0.148.", "description": "The figure shows the test error for IMDB movie reviews (original data) and Rotten Tomatoes movie reviews (surrogate data) using logistic regression. The left plot shows the test error when trained only on the original data, and the right plot shows the test error when trained only on the surrogate data. The black lines are the best fits of the scaling law, which has parameters \u03b2 = 0.27, R* = 0.101, and R(\u221e) = 0.148. ", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_16_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure visualizes the test error achieved when training neural networks on mixtures of real and surrogate data from IMDB and Rotten Tomatoes datasets. The test error is plotted as a function of the weight parameter \u03b1, which controls the contribution of surrogate data. Each subplot represents a different combination of the number of original (n) and surrogate (m) data points.  The black curves represent a scaling law prediction (Eq. 4 from the paper) which aims to model the relationship between the test error and these parameters.", "section": "1 Introduction and overview"}, {"figure_path": "NAcHv7vtL2/figures/figures_17_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure displays the test error achieved when training neural networks on mixtures of real and surrogate data from the IMDB and Rotten Tomatoes datasets.  The x-axis represents the weighting parameter (alpha) used to balance the contribution of real and surrogate data, with 0 representing only real data and 1 representing only surrogate data. The y-axis shows the classification loss (test error). Different plots show the effect of varying the number of original (n) and surrogate (m) samples. The black curves represent the prediction of a scaling law derived in the paper (Eq. (4)), demonstrating how well the law approximates the actual results. The figure highlights the potential benefits of using surrogate data to improve model performance.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_17_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error when training a neural network model on mixtures of original and surrogate data for sentiment analysis.  The x-axis represents the weight parameter (alpha) given to the surrogate data in the weighted ERM approach. The y-axis shows the test error. Different colored dots represent different combinations of the number of original data points (n) and surrogate data points (m). The black curves show predictions based on the scaling law derived in the paper (Equation 4). The figure demonstrates the impact of both the weight parameter and the amount of surrogate data on model performance.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_18_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error achieved by training neural networks on mixtures of real and surrogate data. The x-axis represents the weight given to the surrogate data (\u03b1), ranging from 0 (only real data) to 1 (only surrogate data). Each curve represents a different combination of the number of real (n) and surrogate (m) data points. The red dots indicate the observed test error, while the black curves show the predictions made by the scaling law (equation 4) derived in the paper.  The scaling law attempts to predict the test error as a function of \u03b1, n, and m.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_18_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure displays the test error results from training neural networks on mixtures of original and surrogate data.  The x-axis represents the weight assigned to the surrogate data (alpha), ranging from 0 (only original data) to 1 (only surrogate data).  The y-axis shows the classification loss (test error). Different plots represent varying numbers of original (n) and surrogate (m) data points. The black curves are predictions based on Equation (4) from the paper, which is a scaling law that relates test error to n, m, and alpha.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_19_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error achieved by training neural networks on mixtures of original and surrogate data, for various ratios of original to surrogate data (represented by the weighting parameter \u03b1).  The plot displays the results for different numbers of original (n) and surrogate (m) samples. The black curves represent the predictions of a scaling law (Equation 4 from the paper) which the authors propose to approximate the test error. The comparison of the actual test error (red dots) to the scaling law's prediction illustrates the accuracy of the proposed scaling law in predicting test error based on the amount of original and surrogate data used in training.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_19_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of models trained on mixtures of original and surrogate data for sentiment analysis using IMDB and Rotten Tomatoes reviews.  The x-axis represents the weight parameter \u03b1, which balances the contribution of original and surrogate data.  Different subplots show the results for varying numbers (n and m) of original and surrogate data points, respectively.  The red dots represent the empirical test error obtained through experiments.  The black curves show the prediction of a scaling law (Equation 4 from the paper) that approximates the relationship between the test error, the weight parameter \u03b1, and the number of data points.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_20_1.jpg", "caption": "Figure 12: IMDB and Rotten Tomatoes data and logistic regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots), together with scaling law fits (black lines). Best fit parameters are \u03b2 = 0.27, R* = 0.101 and Rex(\u221e) = 0.148.", "description": "This figure shows the test error for logistic regression models trained on IMDB movie review data (original data) and Rotten Tomatoes movie review data (surrogate data) separately.  The left plot shows the performance of models trained only on varying amounts of original data (IMDB), while the right plot shows performance on surrogate data (Rotten Tomatoes). The red dots represent the actual test error for different sample sizes. The black lines show the best fit to the data using a scaling law, with parameters \u03b2, R*, and Rex(\u221e) estimated to be 0.27, 0.101, and 0.148, respectively. This figure illustrates the behavior of the test error when only one data source is used in the training of the model.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_20_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error for sentiment analysis using neural networks trained on mixtures of original and surrogate data.  The x-axis represents the weight parameter (\u03b1) given to surrogate data in the weighted ERM approach, ranging from 0 (only original data) to 1 (only surrogate data).  Different panels show the results for varying amounts of original (n) and surrogate (m) data. The red dots represent the experimental results, and the black curves show the predictions from a scaling law (Equation 4 in the paper) that attempts to model the relationship between test error, \u03b1, n, and m.", "section": "1 Introduction and overview"}, {"figure_path": "NAcHv7vtL2/figures/figures_20_3.jpg", "caption": "Figure 2: Performance of unweighted vs weighted ERM approach for the setting in Figure 1", "description": "This figure compares the performance of unweighted and optimally weighted empirical risk minimization (ERM) methods.  The x-axis represents the number of surrogate samples (m), and the y-axis shows the test error. The plot demonstrates that the weighted ERM approach consistently outperforms the unweighted approach, highlighting the importance of optimal weighting when integrating surrogate data into training.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_21_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error when training a neural network model on mixtures of original and surrogate data for sentiment analysis using IMDB and Rotten Tomatoes datasets. The x-axis represents the weight parameter (\u03b1) of the surrogate data, and the y-axis represents the test error.  The red dots represent the actual test error obtained experimentally, while the black curves represent the predicted test error based on equation (4) from the paper. The figure demonstrates how the optimal weighting scheme significantly reduces test error compared to using only original or only surrogate data.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_21_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error when training a neural network on mixtures of original and surrogate data for sentiment analysis. The x-axis represents the weight given to the surrogate data (alpha), and the y-axis shows the classification loss.  Different panels show various sizes of the original (n) and surrogate (m) datasets. The black curves represent the predictions of a scaling law (equation 4 from the paper) which is a model for how the test error depends on the amount of original and surrogate data.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_22_1.jpg", "caption": "Figure 10: Gaussian mixture data and ridge regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best fits are shown in black. These gives the estimates \u03b2 = 0.60, R* = 0.49, and Rex(\u221e) = 0.03.", "description": "This figure shows the test error results when training a ridge regression model using only original data (left) and only surrogate data (right). The red dots represent the actual test errors obtained from experiments, while the black curves represent the best-fit curves based on the scaling law. This illustrates the test error behaviour of the model under different sample sizes and the data source.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_22_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error when training neural networks on mixtures of original and surrogate data.  It demonstrates that using a weighted combination of both datasets (optimal \u03b1) generally leads to lower error than using either dataset alone. The black curves represent predictions from a derived scaling law (Eq. 4) which approximates the relationship between test error, the amount of original and surrogate data, and the weighting scheme.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_23_1.jpg", "caption": "Figure 10: Gaussian mixture data and ridge regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best fits are shown in black. These gives the estimates \u03b2 = 0.60, R* = 0.49, and Rex(\u221e) = 0.03.", "description": "This figure shows the test error of ridge regression models trained only on original data (left) and surrogate data (right). The red dots represent the actual test error obtained from the experiments. The black lines represent the best fit curves. The parameters \u03b2, R*, and Rex(\u221e) are estimated from the best fit curves.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_23_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of models trained on mixtures of original and surrogate data for sentiment analysis.  The x-axis represents the weight parameter (\u03b1) of the surrogate data, ranging from 0 (only original data) to 1 (only surrogate data). The y-axis shows the classification loss (test error). The plots demonstrate the performance for different combinations of original (n) and surrogate (m) data points. The black curves represent the prediction of the scaling law described by Equation (4) in the paper, showing a good match with the experimental results (red dots). The figure highlights that optimally weighting the combined data sources leads to lower test error compared to using only original or surrogate data.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_24_1.jpg", "caption": "Figure 10: Gaussian mixture data and ridge regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best fits are shown in black. These gives the estimates \u03b2 = 0.60, R* = 0.49, and Rex(\u221e) = 0.03.", "description": "This figure displays the test error for ridge regression on Gaussian mixture data when trained using only original data (left) and only surrogate data (right). Red dots show the actual test error for different sample sizes. The black lines represent the best fit curves obtained, providing estimates for the scaling exponent (\u03b2), minimal error (R*), and excess test error with infinite surrogate data (Rex(\u221e)). These parameters offer insights into the behavior of the model's performance.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_24_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of sentiment analysis models trained on mixtures of original and surrogate data for various ratios (\u03b1) of surrogate data.  The x-axis represents the weight (\u03b1) given to the surrogate data, ranging from 0 (only original data) to 1 (only surrogate data). The y-axis represents the classification loss.  Each subplot shows results for different sizes of original (n) and surrogate (m) datasets. The black curves represent the predictions from a scaling law presented in the paper (Equation 4), illustrating how well the scaling law can capture the relationship between test error, data size, and the mixture ratio.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_25_1.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of sentiment analysis models trained on mixtures of original and surrogate data.  The x-axis represents the weight parameter (\u03b1) given to the surrogate data in a weighted empirical risk minimization (ERM) approach. The different curves correspond to different numbers of original (n) and surrogate (m) data points. The red circles represent the experimental results, while the black curves are predictions based on a scaling law (Eq. 4) described in the paper. The plot demonstrates that combining original and surrogate data, with optimal weighting, reduces test error.", "section": "1.1 Motivation and formulation"}, {"figure_path": "NAcHv7vtL2/figures/figures_25_2.jpg", "caption": "Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).", "description": "This figure shows the test error of models trained on mixtures of original and surrogate data for different combinations of the number of original data points (n) and surrogate data points (m).  The x-axis represents the weight (\u03b1) given to the surrogate data in the weighted empirical risk minimization (ERM) approach. The red dots represent the actual test error obtained from the experiments, while the black curves show the test error predicted by the scaling law (Equation 4) presented in the paper.  The figure demonstrates that incorporating surrogate data, even with optimal weighting, can significantly reduce the test error.", "section": "Empirical Results"}, {"figure_path": "NAcHv7vtL2/figures/figures_26_1.jpg", "caption": "Figure 10: Gaussian mixture data and ridge regression. Test error when trained on original (left plot) and surrogate (right plot) data only (red dots). Best fits are shown in black. These gives the estimates \u03b2 = 0.60, R* = 0.49, and Rex(\u221e) = 0.03.", "description": "This figure shows the test error when training only on original data (left) and only on surrogate data (right). The red dots represent the actual test errors, while the black lines represent the best fit using the scaling law.  The parameters of the best fit are also provided. This helps to illustrate the behavior of the test error for different amounts of original and surrogate data, highlighting the impact of the surrogate data on the test error.", "section": "4 Empirical results"}, {"figure_path": "NAcHv7vtL2/figures/figures_26_2.jpg", "caption": "Figure 5: Ridge regression on simulated data. Here d = 500, n = 1000, \u03c3\u00b2 = \u03c3\u00b2 = 1, ||0*|| = ||0*,s|| = 1, regul. par. \u03bb = 2-10, and m varies by column. Top row \u03b3 = \u03c0/2, bottom row \u03b3 = \u03c0/6.", "description": "This figure shows the results of ridge regression experiments on simulated data. The experiments vary the number of surrogate samples (m) and the angle \u03b3 between the original and surrogate parameters. The plot shows the excess squared loss as a function of the weight parameter \u03b1.  The top row shows results where the original and surrogate data are orthogonal (\u03b3 = \u03c0/2), while the bottom row shows results where they are closer (\u03b3 = \u03c0/6). The figure demonstrates how the optimal weight parameter \u03b1 and test error changes as a function of the number of samples.", "section": "4 Empirical results"}]