[{"type": "text", "text": "Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yibo Jiang1, Goutham Rajendran2, Pradeep Ravikumar2, and Bryon Aragam3 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science, University of Chicago 2Machine Learning Department, Carnegie Mellon University 3Booth School of Business, University of Chicago ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) have the capacity to store and recall facts. Through experimentation with open-source models, we observe that this ability to retrieve facts can be easily manipulated by changing contexts, even without altering their factual meanings. These findings highlight that LLMs might behave like an associative memory model where certain tokens in the contexts serve as clues to retrieving facts. We mathematically explore this property by studying how transformers, the building blocks of LLMs, can complete such memory tasks. We study a simple latent concept association problem with a one-layer transformer and we show theoretically and empirically that the transformer gathers information using self-attention and uses the value matrix for associative memory. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "What is the first thing that would come to mind if you were asked not to think of an elephant? Chances are, you would be thinking about elephants. What if we ask the same thing to Large Language Models (LLMs)? Obviously, one would expect the outputs of LLMs to be heavily influenced by tokens in the context $[\\mathrm{Bro}{+}20]$ . Could such influence potentially prime LLMs into changing outputs in a nontrivial way? To gain a deeper understanding, we focus on one specific task called fact retrieval $[\\mathrm{Men}{+}22$ ; Men $+23$ ] where expected output answers are given. LLMs, which are trained on vast amounts of data, are known to have the capability to store and recall facts [Men $^{1+22}$ ; Men $^{+23}$ ; DCAT21; $\\mathrm{Mit}{+}21$ ; Mit $+22$ ; Da $+21$ ]. This ability raises natural questions: How robust is fact retrieval, and to what extent does it depend on semantic meanings within contexts? What does it reveal about memory in LLMs? ", "page_idx": 0}, {"type": "text", "text": "In this paper, we first demonstrate that fact retrieval is not robust and LLMs can be easily fooled by varying contexts. For example, when asked to complete \u201cThe Eiffel Tower is in the city of\u201d, GPT-2 [Rad+19] answers with \u201cParis\u201d. However, when prompted with \u201cThe Eiffel Tower is not in Chicago. The Eiffel Tower is in the city of\u201d, GPT-2 responds with \u201cChicago\u201d. See Figure 1 for more examples, including Gemma and LLaMA. On the other hand, humans do not find the two sentences factually confusing and would answer \u201cParis\u201d in both cases. We call this phenomenon context hijacking. Importantly, these findings suggest that LLMs might behave like an associative memory model. Specifically, we refer to an associative memory model in which LLMs rely on certain tokens in contexts to guide the retrieval of memories, even if such associations formed are not inherently semantically meaningful. This contrasts with the ideal behavior, where LLMs would generalize by understanding new contexts, reasoning through them, and integrating prior knowledge. ", "page_idx": 0}, {"type": "text", "text": "This associative memory perspective raises further interpretability questions about how LLMs form such associations. Answering these questions can facilitate the development of more robust LLMs. ", "page_idx": 0}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/6f53f7b2b8666381d99eb7edc592c9334188df936f288c337c3178572bedfda1.jpg", "img_caption": ["Figure 1: Examples of context hijacking for various LLMs, showcasing that fact retrieval is not robust. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Unlike classical models of associative memory in which distance between memory patterns are measured directly and the associations between inputs and outputs are well-specified, fact retrieval relies on a more nuanced notion of similarity measured by latent (unobserved) semantic concepts. To model this, we propose a synthetic task called latent concept association where the output token is closely related to sampled tokens in the context but wherein similarity is measured via a latent space of semantic concepts. We then investigate how a one-layer transformer $[\\mathrm{Vas}{+17}]$ , a fundamental component of LLMs, can tackle this memory retrieval task in which various context distributions correspond to distinct memory patterns. We demonstrate that the transformer accomplishes the task in two stages: The self-attention layer gathers information, while the value matrix functions as associative memory. Moreover, low-rank structure also emerges in the embedding space of trained transformers. These findings provide additional theoretical validation for numerous existing low-rank editing and fine-tuning techniques [Men $+22$ ; $\\mathrm{Hu}+21]$ ]. ", "page_idx": 1}, {"type": "text", "text": "Contributions Specifically, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We systematically demonstrate context hijacking for various open source LLM models including GPT-2 [Rad $+19]$ , LLaMA-2 $[\\mathrm{Tou}+23]$ and Gemma [Tea+24], which show that fact retrieval can be misled by contexts (Section 3), reaffirming that LLMs lack robustness to context changes $[\\mathrm{Shi}+23$ ; $\\mathrm{Pet}+20$ ; CSH22; Yor $+23$ ; PE21].   \n2. We propose a synthetic memory retrieval task termed latent concept association, allowing us to analyze how transformers can accomplish memory recall (Section 4). Unlike classical models of associative memory, our task creates associations in a latent, semantic concept space as opposed to directly between observed tokens. This perspective is crucial to understanding how transformers can solve fact retrieval problems by implementing associative memory based on similarity in the latent space.   \n3. We theoretically (Section 5) and empirically (Section 6) study trained transformers on this latent concept association problem, showing that self-attention is used to aggregate information while the value matrix serves as associative memory. And moreover, we discover that the embedding space can exhibit a low-rank structure, offering additional support for existing editing and fine-tuning methods [Men $^{1+22}$ ; $\\mathrm{Hu}+21]$ . ", "page_idx": 1}, {"type": "text", "text": "2 Literature review ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Associative memory Associative memory has been explored within the field of neuroscience [Hop82; Seu96; BYBOS95; ${\\mathrm{Ska+94}}$ ; SS22]. The most popular models among them is the Hopfield network [Hop82] and its modern successors $[\\mathrm{Ram}{+}20$ ; Mil $+22$ ; Zha23; $\\mathrm{Hu}{+}24\\mathrm{d}$ ; $\\mathrm{Wu}{+}23$ ; $\\mathrm{Hu}{+}24\\mathrm{b}$ ; $\\mathrm{Hu}{+}24\\mathrm{c}$ ; $\\mathrm{Wu}{+}24\\mathrm{a}$ ; $\\mathrm{Hu}{+}24\\mathrm{a}]$ are closely related to the attention layer used in transformers $[\\mathrm{Vas}{+17}]$ . In addition, the attention mechanism has also been shown to approximate another associative memory model known as sparse distributed memory [BP21]. Beyond attention, Radhakrishnan et al. [RBU20] and Jiang and Pehlevan [JP20] show that overparameterzed autoencoders can implement associative memory as well. This paper studies fact retrieval as a form of associative memory. Another closely related area of research focuses on memorization in deep neural networks. Henighan et al. [Hen $+23$ ] shows that a simple neural network trained on toy model will store data points in the overftiting regime while storing features in the underfitting regime. Feldman [Fel20] and Feldman and Zhang [FZ20] study the interplay between memorization and long tail distributions while Kim et al. [KKM22] and Mahdavi et al. [MLT23] study the memorization capacity of transformers. ", "page_idx": 1}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/d971d74a79bd032e11583a3c7245a6fed97a2cc9d2fcb751c203e2855d9d1cd5.jpg", "img_caption": ["Figure 2: Context hijacking can cause LLMs to output false target. The figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset under two hijacking schemes. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Interpreting transformers and LLMs There\u2019s a growing body of work on understanding how transformers and LLMs work [LLR23; AZL23a; AZL23b; AZL24; $\\mathrm{EI}{+}24$ ; Tar $^{+236}$ ; Tar $+23\\mathrm{a}$ ; $\\mathrm{Li}{+}24]$ , including training dynamics $[\\mathrm{Tia}+23\\mathrm{a}$ ; Tia $+236$ ; She $+24$ ] and in-context learning [Xie+21; Gar $+22$ ; Ba $\\pm24$ ; Bai+24]. Recent papers have introduced synthetic tasks to better understand the mechanisms of transformers [Cha22; Liu $+22$ ; Nan $+23$ ; Zha $+22$ ; Zho $+24$ ], such as those focused on Markov chains [Bie $+24$ ; Ede $+24$ ; NDL24; Mak $+24$ ]. Most notably, Bietti et al. [Bie+24] and subsequent works [CDB23; CSB24] study weights in transformers as associative memory but their focus is on understanding induction head [Ols+22b] and one-to-one map between input query and output memory. An increasing amount of research is dedicated to understanding the internals of pretrained LLMs, broadly categorized under the term \u201cmechanistic interpretability\u201d [Elh $+21$ ; $\\mathrm{Ols}{+}22\\mathrm{a}$ ; Gev $+23$ ; Men $+22$ ; Men $^{+23}$ ; Jia+24; Raj $+24$ ; Has $+24$ ; Wan $+22$ ; $\\mathrm{McG}{+}23$ ; Gei $+21$ ; Gei $+22$ ; Ge $+24$ ; $\\mathrm{Wu}{+}24\\mathrm{b}]$ . ", "page_idx": 2}, {"type": "text", "text": "Knowledge editing and adversarial attacks on LLMs Fact recall and knowledge editing have been extensively studied [Men $+22$ ; Men $^{+23}$ ; Has $+24$ ; Sak $^{+23}$ ; DCAT21; $\\mathrm{Mit}{+}21$ ; Mit+22; Da $+21$ ; $Z\\mathrm{ha}{+}23$ ; $\\mathrm{Tia}{+24}$ ; $\\mathrm{Jin}{+}23$ ], including the use of in-context learning to edit facts $[Z\\mathrm{he}{+23}]$ . This paper aims to explore a different aspect by examining the robustness of fact recall to variation in prompts. A closely related line of work focuses on adversarial attacks on LLMs [see Cho $+24$ , for a review]. Specifically, prompt-based adversarial attacks $[X\\mathrm{u}+23$ ; Zhu $^{+23}$ ; Wan+23b] focus on the manipulation of answers within specific classification tasks while other works concentrate on safety issues [Liu $+23\\mathrm{a}$ ; PR22; Zou $+23$ ; Apr $+22$ ; $\\mathrm{Wan}{+}23\\mathrm{a}$ ; $\\mathrm{Si}{+}22$ ; Rao $_{1+23}$ ; SMR23; Liu $1+236$ ]. Yu et al. $[\\mathrm{Yu}+24]$ and Luo et al. [Luo $+24]$ also study jailbreak phenomena within the context of modern Hopfield network. There are also works showing LLMs can be distracted by irrelevant contexts in problem solving $[\\mathrm{Shi}+23]$ , question answering $[\\mathrm{Pet}+20$ ; CSH22; $\\mathrm{Yor}{+23}$ ] and factual reasoning [PE21]. Although phenomena akin to context hijacking have been reported in different instances, the goals of this paper are to give a systematic robustness study for fact retrieval, offer a framework for interpreting it in the context of associative memory, and deepen our understanding of LLMs. ", "page_idx": 2}, {"type": "text", "text": "3 Context hijacking in LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we run experiments on LLMs including GPT-2 [Rad+19], Gemma $[\\mathrm{Tea}+24]$ (both base and instruct models) and LLaMA-2-7B [Tou+23] to explore the effects of context hijacking on manipulating LLM outputs. As an example, consider Figure 1. When we prompt the LLMs with the context \u201cThe Eiffel Tower is in the city of\u201d, all 4 LLMs output the correct answer (\u201cParis\u201d). However, as we see in the example, we can actually manipulate the output of the LLMs simply by modifying the context with additional factual information that would not confuse a human. We call this context-hijacking. Due to the different capacities and capabilties of each model, the examples in Figure 1 use different hijacking techniques. This is most notable on LLaMA-2-7B, which is a much larger model than the others. Of course, as expected, the more sophisticated attack on LLaMA also works on GPT-2 and Gemma. Additionally, the instruction-tuned version of Gemma can understand special words like \u201cnot\u201d to some extent. Nevertheless, it is still possible to systematically hijack these LLMs, as demonstrated below. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We explore this phenomenon at scale with the COUNTERFACT dataset introduced in $[\\mathrm{Men}{+}22]$ , a dataset of difficult counterfactual assertions containing a diverse set of subjects, relations, and linguistic variations. COUNTERFACT has 21, 919 samples, each of which are given by a tuple $(p,o_{*},o_{-},s,r)$ . From each sample, we have a context prompt $p$ with a true target answer $o_{*}$ (target_true) and a false target answer $o_{-}$ (target_false), e.g. the prompt $p=$ \u201cEiffel Tower can be found in\u201d has true target $o_{*}=$ \u201cParis\u201d and false target $o_{_-}=\\mathrm{{}^{\\leftarrow}G u a m^{\\ast}}$ . Additionally, the main entity in $p$ is the subject $s$ ${\\boldsymbol{s}}=$ \u201cEiffel Tower\u201d) and the prompt is categorized into relations $r$ (for instance, other samples with the same relation ID as the example above could be of the form \u201cThe location of {subject} is\u201d, \u201c{subject} can be found in\u201d, \u201cWhere is {subject}? It is in\u201d). For additional details on how the dataset was collected, see [Men $+22$ ]. ", "page_idx": 3}, {"type": "text", "text": "For a hijacking scheme, we report the Efficacy Score (ES) [Men $+22]$ , which is the proportion of samples for which the token probabilities satisfy ${P r}[o_{\\bot}]\\;>\\;{P r}[o_{*}]$ after modifying the context, that is, the proportion of the dataset that has been successfully manipulated. We experiment with two hijacking schemes for this dataset. We first hijack by prepending the text \u201cDo not think of {target_false}\u201d to each context. For instance, the prompt \u201cThe Eiffel Tower is in\u201d gets changed to \u201cDo not think of Guam. The Eiffel Tower is in\u201d. In Figure 2a, we see that the efficacy score rises significantly after hijacking. Here, we prepend the hijacking sentence $k$ times for $k=0,\\ldots,5$ where $k=0$ yields the original prompt. We see that additional prepends increase the score further. ", "page_idx": 3}, {"type": "text", "text": "In the second scheme, we make use of the relation $\\textstyle{\\mathrm{ID}}\\,r$ to prepend factually correct sentences. For instance, one can hijack the example above to \u201cThe Eiffel Tower is not located in Guam. The Eiffel Tower is in\u201d. We test this hijacking philosophy on different relation IDs. In particular, Figure 2b reports hijacking based on relation ID $P190$ (\u201ctwin city\u201d). And we see similar patterns that with more prepends, the ES score gets higher. It is also worth noting that one can even hijack by only including words that are semantically close to the false target (e.g., \u201cFrance\u201d for false target \u201cFrench\u201d). This suggests that context hijacking is more than simply the LLM copying tokens from contexts. Additional details and experiments for both hijacking schemes and for other relation IDs are in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "These experiments show that context hijacking changes the behavior of LLMs, leading them to output incorrect tokens, without altering the factual meaning of the context. It is worth noting that similar fragile behaviors of LLMs have been observed in the literature in different contexts $[\\mathrm{Shi}+23$ ; Pet+20; CSH22; Yor $^{+23}$ ; PE21]. See Section 2 for more details. ", "page_idx": 3}, {"type": "text", "text": "Context hijacking indicates that fact retrieval in LLMs is not robust and that accurate fact recall does not necessarily depend on the semantics of the context. As a result, one hypothesis is to view LLMs as an associative memory model where special tokens in contexts, associated with the fact, provide partial information or clues to facilitate memory retrieval [Zha23]. To better understand this perspective, we design a synthetic memory retrieval task to evaluate how the building blocks of LLMs, transformers, can solve it. ", "page_idx": 3}, {"type": "text", "text": "4 Problem setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the context of LLMs, fact or memory retrieval, can be modeled as a next token prediction problem. Given a context (e.g., \u201cThe capital of France is\u201d), the objective is to accurately predict the next token (e.g., \u201cParis\u201d) based on the factual relation between context and the following token. ", "page_idx": 3}, {"type": "text", "text": "Previous papers $[\\mathrm{Ram}{+}20$ ; $\\mathrm{Mil}+22$ ; BP21; Zha23] have studied the connection between attention and autoassociative and heteroassociative memory. For autoassociative memory, contexts are modeled as a set of existing memories and the goal of self-attention is to select the closest one or approximations to it. On top of this, heteroassociative memory [Mil $+22$ ; BP21] has an additional projection to remap each output to a different one, whether within the same space or otherwise. In both scenarios, the goal is to locate the closest pattern within the context when provided with a query (up to a remapping if it\u2019s heteroassociative). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Fact retrieval, on the other hand, does not strictly follow this framework. The crux of the issue is that the output token is not necessarily close to any particular token in the context but rather a combination of them and the \u201ccloseness\u201d is intuitively measured by latent semantic concepts. For example, consider context sentence \u201cThe capital of France is\u201d with the output \u201cParis\u201d. Here, none of the tokens in the context directly corresponds to the word \u201cParis\u201d. Yet some tokens contain partial information about \u201cParis\u201d. Intuitively, \u201ccapital\u201d aligns with the \u201cisCapital\u201d concept of \u201cParis\u201d, while \u201cFrance\u201d corresponds to the \u201cisFrench\u201d concept linked to \u201cParis\u201d where all the concepts are latent. To model such phenomenon, we propose a synthetic task called latent concept association where the output token is closely related to tokens in the context and similarity is measured via the latent space. ", "page_idx": 4}, {"type": "text", "text": "4.1 Latent concept association ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose a synthetic prediction task where for each output token $y$ , tokens in the context (denoted by $x$ ) are sampled from a conditional distribution given $y$ . Tokens that are similar to $y$ will be favored to appear more in the context, except for $y$ itself. The task of latent concept association is to successfully retrieve the token $y$ given samples from $p(x|y)$ . The synthetic setup simplifies by not accounting for the sequential nature of language, a choice supported by previous experiments on context hijacking (Section 3). We formalize this task below. ", "page_idx": 4}, {"type": "text", "text": "To measure similarity, we define a latent space. Here, the latent space is a collection of $m$ binary latent variables $Z_{i}$ . These could be viewed as semantic concept variables. Let $Z=(Z_{1},...,Z_{m})$ be the corresponding random vector, $z$ be its realization, and $\\mathcal{Z}$ be the collection of all latent binary vectors. For each latent vector $z$ , there\u2019s one associated token $t\\in[V]=\\{0,...,V-1\\}$ where $V$ is the total number of tokens. Here we represent the tokenizer as $\\iota$ where $\\iota(z)=t$ . In this paper, we assume that $\\iota$ is the standard tokenizer where each binary vector is mapped to its decimal number. In other words, there\u2019s a one to one map between latent vectors and tokens. Because the map is one to one, we sometimes use latent vectors and tokens interchangeably. We also assume that every latent binary vector has a unique corresponding token, therefore $V=2^{m}$ . ", "page_idx": 4}, {"type": "text", "text": "Under the latent concept association model, the goal is to retrieve specific output tokens given partial information in the contexts. This is modeled by the latent conditional distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\np(z|z^{*})=\\omega\\pi(z|z^{*})+(1-\\omega)\\mathrm{Unif}(\\mathcal{Z})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi(z|z^{*})\\propto\\left\\{\\exp(-D_{H}(z,z^{*})/\\beta)\\right.\\ \\ \\left.z\\in{\\cal N}(z^{*}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $D_{H}$ is the Hamming distance, $\\mathcal{N}(z^{*})$ is a subset of $\\mathcal{Z}\\backslash\\{z^{*}\\}$ and $\\beta>0$ is the temperature parameter. The use of Hamming distance draws a parallel with the notion of distributional semantics in natural language: \u201ca word is characterized by the company it keeps\u201d [Fir57]. In words, $p(z|z^{*})$ says that with probability $1-\\omega$ , the conditional distribution uniformly generate random latent vectors and with probability $\\omega$ , the latent vector is generated from the informative conditional distribution $\\pi(\\boldsymbol{z}|\\boldsymbol{z}^{*})$ where the support of the conditional distribution is $\\mathcal{N}(z^{*})$ . Here, $\\pi$ represents the informative conditional distribution that depends on $z^{*}$ whereas the uniform distribution is uninformative and can be considered as noise. The mixture model parameter $\\omega$ determines the signal to noise ratio of the contexts. ", "page_idx": 4}, {"type": "text", "text": "Therefore, for any latent vector $z^{*}$ and its associated token, one can generate $L$ context token words with the aforementioned latent conditional distribution: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Uniformly sample a latent vector $z^{*}$ \u2022 For $l=1,...,L-1$ , sample $z_{l}\\sim p(z|z^{*})$ and $t_{l}=\\iota(z_{l})$ . \u2022 For $l=L$ , sample $z\\sim\\pi(z|z^{*})$ and $t_{L}=\\iota(z)$ . ", "page_idx": 4}, {"type": "text", "text": "Consequently, we have $\\boldsymbol{x}=(t_{1},..,t_{L})$ and $y\\,=\\,\\iota\\!\\left(z^{*}\\right)$ . The last token in the context is generated specifically to make sure that it is not from the uniform distribution. This ensures that the last token can use attention to look for clues, relevant to the output, in the context. Let $\\mathcal{D}^{L}$ be the sampling distribution to generate $(x,y)$ pairs. The conditional probability of $y$ given $x$ is given by $\\bar{p(y|x)}$ . ", "page_idx": 4}, {"type": "text", "text": "With slight abuse of notation, given a token $t\\in[V]$ , we define $\\mathcal{N}(t)=\\mathcal{N}(\\iota^{-1}(t))$ . we also define $D_{H}(t,t^{\\flat})=D_{H}(\\iota^{-1}(t),\\iota^{-1}(\\check{t^{\\prime}}))$ for any pair of tokens $t$ and $t^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "For any function $f$ that maps the context to estimated logits of output labels, the training objective is to minimize this loss of the last position: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,y)\\in\\mathcal{D}^{L}}[\\ell(f(x),y)]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell$ is the cross entropy loss with softmax. The error rate of latent concept association is defined by the following: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}^{L}}(f)=\\mathbb{P}_{(x,y)\\sim\\mathcal{D}^{L}}[\\mathrm{argmax}\\,f(x)\\neq y]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "And the accuracy is $1-R_{\\mathcal{D}^{L}}(f)$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Transformer network architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a context $\\boldsymbol{x}=(t_{1},..,t_{L})$ which consists of $L$ tokens, we define $X\\,\\in\\,\\{0,1\\}^{V\\times L}$ to be its one-hot encoding where $V$ is the vocabulary size. Here we use $\\chi$ to represent the one-hot encoding function (i.e., $\\chi(x)\\,=\\,X)$ ). Similar to [LLR23; Tar $+23\\mathrm{a}$ ; $\\mathrm{Li}{+}24]$ , we also consider a simplified one-layer transformer model without residual connections and normalization: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf^{L}(x)=\\left[{W_{E}}^{T}{W_{V}}\\mathrm{attn}(W_{E}\\chi(x))\\right]_{:L}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{attn}(U)=U\\sigma\\Big(\\frac{(W_{K}U)^{T}(W_{Q}U)}{\\sqrt{d_{a}}}\\Big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$W_{K}\\in\\mathbb{R}^{d_{a}\\times d}$ is the key matrix, and $W_{Q}\\in\\mathbb{R}^{d_{a}\\times d}$ is the query matrix and $d_{a}$ is the attention head size. $\\sigma:\\mathbb{R}^{L\\times L}\\,\\rightarrow\\,(0,1)^{L\\times L}$ is the column-wise softmax operation. $W_{V}\\,\\in\\,\\mathbb{R}^{d\\times d}$ is the value matrix and $W_{E}\\in\\mathbb{R}^{d\\times V}$ is the embedding matrix. Here, we adopt the weight tie-in implementation which is used for Gemma $[\\mathrm{Tea}+24]$ . We focus solely on the prediction of the last position, as it is the only one relevant for latent concept association. For convenience, we also use $h(x)$ to mean $\\left[\\arctan(W_{E}\\chi(x))\\right]_{:L}$ , which is the hidden representation after attention for the last position, and $f_{t}^{L}(x)$ to represent the logit for output token $t$ . ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we theoretically investigate how a single-layer transformer can solve the latent concept association problem. We first introduce a hypothetical associative memory model that utilizes self-attention for information aggregation and employs the value matrix for memory retrieval. This hypothetical model turns out to mirror trained transformers in experiments. We also examine the role of each individual component of the network: the value matrix, embeddings, and the attention mechanism. We validate our theoretical claims in Section 6. ", "page_idx": 5}, {"type": "text", "text": "5.1 Hypothetical associative memory model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that a simple single-layer transformer network can solve the latent concept association problem. The formal result is presented below in Theorem 1; first we require a few more definitions. Let $W_{E}(t)$ be the $t$ -th column of the embedding matrix $W_{E}$ . In other words, this is the embedding for token $t$ . Given a token $t$ , define $\\mathcal{N}_{1}(t)$ to be the subset of tokens whose latent vectors are only 1 Hamming distance away from $t$ \u2019s latent vector: $\\mathcal{N}_{1}(t)=\\{t^{\\prime}:D_{H}(t^{\\prime},t))=1\\}\\cap\\mathcal{N}(t)$ . For any output token $t$ , $\\mathcal{N}_{1}(t)$ contains tokens with the highest probabilities to appear in the context. ", "page_idx": 5}, {"type": "text", "text": "The following theorem formalizes the intuition that a one-layer transformer that uses self-attention to summarize statistics about the context distributions and whose value matrix uses aggregated representations to retrieve output tokens can solve the latent concept association problem defined in Section 4.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (informal). Suppose the data generating process follows Section 4.1 where $m\\,\\geq\\,3$ , $\\omega=1$ , and $\\mathcal{N}(t)=V\\setminus\\{t\\}$ . Then for any $\\varepsilon>0$ , there exists a transformer model given by (4.1) that achieves error $\\varepsilon$ , i.e. $\\dot{R_{D^{L}}}(f^{L})<\\varepsilon$ given sufficiently large context length $L$ . ", "page_idx": 5}, {"type": "text", "text": "More precisely, for the transformer in Theorem 1, we will have $W_{K}=0$ and $W_{Q}=0$ . Each row of $W_{E}$ is orthogonal to each other and normalized. And $W_{V}$ is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{V}=\\sum_{t\\in[V]}W_{E}(t)(\\sum_{t^{\\prime}\\in\\mathcal{N}_{1}(t)}W_{E}(t^{\\prime})^{T})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A more formal statement of the theorem and its proof is given in Appendix B (Theorem 7). ", "page_idx": 6}, {"type": "text", "text": "Intuitively, Theorem 1 suggests having more samples from $p(x|y)$ can lead to a better recall rate. On the other hand, if contexts are modified to contain more samples from $p(x|\\tilde{y})$ where ${\\tilde{y}}\\neq y$ , then it is likely for transformer to output the wrong token. This is similar to context hijacking (see Section 5.5). The construction of the value matrix is similar to the associative memory model used in $[\\mathrm{Bie}{+}24$ ; CSB24], but in our case, there is no explicit one-to-one input and output pairs stored as memories. Rather, a combination of inputs are mapped to a single output. ", "page_idx": 6}, {"type": "text", "text": "While the construction in Theorem 1 is just one way that a single-layer transformer can tackle this task, it turns out empirically this construction of $W_{V}$ is close to the trained $W_{V}$ , even in the noisy case $(\\omega\\neq1)$ ). In Section 6.1, we will demonstrate that substituting trained value matrices with constructed ones can retain accuracy, and the constructed and trained value matrices even share close low-rank approximations. Moreover, in this hypothetical model, a simple uniform attention mechanism is deployed to allow self-attention to count occurrences of each individual tokens. Since the embeddings are orthonormal vectors, there is no interference. Hence, the self-attention layer can be viewed as aggregating information of contexts. It is worth noting that, in different settings, more sophisticated embedding structures and attention patterns are needed. This is discussed in the following sections. ", "page_idx": 6}, {"type": "text", "text": "5.2 On the role of the value matrix ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The construction in Theorem 1 relies on the value matrix acting as associative memory. But is it necessary? Could we integrate the functionality of the value matrix into the self-attention module to solve the latent concept association problem? Empirically, the answer seems to be negative as will be shown in Section 6.1. In particular, when the context length is small, setting the value matrix to be the identity would lead to subpar memory recall accuracy. ", "page_idx": 6}, {"type": "text", "text": "This is because if the value matrix is the identity, the transformer would be more susceptible to the noise in the context. To see this, notice that given any pair of context and output token $(x,y)$ , the latent representation after self-attention $h(x)$ must live in the polyhedron $S_{y}$ to be classified correctly where $S_{y}$ is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{y}=\\{v:(W_{E}(y)-W_{E}(t))^{T}v>0\\mathrm{~where~}t\\not\\in[V]\\setminus\\{y\\}\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that, by definition, for any two tokens $y$ and $\\tilde{y}$ $\\mathsf{\\tilde{\\Pi}},S_{y}\\cap S_{\\tilde{y}}=\\emptyset$ . On the other hand, because of the self-attention mechanism, $h(x)$ must also live in the convex hull of all the embedding vectors: ", "page_idx": 6}, {"type": "equation", "text": "$$\nC V=\\operatorname{Conv}(W^{E}(0),...,W^{E}(|V|-1))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In other words, for any pair $(x,y)$ to be classified correctly, $h(x)$ must live in the intersection of $S_{y}$ and $C V$ . Due to the stochastic nature of $x$ , it is likely for $h(x)$ to be outside of this intersection. The remapping effect of the value matrix can help with this problem. The following lemma explains this intuition. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2. Suppose the data generating process follows Section 4.1 where $m\\geq3,$ , $\\omega\\,=\\,1$ and $\\mathcal{N}(t)=\\{t^{\\prime}:\\bar{D_{H}}(t,t^{\\prime}))=1\\}$ . For any single layer transformer given by (4.1) where each row of $W_{E}$ is orthogonal to each other and normalized, $i f W_{V}$ is constructed as in (5.1), then the error rate is 0. If $W_{V}$ is the identity matrix, then the error rate is strictly larger than 0. ", "page_idx": 6}, {"type": "text", "text": "Another intriguing phenomenon occurs when the value matrix is the identity matrix. In this case, the inner product between embeddings and their corresponding Hamming distance varies linearly. This relationship can be formalized by the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Suppose the data generating process follows Section 4.1 where $m\\geq3$ , $\\omega=1$ and $\\mathcal{N}(t)=V\\setminus\\{t\\}$ . For any single layer transformer given by (4.1) with $W_{V}$ being the identity matrix, if the cross entropy loss is minimized so that for any sampled pair $(x,y)$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\np(y|x)=\\hat{p}(y|x)=s o f t m a x(f_{y}^{L}(x))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "there exists $a>0$ and $b$ such that for two tokens $t\\neq t^{\\prime}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle W_{E}(t),W_{E}(t^{\\prime})\\rangle=-a D_{H}(t,t^{\\prime})+b\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5.3 Embedding training and geometry ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The hypothetical model in Section 5.1 requires embeddings to form an orthonormal basis. In the overparameterization regime where the embedding dimension $d$ is larger than the number of tokens $V$ , this can be approximately achieved by Gaussian initialization. However, in practice, the embedding dimension is typically smaller than the vocabulary size, in which case it is impossible for the embeddings to constitute such a basis. Empirically, in Section 6.2, we observe that with overparameterization $(d>V)$ , embeddings can be frozen at their Gaussian initialization, whereas in the underparameterized regime, embedding training is required to achieve better recall accuracy. ", "page_idx": 7}, {"type": "text", "text": "This raises the question: What kind of embedding geometry is learned in the underparameterized regime? Experiments reveal a close relationship between the inner product of embeddings for two tokens and the Hamming distance of these tokens (see Figure 3b and Figure D.5 in Appendix D.2). Approximately, we have the following relationship: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\langle W_{E}(t),W_{E}(t^{\\prime})\\rangle=\\left\\{\\!\\!\\begin{array}{l l}{b_{0}}&{\\ t=t^{\\prime}}\\\\ {-a D_{H}(t,t^{\\prime})+b}&{\\ t\\neq t^{\\prime}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for any two tokens $t$ and $t^{\\prime}$ where $b_{0}>b$ and $a\\,>\\,0$ . One can view this as a combination of the embedding geometry under Gaussian initialization and the geometry when $W_{V}$ is the identity matrix (Theorem 3). Importantly, this structure demonstrates that trained embeddings inherently capture similarity within the latent space. Theoretically, this embedding structure (5.2) can also lead to low error rate under specific conditions on $b_{0},b$ and $a$ , which is articulated by the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Informal). Following the same setup as in Theorem $^{l}$ , but embeddings obey (5.2), then under certain conditions on $a,b$ and if $b_{0}$ and context length $L$ are sufficiently large, the error rate can be arbitrarily small, i.e. $R_{\\mathcal{D}^{L}}(f^{L})<\\varepsilon$ for any $0<\\varepsilon<1$ . ", "page_idx": 7}, {"type": "text", "text": "The formal statement of the theorem and its proof is given in Appendix B (Theorem 8). ", "page_idx": 7}, {"type": "text", "text": "Notably, this embedding geometry also implies a low-rank structure. Let\u2019s first consider the special case when $b_{0}=b$ . In other words, the inner product between embeddings and their corresponding Hamming distance varies linearly. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5. If embeddings follow (5.2) and $b=b_{0}$ and $\\mathcal{N}(t)=V\\setminus\\{t\\}$ , then rank $:\\!(W_{E})\\leq m+2.$ . ", "page_idx": 7}, {"type": "text", "text": "When $b_{0}~>~b$ , the embedding matrix will not be strictly low rank. However, it can still exhibit approximate low-rank behavior, characterized by an eigengap between the top and bottom singular values. This is verified empirically (see Figure D.9-D.12 in Appendix D.4). ", "page_idx": 7}, {"type": "text", "text": "5.4 The role of attention selection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As of now, attention does not play a significant role in the analysis. But perhaps unsurprisingly, the attention mechanism is useful in selecting relevant information. To see this, let\u2019s consider a specific setting where for any latent vector $z^{*}$ , $\\bar{\\mathcal{N}(}z^{*})=\\{z:z_{1}^{*}=z_{1}\\}\\setminus\\{z^{*}\\}$ . ", "page_idx": 7}, {"type": "text", "text": "Essentially, latent vectors are partitioned into two clusters based on the value of the first latent variable, and the informative conditional distribution $\\pi$ only samples latent vectors that are in the same cluster as the output latent vector. Empirically, when trained under this setting, the attention mechanism will pay more attention to tokens within the same cluster (Section 6.3). This implies that the self-attention layer can mitigate noise and concentrate on the informative conditional distribution $\\pi$ . ", "page_idx": 7}, {"type": "text", "text": "To understand this more intuitively, we will study the gradient of unnormalized attention scores. In particular, the unnormalized attention score is defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\nu_{t,t^{\\prime}}=(W_{K}W_{E}(t))^{T}(W_{Q}W_{E}(t^{\\prime}))/\\sqrt{d_{a}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 6. Suppose the data generating process follows Section 4.1 and $\\mathcal{N}(z^{*})\\,=\\,\\{z\\,:\\,z_{1}^{*}\\,=$ $z_{1}\\}\\setminus\\{z^{*}\\}$ . Given the last token in the sequence $t_{L}$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla_{u_{t,t_{L}}}\\ell(f^{L})=\\nabla\\ell(f^{L})^{T}(W_{E})^{T}W^{V}(\\alpha_{t}\\hat{p}_{t}W_{E}(t)-\\hat{p}_{t}\\sum_{l=1}^{L}\\hat{p}_{t_{l}}W_{E}(t_{l}))\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where for token $t$ , $\\begin{array}{r}{\\alpha_{t}=\\sum_{l=1}^{L}\\mathbf{1}[t_{l}=t]}\\end{array}$ and $\\hat{p}_{t}$ is the normalized attention score for token $t$ . ", "page_idx": 7}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/52b8ad1cbcf1f72a3e1caaa64b17561d636c7ec263f4ae0ef49998cfb4d59967.jpg", "img_caption": ["Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix $W_{V}$ as the identity matrix results in lower accuracy compared to training $\\Bar{W}_{V}$ . The figure reports average accuracy for both fixed and trained $W_{V}$ with $L=64$ . (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when $m=8$ . (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with $m=8$ and the cluster structure from Section 5.4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Typically, $\\alpha_{t}$ is larger when token $t$ and $t_{L}$ belong to the same cluster because tokens within the same cluster tend to co-occur frequently. As a result, the gradient contribution to the unnormalized attention score is usually larger for tokens within the same cluster. ", "page_idx": 8}, {"type": "text", "text": "5.5 Context hijacking and the misclassification of memory recall ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In light of the theoretical results on latent concept association, a natural question arises: How do these results connect to context hijacking in LLMs? In essence, for the latent concept association problem, the differentiation of output tokens is achieved by distinguishing between the various conditional distributions $p(x|y)$ . Thus, adding or changing tokens in the context $x$ so that it resembles a different conditional distribution can result in misclassification. In Appendix D.5, we present experiments showing that mixing different contexts can cause transformers to misclassify. This partially explains context hijacking in LLMs (Section 3). On the other hand, it is well-known that the error rate is related to the KL divergence between conditional distributions of contexts [Cov99]. The closer the distributions are, the easier it is for the model to misclassify. Here, longer contexts, primarily composed of i.i.d samples, suggest larger divergences, thus higher memory recall rate. This is theoretically implied by Theorem 1 and Theorem 4 and empirically verified in Appendix D.6. Such result is also related to reverse context hijacking (Appendix C) where prepending sentences including true target words can improve fact recall rate. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The main implications of the theoretical results in the previous section are: ", "page_idx": 8}, {"type": "text", "text": "1. The value matrix is important and has associative memory structure as in (5.1).   \n2. Training embeddings is crucial in the underparameterized regime, where embeddings exhibit certain geometric structures.   \n3. Attention mechanism is used to select the most relevant tokens. ", "page_idx": 8}, {"type": "text", "text": "To evaluate these claims, we conduct several experiments on synthetic datasets. Additional experimental details and results can be found in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "6.1 On the value matrix $W_{V}$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we study the necessity of the value matrix $W_{V}$ and its structure. First, we conduct experiments to compare the effects of training versus freezing $W_{V}$ as the identity matrix, with the context lengths $L$ set to 64 and 128. Figure 3a and Figure D.1 show that when the context length is small, freezing $W_{V}$ can lead to a significant decline in accuracy. This is inline with Lemma 2 and validates it in a general setting, implying the significance of the value matrix in maintaining a high memory recall rate. ", "page_idx": 8}, {"type": "text", "text": "Next, we investigate the degree of alignment between the trained value matrix $W_{V}$ and the construction in (5.1). The first set of experiments examines the similarity in functionality between the two matrices. We replace value matrices in trained transformers with the constructed ones like in (5.1) and then report accuracy with the new value matrix. As a baseline, we also consider randomly constructed value matrix, where the outer product pairs are chosen randomly (detailed construction can be found in Appendix D.1). Figure D.2 indicates that the accuracy does not significantly decrease when the value matrix is replaced with the constructed ones. Furthermore, not only are the constructed value matrix and the trained value matrix functionally alike, but they also share similar low-rank approximations. We use singular value decomposition to get the best low rank approximations of various value matrices where the rank is set to be the same as the number of latent variables $(m)$ . We then compute smallest principal angles between low-rank approximations of trained value matrices and those of constructed, randomly constructed, and Gaussian-initialized value matrices. Figure D.3 shows that the constructed ones have, on average, smallest principal angles with the trained ones. ", "page_idx": 9}, {"type": "text", "text": "6.2 On the embeddings ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we explore the significance of embedding training in the underparamerized regime and embedding structures. We conduct experiments to compare the effects of training versus freezing embeddings with different embedding dimensions. The learning rate is selected as the best option from $\\{0.01,0.001\\}$ depending on the dimensions. Figure D.4 clearly shows that when the dimension is smaller than the vocabulary size $(d<V)$ , embedding training is required. It is not necessary in the overparameterized regime ( $\\stackrel{\\cdot}{d}>\\dot{V}$ ), partially confirming Theorem 1 because if embeddings are initialized from a high-dimensional multi-variate Gaussian, they are approximately orthogonal to each other and have the same norms. ", "page_idx": 9}, {"type": "text", "text": "The next question is what kind of embedding structures are formed for trained transformers in the underparamerized regime. From Figure 3b and Figure D.5, it is evident that the relationship between the average inner product of embeddings for two tokens and their corresponding Hamming distance roughly aligns with (5.2). Perhaps surprisingly, if we plot the same graph for trained transformers with a fixed identity value matrix, the relationship is mostly linear as shown in Figure D.6, confirming our theory (Theorem 3). ", "page_idx": 9}, {"type": "text", "text": "As suggested in Section 5.3, such embedding geometry (5.2) can lead to low rank structures. We verify this claim by studying the spectrum of the embedding matrix $W_{E}$ . As illustrated in Appendix D.4, Figure D.9-D.12 demonstrate that there are eigengaps between top and bottom singular values, suggesting low-rank structures. ", "page_idx": 9}, {"type": "text", "text": "6.3 On the attention selection mechanism ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we examine the role of attention pattern by considering a special class of latent concept association model as defined in Section 5.4. Figure 3c and Figure D.7 clearly show that the selfattention select tokens in the same clusters. This suggests that attention can fliter out noise and focus on the informative conditional distribution $\\pi$ . We extend experiments to consider cluster structures that depend on the first two latent variables (detailed construction can be found in Appendix D.3) and Figure D.8 shows attention pattern as expected. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we first presented the phenomenon of context hijacking in LLMs, which suggested that fact retrieval is not robust against variations of contexts. This indicates that LLMs might function like associative memory where tokens in contexts are clues to guide memory retrieval. To investigate this perspective further, we devised a synthetic task called latent concept association and examined theoretically and empirically how single-layer transformers are trained to solve this task. These results provide further insights into the inner workings of transformers and LLMs, and can hopefully stimulate further work into interpreting and understanding the mechanisms by which LLMs predict tokens and recall facts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments We thank Victor Veitch for insightful discussions that helped shape the initial idea of this work. We acknowledge the support of AFRL and DARPA via FA8750-23-2-1015, ONR via N00014-23-1-2368, NSF via IIS-1909816, IIS-1955532, IIS-1956330, and NIH R01GM140467. We also acknowledge the support of the Robert H. Topel Faculty Research Fund at the University of Chicago Booth School of Business. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[Ach+23] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. \u201cGpt-4 technical report\u201d. arXiv preprint arXiv:2303.08774 (2023) (cit. on p. 17).   \n[AZL23a] Z. Allen-Zhu and Y. Li. Physics of language models: part 3.1, knowledge storage and extraction. 2023. arXiv: 2309.14316 [cs.CL] (cit. on p. 3).   \n[AZL23b] Z. Allen-Zhu and Y. Li. Physics of language models: part 3.2, knowledge manipulation. 2023. arXiv: 2309.14402 [cs.CL] (cit. on p. 3).   \n[AZL24] Z. Allen-Zhu and Y. Li. Physics of language models: part 3.3, knowledge capacity scaling laws. 2024. arXiv: 2404.05405 [cs.CL] (cit. on p. 3).   \n[Apr+22] G. Apruzzese, H. S. Anderson, S. Dambra, D. Freeman, F. Pierazzi, and K. A. Roundy. \"real attackers don\u2019t compute gradients\": bridging the gap between adversarial ml research and practice. 2022. arXiv: 2212.14315 [cs.CR] (cit. on p. 3).   \n[Bai+24] Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. \u201cTransformers as statisticians: provable in-context learning with in-context algorithm selection\u201d. Advances in neural information processing systems (2024) (cit. on p. 3).   \n[BYBOS95] R. Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky. \u201cTheory of orientation tuning in visual cortex.\u201d Proceedings of the National Academy of Sciences 9 (1995) (cit. on p. 2).   \n[Bie+24] A. Bietti, V. Cabannes, D. Bouchacourt, H. Jegou, and L. Bottou. \u201cBirth of a transformer: a memory viewpoint\u201d. Advances in Neural Information Processing Systems (2024) (cit. on pp. 3, 7).   \n[BP21] T. Bricken and C. Pehlevan. \u201cAttention approximates sparse distributed memory\u201d. Advances in Neural Information Processing Systems (2021) (cit. on pp. 2, 4).   \n$[\\mathbf{Bro}+20]$ T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. 2020. arXiv: 2005.14165 [cs.CL] (cit. on p. 1).   \n[CDB23] V. Cabannes, E. Dohmatob, and A. Bietti. \u201cScaling laws for associative memories\u201d. arXiv preprint arXiv:2310.02984 (2023) (cit. on p. 3).   \n[CSB24] V. Cabannes, B. Simsek, and A. Bietti. \u201cLearning associative memories with gradient descent\u201d. arXiv preprint arXiv:2402.18724 (2024) (cit. on pp. 3, 7).   \n[Cha22] F. Charton. \u201cWhat is my math transformer doing?\u2013three results on interpretability and generalization\u201d. arXiv preprint arXiv:2211.00170 (2022) (cit. on p. 3).   \n[Cho+24] A. G. Chowdhury, M. M. Islam, V. Kumar, F. H. Shezan, V. Jain, and A. Chadha. \u201cBreaking down the defenses: a comparative survey of attacks on large language models\u201d. arXiv preprint arXiv:2403.04786 (2024) (cit. on p. 3).   \n[Cov99] T. M. Cover. Elements of information theory. 1999 (cit. on p. 9).   \n[CSH22] A. Creswell, M. Shanahan, and I. Higgins. \u201cSelection-inference: exploiting large language models for interpretable logical reasoning\u201d. arXiv preprint arXiv:2205.09712 (2022) (cit. on pp. 2\u20134).   \n[Dai+21] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. \u201cKnowledge neurons in pretrained transformers\u201d. arXiv preprint arXiv:2104.08696 (2021) (cit. on pp. 1, 3).   \n[DCAT21] N. De Cao, W. Aziz, and I. Titov. \u201cEditing factual knowledge in language models\u201d. arXiv preprint arXiv:2104.08164 (2021) (cit. on pp. 1, 3).   \n[Dev83] L. Devroye. \u201cThe equivalence of weak, strong and complete convergence in l1 for kernel density estimates\u201d. The Annals of Statistics 3 (1983) (cit. on p. 18).   \n[Ede+24] B. L. Edelman, E. Edelman, S. Goel, E. Malach, and N. Tsilivis. \u201cThe evolution of statistical induction heads: in-context learning markov chains\u201d. arXiv preprint arXiv:2402.11004 (2024) (cit. on p. 3).   \n[Elh+21] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, et al. \u201cA mathematical framework for transformer circuits\u201d. Transformer Circuits Thread (2021) (cit. on pp. 3, 17).   \n$[\\mathrm{EI}+24]$ M Emrullah Ildiz, Y. Huang, Y. Li, A. Singh Rawat, and S. Oymak. \u201cFrom selfattention to markov models: unveiling the dynamics of generative transformers\u201d. arXiv e-prints (2024) (cit. on p. 3).   \n[Fel20] V. Feldman. \u201cDoes learning require memorization? a short tale about a long tail\u201d. In: Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing. 2020 (cit. on p. 3).   \n[FZ20] V. Feldman and C. Zhang. \u201cWhat neural networks memorize and why: discovering the long tail via influence estimation\u201d. Advances in Neural Information Processing Systems (2020) (cit. on p. 3).   \n[Fir57] J. Firth. \u201cA synopsis of linguistic theory, 1930-1955\u201d. Studies in linguistic analysis (1957) (cit. on p. 5).   \n$[\\mathrm{Gar}+22]$ S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. \u201cWhat can transformers learn incontext? a case study of simple function classes\u201d. Advances in Neural Information Processing Systems (2022) (cit. on p. 3).   \n[Gei+21] A. Geiger, H. Lu, T. Icard, and C. Potts. \u201cCausal abstractions of neural networks\u201d. Advances in Neural Information Processing Systems (2021) (cit. on p. 3).   \n[Gei+22] A. Geiger, Z. Wu, H. Lu, J. Rozner, E. Kreiss, T. Icard, N. Goodman, and C. Potts. \u201cInducing causal structure for interpretable neural networks\u201d. In: International Conference on Machine Learning. PMLR. 2022 (cit. on p. 3).   \n[Gei+24] A. Geiger, Z. Wu, C. Potts, T. Icard, and N. Goodman. \u201cFinding alignments between interpretable causal variables and distributed neural representations\u201d. In: Causal Learning and Reasoning. PMLR. 2024 (cit. on p. 3).   \n[Gev+23] M. Geva, J. Bastings, K. Filippova, and A. Globerson. \u201cDissecting recall of factual associations in auto-regressive language models\u201d. arXiv preprint arXiv:2304.14767 (2023) (cit. on p. 3).   \n[Has+24] P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun. \u201cDoes localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\u201d. Advances in Neural Information Processing Systems (2024) (cit. on p. 3).   \n[Hen+23] T. Henighan, S. Carter, T. Hume, N. Elhage, R. Lasenby, S. Fort, N. Schiefer, and C. Olah. \u201cSuperposition, memorization, and double descent\u201d. Transformer Circuits Thread (2023) (cit. on p. 3).   \n[Hop82] J. J. Hopfield. \u201cNeural networks and physical systems with emergent collective computational abilities.\u201d Proceedings of the national academy of sciences 8 (1982) (cit. on p. 2).   \n$[\\mathrm{Hu}+21]$ E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: low-rank adaptation of large language models. 2021. arXiv: 2106.09685 [cs.CL] (cit. on p. 2).   \n[Hu+24a] J. Y.-C. Hu, P.-H. Chang, R. Luo, H.-Y. Chen, W. Li, W.-P. Wang, and H. Liu. \u201cOutlier-efficient hopfield layers for large transformer-based models\u201d. arXiv preprint arXiv:2404.03828 (2024) (cit. on p. 2).   \n[Hu+24b] J. Y.-C. Hu, B.-Y. Chen, D. Wu, F. Ruan, and H. Liu. \u201cNonparametric modern hopfield models\u201d. arXiv preprint arXiv:2404.03900 (2024) (cit. on p. 2).   \n[Hu+24c] J. Y.-C. Hu, T. Lin, Z. Song, and H. Liu. \u201cOn computational limits of modern hopfield models: a fine-grained complexity analysis\u201d. arXiv preprint arXiv:2402.04520 (2024) (cit. on p. 2).   \n$[\\mathrm{Hu}+24\\mathrm{d}]$ J. Y.-C. Hu, D. Yang, D. Wu, C. Xu, B.-Y. Chen, and H. Liu. \u201cOn sparse modern hopfield model\u201d. Advances in Neural Information Processing Systems (2024) (cit. on p. 2).   \n[JP20] Y. Jiang and C. Pehlevan. \u201cAssociative memory in iterated overparameterized sigmoid autoencoders\u201d. In: International conference on machine learning. PMLR. 2020 (cit. on p. 2).   \n$[\\mathrm{Jia}+24]$ Y. Jiang, G. Rajendran, P. Ravikumar, B. Aragam, and V. Veitch. \u201cOn the origins of linear representations in large language models\u201d. arXiv preprint arXiv:2403.03867 (2024) (cit. on p. 3).   \n[Jin+23] T. Jin, N. Clement, X. Dong, V. Nagarajan, M. Carbin, J. Ragan-Kelley, and G. K. Dziugaite. \u201cThe cost of down-scaling language models: fact recall deteriorates before in-context learning\u201d. arXiv preprint arXiv:2310.04680 (2023) (cit. on p. 3).   \n[KKM22] J. Kim, M. Kim, and B. Mozafari. \u201cProvable memorization capacity of transformers\u201d. In: The Eleventh International Conference on Learning Representations. 2022 (cit. on p. 3).   \n$[\\mathrm{Li}+24]$ Y. Li, Y. Huang, M. E. Ildiz, A. S. Rawat, and S. Oymak. \u201cMechanics of next token prediction with self-attention\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2024 (cit. on pp. 3, 6).   \n[LLR23] Y. Li, Y. Li, and A. Risteski. \u201cHow do transformers learn topic structure: towards a mechanistic understanding\u201d. In: International Conference on Machine Learning. PMLR. 2023 (cit. on pp. 3, 6).   \n[Liu+23a] Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu. \u201cPrompt injection attack against llm-integrated applications\u201d. arXiv preprint arXiv:2306.05499 (2023) (cit. on p. 3).   \n[Liu+23b] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang, and Y. Liu. \u201cJailbreaking chatgpt via prompt engineering: an empirical study\u201d. arXiv preprint arXiv:2305.13860 (2023) (cit. on p. 3).   \n[Liu+22] Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams. \u201cTowards understanding grokking: an effective theory of representation learning\u201d. Advances in Neural Information Processing Systems (2022) (cit. on p. 3).   \n[LH17] I. Loshchilov and F. Hutter. \u201cDecoupled weight decay regularization\u201d. arXiv preprint arXiv:1711.05101 (2017) (cit. on p. 27).   \n[Luo+24] H. Luo, J. Yu, W. Zhang, J. Li, J. Y.-C. Hu, X. Xin, and H. Liu. \u201cDecoupled alignment for robust plug-and-play adaptation\u201d. arXiv preprint arXiv:2406.01514 (2024) (cit. on p. 3).   \n[MLT23] S. Mahdavi, R. Liao, and C. Thrampoulidis. \u201cMemorization capacity of multi-head attention in transformers\u201d. arXiv preprint arXiv:2306.02010 (2023) (cit. on p. 3).   \n[Mak+24] A. V. Makkuva, M. Bondaschi, A. Girish, A. Nagle, M. Jaggi, H. Kim, and M. Gastpar. Attention with markov: a framework for principled analysis of transformers via markov chains. 2024. arXiv: 2402.04161 [cs.LG] (cit. on p. 3).   \n$[\\mathrm{McG}{+}23]$ ] T. McGrath, M. Rahtz, J. Kramar, V. Mikulik, and S. Legg. \u201cThe hydra effect: emergent self-repair in language model computations\u201d. arXiv preprint arXiv:2307.15771 (2023) (cit. on p. 3).   \n[Men+22] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. \u201cLocating and editing factual associations in gpt\u201d. Advances in Neural Information Processing Systems (2022) (cit. on pp. 1\u20134, 25).   \n[Men+23] K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau. Mass-editing memory in a transformer. 2023. arXiv: 2210.07229 [cs.CL] (cit. on pp. 1, 3).   \n$[\\mathrm{Mil}+22]$ B. Millidge, T. Salvatori, Y. Song, T. Lukasiewicz, and R. Bogacz. \u201cUniversal hopfield networks: a general framework for single-shot associative memory models\u201d. In: International Conference on Machine Learning. PMLR. 2022 (cit. on pp. 2, 4).   \n[Mit+21] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. \u201cFast model editing at scale\u201d. arXiv preprint arXiv:2110.11309 (2021) (cit. on pp. 1, 3).   \n[Mit+22] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn. \u201cMemory-based model editing at scale\u201d. In: International Conference on Machine Learning. PMLR. 2022 (cit. on pp. 1, 3).   \n[Nan+23] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. \u201cProgress measures for grokking via mechanistic interpretability\u201d. arXiv preprint arXiv:2301.05217 (2023) (cit. on p. 3).   \n[NDL24] E. Nichani, A. Damian, and J. D. Lee. How transformers learn causal structure with gradient descent. 2024. arXiv: 2402.14735 [cs.LG] (cit. on p. 3).   \n$[\\mathrm{Ols}{+}22\\mathrm{a}]$ C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. 2022. arXiv: 2209.11895 [cs.LG] (cit. on p. 3).   \n[Ols+22b] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. \u201cIn-context learning and induction heads\u201d. arXiv preprint arXiv:2209.11895 (2022) (cit. on p. 3).   \n[PE21] L. Pandia and A. Ettinger. \u201cSorting through the noise: testing robustness of information processing in pre-trained language models\u201d. arXiv preprint arXiv:2109.12393 (2021) (cit. on pp. 2\u20134).   \n[PR22] F. Perez and I. Ribeiro. \u201cIgnore previous prompt: attack techniques for language models\u201d. arXiv preprint arXiv:2211.09527 (2022) (cit. on p. 3).   \n$[\\mathrm{Pet}+20]$ F. Petroni, P. Lewis, A. Piktus, T. Rockt\u00e4schel, Y. Wu, A. H. Miller, and S. Riedel. \u201cHow context affects language models\u2019 factual predictions\u201d. arXiv preprint arXiv:2005.04611 (2020) (cit. on pp. 2\u20134).   \n[Rad+19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. \u201cLanguage models are unsupervised multitask learners\u201d. OpenAI blog 8 (2019) (cit. on pp. 1\u20133).   \n[RBU20] A. Radhakrishnan, M. Belkin, and C. Uhler. \u201cOverparameterized neural networks implement associative memory\u201d. Proceedings of the National Academy of Sciences 44 (2020) (cit. on p. 2).   \n[Raj $+24]$ G. Rajendran, S. Buchholz, B. Aragam, B. Sch\u00f6lkopf, and P. Ravikumar. \u201cLearning interpretable concepts: unifying causal representation learning and foundation models\u201d. arXiv preprint arXiv:2402.09236 (2024) (cit. on p. 3).   \n[Ram+20] H. Ramsauer, B. Sch\u00e4f,l J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber, M. Holzleitner, M. Pavlovi\u00b4c, G. K. Sandve, et al. \u201cHopfield networks is all you need\u201d. arXiv preprint arXiv:2008.02217 (2020) (cit. on pp. 2, 4).   \n[Rao+23] A. Rao, S. Vashistha, A. Naik, S. Aditya, and M. Choudhury. \u201cTricking llms into disobedience: understanding, analyzing, and preventing jailbreaks\u201d. arXiv preprint arXiv:2305.14965 (2023) (cit. on p. 3).   \n[Sak+23] M. Sakarvadia, A. Ajith, A. Khan, D. Grzenda, N. Hudson, A. Bauer, K. Chard, and I. Foster. \u201cMemory injections: correcting multi-hop reasoning failures during inference in transformer-based language models\u201d. arXiv preprint arXiv:2309.05605 (2023) (cit. on p. 3).   \n[Seu96] H. S. Seung. \u201cHow the brain keeps the eyes still\u201d. Proceedings of the National Academy of Sciences 23 (1996) (cit. on p. 2).   \n[SMR23] M. Shanahan, K. McDonell, and L. Reynolds. \u201cRole play with large language models\u201d. Nature 7987 (2023) (cit. on p. 3).   \n[She+24] H. Sheen, S. Chen, T. Wang, and H. H. Zhou. \u201cImplicit regularization of gradient flow on one-layer softmax attention\u201d. arXiv preprint arXiv:2403.08699 (2024) (cit. on p. 3).   \n$[\\mathrm{Shi}+23]$ F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch\u00e4rli, and D. Zhou. \u201cLarge language models can be easily distracted by irrelevant context\u201d. In: International Conference on Machine Learning. PMLR. 2023 (cit. on pp. 2\u20134, 17).   \n$[\\mathrm{Si}+22]$ W. M. Si, M. Backes, J. Blackburn, E. De Cristofaro, G. Stringhini, S. Zannettou, and Y. Zhang. \u201cWhy so toxic? measuring and triggering toxic behavior in open-domain chatbots\u201d. In: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. 2022 (cit. on p. 3).   \n[Ska+94] W. Skaggs, J. Knierim, H. Kudrimoti, and B. McNaughton. \u201cA model of the neural basis of the rat\u2019s sense of direction\u201d. Advances in neural information processing systems (1994) (cit. on p. 2).   \n[SS22] J. Steinberg and H. Sompolinsky. \u201cAssociative memory of structured knowledge\u201d. Scientific Reports 1 (2022) (cit. on p. 2).   \n[Tar+23a] D. A. Tarzanagh, Y. Li, C. Thrampoulidis, and S. Oymak. \u201cTransformers as support vector machines\u201d. arXiv preprint arXiv:2308.16898 (2023) (cit. on pp. 3, 6).   \n$[\\mathrm{Tar}+23\\mathrm{b}]$ D. A. Tarzanagh, Y. Li, X. Zhang, and S. Oymak. \u201cMargin maximization in attention mechanism\u201d. arXiv preprint arXiv:2306.13596 (2023) (cit. on p. 3).   \n[Tea+24] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi\u00e8re, M. S. Kale, J. Love, et al. \u201cGemma: open models based on gemini research and technology\u201d. arXiv preprint arXiv:2403.08295 (2024) (cit. on pp. 2, 3, 6).   \n[Tia+24] B. Tian, S. Cheng, X. Liang, N. Zhang, Y. Hu, K. Xue, Y. Gou, X. Chen, and H. Chen. \u201cInstructedit: instruction-based knowledge editing for large language models\u201d. arXiv preprint arXiv:2402.16123 (2024) (cit. on p. 3).   \n$[\\mathrm{Tia}+23\\mathrm{a}]$ Y. Tian, Y. Wang, B. Chen, and S. S. Du. \u201cScan and snap: understanding training dynamics and token composition in 1-layer transformer\u201d. Advances in Neural Information Processing Systems (2023) (cit. on p. 3).   \n[Tia+23b] Y. Tian, Y. Wang, Z. Zhang, B. Chen, and S. Du. \u201cJoma: demystifying multilayer transformers via joint dynamics of mlp and attention\u201d. arXiv preprint arXiv:2310.00535 (2023) (cit. on p. 3).   \n[Tou+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. \u201cLlama 2: open foundation and fine-tuned chat models\u201d. arXiv preprint arXiv:2307.09288 (2023) (cit. on pp. 2, 3).   \n[Vas+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. \u201cAttention is all you need\u201d. Advances in neural information processing systems (2017) (cit. on p. 2).   \n[Wan+23a] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer, et al. \u201cDecodingtrust: a comprehensive assessment of trustworthiness in gpt models\u201d. arXiv preprint arXiv:2306.11698 (2023) (cit. on p. 3).   \n[Wan+23b] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng, et al. \u201cOn the robustness of chatgpt: an adversarial and out-of-distribution perspective\u201d. arXiv preprint arXiv:2302.12095 (2023) (cit. on p. 3).   \n[Wan+22] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. \u201cInterpretability in the wild: a circuit for indirect object identification in gpt-2 small\u201d. arXiv preprint arXiv:2211.00593 (2022) (cit. on p. 3).   \n$[\\mathrm{Wu}+24\\mathrm{a}]$ D. Wu, J. Y.-C. Hu, T.-Y. Hsiao, and H. Liu. \u201cUniform memory retrieval with larger capacity for modern hopfield models\u201d. arXiv preprint arXiv:2404.03827 (2024) (cit. on p. 2).   \n$[\\mathrm{Wu}+23]$ D. Wu, J. Y.-C. Hu, W. Li, B.-Y. Chen, and H. Liu. \u201cStanhop: sparse tandem hopfield model for memory-enhanced time series prediction\u201d. arXiv preprint arXiv:2312.17346 (2023) (cit. on p. 2).   \n$[\\mathrm{Wu}+24\\mathrm{b}]$ Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. \u201cInterpretability at scale: identifying causal mechanisms in alpaca\u201d. Advances in Neural Information Processing Systems (2024) (cit. on p. 3).   \n[Xie+21] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. \u201cAn explanation of in-context learning as implicit bayesian inference\u201d. arXiv preprint arXiv:2111.02080 (2021) (cit. on p. 3).   \n$[X\\mathrm{u}+23]$ X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli. \u201cAn llm can fool itself: a prompt-based adversarial attack\u201d. arXiv preprint arXiv:2310.13345 (2023) (cit. on p. 3).   \n$[\\mathrm{Yor}+23]$ O. Yoran, T. Wolfson, O. Ram, and J. Berant. \u201cMaking retrieval-augmented language models robust to irrelevant context\u201d. arXiv preprint arXiv:2310.01558 (2023) (cit. on pp. 2\u20134).   \n[Yu+24] J. Yu, H. Luo, J. Yao-Chieh, W. Guo, H. Liu, and X. Xing. \u201cEnhancing jailbreak attack against large language models through silent tokens\u201d. arXiv preprint arXiv:2405.20653 (2024) (cit. on p. 3).   \n[Zha+22] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. \u201cUnveiling transformers with lego: a synthetic reasoning task\u201d. arXiv preprint arXiv:2206.04301 (2022) (cit. on p. 3).   \n[Zha+23] Z. Zhang, M. Fang, L. Chen, M.-R. Namazi-Rad, and J. Wang. \u201cHow do large language models capture the ever-changing world knowledge? a review of recent advances\u201d. arXiv preprint arXiv:2310.07343 (2023) (cit. on p. 3).   \n[Zha23] J. Zhao. \u201cIn-context exemplars as clues to retrieving from large associative memory\u201d. arXiv preprint arXiv:2311.03498 (2023) (cit. on pp. 2, 4).   \n[Zhe+23] C. Zheng, L. Li, Q. Dong, Y. Fan, Z. Wu, J. Xu, and B. Chang. \u201cCan we edit factual knowledge by in-context learning?\u201d arXiv preprint arXiv:2305.12740 (2023) (cit. on p. 3).   \n[Zho+24] Z. Zhong, Z. Liu, M. Tegmark, and J. Andreas. \u201cThe clock and the pizza: two stories in mechanistic explanation of neural networks\u201d. Advances in Neural Information Processing Systems (2024) (cit. on p. 3).   \n[Zhu+23] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, N. Z. Gong, Y. Zhang, et al. \u201cPromptbench: towards evaluating the robustness of large language models on adversarial prompts\u201d. arXiv preprint arXiv:2306.04528 (2023) (cit. on p. 3).   \n[Zou+23] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models. 2023. arXiv: 2307. 15043 [cs.CL] (cit. on p. 3). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The context hijacking experiments were only conducted on open-source models and not on commercial models like GPT-4. Nevertheless, even in the official GPT-4 technical report [Ach+23], there is an example similar to context hijacking (the Elvis Perkins example). In that example, the prompt is \u201cSon of an actor, this American guitarist and rock singer released many songs and albums and toured with his band. His name is \"Elvis\" what?\u201d. GPT-4 answers with Presley, even though the answer is Perkins (Elvis Presley is not the son of an actor). GPT-4 can be viewed as distracted by all the information related to music and answers Presley. In fact, it is known that LLMs can be easily distracted by contexts in use cases other than fact retrieval such as problem-solving $[\\mathrm{Shi}+23]$ . So we reasonably suspect that similar behavior still exists in larger models but is harder to exploit. On the other hand, the theoretical section only focuses on single-layer transformer network. While single-layer networks already demonstrate some interesting phenomena including low-rank structures, the functionality of multi-layer transformers is much different compared to single-layer transformers with the notable emergence of induction head [Elh $^{1+21}$ ]. ", "page_idx": 16}, {"type": "text", "text": "B Additional Theoretical Results and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proofs for Section 5.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 1 can be stated more formally as follows: ", "page_idx": 16}, {"type": "text", "text": "Theorem 7. Suppose the data generating process follows Section 4.1 where $m\\geq3$ , $\\omega=1$ , and $\\mathcal{N}(t)=V\\setminus\\{t\\}$ . Assume there exists a single layer transformer given by (4.1) such that a) $W_{K}=0$ and $W_{Q}=0,\\,b,$ Each row of $W_{E}$ is orthogonal to each other and normalized, and $c$ ) $W_{V}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{V}=\\sum_{i\\in[V]}W_{E}(i)(\\sum_{j\\in\\mathcal{N}_{1}(i)}W_{E}(j)^{T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$\\begin{array}{r}{L>\\operatorname*{max}\\{\\frac{100m^{2}\\log(3/\\varepsilon)}{(\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta}))^{2}},\\frac{80m^{2}|\\mathcal{N}(y)|}{(\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta}))^{2}}\\}f o r\\,a n y\\,\\,y,\\,t h}\\end{array}$ ", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{\\overline{{{D^{L}}}}}(f^{L})\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $0<\\varepsilon<1$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. First of all, the error is defined to be: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{\\mathcal{D}^{L}}(f^{L})=\\mathbb{P}_{(x,y)\\sim\\mathcal{D}^{L}}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]}\\\\ &{\\qquad\\qquad=\\mathbb{P}_{y}\\mathbb{P}_{x|y}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let\u2019s focus on the conditional probability $\\mathbb{P}_{x|y}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]$ . ", "page_idx": 16}, {"type": "text", "text": "By construction, the single layer transformer model has uniform attention. Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\nh(x)=\\sum_{i\\in\\mathcal{N}(y)}\\alpha_{i}W_{E}(i)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{i}=\\frac{1}{L}\\sum_{k=1}^{L}\\mathbf{1}\\{t_{k}=i\\}}\\end{array}$ which is the number of occurrence of token $i$ in the sequence. By the latent concept association model, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\np(i|y)=\\frac{\\exp(-D_{H}(i,y)/\\beta)}{Z}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{Z=\\sum_{i\\in\\mathcal{N}(y)}\\exp(-D_{H}(i,y)/\\beta)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Thus, the logit for token $y$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)=\\sum_{i\\in\\mathcal{N}_{1}(y)}\\alpha_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And the logit for any other token $\\tilde{y}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{\\tilde{y}}^{L}(x)=\\sum_{i\\in\\mathcal{N}_{1}(\\tilde{y})}\\alpha_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the prediction to be correct, we need ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\tilde{y}}{f_{y}^{L}(x)}-f_{\\tilde{y}}^{L}(x)>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Lemma 3 of [Dev83], we know that for all $\\Delta\\in(0,1)$ , if $\\begin{array}{r}{\\frac{|\\mathcal{N}(y)|}{L}\\leq\\frac{{\\Delta}^{2}}{20}}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\operatorname*{max}_{i\\in\\mathcal{N}(y)}|\\alpha_{i}-p(i|y)|>\\Delta\\big)\\le\\mathbb{P}\\big(\\sum_{i\\in\\mathcal{N}(y)}|\\alpha_{i}-p(i|y)|>\\Delta\\big)\\le3\\exp(-L\\Delta^{2}/25)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, if { 25 lo\u2206g(23/\u03b5), 20|\u2206N 2(y)|}, then with probability at least 1 \u2212\u03b5, we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in{\\mathcal{N}}(y)}|\\alpha_{i}-p(i|y)|\\le\\Delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{y}^{L}(x)-f_{y}^{L}(x)=\\displaystyle\\sum_{i\\in\\mathcal{N}_{1}(y)}\\alpha_{i}-\\sum_{j\\in\\mathcal{N}_{1}(\\Tilde{y})}\\alpha_{j}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i\\in\\mathcal{N}_{1}(y)}\\alpha_{i}-\\sum_{i\\in\\mathcal{N}_{1}(y)}p(i|y)+\\displaystyle\\sum_{i\\in\\mathcal{N}_{1}(y)}p(i|y)}\\\\ &{\\qquad\\qquad\\qquad-\\displaystyle\\sum_{j\\in\\mathcal{N}_{1}(\\Tilde{y})}p(j|y)+\\displaystyle\\sum_{j\\in\\mathcal{N}_{1}(\\Tilde{y})}p(j|y)-\\sum_{j\\in\\mathcal{N}_{1}(\\Tilde{y})}\\alpha_{j}}\\\\ &{\\qquad\\geq\\displaystyle\\sum_{i\\in\\mathcal{N}_{1}(y)}p(i|y)-\\sum_{j\\in\\mathcal{N}_{1}(\\Tilde{y})}p(j|y)-2m\\Delta}\\\\ &{\\qquad\\geq\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})-2m\\Delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that because of Lemma 10, there\u2019s no neighboring set that is the superset of another. ", "page_idx": 17}, {"type": "text", "text": "Therefore as long as $\\begin{array}{r}{\\Delta<\\frac{\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})}{2m}}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)-f_{\\tilde{y}}^{L}(x)>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $\\tilde{y}$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Finally, if $\\begin{array}{r}{L>\\operatorname*{max}\\{\\frac{100m^{2}\\log(3/\\varepsilon)}{(\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta}))^{2}},\\frac{80m^{2}|\\mathcal{N}(y)|}{(\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta}))^{2}}\\}}\\end{array}$ for any $y$ , then $\\mathbb{P}_{x|y}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]\\le\\varepsilon$ ", "page_idx": 17}, {"type": "text", "text": "And ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{\\mathcal{D}^{L}}(f^{L})=\\mathbb{P}_{(x,y)\\sim\\mathcal{D}^{L}}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]}\\\\ &{\\qquad\\qquad=\\mathbb{P}_{y}\\mathbb{P}_{x|y}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]\\leq\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.2 Proofs for Section 5.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 2. Suppose the data generating process follows Section 4.1 where $m\\geq3,$ , $\\omega\\,=\\,1$ and $\\mathcal{N}(t)=\\{t^{\\prime}:\\bar{D_{H}}(t,t^{\\prime}))=1\\}$ . For any single layer transformer given by (4.1) where each row of $W_{E}$ is orthogonal to each other and normalized, if $W_{V}$ is constructed as in (5.1), then the error rate is 0. If $W_{V}$ is the identity matrix, then the error rate is strictly larger than 0. ", "page_idx": 17}, {"type": "text", "text": "Proof. Following the proof for Theorem 7, let\u2019s focus on the conditional probability: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x|y}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By construction, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nh(x)=\\sum_{i\\in\\mathcal{N}_{1}(y)}\\alpha_{i}W_{E}(i)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{i}=\\frac{1}{L}\\sum_{k=1}^{L}\\mathbf{1}\\{t_{k}=i\\}}\\end{array}$ which is the number of occurrence of token $i$ in the sequence. ", "page_idx": 18}, {"type": "text", "text": "Let\u2019s consider the first case where $W_{V}$ is constructed as in (5.1). Then we know that for some other token $\\tilde{y}\\neq y$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)-f_{\\tilde{y}}^{L}(x)=\\sum_{i\\in\\mathcal{N}_{1}(y)}\\alpha_{i}-\\sum_{i\\in\\mathcal{N}_{1}(\\tilde{y})}\\alpha_{i}=1-\\sum_{i\\in\\mathcal{N}_{1}(\\tilde{y})}\\alpha_{i}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 10, we have that for any token ${\\tilde{y}}\\neq y$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)-f_{\\tilde{y}}^{L}(x)>0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the error rate is always 0. ", "page_idx": 18}, {"type": "text", "text": "Now let\u2019s consider the second case where $W_{V}$ is the identity matrix. Let $j$ be a token in the set $\\mathcal{N}_{1}(y)$ . Then there is a non-zero probability that context $x$ contains only $j$ . In that case, ", "page_idx": 18}, {"type": "equation", "text": "$$\nh(x)=W_{E}(j)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However, we know that by the assumption on the embedding matrix, ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)-f_{j}^{L}(x)=(W_{E}(y)-W_{E}(j))^{T}h(x)=-\\|W_{E}(j)\\|^{2}<0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies that there\u2019s non zero probability that $y$ is misclassified. Therefore, when $W_{V}$ is the identity matrix, the error rate is strictly larger than 0. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Theorem 3. Suppose the data generating process follows Section 4.1 where $m\\geq3$ , $\\omega=1$ and $\\mathcal{N}(t)=V\\setminus\\{t\\}$ . For any single layer transformer given by (4.1) with $W_{V}$ being the identity matrix, if the cross entropy loss is minimized so that for any sampled pair $(x,y)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\np(y|x)=\\hat{p}(y|x)=s o f t m a x(f_{y}^{L}(x))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "there exists $a>0$ and $b$ such that for two tokens $t\\neq t^{\\prime}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle W_{E}(t),W_{E}(t^{\\prime})\\rangle=-a D_{H}(t,t^{\\prime})+b\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Because for any pair of $(x,y)$ , the estimated conditional probability matches the true conditional probability. In particular, let\u2019s consider two target tokens $y_{1},\\,y_{2}$ and context $\\boldsymbol{x}=(t_{i},...,t_{i})$ for some token $t_{i}$ such that $p(x|y_{1})>0$ and $p(x|y_{2})>0$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{p(y_{1}|x)}{p(y_{2}|x)}}={\\frac{p(x|y_{1})p(y_{1})}{p(x|y_{2})p(y_{2})}}={\\frac{p(x|y_{1})}{p(x|y_{2})}}={\\frac{{\\hat{p}}(x|y_{1})}{{\\hat{p}}(x|y_{2})}}=\\exp((W_{E}(y_{1})-W_{E}(y_{2}))^{T}h(x))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second equality is because $p(y)$ is the uniform distribution. By our construction, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{p(x|y_{1})}{p(x|y_{2})}=\\frac{p(t_{i}|y_{1})^{L}}{p(t_{i}|y_{2})^{L}}=\\exp((W_{E}(y_{2})-W_{E}(y_{1}))^{T}h(x))=\\exp((W_{E}(y_{1})-W_{E}(y_{2}))^{T}W_{E}(t_{i}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the data generating process, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{L}{\\beta}(D_{H}(t_{i},y_{2})-D_{H}(t_{i},y_{1}))=(W_{E}(y_{1})-W_{E}(y_{2}))^{T}W_{E}(t_{i})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $t_{i}=y_{3}$ such that $y_{3}\\neq y_{1},y_{3}\\neq y_{2}$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{L}{\\beta}D_{H}(y_{3},y_{1})-W_{E}(y_{1})^{T}W_{E}(y_{3})=\\frac{L}{\\beta}D_{H}(y_{3},y_{2})-W_{E}(y_{2})^{T}W_{E}(y_{3})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For simplicity, let\u2019s define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi(y_{1},y_{2})={\\frac{L}{\\beta}}D_{H}(y_{1},y_{2})-W_{E}(y_{1})^{T}W_{E}(y_{2})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi(y_{3},y_{1})=\\Psi(y_{3},y_{2})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now consider five distinct labels: $y_{1},y_{2},y_{3},y_{4},y_{5}$ . We have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi(y_{3},y_{1})=\\Psi(y_{3},y_{2})=\\Psi(y_{4},y_{2})=\\Psi(y_{4},y_{5})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In other words, $\\Psi(y_{3},y_{1})=\\Psi(y_{4},y_{5})$ for arbitrarily chosen distinct labels $y_{1},y_{3},y_{4},y_{5}$ . Therefore, $\\Psi(t,t^{\\prime})$ is a constant for $t\\neq t^{\\prime}$ . ", "page_idx": 19}, {"type": "text", "text": "For any two tokens $t\\neq t^{\\prime}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{L}{\\beta}D_{H}(t,t^{\\prime})-W_{E}(t)^{T}W_{E}(t^{\\prime})=C\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{E}(t)^{T}W_{E}(t^{\\prime})=-\\frac{L}{\\beta}D_{H}(t,t^{\\prime})+C\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.3 Proofs for Section 5.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 4 can be formalized as the following theorem. ", "page_idx": 19}, {"type": "text", "text": ",u i nfogrs  afonlyl w,  a(n5.d2) then if $b>0$ , $\\Delta_{1}>0$ $\\begin{array}{r}{\\mathrm{)},\\,0<\\Delta<\\frac{\\exp\\left(-\\frac{1}{\\beta}\\right)-\\exp\\left(-\\frac{2}{\\beta}\\right)}{2m}}\\end{array}$ $\\begin{array}{r}{L\\geq\\operatorname*{max}\\lbrace\\frac{25\\log(3/\\varepsilon)}{\\Delta^{2}},\\frac{20|\\mathcal{N}(y)|}{\\Delta^{2}}\\rbrace}\\end{array}$ $y$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n0<a<\\frac{2\\exp(\\frac{1}{\\beta})}{(|V|-2)m^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{~}_{0}>\\operatorname*{max}\\{\\frac{a(m-2)m+\\Delta_{1}}{\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})-2m\\Delta}+b,\\frac{(b-a)\\Delta_{1}-\\frac{|V|-2}{2}a b m^{2}\\exp(-\\frac{1}{\\beta})+\\frac{|V|-2}{2}a^{2}(m-2)\\Delta_{1}+\\frac{|V|-2}{2}a b m^{2}\\exp(-\\frac{1}{\\beta})}{1-\\frac{|V|-2}{2}a m^{2}\\exp(-\\frac{1}{\\beta})}\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{\\ensuremath \u1e0a \\mathcal \u1e0a D \u1e0c \u1e0c ^{L}}(f^{L})\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $0<\\varepsilon<1$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Following the proof of Theorem 7, let\u2019s also focus on the conditional probability ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x|y}[\\mathrm{argmax}\\,f^{L}(x)\\neq y]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By construction, the single layer transformer model has uniform attention. Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\nh(x)=\\sum_{i\\in\\mathcal{N}(y)}\\alpha_{i}W_{E}(i)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{i}=\\frac{1}{L}\\sum_{k=1}^{L}\\mathbf{1}\\{t_{k}=i\\}}\\end{array}$ which is the number of occurrence of token $i$ in the sequence. For simplicity, let\u2019s define $\\alpha_{y}=0$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nh(x)=\\sum_{i\\in[V]}\\alpha_{i}W_{E}(i)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we also have that if $\\begin{array}{r}{L\\geq\\operatorname*{max}\\lbrace\\frac{25\\log(3/\\varepsilon)}{\\Delta^{2}},\\frac{20\\left|\\mathcal{N}(y)\\right|}{\\Delta^{2}}\\rbrace}\\end{array}$ , then with probability at least $1-\\varepsilon$ , we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[V]}|\\alpha_{i}-p(i|y)|\\le\\Delta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Also define the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\phi_{k}({\\boldsymbol{x}})=\\sum_{j\\in\\mathcal{N}_{1}(k)}W_{E}(j)^{T}\\big(\\sum_{i\\in[V]}\\alpha_{i}W_{E}(i)\\big)}\\\\ {\\displaystyle v_{k}({\\boldsymbol{y}})=W_{E}(y)^{T}W_{E}(k)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, the logit for token $y$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)=\\sum_{k=0}^{|V|-1}v_{k}(y)\\phi_{k}(x)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let\u2019s investigate $\\phi_{k}(x)$ . By Lemma 9, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\phi_{k}(x)=\\sum_{i\\in[V]}\\alpha_{i}(\\sum_{j\\in{\\cal N}_{1}(k)}W_{E}(j)^{T}W_{E}(i))}}\\\\ {{\\displaystyle\\qquad=(b_{0}-b)\\sum_{j\\in{\\cal N}_{1}(k)}\\alpha_{j}+\\sum_{i\\in[V]}\\alpha_{i}(-a(m-2)D_{H}(k,i)+(b-a)m)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, for any $k_{1},k_{2}\\in[V]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\phi_{k_{1}}(x)-\\phi_{k_{2}}(x)=(b_{0}-b)\\big(\\displaystyle\\sum_{j_{1}\\in\\mathcal{N}_{1}(k_{1})}\\alpha_{j_{1}}-\\displaystyle\\sum_{j_{2}\\in\\mathcal{N}_{1}(k_{2})}\\alpha_{j_{2}}\\big)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\sum_{i\\in[V]}\\alpha_{i}a(m-2)(D_{H}(k_{2},i)-D_{H}(k_{1},i))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because $-m\\le D_{H}(k_{2},i)-D_{H}(k_{1},i)\\le m$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(b_{0}-b)(\\displaystyle\\sum_{j_{1}\\in\\mathcal{N}_{1}(k_{1})}\\alpha_{j_{1}}-\\displaystyle\\sum_{j_{2}\\in\\mathcal{N}_{1}(k_{2})}\\alpha_{j_{2}})-a(m-2)m}&{}\\\\ {\\leq\\phi_{k_{1}}(x)-\\phi_{k_{2}}(x)\\leq}&{}\\\\ {(b_{0}-b)(\\displaystyle\\sum_{j_{1}\\in\\mathcal{N}_{1}(k_{1})}\\alpha_{j_{1}}-\\displaystyle\\sum_{j_{2}\\in\\mathcal{N}_{1}(k_{2})}\\alpha_{j_{2}})+a(m-2)m}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For prediction to be correct, we need ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\tilde{y}}{f_{y}^{L}(x)}-f_{\\tilde{y}}^{L}(x)>0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This also means that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\tilde{y}}\\,\\sum_{k=0}^{|V|-1}\\big(v_{k}(y)-v_{k}(\\tilde{y})\\big)\\phi_{k}(x)>0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "One can show that for any $k$ , if $\\iota^{-1}(\\tilde{k})=\\iota^{-1}(y)\\otimes\\iota^{-1}(\\tilde{y})\\otimes\\iota^{-1}(k)$ where $\\otimes$ means bitwise XOR, then ", "page_idx": 20}, {"type": "equation", "text": "$$\nv_{k}(y)-v_{k}(\\tilde{y})=v_{\\tilde{k}}(\\tilde{y})-v_{\\tilde{k}}(y)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First of all, if $k=y$ , then $\\tilde{k}=\\tilde{y}$ , which means ", "page_idx": 20}, {"type": "equation", "text": "$$\nv_{k}(y)-v_{k}(\\tilde{y})=v_{\\tilde{k}}(\\tilde{y})-v_{\\tilde{k}}(y)=b_{0}+a D_{H}(y,\\tilde{y})-b\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $k\\neq y,\\tilde{y}$ , then (B.1) implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{H}(k,y)-D_{H}(k,\\tilde{y})=D_{H}(\\tilde{k},\\tilde{y})-D_{H}(\\tilde{k},y)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We know that $D_{H}(k,y)$ is the number of 1s in $\\iota^{-1}(k)\\otimes\\iota^{-1}(y)$ and, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\iota^{-1}(\\tilde{k})\\otimes\\iota^{-1}(y)=\\iota^{-1}(y)\\otimes\\iota^{-1}(\\tilde{y})\\otimes\\iota^{-1}(k)\\otimes\\iota^{-1}(y)=\\iota^{-1}(\\tilde{y})\\otimes\\iota^{-1}(k)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\iota^{-1}(\\tilde{k})\\otimes\\iota^{-1}(\\tilde{y})=\\iota^{-1}(y)\\otimes\\iota^{-1}(k)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, (B.1) holds and we can rewrite $f_{y}^{L}(x)-f_{\\tilde{y}}^{L}(x)$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\nf_{y}^{L}(x)-f_{\\bar{y}}^{L}(x)=\\sum_{k=0}^{|V|-1}\\big(v_{k}(y)-v_{k}(\\tilde{y})\\big)\\phi_{k}(x)}\\\\ {=(b_{0}-b+a D_{H}(y,\\tilde{y}))(\\phi_{y}(x)-\\phi_{\\tilde{y}}(x))}\\\\ {+\\sum_{k\\neq y,\\tilde{y},D_{H}(k,y)\\geq D_{H}(k,\\tilde{y})}a(D_{H}(k,y)-D_{H}(k,\\tilde{y}))(\\phi_{k}(x)-\\phi_{\\tilde{k}}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We already know that $b_{0}>b>0$ and $a>0$ , thus, $b_{0}-b+a D_{H}(y,\\tilde{y})>0$ for any pair $y,\\tilde{y}$ . We also want $\\phi_{y}(x)-\\phi_{\\tilde{y}}(x)$ to be positive. Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi_{y}(x)-\\phi_{\\tilde{y}}(x)\\geq(b_{0}-b)(\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})-2m\\Delta)-a(m-2)m\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We need $\\begin{array}{r}{\\Delta<\\frac{\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})}{2m}}\\end{array}$ and for some positive $\\Delta_{1}>0,b_{0}$ needs to be large enough such that $\\phi_{y}(x)-\\phi_{\\tilde{y}}(x)>\\Delta_{1}$ ", "page_idx": 21}, {"type": "text", "text": "which implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{0}>\\frac{a(m-2)m+\\Delta_{1}}{\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})-2m\\Delta}+b\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, for $k\\neq y,\\tilde{y}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ddots(x)-\\phi_{\\tilde{k}}(x)\\geq(b_{0}-b)(\\sum_{j_{1}\\in{\\cal N}_{1}(k)}\\alpha_{j_{1}}-\\displaystyle\\sum_{j_{2}\\in{\\cal N}_{1}(\\tilde{k})}\\alpha_{j_{2}})-a(m-2)m}\\\\ &{\\qquad\\qquad\\qquad\\geq(b_{0}-b)(-(m-1)\\exp(-\\displaystyle\\frac{1}{\\beta})-\\exp(-\\displaystyle\\frac{2}{\\beta})-2m\\Delta)-a(m-2)m}\\\\ &{\\qquad\\qquad\\geq(b_{0}-b)(-(m-1)\\exp(-\\displaystyle\\frac{1}{\\beta})-\\exp(-\\displaystyle\\frac{2}{\\beta})+\\exp(-\\displaystyle\\frac{2}{\\beta})-\\exp(-\\displaystyle\\frac{1}{\\beta}))-a(m-2)r}\\\\ &{\\qquad\\qquad\\geq-(b_{0}-b)m\\exp(-\\displaystyle\\frac{1}{\\beta})-a(m-2)m}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{r}_{y}^{L}(x)-f_{\\tilde{y}}^{L}(x)\\geq(b_{0}-b+a)\\Delta_{1}-\\cfrac{|V|-2}{2}\\bigg((b_{0}-b)a m^{2}\\exp(-\\cfrac{1}{\\beta})+a^{2}(m-2)m^{2}\\bigg)}\\\\ &{\\geq\\bigg(1-\\cfrac{|V|-2}{2}a m^{2}\\exp(-\\cfrac{1}{\\beta})\\bigg)b_{0}-(b-a)\\Delta_{1}+\\cfrac{|V|-2}{2}a b m^{2}\\exp(-\\cfrac{1}{\\beta})-\\cfrac{|V|-2}{2}a^{2}(m-2)m}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The lower bound is independent of $\\tilde{y}$ , therefore, we need it to be positive to ensure the prediction is correct. To achieve this, we want ", "page_idx": 21}, {"type": "equation", "text": "$$\n1-\\frac{|V|-2}{2}a m^{2}\\exp(-\\frac{1}{\\beta})>0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\na<\\frac{2\\exp(\\frac{1}{\\beta})}{(|V|-2)m^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "And finally we need ", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{0}>\\frac{(b-a)\\Delta_{1}-\\frac{|V|-2}{2}a b m^{2}\\exp(-\\frac{1}{\\beta})+\\frac{|V|-2}{2}a^{2}(m-2)m^{2}}{1-\\frac{|V|-2}{2}a m^{2}\\exp(-\\frac{1}{\\beta})}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To summarize, i $\\begin{array}{r}{\\mathrm{~};b>0,\\Delta_{1}>0,0<\\Delta<\\frac{\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})}{2m},L\\geq\\operatorname*{max}\\{\\frac{25\\log(3/\\varepsilon)}{\\Delta^{2}},\\frac{20|\\mathcal{N}(y)|}{\\Delta^{2}}\\}}\\end{array}$ any $y$ , and ", "page_idx": 21}, {"type": "equation", "text": "$$\n0<a<\\frac{2\\exp(\\frac{1}{\\beta})}{(|V|-2)m^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\L_{0}>\\operatorname*{max}\\{\\frac{a(m-2)m+\\Delta_{1}}{\\exp(-\\frac{1}{\\beta})-\\exp(-\\frac{2}{\\beta})-2m\\Delta}+b,\\frac{(b-a)\\Delta_{1}-\\frac{|V|-2}{2}a b m^{2}\\exp(-\\frac{1}{\\beta})+\\frac{|V|-2}{2}a^{2}(m-2)\\Delta_{1}+\\frac{|V|-2}{2}a b m^{2}\\exp(-\\frac{1}{\\beta})}{1-\\frac{|V|-2}{2}a m^{2}\\exp(-\\frac{1}{\\beta})}\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{\\ensuremath \u1e0a \\mathcal \u1e0a D \u1e0c \u1e0c ^{L}}(f^{L})\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $0<\\varepsilon<1$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 5. If embeddings follow (5.2) and $b=b_{0}$ and $\\mathcal{N}(t)=V\\setminus\\{t\\}$ , then rank $:\\!(W_{E})\\leq m+2.$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. By (5.2), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle W_{E}(i),W_{E}(j)\\rangle=-a D_{H}(i,j)+b\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n(W_{E})^{T}W_{E}=-a D_{H}+b\\mathbf{1}\\mathbf{1}^{T}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let\u2019s first look at $D_{H}$ which has rank at most $m+1$ . To see this, let\u2019s consider a set of $m+1$ tokens: $\\{e_{0},e_{1},...,e_{m}\\}\\subseteq V$ where $e_{k}=2^{k}$ . Here $e_{0}$ is associated with the latent vector of all zeroes and the latent vector associated with $e_{k}$ has only the $k$ -th latent variable being 1. ", "page_idx": 22}, {"type": "text", "text": "On the other hand, for any token $i$ , we have that, ", "page_idx": 22}, {"type": "equation", "text": "$$\ni=\\sum_{k:\\iota^{-1}(i)_{k}=1}e_{k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In fact, ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{H}(i)=\\sum_{k:u^{-1}(i)_{k}=1}\\bigg(D_{H}(e_{k})-D_{H}(e_{0})\\bigg)+D_{H}(e_{0})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $D_{H}(i)$ is the $i$ -th row of $D_{H}$ , and for each entry $j$ of $D_{H}(i)$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{H}(i,j)=\\sum_{k:t^{-1}(i)_{k}=1}\\bigg(D_{H}(e_{k},j)-D_{H}(e_{0},j)\\bigg)+D_{H}(e_{0},j)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is because ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{H}(e_{k},j)-D_{H}(e_{0},j)=\\left\\{\\begin{array}{l l}{+1}&{\\mathrm{if~}\\iota^{-1}(j)_{k}=0}\\\\ {-1}&{\\mathrm{if~}\\iota^{-1}(j)_{k}=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we can rewrite $D_{H}(i,j)$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{H}(i,j)=}&{\\quad\\displaystyle\\sum_{k:=1(i)_{k}=1}\\Bigg(\\mathbf{1}[\\iota^{-1}(i)_{k}=1,\\iota^{-1}(j)_{k}=0]-\\mathbf{1}[\\iota^{-1}(i)_{k}=1,\\iota^{-1}(j)_{k}=1]\\Bigg)\\Bigg)+D_{H}(\\ell,\\ell,\\ell,\\ell,\\ell)}\\\\ {=}&{\\displaystyle\\sum_{k=1}^{m}\\Bigg(\\mathbf{1}[\\iota^{-1}(i)_{k}=1,\\iota^{-1}(j)_{k}=0]-\\mathbf{1}[\\iota^{-1}(i)_{k}=1,\\iota^{-1}(j)_{k}=1]\\Bigg)\\Bigg.}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\sum_{k=1}^{m}\\Bigg(\\mathbf{1}[\\iota^{-1}(i)_{k}=0,\\iota^{-1}(j)_{k}=1]+\\mathbf{1}[\\iota^{-1}(i)_{k}=1,\\iota^{-1}(j)_{k}=1]\\Bigg)\\Bigg)}\\\\ {=\\displaystyle\\sum_{k=1}^{m}\\mathbf{1}[\\iota^{-1}(i)_{k}=1,\\iota^{-1}(j)_{k}=0]+\\mathbf{1}[\\iota^{-1}(i)_{k}=0,\\iota^{-1}(j)_{k}=1]}\\\\ &{=D_{H}(i,j)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, every row of $D_{H}$ can be written as a linear combination of $\\{D_{H}(e_{0}),D_{H}(e_{1}),...,D_{H}(e_{m})\\}$ . In other words, $D_{H}$ has rank at most $m+1$ . ", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{rank}((W_{E})^{T}W_{E})=\\operatorname{rank}(W_{E})\\leq m+2.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 9. Let $z^{(0)}$ and $z^{(1)}$ be two binary vectors of size m where $m\\geq2$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{z:D_{H}(z^{(0)},z)=1}D_{H}(z,z^{(1)})=(m-2)D_{H}(z^{(0)},z^{(1)})+m\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. For $z$ such that $D_{H}(z,z^{(0)})=1$ , we know that there are two cases. Either $z$ differs with $z^{(0)}$ on a entry but agrees with $z^{(1)}$ on that entry or $z$ differs with both $z^{(0)}$ and $z^{(1)}$ . ", "page_idx": 23}, {"type": "text", "text": "For the first case, we know that there are $D_{H}(z^{(0)},z^{(1)})$ such entries. In this case, ${\\cal D}_{H}(z,z^{(1)})=$ $D_{H}(z^{(0)},z^{(1)})-1$ . For the second case, $D_{H}(z,z^{(1)})=D_{H}(z^{(0)},z^{(1)})+1$ . ", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{D_{H}(z,z^{(0)})=1}D_{H}(z,z^{(1)})}}\\\\ &{=D_{H}(z^{(0)},z^{(1)})(D_{H}(z^{(0)},z^{(1)})-1)+(m-D_{H}(z^{(0)},z^{(1)}))(D_{H}(z^{(0)},z^{(1)})+1)}\\\\ &{=(m-2)D_{H}(z^{(0)},z^{(1)})+m}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 10. If $m\\geq3$ and ${\\mathcal{N}}(t)=V\\setminus\\{t\\}.$ , then $\\mathcal{N}_{1}(t)\\subsetneq\\mathcal{N}_{1}(t^{\\prime})$ for any t, t\u2032 \u2208[V ]. ", "page_idx": 23}, {"type": "text", "text": "Proof. For any token $t_{\\cdot}$ , $\\mathcal{N}_{1}(t)$ contains any token $t^{\\prime}$ such that $D_{H}(t,t^{\\prime})=1$ by the conditions. Then given a set $\\mathcal{N}_{1}(t)$ , one can uniquely determine token $t$ . This is because for the set of latent vectors associated with $\\mathcal{N}_{1}(t)$ , at each index, there could only be one possible change. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "B.4 Proofs for Section 5.4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 6. Suppose the data generating process follows Section 4.1 and $\\mathcal{N}(z^{*})\\,=\\,\\{z\\,:\\,z_{1}^{*}\\,=$ $z_{1}\\}\\setminus\\{z^{*}\\}$ . Given the last token in the sequence $t_{L}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{u_{t,t_{L}}}\\ell(f^{L})=\\nabla\\ell(f^{L})^{T}(W_{E})^{T}W^{V}(\\alpha_{t}\\hat{p}_{t}W_{E}(t)-\\hat{p}_{t}\\sum_{l=1}^{L}\\hat{p}_{t_{l}}W_{E}(t_{l}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where for token $t$ , $\\begin{array}{r}{\\alpha_{t}=\\sum_{l=1}^{L}\\mathbf{1}[t_{l}=t]}\\end{array}$ and $\\hat{p}_{t}$ is the normalized attention score for token $t$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{L}(x)=\\bigg[{W_{E}}^{T}{W_{V}}\\mathrm{attn}(W_{E}\\chi(x))\\bigg]_{:L}}\\\\ &{\\qquad={W_{E}}^{T}{W_{V}}\\displaystyle\\sum_{l=1}^{L}\\frac{\\exp(u_{t_{l},t_{L}})}{Z}{W_{E}}(t_{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $Z$ is a normalizing constant. ", "page_idx": 23}, {"type": "text", "text": "Define $\\begin{array}{r}{\\hat{p}_{t_{l}}=\\frac{\\exp(u_{t_{l},t_{L}})}{Z}}\\end{array}$ exp(utl,tL). Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nf^{L}(x)={W_{E}}^{T}{W_{V}}\\sum_{l=1}^{L}{\\hat{p}}_{t_{l}}W_{E}(t_{l})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that if $t_{l}=t$ then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial\\hat{p}_{t_{l}}}{\\partial{u}_{t,t_{L}}}=\\hat{p}_{t_{l}}\\big(1-\\hat{p}_{t_{l}}\\big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Otherwise, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial\\hat{p}_{t_{l}}}{\\partial u_{t,t_{L}}}=-\\hat{p}_{t_{l}}\\hat{p}_{t}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By the chain rule, we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{u_{t,t_{L}}}\\ell(f^{L})=\\nabla\\ell(f^{L})^{T}(W_{E})^{T}W^{V}(\\sum_{l=1}^{L}{\\mathbf1}[t_{l}=t]\\hat{p}_{t_{l}}W_{E}(t)-\\sum_{l=1}^{L}\\hat{p}_{t_{l}}\\hat{p}_{t}W_{E}(t_{l}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{u_{t,t_{L}}}\\ell(f^{L})=\\nabla\\ell(f^{L})^{T}(W_{E})^{T}W^{V}(\\alpha_{t}\\hat{p}_{t}W_{E}(t)-\\hat{p}_{t}\\sum_{l=1}^{L}\\hat{p}_{t_{l}}W_{E}(t_{l}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{t}=\\sum_{l=1}^{L}\\mathbf{1}[t_{l}=t]}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "C Additional experiments \u2013 context hijacking ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we show the results of additional context hijacking experiments on the COUNTERFACT dataset [Men $+22$ ]. ", "page_idx": 24}, {"type": "text", "text": "Reverse context hijacking In Figure 2a, we saw the effects of hijacking by adding in \u201cDo not think of {target_false}.\u201d to each context. Now, we measure the effect of the reverse: What if we prepend \u201cDo not think of {target_true}.\u201d ? ", "page_idx": 24}, {"type": "text", "text": "Based on the study in this paper on how associative memory works in LLMs, we should expect the efficacy score to decrease. Indeed, this is what happens, as we see in Figure C.1. ", "page_idx": 24}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/ce52e057552f7b605e4b5d00f64d95e1cd9fab47c5f5b717af1c282edc0776e2.jpg", "img_caption": ["Figure C.1: Prepending \u2018Do not think of {target_true}.\u2019 can increase the chance of LLMs to output correct tokens. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset with the reverse context hijacking scheme. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Hijacking based on relation IDs We first give an example of each of the 4 relation IDs we hijack in Table 1. ", "page_idx": 24}, {"type": "table", "img_path": "WJ04ZX8txM/tmp/b4768e92595392df98d20a519f8832e6acd29f9a6a051bc440571b08a7972f94.jpg", "table_caption": ["Table 1: Examples of contexts in Relation IDs from COUNTERFACT "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "WJ04ZX8txM/tmp/9bf3c46b3180a537d1c064cad0899150f7b9fa031b9c15acb030d44b2f4e3c70.jpg", "table_caption": ["Table 2: Examples of hijack and reverse hijack formats based on Relation IDs "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Similar to Figure 2b, we repeat the hijacking experiments where we prepend factual sentences generated from the relation ID. We use the format illustrated in Table 2 for the prepended sentences. ", "page_idx": 24}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/637dfb3a1b7f3058300aeccb2280ccd2e131678a7f50c9e021f704fd81cc5d21.jpg", "img_caption": ["Figure C.2: Context hijacking based on relation IDs can result in LLMs output incorrect tokens. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset with hijacking scheme presented in Table 2. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "We experiment with 3 other relation IDs and we see similar trends for all the LLMs in Figure C.2a, C.2b, and C.2d. That is, the efficacy score rises for the first prepend and as we increase the number of prepends, the trend of ES rising continues. Therefore, this confirms our intuition that LLMs can be hijacked by contexts without changing the factual meaning. ", "page_idx": 25}, {"type": "text", "text": "Similar to Figure C.1, we experiment with reverse context hijacking where we give the answers based on relation IDs, as shown in Table 2. We again experiment with the same 4 relation IDs and the results are in Figure C.3a - C.3d. We see that the efficacy score decreases when we prepend the answer sentence, thereby verifying the observations of this study. ", "page_idx": 25}, {"type": "text", "text": "Hijacking without exact target words So far, the experiments use prompts that either contain true or false target words. It turns out, the inclusion of exact target words are not necessary. To see this, we experiment a variant of the generic hijacking and reverse hijacking experiments. But instead of saying \u201cDo not think of {target_false}\u201d or \u201cDo not think of {target_true}\u201d. We replace target words with words that are semantically close. Specifically, for relation P1412, we replace words representing language (e.g., \u201cFrench\u201d) with their associated country name (e.g., \u201cFrance\u201d). As shown in Figure C.4, context hijacking and reverse hijacing still work in this case. ", "page_idx": 25}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/03af0a37546d06573d6254f95ed3e5f468cea8b694ca18b65dc6af5fa4ba12a7.jpg", "img_caption": ["Figure C.3: Reverse context hijacking based on relation IDs can result in LLMs to be more likely to be correct. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset with the reverse hijacking scheme presented in Table 2. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/1d5c962bb87b5f257795c167ec69647367411c74c254e830688545d869f490f1.jpg", "img_caption": ["Figure C.4: Hijacking and reverse hijacking experiments on relation P1412 show that context hijacking does not require exact target word to appear in the context. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "D Additional experiments and figures \u2013 latent concept association ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this appendix section, we present additional experimental details and results from the synthetic experiments on latent concept association. ", "page_idx": 26}, {"type": "text", "text": "Experimental setup Synthetic data are generated following the model in Section 4.1. Unless otherwise stated, the default setup has $\\omega\\,=\\,0.5$ , $\\beta\\,=\\,1$ and ${\\bar{\\mathcal N}}(i)=V\\setminus\\{i\\}$ and $L=256$ . The default hidden dimension of the one-layer transformer is also set to be 256. The model is optimized using AdamW [LH17] where the learning rate is chosen from $\\{0.01,0.001\\}$ . The evaluation dataset is drawn from the same distribution as the training dataset and consists of $1024\\left(x,y\\right)$ pairs. Although theoretical results in Section 5 may freeze certain parts of the network for simplicity, in this section, unless otherwise specified, all layers of the transformers are trained jointly. Also, in this section, we typically report accuracy which is 1 \u2212error. ", "page_idx": 26}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/637c660aa0b3ad912b9e4304933f0c87764729037b7236b6242fd6aac43c4b7a.jpg", "img_caption": ["Figure D.1: Fixing the value matrix $W_{V}$ as the identity matrix results in lower accuracy compared to training $W_{V}$ , especially for smaller context length $L$ . The figure reports accuracy for both fixed and trained $W_{V}$ settings, with standard errors calculated over 10 runs. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "D.1 On the value matrix $W_{V}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide additional figures of Section 6.1. Specifically, Figure D.1 shows that fixing the value matrix to be the identity will negatively impact accuracy. Figure D.2 indicates that replacing trained value matrices with constructed ones can preserve accuracy to some extent. Figure D.3 suggests that trained value matrices and constructed ones share similar low-rank approximations. For the last two sets of experiments, we consider randomly constructed value matrix, where the outer product pairs are chosen randomly, defined formally as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{V}=\\sum_{i\\in[V]}W_{E}(i)(\\sum_{\\{j\\}\\sim\\mathrm{Unif}([V])^{|{\\cal N}_{1}(i)|}}W_{E}(j)^{T})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.2 On the embeddings ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This section provides additional figures from Section 6.2. Figure D.4 shows that in the underparameterized regime, embedding training is required. Figure D.5 indicates that the embedding structure in the underparameterized regime roughly follows (5.2). Finally Figure D.6 shows that, when the value matrix is fixed to the identity, the relationship between inner product of embeddings and their corresponding Hamming distance is mostly linear. ", "page_idx": 27}, {"type": "text", "text": "D.3 On the attention selection mechanism ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This section provides additional figures from Section 6.3. Figure D.7-D.8 show that attention mechanism selects tokens in the same cluster as the last token. In particular, for Figure D.8, we extend experiments to consider cluster structures that depend on the first two latent variables. In other words, for any latent vector $z^{*}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nN(z^{*})=\\{z:z_{1}^{*}=z_{1}\\mathrm{~and~}z_{2}^{*}=z_{2}\\}\\setminus\\{z^{*}\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.4 Spectrum of embeddings ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We display several plots of embedding spectra (Figure D.9, Figure D.10, Figure D.11, Figure D.12) that exhibit eigengaps between the top and bottom eigenvalues, suggesting low-rank structures. ", "page_idx": 27}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/2250105b71017982a3af59d4d19f17117f6227bfa345211336486ff7e2cb81ca.jpg", "img_caption": ["Figure D.2: When the value matrix is replaced with the constructed one in trained transformers, the accuracy does not significantly decrease compared to replacing the value matrix with randomly constructed ones. The graph reports accuracy under different embedding dimensions and standard errors are over 5 runs. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.5 Context hijacking in latent concept association ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we want to simulate context hijacking in the latent concept association model. To achieve that, we first sample two output tokens $\\bar{y}^{1}$ (true target) and $y^{2}$ (false target) and then generate contexts $\\boldsymbol{x}^{1}=(t_{1}^{1},...,t_{L}^{1})$ and $x^{2}=\\dot{(t_{1}^{2},...,t_{L}^{2})}$ from $p(x^{\\tilde{1}}|y^{\\tilde{1}})$ and $p(x^{2}|y^{2})$ . Then we mix the two contexts with rate $p_{m}$ . In other words, for the final mixed context $\\boldsymbol{x}=(t_{1},...,t_{L})$ , $t_{l}$ has probability $1-p_{m}$ to be $t_{l}^{1}$ and $p_{m}$ probability to be $t_{l}^{2}$ . Figure D.13 shows that, as the mixing rate increases from 0.0 to 1.0, the trained transformer tends to favor predicting false targets. This mirrors the phenomenon of context hijacking in LLMs. ", "page_idx": 28}, {"type": "text", "text": "D.6 On the context lengths ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As alluded in Section 5.5, the memory recall rate is closely related to the KL divergences between context conditional distributions. Because contexts contain mostly i.i.d samples, longer contexts imply larger divergences. This is empirically verified in Figure D.14 which demonstrates that longer context lengths can lead to higher accuracy. ", "page_idx": 28}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/72330c125960c992fadc6135234c429f8beeb8b9ef680153c9ed2ef4ca58118c.jpg", "img_caption": ["Figure D.3: The constructed value matrix $W_{V}$ has similar low rank approximation with the trained value matrix. The figure displays average smallest principal angles between low-rank approximations of trained value matrices and those of constructed, randomly constructed, and Gaussian-initialized value matrices. Standard errors are over 5 runs. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/ad355ed9e34f4a833026a2794c65d502a5bdcba036ab05925db60318ba17a072.jpg", "img_caption": ["Figure D.4: In the underparameterized regime $[d<V]$ , freezing embeddings to initializations causes a significant decrease in performance. The graph reports accuracy with different embedding dimensions and the standard errors are over 5 runs. Red lines indicate when $d=V$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/e33465c4a29db090e21686d2501ecc45289c44335b74f22ad647ecbae3cdcb45.jpg", "img_caption": ["Figure D.5: The relationship between inner products of embeddings and corresponding Hamming distances of tokens can be approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens. Standard errors are over 5 runs. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/a2030f72e70301d5e0a8341321c58b260b064a1345d5aa10c4e793c46dcf924e.jpg", "img_caption": ["Figure D.6: The relationship between inner products of embeddings and corresponding Hamming distances of tokens is mostly linear when the value matrix $W_{V}$ is fixed to be the identity. The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens. Standard errors are over 10 runs. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/c6bb927e55c3ec89aa693fab9ce379af58184db6d77e344199df516e4e1d57ef.jpg", "img_caption": ["Figure D.7: The attention patterns show the underlying cluster structure of the data generating process. Here, for any latent vector, we have ${\\mathcal{N}}(z^{*})=\\{z:z_{1}^{*}=z_{1}\\}\\setminus\\{z^{*}\\}$ . The figure shows attention score heat maps that are averaged over 10 runs. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/496f5fff85417fb9aa7a849f7c794193430319a54e1ad0cd92635b8670749acf.jpg", "img_caption": ["Figure D.8: The attention patterns show the underlying cluster structure of the data generating process. Here, for any latent vector, we have $\\mathcal{N}(z^{*})=\\{z:z_{1}^{*}=z_{1}$ and $z_{2}^{*}=z_{2}\\}\\setminus\\{z^{*}\\}$ . The figure shows attention score heat maps that are averaged over 10 runs. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/410bcdb71b977eaf285a4f57fa57d93ee0581025a77bc754d35a92d6f6153ca9.jpg", "img_caption": ["Figure D.9: The spectrum of embedding matrix $W_{E}$ has eigengaps between the top and bottom eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs. Number of latent variable $m$ is 7 and the embedding dimension is 32. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/b60bbe5bfcd66cf34055e0ad5023eed9796dc7fb963fc8ac983d07bc912d3b24.jpg", "img_caption": ["Figure D.10: The spectrum of embedding matrix $W_{E}$ has eigengaps between the top and bottom eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs. Number of latent variable $m$ is 7 and the embedding dimension is 64. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/a18cf18e5fa3199958fbd6d1cdbf4b001fbad41564848c05784bf7681264e201.jpg", "img_caption": ["Figure D.11: The spectrum of embedding matrix $W_{E}$ has eigengaps between the top and bottom eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs. Number of latent variable $m$ is 8 and the embedding dimension is 32. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/fabb3b2cd77c1ff65ccf465a96d7ac241ef583479e5b488862796725b64707d0.jpg", "img_caption": ["Figure D.12: The spectrum of embedding matrix $W_{E}$ has eigengaps between the top and bottom eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs. Number of latent variable $m$ is 8 and the embedding dimension is 64. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/ebae1e27d31d1ac8ed8600456b1c1e389c3f31d1572716cfa9270309d4fd2109.jpg", "img_caption": ["Figure D.13: Mixing contexts can cause misclassification. The figure reports accuracy for true target and false target under various context mixing rate. Standard errors are over 5 runs. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "WJ04ZX8txM/tmp/c5353c31aaa4314f7d4616d8a84a8acc3a543264b149daffbed4102578b94c23.jpg", "img_caption": ["Figure D.14: Increasing context lengths can improve accuracy. The figure reports accuracy across various context lengths and dimensions. Standard errors are over 5 runs. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Context hijacking is verified empirically. And the study on how single-layer transformers can solve latent concept association is done both empirically and theoretically. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Studying single-layer transformers is limited. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The assumptions are clearly stated in the theorem statements and proofs are given in the appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The experimental details of both LLM and synthetic experiments are provided in the paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Code is provided in the supplemental material. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: Experimental details are included in the paper. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: Standard errors are provided for the experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Synthetic experiments can just be done on CPUs. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: authors have reviewed the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [No] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper is mostly theoretical and has no societal impacts. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 44}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]