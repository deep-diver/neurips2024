[{"figure_path": "WJ04ZX8txM/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of context hijacking for various LLMs, showcasing that fact retrieval is not robust.", "description": "This figure displays four examples of how the outputs of various LLMs (GPT-2, Gemma-2B, Gemma-2B-IT, LLaMA-7B) can be manipulated by changing the context, even without altering the factual meaning.  The first example shows that all models correctly answer \"Paris\" when asked where the Eiffel Tower is.  However, by adding sentences like \"The Eiffel Tower is not in Chicago\", the models incorrectly answer \"Chicago\" in the following examples.  This demonstrates the vulnerability of LLMs to context manipulation, highlighting the non-robustness of their fact retrieval abilities.", "section": "3 Context hijacking in LLMs"}, {"figure_path": "WJ04ZX8txM/figures/figures_2_1.jpg", "caption": "Figure 2: Context hijacking can cause LLMs to output false target. The figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset under two hijacking schemes.", "description": "This figure shows the efficacy scores achieved when using different LLMs (openai-community/gpt2, google/gemma-2b, google/gemma-2b-it, meta-llama/Llama-2-7b-hf) on the COUNTERFACT dataset. Two hijacking schemes were used, one that generally hijacks the context and one based on a specific relation ID (P190). The x-axis represents the number of times a hijacking sentence is prepended to the context, and the y-axis represents the efficacy score (higher score indicates more successful hijacking). The results demonstrate the vulnerability of LLMs to context hijacking and how easily their factual outputs can be manipulated by cleverly changing contexts, even without altering the factual meanings of the original context.", "section": "3 Context hijacking in LLMs"}, {"figure_path": "WJ04ZX8txM/figures/figures_8_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure shows three plots that illustrate the key components of a single-layer transformer that solves the latent concept association problem. The first plot shows that training the value matrix (Wv) leads to higher accuracy than using a fixed identity matrix. The second plot demonstrates that the embedding structure, when trained in an underparameterized regime, closely approximates the theoretical relationship described by equation (5.2). The final plot displays the self-attention pattern in the network, illustrating its ability to select tokens within the same cluster based on the structure defined in section 5.4.  The results illustrate the collaborative role of the value matrix, embeddings, and attention mechanism in achieving high accuracy.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_24_1.jpg", "caption": "Figure C.1: Prepending 'Do not think of {target_true}.' can increase the chance of LLMs to output correct tokens. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset with the reverse context hijacking scheme.", "description": "This figure shows the efficacy score for several LLMs (openai-community/gpt2, google/gemma-2b, google/gemma-2b-it, meta-llama/Llama-2-7b-hf) as a function of the number of times the phrase \u201cDo not think of {target_true}\u201d is prepended to the prompt. The efficacy score measures the proportion of times the model outputs the correct token after modifying the context.  The results indicate that the reverse context hijacking strategy, in which the true target is mentioned in a negative context, surprisingly leads to an increase in efficacy score.  This implies a more nuanced relationship between context and fact retrieval in LLMs than a simple semantic understanding might suggest.", "section": "Additional experiments \u2013 context hijacking"}, {"figure_path": "WJ04ZX8txM/figures/figures_25_1.jpg", "caption": "Figure 1: Examples of context hijacking for various LLMs, showcasing that fact retrieval is not robust.", "description": "This figure demonstrates how easily LLMs can be manipulated to give incorrect answers simply by changing the context, even without altering the factual meaning of the original prompt.  It highlights that LLMs are heavily influenced by the tokens (words) in the prompt, and these tokens may serve as cues that lead LLMs to retrieve the wrong factual information from memory, rather than relying solely on the semantic meaning of the text.  The examples show how different models (GPT-2, Gemma, LLaMA) respond differently to subtly altered prompts related to the Eiffel Tower's location.", "section": "3 Context hijacking in LLMs"}, {"figure_path": "WJ04ZX8txM/figures/figures_26_1.jpg", "caption": "Figure C.3: Reverse context hijacking based on relation IDs can result in LLMs to be more likely to be correct. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset with the reverse hijacking scheme presented in Table 2.", "description": "This figure shows the efficacy score for various LLMs across different numbers of prepends when using a reverse context hijacking scheme.  The reverse scheme involves prepending sentences that contain the true target answer to the original context.  The results demonstrate that the efficacy score decreases as more sentences are prepended, indicating that the model becomes less susceptible to manipulation by misleading contextual information.", "section": "C Additional experiments - context hijacking"}, {"figure_path": "WJ04ZX8txM/figures/figures_26_2.jpg", "caption": "Figure C.4: Hijacking and reverse hijacking experiments on relation P1412 show that context hijacking does not require exact target word to appear in the context. This figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset.", "description": "This figure shows the efficacy score for four different LLMs (openai-community/gpt2, google/gemma-2b, google/gemma-2b-it, meta-llama/Llama-2-7b-hf) when performing context hijacking and reverse context hijacking experiments on relation P1412 of the COUNTERFACT dataset.  In context hijacking, additional sentences are added to the prompt to mislead the LLM into providing an incorrect answer, while in reverse context hijacking, sentences that reinforce the correct answer are added.  The x-axis represents the number of sentences added (prepends), while the y-axis shows the efficacy score, indicating the percentage of times the LLM was successfully manipulated. This experiment demonstrates that even without using the exact target words in the additional sentences, the manipulation is still effective.  The results show that with more added sentences, the efficacy score improves for hijacking and declines for reverse hijacking.", "section": "Additional experiments \u2013 context hijacking"}, {"figure_path": "WJ04ZX8txM/figures/figures_27_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure demonstrates the importance of training the value matrix and the embedding structure for achieving high accuracy in latent concept association tasks. It also shows how attention mechanisms are used to select relevant tokens for the task, and highlights the relationship between the inner product of embeddings and Hamming distance.", "section": "6 Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_28_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure shows the results of experiments on a single-layer transformer model trained on the latent concept association task.  Panel (a) demonstrates the importance of training the value matrix (Wv) for high accuracy, as opposed to using a fixed identity matrix. Panel (b) illustrates the relationship between the inner product of word embeddings and their Hamming distance, showing an approximation to equation 5.2 in the paper. Panel (c) visualizes the attention mechanism and shows that it tends to select tokens from the same semantic cluster, supporting the theoretical analysis.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_29_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure demonstrates the interplay of different components of a single-layer transformer network in solving the latent concept association problem.  Panel (a) shows that training the value matrix (Wv) leads to higher accuracy than using a fixed identity matrix. Panel (b) illustrates the relationship between the inner product of word embeddings and their Hamming distance, showing an approximation to the theoretical formula (5.2). Finally, panel (c) visualizes the self-attention mechanism's ability to select relevant tokens within the same cluster, highlighting its role in information aggregation.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_30_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure shows three plots that illustrate the key components of a single-layer transformer network solving the latent concept association problem. Plot (a) compares the accuracy of models with a fixed identity value matrix and those with a trained value matrix, demonstrating the importance of training the value matrix. Plot (b) shows the relationship between the inner product of embeddings and Hamming distance in the underparameterized regime, indicating that the embedding structure approximates a specific mathematical form.  Plot (c) depicts the attention pattern within the network, highlighting the ability of the self-attention layer to select tokens from the same cluster, a key mechanism for solving the task.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_30_2.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure demonstrates the importance of the value matrix in the transformer model.  It also shows the relationship between embedding structure, Hamming distance and the ability of the self-attention layer to select relevant tokens.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_31_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure visualizes three key aspects of the single-layer transformer model's performance on the latent concept association task.  Panel (a) compares the accuracy of models with a fixed identity value matrix versus a trained value matrix, showing that training significantly improves accuracy.  Panel (b) illustrates the relationship between the inner product of word embeddings and their Hamming distance, which aligns with the theoretical prediction (5.2) in the paper, indicating low-rank structure in the embeddings. Finally, panel (c) presents the average attention scores, demonstrating the model's ability to focus attention on tokens within the same semantic cluster. ", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_32_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure demonstrates the importance of the value matrix in achieving high accuracy in the latent concept association task. It also shows how the trained transformer learns an embedding space that captures the latent relationship between tokens (approximated by equation 5.2), and that the self-attention mechanism helps to select relevant tokens within the same cluster. The results support the paper's theoretical analysis of the single-layer transformer's behavior.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_33_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure shows three plots that demonstrate the importance of the value matrix, the embedding structure, and the self-attention mechanism in the single-layer transformer model for solving the latent concept association problem. Plot (a) compares the accuracy of using a fixed identity value matrix and a trained value matrix, showing that training leads to better performance. Plot (b) illustrates the relationship between the inner product of trained embeddings and their Hamming distances, indicating that the embedding structure approximates the relationship defined in equation (5.2). Plot (c) visualizes the attention pattern of the model, revealing its ability to select tokens within the same cluster which reflects the underlying cluster structure of the data generation process.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_34_1.jpg", "caption": "Figure 1: Examples of context hijacking for various LLMs, showcasing that fact retrieval is not robust.", "description": "This figure shows several examples of how different LLMs (GPT-2, Gemma-2B, Gemma-2B-IT, and LLaMA-7B) respond differently when prompted with slightly different phrasing of the same question.  It highlights how easily LLMs' factual responses can be manipulated through minor contextual changes, which is referred to as \"context hijacking\". The examples demonstrate that LLMs may not retrieve facts robustly based on semantic meaning alone, but rather rely on specific tokens within the context.", "section": "3 Context hijacking in LLMs"}, {"figure_path": "WJ04ZX8txM/figures/figures_35_1.jpg", "caption": "Figure 1: Examples of context hijacking for various LLMs, showcasing that fact retrieval is not robust.", "description": "This figure demonstrates how changing the context in prompts can easily manipulate the outputs of LLMs in a fact retrieval task, even without altering the factual meaning.  It shows examples for different LLMs (GPT-2, Gemma-2B, Gemma-2B-IT, LLaMA-7B) where providing additional, seemingly unrelated, information in the prompt leads to incorrect answers. This highlights that LLMs are susceptible to context hijacking and suggests that their fact retrieval mechanism is based on associations rather than semantic understanding.", "section": "3 Context hijacking in LLMs"}, {"figure_path": "WJ04ZX8txM/figures/figures_36_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure visualizes the interplay of key components (value matrix, embeddings, attention) in a single-layer transformer trained for the latent concept association task.  Panel (a) compares the accuracy of using a trained value matrix versus a fixed identity matrix, demonstrating the importance of learning the value matrix. Panel (b) shows the relationship between the inner product of embeddings and their Hamming distance, confirming the embedding structure derived in the theory. Panel (c) illustrates the attention mechanism's ability to select tokens within the same cluster, enhancing the understanding of information aggregation in the task.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_37_1.jpg", "caption": "Figure 3: Key components of the single-layer transformer working together on the latent concept association problem. (a) Fixing the value matrix Wv as the identity matrix results in lower accuracy compared to training Wv. The figure reports average accuracy for both fixed and trained Wv with L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated by (5.2). The graph displays the average inner product between embeddings of two tokens against the corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can select tokens within the same cluster. The figure shows average attention score heat map with m = 8 and the cluster structure from Section 5.4.", "description": "This figure shows the results of experiments on a synthetic latent concept association task using a single-layer transformer model.  It demonstrates the importance of training the value matrix (a), the low-rank structure of the learned embedding space (b), and the role of the self-attention mechanism in selecting relevant information based on latent clusters (c) for successful task completion.", "section": "Experiments"}, {"figure_path": "WJ04ZX8txM/figures/figures_38_1.jpg", "caption": "Figure 2: Context hijacking can cause LLMs to output false target. The figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset under two hijacking schemes.", "description": "This figure shows the efficacy score (a measure of how well the context hijacking worked) plotted against the number of times a certain phrase was prepended to the original prompt.  Two different hijacking schemes are compared: one where a generic phrase is added ('Do not think of {target_false}') and another using a relation-specific sentence. The graph demonstrates how the success of context hijacking increases as more phrases are prepended to the prompt, for four different large language models (LLMs). This illustrates the lack of robustness of fact retrieval in LLMs and how easily their outputs can be manipulated by changing the context.", "section": "3 Context hijacking in LLMs"}, {"figure_path": "WJ04ZX8txM/figures/figures_39_1.jpg", "caption": "Figure 2: Context hijacking can cause LLMs to output false target. The figure shows efficacy score versus the number of prepends for various LLMs on the COUNTERFACT dataset under two hijacking schemes.", "description": "This figure shows the efficacy score of two different context hijacking methods on four different LLMs.  The x-axis represents the number of times a hijacking sentence was prepended to the prompt. The y-axis shows the efficacy score, representing how often the LLM was successfully tricked into giving a wrong answer. The two hijacking schemes are: generic hijacking (prepending \"Do not think of {target_false}\") and relation ID-based hijacking (prepending factually correct sentences related to the false target). The figure demonstrates that increasing the number of prepended sentences generally increases the efficacy score, indicating that LLMs' outputs can be easily manipulated by modifying the context.", "section": "3 Context hijacking in LLMs"}]