[{"figure_path": "AXcYtHQnxt/figures/figures_0_1.jpg", "caption": "Figure 1: Our Proposed Cross-view Adaptation Learning Approach. Prior models, e.g., FreeSeg [38], DenseCLIP [40], trained on the car view do not perform well on the drone-view images. Meanwhile, our cross-view adaptation approach is able to generalize well from the car to drone view.", "description": "This figure illustrates the core idea of the paper, which is to improve the generalization of open-vocabulary semantic segmentation models across different camera views (car view vs. drone view).  It shows that existing models trained on one view perform poorly when tested on the other view. The paper's proposed approach, EAGLE, is shown to successfully adapt between the two views.", "section": "1 Introduction"}, {"figure_path": "AXcYtHQnxt/figures/figures_1_1.jpg", "caption": "Figure 2: An Example of Illustration of Cross-View Adaptation From Car View to Drone View.", "description": "This figure shows an example of cross-view adaptation from a car view to a drone view.  The input image from the car view is shown, along with its ground truth segmentation and the model's prediction. Then, an arrow indicates the cross-view adaptation process. After that, the input image from the drone view and the model's prediction on this image are presented. This illustrates the challenge of adapting a model trained on one viewpoint to another, which is addressed by the proposed method in the paper.", "section": "1 Introduction"}, {"figure_path": "AXcYtHQnxt/figures/figures_4_1.jpg", "caption": "Figure 3: Our Cross-View Learning Framework.", "description": "This figure illustrates the EAGLE approach's framework for cross-view adaptation learning. It shows how the model utilizes pretrained CLIP text encoders to generate textual features from prompts specifying objects to find ('car, person, tree') from both car and drone views. These features, along with image data from both views, are fed into separate encoders and decoders. A key element is the use of geodesic flow to model the geometric structural changes between the two views in both image and segmentation spaces. This allows the model to effectively adapt from the source (car) view to the target (drone) view, addressing the challenge of cross-view adaptation in semantic scene understanding.", "section": "3 The Proposed EAGLE Approach"}, {"figure_path": "AXcYtHQnxt/figures/figures_7_1.jpg", "caption": "Figure 4: The Qualitative Results of Cross-View Adaptation (Without Prompt).", "description": "This figure shows a comparison of qualitative results for cross-view adaptation with and without the proposed method on the UAVID dataset.  The left-hand side shows images from the source domain (car view) and the corresponding segmentation results using four methods: input image, results without cross-view adaptation, results with the proposed cross-view adaptation, results from CROVIA, and results from ProDA. The right-hand side shows the same comparison for images from the target domain (drone view). The results demonstrate the effectiveness of the proposed method in improving the quality of cross-view semantic segmentation.", "section": "4 Experiments"}, {"figure_path": "AXcYtHQnxt/figures/figures_8_1.jpg", "caption": "Figure 4: The Qualitative Results of Cross-View Adaptation (Without Prompt).", "description": "This figure shows the qualitative results of cross-view adaptation without prompting. It compares the results of applying the proposed cross-view adaptation method to the input images with the results of not using cross-view adaptation.  The results are displayed in pairs: the left image shows the input, the middle shows the segmentation result without cross-view adaptation, and the right shows the results with cross-view adaptation. This visual comparison demonstrates the effectiveness of the proposed method in improving the quality of segmentation results.", "section": "4 Experiments"}, {"figure_path": "AXcYtHQnxt/figures/figures_9_1.jpg", "caption": "Figure 1: Our Proposed Cross-view Adaptation Learning Approach. Prior models, e.g., FreeSeg [38], DenseCLIP [40], trained on the car view do not perform well on the drone-view images. Meanwhile, our cross-view adaptation approach is able to generalize well from the car to drone view.", "description": "This figure shows a comparison of different approaches to cross-view adaptation for semantic segmentation. The top row shows the input images from a car view, the predictions from a model trained only on car view images, and the predictions from the proposed EAGLE model. The bottom row shows the same comparison but for drone view images. The figure highlights that the proposed EAGLE model is able to generalize better to unseen drone view images compared to models trained only on car view images.", "section": "1 Introduction"}, {"figure_path": "AXcYtHQnxt/figures/figures_16_1.jpg", "caption": "Figure 1: Our Proposed Cross-view Adaptation Learning Approach. Prior models, e.g., FreeSeg [38], DenseCLIP [40], trained on the car view do not perform well on the drone-view images. Meanwhile, our cross-view adaptation approach is able to generalize well from the car to drone view.", "description": "This figure illustrates the core idea of the paper, which is to adapt a semantic segmentation model trained on images from a car's perspective to perform well on images taken from a drone.  It shows that previous methods (FreeSeg and DenseCLIP) failed to generalize across viewpoints, while the proposed EAGLE method successfully transfers knowledge from the car view to the drone view.", "section": "1 Introduction"}, {"figure_path": "AXcYtHQnxt/figures/figures_17_1.jpg", "caption": "Figure 8: The Feature Distribution of Classes in SYNTHIA \u2192 UAVID Experiments.", "description": "This figure visualizes the feature distributions of different classes in the SYNTHIA to UAVID experiments, comparing the results with and without cross-view adaptation.  It shows the impact of the cross-view adaptation approach on the separation and clustering of features for each class.  The visualization helps to understand how well the model is able to distinguish between different semantic categories, before and after applying the proposed method.", "section": "4.2 Ablation Study"}]