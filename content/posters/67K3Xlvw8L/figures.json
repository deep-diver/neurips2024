[{"figure_path": "67K3Xlvw8L/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of alignment for honesty. Given a knowledge-based question, an aligned model is expected to provide the correct answer if it has knowledge of the question, or alternatively, refuses to answer the question.", "description": "This figure shows an example of how alignment for honesty works. Before alignment, the model incorrectly answers a question it doesn't know the answer to. After alignment, the model correctly answers the question it knows the answer to and accurately admits its lack of knowledge for the question it doesn't know.", "section": "1 Introduction"}, {"figure_path": "67K3Xlvw8L/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Illustration of iterative alignment. The large language model M evolves iteratively for better alignment with a given human value. (b) Decision boundary for \"harmless\", which is commonly defined by human \"8\". (c) Decision boundary for \"known\", which is usually determined by model \"\".", "description": "This figure illustrates the iterative alignment process for a given value.  Panel (a) shows how a large language model (LLM) iteratively improves its alignment with human values. Panel (b) depicts a decision boundary for classifying responses as \"harmless\" or \"harmful\", where the \"8\" represents human judgment. Panel (c) displays a decision boundary separating model responses into \"known\" and \"unknown\", based on the model's internal knowledge.", "section": "2.1 LLM Alignment"}, {"figure_path": "67K3Xlvw8L/figures/figures_5_1.jpg", "caption": "Figure 3: Overview of our proposed honesty-oriented fine-tuning methods. \u201cExpected accuracy = 0.3\u201d indicates that out of 10 sampled responses, there are 3 correct responses and 7 wrong responses. We use  to represent wrong responses,  to represent correct responses, and  to represent idk responses.", "description": "This figure illustrates the three different methods used for honesty-oriented fine-tuning: ABSOLUTE, CONFIDENCE, and MULTISAMPLE. Each method uses a different strategy for annotating training samples based on the model's confidence in its response. The example question is \"What was the name of the dwarf who is a chief character in 'Lord of the Rings'?\", where the expected accuracy is 0.3, implying a mix of correct, incorrect, and \"I don't know\" responses.", "section": "3.2 Supervised Fine-tuning"}, {"figure_path": "67K3Xlvw8L/figures/figures_18_1.jpg", "caption": "Figure 1: Illustration of alignment for honesty. Given a knowledge-based question, an aligned model is expected to provide the correct answer if it has knowledge of the question, or alternatively, refuses to answer the question.", "description": "The figure shows an example of how the alignment for honesty works. Before alignment, the model incorrectly answers a question about the authors of a paper. After alignment, the model correctly answers another question, but when it doesn't know the answer to a question, it explicitly states that it doesn't know.", "section": "1 Introduction"}, {"figure_path": "67K3Xlvw8L/figures/figures_22_1.jpg", "caption": "Figure 4: The effect of refusal threshold \u03c4.", "description": "The figure shows the relationship between the prudence score and over-conservativeness score with varying refusal thresholds (\u03c4). As the refusal threshold increases, the model becomes more reliable but also more conservative.  This illustrates the tradeoff between honesty and cautiousness when adjusting the threshold for refusing to answer.", "section": "D.5 Analyses"}]