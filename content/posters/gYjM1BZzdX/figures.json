[{"figure_path": "gYjM1BZzdX/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the Vietoris-Rips filtration on a point cloud in R<sup>d</sup>, focusing on one-dimensional topological features (loops). When the filtration parameter t increases, loops appear and disappear in the filtration. These values are accounted in the resulting persistence diagram (right).", "description": "This figure illustrates the Vietoris-Rips filtration process on a point cloud.  It shows how, as a parameter 't' increases, circles of increasing radii are formed around each point.  Simplices (lines, triangles, etc.) are added to a simplicial complex whenever the points are within a distance 't' of each other. The evolution of topological features (loops in this case) as 't' changes is shown in the sequence of diagrams.  Finally, a persistence diagram summarizes the birth and death times of these topological features, providing quantitative topological information.", "section": "2 Background"}, {"figure_path": "gYjM1BZzdX/figures/figures_4_1.jpg", "caption": "Figure 2: (blue) A point cloud X, and (black) the negative gradient -\u2207L(X) of a simplification loss which aims at destroying the loop by collapsing the circle (reduce the loop's death time) and tearing it (increase the birth time). While \u2207L(X) only affects four points in X, the diffeomorphic interpolation \u1fe6(X) (orange, \u03c3 = 0.1) is defined on R<sup>d</sup>, hence extends smoothly to other points in X.", "description": "This figure illustrates the effect of diffeomorphic interpolation on a point cloud. The black arrows show the sparse gradient of a topological loss function, which only affects a few points. The orange arrows, generated by the diffeomorphic interpolation technique, show a smoother vector field extending the gradient to all points, enabling a more efficient optimization.", "section": "3 Diffeomorphic interpolation of the vanilla topological gradient"}, {"figure_path": "gYjM1BZzdX/figures/figures_6_1.jpg", "caption": "Figure 3: Showcase of the usefulness of subsampling combined with diffeomorphic interpolations to minimize a topological simplification loss, with parameters \u03bb = 0.1, s = 50, n = 500. (a) Initial point cloud X (blue), subsample X' (red), vanilla topological gradient on the subsample (black) and corresponding diffeomorphic interpolation (orange). (b) and (c), the point cloud Xt after running t = 100 and t = 500 steps of vanilla gradient descent. (d) the point cloud Xt after running t = 100 steps of diffeomorphic gradient descent.", "description": "This figure showcases the benefits of combining subsampling with diffeomorphic interpolation for topological optimization. It compares the vanilla gradient descent approach (sparse updates) with the proposed approach (dense updates) using a topological simplification loss.  The figure illustrates how diffeomorphic interpolation significantly improves the efficiency of the optimization process.", "section": "Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_7_1.jpg", "caption": "Figure 4: (Top) From left to right: initial point cloud, and final point cloud for the different flows. (Bottom) Evolution of the losses with respect to the number of iterations and with respect to running time.", "description": "This figure compares four different methods for topological optimization: vanilla gradient descent, diffeomorphic interpolation, Oineus, and a combination of Oineus and diffeomorphic interpolation. The top row shows the initial and final point clouds for each method, illustrating the different ways they modify the point cloud to reduce the topological loss. The bottom row shows plots of the loss over iterations and over time for each method. The results demonstrate that the diffeomorphic interpolation and Oineus methods significantly outperform the vanilla gradient descent method in terms of loss reduction, and that combining Oineus with diffeomorphic interpolation yields the fastest convergence.", "section": "Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_7_2.jpg", "caption": "Figure 5: From left to right: initial Stanford bunny X0, the point cloud after 1, 000 epochs of vanilla topological gradient descent (barely any changes), the point cloud after 200 epochs of diffeomorphic gradient descent, after 1,000 epochs, and eventually the evolution of losses for both methods over iterations.", "description": "This figure compares the vanilla topological gradient descent and the diffeomorphic gradient descent methods on the Stanford Bunny dataset for topological augmentation.  The left side shows the initial bunny point cloud and the results after different numbers of epochs (iterations) for both methods.  The right side shows the loss evolution over the iterations.  The vanilla gradient descent shows minimal changes even after 1000 epochs, while the diffeomorphic gradient descent achieves significant changes in a much shorter timeframe.", "section": "Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_8_1.jpg", "caption": "Figure 7: COIL images, their corresponding initial LSs in blue and final LSs obtained with diffeomorphic gradient descent in orange, and the corresponding topological losses, for both vase (left) and duck (right).", "description": "This figure shows the results of applying the proposed diffeomorphic interpolation method to the latent spaces of a variational autoencoder trained on COIL images (rotating objects).  The left side shows the initial latent space (LS) for the vase image in blue, and how it transforms after gradient descent using the diffeomorphic approach in orange. The corresponding topological loss reduction is also plotted in a graph. The right side shows the same process but for the duck image. The method aims to improve the topological properties of the latent space by making it more circular, reflecting the circular motion of the objects.", "section": "Black-box autoencoder models"}, {"figure_path": "gYjM1BZzdX/figures/figures_8_2.jpg", "caption": "Figure 7: COIL images, their corresponding initial LSs in blue and final LSs obtained with diffeomorphic gradient descent in orange, and the corresponding topological losses, for both vase (left) and duck (right).", "description": "This figure shows the results of applying diffeomorphic interpolation to the latent spaces (LSs) of a variational autoencoder (VAE) trained on the COIL dataset. The initial LSs (blue) are compared to the final LSs (orange) obtained after applying the diffeomorphic gradient descent.  The images show how the algorithm modifies the latent space to improve its topological properties, aiming for a circular structure, as indicated by the topological loss values shown for both the vase and duck objects.", "section": "Black-box autoencoder models"}, {"figure_path": "gYjM1BZzdX/figures/figures_14_1.jpg", "caption": "Figure 8: Topological optimization of an initial point cloud X (in red) by minimizing X \u2192 \u03a3(b,d)\u2208Dgm(X) |d|\u00b2 + x\u2208X dist(x, [\u22121, 1]\u00b2). This loss favors the apparition of topological features (loops) while the regularization term penalizes points that would go to infinity otherwise. Experiment reproduced following the setting of [4], using code available at https://github.com/GUDHI/TDA-tutorial/blob/master/Tuto-GUDHI-optimization.ipynb.", "description": "This figure shows the result of topological optimization on an initial point cloud.  The objective function used encourages the formation of loops (1-dimensional topological features) while regularizing to prevent points from moving to infinity. The three subplots show (1) the initial and final point cloud, (2) the initial and final persistence diagrams, and (3) the loss function's evolution over iterations.  This experiment was replicated from a previously published work, with code provided for reproducibility.", "section": "Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_15_1.jpg", "caption": "Figure 9: Topological optimization with subsampling. From left to right, the initial point cloud X0, the point cloud after 750 steps of vanilla gradient descent (+subsampling), the point cloud after 750 steps of diffeomorphic interpolation gradient descent (+subsampling), loss evolution over epochs. Parameters: \u03bb = 0.1, \u03c3 = 0.1.", "description": "This figure shows the results of topological optimization using both vanilla gradient descent and the proposed diffeomorphic interpolation method, with subsampling applied to handle large datasets. The left three panels display the initial point cloud and the results after 750 epochs of vanilla gradient descent and diffeomorphic interpolation, respectively.  The rightmost panel shows the evolution of the loss function over the 750 epochs, clearly demonstrating the faster convergence and lower final loss achieved by the diffeomorphic interpolation method.  Subsampling is a crucial element, allowing the algorithm to scale to larger datasets.", "section": "Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_15_2.jpg", "caption": "Figure 10: Topologically-optimized LSs and losses for duck, cat, pig, vase and teapot.", "description": "This figure shows the results of applying the proposed diffeomorphic interpolation method to latent spaces (LSs) of a variational autoencoder (VAE) trained on the COIL dataset. The top row displays the original images of five different objects (duck, cat, pig, vase, and teapot). The middle row shows the latent spaces obtained after applying the proposed method (orange dots) and the original latent spaces (blue dots). The bottom row shows the evolution of the topological loss during the optimization process for each object. The results demonstrate that the proposed method effectively improves the topological properties of the latent spaces.", "section": "4 Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_16_1.jpg", "caption": "Figure 4: (Top) From left to right: initial point cloud, and final point cloud for the different flows. (Bottom) Evolution of the losses with respect to the number of iterations and with respect to running time.", "description": "The figure shows the result of applying different gradient descent methods to minimize a topological loss function. The top row displays the initial point cloud and the final point cloud obtained after applying vanilla gradient descent, diffeomorphic interpolation, Oineus algorithm, and a combination of Oineus and diffeomorphic interpolation. The bottom row shows the evolution of the loss function with respect to the number of iterations and the running time for each method. The results demonstrate that the diffeomorphic interpolation and Oineus methods converge faster and achieve lower loss values compared to vanilla gradient descent, and the combination of Oineus and diffeomorphic interpolation shows the best performance. The visualizations help to compare different gradient descent approaches for topological optimization and assess their effectiveness.", "section": "Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_17_1.jpg", "caption": "Figure 12: Topological simplification, point cloud of diameter 2 with median pairwise distance ~ \u221a2. Median and 10\u201390 percentiles over 50 runs. (Left) Time to converge for different values of \u03c3\u2208 [0,5] (\u03c3 = 0 corresponds to Vanilla). (Right) #iterations to converge (or stop after 200 iterations, indicated by the dashed red line).", "description": "This figure shows the impact of the bandwidth (\u03c3) parameter on the convergence speed of the diffeomorphic interpolation method proposed in the paper.  The left panel displays the total running time to converge (or reach the maximum iteration limit of 200), while the right panel displays the number of iterations required for convergence.  Both panels show median values and 10th and 90th percentiles from 50 runs of the algorithm.  The results demonstrate that a value of \u03c3 around 0.3 provides a good balance between convergence speed and the number of iterations, while extremely small or large values of \u03c3 hinder convergence.", "section": "4 Numerical experiments"}, {"figure_path": "gYjM1BZzdX/figures/figures_17_2.jpg", "caption": "Figure 13: Influence of the kernel bandwidth \u03c3 on correlation scores for a few datasets. The values of \u03c3 are evenly spaced between 0.05 and 1 for the COIL datasets, and between 0.025 and 0.5 for the scHiC dataset.", "description": "This figure shows how the correlation scores between latent space angles and repli scores vary with different kernel bandwidths (\u03c3) for several datasets (Cat, Pig, Vase, and scHiC).  The x-axis represents the kernel bandwidth, and the y-axis shows the correlation scores. The plot reveals oscillations in the correlation scores for very small or large bandwidths, suggesting more stable scores in the middle range. These oscillations might also stem from how the correlation scores are calculated.", "section": "4 Numerical experiments"}]