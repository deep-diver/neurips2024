[{"figure_path": "jdCMwF06c6/figures/figures_1_1.jpg", "caption": "Figure 1: Long-tailed backdoor learning. (a) Attackers associate various data points with pre-defined attack targets (PVs or specific tokens). (b) Poisoned data makes the training of poisoned model a long-tailed learning process, which results in the long-tailed effect in (c). (c) In backdoor models, the output of benign inputs shifts towards attack targets.", "description": "This figure illustrates the concept of long-tailed backdoor learning.  Panel (a) shows how attackers associate data points with predefined attack targets (either pre-defined vectors or specific tokens). Panel (b) demonstrates that poisoned data turns the model training into a long-tailed learning process, resulting in a long-tailed effect shown in (c). Finally, (c) visualizes how, in backdoored models, the model's output for benign inputs shifts toward the attack targets, highlighting the feature space shift due to the backdoor.", "section": "1 Introduction"}, {"figure_path": "jdCMwF06c6/figures/figures_4_1.jpg", "caption": "Figure 2: The workflow of LT-Defense. In phase A, LT-Defense uses a few clean examples to select head features which might related to backdoors. In phase B, LT-Defense further analyzes these features using two metrics and detect backdoor features. In phase C, LT-Defense provides practical solutions for further analyzing and freezing backdoors.", "description": "This figure illustrates the three phases of the LT-Defense model.  Phase A involves selecting head features using clean examples; Phase B uses two metrics (Head-Feature Rate and Abnormal Token Score) to identify backdoor features among those head features; and Phase C uses the identified backdoor features for backdoor freezing and attack target prediction.  A running example demonstrates the process using AutoPoison, showing how the model identifies and addresses backdoors in the context of natural language processing.", "section": "4 LT-Defense"}, {"figure_path": "jdCMwF06c6/figures/figures_7_1.jpg", "caption": "Figure 3: Detection accuracy with different test sizes and datasets on RoBERTa-base and RoBERTa-large.", "description": "This figure displays the detection accuracy of LT-Defense under different test set sizes and datasets.  The x-axis represents the number of test examples used, and the y-axis represents the Head-Feature Rate (HFR).  Separate plots are shown for RoBERTa-base and RoBERTa-large models, each tested on two datasets (WikiText and RTE).  The different colored lines represent different attack methods (BTOP, NeuBad, POR).  The shaded green region highlights the range where the HFR values start to stabilize and show clear differences between benign and poisoned models, suggesting that around 500 test examples are sufficient for reliable detection.", "section": "5.4 Ablation Study"}, {"figure_path": "jdCMwF06c6/figures/figures_7_2.jpg", "caption": "Figure 4: Detection accuracy with varying number of triggers and different PVs on ROBERTa-base.", "description": "This figure shows the results of an ablation study on the effect of varying the number of triggers and the type of PVs on the detection accuracy of LT-Defense using the ROBERTa-base model.  The figure consists of two sets of histograms. The top set shows the Head-Feature Rate (HFR) distribution for different numbers of triggers (1-6) using three different attack methods (POR, BTOP, NeuBA).  The bottom set shows the HFR distribution for different PVs (1-6) for the same three attack methods.  The histograms illustrate how the distribution of HFR values varies depending on the attack parameters, providing insights into the robustness and sensitivity of LT-Defense to different attack configurations.", "section": "5.4 Ablation Study"}, {"figure_path": "jdCMwF06c6/figures/figures_8_1.jpg", "caption": "Figure 5: Detection accuracy with different test sizes and datasets against task-ralted attacks.", "description": "This figure shows how the maximum abnormal token score changes as the number of test examples increases for different models and datasets with task-related backdoor attacks. It demonstrates that the accuracy of LT-Defense in detecting task-related backdoors stabilizes with sufficient test examples. The figure helps to illustrate the robustness and effectiveness of the proposed method against various datasets and attack types.", "section": "5.4 Ablation Study"}, {"figure_path": "jdCMwF06c6/figures/figures_8_2.jpg", "caption": "Figure 6: Adaptive attack against LT-Defense. (a) Reducing poisoned features of PVs. (b) Increasing the variance of clean features. (c) Reducing the logits of target tokens when inputting clean examples.", "description": "This figure shows the results of three adaptive attacks against LT-Defense.  LT-Defense leverages the long-tailed effect of backdoors in poisoned models to detect backdoors without searching for triggers.  These adaptive attacks aim to circumvent LT-Defense's detection by modifying features in a way that reduces the long-tailed effect.  (a) shows an attack where the poisoned feature rate is decreased to reduce the impact of poisoned data. (b) shows an attack where the regularization parameter is increased to increase the variance of clean features, making it harder to distinguish poisoned and clean data. (c) shows an attack where the logits of target tokens are reduced when clean examples are used, thus reducing the long-tailed effect on the target tokens.", "section": "5.5 Resistance to Adaptive Attacks"}, {"figure_path": "jdCMwF06c6/figures/figures_13_1.jpg", "caption": "Figure 7: A running example of the HFR-based backdoor detection. The two used models are benign and backdoored (by BToP [27]) RoBERTa-large [15] models, respectively.", "description": "This figure visually demonstrates the Head-Feature Rate (HFR) calculation for both benign and backdoored models.  It uses a heatmap to represent the activation of output features for 500 test samples. Brighter colors indicate higher activation frequency. The benign model shows a relatively even distribution of activation, while the backdoored model exhibits a significantly skewed distribution, with a large portion of features showing consistently high activation, representing the long-tailed effect.", "section": "4.2 Backdoor Feature Detection"}]