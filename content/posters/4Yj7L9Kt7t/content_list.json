[{"type": "text", "text": "Taming Heavy-Tailed Losses in Adversarial Bandits and the Best-of-Both-Worlds Setting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Duo Cheng Xingyu Zhou Bo Ji Virginia Tech Wayne State University Virginia Tech duocheng@vt.edu xingyu.zhou@wayne.edu boji@vt.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study the multi-armed bandits problem in the best-of-both-worlds (BOBW) setting with heavy-tailed losses, where the losses can be negative and unbounded but have $(1+v)$ -th raw moments bounded by $u^{1+v}$ for some known $u\\,>\\,0$ and $v\\,\\in\\,(0,1]$ . Specifically, we consider the BOBW setting where the underlying environment can be either (oblivious) adversarial (i.e., the loss distribution can change arbitrarily over time) or stochastic (i.e., the loss distribution is fixed over time), which is unknown to the decision-maker a prior. We propose an algorithm and prove that it achieves a T1+v -type worst-case (pseudo-)regret in the adversarial regime and a $\\log T$ -type gap-dependent regret in the stochastic regime, where $T$ is the time horizon. Compared to the state-of-the-art results, our algorithm offers stronger high-probability regret guarantees (vs. expected regret guarantees), and more importantly, relaxes a strong technical assumption on the loss distribution, which is generally hard to verify in practice. As a byproduct, relaxing this assumption leads to the first near-optimal regret result for heavy-tailed bandits with Huber contamination in the adversarial regime (vs. the easier stochastic regime studied in all previous works). Our result also implies a high-probability BOBW regret guarantee when the bounded true losses are protected with pure Local Differential Privacy (LDP), while the existing work ensures the (weaker) approximate LDP with the regret bounds in expectation only. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider the multi-armed bandits (MAB) problem (Auer et al., 2002a,b), which is a useful framework for sequential decision-making under uncertainty and can be formulated as repeated interactions between the environment and a (learning) algorithm. In each of the total $T$ rounds indexed by $t$ , the algorithm plays an action $a_{t}$ from a fixed set of $K$ actions (assuming $K\\leqslant T$ ). Simultaneously, the environment determines the losses of all actions $\\ell_{t}\\in\\mathbb{R}^{K}$ . The algorithm observes and suffers the loss associated with $a_{t}$ (denoted by $\\ell_{t,a_{t}}.$ ). The goal of the algorithm is to minimize the cumulative loss over $T$ rounds, or equivalently, to minimize the regret, defined as the difference between its cumulative loss and that incurred by playing the best-fixed action (in hindsight) all the time. Without observing the losses of the other actions, the algorithm must \u201cinfer\u201d the optimal action through interactions on the fly, facing the well-known trade-off between exploration and exploitation. ", "page_idx": 0}, {"type": "text", "text": "Depending on how the losses are determined, MAB problem is typically studied in two regimes: 1) the stochastic regime, where the loss of each action is drawn from a fixed (but unknown) distribution over time; 2) the adversarial regime, where the losses can be arbitrary (within some known class). Typically, the losses are assumed to have a support on a bounded interval (e.g., $[0,1])$ , and the fundamental limits have been well understood: 1) in the stochastic regime, a number of optimismbased algorithms achieve the $\\Theta({\\sqrt{K T}})^{1}$ worst-case (pseudo-)regret and $\\begin{array}{r}{\\Theta(\\sum_{i:\\Delta_{i}\\geq0}(1/\\Delta_{i})\\log T)}\\end{array}$ gap-dependent regret, where $\\Delta_{i}$ (formally defined in Section 2) is the sub-optimality gap between action $i$ and the optimal action (L\u221aai & Robbins, 1985; Auer et al., 2002a; Agrawal & Goyal, 2017); 2) in the adversarial regime, the $\\Theta({\\sqrt{K T}})$ worst-case regret can be achieved by classic Online Learning algorithms, such as Follow-the-Regularized-Leader (FTRL), Online-Mirror-Descent (OMD), and Follow-the-Perturbed-Leader (FTPL) (Audibert & Bubeck, 2009; Lee et al., 2024). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite these progresses, the optimal algorithms in the two regimes fall into different frameworks (i.e., optimism-based algorithms for the stochastic regime vs. Online Learning algorithms for the adversarial regime). Moreover, while the former ones enjoy a logarithmic regret (i.e., $O(\\log T))$ in the stochastic regime, even their $\\widetilde{O}(\\sqrt{K T})$ worst-case guarantees no longer hold when the environment deviates from the stochastic regime with empirical evidence provided in Zimmert & Seldin (2021). On the other hand, while Online Learning algorithms always preserve worst-case optimality, they could be too \u201cconservative\u201d to enjoy logarithmic regrets in stochastic environments. ", "page_idx": 1}, {"type": "text", "text": "These performance discrepancies motivated the study of the Best-of-Both-Worlds (BOBW) setting. That is, one single algorithm preserves the optimal worst-case regret in the adversarial regime and adapts to the stochastic regime with a logarithmic regret, without knowing the type of the regime in advance. Bubeck & Slivki\u221ans (2012) initiated the study by proposing a detect-switch framework, which preserves the optimal $\\widetilde{O}(\\sqrt{K T})$ regret in the adversarial regime and enjoys $O$ $((\\log T)^{2}K/\\Delta)$ regret in the stochastic regi me, where $\\Delta:=\\mathrm{min}_{i:\\Delta_{i}>0}\\,\\Delta_{i}$ is the smallest sub-optimality gap. Under this framework, Auer & Chiang (2016) improved the gap-dependent term from $K/\\Delta$ to $\\bar{\\sum}_{i:\\Delta_{i}>0}(1/\\Delta_{i})$ ", "page_idx": 1}, {"type": "text", "text": "Another line of work showed that without explicit detect-switch, OMD, originally designed for the adversarial regime, can automatically adapt to stochastic environments with a logarithmic regret (Wei & Luo, 2018; Zimmert & Seldin, 2021). In particular, Zimmert & Seldin (2021) achieved optimal regrets in the BOBW setting. Following these works, the power of Online Learning algorithms towards BOBW has been extended to various setups (Ito, 2021; Ito et al., 2022; Kong et al., 2023; Ito & Takemura, 2023; Dann et al., 2023; Jin et al., 2024; Lee et al., 2024; Tsuchiya et al., 2024). ", "page_idx": 1}, {"type": "text", "text": "While the aforementioned works require bounded losses, real-world data from application domains such as finance (Cont, 2001) and imaging (Hamza & Krim, 2001) often exhibits a heavy-tailed distribution. Intuitively, heavy-tailed losses make learning problems harder (compared to bounded losses) as they are \u201cnoisier\u201d and \u201cless informative\u201d (Zhang et al., 2020). When losses are unbounded but have $(1+v)$ -th raw moment bounded by ${\\boldsymbol{u}}^{1+\\boldsymbol{v}}$ for some known $u>0$ and $v\\in(0,1]$ , Bubeck et al. (2013) showed $\\widetilde\\Theta(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}})$ worst-case regret and $\\begin{array}{r}{\\Theta\\left(\\sum_{i:\\Delta_{i}>0}(u^{1+1/v}/\\Delta_{i})^{1/v}\\log T\\right)}\\end{array}$ gap-dependent regret2 in the stochastic regime by integrating robust mean estimators (e.g., trimmed mean and median-of-means) into optimism-based algorithms. ", "page_idx": 1}, {"type": "text", "text": "To address heavy-tailed losses in the adversarial regime and BOBW setting, Huang et al. (2022) showed that with calibrated adaptive loss trimming thresholds, FTRL with Tsallis entropy regularizer (Audibert & Bubeck, 2009) enjoys the optimal BOBW expected regrets under the \u201ctruncated nonnegative losses\u201d assumption (see Assumption 2). Without this strong assumption, it is unclear whether the near-optimal worst-case regret can still be achieved in the adversarial regime, let alone the BOBW setting. The key technical challenge here is that heavy-tailed losses can be both negative and unbounded, which is known to break the regret guarantees of the Online Learning algorithms. Extensive discussions and insights for our solution are provided in Section 3. ", "page_idx": 1}, {"type": "text", "text": "Given this challenge, the gap raises an interesting question: In heavy-tailed MAB, are there any fundamental barriers to the worst-case optimality in the adversarial regime and the BOBW guarantees? ", "page_idx": 1}, {"type": "text", "text": "We offer a positive answer to the above question, which implies that there is no such barrier. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In the adversarial regime, we propose an OMD-based algorithm achieving the (near-)optimal $\\widetilde{\\cal O}(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}})$ pseudo-regret with high probability. Our approach relaxes the undesired \u201ctruncated non-negative losses\u201d assumption, which is needed in the state-of-the-art results ", "page_idx": 1}, {"type": "text", "text": "Table 1: Comparison with related results in heavy-tailed MABs. In this table, column \u201c $\\textstyle(u,v)$ -free\u201d indicates whether the algorithm ensures the stated guarantee for unknown $(u,v)$ ; \u201cAssumption-2- free\u201d indicates whether the stated guarantee holds without Assumption 2; \u201cSto.\u201d and \u201cAdv.\u201d are abbreviations for \u201cStochastic\u201d and \u201cAdversarial\u201d, respectively; \u201cHigh-prob.\u201d indicates whether the stated expected bound is implied by some (stronger) high-probability bound. ", "page_idx": 2}, {"type": "table", "img_path": "4Yj7L9Kt7t/tmp/b930ee7da924536bf9140ef03c60117c39e772c55fdc41940a24b828cd1163d5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "even for the weaker expected regret guarantee (Huang et al., 2022). Relaxing it also allows us to obtain the near-optimal worst-case guarantee against the Huber contamination, which, to our best knowledge, was only studied for the stochastic regime in the literature. This suggests broader implications of our approach. ", "page_idx": 2}, {"type": "text", "text": "\u2022 On top of the above advance in the adversarial regime, by leveraging the detect-switch framework, we further extend the (near-)optimal regret guarantees to the BOBW setting. Specifically, our algorithm preserves the optimal $\\tilde{O}(\\bar{u}K^{\\frac{\\bar{v}}{1+v}}T^{\\frac{1}{1+v}})$ regret in the adversarial regime and enjoys $O(K(u^{1+1/v}/\\Delta)^{1/v}\\log(K)(\\log T)^{4})$ gap-dependent logarithmic regret in the stochastic regime, both with high probability, which implies that there is no fundamental barrier to achieving the BOBW guarantees when the loss distributions are heavy-tailed. This result also immediately imply the first high-probability BOBW regret guarantees with pure Local Differential Privacy (LDP) protection on the true losses, while the existing result ensures the weaker approximate LDP protection with expected regret guarantees only. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Technique-wise, we leverage the inherent stronger stability of log-barrier to relax a strong technical assumption made in previous works and utilize the increasing-learning-rates trick (Lee et al., 2020) to obtain the stronger high-probability guarantee in the adversarial regime. Moreover, we adapt the detect-switch framework by Bubeck & Slivkins (2012), originally designed for BOBW in the bounded-loss case, to the heavy-tailed setup. The adaptation introduces non-trivial challenges in the analysis due to the history-dependent trimmed estimator. In particular, to obtain the desired concentration rate in the adversarial regime, the proof does not follow its existing counterpart in the stochastic regime. Beyond addressing these challenges, we identify a novel use (i.e., handling history-dependent trimming in martingale concentrations) of an adaptive variant of Freedman\u2019s inequality (originally proposed by Lee et al. (2020) and improved by Zimmert & Lattimore (2022) for high-probability regret in adversarial bandits), which may be of independent interest. ", "page_idx": 2}, {"type": "text", "text": "We refer the readers to Table 1 for a summary of the most relevant results, in which we also include adaptive results on the case when $u,v$ are unknown. We also present a comprehensive discussion about related work, which is deferred to Appendix A due to the page limit. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we formally introduce the problem setup and define needed notations. To formulate heavy-tailed losses, we let $\\ell_{t,i}$ denote the loss of action $i$ in round $t$ , which is drawn from distribution $P_{t,i}$ satisfying the following assumption: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The $(1+v)$ -th (raw) moments of losses (which have potentially unbounded support in $\\mathbb{R}$ ) are bounded by $u^{1+v}$ for some constants $u>0$ and $v\\,\\in\\,(0,1]$ , i.e., $\\mathbb{E}_{\\ell_{t,i}\\sim P_{t,i}}\\left[|\\ell_{t,i}|^{1+v}\\right]\\leqslant$ $u^{1+v}$ , $\\forall t\\in[T],i\\in[K]$ , where $[n]$ denotes set $\\{1,\\ldots,n\\}$ for any integer $n\\geqslant1$ . ", "page_idx": 3}, {"type": "text", "text": "In the heavy-tailed MAB problem, the (learning) algorithm and environment perform the following interactions repeatedly in round $t=1,\\dots,T$ : ", "page_idx": 3}, {"type": "text", "text": "1. The algorithm samples action $a_{t}$ from $[K]$ via $a_{t}\\sim w_{t}:=\\left(w_{t,1},\\ldots,w_{t,K}\\right)$ in the probability simplex $\\Omega:=\\{x\\in[0,1]^{K}\\mid\\textstyle\\sum_{i=1}^{K}x(i)=1\\}$ , i.e., action $i\\in[K]$ is sampled with probability $w_{t,i}$ . The environment draws loss $\\ell_{t,i}\\sim P_{t,i}$ for every action $i\\in[K]$ . 2. The algorithm observes $\\ell_{t,a_{t}}$ only; the losses of all other actions are unrevealed. 3. The algorithm determines $w_{t+1}$ based on the history $(w_{1},a_{1},\\ell_{1,a_{1}},\\ldots,w_{t},a_{t},\\ell_{t,a_{t}}).$ ", "page_idx": 3}, {"type": "text", "text": "Remark 1. All of our algorithms and their regret bounds allow moment order $(1+v)\\in(1,2]$ only. That is, one may not obtain any regret guarantee by running our algorithms with $v>1$ . If the losses have higher-order moments $(v>1)$ ), one can simply run our algorithms with $v=1$ and obtaining the corresponding regret bounds (since bounded higher-order moments imply lower-order ones). Note that Bubeck et al. (2013) showed that for all $v\\geqslant1$ , the lower bounds in terms of both worst-case regret and gap-dependent regret are the same as the case of $v=1$ . ", "page_idx": 3}, {"type": "text", "text": "We assume that heavy tail parameters $u$ and $v$ and time horizon $T$ are known to the algorithm a priori.3 The objective of the algorithm is to minimize the pseudo-regret $R_{T}$ , defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{T}:=\\sum_{t=1}^{T}\\left(\\mu_{t,a_{t}}-\\mu_{t,i^{*}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu_{t,i}\\,:=\\,\\mathbb{E}_{\\ell_{t,i}\\sim P_{t,i}}\\left[\\ell_{t,i}\\right]$ denotes the mean loss of action $i~\\in~[K]$ in round $t\\ \\in\\ [T]$ , and $\\begin{array}{r}{i^{*}\\in\\mathrm{argmin}_{i\\in[K]}\\sum_{t=1}^{T}\\mu_{t,i}}\\end{array}$ denotes any best-fixed action in hindsight. ", "page_idx": 3}, {"type": "text", "text": "Depending on how loss distributions are determined, we further define the following two regimes: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Stochastic regime: For every action $i\\,\\in\\,[K]$ , the loss distributions are identical in all rounds. That is, we have $P_{1,i}=\\cdot\\cdot-P_{T,i}=P(i)$ , implying $\\mu_{1,i}=\\cdot\\cdot=\\mu_{T,i}=\\mu(i)$ and $i^{*}\\in\\mathrm{argmin}_{i\\in[K]}\\,\\mu(i)$ . We also define $\\Delta_{i}:=\\mu(i)-\\mu(i^{*})$ and $\\Delta:=\\mathrm{min}_{i:\\Delta_{i}>0}\\,\\Delta_{i}$ . \u2022 (Oblivious) Adversarial regime: All loss distributions are chosen arbitrarily (by some adversary, with the full knowledge of the algorithm) before the interaction begins. Our regret definition and adversarial model are also considered in Huang et al. (2022). ", "page_idx": 3}, {"type": "text", "text": "Remark 2. It is not hard to see that the stochastic regime is a special (and \u201ceasy\u201d) case of the adversarial regime. There are two differences between our heavy-tailed setup and the bounded-loss setup in the adversarial bandits literature: 1) Typically in the (bounded-loss) adversarial regime, the losses are considered to be deterministic rather than randomized (and this difference also leads to some new challenges in the analysis when we adopt the detect-switch framework from the bounded case (Bubeck & Slivkins, 2012) as we will show later in Section 4.2), and 2) a stronger notion of regret, defined as $\\begin{array}{r}{\\overline{{R}}_{T}:=\\sum_{t=1}^{T}\\ell_{t,a_{t}}-\\operatorname*{min}_{i\\in[K]}\\sum_{t=1}^{T}\\ell_{t,i}.}\\end{array}$ , is considered; this is stronger than pseudo-regret since $\\mathbb{E}[\\overline{{R}}_{T}]\\geqslant\\mathbb{E}[R_{T}]$ . However, when adapted to the heavy-tailed case, it is natural to still consider randomized losses and pseudo-regret as in the stochastic regime: The \u201ceasiness\u201d of the regime boils down to how heavy-tailed distributions are chosen. Moreover, a low stronger regret depends not only on playing good actions (with low loss means), but also on the realization of potentially unbounded losses, which loses the standard meaning in evaluating the algorithm. Therefore, when switching from stochastic to adversarial regime in heavy-tailed bandits, we keep pseudo-regret as the metric.4 ", "page_idx": 3}, {"type": "text", "text": "Our goal is to design one single algorithm that can achieve the near-optimal $\\widetilde{\\cal O}(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}})$ worstcase regret in the adversarial regime and enjoys $\\log T$ -type regret when t he regime is stochastic, without being informed of the regime type in advance. ", "page_idx": 4}, {"type": "text", "text": "A closely related work by Huang et al. (2022) studied the same BOBW setup (i.e., achieving BOBW guarantee when $u,v$ are known). They proposed an FTRL-based algorithm with Tsallis entropy regularizer and carefully-chosen history-dependent trimming threshold for loss magnitude control and showed O  i:\u2206i>0(u1+1/v/\u2206i)1/v log T regret in the stochastic regime and O(uK1+vv T1+1v ) regret in the adversarial regime, both of which are in expectation and optimal. However, their regret guarantees rely heavily on a strong technical assumption: ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Truncated non-negative losses (Huang et al., 2022)). Given any fixed $M>0$ , the loss distributions of the optimal action $i^{*}$ satisfy $\\mathbb{E}_{\\ell_{t,i^{*}}\\sim P_{t,i^{*}}}[\\ell_{t,i^{*}}\\cdot\\mathbb{I}\\{|\\ell_{t,i^{*}}|>\\dot{M}\\}]\\geqslant0,\\forall t\\in[T].$ . ", "page_idx": 4}, {"type": "text", "text": "In the following sections, we first show that by resorting to the log-barrier regularizer, we obtain the near-optimal regret bound in the adversarial regime without Assumption 2 and naturally extend it to the stronger high-probability guarantee (Section 3). On top of that, we further adapt the detect-switch framework in Bubeck & Slivkins (2012) and obtain high-probability bounds in BOBW (Section 4). ", "page_idx": 4}, {"type": "text", "text": "Additional Notations. For any round $t$ and action $i$ , we let $I_{t,i}:=\\mathbb{I}\\{a_{t}=i\\}$ denote whether action $i$ is pulled in round $t$ and $\\begin{array}{r}{N_{t,i}:=\\sum_{s=1}^{t}I_{s,i}}\\end{array}$ denote the number of times when action $i$ is pulled before the end of round $t$ . We use $\\widehat{\\ell}_{t,i}\\,:=\\,\\ell_{t,i}I_{t,i}\\mathbb{I}\\{|\\ell_{t,i}|\\,\\leqslant\\,M_{t,i}\\}/w_{t,i}$ to denote the (trimmed) IW estimate with respect to some threshold $M_{t,i}$ and $\\begin{array}{r}{\\widehat{\\mu}_{t,i}:=\\sum_{s=1}^{t}\\ell_{s,i}I_{s,i}{\\mathbb I}\\{|\\ell_{s,i}|\\leqslant B_{s,i}\\}/N_{t,i}}\\end{array}$ to denote the (trimmed) empirical average with resp e ct to s ome threshold $B_{t,i}$ .5 We also use $\\mu_{t,i}^{\\prime}:=\\mathbb{E}_{\\ell_{t,i}\\sim P_{t,i}}[\\ell_{t,i}\\mathbb{I}\\{|\\ell_{t,i}|\\leqslant M_{t,i}\\}]$ to denote the mean of the trimmed loss and $\\begin{array}{r}{\\widehat{L}_{t,i}:=\\sum_{s=1}^{t}\\widehat{\\ell}_{s,i}}\\end{array}$ to denote the cumulative IW estimate. ", "page_idx": 4}, {"type": "text", "text": "3 High-probability Near-optimal Regret in the Adversarial Regime ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section is dedicated to high-probability regret in the adversarial regime. We first present detailed discussions on why Assumption 2 is needed in Huang et al. (2022) and how we get rid of it (Section 3.1), followed by the description of our algorithm design and regret guarantee (Section 3.2). ", "page_idx": 4}, {"type": "text", "text": "3.1 Technical Challenges and Insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The main technical challenge we encounter comes from the potentially-unbounded negative losses (even for regret bounds in expectation only). We illustrate this challenge using the previous work of Huang et al. (2022) as an example. Their algorithm is running standard FTRL with $(1+v)^{-1}$ - Tsallis entropy regularizer over the trimmed loss estimate sequence $\\widehat{\\ell}_{1},\\ldots,\\widehat{\\ell}_{T}$ with respect to some trimming threshold $(M_{t,i})_{t\\in[T],i\\in[K]}$ to be introduced later in this subsection. ", "page_idx": 4}, {"type": "text", "text": "To bound the expected regret, Huang et al. (2022) first rewrite it as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[R_{T}\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle w_{t}-y^{i^{*}},\\mu_{t}-\\mu_{t}^{\\prime}\\rangle\\right]+\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle w_{t}-y^{i^{*}},\\mu_{t}^{\\prime}\\rangle\\right]}\\\\ &{\\qquad=\\underbrace{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{K}w_{t,i}(\\mu_{t,i}-\\mu_{t,i}^{\\prime})\\right]}_{\\mathrm{Part}\\,\\mathrm{I}}+\\underbrace{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}(\\mu_{t,i^{*}}^{\\prime}-\\mu_{t,i^{*}})\\right]}_{\\mathrm{Part}\\,\\mathrm{II}}+\\underbrace{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\langle w_{t}-y^{i^{*}},\\widehat{\\ell}_{t}\\rangle\\right]}_{\\mathrm{Part}\\,\\mathrm{III}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $y^{i}$ is the $K$ -dim vector such that the $i$ -th entry is one and all the others are zero. ", "page_idx": 4}, {"type": "text", "text": "The analyses begin with Part III. The desired upper bound on Part III holds only under the well-known \u201cstability condition\u201d associated with Tsallis entropy (Jin et al., 2024, Lemma C.5.3): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta_{t,i}(w_{t,i})^{1-\\frac{1}{1+\\nu}}\\widehat{\\ell}_{t,i}=\\eta_{t,i}(w_{t,i})^{1-\\frac{1}{1+\\nu}}(w_{t,i})^{-1}\\ell_{t,i}I_{t,i}\\mathbb{I}\\{|\\ell_{t,i}|\\leqslant M_{t,i}\\}\\geqslant-C(u,v),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where constant $C(u,v)>0$ depends on $u$ and $v$ only and the learning rate $\\eta_{t,i}$ is chosen as $u^{-1}t^{\\frac{-1}{1+v}}$ . While this condition is trivially satisfied when losses are non-negative, due to potentially-unbounded ", "page_idx": 4}, {"type": "text", "text": "1: Input: failure probability $\\zeta$ , initial learning rate $\\eta$ , trimming threshold $\\{M_{t,i}\\}_{t\\in[T],i\\in[K]}$   \n2: Define: learning rate increase factor $\\begin{array}{r l r}{\\kappa}&{{}=}&{e^{1/\\log T}}\\end{array}$ ; log-barrier regularizer   \n$\\begin{array}{r}{\\phi_{t}(x)~=~-\\sum_{i=1}^{K}\\mathbf{\\bar{\\log}(}x(i))/\\eta_{t,i}}\\end{array}$ ; Bregman divergence $\\begin{array}{r l r}{D_{\\phi_{t}}(x,x^{\\prime})\\!\\!}&{{}=}&{\\!\\!\\sum_{i=1}^{d}(x(i)/x^{\\prime}(i)\\,-\\,}\\end{array}$   \n$\\log(x(i)/x^{\\prime}(i))-1)/\\eta_{t,i}$ ; simplex truncation parameter $\\lambda=T^{\\frac{-\\,v}{1+v}}K^{\\frac{-1}{1+v}}$ ; truncated probability   \nsimplex $\\Omega^{\\prime}:=\\{x\\in\\Omega:x(i)\\geqslant\\lambda/K,\\forall i\\in[K]\\}$   \n3: Initialization: For every action $i\\in[K]$ , define $w_{1,i}=1/K,\\rho_{1,i}=2K,\\eta_{1,i}=\\eta$   \n4: for $t=1:T$ do   \n5: Take action $a_{t}$ sampled from $w_{t}$ and observe $\\ell_{t,a_{t}}$   \n6: Construct loss estimate $\\widehat{\\ell}_{t,i}=\\mathbb{I}\\{a_{t}=i\\}\\mathbb{I}\\{|\\ell_{t,i}|\\leqslant M_{t,i}\\}\\ell_{t,i}/w_{t,i},\\forall i\\in[K]$   \n7: Calculate $\\begin{array}{r}{w_{t+1}=\\mathrm{argmin}_{x\\in\\Omega^{\\prime}}\\left(\\langle x,\\widehat{\\ell}_{t}\\rangle+D_{\\psi_{t}}(x,w_{t})\\right)}\\end{array}$   \n8: for $i\\in[K]$ do   \n9: if $1/w_{t+1,i}>\\rho_{t,i}$ , then $\\rho_{t+1,i}=2/w_{t+1,i},\\eta_{t+1,i}=\\eta_{t,i}\\kappa$   \n10: else $\\rho_{t+1,i}=\\rho_{t,i}$ , $\\eta_{t+1,i}=\\eta_{t,i}$   \n11: end for   \n12: end for ", "page_idx": 5}, {"type": "text", "text": "negative losses, threshold $M_{t,i}$ here is chosen to be $\\Theta((t\\cdot w_{t,i})^{\\frac{1}{1+v}})$ (in particular, to fully \u201ccancel\u201d the $(w_{t,i})^{-1}$ from $\\widehat{\\ell}_{t,i})$ ). Otherwise, negative losses break this condition whenever $w_{t,a_{t}}$ is very small. While such a threshold suffices to bound Part I and Part III with worst-case optimality (and even in the BOBW setting), applying the analysis for Part I to Part II leads to an upper bound of form $\\sum_{t=1}^{T}(w_{t,i^{*}})^{\\frac{-\\,v}{1+v}}$ on Part II, which is potentially unbounded since $w_{t,i^{*}}$ can be very close to zero. However, with the help of Assumption 2, Part $\\mathrm{II}$ itself is non-positive and hence can be ignored. ", "page_idx": 5}, {"type": "text", "text": "Summary. The key issue above is that, due to unbounded and negative losses, Eq. (3) results in a threshold $M_{t,i}$ which scales with $(w_{t,i})^{\\frac{1}{1+v}}$ (in particular, for $i=i^{*}$ ), rendering Part II hard to bound. Remark 3. This issue may not be fixed by simply shifting all loss estimates to become positive and satisfy Eq. (3). Roughly speaking, the reason is that obtaining desired regret bounds relies heavily on the well-bounded $(1+v)$ -th moment of the losses (before shifting). However, to ensure positive losses, the needed shift is too \u201csignificant\u201d and breaks the \u201cnice\u201d moment conditions. ", "page_idx": 5}, {"type": "text", "text": "To handle this issue and relax Assumption 2, we resort to the log-barrier regularizer, which offers the standard regret guarantee with the following stability condition (Agarwal et al., 2017): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}\\geqslant-0.5.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Importantly, this condition itself already provides a $w_{t,i}$ (rather than the previous $(w_{t,i})^{1-\\frac{1}{1+v}}$ in Eq. (3)) to \u201ccancel\u201d the $(w_{t,i})^{-1}\\,\\mathrm{from}\\,\\widehat{\\ell}_{t,i}$ , meaning that we do not need any additional $w_{t,i}$ contributed from threshold $M_{t,i}$ . As a result, we can choose a different $M_{t,i}$ that scales with $K^{\\frac{-1}{1+v}}$ rather than the previous $(w_{t,i})^{\\frac{1}{1+v}}$ such that all three parts are bounded by $\\widetilde{\\cal O}(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}})$ without Assumption 2. We further adopt the increasing-learning-rates trick (Lee et al., 2020) together with an adaptive variant of Freedman\u2019s inequality (stated in Lemma 12) to obtain the stronger high-probability regret. ", "page_idx": 5}, {"type": "text", "text": "3.2 Algorithm Overview and Regret Guarantee ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now discuss the algorithm design and present the full pseudo-code in Algorithm 1. At a high level, our algorithm is running standard OMD over loss sequence $\\widehat{\\ell}_{1},\\ldots,\\widehat{\\ell}_{T}$ with respect to some fixed threshold $M_{t,i}=u\\cdot(T/K)^{\\frac{1}{1+v}}$ . The key ingredients (to obtain high-probability regret) are the special learning rate schedule and probability simplex truncation (in OMD update) (Wei & Luo, 2018; Lee et al., 2020), which we briefly introduce below. ", "page_idx": 5}, {"type": "text", "text": "Increasing Learning Rates. We use vectors $\\{\\rho_{t}\\}_{t\\in[T]}$ to keep track of smallest sampling probabilities throughout the interaction. To be more specific, for every action $i$ , if $w_{t+1,i}$ is so small that $1/w_{t+1,i}>$ $\\rho_{t,i}$ holds, we increase the learning rate by a factor of $\\kappa>1$ and set $\\rho_{t+1,i}\\,=\\,2/w_{t+1,i}$ (Line 9). Otherwise, we keep the learning rate unchanged and set $\\rho_{t+1,i}=\\rho_{t,i}$ (Line 10). In the analysis, such a schedule introduces some negative term in the upper bound, which cancels out a positive term that could potentially be very large due to the variance of loss estimates (Lee et al., 2020). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Probability Simplex Truncation. We perform the OMD update over the truncated probability simplex $\\Omega^{\\prime}:=\\{\\bar{x}\\in\\Omega:x(i)\\geqslant\\lambda/K,\\bar{\\forall}i\\in[K]\\}$ (where $\\lambda$ controls the degree of truncation), of which the purpose is to ensure that $w_{t,i}$ is always at least $\\lambda/K$ and hence to control the variance of the loss estimates. The value of $\\lambda$ here is $T^{\\frac{-\\,v}{1+v}}K^{\\frac{-1}{1+v}}$ adapted to the heavy-tailed case and differs from the original value $1/T$ for the bounded-loss case in Lee et al. (2020). ", "page_idx": 6}, {"type": "text", "text": "The regret guarantee of Algorithm 1 is formally stated below with full proofs presented in Appendix B. Theorem 1. In the adversarial regime, for any failure probability $\\zeta_{i}$ , by choosing initial learning rate $\\eta=(40M\\log(T)\\log\\left(8K T/\\zeta\\right))^{-1}$ and trimming threshold $M_{t,i}=u\\cdot(T/K)^{\\frac{1}{1+v}}$ , Algorithm 1 ensures that with probability at least $1-\\zeta$ , $R_{T}=O(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}(\\log T)^{2}\\log(T/\\zeta))$ . By further choosing $\\zeta=1/T$ , Algorithm 1 ensures that $\\mathbb{E}\\left[R_{T}\\right]=O(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}(\\log T)^{3})$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4. Removing Assumption 2 is crucial to obtain the first and near-optimal worst-case regret in heavy-tailed MAB when the feedback could be contaminated by the Huber model in the adversarial regime, in contrast to all previous works that study the (easier) stochastic regime (Guan et al., 2020; Agrawal et al., 2024; Wu et al., 2024). We provide all details in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4 High-probability Regrets in the Best-of-Both-Worlds Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With Algorithm 1 achieving high-probability optimal regret in the adversarial regime, we further leverage the detect-switch framework proposed by Bubeck & Slivkins (2012) named SAO to achieve high-probability bounds in the BOBW setting. We first present the BOBW guarantee, followed by an algorithm overview and analysis sketch. The complete proofs are provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. In the adversarial regime, for any failure probability $\\zeta_{i}$ , by choosing constant $c_{1}=6$ , Algorithm 2 ensures that with probability at least $1-\\zeta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}=O\\left(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}\\log(K)(\\log T)^{2}(\\log(\\beta/\\zeta))^{2}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which (by further choosing $\\zeta=1/T,$ ) implies that $\\mathbb{E}\\left[R_{T}\\right]=O\\left(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}\\log(K)(\\log T)^{4}\\right).I n$ the stochastic regime, Algorithm 2 ensures that with probability at least $1-\\zeta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}=O\\left(K\\log(K)(u^{1+1/v}/\\Delta)^{1/v}\\log(T)\\left(\\log(T/\\zeta)\\right)^{3}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which implies that $\\mathbb{E}\\left[R_{T}\\right]=O\\left(K\\log(K)(u^{1+1/v}/\\Delta)^{1/v}(\\log T)^{4}\\right).$ ", "page_idx": 6}, {"type": "text", "text": "Remark 5. High-probability bounds also are powerful tools to handle adaptive adversaries (who determine the current distribution $P_{t,i}$ based on past actions $a_{1},\\dotsc,a_{t-1})$ . Following the literature (Audibert & Bubeck, 2010; Lee et al., 2020; Zimmert $\\&$ Lattimore, 2022), based on our highprobability bounds against oblivious adversaries, one may further derive both high-probability and expected regret bounds against adaptive adversaries, which we leave as future investigation. ", "page_idx": 6}, {"type": "text", "text": "Remark 6. This theorem immediately implies a (high-probability) BOBW regret guarantee when the losses are bounded (in $[0,1];$ ) and protected with $\\varepsilon$ -Local Differential Privacy (LDP) via showing that the privatized losses (with Laplacian noise) have second moments bounded by $O(\\varepsilon^{-1})$ (Agarwal & Singh, 2017; Tossou & Dimitrakakis, 2017; Zheng et al., 2020; Ren et al., 2020). To the best of our knowledge, this is the first result showing (high probability) BOBW regret guarantee with pure LDP protection, while the state-of-the-art result (Zheng et al., 2020) ensures (the weaker) approximate LDP protection, and only expected regret bounds are provided. Full details are given in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "4.1 Algorithm Overview ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our algorithm design follows from the detect-switch framework by Bubeck & Slivkins (2012). The high-level idea is to keep performing statistical tests (to \u201cidentify\u201d the environment) and carefully maintain the sampling distribution over all arms. Once some certain test fails (which implies that the environment is unlikely stochastic), we switch to Algorithm 1 and run it over the remaining rounds. ", "page_idx": 6}, {"type": "text", "text": "In each round after playing action $a_{t}$ sampled from distribution $w_{t}$ , if any active action $i\\in A_{t}$ satisfies the test in Eq. (5), it is deactivated, and we use $\\tau_{i}$ and $q_{i}$ to store the round and the sampling ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 SAO for heavy-tailed MABs (SAO-HT) ", "page_idx": 7}, {"type": "text", "text": "1: Input: failure probability $\\zeta$ ; constant $c_{1}\\geqslant6$ ", "page_idx": 7}, {"type": "text", "text": "2: Define: $\\beta~=~12T^{2}K\\log(T)$ ; $\\begin{array}{r c l}{M_{t,i}}&{=}&{u\\frac{(t/K)^{\\frac{1}{1+v}}}{\\left(\\log(\\log(T)/\\zeta)\\right)^{\\frac{1}{3v+1}}}}\\end{array}$ ; Bt,i = u log(N2tT,i/\u03b6) $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ ", "page_idx": 7}, {"type": "text", "text": "3: Initialization: Play each action once; for every $i\\in[K]$ , let $\\begin{array}{r}{w_{K+1,i}=\\frac{1}{K}}\\end{array}$ , $\\tau_{i}=T$ ; set $A_{K}=[K]$ . ", "page_idx": 7}, {"type": "text", "text": "4: for $t=K+1,\\ldots,T$ do   \n5: Sample and play action $a_{t}\\sim w_{t}$ ; observe $\\ell_{t,a_{t}}$   \n6: for $i\\in A_{t-1}$ do ", "page_idx": 7}, {"type": "text", "text": "7: if ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{L}_{t,i}-\\operatorname*{min}_{j\\in A_{t-1}}\\widehat{L}_{t,j}>c_{1}\\mathrm{Width}(t)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "8: then $A_{t}=A_{t-1}\\backslash\\{i\\}$ , $\\tau_{i}=t$ , and $q_{i}=w_{t,i}$ ", "page_idx": 7}, {"type": "text", "text": "9: end if ", "page_idx": 7}, {"type": "text", "text": "10: end for ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "11: if any of the three conditions holds then run Algorithm 1 for the remaining rounds, let $t_{\\mathrm{sw}}=t$ , and let $q_{i}=w_{t,i}$ , $\\tau_{i}=t$ for every $i\\in A_{t}$ : ", "page_idx": 7}, {"type": "text", "text": "12: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exists i\\in[K]\\mathrm{~such~that~}\\Big|\\widehat{L}_{t,i}/t-\\widehat{\\mu}_{t,i}\\Big|>9u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t,i}}\\right)^{\\frac{\\nu}{1+\\nu}}+\\mathbb{I}\\{i\\in A_{t}\\}\\frac{\\mathrm{Width}(t)}{t}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathbb{I}\\{i\\notin A_{t}\\}\\frac{\\mathrm{Width}(t)}{\\tau_{i}},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\notin A_{t}\\mathrm{~such~that~}(\\widehat{L}_{t,i}-\\operatorname*{min}\\widehat{L}_{t,j})/t>(c_{1}+4)\\mathrm{Width}(t)/(\\tau_{i}-1),}\\end{array}\n$$$\\exists i\\notin A_{t}$ ", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "13: end if ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "14: $\\begin{array}{r l}&{\\stackrel{\\mathrm{exs-s.}}{w_{t+1,i}}=\\mathbb{I}\\{i\\notin A_{t}\\}q_{i}\\tau_{i}/(t+1)+\\mathbb{I}\\{i\\in A_{t}\\}\\left(1-\\sum_{j\\notin A_{t}}q_{i}\\tau_{i}/(t+1)\\right)/\\left|A_{t}\\right|,\\forall i\\in[K]}\\end{array}$   \n15: end for ", "page_idx": 7}, {"type": "text", "text": "16: $q_{i}=w_{T,i}$ , $\\tau_{i}=T,\\forall i\\in A_{T}$ ", "page_idx": 7}, {"type": "text", "text": "probability when it is deactivated, respectively (Line 8). After that, the algorithm performs tests in Eqs. (6)-(8) for environment identification. If any of them is satisfied, the procedure is terminated, and we instead run Algorithm 1 over the remaining rounds. We use $t_{\\mathrm{sw}}$ to denote the round when the algorithm switch happens. Otherwise, we update the distribution $w_{t+1}$ for the next round as in Line 14. To make notations and analyses well-defined, we deactivate all remaining arms in $A_{t_{\\mathrm{sw}}}$ when the algorithm switch happens. ", "page_idx": 7}, {"type": "text", "text": "4.2 Analysis Sketch ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we present the key steps of the regret analysis in the proof of Theorem 2. ", "page_idx": 7}, {"type": "text", "text": "Before diving into the specific regime, we first derive a set of concentration results (good events). Informally, all of the following hold simultaneously with high probability in all rounds $t\\in[T]$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\widehat{L}_{t,i}-\\displaystyle\\sum_{s=1}^{t}\\mu_{s,i}\\right|/t\\leqslant\\mathbb{I}\\{i\\in A_{t}\\}\\frac{\\mathrm{Width}(t)}{t}+\\mathbb{I}\\{i\\notin A_{t}\\}\\frac{\\mathrm{Width}(t)}{\\tau_{i}},}\\\\ {\\left|\\widehat{\\mu}_{t,i}-\\frac{\\sum_{s=1}^{t}\\mu_{s,i}I_{s,i}}{N_{t,i}}\\right|=\\widetilde{O}\\left(\\frac{u}{(N_{t,i})^{\\frac{v}{1+v}}}\\right),\\ \\ \\ \\ \\ }\\\\ {N_{t,i}=\\widetilde{O}\\left(q_{i}\\tau_{i}\\cdot(1+\\log t)\\right),\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathrm{Width}(t):=12u K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}$ . All analyses below are conditioned on these good events, and we omit \u201cwith high probability\u201d in the arguments. ", "page_idx": 8}, {"type": "text", "text": "Remark 7. Our main non-trivial adaptation deviating from the case with bounded losses in Bubeck & Slivkins (2012) is the good event in Eq. (10) due to jointly randomized and heavy-tailed losses. In particular, we need the concentration result of trimmed mean $\\widehat{\\mu}_{t,i}$ specified in Eq. (10) in both regimes. In the stochastic regime, it been shown in Bubeck et al. (2013). We need Eq. (10) in adversarial regime (while Bubeck & Slivkins (2012) does not) as losses are deterministic therein. However, with heavy tails, the proof of Eq. (10) in the adversarial regime does not follow straightforwardly from the stochastic regime. More technical details are presented later in Remark 8. Nonetheless, this matter is specific to the nature of the trimmed estimator we use, which essentially could be replaced with any other estimator, as long as the concentration rate is preserved. However, it is unclear whether other estimators can handle non-identical distributions. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 Analysis Overview in the Stochastic Regime ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Step 1: Showing that tests in Eqs. (6)-(8) are never satisfied. We show by good events that tests in Eqs. (6)-(8) are never satisfied, implying that we never switch to Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "Step 2: Building the connection between $\\Delta_{i}$ and $\\tau_{i}$ . From tests in Eqs. (7) and (8), we can show that for every suboptimal action $i$ with $\\Delta_{i}>0$ , its sub-optimality gap $\\Delta_{i}=\\widetilde{O}((K/\\tau_{i})^{\\frac{v}{1+v}})$ . Intuitively, an action with a smaller sub-optimality gap stays active for a longer time. ", "page_idx": 8}, {"type": "text", "text": "Step 3: Bounding the total regret. By the definition of pseudo-regret, we now have ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{i:\\Delta_{i}>0}\\Delta_{i}N_{T,i}=\\tilde{O}(\\sum_{i:\\Delta_{i}>0}\\Delta_{i}q_{i}\\tau_{i})=\\tilde{O}(\\sum_{i:\\Delta_{i}>0}\\Delta_{i}q_{i}K(\\Delta_{i})^{-1-\\frac{1}{v}}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We complete the proof by showing $\\begin{array}{r}{\\sum_{i:\\Delta_{i}>0}q_{i}\\leqslant\\sum_{i=1}^{K}(1/i)=O(\\log K).}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "4.2.2 Analysis Overview in the Adversarial Regime ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the adversarial regime, whenever we switch to Algorithm 1, it provides $\\widetilde{O}(u(T-t_{\\mathrm{sw}})^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}})$ (high-probability) regret guarantee for the remaining $(T-t_{\\mathrm{sw}})$ rounds. Therefore, it suffices to show that the cumulative regret before the switch is $\\widetilde O(u(t_{\\mathrm{sw}})^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}})$ . ", "page_idx": 8}, {"type": "text", "text": "In our analysis, we trivially bound the regret in the single round $t_{\\mathrm{sw}}$ by $2u$ , and it remains to show that the regret in the first $(t_{\\mathrm{sw}}-1)$ rounds is $\\mathbf{\\bar{\\tilde{O}}}(u(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}})$ , which is explained in the following. ", "page_idx": 8}, {"type": "text", "text": "Step 1: Regret decomposition. We first get an regret upper bound in terms of $i_{t}^{*}~:=$ $\\scriptstyle\\operatorname{argmin}_{i\\in[K]}\\sum_{s=1}^{t}\\mu_{s,i}$ for $t=t_{\\mathrm{sw}}-1$ : ", "page_idx": 8}, {"type": "image", "img_path": "4Yj7L9Kt7t/tmp/7ad9bd5a2b92315b75505fc655fde4e8ea628a46b6a7218345ff624a863bad9b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "and then we bound Parts A, B, and C, separately. ", "page_idx": 8}, {"type": "text", "text": "Step 2: Bounding Part A. We rewrite Part A as P $\\begin{array}{r l}{\\mathrm{~\\mathop{trt}A_{\\theta}~}=}&{{}\\left(\\frac{\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i}I_{s,i}}{N_{t_{\\mathrm{sw}}-1,i}}-\\widehat{\\mu}_{t_{\\mathrm{sw}}-1,i}\\right)~+}\\end{array}$ $\\begin{array}{r}{\\left(\\widehat{\\mu}_{t_{\\mathrm{sw}}-1,i}-\\frac{\\widehat{L}_{t_{\\mathrm{sw}}-1,i}}{t_{\\mathrm{sw}}-1}\\right)}\\end{array}$ , where the first term on the right-hand side is $\\begin{array}{r}{O\\left(u\\left(\\frac{\\log\\left(\\beta/\\zeta\\right)}{N_{t_{s\\mathrm{w}}-1,i}}\\right)^{\\frac{v}{1+v}}\\right)}\\end{array}$ due to Eq. (10) and the second term is $\\begin{array}{r}{O\\left(u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{\\mathrm{sw}}-1,i}}\\right)^{\\frac{v}{1+v}}+\\frac{\\operatorname{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\right)}\\end{array}$ due to test in Eq. (6). Remark 8. Due to heavy tails, the losses are trimmed by some history-dependent threshold $B_{t,i}$ (which depends on the number of pulls $N_{t,i}$ ) for a rate-optimal concentration in good event (10). To show this in the stochastic regime, one can treat the observed losses from one action as i.i.d. samples via the \u201creward tape/table\u201d argument (Slivkins, 2019) and apply Bernstein\u2019s inequality for every fixed $N_{t,i}$ (and set the uniform upper bound as $B_{t,i}$ associated with it) as in Bubeck et al. (2013). However, one cannot simply follow the same path in the adversarial regime, since the distributions are no longer identical, and has to follow the martingale-based analysis (Agarwal et al., 2021, Lemma 6.2). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Now one may readily see the issue: The desired uniform upper bound $B_{t,i}$ is determined on-the-fly, while the standard Freedman\u2019s inequality for martingales (e.g., Lemma 11) requires a fixed uniform upper bound. To close this gap, we again exploit an adaptive variant of Freedman\u2019s inequality by Zimmert & Lattimore (2022) (Lemma 12; which was originally proposed for a totally different use, namely, obtaining high-probability bounds in adversarial bandits), in which we can replace the fixed uniform upper bound with the largest realization, satisfying our need perfectly. ", "page_idx": 9}, {"type": "text", "text": "Step 3: Bounding Part B. By considering two disjoint cases (i.e., action $i\\in A_{t_{\\mathrm{sw}}-1}$ or not), we can show Part $\\begin{array}{r}{\\mathbf{B}=O\\left(\\frac{\\operatorname{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\right)}\\end{array}$ using the tests in Eqs. (5) and (7). ", "page_idx": 9}, {"type": "text", "text": "Step 4: Bounding Part C. The good event in Eq. (9) simply implies Part $\\mathbf{C}=O\\left(\\mathbf{Width}(t_{\\mathrm{sw}}-1)\\right)$ . Step 5: Putting all pieces together. Combing Steps 1-4 and the good event in Eq. (11) yields ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\sum_{s=1}^{n-1}\\mu_{s,a_{t}}-\\sum_{s=1}^{t_{s,w}-1}\\mu_{s,i^{*}}=O\\left(\\sum_{i=1}^{K}N_{t_{s,w}-1,i}\\left(u\\left(N_{t_{s,w}-1,i}\\right)^{\\frac{-v}{1+v}}+\\frac{\\mathrm{Width}(t_{s,w}-1)}{\\tau_{i}-1}\\right)+\\mathrm{Width}(t_{s_{w}}-1)\\right)}&\\\\ &{}&{\\displaystyle=\\widetilde O\\left(u\\sum_{i=1}^{K}(N_{t_{s,w}-1,i})^{\\frac{1}{1+v}}+u\\sum_{i=1}^{K}q_{i}K^{\\frac{v}{1+v}}(t_{s_{w}}-1)^{\\frac{1}{1+v}}\\right)}\\\\ &{}&{\\displaystyle+\\:\\widetilde O\\left(u\\sum_{i=1}^{K}\\frac{K^{\\frac{v}{1+v}}(t_{s_{w}}-1)^{\\frac{1}{1+v}}}{\\tau_{i}-1}+\\mathrm{Width}(t_{s_{w}}-1)\\right)}\\\\ &{}&{\\displaystyle=\\widetilde O(u K^{\\frac{v}{1+v}}(t_{s_{w}}-1)^{\\frac{1}{1+v}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we show that there is indeed no fundamental barrier to achieving the BOBW guarantee in heavy-tailed MAB by relaxing a strong, hard-to-verify technical assumption on the loss distributions of the optimal action needed for the state-of-the-art results. We further leverage the increasing-learningrates trick and the detect-switch framework to achieve the stronger high-probability guarantees. Our results also imply the first and near-optimal regret in the adversarial regime where the feedback could be contaminated in the Huber model, and the high-probability BOBW regret guarantee when losses are bounded and protected with pure LDP, while the state-of-the-art result only ensures the weaker approximate LDP protection with regret guarantees in expectation. ", "page_idx": 9}, {"type": "text", "text": "One follow-up question is whether the gap-dependent term $K(\\Delta)^{-1/v}$ can be improved to the refined $\\sum_{i:\\Delta_{i}>0}(\\Delta_{i})^{-1/v}$ , which is achieved only in the stochastic regime (Bubeck et al., 2013). Under the detect-switch framework, we tend to believe this is possible by adapting more sophisticated tests designed in Auer & Chiang (2016) for the bounded-loss case, where the gap dependency is improved from $K/\\Delta$ to $\\sum_{i:\\Delta_{i}>0}(1/\\Delta)$ , although a higher computational complexity is expected. ", "page_idx": 9}, {"type": "text", "text": "It will also be interesting to understand whether canonical Online Learning algorithms (i.e., without explicit detection and switch) provably enjoy BOBW guarantees in heavy-tailed MAB (and if so, whether a refined gap dependency can be achieved). In other words, do heavy tails break the implicit adaption of Online Learning algorithms to the stochastic regime (in the worst case)? One promising direction is to still utilize log-barrier regularizer together with potentially more advance learning rate and/or trimming threshold design. Notably, one unique advantage of it over the detect-switch framework is that it typically directly extends the BOBW regret guarantee to the corrupted regime, which is an intermediate regime that smoothly extrapolates between the purely adversarial and stochastic regime (Zimmert & Seldin, 2021). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank anonymous paper reviewers for their insightful feedback, especially reviewer $985\\mathrm{G}$ , who carefully read the proof and pointed out some gaps from the analysis in an earlier version of the manuscript. This work is supported in part by the NSF grants under CNS-2312833, CNS-2312835, and CNS-2153220, the Commonwealth Cyber Initiative (CCI), and Nokia Corporation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Agarwal, A., Luo, H., Neyshabur, B., and Schapire, R. E. Corralling a band of bandit algorithms. In Conference on Learning Theory, pp. 12\u201338. PMLR, 2017.   \nAgarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Reinforcement learning: Theory and algorithms. 2021. URL https://rltheorybook.github.io/.   \nAgarwal, N. and Singh, K. The price of differential privacy for online learning. In International Conference on Machine Learning, pp. 32\u201340. PMLR, 2017.   \nAgrawal, S. and Goyal, N. Near-optimal regret bounds for thompson sampling. Journal of the ACM (JACM), 64(5):1\u201324, 2017.   \nAgrawal, S., Juneja, S. K., and Koolen, W. M. Regret minimization in heavy-tailed bandits. In Conference on Learning Theory, pp. 26\u201362. PMLR, 2021.   \nAgrawal, S., Mathieu, T., Basu, D., and Maillard, O.-A. Crimed: Lower and upper bounds on regret for bandits with unbounded stochastic corruption. In International Conference on Algorithmic Learning Theory, pp. 74\u2013124. PMLR, 2024.   \nAshutosh, K., Nair, J., Kagrecha, A., and Jagannathan, K. Bandit algorithms: Letting go of logarithmic regret for statistical robustness. In International Conference on Artificial Intelligence and Statistics, pp. 622\u2013630. PMLR, 2021.   \nAudibert, J.-Y. and Bubeck, S. Minimax policies for adversarial and stochastic bandits. In COLT, pp. 217\u2013226, 2009.   \nAudibert, J.-Y. and Bubeck, S. Regret bounds and minimax policies under partial monitoring. The Journal of Machine Learning Research, 11:2785\u20132836, 2010.   \nAuer, P. and Chiang, C.-K. An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits. In Conference on Learning Theory, pp. 116\u2013120. PMLR, 2016.   \nAuer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235\u2013256, 2002a.   \nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002b.   \nBubeck, S. and Slivkins, A. The best of both worlds: Stochastic and adversarial bandits. In Conference on Learning Theory, pp. 42\u20131. JMLR Workshop and Conference Proceedings, 2012.   \nBubeck, S., Cesa-Bianchi, N., and Lugosi, G. Bandits with heavy tail. IEEE Transactions on Information Theory, 59(11):7711\u20137717, 2013.   \nChen, M., Gao, C., and Ren, Z. Robust covariance and scatter matrix estimation under huber\u2019s contamination model. The Annals of Statistics, 46(5):1932\u20131960, 2018.   \nChen, Y., Huang, J., Dai, Y., and Huang, L. uniinf: Best-of-both-worlds algorithm for parameter-free heavy-tailed mabs. arXiv preprint arXiv:2410.03284, 2024.   \nCont, R. Empirical properties of asset returns: stylized facts and statistical issues. Quantitative finance, 1(2):223, 2001.   \nDann, C., Wei, C.-Y., and Zimmert, J. A blackbox approach to best of both worlds in bandits and beyond. In The Thirty Sixth Annual Conference on Learning Theory, pp. 5503\u20135570. PMLR, 2023.   \nDwork, C., Roth, A., et al. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \nFoster, D. J., Gentile, C., Mohri, M., and Zimmert, J. Adapting to misspecification in contextual bandits. arXiv preprint arXiv:2107.05745, 2021.   \nGarcelon, E., Perchet, V., Pike-Burke, C., and Pirotta, M. Local differential privacy for regret minimization in reinforcement learning. Advances in Neural Information Processing Systems, 34: 10561\u201310573, 2021.   \nGenalti, G., Marsigli, L., Gatti, N., and Metelli, A. M. $(\\varepsilon,u)$ -adaptive regret minimization in heavy-tailed bandits, 2024.   \nGuan, Z., Ji, K., Bucci Jr, D. J., Hu, T. Y., Palombo, J., Liston, M., and Liang, Y. Robust stochastic bandit algorithms under probabilistic unbounded adversarial attack. In Proceedings of the aaai conference on artificial intelligence, volume 34, pp. 4036\u20134043, 2020.   \nHamza, A. B. and Krim, H. Image denoising: A nonlinear robust statistical approach. IEEE transactions on signal processing, 49(12):3045\u20133054, 2001.   \nHsu, D. and Sabato, S. Heavy-tailed regression with a generalized median-of-means. In International Conference on Machine Learning, pp. 37\u201345. PMLR, 2014.   \nHuang, J., Dai, Y., and Huang, L. Adaptive best-of-both-worlds algorithm for heavy-tailed multiarmed bandits. In international conference on machine learning, pp. 9173\u20139200. PMLR, 2022.   \nHuang, J., Zhong, H., Wang, L., and Yang, L. Tackling heavy-tailed rewards in reinforcement learning with function approximation: Minimax optimal and instance-dependent regret bounds. Advances in Neural Information Processing Systems, 36, 2024.   \nHuber, P. J. Robust statistical procedures. SIAM, 1996.   \nIto, S. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In Conference on Learning Theory, pp. 2552\u20132583. PMLR, 2021.   \nIto, S. and Takemura, K. Best-of-three-worlds linear bandit algorithm with variance-adaptive regret bounds. In The Thirty Sixth Annual Conference on Learning Theory, pp. 2653\u20132677. PMLR, 2023.   \nIto, S., Tsuchiya, T., and Honda, J. Adversarially robust multi-armed bandit algorithm with variancedependent regret bounds. In Conference on Learning Theory, pp. 1421\u20131422. PMLR, 2022.   \nJin, C., Jin, T., Luo, H., Sra, S., and Yu, T. Learning adversarial markov decision processes with bandit feedback and unknown transition. In International Conference on Machine Learning, pp. 4860\u20134869. PMLR, 2020.   \nJin, T. and Luo, H. Simultaneously learning stochastic and adversarial episodic mdps with known transition. Advances in neural information processing systems, 33:16557\u201316566, 2020.   \nJin, T., Huang, L., and Luo, H. The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. Advances in Neural Information Processing Systems, 34:20491\u201320502, 2021.   \nJin, T., Liu, J., and Luo, H. Improved best-of-both-worlds guarantees for multi-armed bandits: Ftrl with general regularizers and multiple optimal arms. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nJin, T., Liu, J., and Luo, H. Improved best-of-both-worlds guarantees for multi-armed bandits: Ftrl with general regularizers and multiple optimal arms. Advances in Neural Information Processing Systems, 36, 2024.   \nKato, M. and Ito, S. Best-of-both-worlds linear contextual bandits. arXiv preprint arXiv:2312.16489, 2023.   \nKong, F., Zhao, C., and Li, S. Best-of-three-worlds analysis for linear bandits with follow-theregularized-leader algorithm. In The Thirty Sixth Annual Conference on Learning Theory, pp. 657\u2013673. PMLR, 2023.   \nKuroki, Y., Rumi, A., Tsuchiya, T., Vitale, F., and Cesa-Bianchi, N. Best-of-both-worlds algorithms for linear contextual bandits. arXiv preprint arXiv:2312.15433, 2023.   \nLai, T. L. and Robbins, H. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4\u201322, 1985.   \nLattimore, T. and Szepesv\u00e1ri, C. Bandit algorithms. Cambridge University Press, 2020.   \nLee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. Advances in neural information processing systems, 33: 15522\u201315533, 2020.   \nLee, C.-W., Luo, H., Wei, C.-Y., Zhang, M., and Zhang, X. Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In International Conference on Machine Learning, pp. 6142\u20136151. PMLR, 2021.   \nLee, J., Honda, J., Ito, S., and Oh, M.-h. Follow-the-perturbed-leader with $\\mathrm{fr}\\backslash\\ '\\{\\mathrm{e}\\}$ chet-type tail distributions: Optimality in adversarial bandits and best-of-both-worlds. arXiv preprint arXiv:2403.05134, 2024.   \nLee, K. and Lim, S. Minimax optimal bandits for heavy tail rewards. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \nLi, L.-F., Zhao, P., and Zhou, Z.-H. Improved algorithm for adversarial linear mixture mdps with bandit feedback and unknown transition. In Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.   \nLu, S., Wang, G., Hu, Y., and Zhang, L. Optimal algorithms for lipschitz bandits with heavy-tailed rewards. In international conference on machine learning, pp. 4154\u20134163. PMLR, 2019.   \nNeu, G. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. Advances in Neural Information Processing Systems, 28, 2015.   \nRakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647, 2011.   \nRay Chowdhury, S. and Gopalan, A. Bayesian optimization under heavy-tailed payoffs. Advances in Neural Information Processing Systems, 32, 2019.   \nRen, W., Zhou, X., Liu, J., and Shroff, N. B. Multi-armed bandits with local differential privacy. arXiv preprint arXiv:2007.03121, 2020.   \nShao, H., Yu, X., King, I., and Lyu, M. R. Almost optimal algorithms for linear stochastic bandits with heavy-tailed payoffs. Advances in Neural Information Processing Systems, 31, 2018.   \nSlivkins, A. Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272, 2019.   \nTao, Y., Wu, Y., Zhao, P., and Wang, D. Optimal rates of (locally) differentially private heavy-tailed multi-armed bandits. In International Conference on Artificial Intelligence and Statistics, pp. 1546\u20131574. PMLR, 2022.   \nTossou, A. and Dimitrakakis, C. Achieving privacy in the adversarial multi-armed bandit. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \nTsuchiya, T., Ito, S., and Honda, J. Stability-penalty-adaptive follow-the-regularized-leader: Sparsity, game-dependency, and best-of-both-worlds. Advances in Neural Information Processing Systems, 36, 2024.   \nVershynin, R. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nWei, C.-Y. and Luo, H. More adaptive algorithms for adversarial bandits. In Conference On Learning Theory, pp. 1263\u20131291. PMLR, 2018.   \nWu, Y., Zhou, X., Tao, Y., and Wang, D. On private and robust bandits. Advances in Neural Information Processing Systems, 36, 2024.   \nXue, B., Wang, G., Wang, Y., and Zhang, L. Nearly optimal regret for stochastic linear bandits with heavy-tailed payoffs. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 2936\u20132942, 2021.   \nZhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33: 15383\u201315393, 2020.   \nZheng, K., Cai, T., Huang, W., Li, Z., and Wang, L. Locally differentially private (contextual) bandits learning. Advances in Neural Information Processing Systems, 33:12300\u201312310, 2020.   \nZhong, H., Huang, J., Yang, L., and Wang, L. Breaking the moments condition barrier: No-regret algorithm for bandits with super heavy-tailed payoffs. Advances in Neural Information Processing Systems, 34:15710\u201315720, 2021.   \nZhuang, V. and Sui, Y. No-regret reinforcement learning with heavy-tailed rewards. In International Conference on Artificial Intelligence and Statistics, pp. 3385\u20133393. PMLR, 2021.   \nZimmert, J. and Lattimore, T. Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits. In Conference on Learning Theory, pp. 3285\u20133312. PMLR, 2022.   \nZimmert, J. and Seldin, Y. Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1\u201349, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we give a comprehensive discussion on the related work. ", "page_idx": 14}, {"type": "text", "text": "Regret Minimization in the BOBW Setting (with Bounded Losses). There are mainly two different approaches towards enjoying BOBW regret guarantees. The first approach is the detectswitch framework initially proposed by Bubeck & Slivkins (2012). The framework runs in a two-stage manner: In the first stage, the algorithm performs some tests in every round and carefully maintain the sampling distribution over all actions. Once any of the tests fails, it runs any off-the-shelf algorithm purely for the adversarial regime in the remaining rounds and inherits the regret guarantee of that algorithm. Bubeck & Slivkins (2012) proposed an algorithm achieving near-optimal regret in the adversarial regime and $O$ $((\\log T)^{2}K/\\dot{\\Delta})$ regret in the stochastic regime, both with high-probability. Auer & Chiang (2016) further refined the tests and improved the gap-dependent term to the optimal $\\sum_{i:\\Delta_{i}>0}(1/\\Delta_{i})$ . This framework has been generalized to setups including linear bandits (Lee et al., 2021) and bandits with feedback graph (Kong et al., 2023). ", "page_idx": 14}, {"type": "text", "text": "Somewhat surprisingly, since the work by Wei & Luo (2018), there have been a large group of papers showing that Online Learning algorithms (e.g., OMD), originally for the adversarial regime, implicit adapts to stochastic environments and enjoys logarithmic regrets. BOBW guarantees of Online Learning algorithms have been established in quite broad setups including MAB (Wei & Luo, 2018; Zimmert & Seldin, 2021; Jin et al., 2023), (contextual) linear bandits (Ito & Takemura, 2023; Kong et al., 2023; Kuroki et al., 2023; Kato & Ito, 2023), and (tabular) Markov Decision Processes (MDPs) (Jin & Luo, 2020; Jin et al., 2021). One desirable advantage of this approach is that, thanks to the flexibility of Online Learning framework, it is possible to additionally enjoy some other adaptive regret bounds on top of the standard BOBW guarantee (Ito, 2021; Tsuchiya et al., 2024). ", "page_idx": 14}, {"type": "text", "text": "Regret Minimization in Heavy-tailed Stochastic Environments. There are a large body of works focusing on regret minimization in the stochastic environment where the losses are potentially heavytailed (Bubeck et al., 2013; Shao et al., 2018; Lu et al., 2019; Ray Chowdhury & Gopalan, 2019; Zhong et al., 2021; Agrawal et al., 2021; Xue et al., 2021; Lee & Lim, 2022; Zhuang & Sui, 2021; Huang et al., 2024). The general recipe of the algorithm design is to derive concentration results for the loss mean estimation with robust estimators (e.g., median-of-means (Hsu & Sabato, 2014), truncated/trimmed estimator (Bubeck et al., 2013), and Huber estimator (Huber, 1996)), and then integrate them with celebrated optimism-based algorithms. ", "page_idx": 14}, {"type": "text", "text": "High-probability Regrets in Adversarial MAB and the BOBW Setting. The central piece towards high-probability bounds in adversarial MAB lies in balancing between the variance of the loss estimator and the total regret. In general, there are three types of approaches in the literature: 1) adding negative bonus to the Importance-Weighted (IW) estimates (Auer et al., 2002b; Zimmert & Lattimore, 2022), 2) utilizing log-barrier regularizer together with increasing learning rates (Lee et al., 2020), and 3) replacing the IW estimator with the Implicit-eXploration (IX) estimator (Neu, 2015). While the IX estimator enables a simplified analysis and is thus widely-used (e.g., Jin et al. (2020); Li et al. (2024)), it heavily exploits the non-negativity of the losses, and it is unclear to us how to adapt it to the case of potentially-negative losses. For a comparison between the first two approaches, readers are referred to Foster et al. (2021) and Zimmert & Lattimore (2022). The detect-switch framework by Bubeck & Slivkins (2012) naturally provides high-probability bounds in BOBW, whenever a subroutine algorithm achieving high-probability regret in the adversarial regime (e.g., EXP3.P (Auer et al., 2002b)) is integrated in a plug-and-play manner. While the Online Learning framework naturally provides BOBW guarantee via elegant analysis in expectation (Zimmert & Seldin, 2021), it is unclear how to adapt it for high-probability guarantees. ", "page_idx": 14}, {"type": "text", "text": "Adaptive/Parameter-free Heavy-tailed MAB. Throughout this work, we assume that the heavy tail parameters $u$ and $v$ are known to the learning algorithm. In this case, Bubeck et al. (2013) showed minimax lower bound $\\Omega(u T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}})$ and gap-dependent lower bound $\\begin{array}{r}{\\Omega\\left(\\sum_{i:\\Delta_{i}>0}(u/\\Delta_{i})^{1/v}(\\log T)\\right)}\\end{array}$ using stochastic environments, so these lower bounds also apply to the adversarial regime. However, in practice one may not know the values of $u,v$ exactly in advance, therefore it is natural to ask whether it is possible to enjoy the same regret rate as if $u,v$ were known, which we refer to as \u201cadaptation (to heavy tails) for free\u201d. On the negative side, Genalti et al. (2024) showed that there is no such an algorithm that can enjoy $\\widetilde{O}(u T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}})$ worst-case regret for all unknown $u,v$ in the stochastic regime. In terms of gap-dependent regret, similar \u201cno adaptation for free\u201d effect has been shown in an earlier work by Ashutosh et al. (2021). On the positive side, in the same work by ", "page_idx": 14}, {"type": "text", "text": "Genalti et al. (2024), it is shown that, somewhat surprisingly, with the same \u201ctruncated non-negative losses\u201d assumption (Assumption 2), a UCB-based algorithm proposed by them enjoys \u201cadaptation for free\u201d in terms of both minimax and gap-dependent regrets in the stochastic regime. With the same assumption in the adversarial regime, Huang et al. (2022) proposed a variant of FTRL, which achieves the $O(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}+{\\sqrt{K T}})$ worst-case regret guarantee. In a very recent work by Chen et al. (2024), an FTRL-based algorithm named uniINF is shown to enjoy the BOBW guarantee even when $u$ and $v$ are both unknown, although the gap dependency of their logarithmic regret is $K/\\Delta^{1/v}$ , and still Assumption 2 is needed. Exploring broader scenarios in which we can enjoy \u201cadaptation for free\u201d (and to what extent) is an interesting future direction. ", "page_idx": 15}, {"type": "text", "text": "B Omitted Details in Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide complete proof for Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "To begin with, we first define $\\ell_{t,i}^{\\prime}$ as the trimmed value of $\\ell_{t,i}$ with respect to $M$ , i.e., $\\ell_{t,i}^{\\prime}\\;=\\;$ $\\ell_{t,i}\\mathbb{I}\\{|\\ell_{t,i}|\\leqslant M\\}$ . Then, we decompose the pseudo-regret by ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=1}^{T}\\left(\\mu_{t,a_{t}}-\\ell_{t,a_{t}}^{\\prime}\\right)+\\sum_{t=1}^{T}\\left(\\ell_{t,i^{*}}^{\\prime}-\\mu_{t,i^{*}}\\right)+\\sum_{t=1}^{T}\\left(\\ell_{t,a_{t}}^{\\prime}-\\ell_{t,i^{*}}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.1 Bounding TRIMERR ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 1. For any fixed trimming threshold $M_{t,i}=M>0$ and $\\zeta<1/(e\\log T)$ , with probability at least $1-\\zeta$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TRIMERR}\\leqslant2T u^{1+v}M^{-v}+8\\sqrt{T u^{1+v}M^{1-v}\\log(2\\log(T)/\\zeta)}+8M\\log(2\\log(T)/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 1. First note that \u00b5\u2032t,at \u2212\u2113\u2032t,at t=1,...,T form a martingale difference sequence adapted to the filtration $\\mathcal{F}_{1},\\ldots,\\mathcal{F}_{T}$ where $\\mathcal{F}_{t}:=\\sigma(\\boldsymbol{a}_{1},\\ell_{1,a_{1}},\\dots,\\boldsymbol{a}_{t-1},\\ell_{t-1,a_{t-1}})$ . Moreover, we have $\\left|\\mu_{t,a_{t}}^{\\prime}-\\ell_{t,a_{t}}^{\\prime}\\right|\\leqslant2M$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[(\\mu_{t,a_{t}}^{\\prime}-\\ell_{t,a_{t}}^{\\prime})^{2}\\middle|\\mathcal{F}_{t}\\right]=\\displaystyle\\sum_{i=1}^{K}w_{t,i}\\mathbb{E}\\left[(\\mu_{t,i}^{\\prime}-\\ell_{t,i}^{\\prime})^{2}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {=\\displaystyle\\sum_{i=1}^{K}w_{t,i}\\left(\\mathbb{E}\\left[(\\ell_{t,i}^{\\prime})^{2}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]+(\\mu_{t,i}^{\\prime})^{2}-2\\mu_{t,i}^{\\prime}\\mathbb{E}\\left[\\ell_{t,i}^{\\prime}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]\\right)}\\\\ {\\leqslant\\displaystyle\\sum_{i=1}^{K}w_{t,i}\\mathbb{E}\\left[(\\ell_{t,i}^{\\prime})^{2}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ {\\leqslant\\displaystyle\\sum_{i=1}^{K}w_{t,i}\\mathbb{E}\\left[(\\ell_{t,i}^{\\prime})^{1+\\nu}M^{1-\\nu}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ {\\leqslant\\displaystyle\\sum_{i=1}^{K}w_{t,i}\\mathbb{E}\\left[(\\ell_{t,i}^{\\prime})^{1+\\nu}M^{1-\\nu}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ {\\leqslant u^{1+\\nu}M^{1-\\nu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the last inequality we utilize the fact that $\\textstyle\\sum_{i=1}^{K}w_{t,i}=1$ and Assumption 1. By Lemma 11, we have with probability at least $1-\\zeta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\mu_{t,a_{t}}^{\\prime}-\\ell_{t,a_{t}}^{\\prime}\\right)\\leqslant4\\sqrt{T u^{1+v}M^{1-v}\\log(\\log(T)/\\zeta)}+4M\\log(\\log(T)/\\zeta).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Together with Lemma 8 which bounds $\\mu_{t,a_{t}}-\\mu_{t,a_{t}}^{\\prime}$ , we arrive at ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\mu_{t,a_{t}}-\\ell_{t,a_{t}}^{\\prime}\\right)\\leqslant T u^{1+v}M^{-v}+4\\sqrt{T u^{1+v}M^{1-v}\\log(\\log(T)/\\zeta)}+4M\\log(\\log(T)/\\zeta).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With exactly the same reasoning, we have with probability at least $1-\\zeta$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\ell_{t,i^{*}}^{\\prime}-\\mu_{t,i^{*}}\\right)\\leqslant T u^{1+v}M^{-v}+4\\sqrt{T u^{1+v}M^{1-v}\\log(\\log(T)/\\zeta)}+4M\\log(\\log(T)/\\zeta).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking a union bound over the two terms completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.2 Bounding TRIMREG ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For any $i\\in[K]$ , let $y^{i}$ be the $K$ -dimensional one-hot vector such that the $i$ -th element is 1, and all the others are zero. Define vector $y^{\\prime}:=(1-\\lambda)y^{i^{*}}+\\lambda w_{0}$ , where $w_{0}=(1/K,\\dots,1/K)$ is the uniform exploration. We can rewrite TRIMREG as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\left(\\ell_{t,a_{t}}^{\\prime}-\\ell_{t,i^{*}}^{\\prime}\\right)=\\displaystyle\\sum_{t=1}^{T}\\left(\\langle w_{t},\\widehat{\\ell}_{t}\\rangle-\\langle y^{i^{*}},\\ell_{t}^{\\prime}\\rangle\\right)}&{}\\\\ {=\\displaystyle\\sum_{\\underset{\\mathrm{TRIMREGI}}{=}}^{T}+\\sum_{\\underset{\\mathrm{TRIMREGI}}{=}}^{T}+\\underbrace{\\sum_{t=1}^{T}\\langle y^{\\prime}-y^{i^{*}},\\ell_{t}^{\\prime}\\rangle}_{\\displaystyle\\mathrm{TRIMREGII}}+\\underbrace{\\sum_{t=1}^{T}\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle}_{\\displaystyle\\mathrm{TRIMREGII}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the following subsections, we bound each of the three terms in the right-hand-sight. To give a preview: ", "page_idx": 16}, {"type": "text", "text": "\u2022 TRIMREG I is the standard online learning regret over loss sequence $\\widehat{\\ell}_{1},\\cdot\\cdot\\cdot,\\widehat{\\ell}_{T}$ , and is handled using regret analysis of OMD. \u2022 TRIMREG II is a bias term due to that we cannot directly compete with $y^{i^{*}}$ and have to choose a close neighbor $y^{\\prime}$ . It is under control since $y^{\\prime}$ and $y^{i^{*}}$ are close to each other. \u2022 TRIMREG III is the main challenge in getting high-probability bounds in adversarial bandits. While it contains some terms that could be potentially large due to the high variance of $\\widehat{\\ell}_{t}$ , they would be cancelled by some negative terms in TRIMREG I introduced by the increasi ng learning rates so that eventually the sum of the three terms can be bounded. ", "page_idx": 16}, {"type": "text", "text": "B.2.1 Bounding TRIMREG I ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 2. Suppose \u03b7 \u2a7d101M and $\\lambda=T^{\\frac{-\\,v}{1+v}}K^{\\frac{-1}{1+v}}$ , for any $M_{t,i}=M_{j}$ , Algorithm 1 ensures that with probability at least $1-\\zeta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TRIMREGI}\\leqslant\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho r\\rangle}{10\\eta\\log T}+5\\eta M^{1-\\upsilon}T u^{1+\\upsilon}}\\\\ &{\\qquad\\qquad\\qquad+\\ 20\\eta\\sqrt{T u^{1+\\upsilon}M^{3-\\upsilon}\\log(\\log(T)/\\zeta)}+10\\eta M^{2}\\log(\\log(T)/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 2. The proof is a combination of Agarwal et al. (2017, Lemma 12) and Lee et al. (2020, Lemma 2.1). For the (active) OMD update rule $\\begin{array}{r}{w_{t+1}=\\mathrm{argmin}_{x\\in\\Omega^{\\prime}}\\left(\\langle x,\\widehat{\\ell}_{t}\\rangle+D_{\\psi_{t}}(x,w_{t})\\right)}\\end{array}$ in Algorithm 1, it is equivalent to first obtaining some intermediate vector $\\widetilde w_{t+1}$ such that $\\nabla\\psi_{t}\\left(\\widetilde{\\boldsymbol{w}}_{t+1}\\right)=\\nabla\\psi_{t}\\left(\\boldsymbol{w}_{t}\\right)-\\widehat{\\ell}_{t}$ , and then obtaining $\\begin{array}{r}{w_{t+1}=\\operatorname*{argmin}_{x\\in\\Omega^{\\prime}}D_{\\psi_{t}}(x,\\widetilde{w}_{t+1})}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "For any competitor $y\\in\\Omega^{\\prime}$ , in each round $t\\in[T]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle w_{t}-y,\\widehat{\\ell}_{t}\\rangle=\\langle w_{t}-y,\\nabla\\psi_{t}\\left(w_{t}\\right)-\\nabla\\psi_{t}\\left(\\widetilde{w}_{t+1}\\right)\\rangle}\\\\ &{\\overset{\\mathrm{(a)}}{=}D_{\\psi_{t}}(y,w_{t})-D_{\\psi_{t}}(y,\\widetilde{w}_{t+1})+D_{\\psi_{t}}(w_{t},\\widetilde{w}_{t+1})}\\\\ &{\\overset{\\mathrm{(b)}}{\\leqslant}D_{\\psi_{t}}(y,w_{t})-D_{\\psi_{t}}(y,w_{t+1})+D_{\\psi_{t}}(w_{t},\\widetilde{w}_{t+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where in step (a) we utilize the definition of Bregman divergence (or the so-called \u201cthree-point property\u201d) and step (b) is due to the generalized Pythagorean theorem. ", "page_idx": 16}, {"type": "text", "text": "Throughout this proof, we choose $\\lambda\\;=\\;T^{\\frac{-\\,v}{1+v}}K^{\\frac{-\\,1}{1+v}}$ as in Algorithm 1. Recall that we define $y^{\\prime}:={\\bar{(}}1-\\lambda)y^{i^{*}}+\\lambda w_{0}$ . Therefore, for any $i\\in[K]$ , we have $y_{i}^{\\prime}\\geqslant\\lambda/K$ and hence $y^{\\prime}\\in\\Omega^{\\prime}$ . ", "page_idx": 17}, {"type": "text", "text": "For any scalar $c>0$ , we define function $h(c):=c-1-\\log c.$ , which is always non-negative. Taking the summation over all $T$ rounds and letting $y=y^{\\prime}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{T}\\langle w_{t}-y^{\\prime},\\widehat{\\ell}_{t}\\rangle=\\displaystyle\\sum_{t=1}^{T}\\big(D_{\\psi_{t}}(y^{\\prime},w_{t})-D_{\\psi_{t}}(y^{\\prime},w_{t+1})+D_{\\psi_{t}}(w_{t},\\widetilde{w}_{t+1})\\big)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leqslant D_{\\psi_{1}}(y^{\\prime},w_{1})+\\displaystyle\\sum_{t=1}^{T-1}\\big(D_{\\psi_{t+1}}(y^{\\prime},w_{t+1})-D_{\\psi_{t}}(y^{\\prime},w_{t+1})\\big)+\\displaystyle\\sum_{t=1}^{T}D_{\\psi_{t}}(w_{t},\\widetilde{w}_{t+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the inequality we get rid of $-D_{\\psi_{T}}(y^{\\prime},w_{T+1})$ since Bregman divergence is always nonnegative. In the following, we bound each of these three terms in the right-hand-side. ", "page_idx": 17}, {"type": "text", "text": "Step 1: bounding the first term. Plugging in the exact expression of Bregman divergence concerning the log-barrier regularizer $\\psi_{t}$ we define in Algorithm 1, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{D_{\\psi_{1}}(y^{\\prime},w_{1})=\\frac{1}{\\eta}\\sum_{i=1}^{K}h\\left(\\frac{y_{i}^{\\prime}}{w_{1,i}}\\right)=\\frac{1}{\\eta}\\sum_{i=1}^{K}\\left(\\frac{y_{i}^{\\prime}}{w_{1,i}}-1-\\log\\left(\\frac{y_{i}^{\\prime}}{w_{1,i}}\\right)\\right)}\\\\ &{}&{\\qquad=\\frac{1}{\\eta}\\sum_{i=1}^{K}\\log\\left(\\frac{1}{K y_{i}^{\\prime}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By choosing $\\lambda=T^{\\frac{-\\,v}{1+v}}K^{\\frac{-1}{1+v}}$ , we have $y_{i}^{\\prime}\\geqslant\\lambda/K=T^{\\frac{-v}{1+v}}K^{-1-\\frac{1}{1+v}}$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}\\sum_{i=1}^{K}\\log\\left(\\frac{1}{K y_{i}^{\\prime}}\\right)\\leqslant\\frac{K\\log\\left(T^{\\frac{v}{1+v}}K^{\\frac{1}{1+v}}\\right)}{\\eta}\\leqslant\\frac{K\\log T}{\\eta}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that here we utilize the assumption that $K\\leqslant T$ . ", "page_idx": 17}, {"type": "text", "text": "Step 2: bounding the second term. Plugging in the expression of Bregman divergence associated with log-barrier, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T-1}\\left(D_{\\psi_{t+1}}(y^{\\prime},w_{t+1})-D_{\\psi_{t}}(y^{\\prime},w_{t+1})\\right)=\\sum_{i=1}^{K}\\sum_{t=1}^{T-1}\\left(\\frac{1}{\\eta_{t+1,i}}-\\frac{1}{\\eta_{t,i}}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t+1,i}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we look at each action $i$ . Recall that, if the learning rate does not increase at round $t$ , then $\\begin{array}{r}{\\left(\\frac{1}{\\eta_{t+1,i}}-\\frac{1}{\\eta_{t,i}}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t+1,i}}\\right)}\\end{array}$ is simply 0. Otherwise, we have $\\eta_{t+1,i}~>~\\eta_{t,i}$ , and as a result, $\\begin{array}{r}{\\left(\\frac{1}{\\eta_{t+1,i}}-\\frac{1}{\\eta_{t,i}}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t+1,i}}\\right)<0.}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Therefore, if we let $n_{i}$ denote the total number of learning rate changes in action $i$ , and let $t_{n_{i}}$ denote the round when the last change happens (such that $\\eta_{T,i}=\\eta_{t_{n_{i}}+1,i}=\\kappa\\eta_{t_{n_{i}},i}=\\kappa^{n_{i}}\\eta)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{T-1}\\left(\\frac{1}{\\eta_{t+1,i}}-\\frac{1}{\\eta_{t,i}}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t+1,i}}\\right)\\leqslant\\left(\\frac{1}{\\eta_{t_{n_{i}}+1,i}}-\\frac{1}{\\eta_{t_{n_{i}},i}}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\frac{1}{\\kappa^{n_{i}}\\eta}-\\frac{\\kappa}{\\kappa^{n_{i}}\\eta}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1-\\kappa}{\\kappa^{n_{i}}\\eta}h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Lemma 9, when $\\lambda\\;=\\;T^{\\frac{-\\,v}{1+v}}K^{\\frac{-1}{1+v}}$ , we have $n_{i}\\,\\leqslant\\,\\log_{2}(T^{\\frac{v}{1+v}}K^{\\frac{1}{1+v}})\\,\\leqslant\\,\\log_{2}T$ and $\\eta_{t,i}~\\leqslant$ $e^{\\frac{\\log_{2}T}{\\log T}}\\eta\\leqslant5\\eta$ . Together with $\\begin{array}{r}{1-\\kappa=1-e^{1/\\log T}\\leqslant-\\frac{1}{\\log T}}\\end{array}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1-\\kappa}{\\kappa^{n_{i}}\\eta}h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right)\\leqslant\\frac{-h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right)}{5\\eta\\log T}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is left to upper-bound term $\\begin{array}{r}{-h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right)}\\end{array}$ Noticing that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}=\\frac{y_{i}^{\\prime}\\rho_{T,i}}{2}\\leqslant\\frac{\\rho_{T,i}}{2}=\\frac{1}{w_{t_{n_{i}}+1,i}}\\leqslant T,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n-h\\left(\\frac{y_{i}^{\\prime}}{w_{t_{n_{i}}+1,i}}\\right)=-h\\left(\\frac{y_{i}^{\\prime}\\rho_{T,i}}{2}\\right)=\\log\\left(\\frac{y_{i}^{\\prime}\\rho_{T,i}}{2}\\right)+1-\\frac{y_{i}^{\\prime}\\rho_{T,i}}{2}\\leqslant\\log T+1-\\frac{y_{i}^{\\prime}\\rho_{T,i}}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking the summation over all actions, we finish bounding the second term as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\sum_{t=1}^{T-1}\\left(\\frac{1}{\\eta_{t+1,i}}-\\frac{1}{\\eta_{t,i}}\\right)h\\left(\\frac{y_{i}^{\\prime}}{w_{t+1,i}}\\right)\\leqslant\\sum_{i=1}^{K}\\frac{\\log T+1-\\frac{y_{i}^{\\prime}\\rho_{T,i}}{2}}{5\\eta\\log T}\\leqslant\\frac{K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step 3: bounding the third term. Given the fact that $\\nabla\\psi_{t}\\left(\\widetilde{\\boldsymbol{w}}_{t+1}\\right)=\\nabla\\psi_{t}\\left(\\boldsymbol{w}_{t}\\right)-\\widehat{\\ell}_{t}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{-1}{\\widetilde{w}_{t+1,i}\\eta_{t,i}}=\\frac{-1}{w_{t,i}\\eta_{t,i}}-\\widehat{\\ell}_{t,i},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{w_{t,i}}{\\widetilde{w}_{t+1,i}}=1+\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}D_{\\psi_{t}}(w_{t},\\widetilde{w}_{t+1})=\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{K}\\frac{1}{\\eta_{t,i}}\\cdot h\\left(\\frac{w_{t,i}}{\\widetilde{w}_{t+1,i}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\frac{1}{\\eta_{t,i}}\\cdot h\\left(1+\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\frac{1}{\\eta_{t,i}}\\cdot\\left(\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}-\\log\\left(1+\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To proceed, we first show that if $\\begin{array}{r}{\\eta\\leqslant\\frac{1}{10M}}\\end{array}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}\\geqslant-0.5.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First, it is trivially true whenever $\\ell_{t,i}^{\\prime}\\geqslant0$ as the left-hand-sight is non-negative. Since $\\ell_{t,i}^{\\prime}$ is at most as negative as $-M$ and $\\eta_{t,i}\\leqslant5\\eta$ , it is left to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\eta w_{t,i}\\frac{M}{w_{t,i}}\\leqslant0.1,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is clearly satisfied when $\\begin{array}{r}{\\eta\\leqslant\\frac{1}{10M}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "By applying the fact that $c-\\log(1+c)\\leqslant c^{2},\\forall c\\geqslant-0.5$ to Eq. (31), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}D_{\\psi_{t}}(w_{t},\\widetilde{w}_{t+1})\\leqslant\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\frac{1}{\\eta_{t,i}}\\left(\\eta_{t,i}w_{t,i}\\widehat{\\ell}_{t,i}\\right)^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{t=1}^{T}\\eta_{t,a_{t}}(w_{t,a_{t}})^{2}(\\ell_{t,a_{t}}^{\\prime}/w_{t,a_{t}})^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leqslant5\\eta\\displaystyle\\sum_{t=1}^{T}(\\ell_{t,a_{t}}^{\\prime})^{2}\\leqslant5\\eta M^{1-\\upsilon}\\displaystyle\\sum_{t=1}^{T}\\left\\vert\\ell_{t,a_{t}}^{\\prime}\\right\\vert^{1+\\upsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now finish bounding the third term by deriving a high-probability bound on $\\textstyle\\sum_{t=1}^{T}\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+v}$ (and multiplying it by $5\\eta M^{1-v})$ ). ", "page_idx": 19}, {"type": "text", "text": "Consider the martingale difference sequence $\\left(\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+v}-\\mathbb{E}\\left[\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+v}\\right]\\right)_{t\\in[T]}\\right)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+v}-\\mathbb{E}\\left[\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+v}\\right]\\right|\\leqslant M^{1+v}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "almost surely, and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+\\upsilon}-\\mathbb{E}\\left[\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+\\upsilon}\\right]\\right)^{2}\\Bigg|\\mathcal{F}_{t}\\right]=\\underset{i=1}{\\overset{K}{\\sum}}w_{t,i}\\mathbb{E}\\left[\\left(\\left|\\ell_{t,i}^{\\prime}\\right|^{1+\\upsilon}-\\mathbb{E}\\left[\\left|\\ell_{t,i}^{\\prime}\\right|^{1+\\upsilon}\\right]\\right)^{2}\\Bigg|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leqslant\\underset{i=1}{\\overset{K}{\\sum}}w_{t,i}\\mathbb{E}\\left[\\left|\\ell_{t,i}^{\\prime}\\right|^{2+2\\upsilon}\\Big|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\underset{i=1}{\\overset{K}{\\sum}}w_{t,i}M^{1+\\upsilon}\\mathbb{E}\\left[\\left|\\ell_{t,i}^{\\prime}\\right|^{1+\\upsilon}\\Big|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leqslant u^{1+\\upsilon}M^{1+\\upsilon}\\int_{0}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant u^{1+\\upsilon}M^{1+\\upsilon}\\int_{0}}\\end{array}\\quad\\begin{array}{r l}{(3\\ell)}&{\\mathrm{(i)}}\\\\ {(3\\ell)^{2}}&{\\mathrm{(i)}}\\end{array}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\quad\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 11, we have with probability at least $1-\\zeta$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=1}^{T}\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+\\upsilon}\\leqslant\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+\\upsilon}\\right]+4\\sqrt{T u^{1+\\upsilon}M^{1+\\upsilon}\\log(\\log(T)/\\zeta)}+2M^{1+\\upsilon}\\log(\\log(T)/\\zeta)}\\\\ {\\leqslant T u^{1+\\upsilon}+4\\sqrt{T u^{1+\\upsilon}M^{1+\\upsilon}\\log(\\log(T)/\\zeta)}+2M^{1+\\upsilon}\\log(\\log(T)/\\zeta).\\qquad\\qquad(36)\\mathrm{~a~t~h~}\\mathrm{~W~}^{1+\\upsilon}\\bmod\\ell}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At this point, we have bounded all three terms, and putting them together completes the proof ", "page_idx": 19}, {"type": "text", "text": "B.2.2 Bounding TRIMREG II ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 3 (Upper bound on TRIMREG II). For any fixed $M>0$ and $\\lambda\\in(0,1)$ , with probability at least $1-\\zeta_{i}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{TRIMREG~II}\\leqslant\\lambda u K T+\\lambda\\sqrt{2K T u^{1+v}M^{1-v}\\log(1/\\zeta)}+\\frac43\\lambda M\\log(1/\\zeta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 3. By H\u00f6lder\u2019s inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\langle y^{\\prime}-y^{i^{*}},\\ell_{t}^{\\prime}\\rangle\\leqslant\\sum_{t=1}^{T}\\left\\|y^{\\prime}-y^{i^{*}}\\right\\|_{\\infty}\\|\\ell_{t}^{\\prime}\\|_{1}=\\sum_{t=1}^{T}\\operatorname*{max}\\{\\lambda/K,\\lambda(1-1/K)\\}\\cdot\\|\\ell_{t}^{\\prime}\\|_{1}\\leqslant\\lambda\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\left|\\ell_{t,i}^{\\prime}\\right|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define $\\overline{{\\mu}}_{t,i}=\\mathbb{E}_{\\ell\\sim P_{t,i}}\\left[\\left|\\ell\\right|\\cdot\\mathbb{I}\\{\\left|\\ell\\right|\\leqslant M\\}\\right],\\forall t\\in[T],i\\in[K],$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\overline{{\\mu}}_{t,i}\\leqslant\\mathbb{E}_{\\ell\\sim P_{t,i}}\\left[|\\ell|\\right]\\leqslant\\mathbb{E}_{\\ell\\sim P_{t,i}}\\left[|\\ell|^{1+v}\\right]^{\\frac{1}{1+v}}\\leqslant u.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying Lemma 10 to $\\left(\\left|\\ell_{t,i}^{\\prime}\\right|-\\overline{{\\mu}}_{t,i}\\right)_{t\\in[T],i\\in[K]}$ , since $\\vert\\left\\vert\\ell_{t,i}^{\\prime}\\right\\vert-\\overline{{\\mu}}_{t,i}\\right\\vert\\leqslant2M$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[(\\left|\\ell_{t,i}^{\\prime}\\right|-\\overline{{\\mu}}_{t,i})^{2}\\right]\\leqslant\\mathbb{E}\\left[\\left|\\ell_{t,i}^{\\prime}\\right|^{2}\\right]\\leqslant u^{1+v}M^{1-v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we get that with probability at least $1-\\zeta$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\left|\\ell_{t,i}^{\\prime}\\right|\\leqslant\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\overline{{\\mu}}_{t,i}+\\sqrt{2K T u^{1+v}M^{1-v}\\log(1/\\zeta)}+\\frac{4}{3}M\\log(1/\\zeta),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{TRIMREG~II}\\leqslant\\lambda u K T+\\lambda\\sqrt{2K T u^{1+v}M^{1-v}\\log(1/\\zeta)}+\\frac{4}{3}\\lambda M\\log(1/\\zeta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2.3 Bounding TRIMREG III ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 4 (Upper bound on TRIMREG III). For any fixed $M>0$ and $\\lambda\\in(0,1)$ , with probability at least $1-\\zeta_{i}$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{III}\\leqslant3T u^{1+v}M^{-v}+3M\\langle y^{\\prime},\\rho_{T}\\rangle\\log\\left(\\operatorname*{max}\\{\\sqrt{2T u^{1+v}M^{-1-v}K/\\lambda},4K/\\lambda\\}/\\zeta\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 4. We note that $\\left(\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle/M\\right)_{t\\in[T]}{}^{6}$ form a martingale sequence, and $\\mathbb{E}\\left[\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle/M\\Big|\\mathcal{F}_{t}\\right]$ is clearly finite since $w_{t,i}\\geqslant\\lambda/K$ . Moreover, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[(\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle/M)^{2}\\Big|\\mathcal{F}_{t}\\right]\\leqslant\\mathbb{E}\\left[(\\langle y^{\\prime},\\widehat{\\ell}_{t}\\rangle/M)^{2}\\Big|\\mathcal{F}_{t}\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\frac{(y_{a_{t}}^{\\prime})^{2}(\\ell_{t,a_{t}}^{\\prime}/M)^{2}}{(w_{t,a_{t}})^{2}}\\Big|\\mathcal{F}_{t}\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{K}w_{i}\\frac{(y_{i}^{\\prime})^{2}}{(w_{t,i})^{2}}\\mathbb{E}\\left[(\\ell_{t,i}^{\\prime}/M)^{2}\\Big|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ &{\\leqslant\\displaystyle\\sum_{i=1}^{K}\\frac{(y_{i}^{\\prime})^{2}}{w_{t,i}}u^{1+\\upsilon}M^{-1-\\upsilon}}\\\\ &{\\leqslant u^{1+\\upsilon}M^{-1-\\upsilon}\\langle y^{\\prime},\\rho_{T}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last step we utilize the facts that $y_{i}^{\\prime}\\leqslant1$ and $1/w_{t,i}\\leqslant\\rho_{T,i}$ . ", "page_idx": 20}, {"type": "text", "text": "Noting ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\lvert\\langle y^{\\prime},(\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime})\\rangle\\right\\lvert/M\\leqslant\\sum_{i=1}^{K}y_{i}^{\\prime}(\\frac{1}{w_{t,i}}+1)\\leqslant\\sum_{i=1}^{K}y_{i}^{\\prime}\\cdot\\frac{2}{w_{t,i}}\\leqslant2\\langle y^{\\prime},\\rho_{T}\\rangle,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by Lemma 12, we have with probability at least $1-\\zeta$ that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle\\leqslant3M\\sqrt{T u^{1+v}M^{-1-v}\\langle y^{\\prime},\\rho_{T}\\rangle\\iota}+2M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota}}\\\\ &{}&{\\stackrel{\\mathrm{(a)}}{\\leqslant}\\frac{4.5T u^{1+v}M^{-v}}{2}+\\frac{2M\\langle y^{\\prime},\\rho_{T}\\rangle\\iota}{2}+2M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota}\\\\ &{}&{\\leqslant3\\left(T u^{1+v}M^{-v}+M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\iota:=\\log(2\\operatorname*{max}\\{\\sqrt{T u^{1+v}M^{-1-v}\\langle y^{\\prime},\\rho_{T}\\rangle},1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}/\\zeta)$ and step (a) is due to the elementary inequality $\\begin{array}{r}{\\sqrt{x_{1}x_{2}}\\leqslant\\frac{x_{1}+x_{2}}{2},\\forall x_{1},x_{2}\\geqslant0}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "We complete the proof by noticing that $\\begin{array}{r}{1\\leqslant2K\\leqslant\\langle y^{\\prime},\\rho_{T}\\rangle\\leqslant\\frac{2}{\\lambda/K}}\\end{array}$ and apply it to $\\iota$ . ", "page_idx": 20}, {"type": "text", "text": "B.2.4 Putting things together ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By Lemmas 1, 2, 3, and 4, with probability at least $1-\\zeta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\displaystyle R_{T}\\leqslant2T u^{1+v}M^{-v}+8\\sqrt{T u^{1+v}M^{1-v}\\log(8\\log(T)/\\zeta)}+8M\\log(8\\log(T)/\\zeta)}}&{{}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+5\\eta M^{1-v}T u^{1+v}}}\\\\ {{\\displaystyle\\qquad+\\,20\\eta\\sqrt{T u^{1+v}M^{3-v}\\log(4\\log(T)/\\zeta)}+10\\eta M^{2}\\log(4\\log(T)/\\zeta)}}\\\\ {{\\displaystyle\\qquad+\\,\\lambda u K T+\\lambda\\sqrt{2K T u^{1+v}M^{1-v}\\log(4/\\zeta)}+\\frac{4}{3}\\lambda M\\log(4/\\zeta)}}\\\\ {{\\displaystyle\\qquad+\\,3T u^{1+v}M^{-v}+3M\\langle y^{\\prime},\\rho_{T}\\rangle\\log\\left(4\\operatorname*{max}\\{\\sqrt{2T u^{1+v}M^{-1-v}K/\\lambda},4K/\\lambda\\}/\\zeta\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "6(Re-)Scaling by $M$ is of course not necessary, which however simplifies the algebra calculations after applying Lemma 12. ", "page_idx": 20}, {"type": "text", "text": "Choosing $M=u\\cdot(T/K)^{\\frac{1}{1+v}}$ and $\\lambda=T^{\\frac{-\\,v}{1+v}}K^{\\frac{-1}{1+v}}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}\\leqslant2\\mu K^{\\top}\\Gamma^{+}r+8\\sqrt{a^{2}\\Gamma^{2}\\Gamma^{2}}\\leqslant K^{\\frac{r+1}{\\kappa+1}}\\log(8\\log(T)/\\zeta)+8u T^{\\mathrm{T+}}K^{\\frac{r-1}{\\kappa+1}}\\log(8\\log(T)/\\zeta)}\\\\ &{\\quad+\\frac{2K\\log T}{\\eta}-\\frac{\\langle y,y^{\\prime},p\\rangle}{10\\eta\\log T}+5\\eta M^{-\\mathrm{r}}T u^{4}\\quad}\\\\ &{\\quad+20\\sqrt{T u^{4}\\nu u^{3}\\cdots\\left(\\eta K\\right)^{\\frac{r}{\\kappa+1}}\\varphi}\\frac{1}{M^{2}}M^{2}\\mathrm{Bof}(4\\log(T)/\\zeta)+10\\eta M^{2}\\log(4\\log(T)/\\zeta)}\\\\ &{\\quad+u T^{\\mathrm{T+}}K^{\\frac{r+1}{\\kappa+1}}+u T^{\\mathrm{T+}}K^{\\frac{r+1}{\\kappa+1}}\\sqrt{2\\log(4/\\zeta)}+\\frac{4}{3}u T^{\\mathrm{L+}}K^{\\frac{r-1}{\\kappa+1}}\\log(4/\\zeta)}\\\\ &{\\quad+3u K^{\\mathrm{T+}}T^{\\mathrm{T+}}+3M\\langle y^{\\prime},\\rho\\rangle\\log\\Big(4\\operatorname*{max}\\{\\sqrt{2K^{\\frac{r+1}{\\kappa+1}}\\gamma^{\\frac{\\kappa}{\\kappa+1}}},4K^{\\frac{r+1}{\\kappa+1}}T^{\\mathrm{T+}}\\}/\\zeta\\Big)}\\\\ &{\\leqslant7\\mu K^{\\top}\\Gamma^{+}r^{\\mathrm{T+}}\\sqrt{2\\log(4/\\zeta)}+\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho\\rangle}{10\\eta\\log T}+5\\eta M^{-1}\\pi^{1+\\kappa}\\quad}\\\\ &{\\quad+20\\sqrt{u^{2}T^{2}\\Gamma^{2}\\kappa^{1}\\frac{r+1}{M^{2}}\\log(4\\log(T)/\\zeta)+10\\eta M^{2}\\log(4\\log(T)/\\zeta)}}\\\\ &{\\quad+3M\\langle y^{\\prime},\\rho\\rangle\\log\\left(8\\kappa T/\\zeta\\right)+o\\left(u K^{\\ast}\\mp T^{\\mathrm{i+}}\\log(T)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, choosing \u03b7 $\\begin{array}{r}{\\mathrm{\\min}\\big\\{\\frac{1}{10M},\\frac{1}{40M\\log(T)\\log(8K T/\\zeta)}\\big\\}=\\frac{1}{40M\\log(T)\\log(8K T/\\zeta)}}\\end{array}$ to cancel the terms containing $\\langle y^{\\prime},\\rho_{T}\\rangle$ ensures that ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{T}=O(u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}(\\log T)^{2}\\log(T/\\zeta)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Omitted Details in Section 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Concentration Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we derive concentration results needed for the analysis, adapted from Bubeck & Slivkins (2012) to suit our heavy-tailed case. ", "page_idx": 21}, {"type": "text", "text": "Lemma 5 (Concentration on the trimmed importance-weighted estimator). Suppose $M_{t,i}\\,=\\,u$ \u00b7 $(t/K)^{\\frac{1}{1+v}}\\cdot(\\log(\\log(T)/\\zeta))^{\\frac{-1}{3v+1}}$ . In both regimes, we have with probability at least $1-\\zeta$ that, for any $i\\in[K]$ and $t\\in[T]$ , if $\\begin{array}{r}{\\dot{(K+1)}\\leqslant t\\leqslant t_{s w}.}\\end{array}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{L}_{t,i}-\\displaystyle\\sum_{s=1}^{t}\\mu_{s,i}\\bigg|\\leqslant6u\\left(\\sqrt{(t/K)^{\\frac{1-v}{1+v}}\\left(\\displaystyle\\sum_{s=1}^{\\operatorname*{min}\\{\\tau_{i},t\\}}\\frac{1}{w_{s,i}}+\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{i}\\tau_{i}}\\right)}\\right)+K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\operatorname*{max}\\{t/\\tau_{i},}\\\\ {\\cdot\\left(\\log(2K\\log(T)/\\zeta)\\right)^{\\frac{3v}{3v+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": ",  Wwee  hfaixv se $i\\in[K]$ $\\widehat{L}_{t,i}$ $\\begin{array}{r}{\\widehat{L}_{t,i}=\\sum_{s=1}^{t}\\frac{\\ell_{s,i}^{\\prime}I_{s,i}}{w_{s,i}}}\\end{array}$ . Note that for any $s>\\tau_{i}$ $\\begin{array}{r}{w_{s,i}=\\frac{q_{i}\\tau_{i}}{s}}\\end{array}$ $q_{i}\\geqslant w_{s,i}\\geqslant1/K$ $s\\leqslant\\tau_{i}$ ", "page_idx": 21}, {"type": "text", "text": "We define $\\begin{array}{r}{X_{s,i}:=\\frac{\\ell_{s,i}^{\\prime}I_{s,i}}{w_{s,i}}-\\mu_{s,i}^{\\prime}}\\end{array}$ for any $1\\leqslant s\\leqslant t$ . If $t\\leqslant t_{\\mathrm{sw}}$ , then $X_{1,i},\\ldots,X_{t,i}$ forms a martingale difference sequence. To have an upper bound on $|X_{s,i}|$ : 1) when $s\\leqslant\\tau_{i}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\vert X_{s,i}\\vert\\leqslant M_{s,i}K+M_{s,i}\\leqslant2K M_{s,i},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and 2) when $s>\\tau_{i}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n|X_{s,i}|\\leqslant\\frac{M_{s,i}}{q_{i}\\cdot\\frac{\\tau_{i}}{s}}+M_{s,i}\\leqslant\\frac{M_{s,i}}{\\frac{1}{K}\\cdot\\frac{\\tau_{i}}{s}}+M_{s,i}\\leqslant2K M_{s,i}\\frac{s}{\\tau_{i}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining two cases, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n|X_{s,i}|\\leqslant2K M_{s,i}\\operatorname*{max}\\{\\frac{s}{\\tau_{i}},1\\}\\leqslant2K M_{t,i}\\operatorname*{max}\\{\\frac{t}{\\tau_{i}},1\\}=\\frac{2u t^{\\frac{1}{1+\\nu}}K^{\\frac{\\nu}{1+\\nu}}\\operatorname*{max}\\{\\frac{t}{\\tau_{i}},1\\}}{(\\log(\\log(T)/\\zeta))^{\\frac{1}{3\\nu+1}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{t}\\mathbb{E}\\left[(X_{s,i})^{2}\\middle|\\mathcal{F}_{s}\\right]=\\displaystyle\\sum_{s=1}^{t}\\left(\\mathbb{E}\\left[\\frac{\\left(\\ell_{s,i}^{*}\\right)^{2}I_{s,i}}{(w_{s,i})^{2}}\\middle|\\mathcal{F}_{t}\\right]+(\\mu_{s,i}^{*})^{2}-2\\mu_{s,i}^{*}\\mathbb{E}\\left[\\frac{\\ell_{s,i}^{*}I_{s,i}}{w_{s,i}}\\middle|\\mathcal{F}_{t}\\right]\\right)}\\\\ &{=\\displaystyle\\sum_{s=1}^{t}\\mathbb{E}\\left[\\frac{\\left(\\ell_{s,i}^{*}\\right)^{2}I_{s,i}}{(w_{s,i})^{2}}\\middle|\\mathcal{F}_{t}\\right]-\\displaystyle\\sum_{s=1}^{t}(\\mu_{s,i}^{*})^{2}}\\\\ &{\\leqslant\\displaystyle\\sum_{s=1}^{t}\\frac{1}{w_{s,i}}\\mathbb{E}\\left[\\ell_{s,i}^{*}|^{1+\\nu}(M_{s,i})^{1-\\nu}\\middle|\\mathcal{F}_{t}\\right]}\\\\ &{\\leqslant u^{1+\\nu}(M_{t,i})^{1-\\nu}\\left(\\displaystyle\\sum_{s=1}^{m{\\binom{m}{\\ell_{s,i}}}}\\frac{1}{w_{s,i}}+\\displaystyle\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{(q_{\\ell}\\tau_{i})}\\right)}\\\\ &{=u^{2}(t/K)^{\\frac{1-\\nu}{1+\\nu}}\\left(\\displaystyle\\sum_{s=1}^{m{\\binom{m}{\\ell_{s,i}}}}\\frac{1}{w_{s,i}}+\\displaystyle\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{\\ell}\\tau_{i}}\\right)(\\log(\\log(T)/\\zeta))^{\\frac{s-1}{s+1}},}\\\\ &{\\phantom{=}+u^{2}(t/K)^{\\frac{1-\\nu}{1+\\nu}}\\left(\\displaystyle\\sum_{s=1}^{m{\\binom{m}{\\ell_{s,i}}}}\\frac{1}{w_{s,i}}+\\displaystyle\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{\\ell}\\tau_{i}}\\right)(\\log(\\log(T)/\\zeta))^{\\frac{s-1}{s+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality is because $M_{s,i}$ is non-decreasing in $s$ . ", "page_idx": 22}, {"type": "text", "text": "By Lemma 11 we have that with probability at least $1-\\zeta$ , for any $t\\leqslant t_{\\mathrm{sw}}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}\\left(\\frac{\\ell_{s,i}^{\\prime}I_{s,i}}{w_{s,i}}-\\mu_{s,i}^{\\prime}\\right)\\leqslant4\\sqrt{u^{2}(t/K)^{\\frac{1-v}{1+v}}}\\left(\\sum_{s=1}^{\\operatorname*{min}\\{\\tau_{i},t\\}}\\frac{1}{w_{s,i}}+\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{i}\\tau_{i}}\\right)(\\log(\\log(T)/\\zeta))^{\\frac{4v}{3v+1}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By taking an union bound over all all actions $i\\in[K]$ , we have with probability at least $1-\\zeta$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{L}_{t,i}-\\displaystyle\\sum_{s=1}^{t}\\mu_{s,i}^{\\prime}\\biggr\\rvert\\leqslant4\\sqrt{u^{2}(t/K)^{\\frac{1-v}{1+v}}}\\left(\\displaystyle\\sum_{s=1}^{\\operatorname*{min}\\{\\tau_{i},t\\}}\\frac{1}{w_{s,i}}+\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{i}\\tau_{i}}\\right)\\left(\\log(2K\\log(T)/\\zeta)\\right)^{\\frac{4v}{3v+1}}}\\\\ {+\\,4u t^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}}\\operatorname*{max}\\{t/\\tau_{i},1\\}\\left(\\log(2K\\log(T)/\\zeta)\\right)^{\\frac{3v}{3v+1}},\\forall t\\in[T],i\\in[K].\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma 8, we have almost surely that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{s=1}^{t}\\left|\\mu_{s,i}-\\mu_{s,i}^{\\prime}\\right|\\leqslant\\sum_{s=1}^{t}u^{1+v}(M_{s,i})^{-v}\\leqslant u K^{\\frac{\\nu}{1+\\nu}}\\left(\\log(\\log(T)/\\zeta)\\right)^{\\frac{\\nu}{3\\nu+1}}\\sum_{s=1}^{t}s^{\\frac{-\\nu}{1+\\nu}}}\\\\ {\\leqslant2u K^{\\frac{\\nu}{1+\\nu}}t^{\\frac{1}{1+\\nu}}\\left(\\log(\\log(T)/\\zeta)\\right)^{\\frac{\\nu}{3\\nu+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining two parts above, we get with probability at least $1-\\zeta$ that, for any action $i$ and round $t$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{L}_{t,i}-\\displaystyle\\sum_{s=1}^{t}\\mu_{s,i}\\bigg|\\leqslant6u\\left(\\sqrt{(t/K)^{\\frac{1-v}{1+v}}\\left(\\displaystyle\\sum_{s=1}^{\\operatorname*{min}\\{\\tau_{i},t\\}}\\frac{1}{w_{s,i}}+\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{i}\\tau_{i}}\\right)}\\right)+K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\operatorname*{max}\\{t/\\tau_{i},}\\\\ {\\cdot\\left(\\log(2K\\log(T)/\\zeta)\\right)^{\\frac{3v}{3v+1}}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(50)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 6 (Concentration on the number of pulls). It holds with probability at least $1-\\zeta$ that, for any $i\\in[K]$ and $t\\in[T],\\,i f t\\leqslant t_{s w}.$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\nN_{t,i}\\leqslant q_{i}\\tau_{i}(1+\\log t)+4\\sqrt{q_{i}\\tau_{i}(1+\\log t)\\log(K\\log(T)/\\zeta)}+2\\log(K\\log(T)/\\zeta).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma $^{6}$ . We first fix some action $i$ and round $t$ . Recall that $\\begin{array}{r}{N_{t,i}=\\sum_{s=1}^{t}I_{s,i}}\\end{array}$ . Define $X_{s,i}:=I_{s,i}-w_{s,i}$ . If $t\\leqslant t_{\\mathrm{sw}}$ then $X_{1,i},\\ldots,X_{t,i}$ forms a martingale difference  sequence such that $|X_{s,i}|\\leqslant1$ . Moreover, since $w_{s,i}$ is non-decreasing in $s$ when $s\\leqslant\\tau_{i}$ (and $w_{\\tau_{i},i}=q_{i})$ ), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\mathbb{E}\\left[(X_{s,i})^{2}\\middle|\\mathcal{F}_{s}\\right]\\leqslant\\sum_{s=1}^{t}w_{s,i}\\leqslant q_{i}\\tau_{i}+\\sum_{s=\\tau_{i}+1}^{t}\\frac{q_{i}\\tau_{i}}{s}\\leqslant q_{i}\\tau_{i}(1+\\log t).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by Lemma 11, it holds with probability at least $1-\\zeta$ that, ", "page_idx": 23}, {"type": "equation", "text": "$$\nN_{t,i}-\\sum_{s=1}^{t}w_{s,i}\\leqslant4\\sqrt{q_{i}\\tau_{i}(1+\\log t)\\log(\\log(T)/\\zeta)}+2\\log(\\log(T)/\\zeta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking union bounds over all actions $i\\in[K]$ completes the proof. ", "page_idx": 23}, {"type": "text", "text": "Lemma 7 (Concentration on the trimmed empirical-mean estimator). It holds with probability at least $1-\\zeta$ that, for any $i\\in[K]$ and $t\\in[K,t_{s w}],$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\mu}_{t,i}-\\frac{\\sum_{s=1}^{t}\\mu_{s,i}I_{s,i}}{N_{t,i}}\\right|\\leqslant9u\\left(\\frac{\\log(4K T^{2}/\\zeta)}{N_{t,i}}\\right)^{\\frac{v}{1+v}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Remark 9. Note that in the special case of stochastic regime, the term s=1 \u00b5s,iIs,i is simply \u00b5(i) (which is simply a scalar denoting the loss mean of action $i$ in the stochastic regime as defined in Section 2), and this lemma has been proven in Bubeck et al. (2013). However, we are not aware of how to extend the analysis of stochastic case to adversarial case. ", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that $B_{s,i}$ is defined in Algorithm 2 as $\\begin{array}{r}{B_{s,i}:=u\\left(\\frac{N_{s,i}}{\\log(2T/\\zeta)}\\right)^{\\frac{1}{1+v}},\\forall s\\in[T]}\\end{array}$ . Then we rewrite $\\widehat{\\mu}_{t,i}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{t,i}=\\frac{\\sum_{s=1}^{t}\\ell_{s,i}I_{s,i}\\mathbb{I}\\{|\\ell_{s,i}|\\leqslant B_{s,i}\\}}{N_{t,i}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and define ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{s,i}:=\\left(\\ell_{s,i}{\\mathbb{I}\\{\\left\\lvert\\ell_{s,i}\\right\\rvert\\leqslant B_{s,i}\\}}-{\\mathbb{E}_{\\ell_{s,i}\\sim P_{s,i}}{\\left[\\ell_{s,i}{\\mathbb{I}\\{\\left\\lvert\\ell_{s,i}\\right\\rvert\\leqslant B_{s,i}\\right\\}}\\right]}}\\right)I_{s,i}/u.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, in this lemma we need a slightly different filtration. We define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}^{\\prime}:=\\sigma(a_{1},\\ell_{1,a_{1}},\\dots,a_{t-1},\\ell_{t-1,a_{t-1}},a_{t}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Clearly, given any fixed round $t$ and action $i$ , $X_{1,i},\\ldots,X_{t,i}$ form a martingale difference sequence adapted to $\\mathcal{F}_{1}^{\\prime},\\ldots,\\mathcal{F}_{t}^{\\prime}$ (since both $B_{s,i}$ and $I_{s,i}$ are deterministic conditioned on $\\mathcal{F}_{s.}^{\\prime}$ ), with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\leqslant t}X_{s,i}\\leqslant2B_{t,i}/u=2\\left(\\frac{N_{t,i}}{\\log(2T/\\zeta)}\\right)^{\\frac{1}{1+v}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{s=1}^{t}\\mathbb{E}\\left[(X_{s,i})^{2}\\middle|\\mathcal{F}_{s}^{\\prime}\\right]=\\sum_{s:I_{s,i}=1}\\mathbb{E}\\left[(X_{s,i})^{2}\\middle|\\mathcal{F}_{s}^{\\prime}\\right]}}\\\\ &{\\leqslant\\sum_{s:I_{s,i}=1}\\mathbb{E}\\left[|\\ell_{s,i}|^{1+v}\\left(B_{s,i}\\right)^{1-v}/u^{2}\\middle|\\mathcal{F}_{s}^{\\prime}\\right]}\\\\ &{\\leqslant N_{t,i}(B_{t,i})^{1-v}u^{1+v}/u^{2}=\\left(\\frac{N_{t,i}}{\\log(2T/\\zeta)}\\right)^{\\frac{1-v}{1+v}}N_{t,i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the last inequality we utilize the fact that $B_{s,i}$ is non-decreasing in $s$ . ", "page_idx": 23}, {"type": "text", "text": "Noting that ", "page_idx": 23}, {"type": "equation", "text": "$$\n2\\left(\\frac{N_{t,i}}{\\log(2T/\\zeta)}\\right)^{\\frac{1}{1+v}}\\leqslant2\\sqrt{\\left(\\frac{N_{t,i}}{\\log(2T/\\zeta)}\\right)^{\\frac{1-v}{1+v}}N_{t,i}}\\leqslant2\\left(N_{t,i}\\right)^{\\frac{1}{1+v}}\\leqslant2T,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we apply Lemma 12 and get with probability at least $1-\\zeta$ that, for any fixed $t$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{s=1}^{t}X_{s,i}\\leqslant3\\sqrt{(N_{t,i})^{\\frac{2}{1+v}}(\\log(2T/\\zeta))^{\\frac{v-1}{v+1}}\\log(2T/\\zeta)}+2\\left(\\frac{N_{t,i}}{\\log(2T/\\zeta)}\\right)^{\\frac{1}{1+v}}\\log(2T/\\zeta)}}\\\\ &{\\quad\\quad\\quad=5(N_{t,i})^{\\frac{1}{1+v}}\\left(\\log(2T/\\zeta)\\right)^{\\frac{v}{1+v}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking a (two-sided) union bound over all rounds and actions, we get with probability at least $1-\\zeta$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\sum_{s=1}^{t}X_{s,i}\\right|\\leqslant5(N_{t,i})^{\\frac{1}{1+\\nu}}\\left(\\log(4K T^{2}/\\zeta)\\right)^{\\frac{\\nu}{1+\\nu}},\\forall t\\leqslant t_{\\mathrm{sw}},i\\in[K].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s=u}{\\overset{t}{\\sum}}\\left(\\mathbb{E}_{\\ell_{s},i\\sim P_{s,i}},[\\ell_{s,i}][\\ell_{s,i}]\\leqslant B_{s,i}]\\right)-\\mu_{s,i}]\\,L_{s,i}\\bigg|}\\\\ &{\\leqslant\\underset{s\\in\\mathbb{R}^{d}}{\\sum}\\,\\underset{s=u}{\\sum}\\,|\\mathbb{E}_{\\ell_{s},i\\sim P_{s,i}}[\\ell_{s,i}]\\mathbb{E}[\\ell_{s,i}]\\leqslant B_{s,i}]|-\\mu_{s,i}|}\\\\ &{\\overset{(a)}{\\leqslant}u^{i\\times\\frac{N_{s,i}}{\\sum}}u^{-v}\\left(\\frac{N}{\\log(2T/\\zeta)}\\right)^{\\frac{-w}{1+w}}}\\\\ &{=u\\{\\log(2T/\\zeta)\\}^{\\frac{N_{s,i}}{\\Gamma+\\frac{1}{w}}}\\underset{s=u}{\\sum}\\,N_{1}^{\\frac{1-w}{\\Gamma+w}}}\\\\ &{\\leqslant u\\{\\log(4K T^{2}/\\zeta)\\}^{\\frac{1+w}{1+\\alpha}}(1+v)(N_{t,i}+1)^{\\frac{1+w}{1+\\alpha}}}\\\\ &{\\leqslant4u\\{\\log(4K T^{2}/\\zeta)\\}^{\\frac{1+w}{1+\\alpha}}(N_{t,i})^{\\frac{1+w}{1+\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where step (a) follows from Lemma 8 and the last step relies on the fact that $v\\in(0,1]$ . ", "page_idx": 24}, {"type": "text", "text": "Applying triangle inequality to Eqs. (54) and (55) and then dividing both sides by $N_{t,i}$ completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.2 Logarithmic Regret in the Stochastic Regime ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this subsection, we provide the complete proof for logarithmic regret of Algorithm 2 in the stochastic regime. ", "page_idx": 24}, {"type": "text", "text": "Taking a union bound over Lemmas 5, 6, and 7, with probability at least $1-\\zeta$ , for any $i\\in[K]$ and $t\\leqslant t_{\\mathrm{sw}}$ , all of the following holds in either stochastic or adversarial regime (recall that $\\beta=$ $12T^{2}K\\log(T))$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left|\\displaystyle\\widehat{L}_{t,i}-\\sum_{s=1}^{t}\\mu_{s,i}\\right|\\leqslant6u\\left(\\sqrt{(t/K)^{\\frac{1-v}{1+v}}\\left(\\sum_{s=1}^{\\operatorname*{min}\\{\\tau_{i},t\\}}\\frac{1}{w_{s,i}}+\\frac{t\\operatorname*{max}\\{t-\\tau_{i},0\\}}{q_{i}\\tau_{i}}\\right)}+K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\operatorname*{max}\\{t\\}_{0\\leq t_{*}}^{(\\tau_{i},0)}\\right.}\\\\ &{\\left.\\quad\\cdot\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}},\\ \\ \\right.}&{\\left.\\left.\\left(56\\right)\\right.}\\\\ &{N_{t,i}\\leqslant q_{i}\\tau_{i}(1+\\log t)+4\\sqrt{q_{i}\\tau_{i}(1+\\log t)\\log(\\beta/\\zeta)}+2\\log(\\beta/\\zeta).}&{\\left.\\left.\\left.\\mathrm{(S7)}\\right.\\right.}\\\\ &{\\left|\\displaystyle\\widehat{\\mu}_{t,i}-\\frac{\\sum_{s=1}^{t}\\mu_{s,i}I_{s,i}}{N_{t,i}}\\right|\\leqslant9u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t,i}}\\right)^{\\frac{v}{1+v}}.\\ \\right.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Before we start to derive the regret bound, we simplify Eq. (56) a bit for convenience. ", "page_idx": 24}, {"type": "text", "text": "For any $t$ such that $\\tau_{i}>t$ (i.e., action $i$ is still active by round $t$ ), we have $w_{s,i}\\geqslant1/K,\\forall s\\leqslant t$ , and Eq. (56) implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{L}_{t,i}-\\displaystyle\\sum_{s=1}^{t}\\mu_{s,i}\\right|\\leqslant6u\\left(\\sqrt{(t/K)^{\\frac{1-v}{1+v}}\\displaystyle\\sum_{s=1}^{t}\\frac{1}{w_{s,i}}}+K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\right)(\\log(\\beta/\\zeta))^{\\frac{3v}{3v+1}}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant6u\\left(\\sqrt{(t/K)^{\\frac{1-v}{1+v}}t K}+K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\right)(\\log(\\beta/\\zeta))^{\\frac{3v}{3v+1}}}\\\\ &{\\qquad\\qquad=12u K^{\\frac{v}{1+v}}t^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}=\\mathbb{W}\\mathrm{idth}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "otherwise (when $\\tau_{i}\\leqslant t_{,}$ ), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widehat{L}_{t,i}-\\sum_{s=1}^{t}\\mu_{s,i}\\Bigg|\\leqslant6u\\left(\\sqrt{(t/K)^{\\frac{1-v}{1+v}}}\\,K\\frac{t^{2}}{\\tau_{i}}+K^{\\frac{v}{1+v}}t^{1+\\frac{1}{1+v}}/\\tau_{i}\\right)\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}}}\\\\ &{}&{\\qquad=6u\\left(t K^{\\frac{v}{1+v}}\\sqrt{\\frac{t^{\\frac{1-v}{1+v}}}{\\tau_{i}}}+K^{\\frac{v}{1+v}}t^{1+\\frac{1}{1+v}}/\\tau_{i}\\right)\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}}\\\\ &{}&{\\qquad\\leqslant12u K^{\\frac{v}{1+v}}t^{1+\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}/\\tau_{i}=\\mathrm{Width}(t)\\cdot t/\\tau_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in the first inequality we utilize the facts that $q_{i}\\geqslant1/K$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau_{i}+\\frac{t(t-\\tau_{i})}{\\tau_{i}}\\leqslant\\frac{t^{2}}{\\tau_{i}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the second inequality is because ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{t^{\\frac{1-v}{1+v}}}{\\tau_{i}}}\\leqslant\\sqrt{\\frac{t^{\\frac{1-v}{1+v}}}{\\tau_{i}}\\cdot\\frac{t}{\\tau_{i}}}=\\frac{t^{\\frac{1}{1+v}}}{\\tau_{i}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining two cases (whether $\\tau_{i}>t$ or not), Eq. (56) implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\widehat{L}_{t,i}-\\sum_{s=1}^{t}\\mu_{s,i}\\right|\\leqslant\\mathbb{I}\\{i\\in A_{t}\\}{\\mathrm{Width}}(t)+\\mathbb{I}\\{i\\notin A_{t}\\}{\\mathrm{Width}}(t)\\frac{t}{\\tau_{i}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To show the regret bound, we first show that, in the stochastic regime, when all the three above events hold, we will never start to run Algorithm 1, i.e., tests (6)-(8) always fail (for all $t<T$ ). ", "page_idx": 25}, {"type": "text", "text": "Test (6) always fails. This is simply implied by Eqs. (63) and (58) together with a triangle inequality. ", "page_idx": 25}, {"type": "text", "text": "Test (7) always fails. We first show that test (5) is never satisfied for action $i^{*}$ , so we have $i^{*}\\in$ $A_{t},\\forall t\\leqslant t_{\\mathrm{sw}}$ . To see this, for actions $i,i^{*}\\in A_{t-1}$ , by Eq. (63) we must have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{L}_{t,i^{*}}-\\widehat{L}_{t,i}=(\\widehat{L}_{t,i^{*}}-t\\cdot\\mu(i^{*}))-(\\widehat{L}_{t,i}-t\\cdot\\mu(i))+t\\cdot(\\mu(i^{*})-\\mu(i))}\\\\ &{\\qquad\\qquad\\leqslant2\\mathrm{Width}(t)-t\\Delta_{i}<c_{1}\\mathrm{Width}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which means that $i^{*}$ is never eliminated from $A_{t}$ and hence stays active (due to test (5)). ", "page_idx": 25}, {"type": "text", "text": "Moreover, for any action $i\\not\\in\\mathcal{A}_{t}$ , it must be deactivated at some round no later than $t$ , so we have $\\tau_{i}\\ \\leqslant t$ and that test (5) is satisfied at round $\\tau_{i}$ (and is not satisfied at round $\\tau_{i}-1)$ ). Let $j_{t}^{*}\\in\\mathrm{argmin}_{j\\in A_{t-1}}\\widehat{L}_{t,j}$ . ", "page_idx": 25}, {"type": "text", "text": "Therefore, looking at round $\\tau_{i}$ (and any action $i\\neq i^{*}$ ), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{1}\\mathbf{W}\\mathrm{idth}(\\tau_{i})\\overset{\\mathrm{(a)}}{<}\\widehat{L}_{\\tau_{i},i}-\\widehat{L}_{\\tau_{i},j_{\\tau_{i}}^{*}}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=(\\widehat{L}_{\\tau_{i},i}-\\tau_{i}\\cdot\\mu(i))-(\\widehat{L}_{\\tau_{i}}\\widehat{}_{\\tau_{i}}-\\tau_{i}\\cdot\\mu(j_{\\tau_{i}}^{*}))+\\tau_{i}\\cdot(\\mu(i)-\\mu(j_{\\tau_{i}}^{*}))}\\\\ &{\\phantom{x x x x x x x}\\overset{\\mathrm{(b)}}{\\leqslant}2\\mathbf{W}\\mathrm{idth}(\\tau_{i})+\\tau_{i}\\cdot(\\mu(i)-\\mu(i^{*}))=2\\mathbf{W}\\mathrm{idth}(\\tau_{i})+\\tau_{i}\\Delta_{i}\\mathrm{,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where step (a) is from test (5) (which should hold now) and step (b) is due to Eq. (63) and the optimality of $i^{*}$ (i.e., $\\mu(i)-\\mu(j_{\\tau_{i}}^{*})\\leqslant\\mu(i)-\\mu(i^{*})=\\Delta_{i})$ . ", "page_idx": 26}, {"type": "text", "text": "Moreover, looking at round $\\tau_{i}-1$ (and any action $i\\neq i^{*}$ ), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\tau_{i}-1)\\Delta_{i}-2\\mathbf{W}\\mathrm{idth}(\\tau_{i}-1)\\overset{\\mathrm{(a)}}{\\leqslant}(\\widehat{L}_{\\tau_{i}-1,i}-(\\tau_{i}-1)\\cdot\\mu(i))-(\\widehat{L}_{\\tau_{i}-1,i^{*}}-(\\tau_{i}-1)\\cdot\\mu(i^{*}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left(\\tau_{i}-1\\right)\\cdot(\\mu(i)-\\mu(i^{*}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\widehat{L}_{\\tau_{i}-1,i}-\\widehat{L}_{\\tau_{i}-1,i^{*}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{\\mathrm{(b)}}{\\leqslant}\\widehat{L}_{\\tau_{i}-1,i}-\\widehat{L}_{\\tau_{i}-1,j_{\\tau_{i}-1}^{*}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{()}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{()}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where step (a) follows from the good event in Eq. (63), step (b) is because $i^{*}\\in A_{\\tau_{i}-2}$ , and step (c) is due to test (5) (which now should not hold for action $i$ since it is still active). ", "page_idx": 26}, {"type": "text", "text": "Now we can show that test (7) is never satisfied since in any round $t$ for any $i\\not\\in A_{t}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\widehat{L}_{t,i}-\\underset{j\\in A_{t}}{\\operatorname*{min}}\\widehat{L}_{t,j})/t\\leqslant(\\widehat{L}_{t,i}-\\underset{j\\in A_{t-1}}{\\operatorname*{min}}\\widehat{L}_{t,j})/t}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(\\widehat{L}_{t,i}/t-\\mu(i))-(\\widehat{L}_{t,j_{t}^{*}}/t-\\mu(j_{t}^{*}))+(\\mu(i)-\\mu(j_{t}^{*}))}\\\\ &{\\leqslant\\mathrm{Width}(t)/\\tau_{i}+\\mathrm{Width}(t)/t+\\Delta_{i}}\\\\ &{\\leqslant2\\mathrm{Width}(t)/(\\tau_{i}-1)+\\Delta_{i}}\\\\ &{\\leqslant(2+c_{1}+2)\\mathrm{Width}(t)/(\\tau_{i}-1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last step is from Eq. (66). ", "page_idx": 26}, {"type": "text", "text": "Test (8) always fails. Since $i^{*}\\in A_{t}$ , for any action $i\\not\\in A_{t}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\widehat{L}_{t,i}-\\underset{j\\in A_{t}}{\\operatorname*{min}}\\widehat{L}_{t,j})/t\\geqslant(\\widehat{L}_{t,i}-\\widehat{L}_{t,i^{*}})/t}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=(\\widehat{L}_{t,i}/t-\\mu(i))-(\\widehat{L}_{t,i^{*}}/t-\\mu(i^{*}))+(\\mu(i)-\\mu(i^{*}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geqslant-2\\mathbf{Width}(t)/\\tau_{i}+\\Delta_{i}}\\\\ &{\\qquad\\qquad\\qquad\\geqslant(c_{1}-2-2)\\mathbf{Width}(t)/\\tau_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first step is due to $i^{*}\\in A_{t}$ and the last step is from Eq. (65). ", "page_idx": 26}, {"type": "text", "text": "Putting things together. Now we show two intermediate results, followed by bounding the regret. First, Eq. (66) implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{i}\\leqslant(2+c_{1})\\mathrm{Width}(\\tau_{i}-1)/(\\tau_{i}-1)}\\\\ &{\\quad=(2+c_{1})12u K^{\\frac{v}{1+v}}(\\tau_{i}-1)^{\\frac{-v}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}}\\\\ &{\\quad\\leqslant(2+c_{1})12u K^{\\frac{v}{1+v}}(\\tau_{i}/2)^{\\frac{-v}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}}\\\\ &{\\quad\\leqslant(2+c_{1})24u K^{\\frac{v}{1+v}}(\\tau_{i})^{\\frac{-v}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which after rearranging implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tau_{i}=O\\left(u^{1+\\frac{1}{v}}K\\left(\\log(\\beta/\\zeta)\\right)^{1+\\frac{2}{3v+1}}/(\\Delta_{i})^{1+\\frac{1}{v}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Due to the definition of $q_{i}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}q_{i}\\leqslant\\sum_{i=1}^{K}{\\frac{1}{K-i+1}}\\leqslant1+\\log K.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To see this, let $i^{\\prime}$ denote the $i$ -th earliest action that is deactivated, we have $q_{i^{\\prime}}\\leqslant\\frac{1}{K-i^{\\prime}\\!+\\!1}$ due to the algorithm design. ", "page_idx": 26}, {"type": "text", "text": "Now we have everything needed to arrive at the final result. By the definition of pseudo-regret in stochastic bandits, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R_{T}=\\sum_{i\\leq\\lambda\\leq0}\\Delta_{i}N_{T,i}}}\\\\ &{\\stackrel{(a)}{\\leqslant}\\sum_{i\\leq\\lambda\\leq0}\\Delta_{i}\\left(q_{i}\\tau_{i}(1+\\log T)+4\\sqrt{q_{i}\\tau_{i}(1+\\log T)\\log(\\beta/\\zeta)}+2\\log(\\beta/\\zeta)\\right)}\\\\ &{\\stackrel{(b)}{\\leqslant}\\sum_{i\\leq\\lambda\\leq0}\\Delta_{i}\\left(2q_{i}\\tau_{i}(1+\\log T)+6\\log(\\beta/\\zeta)\\right)}\\\\ &{\\stackrel{(c)}{\\leqslant}\\sum_{i\\leq\\lambda\\leq0}\\Delta_{i}\\left(2q_{i}\\tau_{i}(1+\\log T)+6\\log(\\beta/\\zeta)\\right)}\\\\ &{\\stackrel{(b)}{\\leqslant}Q\\left(\\sum_{i\\leq\\lambda\\leq0}\\Delta_{i}q_{i}\\log(T)\\cdot u^{1+\\frac{1}{\\lambda}}K\\left(\\log(\\beta/\\zeta)\\right)^{1+\\frac{2}{\\kappa\\tau_{i}}}/\\left(\\Delta_{i}\\right)^{1+\\frac{1}{\\kappa}}+\\log(\\beta/\\zeta)\\sum_{i\\leq\\lambda\\leq0}\\Delta_{i}\\right)}\\\\ &{\\stackrel{(c)}{\\leqslant}Q\\left(u^{1+\\frac{1}{\\lambda}}K\\log(T)\\left(\\log(\\beta/\\zeta)\\right)^{3}/\\Delta^{\\frac{1}{\\kappa}}\\cdot\\sum_{i\\in\\mathcal{S}_{i}}q_{i}+\\log(\\beta/\\zeta)\\sum_{i\\in\\mathcal{S}_{i}\\setminus\\mathcal{S}_{i}}\\Delta_{i}\\right)}\\\\ &{\\leqslant\\mathcal{O}\\left(K\\log(T)\\log(K)(\\log(\\beta/\\zeta))^{3}\\left(u^{1+\\frac{1}{\\lambda}}/\\Delta\\right)^{\\frac{1}{\\kappa}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where step (a) follows from Eq. (57), step (b) is due to Eq. (70), and step (c) is from Eq. (71). Choosing $\\zeta=1/T<1/e$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[R_{T}\\right]\\leqslant O\\left(K\\log(T)\\log(K)(\\log T)^{3}(u^{1+\\frac{1}{v}}/\\Delta)^{\\frac{1}{v}}\\right)+\\frac{1}{T}u T}\\\\ &{\\qquad\\quad=O\\left(K\\log(K)(\\log T)^{4}(u^{1+\\frac{1}{v}}/\\Delta)^{\\frac{1}{v}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "C.3 Optimal Worst-case Regret in the Adversarial Regime ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this subsection, we provide the complete proof for (near-)optimal worst-case regret of Algorithm 2 in the adversarial regime. ", "page_idx": 27}, {"type": "text", "text": "Let $\\begin{array}{r}{i_{t}^{*}:=\\operatorname*{argmin}_{i\\in[K]}\\sum_{s=1}^{t}\\mu_{s,i}}\\end{array}$ and $\\begin{array}{r}{I_{t}^{*}:=\\mathrm{argmin}_{i\\in A_{t}}\\sum_{s=1}^{t}\\mu_{s,i}}\\end{array}$ . We first show that $i_{t_{\\mathrm{sw}}-1}^{*}\\in$ $A_{t_{\\mathrm{sw}}-1}$ . ", "page_idx": 27}, {"type": "text", "text": "For any action $i\\notin A_{t_{\\mathrm{sw}}-1}$ , we have $\\tau_{i}\\leqslant t_{\\mathrm{{sw}}}-1$ and test (8) is not satisfied for $i$ at round $t_{\\mathrm{sw}}-1$ (since the algorithm switch has not happened). We get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i}-\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,I_{t_{s w}}^{*}-1}}\\\\ &{=\\left(\\displaystyle\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i}-\\widehat{L}_{t_{s w}-1,i}\\right)+\\left(\\widehat{L}_{t_{s w}-1,I_{t_{s w}-1}^{*}}-\\displaystyle\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,I_{t_{s w}-1}^{*}}\\right)+\\left(\\widehat{L}_{t_{s w}-1,i}-\\widehat{L}_{t_{s w}-1,I_{t_{s w}-1}^{*}}\\right)}\\\\ &{\\overset{(\\mathrm{s})}{\\>}\\displaystyle\\frac{-(t_{\\mathrm{sw}}-1)}{\\tau_{i}}\\mathbf{W}\\mathrm{idth}(t_{\\mathrm{sw}}-1)-\\mathbf{W}\\mathrm{idth}(t_{\\mathrm{sw}}-1)+(c_{1}-4)\\displaystyle\\frac{t_{\\mathrm{sw}}-1}{\\tau_{i}}\\mathbf{W}\\mathrm{idth}(t_{\\mathrm{sw}}-1)}\\\\ &{\\geqslant(c_{1}-6)\\displaystyle\\frac{t_{\\mathrm{sw}}-1}{\\tau_{i}}\\mathbf{W}\\mathrm{idth}(t_{\\mathrm{sw}}-1)\\geqslant0,}\\end{array}\\quad\\mathcal{O}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where step (a) is due to Eq. (63) (applied twice) and test (8). Therefore, we must have $i_{t_{\\mathrm{sw}}-1}^{*}\\in A_{t_{\\mathrm{sw}}-1}$ (which further implies $i_{t_{\\mathrm{sw}}-1}^{*}\\in A_{s},\\forall s\\leqslant t_{\\mathrm{sw}}-1)$ since $\\begin{array}{r}{\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i_{t_{\\mathrm{sw}}-1}^{*}}-\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,I_{t_{\\mathrm{sw}}-1}^{*}}\\leqslant0}\\end{array}$ by the definition, otherwise a contradiction is incurred. ", "page_idx": 27}, {"type": "text", "text": "Now we can bound the cumulative regret up to the algorithm switch. Specifically, we rewrite the regret as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{n-1}\\mu_{s,a_{i}}-\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{i_{w}-1}^{s}}\\equiv\\displaystyle\\sum_{s=1}^{t_{o n-1}}\\sum_{i=1}^{k}\\mu_{s,i_{s}}I_{s,i_{w}-1}^{\\prime}}&{}\\\\ {\\displaystyle\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{w}-1}=\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{s}}I_{s,i_{w}}\\underbrace{\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{s}^{\\prime}}}_{N_{t_{w}-1,i_{s}^{\\prime}}}+\\underbrace{\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{s}^{\\prime}}}_{t_{o n-1}}-1}\\\\ &{=\\displaystyle\\sum_{i=1}^{K}N_{t_{o n-1,i_{s}^{\\prime}}}\\left(\\underbrace{\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{s}}I_{s,i_{w}-1}}_{N_{t_{o n-1,i_{s}^{\\prime}}}}-\\underbrace{\\widehat{L}_{t_{o n-1,i_{s}^{\\prime}}}}_{t_{o n-1}}+\\underbrace{\\sum_{t_{o n-1,i_{s}^{\\prime}}}-\\widehat{L}_{t_{o n-1,i_{s}^{\\prime}}}}_{t_{o n-1}}\\right)}\\\\ &{\\ \\ \\ +\\underbrace{\\hat{L}_{t_{o n-1,i_{w}-1}^{\\prime}}-\\sum_{s=1}^{t_{o n-1}}\\mu_{s,i_{w}-1}}_{P e a v}+\\underbrace{\\mathcal{O}(\\epsilon^{3})}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Ibno uthned  reeawcrhi toifn tgh ae btohvree,e  tweer muss e( Pnaortt hiAn, gP barutt  Bth, ea nsid mPpalret  fCa)c ts ethpaatr $\\begin{array}{r}{\\sum_{i=1}^{K}N_{t_{\\mathrm{sw}}-1,i}=t_{\\mathrm{sw}}-1}\\end{array}$ . We now ", "page_idx": 28}, {"type": "text", "text": "Bounding Part A. We first rewrite Part A as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Part~A}=\\left(\\frac{\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i}I_{s,i}}{N_{t_{\\mathrm{sw}}-1,i}}-\\widehat{\\mu}_{t_{\\mathrm{sw}}-1,i}\\right)+\\left(\\widehat{\\mu}_{t_{\\mathrm{sw}}-1,i}-\\frac{\\widehat{L}_{t_{\\mathrm{sw}}-1,i}}{t_{\\mathrm{sw}}-1}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Eq. (58), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i}I_{s,i}}{N_{t_{\\mathrm{sw}}-1,i}}-\\widehat{\\mu}_{t_{\\mathrm{sw}}-1,i}\\leqslant9u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{\\mathrm{sw}}-1,i}}\\right)^{\\frac{v}{1+v}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By test (6) (which does not hold now), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{t_{\\mathrm{sw}}-1,i}-\\frac{\\widehat{L}_{t_{\\mathrm{sw}}-1,i}}{t_{\\mathrm{sw}}-1}\\leqslant9u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{\\mathrm{sw}}-1,i}}\\right)^{\\frac{v}{1+v}}+\\frac{\\mathbf{W}\\mathrm{idth}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we can conclude that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Part\\,A}\\leqslant18u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{\\mathrm{sw}}-1,i}}\\right)^{\\frac{v}{1+v}}+\\frac{\\mathrm{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bounding Part B. Since $i_{t_{\\mathrm{sw}}-1}^{*}\\in A_{t_{\\mathrm{sw}}-1}$ as we have shown, for any action $i\\notin A_{t_{\\mathrm{sw}}-1}$ , due to test (7) (which is not satisfied at round $t_{\\mathrm{sw}}-1)$ ), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\widehat{L}_{t_{s w}-1,i}-\\widehat{L}_{t_{s w}-1,i_{t_{s w}-1}^{*}}}{t_{s w}-1}\\leqslant\\frac{\\widehat{L}_{t_{s w}-1,i}-\\operatorname*{min}_{j\\in A_{t_{s w}-1}}\\widehat{L}_{t_{s w}-1,j}}{t_{s w}-1}\\leqslant(c_{1}+4)\\frac{\\operatorname{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any other action $i\\in A_{t_{\\mathrm{sw}}-1}$ , due to test (5), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\widehat C_{t_{\\mathrm{sw}}-1,i}-\\widehat L_{t_{\\mathrm{sw}}-1,i_{t_{\\mathrm{sw}}-1}^{*}}}{t_{\\mathrm{sw}}-1}\\leqslant\\frac{\\widehat L_{t_{\\mathrm{sw}}-1,i}-\\operatorname*{min}_{j\\in A_{t_{\\mathrm{sw}}-2}}\\widehat L_{t_{\\mathrm{sw}}-1,j}}{t_{\\mathrm{sw}}-1}\\leqslant c_{1}\\frac{\\mathrm{Width}(t_{\\mathrm{sw}}-1)}{t_{\\mathrm{sw}}-1}=c_{1}\\frac{\\mathrm{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\\leqslant\\frac{\\eta_{\\mathrm{sw}}+\\operatorname*{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining these two cases, we can claim that for any action $i\\in[K]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Part\\,B}=\\frac{\\widehat{L}_{t_{\\mathrm{sw}}-1,i}-\\widehat{L}_{t_{\\mathrm{sw}}-1,i_{t_{\\mathrm{sw}}-1}^{*}}}{t_{\\mathrm{sw}}-1}\\leqslant(c_{1}+4)\\frac{\\mathrm{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bounding Part C. Simply due to Eq. (63), since $i_{t_{\\mathrm{sw}}-1}^{*}\\in A_{t_{\\mathrm{sw}}-1}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Part}\\,\\mathrm{C}=\\widehat{L}_{t_{\\mathrm{sw}}-1,i_{t_{\\mathrm{sw}}-1}^{*}}-\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i_{t_{\\mathrm{sw}}-1}^{*}}\\leqslant c_{1}\\mathrm{Width}(t_{\\mathrm{sw}}-1).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Putting three parts together. Putting the three parts together, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{n=1}^{-1}\\mu_{s,a_{i}}-\\sum_{s=1}^{t_{n-1}}\\mu_{s,a_{i}}-\\sum_{s=1}^{t_{n-1}}\\mu_{s,a_{i}}\\frac{t_{n-1}}{t_{n}}-1}&{}\\\\ {\\displaystyle\\sum_{s=1}^{t_{n}}\\mu_{s,a_{i}}-\\sum_{s=1}^{t_{n}}\\mu_{s,a_{i}}-\\sum_{s=1}^{t_{n}}\\mu_{s,a_{i}}\\frac{t_{n}}{t_{n}}}&{}\\\\ {\\displaystyle}&{=O\\left(\\sum_{i=1}^{K}N_{t_{n-1},i}\\cdot u\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{n-1},i}}\\right)^{\\frac{1+\\pi}{4\\pi}}\\right)+O\\left(\\sum_{i=1}^{K}N_{t_{n-1},i}\\frac{\\mathbb{M}\\mathbb{M}\\mathbb{H}(t_{n\\;i}-1)}{\\tau_{i}-1}\\right.}\\\\ &{\\quad\\left.+O\\left(\\mathbb{M}\\mathbb{H}(t_{n},\\infty-1)\\right)\\mathbb{O}(\\beta/\\zeta)\\right)^{\\frac{1+\\pi}{4\\pi}}}\\\\ &{=O\\left(u\\sum_{i=1}^{K}N_{t_{n-1},i}\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{n-1},i}}\\right)^{\\frac{1+\\pi}{4\\pi}}\\right)+O\\left(\\mathbb{W}\\mathbb{H}(t_{n},\\infty-1)\\right)}\\\\ &{\\quad+O\\left(\\sum_{i=1}^{K}({\\mu_{1}}\\tau_{1}(1+\\log T))\\frac{\\mathbb{M}K^{\\frac{1+\\pi}{4\\pi}}(t_{n}-1)^{\\frac{1+\\pi}{4\\pi}}(\\log(\\beta/\\zeta))^{\\frac{3+\\pi}{4\\pi}}}{\\tau_{i}-1}\\right)}\\\\ &{\\quad+O\\left(\\sum_{i=1}^{K}\\log(\\beta/\\zeta)\\frac{\\mathbb{M}K^{\\frac{1+\\pi}{4\\pi}}(t_{n}-1)^{\\frac{1+\\pi}{4\\pi}}(\\log(\\beta/\\zeta))^{\\frac{3+\\pi}{4\\pi}}}{\\tau_{i}-1}\\right),\\qquad\\mathrm{(A4)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where in the last step we apply Lemma 6 to bound $N_{t_{\\mathrm{sw}}-1,i}$ in the term $\\begin{array}{r}{\\sum_{i=1}^{K}N_{t_{\\mathrm{sw}}-1,i}\\frac{\\operatorname{Width}(t_{\\mathrm{sw}}-1)}{\\tau_{i}-1}}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Now we bound each of the four terms one by one. ", "page_idx": 29}, {"type": "text", "text": "Bounding the first term. Applying Jensen\u2019s inequality, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}(N_{t_{\\mathrm{sw}}-1,i})^{\\frac{1}{1+v}}\\leqslant K\\cdot\\left(\\sum_{i=1}^{K}\\frac{N_{t_{\\mathrm{sw}}-1,i}}{K}\\right)^{\\frac{1}{1+v}}=K^{\\frac{v}{1+v}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and the first term is bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\nu\\sum_{i=1}^{K}N_{t_{\\mathrm{sw}}-1,i}\\left(\\frac{\\log(\\beta/\\zeta)}{N_{t_{\\mathrm{sw}}-1,i}}\\right)^{\\frac{v}{1+v}}=O\\left(u K^{\\frac{v}{1+v}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{v}{1+v}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Bounding the second term. Simply plugging in the definition of $\\operatorname{Width}(\\cdot)$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Width}(t_{\\mathrm{sw}}-1)=O\\left(u K^{\\frac{v}{1+v}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{1+3v}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Bounding the third term. By the fact that $\\textstyle\\sum_{i=1}^{K}q_{i}=O(\\log K)$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{K}\\left(q_{i}\\tau_{i}(1+\\log T)\\right)\\frac{u K^{\\frac{v}{1+v}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}}{\\tau_{i}-1}}}\\\\ &{=O\\left(\\log(K)\\log(T)u K^{\\frac{v}{1+v}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3v}{3v+1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Bounding the forth term. Notice that in the algorithm design, each action will be pulled once in the initialization, and we clearly have $\\tau_{i}\\geq K+1$ . Therefore, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{K}\\log(\\beta/\\zeta)\\frac{u K^{\\frac{\\nu}{1+\\nu}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+\\nu}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3\\nu}{3v+1}}}{\\tau_{i}-1}}\\\\ &{\\lesssim\\displaystyle\\sum_{i=1}^{K}\\log(\\beta/\\zeta)\\frac{u K^{\\frac{\\nu}{1+\\nu}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+\\nu}}\\left(\\log(\\beta/\\zeta)\\right)^{\\frac{3\\nu}{3v+1}}}{K}}\\\\ &{=O\\left(u K^{\\frac{\\nu}{1+\\nu}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+\\nu}}\\left(\\log(\\beta/\\zeta)\\right)^{1+\\frac{3\\nu}{3v+1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining the bounds on these four terms, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,a_{t}}-\\sum_{s=1}^{t_{\\mathrm{sw}}-1}\\mu_{s,i^{*}}=O\\left(\\log(K)\\log(T)u K^{\\frac{v}{1+v}}(t_{\\mathrm{sw}}-1)^{\\frac{1}{1+v}}\\left(\\log(\\beta/\\zeta)\\right)^{1+\\frac{3v}{3v+1}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The regret incurred starting from round $t_{\\mathrm{sw}}+1$ is taken care of by Algorithm 1 (and the regret guarantee is given by Theorem 1). By taking a union bound, we have with probability at least $1-\\zeta$ that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathsf{R}_{T}=O\\left(u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}\\log(K)\\log(T)(\\log(\\beta/\\zeta))^{1+\\frac{3\\nu}{3\\nu+1}}\\right)+O\\left(u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}(\\log T)^{2}\\log(\\beta/\\zeta)\\right)}&\\\\ &{\\quad}&{=O\\left(u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}\\log(K)(\\log T)^{2}(\\log(\\beta/\\zeta))^{2}\\right).}&{(90)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Choosing $\\zeta=1/T$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{T}\\right]=O\\left(u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}\\log(K)(\\log T)^{4}\\right)+2\\frac{1}{T}u T=O\\left(u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}\\log(K)(\\log T)^{4}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "D Heavy-tailed Adversarial Bandits with Huber Contamination ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This section is dedicated to the adversarial regime, with the additional setup that the bandit feedback could be contaminated in the Huber model (Huber, 1996). In the stochastic MAB, Huber contamination has been studied in Guan et al. (2020); Agrawal et al. (2024); Wu et al. (2024). We first formally define the problem setup, and then present the algorithm design and the regret analysis. From the regret analysis one could readily see why removing Assumption 2 is necessary for a near-optimal regret upper bound. Lastly, we provide a matching lower bound which suggests that we obtain the near-optimal worst-case regret guarantee. ", "page_idx": 30}, {"type": "text", "text": "D.1 Problem Setup ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The learning algorithm and environment perform the following interactions repeatedly in round $t=1,\\dots,T$ : ", "page_idx": 30}, {"type": "text", "text": "1. The algorithm samples action $a_{t}$ from $[K]$ via $a_{t}\\sim w_{t}:=\\left(w_{t,1},\\ldots,w_{t,K}\\right)\\in\\Omega$ , i.e., the probability of sampling action $i\\in[K]$ is $w_{t,i}$ . The environment draws loss $\\ell_{t,i}$ from \u201cclean\u201d distribution $P_{t,i}$ satisfying Assumption 1 for every action $i\\in[K]$ .   \n2. Let $\\overline{{\\ell}}_{t,a_{t}}$ denote the feedback revealed to the algorithm associated with $a_{t}$ . With probability $\\alpha\\in(0,1]$ , the algorithm observes the contaminated feedback. That is, it observes $\\overline{{\\ell}}_{t,a_{t}}=$ $\\widetilde{\\ell}_{t,i}$ , which is generated from an arbitrary \u201cbad\u201d distribution $Q_{t,i}$ . With probability $(1-\\alpha)$ , it observes the \u201cclean\u201d loss $\\overline{{\\ell}}_{t,a_{t}}=\\ell_{t,i}$ .   \n3. The algorithm determines $w_{t+1}$ based on all the revealed history so far. ", "page_idx": 30}, {"type": "text", "text": "The goal of the learning algorithm is to minimize the regret in the presence of contaminated feedback. For the ease of presentation, we consider the weaker expected pseudo-regret (rather than the highprobability version), which is defined as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]:=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\langle w_{t}-y^{i^{*}},\\mu_{t}\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the optimal action $i^{*}$ and loss mean $\\mu_{t}$ are both still with respect to the \u201cclean\u201d distributions, i.e., $\\mu_{t,i}\\,:=\\,\\mathbb{E}_{\\ell_{t,i}\\sim P_{t,i}}\\left[\\ell_{t,i}\\right]$ and $\\begin{array}{r}{i^{*}\\;\\in\\;\\mathrm{argmin}_{i\\in[K]}\\sum_{t=1}^{T}\\mu_{t,i}}\\end{array}$ , while the expected pseudo-regret additionally includes the randomness from the contamination. Similar to all previous works, we assume that the \u201ccontamination level\u201d $\\alpha$ is known to the algorithm. ", "page_idx": 30}, {"type": "text", "text": "Assumption 2 adapted to the contaminated case (to bound CONTRIMERR) becomes the following. ", "page_idx": 30}, {"type": "text", "text": "Assumption 3 (Truncated non-negative losses in the contaminated case). Given any fixed $M>0$ , the loss distributions of the optimal action $i^{*}$ satisfy that $\\mathbb{E}_{\\ell_{t,i^{*}}\\sim P_{t,i^{*}}^{\\alpha}}[\\ell_{t,i^{*}}\\cdot\\mathbb{I}\\{|\\ell_{t,i^{*}}|>M\\}]\\geqslant0,\\forall t\\in[T]$ . ", "page_idx": 30}, {"type": "text", "text": "Importantly, even if there is some prior knowledge regarding the \u201cclean\u201d distribution $P_{t,i}$ , due to the arbitrary \u201cbad\u201d distribution component contained in $P_{t,i^{*}}^{\\alpha}$ , the adapted \u201ctruncated non-negativity\u201d may not hold any more and one may not obtain any meaningful regret guarantee. However, by taking advantage of the log-barrier regularizer, we naturally handle all possible \u201cbad\u201d distribution $Q_{t,i}$ . In the following two subsections, we the details of regret upper bound and lower bound, respectively. ", "page_idx": 31}, {"type": "text", "text": "D.2 Regret Guarantee and Analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To obtain near-optimal high-probability regret guarantee again Huber contamination, we simply need to run our Algorithm 1 with different initial learning rate and trimming threshold, both of which now further depend on the contamination level. We formally state the theoretical guarantee below. ", "page_idx": 31}, {"type": "text", "text": "Theorem 3. For any failure probability $\\zeta$ and contamination level $\\alpha>0,$ , by choosing initial learning rate ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{u}\\left(\\frac{K\\log(T)}{T}\\right)^{\\frac{2+v}{2+2v}}\\left(\\frac{1}{\\alpha}\\right)^{\\frac{v}{2+2v}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and trimming threshold ", "page_idx": 31}, {"type": "equation", "text": "$$\nM_{t,i}=\\operatorname*{min}\\{\\frac{u}{\\alpha^{\\frac{1}{1+v}}},\\left(\\frac{u^{1+v}}{\\eta\\alpha}\\right)^{\\frac{1}{2+v}},\\frac{1}{110\\eta\\log(T)\\log(40T^{2}/\\zeta)}\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Algorithm $^{\\,l}$ ensures that with probability at least $1-\\zeta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\nR_{T}=O\\left(u\\alpha^{\\frac{v}{1+v}}T+u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}(\\log(T))^{1.5}\\log(T/\\zeta)\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By further choosing $\\zeta=1/T$ , Algorithm $^{\\,l}$ ensures that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\left(u\\alpha^{\\frac{v}{1+v}}T+u K^{\\frac{v}{1+v}}T^{\\frac{1}{1+v}}(\\log(T))^{2.5}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We first rewrite regret as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}=\\underbrace{\\sum_{t\\in N_{Q}}(\\mu_{t,a_{t}}-\\overline{{\\ell}}_{t,a_{t}}^{\\prime})-\\sum_{t\\in N_{Q}}(\\mu_{t,i^{*}}-\\overline{{\\ell}}_{t,i^{*}}^{\\prime})}_{\\mathrm{CovTRMERRI}}+\\underbrace{\\sum_{t\\in N_{P}}(\\mu_{t,a_{t}}-\\overline{{\\ell}}_{t,a_{t}}^{\\prime})-\\sum_{t\\in N_{P}}(\\mu_{t,i^{*}}-\\overline{{\\ell}}_{t,i^{*}}^{\\prime})}_{\\mathrm{CovTRMERRI}}}\\\\ &{\\ \\ \\ +\\underbrace{\\sum_{t=1}^{T}(\\overline{{\\ell}}_{t,a_{t}}^{\\prime}-\\overline{{\\ell}}_{t,i^{*}}^{\\prime})}_{\\mathrm{CovTRMER}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $N_{Q}$ is the set containing all round indices in which the observation is contaminated (i.e., $\\overline{{\\ell}}_{t,a_{t}}=\\widetilde{\\ell}_{t,a_{t}}\\sim Q_{t,a_{t}})$ , and $N_{P}=[T]\\backslash N_{Q}$ (i.e., those rounds in which $\\overline{{\\ell}}_{t,a_{t}}=\\ell_{t,a_{t}}\\sim P_{t,a_{t}})$ . Similar to the uncontaminated case, it is sufficient to choose some fixed trimming threshold $M_{t,i}=$ $M$ . ", "page_idx": 31}, {"type": "text", "text": "D.2.1 Bounding CONTRIMERR I ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Recall that $|N_{Q}|$ denotes the total number of rounds when the feedback is contaminated, which is exactly the sum of $T$ independent random variables from $\\mathbf{Ber}(\\alpha)$ . By standard concentration results (e.g., Lemma 10), we have with probability at least $1-\\zeta$ that ", "page_idx": 31}, {"type": "equation", "text": "$$\n|N_{Q}|\\leqslant\\alpha T+\\sqrt{2T\\alpha(1-\\alpha)\\log(1/\\zeta)}+\\frac{2\\log(1/\\zeta)}{3}\\leqslant2\\alpha T+\\frac{4\\log(1/\\zeta)}{3},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last step is again due to the fact that $\\begin{array}{r}{\\sqrt{x_{1}x_{2}}\\leqslant\\frac{x_{1}+x_{2}}{2},\\forall x_{1},x_{2}\\geqslant0}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "For any fixed trimming threshold $M$ , it holds almost surely that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mu_{t,i}-\\overline{{\\ell}}_{t,i}^{\\prime}=\\mu_{t,i}-\\mu_{t,i}^{\\prime}+\\mu_{t,i}^{\\prime}-\\overline{{\\ell}}_{t,i}\\leqslant u^{1+v}M^{-v}+2M,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which implies that, with probability at least $1-\\zeta$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{CoNTRIMERR~I\\leqslant2}\\left(2\\alpha T+\\frac{4\\log(1/\\zeta)}{3}\\right)\\cdot\\left(u^{1+v}M^{-v}+2M\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "D.2.2 Bounding CONTRIMERR II ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In round $t\\in N_{P}$ , the losses are not contaminated. By Lemma 1, we have probability with at least $1-\\zeta$ that, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{ConTRIMERR~II}\\leqslant2\\,|N_{P}|\\,u^{1+v}M^{-v}+8\\sqrt{|N_{P}|\\,u^{1+v}M^{1-v}\\log(2\\log(T)/\\zeta)}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}+8M\\log(2\\log(T)/\\zeta)}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x}\\leqslant6T u^{1+v}M^{-v}+12M\\log(2\\log(T)/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "D.2.3 Bounding CONTRIMREG ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We first rewrite this part as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathrm{CoNTRIMREG}=\\sum_{t=1}^{T}\\left(\\left<w_{t},\\widehat{\\ell}_{t}\\right>-\\left<y^{i^{*}},\\ell_{t}^{\\prime}\\right>\\right)}&{}\\\\ &{=\\displaystyle\\sum_{\\mathrm{CoNTRIMREGI}}^{T}+\\sum_{\\mathrm{CoNTRIMREGI}}^{T}+\\sum_{\\mathrm{CoNTRIMREGII}}^{T}+\\sum_{\\mathrm{CoNTRIMREGII}}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "D.2.4 Bounding CONTRIMREGI ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "As long as it holds that $\\begin{array}{r}{\\eta\\leqslant\\frac{1}{10M}}\\end{array}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle w_{t}-y^{\\prime},\\widehat{\\ell}_{t}\\rangle\\leqslant\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+5\\eta M^{1-\\upsilon}\\sum_{t=1}^{T}\\left|\\overline{{\\ell}}_{t,a_{t}}^{\\prime}\\right|^{1+\\upsilon},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and it is left to bound tT=1  \u2113\u2032t,at ", "page_idx": 32}, {"type": "text", "text": "We first rewrite it as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left|\\overline{{\\ell}}_{t,a_{t}}^{\\prime}\\right|^{1+v}=\\sum_{t\\in N_{Q}}\\left|\\widetilde{\\ell}_{t,a_{t}}^{\\prime}\\right|^{1+v}+\\sum_{t\\in N_{P}}\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+v}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, with probability at least $1-\\zeta$ , it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t\\in N_{Q}}\\left|\\widetilde{\\ell}_{t,a_{t}}^{\\prime}\\right|^{1+v}\\leqslant\\left(2\\alpha T+\\frac{4\\log(1/\\zeta)}{3}\\right)M^{1+v}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As in the uncontaminated case, by Lemma 11, we have with probability at least $1-\\zeta$ that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\in{N_{P}}}\\left|\\ell_{t,a_{t}}^{\\prime}\\right|^{1+\\upsilon}\\leqslant\\sum_{t\\in N_{P}}\\mathbb{E}\\left[|\\ell_{t,a_{t}}^{\\prime}|^{1+\\upsilon}\\right]+4\\sqrt{|N_{P}|\\,u^{1+\\upsilon}M^{1+\\upsilon}\\log(\\log(T)/\\zeta)}+2M^{1+\\upsilon}\\log(\\log(T)/\\zeta)}}\\\\ &{}&{\\leqslant T u^{1+\\upsilon}+4\\sqrt{T u^{1+\\upsilon}M^{1+\\upsilon}\\log(\\log(T)/\\zeta)}+2M^{1+\\upsilon}\\log(\\log(T)/\\zeta).\\qquad\\quad(102)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking a union bound, we get with probability at least $1-\\zeta$ that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{CoNTRIMREGI}\\leqslant\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+5\\eta\\left(2\\alpha T+\\frac{4\\log(2/\\zeta)}{3}\\right)M^{2}+5\\eta M^{1-\\upsilon}T u^{1+\\upsilon}}}\\\\ &{\\quad\\quad\\quad\\quad+20\\eta\\sqrt{T u^{1+\\upsilon}M^{3-\\upsilon}\\log(2\\log(T)/\\zeta)}+10\\eta M^{2}\\log(2\\log(T)/\\zeta)}\\\\ &{\\quad\\quad\\quad\\leqslant\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+5\\eta\\left(2\\alpha T+\\frac{4\\log(2/\\zeta)}{3}\\right)M^{2}+15\\eta M^{1-\\upsilon}T u^{1+\\upsilon}}\\\\ &{\\quad\\quad\\quad+20\\eta M^{2}\\log(2\\log(T)/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "D.2.5 Bounding CONTRIMREGII ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "By H\u00f6lder\u2019s inequality, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\langle y^{t}-y^{i^{*}},\\bar{\\ell}_{t}^{\\prime}\\rangle\\leqslant\\displaystyle\\sum_{t=1}^{T}\\left\\|y^{t}-y^{i^{*}}\\right\\|_{\\infty}\\left\\|\\bar{\\ell}_{t}^{\\prime}\\right\\|_{1}}&{}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\operatorname*{max}\\{\\lambda/K,\\lambda(1-1/K)\\}\\cdot\\left\\|\\bar{\\ell}_{t}^{\\prime}\\right\\|_{1}}&{}\\\\ {\\displaystyle\\leqslant\\lambda\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\left|\\bar{\\ell}_{t,i}^{\\prime}\\right|}&{}\\\\ {\\displaystyle=\\lambda\\left(\\sum_{t\\in N_{Q}}\\sum_{i=1}^{K}\\Big|\\tilde{\\ell}_{t,i}^{\\prime}\\Big|+\\displaystyle\\sum_{t\\in N_{P}}\\sum_{i=1}^{K}|\\ell_{t,i}^{\\prime}\\Big|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We clearly have with probability at least $1-\\zeta$ that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{t\\in N_{Q}}\\sum_{i=1}^{K}\\left|\\widetilde{\\ell}_{t,i}^{\\prime}\\right|\\leqslant K M\\left(2\\alpha T+\\frac{4\\log(1/\\zeta)}{3}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and with probability with at least $1-\\zeta$ that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{t\\in N_{Q}}\\sum_{i=1}^{K}\\left|\\ell_{t,i}^{\\prime}\\right|\\leqslant u K T+\\sqrt{2K T u^{1+v}M^{1-v}\\log(1/\\zeta)}+\\frac{4}{3}M\\log(1/\\zeta).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By taking a union bound, we get with probability at least $1-\\zeta$ that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CoNTRIMREGII}\\leqslant\\lambda\\left((2\\alpha M+u)K T+\\sqrt{2K T u^{1+\\nu}M^{1-\\nu}\\log(2/\\zeta)}+2K M\\log(2/\\zeta)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "D.2.6 Bounding CONTRIMREGIII ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We first rewrite CONTRIMREGIII as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{CoNTRIMREGIII}=\\sum_{t\\in N_{Q}}\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\widetilde{\\ell}_{t}^{\\prime}\\rangle+\\sum_{t\\in N_{P}}\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To bound the first part, recall that for any $t\\in N_{Q}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\left<y^{\\prime},\\widehat{\\ell}_{t}-\\widetilde{\\ell}_{t}^{\\prime}\\right>\\right|/M\\leqslant2\\langle y^{\\prime},\\rho_{T}\\rangle\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[(\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle/M)^{2}\\middle|\\mathcal{F}_{t}\\right]\\leqslant\\frac{1}{M^{2}}\\mathbb{E}\\left[(\\langle y^{\\prime},\\widehat{\\ell}_{t}\\rangle)^{2}\\middle|\\mathcal{F}_{t}\\right]}&{}\\\\ {=\\frac{1}{M^{2}}\\mathbb{E}\\left[\\frac{\\left(y_{\\alpha_{t}}^{\\prime}\\right)^{2}\\langle\\widehat{\\ell}_{t,\\alpha_{t}}^{\\prime}\\rangle^{2}}{\\langle w_{t,\\alpha_{t}}\\rangle^{2}}\\Bigg|\\mathcal{F}_{t}\\right]}\\\\ &{=\\frac{1}{M^{2}}\\underset{i=1}{\\overset{K}{\\sum}}w_{i}\\frac{(y_{i}^{\\prime})^{2}}{(w_{t,i})^{2}}\\mathbb{E}\\left[\\langle\\widetilde{\\ell}_{t,i}\\rangle^{2}\\middle|\\mathcal{F}_{t},a_{t}=i\\right]}\\\\ &{\\leqslant\\frac{1}{M^{2}}\\underset{i=1}{\\overset{K}{\\sum}}\\frac{(y_{i}^{\\prime})^{2}}{w_{t,i}^{2}}M^{2}}\\\\ &{\\leqslant\\langle y^{\\prime},\\rho_{T}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, with probability at least $1-\\zeta$ , it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t\\in N_{Q}}\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\widetilde{\\ell}_{t}^{\\prime}\\rangle\\leqslant3M\\sqrt{\\left(2\\alpha T+\\frac{4\\log(2/\\zeta)}{3}\\right)\\langle y^{\\prime},\\rho_{T}\\rangle\\iota^{\\prime}+2M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota^{\\prime}}}}\\\\ &{}&{\\leqslant\\frac{4.5M\\left(2\\alpha T+\\frac{4\\log(2/\\zeta)}{3}\\right)}{2}+\\frac{2M\\langle y^{\\prime},\\rho_{T}\\rangle\\iota^{\\prime}}{2}+2M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota^{\\prime}}\\\\ &{}&{\\leqslant3M\\left(2\\alpha T+\\frac{4\\log(2/\\zeta)}{3}\\right)+3M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota^{\\prime}:=\\log(4\\operatorname*{max}\\{\\sqrt{\\left(2\\alpha T+\\frac{4\\log(2/\\zeta)}{3}\\right)\\langle y^{\\prime},\\rho_{T}\\rangle},1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the uncontaminated part, with probability, by Lemma 12, we have with probability at least $1-\\zeta$ that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in N_{P}}\\langle y^{\\prime},\\widehat{\\ell}_{t}-\\ell_{t}^{\\prime}\\rangle\\leqslant3u\\sqrt{T u^{-1+v}M^{1-v}\\langle y^{\\prime},\\rho_{T}\\rangle\\iota^{\\prime\\prime}}+2u\\operatorname*{max}\\{1,2M\\langle y^{\\prime},\\rho_{T}\\rangle/u\\}\\iota^{\\prime\\prime}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leqslant\\frac{4.5u T u^{v}M^{-v}}{2}+\\frac{2u u^{-1}M\\langle y^{\\prime},\\rho_{T}\\rangle\\iota^{\\prime}}{2}+2u\\operatorname*{max}\\{1,2M\\langle y^{\\prime},\\rho_{T}\\rangle/u\\}\\iota^{\\prime\\prime}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leqslant3T u^{1+v}M^{-v}+5\\operatorname*{max}\\{u,M\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\iota^{\\prime\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\iota^{\\prime\\prime}:=\\log(2\\operatorname*{max}\\{\\sqrt{T u^{-1+v}M^{1-v}\\langle y^{\\prime},\\rho_{T}\\rangle},1,2M\\langle y^{\\prime},\\rho_{T}\\rangle/u\\}/\\zeta)$ . Taking a union bound, we get with probability at least $1-\\zeta$ that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{2onTRIMREGIII}\\leqslant3M\\left(2\\alpha T+\\frac{4\\log(3/\\zeta)}{3}\\right)+3T u^{1+\\upsilon}M^{-\\upsilon}}}\\\\ &{\\rightharpoonup}\\\\ &{\\qquad\\qquad+3M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\log(6\\operatorname*{max}\\{\\sqrt{\\left(2\\alpha T+\\frac{4\\log(3/\\zeta)}{3}\\right)K/\\lambda},2K/\\lambda\\}/\\zeta)}\\\\ &{\\qquad\\qquad+\\,3\\operatorname*{max}\\{1,2M\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\log(6\\operatorname*{max}\\{\\sqrt{2T u^{1+\\upsilon}M^{1-\\upsilon}K/\\lambda},4M K/\\lambda\\}/\\zeta)\\underset{+\\,,\\ldots}{,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "D.2.7 Putting All Pieces Together ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Combining all the parts, we have with probability at least $1-\\zeta$ that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R_{T}\\leqslant4\\left(\\alpha T+\\frac{2\\log(5/\\zeta)}{3}\\right)\\cdot{\\left({\\alpha^{1+\\mathrm{e}}}M^{-\\upsilon}+2M\\right)}+6T{\\alpha^{1+\\upsilon}}M^{-\\upsilon}+12M\\log(101\\log(T)/\\zeta)}}\\\\ {{\\qquad+\\frac{2K\\log T}{\\eta}-\\frac{\\langle{y^{\\prime}},\\rho_{T}\\rangle}{10\\eta\\log T}+10\\eta\\left(\\alpha T+\\frac{2\\log(10/\\zeta)}{3}\\right)M^{2}+15\\eta M^{1-\\upsilon}T{\\alpha^{1+\\upsilon}}}}\\\\ {{\\qquad+20\\eta M^{2}\\log(10\\log(T)/\\zeta)+\\left(2\\alpha M+u\\right)M X T+\\lambda\\sqrt{2K T u^{1+\\upsilon}M^{1-\\upsilon}\\log(10/\\zeta)}}}\\\\ {{\\qquad+2\\lambda K M\\log(10/\\zeta)+3M\\left(2\\alpha T+\\frac{4\\log(15/\\zeta)}{3}\\right)}}\\\\ {{\\qquad+3M\\operatorname*{max}\\{1,2\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\log(15\\operatorname*{max}\\{\\sqrt{\\left(2\\alpha T+\\frac{4\\log(15/\\zeta)}{3}\\right)K/\\lambda},2K/\\lambda\\}/\\zeta)}}\\\\ {{\\qquad+3T u^{1+\\upsilon}M^{-\\upsilon}}}\\\\ {{\\qquad+5\\operatorname*{max}\\{u,M(y/\\zeta,\\pi)\\}\\log(10\\operatorname*{max}\\{\\sqrt{2T u^{-1+\\upsilon}M^{1-\\upsilon}K/\\lambda},4M K/(u\\lambda)\\}/\\zeta),\\qquad\\mathrm{(114)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We first plug in $\\lambda$ and simplify the upper bound as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{T}\\leqslant4\\left(\\alpha T+\\frac{2\\log(5/\\zeta)}{3}\\right)\\cdot\\left(u^{1+v}M^{-v}+2M\\right)+9T u^{1+v}M^{-v}+12M\\log(10\\log(T)/\\zeta)}\\\\ &{\\qquad+\\frac{2K\\log T}{\\eta}-\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+10\\eta\\left(\\alpha T+\\frac{2\\log(10/\\zeta)}{3}\\right)M^{2}+15\\eta M^{1-v}T u^{1+v}}\\\\ &{\\qquad+20\\eta M^{2}\\log(10\\log(T)/\\zeta)+2\\alpha M T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}}+u T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}}+u T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}}\\sqrt{2\\log(10/\\zeta)}}\\\\ &{\\qquad+\\:2T^{\\frac{-v}{1+v}}K^{\\frac{v}{1+v}}M\\log(10/\\zeta)+3M\\left(2\\alpha T+\\frac{4\\log(15/\\zeta)}{3}\\right)}\\\\ &{\\qquad+6M\\langle y^{\\prime},\\rho_{T}\\rangle\\log(15\\operatorname*{max}\\{\\sqrt{\\left(2\\alpha T+\\frac{4\\log(15/\\zeta)}{3}\\right)K T},2K T\\}/\\zeta)}\\\\ &{\\qquad+5\\operatorname*{max}\\{u,M\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\log(10T\\operatorname*{max}\\{\\sqrt{2u^{-1+v}M^{1-v}K},4M K/u\\}/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Assuming $\\begin{array}{r}{\\alpha T\\geqslant\\frac{4}{3}\\log(15\\log(T)/\\zeta)}\\end{array}$ and $T\\geqslant{\\sqrt{T\\log(15\\log(T)/\\zeta)}}$ , we further simplify the upper bound as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}\\leqslant8\\alpha T\\cdot\\left(u^{1+\\alpha}H^{-\\alpha}+2M\\right)+9T u^{1+\\alpha}M^{-\\alpha}+21M\\alpha T}\\\\ &{\\quad\\quad+\\frac{2K\\log{T}}{\\eta}-\\frac{\\langle y^{\\prime},\\rho\\rangle}{10\\eta\\log{T}}+20\\eta\\alpha T M^{2}+35\\eta M^{1-\\alpha}T\\alpha^{1+\\nu}}\\\\ &{\\quad\\quad+20\\eta M^{2}\\alpha T+2\\alpha M T^{\\frac{1}{\\alpha}+K}{K^{\\frac{\\alpha}{\\alpha}}}+{u}T^{\\frac{1}{1+\\alpha}}+{K}T^{\\frac{\\alpha}{1+\\alpha}}+u T^{\\frac{1+\\alpha}{1+\\alpha}}\\sqrt{2\\log(10/\\zeta)}}\\\\ &{\\quad\\quad+2\\alpha M T^{\\frac{1}{\\alpha}+K}{K^{\\frac{1}{\\alpha}}}+6M\\zeta^{\\prime},\\rho\\gamma\\log(15\\operatorname*{max}\\{\\sqrt{3}\\alpha K T^{\\frac{1}{\\alpha}},2K T\\}/\\zeta)}\\\\ &{\\quad\\quad+5\\operatorname*{max}\\{u,M(\\gamma^{\\prime},\\rho\\gamma)\\}\\log(10\\tau/\\operatorname*{max}\\{\\sqrt{2}\\alpha^{-1+\\nu}M^{1-\\nu}K,4M K/u\\}/\\zeta)}\\\\ &{\\leqslant32\\alpha M T+17T u^{1+\\alpha}M^{-\\alpha}+40\\eta\\alpha T M^{2}+35\\eta M^{1-\\alpha}T^{1+\\nu}+\\frac{2K\\log{T}}{\\eta}}\\\\ &{\\quad\\quad-\\frac{\\langle y^{\\prime},\\rho\\gamma\\rangle}{10\\eta\\log{T}}+u T^{\\frac{1}{\\alpha}+K}u^{\\frac{\\alpha}{1+\\alpha}}+u T^{\\frac{1}{\\alpha}+K}T^{\\frac{\\alpha}{1+\\alpha}}\\sqrt{2\\log(10/\\zeta)}}\\\\ &{\\quad\\quad+6M\\langle y^{\\prime},\\rho\\gamma\\rangle\\log(30K T/\\zeta)}\\\\ &{\\quad\\quad+5\\operatorname*{max}\\{u,M(\\gamma^{\\prime},\\rho\\gamma)\\}\\log(10/\\operatorname*{max}\\{\\sqrt{2\\alpha^{-1+\\nu}M^{1-\\nu}K},4M K/u\\}/\\zeta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in the last inequality we apply $K\\leqslant T$ to upper bound terms of form $\\Theta(\\alpha M T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}})$ . ", "page_idx": 35}, {"type": "text", "text": "For any $\\eta>0$ , choosing $\\begin{array}{r}{M=\\operatorname*{min}\\{\\frac{u}{\\alpha^{\\frac{1}{1+v}}},\\left(\\frac{u^{1+v}}{\\eta\\alpha}\\right)^{\\frac{1}{2+v}},\\frac{1}{10\\eta}\\}}\\end{array}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}=O\\left((\\eta\\alpha)^{\\frac{\\upsilon}{2+\\upsilon}}u^{1+\\frac{\\upsilon}{2+\\upsilon}}T+(\\eta)^{\\upsilon}u^{1+\\upsilon}T+\\frac{K\\log T}{\\eta}\\right)}\\\\ &{\\qquad-\\left.\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+6M\\langle y^{\\prime},\\rho_{T}\\rangle\\log(30K T/\\zeta)\\right.}\\\\ &{\\qquad+\\left.5\\operatorname*{max}\\{u,M\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\log(10T\\operatorname*{max}\\{\\sqrt{2u^{-1+\\upsilon}M^{1-\\upsilon}K},4M K/u\\}/\\zeta)\\right.}\\\\ &{\\qquad+\\left.O\\left(u\\alpha^{\\frac{\\upsilon}{1+\\upsilon}}T+u T^{\\frac{1}{1+\\upsilon}}K^{\\frac{\\upsilon}{1+\\upsilon}}\\sqrt{\\log(1/\\zeta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we are going to choose $\\begin{array}{r}{\\eta=\\frac{1}{u}\\cdot\\operatorname*{min}\\{\\left(\\frac{K\\log(T)}{T}\\right)^{\\frac{1}{1+v}},\\left(\\frac{K\\log(T)}{T}\\right)^{\\frac{2+v}{2+2v}}\\left(\\frac{1}{\\alpha}\\right)^{\\frac{v}{2+2v}}\\}}\\end{array}$ . We further assume that $\\alpha^{\\frac{v}{(1+v)(2+v)}}T\\geqslant K\\log(T)$ , which implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left({\\frac{K\\log(T)}{T}}\\right)^{\\frac{1}{1+v}}\\geqslant\\left({\\frac{K\\log(T)}{T}}\\right)^{\\frac{2+v}{2+2v}}\\left({\\frac{1}{\\alpha}}\\right)^{\\frac{v}{2+2v}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and thus $\\begin{array}{r}{\\eta=\\frac{1}{u}\\left(\\frac{K\\log(T)}{T}\\right)^{\\frac{2+v}{2+2v}}\\left(\\frac{1}{\\alpha}\\right)^{\\frac{v}{2+2v}}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "Finally, we need to cancel the terms containing $\\langle y^{\\prime},\\rho_{T}\\rangle$ to obtain the high-probability guarantee. ", "page_idx": 36}, {"type": "text", "text": "Case 1: $u\\geqslant M\\langle y^{\\prime},\\rho_{T}\\rangle$ . If $\\operatorname*{max}\\{u,M\\langle y^{\\prime},\\rho_{T}\\rangle\\}=u$ , it would just introduce a $\\tilde{O}(u)$ term which can be ignored. In this case, $\\eta$ must satisfy that ", "page_idx": 36}, {"type": "equation", "text": "$$\n-\\frac{1}{10\\eta\\log(T)}+6M\\log(30K T/\\zeta)\\leqslant0,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which implies that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\eta\\leqslant{\\frac{1}{60M\\log(T)\\log(30K T/\\zeta)}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Case 2: $u<M\\langle y^{\\prime},\\rho_{T}\\rangle$ . With the current choice of $M$ and $\\eta$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nM\\leqslant\\frac{1}{10\\eta}\\leqslant0.1u\\alpha^{\\frac{v}{2+2v}}\\left(\\frac{T}{K\\log(T)}\\right)^{\\frac{2+v}{2+2v}}\\leqslant u T/K,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which implies that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(10T\\operatorname*{max}\\{\\sqrt{2\\pi^{-1+v}}M^{1-v}K,4M K/u\\}/\\zeta)\\leqslant\\log(10T\\operatorname*{max}\\{\\sqrt{2T},4T\\}/\\zeta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leqslant\\log(40T^{2}/\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now it is sufficient to have $\\eta$ satisfying that ", "page_idx": 36}, {"type": "equation", "text": "$$\n-\\frac{1}{10\\eta\\log(T)}+11M\\log(40T^{2}/\\zeta)\\leqslant0,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "meaning that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\eta\\leqslant\\frac{1}{110M\\log(T)\\log(40T^{2}/\\zeta)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In summary, in either case, it is sufficient to have $\\begin{array}{r}{\\eta M\\,\\leqslant\\,\\frac{1}{110\\log(T)\\log(40T^{2}/\\zeta)}}\\end{array}$ . Note that this is stronger than (and implies) the stability condition $\\eta M\\leqslant0.1$ . ", "page_idx": 36}, {"type": "text", "text": "Starting again from Eq. (117), with any $\\eta>0$ , choosing ", "page_idx": 36}, {"type": "equation", "text": "$$\nM=\\operatorname*{min}\\{\\frac{u}{\\alpha^{\\frac{1}{1+\\nu}}},\\left(\\frac{u^{1+\\nu}}{\\eta\\alpha}\\right)^{\\frac{1}{2+\\nu}},\\frac{1}{110\\eta\\log(T)\\log(40T^{2}/\\zeta)}\\},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}=O\\left((\\eta\\alpha)^{\\frac{\\nu}{2+\\nu}}u^{1+\\frac{\\nu}{2+\\nu}}T+(\\eta)^{\\upsilon}u^{1+\\upsilon}T\\left(\\log(T)\\log(40T^{2}/\\zeta)\\right)^{\\upsilon}+\\frac{K\\log T}{\\eta}\\right)}\\\\ &{\\qquad-\\left.\\frac{\\langle y^{\\prime},\\rho_{T}\\rangle}{10\\eta\\log T}+6M\\langle y^{\\prime},\\rho_{T}\\rangle\\log(30K T/\\zeta)\\right.}\\\\ &{\\qquad+\\left.5\\operatorname*{max}\\{u,M\\langle y^{\\prime},\\rho_{T}\\rangle\\}\\log(10T\\operatorname*{max}\\{\\sqrt{2u^{-1+\\nu}M^{1-\\nu}K},4M K/u\\}/\\zeta)\\right.}\\\\ &{\\qquad+\\left.O\\left(u\\alpha^{\\frac{\\nu}{1+\\nu}}T+u T^{\\frac{1}{1+\\nu}}K^{\\frac{\\nu}{1+\\nu}}\\sqrt{\\log(1/\\zeta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lastly, still choosing ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\prime={\\frac{1}{u}}\\cdot\\operatorname*{min}\\{\\left({\\frac{K\\log(T)}{T}}\\right)^{\\frac{1+v}{1+v}},\\left({\\frac{K\\log(T)}{T}}\\right)^{\\frac{2+v}{2+2v}}\\left({\\frac{1}{\\alpha}}\\right)^{\\frac{v}{2+2v}}\\}={\\frac{1}{u}}\\left({\\frac{K\\log(T)}{T}}\\right)^{\\frac{2+v}{2+2v}}\\left({\\frac{1}{\\alpha}}\\right)^{\\frac{v}{2+2v}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "yields that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}=O\\left(u K^{\\frac{\\nu}{2+2\\nu}}T^{\\frac{2+\\nu}{2+2\\nu}}(\\log T)^{\\frac{\\nu}{2+2\\nu}}\\alpha^{\\frac{\\nu}{2+2\\nu}}+u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}(\\log(T))^{1+\\frac{\\nu}{1+\\nu}}\\left(\\log(40T^{2}/\\zeta)\\right)^{v}\\right)}\\\\ &{\\qquad+O\\left(u\\alpha^{\\frac{\\nu}{1+\\nu}}T+u T^{\\frac{1}{1+\\nu}}K^{\\frac{\\nu}{1+\\nu}}\\sqrt{\\log(1/\\zeta)}\\right)}\\\\ &{\\qquad\\leqslant O\\left(u\\alpha^{\\frac{\\nu}{1+\\nu}}T+u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}\\left((\\log(T))^{1+\\frac{\\nu}{1+\\nu}}\\left(\\log(40T^{2}/\\zeta)\\right)^{v}+\\sqrt{\\log(1/\\zeta)}\\right)\\right)}\\\\ &{\\qquad\\leqslant O\\left(u\\alpha^{\\frac{\\nu}{1+\\nu}}T+u K^{\\frac{\\nu}{1+\\nu}}T^{\\frac{1}{1+\\nu}}(\\log T)^{1.5}\\log(40T^{2}/\\zeta)\\right).}\\end{array}\\quad\\!\\!\\!\\!(12)\\qquad\\qquad\\mathrm{(12)}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "D.3 Regret Lower Bound with Contamination ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Theorem 4 (Lower Bound for Heavy-tailed Adversarial Bandits with Huber Contamination). For any bandit algorithm, there must exist one problem instance in which the algorithm suffers regret ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\Omega\\left(u T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}}+u T\\alpha^{\\frac{v}{1+v}}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Theorem 4. In the presence of heavy tails, every algorithm suffers $\\Omega\\left(u T^{\\frac{1}{1+v}}K^{\\frac{v}{1+v}}\\right)$ regret in the worst case, regardless of contamination level $\\alpha$ (Bubeck et al., 2013). ", "page_idx": 37}, {"type": "text", "text": "Therefore, to show the lower bound with contamination, it suffices to show a lower bound of ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Omega\\left(u T\\alpha^{\\frac{v}{1+v}}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "given any $\\alpha\\in(0,1]$ . ", "page_idx": 37}, {"type": "text", "text": "This proof is a direct modification based on $\\mathrm{Wu}$ et al. (2024, Appendix B). We use $\\pi$ to denote a bandit algorithm. We construct two environments, denoted by $\\nu_{1}$ and $\\nu_{2}$ , respectively. And then we show that any algorithm suffers the claimed regret in one of these two environments. ", "page_idx": 37}, {"type": "text", "text": "Environment $\\nu_{1}$ . In $\\nu_{1}$ , the loss of action 1 in every round $t\\in[T]$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\ell_{t,1}=\\left\\{\\begin{array}{l l}{\\displaystyle u/\\gamma,}&{\\mathrm{with~probability~}\\displaystyle\\frac{1}{2}\\gamma^{1+v},}\\\\ {\\displaystyle0,}&{\\mathrm{with~probability~}1-\\displaystyle\\frac{1}{2}\\gamma^{1+v},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\gamma\\leqslant1$ is some free parameter to choose at the last step of the proof. One can verify that $\\mathbb{E}[|\\ell_{1}|^{1+v}]\\leqslant u^{1+v}$ via direct calculations. ", "page_idx": 37}, {"type": "text", "text": "For any suboptimal action $i\\neq1$ , the loss in every round $t\\in[T]$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\ell_{t,i}=\\left\\{\\!\\!\\begin{array}{l l}{\\displaystyle u/\\gamma,}&{\\mathrm{with~probability~}\\displaystyle\\frac{3}{10}\\gamma^{1+v},}\\\\ {\\displaystyle0,}&{\\mathrm{with~probability~}1-\\displaystyle\\frac{3}{10}\\gamma^{1+v}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "One can verify that $\\mathbb{E}[|\\ell_{t,i}|^{1+v}]\\leqslant u^{1+v},\\forall i\\neq1$ . Moreover, action 1 is the optimal one and we have the \u201csub-optimality gap\u201d $\\begin{array}{r}{\\Delta:=\\mathbb{E}[\\ell_{t,1}-\\ell_{t,i}]=\\frac{u}{5}\\gamma^{v},\\forall i\\neq1}\\end{array}$ . ", "page_idx": 37}, {"type": "text", "text": "Given algorithm $\\pi$ and environment $\\nu_{1}$ , we define $i^{\\prime}=\\mathrm{\\boldmath~\\argmin~}\\,\\mathbb{E\\ }[N_{T,i}]$ , and hence we have $i\\!\\in\\!\\{2,\\!...,\\!K\\}^{\\pi,\\nu_{1}}$ $\\begin{array}{r}{\\frac{\\mathbb{E}}{\\pi,\\nu_{1}}\\left[N_{T,i^{\\prime}}\\right]\\leqslant\\frac{T}{K-1}}\\end{array}$ . Now we are able to construct the second environment. ", "page_idx": 37}, {"type": "text", "text": "Environment $\\nu_{2}$ . In this environment, everything is the same as in $\\nu_{1}$ , except that for action $i^{\\prime}$ , now the loss follows ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\ell_{t,i^{\\prime}}=\\left\\{\\begin{array}{l l}{\\displaystyle u/\\gamma,}&{\\mathrm{with~probability~}\\displaystyle\\frac{7}{10}\\gamma^{1+v},}\\\\ {\\displaystyle0,}&{\\mathrm{with~probability~}1-\\displaystyle\\frac{7}{10}u^{1+v}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "One can verify that $\\begin{array}{r}{\\mathbb{E}[\\ell_{i^{\\prime}}]=\\frac{7}{10}\\gamma^{1+v},\\mathbb{E}[|\\ell_{i^{\\prime}}|^{1+v}]\\leqslant u^{1+v}}\\end{array}$ and now the optimal action is $i^{\\prime}$ . We use $\\widetilde{\\nu}_{2}$ to denote the contaminated version of $\\nu_{2}^{\\prime}$ (where the bad distributions are determined later). ", "page_idx": 37}, {"type": "text", "text": "And then, we use $\\widetilde{\\nu}_{1}\\widetilde{(\\nu_{2})}$ to denote the contaminated version of $\\nu_{1}~(\\nu_{2})$ . The bad distributions will be determined later. ", "page_idx": 37}, {"type": "text", "text": "Environments $\\widetilde{\\nu}_{1}$ and $\\widetilde{\\nu}_{2}$ . We choose $\\gamma\\ =\\ \\alpha^{\\frac{1}{1+v}}\\ \\in\\ (0,1]$ . Then for any $i~\\in~[K]$ , we have $\\begin{array}{r}{\\mathrm{TV}(P_{t,i}||P_{t,i}^{\\prime})\\leqslant\\frac{2}{5}\\gamma^{1+v}=\\frac{2}{5}\\alpha\\leqslant\\frac{\\alpha}{1-\\alpha}}\\end{array}$ . According to Lemma 13, for any action $i\\in[K]$ , there exist bad distributions $G_{i}$ and $G_{i}^{\\prime}$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n(1-\\alpha)P_{t,i}+\\alpha G_{i}=(1-\\alpha)P_{t,i}^{\\prime}+\\alpha G_{i}^{\\prime},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $P_{t,i}~(P_{t,i}^{\\prime})$ denotes the loss distribution of action $i$ in round $t$ under environment 1 (2). We construct ${\\widetilde\\nu}_{1}$ and ${\\widetilde{\\nu}}_{2}$ by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\nu}_{1}=\\{x_{i}=(1-\\alpha)P_{t,i}+\\alpha G_{i}:i\\in[K]\\},}\\\\ {\\widetilde{\\nu}_{2}=\\{x_{i}^{\\prime}=(1-\\beta)P_{t,i}^{\\prime}+\\beta G_{i}^{\\prime}:i\\in[K]\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $x_{i}$ and $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ denote the loss distributions for action $i$ in these two environments, respectively. Following from the regret definition, we first have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\pi,\\widetilde{\\nu}_{1})=\\Delta\\left(T-\\underset{\\pi,\\widetilde{\\nu}_{1}}{\\mathbb{E}}[N_{T,1}]\\right)\\geqslant\\frac{\\Delta T}{2}\\mathbb{P}_{\\pi,\\widetilde{\\nu}_{1}}\\left(N_{T,1}\\leqslant\\frac{T}{2}\\right),}\\\\ {R_{T}(\\pi,\\widetilde{\\nu}_{2})=\\Delta\\underset{\\pi,\\widetilde{\\nu}_{2}}{\\mathbb{E}}[N_{T,1}]+\\underset{i\\not\\in\\{1,i^{\\prime}\\}}{\\sum}2\\Delta\\underset{\\pi,\\widetilde{\\nu}_{2}}{\\mathbb{E}}[N_{T,i}]\\geqslant\\frac{\\Delta T}{2}\\mathbb{P}_{\\pi,\\widetilde{\\nu}_{2}}\\left(N_{T,1}\\geqslant\\frac{T}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By adding them together, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{T}(\\pi,\\widetilde{\\nu}_{1})+R_{T}(\\pi,\\widetilde{\\nu}_{2})\\geqslant\\frac{\\Delta T}{2}\\left(\\mathbb{P}_{\\pi,\\widetilde{\\nu}_{1}}\\left(N_{T,1}\\leqslant\\frac{T}{2}\\right)+\\mathbb{P}_{\\pi,\\widetilde{\\nu}_{2}}\\left(N_{T,1}\\geqslant\\frac{T}{2}\\right)\\right)}}\\\\ {{\\displaystyle\\geqslant\\frac{\\Delta T}{4}\\exp\\left(-\\mathrm{KL}(\\mathbb{P}_{\\pi,\\widetilde{\\nu}_{1}}\\|\\mathbb{P}_{\\pi,\\widetilde{\\nu}_{2}})\\right)}}\\\\ {{\\displaystyle\\stackrel{(b)}{=}\\frac{\\Delta T}{4}\\exp\\left(0\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where step (a) follows from the Bretagnolle\u2013Huber inequality (Lattimore & Szepesv\u00e1ri, 2020, Theorem 14.2), and step (b) is due to the fact that $\\widetilde{\\nu}_{1}$ and $\\widetilde{\\nu}_{2}$ are identical under our construction. ", "page_idx": 38}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\Delta=\\frac{u}{5}\\gamma^{v}}\\end{array}$ and $\\gamma=\\alpha^{\\frac1{1+v}}$ , we arrive at ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{R_{T}(\\pi,\\nu_{1}),R_{T}(\\pi,\\nu_{2})\\}\\geqslant\\Omega(u T\\alpha^{\\frac{\\nu}{1+\\nu}}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which completes the proof. ", "page_idx": 38}, {"type": "text", "text": "E BOBW Guarantees with Bounded Losses and Local Differential Privacy ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We first give the definition of Differential Privacy followed by the learning setup. ", "page_idx": 38}, {"type": "text", "text": "Definition 1 (Differential Privacy (DP)). For any given privacy budget $\\varepsilon>0,\\delta\\geqslant0$ , a mechanism $\\mathcal{M}:\\mathcal{D}\\rightarrow\\mathbb{R}^{m}$ is said to be $(\\varepsilon,\\delta)$ -differentially private (DP) if for all datasets $X,X^{\\prime}$ in $\\mathcal{D}$ that differ on only one element and measurable subset $\\mathcal{E}\\,\\subset\\,\\mathbb{R}^{m}$ , it holds that $\\mathbb{P}(\\mathcal{M}(X)\\;\\in\\;\\mathcal{E})\\;\\leqslant$ $\\exp(\\varepsilon)\\cdot\\mathbb{P}(\\mathcal{M}(X^{\\prime})\\in\\mathcal{E})+\\delta$ . When $\\delta=0$ , we refer to $(\\varepsilon,\\delta)$ -DP as $\\varepsilon$ -DP (pure DP), which is stronger than $(\\varepsilon,\\delta)$ -DP for some $\\delta>0$ (approximate DP). ", "page_idx": 38}, {"type": "text", "text": "Definition 2 (Bandits with Bounded Losses and Local DP (LDP)). All loss distributions $(P_{t,i})_{t\\in[T],i\\in[K]}$ have support bounded in $[0,1]$ . Given any privacy budget $\\varepsilon\\in(0,1],\\delta>0$ , the bandit model is said to be $(\\varepsilon,\\delta)$ -LDP if $a_{t+1}$ lies in the sigma-algebra generated by $\\{a_{t^{\\prime}},\\mathcal{M}(\\ell_{t,a_{t^{\\prime}}})\\}_{t^{\\prime}\\in[t]}$ in any round $t\\in[T]$ where $\\mathcal{M}$ is an $(\\varepsilon,\\delta)$ -DP mechanism. ", "page_idx": 38}, {"type": "text", "text": "Roughly speaking, the algorithm should not touch true losses, and it observes privatized losses only. Here, we adopt the widely-used Laplace mechanism (Dwork et al., 2014). Specifically, when data are bounded in $[0,1]$ , adding noise drawn from $\\mathrm{Lap}(\\varepsilon)^{7}$ to them ensures $\\varepsilon$ -DP. By adopting it, the observed loss is the true loss plus an i.i.d. sample from $\\mathrm{Lap}(\\varepsilon)$ . That is, this setup could be viewed as a specific way of generating heavy-tailed losses (i.e., bounded true loss $^+$ Laplace noise for privacy). ", "page_idx": 38}, {"type": "text", "text": "In the literature, Agarwal & Singh (2017) and Tossou & Dimitrakakis (2017) investigated the adversarial regime and proposed algorithms that achieve $\\widetilde{O}(\\frac{\\sqrt{K T}}{\\varepsilon})$ worst-case regret (in expectation) with $\\varepsilon$ -LDP protection, which matches the lower bound (Garcelon et al., 2021) up to some $\\log T$ factor and implies that their algorithms are (nearly) minimax-optimal. In the stochastic regime, Ren et al. ", "page_idx": 38}, {"type": "text", "text": "(2020) proposed privatize\u221ad Upper Confidence Bound-based algorithms with $O(\\sum_{i:\\Delta_{i}>0}\\frac{\\log T}{\\varepsilon^{2}\\Delta_{i}})$ gapdependent regret and O( KT\u03b5 log T) worst-case regret. They also provided a matching gap-dependent lower bound, which scales as \u2126( i:\u2206i>0l\u03b5o2g \u2206T ) when $\\varepsilon\\leqslant1$ . ", "page_idx": 39}, {"type": "text", "text": "In the BOBW setting, Zheng et al. (2020) showed that by privatizing the Tsallis-INF (an optimal algorithm in the (non-private) BOBW setting when losses are bounded) with Gaussian noise, they achieved O( \u03b5KT) regret in the adversarial regime and O  i:\u2206i>0log(T \u03b5)2 lo\u2206gi( regret in the stochastic regime with approximate $(\\varepsilon,\\delta)$ -LDP protection, which is weaker than pure $\\varepsilon$ -LDP. And these regret bounds hold in expectation only. ", "page_idx": 39}, {"type": "text", "text": "In what follows, we present the BOBW guarantee provided by our Algorithm 2 when the losses are privatized by noise from $\\mathrm{Lap}(\\varepsilon)$ . ", "page_idx": 39}, {"type": "text", "text": "Recall that the privatized loss $\\overline{{\\ell}}_{t,i}=\\ell_{t,i}=\\ell_{t,i}+z$ , where $\\ell_{t,i}$ is bounded in [0, 1] and $z\\sim\\mathrm{Lap}(\\varepsilon)$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\left|\\overline{{\\ell}}_{t,i}\\right|^{2}]=\\mathbb{E}[\\left|\\ell_{t,i}+z\\right|^{2}]\\leqslant2\\mathbb{E}[(\\ell_{t,i})^{2}]+2\\mathbb{E}[z^{2}]\\leqslant2+2(1/\\varepsilon)^{2}=O(1/\\varepsilon^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where in the last inequality we utilize the fact that distribution $\\mathrm{Lap}(\\varepsilon)$ has second moment bounded by $1/\\varepsilon^{2}$ . ", "page_idx": 39}, {"type": "text", "text": "Therefore, plugging $u=O(1/\\varepsilon)$ and $v=2$ into Theorem \u221a2 yields the following high-probability tBheO aBdWve rresagrrieatl  grueagriamnet eaen dw $\\varepsilon$ orteegcrteito inn: $O(\\frac{\\sqrt{K T}\\log(K)(\\log T)^{3}}{\\varepsilon})$ w iwtoh rhsti-gcha sper orbegarbeilti tiyn. $\\begin{array}{r}{O(\\frac{K\\log(K)(\\log T)^{4}}{\\varepsilon^{2}\\Delta\\;.})}\\end{array}$ To the best of our knowledge, this is the first BOBW regret guarantee in MAB with pure LDP protection. We believe that our BOBW results can also be generalized to the case when true losses (to be protected) are heavy-tailed rather than bounded (by properly trimming the true losses before privatization). Related work by Tao et al. (2022) investigated solely the stochastic regime. ", "page_idx": 39}, {"type": "text", "text": "F Auxiliary Lemmas ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma 8. For any fixed $M_{t,i}>0,$ , it holds almost surely that $\\begin{array}{r}{\\left|\\mu_{t,i}-\\mu_{t,i}^{\\prime}\\right|\\leqslant u^{1+v}(M_{t,i})^{-v}}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma 8. Expanding the definitions of $\\mu_{t,i}$ and $\\mu_{t,i}^{\\prime}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\mu_{t,i}-\\mu_{t,i}^{\\prime}\\right|=\\left|\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}]-\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}\\cdot\\mathbb{H}\\{\\ell_{t,i}\\}\\leqslant M_{t,i}]\\right|}\\\\ {=\\left|\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}\\cdot\\mathbb{I}\\{\\ell_{t,i}\\}>M_{t,i}]\\right|}\\\\ {\\leqslant\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}\\cdot\\mathbb{I}\\{\\ell_{t,i}\\}>M_{t,i}]\\right|}\\\\ {\\leqslant\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}]^{1-\\nu}(M_{t,i})^{-\\nu}\\cdot\\mathbb{I}\\{\\ell_{t,i}\\}>M_{t,i}]\\right|}\\\\ {\\leqslant\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}]^{1+\\nu}(M_{t,i})^{-\\nu}]}\\\\ {\\leqslant\\underset{\\ell_{t,i}\\sim P_{t,i}}{\\mathbb{E}}[\\ell_{t,i}]^{1+\\nu}(M_{t,i})^{-\\nu}]}\\\\ {\\leqslant u^{1+\\nu}(M_{t,i})^{-\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma 9 (Adapted from Lemma 19 in Wei & Luo (2018)). In Algorithm 1, for any fixed $\\lambda\\in(0,1)$ , let $n_{i}$ be the number of times the learning rate of arm $i$ changes (and in fact increases) in total, i.e., $\\eta_{T+1,i}=\\kappa^{n_{i}}\\eta_{1,i}=\\kappa^{n_{i}}\\eta.$ . We have $n_{i}\\leqslant\\log_{2}(1/\\lambda)$ and $\\eta_{t,i}\\leqslant e^{\\frac{\\log_{2}(1/\\lambda)}{\\log T}}\\eta,\\forall t\\in[T],i\\in[K]$ . ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma 9. Let $t_{1},\\ldots,t_{n_{i}}$ be the (ordered) rounds when the learning rate of action $i$ changes, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{K}{\\lambda}\\geqslant\\frac{1}{w_{t_{n_{i}},i}}>\\rho_{t_{n_{i}},i}>2\\rho_{t_{n_{i}}-1,i}>\\cdot\\cdot\\cdot>2^{n_{i}-1}\\rho_{t_{1},i}=2^{n_{i}}K.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Clearly, we have $n_{i}\\leqslant\\log_{2}(1/\\lambda)$ and therefore $\\eta_{t,i}\\leqslant\\kappa^{n_{i}}\\eta_{1,i}\\leqslant e^{\\frac{\\log_{2}(1/\\lambda)}{\\log T}}\\eta$ . ", "page_idx": 39}, {"type": "text", "text": "Lemma 10 (Bernstein\u2019s inequality for independent random variables (Vershynin, 2018)). Let $X_{1},\\ldots,X_{T}$ be zero-mean independent random variables such that $\\left|X_{t}\\right|\\leqslant b,\\forall t\\in[T]$ for some fixed constant $b$ and $\\begin{array}{r}{V_{T}=\\sum_{i=1}^{T}\\mathbb{E}\\left[(X_{t})^{2}\\right]}\\end{array}$ . Then, for any $\\zeta>0$ , we have with probability at least $1-\\zeta$ that, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}X_{i}\\leqslant\\sqrt{2V_{T}\\log(1/\\zeta)}+\\frac{2b\\log(1/\\zeta)}{3}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lemma 11 (Freedman\u2019s inequality for martingales, Lemma 3 in Rakhlin et al. (2011)). Let $X_{1},\\ldots,X_{T}$ be a martingale difference sequence adapted to filtration $\\mathcal{F}_{1}\\subset\\cdot\\cdot\\cdot\\subset\\mathcal{F}_{T}$ such that $|X_{t}|\\,\\leqslant\\,b$ almost surely for some fixed constant $b$ and $\\begin{array}{r}{V_{t}\\,=\\,\\sum_{s=1}^{t}\\mathbb{E}\\left[(X_{s})^{2}\\big|\\mathcal{F}_{s}\\right]}\\end{array}$ . Then, for any $\\zeta<1/(e\\log T)$ and $T\\geqslant4$ , we have with probability at least $1-\\zeta$ that for any $t\\bar{\\in}\\left[T\\right]$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}X_{i}\\leqslant2\\operatorname*{max}\\{2\\sqrt{V_{t}},b\\sqrt{\\log(\\log(T)/\\zeta)}\\}\\sqrt{\\log(\\log(T)/\\zeta)}\\leqslant4\\sqrt{V_{t}\\log(\\log(T)/\\zeta)}+2b\\log(\\log(T)/\\zeta).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lemma 12 (Adaptive Freedman\u2019s inequality, Theorem 9 in Zimmert & Lattimore (2022)). Let $X_{1},\\ldots,X_{T}$ be a martingale difference sequence adapted to filtration $\\mathcal{F}_{1}\\subset\\cdots\\subset\\mathcal{F}_{T}$ such that $\\mathbb{E}\\left[X_{t}|\\mathcal{F}_{t}\\right]$ is finite almost surely. Then, for any $\\zeta>0$ , it holds with probability at least $1-\\zeta$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}X_{i}\\leqslant3\\sqrt{V_{T}\\log\\left(\\frac{2\\operatorname*{max}\\{U_{T},\\sqrt{V_{T}}\\}}{\\zeta}\\right)}+2U_{T}\\log\\left(\\frac{2\\operatorname*{max}\\{U_{T},\\sqrt{V_{T}}\\}}{\\zeta}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\begin{array}{r}{V_{T}=\\sum_{i=1}^{T}\\mathbb{E}\\left[(X_{t})^{2}\\middle|\\mathcal{F}_{t}\\right]}\\end{array}$ and $U_{T}=\\operatorname*{max}\\{1,\\operatorname*{max}_{t\\in[T]}X_{t}\\}$ . ", "page_idx": 40}, {"type": "text", "text": "Lemma 13 (Theroem 5.1 of Chen et al. (2018)). Let $R_{1}$ and $R_{2}$ be two distributions on $\\mathcal{X}$ . If for some $\\alpha\\in[0,1]$ it holds that $\\begin{array}{r}{\\mathrm{TV}(R_{1}\\|R_{2})\\leqslant\\frac{\\alpha}{1-\\alpha}}\\end{array}$ , then there exist two distributions on the same probability space $G_{1}$ and $G_{2}$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n(1-\\alpha)R_{1}+G_{1}=(1-\\alpha)R_{2}+G_{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The limitations are discussed in Section 5. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This work focuses on theoretical Machine Learning. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 44}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]